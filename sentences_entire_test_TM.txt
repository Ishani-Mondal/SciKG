sentences1	sentences2	sentences3
Parsing	accuracy	Parsing accuracy using efficient greedy transition systems has improved dramatically in recent years thanks to neural networks.
constituency parsing	O(1) time	We also design the first provably optimal dynamic oracle for constituency parsing, which runs in amortized O(1) time, compared to O(n 3) oracles for standard dependency parsing.
parsing	accuracy	Between the two basic paradigms of parsing, constituency parsing, the subject of this paper, has in general proved to be the more difficult than dependency parsing, both in terms of accuracy and the run time of parsing algorithms.
constituency parsing	accuracy	Between the two basic paradigms of parsing, constituency parsing, the subject of this paper, has in general proved to be the more difficult than dependency parsing, both in terms of accuracy and the run time of parsing algorithms.
dependency parsing	accuracy	Between the two basic paradigms of parsing, constituency parsing, the subject of this paper, has in general proved to be the more difficult than dependency parsing, both in terms of accuracy and the run time of parsing algorithms.
NESTIE	precision	Our experimental study on real-world datasets suggests that NESTIE obtains comparable precision with better minimality and informative-ness than existing approaches.
classification	accuracy	On both datasets, our approach outperforms both LST-M and attention-based LSTM models () in terms of classification accuracy and running speed.
classification	accuracy	Evaluation metric is classification accuracy.
Classification	accuracy	 Table 2: Classification accuracy of different methods on laptop
target extraction	Annotation	They have been used for target extraction, and thus have annotated targets, but no annotation on whether a: Annotation details of the benchmark datasets.
Cross-lingual sentiment prediction	accuracy	 Table 2: Cross-lingual sentiment prediction accuracy of our
dialogue generation	BLEU	Experiments in dialogue generation demonstrate that our approach outperforms previous work with up to a 4 point BLEU improvement.
event detection	precision	For event detection, we use standard precision, recall and F1 metrics.
event detection	recall	For event detection, we use standard precision, recall and F1 metrics.
event detection	F1	For event detection, we use standard precision, recall and F1 metrics.
Semantic role labeling coverage	SRL Ar- guments	 Table 2: Semantic role labeling coverage. We eval- uate both "Predicates over Triggers" and "SRL Ar- guments over Event Arguments". "All" stands for  the combination of Verb-SRL and Nom-SRL. The  evaluation is done on all data.
event detection	F1	 Table 7: Domain Transfer Results. We con- duct the evaluation on TAC-KBP corpus with the  split of newswire (NW) and discussion form (DF)  documents. Here, we choose MSEP-EMD and  MSEP-Coref ESA+AUG+KNOW as the MSEP approach  for event detection and co-reference respectively.  We use SSED and Supervised Base as the supervised  modules for comparison. For event detection, we  compare F1 scores of span plus type match while we  report the average F1 scores for event co-reference.
event detection	F1	 Table 7: Domain Transfer Results. We con- duct the evaluation on TAC-KBP corpus with the  split of newswire (NW) and discussion form (DF)  documents. Here, we choose MSEP-EMD and  MSEP-Coref ESA+AUG+KNOW as the MSEP approach  for event detection and co-reference respectively.  We use SSED and Supervised Base as the supervised  modules for comparison. For event detection, we  compare F1 scores of span plus type match while we  report the average F1 scores for event co-reference.
identifying taxonomic relations	accuracy	Previous works on identifying taxonomic relations are mostly based on statistical and linguistic approaches, but the accuracy of these approaches is far from satisfactory.
SWVP	mistake	We prove the convergence of SWVP for linearly separable training sets, provide mistake and generalization bounds, and show that in the general case these bounds are tighter than those of the CSP special case.
Sentence classification	IMDB	• Experiment I: Sentence classification − IMDB.
machine translation (MT)	accuracy	Manual evaluation is a primary means of interpreting the performance of machine translation (MT) systems and evaluating the accuracy of automatic evaluation metrics.
MT	recall	In addition, MT systems perform substantially worse with user-generated text, such as web forums (Van der, which provide extra motivation to consider alternative translation approaches for higher recall.
translation	similarity	After translation, we compute similarity via scoring the match between the parse of the question text and the parse of the candidate answer, using our finely-tuned IE toolkit [reference removed for anonymization].
Sentiment classification	Precision	 Table 4: Sentiment classification: Precision, Recall, F1 and accuracy from top to down for each domain and each  model
Sentiment classification	Recall	 Table 4: Sentiment classification: Precision, Recall, F1 and accuracy from top to down for each domain and each  model
Sentiment classification	F1	 Table 4: Sentiment classification: Precision, Recall, F1 and accuracy from top to down for each domain and each  model
Sentiment classification	accuracy	 Table 4: Sentiment classification: Precision, Recall, F1 and accuracy from top to down for each domain and each  model
MLL	coverage	Several evaluation criteria typically used in MLL can also be used to measure EDL's ability of distinguishing relevant emotions from irrelevant ones, including hamming loss, one error, coverage, ranking loss, and average precision as suggested by, which are summarized in.2.
MLL	ratio	For the MLL methods, the value of k is set to 8 in ML-KNN, ratio is 0.02 and µ is 2 in ML-RBF.
IRT	accuracy	Our IRT analyses show that different items exhibit varying degrees of difficulty and discrimination power and that high accuracy does not always translate to high scores in relation to human performance.
AMR	accuracy	showed that AMR can significantly improve the accuracy of a biomolecular interaction extraction system compared to only using surface-and syntax-based features.
POS tagging	accuracy	Experiments on POS tagging show that both online and offline pruning can greatly improve the model efficiency with little accuracy loss.
parsing	accuracy	To effectively learn the DMV model for better parsing accuracy, a variety of inductive biases and handcrafted features have been proposed to incorporate prior information into learning.
parsing	accuracy	The correlations between POS tags are automatically captured in the learned POS embeddings and contribute to the improvement of parsing accuracy.
cue identification	F1-score	Previous work have achieved quite success on cue identification (e.g., with F1-score of 86.79 for speculative cue detection in).
spam detection	accuracy	Experimental results on an open Yelp dataset show that our method could effectively enhance the spam detection accuracy compared with the state-of-the-art methods.
spam detection	Illustrated	In summary, this paper makes the following contributions: • It addresses the spam detection issue with a: Illustrated of our method.
detecting restaurant review spam	F1-score	• The method of this paper renders 89.2% F1-score in detecting restaurant review spam which is higher than the F1-score of 86.1% rendered by the method in () (in hotel domain, it's 87.0% vs 84.8%).
sequence classification tasks	accuracy	We show that for sequence classification tasks, incorporating residual connections into recurrent structures yields similar accuracy to Long Short Term Memory (LSTM) RNN with much fewer model parameters.
medical translation task 7	MED	We compare the effect of using different numbers of discount parameters on perplexity using the Finnish (FI), Spanish (ES), German (DE), English (EN) portions of the Europarl v7 (    of news-test 2015 (all denoted as NT) , and ii) extreme using a 24 hour period of streamed Finnish, and Spanish tweets 6 (denoted as TW), and the German and English sections of the patent description of medical translation task 7 (denoted as MED).
SRL	LAS	In particular, we use F 1 for SRL, 0.5 × LAS + 0.5 × U AS for parsing, and 0.5 × F 1 + 0.25 × U AS + 0.25 × LAS for the joint task.
detection	precision	For detection, we report precision, recall and F1 scores in.
detection	recall	For detection, we report precision, recall and F1 scores in.
detection	F1 scores	For detection, we report precision, recall and F1 scores in.
Error detection	preci-	 Table 4: Error detection results on the test set. We report preci-
SVM	F-score	This allows SVM to be learned while optimizing the F-score.
ASR system vocabulary	REF	For example, proper nouns that are not contained in the ASR system vocabulary may breakup into smaller pieces, yielding a difficult problem for the parsing unit (: REF: what can we get at Litanfeeth HYP: what can we get it leaks on feet In this work, we propose a method for joint dependency parsing and disfluency detection that can robustly parse ASR output texts.
Simplification Generation	F-measure	 Table 2: Simplification Generation Results. SimpleScience achieves the highest F-measure with a cosine threshold of 0.4 and a
negation mark	Pearson correlation	Training with the word form of the negation mark is virtually useless, it yields a Pearson correlation of −0.109.
part-ofspeech tagging	accuracy	demonstrate this for part-ofspeech tagging, finding that SAE-trained taggers had disparate accuracy on AAE versus non-AAE tweets.
NER	F 1 score	To evaluate the generalization capability of an NER system, we compute the F 1 score on the unseen entities (Unseen) as well as on all the entities (All) in a test data set.
Improving classifier	accuracy	 Table 2: Improving classifier accuracy via self-training.
Word ordering	BLEU	 Table 1: Word ordering. BLEU Scores of seq2seq, BSO,  constrained BSO, and a vanilla LSTM language model  (from Schmaltz et al, 2016). All experiments above have  K tr = 6.
Machine translation	BLEU	 Table 4: Machine translation experiments on test set; re- sults below middle line are from MIXER model of Ran- zato et al. (2016). SB-∆ indicates sentence BLEU costs  are used in defining ∆. XENT is similar to our seq2seq  model but with a convolutional encoder and simpler at- tention. DAD trains seq2seq with scheduled sampling  (Bengio et al., 2015). BSO, SB-∆ experiments above  have K tr = 6.
predictive	accuracy	3. Experiment results prove that our efforts can significantly improve the predictive accuracy, compared with the existing research.
Binary classification	Area	Binary classification tasks were also measured by Area Under the ROC Curve (AUC).
tagging	accuracy	For tagging, we report accuracy on the tuning set after 50 training epochs.
POS tagging	accuracy	 Table 6: Labeled F1-scores and POS tagging accuracy on the
Translation	BLEU score)	 Table 2: Translation results (BLEU score) for different translation methods in large-scale training data.
SMT	M 2	We find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning out-performs all previously published results for the CoNLL-2014 test set by a large margin (46.37% M 2 over previously 41.75%, by an SMT system with neural features) while being trained on the same, publicly available data.
SBMT	BLEU	Additionally , using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 BLEU, improving the state-of-the-art on low-resource machine translation.
transfer learning	BLEU	We report NMT improvements from transfer learning of 5.6 BLEU on average, and we provide an analysis of why the method works.
IR community	Mean average precision (MAP)	We report three ranking-based measures that are commonly accepted in the IR community: Mean average precision (MAP), which is the official evaluation measure of the task, average recall (AvgRec), and mean reciprocal rank (MRR).
IR community	recall (AvgRec)	We report three ranking-based measures that are commonly accepted in the IR community: Mean average precision (MAP), which is the official evaluation measure of the task, average recall (AvgRec), and mean reciprocal rank (MRR).
IR community	mean reciprocal rank (MRR)	We report three ranking-based measures that are commonly accepted in the IR community: Mean average precision (MAP), which is the official evaluation measure of the task, average recall (AvgRec), and mean reciprocal rank (MRR).
sentiment classification	RM SE	We use two metrics including Accuracy which measures the overall sentiment classification performance and RM SE which measures the divergences between predicted sentiment classes and ground truth classes.
Sentiment classification	Acc.	 Table 2: Sentiment classification results of our model against competitor models on IMDB, Yelp 2014 and  Yelp 2013. Evaluation metrics are classification accuracy (Acc.) and MSE. Models with * use user and  product information as additional features. Best results in each group are in bold.
Sentiment classification	MSE	 Table 2: Sentiment classification results of our model against competitor models on IMDB, Yelp 2014 and  Yelp 2013. Evaluation metrics are classification accuracy (Acc.) and MSE. Models with * use user and  product information as additional features. Best results in each group are in bold.
prediction	regressing	For prediction in, we used the same approach described above to estimate the case parameters a, b, and regressing on amicus brief topics (∆) instead for amicus polarities c p and c r .  shows performance on vote prediction.
VPE detection	F1 score	Our approach yields state-of-the-art results for VPE detection by improving F1 score by over 11%; additionally, we explore an approach to antecedent identification that uses the Margin-Infused-Relaxed-Algorithm, which shows promising results.
Dependency parsing	Reranking	 Table 5: Dependency parsing performance on English, Chinese, and German tasks. The "P?" column indicates the use  of pretrained word embeddings. Reranking/blend indicates that the reranker score is interpolated with the base model's  score. Note that previous works might use different predicted tags for English. We report accuracy without punctuation  for English and Chinese, and with punctuation for German, using the standard evaluation script in each case. We only  consider systems that do not use additional training data. The best overall results are indicated with bold (this was  achieved by the ensemble of greedy stack LSTMs in Chinese and German), while the best non-ensemble model is  denoted with an underline. The  † sign indicates the use of predicted tags for Chinese in the original publication,  although we report accuracy using gold Chinese tags based on private correspondence with the authors.
Dependency parsing	accuracy	 Table 5: Dependency parsing performance on English, Chinese, and German tasks. The "P?" column indicates the use  of pretrained word embeddings. Reranking/blend indicates that the reranker score is interpolated with the base model's  score. Note that previous works might use different predicted tags for English. We report accuracy without punctuation  for English and Chinese, and with punctuation for German, using the standard evaluation script in each case. We only  consider systems that do not use additional training data. The best overall results are indicated with bold (this was  achieved by the ensemble of greedy stack LSTMs in Chinese and German), while the best non-ensemble model is  denoted with an underline. The  † sign indicates the use of predicted tags for Chinese in the original publication,  although we report accuracy using gold Chinese tags based on private correspondence with the authors.
Dependency parsing	accuracy	 Table 5: Dependency parsing performance on English, Chinese, and German tasks. The "P?" column indicates the use  of pretrained word embeddings. Reranking/blend indicates that the reranker score is interpolated with the base model's  score. Note that previous works might use different predicted tags for English. We report accuracy without punctuation  for English and Chinese, and with punctuation for German, using the standard evaluation script in each case. We only  consider systems that do not use additional training data. The best overall results are indicated with bold (this was  achieved by the ensemble of greedy stack LSTMs in Chinese and German), while the best non-ensemble model is  denoted with an underline. The  † sign indicates the use of predicted tags for Chinese in the original publication,  although we report accuracy using gold Chinese tags based on private correspondence with the authors.
LSTM parsing	Fmeasure loss	As another contribution, we present a global LSTM parsing model by adapting an expected Fmeasure loss (.
parser evaluation	accuracy	Previous works on parser evaluation that focused on accuracy and speed have not taken ungrammatical sentences into consideration.
parser evaluation	speed	Previous works on parser evaluation that focused on accuracy and speed have not taken ungrammatical sentences into consideration.
on-and off-topic classification task	F 1-measure	REaCT achieves an accuracy of 92% for the on-and off-topic classification task and an F 1-measure of 72% for the semantic annotation.
SMTL-LLR	accuracy	Training with a large quantity of unlabeled review data in addition to labeled ones, SMTL-LLR improves the performance of MTL-LR, and achieves the best average accuracy of 87.2% across the domains, which is 3.2% better than that of MTRL, and is 4.3% better than TSVM, a semi-supervised single task learning model.
SMTL-LLR	accuracy	plots SMTL-LLR accuracy versus unlabeled data sizes from 0 to 10,000, where 0 corresponds to using only labeled data to build the model, i.e., MTL-LR.
text classification	accuracy	We show that our proposed regularizers are faster than the state-of-the-art ones and still improve text classification accuracy.
SVM based classification	Precision	To prepare for the SVM based classification stage, we split the dataset into a training set DS train , and a testing set DS test . Considering that the data distribution is nearly balanced in terms of true and false instances, DS train was formed by randomly selecting 200 instances from the dataset and the remaining 125 instances were used for DS test .  The performance of each annotator against the majority voting is evaluated in terms of Precision, Recall, Accuracy, and F-measure.
SVM based classification	Recall	To prepare for the SVM based classification stage, we split the dataset into a training set DS train , and a testing set DS test . Considering that the data distribution is nearly balanced in terms of true and false instances, DS train was formed by randomly selecting 200 instances from the dataset and the remaining 125 instances were used for DS test .  The performance of each annotator against the majority voting is evaluated in terms of Precision, Recall, Accuracy, and F-measure.
SVM based classification	Accuracy	To prepare for the SVM based classification stage, we split the dataset into a training set DS train , and a testing set DS test . Considering that the data distribution is nearly balanced in terms of true and false instances, DS train was formed by randomly selecting 200 instances from the dataset and the remaining 125 instances were used for DS test .  The performance of each annotator against the majority voting is evaluated in terms of Precision, Recall, Accuracy, and F-measure.
SVM based classification	F-measure	To prepare for the SVM based classification stage, we split the dataset into a training set DS train , and a testing set DS test . Considering that the data distribution is nearly balanced in terms of true and false instances, DS train was formed by randomly selecting 200 instances from the dataset and the remaining 125 instances were used for DS test .  The performance of each annotator against the majority voting is evaluated in terms of Precision, Recall, Accuracy, and F-measure.
SMT-based English GEC	F 0.5 score	We incorporate this adapted NNJM as a feature in an SMT-based English GEC system and show that adaptation achieves significant F 0.5 score gains on English texts written by L1 Chinese, Russian, and Spanish writers.
adaptation	margin	When the smallersized, high-quality FCE data is used for adaptation the margin of improvement is higher.
SMT-based	F 0.5 score	Our baseline system which is SMT-based, achieves the best F 0.5 score compared to other systems using the SMT approach alone, making it a competitive SMT-based GEC baseline.
SMTbased	F 0.5 score	However, their SMTbased system alone achieves an F 0.5 score of 39.48 only.
keyphrase extractors	F 1 scores	We show that performance of trained keyphrase extractors approximates a classi-fier trained on articles labeled by multiple an-notators, leading to higher average F 1 scores and better rankings of keyphrases.
stacking	recall	The purely performs very poorly by itself; but when combined with stacking gives a boost to recall and thus the overall F 1.
stacking	F 1	The purely performs very poorly by itself; but when combined with stacking gives a boost to recall and thus the overall F 1.
entity discovery	mention CEAF	TEDL provides three different approaches to measuring accuracy: entity discovery, entity linking, and mention CEAF (.
entity linking	mention CEAF	TEDL provides three different approaches to measuring accuracy: entity discovery, entity linking, and mention CEAF (.
machine translation (MT)	BLEU	We evaluate performance using machine translation (MT) metrics ME-TEOR) and BLEU () to compare the machinegenerated descriptions to human ones.
Part-of-speech tagging	accuracy	 Table 1: Part-of-speech tagging with Universal Tags: This table shows test results on 5 languages at different target  runtimes. Each row's best results are in boldface, where ties in accuracy are broken in favor of faster models. Superscript  k indicates that the accuracy is significantly different from the k-CRF (paired permutation test, p < 0.05) and this  superscript is in blue/red if the accuracy is higher/lower than the k-CRF. In all cases, we find a VoCRF (underlined) that  is about as accurate as the 2-CRF (i.e., not significantly less accurate) and far faster, since the 2-CRF has |W| = 4913.
Part-of-speech tagging	accuracy	 Table 1: Part-of-speech tagging with Universal Tags: This table shows test results on 5 languages at different target  runtimes. Each row's best results are in boldface, where ties in accuracy are broken in favor of faster models. Superscript  k indicates that the accuracy is significantly different from the k-CRF (paired permutation test, p < 0.05) and this  superscript is in blue/red if the accuracy is higher/lower than the k-CRF. In all cases, we find a VoCRF (underlined) that  is about as accurate as the 2-CRF (i.e., not significantly less accurate) and far faster, since the 2-CRF has |W| = 4913.
Part-of-speech tagging	accuracy	 Table 1: Part-of-speech tagging with Universal Tags: This table shows test results on 5 languages at different target  runtimes. Each row's best results are in boldface, where ties in accuracy are broken in favor of faster models. Superscript  k indicates that the accuracy is significantly different from the k-CRF (paired permutation test, p < 0.05) and this  superscript is in blue/red if the accuracy is higher/lower than the k-CRF. In all cases, we find a VoCRF (underlined) that  is about as accurate as the 2-CRF (i.e., not significantly less accurate) and far faster, since the 2-CRF has |W| = 4913.
Machine translation	BLEU	Machine translation evaluations show that our constraint significantly outperforms or is comparable to the baseline symmetric constraint () in BLEU on the KFTT Ja-En and IWSLT 2007 Ja-En corpus).
word alignment	AER	We measured the performance of word alignment with AER and F-measure.
word alignment	F-measure	We measured the performance of word alignment with AER and F-measure.
alignment extraction	AER	In the many-to-many alignment extraction, we used the filtering method), where a threshold is optimized on the corresponding AER of the baseline model (i.e., HMM+sym or IBM Model 4+sym) . shows the results of word alignment evaluations 8 , where none denotes that the model has no constraint.
translation	BLEU	We measured translation performance with BLEU ().
prediction	accuracy	While we believe our features can improve prediction accuracy, that is not the primary application of social science research.
slot filling task	F 1-score	In the experiments of a slot filling task, which is an essential component of natural language understanding , with using the standard ATIS corpus , we achieved the state-of-the-art F 1-score of 95.66%.
slot estimation	accuracy	We demonstrate that this approach significantly improves slot estimation accuracy compared to the existing sequential labeling algorithm.
slot estimation	accuracy	The methods are compared in terms of slot estimation accuracy.
Slot estimation	accuracy	 Table 2: Slot estimation accuracy for the DSTC corpus. The
Slot estimation	accuracy	 Table 3: Slot estimation accuracy for the Japanese weather cor-
alignment	accuracy	We also found via crossvalidation that this post-processing improved overall alignment accuracy.
parsing	accuracy	Experiments on English, Chinese, and 12 languages from the CoNLL 2006 shared task show the BiAtt-DP can achieve competitive parsing accuracy with several state-of-the-art parsers.
parsing	accuracy	In this section, we present the parsing accuracy of the proposed BiAtt-DP on 14 languages.
MT (SMT)	length	By contrast, builders of standard statistical MT (SMT) systems must work hard to ensure correct length.
SMT	BLEU	NMT's ability to correctly model length is remarkable for these reasons: • SMT relies on maximum BLEU training to obtain a length ratio that is prized by BLEU, while NMT obtains the same result through generic maximum likelihood training.
SMT	BLEU	NMT's ability to correctly model length is remarkable for these reasons: • SMT relies on maximum BLEU training to obtain a length ratio that is prized by BLEU, while NMT obtains the same result through generic maximum likelihood training.
translation induction	accuracy	For that purpose, we use the translation induction task introduced by, which learns a bilingual mapping on a small dictionary and measures its accuracy on predicting the translation of new words.
MT	BLEU	Previous studies (e.g.) have compared human evaluations of MT to metrics like BLEU and found close correspondence between the two.
MT	BLEU	argued that relatively small differences in BLEU can indicate significant MT quality differences and suggested that human evaluation, the traditional alternative to automated metrics like BLEU, is therefore unnecessarily time-consuming and costly.
parsing	accuracy	non-experts improve parsing accuracy by answering questions automatically generated from the parser's output.
parsing	accuracy	Our experiments for newswire and biomedical text demonstrate improvements to parsing accuracy of 1.7 F1 on the sentences changed by re-parsing, while asking only less than 2 questions per sentence.
parsing	F1	Our experiments for newswire and biomedical text demonstrate improvements to parsing accuracy of 1.7 F1 on the sentences changed by re-parsing, while asking only less than 2 questions per sentence.
relation prediction	Correct	 Table 4: Example results of relation prediction in descending order. Correct predictions are in bold.
article retrieval	accuracy	The model learns to select good actions for both article retrieval and value reconciliation in order to optimize the reward function, which reflects extraction accuracy and includes penalties for extra moves.
TAC KBP 2015 slot filling	F 1 score	When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F 1 score increases markedly from 22.2% to 26.7%.
relation extraction	recall	Existing work on relation extraction (e.g., has been unable to achieve sufficient recall or precision for the results to be usable versus hand-constructed knowledge bases.
relation extraction	precision	Existing work on relation extraction (e.g., has been unable to achieve sufficient recall or precision for the results to be usable versus hand-constructed knowledge bases.
TAC KBP 2015 slot filling evaluation	LR	 Table 5: Model performance on TAC KBP 2015 slot filling evaluation, micro-averaged over queries.  Hop-0 scores are calculated on the simple single-hop slot filling results; hop-1 scores are calculated  on slot filling results chained on systems' hop-0 predictions; hop-all scores are calculated based on the  combination of the two. LR = logistic regression.
relation classification task	precision (Prec)	For relation classification task, which excludes None type from training / testing, we use the classification accuracy (Acc) for evaluation, and for relation extraction task, precision (Prec), recall (Rec) and F1 score () are employed.
relation classification task	recall (Rec)	For relation classification task, which excludes None type from training / testing, we use the classification accuracy (Acc) for evaluation, and for relation extraction task, precision (Prec), recall (Rec) and F1 score () are employed.
relation classification task	F1 score	For relation classification task, which excludes None type from training / testing, we use the classification accuracy (Acc) for evaluation, and for relation extraction task, precision (Prec), recall (Rec) and F1 score () are employed.
relation extraction task	precision (Prec)	For relation classification task, which excludes None type from training / testing, we use the classification accuracy (Acc) for evaluation, and for relation extraction task, precision (Prec), recall (Rec) and F1 score () are employed.
relation extraction task	recall (Rec)	For relation classification task, which excludes None type from training / testing, we use the classification accuracy (Acc) for evaluation, and for relation extraction task, precision (Prec), recall (Rec) and F1 score () are employed.
relation extraction task	F1 score	For relation classification task, which excludes None type from training / testing, we use the classification accuracy (Acc) for evaluation, and for relation extraction task, precision (Prec), recall (Rec) and F1 score () are employed.
WSD	Most Frequent Sense (MFS) baseline	Baseline As a traditional baseline in WSD, we used the Most Frequent Sense (MFS) baseline given by the first sense in WordNet.
POS tagging	accuracy	 Table 7: Absolute gain in POS tagging accuracy  from using MIMICK for 10,000-token datasets (all  tokens for Tamil and Kazakh). Bold denotes sta- tistical significance (McNemar's test,p < 0.01).
parsing	accuracy	Therefore, the results suggest that pre-training the parsing and tagging components can improve the translation accuracy of our proposed model.
translation	accuracy	Therefore, the results suggest that pre-training the parsing and tagging components can improve the translation accuracy of our proposed model.
translation	accuracy	Now that we have observed the effectiveness of pre-training our model, one question arises naturally: how many training samples for parsing and tagging are necessary for improving the translation accuracy?
machine translation tasks	BLEU	Experiments on Chinese-English and German-English machine translation tasks show BLEU improvements by 4.53 and 1.3, respectively.
MSR word analogy test set	accuracy	We evaluate this approach on MSR word analogy test set (Mikolov et al., 2013d) with an accuracy of 85% which is 12% higher than the previous best known system.
word similarity tasks	FOFE-based	Next, we report our experimental results on several popular word similarity tasks, which demonstrate that the proposed FOFE-based approach leads to significantly better performance in these tasks, comparing with the conventional vector space models as well as the popular neural prediction methods, such as word2vec, GloVe and more recent Swivel.
NER	F 1 -score	Two recent, state-of-the-art systems for NER are proposed by and by . Lample et al. report an F 1 -score of 90.94% and Ma and Hovy report an F 1 -score of 91.21%.
NER	F 1 -score	Two recent, state-of-the-art systems for NER are proposed by and by . Lample et al. report an F 1 -score of 90.94% and Ma and Hovy report an F 1 -score of 91.21%.
NER task	F 1	For the NER task, the median difference was ∆F 1 = −0.66%, i.e. the setup with a Softmax classifier achieved on average an F 1 -score of 0.66 percentage points below that of the CRF setup.
NER task	F 1 -score	For the NER task, the median difference was ∆F 1 = −0.66%, i.e. the setup with a Softmax classifier achieved on average an F 1 -score of 0.66 percentage points below that of the CRF setup.
parsing domain adaptation	Accuracy	 Table 3: Results for data selection for part-of-speech tagging and parsing domain adaptation on the  SANCL 2012 shared task dataset (Petrov and McDonald, 2012). POS: Part-of-speech tagging. Pars:  Parsing. POS tagging models: Structured Perceptron (P); Bi-LSTM tagger (B) (Plank et al., 2016). Pars- ing model: Bi-LSTM parser (BIST) (Kiperwasser and Goldberg, 2016). Evaluation metrics: Accuracy  (POS tagging); Labeled Attachment Score (parsing). Best: bold; second-best: underlined.
topic modeling	ADAGRAD	Online topic modeling, i.e., topic modeling with stochastic variational inference, is a powerful and efficient technique for analyzing large datasets, and ADAGRAD is a widely-used technique for tuning learning rates during online gradient optimization.
generative	accuracy	Experiment re-sults on a Yelp 1 review data set show that the features constructed based on the proposed generative model are much more effective than the major features proposed in the existing literature, allowing us to achieve almost 86% accuracy.
sentiment classification	M AE	Specifically, we use accuracy to measure the overall sentiment classification performance, M AE and RM SE to measure the divergences between prediction py and ground truth gy.
sentiment classification	RM SE	Specifically, we use accuracy to measure the overall sentiment classification performance, M AE and RM SE to measure the divergences between prediction py and ground truth gy.
Sentiment classification	accuracy	The classification accuracies are summarization in: Sentiment classification accuracy.
Sentiment classification	accuracy	 Table 2: Sentiment classification accuracy.
Neural Response Generation	Approximate	Neural Response Generation via GAN with an Approximate Embedding Layer *
MWS	F-score	First, the results suggest that using pseudo training and dev datasets to build a MWS model is feasible, based on two evidences: 1) our simple benchmark model can reach a high F-score of 96.07% on the manually annotated test data, which is 1.77% higher than directly aggregating outputs of three SWS models; 2) the P/R/F scores on the pseudo dev data and on the manually labeled test data are quite consistent in general, indicating that it is reliable to use the pseudo dev data for model selection and tuning.
SWS	recall	Third, the SWS aggregation approach achieves the best recall at the price of very low precision on both dev/test data.
SWS	precision	Third, the SWS aggregation approach achieves the best recall at the price of very low precision on both dev/test data.
MWS	F-score	Finally, using bichar embeddings turns out very helpful for MWS, and leads to 0.97 ∼ 1.18% F-score improvement on dev data and 0.62 ∼ 0.85% on test data, which is consistent with the SWS results in.
Size  Evaluation	accuracy	Size  Evaluation: We use accuracy as the evaluation metric for all experiments reported in the paper.
OOV analysis	accuracy	This increase in OOV analysis accuracy is the result of modeling the data on a semantic level, with the embeddings and neural networks, instead of pure lexical approach.
Morphological tagging	absolute increase	 Table 6: Morphological tagging results. The absolute increase and error reduction are of the disam- biguated Bi-LSTM against MADAMIRA.
Morphological tagging	error reduction	 Table 6: Morphological tagging results. The absolute increase and error reduction are of the disam- biguated Bi-LSTM against MADAMIRA.
Morphological tagging	Bi-LSTM	 Table 6: Morphological tagging results. The absolute increase and error reduction are of the disam- biguated Bi-LSTM against MADAMIRA.
derivation	accuracy	Our best neural model for derivation achieves 71.7% accuracy, beating the non-neural baseline by 16.4 points.
CRF  segmentation	Speed	 Table 2: Comparisons between greedy and CRF  segmentation. Speed: tokens per millisecond.
RACE	Len	 Table 3: Statistics of RACE where Len denotes  length and Vocab denotes Vocabulary.
RACE	length	 Table 3: Statistics of RACE where Len denotes  length and Vocab denotes Vocabulary.
RACE	Vocab	 Table 3: Statistics of RACE where Len denotes  length and Vocab denotes Vocabulary.
template retrieval	accuracy	For template retrieval accuracy, Hit@N means the correct template fora problem is included in the top N list returned by our model.
template retrieval	accuracy	 Table 5: Results of template retrieval and final accuracy with different top N templates retrieved.
Number identification	Apply	Our main experimental result is to show a significant improvement over the baseline Algorithm 1 Hybrid model Input: Q: problems in training data; PT : testing problem; θ: pre-defined threshold of similarity Output: Problem solution 1: Get equation templates and number mappings for training problems Q and testing problem PT . 2: Number identification: identify significant numbers 3: Retrieval: choose problem Q 1 from Q that has the maximal Jaccard similarity with PT 4: if J(P T , Q 1 ) > θ then 5: Apply the retrieval model: select equation template T of Q 1 6: else  Datasets: As introduced in Section 3.2, we collected a dataset called Math23K which contains 23161 math word problems labeled with equation templates and answers.
question generation	Multi-Fact Error	In order to better understand the question generation quality, we manually check a set of sampled outputs, and list the main errors in: • Multi-Fact Error (40%).
word matching	BILSTM	 Table 5: Model performance on WIKIQA. +CNT:  word matching features introduced in Yang et al.  (2015). The base QA model is BILSTM. Best re- sults in each group are shown in bold.
AMR parsing parsing	accuracy	However, AMR parsing parsing accuracy is still in the high 60%, as measured by the SMatch score , and a significant improvement is needed in order for it to positively impact a larger number of applications.
AMR parser	F-score	For example, JAMR), the first AMR parser, is able to achieve an F-score of 80% (close to the interannotator agreement of 83) if gold concepts are provided.
parsing	accuracy	Its parsing accuracy drops sharply to 62.3% when the concepts are identified automatically.
RST parsing	O(n 3 ) time complexity	State-of-the-art performance on RST parsing is achieved by cubic-time parsers, with O(n 3 ) time complexity (where n denotes the number of sen-tences in the document).
RST-trees	F1	Predicted RST-trees are typically evaluated by computing F1 against gold standard trees).
RST-style discourse parsing	span	Evaluation metrics for RST-style discourse parsing include: (a) span (S) which measures whether the predicted subtrees match the goldstandard; (b) nucleus (N) which measures whether subtrees have the same nucleus as in the goldstandard and (c) relation (R) which measures whether discourse relations have been identified correctly.
MT	BLEU absolute	Experimental results on German→English news domain and English→Russian e-commerce domain translation tasks show that using phrase-based models in NMT search improves MT quality by up to 2.3% BLEU absolute as compared to a strong NMT baseline.
SMT	minimum error rate training (MERT)	The training corpora consisted of about 1.25 million sentence pairs  We compared our proposed model with two state-of-the-art systems: * Moses: a state-of-the-art phrase-based SMT system () with its default settings, where feature function weights are tuned by the minimum error rate training (MERT) algorithm.
NIST Chinese-English translation task	BLEU	 Table 1: Main experiment results on the NIST Chinese-English translation task. BLEU scores in the  table are case insensitive. Moses and RNNSearch are SMT and NMT baseline system respectively. " †":  significantly better than RNNSearch (p < 0.05); " ‡": significantly better than RNNSearch (p < 0.01).
Translation	BLEU	 Table 3: Translation results for the various models. The first column shows the models; the second  column indicates whether the corresponding experiment uses BPE data. The number of parameters (M  = millions) in each model is given in the third column. The remaining columns are the translation  accuracies for the test sets and development set, evaluated using BLEU scores (%). "↑ / ⇑": indicates  that the hierarchical encoder is significantly better than the vanilla tree-based encoder (p < 0.05/p <  0.01).
MT	accuracy	The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (: Our framework allows us to use a diverse range of signals to learn translations, including incomplete bilingual dictionaries, information from related languages (like Indonesian loan words from Dutch shown here), word embeddings, and even visual similarity cues.
translation	BLEU	This shows how much our method improves translation quality, since corpus-level BLEU correlates better with human judgments than sentence-level BLEU.
NMT	BLEU	Empirical results on the IWSLT English-German/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points.
NMT	BLEU	Empirical results on the IWSLT English-German/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points.
question answering	accuracy	We report question answering accuracy measured using the official evaluation script, which performs some simple normalization of numbers, dates, and strings before comparing predictions and answers.
Semantic parsing	exact  match)	 Table 5: Semantic parsing performance (exact  match) for proposed weakly supervised methods vs  full supervision (completely labeled logical forms)
emoji  prediction task	Parameters	 Table 3: Accuracy of classifiers on the emoji  prediction task. d refers to the dimensionality of  each LSTM layer. Parameters are in millions.
Parsing	accuracy	 Table 4: Parsing accuracy on seven languages. Our model is compared with DMV (Klein and Manning,  2004), Neural DMV (Jiang et al., 2016), and Convex-MST (Grave and Elhadad, 2015)
Lexicalized grammar induction	accuracy	Lexicalized grammar induction aims to incorporate lexical information into the learned grammar to increase its representational power and improve the learning accuracy.
parsing	accuracy	We evaluated our method on thirty languages and found that the jointly trained models surpass their separately trained counterparts in parsing accuracy.
POS tagging	HEM	 Table 3: Semi-supervised learning accuracy of POS tagging on 8 UD languages. HEM means hard-EM,  used as a self-training approach, and OL means only 20% of the labeled data is used and no unlabeled  data is used.
POS tagging	OL	 Table 3: Semi-supervised learning accuracy of POS tagging on 8 UD languages. HEM means hard-EM,  used as a self-training approach, and OL means only 20% of the labeled data is used and no unlabeled  data is used.
parsing	accuracy	This claim has been confirmed in subsequent work: it has been shown that the task of parsing given a gold sequence of supertags can achieve high accuracy (TAG: (, CCG: ().
parsing	TAG	This claim has been confirmed in subsequent work: it has been shown that the task of parsing given a gold sequence of supertags can achieve high accuracy (TAG: (, CCG: ().
classification	accuracy	When classification accuracy does not improve on two consecutive epochs, we end the training.
Supertag Analysis	indi- cates	 Table 1: 1-best Supertag Analysis on Section 00. # indi- cates the number of possible classes in a property. The Prec  and Rec columns show macro-averaging precision and recall.  The Acc columns indicate simple accuracy. For a complete  description of the properties, see Chung et al. (2016).
Supertag Analysis	Prec	 Table 1: 1-best Supertag Analysis on Section 00. # indi- cates the number of possible classes in a property. The Prec  and Rec columns show macro-averaging precision and recall.  The Acc columns indicate simple accuracy. For a complete  description of the properties, see Chung et al. (2016).
Supertag Analysis	precision	 Table 1: 1-best Supertag Analysis on Section 00. # indi- cates the number of possible classes in a property. The Prec  and Rec columns show macro-averaging precision and recall.  The Acc columns indicate simple accuracy. For a complete  description of the properties, see Chung et al. (2016).
Supertag Analysis	recall	 Table 1: 1-best Supertag Analysis on Section 00. # indi- cates the number of possible classes in a property. The Prec  and Rec columns show macro-averaging precision and recall.  The Acc columns indicate simple accuracy. For a complete  description of the properties, see Chung et al. (2016).
Supertag Analysis	Acc	 Table 1: 1-best Supertag Analysis on Section 00. # indi- cates the number of possible classes in a property. The Prec  and Rec columns show macro-averaging precision and recall.  The Acc columns indicate simple accuracy. For a complete  description of the properties, see Chung et al. (2016).
Supertag Analysis	accuracy	 Table 1: 1-best Supertag Analysis on Section 00. # indi- cates the number of possible classes in a property. The Prec  and Rec columns show macro-averaging precision and recall.  The Acc columns indicate simple accuracy. For a complete  description of the properties, see Chung et al. (2016).
Supertagging	B	 Table 5: Supertagging and Parsing Results on Section 23. For  the NN parser, k=5 and B=16 throughout. We trained Syn- taxnet (Andor et al., 2016) with global normalization beam  size 16 using the TensorFlow toolkit.
hypernymy detection	BLESS	We additionally evaluate the Dual Tensor model on four smaller datasets for hypernymy detection: (1) BLESS dataset ( WN-Hy and WN-Me datasets.
hypernymy classification	DUAL-T	Same as in the case of hypernymy classification, DUAL-T significantly outperforms all three baselines, with SINGLE-T outperforming BILIN-PROD.
hypernymy classification	SINGLE-T	Same as in the case of hypernymy classification, DUAL-T significantly outperforms all three baselines, with SINGLE-T outperforming BILIN-PROD.
hypernymy classification	BILIN-PROD	Same as in the case of hypernymy classification, DUAL-T significantly outperforms all three baselines, with SINGLE-T outperforming BILIN-PROD.
relation extraction	precisionrecall curve	In this setting, relation extraction is much harder than the canonical classification problem in two respects: (1) although distant supervision can provide a large amount of data, the training labels are very noisy, and due to the multi-instance framework, the supervision is much weaker; (2) the evaluation metric of relation extraction is often the precisionrecall curve or F1 score, which cannot be represented (and thereby optimized) directly in the loss function.
relation extraction	F1 score	In this setting, relation extraction is much harder than the canonical classification problem in two respects: (1) although distant supervision can provide a large amount of data, the training labels are very noisy, and due to the multi-instance framework, the supervision is much weaker; (2) the evaluation metric of relation extraction is often the precisionrecall curve or F1 score, which cannot be represented (and thereby optimized) directly in the loss function.
relation extraction	precisionrecall curve	In this setting, relation extraction is much harder than the canonical classification problem in two respects: (1) although distant supervision can provide a large amount of data, the training labels are very noisy, and due to the multi-instance framework, the supervision is much weaker; (2) the evaluation metric of relation extraction is often the precisionrecall curve or F1 score, which cannot be represented (and thereby optimized) directly in the loss function.
relation extraction	F1 score	In this setting, relation extraction is much harder than the canonical classification problem in two respects: (1) although distant supervision can provide a large amount of data, the training labels are very noisy, and due to the multi-instance framework, the supervision is much weaker; (2) the evaluation metric of relation extraction is often the precisionrecall curve or F1 score, which cannot be represented (and thereby optimized) directly in the loss function.
Information Retrieval	accuracy	To assess the effectiveness of the different approaches, we employed a set of well-known evaluation metrics inherited from Information Retrieval: accuracy, macro-average F1-score and for the story link detection task DET curves ().
Information Retrieval	F1-score	To assess the effectiveness of the different approaches, we employed a set of well-known evaluation metrics inherited from Information Retrieval: accuracy, macro-average F1-score and for the story link detection task DET curves ().
parsing	BLEU	Goodman applies his insight to parsing algorithms, but this insight has had an even larger impact in machine translation, where the introduction of the fully automatic BLEU metric makes it possible to tune systems using a score correlated with human rankings of MT system performance ().
machine translation	BLEU	Goodman applies his insight to parsing algorithms, but this insight has had an even larger impact in machine translation, where the introduction of the fully automatic BLEU metric makes it possible to tune systems using a score correlated with human rankings of MT system performance ().
neuron removal	Row	 Table 2: Compensating for neuron removal in  the data-bound algorithm. Row (d) corresponds  to row (f) in Tab. 1.
summarization tasks	ROUGE	Automatic Evaluation: Similar to traditional summarization tasks, we use the ROUGE metrics ( to automatically evaluate the quality of peer overview articles against the gold-standard references.
neural generation	BLEU	In the decoder of neural generation, beam search is widely employed to boost the output text quality, often leading to substantial improvement over greedy search (equivalent to beam size 1) in metrics such as BLEU or † Current address: Google Inc., New York, NY, USA.
sequential labeling	DA label	For sequential labeling, the DA label in the current time step is very important (.
DA recognition	accuracy	We evaluate the DA recognition accuracy of our model and compare the result with other latest models.
conversation generation task	BLEU	To evaluate the performance of different methods for the conversation generation task, we leverage BLEU () as the automatic evaluation metric, which is originally designed for machine translation and evaluates the output by using n-gram matching between the output and the reference.
policy learning	R turn = −1	And for policy learning, we set a small per-turn penalty of one to encourage short interactions, i.e. R turn = −1, and a large dialogue success reward of thirty to appeal to successful interactions, i.e. R succ = 30 , and the discount factor γ is set to one.
NLG evaluation	BLEU	The majority of NLG evaluation relies on automatic metrics, such as BLEU.
IE	SPOUSE	For example, IE systems would extract SPOUSE(John, Mary) or MAR-RIED from John and Mary have been married since 1994.
IE	MAR-RIED	For example, IE systems would extract SPOUSE(John, Mary) or MAR-RIED from John and Mary have been married since 1994.
Implicit Supervision	F 1	 Table 3: Implicit Supervision: Semantic Pars- ing. By updating the entity linking and semantic  parsing models jointly, MMRN-JOINT improves over  MMRN-PIPELINE by 5 points in F 1 and outperforms  REINFORCE+ (SP). It also improves the entity  linking result on the WebQSP questions (EL).
Implicit Supervision	REINFORCE+ (SP)	 Table 3: Implicit Supervision: Semantic Pars- ing. By updating the entity linking and semantic  parsing models jointly, MMRN-JOINT improves over  MMRN-PIPELINE by 5 points in F 1 and outperforms  REINFORCE+ (SP). It also improves the entity  linking result on the WebQSP questions (EL).
sentence classification	MR	Classification benchmarks We first study the task of sentence classification on 5 datasets: MR (Pang and), CR (Hu and Liu, 2004), SUBJ (), MPQA () and TREC ().
sentence classification	CR	Classification benchmarks We first study the task of sentence classification on 5 datasets: MR (Pang and), CR (Hu and Liu, 2004), SUBJ (), MPQA () and TREC ().
sentence classification	TREC	Classification benchmarks We first study the task of sentence classification on 5 datasets: MR (Pang and), CR (Hu and Liu, 2004), SUBJ (), MPQA () and TREC ().
paraphrase detection	F1 score	The last column shows results on the task of paraphrase detection, where the evaluation metrics are classification accuracy and F1 score.
paraphrase detection	accuracy	 Table 3: Classification accuracies on several standard benchmarks. The last column shows results on the  task of paraphrase detection, where the evaluation metrics are classification accuracy and F1 score.  † The  first and second block in our results were obtained using the first and second method of considering words  not in the training set, respectively.  ‡ "combine" means concatenating the feature vectors learned from  both the hierarchical model and the composite model.
paraphrase detection	F1 score	 Table 3: Classification accuracies on several standard benchmarks. The last column shows results on the  task of paraphrase detection, where the evaluation metrics are classification accuracy and F1 score.  † The  first and second block in our results were obtained using the first and second method of considering words  not in the training set, respectively.  ‡ "combine" means concatenating the feature vectors learned from  both the hierarchical model and the composite model.
RL-based policy	safety	Most traditional RL-based policy training suf-fers poor initial performance, i.e. causes the safety problem.
MT	BLEU	Despite it being known for sometime now that automatic metrics, such as BLEU (), provide a less than perfect substitute for human assessment), evaluation in MT more often than not still comprises BLEU scores.
Machine Translation (WMT) shared tasks	relative ranking (RR)	In recent Conference on Machine Translation (WMT) shared tasks, for example, manual evaluators complete a relative ranking (RR) of the output of five alternate MT systems, where they must rank the quality of competing translations from best to worst.
Translation	accuracy	 Table 1: Translation accuracy on the English to  Italian dataset of (Dinu et al., 2014).
conversion of the first sentence of the first article of UDHR	PM+2g LM	visualizes the conversion of the first sentence of the first article of UDHR from Afrikaans (afr) to Dutch (dut) using PM+2g LM (4.3 BLEU4, 36.2 BLEU1).
conversion of the first sentence of the first article of UDHR	BLEU4	visualizes the conversion of the first sentence of the first article of UDHR from Afrikaans (afr) to Dutch (dut) using PM+2g LM (4.3 BLEU4, 36.2 BLEU1).
conversion of the first sentence of the first article of UDHR	BLEU1	visualizes the conversion of the first sentence of the first article of UDHR from Afrikaans (afr) to Dutch (dut) using PM+2g LM (4.3 BLEU4, 36.2 BLEU1).
RL-to-IL translation of UDHR text	BLEU4	 Table 3: BLEU scores for RL-to-IL translation of UDHR text. Format is BLEU4/BLEU1. Polish /  Belorussian and Serbian / Bosnian have different orthographies hence copying is not applicable.
RL-to-IL translation of UDHR text	BLEU1	 Table 3: BLEU scores for RL-to-IL translation of UDHR text. Format is BLEU4/BLEU1. Polish /  Belorussian and Serbian / Bosnian have different orthographies hence copying is not applicable.
pairwise classification	error reduction	On the task of pairwise classification, we achieve a 42% error reduction with respect to the state of the art.
SMT tasks	BLEU	We present experiments using simulated bandit feedback for two different SMT tasks, showing improvements of up to 2 BLEU in SMT domain adaptation from deterministically logged bandit feedback.
relation extraction	F-score gain	Experiments show that this framework outperforms state-of-the-art on both relation extraction (16% absolute F-score gain) and slot filling validation for each individual system (up to 8.5% absolute F-score gain).
Scientific Information Extraction (ScienceIE)	TASK	Very recently anew challenge on Scientific Information Extraction (ScienceIE) ( ) 1 provides a dataset consisting of 500 scientific paragraphs with keyphrase annotations for three categories: TASK, PROCESS, MATERIAL across three scientific domains, Computer Science (CS), Material Science (MS), and Physics (Phy), as in.
Scientific Information Extraction (ScienceIE)	MATERIAL	Very recently anew challenge on Scientific Information Extraction (ScienceIE) ( ) 1 provides a dataset consisting of 500 scientific paragraphs with keyphrase annotations for three categories: TASK, PROCESS, MATERIAL across three scientific domains, Computer Science (CS), Material Science (MS), and Physics (Phy), as in.
semantic parsing'	TASK	For example, 'semantic parsing' can be labeled as a TASK in one article and as a PROCESS in another.
Information Extraction (IE)	accuracy	() combine search and Information Extraction (IE) using Reinforcement Learning (RL), with the goal of selecting good actions while staying within resource constraints, but don't optimze for extraction accuracy.
speech recognition	length normalized edit distance	The error rates area commonly used metric in speech recognition and machine translation evaluation and can also be referred to as length normalized edit distance.
machine translation evaluation	length normalized edit distance	The error rates area commonly used metric in speech recognition and machine translation evaluation and can also be referred to as length normalized edit distance.
outlier detection	recall	In most outlier detection applications, people are more concerned with recall.
inference generation task	BLEU	Also, we tested the neural reasoner on the inference generation task using the BLEU score.
SMT	F 0.5	Compared to just doing SMT with CAMB16 SMT , re-ranking improves F 0.5 from 52.90 to 55.60 on FCE (performance increases further even though CAMB16 SMT 's training set includes a large set of FCE data), from 37.33 to 42.44 on CoNLL, and from 52.44 to 54.66 on JFLEG.
machine translation	accuracy	Attentional sequence-to-sequence models have become the new standard for machine translation over the last two years, and with the unprecedented improvements in translation accuracy comes anew set of technical challenges.
NIST Chinese-to-English translation task	BLEU	Empirical results on NIST Chinese-to-English translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system.
MIN	F 1 score	To better reveal the capability of the proposed MIN, we train 5 models with the same group of hyper-parameters and report the average F 1 score over the testing set.
metaphor identification	accuracy	We then apply NLP algorithms for metaphor identification and sentiment analysis to automatically generate features fora classifier that, with high accuracy, can predict which patients will develop schizophrenia and which patients would currently be diagnosed with schizophrenia by psychiatrists.
sentiment analysis	accuracy	We then apply NLP algorithms for metaphor identification and sentiment analysis to automatically generate features fora classifier that, with high accuracy, can predict which patients will develop schizophrenia and which patients would currently be diagnosed with schizophrenia by psychiatrists.
semantic parsing	YAGO	This is usually done through semantic parsing: translating the utterance to a formal query in a language such as SPARQL, and executing this query over a KB like) or YAGO ( to return one or more answer entities.
WSD	accuracy	WSD performance is measured using the accuracy with respect to the sentences labeled with the direct hypernyms (Hypers) or an extended set of hypernym including hypernyms of hypernyms (Hy-11 Most of the nouns come from the TWSI (Biemann, 2012) dataset, while the remaining nouns were manually selected.  perHypers).
statistical machine translation (SMT)	accuracy	In statistical machine translation (SMT), the optimization of the system parameters to maximize translation accuracy is now a fundamental part of virtually all modern systems.
translation	accuracy	However, over approximately the past decade, huge gains in translation accuracy have been achieved (), and commercial systems deployed for hundreds of language pairs are being used by hundreds of millions of users.
SMT	accuracy	The second major advance in SMT is the discriminative training framework proposed by and , who propose log-linear models for MT, optimized to maximize either the probability of getting the correct sentence from a k-best list of candidates, or to directly achieve the highest accuracy over the entire corpus.
MT	accuracy	The second major advance in SMT is the discriminative training framework proposed by and , who propose log-linear models for MT, optimized to maximize either the probability of getting the correct sentence from a k-best list of candidates, or to directly achieve the highest accuracy over the entire corpus.
MT	accuracy	In addition, the evaluation of MT accuracy is not straightforward, with automatic evaluation measures for MT still being researched to this day.
MT optimization	BLEU	We discuss the role of evaluation in MT optimization more completely in Section 8.3. 2.5.1 BLEU.
MT	BLEU	In the entirety of this article, we have assumed that optimization for MT aims to reduce MT error defined using an evaluation measure, generally BLEU.
MT	BLEU	In the entirety of this article, we have assumed that optimization for MT aims to reduce MT error defined using an evaluation measure, generally BLEU.
MT	BLEU	However, as mentioned in Section 2.5, evaluation of MT is an active research field, and there are many alternatives in addition to BLEU.
compound splitting	accuracy	For example, if an application requires high-accuracy compound splitting, one could choose to apply a model with a good compound-splitting capability even if its affix accuracy does not reach state of the art.
SMT	MERT	 Table 4  BLEU, WER, and KSMR results for the XRCE test corpora using conventional (batch learning  without retraining) and online SMT systems. Both systems used MERT to adjust log-linear  weights. The average online learning time (LT) in seconds is shown for the online system.
SMT	MERT	 Table 6  BLEU, WER, and KSMR results for the Europarl test corpora using conventional (batch learning  without retraining) and online SMT systems. Log-linear weights were adjusted via MERT. The  average online learning time (LT) in seconds is shown for the online system.
SMT	recall	That is, humans usually like high precision, whereas what the downstream SMT system really needs should be high recall.
SMT	BLEU	 Table 9  Improving Macedonian-English SMT by adapting Bulgarian to Macedonian. The BLEU scores  that are significantly better (p < 0.01) than BG2EN and MK2EN are in bold and underlined,  respectively. The last line shows system combination results using MEMT.
SMT	BG2EN	 Table 9  Improving Macedonian-English SMT by adapting Bulgarian to Macedonian. The BLEU scores  that are significantly better (p < 0.01) than BG2EN and MK2EN are in bold and underlined,  respectively. The last line shows system combination results using MEMT.
MT	BLEU	 Table 8  Translation experiment results for different language pairs on the Twitter data. Each column  shows the MT results for a given source language into English. The Out-of-domain size and  In-domain size rows represent the number of sentence pairs in the in-domain and out-of-domain  training sets. The Out-of-domain and +In-domain rows show the BLEU scores for a set-up where  only the out-of-domain data was used and the same set-up after adding the in-domain data set,  respectively.
POS tagging	accuracy	We implement a discriminative sequential classification model for POS tagging that achieves state-of-the-art accuracy.
stacking	error	Cluster-based features and the stacking model result in a relative error reduction of 18% in terms of the word classification accuracy.
POS tagging	accuracy	To improve POS tagging efficiency without loss of accuracy, we explore unlabeled data to transfer the predictive power of complex, inefficient models to simple, efficient models.
Tagging	F1	 Table 5  Tagging F1 scores relative to POS types.
Tagging	F1	 Table 10  Tagging F1 scores of relative to word classes.
CFG recognition	O	With current state-of-the-art results in matrix multiplication, this means that CFG recognition can be done with an asymptotic complexity of O(n 2.38 ).
inversion transduction grammars	O	In addition, we show that inversion transduction grammars (ITGs; Wu 1997) can be parsed in time O(nM(n 2 )) = O(n 5.76 ), improving the best asymptotic complexity previously known for ITGs.
coreference resolution	F-measure	Think, for example, of coreference resolution where a combined F-measure of 60% is considered state-of-the-art.
readability prediction tasks	RMSE	 Table 8  Results of the optimization experiments on the Dutch automatic and gold-standard data sets for  our two readability prediction tasks running 10-fold cross validation experiments. The  regression task is evaluated with RMSE and the classification task with accuracy. Boldface  represents the best individual results.
readability prediction tasks	accuracy	 Table 8  Results of the optimization experiments on the Dutch automatic and gold-standard data sets for  our two readability prediction tasks running 10-fold cross validation experiments. The  regression task is evaluated with RMSE and the classification task with accuracy. Boldface  represents the best individual results.
syntactic parsing	accuracy	In the domain of syntactic parsing, a call has been made for "grammatical construction-focused" parser evaluation), focusing on individual, often challenging syntactic structures, in order to tease out parser performance in these areas from overall accuracy scores.
MRR evaluation	RELPRON	The MRR evaluation is also another use of RELPRON that can be exploited in future work.
WSD	F1	 Table 6  Comparison with state-of-the-art algorithms on WSD and entity linking. The results are  provided as F1 for S13 and as accuracy for KORE50. The first result with a statistically significant  difference from the best (bold result) is marked with * (χ 2 , p < 0.05).
WSD	accuracy	 Table 6  Comparison with state-of-the-art algorithms on WSD and entity linking. The results are  provided as F1 for S13 and as accuracy for KORE50. The first result with a statistically significant  difference from the best (bold result) is marked with * (χ 2 , p < 0.05).
entity linking	F1	 Table 6  Comparison with state-of-the-art algorithms on WSD and entity linking. The results are  provided as F1 for S13 and as accuracy for KORE50. The first result with a statistically significant  difference from the best (bold result) is marked with * (χ 2 , p < 0.05).
entity linking	accuracy	 Table 6  Comparison with state-of-the-art algorithms on WSD and entity linking. The results are  provided as F1 for S13 and as accuracy for KORE50. The first result with a statistically significant  difference from the best (bold result) is marked with * (χ 2 , p < 0.05).
transliteration mining	precision	 Table 3  Results of our unsupervised, semi-supervised, and supervised transliteration mining systems  trained on the cross-product list and using the unigram, bigram, and trigram models for  transliteration and non-transliteration. The bolded values show the best precision, recall, and  F-measure for each language pair.
transliteration mining	recall	 Table 3  Results of our unsupervised, semi-supervised, and supervised transliteration mining systems  trained on the cross-product list and using the unigram, bigram, and trigram models for  transliteration and non-transliteration. The bolded values show the best precision, recall, and  F-measure for each language pair.
transliteration mining	F-measure	 Table 3  Results of our unsupervised, semi-supervised, and supervised transliteration mining systems  trained on the cross-product list and using the unigram, bigram, and trigram models for  transliteration and non-transliteration. The bolded values show the best precision, recall, and  F-measure for each language pair.
Transliteration mining	FP	 Table 8  Transliteration mining results of the heuristic-based system SJD and the unsupervised unigram  system OUR trained and tested on the word-aligned list of the English/Hindi and English/  Arabic parallel corpus. TP, FN, TN, and FP represent true positive, false negative, true negative,  and false positive, respectively.
recognition of star shapes and labels	overspec.S	Analogously to the recognition of star shapes and labels in Experiment 1, we assume that recognizing whether a character is a letter is easier than recognizing whether it is a consonant, and we will make use of three experimental conditions min, overspec.G, and overspec.S in.
LCFRS parsing	FA	For probabilistic LCFRS parsing we use two off-the-shelf systems: If the induced grammar's first component is equivalent to a FA, then we use the OpenFST () framework with the Python bindings of.
SMT	accuracy	It is natural then to expect that incorporating syntactic structures into SMT models would lead to improved MT accuracy.
MT	accuracy	It is natural then to expect that incorporating syntactic structures into SMT models would lead to improved MT accuracy.
translation divergences	accuracy	The hope is that more abstract semantic representations can better address translation divergences than syntactic structures, and this advantage will hopefully offset the potential harm caused by the expected drop in the accuracy of semantic analyzers that are more difficult to develop than syntactic parsers.
WSD	accuracy	 Table 4  WSD and Entity Linking accuracy for different feature sets and systems. Best result in each  column in bold. Results of development sets are italic. Results significantly worse than the best  (bold) result in each column are marked  † for α = 0.05 and  ‡ for α = 0.10 (one-tailed Z-test).
MT evaluation	BLEU	Group III contains other important individual evaluation metrics that are commonly used in MT evaluation: BLEU (, NIST (Doddington 2002), ROUGE (Lin 2004), and TER ().
MT evaluation	ROUGE	Group III contains other important individual evaluation metrics that are commonly used in MT evaluation: BLEU (, NIST (Doddington 2002), ROUGE (Lin 2004), and TER ().
MT evaluation	TER	Group III contains other important individual evaluation metrics that are commonly used in MT evaluation: BLEU (, NIST (Doddington 2002), ROUGE (Lin 2004), and TER ().
MT evaluation	DISCOTK	In this section we show how the simple combination of DR-based metrics with a selection of other existing strong MT evaluation metrics can lead to a very competitive evaluation metric, DISCOTK party (), which we presented at the metrics task of WMT14.
predicting the representations of the visual scene corresponding to an input sentence	TEXTUAL	As a case study, we use a standard standalone language model, and a multi-task gated recurrent network architecture consisting of two parallel pathways with shared word embeddings: The VISUAL pathway is trained on predicting the representations of the visual scene corresponding to an input sentence, and the TEXTUAL pathway is trained to predict the next word in the same sentence.
LE directionality task	precision	 Table 14  Results in the ungraded LE directionality task (precision) using a subset of 940 HyperLex pairs  converted to the ungraded directionality data set. Graded LE results (Spearman's ρ correlation) on  the same subset are also provided for comparison purposes, using the best model configurations  from
Counting crossing brackets	accuracy	Counting crossing brackets is not necessary because this number is directly correlated to the accuracy on the spans, whereas in syntactic parsing this measure is necessary to mitigate the effect of missing spans (empty categories, for instance) on pure precision and recall.
Counting crossing brackets	precision	Counting crossing brackets is not necessary because this number is directly correlated to the accuracy on the spans, whereas in syntactic parsing this measure is necessary to mitigate the effect of missing spans (empty categories, for instance) on pure precision and recall.
Counting crossing brackets	recall	Counting crossing brackets is not necessary because this number is directly correlated to the accuracy on the spans, whereas in syntactic parsing this measure is necessary to mitigate the effect of missing spans (empty categories, for instance) on pure precision and recall.
MT	BLEU	Overall, the evidence supports using BLEU for diagnostic evaluation of MT systems (which is what it was originally proposed for), but does not support using BLEU outside of MT, for evaluation of individual texts, or for scientific hypothesis testing.
classification	accuracy	In the same manner as many previous NLI studies and also the NLI 2013 shared task, we report our results as classification accuracy under k-fold cross-validation, with k = 10.
information extraction (IE)	F 1 scores	Though information extraction (IE) research has more than a 25-year history, F 1 scores remain low.
irony detection	F 1	This observation is inline with the findings of, who report irony detection scores between F 1 = 14% and F 1 = 47% when using merely sentiment lexicons.
irony detection	F 1	This observation is inline with the findings of, who report irony detection scores between F 1 = 14% and F 1 = 47% when using merely sentiment lexicons.
irony detection	F 1 score	In particular, combining lexical with semantic and syntactic features seems to work well for irony detection, yielding atop F 1 score of 70.11%.
Sentiment analysis	accuracy	 Table 12  Sentiment analysis accuracy based on a Twitter crawl using content word targets.
Sentiment analysis	accuracy	 Table 14  Sentiment analysis accuracy based on a Twitter crawl using dependency heads as queries.
Sentiment analysis	accuracy	 Table 16  Sentiment analysis accuracy based on a Twitter crawl using dependency heads as queries.
word prediction	Proportion	 Table 7  Results of the test on word prediction, based on 1,000 random samples for each subset of the  data. The column Proportion reflects the different proportions of the data that was deleted  during the experiments. Patterns refers to the average number of correspondence patterns  inferred in each trial, and Reg. Patterns points to the proportion of alignment sites covered by  patterns recurring at least twice.
word prediction	Reg. Patterns	 Table 7  Results of the test on word prediction, based on 1,000 random samples for each subset of the  data. The column Proportion reflects the different proportions of the data that was deleted  during the experiments. Patterns refers to the average number of correspondence patterns  inferred in each trial, and Reg. Patterns points to the proportion of alignment sites covered by  patterns recurring at least twice.
response selection	matching degree	A key step in response selection is measuring matching degree between an input and response candidates.
entity prediction task	Filtered	Just as in the entity prediction task, we use two setting protocols, "Raw" and "Filtered", and evaluate on mean rank, MRR and Hits@10.
entity prediction task	MRR	Just as in the entity prediction task, we use two setting protocols, "Raw" and "Filtered", and evaluate on mean rank, MRR and Hits@10.
classification	accuracy	These systems are implemented using  The classification accuracy is reported in Table 2.
classification	accuracy	It maybe noted that the addition of gaze features may seem to bring meager improvements in the classification accuracy but the improvements are consistent across datasets and several classifiers.
translation prediction (ITP)	accuracy	The rationale behind this interactive translation prediction (ITP) approach is to combine the accuracy provided by the human expert with the efficiency of the MT system in contrast to decoupled post-editing (PE).
MT	TER	In addition to bean MT quality metric, TER can also be seen as a human-effort measure in PE scenarios.
argument recognition of explicit discourse relations	Arg2	From the table, we observe: • For argument recognition of explicit discourse relations, the performance of Arg2 is much better than that of Arg1 on all the three datasets.
argument recognition	Arg2	From the table, we observe: • For argument recognition, the performance of Arg2 is much better than that of Arg1 on the development and test datasets.
sense classification	Arg1	• With respect to explicit discourse relations, the sense classification works almost perfectly on development data (e.g., almost no performance gap from Arg1 & Arg2 to Overall.
sense classification	Arg2	• With respect to explicit discourse relations, the sense classification works almost perfectly on development data (e.g., almost no performance gap from Arg1 & Arg2 to Overall.
SVM classifier	precisions	Because of the serious imbalance and small quantity of training samples, the SVM classifier gets a poor classification precisions.
agreement prediction	accuracy	When a limited amount of CCG training data was available, joint training on agreement prediction led to improved supertagging accuracy.
predicting right vs. wrong endings	length	 Table 5: The top 5 most heavily weighted features for predicting right vs. wrong endings (5a) and original  vs. new (right) endings (5b). length is the sentence length feature (see Section 4).
IR	justification ranking	We show that with this end-to-end approach we are able to significantly improve upon a strong IR baseline in both justification ranking (+9% rated highly relevant) and answer selection (+6% P@1).
IR	P@1	We show that with this end-to-end approach we are able to significantly improve upon a strong IR baseline in both justification ranking (+9% rated highly relevant) and answer selection (+6% P@1).
analogy completion task	accuracy	 Table 1: Results for the analogy completion task  (accuracy). Reg-li-* and Reg-qu-* are our models  with a linear and quadratic kernel.
Vanilla setup	accuracy	 Table 2. Vanilla setup accuracy in paradigmatic and  syntagmatic tasks with different size training corpuses.
Sentiment classification	indi- cates t-test significance	 Table 3: Sentiment classification results on IMDB  data set (Maas et al., 2011). Bold indicates the  highest score for each embedding type. * indi- cates t-test significance at p < 0.05 level when  compared with the baseline.
MERT weight optimization	accuracy	As the objective for the MERT weight optimization we use accuracy on the development set.
summarisation	AbstractROUGE	show empirically that this improves summarisation performance • Taking inspiration from previous work in summarising scientific literature (, we introduce a metric we use as a feature, AbstractROUGE, which can be used to extract summaries by exploiting the abstract of a paper • We benchmark several neural as well traditional summarisation methods on the dataset and use simple features to model the global context of a summary statement, which contribute most to the overall score • We compare our best performing system to several well-established baseline methods, some of which use more elaborate methods to model the global context than we do, and show that our best performing model outperforms them on this extractive summarisation
summarising scientific literature	AbstractROUGE	show empirically that this improves summarisation performance • Taking inspiration from previous work in summarising scientific literature (, we introduce a metric we use as a feature, AbstractROUGE, which can be used to extract summaries by exploiting the abstract of a paper • We benchmark several neural as well traditional summarisation methods on the dataset and use simple features to model the global context of a summary statement, which contribute most to the overall score • We compare our best performing system to several well-established baseline methods, some of which use more elaborate methods to model the global context than we do, and show that our best performing model outperforms them on this extractive summarisation
entity linking	F1 score	Our entity linking model significantly outperforms the previous work, showing the F1 score of 86.76% and the accuracy of 95.30% for character identification.
entity linking	accuracy	Our entity linking model significantly outperforms the previous work, showing the F1 score of 86.76% and the accuracy of 95.30% for character identification.
Information Retrieval	mean average precision (MAP)	We show three ranking-oriented evaluation measures that are standard in the field of Information Retrieval: mean average precision (MAP), mean reciprocal rank (MRR), and average recall (AvgRec).
Information Retrieval	mean reciprocal rank (MRR)	We show three ranking-oriented evaluation measures that are standard in the field of Information Retrieval: mean average precision (MAP), mean reciprocal rank (MRR), and average recall (AvgRec).
Information Retrieval	recall (AvgRec)	We show three ranking-oriented evaluation measures that are standard in the field of Information Retrieval: mean average precision (MAP), mean reciprocal rank (MRR), and average recall (AvgRec).
information compression	repetition	Accounts range from information compression to a diachronically visible hypercorrection by listeners who misperceive the signal and make the assumption that repetition is unlikely.
SID	PID	While SID performs better on the normative dataset, adding PID leads to a small but significant improvement (+1.7 F-score).
SID	F-score	While SID performs better on the normative dataset, adding PID leads to a small but significant improvement (+1.7 F-score).
stochastic gradient descent (SGD) training	momentum	AE-SCL and AE-SCL-SR: For the stochastic gradient descent (SGD) training algorithm we set the learning rate to 0.1, momentum to 0.9 and weightdecay regularization to 10 −5 . The number of pivots was chosen among {100, 200, . .
Sentiment classification	accuracy	 Table 1: Sentiment classification accuracy for the Blitzer et al. (2007) task (top tables), and for adaptation  from the Blitzer's product review domains to the Blog domain (bottom table). Test-All presents average  results across setups. Statistical significance (with the McNemar paired test for labeling disagreements  (
tagging	accuracy	We report tagging accuracy on two data sets: the Penn Arabic Treebank (PATB) data set and the Arabic Universal Dependencies Treebank (UD Arabic) data set.
dependency parsing	labeled attachment score (LAS)	The standard evaluation metric of dependency parsing is the labeled attachment score (LAS), i.e., the percentage of nodes with correctly assigned reference to parent node, including the label (type) of the relation.
parsing	LAS F1 score	Though relying on baseline tokeniz-ers and focusing only on parsing, our system ranked second in the official end-to-end evaluation with a macro-average of 75.00 LAS F1 score over 81 test treebanks.
parsing	accuracy	It turns out that, although the UDPipe baseline is a strong one, considerable parsing accuracy improvements can be gained by improving the preprocessing steps.
parsing	accuracy	It has also been demonstrated that, with current machine learning approaches, parsing accuracy improves when using multilingual word embeddings (i.e. word embeddings inferred from corpora in different languages) even for resource-rich languages.
parsing surprise  languages	LAS scores	 Table 2: Parent models used for parsing surprise  languages and LAS scores obtained after pre-train  and finetuning.
parsing	accuracy	The overall effect of search-based oracle with various transition systems on parsing accuracy is.
dependency parsing	accuracy	Some attempts have been made to avoid using POS tags during dependency parsing , however, these approaches still additionally use the automatic POS tags to achieve the best accuracy.
dependency parsing	labeled attachment score (LAS)	2 For dependency parsing, the evaluation metric is the labeled attachment score (LAS).
UD parsing	PUD	 Table 2: Official macro-averaged LAS F1 scores  of MQuni and baselines from the CoNLL 2017  shared task on UD parsing (Zeman et al., 2017):  http://universaldependencies.org/  conll17/results-las.html. "All" refers  to the averaged score over all 81 test sets, which is  used as the main metric for ranking participating  systems. Big: the averaged score over 55/81 test  sets whose training treebanks are big and have  development data available. PUD: the averaged  score over 14/81 test sets that are additional  parallel ones, produced separately and their  domain may be different from their training data.  Sma.: the averaged score over 8/81 test sets  whose training treebanks are small, i.e., they lack  development data and some of them have very  little training data. Sur.: the averaged score over  4/81 remaining test sets for surprise languages.  Here the subscript denotes the official rank out of  33 participating systems. R -S is the system rank  where the 4 surprise language test sets are not  taken into account.
sentence splitting	F1 score	For sentence splitting, also based on DT, we obtained an average F1 score of 87.52 versus the top score of 89.10.
Multilingual Parsing from Raw Text	LAS F1 score	In the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, the official results show that the macro-averaged LAS F1 score of our system Mengest is 61.33%.
SOWE	FOUN-based	We observe that SOWE outperform FOUN-based methods in two out of four cases.
word similarity task	FOUN	 Table 4: Performance on word similarity task using text8 dataset. "ideal" means the matrix factorization  method proposed in Levy and Goldberg (2014). FOUN and FOUN+annealing are the baseline that we  are comparing with.
word similarity task	FOUN	 Table 4: Performance on word similarity task using text8 dataset. "ideal" means the matrix factorization  method proposed in Levy and Goldberg (2014). FOUN and FOUN+annealing are the baseline that we  are comparing with.
word analogy task	FOUN	 Table 6: Performance on word analogy task using existing embedding results and text8 dataset with  different folds of splits. In the row "Ideal", we use the well-trained word embedding results downloaded  from Internet. FOUN and FOUN+annealing are the baseline that we are comparing with.
word analogy task	FOUN	 Table 6: Performance on word analogy task using existing embedding results and text8 dataset with  different folds of splits. In the row "Ideal", we use the well-trained word embedding results downloaded  from Internet. FOUN and FOUN+annealing are the baseline that we are comparing with.
tokenization	accuracy	The tokenization and PoS tagging are performed using the publicly available Hindi Shallow Parser  the performance of CMQG in terms of accuracy, BLEU () and ROUGE) score.
tokenization	BLEU	The tokenization and PoS tagging are performed using the publicly available Hindi Shallow Parser  the performance of CMQG in terms of accuracy, BLEU () and ROUGE) score.
tokenization	ROUGE) score	The tokenization and PoS tagging are performed using the publicly available Hindi Shallow Parser  the performance of CMQG in terms of accuracy, BLEU () and ROUGE) score.
classification	accuracy	The evaluation metric is classification accuracy.
Generalized Procrustes Analysis (GPA)	PA	Contributions We show that Generalized Procrustes Analysis (GPA), a method that maps two vector spaces into a third, latent space, is superior to PA for BDI, e.g., improving the state-of-the-art on the widely used EnglishItalian dataset () from a P@1 score of 66.2% to 67.6%.
Generalized Procrustes Analysis (GPA)	P@1 score	Contributions We show that Generalized Procrustes Analysis (GPA), a method that maps two vector spaces into a third, latent space, is superior to PA for BDI, e.g., improving the state-of-the-art on the widely used EnglishItalian dataset () from a P@1 score of 66.2% to 67.6%.
Bilingual dictionary induction	PA	 Table 2: Bilingual dictionary induction performance, measured in P@k, of PA and GPA across five language pairs.
TLS	ROUGE	Automatic evaluation of TLS is done by ROUGE).
Planning	re	Surprisingly, Planning gets the worst re-
word analogy	accuracy	More concretely, word analogy measures the accuracy in answering questions like "what is the word that is similar to France in the same sense as Berlin is similar to Germany?"
Sentence classification	F 1	 Table 2: Sentence classification results. P(recision), R(ecall) and F 1 . Averages over 10 random seeds. The best  average F 1 score per task is shown in bold.
Sentence classification	F 1 score	 Table 2: Sentence classification results. P(recision), R(ecall) and F 1 . Averages over 10 random seeds. The best  average F 1 score per task is shown in bold.
POS	accuracy	 Table 3: Impact of vocabulary sorting on POS accuracy with sparse embeddings: up vs. down (most fre- quent words get longest vs. shortest embeddings, resp.) or not sorted, for different embedding densities  δ E .
Morphological tagging	accuracies	 Table 3: Morphological tagging accuracies on UDv2.1 test sets for MarMot (MMT) and MC baselines as well as for MCML,
Sequence Labeling Tasks	accuracy	 Table 2: Performance on Sequence Labeling Tasks. % accuracy shown for PTB, and % F-measure otherwise
Sequence Labeling Tasks	F-measure	 Table 2: Performance on Sequence Labeling Tasks. % accuracy shown for PTB, and % F-measure otherwise
NER task	accuracy	shows the F1 scores for NER task and accuracy scores for TC task.
parsing task	LAS	Upon the official evaluation on 82 test sets, our system (SLT-Interactions) obtained the 12 th position in the parsing task and achieved an average improvement of 4.18% in LAS over the UDPipe baseline.
parsing	BLEX	For a full description of the metrics, see (Zeman et al., 2018) or the shared task website; here, we only note that while LAS only evaluates parsing accuracy, MLAS also includes evaluation of tagging (UPOS and morphological features), while BLEX also includes lemmatization.
POS tagging	LAS	The prototype utilizes an artificial neu-ral network with a single joint model for POS tagging, lemmatization and dependency parsing, and is trained only using the CoNLL-U training data and pretrained word embeddings, contrary to both systems surpassing the prototype in the LAS and BLEX ranking in the shared task.
POS tagging	BLEX	The prototype utilizes an artificial neu-ral network with a single joint model for POS tagging, lemmatization and dependency parsing, and is trained only using the CoNLL-U training data and pretrained word embeddings, contrary to both systems surpassing the prototype in the LAS and BLEX ranking in the shared task.
dependency parsing	LAS	The prototype utilizes an artificial neu-ral network with a single joint model for POS tagging, lemmatization and dependency parsing, and is trained only using the CoNLL-U training data and pretrained word embeddings, contrary to both systems surpassing the prototype in the LAS and BLEX ranking in the shared task.
dependency parsing	BLEX	The prototype utilizes an artificial neu-ral network with a single joint model for POS tagging, lemmatization and dependency parsing, and is trained only using the CoNLL-U training data and pretrained word embeddings, contrary to both systems surpassing the prototype in the LAS and BLEX ranking in the shared task.
segmentation	F1 score	Finally, we note that the segmentation improvements outlined in Section 4.1 resulted in third average F1 score of our system.: Statistics of the model sizes.
parse input morphologically disambiguated	LAS	In the task we used yap's stan-dalone dependency parser to parse input morphologically disambiguated by UD-Pipe, and obtained the official score of 58.35 LAS.
Event Extraction	F1 score	In an extrinsic evaluation setup, ELMoLex ranked 7 th for Event Extraction, Negation Resolution tasks and 11 th for Opinion Analysis task by F1 score.
Negation Resolution tasks	F1 score	In an extrinsic evaluation setup, ELMoLex ranked 7 th for Event Extraction, Negation Resolution tasks and 11 th for Opinion Analysis task by F1 score.
Opinion Analysis task	F1 score	In an extrinsic evaluation setup, ELMoLex ranked 7 th for Event Extraction, Negation Resolution tasks and 11 th for Opinion Analysis task by F1 score.
Event Extraction	F1 score	In an extrinsic evaluation setup), ELMoLex ranked 7 th for Event Extraction, Negation Resolution tasks and 11 th for Opinion Analysis task by F1 score.
Negation Resolution tasks	F1 score	In an extrinsic evaluation setup), ELMoLex ranked 7 th for Event Extraction, Negation Resolution tasks and 11 th for Opinion Analysis task by F1 score.
Opinion Analysis task	F1 score	In an extrinsic evaluation setup), ELMoLex ranked 7 th for Event Extraction, Negation Resolution tasks and 11 th for Opinion Analysis task by F1 score.
Event Extraction	F1 score	ELMoLex ranked 7 th for Event Extraction, Negation Resolution tasks and 11 th for Opinion Analysis task by F1 score.
Negation Resolution tasks	F1 score	ELMoLex ranked 7 th for Event Extraction, Negation Resolution tasks and 11 th for Opinion Analysis task by F1 score.
Opinion Analysis task	F1 score	ELMoLex ranked 7 th for Event Extraction, Negation Resolution tasks and 11 th for Opinion Analysis task by F1 score.
translation	BLEU	 Table 2: Automatic evaluation of translation quality (BLEU and RIBES). Results marked with  † are significantly
translation	RIBES	 Table 2: Automatic evaluation of translation quality (BLEU and RIBES). Results marked with  † are significantly
statistical machine translation (SMT)	accuracy	In statistical machine translation (SMT), large quantities of high-quality bilingual data are essential to achieve high translation accuracy.
translation	BLEU	Experiments show that the choice of honorifics has a big impact on translation quality as measured by BLEU, and oracle experiments show that substantial improvements are possible by constraining the translation to the desired level of politeness.
translation	BLEU	• how important is the T-V distinction for translation quality (as measured by BLEU)?
MMI	BLEU	Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and inhuman evaluations.
sense annotation	TUHOI	For sense annotation, we selected only verbs for which either COCO or TUHOI contained five or more images, resulting in a set of 90 verbs (out of the total 148).
CCG parsing	F1	We apply the model to CCG parsing, where it improves over a strong greedy RNN baseline, by 1.47% F1, yielding state-of-the-art results for shift-reduce CCG parsing.
part-of-speech tagging	accuracy	Our approach was evaluated on two core NLP tasks, part-of-speech tagging and named entity recognition , and showed the state-of-the-art results for both tasks, achieving the accuracy of 97.64 and the F1-score of 91.00 respectively, with about a 25% increase in the feature space.
part-of-speech tagging	F1-score	Our approach was evaluated on two core NLP tasks, part-of-speech tagging and named entity recognition , and showed the state-of-the-art results for both tasks, achieving the accuracy of 97.64 and the F1-score of 91.00 respectively, with about a 25% increase in the feature space.
Sentence Completion Challenge	accuracy	On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2% accuracy, surpassing the previous state of the art by a large margin.
Sentence Completion Challenge	accuracy	On the Sentence Completion Challenge (, our model achieves an impressive 69.2% accuracy, surpassing the previous state of the art 58.9% by a large margin.
I-measure	Improvement (I) score	In the I-measure, an Improvement (I) score is computed by comparing system performance with that of a baseline which leaves the original text uncorrected (i.e. the source).
topic classification	equal error rate (EER)	In consideration of topic classification is similar to topic verification, we choose equal error rate (EER) to be the second criterion, which is the equal value of miss probability and false probability.
topic verification	equal error rate (EER)	In consideration of topic classification is similar to topic verification, we choose equal error rate (EER) to be the second criterion, which is the equal value of miss probability and false probability.
Link prediction	MR	 Table 3: Link prediction results. MR and H10 denote  evaluation metrics of mean rank and Hits@10 (in %), re- spectively. "NLFeat" abbreviates Node+LinkFeat. The  results for NTN (Socher et al., 2013) listed in this table  are taken from Yang et al. (2015) since NTN was origi- nally evaluated on different datasets. The results marked  with + are obtained using the optimal hyper-parameters  chosen to optimize Hits@10 on the validation set; trained  in this manner, STransE obtains a mean rank of 244 and  Hits@10 of 94.7% on WN18, while producing the same  results on FB15k.
diplomatic transcription	error reduction	For diplomatic transcription, our unsupervised joint model achieves an error reduction of 35% over the baseline Ocular system without support for orthographic variation, and nearly matches the error rate of an approach proposed by earlier work that uses a hand-constructed ruleset of orthographic rewrites.
normalized transcription	error reduction	However, for the new task of normalized transcription, we achieve a 46% error reduction over the baseline, as well as a 28% reduction over the hand-built ruleset approach.
Medication	Frequency	The Medication group contains Drugname, Dosage, Frequency, Duration and Route.
Medication	Duration	The Medication group contains Drugname, Dosage, Frequency, Duration and Route.
Medication	Route	The Medication group contains Drugname, Dosage, Frequency, Duration and Route.
Lexicon learning	accuracy	Lexicon learning is the first step of training a semantic parser fora new application domain and the quality of the learned lexicon significantly affects both the accuracy and efficiency of the final semantic parser.
semantic parser	accuracy	Our models improve semantic parser accuracy (35-70% error reduction) over prior work despite using less human input.
semantic parser	error reduction	Our models improve semantic parser accuracy (35-70% error reduction) over prior work despite using less human input.
Individual form prediction	accuracy	 Table 6: Individual form prediction accuracy for factored
Dependency parsing	F1 score	We consider two evaluation tasks: (a) Dependency parsing, reporting Unlabeled Attachment Score (UAS); and (b) Query Segmentation, reporting the F1 score, where each segment is represented by its boundaries: in order for an observed segment to be considered correct, both of its ends must match those of a gold segment.
Query Segmentation	F1 score	We consider two evaluation tasks: (a) Dependency parsing, reporting Unlabeled Attachment Score (UAS); and (b) Query Segmentation, reporting the F1 score, where each segment is represented by its boundaries: in order for an observed segment to be considered correct, both of its ends must match those of a gold segment.
language generation	BLEU	We evaluate our model on a benchmark weather forecasting dataset (WEATHERGOV) and achieve the best results reported to-date on content selection (12% relative improvement in F-1) and language generation (59% relative improvement in BLEU), despite using no domain-specific resources.
content selection	F-1 score	For content selection, we use the F-1 score of the set of selected records as defined by the harmonic mean of precision and recall with respect to the ground-truth selection record set.
content selection	precision	For content selection, we use the F-1 score of the set of selected records as defined by the harmonic mean of precision and recall with respect to the ground-truth selection record set.
content selection	recall	For content selection, we use the F-1 score of the set of selected records as defined by the harmonic mean of precision and recall with respect to the ground-truth selection record set.
surface realization	BLEU score 6	We evaluate the quality of surface realization using the BLEU score 6 (a 4-gram matching-based precision) () of the generated description with respect to the human-created reference.
SMT	reporting Precision (P)	 Table 3: SMT results on NLMAPS, reporting Precision (P), Re-
SMT	Re-	 Table 3: SMT results on NLMAPS, reporting Precision (P), Re-
PROP	precision-recall	For PROP, only one precision-recall point was obtained.
relation extraction	precision	1. This simulates the task of relation extraction where we predict relation(s) that exist between the entity pair int . We compare predicted labels of these test instances to the actual labels and measure precision, recall and F1 values of the prediction.
relation extraction	recall	1. This simulates the task of relation extraction where we predict relation(s) that exist between the entity pair int . We compare predicted labels of these test instances to the actual labels and measure precision, recall and F1 values of the prediction.
relation extraction	F1	1. This simulates the task of relation extraction where we predict relation(s) that exist between the entity pair int . We compare predicted labels of these test instances to the actual labels and measure precision, recall and F1 values of the prediction.
type checking	EM)	Adding type checking improves performance even further (EM).
Statistical Machine Translation (SMT)	BLEU	Research in Statistical Machine Translation (SMT) aims to improve translation quality, typically measured by BLEU scores (), over a baseline system.
phrase-based translation (PBMT)	BLEU	For example, in phrase-based translation (PBMT) (Koehn et al., * This work was done during an internship of the first author at SDL Research, 2003), decoder parameters such as pruning thresholds and reordering constraints can have a dramatic impact on both BLEU and decoding speed.
Zero-annotation transfer learning	F1	 Table 4: Zero-annotation transfer learning F1 scores on  2012 Spanish TAC KBP slot-filling task. Adding a trans- lation dictionary improves all encoder-based models. En- sembling LSTM and USchema models performs the best.
relation extraction	precision	It is quite surprising, therefore, that researchers who have applied crowdsourced annotation to relation extraction argue the opposite, that crowdsourcing provides only minor improvement: • conclude that "Human feedback has relatively small impact on precision and recall."
relation extraction	recall	It is quite surprising, therefore, that researchers who have applied crowdsourced annotation to relation extraction argue the opposite, that crowdsourcing provides only minor improvement: • conclude that "Human feedback has relatively small impact on precision and recall."
Gated Instruction	precision	• We demonstrate that Gated Instruction increases the annotation quality of crowdsourced training data, raising precision from 0.50 to 0.77 and recall from 0.70 to 0.78, compared to Angeli et al.'s crowdsourced tagging of the same sentences.
Gated Instruction	recall	• We demonstrate that Gated Instruction increases the annotation quality of crowdsourced training data, raising precision from 0.50 to 0.77 and recall from 0.70 to 0.78, compared to Angeli et al.'s crowdsourced tagging of the same sentences.
ASR	mean average precision (MAP)	Finally, for ASR, we adopt two metrics widely used in information retrieval: mean average precision (MAP) and mean reciprocal rank (MRR).
ASR	mean reciprocal rank (MRR)	Finally, for ASR, we adopt two metrics widely used in information retrieval: mean average precision (MAP) and mean reciprocal rank (MRR).
information retrieval	mean average precision (MAP)	Finally, for ASR, we adopt two metrics widely used in information retrieval: mean average precision (MAP) and mean reciprocal rank (MRR).
information retrieval	mean reciprocal rank (MRR)	Finally, for ASR, we adopt two metrics widely used in information retrieval: mean average precision (MAP) and mean reciprocal rank (MRR).
answer selection task	Mean Average Precision (MAP)	For the answer selection task (Wiki-QA and TrecQA), we used the official trec eval scorer to compute the metrics Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) and  selected the best development model based on MRR for final testing.
answer selection task	Mean Reciprocal Rank (MRR)	For the answer selection task (Wiki-QA and TrecQA), we used the official trec eval scorer to compute the metrics Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) and  selected the best development model based on MRR for final testing.
translation	F-score	Experimental results show that our approach achieves a significant improvement of 1.58 BLEU points in translation performance with 66% F-score for DP generation accuracy.
translation	accuracy	Experimental results show that our approach achieves a significant improvement of 1.58 BLEU points in translation performance with 66% F-score for DP generation accuracy.
DP generation	accuracy	Experimental results show that our approach achieves a significant improvement of 1.58 BLEU points in translation performance with 66% F-score for DP generation accuracy.
translation	BLEU	Furthermore, translation performance with N -best integration is much better than its 1-best counterpart (i.e. +0.84 BLEU points).
coreference resolution	accuracy	(iii) considerably narrowing the gap to supervised coreference resolution accuracy.
relation extraction	Y(es)/N(o)	 Table 3: Results on test for relation extraction. Y(es)/N(o) indicates whether embeddings are updated during training.
translation	BLEU score	The translation performance was evaluated using the BLEU score ().
Dependency Labeling	Accurate	Making Dependency Labeling Simple, Fast and Accurate
labeling	accuracy	It improves labeling accuracy over the outputs of top parsers, achieving the best LAS on 5 out of 7 datasets 1 .
labeling	LAS	It improves labeling accuracy over the outputs of top parsers, achieving the best LAS on 5 out of 7 datasets 1 .
labeling	accuracy	This restriction greatly limits the labeling accuracy.
labeling	speed	In this work, we show that the labeling procedure, when optimized with recent advanced techniques in parsing, can achieve very high speed and accuracy.
labeling	accuracy	In this work, we show that the labeling procedure, when optimized with recent advanced techniques in parsing, can achieve very high speed and accuracy.
parsing	speed	In this work, we show that the labeling procedure, when optimized with recent advanced techniques in parsing, can achieve very high speed and accuracy.
parsing	accuracy	In this work, we show that the labeling procedure, when optimized with recent advanced techniques in parsing, can achieve very high speed and accuracy.
MT	BLEU	MT systems were evaluated with the standard BLEU metric) on two official WMT test sets that cover different domains: News (WMT'11) and Europarl (WMT'08).
translation news story pairs	He distance	We then again create a single ranked list of translation news story pairs by computing divergence based similarity using He distance ( §3.2).
stance classification	accuracy	We develop a stance classification approach based on multiclass logistic regression, using features extracted from the article headline and the claim, achieving an accuracy of 73% on our test data-set, also demonstrating that features relying on syntax, word alignment and paraphrasing contribute to the performance.
Captions	METEOR	 Table 6: Captions generated per-image with METEOR  scores.
translation	BLEU	Experiment results show that by performing only one mouse click, the translation quality could be significantly improved (around +2 BLEU points in one PR cycle).
question answering task	accuracy	The results show that (i) both approaches achieve the state of the art on a question answering task, where CTKs produce higher accuracy and (ii) combining such methods leads to unprecedented high results.
spelling normalization	accuracy	Feature Embeddings also give better performance than spelling normalization, but the combination of the two methods is better still, yielding a 5% raw improvement in tagging accuracy on Early Modern English texts.
end-to-end argument mining	F-score	We examine the under-investigated task of end-to-end argument mining in persuasive student essays , where we (1) present the first results on end-to-end argument mining in student essays using a pipeline approach; (2) address error propagation inherent in the pipeline approach by performing joint inference over the outputs of the tasks in an Integer Linear Programming (ILP) framework; and (3) propose a novel objective function that enables F-score to be maximized directly by an ILP solver.
argument mining	F-score optimizing objective function	We believe that the impact of our work goes beyond argument mining, as our F-score optimizing objective function is general enough to be applied to any ILP-based joint inference tasks.
SRL	GOAL/NI	In its traditional form, however, SRL is restricted to the local syntactic context of the predicate as in the following example from: [ GOAL/NI In the centre of this room] there was an upright beam, [ THEME which] had been placed [ TIME at some period] as a support for the old worm-eaten baulk of timber which spanned the roof.
SRL	THEME	In its traditional form, however, SRL is restricted to the local syntactic context of the predicate as in the following example from: [ GOAL/NI In the centre of this room] there was an upright beam, [ THEME which] had been placed [ TIME at some period] as a support for the old worm-eaten baulk of timber which spanned the roof.
SRL	TIME	In its traditional form, however, SRL is restricted to the local syntactic context of the predicate as in the following example from: [ GOAL/NI In the centre of this room] there was an upright beam, [ THEME which] had been placed [ TIME at some period] as a support for the old worm-eaten baulk of timber which spanned the roof.
spammer classification	Latent Dirichlet Allocation (LDA)	In this paper, we present a novel spammer classification approach based on Latent Dirichlet Allocation (LDA), a topic model.
machine translation of patents or scientific papers	BLEU	In recent years, Chinese-Japanese machine translation of patents or scientific papers has made rapid progress with the large quantities of parallel corpora provided by the organizers of the Workshop on Asian Translation (WAT) 3 4 . In the "patents subtask" of, a Chinese to Japanese translation system is described that achieves higher BLEU scores by combination of results between Statistical Post Editing (SPE) based on their rule-based translation system and SMT system equipped with a recurrent neural language model (RNNLM).
Asian Translation (WAT)	BLEU	In recent years, Chinese-Japanese machine translation of patents or scientific papers has made rapid progress with the large quantities of parallel corpora provided by the organizers of the Workshop on Asian Translation (WAT) 3 4 . In the "patents subtask" of, a Chinese to Japanese translation system is described that achieves higher BLEU scores by combination of results between Statistical Post Editing (SPE) based on their rule-based translation system and SMT system equipped with a recurrent neural language model (RNNLM).
translation	accuracy	We obtain a significant improvement in translation accuracy as evaluated by BLEU ().
translation	BLEU	We obtain a significant improvement in translation accuracy as evaluated by BLEU ().
identification	accuracy	Including available demographic information as features should thus help identification accuracy.
Extractive summarization	ROUGE	Extractive summarization techniques typically aim to maximize the information coverage of the summary with respect to the original corpus and report accuracies in ROUGE scores.
SMT	coverage	Since SMT systems are often trained on large amounts of data, we expected poor coverage with this dataset.
solution prediction	precision	A system deployment in multiple high school classrooms shows that our solution prediction model triggers instant feedback with high precision, and that the feedback is successful in increasing the number of peer reviews with solutions.
machine translation	BLEU	We also evaluate in a machine translation setting, resulting in shorter training times achieving the same translation quality measured in BLEU scores.
classification	accuracy	reports the classification accuracy of SODA with few labeled comments from the target collection (ranging from 0 to 100).
relation extraction	BREJ	However, in context of relation extraction, we call it BREJ.
word discovery task	F-score	Finally, on the word discovery task, we improve upon previous work by about 3% F-score on both tokens and types.
RST	repetition	Combination of RST ( §4.2.3), repetition ( §4.2.1), and delta features ( §4.2.2).
Binary Classification	F1-scores	 Table 4: Binary Classification Results on PDTB. We report F1-scores for implicit discourse relations.
Listenability"	Pseudo-melody	Regarding the "Listenability" evaluation, workers gave high scores to the Fine-tuned and Pseudo-melody models that are trained using both the melody and lyrics.
ASAP	QWK	 Table 2: Model performance on ASAP and synthetic  test data. Evaluation is based on the average QWK,  PRA and TRPA across the 8 prompts. * indicates  significantly different results compared to LSTM T&N  (two-tailed test with p < 0.01).
ASAP	PRA	 Table 2: Model performance on ASAP and synthetic  test data. Evaluation is based on the average QWK,  PRA and TRPA across the 8 prompts. * indicates  significantly different results compared to LSTM T&N  (two-tailed test with p < 0.01).
ASAP	TRPA	 Table 2: Model performance on ASAP and synthetic  test data. Evaluation is based on the average QWK,  PRA and TRPA across the 8 prompts. * indicates  significantly different results compared to LSTM T&N  (two-tailed test with p < 0.01).
mathematical information retrieval (MIR)	precision	In mathematical information retrieval (MIR), for instance, enriching formulae with types may improve precision.
Typing information	precision	Typing information maybe helpful in reducing such instances and improving retrieval precision.
Beam Search Optimization	best likelihood (MLE)	 Table 5: Comparison to Beam Search Optimization.  We report the best likelihood (MLE) and BSO results  from Wiseman and Rush (2016), as well as results from  our MLE reimplementation and training with Risk.  Results based on unnormalized beam search (k = 5).
Beam Search Optimization	BSO	 Table 5: Comparison to Beam Search Optimization.  We report the best likelihood (MLE) and BSO results  from Wiseman and Rush (2016), as well as results from  our MLE reimplementation and training with Risk.  Results based on unnormalized beam search (k = 5).
information retrieval evaluation	precision	Therefore, popular information retrieval evaluation metrics, including precision at K (P@K), mean average precision (MAP) (, and normalized Discounted Cumulative Gain at K (nDCG@K)) are reported.
information retrieval evaluation	mean average precision (MAP)	Therefore, popular information retrieval evaluation metrics, including precision at K (P@K), mean average precision (MAP) (, and normalized Discounted Cumulative Gain at K (nDCG@K)) are reported.
AMRs	interannotator agreement	Finally, even human annotations do not yield perfect AMRs, as the interannotator agreement reported in the literature for AMR is around 80% (.
paraphrase detection	BOW	 Table 1: Baseline results for paraphrase detection with  AMR and with bag-of-words (BOW). "linear," "poly"  and "rbf" denote the kernel which is used with a  support vector machine classifier. "Smatch" denotes  the use of the additional graph similarity feature and  "BOC" the use of the additional Jaccard score on the  bag of concept. Best result in each column is in bold.
paraphrase detection	BOC	 Table 1: Baseline results for paraphrase detection with  AMR and with bag-of-words (BOW). "linear," "poly"  and "rbf" denote the kernel which is used with a  support vector machine classifier. "Smatch" denotes  the use of the additional graph similarity feature and  "BOC" the use of the additional Jaccard score on the  bag of concept. Best result in each column is in bold.
SRL	precision	For SRL, precision is defined as the proportion of semantic roles predicted by a system which are correct, recall is the proportion of gold roles which are predicted by a system, F1 score is the harmonic mean of precision and recall.
SRL	recall	For SRL, precision is defined as the proportion of semantic roles predicted by a system which are correct, recall is the proportion of gold roles which are predicted by a system, F1 score is the harmonic mean of precision and recall.
SRL	F1 score	For SRL, precision is defined as the proportion of semantic roles predicted by a system which are correct, recall is the proportion of gold roles which are predicted by a system, F1 score is the harmonic mean of precision and recall.
SRL	precision	For SRL, precision is defined as the proportion of semantic roles predicted by a system which are correct, recall is the proportion of gold roles which are predicted by a system, F1 score is the harmonic mean of precision and recall.
SRL	recall	For SRL, precision is defined as the proportion of semantic roles predicted by a system which are correct, recall is the proportion of gold roles which are predicted by a system, F1 score is the harmonic mean of precision and recall.
question decomposition	precision	We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 preci-sion@1 on this new dataset.
question decomposition	precision	We evaluate our model on COMPLEXWEBQUESTIONSand find that question decomposition substantially improves precision@1 from 20.8 to 27.5.
summarization diversity	coverage	In Section 4, we discuss three measures of summarization diversity: coverage, density, and compression.
summarization diversity	compression	In Section 4, we discuss three measures of summarization diversity: coverage, density, and compression.
claim verification classification	precision	To ensure annotation consistency, we developed suitable guidelines and user interfaces, resulting in inter-annotator agreement of 0.6841 in Fleiss κ) in claim verification classification, and 95.42% precision and 72.36% recall in evidence retrieval.
claim verification classification	recall	To ensure annotation consistency, we developed suitable guidelines and user interfaces, resulting in inter-annotator agreement of 0.6841 in Fleiss κ) in claim verification classification, and 95.42% precision and 72.36% recall in evidence retrieval.
NER	precision	 Table 1: NER performance on the SnapCaptions dataset with varying modalities (W: word, C: char, V: visual).  We report precision, recall, and F1 score for both entity types recognition (PER, LOC, ORG, MISC) and entity  segmentation (untyped recognition -named entity or not) tasks.
NER	recall	 Table 1: NER performance on the SnapCaptions dataset with varying modalities (W: word, C: char, V: visual).  We report precision, recall, and F1 score for both entity types recognition (PER, LOC, ORG, MISC) and entity  segmentation (untyped recognition -named entity or not) tasks.
NER	F1 score	 Table 1: NER performance on the SnapCaptions dataset with varying modalities (W: word, C: char, V: visual).  We report precision, recall, and F1 score for both entity types recognition (PER, LOC, ORG, MISC) and entity  segmentation (untyped recognition -named entity or not) tasks.
joint extraction of entity mentions	head span	Similarly, for the task of joint extraction of entity mentions and mention heads, the mention span, head span and the entity type should all exactly match the gold label.
Dependency parsing	accuracy	Dependency parsing research, which has made significant gains in recent years, typically fo-cuses on improving the accuracy of single-tree predictions.
Dependency parsers	accuracy	Dependency parsers typically predict a singletree fora sentence to be used in downstream applications, and most work on dependency parsers seeks to improve accuracy of such single-tree predictions.
parsing	accuracy	Despite tremendous gains in the last few decades of parsing research, accuracy is far from perfect-but perfect accuracy maybe impossible since syntax models by themselves do not incorporate the discourse, pragmatic, or world knowledge necessary to resolve many ambiguities.
parsing	accuracy	Despite tremendous gains in the last few decades of parsing research, accuracy is far from perfect-but perfect accuracy maybe impossible since syntax models by themselves do not incorporate the discourse, pragmatic, or world knowledge necessary to resolve many ambiguities.
tagging	accuracy	In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words.
dependency parsing	AT	We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations.
text classification	accuracy	Recently, applied AT on text classification, achieving state-of-the-art accuracy.
POS tagging	accuracy	In order to demystify the effects of adversarial training in the context of NLP, we conduct POS tagging experiments on multiple languages using the Penn Treebank WSJ corpus (Englsih) and the Universal Dependencies dataset, with thorough analyses of the following points: • Effects on different target languages • Vocabulary statistics and tagging accuracy • Influence on downstream tasks • Representation learning of words In our experiments, we find that our adversarial training model consistently outperforms the baseline POS tagger, and even achieves state-of-the-art results on 22 languages.
POS tag- ging	accuracy	 Table 1 shows the POS tag- ging results. As expected, our baseline (BiLSTM- CRF) model (accuracy 97.54%) performs on par  with other state-of-the-art systems. Built upon  this baseline, our adversarial training (AT) model  reaches accuracy 97.58% thanks to its regulariza- tion power, outperforming recent POS taggers ex- cept Ling et al. (2015). The improvement over the  baseline is statistically significant, with p-value <  0.05 on the t-test. We provide additional analysis  on this result in later sections.
POS tag- ging	accuracy	 Table 1 shows the POS tag- ging results. As expected, our baseline (BiLSTM- CRF) model (accuracy 97.54%) performs on par  with other state-of-the-art systems. Built upon  this baseline, our adversarial training (AT) model  reaches accuracy 97.58% thanks to its regulariza- tion power, outperforming recent POS taggers ex- cept Ling et al. (2015). The improvement over the  baseline is statistically significant, with p-value <  0.05 on the t-test. We provide additional analysis  on this result in later sections.
POS tagging	accuracy	 Table 3: POS tagging accuracy (test) on different sub- sets of words, categorized by their frequency of occur- rence in training. The second row shows the number  of tokens in the test set that are in each category. The  third and fourth rows show the performance of our two  models. Better scores are underlined. The biggest im- provement is in bold.
POS tagging	occur- rence	 Table 3: POS tagging accuracy (test) on different sub- sets of words, categorized by their frequency of occur- rence in training. The second row shows the number  of tokens in the test set that are in each category. The  third and fourth rows show the performance of our two  models. Better scores are underlined. The biggest im- provement is in bold.
Chunking	F1	 Table 8: Chunking F1 scores on the CoNLL-2000 task,  with other top performing models.
cross-domain classification of microposts	F1-score	 Table 11: Different classifiers on cross-domain classification of microposts; best result in bold; (eval.: F1-score).
Power prediction	THR	 Table 3: Power prediction results using different con- figurations of LEX features. (The full feature set also  includes THR.)
recognizing textual entailment	ENTAILMENT	In this task, also known as recognizing textual entailment), a model is presented with a pair of sentences-like one of those in-and asked to judge the relationship between their meanings by picking a label from a small set: typically ENTAILMENT, NEUTRAL, and CONTRADICTION.
Lexical alignment	precision	 Table 3: Lexical alignment (precision, recall, F 1 -score).  Our lexical alignment algorithm does not use syntax.
Lexical alignment	recall	 Table 3: Lexical alignment (precision, recall, F 1 -score).  Our lexical alignment algorithm does not use syntax.
Lexical alignment	F 1 -score	 Table 3: Lexical alignment (precision, recall, F 1 -score).  Our lexical alignment algorithm does not use syntax.
POS tagging	LAS	Our best end-to-end parser, which jointly performs supertagging, POS tagging , and parsing, outperforms the previously reported best results by more than 2.2 LAS and UAS points.
parsing	LAS	Our best end-to-end parser, which jointly performs supertagging, POS tagging , and parsing, outperforms the previously reported best results by more than 2.2 LAS and UAS points.
Syntactic analogy	Avg. rank	 Table 4: Syntactic analogy test results on the 300 most  frequent supertags. Avg. rank is the average position  of the correct choice in the ranked list of the closest  neighbors; the top line indicates the result of using su- pertag embeddings that are trained jointly with a tran- sition based parser (Friedman et al., 2017).
sentiment analysis	accuracy	Extensive quantitative evaluations on real-world sentiment analysis and dialog intent classification datasets demonstrate that the proposed method performs favorably against state-of-the-art few shot learning algorithms in terms of predictive accuracy.
dialog intent classification	accuracy	Extensive quantitative evaluations on real-world sentiment analysis and dialog intent classification datasets demonstrate that the proposed method performs favorably against state-of-the-art few shot learning algorithms in terms of predictive accuracy.
sentiment classifier	AE-SCL-SR	The sentiment classifier employed with the SCL-MI, MSDA and AE-SCL-SR representations is the same logistic regression classifier as in the NoSt condition mentioned above.
neural machine translation (NMT)	recall	One of the difficulties of neural machine translation (NMT) is the recall and appropriate translation of low-frequency words or phrases.
NMT translation	BLEU	We show our method improves NMT translation results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness of the target sentences is particularly salient.
translation	accuracy	It also causes little increase in the translation time, and compares favorably to another alternative retrieval-based method with respect to accuracy, speed, and simplicity of implementation.
Translation	BLEU)	 Table 7:  Translation results (BLEU) for the  whole/divided test sets.
bootstrap resampling	precision	We performed bootstrap resampling for 1000 times: our best model improved more on homographs than all words in terms of either f1, precision, or recall with p < 0.05, indicating statistical significance across all measures.
bootstrap resampling	recall	We performed bootstrap resampling for 1000 times: our best model improved more on homographs than all words in terms of either f1, precision, or recall with p < 0.05, indicating statistical significance across all measures.
Translation	precision	 Table 3: Translation results for homographs and all words in our NMT vocabulary. We compare scores for baseline  and our best proposed model on three different language pairs. Improvements are in italic. We performed bootstrap  resampling for 1000 times: our best model improved more on homographs than all words in terms of either f1,  precision, or recall with p < 0.05, indicating statistical significance across all measures.
Translation	recall	 Table 3: Translation results for homographs and all words in our NMT vocabulary. We compare scores for baseline  and our best proposed model on three different language pairs. Improvements are in italic. We performed bootstrap  resampling for 1000 times: our best model improved more on homographs than all words in terms of either f1,  precision, or recall with p < 0.05, indicating statistical significance across all measures.
beam search	length penalty α	We use beam search with abeam size of 4 and length penalty α = 0.6.
alignment	accuracy	• It empirically shows that the proposed TFA-NMT can lead to better alignment accuracy, and achieves significant improvements on both Chinese-toEnglish and Japanese-to-English translation tasks.
morphological analysis	PyMorphy	For morphological analysis we use Omorfi 2 for Finnish, the analyzer of for Turkish, and PyMorphy 3 for Russian.
Morphological disambiguation	accuracy	 Table 5: Morphological disambiguation accuracy results for Turkish.
Answering	EM	 Table 4: Answering results on the test set. EM and F1  scores are percentages. The human scores (in italics)  are based on the validation set.
Answering	F1	 Table 4: Answering results on the test set. EM and F1  scores are percentages. The human scores (in italics)  are based on the validation set.
transfer learning	accuracy	 Table 2: Results of transfer learning on the target  datasets. The number in the parenthesis indicates  the accuracy increased via transfer learning (com- pared to rows (a) and (g)). The best performance  for each target dataset is marked in bold. We also  include the results of the previous best performing  models on the target datasets in the last three rows.
sentiment analysis-predicting	clarity	As an NLP resource, peer reviews raise interesting challenges, both from the realm of sentiment analysis-predicting various properties of the reviewed paper, e.g., clarity and novelty, as well as that of text generation-given a paper, automatically generate its review.
summaries	ROUGE	Although the above extractive methods successfully obtain summaries with high ROUGE scores, they have the following shortcoming: A long sentence typically has redundant parts, which means a summary constructed simply by extracting some sentences often includes many redundant parts.
summarization tasks	ROUGE 1	We applied our method to compressive summarization tasks with the three kinds of objective functions: the coverage function, the one with rewards, and ROUGE 1 . To benchmark our method, we also applied the ILP-based method to the tasks.
summarization tasks	ROUGE 1	 Table 1: Approximation ratios, ROUGE 1 scores, and running times for our method (Greedy) and the  ILP-based method (ILP); the average values over the 50 topics are presented. The two methods are applied  to compressive summarization tasks with three types of objective functions: Coverage, Coverage with  rewards, and ROUGE 1 . Summaries obtained with the ILP-based method and ROUGE 1 objective function  are oracle summaries.
relation extractors	recall	We measure these two relation extractors because Stanford OpenIE is included with the popular CoreNLP software and ClausIE achieves the highest recall in two systematic studies of relation extractors).
summarization	recall	Transferred to summarization, this means that space wasted by recall-oriented selection cannot be used anymore whereas a low recall in a partial summary can be repaired by adding more sentences.
Link prediction	Mean Averaged Precision, MAP)	Link prediction is to rank a list of target entities (e − 1 , e − 2 , · · · , e + n ) given a query entity e q and query relation r q . The dataset is arranged in the format of (e q , r q , , and the evaluation score (Mean Averaged Precision, MAP) is based on the ranked position of the positive sample.
classification	accuracy	Besides, the classification performance suggests the new data achieve reasonable accuracy, although accuracy numbers are not directly comparable.
classification	accuracy	Besides, the classification performance suggests the new data achieve reasonable accuracy, although accuracy numbers are not directly comparable.
sentiment analysis	precision	F1-score is a common way to measure classifier performance in sentiment analysis as it computes the harmonic mean between precision and recall.
sentiment analysis	recall	F1-score is a common way to measure classifier performance in sentiment analysis as it computes the harmonic mean between precision and recall.
Instruction generation	accuracies	 Table 3: Instruction generation results. We report the  accuracies of human evaluators at following the outputs  of the speaker systems (as well as other humans) on 50- instance samples from the SAIL dataset and SCONE  domains. DBW is the system of Daniele et al. (2017).  Bold numbers are new state-of-the-art results.
sentence classification task	RILE index	Our contributions are as follows: • we employ a hierarchical sequential deep model that encodes the structure in manifesto text for the sentence classification task; • we capture the dependency between the sentence-and document-level tasks, and also utilize additional label structure (categorization into LEFT/RIGHT/NEUTRAL:) using a joint-structured model; • we incorporate contextual information (such as political coalitions) and encode temporal dependencies to calibrate the coarse-level manifesto position using probabilistic soft logic (, which we evaluate on the prediction of the RILE index or expert survey party position score.
Passage completion	SAT	Passage completion is a popular method of evaluating reading comprehension that is adapted by several standardized tests (e.g., SAT, TOEFL, GRE).
Passage completion	GRE	Passage completion is a popular method of evaluating reading comprehension that is adapted by several standardized tests (e.g., SAT, TOEFL, GRE).
Machine Translation	BLEU	The widely accepted evaluating methods employed by the existing NRG models can be categorized as: a) metrics inherited from Machine Translation, e.g., BLEU, Perplexity, etc.
SRL	F 1	modeled SRL as a BIO tagging problem and used an 8-layer deep biLSTM with forward and backward directions interleaved, following, our ELMo enhanced biLSTM-CRF achieves 92.22% F 1 averaged over five runs.
entity prediction	ENGEN	Recall that S2SA does not have a component for entity prediction, therefore we only compare it with ENGEN in the mention only case.
mention generation task	ENTITYNLM	Unlike the mention generation task, S2SA beats ENTITYNLM at this task; this difference in performance shows the importance of local context.
validation	accuracy	During training, we save the model every 3000 iterations, and calculate the validation accuracy.
NLP hate speech detection	accuracy	Despite the existence of numerous studies dedicated to the development of NLP hate speech detection approaches, the accuracy is still poor.
sluice resolution	F1	Baselines In addition to comparing to, the only previous work on sluice resolution, we compare our performance to two baseline neural network architectures: a singletask architecture and a multi-task architecture similar to . Our first baseline is a single-task, two-layered long-short-term memory (LSTM) network, with: F1 scores on embedded sluices from ESC (Newswire) and embedded and root sluices from OpenSubtitles (Dialogue).
termination	patience	For termination, we use the early-stopping procedure with a patience value of 10 that is monitored on the validation loss.
entity detection	precision	For entity detection, we evaluate by extracting every sequence of contiguous ENTITY tags and compute precision, recall, and F1 against the ground truth.
entity detection	recall	For entity detection, we evaluate by extracting every sequence of contiguous ENTITY tags and compute precision, recall, and F1 against the ground truth.
entity detection	F1	For entity detection, we evaluate by extracting every sequence of contiguous ENTITY tags and compute precision, recall, and F1 against the ground truth.
entity linking	recall	For both entity linking and relation prediction, we evaluate recall at N (R@N ), i.e., whether the correct answer appears in the top N results.
relation prediction	recall	For both entity linking and relation prediction, we evaluate recall at N (R@N ), i.e., whether the correct answer appears in the top N results.
intent detection task	F1-score	The performance is evaluated based on the classification accuracy for intent detection task and F1-score for slot filling task.
slot filling task	F1 scores	Some of the models are designed for single slot filling task, hence only F1 scores are given.
slot filling  task	F1	 Table 1. Some  of the models are designed for single slot filling  task, hence only F1 scores are given. It can  be observed that the new proposed Bi-model  structures outperform the current state-of-the-art  results on both intent detection and slot filling  tasks, and the Bi-model with a decoder also  outperform that without a decoder on our ATIS  dataset. The current Bi-model with a decoder  shows the state-of-the-art performance on ATIS  benchmark dataset with 0.9% improvement on F1  score and 0.5% improvement on intent accuracy.  Remarks:
slot filling  task	F1  score	 Table 1. Some  of the models are designed for single slot filling  task, hence only F1 scores are given. It can  be observed that the new proposed Bi-model  structures outperform the current state-of-the-art  results on both intent detection and slot filling  tasks, and the Bi-model with a decoder also  outperform that without a decoder on our ATIS  dataset. The current Bi-model with a decoder  shows the state-of-the-art performance on ATIS  benchmark dataset with 0.9% improvement on F1  score and 0.5% improvement on intent accuracy.  Remarks:
slot filling  task	Remarks	 Table 1. Some  of the models are designed for single slot filling  task, hence only F1 scores are given. It can  be observed that the new proposed Bi-model  structures outperform the current state-of-the-art  results on both intent detection and slot filling  tasks, and the Bi-model with a decoder also  outperform that without a decoder on our ATIS  dataset. The current Bi-model with a decoder  shows the state-of-the-art performance on ATIS  benchmark dataset with 0.9% improvement on F1  score and 0.5% improvement on intent accuracy.  Remarks:
NEC	accuracy	(2) We empirically demonstrate, for the task of semi-supervised NEC on two standard datasetsCoNLL and -that we obtain a classification accuracy of 66.11% and 63.12% with minimal supervision on only 0.3% and 0.6% of the data, respectively.
event extraction	accuracy	Supervised event extraction systems are limited in their accuracy due to the lack of available training data.
parsing	PANPARSER	For parsing, we use PANPARSER, our own open source 5 implementation of a greedy ARCEAGER parser (using an averaged perceptron and a dynamic oracle).
Image Captioning (IC)	automatic metric scores	Models tackling vision-to-language (V2L) tasks, for example Image Captioning (IC) and Visual Question Answering (VQA), have demonstrated impressive results in recent years in terms of automatic metric scores.
Visual Question Answering (VQA)	automatic metric scores	Models tackling vision-to-language (V2L) tasks, for example Image Captioning (IC) and Visual Question Answering (VQA), have demonstrated impressive results in recent years in terms of automatic metric scores.
RSA speakers	accuracy	We show that such character-level RSA speakers are more effective than literal captioning systems at the task of helping a reader identify the target image among close competitors, and outperform word-level RSA captioners in both efficiency and accuracy.
RSA captioners	accuracy	We show that such character-level RSA speakers are more effective than literal captioning systems at the task of helping a reader identify the target image among close competitors, and outperform word-level RSA captioners in both efficiency and accuracy.
segmentation	accuracy	In addition to segmentation accuracy, we also report runtime when running on a mid-range laptop CPU.
machine translation (MT) evaluation	HyTER	We propose a variant of a well-known machine translation (MT) evaluation metric, HyTER (Dreyer and Marcu, 2012), which exploits reference translations enriched with meaning equivalent expressions.
Validation	BLEU	 Table 3: Validation BLEU, News commentary only
segmentation	BLEU loss and Average Proportion (AP)	We propose a tunable agent which decides the best segmentation strategy fora user-defined BLEU loss and Average Proportion (AP) constraint.
alignment fscore	BLEU	We test our method on both alignment fscore and machine translation BLEU (Section 4).
Machine translation	BLEU	 Table 2: Machine translation experiments (BLEU).  For languages with less than 10M monolingual tokens  (first five) we only use L e , otherwise we use both lexi- cons L e +L f . This way we improve baseline for almost  all languages.
CNNs	ReLU	For CNNs, the activation function is ReLU, and the channel size is set to 128.
summarization	F-1 ROUGE scores	Setup Similar to the majority of published research in the summarization literature (, evaluation was done using the ROUGE automatic summarization evaluation metric) with full-length F-1 ROUGE scores.
summarization	accuracy	When developing datasets for summarization, we are concerned with two key properties: accuracy and fluency.
emoji prediction	accuracy	We show that a multimodal approach (textual and visual content of the posts) increases the emoji prediction accuracy compared to the one that only uses textual information.
ASR	Word Error Rate (WER) metric	To evaluate the performance of the ASR system we used the Word Error Rate (WER) metric.
MT evaluation	BLEU	The use of MT evaluation metrics to evaluate dialogue fluency with just one reference has been debated ( . There is still no good alternative to evaluate dialog systems, and so we continue to report fluency using BLEU (BiLingual Evaluation Understudy (), in addition to other metrics and human evaluations.
SME	accuracy	This clearly shows that, though a SME only provides feedback on Heads during the training process, PANDA is still able to extract full part names from noisy data with a high degree of accuracy.
rejection confidence scoring	accuracy	2. Proposing an evaluation methodology for practical applications of rejection confidence scoring, based on which an operating point can be selected to balance cost vs. accuracy.
WSC questions	precision	There have been efforts to encode such knowledge directly, using logical formalisms ( or by using deep learning models (; however, these approaches have so far solved only restricted subsets of WSC questions with high precision, and show limited ability to generalize to new instances.
NER	F1 score	We evaluate NER using the F1 score on the CoNLL NER dataset).
classification	accuracy	Both pre-trained RNTN and the RNTN algorithm retrained on the data sets considered here are used to obtain classification accuracy.
sentiment classification task	Precision	 Table 3: This table shows results obtained by us- ing sentence embeddings from the InferSent en- coder in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard devi- ations (STD). Best results are obtained by KCCA  (GlvCC, LSA) and are highlighted in boldface.
sentiment classification task	F-score	 Table 3: This table shows results obtained by us- ing sentence embeddings from the InferSent en- coder in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard devi- ations (STD). Best results are obtained by KCCA  (GlvCC, LSA) and are highlighted in boldface.
sentiment classification task	AUC	 Table 3: This table shows results obtained by us- ing sentence embeddings from the InferSent en- coder in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard devi- ations (STD). Best results are obtained by KCCA  (GlvCC, LSA) and are highlighted in boldface.
MTE	BLEU	However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference translation based on character N-grams or word N-grams, such as SentBLEU (), which is a smoothed version of BLEU (), Blend (, MEANT 2.0 (, and chrF++, which achieved excellent results in the WMT-2017 Metrics task).
MT hypothesis	BLEU	However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference translation based on character N-grams or word N-grams, such as SentBLEU (), which is a smoothed version of BLEU (), Blend (, MEANT 2.0 (, and chrF++, which achieved excellent results in the WMT-2017 Metrics task).
neural machine translation evaluation	BLEU score	For the statistical machine translation and neural machine translation evaluation we use the BLEU score () as an evaluation metric, computed using the multi-bleu script from Moses ().
Vietnamese sequence labeling tasks	BiLSTM-CNN-CRF	built the NNVLP toolkit for Vietnamese sequence labeling tasks by applying a BiLSTM-CNN-CRF model.
Machine Translation	BLEU	Machine Translation systems are usually evaluated and compared using automated evaluation metrics such as BLEU and METEOR to score the generated translations against human translations.
Machine Translation	METEOR	Machine Translation systems are usually evaluated and compared using automated evaluation metrics such as BLEU and METEOR to score the generated translations against human translations.
MT	BLEU	Many metrics have been proposed for MT that compare system translations against human references, with the most popular being BLEU), METEOR), TER (), and, more recently, BEER).
MT	METEOR	Many metrics have been proposed for MT that compare system translations against human references, with the most popular being BLEU), METEOR), TER (), and, more recently, BEER).
MT	TER	Many metrics have been proposed for MT that compare system translations against human references, with the most popular being BLEU), METEOR), TER (), and, more recently, BEER).
MT	BEER	Many metrics have been proposed for MT that compare system translations against human references, with the most popular being BLEU), METEOR), TER (), and, more recently, BEER).
pattern matching	accuracy	With sufficient pattern-writing skill and effort, pattern matching with ChatScript can achieve relatively high accuracy, but it is unable to easily leverage increasing amounts of training data, somewhat brittle regarding misspellings, and can be difficult to maintain as new questions and patterns are added.
interpretability	accuracy	While this offers interpretability, these models have lower accuracy for shorter texts.
revision assistant	recall	However, for our long-term goal of building an effective revision assistant tool, intuitively we will  also need to identify 'NotBetter' revisions with higher recall, which is very low for this model.
ATS	consistency	The goal of ATS is to improve consistency and reduce human resource overheads.
Cross-language classification	F-score	 Table 5: Cross-language classification: F-score for complex word class / accurracy for cross-language classifica- tion.
predicting student	recall	It had demonstrated state-ofart performance for predicting student recall rates.
RTA	MUGS	Instead of evaluating holistic writing skills, the RTA was designed to evaluate students' writing skills along five dimensions: Analysis, Evidence, Organization, Style, and MUGS (Mechanics, Usage, Grammar, and Spelling).
Multiclass classification evaluation	F-measure	 Table 2: Multiclass classification evaluation results (we indicate the percentage of posts belonging to a class in the  sample; ¯ l doc refers to average document length in sentences; ¯ l sent -average sentence length in tokens; FM refers  to F-measure; PR -to precision; RC -to recall; )
predicting distress	Disatten-uated Pearson Correlation	Our approach showed best results for predicting distress at the age of 42 and for predicting current anxiety on Disatten-uated Pearson Correlation, and ranked fourth in the future health prediction task.
predicting current anxiety	Disatten-uated Pearson Correlation	Our approach showed best results for predicting distress at the age of 42 and for predicting current anxiety on Disatten-uated Pearson Correlation, and ranked fourth in the future health prediction task.
BSAG	anxiety BSAG score	Given the written essays and social control variables (gender and social class), CLPsych participants are to predict three types of BSAG scores: (i) total BSAG score, (ii) the depression BSAG score, and (iii) the anxiety BSAG score.
coreference	accuracy	[One hospital] in Ramallah tells us have treated seven people While previous studies have focused on SV agreement (den, there have been few corpus studies of notional pronouns, due at least in part to the lack of sizable corpora reliably annotated for coreference, and the low accuracy of automatic systems on difficult cases.
bridging resolution	precision	For bridging resolution, we report performance in precision, recall and F1.
bridging resolution	recall	For bridging resolution, we report performance in precision, recall and F1.
bridging resolution	F1	For bridging resolution, we report performance in precision, recall and F1.
bridging resolution	precision	The evaluation of bridging resolution is computed using the widely known precision and recall measures (and the harmonic mean between them, F1).
bridging resolution	recall	The evaluation of bridging resolution is computed using the widely known precision and recall measures (and the harmonic mean between them, F1).
bridging resolution	F1	The evaluation of bridging resolution is computed using the widely known precision and recall measures (and the harmonic mean between them, F1).
anaphor detection	GRAIN	 Table 3: Baseline results for anaphor detection and full  bridging resolution on the test set of GRAIN.
classification	precision	Since the first question concerns classification performance, I use precision, recall, and F 1 score to answer this question.
classification	recall	Since the first question concerns classification performance, I use precision, recall, and F 1 score to answer this question.
classification	F 1 score	Since the first question concerns classification performance, I use precision, recall, and F 1 score to answer this question.
binary classification task	Pearson (.75) and Spearman (.68) correlations	It reaches an encouraging 75% accuracy on the binary classification task, and high Pearson (.75) and Spearman (.68) correlations on the gradient judgment prediction task.
Sentence Retrieval	TF-IDF overlap	We show in subsequent sections that models which perform well on SQuAD rely on lexical pattern matching, and are also not robust to variance in language style.: Sentence Retrieval Performance using Jaccard similarity, TF-IDF overlap and BM-25 overlap) scoring metrics
Sentence Retrieval	BM-25 overlap) scoring metrics	We show in subsequent sections that models which perform well on SQuAD rely on lexical pattern matching, and are also not robust to variance in language style.: Sentence Retrieval Performance using Jaccard similarity, TF-IDF overlap and BM-25 overlap) scoring metrics
politeness detection	accuracy	We also evaluate our model for formality and politeness detection and report comparable accuracy as against the state-of-the-art prior work.
Frustration	accuracies	Random Forest is the best predictor for Frustration while Logistic Regression has the highest accuracies for Formality and Politeness prediction.
Frustration prediction	precision	 Table 6: Accuracy for Frustration prediction when modeled as a 2-class classification problem. The  positive class is oversampled to correct for class imbalance. Random forest is the best performing  classifier with a precision= 0.88, recall= 0.85, and F1-Score= 0.85. The Affect features contribute  more to the accuracy as compared to the derived features. All values are reported for the experimental  setup with a 80-20 train-test split with 10 fold cross validation.
Frustration prediction	recall	 Table 6: Accuracy for Frustration prediction when modeled as a 2-class classification problem. The  positive class is oversampled to correct for class imbalance. Random forest is the best performing  classifier with a precision= 0.88, recall= 0.85, and F1-Score= 0.85. The Affect features contribute  more to the accuracy as compared to the derived features. All values are reported for the experimental  setup with a 80-20 train-test split with 10 fold cross validation.
Frustration prediction	F1-Score	 Table 6: Accuracy for Frustration prediction when modeled as a 2-class classification problem. The  positive class is oversampled to correct for class imbalance. Random forest is the best performing  classifier with a precision= 0.88, recall= 0.85, and F1-Score= 0.85. The Affect features contribute  more to the accuracy as compared to the derived features. All values are reported for the experimental  setup with a 80-20 train-test split with 10 fold cross validation.
Frustration prediction	accuracy	 Table 6: Accuracy for Frustration prediction when modeled as a 2-class classification problem. The  positive class is oversampled to correct for class imbalance. Random forest is the best performing  classifier with a precision= 0.88, recall= 0.85, and F1-Score= 0.85. The Affect features contribute  more to the accuracy as compared to the derived features. All values are reported for the experimental  setup with a 80-20 train-test split with 10 fold cross validation.
affix prediction (AP)	OOV rate	 Table 4: Results on affix prediction (AP) and sequence labeling (SL) tasks. Sequence labeling tasks have  16.5%, 27.1%, 28.5% OOV rate respectively.
sequence labeling (SL) tasks	OOV rate	 Table 4: Results on affix prediction (AP) and sequence labeling (SL) tasks. Sequence labeling tasks have  16.5%, 27.1%, 28.5% OOV rate respectively.
Sequence labeling tasks	OOV rate	 Table 4: Results on affix prediction (AP) and sequence labeling (SL) tasks. Sequence labeling tasks have  16.5%, 27.1%, 28.5% OOV rate respectively.
information extraction	certainty	Hence, it is important not only to automate information extraction but also to quantify the certainty of this information.
validation	accuracy	A grid search was employed to examine the effects of these parameters on the validation accuracy.
tagger	accuracy	As a result, tagger accuracy depends on training from a balanced sample of the network, rather than training on texts from a narrow subcommunity.
tagger	accuracy	 Table 3: Comparison of tagger accuracy using  network-based and random training/test splits
ASR	BLEU score	After training an ASR system on 300 hours of English, fine-tuning on 20 hours of Spanish-English yields a BLEU score of 20.2, compared to only 10.8 for an ST model without ASR pre-training.
ASR	WER	As a consequence, our ASR results are far from state-of-the-art: current end-to-end Kaldi systems obtain 16% WER on Switchboard train-dev, and 22.7% WER on the French Globalphone dev set.
ASR	WER	As a consequence, our ASR results are far from state-of-the-art: current end-to-end Kaldi systems obtain 16% WER on Switchboard train-dev, and 22.7% WER on the French Globalphone dev set.
ASR pre-training	ST	We believe that better ASR pre-training may produce better ST results, but we leave this for future work.
disfluency detection	repair	Most work on disfluency detection builds on the framework that annotates a disfluency in terms of a reparandum followed by an interruption point (+), an optional interregnum ({ }), and then the repair, if any.
incident response	LORELEI	The second case is relevant to incident response as modelled by LORELEI (, where there may only be a single target-language consultant available for which transcribed speech can be elicited, but the goal is to have an ASR model that generalizes to multiple speakers.
Classification	accuracy	 Table 5: Classification accuracy for various funniness- sorted dataset proportions and classifiers/feature sets.  MaxUF is the highest score for the not-funny class,  MinF is the lowest score for the funny class, and we  also provide Krippendorff's α for judge agreement.
Classification	MaxUF	 Table 5: Classification accuracy for various funniness- sorted dataset proportions and classifiers/feature sets.  MaxUF is the highest score for the not-funny class,  MinF is the lowest score for the funny class, and we  also provide Krippendorff's α for judge agreement.
GEC	accuracy	However, even with millions of labeled sentences, automatic GEC is challenging due to the lack of enough labeled training data to achieve high accuracy.
Rustomata	discodop	In case of Rustomata, a binarised and markovized grammar was induced with discodop (head-outward binarisation, v = 1, h = 2, cf.) in each iteration.
PoS tagging	PoS	This model reduces PoS tagging to a sequence of multi-class classification problems : the PoS of the words in the sentence are predicted one after the other using an averaged perceptron.
subgraph selection	recall	(1) First, in subgraph selection, there is no effective way to deal with inexact matches and the facts in subgraph are not ranked by relevance to the mention; however, we will later show that effective ranking can substantially improve the subgraph recall.
subgraph ranking	recall	Addressing these points, the contributions of this paper are three-fold: (1) We propose a subgraph ranking method with combined literal and semantic score to improve the recall of the subgraph selection.
sentiment classification	F1-score	We compute F1-score and accuracy values for sentiment classification and F1-score and weighted accuracy (Tong et al., 2017) for emotion classification.
sentiment classification	F1-score	For sentiment classification, our single-task framework reports an F1-score of 77.67% and accuracy value of 79.8% for the tri-modal inputs.
sentiment classification	accuracy	For sentiment classification, our single-task framework reports an F1-score of 77.67% and accuracy value of 79.8% for the tri-modal inputs.
emotion classification	F-score	For emotion classification, we also observe an improved F-score (78.6 (MTL) vs. 77.7 (STL)) and weighted accuracy (62.8 (MTL) vs. 60.8 (STL)): Single-task learning (STL) and Multi-task (MTL) learning frameworks for the proposed approach.: Example video for heatmap analysis of the contextual inter-modal (CIM) attention mechanism of the proposed MTMM-ES framework.
emotion classification	accuracy	For emotion classification, we also observe an improved F-score (78.6 (MTL) vs. 77.7 (STL)) and weighted accuracy (62.8 (MTL) vs. 60.8 (STL)): Single-task learning (STL) and Multi-task (MTL) learning frameworks for the proposed approach.: Example video for heatmap analysis of the contextual inter-modal (CIM) attention mechanism of the proposed MTMM-ES framework.
sentence-pair classification task	BERT	Since the input representation of BERT can represent both a single text sentence and a pair of text sentences, we can convert (T)ABSA into a sentence-pair classification task and fine-tune the pre-trained BERT.
content preservation	BLEU	For content preservation, WMD (based on EMD) works in a similar fashion, but with word embeddings of x and of x . BLEU, used widely in previous work, may yield weaker correlations with human judgment in comparison as it was designed to have multiple reference texts per candidate text ().
intent classification	accuracy	For intent classification, we measure accuracy of two models: an SVM () using bag of words feature representation, and FastText (), a neural network that averages across sentence embeddings and passes the result through feedforward layers.
intent detection	accuracy	We use F1 scores for intent detection at the token-level and accuracy for sentence-level intent detection.
slot labeling	F1	 Table 1: Performance of the model against Model 1 and Model 2. We report F1 scores for slot labeling. For intent  detection, we use F1 scores for intent detection at the token-level (T-level) and accuracies (acc) for sentence-level  (S-level) intent detection. N/A: as Models 1 and 2 perform single intent detection only at S-level.
intent  detection	F1	 Table 1: Performance of the model against Model 1 and Model 2. We report F1 scores for slot labeling. For intent  detection, we use F1 scores for intent detection at the token-level (T-level) and accuracies (acc) for sentence-level  (S-level) intent detection. N/A: as Models 1 and 2 perform single intent detection only at S-level.
SVM classification	accuracy	 Table 2: SVM classification accuracy: bag-of-words features; 80-20 train-test split; 5-fold cross validation. For  the first question, this distinguishes highlighted text vs. its complement (excluded vs. included). For the rest of the  questions, this distinguishes text of true instances from text of false instances, and is different from majority class  baseline  *  at p < 0.04, t = −3.5 and  *  *  at p < 0.01, t > |2.49|.
Cross-domain classification	F1	This would result in a more balanced distribution of microposts per author.: Cross-domain classification (eval.: F1).
Cross-domain classification	F1	 Table 5: Cross-domain classification (eval.: F1).
SRL integration	F-scores	In addition , we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.
SRL-SAWR	ORL	Meanwhile, our implicit SRL-SAWR method can achieve the best ORL performance, 2.23% higher F-scores than the second best method.
SRL-SAWR	F-scores	Meanwhile, our implicit SRL-SAWR method can achieve the best ORL performance, 2.23% higher F-scores than the second best method.
authorship verification of texts as short	precision	In this paper , we present a generalized unmasking approach which allows for authorship verification of texts as short as four printed pages with very high precision at an adjustable recall tradeoff.
authorship verification of texts as short	recall	In this paper , we present a generalized unmasking approach which allows for authorship verification of texts as short as four printed pages with very high precision at an adjustable recall tradeoff.
authorship verification	precision	To tackle the uncertainty problem of authorship verification on short texts, we propose a generalized unmasking approach which prioritizes precision so as to verify authorship with reliable results while rejecting cases of low certainty.
satire detection	precision	For evaluating satire detection, we use precision, recall and F1 score of the satire class.
satire detection	recall	For evaluating satire detection, we use precision, recall and F1 score of the satire class.
satire detection	F1 score	For evaluating satire detection, we use precision, recall and F1 score of the satire class.
publication identification	precision	For publication identification, we calculate a weighted macro precision, recall and F1 score, i.e., a weighted sum of class-specific scores with weights determined by the class distribution.
publication identification	recall	For publication identification, we calculate a weighted macro precision, recall and F1 score, i.e., a weighted sum of class-specific scores with weights determined by the class distribution.
publication identification	F1 score	For publication identification, we calculate a weighted macro precision, recall and F1 score, i.e., a weighted sum of class-specific scores with weights determined by the class distribution.
parsing	accuracy	Apart from increasing the parsing speed twofold (while keeping the same quadratic time complexity), it achieves the best accuracy to date among fully-supervised single-model dependency parsers on the PTB-SD, and obtains competitive accuracies on twelve different languages in comparison to the original top-down version.
NER	accuracy	 Table 4: NER accuracy on the CONLL03 and the  CHEMDNER datasets (Task 1). Scores for our methods  are the average of 5 runs. * indicates statistical signifi- cance of the improvement over SeqIE (p < 0.01).
OpenIE extractions	KB	OpenIE extractions and KB, we show that our method improves over state-of-the-arts by 33.5% on average across different datasets.
GEM	memory size	For GEM and EMR related methods, memory size of each task is set to be 50.
Pronoun coreference resolution	Precision (P)	 Table 2: Pronoun coreference resolution performance of different models on the evaluation dataset. Precision (P),  recall (R), and F1 score are reported, with the best one in each F1 column marked bold.
Pronoun coreference resolution	recall (R)	 Table 2: Pronoun coreference resolution performance of different models on the evaluation dataset. Precision (P),  recall (R), and F1 score are reported, with the best one in each F1 column marked bold.
Pronoun coreference resolution	F1 score	 Table 2: Pronoun coreference resolution performance of different models on the evaluation dataset. Precision (P),  recall (R), and F1 score are reported, with the best one in each F1 column marked bold.
Pronoun coreference resolution	F1	 Table 2: Pronoun coreference resolution performance of different models on the evaluation dataset. Precision (P),  recall (R), and F1 score are reported, with the best one in each F1 column marked bold.
dependency parsing task	LAS	 Table 5: Results on the dependency parsing task. The  two best configurations are selected according to LAS.
MWE annotation analysis	Irreg	 Table 7: MWE annotation analysis: numbers of MWEs  labelled complex (C) and non-complex (NC), numbers  with at least one SW (≥ 1 Irreg) and all SWs (All Ir- reg.) having the opposite label.
link prediction task	MRR	(2) We evaluate ConvR in the link prediction task on KBs and achieve very promising results on multiple benchmark datasets, including not only the popular WN18 and FB15K (), but also the more difficult WN18RR () and FB15K-237 (   increase in MRR and a 6% increase in Hits@10, with the total parameter number only 88% as large as that of ConvE.
link prediction task	Hits	(2) We evaluate ConvR in the link prediction task on KBs and achieve very promising results on multiple benchmark datasets, including not only the popular WN18 and FB15K (), but also the more difficult WN18RR () and FB15K-237 (   increase in MRR and a 6% increase in Hits@10, with the total parameter number only 88% as large as that of ConvE.
label prediction	precision	Experimental results show that our proposed approach improves label prediction quality, in terms of precision and nDCG, by 3% to 5% in three of the 5 tasks and is competitive in the others, even with a simple linear prediction model.
label prediction	nDCG	Experimental results show that our proposed approach improves label prediction quality, in terms of precision and nDCG, by 3% to 5% in three of the 5 tasks and is competitive in the others, even with a simple linear prediction model.
single-label classification	similarity score	We adopted this approach to do single-label classification by predicting the class that got the highest similarity score among all classes.
SMD	OOV	 Table 13: Example from SMD with 50% OOV. The OOV entity present in the dialog is {pittsburgh}
MTSA	sequence length	To summarize the comparison of MTSA with recently popular models, we show the memory consumption and time cost vs. sequence length respectively in(a) and 1(b) on synthetic data (batch size of 64 and feature channels of 300).
Transfer	Merge	We compare our adversarial adaptation method with three baseline methods: Transfer, Merge and Fine-tune.
Transfer	Fine-tune	We compare our adversarial adaptation method with three baseline methods: Transfer, Merge and Fine-tune.
sentiment classification	BRAND	Data: We investigate VLAEs in three binary classification tasks: sentiment classification on IMDb (Maas et al., 2011), disease-text classification on Reddit, where the task is to classify reddit posts as relevant or irrelevant to specific diseases, and churn prediction on Twitter (, where the task is to classify/predict if given tweets indicate user intention about leaving brands, e.g. the tweet "my days with BRAND are numbered" is a churny tweet.
offense target identification	Precision (P)	 Table 6: Results for offense target identification (level C). We report Precision (P), Recall (R), and F1 for each  model/baseline on all classes (GRP, IND, OTH), and weighted averages. Macro-F1 is also listed (best in bold).
offense target identification	Recall (R)	 Table 6: Results for offense target identification (level C). We report Precision (P), Recall (R), and F1 for each  model/baseline on all classes (GRP, IND, OTH), and weighted averages. Macro-F1 is also listed (best in bold).
offense target identification	F1	 Table 6: Results for offense target identification (level C). We report Precision (P), Recall (R), and F1 for each  model/baseline on all classes (GRP, IND, OTH), and weighted averages. Macro-F1 is also listed (best in bold).
phenotypegene relation extraction	RE	To generate a silver standard for phenotypegene relation extraction, we used a pipeline that performs: i) NER to recognize genes and human phenotype entities; ii) RE to classify a relation between human phenotype and gene entities.
MT	BLEU	We trained the MT models using a concatenation of the NEWS and the TED training datasets, and we tested on official TED test sets (testsets-11-13) to perform the evaluation using BLEU ().
parsing	accuracy	Ina parallel development, showed that using BiLSTMs for feature extraction can lead to high parsing accuracy even with fairly simple parsing architectures, and using BiLSTMs for feature extraction has therefore become very popular in dependency parsing.
early stopping	accuracy	We also apply early stopping, where validation accuracy is measured as average LAS score on the development set across all training languages.
early stopping	LAS score	We also apply early stopping, where validation accuracy is measured as average LAS score on the development set across all training languages.
Word translation	Failed	 Table 2: Word translation to English precision @5 using CSLS (Conneau et al., 2018a) with a dictionary (su- pervised) and without (unsupervised) for German (DE), Spanish (ES), French (FR), Italian (IT), Portuguese (PT)  and Swedish (SV). Each of the unsupervised results is followed by a line with the results post the anchor-based  refinement steps.  * stands for 'Failed to converge'.
rumour detection	accuracy	Most studies consider rumour detection as a binary classification problem, where they extract various features to capture rumour indicative signals for detecting a rumour, and a few recent works explore deep learning approaches to enhance detection accuracy (.
detection	accuracy	Our detection accuracy performance is better than a state-ofthe-art system that that detects rumours within 12 hours.
ERD	accuracy	Our experimental results showed that ERD outperforms state-of-the-art methods over two benchmark data sets in detection accuracy and timeliness.
ERD	timeliness	Our experimental results showed that ERD outperforms state-of-the-art methods over two benchmark data sets in detection accuracy and timeliness.
beam search	HUSE	We excluded beam search since it qualitatively behaves similarly to temperature annealing with low temperatures and HUSE ≈ 0 due to beam search being extremely Score Summarization Story generation Chit-chat dialogue LM t = 1.0 t = 0.7 t = 1.0 Retrieval t = 1.0 t = 0.7 t = 1.0   under diverse.
classification of deception	accuracy	Moreover, the automatic classification of deception can be performed with an accuracy that is better than random guessing and ourperforms human judgments.
event causal relation identification	precision	For event causal relation identification, we report precision, recall, and F1-score.
event causal relation identification	recall	For event causal relation identification, we report precision, recall, and F1-score.
event causal relation identification	F1-score	For event causal relation identification, we report precision, recall, and F1-score.
MT evaluation	EMD	WE WPI  We obtain WE WPI as new automatic MT evaluation metrics by adjusting EMD to the automatic MT evaluation task.
MT	BLEU	For these experiments, we used different automatic MT evaluation metrics for comparison with our WE WPI: BLEU, METEOR, IMPACT, RIBES, and WE.
MT	METEOR	For these experiments, we used different automatic MT evaluation metrics for comparison with our WE WPI: BLEU, METEOR, IMPACT, RIBES, and WE.
MT	IMPACT	For these experiments, we used different automatic MT evaluation metrics for comparison with our WE WPI: BLEU, METEOR, IMPACT, RIBES, and WE.
MT	RIBES	For these experiments, we used different automatic MT evaluation metrics for comparison with our WE WPI: BLEU, METEOR, IMPACT, RIBES, and WE.
MT	WE	For these experiments, we used different automatic MT evaluation metrics for comparison with our WE WPI: BLEU, METEOR, IMPACT, RIBES, and WE.
beam search	EXPERT-MML	We used randomized beam-search, which addsgreedy exploration to beam search, which was proposed by and performed better 4 . 2. EXPERT-MML: An alternative way of using the target denotation y at training time, based on imitation learning (, is to train an expert policy π expert θ , which receives y as input in addition to the parsing state, and trains with the MML objective.
automatic detection of AD	accuracy	Previous models have applied machine learning to automatic detection of AD, for example, extracted a wide variety of lexicosyntactic and acoustic features to classify AD and obtained 82% accuracy on the DementiaBank (DB) dataset.
classify AD	accuracy	Previous models have applied machine learning to automatic detection of AD, for example, extracted a wide variety of lexicosyntactic and acoustic features to classify AD and obtained 82% accuracy on the DementiaBank (DB) dataset.
classification	accuracy	augmented DB with more a much larger corpus of normative data and improved the classification accuracy to 93% on DB.
MT	BPE	For MT, we segment the English words with the BPE algorithm () using a maximum of 30K merge operations.
ASR	word error rate (WER)	ASR performance is measured with word error rate (WER) computed on lower-cased, tokenized texts without punctuation.
Adaptation	BLEU-or TER-derived metrics	Adaptation is most commonly assessed using corpus-level BLEU-or TER-derived metrics that do not explicitly take adaptation speed into account.
MT quality metrics	SBLEU	 Table 1: Results on the Autodesk test set for tradi- tional MT quality metrics. SBLEU refers to an average  of sentence-wise BLEU scores as described by Nakov  et al. (2012). The best result in each column is denoted  with bold font.
MT quality metrics	BLEU	 Table 1: Results on the Autodesk test set for tradi- tional MT quality metrics. SBLEU refers to an average  of sentence-wise BLEU scores as described by Nakov  et al. (2012). The best result in each column is denoted  with bold font.
IWSLT translation tasks	BLEU	Experiments on IWSLT translation tasks show that our approach improves BLEU compared to maximum likelihood and scheduled sampling baselines.
irony detection	accuracy	Regarding irony detection, we report accuracy and macro-average F1 (all labels weighted equal regardless of frequency).
irony detection	F1	Regarding irony detection, we report accuracy and macro-average F1 (all labels weighted equal regardless of frequency).
sentiment analysis	accuracy	Regarding sentiment analysis, we report accuracy, average recall and F1.
sentiment analysis	average	Regarding sentiment analysis, we report accuracy, average recall and F1.
sentiment analysis	recall	Regarding sentiment analysis, we report accuracy, average recall and F1.
sentiment analysis	F1	Regarding sentiment analysis, we report accuracy, average recall and F1.
irony detection	Accuracy	This architecture is simpler than previous proposals, but as we shall see,: Results on irony detection (Accuracy and Macro F1).
irony detection	Accuracy	 Table 3: Results on irony detection (Accuracy and Macro F1). Task A is a binary classification (yes / no) and Task  B is a four-way classification (verbal irony with polarity contrast, other verbal irony, situational irony, non-irony).
AMR generation	BLEU	The highest reported result for AMR generation is 33.8 BLEU (); on the same dataset our best model obtains 75.8 BLEU.
AMR generation	BLEU	The highest reported result for AMR generation is 33.8 BLEU (); on the same dataset our best model obtains 75.8 BLEU.
text generation tasks	METEOR	We use BLEU (), an n-gram overlap measure popular in text generation tasks, and METEOR), a machine translation with paraphrase and language-specific considerations.
RRC	BERT	5) for RRC, which is insufficient to fine-tune BERT to ensure full task-awareness of the system 3 . To address all the above challenges, we propose a novel joint post-training technique that takes BERT's pre-trained weights as the initialization 4 for basic language understanding and adapt BERT with both domain knowledge and task (MRC) knowledge before fine-tuning using the domain end task annotated data for the domain RRC.
MRC	Exact Match (EM)	To be consistent with existing research on MRC, we use the same evaluation script from SQuAD 1.1 ( for RRC, which reports Exact Match (EM) and F1 scores.
MRC	F1 scores	To be consistent with existing research on MRC, we use the same evaluation script from SQuAD 1.1 ( for RRC, which reports Exact Match (EM) and F1 scores.
ASC	accuracy	For ASC, we compute both accuracy and Macro-F1 over 3 classes of polarities, where Macro-F1 is the major metric as the imbalanced classes introduce biases on accuracy.
ASC	accuracy	For ASC, we compute both accuracy and Macro-F1 over 3 classes of polarities, where Macro-F1 is the major metric as the imbalanced classes introduce biases on accuracy.
ASC	Accuracy	 Table 6: ASC in Accuracy and Macro-F1(MF1).
RL	accuracy	Regarding the performance of various RL approaches such as ReMatch (), SIBKB ( ) is still low concerning accuracy and run-time even if they are purposefully developed fora particular domain or task.
GED	BERT-LM	We compare our neural model trained on both artificially generated errors and ESL data (LSTM ESL+art ) to three baselines: a neural model trained only on ESL data (LSTM ESL ) (i.e., reflecting the performance of current state-of-the-art approaches for GED), a language model based method (BERT-LM) and our rule-based system.
name tagging	F1 score	In the name tagging task, our LSTM-CNN baseline obtains 78.76% and 70.76% F1 score for Kinyarwanda and Sinhalese, respectively.
summarization	F1 ROUGE scores	We evaluate the summarization performance with two language metrics: perplexity and standard F1 ROUGE scores).
classification	F1 score	For classification, we report multiclass classification accuracy A(c) where c denotes the number of classes and F1 score.
neural natural language generation (NLG)	length	Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style), affect (), politeness), persona ( voice, grammatical correctness (, and length (.
MRC task	accuracy	We further fine-tune the resulting model on a target MRC task, leading to an absolute improvement of 6.2% in average accuracy over previous state-of-the-art approaches on six representative non-extractive MRC datasets from different domains (i.e., ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, ROCStories, and MultiRC).
strategy highlighting	accuracy	For strategy highlighting (Section 3.3), the contentword POS tagset T = {NN, NNP, NNPS, NNS, VB, VBD, VBG, VBN, VBP, VBZ, JJ, JJR, JJS, RB, RBR, RBS, CD, FW}, and we randomly initialize + and − .  In, we first report the accuracy of the state-of-the-art models (MMN and original finetuned GPT) and Amazon Turkers (Human performance).
RACE	BF	 Table 2: Accuracy (%) on the test set of RACE (#:  number of (ensemble) models; SA: Self-Assessment;  HL: Highlighting; BF: Back and Forth Reading; : our  implementation).
prediction	exponential moving average (EMA,	For prediction, we compute an exponential moving average (EMA, of model parameters with a decay rate of 0.995 and use it to compute the model performance.
parsing	accuracy	Firstly, we propose an iterative search procedure for gradually increasing the complexity of candidate logical forms for each training instance, leading to better training data and better parsing accuracy.
QA	AHE	Many neural approaches for QA have been proposed in the past few years, with impressive results on several QA tasks; 1 Code: https://github.com/vikas95/AHE Question -Which sequence of energy transformations occurs after a battery-operated flashlight is turned on?
CWS	F-scores	Existing studies have achieved satisfactory results for in-domain CWS, with F-scores over 96.0% in the newspaper domain . Nevertheless, cross-domain CWS remains a big challenge (.
syntactic parsing of transcribed speech	accuracy	In summary, the main contributions of this paper are: • We show that the self-attentive constituency parser sets anew state-of-the-art for syntactic parsing of transcribed speech, • A neural constituency parser can detect EDITED words with an accuracy surpassing that of specialized disfluency detection models, • We demonstrate that syntactic information helps the neural syntactic parsing detect disfluent words more accurately, • Replacing the constituent-based representation of disfluencies with a word-based representation of disfluencies improves the detection of disfluent words, • Modifying the training loss function to put more weight on EDITED nodes during training also improves disfluency detection.
parsing	accuracy	We evaluate the self-attentive parser in terms of parsing accuracy and disfluency detection performance.
relation extraction	accuracy	Our method also achieves better relation extraction accuracy than state-of-the-art methods on this dataset 1 .
text classification tasks	reliability	 Table 7: F1 scores on text classification tasks when  only the reliability differs between the annotators.
IE	precision	Semi-structured websites have recently been shown to be a rich target for IE; the Knowledge Vault large-scale web extraction experiment, which extracted from semi-structured websites, natural language text, webtables, and Semantic Web annotations, found that semi-structured websites contributed 75% of total extracted facts and 94% of high-confidence extractions (); the Ceres system showed that one can automatically extract from semi-structured sites with a precision over 90% using distant supervision ().
medical diagnosis system (van der)	accuracy	For example, a medical diagnosis system (van der) is expected to have a very high accuracy to support correct decisionmaking for medical practitioners.
meaning preservation	simplicity	As expected, meaning preservation does substantially increase as we increase the average sentence length, while simplicity decreases.
G2P conversion	accuracy	(iv) We perform extensive simulations on largescale dataset and show that our methodology outperforms other state-of-the-art approaches for G2P conversion in Bangla language by providing word-level accuracy of 90.2%.
POS tagging	early stop (small difference on validation error)	 Table 2: Performance on POS tagging task for UPOS tags using CRF. The models were trained on 100 epochs  with an early stop (small difference on validation error) mechanism enabled. Considering F1 score, we evaluate  on 2 variants of test data: Original (correctly spelled) on the right hand side of the table and 100% misspelled on  the left hand side.
POS tagging	F1 score	 Table 2: Performance on POS tagging task for UPOS tags using CRF. The models were trained on 100 epochs  with an early stop (small difference on validation error) mechanism enabled. Considering F1 score, we evaluate  on 2 variants of test data: Original (correctly spelled) on the right hand side of the table and 100% misspelled on  the left hand side.
classification	accuracy	We run all the experiments 5 times and report the average classification accuracy in.
link prediction task	mean reciprocal rank (MRR)	To capture the effect of our method on link prediction task, we study the change in commonly-used metrics for evaluation in this task: mean reciprocal rank (MRR) and Hits@K. Further, we use the same hyperparameters as in Dettmers et al. for training link prediction models for these knowledge graphs.
parsing	speed	In an alternative line of work, some authors have proposed new parsing paradigms that aim to both reduce the complexity of existing parsers and improve their speed.
NER	BiGRU	• We introduce a character-based Chinese NER model that consists of combined CNN with local attention and BiGRU with global selfattention layers.
POS taggers	accuracy	Existing POS taggers for canonical German text already achieve very good results around 97% accuracy, e.g..
vocabulary selection problem	accuracy	The rest of the paper is organized as follows: We first formally define the vocabulary selection problem (subsection 2.1) and present a quantitative study on classification accuracy with different vocabulary selections to showcase its importance in the end task (subsection 2.2).
Cross-Lingual Parsing Evaluation	LAS	 Table 2: Cross-Lingual Parsing Evaluation. LAS of an  ensemble of = 9 SINGLE TASK (BASE++) models,  and the performance gain by the Bayesian MULTI TASK  (+PRECOND), placed as subscript.
Cross-Lingual Parsing Evaluation	MULTI TASK  (+PRECOND)	 Table 2: Cross-Lingual Parsing Evaluation. LAS of an  ensemble of = 9 SINGLE TASK (BASE++) models,  and the performance gain by the Bayesian MULTI TASK  (+PRECOND), placed as subscript.
Subject and object plurality prediction	recall	 Table 3: Subject and object plurality prediction for different word orders (recall for the subject is 100% and is not  indicated). The % attractors columns indicate the percentage of sentences containing verb-argument attractors.  The number are averaged over four runs and the error interval represents the standard deviation.
translation	TRANSFORMER	For translation tasks, we validate our approach on top of the advanced TRANSFORMER model () on both WMT14 English⇒German and WMT17 Chinese⇒English data.
citation intent classification	F1	Our contributions are: (i) we propose a neural scaffold framework for citation intent classification to incorporate into citations knowledge from structure of scientific papers; (ii) we achieve anew state-of-the-art of 67.9% F1 on the ACL-ARC citations benchmark, an absolute 13.3% increase over the previous state-of-the-art (; and (iii) we introduce SciCite, anew dataset of citation intents which is at least five times as large as existing datasets and covers a variety of scientific domains.
SemEval 2018 Task 6	Precision	Chrono's annotation of the Evaluation Corpus was uploaded to the Post-Evaluation submission system for SemEval 2018 Task 6, and overall Precision, Recall, and F1 measures are reported in.
SemEval 2018 Task 6	Recall	Chrono's annotation of the Evaluation Corpus was uploaded to the Post-Evaluation submission system for SemEval 2018 Task 6, and overall Precision, Recall, and F1 measures are reported in.
SemEval 2018 Task 6	F1	Chrono's annotation of the Evaluation Corpus was uploaded to the Post-Evaluation submission system for SemEval 2018 Task 6, and overall Precision, Recall, and F1 measures are reported in.
cross-sentence relation extraction	recall	Such relations often involve entity mentions that are far apart in the document, yet existing work on cross-sentence relation extraction is generally confined to small text spans (e.g., three consecutive sentences), which severely limits recall.
Maximiz-ing	recall	Maximiz-ing recall is thus of paramount importance.
document-level extraction	precision	To combat the ensuing difficulties in document-level extraction, such as low precision, we introduce multiscale learning, which combines representations learned over text spans of varying scales and for various subrelations).
Dialog State Tracking Challenge 4	F1 scores	In experiments on the benchmark dataset used in Dialog State Tracking Challenge 4, the proposed models achieved significantly higher F1 scores than the state-of-the-art contextual models.
SLU	accuracy	The analysis demonstrates that the proposed methods were effective to improve SLU accuracy individually.
SLU	accuracy	In SLU, selecting important history information is crucial, and it directly influences the improvement of SLU accuracy.
SLU	accuracy	In this paper, we propose flexible and effective time-aware attention models to improve SLU accuracy.
SLU	accuracy	Also, we examine how the proposed methods affect the SLU accuracy in detail.
SLU	accuracy	This result shows that the proposed methods were effective to improve SLU accuracy individually.
SLU	accuracy	To evaluate SLU accuracy, we used the F1 score, which is the harmonic mean of precision and recall.
SLU	F1 score	To evaluate SLU accuracy, we used the F1 score, which is the harmonic mean of precision and recall.
SLU	precision	To evaluate SLU accuracy, we used the F1 score, which is the harmonic mean of precision and recall.
SLU	recall	To evaluate SLU accuracy, we used the F1 score, which is the harmonic mean of precision and recall.
classification	accuracy	We evaluate the classification accuracy on the test set.
Cross-lingual transfer	F-score	 Table 4: Cross-lingual transfer when the target lan- guage has no resources (F-score %).
Cross-lingual transfer	F-score	 Table 5: Cross-lingual transfer when the target language has resources (F-score %).
annotation projection	accuracy	We excluded the following languages from the set of source languages for annotation projection due to their low supervised accuracy: Estonian, Hungarian, Korean, Latin, Lithuanian, Latvian, Turkish, Ukrainian, Vietnamese, and Chinese.
Dependency parsing	labeled attachment  accuracy (LAS)	 Table 2: Dependency parsing results, in terms of unlabeled attachment accuracy (UAS) and labeled attachment  accuracy (LAS) after ignoring punctuations, on the Universal Dependencies v2 test sets (Nivre et al., 2017) using  supervised part-of-speech tags. The results are sorted by their "difference" between the ensemble model and the  baseline. The rows for non-European languages are highlighted with cyan. The rows that are highlighted by pink  are the ones that the transfer model outperforms the supervised model. For all of the non-European datasets except  "hi", our model outperforms significantly better in terms of UAS with p < 0.001 using McNemar's test.
Dependency parsing	UAS	 Table 2: Dependency parsing results, in terms of unlabeled attachment accuracy (UAS) and labeled attachment  accuracy (LAS) after ignoring punctuations, on the Universal Dependencies v2 test sets (Nivre et al., 2017) using  supervised part-of-speech tags. The results are sorted by their "difference" between the ensemble model and the  baseline. The rows for non-European languages are highlighted with cyan. The rows that are highlighted by pink  are the ones that the transfer model outperforms the supervised model. For all of the non-European datasets except  "hi", our model outperforms significantly better in terms of UAS with p < 0.001 using McNemar's test.
word translation	accuracy	Following the tradition, we evaluate our model on word translation (a.k.a. bilingual lexicon induction) task, which measures the accuracy of the predicted dictionary to a gold standard dictionary.
bilingual lexicon induction)	accuracy	Following the tradition, we evaluate our model on word translation (a.k.a. bilingual lexicon induction) task, which measures the accuracy of the predicted dictionary to a gold standard dictionary.
sentence translation retrieval	accuracy	We evaluated the cross-lingual mapping approaches on sentence translation retrieval, where we calculate the accuracy of retrieving the correct translation from the target side of a test parallel corpus using nearest neighbor search with cosine similarity.
Word translation	precision	 Table 1: Word translation precision at k (%) using k  nearest neighbor search, with k ∈ {1, 5}.
dependency parsing	labelled attachment score (LAS)	Metrics The quality of the predicted trees is assessed with a standard measure for dependency parsing: labelled attachment score (LAS).
headline generation	ROUGE scores	Recent work in the field of automatic sum-marization and headline generation focuses on maximizing ROUGE scores for various news datasets.
summarization evaluation	ROUGE	Our contributions in this work are: (1) We first present APES, anew extrinsic summarization evaluation metric; (2) We show APES strength through an analysis of its correlation with Pyramid and Responsiveness manual metrics; (3) we present anew abstractive model which maximizes APES by increasing attention scores of salient entities, while increasing ROUGE to competitive level.
APES	ROUGE	Our contributions in this work are: (1) We first present APES, anew extrinsic summarization evaluation metric; (2) We show APES strength through an analysis of its correlation with Pyramid and Responsiveness manual metrics; (3) we present anew abstractive model which maximizes APES by increasing attention scores of salient entities, while increasing ROUGE to competitive level.
summaries quality assessment	clarity	Other relevant quantities for summaries quality assessment include: readability (or fluency), grammaticality, coherence and structure, focus, referential clarity, and non-redundancy.
Neural abstractive summarizers generate summary texts	ROUGE	Neural abstractive summarizers generate summary texts using a language model conditioned on the input source text, and have recently achieved high ROUGE scores on benchmark summarization datasets.
summarization tasks	BLEU	We use different metrics for each part because ROUGE is often used for summarization tasks while BLEU is used for conversational tasks.
SMT	error rate	Expanding the SMT lattice significantly reduces the oracle error rate from 55.63% to 48.17%.
document classification	accuracy	We show that this is also the case for document classification: in a large-scale reproducibility study of several recent neural models, we find that a simple BiLSTM architecture with appropriate regu-larization yields accuracy and F 1 that are either competitive or exceed the state of the art on four standard benchmark datasets.
document classification	F 1	We show that this is also the case for document classification: in a large-scale reproducibility study of several recent neural models, we find that a simple BiLSTM architecture with appropriate regu-larization yields accuracy and F 1 that are either competitive or exceed the state of the art on four standard benchmark datasets.
machine translation	BLEU	Our experiments in machine translation show gains of up to 5.3 BLEU in a simulated resource-poor setup.
SNLI-style sentence generation	VAE	 Table 1: Results of SNLI-style sentence generation,  where WAE is compared with DAE and VAE. D and  S refer to the deterministic and stochastic encoders, re- spectively. ↑/↓ The larger/lower, the better. For En- tropy and AvgLen, the closer to corpus statistics, the  better (indicated by the → arrow).
word recognition	ease	Some prominent theories of word recognition claim that ease of lexical access is modulated by the strength of a word's representation in memory, independently of contextual factors that guide prediction).
negation	accuracy	Examples that involve negation or understanding antonyms have lower accuracy (42.8%), similarly to examples that require factoid knowledge (38.4%).
single-sentence classification task consisting of sentences extracted from movie reviews	BERT E E 1 E	SST-2 The Stanford Sentiment Treebank is a binary single-sentence classification task consisting of sentences extracted from movie reviews BERT E E 1 E ...
domain classification model	accuracy	In this paper, we introduce an approach for leveraging available data across multiple lo-cales sharing the same language to 1) improve domain classification model accuracy in Spoken Language Understanding and user experience even if new locales do not have sufficient data and 2) reduce the cost of scaling the domain classifier to a large number of lo-cales.
Domain classification	accuracy	 Table 4: Domain classification accuracy over different domain categories and different locales.
VRDs	soon	Examples of VRDs are purchase receipts, insurance policy documents, custom declaration forms and soon.
Machine Translation	BLEU score	Furthermore, research on sequence-to-sequence models often borrows automated evaluation metrics from the Machine Translation literature such as BLEU score (, which have been shown to correlate only weakly with human judgment (.
slot tagging	F-score	Results show that the use of neural lexicon information leads to a significant improvement in slot tagging, with improvements in the F-score of up to 12%.
dialog system development	accuracy	In our company, we are exploring and comparing several toolsets in an effort to determine their strengths and weaknesses in meeting our goals for dialog system development: accuracy, time to market, ease of replicating and extending applications, and efficiency and ease of use by developers.
link prediction	Mean Reciprocal Rank (MRR)	This setup originates from the link prediction and knowledge base completion tasks in which the evaluation criteria are hits@k and Mean Reciprocal Rank (MRR), where k ranges from 1 to 20.
Infobox Extraction (IBE) problem	precision	Despite previous successes in the Infobox Extraction (IBE) problem (e.g., DB-pedia), three major challenges remain: 1) De-terministic extraction patterns used in DBpe-dia are vulnerable to template changes; 2) Over-trusting Wikipedia anchor links can lead to entity disambiguation errors; 3) Heuristic-based extraction of unlinkable entities yields low precision, hurting both accuracy and completeness of the final KB.
Infobox Extraction (IBE) problem	accuracy	Despite previous successes in the Infobox Extraction (IBE) problem (e.g., DB-pedia), three major challenges remain: 1) De-terministic extraction patterns used in DBpe-dia are vulnerable to template changes; 2) Over-trusting Wikipedia anchor links can lead to entity disambiguation errors; 3) Heuristic-based extraction of unlinkable entities yields low precision, hurting both accuracy and completeness of the final KB.
Heuristic-based extraction of unlinkable entities	precision	Despite previous successes in the Infobox Extraction (IBE) problem (e.g., DB-pedia), three major challenges remain: 1) De-terministic extraction patterns used in DBpe-dia are vulnerable to template changes; 2) Over-trusting Wikipedia anchor links can lead to entity disambiguation errors; 3) Heuristic-based extraction of unlinkable entities yields low precision, hurting both accuracy and completeness of the final KB.
Heuristic-based extraction of unlinkable entities	accuracy	Despite previous successes in the Infobox Extraction (IBE) problem (e.g., DB-pedia), three major challenges remain: 1) De-terministic extraction patterns used in DBpe-dia are vulnerable to template changes; 2) Over-trusting Wikipedia anchor links can lead to entity disambiguation errors; 3) Heuristic-based extraction of unlinkable entities yields low precision, hurting both accuracy and completeness of the final KB.
ASR	F-score	In the Experiment IV, we show that with the accuracy improvement from ASR, our F-score is improving as well.
ASR	F-score	Compared to testing on human transcribed data, testing on ASR data performed much worse with a decrease of almost fourteen points in terms of F-score.
MLP	relu activation	For the MLP in duplicate classification we use 2 layers of fully connected layers of 100 hidden neurons each, with relu activation and 20% dropout rate.
duplicate classification	relu activation	For the MLP in duplicate classification we use 2 layers of fully connected layers of 100 hidden neurons each, with relu activation and 20% dropout rate.
duplicate classification	statistical  significance	 Table 2: Comparison of different methods for duplicate classification task on multiple datasets. * denotes statistical  significance with the runner up for p-value ă 0.01
speech recognition	P- value	 Table 4: Statistical significance of various factors on speech recognition performance. For entries where the P- value is not mentioned, it is almost zero.
NER	F 1 -scores	Note that in our NER setup, we evaluate using BIO-2 labels, so F 1 -scores reported below might not be comparable to prior work.
AMR generation	BLEU	We also address the difficulties of automatically evaluating AMR generation systems and the problems with BLEU for this task.
AMR generation	BLEU	Most previous work in AMR generation has reported results exclusively using BLEU scores, with the original sentence as the only reference.
AMR generation	METEOR	It is a particularly appealing alternative to BLEU for AMR generation because, instead of only giving credit to exact word matches, METEOR also allows matching based on stems, synonyms, and paraphrases.
AMR generation task	BLEU	This robustness against lack of extra references makes it, too, likely to be better suited to the AMR generation task than BLEU is.
AMR Generation	BLEU	As discussed in 2.2, AMR Generation is usually evaluated only with BLEU, but one shared task obtained human judgments of five systems which were shown not to correlate well with BLEU scores.
AMR Generation	BLEU	As discussed in 2.2, AMR Generation is usually evaluated only with BLEU, but one shared task obtained human judgments of five systems which were shown not to correlate well with BLEU scores.
SemEval shared task	BLEU	In the SemEval shared task, only BLEU scores were originally reported alongside measures of human rankings.
event detection from tweets	SEDTWik	This paper presents the problems associated with event detection from tweets and a tweet-segmentation based system for event detection called SEDTWik, an extension to a previous work, that is able to detect newsworthy events occurring at different locations of the world from a wide range of categories.
event detection	SEDTWik	This paper presents the problems associated with event detection from tweets and a tweet-segmentation based system for event detection called SEDTWik, an extension to a previous work, that is able to detect newsworthy events occurring at different locations of the world from a wide range of categories.
translation	accuracy	This model performed as well as conventional NMT, and it significantly improved the translation accuracy for rare words.
intent prediction	accuracy	We achieved performance improvements of up to 0.57% and 3.25 in intent prediction (accuracy) and slot filling (f1-score), respectively.
question answering	BERT	We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit.
question answering	BERT-based	In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion.
stance prediction	accuracy	Our best model achieved the result with a stance prediction accuracy of 63.2% which is a 4.5% overall accuracy improvement compared to the current supervised classification systems developed using the benchmark dataset for code-mixed data stance detection.
stance prediction	accuracy	Our best model achieved the result with a stance prediction accuracy of 63.2% which is a 4.5% overall accuracy improvement compared to the current supervised classification systems developed using the benchmark dataset for code-mixed data stance detection.
classification	accuracy	Altogether, we improve classification accuracy over the previous state of the art on the Wikipedia Toxicity, Personal Attack, and Aggression datasets (.
classification	Aggression	Altogether, we improve classification accuracy over the previous state of the art on the Wikipedia Toxicity, Personal Attack, and Aggression datasets (.
lexicon expansion	accuracy	From that, we conclude that lexicon expansion is worthwhile as it improves prediction accuracy in the majority of our experiments, especially for feature-based learning.
predicting stance	morality	 Table 2: Result of predicting stance (first 12 columns) and morality (last two columns) with SVM and RF for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each half  column) in bold, highest accuracy per each model (each column) in gray)
predicting stance	RF	 Table 2: Result of predicting stance (first 12 columns) and morality (last two columns) with SVM and RF for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each half  column) in bold, highest accuracy per each model (each column) in gray)
predicting stance	Accuracy	 Table 2: Result of predicting stance (first 12 columns) and morality (last two columns) with SVM and RF for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each half  column) in bold, highest accuracy per each model (each column) in gray)
predicting stance	accuracy	 Table 2: Result of predicting stance (first 12 columns) and morality (last two columns) with SVM and RF for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each half  column) in bold, highest accuracy per each model (each column) in gray)
predicting stance	morality	 Table 3: Result of predicting stance (first 7 columns) and morality (last column) with LSTM model for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each  half column) in bold, highest accuracy per each model (each column) in gray)
predicting stance	Accuracy	 Table 3: Result of predicting stance (first 7 columns) and morality (last column) with LSTM model for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each  half column) in bold, highest accuracy per each model (each column) in gray)
predicting stance	accuracy	 Table 3: Result of predicting stance (first 7 columns) and morality (last column) with LSTM model for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each  half column) in bold, highest accuracy per each model (each column) in gray)
CMA task	F-scores	 Table 3: Results for the CMA task. Bold indicates the best scoring system, while italics indicates an 'unofficial'  result that was submitted after the deadline. These scores are F-scores. For the Analysis column every part of the  analysis had to be correct, for the Lemma column the lemma had to be correct and for the Tag column just the  part-of-speech tag had to be correct. BASELINE-I refers to the neural system and BASELINE-II to the neural  ensemble described in Section 5.3.
CMA task	BASELINE-I	 Table 3: Results for the CMA task. Bold indicates the best scoring system, while italics indicates an 'unofficial'  result that was submitted after the deadline. These scores are F-scores. For the Analysis column every part of the  analysis had to be correct, for the Lemma column the lemma had to be correct and for the Tag column just the  part-of-speech tag had to be correct. BASELINE-I refers to the neural system and BASELINE-II to the neural  ensemble described in Section 5.3.
CMA task	BASELINE-II	 Table 3: Results for the CMA task. Bold indicates the best scoring system, while italics indicates an 'unofficial'  result that was submitted after the deadline. These scores are F-scores. For the Analysis column every part of the  analysis had to be correct, for the Lemma column the lemma had to be correct and for the Tag column just the  part-of-speech tag had to be correct. BASELINE-I refers to the neural system and BASELINE-II to the neural  ensemble described in Section 5.3.
Improving Cuneiform Language Identification	BERT	Improving Cuneiform Language Identification with BERT
text normalization	BLEU score	The results show that character-based neural machine translation was the most promising strategy for text normalization and that in combination with phrase-based statistical machine translation it achieved 36% BLEU score.
SMT	BLEU	 Table 5: Comparison of SMT and NMT (top BLEU scores) on two segmentation schemes
POS tagging	accuracy	 Table 2: POS tagging accuracy (%) on the four corpora. Average over five runs with different random  seeds. Bold and italics font indicates the best result in our experiments, while bold font indicates the best  results compared to the state-of-the-art systems. We refer to BiLSTM-CRF Tagger as (1), MTL-POS  Tagger that learns POS tag for related languages as (2), and MTL-POS+LID that learns jointly POS  tagging and language identification as (3). Random-Initi-Embed refers to Random initialized embedding
text classification	F 1 -score	This work investigates different machine learning methods which are proven to be effective in text classification and compares them by their obtained F 1 -score, accuracy, and training time.
text classification	accuracy	This work investigates different machine learning methods which are proven to be effective in text classification and compares them by their obtained F 1 -score, accuracy, and training time.
GDI	F-1	Our system for GDI achieved third place out of 6 teams, with a macro averaged F-1 of 74.6%.
MRC subtasks	macro-F 1 score	We participated in all three MRC subtasks and we managed to rank on the first place in the first subtask (Moldavian vs. Romanian dialect identification), with a macro-F 1 score of 0.89, surpassing the other five participants by more than 10%.
ERE detection	RELATION	A particularly important class of extraction tasks is ERE detection in which an object, typically an element in a knowledge base, is created for each ENTITY, RELATION, and EVENT identified in a given text.
ERE detection	EVENT	A particularly important class of extraction tasks is ERE detection in which an object, typically an element in a knowledge base, is created for each ENTITY, RELATION, and EVENT identified in a given text.
R2R validation unseen paths	SR	 Table 3: Results on R2R validation unseen paths (U) and seen paths (S) when trained only with small fraction of  Fried-Augmented ordered by discriminator scores. For Random Full study, examples are sampled uniformly  over entire dataset. For Random Top/Bottom study, examples are sampled from top/bottom 40% of ordered dataset.  SPL and SR are reported as percentages and NE and PL in meters.
R2R validation unseen paths	NE	 Table 3: Results on R2R validation unseen paths (U) and seen paths (S) when trained only with small fraction of  Fried-Augmented ordered by discriminator scores. For Random Full study, examples are sampled uniformly  over entire dataset. For Random Top/Bottom study, examples are sampled from top/bottom 40% of ordered dataset.  SPL and SR are reported as percentages and NE and PL in meters.
Biomedical image captioning publicly	PEIR GROSS)	 Table 1: Biomedical image captioning publicly available datasets. Images are annotated with tags, which may be  medical terms (IU X-RAY) or words from the captions (PEIR GROSS) (Jing et al., 2018). A text may be linked to  a single image (PEIR GROSS & ICLEF-CAPTION) or multiple ones (IU X-RAY). It may comprise a single sentence  (PEIR GROSS) or multiple sentences (ICLEF-CAPTION, IU X-RAY). The lower table shows the number of training  and test instances (image-text-tags triples) in each dataset, as used in our experiments. We excluded 40 out of the  7,470 IU X-RAY instances, as discussed in the main text.
MLT	UA	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
MLT	APS	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
MLT	APHW	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
MLT	TCPA	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
MLT	SR	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
MLT	Skewness Ratio	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
MLT	WSR	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
MLT	Weigthed average of SRs	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
relation extraction task	F1	In the end-to-end relation extraction task (Table 3, bottom), the neural system is again two points better than the SVM in F1 score.
Natural language processing (NLP)	BERT	Natural language processing (NLP) has been shaken in recent months with the dramatic successes enabled by transfer learning and contextual word embedding models, such as ELMo (,, and BERT ().
normalization	agreement	Section 4 proposes a methodology for the annotation, normalization, agreement and evaluation of a corpus based on this annotation model.
ICU transfer	ICU	 Table 1: Patient, visit, and messages information of data between years 2015 and 2017 used to train models for  predicting ICU transfer. We indicate standard deviation in parentheses. ICU % is the ratio of mheaders resulting  in ICU transfer within 3 days of the message send date.
QA	MEANS	Accordingly, there has been vital interest in developing clinical QA systems, e.g.,),, and MEANS.
sentiment analysis	VADER	Traditionally, sentiment analysis has been approached with a lexicon-based majority vote approach, where a dictionary of terms and their associated sentiments (e.g. SentiWordnet, Pattern, SO-CAL, VADER) are queried to determine the sentiment of a given text).
Sentiment classification	LSTM	• Sentiment classification: IMDB movie reviews (Maas et al., 2011), with a single 100-d LSTM.
stance prediction	F1	 Table 3: Performance of stance prediction models trained only on user attributes, shown here for each of the  different stance targets. Bold indicates best in column for user attributes and inferred factors. The weighted F1  is shown for each target and the last column is the unweighted average across all targets.  † indicates statistical  significance at the 0.05 level compared to the MFC baseline.
MMI	maximum likelihood estimation (MLE) objective	More specifically, p(r|q) in MMI is trained only by maximum likelihood estimation (MLE) objective at training time (we use p(r|q) to denote the probability distribution of predicting the response r given the query q).
summarization	length	Neural approaches to summarization however have done away with length requirements.
summarization	ROUGE	For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation.
summarization	recall	For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation.
Surface Realization	BLEU score	However in the Surface Realization shared task correlation between BLEU score and human evaluation was noted to be highly significant ().
Article identification	BYISSUE	 Table 2: Experiment 1: Article identification with gold  standard segments, BYISSUE setting
Article identification	BYISSUE	 Table 5: Article identification on pages filtered by OCR  quality (Exp. 1, BYISSUE, B-Cubed F1, 14 clusters)
segmentation	accuracy	We show that contextualization yields significant improvements in segmentation accuracy.
segmentation	accuracy	To measure segmentation accuracy, two of the authors manually annotated a randomly-selected subset of 200 terms that occur in at least 5 contexts in the corpus.
segmentation	recall	 Table 1: Maximum segmentation recall at various false positive rates.
Scientific Information Extraction (ScienceIE))	TASK	A recent challenge on Scientific Information Extraction (ScienceIE)) provided a dataset consisting of 500 scientific paragraphs with keyphrase annotations for three categories: TASK, PROCESS, MA-TERIAL across three scientific domains, Computer Science, Material Science, and Physics.
Scientific Information Extraction (ScienceIE))	MA-TERIAL	A recent challenge on Scientific Information Extraction (ScienceIE)) provided a dataset consisting of 500 scientific paragraphs with keyphrase annotations for three categories: TASK, PROCESS, MA-TERIAL across three scientific domains, Computer Science, Material Science, and Physics.
extraction	Parscit	For extraction of citation context, we used Parscit ().
RST segmentation	F1	RST segmentation is often regarded as a solved problem because automated segmenters achieve high performance (F1=94.3) on a task with high inter-annotator agreement (kappa=0.92) ( ).
Rhetorical Structure Theory	QUOTE	In the classical formulation of Rhetorical Structure Theory, considered, but decided against, QUOTE as one of the baselined relations.
sentence- splitter	BERT-E	 Table 5: Specific results on English test data at the doc- ument level. 'Rand.-50d' and 'GloVe-50d' correspond  to the baseline model, taking a whole document as in- put. BERT models are still pipelined to a sentence- splitter, but ELMo-based models take the whole docu- ment as input. BERT-E uses English embeddings and  BERT-M uses multilingual embeddings.
sentence- splitter	BERT-M	 Table 5: Specific results on English test data at the doc- ument level. 'Rand.-50d' and 'GloVe-50d' correspond  to the baseline model, taking a whole document as in- put. BERT models are still pipelined to a sentence- splitter, but ELMo-based models take the whole docu- ment as input. BERT-E uses English embeddings and  BERT-M uses multilingual embeddings.
Discourse parsing	F1  scores	 Table 3: Discourse parsing performance in terms of F1  scores (%) on three levels of Span, Nuclearity, and Re- lation. Human agreements are also listed for compari- son. Within each cell, two micro F1 scores according to  the gold standards from each of two human annotators  are both reported.
Discourse parsing	Re- lation	 Table 3: Discourse parsing performance in terms of F1  scores (%) on three levels of Span, Nuclearity, and Re- lation. Human agreements are also listed for compari- son. Within each cell, two micro F1 scores according to  the gold standards from each of two human annotators  are both reported.
Detection of named mentions	accuracy	Detection of named mentions can be done with high accuracy by named entity recognition systems () and the matching of names can also be done accurately via string matching (.
coreference resolution	BLANC	For coreference resolution, we use the CoNLL-2012 metrics () including BLANC.
predicting the reference sentence occurring in a corpus	accuracy	For the task of predicting the reference sentence occurring in a corpus (amidst meaning-equivalent grammatical variants) using a machine learning model, surprisal estimates from an artificial version of the language (i.e., Hindi without any case markers) result in lower prediction accuracy compared to natural Hindi.
dependency parser surprisal	accuracy	Moreover, dependency parser surprisal has much lower classification accuracy compared to trigram surprisal and has a very negligible impact on performance on top of trigram surprisal.
Pairwise classification	accuracy	 Table 5: Pairwise classification and ranking accuracy (***
automated speech recognition (ASR)	recall quality	The study further examined how well these measures can be incorporated into a full analysis pipeline starting from data collection on a mobile platform outside of the traditional laboratory (thus in the real-world, perhaps noisy, environment), to automated speech recognition (ASR), and then to the conversion of the language to predictions of recall quality.
classification	accuracy	The SLM offers good classification accuracy, but it does not provide any human interpretable reason for its classification decisions.
AMR task	Recall	 Table 4: Comparison of previous work on the AMR task. R, P and F are Recall, Precision and F-Score.
AMR task	Precision	 Table 4: Comparison of previous work on the AMR task. R, P and F are Recall, Precision and F-Score.
AMR task	F-Score	 Table 4: Comparison of previous work on the AMR task. R, P and F are Recall, Precision and F-Score.
SMT	aver- age	 Table 8:  Phrase-based SMT results  (English→German) on WMT test sets (aver- age of newstest201{4,5}), and IWSLT test sets  (average of tst201{3,4,5}), and average BLEU  gain from adding synthetic data for both PBSMT  and NMT.
SMT	BLEU	 Table 8:  Phrase-based SMT results  (English→German) on WMT test sets (aver- age of newstest201{4,5}), and IWSLT test sets  (average of tst201{3,4,5}), and average BLEU  gain from adding synthetic data for both PBSMT  and NMT.
MWE identification	accuracy	Experiments on two different data sets show that the approach significantly improves MWE identification accuracy (and sometimes syntactic accuracy) compared to existing joint approaches.
MWE identification	accuracy	Experiments on two different data sets show that the approach significantly improves MWE identification accuracy (and sometimes syntactic accuracy) compared to existing joint approaches.
MWE identification	F-score	MWE identification is evaluated with the F-score of the MWE segmentation, namely MWE for all MWEs and FMWE for fixed MWEs only.
MWE identification	accuracy	When comparing joint systems with pipeline ones, we can see that preidentifying fixed MWEs seems to help MWE identification whereas syntactic parsing accuracy tends to be slightly lower.
metaphor identification task	F-score	We then use these representations in a metaphor identification task, achieving a high performance of 0.82 in terms of F-score.
Schema Coverage Comparison	ERE	 Table 5: Schema Coverage Comparison on ACE and ERE.
predicting the next verb with its nominal arguments	BLEU	gives results on the task of predicting the next verb with its nominal arguments; that is, whereas gave results on a text analog to the Narrative Cloze evaluation (BLEU),   results on the verb-with-arguments prediction version.
Classification	accuracy	Classification accuracy was selected as the evaluation metric.
answer triggering	Precision	Similar to, we also evaluate answer triggering using Precision, Recall, and F1 score as metrics.
answer triggering	Recall	Similar to, we also evaluate answer triggering using Precision, Recall, and F1 score as metrics.
answer triggering	F1 score	Similar to, we also evaluate answer triggering using Precision, Recall, and F1 score as metrics.
coreference resolution	recall	By comparing the evaluation scores, we determine which system performs best, which model suits coreference resolution better, and which feature set is useful for improving the recall or precision of a coreference resolver.
coreference resolution	precision	By comparing the evaluation scores, we determine which system performs best, which model suits coreference resolution better, and which feature set is useful for improving the recall or precision of a coreference resolver.
POS tagging	accuracy	Finally, we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy.
parsing	accuracy	Finally, we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy.
tagging	accuracy	Our model leads to an error reduction of more than 4% in tagging accuracy despite very little training data, which to the best of our knowledge is the first positive result on weakly supervised part-of-speech induction from fMRI data in the literature.
Tagging	accuracy	 Table 1: Tagging accuracy on test data for the dif- ferent models. The fMRI model is significantly  better than the baseline (p = 0.014, Bootstrap).
Tagging	Bootstrap	 Table 1: Tagging accuracy on test data for the dif- ferent models. The fMRI model is significantly  better than the baseline (p = 0.014, Bootstrap).
SemEval-2010 relation classification task	F 1 -score	We evaluate our method on the SemEval-2010 relation classification task, and achieve a state-ofthe-art F 1 -score of 86.3%.
Sentence rewriting	F1	2. Sentence rewriting is a promising technique for semantic parsing: By employing sentence rewriting, our system gains a 3.4% F1 improvement over the base system we used (Berant and Liang, 2015).
semantic parsing	F1	2. Sentence rewriting is a promising technique for semantic parsing: By employing sentence rewriting, our system gains a 3.4% F1 improvement over the base system we used (Berant and Liang, 2015).
MTL	TAP	In general, we see that the MTL versions nearly always outperform the baseline TAP when using the same training data.
SMT	accuracy	In SMT, it is known that incorporating syntactic constituents of the source language into the models improves word alignment) and translation accuracy (;).
WAT'15 English-toJapanese translation task	accuracy	Our experimental results on the WAT'15 English-toJapanese translation task show that our proposed model achieves state-of-the-art translation accuracy.
parsing	accuracy	Here, we see a small drop in parsing accuracy when using the new annotation.
MT	BLEU	As shown in, no significant difference in MT quality as measured by BLEU was observed; the best BLEU score came from separate features at 18.40 while log-linear scored 18.15.
MT	BLEU	As shown in, no significant difference in MT quality as measured by BLEU was observed; the best BLEU score came from separate features at 18.40 while log-linear scored 18.15.
response generation task	BLEU	BLEU has been shown to correlate well with human judgment on the response generation task, as demonstrated in ( . Besides BLEU scores, we also report perplexity as an indicator of model capability.
WMT'15 English to Czech translation task	BLEU	On the WMT'15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1−11.4 BLEU points over models that already handle unknown words.
WMT'15 English to Czech translation task	BLEU	We demonstrate at scale that on the WMT'15 English to Czech translation task, such a hybrid approach provides an additional boost of +2.1−11.4 BLEU points over models that already handle unknown words.
POS tagging	F1	We obtain state-of-the-art performance on both datasets-97.55% accuracy for POS tagging and 91.21% F1 for NER.
POS tagging	F1	Our end-to-end model outperforms previous stateof-the-art systems, obtaining 97.55% accuracy for POS tagging and 91.21% F1 for NER.
POS  tagging	F1	 Table 6: Results with different choices of word  embeddings on the two tasks (accuracy for POS  tagging and F1 for NER).
POS tagging	F1	 Table 7: Results with and without dropout on two  tasks (accuracy for POS tagging and F1 for NER).
MT evaluation	BLEU	Since German compounds are relatively rare, their impact on the standard MT evaluation metrics (e.g., BLEU) is minimal, as we show with an oracle experiment, and we find that our synthetic phrase approach obtains only modest improvements in overall translation quality.
sarcasm detection	F-score	The augmented cognitive features improve sarcasm detection by 3.7% (in terms of F-score), over the performance of the best reported system.
classification	accuracy	Our experiments show significant improvement in classification accuracy over the state of the art, by performing such augmentation.
nominal relation classification	F1-score	On nominal relation classification, our model compares favorably to the state-of-the-art CNNbased model in F1-score.
stem extraction	accuracy	In this section, we describe three experiments for evaluating stem extraction, intrinsic accuracy, and consistency.
stem extraction	consistency	In this section, we describe three experiments for evaluating stem extraction, intrinsic accuracy, and consistency.
SRL task	precision	Our hypotheses are that (1) modeling dependency paths as sequences will lead to better representations for the SRL task, thus increasing labeling precision overall, and that (2) embeddings will address the problem of data sparsity, leading to higher recall.
SRL task	recall	Our hypotheses are that (1) modeling dependency paths as sequences will lead to better representations for the SRL task, thus increasing labeling precision overall, and that (2) embeddings will address the problem of data sparsity, leading to higher recall.
prediction	accuracy	We also discuss the trade-off between prediction accuracy and instancy.
pattern balancing	precision	(1) Can pattern balancing lead to a higher performance in trigger classification, argument identification, and classification while retaining the precision value?
trigger classification	precision	(1) Can pattern balancing lead to a higher performance in trigger classification, argument identification, and classification while retaining the precision value?
translation extraction	accuracy	Experiments using an English-Japanese and English-French Wikipedia corpus show that the proposed models (BiSTM and BiSTM+TS) significantly outperform the standard bilingual topic model (BiLDA) in terms of perplexity, and that they improve performance in translation extraction (up to +0.083 top 1 accuracy).
translation extraction	accuracy (ACC N )	We measured the performance of translation extraction with top N accuracy (ACC N ), the number of test words whose top N translation candidates contain a correct translation over the total number of test words.
relation classification	F1 score	3. We obtain the new state-of-the-art results for relation classification with an F1 score of 88.0% on the SemEval 2010 Task 8 dataset, outperforming methods relying on significantly richer prior knowledge.
KB completion	stopping criterion δ	 Table 3: KB completion results on the 96 relations that have been grouped into clusters with size larger  than one (with the stopping criterion δ = 0.5), and hence involved in multi-tasking learning.
analogical reasoning	accuracy	We train NNs by back-propagation with AdaGrad () and mini-batches.: Results on five word similarity tasks (Spearman correlation metric) and analogical reasoning (accuracy).
analogical reasoning	accuracy	 Table 3: Results on five word similarity tasks (Spearman correlation metric) and analogical reasoning  (accuracy). The number of OOVs is given in parentheses for each result. "ind-full/ind-overlap": indi- vidual embedding sets with respective full/overlapping vocabulary; "ensemble": ensemble results using  all five embedding sets; "discard": one of the five embedding sets is removed. If a result is better  than all methods in "ind-overlap", then it is bolded. Significant improvement over the best baseline  in "ind-overlap" is underlined (online toolkit from http://vassarstats.net/index.html for  Spearman correlation metric, test of equal proportions for accuracy, p < .05).
analogical reasoning	OOVs	 Table 3: Results on five word similarity tasks (Spearman correlation metric) and analogical reasoning  (accuracy). The number of OOVs is given in parentheses for each result. "ind-full/ind-overlap": indi- vidual embedding sets with respective full/overlapping vocabulary; "ensemble": ensemble results using  all five embedding sets; "discard": one of the five embedding sets is removed. If a result is better  than all methods in "ind-overlap", then it is bolded. Significant improvement over the best baseline  in "ind-overlap" is underlined (online toolkit from http://vassarstats.net/index.html for  Spearman correlation metric, test of equal proportions for accuracy, p < .05).
analogical reasoning	accuracy	 Table 3: Results on five word similarity tasks (Spearman correlation metric) and analogical reasoning  (accuracy). The number of OOVs is given in parentheses for each result. "ind-full/ind-overlap": indi- vidual embedding sets with respective full/overlapping vocabulary; "ensemble": ensemble results using  all five embedding sets; "discard": one of the five embedding sets is removed. If a result is better  than all methods in "ind-overlap", then it is bolded. Significant improvement over the best baseline  in "ind-overlap" is underlined (online toolkit from http://vassarstats.net/index.html for  Spearman correlation metric, test of equal proportions for accuracy, p < .05).
POS tagging	FLORS	 Table 5: POS tagging results on six target domains. "baselines" lists representative systems for this task,  including FLORS. "+indiv / +meta": FLORS with individual embedding set / metaembeddings. Bold  means higher than "baselines" and "+indiv".
summarization tasks	ROUGE	Similar to the evaluation for traditional summarization tasks, we use the ROUGE metrics ( to automatically evaluate the quality of produced summaries given the goldstandard reference news.
translation tasks	BLEU score	We tested our model on different translation tasks and the CSRS model outperformed a base-line without rule selection and the previous MERS model by up to 2.2 and 1.1 points of BLEU score respectively.
SMT	MERS	2 Tree-to-String SMT and MERS 2.1 Tree-to-String SMT In tree-to-string SMT (), a parse tree for the source sentence F is transformed into a target sentence E using translation rules R.
Cross-lingual sentiment classification	accuracy	 Table 2: Cross-lingual sentiment classification accuracy for the nine tasks. For all the methods, we get  ten different runs of the algorithm and calculate the mean accuracy.
causal inference	F-measure	We show that these features improve causal inference by an 11.05 point increase in F-measure over a naive baseline in 6.
SNLI 3-way inference classification	Params.	 Table 3: Results on SNLI 3-way inference classification. Params. is the approximate number of trained  parameters (excluding word embeddings for all models). Trans. acc. is the model's accuracy in predicting  parsing transitions at test time. Train and test are SNLI classification accuracy.
SNLI 3-way inference classification	accuracy	 Table 3: Results on SNLI 3-way inference classification. Params. is the approximate number of trained  parameters (excluding word embeddings for all models). Trans. acc. is the model's accuracy in predicting  parsing transitions at test time. Train and test are SNLI classification accuracy.
LAMBADA	accuracy	Results Results of models and baselines are reported in for LAMBADA is the average success of a model at predicting the target word, i.e., accuracy (unlike in standard language modeling, we know that the missing LAMBADA words can be precisely predicted by humans, so good models should be able to accomplish the same feat, rather than just assigning a high probability to them).
parsing	accuracy	To measure parsing accuracy, we report unlabeled attachment score (UAS) and labeled attachment score (LAS) computed on all tokens (including punctuation), as is standard for non-English datasets.
parsing	labeled attachment score (LAS) computed	To measure parsing accuracy, we report unlabeled attachment score (UAS) and labeled attachment score (LAS) computed on all tokens (including punctuation), as is standard for non-English datasets.
Emotion classification	ALL	 Table 4: Emotion classification results (one vs. all for each emotion and 6 way for ALL) using our models compared to others.
early stopping	BLEU	We follow the standard practice of early stopping by measuring performance on a development set, and present results of an extensive evaluation on several tasks with different loss functions, including BLEU for SMT, Hamming loss for optical character recognition, and F1 score for chunking.
early stopping	F1 score	We follow the standard practice of early stopping by measuring performance on a development set, and present results of an extensive evaluation on several tasks with different loss functions, including BLEU for SMT, Hamming loss for optical character recognition, and F1 score for chunking.
SMT	F1 score	We follow the standard practice of early stopping by measuring performance on a development set, and present results of an extensive evaluation on several tasks with different loss functions, including BLEU for SMT, Hamming loss for optical character recognition, and F1 score for chunking.
chunking task	F1-score	For the chunking task, the F1-score results obtained for bandit learning are close to the full-information baseline.
SMT	MERT	For all SMT experiments we tokenized, lowercased and aligned words using cdec tools, trained 4-gram in-domain and out-of-domain language models (on the English sides of Europarl and in-domain NewsCommentary) For dense feature models, the out-of-domain baseline SMT model was trained on 1.6M parallel Europarl data and tuned with cdec's lattice MERT on out-of-domain Europarl dev2006 dev set (2,000 sent.).
SMT	MERT	For all SMT experiments we tokenized, lowercased and aligned words using cdec tools, trained 4-gram in-domain and out-of-domain language models (on the English sides of Europarl and in-domain NewsCommentary) For dense feature models, the out-of-domain baseline SMT model was trained on 1.6M parallel Europarl data and tuned with cdec's lattice MERT on out-of-domain Europarl dev2006 dev set (2,000 sent.).
SMT	BLEU	The full-information in-domain SMT model tuned by MERT on news in-domain sets (nc-dev2007, 1,057 sent.) gives the range of possible improvements by the difference of its BLEU score to the one of the out-of-domain model (2.5 BLEU points).
SMT	BLEU	The full-information in-domain SMT model tuned by MERT on news in-domain sets (nc-dev2007, 1,057 sent.) gives the range of possible improvements by the difference of its BLEU score to the one of the out-of-domain model (2.5 BLEU points).
classification	accuracy	The performance is reported in terms of classification accuracy on the target domain.
analogy completion	DIFFVEC	The success of the simple offset method on analogy completion suggests that the difference vectors ("DIFFVEC" hereafter) must themselves be meaningful: their direction and/or magnitude encodes a lexical relation.
MLE translations	MRE	The two human evaluators made close judgements: around 54% of MLE translations are worse than MRE, 23% are equal, and 23% are better.
WMT 15 translation tasks English→German	BLEU	We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU, respectively.
translation	accuracy	In addition to making the translation process simpler, we also find that the subword models achieve better accuracy for the translation of rare words than large-vocabulary models and back-off dictionaries, and are able to productively generate new words that were not seen at training time.
Temporal ordering pairwise photo	accuracy	 Table 3: Temporal ordering pairwise photo results. The metric reported is accuracy, the percentage of time the correct photo
object recognition labels	accuracy	In the setting of no object recognition labels, VC-Knowledge and VC-Learning also generate significantly better grounding accuracy than the two baselines.
Extractive MDS	ROUGE	Extractive MDS can leverage the ROUGE scores of individual sentences in various ways, in particular, as part of an optimization step.
MDS task	ROUGE	Most importantly, the resulting framework reduces the MDS task to the problem of scoring individual sentences with their ROUGE scores.
summarization task	ROUGE score	The overall summarization task is converted to two sequential tasks: (i) scoring single sentences, and (ii) selecting summary sentences by solving an optimization problem where the ROUGE score of the selected sentences is maximized.
phrase structure parsing	F-measure	In addition, we explore methods to improve phrase structure parsing for learner English (achieving an F-measure of 0.878).
sentence boundary detection	accuracy	When working with carefully copy-edited text documents, sentence boundary detection can be viewed as a minor preprocessing task in Natural Language Processing, solvable with very high accuracy.
Sentence boundary detection	F 1 )	 Table 3: Sentence boundary detection results (F 1 )  on test sets.
parsing	accuracy	The parsing performance is insignificantly different from using English as source, despite the accuracy of the tags projected   from Danish being higher.
dependency parsing	accuracy	show that features from induced morpho-syntactic lexicons can also improve dependency parsing accuracy.
STACC	similarity coefficient	The method we describe, termed STACC, is based on expanded lexical sets and the Jaccard similarity coefficient, which is computed as the ratio of set intersection over union.
summarization	METEOR	For summarization, we evaluate using automatic metrics such as METEOR and BLEU-4, together with a human study for naturalness and informativeness of the output.
summarization	BLEU-4	For summarization, we evaluate using automatic metrics such as METEOR and BLEU-4, together with a human study for naturalness and informativeness of the output.
sentiment analyzer	F-measures	It is not at all uncommon within this community to evaluate a sentiment analyzer with a variety of classification accuracy or hypothesis testing scores such as F-measures, SARs, kappas or Krippendorf alphas derived from human-subject annotationseven when more extensional measures are available, such as actual market returns from historical data in the case of securities trading.
sentiment analyzer	SARs	It is not at all uncommon within this community to evaluate a sentiment analyzer with a variety of classification accuracy or hypothesis testing scores such as F-measures, SARs, kappas or Krippendorf alphas derived from human-subject annotationseven when more extensional measures are available, such as actual market returns from historical data in the case of securities trading.
Sentiment classification	accuracy	 Table 3: Sentiment classification accuracy (aver- age 10-fold cross-validation) and trade returns of  different feature sets and term frequency weight- ing schemes in Exp. 3. The same folds were  used for the different representations. The non- annualized returns are presented in columns 3-6.
MST	O	This allows us to utilize state-of-the-art undirected MST algorithms whose run time is O(m) at expectation and with a very high probability.
MST problem	O	Particularly, the undirected MST problem ( § 2) has a randomized algorithm which is O(m) at expectation and with a very high probability), as well as an O(m · α(m, n)) worst-case deterministic algorithm, where α(m, n) is a certain natural inverse of Ackermann's function.
MST problem	O	Particularly, the undirected MST problem ( § 2) has a randomized algorithm which is O(m) at expectation and with a very high probability), as well as an O(m · α(m, n)) worst-case deterministic algorithm, where α(m, n) is a certain natural inverse of Ackermann's function.
dependency parsing inference problem	O	First, it encodes the first-order dependency parsing inference problem as an undirected MST problem, in up to O(m) time.
leader detection	F1-score	The leader detection model was implemented by using CRF++ , which was trained on the public dataset composed of 1,300 conversation paths and achieved state-of-the-art 73.7% F1-score of classification accuracy ().
stance classification in student essays	F-score	In an evaluation on 826 essays, our approach significantly outperforms four baselines, one of which relies on features previously developed specifically for stance classification in student essays, yielding relative error reductions of at least 11.3% and 5.3%, in micro and macro F-score, respectively.
stance classification	precision	When training our approach, we perform exhaustive feature selection to determine which sub- Since stance classification is a multiclass, single-label task, micro F-score, precision, recall, and accuracy are all equivalent.
stance classification	recall	When training our approach, we perform exhaustive feature selection to determine which sub- Since stance classification is a multiclass, single-label task, micro F-score, precision, recall, and accuracy are all equivalent.
stance classification	accuracy	When training our approach, we perform exhaustive feature selection to determine which sub- Since stance classification is a multiclass, single-label task, micro F-score, precision, recall, and accuracy are all equivalent.
grammatical error correction (GEC)	HOO	Recently, there has been a spike in research on grammatical error correction (GEC), correcting writing mistakes made by learners of English as a Second Language, including four shared tasks: HOO (: (Lack of) progress in GEC over the last few years.
MT	Output	 Table 5: Complex and interacting mistakes that MT successfully addresses. Output of the MT-based  AMU system.
SCL	accuracy	In contrast, our proposed improvements to SCL reach an accuracy boost of more than 15% over the no domain adaptation model and of 14% over the standard SCL formulation.
SMT	BLEU	From, we can find that our proposed method can achieve much higher BLEU than SMT system, and we can also achieve 1.9 and 3.6 BLEU points improvement compared with the raw encoder-decoder system on both eletric business and movies data.
EL	accuracy	To evaluate EL accuracy on ACE and MSNBC, we report on a Bag-of-Titles (BOT) F1 evaluation as introduced by).
summarization	MEAD	In particular, we compare our proposed summarization method (denoted as Our Method) with the following typical summarization methods and all of them extract summaries from the same candidate sentence set for each topic: MEAD: It uses a heuristic way to obtain each sentence's score by summing the scores based on different features ( ): centroidbased weight, position and similarity with first sentence.
dependency parsing	attachment score	In particular for dependency parsing on the Wall Street Journal we achieve the best-ever published unlabeled attachment score of 94.61%.
SVM	accuracy	For the baseline, SVM without embedding (w/o Embed) achieved 91.99% accuracy, which is already very competitive.
classification	accuracy	As we can see, in all cases the target regularization (MDA+TR) helps improve the classification accuracy.
constituency parsing	ADADELTA ρ  0.99  0.99  ADADELTA  1	 Table 5: Test F-scores for constituency parsing on  Penn Treebank and CTB-5. Dependency Constituency  Embeddings  Word (dims)  50  100  Tags (dims)  20  100  Nonterminals (dims)  - 100  Pretrained  No  No  Network details  LSTM units (each direction)  200  200  ReLU hidden units  200 / decision  1000  Training  Training epochs  10  10  Minibatch size (sentences)  10  10  Dropout (LSTM output only)  0.5  0.5  L2 penalty (all weights)  none  1 × 10 −8  ADADELTA ρ  0.99  0.99  ADADELTA  1 × 10 −7  1 × 10 −7
Result Analysis	M	 Table 2: Result Analysis: M denotes the number  of matches of system outputs (O) with the gold.
labeling of semantic parses	accuracy	We employ a staged labeling paradigm that enables efficient labeling of semantic parses and improves the accuracy, consistency and efficiency of ob-1 Available at http://aka.ms/WebQSP.
labeling of semantic parses	consistency	We employ a staged labeling paradigm that enables efficient labeling of semantic parses and improves the accuracy, consistency and efficiency of ob-1 Available at http://aka.ms/WebQSP.
NE recognizer	accuracy	As we show in later section of this paper, we end up with an NE recognizer that refers to real world information in addition to text information, which increases its accuracy.
Semantic Classification of German Preposition Types	Approaches	Automatic Semantic Classification of German Preposition Types: Comparing Hard and Soft Clustering Approaches across Features
APE translations	BLEU	APE translations produced by NNAPE show statistically significant improvements of 3.96, 2.68 and 1.35 BLEU points absolute over the original MT, phrase-based APE and hierarchical APE outputs, respectively.
translation	BLEU	As a consequence, we obtain a significant improvement in terms of translation quality (up to 3 BLEU points).
word analogy tasks	accuracy	 Table 2: Results on word analogy tasks, given as percent accuracy.
MSR Sentence Completion Challenge	accuracy	 Table 1: Best performance of various models on  the MSR Sentence Completion Challenge. Values  reflect overall accuracy (%).
POS tagging	F	 Table 3: Results for POS tagging. LSJU = Stanford. SVM = SVMTool. F=FLORS. We show three state- of-the-art taggers (lines 1-3), FLORS extended with 300-dimensional embeddings (4) and extended with  UD embeddings (5).  †: significantly better than the best result in the same column (α = .05, one-tailed  Z-test).
POS tagging	FLORS	 Table 3: Results for POS tagging. LSJU = Stanford. SVM = SVMTool. F=FLORS. We show three state- of-the-art taggers (lines 1-3), FLORS extended with 300-dimensional embeddings (4) and extended with  UD embeddings (5).  †: significantly better than the best result in the same column (α = .05, one-tailed  Z-test).
POS tagging	FLORS	 Table 3: Results for POS tagging. LSJU = Stanford. SVM = SVMTool. F=FLORS. We show three state- of-the-art taggers (lines 1-3), FLORS extended with 300-dimensional embeddings (4) and extended with  UD embeddings (5).  †: significantly better than the best result in the same column (α = .05, one-tailed  Z-test).
claim detection	mean	We note that this precision is significantly higher than reported for claim detection (, where, for example, mean precision at 5 is 0.28 (in our case it is 0.7 − 0.8).
MED	CELEX	We evaluate MED on two MRI tasks: CELEX and SIGMORPHON16.
Tagging and parsing (UAS)	accuracy	 Table 2: Tagging and parsing (UAS) accuracy.  Scores are macro-averaged, and all parsers use  predicted POS from respective EBC or WTC tag- gers. *: True target languages, not used as sources.
PoS tagging	accuracy	We find that eyetracking features lead to a significant increase in PoS tagging accuracy, and that type-level aggregates work better than token-level features.
substitute extraction task	agreement	During the substitute extraction task, agreement among the annotators was 0.664, whereas during the simplification ranking task, Spearman's rank correlation coefficient score was 0.332.
substitute extraction task	rank correlation coefficient score	During the substitute extraction task, agreement among the annotators was 0.664, whereas during the simplification ranking task, Spearman's rank correlation coefficient score was 0.332.
singleton detection	accuracy	Here, a singleton detection system based on word embed-dings and neural networks is presented, which achieves state-of-the-art performance (79.6% accuracy) on the CoNLL-2012 shared task development set.
Singleton detection	BL	 Table 1: Singleton detection performance using  the best-scoring model. The CoNLL-2012 train- ing set was used as training data. 'dM15' marks  the results by de Marneffe et al. (2015) 'BL' marks  the baseline performance.
Disambiguation of Entities	YAGO	Although, AIDA datasets are widely used for Disambiguation of Entities, AIDA uses YAGO, an unique Knowledge Base derived from Wikipedia, GeoNames and Wordnet, which makes it difficult to compare.
classification	accuracy	This personalization which is essentially training DBN for each cluster improves the classification accuracy and features extraction.
text annotation operations	accuracy	The Stanford parser is responsible for text annotation operations, and its shift-reduce constituent parser offers best-in-class performance and accuracy.
event extraction	accuracy	To evaluate event extraction accuracy, we manually judge a random sample of 150 documents (50 per source) for each event.
automatic removing "bad" TUs	accuracy	Results indicate its effectiveness in automatic removing "bad" TUs, with comparable performance to a state-of-the-art supervised method (76.3 vs. 77.7 balanced accuracy).
Coreference Resolution	Precision	 Table 3: Performance with type inference and  Coreference Resolution using Named Entities  and Nouns as entity markers, comparing to Ak- bik (2014), reporting Precision, Recall and F- measures.
Coreference Resolution	Recall	 Table 3: Performance with type inference and  Coreference Resolution using Named Entities  and Nouns as entity markers, comparing to Ak- bik (2014), reporting Precision, Recall and F- measures.
Coreference Resolution	F- measures	 Table 3: Performance with type inference and  Coreference Resolution using Named Entities  and Nouns as entity markers, comparing to Ak- bik (2014), reporting Precision, Recall and F- measures.
summarization)	POLY	Practical running times for MUSE (summarization) and POLY are tens of milliseconds per a text document of a few thousand words.
Sequence Tagging	BLCC	(b) Sequence Tagging For these experiments, we use the BLCC tagger from and refer to the resulting system as STag BLCC . Again, we observe that paragraph level is considerably easier than essay level; e.g., for relations, there is ∼5% points increase from essay to paragraph level.
AMR	PARSEVAL	Measures include the SMATCH measure for AMR, and the PARSEVAL F-score ( adapted for DAGs for UCCA.
TAC KBP 2016 Event Nugget Detection	F-score	In fact, in the recent TAC KBP 2016 Event Nugget Detection and Coreference task, trigger detection (a.k.a. event nugget detection in KBP) is deliberately made more challenging by focusing only on detecting the 18 subtypes of triggers on which the KBP 2015 participating systems' performances were the poorest ( . The best-performing KBP 2016 system on English trigger detection achieved only an F-score of 47 ( . Given the difficulty of trigger detection, it is conceivable that many errors will propagate from the trigger detection component to the event coreference component in any pipeline architecture where trigger detection precedes event coreference resolution.
trigger detection	F-score	In fact, in the recent TAC KBP 2016 Event Nugget Detection and Coreference task, trigger detection (a.k.a. event nugget detection in KBP) is deliberately made more challenging by focusing only on detecting the 18 subtypes of triggers on which the KBP 2015 participating systems' performances were the poorest ( . The best-performing KBP 2016 system on English trigger detection achieved only an F-score of 47 ( . Given the difficulty of trigger detection, it is conceivable that many errors will propagate from the trigger detection component to the event coreference component in any pipeline architecture where trigger detection precedes event coreference resolution.
AMR parsing	SMATCH	For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources.
AMR generation	BLEU	For AMR generation, our model establishes anew state-of-the-art performance of BLEU 33.8.
AMR generation	BLEU	For the paired training procedure of Algorithm 1, we use Gigaword as our external corpus and sample sentences that only contain words from the AMR corpus vocabulary W . We subsampled the original sentence to ensure there is no overlap with the AMR training or test sets., and AMR generation using BLEU) . We validated word embedding sizes and RNN hidden representation sizes by maximizing AMR development set performance (Algorithm 1 -line 1).
sentence selection	accuracy	For sentence selection, since we do not know which sentence contains the answer, we report approximate accuracy by matching sentences that contain the answer string (y * ).
Answer prediction	accuracy	 Table 3: Answer prediction accuracy on the test set. K is the  number of sentences in the document summary.
Answer prediction	K	 Table 3: Answer prediction accuracy on the test set. K is the  number of sentences in the document summary.
Morphology Reinflection	ULD	 Table 1: Results for Task 3 of SIGMORPHON 2016 on Morphology Reinflection.  † represents the best single supervised  model score,  ‡ represents the best model including semi-supervised models, and bold represents the best score overall. #LD  and #ULD are the number of supervised data and unlabeled words respectively.
sentiment analysis	Abbreviations (P,R,F)→ Precision	 Table 1: Results for different traditional feature based systems and CNN model variants for the task of  sentiment analysis. Abbreviations (P,R,F)→ Precision, Recall, F-score. SVM→Support Vector Machine
sarcasm detection	F-score	 Table 2: Results for different traditional feature based systems and CNN model variants for the task of  sarcasm detection on dataset 1. Abbreviations (P,R,F)→ Precision, Recall, F-score
stance detection	accuracy	observed that existing methods on stance detection fail on "ordinary" users because such methods primarily obtain training and test data from politically vocal users (e.g., politicians); for example, they found that a stance detector trained on a dataset with politicians achieved 91% accuracy on other politicians but only achieved 54% accuracy on "ordinary" users.
stance detection	accuracy	observed that existing methods on stance detection fail on "ordinary" users because such methods primarily obtain training and test data from politically vocal users (e.g., politicians); for example, they found that a stance detector trained on a dataset with politicians achieved 91% accuracy on other politicians but only achieved 54% accuracy on "ordinary" users.
SRL	F1	 Table 3: Predicate detection performance and end-to-end SRL results using predicted predicates. ∆ F1  shows the absolute performance drop compared to our best ensemble model with gold predicates.
ASR hypotheses	recall	L2 normalization One would expect that access to ASR hypotheses should improve the recall scores, but the performance gap is not enormous.
image search	Recall	 Table 1: Results for image search and annotation  on the Places audio caption data (214k training  pairs, 1k testing pairs). Recall is shown for the  top 1, 5, and 10 hits. The model we use in this  paper is compared against the meanpool variant of  the model architecture presented in Harwath et al.  (2016). For both training and testing, the captions  were truncated/zero-padded to 10 seconds.
relation detection	accuracy	Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks .
regularization	dropout rate	•  For regularization purpose, we adopted l 2 -regularization to 0.0001 and dropout rate of 0.1 ().
Image retrieval	recall	 Table 1: Image retrieval performance on Flickr8K.  R@N stands for recall at N; ˜  r stands for median  rank of the correct image.
Image retrieval	recall	 Table 2:  Image retrieval performance on  MS COCO. R@N stands for recall at N; ˜  r stands  for median rank of the correct image.
translation	accuracy	Experimental results show that our model significantly improves translation accuracy over the conventional NMT and SMT baseline systems.
Rumor detection	FR: False Rumor	 Table 2: Rumor detection results (NR: Non- Rumor; FR: False Rumor; TR: True Rumor; UR:  Unverified Rumor)
POS tagging	accuracy	 Table 4: POS tagging accuracy using word-based  and char-based encoder/decoder representations.
POS tagging	accuracy	 Table 6: Impact of changing the target language  on POS tagging accuracy. Self = German/Czech  in rows 1/2 respectively.
classification	accuracy	 Table 5: Cross-dataset comparison in terms of classification  accuracy.
Sentiment analysis	accuracy	 Table 5: Sentiment analysis accuracy for binary  predictions of signed clustering algorithm (SC)  versus other models. SC(W2V) are the signed  clusters using word2vec word representations.
Link prediction	Mean Rank	 Table 2: Link prediction results on two datasets. Higher Hits@10 or lower Mean Rank indicates better  performance. Following Nguyen et al. (2016b) and Shen et al. (2016), we divide the models into two  groups. The first group contains intrinsic models without using extra information. The second group  make use of additional information. Results in the brackets are another set of results STransE reported.
SVM classifiers	F1	Results on AMI discussions show that SVM classifiers trained with our features significantly outperform the state-ofthe-art results) (F1: 63.1 vs. 50.5) and non-trivial baselines.
parsing	accuracy	Unsurprisingly, lower parsing performance implies lower classification accuracy.
Pyramid scoring	PEAK	Recently, developed a freely available off-the-shelf system for automatic Pyramid scoring called PEAK, which uses open Information Extraction (open IE) propositions as SCUs and relies on proposition comparison.
error  detection	ED prec)	 Table 2: Label accuracies on 5,000 tokens of  WSJ text after N iterations, and precision for error  detection (ED prec).
Joint segmentation	F-measure	 Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
POS tagging	F-measure	 Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
POS Tagging	EAG	 Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
MRS	precision	Existing parsers for full MRS (as opposed to bilexical semantic graphs derived from, but simplifying MRS) are grammar-based, performing disambiguation with a maximum entropy model (; this approach has high precision but incomplete coverage.
AMR parsers	F1	Recently a number of AMR parsers have been developed), but corpora are still under active development and low inter-annotator agreement places on upper bound of 83% F1 on expected parser performance (.
AMR parsing	Smatch	On AMR parsing our model obtains 60.11% Smatch, an improvement of 8% over an existing neural AMR parser.
Classification	accuracy	 Table 4: Classification accuracy achieved on Cookie Theft dataset.
Classification	accuracy	 Table 6: Classification accuracy achieved on ABCD dataset.
Classification	accuracy	 Table 7: Classification accuracy achieved on Cin- derella dataset manually processed to revise non- grammatical sentences.
slot  tagging	F1-score	 Table 6: Intent classification accuracy (%) and slot  tagging F1-score (%) for the unsupervised domain  adaptation with two different adversarial classifi- cation losses -our claimed random domain pre- dictions (RAND) and adversarial loss (ADVR) of  Ganin et al. (2016) as explained in 3.2.3.
question  generation	difficulty	 Table 3: Human evaluation results for question  generation. Naturalness and difficulty are rated  on a 1-5 scale (5 for the best). Two-tailed t- test results are shown for our method compared to  H&S (statistical significance is indicated with  *  (p  < 0.005),  *  *  (p < 0.001)).
named entity recognition (NER)	accuracy	The state-of-the-art named entity recognition (NER) systems are supervised machine learning models that require large amounts of manually annotated data to achieve high accuracy.
Named entity recognition (NER)	EVENT	Named entity recognition (NER) is a fundamental information extraction task that automatically detects named entities in text and classifies them into pre-defined entity types such as PERSON, ORGANIZATION, GPE (GeoPolitical Entities), EVENT, LOCATION, TIME, DATE, etc.
Named entity recognition (NER)	TIME	Named entity recognition (NER) is a fundamental information extraction task that automatically detects named entities in text and classifies them into pre-defined entity types such as PERSON, ORGANIZATION, GPE (GeoPolitical Entities), EVENT, LOCATION, TIME, DATE, etc.
Named entity recognition (NER)	DATE	Named entity recognition (NER) is a fundamental information extraction task that automatically detects named entities in text and classifies them into pre-defined entity types such as PERSON, ORGANIZATION, GPE (GeoPolitical Entities), EVENT, LOCATION, TIME, DATE, etc.
information extraction task	EVENT	Named entity recognition (NER) is a fundamental information extraction task that automatically detects named entities in text and classifies them into pre-defined entity types such as PERSON, ORGANIZATION, GPE (GeoPolitical Entities), EVENT, LOCATION, TIME, DATE, etc.
information extraction task	TIME	Named entity recognition (NER) is a fundamental information extraction task that automatically detects named entities in text and classifies them into pre-defined entity types such as PERSON, ORGANIZATION, GPE (GeoPolitical Entities), EVENT, LOCATION, TIME, DATE, etc.
information extraction task	DATE	Named entity recognition (NER) is a fundamental information extraction task that automatically detects named entities in text and classifies them into pre-defined entity types such as PERSON, ORGANIZATION, GPE (GeoPolitical Entities), EVENT, LOCATION, TIME, DATE, etc.
representation projection	precision	For representation projection, neural network model NN2 (with a smoothing layer) is better than NN1, and NN2 tends to have a more balanced precision and recall.
representation projection	recall	For representation projection, neural network model NN2 (with a smoothing layer) is better than NN1, and NN2 tends to have a more balanced precision and recall.
NER	Precision	 Table 3: In-house NER data: Precision, Recall and  F 1 score on exact phrasal matches. The highest F 1  score among all the weakly supervised approaches  is shown in bold. Same for Tables 4 and 5.
NER	Recall	 Table 3: In-house NER data: Precision, Recall and  F 1 score on exact phrasal matches. The highest F 1  score among all the weakly supervised approaches  is shown in bold. Same for Tables 4 and 5.
NER	F 1 score	 Table 3: In-house NER data: Precision, Recall and  F 1 score on exact phrasal matches. The highest F 1  score among all the weakly supervised approaches  is shown in bold. Same for Tables 4 and 5.
NER	F 1  score	 Table 3: In-house NER data: Precision, Recall and  F 1 score on exact phrasal matches. The highest F 1  score among all the weakly supervised approaches  is shown in bold. Same for Tables 4 and 5.
neural machine translation task	BLEU	We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 BLEU points for domain adaptation from simulated bandit feedback.
ChineseEnglish translation task	BLEU	We validate our models on the ChineseEnglish translation task and achieve notable improvements: • On 16K vocabularies, NMT models are usually inferior in comparison with the phrase-based SMT, but our model surpasses phrase-based Moses by average 4.43 BLEU points and outperforms the attention-based NMT baseline system by 5.09 BLEU points.
ChineseEnglish translation task	BLEU	We validate our models on the ChineseEnglish translation task and achieve notable improvements: • On 16K vocabularies, NMT models are usually inferior in comparison with the phrase-based SMT, but our model surpasses phrase-based Moses by average 4.43 BLEU points and outperforms the attention-based NMT baseline system by 5.09 BLEU points.
Entity Relatedness	ALIGN	 Table 2: Entity Relatedness.  NDCG  MAP  @1  @5  @10  ALIGN  0.416 0.432 0.472 0.410  Entity2vec 0.593 0.595 0.636 0.566  SPME  0.593 0.594 0.636 0.566  MPME  0.613 0.613 0.654 0.582
MT evaluation	SIGN	Lastly, in Section 7 we assess the performance of the various algorithms and show that while they perform similarly in terms of automatic MT evaluation, SIGN is superior according to the human measures.
Sentiment classification	Acc	 Table 2: Sentiment classification performance of  different methods in different domains. Acc and  Fscore represent accuracy and macro-averaged  Fscore respectively.
Sentiment classification	Fscore	 Table 2: Sentiment classification performance of  different methods in different domains. Acc and  Fscore represent accuracy and macro-averaged  Fscore respectively.
Sentiment classification	accuracy	 Table 2: Sentiment classification performance of  different methods in different domains. Acc and  Fscore represent accuracy and macro-averaged  Fscore respectively.
Sentiment classification	Fscore	 Table 2: Sentiment classification performance of  different methods in different domains. Acc and  Fscore represent accuracy and macro-averaged  Fscore respectively.
POS tagging	unlabeled attachment score (UAS)	Results show that English syntax knowledge brings 51.50% and 25.01% relative error reduction on POS tagging and dependency parsing respectively, resulting in a Singlish dependency parser with 84.47% unlabeled attachment score (UAS) and 77.76% labeled attachment score (LAS).
POS tagging	labeled attachment score (LAS)	Results show that English syntax knowledge brings 51.50% and 25.01% relative error reduction on POS tagging and dependency parsing respectively, resulting in a Singlish dependency parser with 84.47% unlabeled attachment score (UAS) and 77.76% labeled attachment score (LAS).
dependency parsing	unlabeled attachment score (UAS)	Results show that English syntax knowledge brings 51.50% and 25.01% relative error reduction on POS tagging and dependency parsing respectively, resulting in a Singlish dependency parser with 84.47% unlabeled attachment score (UAS) and 77.76% labeled attachment score (LAS).
dependency parsing	labeled attachment score (LAS)	Results show that English syntax knowledge brings 51.50% and 25.01% relative error reduction on POS tagging and dependency parsing respectively, resulting in a Singlish dependency parser with 84.47% unlabeled attachment score (UAS) and 77.76% labeled attachment score (LAS).
SQA	accuracy	 Table 2: Accuracies of all systems on SQA; the  models in the first half of the table treat questions  independently, while those in the second half con- sider sequential context. Our method outperforms  existing ones both in terms of overall accuracy as  well as sequence accuracy.
SQA	accuracy	 Table 2: Accuracies of all systems on SQA; the  models in the first half of the table treat questions  independently, while those in the second half con- sider sequential context. Our method outperforms  existing ones both in terms of overall accuracy as  well as sequence accuracy.
number prediction	accuracy	In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.
sentiment analysis	accuracy	In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.
Q&A	accuracy	In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.
Name Tagging	F-score	 Table 5: Name Tagging F-score (%) on Non- Wikipedia Data
cross-lingual transfer learning	AR	 Table 3: Accuracy (acc; the higher the better; indicated by ↑) and edit distance (ED; the lower the better;  indicated by ↓) of cross-lingual transfer learning for paradigm completion. The target language is indicated  by "→", e.g., it is Spanish for "→ES". Sources are indicated in the row "source"; "0" is the monolingual  case. Except for Estonian, we train on n s = 12,000 source samples and n t ∈ {50, 200} target samples  (as indicated by the row). There are two baselines in the table. (i) "0": no transfer, i.e., we consider only  in-domain data; (ii) "AR": the Semitic language Arabic is unrelated to all target languages and functions  as a dummy language that is unlikely to provide relevant information. All languages are denoted using the  official codes (SME=Northern Sami).
word analogy inference	Accuracy	For word analogy inference, we consider two evaluation metrics: (1) Accuracy.
VISUAL	LOOKUP	In this section, we compare our proposed VISUAL model with the baseline LOOKUP model through three different sets of experiments.
classification	accuracy	The model was able to achieve around 40% classification accuracy when we use the full training set, compared to 55%, which is achieved by the model trained on simplified Chinese.
parse	accuracy	The relatively satisfactory coverage makes it possible to parse with high accuracy.
ASR	WER	The ASR system has a WER of 32% on a development set.
RTE decisions	agreement rate	Second, human agreement on the binary RTE decisions is very high, e.g., on the dataset used in our experiments, an average agreement rate of 87.8% with a κ level of 0.75 was reported (.
WMT16 German-English news translation task	BLEU	Experiments on the WMT16 German-English news translation task shown improved BLEU scores when compared to a syntax-agnostic NMT baseline trained on the same dataset.
Aspect extraction	precision	 Table 3: Aspect extraction results in precision, recall and F 1 score: Cross-Domain and In-Domain (−X  means all except domain X)
Aspect extraction	recall	 Table 3: Aspect extraction results in precision, recall and F 1 score: Cross-Domain and In-Domain (−X  means all except domain X)
Aspect extraction	F 1 score	 Table 3: Aspect extraction results in precision, recall and F 1 score: Cross-Domain and In-Domain (−X  means all except domain X)
caption generation	BLEU	In contrast to caption generation, our task does not require approximate metrics like BLEU.
RST annotations	F1-Measure	 Table 1: Human agreement on RST annotations in  terms of κ and F1-Measure.  Span Nuclearity Relation  κ  0.848  0.766  0.653  F1-Measure 0.872  0.724  0.522
IE	recall	note that bootstrapping for numerical IE is challenging; it can lead to high noise and missed recall, since numbers can easily match out of context, and numbers may not match due to approximations.
IE	recall	However, some facts are never fully mentioned, and no IE method has perfect recall.
semantic parsing	F 1	Universal schema () avoids the alignment problem by jointly embedding KB facts k b :h a s _ cit y k b :h a s _ co mp an y k b :p re s id en t_ of a rg 2 is the fi rs tn on -w hit e p re s id en to fa rg  The contributions of the paper are as follows (a) We show that universal schema representation is a better knowledge source for QA than either KB or text alone, (b) On the SPADES dataset, containing real world fill-in-the-blank questions, we outperform state-of-the-art semantic parsing baseline, with 8.5 F 1 points.
NER	BLEU	Comparison We report validation and test metrics for NER and MT tasks in and BLEU respectively.
Chinese-to-English translation task	BLEU	Extensive experiments on the Chinese-to-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods.
Chinese-to-English translation task	BLEU	Extensive experiments on the Chinese-to-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods.
ChineseEnglish translation	BLEU	• The extensive experiments on ChineseEnglish translation show that our model archives significant improvement by 3.4 BLEU points over the state-of-the-art system combination methods and 5.3 BLEU points over the best individual system output.
ChineseEnglish translation	BLEU	• The extensive experiments on ChineseEnglish translation show that our model archives significant improvement by 3.4 BLEU points over the state-of-the-art system combination methods and 5.3 BLEU points over the best individual system output.
Translation	BLEU score	 Table 3: Translation results (BLEU score) when  we replace original NMT with strong E-NMT,  which uses ensemble strategy with four NMT  models. All results of system combination are  based on strong outputs of E-NMT.
Domain adaptation	BLEU-4	 Table 2: Domain adaptation results (BLEU-4  scores) for WIKI-CJ using ASPEC-CJ.
Japanese caption generation	BLEU-1	 Table 2: Experimental results of Japanese caption generation. The numbers in boldface indicate the best  score for each evaluation measure.  BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE_L CIDEr
Japanese caption generation	BLEU-2	 Table 2: Experimental results of Japanese caption generation. The numbers in boldface indicate the best  score for each evaluation measure.  BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE_L CIDEr
Japanese caption generation	BLEU-3	 Table 2: Experimental results of Japanese caption generation. The numbers in boldface indicate the best  score for each evaluation measure.  BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE_L CIDEr
Japanese caption generation	BLEU-4 ROUGE_L CIDEr	 Table 2: Experimental results of Japanese caption generation. The numbers in boldface indicate the best  score for each evaluation measure.  BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE_L CIDEr
MWE recognition	F-measure	To evaluate MWE recognition, we used the F-measure for untagged / tagged MWEs (FUM/FTM) . For the pipeline model, we compared the gold MWEs with predictions by CRF.
MWE recognition	FUM/FTM)	To evaluate MWE recognition, we used the F-measure for untagged / tagged MWEs (FUM/FTM) . For the pipeline model, we compared the gold MWEs with predictions by CRF.
Analogy prediction	accuracy	 Table 2: Analogy prediction accuracy.
NER task	PER	In the NER task, the goal is to label each word in a given sequence using one of the following labels: PER, LOC, ORG, and MISC, which represent different Named Entity classes.
NER task	ORG	In the NER task, the goal is to label each word in a given sequence using one of the following labels: PER, LOC, ORG, and MISC, which represent different Named Entity classes.
NER task	MISC	In the NER task, the goal is to label each word in a given sequence using one of the following labels: PER, LOC, ORG, and MISC, which represent different Named Entity classes.
Document Classification task	F-measure	As can be seen in, the first three methods CBOW, Skip-gram and GloVe seem to perform relatively well for both the Document Classification task as well as the NER task with very comparable performance in terms of F-measure.
NER task	F-measure	As can be seen in, the first three methods CBOW, Skip-gram and GloVe seem to perform relatively well for both the Document Classification task as well as the NER task with very comparable performance in terms of F-measure.
Vertex classification	Gender	 Table 2: Vertex classification results on Gender  (e.g., GNRNN with depth 1 is represented by GN- RNN d1). Numbers in bold represent the highest  performance in each column in all tables.
relation extraction	F1	The analysis shows that our method relatively improves the relation extraction system by about 11% in F1.
RE	precision	Although DS has shown to be promising for RE, it also produces many noisy labels in the automatic annotated data, which deteriorate the performance of the system trained on it. showed that by simply adding a small set of high quality labeled instances (i.e., human-annotated training data) to a larger set of instances annotated by DS, makes the overall precision of the system significantly increases.
classification	accuracy	The performance is evaluated by classification accuracy, as was done in the NLI shared task.
NMT	BLEU	The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points.
NMT	BLEU	The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points.
translation	BLEU	We measure translation quality by singlereference case-insensitive BLEU () computed with the multi-bleu.perl script from Moses.
word alignment	BLEU	2. For our word alignment method, we find that only the candidate list derived from lexical translation table of IBM model 4 is adequate to achieve good BLEU/speedup trade-off for decoding on GPU.
tagging	accuracy	Results reports the tagging accuracy, showing that our models consistently outperform the baseline techniques.
POS tagging	accuracy	 Table 1: POS tagging accuracy on over the ten target languages, showing first approaches using only the  gold data; next methods using only distant cross-lingual supervision, and lastly joint multi-task learning.
POS tagging	accuracy	Character-based methods, in comparison, perform better on the task of POS tagging, reaching an accuracy of 95.9%, only 1.3% worse than morphological segmentation.
parsing	accuracy	We find that we can substantially improve parsing accuracy by training a single sequence-to-sequence model over multiple KBs, when providing an encoding of the domain at decoding time.
argument detection	F1 0.01 drop	The argument detection was less effective (F1 0.01 drop) than we expected due to the relatively simple and concise structure of tweets in general (Args in).
argument detection	Args	The argument detection was less effective (F1 0.01 drop) than we expected due to the relatively simple and concise structure of tweets in general (Args in).
POS tagging	accuracy	As POS tagging accuracy drops due to domain change, the parsing quality declines proportionally.
question answering	accuracy	We achieve state of the art results on this dataset and beat the previous benchmark on this dataset by a 1.5% margin improving the question answering accuracy from 54.1% to 55.6% and demonstrate improvements in each of the question categories.
RTA	MUGS	The RTA was designed to evaluate writing skills in Analysis, Evidence, Organization, Style, and MUGS (Mechanics, Usage, Grammar, and Spelling) dimensions.
SRL	precision	To evaluate the SRL system, precision, recall, F1 and accuracy will be calculated.
SRL	recall	To evaluate the SRL system, precision, recall, F1 and accuracy will be calculated.
SRL	F1	To evaluate the SRL system, precision, recall, F1 and accuracy will be calculated.
SRL	accuracy	To evaluate the SRL system, precision, recall, F1 and accuracy will be calculated.
ATC ASR	word error rates (WER)	Due to a constrained grammar and vocabulary, ATC ASR systems have relatively low word error rates (WER) when compared to other domains, such as broadcast news.
Depression prediction	accuracy	Depression prediction achieved 95.5% accuracy using selected lemmas as features.
personality recognition	F-score	From text, we improved personality recognition results with a CNN model on top of pre-trained word embeddings and obtained an average F-score of 71.0.
WSD tasks	accuracy	 Table 2: Application ofàof`ofà la carte synset embeddings to two standard WSD tasks. As all systems always  return exactly one answer, performance is measured in terms of accuracy. Results due to
beam search	length penalty α	We use beam search with abeam size of 4 and length penalty α = 0.6.
SNACS disambiguation	Accuracies	 Table 5: Overall performance of SNACS disambiguation systems on the test set. Results are reported for the role supersense  (Role), the function supersense (Func.), and their conjunction (Full). All figures are percentages. Left: Accuracies with gold  standard target identification (480 targets). Right: Precision, recall, and F 1 with automatic target identification ( §6.2 and table 4).
SNACS disambiguation	Precision	 Table 5: Overall performance of SNACS disambiguation systems on the test set. Results are reported for the role supersense  (Role), the function supersense (Func.), and their conjunction (Full). All figures are percentages. Left: Accuracies with gold  standard target identification (480 targets). Right: Precision, recall, and F 1 with automatic target identification ( §6.2 and table 4).
SNACS disambiguation	recall	 Table 5: Overall performance of SNACS disambiguation systems on the test set. Results are reported for the role supersense  (Role), the function supersense (Func.), and their conjunction (Full). All figures are percentages. Left: Accuracies with gold  standard target identification (480 targets). Right: Precision, recall, and F 1 with automatic target identification ( §6.2 and table 4).
SNACS disambiguation	F 1	 Table 5: Overall performance of SNACS disambiguation systems on the test set. Results are reported for the role supersense  (Role), the function supersense (Func.), and their conjunction (Full). All figures are percentages. Left: Accuracies with gold  standard target identification (480 targets). Right: Precision, recall, and F 1 with automatic target identification ( §6.2 and table 4).
argument generation	BLEU	 Table 3: Results on argument generation by BLEU
Classification	accuracy	 Table 3: Classification accuracy on WordNet test  set.
text classification tasks	error	Our method significantly outper-forms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets.
gradient clipping	early stopping	We used gradient clipping and early stopping to prevent overfitting.
PSD	F1 scores	However, PSD is considered a simpler task with higher F1 scores 3 . Therefore, we believe omitting PSD helps us gain more useful insights on character level models.
ERG-guided parsing	coverage	The main challenge for ERG-guided parsing is limited coverage.
AMR graphs	F 1	D-match is based on Smatch 5 , a metric used to evaluate AMR graphs ; it calculates F 1 on discourse representation graphs (DRGs), i.e., triples of nodes, arcs, and their referents, applying multiple restarts to obtain a good referent (node) mapping between graphs.
PAS analysis	accuracy	The state-of-the-art models for Japanese PAS analysis achieve an accuracy of around 50% for zero pronouns (.
coreference resolution	PA	This is realized by a combination of coreference resolution (called CR, hereafter) and predicate argument structure analysis (called PA, hereafter).
PA	accuracy	In English, there are few omissions of arguments, and thus PA is relatively easy, around 83% accuracy ( , while CR is relatively difficult, around 70% accuracy ( . On the other hand, in Japanese and Chinese, where arguments are often omitted, PA is a difficult task, and even state-of-the-art systems only achieve around 50% accuracy.
alignment	Pearson corre- lation r	 Table 1. It indicates that the observed effect of  power on alignment depends on the presence of  C pLen in the model. No collinearity is found be- tween C power and other predictors: Pearson corre- lation r < 0.2; Variance inflation factor (VIF) is  low (< 2.0)
alignment	Variance inflation factor (VIF)	 Table 1. It indicates that the observed effect of  power on alignment depends on the presence of  C pLen in the model. No collinearity is found be- tween C power and other predictors: Pearson corre- lation r < 0.2; Variance inflation factor (VIF) is  low (< 2.0)
Uncertainty interpretation	Overlap	 Table 6: Uncertainty interpretation against in- ferred ground truth; we compute the overlap be- tween tokens identified as contributing to uncer- tainty by our method and those found in the gold  standard. Overlap is shown for top 2 and 4 tokens.  Best results are in bold.
Name Tagging	F-score	Using Name Tagging as a target task, our approach achieved 4.3%-50.5% absolute F-score gains compared to the mono-lingual single-task baseline model.
question answering	Flips	These bugs are prevalent in image classifi- Figure 2: Semantically Equivalent Adversarial Rules: For the task of question answering, the proposed approach identifies transformation rules for questions in (a) that result in paraphrases of the queries, but lead to incorrect answers (#Flips is the number of times this happens in the validation data).
Meaning preservation	A/B	Meaning preservation was evaluated manually, using A/B testing ( §4).
style transfer	accuracy	The main contribution of this work is anew approach to style transfer that outperforms stateof-the-art baselines in both the quality of inputoutput correspondence (meaning preservation and fluency), and the accuracy of style transfer.
inverse reinforcement learning	AREL	We draw our inspiration from recent progress in inverse reinforcement learning and propose the AREL algorithm to learn a more intelligent reward function.
Machine Translation	BPE	 Table 7: Machine Translation results on the Multi30k English ! German task. We note that our models  do not use BPE, and we perform better in BLEU relative to METEOR.
Machine Translation	BLEU	 Table 7: Machine Translation results on the Multi30k English ! German task. We note that our models  do not use BPE, and we perform better in BLEU relative to METEOR.
sentiment prediction	AE-LSTM	• SVM (): It is a traditional support vector machine based model with extensive feature engineering; • AdaRNN (): It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree; • AE-LSTM, and ATAE-LSTM (: AE-LSTM is a simple LSTM model incorporating the target embedding as input, while ATAE-LSTM extends AE-LSTM with attention; • IAN (Ma et al., 2017): IAN employs two LSTMs to learn the representations of the context and the target phrase interactively; • CNN-ASP: It is a CNN-based model implemented by us which directly concatenates target representation to each word embedding; • based on the top-most sentence representations; • BILSTM-ATT-G (Liu and Zhang, 2017): It models left and right contexts using two attention-based LSTMs and introduces gates to measure the importance of left context, right context, and the entire sentence for the prediction; • RAM (Chen et al., 2017): RAM is a multilayer architecture where each layer consists of attention-based aggregation of word features and a GRU cell to learn the sentence representation.
sentiment prediction	BILSTM-ATT-G	• SVM (): It is a traditional support vector machine based model with extensive feature engineering; • AdaRNN (): It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree; • AE-LSTM, and ATAE-LSTM (: AE-LSTM is a simple LSTM model incorporating the target embedding as input, while ATAE-LSTM extends AE-LSTM with attention; • IAN (Ma et al., 2017): IAN employs two LSTMs to learn the representations of the context and the target phrase interactively; • CNN-ASP: It is a CNN-based model implemented by us which directly concatenates target representation to each word embedding; • based on the top-most sentence representations; • BILSTM-ATT-G (Liu and Zhang, 2017): It models left and right contexts using two attention-based LSTMs and introduces gates to measure the importance of left context, right context, and the entire sentence for the prediction; • RAM (Chen et al., 2017): RAM is a multilayer architecture where each layer consists of attention-based aggregation of word features and a GRU cell to learn the sentence representation.
Classification	accuracy	 Table 4: Classification accuracy in % given by C s ,  C t and WSM with different feature sets for elec- tronics as source and movie as target.
Classification	WSM	 Table 4: Classification accuracy in % given by C s ,  C t and WSM with different feature sets for elec- tronics as source and movie as target.
Cross-domain sentiment classification	accuracy	 Table 5: Cross-domain sentiment classification accuracy in the target domain (Source (S) → Target (T)).
sentiment transformation	accuracy	We quantitatively measure sentiment transformation by evaluating the accuracy of generating designated sentiment.
content preservation	BLEU score	To evaluate the content preservation performance, we use the BLEU score () between the transferred sentence and the source sentence as an evaluation metric.
sentiment transformation	accuracy	For evaluating sentiment transformation, even with a high accuracy, the sentiment classifier sometimes generates noisy results, especially for those neutral sentences (e.g., "I ate a cheese sandwich").
content preservation	BLEU score	For evaluating content preservation, the BLEU score Input: I would strongly advise against using this company.
sentiment  transformation	BLEU	 Table 1: Automatic evaluations of the proposed  method and baselines. ACC evaluates sentiment  transformation. BLEU evaluates content preserva- tion. G-score is the geometric mean of ACC and  BLEU.
sentiment  transformation	G-score	 Table 1: Automatic evaluations of the proposed  method and baselines. ACC evaluates sentiment  transformation. BLEU evaluates content preserva- tion. G-score is the geometric mean of ACC and  BLEU.
sentiment  transformation	BLEU	 Table 1: Automatic evaluations of the proposed  method and baselines. ACC evaluates sentiment  transformation. BLEU evaluates content preserva- tion. G-score is the geometric mean of ACC and  BLEU.
information extraction	F-measure	This paper focuses on detection tasks in information extraction, where positive instances are sparsely distributed and models are usually evaluated using F-measure on positive classes.
POS tagging	Avg	 Table 3: Accuracy scores on dev set of target domain for POS tagging for 10% labeled data. Avg: average  over the 5 SANCL domains. Hyperparameter ep (epochs) is tuned on Answers dev. µ pseudo : average  amount of added pseudo-labeled data. FLORS: results for Batch (u:big) from (Yin et al., 2015) (see  §3).
POS tagging	Hyperparameter ep (epochs)	 Table 3: Accuracy scores on dev set of target domain for POS tagging for 10% labeled data. Avg: average  over the 5 SANCL domains. Hyperparameter ep (epochs) is tuned on Answers dev. µ pseudo : average  amount of added pseudo-labeled data. FLORS: results for Batch (u:big) from (Yin et al., 2015) (see  §3).
POS tagging	FLORS	 Table 3: Accuracy scores on dev set of target domain for POS tagging for 10% labeled data. Avg: average  over the 5 SANCL domains. Hyperparameter ep (epochs) is tuned on Answers dev. µ pseudo : average  amount of added pseudo-labeled data. FLORS: results for Batch (u:big) from (Yin et al., 2015) (see  §3).
coding	accuracy	This hierarchy can be leveraged to improve coding accuracy.
parsing	accuracy	By having weight functions of varying smoothness for different production rules, LVeGs can also control the level of subtype granularity for different productions, which has been shown to improve the parsing accuracy ().
translation	BLEU score	Second, translation performance is measured in BLEU score.
SDEC	ELBO	We observed that SDEC often reached better ELBO values than SENT indicating a better model fit.
SDEC	SENT	We observed that SDEC often reached better ELBO values than SENT indicating a better model fit.
parsing	accuracy	However, the parsing accuracy is still behind state-of-the-art graph-based parsers.
AAE syntactic	AA	 Table 5: Examples of AAE syntactic phenomena and occurrence counts in the 250 AA and 250 WH  tweet sets.
Roundtrip translation	F 1 )	 Table 3: Roundtrip translation (mean/median accuracy) and sentiment analysis (F 1 ) results for word- based (WORD) and character-based (CHAR) multilingual embeddings. N (coverage): # queries con- tained in the embedding space. The best result across WORD and CHAR is set in bold.
binary relation schema induction	speed	Recently, tensor factorization-based methods have been proposed for binary relation schema induction (, with gains in both speed and accuracy over previously proposed generative models.
binary relation schema induction	accuracy	Recently, tensor factorization-based methods have been proposed for binary relation schema induction (, with gains in both speed and accuracy over previously proposed generative models.
sequence prediction	RAML	Inspired by the connection , we propose two sequence prediction algorithms, one extending RAML with fine-grained credit assignment and the other improving Actor-Critic with a systematic entropy regularization.
MLE	exposure bias	Secondly, MLE can suffer from the exposure bias, which refers to the phenomenon that the model is never exposed to its own failures during training, and thus cannot recover from an error attest time.
extractive RC	exact match (EM)	The often used evaluation metrics for extractive RC are exact match (EM) and F1 (.
extractive RC	F1	The often used evaluation metrics for extractive RC are exact match (EM) and F1 (.
translation	BLEU	displays the translation performance measured in terms of BLEU and TER scores.
translation	TER	displays the translation performance measured in terms of BLEU and TER scores.
RL	BLEU	These RL approaches focus on improving performance in automatic evaluation by simulating reward signals by evaluation metrics such as BLEU, F1-score, or ROUGE, computed against gold standards.
RL	F1-score	These RL approaches focus on improving performance in automatic evaluation by simulating reward signals by evaluation metrics such as BLEU, F1-score, or ROUGE, computed against gold standards.
RL	ROUGE	These RL approaches focus on improving performance in automatic evaluation by simulating reward signals by evaluation metrics such as BLEU, F1-score, or ROUGE, computed against gold standards.
Reinforcement learning	REINFORCE	Several solutions are readily available: • Reinforcement learning (most notably the REINFORCE algorithm;, and structured attention (SA;).
classification	accuracy	Pipelined semantic dependency predictions brings 0.9% absolute improvement in classification accuracy, and SPIGOT outperforms all baselines.
Semantic dependency parsing	F 1 scores	 Table 1: Semantic dependency parsing perfor- mance in both unlabeled (UF ) and labeled (LF )  F 1 scores. Bold font indicates the best perfor- mance. Peng et al. (2017) does not report UF .
Semantic dependency parsing	UF	 Table 1: Semantic dependency parsing perfor- mance in both unlabeled (UF ) and labeled (LF )  F 1 scores. Bold font indicates the best perfor- mance. Peng et al. (2017) does not report UF .
relation extraction	F1 scores	On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5-100 faster by providing explanations instead of just labels.
question generation evaluation	BLEU	For question generation evaluation, we use BLEU () and ME-TEOR).
question generation evaluation	ME-TEOR	For question generation evaluation, we use BLEU () and ME-TEOR).
answer candidate extraction evaluation	precision	For answer candidate extraction evaluation, we use precision, recall and F-measure vs. the gold standard SQuAD answers.
answer candidate extraction evaluation	recall	For answer candidate extraction evaluation, we use precision, recall and F-measure vs. the gold standard SQuAD answers.
answer candidate extraction evaluation	F-measure	For answer candidate extraction evaluation, we use precision, recall and F-measure vs. the gold standard SQuAD answers.
stock prediction	accuracy	Following previous work for stock prediction, we adopt the standard measure of accuracy and Matthews Correlation Coefficient (MCC) as evaluation metrics.
stock prediction	Matthews Correlation Coefficient (MCC)	Following previous work for stock prediction, we adopt the standard measure of accuracy and Matthews Correlation Coefficient (MCC) as evaluation metrics.
sentence extraction	ROUGE score	 Table 2: Final results on the test set. POINTER- NET is the sentence extraction system of Cheng  and Lapata. XNET is our best model from Table  1. Best ROUGE score in each block and each col- umn is highlighted in boldface.
Span Detection	F-score	 Table 4: Results for Span Detection on the dense  development dataset. Span detection results are  given with the cutoff threshold τ at 0.5, and at  the value which maximizes F-score. The top chart  lists precision, recall and F-score with exact span  match, while the bottom reports matches where  the intersection over union (IOU) is ≥ 0.5.
Span Detection	precision	 Table 4: Results for Span Detection on the dense  development dataset. Span detection results are  given with the cutoff threshold τ at 0.5, and at  the value which maximizes F-score. The top chart  lists precision, recall and F-score with exact span  match, while the bottom reports matches where  the intersection over union (IOU) is ≥ 0.5.
Span Detection	recall	 Table 4: Results for Span Detection on the dense  development dataset. Span detection results are  given with the cutoff threshold τ at 0.5, and at  the value which maximizes F-score. The top chart  lists precision, recall and F-score with exact span  match, while the bottom reports matches where  the intersection over union (IOU) is ≥ 0.5.
Span Detection	F-score	 Table 4: Results for Span Detection on the dense  development dataset. Span detection results are  given with the cutoff threshold τ at 0.5, and at  the value which maximizes F-score. The top chart  lists precision, recall and F-score with exact span  match, while the bottom reports matches where  the intersection over union (IOU) is ≥ 0.5.
Span Detection	exact span  match	 Table 4: Results for Span Detection on the dense  development dataset. Span detection results are  given with the cutoff threshold τ at 0.5, and at  the value which maximizes F-score. The top chart  lists precision, recall and F-score with exact span  match, while the bottom reports matches where  the intersection over union (IOU) is ≥ 0.5.
Span detection	F-score	 Table 4: Results for Span Detection on the dense  development dataset. Span detection results are  given with the cutoff threshold τ at 0.5, and at  the value which maximizes F-score. The top chart  lists precision, recall and F-score with exact span  match, while the bottom reports matches where  the intersection over union (IOU) is ≥ 0.5.
Span detection	precision	 Table 4: Results for Span Detection on the dense  development dataset. Span detection results are  given with the cutoff threshold τ at 0.5, and at  the value which maximizes F-score. The top chart  lists precision, recall and F-score with exact span  match, while the bottom reports matches where  the intersection over union (IOU) is ≥ 0.5.
Span detection	recall	 Table 4: Results for Span Detection on the dense  development dataset. Span detection results are  given with the cutoff threshold τ at 0.5, and at  the value which maximizes F-score. The top chart  lists precision, recall and F-score with exact span  match, while the bottom reports matches where  the intersection over union (IOU) is ≥ 0.5.
Span detection	F-score	 Table 4: Results for Span Detection on the dense  development dataset. Span detection results are  given with the cutoff threshold τ at 0.5, and at  the value which maximizes F-score. The top chart  lists precision, recall and F-score with exact span  match, while the bottom reports matches where  the intersection over union (IOU) is ≥ 0.5.
Span detection	exact span  match	 Table 4: Results for Span Detection on the dense  development dataset. Span detection results are  given with the cutoff threshold τ at 0.5, and at  the value which maximizes F-score. The top chart  lists precision, recall and F-score with exact span  match, while the bottom reports matches where  the intersection over union (IOU) is ≥ 0.5.
SRL	labeled attachment score (LAS)	With the help of the proposed k-order argument pruning algorithm over syntactic tree, our model obtains state-of-the-art scores on the CoNLL benchmarks for both In order to quantitatively evaluate the contribution of syntax to SRL, we adopt the ratio between labeled F 1 score for semantic dependencies (Sem-F 1 ) and the labeled attachment score (LAS) for syntactic dependencies introduced by CoNLL-2008 Shared Task 1 as evaluation metric.
REs	precision	As a result, REs are often ensembled with data-driven methods, such as neural network (NN) based techniques, where a set of carefully-written REs are used to handle certain cases with high precision, leaving the rest for data-driven methods.
intent detection	feat	As seen in, for intent detection, while two+both still works, feat and logit no longer give improvements.
domain classification	accuracy	We demonstrate that incorporating personalization significantly improves domain classification accuracy in a setting with thousands of overlapping domains.
emotion recognition	accuracy	For emotion recognition due to the natural imbalances across various emotions, we use weighted accuracy () and F1 measure.
emotion recognition	F1 measure	For emotion recognition due to the natural imbalances across various emotions, we use weighted accuracy () and F1 measure.
multiclass classification	F1 score	For binary classification and multiclass classification, we report F1 score and accuracy Acc−k where k denotes the number of classes.
multiclass classification	accuracy	For binary classification and multiclass classification, we report F1 score and accuracy Acc−k where k denotes the number of classes.
multiclass classification	Acc	For binary classification and multiclass classification, we report F1 score and accuracy Acc−k where k denotes the number of classes.
CONTRAST	BUT	We speculated that this maybe because a passage such as (14) below presents an argument in which the second segment serves as a REASON (hence, BECAUSE) for the first segment, but also serves to CONTRAST with it (hence, BUT).
sentiment analysis	QA	 Table 2: Dataset information. Here SA refers to sentiment analysis, and QA refers to question answering.
Classification	accuracy	 Table 1: Classification accuracy of different models on Web snippet and 20NG, with different number of  topic K settings.
document classification	F 1	We then discuss additional settings and present experimental results of the two tasks, i.e., document classification and citation recommendation, respectively.: F 1 on DBLP when newcomers are discarded.
citation recommendation	F 1	We then discuss additional settings and present experimental results of the two tasks, i.e., document classification and citation recommendation, respectively.: F 1 on DBLP when newcomers are discarded.
citation recommendation	dimension size k	 Table 6: Top-10 citation recommendation results (dimension size k = 100).
classification	accuracy	We use the same data split as in previous work and classification accuracy as the evaluation metric.
negate	accuracy	Notably, even a single handwritten rule, negate, improves the accuracy on the negation examples in SNLI by 6.1%.
sentiment classification	ac- curacy	 Table 3: Performance of sentiment classification  task. 3-5 jamo n-grams and 1-6 chracter n-grams  show slightly higher performance in terms of ac- curacy and f1-score over comparison models.
multi-label classification task	mean average precision (MAP)	We cast this as a multi-label classification task, and adopt mean average precision (MAP) as the evaluation metric.
sentiment classification	RMSE	Evaluation is done using two metrics: the Accuracy which measures the overall sentiment classification performance and the RMSE which measures the diver-  gence between predicted and ground truth classes.
paragraph generation	BLUE-n	 Table 1: Main results for paragraph generation on the IU X-Ray dataset (upper part), and single sentence  generation on the PEIR Gross dataset (lower part). BLUE-n denotes the BLEU score that uses up to  n-grams.
paragraph generation	BLEU score	 Table 1: Main results for paragraph generation on the IU X-Ray dataset (upper part), and single sentence  generation on the PEIR Gross dataset (lower part). BLUE-n denotes the BLEU score that uses up to  n-grams.
part-of-speech tagging	accuracy	The rise of neural networks, and particularly recurrent neural networks, has produced significant advances in part-of-speech tagging accuracy (Zeman et al., 2017).
Morphosyntactic tagging	accuracy	Morphosyntactic tagging accuracy has seen dramatic improvements through the adoption of recurrent neural networks-specifically BiLSTMs () to create sentence-level context sensitive encodings of words.
parsing	accuracy	However, the recent popularization of bidirectional long-short term memory networks (biLSTMs;) to derive feature representations for parsing, given their capacity to capture long-range information, has demonstrated that one may not need to use complex feature models to obtain good accuracy.
parsing	accuracy	For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy.
parsing	accuracy	In Section 3 we present our find-ing that when our parser learns to make an implicit trade-off between these two types of attention, it predominantly makes use of position-based attention, and show that explicitly factoring the two types of attention can noticeably improve parsing accuracy.
parsing	accuracy	We find that using the deep contextualized representations proposed by can boost parsing accuracy.
parsing	accuracy	The addition of pre-trained word representations following increases parsing accuracy to 95.13 F1, anew stateof-the-art for this dataset.
parsing	F1	The addition of pre-trained word representations following increases parsing accuracy to 95.13 F1, anew stateof-the-art for this dataset.
parsing	accuracy	Treebank conversion is a straightforward and effective way to exploit various heterogeneous treebanks for boosting parsing accuracy.
parsing	accuracy	We use the standard labeled attachment score (LAS, UAS for unlabeled) to measure the parsing and conversion accuracy.
automatic speech recognition (ASR)	word error rate (WER)	Measuring the performance of automatic speech recognition (ASR) systems requires manually transcribed data in order to compute the word error rate (WER), which is often time-consuming and expensive.
ASR quality estimation	WER	developed a tool for ASR quality estimation, TranscRater, which is capable of predicting WER per utterance.
sentiment analysis	accuracy	We evaluate on the tasks of POS-tagging and sentiment analysis, protecting several demographic attributes -gender, age, and location -and show empirically that doing so does not hurt accuracy, but instead can lead to substantial gains, most notably in out-of-domain evaluation.
sentiment classification task	Precision	 Table 2: This table shows results obtained by us- ing sentence embeddings from the InferSent en- coder in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard devi- ations (STD). Best results are obtained by KCCA  (GlvCC, LSA) and are highlighted in boldface.
sentiment classification task	F-score	 Table 2: This table shows results obtained by us- ing sentence embeddings from the InferSent en- coder in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard devi- ations (STD). Best results are obtained by KCCA  (GlvCC, LSA) and are highlighted in boldface.
sentiment classification task	AUC	 Table 2: This table shows results obtained by us- ing sentence embeddings from the InferSent en- coder in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard devi- ations (STD). Best results are obtained by KCCA  (GlvCC, LSA) and are highlighted in boldface.
validation	accuracy	However, the difficulty and cost of data generation and validation are still substantial if we need a large amount of data for the system to achieve high accuracy; if the logical forms can express complex combinations of semantic primitives that must be covered; or if the target language is one with relatively few crowd workers.
Relation extraction	significance	 Table 1: Relation extraction performance on ACE  2005 test dataset. * denotes significance at p <  0.05 compared to SPTree, denotes significance  at p < 0.05 compared to the Baseline.
co-location classification task	Acc.	 Table 2: Performance of baselines on co-location classification task with ablation. (Acc.=Accuracy,  P=Precision, R=Recall, "-" means without certain feature)
co-location classification task	Accuracy	 Table 2: Performance of baselines on co-location classification task with ablation. (Acc.=Accuracy,  P=Precision, R=Recall, "-" means without certain feature)
co-location classification task	Precision	 Table 2: Performance of baselines on co-location classification task with ablation. (Acc.=Accuracy,  P=Precision, R=Recall, "-" means without certain feature)
text alignment task	plagdet	Standard evaluation framework for text alignment task is plagdet (, which consists of macro-and micro-averaged precision, recall, granularity and the overall plagdet score.
text alignment task	precision	Standard evaluation framework for text alignment task is plagdet (, which consists of macro-and micro-averaged precision, recall, granularity and the overall plagdet score.
text alignment task	recall	Standard evaluation framework for text alignment task is plagdet (, which consists of macro-and micro-averaged precision, recall, granularity and the overall plagdet score.
classification	accuracy	In particular, when both of them are removed (last row in), although the classification accuracy improves, the perplexity and the content preservation drop significantly.
classification	perplexity	In particular, when both of them are removed (last row in), although the classification accuracy improves, the perplexity and the content preservation drop significantly.
Classification	accuracy	 Table 1: Classification accuracy, content preserva- tion and perplexity for two datasets.
Symptom Extraction	BIO	Symptom Extraction We follow the BIO (begin-in-out) schema for symptom identification).
classification	accuracy	As for classification models, we use accuracy as the metric.
translation	accuracy	The 5th column is the translation accuracy of the map M x 0 , trained on the train- ing data of x 0 , and tested on the test data in xi . We use precision at top-10 as a measure of translation accuracy.
translation	precision	The 5th column is the translation accuracy of the map M x 0 , trained on the train- ing data of x 0 , and tested on the test data in xi . We use precision at top-10 as a measure of translation accuracy.
translation	accuracy	We also see a correlation with the translation accuracy in the 5th column.
Topic modeling	Latent Dirichlet Allocation (LDA)	Topic modeling is the standard technique for such purposes, and Latent Dirichlet Allocation (LDA) () is the most used algorithm, which models the documents as distribution over topics and topics as distribution over words.
SemEval challenge	accuracy	These were both organized in academia, e.g.,, or companies, e.g., Quora . An interesting outcome of the SemEval challenge was that syntactic information is essential to achieve high accuracy in question reranking tasks.
Neural machine translation (NMT)	accuracy	Neural machine translation (NMT) models are typically trained with fixed-size input and output vocabularies, which creates an important bottleneck on their accuracy and generalization capability.
translation	Incorrect se- lection ratio	 Table 2: Impact of user errors on the translation  performance. Under selection ratio% indicates  on average what percentage of words in a correct  chunk have not been selected in user simulation,  but all selected words are correct. Incorrect se- lection ratio% indicates what percentage of words  are incorrectly selected, here the total number of  marked words is the same as in chunk-level feed- back. In the last row, 10% of marked words are  actually incorrect and the total number of marked  words is 25% less compared to system in row 1.
slot value prediction	vary- ing training size	 Table 2: Results of slot value prediction with vary- ing training size and OOV ratio.
slot value prediction	OOV ratio	 Table 2: Results of slot value prediction with vary- ing training size and OOV ratio.
beam search	F1	We found that beam search improved performance for these two parsers by around 0.1-0.3 F1 on the development sets, and use it at inference time in every setting for these two parsers.
beam search	O	We further employ beam search to have a practical runtime of O(nb 2 ) at the cost of exact search where b is the beam size.
WLD	accuracy	For the WLD constructed by the "that + topic concept" query the accuracy is well below the accuracy achieved when using SLD alone, as can be seen in the right plot.
WLD	accuracy	For the WLD constructed by the "that + topic concept" query the accuracy is well below the accuracy achieved when using SLD alone, as can be seen in the right plot.
MT output	BLEU	first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU () rely on the availability of reference translations to evaluate MT output quality, which aren't always available.
MT	WER	There is an abundance of evaluation metrics available for MT including WER (Su et al.), BLEU (), NIST) and ME-TEOR (), all of which compare the similarity between reference translations and translations.
MT	BLEU	There is an abundance of evaluation metrics available for MT including WER (Su et al.), BLEU (), NIST) and ME-TEOR (), all of which compare the similarity between reference translations and translations.
MT	ME-TEOR	There is an abundance of evaluation metrics available for MT including WER (Su et al.), BLEU (), NIST) and ME-TEOR (), all of which compare the similarity between reference translations and translations.
MT	BLEU	As such, measuring interpretation quality by some metrics employed in MT such as BLEU can result in artificially low scores (.
DDI extraction	F-score	In the experiments, we show that GCNs can predict DDIs from the molecular structures of drugs in high accuracy and the molecular information can enhance text-based DDI extraction by 2.39 percent points in the F-score on the DDIExtraction 2013 shared task data set.
DDI extraction from texts	F-score	We also show molecular information can enhance the performance of DDI extraction from texts in 2.39 percent points in F-score.
TS	BLEU	We show that it outperforms state-of-the-art TS approaches (up to 3 and 12 BLEU and SARI points, respectively), including when training data for the specific complex-simple combination of grade levels is not available, i.e. zero-shot learning.
TS	SARI	We show that it outperforms state-of-the-art TS approaches (up to 3 and 12 BLEU and SARI points, respectively), including when training data for the specific complex-simple combination of grade levels is not available, i.e. zero-shot learning.
medical captioning task	BLEU	For medical captioning task, we will use standard image captioning metrics such as BLEU (), ROUGE), METEOR (Banerjee and), CIDEr (, and SPICE).
medical captioning task	ROUGE	For medical captioning task, we will use standard image captioning metrics such as BLEU (), ROUGE), METEOR (Banerjee and), CIDEr (, and SPICE).
medical captioning task	METEOR	For medical captioning task, we will use standard image captioning metrics such as BLEU (), ROUGE), METEOR (Banerjee and), CIDEr (, and SPICE).
information retrieval	Precision	For this task, standard information retrieval metrics such as Precision, Recall, and F-score will be used.
information retrieval	Recall	For this task, standard information retrieval metrics such as Precision, Recall, and F-score will be used.
information retrieval	F-score	For this task, standard information retrieval metrics such as Precision, Recall, and F-score will be used.
SAS	F1 score	Our algorithm outperforms the state of the art SAS method by 1.7% F1 score in node prediction.
SAS	F1	Our algorithm outperforms the previous state of the art methods for SAS by 1.7% F1 score on Node prediction.
SAS	F1	We outperform the state-of-the-art in SAS by 1.7% F1 scores in node prediction.
MT	BLEU score	Ina Japanese-to-English MT experiment, our method achieves a BLEU score that is 0.56 points more than that of the frequency-based method.
name tagging	F 1 -score	For example, under DARPA LORELEI, 2 the performance of two native Uighur speakers on name tagging was only 69% and 73% F 1 -score respectively.
Translation	BPE	 Table 2: Translation time in seconds for newstest- 2017 (3,004 sentences, 76,501 source BPE tokens)  for different architectures and batch sizes.
MMR	P(interaction)	The observed MMR by tumor site interaction was validated in an independent cohort of stage III colon cancers (P(interaction) = .037).
Query refinement	recall	Query refinement models are evaluated both, by their ability to rank relevant query terms high, and by the recall of retrieved relevant documents when the query automatically is refined by the 1st, 2nd and 3rd proposed query term.
parsing	BLLIP	For parsing the questions, we used BLLIP reranking parser) (Charniak-Johnson parser) and used the model GENIA+PubMed for biomedical text.
detecting bacteria boundaries	SER	According to the official evaluation, TagIt system achieved the best performance on detecting bacteria boundaries (SER: 0.236, recall: 0.772, precision: 0.954), while LIMSI system worked best on habitat entities (SER: 0.597, recall: 0.504, precision: 0.728).
detecting bacteria boundaries	recall	According to the official evaluation, TagIt system achieved the best performance on detecting bacteria boundaries (SER: 0.236, recall: 0.772, precision: 0.954), while LIMSI system worked best on habitat entities (SER: 0.597, recall: 0.504, precision: 0.728).
detecting bacteria boundaries	precision	According to the official evaluation, TagIt system achieved the best performance on detecting bacteria boundaries (SER: 0.236, recall: 0.772, precision: 0.954), while LIMSI system worked best on habitat entities (SER: 0.597, recall: 0.504, precision: 0.728).
detecting bacteria boundaries	SER	According to the official evaluation, TagIt system achieved the best performance on detecting bacteria boundaries (SER: 0.236, recall: 0.772, precision: 0.954), while LIMSI system worked best on habitat entities (SER: 0.597, recall: 0.504, precision: 0.728).
detecting bacteria boundaries	recall	According to the official evaluation, TagIt system achieved the best performance on detecting bacteria boundaries (SER: 0.236, recall: 0.772, precision: 0.954), while LIMSI system worked best on habitat entities (SER: 0.597, recall: 0.504, precision: 0.728).
detecting bacteria boundaries	precision	According to the official evaluation, TagIt system achieved the best performance on detecting bacteria boundaries (SER: 0.236, recall: 0.772, precision: 0.954), while LIMSI system worked best on habitat entities (SER: 0.597, recall: 0.504, precision: 0.728).
PMC articles	maxi- mum	 Table 3. The value of full text PMC articles  in the retrieval performance. In combined  retrieval, we assign each article the maxi- mum of its PubMed and PMC score and  evaluate based on that maximum.
prediction	accuracy	Ina previous study, prediction accuracy was increased slightly by adding diagnosis disease name and independent variables such as prescription medicine.
classification	accuracy	Several studies have reported the effectiveness of using machine learning technique to improve classification accuracy (.
sentence scoring	learning rate	The sentence scoring neural network is trained by) with a learning rate of 0.01 and a batch size of 25.
generative models of text	BLEU	We can then use standard automatic evaluation metrics for generative models of text such as BLEU.
question answering based machine reading comprehension (MRC)	ROUGE	Current evaluation metrics to question answering based machine reading comprehension (MRC) systems generally focus on the lexical overlap between candidate and reference answers, such as ROUGE and BLEU.
question answering based machine reading comprehension (MRC)	BLEU	Current evaluation metrics to question answering based machine reading comprehension (MRC) systems generally focus on the lexical overlap between candidate and reference answers, such as ROUGE and BLEU.
adaptation	ROUGE	Then we give details about our adaptation on ROUGE and BLEU in section 3.
adaptation	BLEU	Then we give details about our adaptation on ROUGE and BLEU in section 3.
translation	accuracy	Accuracy Measures: As a measure of translation accuracy, we used BLEU () and NIST) scores.
translation	BLEU	Accuracy Measures: As a measure of translation accuracy, we used BLEU () and NIST) scores.
WMT News Translation Task  English↔German	BLEU	 Table 2:  WMT News Translation Task  English↔German, reporting cased BLEU on  newstest2017, evaluating the impact of the quality  of the back-translation system on the final system.  Note that the back-translation systems run in the  opposite direction and are not comparable to the  numbers in the same row.
translating En↔De	FA-NMT	 Table 2: Results for translating En↔De, En↔Ru, and Ru→Ar. Statistical significances are marked as   † p < 0.05 and  ‡ p < 0.01 when compared against the baselines and / when compared against the  FA-NMT (no-shared). The results indicate the strength of our proposed shared-attention for NMT.
tokenization	UAS	 Table 5: F1 scores of tokenization and UAS on the UD Japanese-GSD test set. The top section shows the  systems which used their own tokenizers. The second section is a comparison with the systems relying  on the default settings of UDPipe, and the bottom section is the situation to ru parsers using the gold PoS  as input.
machine translation	accuracy	Recent work in machine translation has demonstrated that self-attention mechanisms can be used in place of recurrent neural networks to increase training speed without sacrificing model accuracy.
sentiment analysis	TREC -)	Following ( we have tested our approach on a wide array of classification tasks, including sentiment analysis, SST -Socher et al.), question-type (TREC -), product reviews (CR -Hu and Liu), subjectivity/objectivity (SUBJ -Pang and) and opinion polarity).
Cross-lingual synonym comparison	av- erage rates	 Table 4: Cross-lingual synonym comparison  results on 200 one-to-one word pairs, the av- erage rates(%) of each models.
translation	length normalization	We perform translation via beam search with a beam-size of 10 and length normalization of 0.9.
validation	F1-Score	In the validation stage, we used F1-Score for 7 and 32 classes to measure the effects of the coarse and fine-grained annotation levels.
NER	B-LOCATION	This is essential for NER, where for instance, B-LOCATION cannot be followed by I-PERSON.
validate	accuracy	In order to validate, we split our data and used 80% for training and rest for validating, achieving an accuracy of 79% on this validation data.
NER	accuracy	Firstly, NER has proved to be more difficult for Tweets than for longer text, as accuracy in NER ranges from 85-90% on longer texts compared to 30-50% on Tweets (.
NER on Noisy Usergenerated Text (W-NUT)	F1	The 2015 and 2016 shared tasks for NER on Noisy Usergenerated Text (W-NUT) reported F1 scores between 16.47 and 52.41 for identifying 10 different NE categories ().
classification of sentiment	precision	We evaluated our model's performance on basic classification of sentiment using precision, recall and F 1 -scores.
classification of sentiment	recall	We evaluated our model's performance on basic classification of sentiment using precision, recall and F 1 -scores.
classification of sentiment	F 1 -scores	We evaluated our model's performance on basic classification of sentiment using precision, recall and F 1 -scores.
sentiment analysis	MAE	 Table 1: Performance of individual modality and multimodal fusion for sentiment analysis on the vali- dation set of CMU-MOSEI. MAE is the Mean Absolute Error.
sentiment analysis	Mean Absolute Error	 Table 1: Performance of individual modality and multimodal fusion for sentiment analysis on the vali- dation set of CMU-MOSEI. MAE is the Mean Absolute Error.
Bimodal prediction	overall mean- absolute error (MAE)	 Table 3: Bimodal prediction results, overall mean- absolute error (MAE) for each DNN and ablation.
Emotion Recognition	MAE	 Table 2: Emotion Recognition Model Results -MAE scores
validation	accuracy	The overall validation accuracy (weighted) is 83.11% and class validation accuracy (unweighted) is 77.23% as shown in.
ASR view	accuracy	 Table 2: Improvement in ASR view accuracy us- ing a non contextual classifier.  MOSI MOSEI  MT  71.1  67.5  AT  63.7  63.8  AT ↑  65.1  65.7
ASR view	AT	 Table 2: Improvement in ASR view accuracy us- ing a non contextual classifier.  MOSI MOSEI  MT  71.1  67.5  AT  63.7  63.8  AT ↑  65.1  65.7
ASR view	AT	 Table 2: Improvement in ASR view accuracy us- ing a non contextual classifier.  MOSI MOSEI  MT  71.1  67.5  AT  63.7  63.8  AT ↑  65.1  65.7
multimodal sentiment analysis	F1 Score	Our experiments on multimodal sentiment analysis using the CMU-MOSI dataset indicate that our methods learn informative mul-timodal representations that outperform the baselines and achieve improved performance on multimodal sentiment analysis , specifically in the Bimodal case where our model is able to improve F1 Score by twelve points.
prediction	accuracy	The "pure" condition resulted in a relatively smaller prediction accuracy.
translation	BLEU	Finally, we evaluate translation quality quantitatively in terms of BLEU () and METEOR) and report statistical significance for the metrics using approximate randomisation computed with).
translation	METEOR	Finally, we evaluate translation quality quantitatively in terms of BLEU () and METEOR) and report statistical significance for the metrics using approximate randomisation computed with).
sentiment classification task	Precision	 Table 2: This table shows results obtained by ini- tializing InferSent encoder with different embed- dings in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard de- viations. Best performing embeddings and corre- sponding metrics are highlighted in boldface We  use α = 0.5 for all of our experiments here.
sentiment classification task	F-score	 Table 2: This table shows results obtained by ini- tializing InferSent encoder with different embed- dings in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard de- viations. Best performing embeddings and corre- sponding metrics are highlighted in boldface We  use α = 0.5 for all of our experiments here.
sentiment classification task	AUC	 Table 2: This table shows results obtained by ini- tializing InferSent encoder with different embed- dings in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard de- viations. Best performing embeddings and corre- sponding metrics are highlighted in boldface We  use α = 0.5 for all of our experiments here.
MTurk DA human evaluation	DA score	 Table 6: MTurk DA human evaluation results for English Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.
MTurk DA human evaluation	Assess	 Table 6: MTurk DA human evaluation results for English Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.
MTurk DA human evaluation	DA score	 Table 7: MTurk DA human evaluation results for French Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.
MTurk DA human evaluation	Assess	 Table 7: MTurk DA human evaluation results for French Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.
MTurk DA human evaluation	DA score	 Table 8: MTurk DA human evaluation results for Spanish Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.
MTurk DA human evaluation	Assess	 Table 8: MTurk DA human evaluation results for Spanish Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.
predicting interventions	F 1	We show that our de-biased classifier improves predicting interventions over the state-of-the-art on courses with sufficient number of interventions by 8.2% in F 1 and 24.4% in recall on average.
predicting interventions	recall	We show that our de-biased classifier improves predicting interventions over the state-of-the-art on courses with sufficient number of interventions by 8.2% in F 1 and 24.4% in recall on average.
identification task	F1	In the identification task, the F1 of run1 and run2 ranked the second and the third respectively.
error detection	recall rate	At the error detection level and error identification level, our system achieves a third recall rate and gets a good F1 value.
error detection	F1	At the error detection level and error identification level, our system achieves a third recall rate and gets a good F1 value.
error identification	recall rate	At the error detection level and error identification level, our system achieves a third recall rate and gets a good F1 value.
error identification	F1	At the error detection level and error identification level, our system achieves a third recall rate and gets a good F1 value.
fine tuning	weight decay	For fine tuning we used the same Adam optimizer with weight decay and learning rate decay as used for BERT pre-training.
fine tuning	learning rate decay	For fine tuning we used the same Adam optimizer with weight decay and learning rate decay as used for BERT pre-training.
path selection (routing)	Random Routing (RR)	Methods for Comparison We conduct experimental comparisons on different methods described in Section 3 for path selection (routing), including Random Routing (RR), Prior Pivoting (PP), Hop Average (HA) and Learning to Route (LTR).
path selection (routing)	Hop Average (HA)	Methods for Comparison We conduct experimental comparisons on different methods described in Section 3 for path selection (routing), including Random Routing (RR), Prior Pivoting (PP), Hop Average (HA) and Learning to Route (LTR).
BiLSTM tagger	accuracy	 Table 4: Impact of constraints on BiLSTM tagger.  Each score represents the average accuracy on test set  of 3 random runs. The columns of +CRF, +C 1:5 , and  +CRF,C 1:5 are on top of the BiLSTM baseline. For  C 1:4 , ρ = 4 for all percentages. For C 5 , ρ = 16.
Classification	accuracy	 Table 1: Classification accuracy on latent spaces.
WSC	Var	 Table 3: acd for WSC testset. Var ∈ (0.0 − 0.01). CH  = changed word, ST = stable word, DIFF = difference  between ACD for change and stable in percent.
WSC	DIFF	 Table 3: acd for WSC testset. Var ∈ (0.0 − 0.01). CH  = changed word, ST = stable word, DIFF = difference  between ACD for change and stable in percent.
CoNLL-2012 corefer-ence resolution task	F1	Our approach, which also employs BERT embeddings, results in new state-of-the-art results on the CoNLL-2012 corefer-ence resolution task, improving average F1 by 3.6%.
coreference resolution	BERT	It is challenging to apply BERT to the coreference resolution setting because BERT is limited to a fixed sequence length which is shorter than most coreference resolution documents.
coreference resolution	recall-the	But we also pointed out that the mention detectors used for coreference resolution systems are optimised to achieve extremely high recall-the assumption being that the extra mentions will be filtered during coreference resolution proper-and that this optimisation may not be optimal when using an automatic mention detector for annotation-in our case, treating it as an agent from which the other players will derive feedback.
headline generation	ROUGE	It was also applied for optimizing the neural summarization model for headline generation with respect to ROUGE (, which is based on an overlap of words with reference summaries.
sumeval	ROUGE	Therefore, we used sumeval 4 with the MeCab on the ROUGE evaluation of the Mainichi dataset.
MRT	ROUGE	Although ROUGE scores of PG w/ LE trained with MRT showed better ROUGE scores than GOLC, %overs are higher than those of GOLC.
Label Distribution Learning	Match	 Table 2: Experimental results of Label Distribution Learning and Single Label Learning models in three evaluation  settings, Match m , TopK, and MAX. F represents F1-score.
Label Distribution Learning	MAX	 Table 2: Experimental results of Label Distribution Learning and Single Label Learning models in three evaluation  settings, Match m , TopK, and MAX. F represents F1-score.
Label Distribution Learning	F1-score	 Table 2: Experimental results of Label Distribution Learning and Single Label Learning models in three evaluation  settings, Match m , TopK, and MAX. F represents F1-score.
machine translation	accuracy	As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality ().
statistical machine translation (SMT)	accuracy	As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality ().
SMT	accuracy	As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality ().
Translation	BLEU	Translation quality is measured using detokenized, cased BLEU ().
relation extraction	precision	For relation extraction, we use the embedding to augment PCNN+ATT () and improve the precision for top 1000 predictions from 83.9% to 89.8%.
knowledge base completion	HITS	For knowledge base completion, we replace the neural network in () with our pre-trained embedding followed by a simple projection layer, and gain improvements on both MRR and HITS@10 measures.
Relation extraction manual evaluation	Precision	 Table 1: Relation extraction manual evaluation results:  Precision of top 1000 predictions.
KG completion	AMIE	Specifically, for KG completion, we first employ AMIE+ () on each KG to induce rules, then transfer them between KGs towards consistent completion.
rule mining	max number	In  For each dataset, we employ AMIE+ for rule mining by setting the max number of premise asp = 2 and PCA confidence not less than 0.8.
rule mining	PCA confidence	In  For each dataset, we employ AMIE+ for rule mining by setting the max number of premise asp = 2 and PCA confidence not less than 0.8.
entity alignment	learning rate	The optimal configuration of MuGNN for entity alignment is: learning rate= 0.001, L2= 0.01, dropout = 0.2.
sentiment analysis	Evaluation	 Table 2: Experimental results on sentiment analysis on IMDb and Stanford Sentiment Treebank (SST) data sets.  Evaluation measure is accuracy.
sentiment analysis	accuracy	 Table 2: Experimental results on sentiment analysis on IMDb and Stanford Sentiment Treebank (SST) data sets.  Evaluation measure is accuracy.
Morphological inflection	exactness	Morphological inflection is a character-level task with mostly monotonic alignments, but the evaluation demands exactness: the predicted sequence must match the gold standard.
Machine translation	BLEU	 Table 2: Machine translation comparison of softmax, sparsemax, and the proposed 1.5-entmax as both attention  mapping and loss function. Reported is tokenized test BLEU averaged across three runs (higher is better).
Morphological inflection	accuracy (ACC)	Morphological inflection is evaluated byword accuracy (ACC) and average edit distance (MLD) (.
Morphological inflection	average edit distance (MLD)	Morphological inflection is evaluated byword accuracy (ACC) and average edit distance (MLD) (.
MTL	TARS	First, for MTL, we propose Task-Adaptive Representation learning using Soft-coding, namely TARS, wherein shared and private features are both mixtures of features.
Task-Adaptive Representation learning	TARS	First, for MTL, we propose Task-Adaptive Representation learning using Soft-coding, namely TARS, wherein shared and private features are both mixtures of features.
ASP	accuracy	For example, ASP with three-source tasks achieves 82.23% and 66.92% accuracy, respectively, in SNLI and MNLI, which are lower than 82.28% and 67.39% accuracy with its best performance with two-source tasks.
ASP	accuracy	For example, ASP with three-source tasks achieves 82.23% and 66.92% accuracy, respectively, in SNLI and MNLI, which are lower than 82.28% and 67.39% accuracy with its best performance with two-source tasks.
CASE	BERT	Lastly, we show that CASE with multilingual BERT model achieves the state-of-the-art, and even significantly outperforms the supervised approach of enjoying an unfair advantage of extremely large amounts of parallel sentences.
CASEs	defense rate	As shown in the, CASEs are able to achieve higher defense rate (or lower success rate) in performance of 36.6%, while baselines obtained 15.7% and 21.4% respectively, which demonstrates incorporating pseudoadversarial example is indeed helpful to the robustness of the model.
SST-5	BERT	 Table 2: SST-5 and SST-2 performance on all and root  nodes respectively. Model results in the first section are  from the Stanford Treebank paper (2013). GenSen and  BERT BASE results are from (Subramanian et al., 2018)  and (Devlin et al., 2018) respectively.
RL	F -measures	Experiment results on five real-world datasets show that our RL approach consistently improves the performance of the state-of-the-art models in terms of F -measures.
MRC	BiDAF	Consequently, many pioneering MRC models have been proposed, such as BiDAF (, R-NET (, and QANet ().
model optimization	learning rate	For model optimization, we apply the Adam () optimizer with a learning rate of 0.0005 and a minibatch size of 32.
RC	FEVER	Although designed for RC, it also achieves a state-of-the-art evidence extraction score on FEVER, which is a recognizing textual entailment task on a large textual database.
recognizing textual entailment (RTE) task	FEVER	Moreover, we find that the recognizing textual entailment (RTE) task on a large textual database, FEVER, can be regarded as an explainable multi-hop QA task.
NSMN document retriever	prediction of A T	Because C is large, we used the NSMN document retriever  We evaluated the prediction of A T and the evidence E by using the official metrics in FEVER.
NSMN document retriever	FEVER	Because C is large, we used the NSMN document retriever  We evaluated the prediction of A T and the evidence E by using the official metrics in FEVER.
evidence extraction	precision	 Table 11: Performance of evidence extraction. The top  five rows are evaluated on the test set. The comparison  of our models is on the development set. The models  submitted after the Shared Task have no information  about precision or recall.
evidence extraction	recall	 Table 11: Performance of evidence extraction. The top  five rows are evaluated on the test set. The comparison  of our models is on the development set. The models  submitted after the Shared Task have no information  about precision or recall.
constituent parsing	F1 score	For constituent parsing, we use the standard evalb 8 tool to evaluate the F1 score.
sentiment analysis	recall	Finally, we demonstrate that a deeper understanding of hash-tag semantics obtained through segmentation is useful for downstream applications such as sentiment analysis, for which we achieved a 2.6% increase in average recall on the Se-mEval 2017 sentiment analysis dataset.
hashtag segmentation	accuracy	While our hashtag segmentation model is achieving a very high accuracy@2, to be practically useful, it remains a challenge to get the top one predication exactly correct.
automatic hashtag segmentation  (MSE)	A	 Table 6: Evaluation of automatic hashtag segmentation  (MSE) with different features on the STAN large dev set.  A denotes accuracy@1. While Kneser-Ney features  perform well on single-token hashtags, GT+Ling fea- tures perform better on multi-token hashtags.
automatic hashtag segmentation  (MSE)	accuracy	 Table 6: Evaluation of automatic hashtag segmentation  (MSE) with different features on the STAN large dev set.  A denotes accuracy@1. While Kneser-Ney features  perform well on single-token hashtags, GT+Ling fea- tures perform better on multi-token hashtags.
segmentation-level	accuracy	 Table 8. With a segmentation-level  accuracy of 94.6% and average token-level F 1  score of 95.6%, our approach performs favorably  on 2019 hashtags.
counter-argument generation	CANDELA	In this paper, we study the specific problem of counter-argument generation, and present a novel framework, CANDELA.
summarization	METEOR	Due to their computational efficiency, metrics based on word-matching are common, such as ROUGE) for summarization,) for machine translation, and METEOR (Banerjee and) or CIDER () for image captioning.
machine translation	METEOR	Due to their computational efficiency, metrics based on word-matching are common, such as ROUGE) for summarization,) for machine translation, and METEOR (Banerjee and) or CIDER () for image captioning.
summarization task	ROUGE-L	As in the summarization task, SMS outperforms both ROUGE-L and WMS.
translation	BLEU	 Table 2: Evaluation of translation performance on NIST Zh-En dataset. RNN and Lattice-RNN results are from  (Su et al., 2017). We highlight the highest BLEU score in bold for each set. ↑ indicates statistically significant  difference (p <0.01) from best baseline.
Name normalization	accuracy	 Table 3: Name normalization accuracy on disease (Di)  and chemical (Ch) datasets. The last row group in- cludes the results of supervised models that utilize  training annotations in each specific dataset. XM de- notes the use of 'exact match' rule to assign the corre- sponding concept to a mention if the mention is found  in the training data.  † indicates the results reported  by Wright et al. (2019).
sentiment labeling	accuracy	By augmenting scarce human-labeled code-switched text with plentiful synthetic code-switched text, we achieve significant improvements in sentiment labeling accuracy (1.5%, 5.11%, 7.20%) for three different language pairs (English-Hindi, English-Spanish and English-Bengali).
sentiment detection	accuracy	Extensive experiments show that augmenting scarce natural labeled code-switched text with plentiful synthetic text associated with 'borrowed' source labels enriches the feature space, enhances its coverage, and improves sentiment detection accuracy, compared to using only natural text.
machine translation	accuracy	Our experimental results reveal that: (Q1) SAN indeed underperforms the architectures with recurrence modeling (i.e. RNN and DiSAN) on the WRD task, while this conclusion does not hold in machine translation: SAN trained with the translation objective outperforms both RNN and DiSAN on detection accuracy; (Q2) Learning objectives matter more than model architectures in downstream tasks such as machine translation; and (Q3) Position encoding is good enough for SAN in machine translation, while DiSAN is a more universally-effective mechanism to learn word order information for SAN.
Classification	accuracy	 Table 4: Classification accuracy on the NLPCC dataset.
HiCE	Spearman correlation	For example, HiCE achieves 9.3% relative improvement in terms of Spearman correlation compared to the state-of-the-art approach, ` a la carte, regarding 6-shot learning case.
MT-DNN	BERT	To this end, we extend the MT-DNN model originally proposed in by incorporating BERT as its shared text encoding layers.
domain adaptation	BERT	The representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations.
MT-DNN	GLUE	We evaluate the proposed MT-DNN on three popular NLU benchmarks: GLUE (), SNLI (Bowman et al., 2015b), and SciTail ().
MT-DNN	BERT	We compare MT-DNN with existing state-of-the-art models including BERT and demonstrate the effectiveness of MTL with and without model fine-tuning using GLUE and domain adaptation using both SNLI and SciTail.
MT-DNN	GLUE	We compare MT-DNN with existing state-of-the-art models including BERT and demonstrate the effectiveness of MTL with and without model fine-tuning using GLUE and domain adaptation using both SNLI and SciTail.
MT-DNN	BERT LARGE	 Table 2: GLUE test set results scored using the GLUE evaluation server. The number below each task denotes the  number of training examples. The state-of-the-art results are in bold, and the results on par with or pass human  performance are in bold. MT-DNN uses BERT LARGE to initialize its shared layers. All the results are obtained  from https://gluebenchmark.com/leaderboard on February 25, 2019. Model references: 1 :(Wang et al., 2018) ;  2 :(Radford et al., 2018); 3 : (Phang et al., 2018); 4 :(Devlin et al., 2018).
AMR parsing	accuracy	Evaluating AMR parsing accuracy involves comparing pairs of AMR graphs.
sarcasm detection	F-score	Our initial results show that the use of multimodal information can reduce the relative error rate of sarcasm detection by up to 12.9% in F-score when compared to the use of individual modalities.
Summarization Evaluation Metrics	Appropriate	Studying Summarization Evaluation Metrics in the Appropriate Scoring Range
classification	accuracy	We perform a pair-wise test for the classification accuracy (Acc) ().
classification	Acc)	We perform a pair-wise test for the classification accuracy (Acc) ().
verb prediction	BERT	Both Multitask and ELMo models dominate verb prediction results, while BERT lags in this category.
representation learning	BERT	Such a model can also take advantage of recent advances in representation learning, such as BERT (, in defining this similarity.
response generation	Utts	gives an example of response generation with various meta-words, where "Act", "Len", "Copy", "Utts", and "Spe" are variables of a meta-word and refer to dialogue act, response length (including punctuation marks), if copy from the message, if made up of multiple utterances, and specificity level () respectively 2 . Advantages of response generation with meta-words are three-folds: (1) the generation model is explainable as the meta-words inform the model, developers, and even end users what responses they will have before the responses are generated; (2) the generation process is controllable.
greeting	HRED	easy questions, like greeting (Example 4), both HRED and CSRR perform well.
tagging	accuracy	In practice, we observe 100% tagging accuracy with the gold dependencies.
SST	Ablation	 Table 3: (a) SST results. Stars indicate latent tree models. (b) MultiNLI results. Stars indicate latent tree models.  (c) Ablation tests on MultiNLI (results on the matched and mismatched development sets).
gradient reversal	accuracy	In doing so, gradient reversal learns features that explicitly compensate for domain mismatch, while still distilling domain specific knowledge that can improve target domain accuracy.
generative	F1 score	We conduct experiments on three human-annotated datasets, and results demonstrate that our proposed generative model outperforms all competitor models by a large margin of up to 20% F1 score, achieving state-of-the-art performance on three datasets.
translation	BLEU	 Table 2: Evaluation of translation performance over four language pairs. Rows 1 and 2 show pre-training BLEU  scores. Rows 3-13 show scores after fine tuning. Statistically significantly best scores are highlighted (p < 0.05).
TCS	BLEU	Experiments show that TCS brings significant gains of up to 2 BLEU on three of four languages we test, with minimal training overhead 1 .
translation	accuracy	Multilingual NMT has led to impressive gains in translation accuracy of low-resource languages (LRL) ().
STANCE	mean average precision	Of the two cases in which STANCE is outperformed by other methods in terms of mean average precision, one is by a variant of STANCE in an ablation study.
MR extraction	YELPNLG MR	We examine the quality of the MR extraction with a qualitative study evaluating YELPNLG MR to NL pairs on various dimensions.
sentiment error	sentiment error	In the case of sentiment error, the largest error is 0.75 (out of 3), with the smallest sentiment error (0.56) achieved by the +STYLE model.
paraphrase generation	BLEU	 Table 2: Performance of our paraphrase generation models on text simplification (complex → simple) in Newsela  dataset and formality transfer (informal → formal) in GYAFC dataset. For both RNN and SAN models, our method  consistently improves BLEU and SARI scores across styles or domains. In addition, a consistent improvement on  Add and Del means that our method promotes active rewriting.
formality transfer	BLEU	 Table 3: Comparison with previous models on text simplification in Newsela dataset and formality transfer in  GYAFC dataset. Our models achieved the best BLEU scores across styles and domains.
segmentation	accuracy	We evaluated our models with bits-percharacter (bpc) and segmentation accuracy.
Teller utterances	vocabulary size	On the other hand, Teller utterances have a median length of 16 tokens and a vocabulary size of 4,555.
image captioning	BLEU-4	 Table 1: Overall performance of different models for image captioning, where B-4, R, M and C are short for  BLEU-4, ROUGE, METEOR and CIDEr-D scores, respectively. Numbers in bold denote the best performance in  each column.
image captioning	ROUGE	 Table 1: Overall performance of different models for image captioning, where B-4, R, M and C are short for  BLEU-4, ROUGE, METEOR and CIDEr-D scores, respectively. Numbers in bold denote the best performance in  each column.
image captioning	METEOR	 Table 1: Overall performance of different models for image captioning, where B-4, R, M and C are short for  BLEU-4, ROUGE, METEOR and CIDEr-D scores, respectively. Numbers in bold denote the best performance in  each column.
NER	EL	This paper introduces two main contributions: • A system that jointly performs NER and EL, with competitive results in both tasks.
labeling	accuracy	Extensive experiments on five corpora show that extra embed-dings help obtain a significant improvement in labeling accuracy.
machine translation task	BLEU	• We test the scheduled sampling with transformers in a machine translation task on two language pairs and achieve results close to a teacher forcing baseline (with a slight improvement of up to 1 BLEU point for some models).
hyperparameter tuning	BLEU score	We did hyperparameter tuning by trying several different values for dropout and warmup steps, and choosing the best BLEU score on the validation set for the baseline model.
text generation toolkit	D í µí±¥ í µí	We introduce Texar, a general-purpose text generation toolkit aiming to support popular and emerging applications in the field, by providing researchers and practitioners a unified and D í µí±¥ í µí±¦  flexible framework for building their models.
parsing	accuracy	The parsing accuracy varies a great deal for different meaning representations.
Node identification	MRS	 Table 2: Node identification and WSD results on MRS  in terms of noun (n), verb (v), quantifier (q), preposi- tion (p), adjective (a), conjunction (c), and others (x),  and on AMR in terms of predicate (pred). Both are  measured on the test set in terms of accuracy based on  SMATCH.
Node identification	accuracy	 Table 2: Node identification and WSD results on MRS  in terms of noun (n), verb (v), quantifier (q), preposi- tion (p), adjective (a), conjunction (c), and others (x),  and on AMR in terms of predicate (pred). Both are  measured on the test set in terms of accuracy based on  SMATCH.
WSD	MRS	 Table 2: Node identification and WSD results on MRS  in terms of noun (n), verb (v), quantifier (q), preposi- tion (p), adjective (a), conjunction (c), and others (x),  and on AMR in terms of predicate (pred). Both are  measured on the test set in terms of accuracy based on  SMATCH.
WSD	accuracy	 Table 2: Node identification and WSD results on MRS  in terms of noun (n), verb (v), quantifier (q), preposi- tion (p), adjective (a), conjunction (c), and others (x),  and on AMR in terms of predicate (pred). Both are  measured on the test set in terms of accuracy based on  SMATCH.
NER	Relaxed	For NER, two types of evaluations were carried out: • Relaxed: An entity mentioned in a given document is considered to be extracted correctly if the system response includes at least one annotation of a named mention of this entity (regardless of whether the extracted mention is in base form); • Strict: The system response should include exactly one annotation for each unique form of a named mention of an entity in a given document, i.e., identifying all variants of an entity is required.
Recognition task	F1	 Table 3: Official results on the Recognition task of BSNLP19, measured as F1 with Strict evaluation. The training  languages used are: Bulgarian (BG), Czech (CS), Polish (PL), Russian (RU), English (EN, CoNLL2003) and  the BSNLP17 languages (Croatian, Slovak, Slovene and Ukrainian). The top section of the table shows single- source experiments, in which each model is trained on a single language. The bottom section shows multi-source  experiments. The rightmost column, ALL, is a micro-average of the test results over the 4 test languages.
Recognition task	ALL	 Table 3: Official results on the Recognition task of BSNLP19, measured as F1 with Strict evaluation. The training  languages used are: Bulgarian (BG), Czech (CS), Polish (PL), Russian (RU), English (EN, CoNLL2003) and  the BSNLP17 languages (Croatian, Slovak, Slovene and Ukrainian). The top section of the table shows single- source experiments, in which each model is trained on a single language. The bottom section shows multi-source  experiments. The rightmost column, ALL, is a micro-average of the test results over the 4 test languages.
classification	accuracy	With a relatively minor adjustment , however, we show how these same techniques can be used to simultaneously reduce bias and maintain high classification accuracy .
Gendered Pronoun Resolution	Fujisawa	The shared task Gendered Pronoun Resolution aims to classify the pronoun resolution in the sentences, hereby to find the true name referred by a given pronoun, such as she in: In May, Fujisawa joined Mari Motohashi's rink as the team's skip, moving back from Karuizawa to Kitami where she had spent her junior days.
classify the pronoun resolution	Fujisawa	The shared task Gendered Pronoun Resolution aims to classify the pronoun resolution in the sentences, hereby to find the true name referred by a given pronoun, such as she in: In May, Fujisawa joined Mari Motohashi's rink as the team's skip, moving back from Karuizawa to Kitami where she had spent her junior days.
resolvers	Masculine	 Table 4: Comparison to off-the-shelf resolvers, split  by Masculine and Feminine (Bias shows F/M), and  Overall. Bold indicates the best performance.
resolvers	Bias	 Table 4: Comparison to off-the-shelf resolvers, split  by Masculine and Feminine (Bias shows F/M), and  Overall. Bold indicates the best performance.
resolvers	F/M	 Table 4: Comparison to off-the-shelf resolvers, split  by Masculine and Feminine (Bias shows F/M), and  Overall. Bold indicates the best performance.
Resolving Gendered Ambiguous Pronouns	BERT	Resolving Gendered Ambiguous Pronouns with BERT
morphological tagging	F1 score	The performance of morphological tagging is measured by the F1 score calculated over the predicted and actual individual morphological tags.
SIG-MORPHON shared tasks	accuracy	The low-resource settings in earlier SIG-MORPHON shared tasks often resulted in much worse accuracy compared to the high-resource settings.
translation	accuracy	In particular , we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks.
cross-lingual transfer	Ar	 Table 2: Average precision (AP) of POSTLE models  in cross-lingual transfer. Results are shown for both  POSTLE models (DFFN and ADV), two target languages  (Spanish and French) and three methods for inducing  bilingual vector spaces: Ar (Artetxe et al., 2018), Co  (Conneau et al., 2018), and Sm (Smith et al., 2017).
SMDs	FAIR	 Table 6. As in  the case of simulated models, SMDs allowed us  to reach the same general conclusion: L1 model  is the least fair and FAIR(ER) model is proba- bly the most fair of the three models. Yet in  this case SMDs also obscure the fact that both  BASE and FAIR(ER) model might be over-scoring  GER speakers: in fact the SMDs for the two mod- els have the opposite sign.
SMDs	BASE	 Table 6. As in  the case of simulated models, SMDs allowed us  to reach the same general conclusion: L1 model  is the least fair and FAIR(ER) model is proba- bly the most fair of the three models. Yet in  this case SMDs also obscure the fact that both  BASE and FAIR(ER) model might be over-scoring  GER speakers: in fact the SMDs for the two mod- els have the opposite sign.
SMDs	FAIR	 Table 6. As in  the case of simulated models, SMDs allowed us  to reach the same general conclusion: L1 model  is the least fair and FAIR(ER) model is proba- bly the most fair of the three models. Yet in  this case SMDs also obscure the fact that both  BASE and FAIR(ER) model might be over-scoring  GER speakers: in fact the SMDs for the two mod- els have the opposite sign.
SMDs	BASE	 Table 6. As in  the case of simulated models, SMDs allowed us  to reach the same general conclusion: L1 model  is the least fair and FAIR(ER) model is proba- bly the most fair of the three models. Yet in  this case SMDs also obscure the fact that both  BASE and FAIR(ER) model might be over-scoring  GER speakers: in fact the SMDs for the two mod- els have the opposite sign.
SMDs	FAIR	 Table 6. As in  the case of simulated models, SMDs allowed us  to reach the same general conclusion: L1 model  is the least fair and FAIR(ER) model is proba- bly the most fair of the three models. Yet in  this case SMDs also obscure the fact that both  BASE and FAIR(ER) model might be over-scoring  GER speakers: in fact the SMDs for the two mod- els have the opposite sign.
model prediction	er- ror	 Table 7: GLMM summary for model prediction er- ror on Formative-K12-SAS for the models without  spelling correction.
GEC	precision	In particular , GEC with sentence-level grammatical error detection is a novel and versatile approach, and we experimentally demonstrate that it significantly improves the precision of the base model.
grammatical error detection (GED)	BERT	They proposed a grammatical error detection (GED) model based on BERT that achieved state-of-the-art results in word-level GED tasks.
classification	accuracy	We report classification accuracy on 10-fold cross-validation experiments.
writing development	accuracy	The article is structured as follows: We first give a brief overview of research on writing development in terms of complexity and accuracy.
segmentations	edit distance	The segmentations obtained by BPE and SR were also relatively similar with an average edit distance of 5.03.
domain adaptation	A-distance	To assess the success of domain adaptation, we use the proxy A-distance as a matrix.
MADAR travel domain dialect identification	accuracy	We participate in Task 1: MADAR travel domain dialect identification, and we ranked 1st in the task with accuracy of 67.3%.
tagging	accuracy	 Table 1: Attributes and tagging accuracy by lan- guage (Irish and Thai do not have both dev and test  sets).  † Affixation: S/s is strongly/weakly suffixing;  P/p is strongly/weakly prefixing; = is equally prefix- ing/suffixing; ∅ is little affixation.  ‡ Morphological syn- thesis: agglutinative, fusional, introflexive, isolating.
tagging	Affixation	 Table 1: Attributes and tagging accuracy by lan- guage (Irish and Thai do not have both dev and test  sets).  † Affixation: S/s is strongly/weakly suffixing;  P/p is strongly/weakly prefixing; = is equally prefix- ing/suffixing; ∅ is little affixation.  ‡ Morphological syn- thesis: agglutinative, fusional, introflexive, isolating.
sentiment classification	accuracy	The first criteria is the sentiment classification accuracy of the adversarial examples which is predicted by human, and the second is the similarity between the adversarial examples and the original sentence.
classification	accuracy	In addition to classification accuracy, we also care about how similar the generated adversaries examples are to the original unaltered sentences.
coreference resolution	BERT	We perform a similar analysis for coreference resolution, also finding a BERT head that performs quite well.
dependency parsing	BERT	The classifier achieves 77 UAS at dependency parsing, showing BERT's attention captures a substantial amount about syntax.
dependency parsing	BERT	 Table 3: Results of attention-based probing tasks on  dependency parsing. A simple model taking BERT at- tention maps and GloVe word embeddings as input per- forms quite well at dependency parsing. *Not directly  comparable to our numbers; see text.
RQE	accuracy	The evaluation metric used for RQE is accuracy.
Coverage of Validation	RQE  threshold	 Table 3: Coverage of Validation set based on RQE  threshold for Task 3.
RQE task	accuracy	To demonstrate the usefulness of parallel datasets for the RQE task and for easy comparison with the results on the leaderboard, we measure the test accuracy for different dataset combinations using the test dataset labels released by the task organizers post completion of the shared task.
Question Answering	Bi-directional	Further, a closed-domain Question Answering technique that uses Bi-directional LSTMs trained on the SquAD dataset to determine relevant ranks of answers fora given question is also discussed.
classification task	accuracy	The KNN algorithm was also used for this classification task and an accuracy of 62.4% was obtained with K=47.
translation	accuracy	Our experiments show that learning to translate with the XML tags improves translation accuracy, and the beam search accurately generates XML structures.
en2de translation task	BLEU	 Table 2: BLEU scores of the models trained for the  en2de translation task. The boldfaced ensembled  model was submitted as the primary submission; the  best performing model with boldfaced BLEU scores  was not ready at submission time.
WMT19 News Translation Task	DA score z-score	 Table 8: Preliminary results of WMT19 News Translation Task. Systems ordered by DA score z-score
MT evaluation	MEANT	Semantic MT evaluation metrics, such as ME-TEOR) and MEANT, require additional linguistic resources to more accurately evaluate the meaning similarity between the MT output and the reference translation.
Biomedical Translation UCAM Biomedical Translation	AFRL	Transformer-based Automatic Post-Editing Model with Joint Encoder and Multi-source Attention of De- coder Terminology-Aware Segmentation and Domain Feature for the WMT19 Biomedical Translation Task Exploring Transfer Learning and Domain Data Selection for the Biomedical Translation UCAM Biomedical Translation at WMT19: Transfer Learning Multi-domain Ensembles BSC Participation in the WMT Translation of Biomedical Abstracts The MLLP-UPV Spanish-Portuguese and Portuguese-Spanish Machine Translation Systems for WMT19 Similar Language Translation Task The TALP-UPC System for the WMT Similar Language Task: Statistical vs Neural Machine Translation Machine Translation from an Intercomprehension Perspective Utilizing Monolingual Data in NMT for Similar Languages: Submission to Similar Language Translation Task Neural Machine Translation: Hindi-Nepali Neural Machine Translation of Low-Resource and Similar Languages with Backtranslation Dual Monolingual Cross-Entropy Delta Filtering of Noisy Parallel Data Low-Resource Corpus Filtering Using Multilingual Sentence Embeddings Quality and Coverage: The AFRL Submission to the WMT19 Parallel Corpus Filtering for Low-Resource Conditions Task
WMT Translation of Biomedical Abstracts	AFRL	Transformer-based Automatic Post-Editing Model with Joint Encoder and Multi-source Attention of De- coder Terminology-Aware Segmentation and Domain Feature for the WMT19 Biomedical Translation Task Exploring Transfer Learning and Domain Data Selection for the Biomedical Translation UCAM Biomedical Translation at WMT19: Transfer Learning Multi-domain Ensembles BSC Participation in the WMT Translation of Biomedical Abstracts The MLLP-UPV Spanish-Portuguese and Portuguese-Spanish Machine Translation Systems for WMT19 Similar Language Translation Task The TALP-UPC System for the WMT Similar Language Task: Statistical vs Neural Machine Translation Machine Translation from an Intercomprehension Perspective Utilizing Monolingual Data in NMT for Similar Languages: Submission to Similar Language Translation Task Neural Machine Translation: Hindi-Nepali Neural Machine Translation of Low-Resource and Similar Languages with Backtranslation Dual Monolingual Cross-Entropy Delta Filtering of Noisy Parallel Data Low-Resource Corpus Filtering Using Multilingual Sentence Embeddings Quality and Coverage: The AFRL Submission to the WMT19 Parallel Corpus Filtering for Low-Resource Conditions Task
black-box neural machine translation	BLEU	This model improves over the raw black-box neural machine translation system by 0.9 and 1.0 absolute BLEU points on the WMT 2019 APE development and test set.
MT	BLEU	Participants were ranked based on the performance of these MT systems on a test set of Wikipedia translations, as measured by BLEU ().
Sentence pair classification	accuracy	 Table 3: Sentence pair classification accuracy of XLM model on dev sets. Confounders are sentences that we draw  at random to create inadequate translations.
information retrieval	precision	Examples include information retrieval and question answering, where time-related information could increase the precision of query disambiguation and document retrieval (e.g., by returning documents with newly created senses or filtering out documents with obsolete senses).
question answering	precision	Examples include information retrieval and question answering, where time-related information could increase the precision of query disambiguation and document retrieval (e.g., by returning documents with newly created senses or filtering out documents with obsolete senses).
sense detection task	pairwise Pearson correlation)	Inter-annotator agreement on the novel sense detection task was 0.51 (pairwise Pearson correlation) and can be regarded as an upper bound on model performance.
GEC	MaxMatch (M 2 )	Three evaluation metrics have been proposed for GEC: MaxMatch (M 2 ) (Dahlmeier and Ng, 2012), I-measure, and GLEU ().
GEC	GLEU	Three evaluation metrics have been proposed for GEC: MaxMatch (M 2 ) (Dahlmeier and Ng, 2012), I-measure, and GLEU ().
neural network classification	accuracy	Recent work on the application of neural network classification to drive greedy transition-based dependency parsing has achieved high accuracy, showing * Both authors contributed equally to this paper.
PI	focus	For PI, focus should be removed from "today" to correctly recognize < s 0 , s + 1 > as paraphrases and < s 0 , s − 1 > as non-paraphrases.
TE	TE	For TE, we need to focus on "full of people" (to recognize TE for < s 0 , s + 1 >) and on "outdoors" / "indoors" (to recognize non-TE for < s 0 , s − 1 >).
SICK	ORIG	 Table 5: SICK data: Converting the original sentences  (ORIG) into the NONOVER format
topic modeling	likelihood	Despite their frequent use in topic modeling, we find that stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability.
SMT	BLEU score	As a reference, previous work showed that oracle rescoring of the 1000-best sequences generated by the SMT model can achieve the BLEU score of about).
word similarity task	accuracy	Interestingly, CCAPrior+RF embeddings also often perform better than eigenword vectors (Eigen) of when retrofitted using the method of . For example, in the word similarity task, eigenwords retrofitted with WordNet get an accuracy of 62.2 whereas encoding prior knowledge using both CCA and retrofitting gets a maximum accuracy of 63.3.
word similarity task	accuracy	Interestingly, CCAPrior+RF embeddings also often perform better than eigenword vectors (Eigen) of when retrofitted using the method of . For example, in the word similarity task, eigenwords retrofitted with WordNet get an accuracy of 62.2 whereas encoding prior knowledge using both CCA and retrofitting gets a maximum accuracy of 63.3.
NP bracketing	FN	 Table 1: Results for the word similarity datasets, geographic analogies and NP bracketing. The first upper blocks  (A-C) present the results with retrofitting. NPK stands for no prior knowledge (no retrofitting is used), WN for  WordNet, PD for PPDB and FN for FrameNet. Glove, Skip-Gram, Global Context, Multilingual and Eigen are the  word embeddings of Pennington et al. (2014), Mikolov et al. (2013b), Huang et al. (2012), Faruqui and Dyer
parsing	accuracy	The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality, but these models may sacrifice accuracy on source languages with a large treebank.
Dependency parsing	MALOPA	 Table 3: Dependency parsing: labeled attachment scores (LAS) for monolingually-trained parsers and  MALOPA in the fully supervised scenario where L t = L s . Note that we use the universal dependencies  verson 1.2 which only includes annotations for ∼13,000 English sentences, which explains the relatively  low scores in English. When we instead use the universal dependency treebanks version 2.0 which includes  annotations for ∼40,000 English sentences (originally from the English Penn Treebank), we achieve UAS  score 93.0 and LAS score 91.5.
Dependency parsing	UAS  score 93.0	 Table 3: Dependency parsing: labeled attachment scores (LAS) for monolingually-trained parsers and  MALOPA in the fully supervised scenario where L t = L s . Note that we use the universal dependencies  verson 1.2 which only includes annotations for ∼13,000 English sentences, which explains the relatively  low scores in English. When we instead use the universal dependency treebanks version 2.0 which includes  annotations for ∼40,000 English sentences (originally from the English Penn Treebank), we achieve UAS  score 93.0 and LAS score 91.5.
Dependency parsing	LAS score 91.5	 Table 3: Dependency parsing: labeled attachment scores (LAS) for monolingually-trained parsers and  MALOPA in the fully supervised scenario where L t = L s . Note that we use the universal dependencies  verson 1.2 which only includes annotations for ∼13,000 English sentences, which explains the relatively  low scores in English. When we instead use the universal dependency treebanks version 2.0 which includes  annotations for ∼40,000 English sentences (originally from the English Penn Treebank), we achieve UAS  score 93.0 and LAS score 91.5.
predicting language ID and POS tags	MALOPA	 Table 5: Effect of automatically predicting language ID and POS tags with MALOPA on LAS scores.
Tagging	accuracy	 Table 4: Tagging accuracy on the 10 dev languages, and  UAS of the selected source parser with these noisy target- language tag sequences. The results are formatted as in
Tagging	UAS	 Table 4: Tagging accuracy on the 10 dev languages, and  UAS of the selected source parser with these noisy target- language tag sequences. The results are formatted as in
grammaticality judgment task	error rate	Surprisingly, a similar experiment using the grammaticality judgment task led to a slight increase in error rate.
taxonomy construction	Precision	 Table 2: Experimental results for taxonomy construction.  P stands for Precision, R for Recall, and F for F-measure.
taxonomy construction	Recall	 Table 2: Experimental results for taxonomy construction.  P stands for Precision, R for Recall, and F for F-measure.
parsing	labelled precision (LP )	We use EVALB to evaluate parsing performance, including labelled precision (LP ), labelled recall (LR), and bracketing F 1 .
parsing	recall (LR)	We use EVALB to evaluate parsing performance, including labelled precision (LP ), labelled recall (LR), and bracketing F 1 .
SMT	BLEU score	Using an encoder-decoder framework that incorporates gating and attention techniques, it has been reported that the performance of NMT can surpass the performance of traditional SMT as measured by BLEU score (.
translation	BLEU score	 Table 2: Evaluation of translation quality measured by case-insensitive BLEU score. "GroundHog  (vanilla)" and "GroundHog (GRU )" denote attention-based NMT (
Cross-sentence extraction	accuracy	Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy.
connective discovery	precision	For connective discovery, we report precision, recall, and F 1 for connectives, requiring connectives to match exactly.
connective discovery	recall	For connective discovery, we report precision, recall, and F 1 for connectives, requiring connectives to match exactly.
connective discovery	F 1	For connective discovery, we report precision, recall, and F 1 for connectives, requiring connectives to match exactly.
connective discovery	F 1	Even our end-to-end baseline offers moderate performance, but Causeway-L outperforms it at connective discovery by over 14 F 1 points, and Causeway-S outperforms it by 18 points.
sentiment classification	accuracies	Results on a standard sentiment classification benchmark and a question type classification benchmark show that our tree LSTM structure gives significantly better accuracies compared with the method of . We achieve the best reported results for sentiment classification.
MT	BAD quality labels	Words in the MT output that need to be edited are marked by the BAD quality labels.
CASI canonicalizes	Precision	 Table 6: CASI canonicalizes five times more schemas than AMIE*, and also achieves a small (9%) increase in preci- sion, demonstrating how additional knowledge resources can help the canonicalization process (Section 4.2). Precision  estimates are within +/-4% with 95% confidence interval.
POS tagging	accuracy	The proposed model has favorable generalization properties as it retains over 89.8% of its average POS tagging accuracy when trained at 1.2% of the total available training data, i.e. 150 sentences per language.
POS tagging	accuracy	illustrates the average POS tagging accuracy over the 12 CoNLL-X datasets for different amounts of training data and models.
SemEval challenge	F1 score	We report the same evaluation metric as the SemEval challenge: the average F1 score of positive and negative classes.
morphological family cluster- ing	P	 Table 6: Results for morphological family cluster- ing. P = precision, R = recall.
morphological family cluster- ing	precision	 Table 6: Results for morphological family cluster- ing. P = precision, R = recall.
morphological family cluster- ing	R	 Table 6: Results for morphological family cluster- ing. P = precision, R = recall.
morphological family cluster- ing	recall	 Table 6: Results for morphological family cluster- ing. P = precision, R = recall.
dependency parsing	UAS	By converting to Stanford dependencies, our final model achieves the state-ofthe-art results on dependency parsing by obtaining a 96.2% UAS and a 95.2% LAS.
dependency parsing	LAS	By converting to Stanford dependencies, our final model achieves the state-ofthe-art results on dependency parsing by obtaining a 96.2% UAS and a 95.2% LAS.
replicability	FS	Benefits of this style of evaluation include replicability, the diversity of FS, and the ability to calculate a true F-score.
replicability	F-score	Benefits of this style of evaluation include replicability, the diversity of FS, and the ability to calculate a true F-score.
FS identification	Countrank	 Table 2: Results of FS identification in various test sets: Countrank = ranking with frequency; PMIrank = PMI-based  ranking; minLPRrank = ranking with minLPR; LPRseg = the method of Brooke et al.
FS identification	LPRseg	 Table 2: Results of FS identification in various test sets: Countrank = ranking with frequency; PMIrank = PMI-based  ranking; minLPRrank = ranking with minLPR; LPRseg = the method of Brooke et al.
segmentation	accuracy	We use three metrics to evaluate segmentation accuracy.
Segmentation	accuracy	(i) Segmentation accuracy measures whether every single canonical morpheme in the returned sequence is correct.
popularity prediction	F1	Since the rare high karma comments are of greatest interest in popularity prediction, proposes a task of predicting quantized karma (using a nonlinear head-tail break rule for binning) with evaluation using a macro average of the F1 scores for predicting whether a comment exceeds each different level.
Maintaining	PAST	3. Maintaining what parts in the source have been translated (PAST) and what parts have not (FUTURE).
Maintaining	FUTURE	3. Maintaining what parts in the source have been translated (PAST) and what parts have not (FUTURE).
neural machine translation	FUTURE	In this paper, we propose a novel neural machine translation system that explicitly models PAST and FUTURE contents with two additional RNN layers.
translation	BLEU	We measure the translation quality with BLEU scores ().
validation	accuracy	It achieved across validation accuracy of 77.19%, which is competitive with the state of the art accuracy of 78% achieved with the same level of supervision.
validation	accuracy	It achieved across validation accuracy of 77.19%, which is competitive with the state of the art accuracy of 78% achieved with the same level of supervision.
grammar acquisition from transcribed child-directed speech and newswire text	accuracy	Results for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy.
grammar acquisition from transcribed child-directed speech and newswire text	accuracy	Results for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy.
Translation	BLEU	Translation performances are reported in case-sensitive BLEU ().
translation	BLEU	We report translation quality using tokenized 1 BLEU comparable with existing Neural Machine Translation papers.
document sentiment classification	accuracy	On several benchmark datasets for document sentiment classification, our end-to-end pipelined approach which is overall unsupervised (ex-cept fora tiny set of seed words) outper-forms existing unsupervised approaches and achieves an accuracy comparable to that of fully supervised approaches.
SVM	accuracy	This improvement is statistically significant (p .01 using two-tailed Wilcoxon signed-rank test) for SVM with all metrics except accuracy, for BiLSTM with AUC only, and for GPPL medi. with Pearson correlation only.
SVM	AUC	This improvement is statistically significant (p .01 using two-tailed Wilcoxon signed-rank test) for SVM with all metrics except accuracy, for BiLSTM with AUC only, and for GPPL medi. with Pearson correlation only.
SVM	Pearson correlation	This improvement is statistically significant (p .01 using two-tailed Wilcoxon signed-rank test) for SVM with all metrics except accuracy, for BiLSTM with AUC only, and for GPPL medi. with Pearson correlation only.
classification	accuracy	Optimizing the length-scale using MLII improves classification accuracy by 1% over the median heuristic, and significantly improves accuracy (p = .043) and AUC (p = .013) over the previous stateof-the-art, SVM ling.
classification	accuracy	Optimizing the length-scale using MLII improves classification accuracy by 1% over the median heuristic, and significantly improves accuracy (p = .043) and AUC (p = .013) over the previous stateof-the-art, SVM ling.
classification	AUC	Optimizing the length-scale using MLII improves classification accuracy by 1% over the median heuristic, and significantly improves accuracy (p = .043) and AUC (p = .013) over the previous stateof-the-art, SVM ling.
uncertainty sampling	accuracy	It has previously been shown) that uncertainty sampling sometimes causes accuracy to decrease.
segmentation	accuracy	The experimental results indicate that segmentation accuracy is positively related to word boundary markers and negatively to the number of unique non-segmental terms.
segmentation	accuracy	3. Our segmentation system achieves state-of-theart accuracy on the UD datasets and improves on previous work especially for the most challenging languages.
generative model text classification	accuracy	Specifically, results are reported for contextconditioned perplexity and generative model text classification accuracy, using contexts that capture a range of phenomena and dimensionalities.
sentiment classification	BWE	 Table 1: ADAN performance for Chinese (5-cls) and Arabic (3-cls) sentiment classification without using labeled  TARGET data. All systems but the CLD ones use BWE to map SOURCE and TARGET words into the same space.  CLD-based CLTC represents cross-lingual text classification methods based on cross-lingual distillation (
Spammer analysis	MACE	 Table 8: Spammer analysis example. D&S provides a  confusion matrix; MACE shows the spamming prefer- ence and the credibility.
WSD	V -score	 Table 3: WSD results from three SemEval 2010 systems and our six systems, in terms of V -score, F 1 score, and  their average. C = the average number of clusters. The adaptive k-means using definitions outperforms the others  on the average of V and F 1 , when considering both nouns and verbs, or nouns only. The SemEval systems are UoY  (Korkontzelos and Manandhar, 2010); KCDC-GD (Kern et al., 2010); and Duluth-Mix-Gap (Pedersen, 2010).
WSD	F 1 score	 Table 3: WSD results from three SemEval 2010 systems and our six systems, in terms of V -score, F 1 score, and  their average. C = the average number of clusters. The adaptive k-means using definitions outperforms the others  on the average of V and F 1 , when considering both nouns and verbs, or nouns only. The SemEval systems are UoY  (Korkontzelos and Manandhar, 2010); KCDC-GD (Kern et al., 2010); and Duluth-Mix-Gap (Pedersen, 2010).
parsing	accuracy	We show experimentally across multiple languages: (1) Features computed from the unparsed corpus improve parsing accuracy.
parsing	accuracy	• Model error: Likelihood does not correlate well with parsing accuracy anyway (Smith, 2006,.2).
grammar correction	F	Because precision is more important than recall in grammar correction, it is weighed twice as high, and is denoted as F 0.5 . Other metrics have been proposed recently).
Head assignment	REDUCE-RIGHT	Head assignment is typically performed with REDUCE-RIGHT and REDUCE-LEFT actions.
Head assignment	REDUCE-LEFT	Head assignment is typically performed with REDUCE-RIGHT and REDUCE-LEFT actions.
Question answering	Accuracy	 Table 2: Question answering results. Accuracy aver- aged over the 20 bAbI tasks. Using tanh is worse than  ReLU (line 13 vs. 15). RUM 150 λ = 0 without an  update gate drops by 1.7% compared with line 13.
Question answering	ReLU	 Table 2: Question answering results. Accuracy aver- aged over the 20 bAbI tasks. Using tanh is worse than  ReLU (line 13 vs. 15). RUM 150 λ = 0 without an  update gate drops by 1.7% compared with line 13.
Question answering	RUM 150 λ	 Table 2: Question answering results. Accuracy aver- aged over the 20 bAbI tasks. Using tanh is worse than  ReLU (line 13 vs. 15). RUM 150 λ = 0 without an  update gate drops by 1.7% compared with line 13.
early stopping	accuracy	We do early stopping using the accuracy of the development set.
AMR-to-text generation task	BLEU	For the AMR-to-text generation task, our model surpasses the current state-ofthe-art neural models trained on LDC2015E86 and LDC2017T10 by 2 and 4.3 BLEU points, respectively.
sentiment analysis of tweets	BREAKING	It is worth mentioning that sentiment analysis of tweets presents additional challenges to natural language processing, because of the small amount of text (less than 140 characters in each document), usage of creative spelling (e.g. "happpyyy", "some1 yg bner2 tulus"), abbreviations (such as "wth" or "lol"), informal constructions ("hahahaha yava quiet so !ma I m bored av even home nw") and hashtags (BREAKING: US GDP growth is back!
classification	accuracy	Our results indicate an increase in classification accuracy with the addition of emoticon retention and word stemming, while token saliency shows mixed performance.
SemEval 2016 Task 4	F1	In the SemEval 2016 Task 4, the OPAL system obtained the following results (: In subtask A, 0.50521 average F1, 0.56020 average Recall and 0.54122 accuracy.
SemEval 2016 Task 4	Recall	In the SemEval 2016 Task 4, the OPAL system obtained the following results (: In subtask A, 0.50521 average F1, 0.56020 average Recall and 0.54122 accuracy.
SemEval 2016 Task 4	accuracy	In the SemEval 2016 Task 4, the OPAL system obtained the following results (: In subtask A, 0.50521 average F1, 0.56020 average Recall and 0.54122 accuracy.
Slot 1	precision	 Table 3: Results for Slot 1 in terms of precision, re- call, and F-Score.
Slot 1	re- call	 Table 3: Results for Slot 1 in terms of precision, re- call, and F-Score.
Slot 1	F-Score	 Table 3: Results for Slot 1 in terms of precision, re- call, and F-Score.
Slot 3	F-Score	 Table 4: Results for Slot 3 in terms of positive, nega- tive, neutral F-Score and accuracy.
Slot 3	accuracy	 Table 4: Results for Slot 3 in terms of positive, nega- tive, neutral F-Score and accuracy.
prediction	accuracy	We design a "vote scheme" for prediction instead of predicting when the accuracy of validation set reaches its maximum.
Detecting Stance in Tweets (DST)	Favor	The task of Detecting Stance in Tweets (DST) in SemEval 2016 aims at classifying the provided tweets into three stance classes, i.e., Favor (directly or indirectly by supporting given target), Against (directly or indirectly by opposing or criticizing given target) and None (none of the above) refer to a given target.
Detecting Stance in Tweets (DST)	None	The task of Detecting Stance in Tweets (DST) in SemEval 2016 aims at classifying the provided tweets into three stance classes, i.e., Favor (directly or indirectly by supporting given target), Against (directly or indirectly by opposing or criticizing given target) and None (none of the above) refer to a given target.
stance detection	F1 score	This effort achieved the top score in Task A on supervised stance detection, producing an average F1 score of 67.8 when assessing whether a tweet author was in favor or against a topic.
sentiment analysis	Latent Dirichlet Allocation (LDA	Another sentiment analysis approach based on Latent Dirichlet Allocation (LDA) is proposed in ().
supersense classification	precision	To isolate the supersense classification performance, we compute precision, recall, and F 1 of the supersense-labeled word tokens.
supersense classification	recall	To isolate the supersense classification performance, we compute precision, recall, and F 1 of the supersense-labeled word tokens.
supersense classification	F 1	To isolate the supersense classification performance, we compute precision, recall, and F 1 of the supersense-labeled word tokens.
SemEval 2012	Explicit Semantic Analysis (ESA)	The best STS system at SemEval 2012) used lexical similarity and Explicit Semantic Analysis (ESA) (.
Information Retrieval (IR)	BLEU	Therefore, STS is widely used in many research areas such as Natural Language Processing, fora large amount of tasks like Information Retrieval (IR), Natural Language Understanding (NLU) or even Machine Translation (MT) evaluation, for which STS allows capturing more information than traditional metrics based on n-grams match like BLEU ().
Machine Translation (MT) evaluation	BLEU	Therefore, STS is widely used in many research areas such as Natural Language Processing, fora large amount of tasks like Information Retrieval (IR), Natural Language Understanding (NLU) or even Machine Translation (MT) evaluation, for which STS allows capturing more information than traditional metrics based on n-grams match like BLEU ().
SemEval	Pearson's correlation	 Table 2: Official results from SemEval, giving Pearson's correlation on each dataset
MT evaluation	TER	We believe that this neural network is interesting for the cQA problem because: (i) it works in a pairwise fashion, i.e., given two translation hypotheses and a reference translation to compare to, the network decides which translation hypothesis is better; this is appropriate fora ranking problem; (ii) it allows for an easy incorporation of rich syntactic and semantic embedded representations of the input texts, and it efficiently models complex non-linear relationships among them; (iii) it uses a number of MT evaluation measures that have not been explored for the cQA task (e.g., TER, Meteor and BLEU).
MT evaluation	BLEU	We believe that this neural network is interesting for the cQA problem because: (i) it works in a pairwise fashion, i.e., given two translation hypotheses and a reference translation to compare to, the network decides which translation hypothesis is better; this is appropriate fora ranking problem; (ii) it allows for an easy incorporation of rich syntactic and semantic embedded representations of the input texts, and it efficiently models complex non-linear relationships among them; (iii) it uses a number of MT evaluation measures that have not been explored for the cQA task (e.g., TER, Meteor and BLEU).
SST of unambigu-ous nouns	accuracy	first trained and tested a discriminative model for SST of unambigu-ous nouns on data extracted from different versions of WordNet and achieved an accuracy of slightly over 52%.
MWE	detection	The supersense rankings improve MWE detection (+0.49) as well as SST (+0.62).
MWE	SST	The supersense rankings improve MWE detection (+0.49) as well as SST (+0.62).
SemEval-2016 Task 8 on Meaning Representation Parsing	accuracy-increasing	Available at http://c60.ailab.lv For SemEval-2016 Task 8 on Meaning Representation Parsing we took a dual approach: besides developing our own neural AMR parser, we also extended the AMR smatch scoring tool ) with a rule-based C6.0 classifier 1 to guide development of an accuracy-increasing wrapper for the state-of-art AMR parser CAMR (.
AMR parsing	F-score	Despite the usefulness of preposition semantic role labeling information for AMR parsing, it does not have an impact to the parsing F-score of CAMR, but reduces the parsing recall by 1%.
AMR parsing	recall	Despite the usefulness of preposition semantic role labeling information for AMR parsing, it does not have an impact to the parsing F-score of CAMR, but reduces the parsing recall by 1%.
parsing	F-score	Despite the usefulness of preposition semantic role labeling information for AMR parsing, it does not have an impact to the parsing F-score of CAMR, but reduces the parsing recall by 1%.
parsing	recall	Despite the usefulness of preposition semantic role labeling information for AMR parsing, it does not have an impact to the parsing F-score of CAMR, but reduces the parsing recall by 1%.
parsing	recall	Despite the usefulness of preposition semantic role labeling information for AMR parsing, it does not have an impact to the parsing F-score of CAMR, but reduces the parsing recall by 1%.
AMR parsing	BROWN	 Table 2: AMR parsing performance on the official SemEval development set (LDC2015E86).VERB: ISI  verbalization list. BROWN: Brown cluster features.RNE: Rich (OntoNotes) named entities.SRL: semantic  role labeling features. WIKI: Addition of wikification of named entities in AMR.
SemEval 2016 Task 9 Chinese Semantic Dependency Parsing shared task	TEXT	In the SemEval 2016 Task 9 Chinese Semantic Dependency Parsing shared task, semantic dependency parsing for two text genres, TEXT, which includes sentences from conversations and primary school textbooks, and NEWS, which contains newswire text, is explored.
identification of narrative container relations	precision	The top result of our system was in phase2 for the identification of narrative container relations where it obtained the maximum score of precision (0.823) from all participants.
Classification	accuracy	 Table 5: Classification accuracy for LVF readings, with com- plete feature set and selected in feature selection process.
MT	BLEU TER BLEU TER	 Table 1: Comparison of translation quality for three methods used to train Moses for Chinese-English  MT under small corpus IWSLT 2007 conditions  cased  uncased  System  BLEU TER BLEU TER  Giza++ based induction  19.23 63.94 19.83 63.40  ITG based induction  20.05 63.19 20.42 62.61  XMEANT outside probabilities based 27.59 59.48 28.54 58.81
MT	XMEANT	 Table 1: Comparison of translation quality for three methods used to train Moses for Chinese-English  MT under small corpus IWSLT 2007 conditions  cased  uncased  System  BLEU TER BLEU TER  Giza++ based induction  19.23 63.94 19.83 63.40  ITG based induction  20.05 63.19 20.42 62.61  XMEANT outside probabilities based 27.59 59.48 28.54 58.81
stance detection	F-score	(3) We propose a stance detection system that is much simpler than the SemEval-2016 Task #6 winning system (described above), and yet obtains an even better F-score of 70.32 on the shared task's test set.
Stance Classification	F-scores	 Table 7: Stance Classification: F-scores obtained for each of the targets (the columns) when one or more  of the feature groups are added. Highest scores in each column is shown in bold.
classification	accuracy	shows the results in terms of classification accuracy for 10-fold cross-validation on the training data.
negation marking	distance	 Table 7: Comparing the novel negation marking feature with the distance feature proposed by Nguyen  et al. (2017).
SPR	ATTEMPT	SPR replaces categorical roles ATTEMPT She said Bill would try the same tactic again. with judgements about multiple underlying properties about what is likely true of the entity filling the role.
classification	accuracy	Evaluation is through classification accuracy.
classification	confidence score	We use Support Vector Machine (SVM) for classification, and its confidence score for ranking.
MT	similarity	In MT run, we translated all the other languages in the test dataset into English by Google Translate and used the English model to evaluate all similarity scores.
machine translation	accuracy	In addition to the machine translation accuracy challenge, the difficulty of this track lies in providing longer sentences with less informative surface overlap between the sentences compared to other tracks.
paraphrase detection	IR system rank	We reviewed existing performant paraphrase detection methods and selected several to implement and ensemble (; along with the related question IR system rank provided in the dataset.
paraphrase detection	mean average precision (MAP)	As paraphrase detection is a classification problem while subtask B is a ranking problem, we also incorporated pairwisepreference learning to aid in improving the key metric of mean average precision (MAP).
pun disambiguation	accuracy	The general conclusion is that pun disambiguation results are poorer when compared with traditional WSD and traditional WSD must be extended with pun-specific features to increase accuracy in this area.
WSD	secondbest scoring sense	suggest a straightforward extension of the WSD approach to pun interpretation: choose the best scoring sense and secondbest scoring sense for the word in its context.
pun interpretation	secondbest scoring sense	suggest a straightforward extension of the WSD approach to pun interpretation: choose the best scoring sense and secondbest scoring sense for the word in its context.
Veracity	Accuracy	 Table 4: Veracity: Accuracy on Development Set
SemEval-2017 Task 4	POSITIVE	Ordinal Classification As last year, SemEval-2017 Task 4 includes sentiment analysis on a fivepoint scale {HIGHLYPOSITIVE, POSITIVE, NEU-TRAL, NEGATIVE, HIGHLYNEGATIVE}, which is inline with product ratings occurring in the corporate world, e.g., Amazon, TripAdvisor, and Yelp.
Main parsing	standard deviation	 Table 2: Main parsing results: For Smatch, a mean of ten runs with ten restarts per run is shown;  standard deviation was about 0.0003 per system. For the remaining ablations, a single run was used.
Smatch	standard deviation	 Table 2: Main parsing results: For Smatch, a mean of ten runs with ten restarts per run is shown;  standard deviation was about 0.0003 per system. For the remaining ablations, a single run was used.
sentiment analysis of tweets	BREAKING	It is worth mentioning that sentiment analysis of tweets presents additional challenges to natural language processing, because of the small amount of text (less than 140 characters in each document), usage of creative spelling (e.g. "happpyyy", "some1 yg bner2 tulus"), abbreviations (such as "wth" or "lol"), informal constructions ("hahahaha yava quiet so !ma I m bored av even home nw") and hashtags (BREAKING: US GDP growth is back!
sentiment analyzer	F1	The proposed sentiment analyzer obtained an average F1 of 55.2%, an average recall of 58.9% and an accuracy of 61.4%.
sentiment analyzer	recall	The proposed sentiment analyzer obtained an average F1 of 55.2%, an average recall of 58.9% and an accuracy of 61.4%.
sentiment analyzer	accuracy	The proposed sentiment analyzer obtained an average F1 of 55.2%, an average recall of 58.9% and an accuracy of 61.4%.
submission	LR	Therefore, the system configuration for submission is all features and LR algorithm.
SemEval-2017 Task	accuracy	The final result for the SemEval-2017 Task 5 by cosine similarity are 35.66% and 38.28%, and by evaluation method 2 4 are 55.34% and 56.68%, as the test of 4-fold validation that ensemble result got the best accuracy.
Logistic regression	accuracy	Random forest and Logistic regression had the highest accuracy of 83.3% and 85.3% respectively.
CAMR	accuracy	According to, the accuracy of CAMR depends greatly on the accuracy of input dependency parser.
CAMR	terminal	Another weakness of CAMR is the terminal condition.
AMR parsing	accuracies	Neural encoder-decoders have previously been proposed for AMR parsing, but reported accuracies are well below the state-of-the-art (, even with sophisticated pre-processing and categorization (.
AMR linearization	accuracy	Our second contribution is to introduce more structure in the AMR linearization by distinguishing between lexical and non-lexical concepts, noting that lexical concepts (excluding sense labels) can be predicted with high accuracy from their lemmas.
parsing	accuracy	Experiments show that our extensions increase parsing accuracy by a large margin over a standard attention-based model.
AMR parsing	Smatch F1	As for AMR parsing, we added NER extensions to our SemEval-2016 general-domain AMR parser to handle the biomedical genre, rich inorganic compound names, achieving Smatch F1=54.0%.
identification	BANNER	For the task of identification, we use the BANNER tool, a named entity recognition system, which is based on conditional random fields (CRF) and has obtained successful results in the biomedi-cal domain.
classifying keyphrases from scientific articles	BANNER	Our approach for identifying and classifying keyphrases from scientific articles combines the use of the BANNER tool () and the UMLS semantic network . The paper is organized as follows.
span extraction	Top score	 Table 2: Evaluation results for span extraction.   †Top score.  ‡Second highest score.
relation extraction	Top score	 Table 4: Evaluation results for relation extraction.   †Top score.
sentiment classification	F 1	The results revealed that, while sentiment classification performance on regular tweets reached up to F 1 = 0.71, scores on the ironic tweets varied between F 1 = 0.29 and F 1 = 0.57.
sentiment classification	F 1 = 0.29	The results revealed that, while sentiment classification performance on regular tweets reached up to F 1 = 0.71, scores on the ironic tweets varied between F 1 = 0.29 and F 1 = 0.57.
sentiment classification	F 1	The results revealed that, while sentiment classification performance on regular tweets reached up to F 1 = 0.71, scores on the ironic tweets varied between F 1 = 0.29 and F 1 = 0.57.
ALL	F1-scores	gives detailed evaluation for ALL, showing the F1-scores for the top-12 most frequently appeared secondary characters and Others that appear only in the training or the evaluation set but not both.
parsing	F1	Though 40 participants registered for the task, only one team submitted output, achieving 0.55 F1 in Track 1 (parsing) and 0.70 F1 in Track 2 (intervals).
information extraction	precision	The first follows a more traditional information extraction evaluation: measure the precision and recall of finding and linking the various time entities.
information extraction	recall	The first follows a more traditional information extraction evaluation: measure the precision and recall of finding and linking the various time entities.
prediction	TL	Moreover, combining systems enhances greatly the prediction without TL, but decreases the score with TL : the combination of independent systems compensates a small lack of data, but becomes useless with enough training.
prediction	TL	Moreover, combining systems enhances greatly the prediction without TL, but decreases the score with TL : the combination of independent systems compensates a small lack of data, but becomes useless with enough training.
opinion mining	ArSenL	Building on our previous work on opinion mining which involved development of sentiment lexicons (ArSenL (), opinion mining models () and applications (, and building on our analysis and characterization for Twitter Data (,c), we participate in: Affect in Arabic Tweets.
Transfer Learning	FR	As for the neural models, RD stands for random initialization, TL for Transfer Learning, FR for Frozen pretrained layers (without fine-tuning) and FT for Fine-Tuning.
Transfer Learning	FT	As for the neural models, RD stands for random initialization, TL for Transfer Learning, FR for Frozen pretrained layers (without fine-tuning) and FT for Fine-Tuning.
sentiment analysis ordinal classification task	max depth	For emotion intensity ordinal classification task (Task 2), sentiment analysis ordinal classification task (Task 4) and emotion classification task (Task 5) Doc2Vec size was varied from 10 to 1000 with an increment of 10 in each iteration, n estimator of Random Forest Tree was varied from 10 to 150 with an increment of 10 in each iteration and max depth of Random Forest Tree was varied from 2 to 20 with an increment of 1 in each iteration.
emotion classification task	max depth	For emotion intensity ordinal classification task (Task 2), sentiment analysis ordinal classification task (Task 4) and emotion classification task (Task 5) Doc2Vec size was varied from 10 to 1000 with an increment of 10 in each iteration, n estimator of Random Forest Tree was varied from 10 to 150 with an increment of 10 in each iteration and max depth of Random Forest Tree was varied from 2 to 20 with an increment of 1 in each iteration.
classification tasks	accuracy	Variables used to estimate the ideal parameters for classification tasks was accuracy of the Random Forest Tree algorithm.
Feature representation	term frequency-inverse frequency (TFIDF) matrix	In more specific, Feature representation is done based on the term-document matrix (TDM) and term frequency-inverse frequency (TFIDF) matrix.
Emoji Prediction	MAP	In sub-task 1: Emoji Prediction in English, our primary submission obtain a MAP of 16.45, Precision of 31.557, Recall of 16.771 and Accuracy of 30.992.
Emoji Prediction	Precision	In sub-task 1: Emoji Prediction in English, our primary submission obtain a MAP of 16.45, Precision of 31.557, Recall of 16.771 and Accuracy of 30.992.
Emoji Prediction	Recall	In sub-task 1: Emoji Prediction in English, our primary submission obtain a MAP of 16.45, Precision of 31.557, Recall of 16.771 and Accuracy of 30.992.
Emoji Prediction	Accuracy	In sub-task 1: Emoji Prediction in English, our primary submission obtain a MAP of 16.45, Precision of 31.557, Recall of 16.771 and Accuracy of 30.992.
classification	accuracy	One can use these statistics in order to create more features and test them to seethe changes in classification accuracy.
LDR classification	Accuracy	 Table 3: LDR classification results of Task A and  B using Accuracy and Macro average F-score.
irony detection	F 1-score	We create a targeted feature set and analyse how different features are useful in the task of irony detection , achieving an F 1-score of 0.5914.
SubTask 1	F1 score	 Table 6: SubTask 1 results sorted by F1 score, the highest score in each column from the baselines and the  participants outputs are marked in bold.
detecting semantic difference	F1 measure	Our system is demonstrated to be effective in detecting semantic difference and is ranked 1 st in the competition in terms of F1 measure.
relation extraction	LIGHTREL	The main motivation for our participation at) was the ideal opportunity to test and improve our relation extraction system LIGHTREL.
SemEval-2010 task	stopping tolerance	For the SemEval-2010 task, we obtained the best results using LibLinear's support vector classifier by Crammer and Singer) with a cost of 0.1 and a stopping tolerance of 0.3.
relation extraction	F1	Furthermore , for combined relation extraction and classification on clean data, it obtained F1 scores of 37.4 and 33.6 for each phase.
SemEval challenge	F1 score	The winning system of an earlier SemEval challenge on relation classification () adopted the first approach and achieved an F1 score of 82.2% (.
relation classification	F1 score	The winning system of an earlier SemEval challenge on relation classification () adopted the first approach and achieved an F1 score of 82.2% (.
relation identification and classification	F 1 scores	For relation identification and classification in subtask 2, it achieved F 1 scores of 33.9% and 17.0%,
SemEval 2018 Task 7	F 1	Section 4 discusses the results of the final evaluation of SemEval 2018 Task 7, where the system achieved 47.4% and 66.0% F 1 scores on the relation classification subtasks 1.1 and 1.2.
SemEval submission	F1 score	 Table 1: Performance on Task 1 (dev-data) for each of the model we implemented. We used the best MLP model  for SemEval submission (test-data) and had F1 score of 18. We found LR and MLP to always label majority class  resulting in zero F1 score using Wiki embeddings.
SemEval submission	F1 score	 Table 1: Performance on Task 1 (dev-data) for each of the model we implemented. We used the best MLP model  for SemEval submission (test-data) and had F1 score of 18. We found LR and MLP to always label majority class  resulting in zero F1 score using Wiki embeddings.
semantic difference detection	F1 score	We propose a method for semantic difference detection that uses an SVM classifier with features based on co-occurrence counts and shallow semantic parsing, achieving 0.63 F1 score in the competition .
semantic parsing	F1 score	We propose a method for semantic difference detection that uses an SVM classifier with features based on co-occurrence counts and shallow semantic parsing, achieving 0.63 F1 score in the competition .
SemEval-2018 Task 10	F1 score	It participated in the SemEval-2018 Task 10 on Capturing Discriminative Attributes , achieving an F1 score of 0.73 and ranking 2nd out of 26 participant systems.
Capturing Discriminative Attributes	F1 score	It participated in the SemEval-2018 Task 10 on Capturing Discriminative Attributes , achieving an F1 score of 0.73 and ranking 2nd out of 26 participant systems.
named entity recognition (NER)	term frequency (TF)	We also combine the neural network model with a variety of manual features, including word exact match features and token features such as part-of-speech (POS) ,named entity recognition (NER) and term frequency (TF).
argument reasoning comprehension task	validity	More details about dataset can be found in the work of All the data of the argument reasoning comprehension task is provided in the GitHub by task organizers 2 . For the availability and validity, after complex manual processing, only 1955 instances are selected from 11k comments.
CP decomposition	accuracy	We observe that prioritising features that are common to both training and test instances as cores during the CP decomposition to further improve the accuracy of text classification.
text classification	accuracy	Moreover, the consideration of coreness during the CP-decomposition improves the text classification accuracy.
MAP	association measure	 Table 1: MAP scores on three categories. The first four  rows use various techniques with Word2Vec. The next  four demonstrate Category Builder built on the same  corpus, to show the effect of ρ and association measure  used. For all four Category Builder rows, we used n =  100. Both increasing ρ and switching to APPMI can be  seen to be individually and jointly beneficial.  † The last  line reports the score on a different corpus, the release  data, with APPMI and ρ = 3, n = 100.
sentence translation retrieval	accuracy	As discussed in this can be measured with sentence translation retrieval: the accuracy of retrieving the correct translation for each source sentence from the target side of a test parallel corpus.
sentence translation retrieval	accuracy	Note also that the FastText model achieved relatively high performance even when it was aligned with only 1K parallel sentences, a condition in which sentence translation retrieval accuracy was less that 40%.
machine translation	ARG1-of	Among other applications it has been used in machine translation (), text (a / asbestos :polarity -:time (n / now) :location (t / thing :ARG1-of (p / produce-01 :ARG0 (w / we)))) Figure 1: Humanly produced AMR for: There is no asbestos in our products now.
AMR  parsing	accuracy	 Table 5: Results (sentence averages) of different AMR  parsing (bottom part) and ranking (top part) systems on  two test sets. Upper part: results when selecting from  alternative parses: lower-bound (upper-bound): oracle  selecting the worst (best) AMR parse; ours: results  when selecting the best parse according to our models'  accuracy prediction (hierarchical model).
MT task	BERT-base	 Table 3: Main results, system properties and requirements of SPRL systems. Overall best system is marked in bold,  best system using GloVe is underlined, best single-model system is marked by  †. STL: supervised transfer-learning  (e.g., RUD'18: pre-training on MT task). C-embeddings: contextual word embeddings (BERT-base
Contextual Emotion Detection (HRLCE)	BERT	We propose an ensemble approach composed of two deep learning models, the Hierarchical LSTMs for Contextual Emotion Detection (HRLCE) model and the BERT model.
SemEval-2019 Task 6	BNU-HBKU	 Table 5: All the teams that participated in SemEval-2019 Task 6 with their ranks for each sub-task. The symbol '-'  indicates that the team did not participate in some of the subtasks. Please, refer to Table 4 to see the scores based  on a team's rank. The top team for each task is in bold, and the second-place team is underlined. Note: ASE -CSE  stands for Amrita School of Engineering -CSE, and BNU-HBKU stands for BNU-HKBU UIC NLP Team 2.
parsing	accuracy	The cascaded BiLSTM parser significantly enhances the parsing accuracy on all tasks.
classification	accuracy	Then using a gated recurrent neural network for classification, they achieved an accuracy of 95.68%, superior to the best previously published results by.
transfer learning	BERT	We experiment with transfer learning using pre-trained language models (ULMFiT, OpenAI GPT, and BERT) and fine-tune them on this task.
utterance vector encoding	microaveraged F1 score (F1)	We compare the MVTT model with some variants focusing on utterance vector encoding and utterance vector combination methods with microaveraged F1 score (F1) which is the main evaluation metric.
sentiment analysis	BERT	In recent years, pre-trained word and sentence representations achieved very competitive performance in many NLP tasks, e.g., fine-tuned word embeddings using distant training and tweet sentence representations DeepMoji () on sentiment analysis, and contextualized word representations BERT (Devlin et al., 2018) on 11 NLP tasks.
emotion classification in textual conversation	F1µ score	For the specific task of emotion classification in textual conversation, achieved a F1µ score of 0.7134 on the same dataset, using an architecture based on LSTM.
sentiment analysis	Bi-LSTM	For sentiment analysis, other successful approaches also used Bi-LSTM () as well as transfer learning.
TokyoTech NLP contextual emotion  detection	F1 score	 Table 2: Micro F1 score and F1 scores of happy, sad, and angry obtained from TokyoTech NLP contextual emotion  detection system with three different settings, and SemEval-2019 Task 3 baseline's micro F1 score.
text classification	term frequency (TF)	Therefore, our chosen statistic feature for the text classification was the term frequency (TF) taking into account unigrams and bigrams because it provided the best perfomance.
hate speech  detection)	Precision	 Table 1: Evaluation measures (in percentage) of all models for development and testing on task A (hate speech  detection). Precision and recall are averaged over the two classes.
hate speech  detection)	recall	 Table 1: Evaluation measures (in percentage) of all models for development and testing on task A (hate speech  detection). Precision and recall are averaged over the two classes.
Subtask B	exact match	Evaluation for Subtask B was based on two criteria -partial match and exact match.
stacking	BiLSTM	The three best models that performed best on individual sub tasks are stacking of CNN-Bi-LSTM with Attention, BiLSTM with POS information added with word features and Bi-LSTM for third task.
Offensive Tweet Classification	BERT	Nikolov-Radivchev at SemEval-2019 Task 6: Offensive Tweet Classification with BERT and Ensembles
SemEval-2019 task	Bidirectional GRU	Our approach for the SemEval-2019 task 6 (identifying and categorizing offensive language in social media) comprises of deep learning models: Bidirectional LSTM, Bidirectional GRU and standard LSTM.
prediction	accuracy	The mix of colloquial slang in tweets, veiled references, missing data, usage of symbols and emojis are further hurdles that lowered the prediction accuracy ().
Identifying Offensive Tweets	BERT	UM-IU@LING at SemEval-2019 Task 6: Identifying Offensive Tweets Using BERT and SVMs
SubTask	A	 Table 5: SubTask A result on actual test data
feature extraction	F1-scores	We try combinations of many techniques in pre-processing, feature extraction and classification while tuning their hyper-parameters to find the best models with the leading F1-scores.
rumour detection	False	Twitter 15 and 16 datasets () contain claim propagation trees and combine tasks of rumour detection and verification in one four-way classification task (Non-rumour, True, False, Unverified).
rumor verification task	F1 measure	Our system is ranked 1 st place in the rumor verification task by both the macro F1 measure and the RMSE measure.
SemEval-2019 task 8	BERT	In order to address the SemEval-2019 task 8, we propose a system based on the BERT model).
Detection	DA	 Table 1: Experiment results of Detection. Abbreviations: DA, Data Augment; PPE, Pubmed Pre-trained Embed- ding; PP, Post Process; TP, Table Process; LF, Linguistic Features; ME, Model Ensemble
Detection	ME	 Table 1: Experiment results of Detection. Abbreviations: DA, Data Augment; PPE, Pubmed Pre-trained Embed- ding; PP, Post Process; TP, Table Process; LF, Linguistic Features; ME, Model Ensemble
binary classification task	accuracy score	Our system was ranked seventeenth out of forty two participating teams in the binary classification task with an accuracy score of 0.742 on the blind test set (the accuracy of the top ranked system was 0.822).
binary classification task	accuracy	Our system was ranked seventeenth out of forty two participating teams in the binary classification task with an accuracy score of 0.742 on the blind test set (the accuracy of the top ranked system was 0.822).
SemEval-2019 Task	BERT	Team Howard Beale at SemEval-2019 Task 4: Hyperpartisan News Detection with BERT
Classification	BERT	 Table 1: Classification results on our portal-wise data  splits with fine-tuned BERT.
SemEval-2019 Task	BERT-Based	Team Peter-Parker at SemEval-2019 Task 4: BERT-Based Method in Hyperpartisan News Detection
bias detection	accuracy	So far, the machine learning approaches to do the bias detection were mostly using the) or the Word Vectors, which can get the accuracy for more than 70%.
stance classification	F1	Our system ranked 6th overall with an F1-score of 36.25 on stance classification and F1 of 22.44 on rumour verification.
classification of questions/answers	veracity	This work doesn't involve classification of questions/answers based on factuality but it determines the veracity of the answer given a particular question.
question classification task	Factual	Subtask A is a question classification task, where the questions types are: • Factual: The question is asking for factual information, which can be answered by checking various information sources, and it is not ambiguous (e.g., "What is Ooredoo customer service number?").
answer classification task	Factual -TRUE	Subtask B is an answer classification task: are the answers to factual questions factual or not, and if they are factual are they true or false: • Factual -TRUE: The answer is True and can be proven with an external resource.
classification	accuracy	In order to improve classification accuracy, we developed a customized list of stopwords, retaining some opinion-and fact-denoting common function words which would have been removed by standard stoplisting.
sequence classification	BERT	Finetuning was applied to sequence classification: the BERT directly takes the final hidden state of the first token, adds a layer of weight, and then softmax predicts the label probability.
classification	accuracy	The impact of filter size on classification accuracy is shown as.
classification	accuracy	 Table 3: The classification accuracy of different class- es.
ML classifiers	accuracy	The paper evaluates the performance of several ML classifiers, as well as how the gazetteers influence the accuracy of the system.
toponym detection	overlap macro  F1	 Table 1: Performance of different toponym detection  methods. SMA, SMI, OMA, and OMI respectively de- note the strict macro F1, strict micro F1, overlap macro  F1 and overlap micro F1.
toponym detection	overlap micro F1	 Table 1: Performance of different toponym detection  methods. SMA, SMI, OMA, and OMI respectively de- note the strict macro F1, strict micro F1, overlap macro  F1 and overlap micro F1.
generalization	precision	As a generalization test, we apply the model trained on political discourse to literary text (the Sherlock Holmes novels and short stories) and obtain an improvement of 17% average precision compared to the baseline.
classification	accuracy	This process is performed by iteratively measuring classification accuracy at values of K ranging from 5 to 100, in multiples of 5, using the training data from the heldout evaluation split.
negation detection	recall (sensitivity)	Our negation detection approach yields a recall (sensitivity) value of 94.6% for the positive cases and an overall accuracy value of 91.9%.
negation detection	accuracy	Our negation detection approach yields a recall (sensitivity) value of 94.6% for the positive cases and an overall accuracy value of 91.9%.
classification	accuracy	This meta-classifier was then extended to a Random Forest of meta-classifiers, yielding further improvements in classification accuracy.
grid search	mean	The scoring metric used for this grid search was mean accuracy.
rumor detection	recall	Evaluating the performance of the proposed technique in rumor detection should rely on both the number of relevant rumors that are selected (recall) and the number of selected rumors that are relevant (precision), since both of them are presented in this work.
rumor detection	precision	Evaluating the performance of the proposed technique in rumor detection should rely on both the number of relevant rumors that are selected (recall) and the number of selected rumors that are relevant (precision), since both of them are presented in this work.
sarcasm detection	Bag-of-Words(BOW)	For sarcasm detection over social media content, researchers so far have counted on Bag-of-Words(BOW), N-grams etc.
sarcasm detection	F-score	The proposed model outperforms state-of-the-art text-based methods for sarcasm detection, yielding an F-score of .92.
promptadherence	Pearson r = 0.243	annotated essays for promptadherence, and found that achieving inter-rater reliability was very challenging, reporting Pearson r = 0.243 between two raters.
predictive	accuracy	It has greatly improved the predictive accuracy by enabling a multi-dimensional characterization of a text's reading level).
ASR	accuracy	We further explore the role of ASR accuracy and the performance and construct coverage of the combined model which includes all three engines.
identification of erroneous sentences	F1-score	The evaluation score works well fora system whose only purpose is the identification of erroneous sentences, so for the binary classification task the F1-score is perfectly suitable.
information retrieval weighting	F1 scores	We show results from standard information retrieval weighting and ranking methods as well as an NLP-inspired approach, achieving F1 scores ranging from 0.63, to 0.83 on science topics.
subgraph selection	BLEU	After integrating the graph segmentation model to help subgraph selection, our system (GSM) achieves significantly better BLEU than DTU on both language pairs.
VPE target detection	precision (Prec)	VPE target detection is a per-word binary classification problem, which can be evaluated using the conventional precision (Prec), recall (Rec) and F1 scores.
VPE target detection	recall (Rec)	VPE target detection is a per-word binary classification problem, which can be evaluated using the conventional precision (Prec), recall (Rec) and F1 scores.
VPE target detection	F1	VPE target detection is a per-word binary classification problem, which can be evaluated using the conventional precision (Prec), recall (Rec) and F1 scores.
coreference resolution	BART	We explain how we extend the coreference resolution toolkit, BART, in order to enable it to process Basque.
Coreference for Basque	BART	However, preliminary work on Coreference for Basque is starting to emerge, and in this paper we describe our work on extending the coreference resolution toolkit, BART) to the Basque language.
coreference resolution toolkit	BART	However, preliminary work on Coreference for Basque is starting to emerge, and in this paper we describe our work on extending the coreference resolution toolkit, BART) to the Basque language.
Salience	Weights	 Table 1: Salience Factors and their Weights
classification	accuracy	The system achieves a classification accuracy in the range of 60-75%, which exceeds by a large margin the non-expert human performance on this task.
Classification	F1	 Table 3: Classification results in terms of F1, precision  (P), and recall (R) based on individual feature types.
Classification	precision  (P)	 Table 3: Classification results in terms of F1, precision  (P), and recall (R) based on individual feature types.
Classification	recall (R)	 Table 3: Classification results in terms of F1, precision  (P), and recall (R) based on individual feature types.
parsing	accuracy	The question of why parsing accuracy decreases with this approach in the case of UD is left open.
constituency parsing	accuracy	It has been done for constituency parsing for example by but also for dependency parsing for example by. modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.
dependency parsing	accuracy	It has been done for constituency parsing for example by but also for dependency parsing for example by. modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.
parsing	accuracy	It has been done for constituency parsing for example by but also for dependency parsing for example by. modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.
test set translation	GLOSSES-DA	The methods involving test set translation (DA-TR and GLOSSES-DA) consistently yield the best cross-lingual accuracies for each model, with a relative error reduction of 45%, 52% and 59% respectively for DA-TR and scores comparable to the full English-Swedish ones.
test set translation	error reduction	The methods involving test set translation (DA-TR and GLOSSES-DA) consistently yield the best cross-lingual accuracies for each model, with a relative error reduction of 45%, 52% and 59% respectively for DA-TR and scores comparable to the full English-Swedish ones.
relation extraction tasks	recall	For proper training and evaluation of relation extraction tasks, the weaknesses of datasets used so far need to be tackled: mainly the size (too small) and the amount of data that is actually labelled (unlabelled data leading to recall problems).
KB completion	RESCAL	The general merit of factorization methods for KB completion has been demonstrated by a variety of models, such as RESCAL (),) and DistMult ().
Normalizing ill-formed texts	accuracy	Normalizing ill-formed texts is a promising preprocessing step to address experienced accuracy drops in existing NLP tools.
evidence weighing	accuracy	Interestingly, evidence weighing does play an important role on the MCTest task as shown in, significantly boosting model accuracy.
MCTest	accuracy	Interestingly, evidence weighing does play an important role on the MCTest task as shown in, significantly boosting model accuracy.
Semantic relation classification	accuracy	 Table 2: Semantic relation classification results on the Tratz and´Oand´ and´O Séaghdha datasets using accuracy  as a classification measure. Results obtained through 10-fold cross-validation on the Tratz dataset and  5-fold CV on thé  O Séaghdha dataset (with the original folds).
paraphrase ranking	MRC	 Table 5: Results on paraphrase ranking. accu- racy@k is the proportion of sentences for which  the true paraphrase received rank at most k. MRC  is the Mean Rank of the Correct paraphrase.
paraphrase ranking	Mean Rank	 Table 5: Results on paraphrase ranking. accu- racy@k is the proportion of sentences for which  the true paraphrase received rank at most k. MRC  is the Mean Rank of the Correct paraphrase.
semantic relatedness (SICK)	PR	 Table 7: Results on semantic relatedness (SICK)  based on cosine distances. PR, SR, SE: Pear- son, Spearman correlation coefficient and mean  squared error, resp. between model scores and hu- man scores.
semantic relatedness (SICK)	SR	 Table 7: Results on semantic relatedness (SICK)  based on cosine distances. PR, SR, SE: Pear- son, Spearman correlation coefficient and mean  squared error, resp. between model scores and hu- man scores.
semantic relatedness (SICK)	mean  squared error	 Table 7: Results on semantic relatedness (SICK)  based on cosine distances. PR, SR, SE: Pear- son, Spearman correlation coefficient and mean  squared error, resp. between model scores and hu- man scores.
alignment	accuracy	While these aligners may seem to be very accurate, a 10% error rate in alignment imposes a serious limitation on the overall AMR parsing accuracy.
AMR parsing	accuracy	While these aligners may seem to be very accurate, a 10% error rate in alignment imposes a serious limitation on the overall AMR parsing accuracy.
Tagging	accuracy	 Table 4: Tagging accuracy by sentence type for manual and automatic annotation. Significance only indicated for  deviations of more than 2% below the mean (with *) or above (with +).
Tagging	Significance	 Table 4: Tagging accuracy by sentence type for manual and automatic annotation. Significance only indicated for  deviations of more than 2% below the mean (with *) or above (with +).
parsing	accuracy	 Table 7: t values from mixed effects models for parsing  accuracy using sentence type, genre and length, with docu- ment random effects.
classification	accuracy	Overall, a Consensus Cost cutoff of 0.75 gives the optimal trade-off between both system variants, yielding 83.2% classification accuracy.
Coverage	Accuracy	 Table 1: Coverage and Accuracy
paraphrase extraction	precision	Similarly to Cocos and Callison-Burch (2016), we incorporate a graph-based clustering approach for word sense discrimination into an existing paraphrase extraction system, (i) to improve the precision of synonym identification and ranking, and (ii) to enlarge the diversity of synonym senses.
MWE tagging	recall	We evaluate the performance of the proposed network on MWE tagging using the three metrics described in , reporting for each of them the recall, precision and F-score.
MWE tagging	precision	We evaluate the performance of the proposed network on MWE tagging using the three metrics described in , reporting for each of them the recall, precision and F-score.
MWE tagging	F-score	We evaluate the performance of the proposed network on MWE tagging using the three metrics described in , reporting for each of them the recall, precision and F-score.
SVM implementation	accuracy	We use libsvm for SVM implementation and ScikitLearn   Since the main aspect of our work is mention pair pruning, we first check the mention pair pruning accuracy.
prediction	accuracy	The result is a sharp decline in prediction accuracy.
prediction	accuracy	Finally, we can see that the prediction accuracy is higher when the information sources are combined, as we would expect.
classification	accuracy	We also explored how short-term memory decay influenced classification accuracy by comparing three classifiers with a memory half-life of 1, 3 and 10 seconds, respectively.
classification	accuracy	For classification, we use k-nearest neighbor (KNN) and Decision Tree, both of which are widely used in time-series classification . We report only accuracy on the classification experiments considering the balanced nature of the data set.
SAX	MINDIST distance measure	To optimize SAX parameters (with MINDIST distance measure), for w, we search from 6 up to 2n/3 (n is the length of the time series); fora, we search each value between 6 and 20.
classification	accuracy	shows how classification accuracy varies depending on wand a.
inflection generation	exact-match	For the simplest task, inflection generation from lemmas, the best system averaged 95.56% exact-match accuracy across all languages, ranging from Maltese (88.99%) to Hungarian (99.30%).
inflection generation	accuracy	For the simplest task, inflection generation from lemmas, the best system averaged 95.56% exact-match accuracy across all languages, ranging from Maltese (88.99%) to Hungarian (99.30%).
morphological segmentation	reporting Precision (P)	 Table 1: Results of morphological segmentation using Morfessor, reporting Precision (P) and Recall (R)
morphological segmentation	Recall (R)	 Table 1: Results of morphological segmentation using Morfessor, reporting Precision (P) and Recall (R)
picture naming task	accuracy	For example, in English, the neighbourhood density of a word was found to correlate positively with reaction times in a picture naming task), and negatively with the speed and accuracy of participants' responses in a lexical decision task).
classifier design	accuracy-efficiency	Supervised topic classification offers a cost-effective and reliable alternative, yet it introduces new challenges , the most significant of which are the training set coding, classifier design, and accuracy-efficiency trade-off.
ATC	validity	First and foremost, ATC does not get around the problem of validity: ATC generally cannot detect nuances in the text as well as a human can, thereby limiting the validity of content analysis results.
POS tagging	occur- rence	 Table 2: Labels annotated for POS tagging along  with the explanation for each label and the occur- rence in percent.
POS tagging	Precision (P)	 Table 5: Performance of the CRF systems for POS tagging compared to the majority baseline (BL1),  the confidence baseline (BL2). CRF base : system with the 13 basic features, CRF predLID : system with  predicted LID as an additional feature, CRF goldLID : system with gold-standard LID as an additional  feature. Precision (P), Recall (R) and F-score (F) per class and macro-average of all classes are given.  The task-relevant results are emphasized in bold.
POS tagging	Recall (R)	 Table 5: Performance of the CRF systems for POS tagging compared to the majority baseline (BL1),  the confidence baseline (BL2). CRF base : system with the 13 basic features, CRF predLID : system with  predicted LID as an additional feature, CRF goldLID : system with gold-standard LID as an additional  feature. Precision (P), Recall (R) and F-score (F) per class and macro-average of all classes are given.  The task-relevant results are emphasized in bold.
POS tagging	F-score (F)	 Table 5: Performance of the CRF systems for POS tagging compared to the majority baseline (BL1),  the confidence baseline (BL2). CRF base : system with the 13 basic features, CRF predLID : system with  predicted LID as an additional feature, CRF goldLID : system with gold-standard LID as an additional  feature. Precision (P), Recall (R) and F-score (F) per class and macro-average of all classes are given.  The task-relevant results are emphasized in bold.
Segmentation	accuracy	 Table 1: Segmentation accuracy in 10-fold cross validation.
WMT	acceptance rate	In total, WMT 2016 featured 13 full paper oral presentations (31% acceptance rate) and 87 shared task poster presentations.
MT	BLEU	The evaluation on the well-formed data is given in F 1 -scores while the MT output is evaluated with BLEU.
MT	BLEU	In, the BLEU scores () of the MT output with predicted verbal inflection are presented.: BLEU scores of MT outputs with corrected verbal inflection.
IWSLT 2013 German→English shared translation task	BOLT	We carryout experiments on two tasks: the IWSLT 2013 German→English shared translation task, 1 and the BOLT Chinese→English task.
translation	BLEU	To evaluate whether the features also increase translation performance, we report casesensitive BLEU scores with mteval-13b.perl on two test sets, newstest2015 and newstest2016.
translation	BLEU	We measure the overall translation quality using 4-gram BLEU (), which is computed on tokenized and lowercased data for all systems.
APE	TER	To assess the quality of APE systems and produce a ranking based on human judgement, as well as analyze how humans perceive TER/BLEU performance differences between the submitted systems, two runs of human evaluations were conducted.
SMT	BLEU points difference	ParFDA obtains results close to the top constrained phrase-based SMT with an average of 2.52 BLEU points difference using significantly less computation for building SMT systems than the computation that would be spent using all available corpora.
SMT	BLEU	Similar bounds show that top constrained SMT results at WMT16 can be improved by 8 BLEU points on average while German to En-glish and Romanian to English translations results are already close to the bounds.
SMT	BLEU	ParFDA Moses SMT results for each translation direction at WMT16 are in using BLEU over cased text, and F 1 (Biçici, 2011).
SMT	F 1	ParFDA Moses SMT results for each translation direction at WMT16 are in using BLEU over cased text, and F 1 (Biçici, 2011).
SMT submission	BLEU	 Table 2: BLEU, BLEU-Cased and Translation Er- ror Rate (TER) scores on newstest2016 of our  phrase-based SMT submission with and without  the use of n-best rescoring. The third line shows  the upper bound of our system with the n-best en- tries scored and sorted against the reference trans- lations using Meteor. The improvement in BLEU  for our n-best rescoring over the baseline MERT  is statistically significant with p ≤ 0.05.
SMT submission	MERT	 Table 2: BLEU, BLEU-Cased and Translation Er- ror Rate (TER) scores on newstest2016 of our  phrase-based SMT submission with and without  the use of n-best rescoring. The third line shows  the upper bound of our system with the n-best en- tries scored and sorted against the reference trans- lations using Meteor. The improvement in BLEU  for our n-best rescoring over the baseline MERT  is statistically significant with p ≤ 0.05.
SMT	alignment	In the case of SMT, linguistic morphs may provide too high granularity compared to the second language, and deteriorate alignment ().
speech recognition	ROVER	For example, in speech recognition, ROVER (Fiscus, 1997) is a system combination approach based on a soft voting scheme.
Translation	BLEU	 Table 5: Translation results on the development  system for English→Czech with unification-based  constraints. Cased BLEU scores are shown. They  are averaged over three tuning runs (note that base- line weights are reused in the experiments with  constraints).
Translation	BLEU	 Table 6: Translation results on the final system  for English→Czech with unification-based con- straints. Cased BLEU scores are shown. Note  that baseline weights are reused in the experiments  with constraints.
SMT	EXT	 Table 1. Each lan- guage pair was translated in both directions.  "BASE" in the tables represents the baseline  SMT system. "EXT" indicates results for the  baseline system, using the baseline settings but  extended with additional permissible data (lim- ited to parallel Europarl v7, Common Crawl,
Translation	accuracy	A postmortem analysis attributes this loss to a logical bug in the menu items detection, which often erroneously included title-cased verbs in the beginning of the sentence, preventing them from being translated as an active part of the sentence.: Translation accuracy on manually evaluated sentences focusing on particular phenomena.
Translation	accuracy	 Table 4: Translation accuracy on manually evaluated sentences focusing on particular phenomena. Test- sets consist of hand-picked source sentences of Batch 2 that include the respective phenomenon. Bold- face indicates best systems on each phenomenon (row) with a 0.95 confidence level.
SMT	word error rate (WER)	first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER).
MT evaluation	BLEU	We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (), METEOR ( and TER ().
MT evaluation	METEOR	We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (), METEOR ( and TER ().
MT evaluation	TER	We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (), METEOR ( and TER ().
WMT	Kendall rank correlation coefficient (τ )	Details on the WMT data for each language pair are given in In our work we focus on sentence-level metrics' performance, which is assessed by converting metrics' scores to ranks and comparing them to the human judgements with Kendall rank correlation coefficient (τ ).
Sentence-level	Kendall rank correlation coef- ficient (τ	 Table 2: Sentence-level evaluation results for WMT15 dataset in terms of Kendall rank correlation coef- ficient (τ )
machine translation	BLEU	The evaluation metric not only provides the most reliable basis for machine translation systems, but also can be applied as the optimizing criterion in the parameter tuning step like BLEU.
translation edit	CharacTER)	In this work, we propose a novel translation edit rate on character level (CharacTER), which achieves a better correlation on the system-level for four different morphologically rich languages compared to BEER and CHRF.
translation edit	BEER	In this work, we propose a novel translation edit rate on character level (CharacTER), which achieves a better correlation on the system-level for four different morphologically rich languages compared to BEER and CHRF.
Machine translation (MT) automatic evaluation	BLEU	Machine translation (MT) automatic evaluation metrics, such as BLEU (), NIST), METEOR (), TER (), MAXSIM ( etc., evaluate the quality of the MT system output by calculating the similarity between the translation output and the human reference.
Machine translation (MT) automatic evaluation	METEOR	Machine translation (MT) automatic evaluation metrics, such as BLEU (), NIST), METEOR (), TER (), MAXSIM ( etc., evaluate the quality of the MT system output by calculating the similarity between the translation output and the human reference.
Machine translation (MT) automatic evaluation	TER	Machine translation (MT) automatic evaluation metrics, such as BLEU (), NIST), METEOR (), TER (), MAXSIM ( etc., evaluate the quality of the MT system output by calculating the similarity between the translation output and the human reference.
Machine translation (MT) automatic evaluation	MAXSIM	Machine translation (MT) automatic evaluation metrics, such as BLEU (), NIST), METEOR (), TER (), MAXSIM ( etc., evaluate the quality of the MT system output by calculating the similarity between the translation output and the human reference.
pronoun translation	accuracy	The above constraints start playing a role in pronoun translation in situations where several translation options are possible fora given sourcelanguage pronoun, a large number of options being likely to affect negatively the translation accuracy.
machine translation	standard	This is to mimic the representation which many standard machine translation systems produce and to complicate the matter of standard Both authors contributed equally to this work.
MT-based	BLEU score	The MT-based approach to sentence alignment uses the BLEU score between sentences of one document and machine translated sentences of the other, as an indicator of parallelism between sentences.
sentence alignment	BLEU score	The MT-based approach to sentence alignment uses the BLEU score between sentences of one document and machine translated sentences of the other, as an indicator of parallelism between sentences.
information extraction	precision	Work on information extraction typically uses precision and recall of the extracted information as an evaluation measure.
information extraction	recall	Work on information extraction typically uses precision and recall of the extracted information as an evaluation measure.
Bilingual Document Alignment Task (WMT16)	recall	In this paper we present our approach to the Bilingual Document Alignment Task (WMT16), where the main goal was to reach the best recall on extracting aligned pages within the provided data.
MT	recall	If we consider all these pairs as valid for extracting data in order to train MT systems, our system reaches a recall of 92.5%.
APE	TER reduction	A recent study on APE by over six language pairs have reported consistent improvement (7.3% to 14.7% TER reduction) in the quality of machine translated text across all language pairs.
MT	BLEU	The baseline results reported in show that the naive monolingual APE system already outperforms the MT baseline by 1.5 BLEU score.
MT output	consistency	A number of studies confirm that post-editing MT output improves translators' performance in terms of productivity and it may also impact translation quality and consistency.
word alignment	METEOR	The other experimental settings were concerned with word alignment model between T L MT and T LP E are trained on three different aligners: Berkeley Aligner (), METEOR aligner ( and TER ().
word alignment	TER	The other experimental settings were concerned with word alignment model between T L MT and T LP E are trained on three different aligners: Berkeley Aligner (), METEOR aligner ( and TER ().
prediction	accuracy	These features are combined with more generic linguistically motivated black-box features that improved the prediction accuracy.
noun translation errors	MAE	The performance of the novel features in the noun translation errors and reordering measure groups in: Performance in terms of MAE and RMSE for the indiviual features describing noun translation errors and reordering reordering based on word alignments, we notice that only Tau give a positive impact.
noun translation errors	RMSE	The performance of the novel features in the noun translation errors and reordering measure groups in: Performance in terms of MAE and RMSE for the indiviual features describing noun translation errors and reordering reordering based on word alignments, we notice that only Tau give a positive impact.
tweet normalization	PT	 Table 2: Results for tweet normalization. UC  refers to the unstructured classifier presented in  Section 3.1, PT to the perceptron tagger presented  in Section 3.2 and AliSeTra to the system pre- sented by
word similarity evaluation	similarity rating	In word similarity evaluation, a list of pairs of words along with their similarity rating (as judged by human annotators) is provided.
parsing	accuracy	Stripping a single morphological feature (other than PoS) has little impact on parsing accuracy.
Sentence labeling	AGENT	 Table 2: Sentence labeling for two classification tasks: "contains professor as AGENT of recommend"  (column 2), and "sentence meaning involves professor performing act of recommending" (column 3).
classification	accuracy	shows the classification accuracy using 20 to 90 LSI topics.
classification	accuracy	The two lower bands show the classification accuracy in a 10-fold cross-validation (10CV), again with the reduced set of topic domains performing roughly 5% better.
tokenization task	unweighted harmonic average (F 1 )	Following, results for the tokenization task were evaluated based on the unweighted harmonic average (F 1 ) between precision (pr) and recall (rc) of the token boundaries in the participants' submissions.
tokenization task	precision (pr)	Following, results for the tokenization task were evaluated based on the unweighted harmonic average (F 1 ) between precision (pr) and recall (rc) of the token boundaries in the participants' submissions.
tokenization task	recall (rc)	Following, results for the tokenization task were evaluated based on the unweighted harmonic average (F 1 ) between precision (pr) and recall (rc) of the token boundaries in the participants' submissions.
PoS tagging task	accuracy (acc)	Following Giesbrecht and Evert (2009), the PoS tagging task was evaluated in terms of the accuracy (acc) of the PoS tag assignments in the participants' submissions.
PoS tagging	accuracy (acc)  percentages	 Table 3: Agreement between annotators and gold  standard for PoS tagging of the CMC data subset  (training and test sets). Values are accuracy (acc)  percentages.
tokenization	F 1 scores	 Table 4: Agreement between annotators and gold  standard for tokenization of the Web corpora test  data. Values are F 1 scores given as percentages.
PoS tagging	accuracy (acc) percentages	 Table 5: Agreement between annotators and gold  standard for PoS tagging of the Web corpora test  data. Values are accuracy (acc) percentages.
tokenization subtask	F 1	 Table 7: Results of the tokenization subtask including non-competitive submissions (marked with  *  ) and  baseline systems (marked with  † ). The last column gives the official EmpiriST 2015 "podium" ranking.  pr, rc, and F 1 are given as percentages for better readability.
PoS tagging subtask	accuracy	 Table 8: Results of the PoS tagging subtask including non-competitive or late submissions (marked  with  *  ) and baseline systems (marked with  † ). If applicable, a subscript indicates the best run of the  respective system (based on overall accuracy), which is listed in the table. The last column gives the  official EmpiriST 2015 "podium" ranking. acc is given as a percentage for better readability.
PoS tagging subtask	acc	 Table 8: Results of the PoS tagging subtask including non-competitive or late submissions (marked  with  *  ) and baseline systems (marked with  † ). If applicable, a subscript indicates the best run of the  respective system (based on overall accuracy), which is listed in the table. The last column gives the  official EmpiriST 2015 "podium" ranking. acc is given as a percentage for better readability.
tagger	accuracy	Therefore, our previous system focuses on OOV words in two ways: First, tagger accuracy can be improved substantially by adding relatively small amounts of manually annotated in-domain (CMC) data to a standard training set ().
Automatic Speech Recognition (ASR)	Out-of-Vocabulary (OOV) rates-words	We describe a system to collect web data for Low Resource Languages, to augment language model training data for Automatic Speech Recognition (ASR) and keyword search by reducing the Out-of-Vocabulary (OOV) rates-words in the test set that did not appear in the training set for ASR.
LRL web collection	Word Error Rate (WER)	In this paper we describe the properties which LRL web collection requires of systems, compare ours with other popular web collection and scraping software, and describe results achieved for reducing Word Error Rate (WER) for ASR and OOVs and improvements in the IARPA Babel keyword search task.
PoS tagging	accuracy	For PoS tagging, adding unsu-pervised knowledge beyond the available training data is the most important factor for reaching acceptable tagging accuracy.
transliterating names from seven languages to English	absolute gain	Experiments on transliterating names from seven languages to English demonstrate that our approach achieves 2.6% to 15.7% absolute gain over the baseline model, and significantly advances state-of-the-art.
alignment	recall	And, hand-picking FB categories for alignment hurts recall.
segmentation	accuracy	The errors arising at the segmentation stage will propagate to the decoding stage and inevitably detriment the overall transliteration accuracy.
evidence type classification	F1 score	Evidence type classification our second goal was for evidence type classification, results across the training techniques were comparable; the best results were again achieved by using SVM, which resulted in a 78.6% F1 score.
sentence extraction	Plain	• Stock: A summary generated using an implementation of the state of the art sentence extraction approach described by • Plain: A collection of point extracts with the same unstructured style and length as the Stock summary.
prediction of highly voted comments	delta score	They evaluated 1 http://idebate.org/ 2 http://convinceme.net 3 http://www.createdebate.com/: A disputation example from createddebate.com (The debating topic is "Should the Gorilla have died?") the effectiveness of different features for the prediction of highly voted comments in terms of delta score and karma score respectively.
prediction of highly voted comments	karma score	They evaluated 1 http://idebate.org/ 2 http://convinceme.net 3 http://www.createdebate.com/: A disputation example from createddebate.com (The debating topic is "Should the Gorilla have died?") the effectiveness of different features for the prediction of highly voted comments in terms of delta score and karma score respectively.
SSL	precision	 Table 3: Graph-based SSL improves BANNER by increasing the precision.
named entity  recognition	recall	 Table 1: Evaluation of the named entity recognition for each entity type on the test sets, measured with  strict entity level metrics. Reported results for corresponding state-of-the-art approaches are shown for  comparison.  * The evaluation of the best performing system for disease mentions is the combination of named entity  recognition and normalization.  ** The official BioCreative II evaluation for our GGP model results in 84.67, 84.54 and 84.60 for preci- sion, recall and F-score respectively. These numbers are comparable to the listed state-of-the-art method.
named entity  recognition	F-score	 Table 1: Evaluation of the named entity recognition for each entity type on the test sets, measured with  strict entity level metrics. Reported results for corresponding state-of-the-art approaches are shown for  comparison.  * The evaluation of the best performing system for disease mentions is the combination of named entity  recognition and normalization.  ** The official BioCreative II evaluation for our GGP model results in 84.67, 84.54 and 84.60 for preci- sion, recall and F-score respectively. These numbers are comparable to the listed state-of-the-art method.
detecting mention of proteins, genes	accuracy	The performance of concept entity recognition systems for detecting mention of proteins, genes, drugs, diseases, tests and treatments has achieved sufficient level of accuracy, which gives us opportunity for using these data to do next level tasks of natural language processing (NLP).
relation identification tasks	F-measure	 Table 4: Results on the development dataset  on the relation identification tasks (P=Precision,  R=Recall, F=F-measure)
relation extraction	Prec	 Table 1: Results for relation extraction. NB is Multinomial Naive Bayes. Prec is Precision.
edge detection	Passive-aggressive (PA	In the edge detection, we adopt Passive-aggressive (PA) online algorithm, then we constitute events by post-processing of TEES.
MTI	F 1	 Table 3: Performance of MTI with L2R on indi- vidual "As Topic" headings with F 1 values of at  least 50% ("D,A", "A&I", "W&H", and "RS"  denote, respectively, "Dissertations, Academic as  Topic" , "Abstracting and Indexing as Topic",  "Wit and Humor as Topic", and "Research Sup- port as Topic"), as well as collectively for all 83  "As Topic" headings.
translation	MERT	We also estimated the translation quality in the presence of the all features (we run MERT for each row of).
machine translation (MT) unacceptability	precision	This paper reports on an initial study that aims to understand whether the acceptability of translation memory (TM) among translators when contrasted with machine translation (MT) unacceptability is based on users' ability to optimise precision in match suggestions.
MT predicated	precision	Accordingly, this paper reports on an initial study that seeks to answer the following question: Is comparative acceptability of TM over MT predicated on the user's ability to optimise the precision and usefulness of match suggestions by setting a minimum match threshold?
SMT	recall	While the X(T=1) system shows a higher precision for the SMT system, it shows higher recall for the RBMT system.
MT	precision	When we evaluate the X system using increasing T values, we see a similar trend for both types of MT output, namely minor losses in precision and major gains on recall, which leads to increased F1 scores.
MT	recall	When we evaluate the X system using increasing T values, we see a similar trend for both types of MT output, namely minor losses in precision and major gains on recall, which leads to increased F1 scores.
MT	F1 scores	When we evaluate the X system using increasing T values, we see a similar trend for both types of MT output, namely minor losses in precision and major gains on recall, which leads to increased F1 scores.
MT	T	In, we see that for both types of MT output, the increasing T values brings minor losses on precision and major gains on recall, similar to our observations from.
MT	precision	In, we see that for both types of MT output, the increasing T values brings minor losses on precision and major gains on recall, similar to our observations from.
MT	recall	In, we see that for both types of MT output, the increasing T values brings minor losses on precision and major gains on recall, similar to our observations from.
MT	accuracy	It seems that even though these systems perform well on sentence-level error detection, they are notable to locate the errors within the MT output with high accuracy.
SMT	MERT	 Table 8: Effects of source language on tuning of an SMT system: MERT tuning on BLEU  using independent reference, post-edit from the corresponding source language and  post-edit from another source language. The best BLEU and METEOR scores are ob- tained when the corresponding source language post-edit is used.
SMT	BLEU	 Table 8: Effects of source language on tuning of an SMT system: MERT tuning on BLEU  using independent reference, post-edit from the corresponding source language and  post-edit from another source language. The best BLEU and METEOR scores are ob- tained when the corresponding source language post-edit is used.
SMT	BLEU	 Table 8: Effects of source language on tuning of an SMT system: MERT tuning on BLEU  using independent reference, post-edit from the corresponding source language and  post-edit from another source language. The best BLEU and METEOR scores are ob- tained when the corresponding source language post-edit is used.
SMT	METEOR	 Table 8: Effects of source language on tuning of an SMT system: MERT tuning on BLEU  using independent reference, post-edit from the corresponding source language and  post-edit from another source language. The best BLEU and METEOR scores are ob- tained when the corresponding source language post-edit is used.
document alignment	accuracy	We first performed document alignment on the EUROPARL corpus as a testbench for accuracy on parallel document alignment.
candidate translation initialised	MERT	Two types of experiments were carried out, firstly with the candidate translation initialised by running Moses (with a 5-gram language model trained with KenLM on 2.2 million Europarl sentences and feature weights tuned using MERT on a development set of 2525 sentences from the newstest2009 data), and secondly by random initialisation (i.e. random segmentation and random phrase translation).
SMT	BL	We compare two systems: (1) the Moses phrase-based SMT system trained as above, noted 'BL' (baseline); and (2) the system which re-ranks the N-best list generated by BL using the PLM, as described in the previous section, noted 'RR'.
SMT	RR	We compare two systems: (1) the Moses phrase-based SMT system trained as above, noted 'BL' (baseline); and (2) the system which re-ranks the N-best list generated by BL using the PLM, as described in the previous section, noted 'RR'.
MT	BLEU	MT researchers routinely rely on automatic evaluation metrics such as BLEU () to guide their development efforts.
MT outputs	Appraise	The human evaluation consists of ranking MT outputs with Appraise.
summarisation	Fmeasure	Compared to the state of the art in summarisation, the results with WE are also encouraging, since previous published results with the same corpus) are close to 44% (Fmeasure for ROUGE-1).
summarisation	ROUGE-1	Compared to the state of the art in summarisation, the results with WE are also encouraging, since previous published results with the same corpus) are close to 44% (Fmeasure for ROUGE-1).
SLU tasks	F1 score	On SLU tasks, our joint model outperforms the independent task training model by 22.3% on intent detection error rate, with slight degradation on slot filling F1 score.
ASR	accuracy	For example , two values, whose associated concepts have different ASR accuracy, may have different state tracking performance.
ASR	accuracy	For example, two values, whose associated concepts have different ASR accuracy, have differ-ent state tracking performance.
coreference resolution	B 3	All systems are evaluated with the official CoNLL scorer on three metrics concerning coreference resolution: MUC, B 3 , and CEAF e . MUC MUC ( concerns the number of pairwise links needed to be inserted or removed to map system responses to gold keys.
coreference resolution	CEAF	All systems are evaluated with the official CoNLL scorer on three metrics concerning coreference resolution: MUC, B 3 , and CEAF e . MUC MUC ( concerns the number of pairwise links needed to be inserted or removed to map system responses to gold keys.
topic changing"	tempo	In addition, by analyzing the effectiveness of their thoughts, we found that dialogue strategies for personalization related to "topic elaboration", "topic changing" and "tempo" significantly increased the satisfaction with regard to the dialogues.
recognition	accuracy	We show that its recognition accuracy performs on par with stateof-the-art systems while maintaining low power consumption and real-time ability in Section 3.
DA segmentation	accuracy	Our case study suggests that incremental DA segmentation can be performed with sufficient accuracy for us to begin to extend our baseline agent's conversational abilities without significantly degrading its current performance.
ASR	CER	DA multiset precision and recall metrics When ASR is used, the CER and LevenshteinLenient metrics give an indication of how well you are doing at replicating the ordered sequence of DA tags.
predicting AFS	Ridge Regression (RR)	 Table 2: Results for predicting AFS with individual features using Ridge Regression (RR) and Support  Vector Regression (SVR) with 10-fold Cross-Validation on the 1800 training items for each topic.
predicting AFS	Support  Vector Regression (SVR)	 Table 2: Results for predicting AFS with individual features using Ridge Regression (RR) and Support  Vector Regression (SVR) with 10-fold Cross-Validation on the 1800 training items for each topic.
predicting AFS	Support Vector Regression (SVR)	 Table 3: Results for feature combinations for predicting AFS, using Support Vector Regression (SVR)  with 10-fold Cross-Validation on the 1800 training items for each topic.
Request Type Classification	Accuracy	 Table 4: Request Type Classification Accuracy
reward design	completion rate	Consequently, evaluation metrics that are traditionally used for reward design, such as task completion rate, are no longer appropriate.
morphological disambiguation of Sanskrit	accuracy	The paper describes anew tagset for the morphological disambiguation of Sanskrit, and compares the accuracy of two machine learning methods (CRF, deep recurrent neural networks) for this task, with a special focus on how to model the lexicographic information.
PRF	accuracy	 Table 2: Macro-average PRF and accuracy on the full training set, features: 1h. First row: Results for  ambiguous words; second row: Results for all words.
POS tagging	accuracy	As reported by, out of the 100,000 words in the manually POS tagged corpus, 3989 words do not fall into any category of the UCSC Tag Set, which means that even manual POS tagging cannot achieve 100% accuracy.
Tagging	accuracy	 Table 6: Tagging accuracy per POS tag
MT	outof-vocabulary (OOV) rates	Moreover, the standard data sets used by the MT research community do not contain Arabizi, meaning that any attempt to translate Arabic in Arabizi writing will suffer from extremely high (close to 100%) outof-vocabulary (OOV) rates.
SMT	BLEU	shows SMT quality measured with case-insensitive BLEU () fora number of transliteration scenarios.
stemming Arabic tweets	accuracy	Our Arabic stemming approach is not dictionary based, which is crucial for stemming Arabic tweets, since they have a very open lexicon, and we are able to reach around 78% stemming accuracy.
tweet-level sentiment analysis	F1-Score	We find that generic tools perform well for tweet-level language identification and tweet-level sentiment analysis (both 0.94 F1-Score).
detection of effect mentions	F1-Score	For detection of effect mentions we are able to achieve 0.87 F1-Score, while effect-level adverse-vs.-beneficial analysis proves harder with an F1-Score of 0.64.
detection of effect mentions	F1-Score	For detection of effect mentions we are able to achieve 0.87 F1-Score, while effect-level adverse-vs.-beneficial analysis proves harder with an F1-Score of 0.64.
Tweet-level sentiment analysis	F1- score	 Table 2: Tweet-level sentiment analysis results for  3 best sentiment analysis tools on test set B of 300  manually annotated tweets (weighted-average F1- score over POS, NEG and NEU labels; Accuracy).
Tweet-level sentiment analysis	Accuracy	 Table 2: Tweet-level sentiment analysis results for  3 best sentiment analysis tools on test set B of 300  manually annotated tweets (weighted-average F1- score over POS, NEG and NEU labels; Accuracy).
sentiment analysis	F1- score	 Table 4: Effect-level sentiment analysis results for  4 best sentiment analysis tools on test set B of 300  manually annotated tweets (weighted-average F1- score over ADV, BEN and NEU labels; Accuracy).
sentiment analysis	BEN	 Table 4: Effect-level sentiment analysis results for  4 best sentiment analysis tools on test set B of 300  manually annotated tweets (weighted-average F1- score over ADV, BEN and NEU labels; Accuracy).
sentiment analysis	Accuracy	 Table 4: Effect-level sentiment analysis results for  4 best sentiment analysis tools on test set B of 300  manually annotated tweets (weighted-average F1- score over ADV, BEN and NEU labels; Accuracy).
classification of tweets	precision	In Section 5 we present the experiments and results obtained in the classification of tweets, in terms of precision.
NER	precision	The application of NER helps increase the precision.
segmentation and categorisation'	F1	Importantly, for the 'segmentation and categorisation' subtask, our approach significantly outperformed the second best system (namely, Talos) by 6.2 F1 score.
token-level tagging	BIOES	We also investigated the effect of using different token-level tagging schemes and confirmed that improved performance can be obtained by applying the finer-grained BIOES scheme rather than the more popularly used BIO convention.
classification task	F 1 measure	The system performance on the classification task was worse, with an F 1 measure of 40.06% on unseen test data, which was the fourth best of the ten systems participating in the shared task.
identification of Twitter names	F 1 score	A Conditional Random Field () classifier was trained on a rich set of features and used for identification of Twitter names from the tweets, giving an F 1 score of 63.22%.
SI	F1 score	Finally, the SI model's evaluation on the test data achieved a F1 score of 47.3% (~1.15% increase over the 2 nd best submitted solution).
Tweet-level Geolocation Prediction	accuracy	 Table 2: Results for Tweet-level Geolocation Prediction. The bolded results indicate the best performing  statistics (highest value for accuracy and lowest value for mean/median error) for the validation and  testing set.
message-level prediction	accuracy	The message-level prediction system 2 and the user-level prediction system 2 were evaluated on the test run with classification accuracy, median distance error, and mean distance error.
message-level prediction	median distance error	The message-level prediction system 2 and the user-level prediction system 2 were evaluated on the test run with classification accuracy, median distance error, and mean distance error.
message-level prediction	mean distance error	The message-level prediction system 2 and the user-level prediction system 2 were evaluated on the test run with classification accuracy, median distance error, and mean distance error.
tagging	accuracy	We set out to determine if (a) our sentence selection algorithm is efficient and (b) we can reliably predict tagging accuracy based on how many sentences we have already selected and annotated.
Alignment	RMSE	Alignment quality seems reasonable but we have not yet formally evaluated it (e.g. in terms of root-mean-squared-error -RMSE -of boundary placement).
PHI identification task	Recall	 Table 5: Detailed performance analysis with different models for PHI identification task. Here R,P and  F denotes Recall, Precision and F-score respectively.
PHI identification task	Precision	 Table 5: Detailed performance analysis with different models for PHI identification task. Here R,P and  F denotes Recall, Precision and F-score respectively.
PHI identification task	F-score	 Table 5: Detailed performance analysis with different models for PHI identification task. Here R,P and  F denotes Recall, Precision and F-score respectively.
paraphrase generation	BLEU	To quantitatively evaluate the performance of our paraphrase generation models, we use two well-known automatic metrics for machine translation evaluation: BLEU () and METEOR.
paraphrase generation	METEOR	To quantitatively evaluate the performance of our paraphrase generation models, we use two well-known automatic metrics for machine translation evaluation: BLEU () and METEOR.
machine translation evaluation	BLEU	To quantitatively evaluate the performance of our paraphrase generation models, we use two well-known automatic metrics for machine translation evaluation: BLEU () and METEOR.
machine translation evaluation	METEOR	To quantitatively evaluate the performance of our paraphrase generation models, we use two well-known automatic metrics for machine translation evaluation: BLEU () and METEOR.
RNN-based concept recognition	precision  (P)	 Table 6: Results for RNN-based concept recognition on the i2b2 2010 corpus, measured with precision  (P), recall (R), and F 1 -measure.
RNN-based concept recognition	recall (R)	 Table 6: Results for RNN-based concept recognition on the i2b2 2010 corpus, measured with precision  (P), recall (R), and F 1 -measure.
RNN-based concept recognition	F 1 -measure	 Table 6: Results for RNN-based concept recognition on the i2b2 2010 corpus, measured with precision  (P), recall (R), and F 1 -measure.
RM	SE	 Table 3: RM SE tweet across five traits level. Bold highlights best performance. indicates N/A.
RM	N/A	 Table 3: RM SE tweet across five traits level. Bold highlights best performance. indicates N/A.
stance taking	contrast	Automatic detection of five language components, which are all relevant for expressing opinions and for stance taking, was studied: positive sentiment, negative sentiment, speculation, contrast and condition.
stance taking	contrast	Five language components, which are relevant as topic-independent components for expressing opinions and for stance taking, were investigated: positive and negative sentiment, speculation, contrast and condition.
relation extraction	accuracy	To expand a knowledge graph, not only do we need a relation extraction model with high accuracy, but also the model is better to be scalable.
machine translation	BLEU	For the evaluation of machine translation quality, some standard automatic evaluation metrics have been used, like BLEU () and RIBES () in all experiments.
machine translation	RIBES	For the evaluation of machine translation quality, some standard automatic evaluation metrics have been used, like BLEU () and RIBES () in all experiments.
Multi-system machine translation (MT)	accuracy	Multi-system machine translation (MT) is a subset of hybrid MT where multiple MT systems are combined in a single system in order to boost the accuracy and fluency of the translations.
machine translation evaluation	BLEU	Traditional machine translation evaluation metrics such as BLEU and WER have been widely used, but these metrics have poor correlations with human judgements because they badly represent word similarity and impose strict identity matching.
machine translation evaluation	WER	Traditional machine translation evaluation metrics such as BLEU and WER have been widely used, but these metrics have poor correlations with human judgements because they badly represent word similarity and impose strict identity matching.
corpus pattern analysis annotation	MERT	 Table 3: Evaluation results for corpus pattern analysis annotation, best MERT run
corpus pattern analysis	MERT	 Table 4: Multeval results for corpus pattern analysis, based on 36 MERT runs
statistical machine translation (SMT)	MEANT	Recent research showed that including a semantic frame based objective function at an early stage of training statistical machine translation (SMT) systems helps to learn more meaningful word alignments) rather than relying on tuning against a semantic based objective function such as MEANT ( , which improves the translation adequacy).
Translation	BLEU  METEOR TER  WER  PER  CDER  GIZA++	 Table 4: Translation quality of an Uzbek-English phrase based SMT system build on three different  alignment methods.  cased/uncased  Alignments  BLEU  METEOR TER  WER  PER  CDER  GIZA++  16.28/17.09 40.7/42.8 82.20/80.91 88.51/87.71 66.70/64.61 79.47/78.11  BITG  16.85/17.66 38.8/40.9 79.75/78.12 85.53/84.60 65.04/62.89 76.93/75.51  Monolingual English SRL
Translation	BITG	 Table 4: Translation quality of an Uzbek-English phrase based SMT system build on three different  alignment methods.  cased/uncased  Alignments  BLEU  METEOR TER  WER  PER  CDER  GIZA++  16.28/17.09 40.7/42.8 82.20/80.91 88.51/87.71 66.70/64.61 79.47/78.11  BITG  16.85/17.66 38.8/40.9 79.75/78.12 85.53/84.60 65.04/62.89 76.93/75.51  Monolingual English SRL
NMT	accuracy	These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential.
NMT	speed	These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential.
SMT	BLEU	Our experiments on Japanese-Chinese patent sentences show that the proposed NMT system achieves a substantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over traditional SMT systems and an improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent NMT system without our proposed technique.
SMT	BLEU	Our experiments on Japanese-Chinese patent sentences show that our proposed NMT system achieves a substantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over a traditional SMT system and an improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent NMT system without our proposed technique.
SMT	BLEU	When compared with the baseline SMT, the performance gain of the proposed system is approximately 3.1 BLEU points if translations are produced by the proposed NMT system of Section 4.3 or 2.3 RIBES points if translations are produced by the proposed NMT system of Section 4.2.
SMT	RIBES	When compared with the baseline SMT, the performance gain of the proposed system is approximately 3.1 BLEU points if translations are produced by the proposed NMT system of Section 4.3 or 2.3 RIBES points if translations are produced by the proposed NMT system of Section 4.2.
title translation	BLEU	PBSMT shows better performance for title translation than NMT both in BLEU and RIBES, because it is possible for PBSMT to partially translate title using the phrase table created from infrequent expressions.
title translation	RIBES	PBSMT shows better performance for title translation than NMT both in BLEU and RIBES, because it is possible for PBSMT to partially translate title using the phrase table created from infrequent expressions.
translation	BLEU	In the translation of step, NMT shows better performance than PBSMT in BLEU and RIBES.
SMT	BLEU	And the optimally tuned SMT system is able to give a BLEU score of 22.14.
phrase translation	accuracy	Furthermore, in NRMs, the use of phrase translation and the word alignment improves the accuracy by 0.31 and 0.82 points, respectively.
translation	accuracy	We investigate whether our reordering system improves translation accuracy.
translation	accuracy	Consequently, we consider that there is little influence on the translation results, because the change in each label of reordering is small, although the reordering accuracy rate of the NRM and the proposed method differ by 1.67 points.
MT	BLEU score	In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score.
translation	accuracy	Second, we incorporate minimum-risk training to optimize the parameters of the model to improve translation accuracy ( §4).
ASR	punctuation	First, because ASR engines normally do not output punctuation, punctuation was removed.
machine translation	EC	 Table 2: machine translation results with empty categories. dl means the distortion limit. EC indicates  empty categories are detected in dataset
SMT	BLEU	Compared to these SMT baselines, each of the character-based models clearly outperforms the phrase-based system in both of the BLEU and RIBES scores.
SMT	RIBES	Compared to these SMT baselines, each of the character-based models clearly outperforms the phrase-based system in both of the BLEU and RIBES scores.
SMT	BLEU	Although the hierarchical phrase-based SMT system and the tree-to-string SMT system outperforms the single character-based model without phrase label inputs by +1.04 and by +1.92 BLEU scores, respectively, our best ensemble of character-based models shows better performance (+5.65 RIBES scores) than the tree-to-string SMT system.
SMT	RIBES	Although the hierarchical phrase-based SMT system and the tree-to-string SMT system outperforms the single character-based model without phrase label inputs by +1.04 and by +1.92 BLEU scores, respectively, our best ensemble of character-based models shows better performance (+5.65 RIBES scores) than the tree-to-string SMT system.
translation	accuracy	To improve the translation accuracy, we adjust and balance the granularity of segmentation results around terms for Chinese-Japanese patent corpus for training translation model.
SMT translation	accuracy	To improve SMT translation accuracy, we change and adjust the segmentation for terms using extracted bilingual multi-word terms for both languages (not only for Chinese or Japanese).
MT	BLEU	For all test sets and MT systems, BLEU scores () and character n-gram F-scores CHRF3 are reported.
SMT	accuracy	Ina phrase-based SMT framework, we investigate different choices of decoder parameters as well as data format and their impact on decoding time and translation accuracy.
translation	accuracy	We suggest best options for these settings that significantly improve decoding time with little impact on the translation accuracy.
translation	accuracy	We use BLEU () for evaluating translation accuracy.
classification	accuracy	Consistent with most previous studies, we report our results as classification accuracy under k-fold crossvalidation, with k = 10.
Discriminating between similar languages and national language varieties	accuracy	For the sub-task 1 which deals with Discriminating between similar languages and national language varieties, the system achieved an accuracy of 87.79% on the closed track, ending up ninth (the best results being 89.38%).
language detection	accuracy	argued that language detection is a solved problem since the performance of most systems approaches 100% accuracy.
identification-level	recall score	In identification-level, the recall score also ranked in the fourth place.
NER tasks	f1-score	We evaluate the performance on the three biomedical NER tasks in terms of f1-score, precision and recall measures: where T P (true positive) is the number of named entity chunks that are correctly identified, F P (false positive) is the number of chunks that are mistakenly identified as entities, and F N (false negative) are the number of named entity chunks that are not identified.
NER tasks	precision	We evaluate the performance on the three biomedical NER tasks in terms of f1-score, precision and recall measures: where T P (true positive) is the number of named entity chunks that are correctly identified, F P (false positive) is the number of chunks that are mistakenly identified as entities, and F N (false negative) are the number of named entity chunks that are not identified.
NER tasks	recall	We evaluate the performance on the three biomedical NER tasks in terms of f1-score, precision and recall measures: where T P (true positive) is the number of named entity chunks that are correctly identified, F P (false positive) is the number of chunks that are mistakenly identified as entities, and F N (false negative) are the number of named entity chunks that are not identified.
Entity classification	F 1 score	 Table 3: Entity classification results of Char-BiLSTM on Stockholm EPR corpus. Given the entity  boundaries, we can see that the classification of entities work very well, obtaining a total F 1 score of  0.75.
negation detection task	FP	 Table 6: Performance on the negation detection task for both datasets with NegEx and with the baseline.  TP refers to True Positive results, FP to False Positive, TN to True Negatives and to False Negatives.  NTS refers to NegEx original triggers and OTS to our trigger set.
negation detection task	OTS	 Table 6: Performance on the negation detection task for both datasets with NegEx and with the baseline.  TP refers to True Positive results, FP to False Positive, TN to True Negatives and to False Negatives.  NTS refers to NegEx original triggers and OTS to our trigger set.
data mining	precision	Results showed that the distributional semantics method provided higher recall, the data mining approach provided higher precision and the stacked ensemble of the two methods achieved the overall best performance.
JDs	detail	The JDs add more detail by describing the different specialties associated to the articles.
prediction	F1 score	By using Bag-of-Words with TF-IDF, we can get the F1 score for prediction reach only about 75%, which all of Embedding Layer with Neural Network hidden layer can completely surpass this F1 score after a few epochs of training.
relation detection	F1	On subtask-1, i.e. for relation detection, in the cross-validation setting, it is shown that CNN with word2vec embedding setting performed (16%F1) better than the random embeddings.
relation type detection	F1	On subtask-2, i.e for relation type detection, in the cross-validation setting, it is shown that CNN with word2vec embedding setting performed (14.63%F1) better than the random embeddings learned from the training set.
Machine Translation (MT) evaluation	BLEU	Recent Machine Translation (MT) evaluation tends to be conducted based on (1) Automatic evaluation metrics use reference translations for each segment such as BLEU, NIST, METEOR (;).
Machine Translation (MT) evaluation	METEOR	Recent Machine Translation (MT) evaluation tends to be conducted based on (1) Automatic evaluation metrics use reference translations for each segment such as BLEU, NIST, METEOR (;).
SCTB	F-Measure	We can see that SCTB significantly improves both segmentation and joint segmentation and POS tagging performance by a large margin, i.e., 4.27% and 7.26% F-Measure, respectively.
segmentation	F-Measure	We can see that SCTB significantly improves both segmentation and joint segmentation and POS tagging performance by a large margin, i.e., 4.27% and 7.26% F-Measure, respectively.
parsing	accuracy	We used the Evalb toolkit 7 for the parsing accuracy calculation.
AP	AP	As for AP, we have not observed any clear convergence in the exploration process, however, at the initial stage of exploration we have observed instead that generated sentences do not satisfy the generation constraints, e.g., whose length is too short or too long, therefore, the values of more than 100 or less than 20 have been observed. and shows the values of AP of the initial and final 1,000 simulations, respectively.
influence detection	TP FN]	 Table 5: The results of all groups of features on influence detection  using author traits. The confusion matrix is filled, by row, as [TP FN]
stance prediction	accuracy	Using Model 1 (M1), we improve stance prediction accuracy for 11 of the issues and agreement accuracy for all issues.
stance prediction	accuracy	Model 3 (M3) increases the stance prediction accuracy of M2 for 4 issues and the agreement accuracy for 9 issues.
prediction	accuracy	While having tensor hundreds of messages for each user can improve prediction accuracy, collecting more data for every user of interest maybe prohibitive either in terms of API access, or in terms of the time required.
parsing	yield	We compare NPFST to both n-gram and parsing methods in terms of yield, recall, and efficiency.
parsing	recall	We compare NPFST to both n-gram and parsing methods in terms of yield, recall, and efficiency.
Translation	GOD	Translation: Buddy you are GOD.
Token level	F-measure	 Table 7: Token level performance results. We ranked the systems using the weighted average F-measure, Avg-F. A '-' indicates that
Token level	Avg-F	 Table 7: Token level performance results. We ranked the systems using the weighted average F-measure, Avg-F. A '-' indicates that
identifying the language of every word	accuracy	This paper explores the English-Bengali code-mixing phenomenon and presents algorithms capable of identifying the language of every word to a reasonable accuracy in specific cases and the general case.
MSA-DA classification task	F1-score	On the MSA-DA classification task, our system managed to obtain F1-score of 0.66 on tweet level and overall token level accuracy of 74.7%.
MSA-DA classification task	accuracy	On the MSA-DA classification task, our system managed to obtain F1-score of 0.66 on tweet level and overall token level accuracy of 74.7%.
frame semantic parsing (FSP)	precision	All performance values shown here are measured for the task of frame semantic parsing (FSP), meaning that we measure precision, recall, and F-measure where every index inf and k are considered predictions.
frame semantic parsing (FSP)	recall	All performance values shown here are measured for the task of frame semantic parsing (FSP), meaning that we measure precision, recall, and F-measure where every index inf and k are considered predictions.
frame semantic parsing (FSP)	F-measure	All performance values shown here are measured for the task of frame semantic parsing (FSP), meaning that we measure precision, recall, and F-measure where every index inf and k are considered predictions.
word prediction	recall	For word prediction with a list of 5 suggestions, it improves recall from 25.03% to 71.28% and for word completion it improves keystroke savings from 34.35% to 44.81%, where theoretical bound for this dataset is 58.78%.
vocabulary exploration	recall	A vocabulary exploration system will need to have a high recall.
Alzheimer's Disease (AD)	UNS	Current state-of-the-art diagnostic measures for Alzheimer's Disease (AD) are invasive, expensive, Currently in e.g. Sweden () only 30% of all Alzheimer's disease receive a complete memory investigation, diagnosis and symptomatic drugs, the rest of the cases are assigned the codes UNS, "unspecified dementia". and time-consuming.
AA identification	expansion	Results show an F1 score for AA identification of 0.96, an overall expansion accuracy of 74.3%, and a disambiguation accuracy of 71.6%, all without any supervised annotations used during training.
AA identification	precision	AA identification can be considered a binary detection problem and can thus be evaluated by precision, recall, and F1 score.
AA identification	recall	AA identification can be considered a binary detection problem and can thus be evaluated by precision, recall, and F1 score.
AA identification	F1 score	AA identification can be considered a binary detection problem and can thus be evaluated by precision, recall, and F1 score.
character confusions	OCR	While we do not have annotated training material to learn character confusions, we can exploit the natural redundancy in the corpus: Using a string alignment algorithm we identified near-duplicates in the subset of documents with 'Good' OCR quality.
document-level sentiment classification	MSE	 Table 8: Results of document-level sentiment classification. MSE: mean squared error (lower is better).
document-level sentiment classification	mean squared error	 Table 8: Results of document-level sentiment classification. MSE: mean squared error (lower is better).
tagging	accuracy	The improvement obtained with word cluster features lends support to the intuition that capturing similarity in vocabulary within the feature space helps with tagging accuracy.
prediction	accuracy	In addition, the latent modes can be used to weight text features thereby improving prediction accuracy.
Witness identification	F-score	 Table 5: Witness identification F-score for each  event and model: Baseline
Witness identification	F-score	 Table 6: Witness identification F-score for each event and model
discourse classification	F-Score	It can be observed that the best performance for discourse classification is 72.6%: F-Score for discourse classification models.
stacking	Logistic Regression (LR)	In stacking, Linear Support Vector Classification (Linear SVC) 2 , Logistic Regression (LR) and Stochastic Gradient Descent (SGD) from Scikit-learn (sklearn) have been used as base-level learning algorithms.
POS tagger	accuracy	For this reason, we assume that the POS tagger can reach a higher accuracy if we can split the data sets into more homogeneous subsets and then train individual expert POS taggers, specialized for individual subsets.
NER task	accuracies	We propose a method to be able to model the NER task using RNN based approaches using the unsupervised data available and achieve good improvements in accuracies over many other models without any hand-engineered features or any rule-based approach.
translation	BLEU	The labelling scheme can be reduced to two adjunct/non-adjunct labels, and improves translation over Hiero by up to 0.6 BLEU points for English-Chinese.
MT	BLEU	Especially when using MT approaches other than SMT, this makes sure that researchers striving for insights and ideas for improvement are not discouraged by using (only) automatic scores like BLEU that are by design unable to detect changes at the needed level of detail.
SMT	BLEU	Especially when using MT approaches other than SMT, this makes sure that researchers striving for insights and ideas for improvement are not discouraged by using (only) automatic scores like BLEU that are by design unable to detect changes at the needed level of detail.
Translation	accuracy	The results appear in: Translation accuracy on manually evaluated sentences focusing on particular phenomena.
Translation	accuracy	 Table 4: Translation accuracy on manually evaluated sentences focusing on particular phenomena. Test- sets consist of hand-picked source sentences that include the respective phenomenon. Simple RBMT is  separated as it does not participate in the selection mechanism. The percentage of the best system in each  category is bold-faced, whereas (*) indicates that there is no significant difference (α = 0.05) between  the selection mechanism and the best system.
bundle creation	accuracy	We find that the bundle creation algorithm used by is disambiguating bundles with a much higher accuracy compared to selecting sentences by chance, while under both conditions the difference to maximally ambiguous bundles is quite high.
Machine Translation	ROUGE	To  First, we perform automatic evaluation using regular summarization and text generation evaluation metrics, such as BLEU (), which is generally used for Machine Translation and variants of ROUGE, which is generally used for summarization evaluation.
Character compression	similarity	 Table 3: Character compression rates and similarity to the
OA	precision	Hence, extra words in OA are penalized for precision, where the length of generated sentences is critical for the calculation of these evaluation metrics.
parsing	recall	For parsing, this emphasis has been shown to payoff in improved recall of unbounded dependencies ().
dependency parsing	accuracy	Further , we discuss the interplay of POS and lexical information for dependency parsing and provide a detailed analysis and a discussion of results: while we observe significant improvements for count-based methods, neural vectors do not increase the overall accuracy.
POS tagging	accuracy	We show consistent significant improvements both for POS tagging accuracy as well as for Labeled Attachment Scores (LAS) for graph-based semantic similarities.
POS	accuracy	The successful strategies mostly improve POS accuracy on open class words, which results in better dependency parses.
POS tagging	accuracy	Beyond improving POS tagging, the strategy also contributes to parsing accuracy.
parsing	accuracy	Beyond improving POS tagging, the strategy also contributes to parsing accuracy.
parsing	accuracy	UDPipe () adds abeam search decoding to Parsito in order to improve the parsing accuracy at the cost of decreasing the parsing speed.
parsing	accuracy	We propose to improve the parsing accuracy in the architecture introduced by through using more informative word vectors.
parsing	accuracy	Thus, it is expected that more qualified word vectors positively affect the parsing accuracy.
MAP adaptation	accuracy	With our method we are able to decrease the model complexity with MAP adaptation while increasing the accuracy.
ASR	accuracy	While today many out-of-the-box ASR systems work fairly well, in noisy real-world conditions the accuracy and speed are often insufficient for large-vocabulary open-domain dictation.
character recognition	accuracy	Both model training and testing were done on Finnish corpora of historical newspaper text and the best combination of OCR and post-processing models give 95.21% character recognition accuracy.
linguistic analysis	accuracy	Although researchers in natural language processing have focused for decades on the development of tools for the automatic linguistic analysis of languages and state-of-the-art systems for linguistic analysis have achieved a high degree of accuracy today, these tools are still not widely used by scholars in the humanities and social sciences.
parsing	accuracy	Our results show that parsing accuracy can be significantly improved by introducing more fine-grained morphological information in the tagset, even if tagger accuracy is compromised.
parsing	accuracy	Our results show that parsing accuracy can be significantly improved by introducing more fine-grained morphological information in the tagset, even if tagger accuracy is compromised.
parsing	accuracy	Our results show that the introduction of morphological distinctions not present in the original tagset, whilst compromising tagger accuracy, actually leads to significantly improved parsing accuracy.
Tagging	accuracy	Tagging is evaluated in terms of accuracy (computed by the TnT-included tnt-diff script; denoted Acc in the following tables), while parsing is evaluated in terms of labeled and unlabeled attachment score (LAS and UAS; computed by the eval.pl 2 script from the CoNLL shared tasks).
Tagging	Acc	Tagging is evaluated in terms of accuracy (computed by the TnT-included tnt-diff script; denoted Acc in the following tables), while parsing is evaluated in terms of labeled and unlabeled attachment score (LAS and UAS; computed by the eval.pl 2 script from the CoNLL shared tasks).
Tagging	LAS	Tagging is evaluated in terms of accuracy (computed by the TnT-included tnt-diff script; denoted Acc in the following tables), while parsing is evaluated in terms of labeled and unlabeled attachment score (LAS and UAS; computed by the eval.pl 2 script from the CoNLL shared tasks).
tagger	accuracy	Unsurprisingly, we see that the tagger accuracy plummets when we move from the original to the full tagset.
parsing	LAS	Introducing the distinction of type (demonstrative, amplifier, quantifier, possessive or interrogative) led to an increase in tagger accuracy of 0.14 percentage points to 97.61%, while marginally impacting the parsing, with LAS of 87.00%, 0.01 percentage points below that of the original tagset.
tagger	accuracy	The increase in tagger accuracy when introducing the distinction of type is noteworthy, as we expected the finer granularity to lead to a decrease inaccuracy.
parsing	LAS	Gender, on the other hand, improved parsing (87.09% LAS), but complicated tagging, as the various genders are often difficult to differentiate, in particular masculine and feminine, which share many of the same forms.
tagger	accuracy	The number of a determiner, i.e., singular or plural, led to a small increase in tagger accuracy and LAS, while marginally lower UAS.
tagger	LAS	The number of a determiner, i.e., singular or plural, led to a small increase in tagger accuracy and LAS, while marginally lower UAS.
tagger	UAS	The number of a determiner, i.e., singular or plural, led to a small increase in tagger accuracy and LAS, while marginally lower UAS.
parsing	LAS	The introduction of definiteness gave the best parsing results, LAS of 87.30% and UAS of 90.42%, and additionally increased tagger accuracy slightly.
parsing	UAS	The introduction of definiteness gave the best parsing results, LAS of 87.30% and UAS of 90.42%, and additionally increased tagger accuracy slightly.
parsing	accuracy	The introduction of definiteness gave the best parsing results, LAS of 87.30% and UAS of 90.42%, and additionally increased tagger accuracy slightly.
tagger	accuracy	The introduction of definiteness gave the best parsing results, LAS of 87.30% and UAS of 90.42%, and additionally increased tagger accuracy slightly.
parsing	LAS	The results in show that number, person and type are the most useful features for parsing, with LAS of 87.21%, 87.22% and 87.19%, respectively.
Tagging	accuracy	 Table 4: Tagging and parsing our development  section of NDT with the two initial tagsets. From  left to right, we report the tagger accuracy of the  most-frequent-tag baseline, the tagger accuracy of  TnT, and the labeled and unlabeled attachment  score for the Mate parser.
SMT	length ratio	There is a tendency that tuning in the same direction as the SMT system performs best, especially as measured by length ratio and Meteor.
translationese	Bleu	In much of the work on translationese, with the exception of, only Bleu () has been used for evaluation.
SMT	Norm	 Table 1: Data normalization percentages for Lev- enshtein (LEV) and SMT. Norm = Normalized.
Strict matching	F1-values	 Table 1: Strict matching results. F1-values are presented in pairs of in-domain training data (left) and  mixed training data (right). Cases where in-domain training data gets the better result are highlighted.
quote extraction	recall	The quote extraction performance of the core method, and of the two extensions, was evaluated on the basis of the testing set (Section 2) with measures of recall and precision.
quote extraction	precision	The quote extraction performance of the core method, and of the two extensions, was evaluated on the basis of the testing set (Section 2) with measures of recall and precision.
quote extraction	recall	The results for quote extraction are presented in, with a best recall of .570 and a best precision of .978.
quote extraction	precision	The results for quote extraction are presented in, with a best recall of .570 and a best precision of .978.
hypernym selection	accuracy	We evaluate hypernym selection according to both accuracy and attachment.
hypernym selection	attachment	We evaluate hypernym selection according to both accuracy and attachment.
parsing	accuracy	Furthermore, while parsing accuracy is measured on test sets of ∼1000 sentences on average, the real input can take a much wider size range.
word selection errors	POS	Further analysis revealed that agreement is especially challenging for word selection errors whose target hypothesis has a different POS.
parsing	accuracy	We examined parsing accuracy in scenarios involving manual and automatic annotations for morphology and lemmas.
POS tagging	accuracy	In the latter setting, POS tagging is conducted with a tagger () with an accuracy of 97.49 when only basic POS is considered.
parsing UD_Greek	Bist-and	 Table 1: Results from parsing UD_Greek and GDT with the Bist-and Mate parsers. UD_Greek contains  63K tokens, a subset of GDT's 178K tokens. (M/A)PL suffixes refer to training and testing on gold  and automatic POS, morphological features and lemmas, respectively. All scores are calculated with  punctuation excluded, on a test partition containing circa 10% of the tokens of each dataset.
parsing	accuracy	Since () many works have shown that well-chosen transformations of syntactic representations can greatly improve the parsing accuracy achieved by dependency parsers.
parsing	accuracy	They highlight that the parsing benefits from the disambiguation of PoS tags for main verbs and auxiliaries in UD PoS tagset even if the overall parsing accuracy decreases.
Improving POS Tagging	TEITOK	Improving POS Tagging in Old Spanish Using TEITOK
MT	accuracy	The neural MT system trained on the baseline combined with showed the best accuracy score, 0.81.
SMT	error similarity	The pair SMT/neural MT leads with 51% of error similarity.
text reuse detection	BLAST	To solve this problem, we have developed a novel text reuse detection solution based on BLAST () that is accurate and resistant to OCR mistakes and other noise, making the text circulation and virality of newspaper publicity in Finland a feasible research question.
role labeling	VN	#BL: for role labeling with oracle senses (ai+ac), PB performs best, VN is around 5 percentage points (pp.) lower, and FN again 5 pp. lower.
role labeling	FN	#BL: for role labeling with oracle senses (ai+ac), PB performs best, VN is around 5 percentage points (pp.) lower, and FN again 5 pp. lower.
role labeling	F1	We report statistical significance of role labeling F1 with expanded data #EX to the respective #BL ( * : p < 0.05; ** : p < 0.01).
role labeling	EX	We report statistical significance of role labeling F1 with expanded data #EX to the respective #BL ( * : p < 0.05; ** : p < 0.01).
role labeling	BL	We report statistical significance of role labeling F1 with expanded data #EX to the respective #BL ( * : p < 0.05; ** : p < 0.01).
sentiment analysis	accuracy	While previous research found no contribution from sentiment analysis to the accuracy on this task, we demonstrate that sentiment is an important aspect.
Alignment	accuracy	Alignment accuracy approaches human performance at 81%, and we show that while lexico-graphic features are most important, the semantic context of an alias further improves classification accuracy.
Fuzzy string matching	Levenstein (edit) distance	Fuzzy string matching uses the Levenstein (edit) distance between the two input strings as the measure of similarity.
GDI task	F1 (micro)	 Table 5: Official results for the GDI task. The  baseline predicts the majority class. For all  classes, F1 (micro) is the same as accuracy.
GDI task	accuracy	 Table 5: Official results for the GDI task. The  baseline predicts the majority class. For all  classes, F1 (micro) is the same as accuracy.
segmentation	POS	We report in terms of CODA spelling, segmentation and POS.
classification	accuracy	These tokens are viewed as functional words that do not usually contribute to classification accuracy.
Subjectivity classification	Th prefixes gold-and  asma-refer	 Table 3: Subjectivity classification with syntactically motivated feature selection. Th prefixes gold-and  asma-refer to Treebank-acquired and ASMA-acquired segments (i.e., -segs), content segments (i.e.,  -cont), and select content segments (i.e., -cont-M * ), respectively.
parsing	TRAIN	For our parsing experiment, we followed the guidelines detailed by, to split the treebanks into TRAIN, DEV, and TEST.
parsing	DEV	For our parsing experiment, we followed the guidelines detailed by, to split the treebanks into TRAIN, DEV, and TEST.
parsing	TEST	For our parsing experiment, we followed the guidelines detailed by, to split the treebanks into TRAIN, DEV, and TEST.
sentiment classifiers	F1-scores	Following the established practice in evaluating sentiment classifiers (, we evaluated using the average of the F1-scores for the positive and the negative classes.
recognition	F1-scores	The figures provided for the recognition are micro-averaged F1-scores.
entity matching	F1-scores	Lastly, for entity matching, the micro-averaged F1-scores are provided, computed using LEA precision and recall values (see Section 4).
entity matching	recall	Lastly, for entity matching, the micro-averaged F1-scores are provided, computed using LEA precision and recall values (see Section 4).
gender prediction	WRB	 Table 1: Results of gender prediction experiments based on tokenized text and on lemmas. Abbrevia- tions: WRB = Weighted Random Baseline, MAJ = Majority Baseline. Precision, Recall and F1-score  are averaged over both classes (since both classes matter).
gender prediction	MAJ	 Table 1: Results of gender prediction experiments based on tokenized text and on lemmas. Abbrevia- tions: WRB = Weighted Random Baseline, MAJ = Majority Baseline. Precision, Recall and F1-score  are averaged over both classes (since both classes matter).
gender prediction	Precision	 Table 1: Results of gender prediction experiments based on tokenized text and on lemmas. Abbrevia- tions: WRB = Weighted Random Baseline, MAJ = Majority Baseline. Precision, Recall and F1-score  are averaged over both classes (since both classes matter).
gender prediction	Recall	 Table 1: Results of gender prediction experiments based on tokenized text and on lemmas. Abbrevia- tions: WRB = Weighted Random Baseline, MAJ = Majority Baseline. Precision, Recall and F1-score  are averaged over both classes (since both classes matter).
gender prediction	F1-score	 Table 1: Results of gender prediction experiments based on tokenized text and on lemmas. Abbrevia- tions: WRB = Weighted Random Baseline, MAJ = Majority Baseline. Precision, Recall and F1-score  are averaged over both classes (since both classes matter).
Coreference resolution	accuracy	Coreference resolution systems are believed to suffer from lack of integration of "deeper" knowledge, with respect to both semantics and world knowledge, while it has been recognized from the very beginning that they make very important and at the same time difficult factors in the process and that present attempts of integration of such features are bringing only small improvements to the overall accuracy (see next section for examples).
pronoun translation from Spanish to English	BLEU	Finally, the results presented in Section 6 show that the second method increases the accuracy of pronoun translation from Spanish to English, while obtaining BLEU scores similar to those of the MT baseline.
MT	BLEU n-gram precision metric	The evaluation of global MT quality is made with the well-known BLEU n-gram precision metric (), while the evaluation of mentions, being less standardized, is performed in several ways.
MT	stan- dard deviation	 Table 3: Comparison of baseline MT and our pro- posals for reranking or post-editing, for three met- rics. In addition to the average scores and stan- dard deviation over the ten test documents, we in- dicate the statistical significance level of the differ- ence between each of our systems and the baseline  (  *  for 95.0%,  *  *  for 99.0% and  *  *  *  for 99.9%).
resolver	CoNLL score	Analysis of the evaluation results show that the resolver for Russian is able to preserve 66% of the English resolver's quality in terms of CoNLL score.
automatic speech recognition (ASR)	word error rate (WER)	The overall accuracy of automatic speech recognition (ASR) has increased substantially over the past decade: a decade ago it was not uncommon to report a ASR error rates of 27% (, while a recent Microsoft system achieved a word error rate (WER) of just 6.3% on the Switchboard corpus (.
VMWE type identification	BOLD	 Table 7: Strict evaluation results for VMWE type identification. Best results in the challenge are BOLD
SMOR-based splitting	precision	However, the SMOR-based splitting system has a higher precision.
parsing	accuracy	Our other hypothesis is that enriching treebanks with explicit annotation of MWE status and MWE POS should help parsing accuracy.
MWE detection	BN	This demonstrates that manual modeling of feature interactions is indeed important for MWE detection, and that BN does a reasonably good job in modeling these interactions.
MT	BLEU4	In our experiments, we wish to contrast the human assessments of the adequacy of translations obtained with two text-only baselines, PBSMT and NMT t , and one multi-modal model NMT m , with scores computed with four automatic MT metrics: BLEU4 (), ME-TEOR (), TER), and chrF3.
MT	ME-TEOR	In our experiments, we wish to contrast the human assessments of the adequacy of translations obtained with two text-only baselines, PBSMT and NMT t , and one multi-modal model NMT m , with scores computed with four automatic MT metrics: BLEU4 (), ME-TEOR (), TER), and chrF3.
MT	TER	In our experiments, we wish to contrast the human assessments of the adequacy of translations obtained with two text-only baselines, PBSMT and NMT t , and one multi-modal model NMT m , with scores computed with four automatic MT metrics: BLEU4 (), ME-TEOR (), TER), and chrF3.
enjambment detection	Precision	 Table 2: Overall enjambment detection results.  Number of test-items (N), Precision, Recall, F1 in  our two test-corpora, for the untyped and typed- match tasks.
enjambment detection	Recall	 Table 2: Overall enjambment detection results.  Number of test-items (N), Precision, Recall, F1 in  our two test-corpora, for the untyped and typed- match tasks.
enjambment detection	F1	 Table 2: Overall enjambment detection results.  Number of test-items (N), Precision, Recall, F1 in  our two test-corpora, for the untyped and typed- match tasks.
POS taggers	annotation quality	For instance, POS taggers used as pre-annotators have been shown to increase both annotation speed and annotation quality, even when trained on limited amounts of data ().
stemming Hittite and German texts prior to annotation projection	accuracy	Our evaluation shows that stemming Hittite and German texts prior to annotation projection largely improves POS tagging accuracy for Hittite as compared to a POS tagger trained on unstemmed projections.
POS tagging	accuracy	Our evaluation shows that stemming Hittite and German texts prior to annotation projection largely improves POS tagging accuracy for Hittite as compared to a POS tagger trained on unstemmed projections.
tagging	accuracy	This baseline reached only 25.4% tagging accuracy.
tagging	accuracy	All in all, the evaluation results show that our stemming approach to data sparsity reduction improves tagging accuracy by a large margin.
Classification	accuracy	 Table 2: Classification accuracy by model. Base- line accuracy of choosing the most common error  type is .510.
Classification	Base- line accuracy	 Table 2: Classification accuracy by model. Base- line accuracy of choosing the most common error  type is .510.
DDI extraction	F-score	Second, the DDI extraction model with the attention mechanism achieves: Overview of our model the performance with an F-score of 69.12% that is competitive with other state-of-the-art DDI extraction models when we compare the performance without negative instance filtering).
NER task	HABITAT	The NER task includes three relevant entity types: HABITAT, BACTERIA and GEOGRAPHICAL, the categorization task focuses on normalizing the mentions to established ontology concepts, although GEOGRAPHICAL entities are excluded from this task, whereas the event extraction aims at finding the relations between these entities, i.e. extracting in which locations certain bacteria live in.
NER task	BACTERIA	The NER task includes three relevant entity types: HABITAT, BACTERIA and GEOGRAPHICAL, the categorization task focuses on normalizing the mentions to established ontology concepts, although GEOGRAPHICAL entities are excluded from this task, whereas the event extraction aims at finding the relations between these entities, i.e. extracting in which locations certain bacteria live in.
NER	F-score	As shows, using previous BB-ST data for training the NER leads to 3pp increase in F-score of (BACTERIA,GEOGRAPHICAL) relations on the development set and about 11pp for the test set, probably due to the drastically increased performance for GEOGRAPHICAL entity detection.
NER	BACTERIA,GEOGRAPHICAL) relations	As shows, using previous BB-ST data for training the NER leads to 3pp increase in F-score of (BACTERIA,GEOGRAPHICAL) relations on the development set and about 11pp for the test set, probably due to the drastically increased performance for GEOGRAPHICAL entity detection.
event extraction	F-score	 Table 6: Combined performance of our named en- tity recognition and event extraction systems on  the event+ner task reported in F-score as measured  by the official evaluation service.
recognizing and normalizing viral species	precision	We achieve 81.0% precision and 72.7% recall at the task of recognizing and normalizing viral species and 76.2% precision and 34.9% recall on viral proteins.
recognizing and normalizing viral species	recall	We achieve 81.0% precision and 72.7% recall at the task of recognizing and normalizing viral species and 76.2% precision and 34.9% recall on viral proteins.
AMR parsing	accuracy	Given that AMR parsing is still a young field, our model, which currently uses a parser of 67% accuracy, would perform better with improved AMR parsers.
relation extraction	ease-of-use	In order to overcome some of the challenges in the relation extraction community in terms of ease-of-use and integration, we present Kindred.
Dementia	prevalence	Dementia is estimated to become a trillion dollar disease worldwide by 2018, and prevalence is expected to double to 74.7 million by.
event extraction classifiers	F-score	We show that the choice of event extraction classifiers increases F-score by up to 20% compared to state-of-the-art system.
septic shock	mortality rate	Severe sepsis and septic shock are conditions that affect millions of patients and have close to 50% mortality rate.
Diagnosis autocoding	accuracy	Diagnosis autocoding is intended to both improve the productivity of clinical coders and the accuracy of the coding.
SRL	F-Measure	For list questions, the SRL approach achieved much higher F-Measure scores than Olelo.
sentence classification	F1-scores	 Table 4: Comparison of SC and NER for sentence classification, for pain corpus test set, evaluated on  micro-averaged F1-scores.
PRA-based	breadth-first	Regarding our PRA-based technique, the breadth-first search is carried out to a depth of 2.
String matching	recall	String matching approaches often result in low recall as well, due to the variations in patterns in the derived and source word pairs, even for the same affix.
MT	errors	Notably, recent studies ( confirm that neural MT () significantly reduces errors, therefore requiring less post-editing.
MT tasks	ADM	Intuitively, for closely aligned document pairs, as prevalent in bilingual alignment or MT tasks, one would expect an ADM value close to 1.
coreference resolution	accuracy	We also check for the effectiveness of attention mechanism in memory networks to aid coreference resolution, through attention mechanism accuracy.
prediction	accuracy	discusses prediction accuracy and attention accuracy with MemN2N and the modifications described in Section 3.1.
prediction	accuracy	discusses prediction accuracy and attention accuracy with MemN2N and the modifications described in Section 3.1.
IR	accuracy	The rest of the paper describes novel ways of encoding dense vectors into text documents, allowing the use of traditional inverted index engines, and explores the trade-offs between IR accuracy and speed.
IR	speed	The rest of the paper describes novel ways of encoding dense vectors into text documents, allowing the use of traditional inverted index engines, and explores the trade-offs between IR accuracy and speed.
trimming	accuracy	To achieve the optimal results, we suggest retrieving as large a set of candidates (Elasticsearch page size, E) as the response-time constraints allow, as the page size seems to have significantly lower influence on the response time compared to trimming, while having a significant positive effect on accuracy.
WSI task	FULL	 Table 3: Effect (in ARI) of language family distance on WSI task. Best results for each column is shown in bold. The  improvement from MONO to FULL is also shown as (3) -(1). Note that this is not comparable to results in Table 2, as we use a  different training corpus to control for the domain.
F2F Relation Prediction task	accuracy	To evaluate the predictions of our systems for the F2F Relation Prediction task, we compare the measurements of accuracy, mean rank of the true relation and hits amongst the 5 first predictions, see.
F2F Relation Prediction task	mean rank	To evaluate the predictions of our systems for the F2F Relation Prediction task, we compare the measurements of accuracy, mean rank of the true relation and hits amongst the 5 first predictions, see.
transfer learning	accuracy	Secondly, we apply a technique called transfer learning to significantly reduce the amount of non-English training data needed to achieve competitive accuracy in an ASR task.
machine translation	BLEU	Many important problems in NLP such as machine translation and abstractive summarization are trained via a maximum-likelihood training objective (, but require the generation of extended sequences and are evaluated based on sequence-level metrics such as BLEU () and ROUGE.
machine translation	ROUGE	Many important problems in NLP such as machine translation and abstractive summarization are trained via a maximum-likelihood training objective (, but require the generation of extended sequences and are evaluated based on sequence-level metrics such as BLEU () and ROUGE.
change detection	accuracy	We show that our method produces large gains in change detection accuracy on both datasets.
Event Detection task	recall	In this work, we investigate a different approach on the Event Detection task, that achieves higher recall by generating a large set of candidate events using a semantic-frame parser.
classification task	PLOT LINK	By observing the results on the classification task, it immediately appears that the textual order of presentation of the information badly correlates with PLOT LINK values.
classification	accuracy	Experiments with the identified features show an improvement in the classification accuracy of influence by 3.15%.
Constructiveness prediction	Random  baseline	 Table 1: Constructiveness prediction results using  argumentation corpora. The test data was our an- notated constructiveness data in all cases. Random  baseline accuracy = 49.44%.
Depression detection	severity	Depression detection can be divided into three different prediction tasks: presence (depressed vs. not depressed), severity (normal, mild, moderate, severe, and very severe), and score level prediction.
detection	accuracy	The neural models significantly outperform the logistic model in detection accuracy, with the best neural model achieving a .80 F1 on the crisis detections, compared to .66 for the logistic model.
detection	F1	The neural models significantly outperform the logistic model in detection accuracy, with the best neural model achieving a .80 F1 on the crisis detections, compared to .66 for the logistic model.
Flexible Attention	BLEU	For Flexible Attention, we search a good τ among (0.8, 1.0, 1.2, 1.4, 1.6) on a development corpus so that the development BLEU(%) does not degrade more than 0.5 compared to τ = ∞.
detection	accuracy	The detection accuracy further improved when ATN and BT were used together.
Translation	BLEU	 Table 2: Translation results (BLEU)
summarization tasks	ROUGE	Similar to the evaluation for traditional summarization tasks, we use the ROUGE metrics ( to automatically evaluate the quality of produced summaries given the gold-standard reference news.
word segmentation	accuracy	In this work the evaluation is based on another popular Chinese word segmentation toolkit called Jieba, 5 that performs word segmentation results with satisfactory level of accuracy, when provided external sports dictionary.
REG evaluation	MASI	Traditional REG evaluation metrics (e.g., and MASI (Passonneau, 2006)) compare algorithm-and human-chosen attributes by measuring the distance (e.g., set difference) between machine-and human-generated attribute sets.
PASS	clarity	Human-based evaluation shows that people are generally positive towards PASS in regards to its clarity and fluency, and that the tailoring is accurately recognized inmost cases.
SRGMs	BLEU-2	Compared to the retrieval based baselines, SRGMs get higher scores in BLEU-2, BLEU-3, and BLEU-4.
SRGMs	BLEU-3	Compared to the retrieval based baselines, SRGMs get higher scores in BLEU-2, BLEU-3, and BLEU-4.
SRGMs	BLEU-4	Compared to the retrieval based baselines, SRGMs get higher scores in BLEU-2, BLEU-3, and BLEU-4.
WMT	BLEU	This model performs particularly strongly on WMT data and is complementary to an RNNLM: combining both yields large BLEU gains even for small beam sizes.
Referring Expression Generation	clarity	Our findings suggest that function words deserve more attention in Referring Expression Generation than they have so far received , and they have a bearing on the debate about whether different languages make different trade-offs between clarity and brevity.
ML	precision	It is obvious, that the ML approach performed significantly better, with regard to precision, recall, and F-score.
ML	recall	It is obvious, that the ML approach performed significantly better, with regard to precision, recall, and F-score.
ML	F-score	It is obvious, that the ML approach performed significantly better, with regard to precision, recall, and F-score.
paraphrase generation	BLEU score	Although no standard metric has proved conclusive for evaluating paraphrase generation, BLEU score has been shown to correlate fairly well with human judgements (Chen and Dolan, 2011), especially when more references are being used.
RST relations	accuracy	RST relations were found to have limited discriminatory power (63% accuracy), due to the latent influence of humour on linguistic profiles of both truths and lies.
deception detection	accuracy	Previous deception detection research on this dataset has built machine learning models utilizing stylometric measures with relatively high accuracy levels of 75%-85% (; Hernández-; this study hypothesized that those observed stylometric differences reviews would manifest as significant variation in the coherence relational structure of fake compared to authentic reviews and qualitative differences in the pragmatic strategies of fake and true review writers.
transliteration modeling	accuracy	We then describe our transliteration modeling approach and a number of WFST optimizations that we perform to achieve the accuracy, latency and memory usage operating points.
Size	LOUDS	 Table 1: Size savings via LOUDS L • G construction.
Translation	merge op- erations (BLEU)	 Table 6: Translation accuracies for Joint BPE  models trained with different number of merge op- erations (BLEU). The Best prev indicates the best  performing units and their accuracy scores from
Translation	accuracy	 Table 6: Translation accuracies for Joint BPE  models trained with different number of merge op- erations (BLEU). The Best prev indicates the best  performing units and their accuracy scores from
prediction	accuracy	Where nothing else is specified, reported numbers are prediction accuracy.
Tagging	accuracy	 Table 2: Tagging accuracy on the WSJ test set.
named-entity recognition (NER)	UAS	They are shown to improve the accuracy of partof-speech (POS) tagging from 97.13 to 97.55, the F1 score of named-entity recognition (NER) from 83.63 to 90.94, and the UAS of dependency parsing from 93.1 to 93.9.
inflection generation	accuracy	Our contributions to the field are as follows: we present a novel and efficient method for tackling the challenge of inflection generation for Spanish verbs using an ensemble of algorithms; we provide a high-quality dataset which includes inflection rules of Spanish verbs for all the grammatical moods (i.e. indicative, subjunctive and imperative, being this last one do not tackled by the current approaches); our models are trained with fewer resources than the state-of-art methods; and finally, our method outperforms the state-of-the-art methods achieving a 2% higher accuracy.
SVM-based classification	accuracy	We compare an SVM-based classification method with a Feed Forward Neural Network (FFNN), and find that the best accuracy of 93.5% is achieved by the FFNN.
topic modelling	Latent Dirichlet Allocation (LDA)	We therefore propose a topic modelling method to generate topics using Latent Dirichlet Allocation (LDA), and then cluster the articles into groups with similar topics.
stacking ensemble	accuracy	We show how we used the stacking ensemble method for this purpose and obtained improvements in classification accuracy exceeding each of the individual models' performance on the development data.
SVM classifier	recall	They train an SVM classifier on a skewed dataset containing 94 NYT Picks and 5,174 nonpicks and achieve a cross-validation precision of 0.13 and recall of 0.60.
Classification	accuracy	 Table 5: Classification accuracy of BOW unigram approach.
Emote prediction	Precision	 Table 3: Detailed results for each class in the  Emote prediction experiment. We report the re- sults of the B-LSTMs model. We report Precision,  Recall, F-Measure, Rank and thousand of occur- rences in the Test (Te) and in the Train (Tr) for  each emote.
Emote prediction	Recall	 Table 3: Detailed results for each class in the  Emote prediction experiment. We report the re- sults of the B-LSTMs model. We report Precision,  Recall, F-Measure, Rank and thousand of occur- rences in the Test (Te) and in the Train (Tr) for  each emote.
Emote prediction	F-Measure	 Table 3: Detailed results for each class in the  Emote prediction experiment. We report the re- sults of the B-LSTMs model. We report Precision,  Recall, F-Measure, Rank and thousand of occur- rences in the Test (Te) and in the Train (Tr) for  each emote.
Emote prediction	Rank	 Table 3: Detailed results for each class in the  Emote prediction experiment. We report the re- sults of the B-LSTMs model. We report Precision,  Recall, F-Measure, Rank and thousand of occur- rences in the Test (Te) and in the Train (Tr) for  each emote.
tagging	accuracy	Thus little is known on how normalization impacts tagging accuracy in a realworld scenario (not assuming gold error detection).
identifying unnatural language blocks in plain text	TABLE	This paper proposes anew approach for identifying unnatural language blocks in plain text into four types of categories: (1) TABLE (2) CODE (3) MATH-EMATICAL FORMULA, and (4) MISCELLANEOUS (MISC).
identifying unnatural language blocks in plain text	CODE	This paper proposes anew approach for identifying unnatural language blocks in plain text into four types of categories: (1) TABLE (2) CODE (3) MATH-EMATICAL FORMULA, and (4) MISCELLANEOUS (MISC).
identifying unnatural language blocks in plain text	MATH-EMATICAL	This paper proposes anew approach for identifying unnatural language blocks in plain text into four types of categories: (1) TABLE (2) CODE (3) MATH-EMATICAL FORMULA, and (4) MISCELLANEOUS (MISC).
Multi-domain classification	Identifica- tion	 Table 4: Multi-domain classification improves the  single-domain classification in Table 3. Identifica- tion of categories with particularly low accuracy in  each datasets (TABLE and FORMULA in T SLIDES  and CODE in T ACL ) are improved to be as good as  the other categories.
Multi-domain classification	accuracy	 Table 4: Multi-domain classification improves the  single-domain classification in Table 3. Identifica- tion of categories with particularly low accuracy in  each datasets (TABLE and FORMULA in T SLIDES  and CODE in T ACL ) are improved to be as good as  the other categories.
Multi-domain classification	TABLE	 Table 4: Multi-domain classification improves the  single-domain classification in Table 3. Identifica- tion of categories with particularly low accuracy in  each datasets (TABLE and FORMULA in T SLIDES  and CODE in T ACL ) are improved to be as good as  the other categories.
Multi-domain classification	FORMULA	 Table 4: Multi-domain classification improves the  single-domain classification in Table 3. Identifica- tion of categories with particularly low accuracy in  each datasets (TABLE and FORMULA in T SLIDES  and CODE in T ACL ) are improved to be as good as  the other categories.
Single-domain Classification Result	F1-score	 Table 3: Single-domain Classification Result in F1-score: Proposed method is much better than baselines  for classifying unnatural language. Note that we borrowed the F1-score reported on their dataset for  reference. The number is not directly comparable to other numbers since the datasets are different.
Single-domain Classification Result	F1-score	 Table 3: Single-domain Classification Result in F1-score: Proposed method is much better than baselines  for classifying unnatural language. Note that we borrowed the F1-score reported on their dataset for  reference. The number is not directly comparable to other numbers since the datasets are different.
summarization)	recall	Named entities form the basis of many modern approaches to other tasks (like event clustering and summarization), but recall on them is areal problem in noisy text-even among annotators.
NER	recall	Another challenge of NER in noisy text is the fact that there are large amounts of emerging named entities and rare surface forms among the user-generated text, which tend to be tougher to detect) and recall thus is a significant problem.
named entity recognition shared task	precision	5-fold cross-validation based on training and development data for the emerging and rare named entity recognition shared task showed precision, recall and F 1-score of 66.87%, 46.75% and 54.97%, respectively.
named entity recognition shared task	recall	5-fold cross-validation based on training and development data for the emerging and rare named entity recognition shared task showed precision, recall and F 1-score of 66.87%, 46.75% and 54.97%, respectively.
named entity recognition shared task	F 1-score	5-fold cross-validation based on training and development data for the emerging and rare named entity recognition shared task showed precision, recall and F 1-score of 66.87%, 46.75% and 54.97%, respectively.
evaluation	gold standard	In particular, the question of evaluation is notably difficult due to the inherent lack of gold standard.
summarization	ROUGE	Experimental results show that this step improves summarization performance measured by ROUGE) and BLEU ().
summarization	BLEU	Experimental results show that this step improves summarization performance measured by ROUGE) and BLEU ().
DST	ASR errors	We show in this paper that DST suffers from ASR errors, which was also noted by.
SWB	scoring mean	For SWB the problem is more acute, with 25.5% of units at least 11 tokens long and scoring mean UAS 50% or less.
ASR Lattices	POS Tags	Enriching ASR Lattices with POS Tags for Dependency Parsing
WSD	BLEU scores	Our results, presented in Section 5, show first that our WSD system is competitive on the SemEval 2010 WSD task, but especially that it helps SMT to increase its BLEU scores and to improve the translation of polysemous nouns and verbs, when translating from English into Chinese, German, French, Spanish or Dutch, in comparison to an SMT baseline that is not aware of word senses.
SMT	BLEU scores	Our results, presented in Section 5, show first that our WSD system is competitive on the SemEval 2010 WSD task, but especially that it helps SMT to increase its BLEU scores and to improve the translation of polysemous nouns and verbs, when translating from English into Chinese, German, French, Spanish or Dutch, in comparison to an SMT baseline that is not aware of word senses.
MT	BLEU	We select the optimal model configuration based on the MT performance, measured with the traditional BLEU score (), on the WIT3 corpus for EN/ZH and EN/DE.
WSD	F 1 -metric	For the evaluation of intrinsic WSD performance, we use the V -metric, the F 1 -metric, and their average, as used for instance at SemEval 2010 ( . To measure the impact of WSD on MT, besides BLEU, we also measure the actual impact on the nouns and verbs that appear in WordNet with several senses, by comparing how many of them are translated as in the reference translation, by our system vs. the baseline.
WSD	BLEU	For the evaluation of intrinsic WSD performance, we use the V -metric, the F 1 -metric, and their average, as used for instance at SemEval 2010 ( . To measure the impact of WSD on MT, besides BLEU, we also measure the actual impact on the nouns and verbs that appear in WordNet with several senses, by comparing how many of them are translated as in the reference translation, by our system vs. the baseline.
MT	BLEU	For the evaluation of intrinsic WSD performance, we use the V -metric, the F 1 -metric, and their average, as used for instance at SemEval 2010 ( . To measure the impact of WSD on MT, besides BLEU, we also measure the actual impact on the nouns and verbs that appear in WordNet with several senses, by comparing how many of them are translated as in the reference translation, by our system vs. the baseline.
translation	BLEU	For Czech, this has nearly no impact on translation quality according to the metrics, whereas it provides an important improvement in Latvian: +2.03 and +0.84 BLEU in the split cluster setup.
MT	BLEU	Furthermore, the steady progress of MT engines makes automatic metrics such as BLEU () or METEOR () less appropriate to evaluate and compare modern NMT systems.
MT	METEOR	Furthermore, the steady progress of MT engines makes automatic metrics such as BLEU () or METEOR () less appropriate to evaluate and compare modern NMT systems.
predicting the morphology of closed class words	accuracy	However, when it comes to predicting the morphology of closed class words, this systems performs much better: the accuracy computed for pronoun gender and number is similar to the ones of best BPE-based systems.
NMT	BLEU	We demonstrate that linguistically informed target word seg-mentation is better suited for NMT, leading to improved translation quality on the order of magnitude of +0.5 BLEU and −0.9 TER fora medium-scale English→German translation task.
NMT	TER	We demonstrate that linguistically informed target word seg-mentation is better suited for NMT, leading to improved translation quality on the order of magnitude of +0.5 BLEU and −0.9 TER fora medium-scale English→German translation task.
translation	BLEU	Experiments show that the translation quality can be improved by up to 1.5 BLEU points under the low-resource condition.
machine translation task	BLEU	 Table 1:  Results of multi-task learning architectures on the machine translation task  (BLEU/BEER/characTER)
machine translation task	BEER	 Table 1:  Results of multi-task learning architectures on the machine translation task  (BLEU/BEER/characTER)
machine translation task	BLEU	 Table 2: Impact of the training schedule in the machine translation task (BLEU/BEER/characTER)
machine translation task	BEER	 Table 2: Impact of the training schedule in the machine translation task (BLEU/BEER/characTER)
statistical machine translation (SMT)	accuracy	In statistical machine translation (SMT), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy.
Translation	BLEU	 Table 4: Translation results for the XRCE and IT tasks. BLEU and TER results given in percentage.  Σ denotes an ensemble of 4 neural models. |W | is the average number of words per sentence.
Translation	TER	 Table 4: Translation results for the XRCE and IT tasks. BLEU and TER results given in percentage.  Σ denotes an ensemble of 4 neural models. |W | is the average number of words per sentence.
Translation	‡	 Table 2: Translation performance in BLEU with and without copied monolingual data. Statistically  significant differences are marked with  † (p < 0.01) and  ‡ (p < 0.05).
MT output	TER	To this aim, we first aligned each MT output with the corresponding human post-edit using TER).
WMT17 News translation task	standardized mean DA score	 Table 7: Official results of WMT17 News translation task. Systems ordered by standardized mean DA score, though systems  within a cluster are considered tied. Lines between systems indicate clusters according to Wilcoxon rank-sum test at p-level  p ≤ 0.05. Systems with gray background indicate use of resources that fall outside the constraints provided for the shared task.
Translation	TER	 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16,  WMT17 EN-DE and WMT17 DE-EN data.
Translation	TER	 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16,  WMT17 EN-DE and WMT17 DE-EN data.
WMT17 English-German Multimodal Translation task	standardized mean DA scores (z)	 Table 9: Results of the human evaluation of the WMT17 English-German Multimodal Translation task  (Multi30K 2017 test data). Systems are ordered by standardized mean DA scores (z) and clustered  according to Wilcoxon signed-rank test at p-level p ≤ 0.05. Systems within a cluster are considered tied,  although systems within a cluster may be statistically significantly different from each other (see
WMT17 English-French Multimodal Translation  task	standardized mean DA score (z)	 Table 10: Results of the human evaluation of the WMT17 English-French Multimodal Translation  task (Multi30K 2017 test data). Systems are ordered by standardized mean DA score (z) and clustered  according to Wilcoxon signed-rank test at p-level p ≤ 0.05. Systems within a cluster are considered  tied, although systems within a cluster may be statistically significantly different from each other (see
SMT	BLEU	Amongst other aspects, SMT research at LMU is Our LMU Munich primary system is ranked second in BLEU on the submission website, http://matrix.
SMT	TER	The sample was chosen from sentences where SMT has a sentence-level TER that is at least 10 points lower than the corresponding NMT TER, since such differences can indicate evaluation errors.
MT	BLEU	 Table 7: Statistical MT for English-Latvian tested  on newstest 2017 (lowercased BLEU). The offi- cial score in the on-line evaluation system (low- ercased) is surprisingly different from our own  evaluations. The manual evaluation for English- Latvian produced no statistically significant rank- ing.
SMT	BLEU	 Table 6: Automatic evaluation results of Tilde's systems (CS stands for case sensitive evaluation; the  results are significant compared to the SMT system with p = 0.01 †; the BLEU scores are given with a  95% confidence interval that was calculated using bootstrap resampling
translation model training	BPE	To accomplish this, we performed translation model training, created adaptations of training settings for each language pair, and implemented BPE (subword units) for our SMT systems.
translating from En-glish	METEOR	Our submissions ranked 3rd best for translating from En-glish into French, always improving considerably over an neural machine translation baseline across all language pair evaluated , e.g. an increase of 7.0-9.2 METEOR points.
translation	BLEU4	We evaluate translation quality quantitatively in terms of BLEU4 (), METEOR, and TER ().
translation	METEOR	We evaluate translation quality quantitatively in terms of BLEU4 (), METEOR, and TER ().
translation	TER	We evaluate translation quality quantitatively in terms of BLEU4 (), METEOR, and TER ().
MT evaluation	BLEU	This highly portable and open source semantic MT evaluation metric is a more accurate alternative to BLEU in evaluating translation quality for low-resource languages.
MT	Blend 2	The recent development inhuman evaluation of MT motivates us to propose anew combined metric, named as Blend 2 , by adopting DA, as opposed to RR, to guide the training process indicating that a more reliable gold standard can lead to more reliable results even with less training data.
MT	DA	The recent development inhuman evaluation of MT motivates us to propose anew combined metric, named as Blend 2 , by adopting DA, as opposed to RR, to guide the training process indicating that a more reliable gold standard can lead to more reliable results even with less training data.
MT	RR	The recent development inhuman evaluation of MT motivates us to propose anew combined metric, named as Blend 2 , by adopting DA, as opposed to RR, to guide the training process indicating that a more reliable gold standard can lead to more reliable results even with less training data.
MT	TER	For the latter, where the higher quality of the original MT output reduces the room for improvement, the gains are lower but still significant (-0.25 TER and +0.3 BLEU).
MT	BLEU	For the latter, where the higher quality of the original MT output reduces the room for improvement, the gains are lower but still significant (-0.25 TER and +0.3 BLEU).
tagging	accuracy	Overall tagging accuracy is included as a secondary metric for QE.
domain adaptation	BLEU	data selection for domain adaptation alone improves translation quality by about 1.5 BLEU points.
translation	BLEU	data selection for domain adaptation alone improves translation quality by about 1.5 BLEU points.
MT	MERT	First, the parameters of the MT system must be estimated without knowing the reference translation which rules out most of the usual optimization methods for MT such as MERT, MIRA or the computation of likelihood at the heart of NMT systems.
MT	MIRA	First, the parameters of the MT system must be estimated without knowing the reference translation which rules out most of the usual optimization methods for MT such as MERT, MIRA or the computation of likelihood at the heart of NMT systems.
MT	MERT	First, the parameters of the MT system must be estimated without knowing the reference translation which rules out most of the usual optimization methods for MT such as MERT, MIRA or the computation of likelihood at the heart of NMT systems.
MT	MIRA	First, the parameters of the MT system must be estimated without knowing the reference translation which rules out most of the usual optimization methods for MT such as MERT, MIRA or the computation of likelihood at the heart of NMT systems.
MT evaluation	accuracy	As often with MT evaluation issues at the semantic and discourse levels, measuring the accuracy of pronoun translation was found difficult, due to the interplay between the translation of pronouns and of their antecedents, and to variations in the use of non-referential pronouns.
classification	accuracy	They report classification results using a leave-one-out strategy over the dataset, with a classification accuracy of 57.2%.
authorship attribution	accuracy	This would demonstrate that authorship attribution (in a probabilistic sense) is possible based on stylometric features alone, but not to the same level of accuracy as when content clues are used as well.
PTT analysis	Answer variation	Once test-taker answers are available, but before grading makes PTT analysis possible, another predictor for question difficulty becomes available: Answer variation, the average amount of variation within the student answers for each question, is computed based only on the answer strings.
pattern matching	accuracy	While pattern matching with ChatScript can achieve relatively high accuracy with sufficient pattern-writing skill and effort, it is unable to take advantage of large amounts of training data, somewhat brittle regarding misspellings, and difficult to maintain as new questions and patterns are added.
prediction	accuracy	The fusion track showed that combining the written and spoken responses provides a large boost in prediction accuracy.
RF	precision	With the exception of Software Manuals, RF is able to predict roles with more samples (Reference Work, Empirical Results, Other) with higher precision compared to roles with fewer samples.
SRA	F1	 Table 4: Test set results for all datasets across prompts. Scores for ASAP-SAS and PG are QWK.  Mean Fisher is the Fisher-transformed mean QWK used in the ASAP-SAS competition. Scores for SRA  are weighted F1 scores.
parsing	accuracy	For well over a decade, the field was heavily focused on improving parsing accuracy on the Penn Treebank), but robustness was greatly improved with the advent of Ontonotes () and the Google Web Treebank (Petrov and: Metric scores of three artificially contrived systems (Game), input source sentences (Src), and top 3 system outputs (Sys) on CoNLL14 data.
Stance classification	Majority baseline	 Table 2: Stance classification results. Majority baseline Accuracy@1.0=51.9%
Stance classification	Accuracy	 Table 2: Stance classification results. Majority baseline Accuracy@1.0=51.9%
Stance classification	F1- score	 Table 4: Stance classification macro-averaged F1- score using segments (seg), paraphrases (par), and  microstructures (ms) as features. The best result in  each group is shown in boldface.
Appraisal theory	recall	The Goals section of shows that Appraisal theory does well at predicting positive events, but performs poorly for negative events, primarily due to low recall.
validation	F1-score	We used exponential decay of ratio 0.9 and early stopping on the validation when there was no improvement in the F1-score after 1000 training steps.: Best results for our ARNN on AE for the Laptops dataset.
validation	AE	We used exponential decay of ratio 0.9 and early stopping on the validation when there was no improvement in the F1-score after 1000 training steps.: Best results for our ARNN on AE for the Laptops dataset.
NMI prediction task	Mean Absolute Error (MAE)	To evaluate the performance of our NMI prediction task we employ the commonly used Mean Absolute Error (MAE) as our metric.
naming task	onset	Four conditions are considered: LDT-200, LDT-1200, NT-200, NT-1200 (for lexical decision task, and naming task, both with an onset of 200 ms and 1,200 ms).
utterance intent classification	accuracy	The experimental result of the utterance intent classification showed an improved accuracy with the proposed method compared with the basic tied RAE and untied RAE based on a manual rule.
RQs	F1	In this study, we first show that RQs can clearly be distinguished from sincere, informationseeking questions (0.76 F1).
knocking	PHYSICAL	Nor was it entailed that turning would cause knocking, which would have been PHYSICAL, because he clearly could have missed hitting his head if he had been more careful.
MT	BLEU score	When it comes to commercialising MT, ensuring that these expectations are met is as important as improvements in BLEU score.
Translation	accuracy	Translation accuracy is measured by BLEU ().
Translation	BLEU	Translation accuracy is measured by BLEU ().
Translation	accuracy	 Table 6: Translation accuracy of prefix constraint  prediction and prefix-constrained decoding
Sorting	sent-BLEU score	Sorting was performed using sent-BLEU score (Bootstrap 1).
Sorting	Bootstrap 1	Sorting was performed using sent-BLEU score (Bootstrap 1).
SMT	BLEU	On the WAT 2017 Japanese-Chinese JPO patent dataset, when compared with the baseline SMT, the performance gains of the NMT model of  are approximately 5.6 BLEU points when translating Japanese into Chinese and 5.4 BLEU when translating Chinese into Japanese.
SMT	BLEU	On the WAT 2017 Japanese-Chinese JPO patent dataset, when compared with the baseline SMT, the performance gains of the NMT model of  are approximately 5.6 BLEU points when translating Japanese into Chinese and 5.4 BLEU when translating Chinese into Japanese.
SMT	BLEU	On the WAT 2017 Japanese-English JPO patent dataset, when compared with the baseline SMT, the performance gains of the NMT model of  are approximately 15.9 BLEU points when translating Japanese into English and 13.1 BLEU when translating English into Japanese.
SMT	BLEU	On the WAT 2017 Japanese-English JPO patent dataset, when compared with the baseline SMT, the performance gains of the NMT model of  are approximately 15.9 BLEU points when translating Japanese into English and 13.1 BLEU when translating English into Japanese.
SMT	IMPACT	We compare n-best NMT outputs with a SMT output by the measure of IMPACT which is one of the automatic evaluation measure of machine translation results.
translation	accuracy	According to the official results, our system achieves higher translation accuracy than any systems submitted previous campaigns despite simple model architecture.
translation	accuracy	Results show that our system achieves higher translation accuracy than any systems submitted in previous WAT campaigns.
detecting outbreaks	delay	More timely methods of detecting outbreaks could reduce the delay of disease control measures, particularly important during the early hours and days of an outbreak.
early detection and identification of outbreaks	validity	It has shown increasing promise in the early detection and identification of outbreaks, despite concerns regarding the validity and accuracy of disease predictions ().
early detection and identification of outbreaks	accuracy	It has shown increasing promise in the early detection and identification of outbreaks, despite concerns regarding the validity and accuracy of disease predictions ().
NE recognition	F1-score	As a result, our system outperforms biomedical NE recognition system that using MetaMap only with 26.03%p improvements on F1-score.
NE recognition	precision	To evaluate the quality of machine-labeled data, we evaluated biomedical NE recognition systems that are trained with the machine-labeled data of each self-training iteration with precision, recall, and f1-score.
NE recognition	recall	To evaluate the quality of machine-labeled data, we evaluated biomedical NE recognition systems that are trained with the machine-labeled data of each self-training iteration with precision, recall, and f1-score.
relation detection	precision	We evaluated the performance of relation detection in terms of the precision, recall, and the F 1 -score.
relation detection	recall	We evaluated the performance of relation detection in terms of the precision, recall, and the F 1 -score.
relation detection	F 1 -score	We evaluated the performance of relation detection in terms of the precision, recall, and the F 1 -score.
relation detection	precision	 Table 1. We evaluated the  performance of relation detection in terms of the  precision, recall, and the F 1 -score. The F 1 -score  is the harmonic mean of the precision and recall,  and is often selected to determine the overall ef- fectiveness of a system.
relation detection	recall	 Table 1. We evaluated the  performance of relation detection in terms of the  precision, recall, and the F 1 -score. The F 1 -score  is the harmonic mean of the precision and recall,  and is often selected to determine the overall ef- fectiveness of a system.
relation detection	F 1 -score	 Table 1. We evaluated the  performance of relation detection in terms of the  precision, recall, and the F 1 -score. The F 1 -score  is the harmonic mean of the precision and recall,  and is often selected to determine the overall ef- fectiveness of a system.
relation detection	F 1 -score	 Table 1. We evaluated the  performance of relation detection in terms of the  precision, recall, and the F 1 -score. The F 1 -score  is the harmonic mean of the precision and recall,  and is often selected to determine the overall ef- fectiveness of a system.
relation detection	precision	 Table 1. We evaluated the  performance of relation detection in terms of the  precision, recall, and the F 1 -score. The F 1 -score  is the harmonic mean of the precision and recall,  and is often selected to determine the overall ef- fectiveness of a system.
relation detection	recall	 Table 1. We evaluated the  performance of relation detection in terms of the  precision, recall, and the F 1 -score. The F 1 -score  is the harmonic mean of the precision and recall,  and is often selected to determine the overall ef- fectiveness of a system.
MST  parser	Precision	 Table 3: We measure the performance of the MST  parser using the following performance measures:  Precision, Recall, and F1 measure.
MST  parser	Recall	 Table 3: We measure the performance of the MST  parser using the following performance measures:  Precision, Recall, and F1 measure.
MST  parser	F1 measure	 Table 3: We measure the performance of the MST  parser using the following performance measures:  Precision, Recall, and F1 measure.
event detection task	precision	For the event detection task, the proposed algorithm achieves precision, recall and F1-measure of 0.647, 0.407 and 0.500 respectively.
event detection task	recall	For the event detection task, the proposed algorithm achieves precision, recall and F1-measure of 0.647, 0.407 and 0.500 respectively.
event detection task	F1-measure	For the event detection task, the proposed algorithm achieves precision, recall and F1-measure of 0.647, 0.407 and 0.500 respectively.
MT	BLEU	The training data  All the MT systems are tuned by the development set (1,000 sentences) using ZMERT with the objective to optimize BLEU ().
MWE-based	accuracy	We showed that considerable speed-up gains can be achieved by promoting MWE-based analyses with virtually no loss in syntactic parsing accuracy.
parsing	accuracy	For Chinese joint analysis, where the parsing accuracy of a baseline pipeline model is around 80%, an F1 improvement of around 2% was reported ().
parsing	F1	For Chinese joint analysis, where the parsing accuracy of a baseline pipeline model is around 80%, an F1 improvement of around 2% was reported ().
parsing	accuracy	For Japanese joint analysis, where the parsing accuracy of a pipeline model is around 90%, there have been no studies that report a significant improvement (.
segmentation	accuracy	This is reasonable because the segmentation accuracy of JU-MAN++ is between 98%-99% and its N-best output contains only plausible words.
parsing	accuracy	3. We used DLMs in joint tagging and parsing, and gained up to 0.4% on tagging accuracy.
POS taggers	accuracy	When the data for training POS taggers is small -as for the small languages scenario as well as for many upcoming UD treebanks 3 -the delexicalized methods might be affected by poor POS accuracy.
PP-attachment resolution problem	accuracy	Our contributions in this study are the selection and the manual annotation of a corpus of ambiguous PP-attachments from the multimodal corpus Flickr30k Entities (; the study of the relative importance of different kinds of features for the PP-attachment resolution problem, from very specific ones (lexical features) to very generic ones (spatial features); and the combination of them in a single model for improving the accuracy of a syntactic dependency parser.
classification	accuracy	Similarly as the results obtained on the classification accuracy, textual features are the most useful ones.
dependency parsing	accuracy	Our primary evaluation is based on dependency parsing, where we evaluate parsing accuracy using different pre-trained word embeddings during parser training.
parsing	accuracy	In the paper, we ask what exactly causes the decrease in parsing accuracy when training a parser on UD-style annotations and whether the effect is similarly strong for all languages.
parsing	accuracy	We show that the encoding in the UD scheme, in particular the decision to encode content words as heads, causes an increase in dependency length for nearly all treebanks and an increase in arc direction entropy for many languages, and evaluate the effect this has on parsing accuracy.
parsing	accuracy	We further show that the changes in dependency length that result from the different encoding styles are not responsible for the changes in parsing accuracy.
Semantic Role Labeling (SRL)	agility	We demonstrate through experimental evaluations that the application of the Semantic Role Labeling (SRL) techniques and Natural Language Processing (NLP) in digital forensic increases the performance of the forensic experts in terms of agility, precision and recall.
Semantic Role Labeling (SRL)	precision	We demonstrate through experimental evaluations that the application of the Semantic Role Labeling (SRL) techniques and Natural Language Processing (NLP) in digital forensic increases the performance of the forensic experts in terms of agility, precision and recall.
Semantic Role Labeling (SRL)	recall	We demonstrate through experimental evaluations that the application of the Semantic Role Labeling (SRL) techniques and Natural Language Processing (NLP) in digital forensic increases the performance of the forensic experts in terms of agility, precision and recall.
STS	Pearson ρ)	 Table 3: STS performance on the MSRVID and NEWS-16 datasets (Pearson ρ).
Concept extraction	recall	 Table 2: Concept extraction performance by dataset. For a definition of the metrics, please refer to the  text. Bold indicates best recall per group. Concept length given as average number of tokens.
fine-grained domain classification of texts	accuracy	Our research was stimulated by the gap mentioned above, and our contribution, highlighted in this paper, is both to provide annotated datasets for fine-grained domain classification of texts, and a classification method that achieves high accuracy.
image retrieval	Recall@K (R@K)	To evaluate both image annotation and image retrieval we use the following metrics: • Recall@K (R@K) is the fraction of images for which a correct caption is ranked within the top-K retrieved results (and vice-versa for sentences).
AMR parsing	reentrancy	That there is a lotto gain in this area can be seen by applying the AMR evaluation suite of, which calculates nine different metrics to evaluate AMR parsing, reentrancy being one of them.
Classification	accuracy	 Table 3: Classification results in terms of accuracy (in %) obtained on collected 'original' human-human and
Robustness comparison	BLEU	 Table 6: Robustness comparison of models over different domains (in terms of BLEU scores)
MT	accuracy	We also describe the challenges in evaluating MT accuracy keeping this language pair in consideration, however it should be noted that the same or similar challenges are faced when dealing with other language pairs as well.
POS taggers	accuracy	Statistical POS taggers trained on Hindi Treebank data for Hindi achieved around 93% accuracy (.
MT	TAG	Note that each run of MT takes a week of training, LM takes a day and a half and TAG and LEM need several hours each.
MT	LEM	Note that each run of MT takes a week of training, LM takes a day and a half and TAG and LEM need several hours each.
NE identification	accuracy	The overall NE identification accuracy reported by the system for held out dataset1 and 2 is 93.35% and 98.14% respectively.
NE classification	accuracy	The average NE classification accuracy reported is 95.24% and 97.79% respectively.
NE identification	accuracy	The overall NE identification accuracy reported by the system for unseen dataset1 and 2 is 81.37% and 83.33% respectively which is relatively satisfactory.
NE classification	accuracy	The average NE classification accuracy reported for unseen dataset1 and 2 is 83.09% and 84.23% respectively.
novelty detection	TE	In this work we take forward this view to investigate novelty detection at the document level via TE with emphasis to textual similarity measures.
PoS tagging	accuracy	The performance of this tool for PoS tagging is 98.26% accuracy and for chunking, the F-score obtained is 88.9% for Noun Phrases and 95.2% for Verb phrases).
PoS tagging	F-score	The performance of this tool for PoS tagging is 98.26% accuracy and for chunking, the F-score obtained is 88.9% for Noun Phrases and 95.2% for Verb phrases).
chunking	F-score	The performance of this tool for PoS tagging is 98.26% accuracy and for chunking, the F-score obtained is 88.9% for Noun Phrases and 95.2% for Verb phrases).
Salience	Weights	 Table 1 Salience Factors and its Weights
syntactic parsing	F-score	Research in syntactic parsing is largely driven by progress in intrinsic evaluation and there have been impressive developments in recent years in terms of evaluation measures, such as F-score or labeled accuracy.
MT	POS	Based on a quick inspection of the data, we currently hypothesise that disallowing reordering forces the MT system to produce more literal translations, which better preserve the sentence structure (POS and dependency relations).
poem generation	accuracy	However, in other NLG tasks outside of the scope of poem generation, the objects might be important and thus having a higher accuracy is something to work towards.
word alignment	Alignment Error Rate (AER)	An advantage of the word alignment task is that we can straightforwardly quantify the results via the Alignment Error Rate (AER), defined as where P is the number of predicted alignments that are correct, c is the number of alignments in the gold standard, and p is the number of predicted alignments (where an alignment is defined as a connection between one Spanish word and one English word).
pattern matching	accuracy	With sufficient pattern-writing skill and effort, pattern matching with ChatScript can achieve relatively high accuracy, but it is unable to easily leverage increasing amounts of training data, somewhat brittle regarding misspellings, and can be difficult to maintain as new questions and patterns are added.
interpretability	accuracy	While this offers interpretability, these models have lower accuracy for shorter texts.
revision assistant	recall	However, for our long-term goal of building an effective revision assistant tool, intuitively we will  also need to identify 'NotBetter' revisions with higher recall, which is very low for this model.
ATS	consistency	The goal of ATS is to improve consistency and reduce human resource overheads.
Cross-language classification	F-score	 Table 5: Cross-language classification: F-score for complex word class / accurracy for cross-language classifica- tion.
predicting student	recall	It had demonstrated state-ofart performance for predicting student recall rates.
RTA	MUGS	Instead of evaluating holistic writing skills, the RTA was designed to evaluate students' writing skills along five dimensions: Analysis, Evidence, Organization, Style, and MUGS (Mechanics, Usage, Grammar, and Spelling).
Multiclass classification evaluation	F-measure	 Table 2: Multiclass classification evaluation results (we indicate the percentage of posts belonging to a class in the  sample; ¯ l doc refers to average document length in sentences; ¯ l sent -average sentence length in tokens; FM refers  to F-measure; PR -to precision; RC -to recall; )
predicting distress	Disatten-uated Pearson Correlation	Our approach showed best results for predicting distress at the age of 42 and for predicting current anxiety on Disatten-uated Pearson Correlation, and ranked fourth in the future health prediction task.
predicting current anxiety	Disatten-uated Pearson Correlation	Our approach showed best results for predicting distress at the age of 42 and for predicting current anxiety on Disatten-uated Pearson Correlation, and ranked fourth in the future health prediction task.
BSAG	anxiety BSAG score	Given the written essays and social control variables (gender and social class), CLPsych participants are to predict three types of BSAG scores: (i) total BSAG score, (ii) the depression BSAG score, and (iii) the anxiety BSAG score.
coreference	accuracy	[One hospital] in Ramallah tells us have treated seven people While previous studies have focused on SV agreement (den, there have been few corpus studies of notional pronouns, due at least in part to the lack of sizable corpora reliably annotated for coreference, and the low accuracy of automatic systems on difficult cases.
bridging resolution	precision	For bridging resolution, we report performance in precision, recall and F1.
bridging resolution	recall	For bridging resolution, we report performance in precision, recall and F1.
bridging resolution	F1	For bridging resolution, we report performance in precision, recall and F1.
bridging resolution	precision	The evaluation of bridging resolution is computed using the widely known precision and recall measures (and the harmonic mean between them, F1).
bridging resolution	recall	The evaluation of bridging resolution is computed using the widely known precision and recall measures (and the harmonic mean between them, F1).
bridging resolution	F1	The evaluation of bridging resolution is computed using the widely known precision and recall measures (and the harmonic mean between them, F1).
anaphor detection	GRAIN	 Table 3: Baseline results for anaphor detection and full  bridging resolution on the test set of GRAIN.
classification	precision	Since the first question concerns classification performance, I use precision, recall, and F 1 score to answer this question.
classification	recall	Since the first question concerns classification performance, I use precision, recall, and F 1 score to answer this question.
classification	F 1 score	Since the first question concerns classification performance, I use precision, recall, and F 1 score to answer this question.
binary classification task	Pearson (.75) and Spearman (.68) correlations	It reaches an encouraging 75% accuracy on the binary classification task, and high Pearson (.75) and Spearman (.68) correlations on the gradient judgment prediction task.
Sentence Retrieval	TF-IDF overlap	We show in subsequent sections that models which perform well on SQuAD rely on lexical pattern matching, and are also not robust to variance in language style.: Sentence Retrieval Performance using Jaccard similarity, TF-IDF overlap and BM-25 overlap) scoring metrics
Sentence Retrieval	BM-25 overlap) scoring metrics	We show in subsequent sections that models which perform well on SQuAD rely on lexical pattern matching, and are also not robust to variance in language style.: Sentence Retrieval Performance using Jaccard similarity, TF-IDF overlap and BM-25 overlap) scoring metrics
politeness detection	accuracy	We also evaluate our model for formality and politeness detection and report comparable accuracy as against the state-of-the-art prior work.
Frustration	accuracies	Random Forest is the best predictor for Frustration while Logistic Regression has the highest accuracies for Formality and Politeness prediction.
Frustration prediction	precision	 Table 6: Accuracy for Frustration prediction when modeled as a 2-class classification problem. The  positive class is oversampled to correct for class imbalance. Random forest is the best performing  classifier with a precision= 0.88, recall= 0.85, and F1-Score= 0.85. The Affect features contribute  more to the accuracy as compared to the derived features. All values are reported for the experimental  setup with a 80-20 train-test split with 10 fold cross validation.
Frustration prediction	recall	 Table 6: Accuracy for Frustration prediction when modeled as a 2-class classification problem. The  positive class is oversampled to correct for class imbalance. Random forest is the best performing  classifier with a precision= 0.88, recall= 0.85, and F1-Score= 0.85. The Affect features contribute  more to the accuracy as compared to the derived features. All values are reported for the experimental  setup with a 80-20 train-test split with 10 fold cross validation.
Frustration prediction	F1-Score	 Table 6: Accuracy for Frustration prediction when modeled as a 2-class classification problem. The  positive class is oversampled to correct for class imbalance. Random forest is the best performing  classifier with a precision= 0.88, recall= 0.85, and F1-Score= 0.85. The Affect features contribute  more to the accuracy as compared to the derived features. All values are reported for the experimental  setup with a 80-20 train-test split with 10 fold cross validation.
Frustration prediction	accuracy	 Table 6: Accuracy for Frustration prediction when modeled as a 2-class classification problem. The  positive class is oversampled to correct for class imbalance. Random forest is the best performing  classifier with a precision= 0.88, recall= 0.85, and F1-Score= 0.85. The Affect features contribute  more to the accuracy as compared to the derived features. All values are reported for the experimental  setup with a 80-20 train-test split with 10 fold cross validation.
affix prediction (AP)	OOV rate	 Table 4: Results on affix prediction (AP) and sequence labeling (SL) tasks. Sequence labeling tasks have  16.5%, 27.1%, 28.5% OOV rate respectively.
sequence labeling (SL) tasks	OOV rate	 Table 4: Results on affix prediction (AP) and sequence labeling (SL) tasks. Sequence labeling tasks have  16.5%, 27.1%, 28.5% OOV rate respectively.
Sequence labeling tasks	OOV rate	 Table 4: Results on affix prediction (AP) and sequence labeling (SL) tasks. Sequence labeling tasks have  16.5%, 27.1%, 28.5% OOV rate respectively.
information extraction	certainty	Hence, it is important not only to automate information extraction but also to quantify the certainty of this information.
validation	accuracy	A grid search was employed to examine the effects of these parameters on the validation accuracy.
tagger	accuracy	As a result, tagger accuracy depends on training from a balanced sample of the network, rather than training on texts from a narrow subcommunity.
tagger	accuracy	 Table 3: Comparison of tagger accuracy using  network-based and random training/test splits
MT output	TER) scores	In the first case (QE as activator), sentence-level predictions on the raw MT output quality are used to trigger its automatic correction when the estimated (TER) scores are below a certain threshold.
Translation	BLEU	Translation quality is measured case-sensitive with BLEU ().
tagging	Support	 Table 5: Performance of tagging considering single morphological features. Support is defined  as the number of labels where a given feature is defined, divided by the total number of labels  in the training set.
MT	BLEU	We use the automatic MT evaluation metrics BLEU (), METEOR (Denkowski and Lavie) and TER (Snover et al.) to evaluate the absolute translation quality obtained.
MT	METEOR	We use the automatic MT evaluation metrics BLEU (), METEOR (Denkowski and Lavie) and TER (Snover et al.) to evaluate the absolute translation quality obtained.
MT	TER	We use the automatic MT evaluation metrics BLEU (), METEOR (Denkowski and Lavie) and TER (Snover et al.) to evaluate the absolute translation quality obtained.
MT	Fair	"Wrong" means the MT output contains at least one obvious mistake; "Fair" means the output is technically correct but can be made more fluent.
subword segmentation	BPE	The two subword segmentation methods, LMVR and BPE, are also compared with the frequency-based vocabulary pruning method suggested by, and described at the end of Section 2, which is henceforth referred to as Word method.
Statistical machine translation (SMT)	BLEU	Statistical machine translation (SMT) systems have achieved consistently high BLEU scores because they explicitly try to model features such as word or phrase alignments.
SVM classifier	accuracy	 Table 2: The SVM classifier accuracy at predicting sense from hidden activations.
segmentation	accuracy	We conduct various experiments to evaluate the segmentation model and show that this model outperforms previous segmentation models in terms of accuracy.
translation	accuracy	The common requirements for such toolkits are speed, memory efficiency, and translation accuracy, which are essential for the use of such systems in practical translation settings.
SMT lattice rescoring	BLEU	 Table 1: BPE-level SMT lattice rescoring with different search strategies. The BLEU score  does not benefit from less search errors due to modeling errors.
SMT	BLEU	 Table 2: BLEU scores of SGNMT with different NMT back ends on the complete KFTT test  set (Neubig, 2011) computed with multi-bleu.pl. All neural systems are BPE-based (Sen- nrich et al., 2016) with vocabulary sizes of 30K. The SMT baseline achieves 18.1 BLEU.
translating natural languages as they are spoken by humans to machine readable text	accuracy	The NP-complete problem of translating natural languages as they are spoken by humans to machine readable text is a complex problem; yet, is partially solvable due to the accuracy of machine language translations when compared to human translations ().
SMT	BLEU	Both the SMT and NMT systems were tested on the same test set that were used in earlier experiments (, consisting of 1,500 in-domain sentences randomly selected and set aside from the bilingual corpus.: BLEU scores for SMT and NMT EN-GA systems before and after applying the automated post-editing module.
SMT	BLEU score	 Table 3: BLEU scores for SMT and NMT EN-GA systems before and after applying the auto- mated post-editing module. The highest BLEU score and lowest TER score are highlighted in  bold.
SMT	TER score	 Table 3: BLEU scores for SMT and NMT EN-GA systems before and after applying the auto- mated post-editing module. The highest BLEU score and lowest TER score are highlighted in  bold.
MMR	P(interaction)	The observed MMR by tumor site interaction was validated in an independent cohort of stage III colon cancers (P(interaction) = .037).
Query refinement	recall	Query refinement models are evaluated both, by their ability to rank relevant query terms high, and by the recall of retrieved relevant documents when the query automatically is refined by the 1st, 2nd and 3rd proposed query term.
parsing	BLLIP	For parsing the questions, we used BLLIP reranking parser) (Charniak-Johnson parser) and used the model GENIA+PubMed for biomedical text.
detecting bacteria boundaries	SER	According to the official evaluation, TagIt system achieved the best performance on detecting bacteria boundaries (SER: 0.236, recall: 0.772, precision: 0.954), while LIMSI system worked best on habitat entities (SER: 0.597, recall: 0.504, precision: 0.728).
detecting bacteria boundaries	recall	According to the official evaluation, TagIt system achieved the best performance on detecting bacteria boundaries (SER: 0.236, recall: 0.772, precision: 0.954), while LIMSI system worked best on habitat entities (SER: 0.597, recall: 0.504, precision: 0.728).
detecting bacteria boundaries	precision	According to the official evaluation, TagIt system achieved the best performance on detecting bacteria boundaries (SER: 0.236, recall: 0.772, precision: 0.954), while LIMSI system worked best on habitat entities (SER: 0.597, recall: 0.504, precision: 0.728).
detecting bacteria boundaries	SER	According to the official evaluation, TagIt system achieved the best performance on detecting bacteria boundaries (SER: 0.236, recall: 0.772, precision: 0.954), while LIMSI system worked best on habitat entities (SER: 0.597, recall: 0.504, precision: 0.728).
detecting bacteria boundaries	recall	According to the official evaluation, TagIt system achieved the best performance on detecting bacteria boundaries (SER: 0.236, recall: 0.772, precision: 0.954), while LIMSI system worked best on habitat entities (SER: 0.597, recall: 0.504, precision: 0.728).
detecting bacteria boundaries	precision	According to the official evaluation, TagIt system achieved the best performance on detecting bacteria boundaries (SER: 0.236, recall: 0.772, precision: 0.954), while LIMSI system worked best on habitat entities (SER: 0.597, recall: 0.504, precision: 0.728).
PMC articles	maxi- mum	 Table 3. The value of full text PMC articles  in the retrieval performance. In combined  retrieval, we assign each article the maxi- mum of its PubMed and PMC score and  evaluate based on that maximum.
prediction	accuracy	Ina previous study, prediction accuracy was increased slightly by adding diagnosis disease name and independent variables such as prescription medicine.
classification	accuracy	Several studies have reported the effectiveness of using machine learning technique to improve classification accuracy (.
sentence scoring	learning rate	The sentence scoring neural network is trained by) with a learning rate of 0.01 and a batch size of 25.
generative models of text	BLEU	We can then use standard automatic evaluation metrics for generative models of text such as BLEU.
question answering based machine reading comprehension (MRC)	ROUGE	Current evaluation metrics to question answering based machine reading comprehension (MRC) systems generally focus on the lexical overlap between candidate and reference answers, such as ROUGE and BLEU.
question answering based machine reading comprehension (MRC)	BLEU	Current evaluation metrics to question answering based machine reading comprehension (MRC) systems generally focus on the lexical overlap between candidate and reference answers, such as ROUGE and BLEU.
adaptation	ROUGE	Then we give details about our adaptation on ROUGE and BLEU in section 3.
adaptation	BLEU	Then we give details about our adaptation on ROUGE and BLEU in section 3.
translation	accuracy	Accuracy Measures: As a measure of translation accuracy, we used BLEU () and NIST) scores.
translation	BLEU	Accuracy Measures: As a measure of translation accuracy, we used BLEU () and NIST) scores.
WMT News Translation Task  English↔German	BLEU	 Table 2:  WMT News Translation Task  English↔German, reporting cased BLEU on  newstest2017, evaluating the impact of the quality  of the back-translation system on the final system.  Note that the back-translation systems run in the  opposite direction and are not comparable to the  numbers in the same row.
translating En↔De	FA-NMT	 Table 2: Results for translating En↔De, En↔Ru, and Ru→Ar. Statistical significances are marked as   † p < 0.05 and  ‡ p < 0.01 when compared against the baselines and / when compared against the  FA-NMT (no-shared). The results indicate the strength of our proposed shared-attention for NMT.
tokenization	UAS	 Table 5: F1 scores of tokenization and UAS on the UD Japanese-GSD test set. The top section shows the  systems which used their own tokenizers. The second section is a comparison with the systems relying  on the default settings of UDPipe, and the bottom section is the situation to ru parsers using the gold PoS  as input.
machine translation	accuracy	Recent work in machine translation has demonstrated that self-attention mechanisms can be used in place of recurrent neural networks to increase training speed without sacrificing model accuracy.
sentiment analysis	TREC -)	Following ( we have tested our approach on a wide array of classification tasks, including sentiment analysis, SST -Socher et al.), question-type (TREC -), product reviews (CR -Hu and Liu), subjectivity/objectivity (SUBJ -Pang and) and opinion polarity).
Cross-lingual synonym comparison	av- erage rates	 Table 4: Cross-lingual synonym comparison  results on 200 one-to-one word pairs, the av- erage rates(%) of each models.
translation	length normalization	We perform translation via beam search with a beam-size of 10 and length normalization of 0.9.
validation	F1-Score	In the validation stage, we used F1-Score for 7 and 32 classes to measure the effects of the coarse and fine-grained annotation levels.
NER	B-LOCATION	This is essential for NER, where for instance, B-LOCATION cannot be followed by I-PERSON.
validate	accuracy	In order to validate, we split our data and used 80% for training and rest for validating, achieving an accuracy of 79% on this validation data.
NER	accuracy	Firstly, NER has proved to be more difficult for Tweets than for longer text, as accuracy in NER ranges from 85-90% on longer texts compared to 30-50% on Tweets (.
NER on Noisy Usergenerated Text (W-NUT)	F1	The 2015 and 2016 shared tasks for NER on Noisy Usergenerated Text (W-NUT) reported F1 scores between 16.47 and 52.41 for identifying 10 different NE categories ().
classification of sentiment	precision	We evaluated our model's performance on basic classification of sentiment using precision, recall and F 1 -scores.
classification of sentiment	recall	We evaluated our model's performance on basic classification of sentiment using precision, recall and F 1 -scores.
classification of sentiment	F 1 -scores	We evaluated our model's performance on basic classification of sentiment using precision, recall and F 1 -scores.
sentiment analysis	MAE	 Table 1: Performance of individual modality and multimodal fusion for sentiment analysis on the vali- dation set of CMU-MOSEI. MAE is the Mean Absolute Error.
sentiment analysis	Mean Absolute Error	 Table 1: Performance of individual modality and multimodal fusion for sentiment analysis on the vali- dation set of CMU-MOSEI. MAE is the Mean Absolute Error.
Bimodal prediction	overall mean- absolute error (MAE)	 Table 3: Bimodal prediction results, overall mean- absolute error (MAE) for each DNN and ablation.
Emotion Recognition	MAE	 Table 2: Emotion Recognition Model Results -MAE scores
validation	accuracy	The overall validation accuracy (weighted) is 83.11% and class validation accuracy (unweighted) is 77.23% as shown in.
ASR view	accuracy	 Table 2: Improvement in ASR view accuracy us- ing a non contextual classifier.  MOSI MOSEI  MT  71.1  67.5  AT  63.7  63.8  AT ↑  65.1  65.7
ASR view	AT	 Table 2: Improvement in ASR view accuracy us- ing a non contextual classifier.  MOSI MOSEI  MT  71.1  67.5  AT  63.7  63.8  AT ↑  65.1  65.7
ASR view	AT	 Table 2: Improvement in ASR view accuracy us- ing a non contextual classifier.  MOSI MOSEI  MT  71.1  67.5  AT  63.7  63.8  AT ↑  65.1  65.7
multimodal sentiment analysis	F1 Score	Our experiments on multimodal sentiment analysis using the CMU-MOSI dataset indicate that our methods learn informative mul-timodal representations that outperform the baselines and achieve improved performance on multimodal sentiment analysis , specifically in the Bimodal case where our model is able to improve F1 Score by twelve points.
prediction	accuracy	The "pure" condition resulted in a relatively smaller prediction accuracy.
translation	BLEU	Finally, we evaluate translation quality quantitatively in terms of BLEU () and METEOR) and report statistical significance for the metrics using approximate randomisation computed with).
translation	METEOR	Finally, we evaluate translation quality quantitatively in terms of BLEU () and METEOR) and report statistical significance for the metrics using approximate randomisation computed with).
sentiment classification task	Precision	 Table 2: This table shows results obtained by ini- tializing InferSent encoder with different embed- dings in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard de- viations. Best performing embeddings and corre- sponding metrics are highlighted in boldface We  use α = 0.5 for all of our experiments here.
sentiment classification task	F-score	 Table 2: This table shows results obtained by ini- tializing InferSent encoder with different embed- dings in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard de- viations. Best performing embeddings and corre- sponding metrics are highlighted in boldface We  use α = 0.5 for all of our experiments here.
sentiment classification task	AUC	 Table 2: This table shows results obtained by ini- tializing InferSent encoder with different embed- dings in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard de- viations. Best performing embeddings and corre- sponding metrics are highlighted in boldface We  use α = 0.5 for all of our experiments here.
MTurk DA human evaluation	DA score	 Table 6: MTurk DA human evaluation results for English Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.
MTurk DA human evaluation	Assess	 Table 6: MTurk DA human evaluation results for English Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.
MTurk DA human evaluation	DA score	 Table 7: MTurk DA human evaluation results for French Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.
MTurk DA human evaluation	Assess	 Table 7: MTurk DA human evaluation results for French Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.
MTurk DA human evaluation	DA score	 Table 8: MTurk DA human evaluation results for Spanish Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.
MTurk DA human evaluation	Assess	 Table 8: MTurk DA human evaluation results for Spanish Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.
predicting interventions	F 1	We show that our de-biased classifier improves predicting interventions over the state-of-the-art on courses with sufficient number of interventions by 8.2% in F 1 and 24.4% in recall on average.
predicting interventions	recall	We show that our de-biased classifier improves predicting interventions over the state-of-the-art on courses with sufficient number of interventions by 8.2% in F 1 and 24.4% in recall on average.
identification task	F1	In the identification task, the F1 of run1 and run2 ranked the second and the third respectively.
error detection	recall rate	At the error detection level and error identification level, our system achieves a third recall rate and gets a good F1 value.
error detection	F1	At the error detection level and error identification level, our system achieves a third recall rate and gets a good F1 value.
error identification	recall rate	At the error detection level and error identification level, our system achieves a third recall rate and gets a good F1 value.
error identification	F1	At the error detection level and error identification level, our system achieves a third recall rate and gets a good F1 value.
Arabic dialect identification (ADI)	F 1 scores	Based solely on speech transcripts, the top two Arabic dialect identification (ADI) systems) that participated in the 2016 ADI Shared Task ( ) attained weighted F 1 scores just over 50%, in a 5-way classification setting.
Confidence voting	F-score	 Table 3: Combinations of feature groups using one of the three proposed methods. Meta-classification  boosts performance in cases when the PoS feature group is included. The best combination of fea- ture groups corresponds to individual feature group testing and our motivation to have features capture  linguistic information. Confidence voting works best in this case, yielding 71 percent F-score on the  development set.
translating from European to Brazilian Portuguese	BLEU	We report a performance improvement of 0.9 BLEU points in translating from European to Brazilian Portuguese and 0.2 BLEU points when translating in the opposite direction.
semantic relatedness scoring	Mean Squared Error (MSE) loss function	For semantic relatedness scoring, we train the Replicated Siamese, using backpropagation-through-time under the Mean Squared Error (MSE) loss function (after rescaling the training-set relatedness labels to lie ).
IR task	Acc	We then evaluate the trained model for IR task, where we retrieve the top 10 similar results (SUB2+DESC2+SOL2), ranked by their similarity scores, for each query (SUB1+DESC1) in the development set and compute MAP@K, MRR@K and Acc@K, where K=1, 5, and 10.
WS problems	accuracy	It has been argued that a computer that is able to solve WS problems with human-like accuracy must be able to perform "human-like" reasoning and that the WSC can be seen as an alternative to the Turing test.
label prediction	precision (P )	 Table 4: Results from label prediction using SVM. All results are micro-averaged across instances,  including precision (P ), recall (R), and balanced F-measure (F 1 ). For the final three classifiers, all four  features are described in  §5.1.
label prediction	recall (R)	 Table 4: Results from label prediction using SVM. All results are micro-averaged across instances,  including precision (P ), recall (R), and balanced F-measure (F 1 ). For the final three classifiers, all four  features are described in  §5.1.
label prediction	F-measure (F 1 )	 Table 4: Results from label prediction using SVM. All results are micro-averaged across instances,  including precision (P ), recall (R), and balanced F-measure (F 1 ). For the final three classifiers, all four  features are described in  §5.1.
machine translation	similarity	As a popular measure in machine translation, it evaluates the similarity between the machine translations and the gold standard.
classification	accuracy	Surprisingly the classification accuracy was much better (+10%) in general.
SVM classifiers	accuracy	This time, the Passive-Aggressive and linear SVM classifiers attained about 70% of accuracy in CAG identification by segmenting the input posts into a range of character-based n-grams.
ADD	COS	These overall results for ADD are expected, since NO has higher COS scores than ADD for almost all concepts, independent of alignment.
TRANS	COS	Surprisingly, without alignment TRANS often yields more relevant predictions than the two baselines, even though its COS scores are mostly lower (cf.).
rhyme detection	accuracy	We present the first supervised approach to rhyme detection with Siamese Recurrent Networks (SRN) that offer near perfect performance (97% accuracy) with a single model on rhyme pairs for German, English and French, allowing future large scale analyses.
IR	precision	Unlike the IR patterns for which we resort to a complex evaluation strategy, with this corpus, we can directly and fully automatically estimate both precision and recall.
IR	recall	Unlike the IR patterns for which we resort to a complex evaluation strategy, with this corpus, we can directly and fully automatically estimate both precision and recall.
classification	accuracy	We find that classification accuracy improves when oracle segmentations of the interlocutors' sentences are provided compared to directly classifying unsegmented sentences.
Alignment	Alig	 Table 1: Alignment results between language pairs. The Token column stands for a word or morpheme  (morphemes contains the -symbol), Alig is the number of times that the token was aligned, Non is the  number of times the token was not aligned, and Diff is the difference between the numbers of aligned  and non-aligned tokens.
Alignment	Diff	 Table 1: Alignment results between language pairs. The Token column stands for a word or morpheme  (morphemes contains the -symbol), Alig is the number of times that the token was aligned, Non is the  number of times the token was not aligned, and Diff is the difference between the numbers of aligned  and non-aligned tokens.
parsing	accuracy	In addition to the issue of sparse question data, parsing accuracy is further affected by the difference between the training data domain and the test data domain.
parsing	accuracy	In this paper, we evaluate the efficacy of two low-cost synthetic treebank generation methods at improving the parsing accuracy of questions for Arabic in a number of domains.
SMD	BLEU	Meanwhile, since the oracle dialog acts and KB queries are not provided in the SMD data), we only report BLEU and entity F 1 results on SMD.
Prosodic analysis	F0	 Table 1: Prosodic analysis: average F0 values for  each setting.
DA recognition task	accuracy	The DA recognition task is evaluated by accuracy.
Classification	accuracy	 Table 10: Classification accuracy of PARSEQ  when trained on data from all four domains.
classification	accuracy	(4) We calculate the impact of varying confidence thresholds (above which the classifier's prediction is considered) on classification accuracy and savings in terms of number of words.
classification of aggressive posts	precision	We are interested in the successful classification of aggressive posts only and therefore, rather than reporting precision, recall and F -measures, we re-  port accuracy as in equation: Accuracy = true positives true positives + false negatives (1)
classification of aggressive posts	recall	We are interested in the successful classification of aggressive posts only and therefore, rather than reporting precision, recall and F -measures, we re-  port accuracy as in equation: Accuracy = true positives true positives + false negatives (1)
classification of aggressive posts	F -measures	We are interested in the successful classification of aggressive posts only and therefore, rather than reporting precision, recall and F -measures, we re-  port accuracy as in equation: Accuracy = true positives true positives + false negatives (1)
classification of aggressive posts	accuracy	We are interested in the successful classification of aggressive posts only and therefore, rather than reporting precision, recall and F -measures, we re-  port accuracy as in equation: Accuracy = true positives true positives + false negatives (1)
classification of aggressive posts	Accuracy	We are interested in the successful classification of aggressive posts only and therefore, rather than reporting precision, recall and F -measures, we re-  port accuracy as in equation: Accuracy = true positives true positives + false negatives (1)
Hate and Abusive Speech	reliability	Recently, Hate and Abusive Speech on Twitter, a dataset much greater in size and reliability, has been released.
classification	accuracy	show the classification accuracy obtained in those settings.
reputation defence classification	Accuracy	We evaluated the performance of reputation defence classification using the metrics Accuracy, Precision, Recall, and F 1 . shows the results of five-fold crossvalidation on a balanced set from all parliaments in the period 1994-2014, on just the Liberal governments, and on just the Conservative governments.
reputation defence classification	Precision	We evaluated the performance of reputation defence classification using the metrics Accuracy, Precision, Recall, and F 1 . shows the results of five-fold crossvalidation on a balanced set from all parliaments in the period 1994-2014, on just the Liberal governments, and on just the Conservative governments.
reputation defence classification	Recall	We evaluated the performance of reputation defence classification using the metrics Accuracy, Precision, Recall, and F 1 . shows the results of five-fold crossvalidation on a balanced set from all parliaments in the period 1994-2014, on just the Liberal governments, and on just the Conservative governments.
reputation defence classification	F 1	We evaluated the performance of reputation defence classification using the metrics Accuracy, Precision, Recall, and F 1 . shows the results of five-fold crossvalidation on a balanced set from all parliaments in the period 1994-2014, on just the Liberal governments, and on just the Conservative governments.
reputation defence language	accuracy	These results show that reputation defence language can be detected with high accuracy regardless of differences in ideologies and framing strategies.
SRL	BIOSMILE	We analyzed the performance of three SRL tools (BioKIT, BIOSMILE and PathLSTM) on 1776 questions from the BioASQ challenge.
SRL	BIOSMILE	We investigated three SRL tools, two of which were specifically developed for the biomedical domain, namely, BioKIT and BIOSMILE ), and one which is based on deep learning, i.e.,.
reinforcement learning	REINFORCE	The reinforcement learning approach was a proof-of-concept prototype that trained a global policy using REINFORCE.
interpretability evaluation	recall	We model the interpretability evaluation as an information retrieval task, and evaluate each method's recall at different numbers of outputs in.
information retrieval task	recall	We model the interpretability evaluation as an information retrieval task, and evaluate each method's recall at different numbers of outputs in.
factoid type questions	accuracy	Exact answers for factoid type questions are evaluated using strict accuracy, lenient accuracy, and MRR (Mean Reciprocal Rank).
factoid type questions	accuracy	Exact answers for factoid type questions are evaluated using strict accuracy, lenient accuracy, and MRR (Mean Reciprocal Rank).
factoid type questions	MRR	Exact answers for factoid type questions are evaluated using strict accuracy, lenient accuracy, and MRR (Mean Reciprocal Rank).
factoid type questions	Mean Reciprocal Rank)	Exact answers for factoid type questions are evaluated using strict accuracy, lenient accuracy, and MRR (Mean Reciprocal Rank).
answer generation	ROUGE score	This paper presents a system for ideal answer generation (using ontology-based retrieval and a neu-ral learning-to-rank approach, combined with extractive and abstractive summarization techniques) which achieved the highest ROUGE score of 0.659 on the BioASQ 5b batch 2 test.
ASR performance prediction	T EST )	 Table 5: Evaluation of ASR performance prediction with multi-tasks models (DEV ||T EST ) computed  with MAE and Kendall -secondary classification tasks accuracy is also reported
ASR performance prediction	MAE	 Table 5: Evaluation of ASR performance prediction with multi-tasks models (DEV ||T EST ) computed  with MAE and Kendall -secondary classification tasks accuracy is also reported
ASR performance prediction	accuracy	 Table 5: Evaluation of ASR performance prediction with multi-tasks models (DEV ||T EST ) computed  with MAE and Kendall -secondary classification tasks accuracy is also reported
generalization	accuracy	On the one hand, this is due to the use of a larger unlabeled corpus which reduces data sparsity and thus improves generalization accuracy.
SCAN	length	 Table 4: SCAN test scores on the simple, length, and primitive (turn left and jump) tasks. For '+Attn-Dep' models  we removed the connections from the previous target word embedding to the decoder state and the pre-output layer.
text classification	accuracy	Without harming text classification accuracy, this algorithm provides a more robust uncertainty metric which we use to generate feature importance values.
sentiment analysis	accuracy	We combine DKNN with CNN and LSTM models on six NLP text classification tasks, including sentiment analysis and textual entailment, with no loss in classification accuracy (Section 4).
text classification	accuracy	We consider six common text classification tasks: binary sentiment analysis using Stanford Senti- Nearest Neighbor Search For accurate interpretations, we trade efficiency for accuracy and replace locally sensitive hashing () used by Papernot and McDaniel (2018) with a k-d tree.
binary sentiment analysis	accuracy	We consider six common text classification tasks: binary sentiment analysis using Stanford Senti- Nearest Neighbor Search For accurate interpretations, we trade efficiency for accuracy and replace locally sensitive hashing () used by Papernot and McDaniel (2018) with a k-d tree.
classification	accuracy	 Table 1: Replacing a neural network's softmax classifier with DKNN maintains classification accuracy  on standard text classification tasks.
prediction	accuracy	In terms of perplexity and prediction accuracy, the RNNs get close to the theoretical baseline inmost cases.
Classification	accuracy	 Table 1: Classification accuracy on the test sets
word optimization	argmax	For word optimization, we take the argmax over the vocabulary dimension of X.
MTL	accuracy	We also observe an interesting pattern in which MTL achieves better tagging accuracy than the pipeline model but lower performance in parsing.
tagging	accuracy	We also observe an interesting pattern in which MTL achieves better tagging accuracy than the pipeline model but lower performance in parsing.
SUPPORTED	REFUTED	Evidence to justify a given claim is required for SUPPORTED or REFUTED claims.
claim classification	NEI	 Table 3: confusion matrix for claim classification.  (NEI = "not enough info")
sentence extraction	FEVER	We thus jointly model sentence extraction and verification on the FEVER shared task.
information retrieval task	F 1	Though the overall classification of our best version was lower than the best approach from, which used a more sophisticated classification approach, we scored 10 th out of 24 for the information retrieval task (measured by F 1 ).
IR task	F1	The improvement in our system worked well on the IR task, obtaining a relative improvement of 131% on retrieving evidence over the baseline F1 measure.
fact verification challenge	FEVER	This year the fact verification challenge (FEVER) () was organized to further this.
classification	accuracy	In all cases we report classification accuracy.
TL	precision (P)	 Table 2: Results of our TL strategy in terms of precision (P), recall (R) and f-measure (F). σ F is the standard  deviation of the f-measure. The + in the column Train corpus indicates that we trained our model using the target  corpus plus one additional source corpus.
TL	recall (R)	 Table 2: Results of our TL strategy in terms of precision (P), recall (R) and f-measure (F). σ F is the standard  deviation of the f-measure. The + in the column Train corpus indicates that we trained our model using the target  corpus plus one additional source corpus.
TL	f-measure (F)	 Table 2: Results of our TL strategy in terms of precision (P), recall (R) and f-measure (F). σ F is the standard  deviation of the f-measure. The + in the column Train corpus indicates that we trained our model using the target  corpus plus one additional source corpus.
SMT	Paired t-test significance	 Table 2: Comparative effectiveness analysis of our approach. %Chg: improvement of SMT+RL over  corresponding baselines. Paired t-test significance *: 0.01 < t ≤ 0.05 ; **: 0.001 < t ≤ 0.01 ; ***:  t ≤ 0.001.
ASR	accuracy	We discover which acoustic features contain the signal of stød, how to use these features to predict stød and how we can make use of stød and stød-predictive acoustic features to improve overall ASR accuracy and decoding speed.
ASR	WER	• Integrate stød in ASR and improve WER on read-aloud and spontaneous speech.
word discovery task	F-score	Specializing a generic grammar with language specific knowledge leads to great improvements for the word discovery task, ultimately achieving a leap of about 30% token F-score from the results of a strong baseline.
parsing of raw transcriptions	F 1 LAS score	For baseline experiments involving parsing of raw transcriptions (see Section 4.2), for which the number of nodes in gold-standard annotation and in the system output might vary, the F 1 LAS score, marking the harmonic mean of precision an recall LAS scores, was used instead.
parsing of raw transcriptions	precision	For baseline experiments involving parsing of raw transcriptions (see Section 4.2), for which the number of nodes in gold-standard annotation and in the system output might vary, the F 1 LAS score, marking the harmonic mean of precision an recall LAS scores, was used instead.
parsing of raw transcriptions	recall LAS scores	For baseline experiments involving parsing of raw transcriptions (see Section 4.2), for which the number of nodes in gold-standard annotation and in the system output might vary, the F 1 LAS score, marking the harmonic mean of precision an recall LAS scores, was used instead.
Dependency parsing of natural language text	labeled attachment score (LAS)	Dependency parsing of natural language text may seem like a solved problem, at least for resourcerich languages and domains, where state-of-theart parsers attack or surpass 90% labeled attachment score (LAS) ( . However, certain syntactic phenomena such as coordination and ellipsis are notoriously hard and even stateof-the-art parsers could benefit from better models of these constructions.
dependency parsing	label accuracy (LA)	Statistical significance (T-test>0.05) is marked with † .  For dependency parsing, we calculate the label accuracy (LA), the unlabeled attachment score (UAS) and the labeled attachment score (LAS).
dependency parsing	labeled attachment score (LAS)	Statistical significance (T-test>0.05) is marked with † .  For dependency parsing, we calculate the label accuracy (LA), the unlabeled attachment score (UAS) and the labeled attachment score (LAS).
dependency parsing	statistical significance	 Table 2: Results of dependency parsing using PoS (P) and/or NP-chunk (C) features.The baseline uses only word  and character embeddings. Highest scores are in bold.  † indicates statistical significance.
parsing	consistency	Indeed, ease of parsing should not be the indication for selecting one scheme over another, but the hypothesis is that, within one and the same set of guidelines, aversion that presents better coherence and consistency will also be more suitable for statistical training and will yield better results.
Monolingual parsing	accuracy	 Table 10: Monolingual parsing accuracy for un- labeled (UAS) and labeled (LAS) attachment with  gold POS tags and raw text as inputs
POS tagging	accuracy	In the evaluation section, we empirically show that it is effective for POS tagging in learner English, achieving an accuracy of 0.964, which significantly outperforms the state-of-the-art POS-tagger.
paraphrase detection task	ROC AUC	 Table 1: Results of the paraphrase detection task in terms of ROC AUC.
WA	accuracy	 Table 3: Results on Opusparcus for GRAN (all languages) and WA (English only). The first six rows show the  accuracies of the GRAN model at different estimated levels of correctly labeled positive training pairs: 80%, 70%,  and 60%. In each entry in the table, the first number is the classification accuracy and the number in brackets is  the number of assumed positive training pairs in millions. For comparison, the 1M column to the left repeats the  values from
Stack Overflow	GloVe	For Stack Overflow, the parameters for Word embeddings were pretrained using GloVe () with the FULL data (by combining all questions and answers in the sequence they appeared) for 50 epochs.
Stack Overflow	FULL	For Stack Overflow, the parameters for Word embeddings were pretrained using GloVe () with the FULL data (by combining all questions and answers in the sequence they appeared) for 50 epochs.
classification	accuracy	Therefore, we propose providing user level information to classification systems to improve classification accuracy.
gun control	FAVOR	We labeled tweets based on their stance towards gun control: FAVOR was supportive of gun control, AGAINST was supportive of gun rights.
gun control	AGAINST	We labeled tweets based on their stance towards gun control: FAVOR was supportive of gun control, AGAINST was supportive of gun rights.
text classification	convergence	While recurrent neural networks (RNNs) are widely used for text classification, they demonstrate poor performance and slow convergence when trained on long sequences.
WASSA 2018 Implicit Emotion Shared Task (IEST)	F1 score	In this paper we describe our system designed for the WASSA 2018 Implicit Emotion Shared Task (IEST), which obtained 2 nd place out of 30 teams with a test macro F1 score of 0.710.
predictive feature selection	accuracy	Additionally, we show that predictive feature selection, trying to counteract shifts in polarity, significantly improves model accuracy overtime.
MT	F1-Score	For subset A, in contrast to only using MT, the weighted F1-Score and Euclidean distance are improved by 1.91% and 3.59%.
MT	Euclidean distance	For subset A, in contrast to only using MT, the weighted F1-Score and Euclidean distance are improved by 1.91% and 3.59%.
classification	accuracy	We report classification accuracy on 10-fold cross-validation experiments.
Adaptive Moment Estimation	learning rate	The default parameters of Adaptive Moment Estimation is learning rate=0.001, beta 1=0.9, beta 2=0.999, eposilon=1e-08.
Adaptive Moment Estimation	eposilon	The default parameters of Adaptive Moment Estimation is learning rate=0.001, beta 1=0.9, beta 2=0.999, eposilon=1e-08.
recognition of emotions	F-score	This makes recognition of emotions intricate even for human readers as evident from the noticeably low inter-annotator agreement reported by or the  "testing" of the IEST dataset on English nativespeakers which resulted in an F-score of 0.45 ().
humor recognition	recall	For the task of humor recognition, recall appears to be a more important quality measure than precision.
humor recognition	precision	For the task of humor recognition, recall appears to be a more important quality measure than precision.
sentiment tasks	accuracy score	For sentiment tasks, accuracy score is reported.
MT evaluation	Appraise	To this end we use the TrueSkill method adapted to MT evaluation () following its usage at WMT15, 7 i.e. we run 1,000 iterations of the rankings recorded with Appraise followed by clustering (significance level α = 0.05).
MT evaluation	significance level α	To this end we use the TrueSkill method adapted to MT evaluation () following its usage at WMT15, 7 i.e. we run 1,000 iterations of the rankings recorded with Appraise followed by clustering (significance level α = 0.05).
document-level matching	accuracy	When adapted to document-level matching, we achieve a parallel document matching accuracy that is comparable to the significantly more computa-tionally intensive approach of Uszkoreit et al.
parsing	BLEU	Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide anew tool, SACREBLEU, 1 to facilitate this.
machine translation	BLEU	Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide anew tool, SACREBLEU, 1 to facilitate this.
machine translation research	BLEU	In machine translation research, the predictions are made by models whose development is the focus of the research, and the measurement, more often than not, is done via BLEU ().
MT	Adequacy	It shows very clear that data selection does improve the performance of all MT systems evaluated in this paper, in both Adequacy and Fluency.
MT	Fluency	It shows very clear that data selection does improve the performance of all MT systems evaluated in this paper, in both Adequacy and Fluency.
MT	BLEU	In the future, we would like to make more comparisons between human evaluation metrics, e.g. Adequacy and Fluency as defined by Unbabel Quality Team, with commonly used MT performance metrics, e.g. BLEU and TER.
MT	TER	In the future, we would like to make more comparisons between human evaluation metrics, e.g. Adequacy and Fluency as defined by Unbabel Quality Team, with commonly used MT performance metrics, e.g. BLEU and TER.
generalization	BLEU	Secondly, we conduct an ablation study to explore how generalization changes with different amounts of data and find that we only need a small amount of low-resource language data to produce reasonably good BLEU scores.
MMT	BLEU	 Table 1: Quantitative results of the MMT experiments on the 2016 test set. Column 'adv. BLEU' is an  adversarial evaluation with randomized image input.
MMT	BLEU score	 Table 2: Quantitative results of the MMT experiment. The adversarial evaluation shows the BLEU score  when one input language was changed randomly.
translation	accuracy	However, even in the case where target languages are from different families where full parameter sharing leads to a noticeable drop in BLEU scores, our proposed methods for partial sharing of parameters can lead to substantial improvements in translation accuracy.
MT	accuracy	Neural machine translation (NMT; ; ) is now the de-facto standard in MT research due to its relative simplicity of implementation, ability to perform end-to-end training, and high translation accuracy.
WMT18 News Translation Task	standardized mean DA score	 Table 8: Official results of WMT18 News Translation Task. Systems ordered by standardized mean DA score, though systems
WMT18 English-German Multimodal Translation task	standardized mean DA scores (z)	 Table 10: Results of the human evaluation of the WMT18 English-German Multimodal Translation task  (Test 2018 dataset). Systems are ordered by standardized mean DA scores (z) and clustered according  to Wilcoxon signed-rank test at p-level p ≤ 0.05. Systems within a cluster are considered tied, although  systems within a cluster may be statistically significantly different from each other (see
SMT	BLEU points difference	parfda obtains results close to the top constrained phrase-based SMT with an average of 2.252 BLEU points difference on WMT 2017 datasets using significantly less computation for building SMT systems than that would be spent using all available corpora.
translation tasks	BLEU score	For the translation tasks in which we have participated, our resulting systems achieved the best case sensitive BLEU score in all 5 directions.
WMT	BLEU	 Table 3: Comparison of different training corpora conditions. Scores for various WMT test sets measured in cased  BLEU.
SMT	BLEU	We found this particularly striking as the SMT baselines are often more than 10 BLEU points below our strongest neural models.
translation	BLEU	We further improve the translation performance 2.4-2.6 BLEU points from four aspects, including architectural improvements, diverse ensemble decoding, reranking, and post-processing.
MT sys- tems	EVALD	 Table 3: Overall EVALD scores for individual MT sys- tems. L1: EVALD for native speakers with 5 being the  best mark, L2: EVALD for non-natives with 6 being  the best possible mark.
MT sys- tems	EVALD	 Table 3: Overall EVALD scores for individual MT sys- tems. L1: EVALD for native speakers with 5 being the  best mark, L2: EVALD for non-natives with 6 being  the best possible mark.
WMT'18 shared translation task	accuracy	We evaluate all German-English submissions to the WMT'18 shared translation task, plus a number of submissions from previous years, and find that performance on the task has markedly improved compared to the 2016 WMT submissions (81%→93% accuracy on the WSD task).
WSD	accuracy	We report the WSD accuracy for each system, in two variants: automatic and full.
image caption translation	BLEU% scores	 Table 2: Adding subtitle data and domain tuning  for image caption translation (BLEU% scores). All  results with Marian Amun.
WSD	accuracy	 Table 2: Performance of cross-lingual WSD models  (Section 3.1.2) measured in terms of accuracy: propor- tion of correctly translated ambiguous words.
Combining translators	BLEU	We aimed to Combining translators is not new, the most interesting result known to us is (, where the authors report improvements of over 5 BLEU points in Chinese-to-English translation by combining the outputs of SMT and NMT systems using a neural network.
translation	BLEU	Together with a pre-trained out-of-domain News model, we enhanced translation quality with 3.73 BLEU points over the baseline.
MT	DA score	The human post-edited MT output reaches an averaged DA score of 96.13%, ranked above the first system (MS UEdin) with an averaged DA score of 91.11%.
MT	DA score	The human post-edited MT output reaches an averaged DA score of 96.13%, ranked above the first system (MS UEdin) with an averaged DA score of 91.11%.
MT	DA score	In addition, the human post-edited MT output reaches a higher averaged DA score for the NMT compared to the PB-SMT subtask (similarly to automatic metrics results), which could indicate a higher overall translation quality of the final translation after manually post-editing the baseline NMT output compared to a baseline PBSMT output.
SMT	BLEU	 Table 5: Detailed results for SMT performance. BLEU scores (case-insensitive) are reported on all the 6  test sets. The best performance on a test set is reported in bright green, scores within 0.5 BLEU points  off the best in light green, and scores within 1 BLEU point off the best in light yellow.
SMT	BLEU	 Table 5: Detailed results for SMT performance. BLEU scores (case-insensitive) are reported on all the 6  test sets. The best performance on a test set is reported in bright green, scores within 0.5 BLEU points  off the best in light green, and scores within 1 BLEU point off the best in light yellow.
SMT	BLEU	 Table 5: Detailed results for SMT performance. BLEU scores (case-insensitive) are reported on all the 6  test sets. The best performance on a test set is reported in bright green, scores within 0.5 BLEU points  off the best in light green, and scores within 1 BLEU point off the best in light yellow.
NMT	BLEU	 Table 6: Detailed results for NMT performance. BLEU scores (case-insensitive) are reported on all the 6  test sets. The best performance on a test set is reported in bright green, scores within 0.5 BLEU points  off the best in light green, and scores within 1 BLEU point off the best in light yellow.
NMT	BLEU	 Table 6: Detailed results for NMT performance. BLEU scores (case-insensitive) are reported on all the 6  test sets. The best performance on a test set is reported in bright green, scores within 0.5 BLEU points  off the best in light green, and scores within 1 BLEU point off the best in light yellow.
NMT	BLEU	 Table 6: Detailed results for NMT performance. BLEU scores (case-insensitive) are reported on all the 6  test sets. The best performance on a test set is reported in bright green, scores within 0.5 BLEU points  off the best in light green, and scores within 1 BLEU point off the best in light yellow.
MT evaluation	BLEU	Over the years a number of automatic MT evaluation metrics have been proposed like BLEU (), METEOR), Translation Edit Rate (), NIST, etc., which are widely used in the MT research and development community.
MT evaluation	METEOR	Over the years a number of automatic MT evaluation metrics have been proposed like BLEU (), METEOR), Translation Edit Rate (), NIST, etc., which are widely used in the MT research and development community.
MTE	BLEU	However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference based on the character or word N-grams, such as SentBLEU (), which is a smoothed version of BLEU (), Blend (, MEANT 2.0 (, and chrF++.
MT hypothesis.	Blend	ENTFp ( evaluates the fluency of an MT hypothesis. After the success of DPMF comb , Blend) achieved the best performance in the WMT17 metrics task (.
MT	PE	Some studies ( show that the quality of MT output along with PE can produce better result than human editor in certain situations.
Segment	Threshold	 Table 2: Segment versus Threshold values for the BOW  model
sentence-level prediction of translation	RMSE	With improved prediction combination using weights based on their training performance and stacking and multilayer perceptrons to build deeper prediction models, RTMs become the 3rd system in general at the sentence-level prediction of translation scores and achieve the lowest RMSE in English to German NMT QET results.
MT QE	BAD	Our approach to word-level MT QE builds on previous work to mark the words in the machine-translated sentence as OK or BAD, and is extended to determine if a word or sequence of words need to be inserted in the gap after each word.
MT QE	Pearson's correlation r	 Table 1: Results for sentence-level MT QE (columns 2-5) in terms of the Pearson's correlation r, MAE, RMSE,  and Sperman's correlation ρ (for ranking). Results for the task of word labelling (columns 6-8) and gap labelling  (columns 9-11) in terms of the F 1 score for class BAD (F BAD ), the F 1 score for class OK (F OK ) and the product of  both (F MULTI ).
MT QE	MAE	 Table 1: Results for sentence-level MT QE (columns 2-5) in terms of the Pearson's correlation r, MAE, RMSE,  and Sperman's correlation ρ (for ranking). Results for the task of word labelling (columns 6-8) and gap labelling  (columns 9-11) in terms of the F 1 score for class BAD (F BAD ), the F 1 score for class OK (F OK ) and the product of  both (F MULTI ).
MT QE	RMSE	 Table 1: Results for sentence-level MT QE (columns 2-5) in terms of the Pearson's correlation r, MAE, RMSE,  and Sperman's correlation ρ (for ranking). Results for the task of word labelling (columns 6-8) and gap labelling  (columns 9-11) in terms of the F 1 score for class BAD (F BAD ), the F 1 score for class OK (F OK ) and the product of  both (F MULTI ).
MT QE	F 1 score	 Table 1: Results for sentence-level MT QE (columns 2-5) in terms of the Pearson's correlation r, MAE, RMSE,  and Sperman's correlation ρ (for ranking). Results for the task of word labelling (columns 6-8) and gap labelling  (columns 9-11) in terms of the F 1 score for class BAD (F BAD ), the F 1 score for class OK (F OK ) and the product of  both (F MULTI ).
MT QE	F MULTI )	 Table 1: Results for sentence-level MT QE (columns 2-5) in terms of the Pearson's correlation r, MAE, RMSE,  and Sperman's correlation ρ (for ranking). Results for the task of word labelling (columns 6-8) and gap labelling  (columns 9-11) in terms of the F 1 score for class BAD (F BAD ), the F 1 score for class OK (F OK ) and the product of  both (F MULTI ).
MT	BLEU	Currently, the classical and widely-used method to evaluate an MT system is measured by BLEU (), a statistical language-independent metric that requires human golden references for validation.
SMT post-editing	TER	Our submissions decisively wins the SMT post-editing sub-task establishing the new state-of-the-art and is a very close second (or equal, 16.46 vs 16.50 TER) in the NMT sub-task.
SMT postediting	TER	Our submissions decisively wins the SMT postediting sub-task establishing the new state-of-theart and is a very close second (or equal, 16.46 vs 16.50 TER) in the NMT sub-task.
MT	TER	 Table 2: Results of the multi-source Transformer with  specific losses on the NMT outputs. The performance  of the MT baseline are 15.08 TER and 76.76 BLEU.
MT	BLEU	 Table 2: Results of the multi-source Transformer with  specific losses on the NMT outputs. The performance  of the MT baseline are 15.08 TER and 76.76 BLEU.
SMT	Filter mean	 Table 1: BLEU scores of created systems, 10 6 -word SMT. Filter mean excludes the development set (new- stest2017). The two additional systems listed are the best performing in the task, by mean test set BLEU score. Set  score statistics are over the 43 task submissions from other participants.
SMT	BLEU	 Table 1: BLEU scores of created systems, 10 6 -word SMT. Filter mean excludes the development set (new- stest2017). The two additional systems listed are the best performing in the task, by mean test set BLEU score. Set  score statistics are over the 43 task submissions from other participants.
SMT	Filter mean	 Table 2: BLEU scores of created systems, 10 7 -word SMT. Filter mean excludes the development set (new- stest2017). The two additional systems listed are the best performing in the task, by mean test set BLEU score. Set  score statistics are over the 43 task submissions from other participants.
SMT	BLEU	 Table 2: BLEU scores of created systems, 10 7 -word SMT. Filter mean excludes the development set (new- stest2017). The two additional systems listed are the best performing in the task, by mean test set BLEU score. Set  score statistics are over the 43 task submissions from other participants.
MT	BLEU	Although, our method is fully unsupervised it achieves good performance on the extrinsic task of training MT systems on the filtered parallel data, scoring only 2.17 BLEU points behind the best systems.
MT	BLEU	The success of the participating scoring systems was determined by the quality of the MT output from the four MT systems as measured by BLEU () on some in-domain and out-ofdomain evaluation sets.
MT	BLEU	The success of the participating scoring systems was determined by the quality of the MT output from the four MT systems as measured by BLEU () on some in-domain and out-ofdomain evaluation sets.
neural machine translation training	BLEU	The paper describes parallel corpus filtering methods that allow reducing noise of noisy "parallel" corpora from a level where the corpora are not usable for neural machine translation training (i.e., the resulting systems fail to achieve reasonable translation quality; well below 10 BLEU points) up to a level where the trained systems show decent (over 20 BLEU points on a 10 million word dataset and up to 30 BLEU points on a 100 million word dataset).
neural machine translation training	BLEU	The paper describes parallel corpus filtering methods that allow reducing noise of noisy "parallel" corpora from a level where the corpora are not usable for neural machine translation training (i.e., the resulting systems fail to achieve reasonable translation quality; well below 10 BLEU points) up to a level where the trained systems show decent (over 20 BLEU points on a 10 million word dataset and up to 30 BLEU points on a 100 million word dataset).
neural machine translation training	BLEU	The paper describes parallel corpus filtering methods that allow reducing noise of noisy "parallel" corpora from a level where the corpora are not usable for neural machine translation training (i.e., the resulting systems fail to achieve reasonable translation quality; well below 10 BLEU points) up to a level where the trained systems show decent (over 20 BLEU points on a 10 million word dataset and up to 30 BLEU points on a 100 million word dataset).
SMT	MIRA	10 SMT systems were built with Moses and tuned with Batch MIRA.
KB reconstruction	precision	We propose a KB reconstruction based metric as follows: for each entity, construct a KB from the generated paragraph, and compute precision, recall and F-score by comparing it with the input KB from two aspects: (1).
KB reconstruction	recall	We propose a KB reconstruction based metric as follows: for each entity, construct a KB from the generated paragraph, and compute precision, recall and F-score by comparing it with the input KB from two aspects: (1).
KB reconstruction	F-score	We propose a KB reconstruction based metric as follows: for each entity, construct a KB from the generated paragraph, and compute precision, recall and F-score by comparing it with the input KB from two aspects: (1).
SMT	BLEU	BLEU scores for techniques do not show large differences: especially the sentences generated by SMT and NMT obtained close BLEU scores.
response generation	BLEU	Additionally, some other work on response generation () did not use BLEU for their evaluation method, relying on human judgment instead.
natural language generation of explanations	MIRIAM	We focus on the natural language generation of explanations as apart of an interactive multimodal system called MIRIAM for situation awareness for autonomous underwater vehicles (AUVs).
automatic Natural Language Generation (NLG) evaluation	reliability	The current most popular method for automatic Natural Language Generation (NLG) evaluation is comparing generated text with human-written reference sentences using a metrics system, which has drawbacks around reliability and scalabil-ity.
NLG evaluation	accuracy	Automatic NLG evaluation focuses on two areas: accuracy and fluency.
ML	RWA	We carried out experiments for all ML methods above, and for the following feature combinations: (i) the 18 geometrical features ('G' in results tables) from Section 3, (ii) the language features derived from the object labels ('L' in the tables), (iii) average depth ('avg' in tables), (iv) RWA ('rwa' in tables) and (V) human-estimated depth ('man' in tables).
predicting DNZPs	accuracies	Adding a dropping ratio could significantly improve the performance on predicting DNZPs without decreasing the accuracies of AZPs and NZREs very much (i.e., accuracy increase from 62.02% to 95.35%).
predicting DNZPs	accuracy	Adding a dropping ratio could significantly improve the performance on predicting DNZPs without decreasing the accuracies of AZPs and NZREs very much (i.e., accuracy increase from 62.02% to 95.35%).
translation	BLEU	It is well known that evaluation metrics used for translation such as BLEU are not well suited to evaluating generation outputs (;: they penalize stylistic variation, and don't account for the fact that different dialogue responses can be equally good, and can vary due to contextual factors).
RTE	accuracy	In combination with the restricted use of higherorder logic (HOL) developed informal semantics, those logical formulas have recently been used for RTE ( and Semantic Textual Similarity (STS) () and achieved high accuracy.
AMR generation tasks	BLEU	For evaluation, AMR generation tasks () use BLEU, which does not directly consider the meaning and structure of a sentence.
parsing	precision	The parsing and inference system of ccg2lambda achieved high precision in RTE tasks;  reported that the precision was nearly 100% for the SICK dataset).
parsing	precision	The parsing and inference system of ccg2lambda achieved high precision in RTE tasks;  reported that the precision was nearly 100% for the SICK dataset).
RTE tasks	precision	The parsing and inference system of ccg2lambda achieved high precision in RTE tasks;  reported that the precision was nearly 100% for the SICK dataset).
RTE	accuracy	For the RTE accuracy, the increase in the score of the graph + mask model was slightly larger than the increase for the token + mask model.
Parsing	accuracy	 Table 4: Parsing accuracy settings, the F1 scores  are measured on the training set.
Parsing	F1	 Table 4: Parsing accuracy settings, the F1 scores  are measured on the training set.
TS evaluation	BLEU	The standard approach to automatic TS evaluation is therefore to view the task as a translation problem and to use machine translation (MT) evaluation metrics such as BLEU ().
machine translation (MT) evaluation	BLEU	The standard approach to automatic TS evaluation is therefore to view the task as a translation problem and to use machine translation (MT) evaluation metrics such as BLEU ().
MT evaluation	edit distance	The translation outputs were post-edited minimally and the edited translations were used as reference translations to calculate two MT evaluation scores: the character ngram F-score, chrF, and edit distance.
MT	BLEU	As a result, MT metrics such as BLEU (), which compare the output of an MT system to a reference translation, have been extensively used for TS).
MT	TER	Other successful MT metrics include TER (), ROUGE) and METEOR (), but they have not gained much traction in the TS literature.
MT	ROUGE	Other successful MT metrics include TER (), ROUGE) and METEOR (), but they have not gained much traction in the TS literature.
MT	METEOR	Other successful MT metrics include TER (), ROUGE) and METEOR (), but they have not gained much traction in the TS literature.
machine translation (MT)	BLEU	As the task has common points with machine translation (MT), TS is often evaluated using MT metrics such as BLEU.
TS	BLEU	As the task has common points with machine translation (MT), TS is often evaluated using MT metrics such as BLEU.
MT	BLEU	We show that n-gram-based MT metrics such as BLEU and METEOR correlate the most with human judgment of grammaticality and meaning preservation, whereas simplicity is best evaluated by basic length-based metrics.
MT	METEOR	We show that n-gram-based MT metrics such as BLEU and METEOR correlate the most with human judgment of grammaticality and meaning preservation, whereas simplicity is best evaluated by basic length-based metrics.
MT	simplicity	We show that n-gram-based MT metrics such as BLEU and METEOR correlate the most with human judgment of grammaticality and meaning preservation, whereas simplicity is best evaluated by basic length-based metrics.
TS evaluation	BLEU	The standard approach to automatic TS evaluation is therefore to view the task as a translation problem and to use machine translation (MT) evaluation metrics such as BLEU ().
machine translation (MT) evaluation	BLEU	The standard approach to automatic TS evaluation is therefore to view the task as a translation problem and to use machine translation (MT) evaluation metrics such as BLEU ().
MT	BLEU	As a result, MT metrics such as BLEU (), which compare the output of an MT system to a reference translation, have been extensively used for TS).
MT	TER	Other successful MT metrics include TER (), ROUGE) and METEOR (), but they have not gained much traction in the TS literature.
MT	ROUGE	Other successful MT metrics include TER (), ROUGE) and METEOR (), but they have not gained much traction in the TS literature.
MT	METEOR	Other successful MT metrics include TER (), ROUGE) and METEOR (), but they have not gained much traction in the TS literature.
MT evaluation	edit distance	The translation outputs were post-edited minimally and the edited translations were used as reference translations to calculate two MT evaluation scores: the character ngram F-score, chrF, and edit distance.
ASAP)	error	For high-volume testing (ASAP), we see much lower rates at 6% error.
ASAP	error	The results of relaxed evaluation and review are very encouraging for practical application: For ASAP, error drops to 7% when reviewing just 25% of the data.
SMF	cosine similarity score	For the SMF model, we calculated the maximum cosine similarity score overall the sets of features associated with the word and the feature representation of the item (i.e., we considered the sense of the word most similar to the object in question).
CRF	F 1 -score	The semi-supervised CRF shows the best performance both according to F 1 -score and word-type level accuracy.
CRF	accuracy	The semi-supervised CRF shows the best performance both according to F 1 -score and word-type level accuracy.
machine translation	BLEU	For machine translation the standard shared task evaluation method is to use wellknown metrics that compare translations to reference, specifically BLEU.
translation evaluations	BLEU	 Table 6: Automatic translation evaluations, metrics from WMT shared tasks 2018 and  corpora from europarl evaluation section. BLEU scores have been measured with the  tool mteval-14.perl.
frame identification	accuracy	The general evaluation metric for frame identification is accuracy: the relative frequency of correct assignments to predicates.
frame identification	accuracy	Since the task of frame identification is moot for single-frame lexical units, frame identification systems standardly ( report accuracy on two different subsets of the data: (1) all instances from the test set, called "Full Lexicon", because it includes lexical units that are unambiguous; and (2) only instances of predicates from the test set that can evoke multiple frames, called "Ambiguous".
frame identification	Ambiguous	Since the task of frame identification is moot for single-frame lexical units, frame identification systems standardly ( report accuracy on two different subsets of the data: (1) all instances from the test set, called "Full Lexicon", because it includes lexical units that are unambiguous; and (2) only instances of predicates from the test set that can evoke multiple frames, called "Ambiguous".
frame identification	accuracy	Since the task of frame identification is moot for single-frame lexical units, frame identification systems standardly ( report accuracy on two different subsets of the data: (1) all instances from the test set, called "Full Lexicon", because it includes lexical units that are unambiguous; and (2) only instances of predicates from the test set that can evoke multiple frames, called "Ambiguous".
frame identification	Ambiguous	Since the task of frame identification is moot for single-frame lexical units, frame identification systems standardly ( report accuracy on two different subsets of the data: (1) all instances from the test set, called "Full Lexicon", because it includes lexical units that are unambiguous; and (2) only instances of predicates from the test set that can evoke multiple frames, called "Ambiguous".
TTR modelling	YOLO	I refine the TTR modelling of the image and object classification and replace YOLO with a set of binary word classifiers.
Tuning	AVG	 Table 1: Tuning Data Results AVG COS SIM. Top F per Concept Input Type in Bold.
Tuning	TOP	 Table 2: Tuning Data Results TOP n COS SIM AVG. Top F per Concept Input Type in Bold.
stance prediction	accuracy	Our best model achieved the result with a stance prediction accuracy of 63.2% which is a 4.5% overall accuracy improvement compared to the current supervised classification systems developed using the benchmark dataset for code-mixed data stance detection.
stance prediction	accuracy	Our best model achieved the result with a stance prediction accuracy of 63.2% which is a 4.5% overall accuracy improvement compared to the current supervised classification systems developed using the benchmark dataset for code-mixed data stance detection.
classification	accuracy	Altogether, we improve classification accuracy over the previous state of the art on the Wikipedia Toxicity, Personal Attack, and Aggression datasets (.
classification	Aggression	Altogether, we improve classification accuracy over the previous state of the art on the Wikipedia Toxicity, Personal Attack, and Aggression datasets (.
lexicon expansion	accuracy	From that, we conclude that lexicon expansion is worthwhile as it improves prediction accuracy in the majority of our experiments, especially for feature-based learning.
predicting stance	morality	 Table 2: Result of predicting stance (first 12 columns) and morality (last two columns) with SVM and RF for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each half  column) in bold, highest accuracy per each model (each column) in gray)
predicting stance	RF	 Table 2: Result of predicting stance (first 12 columns) and morality (last two columns) with SVM and RF for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each half  column) in bold, highest accuracy per each model (each column) in gray)
predicting stance	Accuracy	 Table 2: Result of predicting stance (first 12 columns) and morality (last two columns) with SVM and RF for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each half  column) in bold, highest accuracy per each model (each column) in gray)
predicting stance	accuracy	 Table 2: Result of predicting stance (first 12 columns) and morality (last two columns) with SVM and RF for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each half  column) in bold, highest accuracy per each model (each column) in gray)
predicting stance	morality	 Table 3: Result of predicting stance (first 7 columns) and morality (last column) with LSTM model for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each  half column) in bold, highest accuracy per each model (each column) in gray)
predicting stance	Accuracy	 Table 3: Result of predicting stance (first 7 columns) and morality (last column) with LSTM model for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each  half column) in bold, highest accuracy per each model (each column) in gray)
predicting stance	accuracy	 Table 3: Result of predicting stance (first 7 columns) and morality (last column) with LSTM model for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each  half column) in bold, highest accuracy per each model (each column) in gray)
CMA task	F-scores	 Table 3: Results for the CMA task. Bold indicates the best scoring system, while italics indicates an 'unofficial'  result that was submitted after the deadline. These scores are F-scores. For the Analysis column every part of the  analysis had to be correct, for the Lemma column the lemma had to be correct and for the Tag column just the  part-of-speech tag had to be correct. BASELINE-I refers to the neural system and BASELINE-II to the neural  ensemble described in Section 5.3.
CMA task	BASELINE-I	 Table 3: Results for the CMA task. Bold indicates the best scoring system, while italics indicates an 'unofficial'  result that was submitted after the deadline. These scores are F-scores. For the Analysis column every part of the  analysis had to be correct, for the Lemma column the lemma had to be correct and for the Tag column just the  part-of-speech tag had to be correct. BASELINE-I refers to the neural system and BASELINE-II to the neural  ensemble described in Section 5.3.
CMA task	BASELINE-II	 Table 3: Results for the CMA task. Bold indicates the best scoring system, while italics indicates an 'unofficial'  result that was submitted after the deadline. These scores are F-scores. For the Analysis column every part of the  analysis had to be correct, for the Lemma column the lemma had to be correct and for the Tag column just the  part-of-speech tag had to be correct. BASELINE-I refers to the neural system and BASELINE-II to the neural  ensemble described in Section 5.3.
Improving Cuneiform Language Identification	BERT	Improving Cuneiform Language Identification with BERT
text normalization	BLEU score	The results show that character-based neural machine translation was the most promising strategy for text normalization and that in combination with phrase-based statistical machine translation it achieved 36% BLEU score.
SMT	BLEU	 Table 5: Comparison of SMT and NMT (top BLEU scores) on two segmentation schemes
POS tagging	accuracy	 Table 2: POS tagging accuracy (%) on the four corpora. Average over five runs with different random  seeds. Bold and italics font indicates the best result in our experiments, while bold font indicates the best  results compared to the state-of-the-art systems. We refer to BiLSTM-CRF Tagger as (1), MTL-POS  Tagger that learns POS tag for related languages as (2), and MTL-POS+LID that learns jointly POS  tagging and language identification as (3). Random-Initi-Embed refers to Random initialized embedding
text classification	F 1 -score	This work investigates different machine learning methods which are proven to be effective in text classification and compares them by their obtained F 1 -score, accuracy, and training time.
text classification	accuracy	This work investigates different machine learning methods which are proven to be effective in text classification and compares them by their obtained F 1 -score, accuracy, and training time.
GDI	F-1	Our system for GDI achieved third place out of 6 teams, with a macro averaged F-1 of 74.6%.
MRC subtasks	macro-F 1 score	We participated in all three MRC subtasks and we managed to rank on the first place in the first subtask (Moldavian vs. Romanian dialect identification), with a macro-F 1 score of 0.89, surpassing the other five participants by more than 10%.
ERE detection	RELATION	A particularly important class of extraction tasks is ERE detection in which an object, typically an element in a knowledge base, is created for each ENTITY, RELATION, and EVENT identified in a given text.
ERE detection	EVENT	A particularly important class of extraction tasks is ERE detection in which an object, typically an element in a knowledge base, is created for each ENTITY, RELATION, and EVENT identified in a given text.
R2R validation unseen paths	SR	 Table 3: Results on R2R validation unseen paths (U) and seen paths (S) when trained only with small fraction of  Fried-Augmented ordered by discriminator scores. For Random Full study, examples are sampled uniformly  over entire dataset. For Random Top/Bottom study, examples are sampled from top/bottom 40% of ordered dataset.  SPL and SR are reported as percentages and NE and PL in meters.
R2R validation unseen paths	NE	 Table 3: Results on R2R validation unseen paths (U) and seen paths (S) when trained only with small fraction of  Fried-Augmented ordered by discriminator scores. For Random Full study, examples are sampled uniformly  over entire dataset. For Random Top/Bottom study, examples are sampled from top/bottom 40% of ordered dataset.  SPL and SR are reported as percentages and NE and PL in meters.
MLT	UA	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
MLT	APS	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
MLT	APHW	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
MLT	TCPA	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
MLT	SR	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
MLT	Skewness Ratio	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
MLT	WSR	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
MLT	Weigthed average of SRs	 Table 2: Some key statistics of the original dataset for  MLT. UA: Unique Ambiguous words. APS: Ambigu- ous words Per Sentence. APHW: Ambiguous words  Per Hundred Words. TCPA: Translation Candidates  Per Ambiguous word. SR: Skewness Ratio as de- scribed in Section 2.1. WSR: Weigthed average of SRs.
relation extraction task	F1	In the end-to-end relation extraction task (Table 3, bottom), the neural system is again two points better than the SVM in F1 score.
Natural language processing (NLP)	BERT	Natural language processing (NLP) has been shaken in recent months with the dramatic successes enabled by transfer learning and contextual word embedding models, such as ELMo (,, and BERT ().
normalization	agreement	Section 4 proposes a methodology for the annotation, normalization, agreement and evaluation of a corpus based on this annotation model.
ICU transfer	ICU	 Table 1: Patient, visit, and messages information of data between years 2015 and 2017 used to train models for  predicting ICU transfer. We indicate standard deviation in parentheses. ICU % is the ratio of mheaders resulting  in ICU transfer within 3 days of the message send date.
QA	MEANS	Accordingly, there has been vital interest in developing clinical QA systems, e.g.,),, and MEANS.
sentiment analysis	VADER	Traditionally, sentiment analysis has been approached with a lexicon-based majority vote approach, where a dictionary of terms and their associated sentiments (e.g. SentiWordnet, Pattern, SO-CAL, VADER) are queried to determine the sentiment of a given text).
Sentiment classification	LSTM	• Sentiment classification: IMDB movie reviews (Maas et al., 2011), with a single 100-d LSTM.
stance prediction	F1	 Table 3: Performance of stance prediction models trained only on user attributes, shown here for each of the  different stance targets. Bold indicates best in column for user attributes and inferred factors. The weighted F1  is shown for each target and the last column is the unweighted average across all targets.  † indicates statistical  significance at the 0.05 level compared to the MFC baseline.
MMI	maximum likelihood estimation (MLE) objective	More specifically, p(r|q) in MMI is trained only by maximum likelihood estimation (MLE) objective at training time (we use p(r|q) to denote the probability distribution of predicting the response r given the query q).
summarization	length	Neural approaches to summarization however have done away with length requirements.
summarization	ROUGE	For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation.
summarization	recall	For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation.
Surface Realization	BLEU score	However in the Surface Realization shared task correlation between BLEU score and human evaluation was noted to be highly significant ().
Article identification	BYISSUE	 Table 2: Experiment 1: Article identification with gold  standard segments, BYISSUE setting
Article identification	BYISSUE	 Table 5: Article identification on pages filtered by OCR  quality (Exp. 1, BYISSUE, B-Cubed F1, 14 clusters)
segmentation	accuracy	We show that contextualization yields significant improvements in segmentation accuracy.
segmentation	accuracy	To measure segmentation accuracy, two of the authors manually annotated a randomly-selected subset of 200 terms that occur in at least 5 contexts in the corpus.
segmentation	recall	 Table 1: Maximum segmentation recall at various false positive rates.
Scientific Information Extraction (ScienceIE))	TASK	A recent challenge on Scientific Information Extraction (ScienceIE)) provided a dataset consisting of 500 scientific paragraphs with keyphrase annotations for three categories: TASK, PROCESS, MA-TERIAL across three scientific domains, Computer Science, Material Science, and Physics.
Scientific Information Extraction (ScienceIE))	MA-TERIAL	A recent challenge on Scientific Information Extraction (ScienceIE)) provided a dataset consisting of 500 scientific paragraphs with keyphrase annotations for three categories: TASK, PROCESS, MA-TERIAL across three scientific domains, Computer Science, Material Science, and Physics.
extraction	Parscit	For extraction of citation context, we used Parscit ().
RST segmentation	F1	RST segmentation is often regarded as a solved problem because automated segmenters achieve high performance (F1=94.3) on a task with high inter-annotator agreement (kappa=0.92) ( ).
Rhetorical Structure Theory	QUOTE	In the classical formulation of Rhetorical Structure Theory, considered, but decided against, QUOTE as one of the baselined relations.
sentence- splitter	BERT-E	 Table 5: Specific results on English test data at the doc- ument level. 'Rand.-50d' and 'GloVe-50d' correspond  to the baseline model, taking a whole document as in- put. BERT models are still pipelined to a sentence- splitter, but ELMo-based models take the whole docu- ment as input. BERT-E uses English embeddings and  BERT-M uses multilingual embeddings.
sentence- splitter	BERT-M	 Table 5: Specific results on English test data at the doc- ument level. 'Rand.-50d' and 'GloVe-50d' correspond  to the baseline model, taking a whole document as in- put. BERT models are still pipelined to a sentence- splitter, but ELMo-based models take the whole docu- ment as input. BERT-E uses English embeddings and  BERT-M uses multilingual embeddings.
Discourse parsing	F1  scores	 Table 3: Discourse parsing performance in terms of F1  scores (%) on three levels of Span, Nuclearity, and Re- lation. Human agreements are also listed for compari- son. Within each cell, two micro F1 scores according to  the gold standards from each of two human annotators  are both reported.
Discourse parsing	Re- lation	 Table 3: Discourse parsing performance in terms of F1  scores (%) on three levels of Span, Nuclearity, and Re- lation. Human agreements are also listed for compari- son. Within each cell, two micro F1 scores according to  the gold standards from each of two human annotators  are both reported.
Detection of named mentions	accuracy	Detection of named mentions can be done with high accuracy by named entity recognition systems () and the matching of names can also be done accurately via string matching (.
coreference resolution	BLANC	For coreference resolution, we use the CoNLL-2012 metrics () including BLANC.
predicting the reference sentence occurring in a corpus	accuracy	For the task of predicting the reference sentence occurring in a corpus (amidst meaning-equivalent grammatical variants) using a machine learning model, surprisal estimates from an artificial version of the language (i.e., Hindi without any case markers) result in lower prediction accuracy compared to natural Hindi.
dependency parser surprisal	accuracy	Moreover, dependency parser surprisal has much lower classification accuracy compared to trigram surprisal and has a very negligible impact on performance on top of trigram surprisal.
Pairwise classification	accuracy	 Table 5: Pairwise classification and ranking accuracy (***
automated speech recognition (ASR)	recall quality	The study further examined how well these measures can be incorporated into a full analysis pipeline starting from data collection on a mobile platform outside of the traditional laboratory (thus in the real-world, perhaps noisy, environment), to automated speech recognition (ASR), and then to the conversion of the language to predictions of recall quality.
classification	accuracy	The SLM offers good classification accuracy, but it does not provide any human interpretable reason for its classification decisions.
relation extraction	TP	A relation extraction was considered as TP if both the NER and RE tasks were correctly captured.
spelling correction	F 0.5	Our method outper-forms state-of-the-art spelling correction and can detect mistakes with an F 0.5 of 0.888.
spelling correction	accuracy	2. To what extent can our corpus-driven spelling correction improve accuracy of health-related classification tasks with social media text?
SMM4H workshop	F 1 score (0.522)	Although the overall classification accuracy on Task 1 of the SMM4H workshop is low, this is inline with the low F 1 score (0.522) of the best performing system on the comparable task in.
Passive Diagnosis	PHQ-4	Passive Diagnosis incorporating the PHQ-4 for Depression and Anxiety
ADR normalization	F1	The end-to-end model based on BERT for ADR normalization ranked first at the SMM4H 2019 Task 3 and obtained a relaxed F1 of 43.2%.
ADR extraction	BERT	For both ADR extraction and medical concept normalization, we conclude that BERT outperforms previous state-of-the-art baselines based on recurrent neural architectures (RNNs), including bidirectional Long Short-Term Memory (LSTM), and Gated Recurrent Units () paired with word2vec word embeddings.
Transfer Learning	BERT	Our main contribution is to show the effectiveness of Transfer Learning approaches like BERT and ULM-FiT, and how they generalize for the classification tasks like identification of adverse drug reaction mentions and reporting of personal health problems in tweets.
parsing	accuracy	The parsing accuracy varies a great deal for different meaning representations.
Node identification	MRS	 Table 2: Node identification and WSD results on MRS  in terms of noun (n), verb (v), quantifier (q), preposi- tion (p), adjective (a), conjunction (c), and others (x),  and on AMR in terms of predicate (pred). Both are  measured on the test set in terms of accuracy based on  SMATCH.
Node identification	accuracy	 Table 2: Node identification and WSD results on MRS  in terms of noun (n), verb (v), quantifier (q), preposi- tion (p), adjective (a), conjunction (c), and others (x),  and on AMR in terms of predicate (pred). Both are  measured on the test set in terms of accuracy based on  SMATCH.
WSD	MRS	 Table 2: Node identification and WSD results on MRS  in terms of noun (n), verb (v), quantifier (q), preposi- tion (p), adjective (a), conjunction (c), and others (x),  and on AMR in terms of predicate (pred). Both are  measured on the test set in terms of accuracy based on  SMATCH.
WSD	accuracy	 Table 2: Node identification and WSD results on MRS  in terms of noun (n), verb (v), quantifier (q), preposi- tion (p), adjective (a), conjunction (c), and others (x),  and on AMR in terms of predicate (pred). Both are  measured on the test set in terms of accuracy based on  SMATCH.
scene extraction	F1 score differences	For scene extraction, the F1 score differences of all method pairs are statistically significant.
action extraction	BERT	For action extraction, only the difference between BERT and Parser is significant.
action extraction	Parser	For action extraction, only the difference between BERT and Parser is significant.
scene extraction	BiLSTM-CRF	Only in scene extraction, BiLSTM-CRF is the best and the differences are significant.
Paragraph-level character detection	accuracy	Paragraph-level character detection reaches relatively higher accuracy than sentence-level character detection, with F 1 score of over 0.91.
Paragraph-level character detection	F 1 score	Paragraph-level character detection reaches relatively higher accuracy than sentence-level character detection, with F 1 score of over 0.91.
text classification	accuracy	 Table 5: Accuracy of classifiers using only schema fea- tures for text classification. Majority vote accuracy is  53.34%
Classification	accuracy	 Table 4: Classification accuracy for Brown Corpus and Baby BNC with different feature sets (most frequent class  i.e., non-fiction baseline results reported).
scenario detection	precision	We evaluate scenario detection performance at the sentence level using micro-average precision, recall and F 1 -score.
scenario detection	recall	We evaluate scenario detection performance at the sentence level using micro-average precision, recall and F 1 -score.
scenario detection	F 1 -score	We evaluate scenario detection performance at the sentence level using micro-average precision, recall and F 1 -score.
classifiers analysis	accuracy	Next, for classifiers analysis, the ensemble method on RFDT relatively can give better accuracy compared to NB and SVM.
Sample document split	BERT	 Table 2: Sample document split created by BERT BPE tokenizer, Custom BPE tokenizer
NER	Relaxed	For NER, two types of evaluations were carried out: • Relaxed: An entity mentioned in a given document is considered to be extracted correctly if the system response includes at least one annotation of a named mention of this entity (regardless of whether the extracted mention is in base form); • Strict: The system response should include exactly one annotation for each unique form of a named mention of an entity in a given document, i.e., identifying all variants of an entity is required.
MT evaluation	BLEU	We first evaluated all translation outputs using the following overall automatic MT evaluation metrics: BLEU (), METEOR (Lavie and Denkowski, 2009), TER (), chrF (Popovi´cPopovi´c, 2015) and characTER (.
MT evaluation	METEOR	We first evaluated all translation outputs using the following overall automatic MT evaluation metrics: BLEU (), METEOR (Lavie and Denkowski, 2009), TER (), chrF (Popovi´cPopovi´c, 2015) and characTER (.
MT evaluation	TER	We first evaluated all translation outputs using the following overall automatic MT evaluation metrics: BLEU (), METEOR (Lavie and Denkowski, 2009), TER (), chrF (Popovi´cPopovi´c, 2015) and characTER (.
classification	accuracy	Testing the approach with several machine learning methods on Polish, Slovenian and Croatian Twitter corpora returns up to 86 % of classification accuracy on the out-of-sample data.
translation	accuracy	We expect the best translation accuracy on this setup.
classification	accuracy	With a relatively minor adjustment , however, we show how these same techniques can be used to simultaneously reduce bias and maintain high classification accuracy .
mask language modeling	BERT	We have implemented two models for mask language modeling using pre-trained BERT adjusted to work fora classification problem.
Gendered Pronoun Resolution	Fujisawa	The shared task Gendered Pronoun Resolution aims to classify the pronoun resolution in the sentences, hereby to find the true name referred by a given pronoun, such as she in: In May, Fujisawa joined Mari Motohashi's rink as the team's skip, moving back from Karuizawa to Kitami where she had spent her junior days.
classify the pronoun resolution	Fujisawa	The shared task Gendered Pronoun Resolution aims to classify the pronoun resolution in the sentences, hereby to find the true name referred by a given pronoun, such as she in: In May, Fujisawa joined Mari Motohashi's rink as the team's skip, moving back from Karuizawa to Kitami where she had spent her junior days.
resolvers	Masculine	 Table 4: Comparison to off-the-shelf resolvers, split  by Masculine and Feminine (Bias shows F/M), and  Overall. Bold indicates the best performance.
resolvers	Bias	 Table 4: Comparison to off-the-shelf resolvers, split  by Masculine and Feminine (Bias shows F/M), and  Overall. Bold indicates the best performance.
resolvers	F/M	 Table 4: Comparison to off-the-shelf resolvers, split  by Masculine and Feminine (Bias shows F/M), and  Overall. Bold indicates the best performance.
Resolving Gendered Ambiguous Pronouns	BERT	Resolving Gendered Ambiguous Pronouns with BERT
label smoothing	confidence penalty	The idea is similar to label smoothing () and confidence penalty ( All the training was done in Google Colab with a single GPU.
text generation	BLEU	However, we are concerned about the deficiency of using only BLEU as a measurement of a correct translation) or text generation in general (, and currently there are other proposed metrics to cover the correlation gap between BLEU and a human assessment.
Annotating Information Structure	Applicability	Annotating Information Structure in Italian: Characteristics and Cross-Linguistic Applicability of a QUD-Based Approach
negation	IAA rates	As far as negation is concerned, the results were counter-intuitive at first glance: pairs with negation in one of the sentences A or B had statistically higher IAA rates (p < 0.001) than pairs with no negation at all.
parsing	accuracy	In light of the comparisons , even though we observe a slight decrease in the attachment scores of the Turkish PUD treebank, we demonstrate that the annotation of the TNC-UD improves the parsing accuracy of Turkish.
parsing	accuracy	To seethe effect of re-annotation on the parsing accuracy, we trained a state-of-the-art graph-based neural parser on the previous and re-annotated versions of the PUD and TNC-UD treebanks.
parsing	accuracy	From the results, we observe a decrease in the parsing accuracy in terms of the attachment scores.
parsing	accuracy	We also made an experiment to seethe impact of the TNC-UD Treebank on the parsing accuracy of the parser.
Salary	De-	About 25% of recent end-user queries in English fall into 5 categories: (1) Application Process; (2) Salary; (3) Professional Growth and De- About 40% of overall requests were not recognized (with a confidence of 0.5 or higher) as any of the categories in the anonymous benchmarking set.
morphology tagging	accuracy	Out of all submissions for this shared task, our system achieves the highest average accuracy and f1 score in morphology tagging and places second in average lemmatization accuracy.
morphological tagging	F1 score	The performance of morphological tagging is measured by the F1 score calculated over the predicted and actual individual morphological tags.
SIG-MORPHON shared tasks	accuracy	The low-resource settings in earlier SIG-MORPHON shared tasks often resulted in much worse accuracy compared to the high-resource settings.
morphological analysis	accuracy	In the morphological analysis, our system placed tightly second: our morphological analysis accuracy was 93.19, the winning system's 93.23.
tagging	accuracy	Even the baseline model without any of the mentioned enhancements achieves relatively high performance and would place third in both lemmatization and tagging accuracy (when not considering our competition entry).
translation	accuracy	In particular , we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks.
MT	BLEU score metric	holds that neural MT systems are unfairly compared in the literature using different variants of the BLEU score metric.
cross-lingual transfer	Ar	 Table 2: Average precision (AP) of POSTLE models  in cross-lingual transfer. Results are shown for both  POSTLE models (DFFN and ADV), two target languages  (Spanish and French) and three methods for inducing  bilingual vector spaces: Ar (Artetxe et al., 2018), Co  (Conneau et al., 2018), and Sm (Smith et al., 2017).
SMLM	F1 score	But this assumption does not hold for the language model that was trained on synthetic data via SMLM objective: The perplexity for this language model (both forward and backward) is relatively high compared to other language models, but the F1 score results are better than some other language models with lower perplexity.
Repetition-averaged unseen type generation	precision	 Table 2: Repetition-averaged unseen type generation  and precision.
word similarity and analogy tasks	accuracies	Evaluating our models on word similarity and analogy tasks in English and German, we find that they not only achieve higher accuracies than the original skip-gram and fastText models but also require significantly less training data and time.
Sentiment classification	accuracy	 Table 6: Sentiment classification accuracy(%) on target language Amazon review test data after training  on English Amazon review data and a portion of French of German data. The second row shows the  percent of French (fr) or German (de) data is used for training in each model.
dropout regularisation	minibatches	The effects of dropout regularisation and batch normalisation were examined with the previously selected parameters for vector size=100, minibatches=8 and dropout rate=30%.
SMDs	FAIR	 Table 6. As in  the case of simulated models, SMDs allowed us  to reach the same general conclusion: L1 model  is the least fair and FAIR(ER) model is proba- bly the most fair of the three models. Yet in  this case SMDs also obscure the fact that both  BASE and FAIR(ER) model might be over-scoring  GER speakers: in fact the SMDs for the two mod- els have the opposite sign.
SMDs	BASE	 Table 6. As in  the case of simulated models, SMDs allowed us  to reach the same general conclusion: L1 model  is the least fair and FAIR(ER) model is proba- bly the most fair of the three models. Yet in  this case SMDs also obscure the fact that both  BASE and FAIR(ER) model might be over-scoring  GER speakers: in fact the SMDs for the two mod- els have the opposite sign.
SMDs	FAIR	 Table 6. As in  the case of simulated models, SMDs allowed us  to reach the same general conclusion: L1 model  is the least fair and FAIR(ER) model is proba- bly the most fair of the three models. Yet in  this case SMDs also obscure the fact that both  BASE and FAIR(ER) model might be over-scoring  GER speakers: in fact the SMDs for the two mod- els have the opposite sign.
SMDs	BASE	 Table 6. As in  the case of simulated models, SMDs allowed us  to reach the same general conclusion: L1 model  is the least fair and FAIR(ER) model is proba- bly the most fair of the three models. Yet in  this case SMDs also obscure the fact that both  BASE and FAIR(ER) model might be over-scoring  GER speakers: in fact the SMDs for the two mod- els have the opposite sign.
SMDs	FAIR	 Table 6. As in  the case of simulated models, SMDs allowed us  to reach the same general conclusion: L1 model  is the least fair and FAIR(ER) model is proba- bly the most fair of the three models. Yet in  this case SMDs also obscure the fact that both  BASE and FAIR(ER) model might be over-scoring  GER speakers: in fact the SMDs for the two mod- els have the opposite sign.
question generation	OAR	The question generation method performs well with the average OAR of 53% and the AAR of 81.67%.
question generation	AAR	The question generation method performs well with the average OAR of 53% and the AAR of 81.67%.
model prediction	er- ror	 Table 7: GLMM summary for model prediction er- ror on Formative-K12-SAS for the models without  spelling correction.
GEC	precision	In particular , GEC with sentence-level grammatical error detection is a novel and versatile approach, and we experimentally demonstrate that it significantly improves the precision of the base model.
grammatical error detection (GED)	BERT	They proposed a grammatical error detection (GED) model based on BERT that achieved state-of-the-art results in word-level GED tasks.
error correction	F 0.5	Our system achieves 66.75% F 0.5 on error correction (ranking 4th), and 82.52% F 0.5 on token-level error detection (ranking 2nd) in the restricted track of the shared task.
GEC	BERT	In this paper, we describe our approach to GEC using the BERT model for creation of encoded representation and some of our enhancements, namely, "Heads" are fully-connected networks which are used for finding the errors and later receive recommendation from the networks on dealing with a highlighted part of the sentence only.
Collocation learning	accurate	Collocation learning comes down to three main benefits for language learners: accurate production, efficient comprehension and increased fluency of processing (e.g.,).
classification	accuracy	We report classification accuracy on 10-fold cross-validation experiments.
writing development	accuracy	The article is structured as follows: We first give a brief overview of research on writing development in terms of complexity and accuracy.
OSE	difficulty	Overall, OSE texts are somewhat more complex than Weebit's, with OSE level 1 comparable in difficulty to Weebit level 4.
gap filling	Kendall tau-b correlation	 Table 5: Completion rates broken down per readability level for gap filling systems of variable strength tested on  the OSE dataset. Kendall tau-b correlation is reported as τ .
gap filling	Kendall tau-b correlation	 Table 6: Completion rates broken down per readability level for gap filling systems of variable strength tested on  the Weebit dataset. Kendall tau-b correlation is reported as τ .
GPT-2 gap filling	Kendall tau-b correlation	 Table 7: Completion rates broken down per readability  level for the GPT-2 gap filling system pre-trained on  WebText and tested on the Weebit and OSE datasets.  Kendall tau-b correlation is reported as τ .
Document representation	Mean	Document representation: We vary the document representation method while fixing the seed corpus size at 10 3 , and the distance function as Mean (c.f..
argument unit segmentation task	RQ1	This paper answers the following research questions, which will help to assess the importance of the attention layers and contextualized word embeddings for the argument unit segmentation task: • RQ1: To what extent can seperate attention layers help the model focus on the, for the task of unit segmentation relevant, sequence parts and how much do they influence the predictions?
document classification	accuracy	On the other hand, existing unsupervised document classification has the advantage of being able to use the whole document, but accuracy of these methods is not so high.
component classification	precision	For component classification, we employed the precision, recall, and F-score as evaluation met-  rics that are often used in text classification.
component classification	recall	For component classification, we employed the precision, recall, and F-score as evaluation met-  rics that are often used in text classification.
component classification	F-score	For component classification, we employed the precision, recall, and F-score as evaluation met-  rics that are often used in text classification.
comparative argument mining	BETTER	As there is no large publicly available crossdomain dataset for comparative argument mining, we create one composed of sentences annotated with markers BETTER (the first item is better or "wins") / WORSE (the first item is worse or "looses") or NONE (the sentence does not contain a comparison of the target items).
comparative argument mining	WORSE	As there is no large publicly available crossdomain dataset for comparative argument mining, we create one composed of sentences annotated with markers BETTER (the first item is better or "wins") / WORSE (the first item is worse or "looses") or NONE (the sentence does not contain a comparison of the target items).
comparative argument mining	NONE	As there is no large publicly available crossdomain dataset for comparative argument mining, we create one composed of sentences annotated with markers BETTER (the first item is better or "wins") / WORSE (the first item is worse or "looses") or NONE (the sentence does not contain a comparison of the target items).
stance detection classifiers	BiLSTM	We developed our own stance detection classifiers using gradient boosting as well as (three forms of) neural networks, of which we selected two (LSTM and BiLSTM) as best performing instance prediction, to generate BAFs.
segmentations	edit distance	The segmentations obtained by BPE and SR were also relatively similar with an average edit distance of 5.03.
data cleaning	SPLIT	For data cleaning, we replace e-mails and URLs with a unified token and use SPLIT tool (Al-Badrashiny et al., 2016) to clean UTF8 characters (e.g. Latin and Chinese), remove diacritics in the original data, and separate punctuation, symbols, and numbers in the text, and replace them with separate unified tokens.
transfer learning (TL)	accuracy	The utilization of transfer learning (TL) has recently shown promising results for advancing accuracy of text classification in English.
Transfer learning (TL	accuracy	Transfer learning (TL) with universal language models (ULMs) have recently shown to achieve state of the art accuracy for several natural language processing (NLP) tasks.
root extraction	accuracy	To address these challenges, we present a supervised root extraction algorithm that, given a word, directly extracts the root with high accuracy.
open domain question answering in Arabic (SOQAL)	BERT	Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT.
POS tagging	f-score	We show that adding a small amount of dialectal segmentation training data reduced OOVs by 5% and remarkably improves POS tagging for dialects by 7.37% f-score, even though no dialect-specific POS training data is included.
MT	BLEU	 Table 5: Arabic-to-Turkish MT BLEU scores due to  the different traning corpora
domain adaptation	A-distance	To assess the success of domain adaptation, we use the proxy A-distance as a matrix.
SemEval 2017 task	F P N	The best performing system in the SemEval 2017 task is the one described in which achieved an F P N of 0.61.
ASTD	F P N	For the ASTD, the best reported results are by who used an ensemble system combining output of CNN and Bi-LSTM architectures, which achieved an F P N of 0.71.
dialect classification	accuracy	Results show that these features help to improve dialect classification accuracy.
MADAR travel domain dialect identification	accuracy	We participate in Task 1: MADAR travel domain dialect identification, and we ranked 1st in the task with accuracy of 67.3%.
tweet-level identification	BERT	We develop tweet-level identification models based on GRUs and BERT in supervised and semi-supervised settings.
Shifters Shifters	abandon	Shifters Shifters (50 errors), such as "abandon", "lessen", or "reject" are less common within the dataset, but normally move positive polarity words towards a more negative sentiment.
tagging	accuracy	 Table 1: Attributes and tagging accuracy by lan- guage (Irish and Thai do not have both dev and test  sets).  † Affixation: S/s is strongly/weakly suffixing;  P/p is strongly/weakly prefixing; = is equally prefix- ing/suffixing; ∅ is little affixation.  ‡ Morphological syn- thesis: agglutinative, fusional, introflexive, isolating.
tagging	Affixation	 Table 1: Attributes and tagging accuracy by lan- guage (Irish and Thai do not have both dev and test  sets).  † Affixation: S/s is strongly/weakly suffixing;  P/p is strongly/weakly prefixing; = is equally prefix- ing/suffixing; ∅ is little affixation.  ‡ Morphological syn- thesis: agglutinative, fusional, introflexive, isolating.
sentiment classification	accuracy	The first criteria is the sentiment classification accuracy of the adversarial examples which is predicted by human, and the second is the similarity between the adversarial examples and the original sentence.
classification	accuracy	In addition to classification accuracy, we also care about how similar the generated adversaries examples are to the original unaltered sentences.
coreference resolution	BERT	We perform a similar analysis for coreference resolution, also finding a BERT head that performs quite well.
dependency parsing	BERT	The classifier achieves 77 UAS at dependency parsing, showing BERT's attention captures a substantial amount about syntax.
dependency parsing	BERT	 Table 3: Results of attention-based probing tasks on  dependency parsing. A simple model taking BERT at- tention maps and GloVe word embeddings as input per- forms quite well at dependency parsing. *Not directly  comparable to our numbers; see text.
Medical diagnosis	accuracy	Medical diagnosis is an important task which requires high accuracy and efficiency, especially for patients admitted to the accident and emergency (A&E) department of a hospital.
diagnosis classification task	Precision	 Table 2: Results (average over 5 folds) for the diagnosis classification task for the masked dataset. Precision, recall  and F1 score are macro-averaged across the classes.
diagnosis classification task	recall	 Table 2: Results (average over 5 folds) for the diagnosis classification task for the masked dataset. Precision, recall  and F1 score are macro-averaged across the classes.
diagnosis classification task	F1 score	 Table 2: Results (average over 5 folds) for the diagnosis classification task for the masked dataset. Precision, recall  and F1 score are macro-averaged across the classes.
abbreviation expansion	Accuracy	We model abbreviation expansion as a multi-label classification task, and use the following metrics to measure the performance of different models: Accuracy: Accuracy is defined as the proportion of right predictions in all predictions.
abbreviation expansion	Accuracy	We model abbreviation expansion as a multi-label classification task, and use the following metrics to measure the performance of different models: Accuracy: Accuracy is defined as the proportion of right predictions in all predictions.
CTD	precision threshold	 Table 5: Intrinsic evaluation results (Recall, Specificity,  Precision and F-score) for CTD as we vary size of the  seed training data for the 'no expert but labels' work- flow. Experiments are done with a precision threshold  of 0.8 and we restrict to simplifications with at least 5  words to ensure that they are reasonably specific.
relation extraction	BERT	 Table 5:  Results of relation extraction with  BERT/BioBERT/SciBERT
RQE	accuracy	The evaluation metric used for RQE is accuracy.
Coverage of Validation	RQE  threshold	 Table 3: Coverage of Validation set based on RQE  threshold for Task 3.
RQE task	accuracy	To demonstrate the usefulness of parallel datasets for the RQE task and for easy comparison with the results on the leaderboard, we measure the test accuracy for different dataset combinations using the test dataset labels released by the task organizers post completion of the shared task.
question answering ranking	accuracy	Our models achieve 80% accuracy on medical natural language inference (6.5% absolute improvement over the original baseline), 48.9% accuracy on recognising medical question entail-ment, 0.248 Spearman's rho for question answering ranking and 68.6% accuracy for question answering classification.
Question Answering	Bi-directional	Further, a closed-domain Question Answering technique that uses Bi-directional LSTMs trained on the SquAD dataset to determine relevant ranks of answers fora given question is also discussed.
classification task	accuracy	The KNN algorithm was also used for this classification task and an accuracy of 62.4% was obtained with K=47.
PR	recall	includes two examples of different PR curves in English and Portuguese (where x-axis is recall and y-axis precision).
PR	precision	includes two examples of different PR curves in English and Portuguese (where x-axis is recall and y-axis precision).
sentiment analysis tasks	accuracy	Along with meaning, past work has already shown that ignoring idioms in sentiment analysis tasks will lower the accuracy of a sentiment classifier (, but the non-compositionality of idiom sentiment is not included in the currently acknowledged definition of an idiom and should not be immediately assumed without further research.
MWE-based	accuracy	 Table 3: MWE-based accuracy on dev section for  French with different architectures and comparison  with baselines.
Computer Assisted Translation (CAT)	ATE	Moreover, Computer Assisted Translation (CAT) tools often use ATE methods to aid translators in finding and extracting translation equivalent terms in the target language.
low-resource translation from English	BLEU	We evaluate our methods on low-resource translation from English into twenty target languages , showing consistent improvements of 1.3 BLEU on average across diverse target languages for the multi-task technique.
MT	ORG	 Table 2: DA scores for the best MT system for each translation direction of WMT's 2016-2018 news translation  shared task. Columns ORG and TRS show the absolute difference of the DA scores in those subsets compared to  the whole test set (WMT).
MT	TRS	 Table 2: DA scores for the best MT system for each translation direction of WMT's 2016-2018 news translation  shared task. Columns ORG and TRS show the absolute difference of the DA scores in those subsets compared to  the whole test set (WMT).
WMT's 2016-2018 news translation  shared task	ORG	 Table 2: DA scores for the best MT system for each translation direction of WMT's 2016-2018 news translation  shared task. Columns ORG and TRS show the absolute difference of the DA scores in those subsets compared to  the whole test set (WMT).
WMT's 2016-2018 news translation  shared task	TRS	 Table 2: DA scores for the best MT system for each translation direction of WMT's 2016-2018 news translation  shared task. Columns ORG and TRS show the absolute difference of the DA scores in those subsets compared to  the whole test set (WMT).
MT	TER	We computed automatic MT metrics BLEU (), TER (), and CharacTER () on the first part of each template for which we now had two independent human reference translations.
MT	CharacTER	We computed automatic MT metrics BLEU (), TER (), and CharacTER () on the first part of each template for which we now had two independent human reference translations.
MT	BLEU	We observed only insignificant reduction of the pure MT quality in BLEU and TER; the CharacTER even improved.
MT	TER	We observed only insignificant reduction of the pure MT quality in BLEU and TER; the CharacTER even improved.
MT	HTER	However, even when we compare the adapted MT output on part 2 against the post-edited baseline MT output, we obtain lower HTER and SER scores than for the baseline MT output itself.
MT	SER	However, even when we compare the adapted MT output on part 2 against the post-edited baseline MT output, we obtain lower HTER and SER scores than for the baseline MT output itself.
translation	BLEU	We measure the translation performance in terms of BLEU () as computed with the SacreBLEU software 8.
WMT translation tasks	BLEU	We show that the proposed modification yields consistent improvements over a baseline transformer on standard WMT translation tasks in 5 translation directions (0.9 BLEU on average) and reduces the amount of lexical information passed along the hidden layers.
translation	accuracy	Our experiments show that learning to translate with the XML tags improves translation accuracy, and the beam search accurately generates XML structures.
WMT19 News Translation Task	DA	 Table 11: Official results of WMT19 News Translation Task. Systems ordered by DA score z-score; systems within a cluster
Decoding	length normalisation	Decoding is performed with beam search with abeam size of 6 with length normalisation.
Transfer learning	BLEU	Transfer learning has been found effective in submissions to WMT in previous years:  reported improvements of +2.4 BLEU on the low-resource Estonian→English translation task by transfer learning from Finnish→English.
tokenization	BLEU-cased score	Different writing system between language pair needs specific attention in the tokenization step because of its segmentation results that affect the BLEU-cased score.
word alignment	MERT	We used open source Moses decoder ( and Giza++ for word alignment, Ken-LM (Heafield, 2011) for language model, and MERT for tuning the weight.
translation	BLEU	The translation results were measured by five automatic evaluations provided by the organizer, namely BLEU, BLEU-cased, TER, BEER 2.0, and CharacTER.
translation	BLEU-cased	The translation results were measured by five automatic evaluations provided by the organizer, namely BLEU, BLEU-cased, TER, BEER 2.0, and CharacTER.
translation	TER	The translation results were measured by five automatic evaluations provided by the organizer, namely BLEU, BLEU-cased, TER, BEER 2.0, and CharacTER.
translation	BEER 2.0	The translation results were measured by five automatic evaluations provided by the organizer, namely BLEU, BLEU-cased, TER, BEER 2.0, and CharacTER.
en2de translation task	BLEU	 Table 2: BLEU scores of the models trained for the  en2de translation task. The boldfaced ensembled  model was submitted as the primary submission; the  best performing model with boldfaced BLEU scores  was not ready at submission time.
MT	BLEU 1	This simple combination method performed the best among unsupervised MT systems at WMT19 by BLEU 1 and human evaluation (.
RBMT development	BLEU scores	I describe in this article the use of the shared task data as a kind of a test-driven development workflow in RBMT development and show that it suits perfectly to a modern software engineering continuous integration workflow of RBMT and yields big increases to BLEU scores with minimal effort.
translation	BLEU	 Table 6: Using different kinds of language models for translation on news-test2018. The PBSMT baseline  gets 26.7 BLEU on English-German and 27.5 BLEU on German-English.
translation	BLEU	 Table 6: Using different kinds of language models for translation on news-test2018. The PBSMT baseline  gets 26.7 BLEU on English-German and 27.5 BLEU on German-English.
MT	BLEU	 Table 6: Performance of MT systems with and without  backtranslation for EN→KK (CHRF) and KK→EN  (BLEU).
MT	BPE	 Table 12: Performance of MT systems using differ- ent segmentations (BPE, LVMR and Apertium) for  EN→KK (CHRF) and KK→EN (BLEU). Apertium  was not used for the KK→EN due to time constraints.
MT	Apertium	 Table 12: Performance of MT systems using differ- ent segmentations (BPE, LVMR and Apertium) for  EN→KK (CHRF) and KK→EN (BLEU). Apertium  was not used for the KK→EN due to time constraints.
MT	BLEU	 Table 12: Performance of MT systems using differ- ent segmentations (BPE, LVMR and Apertium) for  EN→KK (CHRF) and KK→EN (BLEU). Apertium  was not used for the KK→EN due to time constraints.
WMT19 News Translation Task	DA score z-score	 Table 8: Preliminary results of WMT19 News Translation Task. Systems ordered by DA score z-score
MT evaluation	BLEU	A lot of MT evaluation metrics from different perspective have been proposed to measure how close machine-generated translations are to professional human translations such as BLEU (), Meteor (Banerjee and), TER () etc.
MT evaluation	TER	A lot of MT evaluation metrics from different perspective have been proposed to measure how close machine-generated translations are to professional human translations such as BLEU (), Meteor (Banerjee and), TER () etc.
MT evaluation	MEANT	Semantic MT evaluation metrics, such as ME-TEOR) and MEANT, require additional linguistic resources to more accurately evaluate the meaning similarity between the MT output and the reference translation.
MT	Extended Edit Distance (EED)	This paper introduces anew MT metric: Extended Edit Distance (EED), based on an extension of the Levenshtein distance.
APE shared task	repetition rate	 Table 2: Basic information about the APE shared task data released since 2015: languages, domain, type of MT technology,  repetition rate and initial translation quality (TER/BLEU of TGT). Grey columns refer to data covering different language pairs  and domains with respect to this year's evaluation round.
MT	repetition rate	 Table 2: Basic information about the APE shared task data released since 2015: languages, domain, type of MT technology,  repetition rate and initial translation quality (TER/BLEU of TGT). Grey columns refer to data covering different language pairs  and domains with respect to this year's evaluation round.
translation	BLEU	The evaluation considered the accuracy of translation (on lower-cased terms), rather than BLEU.
translation quality estimation	BERT	For translation quality estimation at word and sentence levels, this paper presents a novel approach based on BERT that recently has achieved impressive results on various natural language processing tasks.
machine translation	BLEU	Current methods of assessing the quality of machine translation, like BLEU (), are based on comparing the output of a machine translation system with several gold reference translations.
machine translation (MT) outputs	BLEU	Quality estimation () aims to predict the quality of machine translation (MT) outputs without human references, which is what sets it apart from translation metrics like BLEU () or TER ().
machine translation (MT) outputs	TER	Quality estimation () aims to predict the quality of machine translation (MT) outputs without human references, which is what sets it apart from translation metrics like BLEU () or TER ().
MT	DA	Usually MT metrics ( are compared to DA, but we decided to compare our predictions as well, because there is a difference between a number of post-edits and a human assessment.
Neural Machine Translation (NMT)	TER	As the official results show, when trained on a weighted combination of in-domain and artificial training data, our BED system with the conservative-ness penalty improves significantly the translations of a strong Neural Machine Translation (NMT) system by −0.78 and +1.23 in terms of TER and BLEU, respectively.
Neural Machine Translation (NMT)	BLEU	As the official results show, when trained on a weighted combination of in-domain and artificial training data, our BED system with the conservative-ness penalty improves significantly the translations of a strong Neural Machine Translation (NMT) system by −0.78 and +1.23 in terms of TER and BLEU, respectively.
MT	TER	As discussed in §4, our system is able to improve significantly the MT outputs by −0.78 TER () and +1.23 BLEU (), achieving an ex-aequo firstplace in the English-German track.
MT	BLEU	As discussed in §4, our system is able to improve significantly the MT outputs by −0.78 TER () and +1.23 BLEU (), achieving an ex-aequo firstplace in the English-German track.
black-box neural machine translation	BLEU	This model improves over the raw black-box neural machine translation system by 0.9 and 1.0 absolute BLEU points on the WMT 2019 APE development and test set.
MT	BLEU	In the first subtask, our best submission improved the original MT output quality up to +0.98 BLEU and-0.47 TER.
MT	TER	In the first subtask, our best submission improved the original MT output quality up to +0.98 BLEU and-0.47 TER.
translation	BLEU	The translation results are evaluated using BLEU.
MT	accuracy	The NMT falls under the category of corpus-based MT system, which provides better accuracy than Statistical Machine Translation (SMT), corpus-based MT system.
SMT	accuracy	The NMT system used to overcome the demerits of SMT, such as the issue of accuracy and requirement of large datasets.
MT evaluation	BLEU () score	The standard MT evaluation metrics, BLEU () score and TER), were used for the automatic evaluation.
MT evaluation	TER	The standard MT evaluation metrics, BLEU () score and TER), were used for the automatic evaluation.
MT	BLEU	Participants were ranked based on the performance of these MT systems on a test set of Wikipedia translations, as measured by BLEU ().
Sentence pair classification	accuracy	 Table 3: Sentence pair classification accuracy of XLM model on dev sets. Confounders are sentences that we draw  at random to create inadequate translations.
SMT	Baseline	 Table 6: BLEU scores on devtest for SMT systems  trained on two sub-sampled sets. Baseline is the of- ficial baseline as reported in shared task page. We use  sacreBLEU
sentence boundary detection task	F1 score	The CoFiF word embeddings have shown their use in a sentence boundary detection task which achieved good performance obtaining an F1 score of 0.91.
sales prediction task	BIC	Moreover, the preferred models for the sales prediction task are usually linear (e.g., BIC, ARIMA) due to the particularly small datasets.
sales prediction task	ARIMA	Moreover, the preferred models for the sales prediction task are usually linear (e.g., BIC, ARIMA) due to the particularly small datasets.
sentiment analyzer	BERT	To extract reliable sentiment index from Weibo, we build an accurate sentiment analyzer by applying the state-ofthe-art pre-trained model BERT: Bidirectional Encoder Representation from Transformer.
Sentiment analyzer	Conclusion	In the following sections, we introduce 1) Corpus collection and annotation, and the historical sales dataset used in our experiments; 2) Sentiment analyzer and sales prediction models; 3) Experiments and results; 4) Related work; and 5) Conclusion.
Sentence Boundary Detection	BERT Fine-tuning	AIG Investments.AI at the FinSBD Task: Sentence Boundary Detection through Sequence Labelling and BERT Fine-tuning
SBD	accuracy	Until now, research about SBD has been confined to formal texts, such as news and European parliament proceedings, which have high accuracy using rule-based machine learning and deep learning methods due to the perfectly clean text data.
prediction of the beginning/ending of the sentences	BS	 Table 2: Results of the English language using the pointwise pre- diction model and its hyperparameter sets. Here, BS/ES indicates  f1-score for the prediction of the beginning/ending of the sentences  and Ave. denotes the average of BS and ES.
prediction of the beginning/ending of the sentences	ES	 Table 2: Results of the English language using the pointwise pre- diction model and its hyperparameter sets. Here, BS/ES indicates  f1-score for the prediction of the beginning/ending of the sentences  and Ave. denotes the average of BS and ES.
prediction of the beginning/ending of the sentences	BS	 Table 4: Results of the French language using the pointwise predic- tion model and its each hyperparameter sets. Here, BS/ES indicates  f1-score for the prediction of the beginning/ending of the sentences  and Ave. denotes the average of BS and ES.
prediction of the beginning/ending of the sentences	ES	 Table 4: Results of the French language using the pointwise predic- tion model and its each hyperparameter sets. Here, BS/ES indicates  f1-score for the prediction of the beginning/ending of the sentences  and Ave. denotes the average of BS and ES.
SBD	accuracy	With such endeavors, we can expect chances of achieving SBD with reasonable levels of accuracy.
ML classifiers	accuracy	We find that inter-annotator agreement is 0.84 for this task, and ML classifiers are able to correctly identify the type of source of a tweet with 77.2% accuracy without knowledge of the user and their bio or profile, but with 99.9% accuracy when provided with this information.
ML classifiers	accuracy	We find that inter-annotator agreement is 0.84 for this task, and ML classifiers are able to correctly identify the type of source of a tweet with 77.2% accuracy without knowledge of the user and their bio or profile, but with 99.9% accuracy when provided with this information.
ML	accuracy	The results show that the ML algorithms correctly classified tweets with up to 77.2% accuracy.
identifying Ha-dith segments	accuracy	The result demonstrates that bi-grams are effective in identifying Ha-dith segments with 92.5% accuracy.
classification of Arabic dialect texts	accuracy	This includes research about classification of Arabic dialect texts, but due to the lack of Arabic dialect text corpora this research has not achieved a high accuracy.
Dialog-acts prediction	accuracy	 Table 2: Dialog-acts prediction accuracy for classifiers  trained on validation set of different datasets.
state tracking	accuracy	Mrkši´ employs CNN and pretrained embeddings to further improve the state tracking accuracy.
SLU	accuracy-speed	Co, but to compare to prior work, in one test experiment we use the lowercased, tokenized version of (Goo et al., 2018) 2 .  We evaluate multiple models from each of our model paradigms to help determine what modeling structures are necessary for SLU, and where the best accuracy-speed tradeoffs are.
DST	accuracy	Furthermore, we demonstrate how domain and intent information can be mixed in with the temporal information in this framework to improve DST accuracy.
SwDA DA	accuracy	 Table 3: SwDA DA categories that improve using pre- trained utterance embeddings with % improvements in  accuracy over other experimental settings.
coreference resolution	precision	Performance on out-of-context questions was evaluated on Bench'It, a dataset containing 150 open ended questions about general knowledge in French) 4 . The system reached a macro precision, recall and F-1 of 64.14%, 64.33% and 63.46% respectively 5 . We also evaluated the coreference resolution model on the test-set of CALOR, obtaining an average precision, recall and F-1 of 65.59%, 48.86% and 55.77% respectively.
coreference resolution	recall	Performance on out-of-context questions was evaluated on Bench'It, a dataset containing 150 open ended questions about general knowledge in French) 4 . The system reached a macro precision, recall and F-1 of 64.14%, 64.33% and 63.46% respectively 5 . We also evaluated the coreference resolution model on the test-set of CALOR, obtaining an average precision, recall and F-1 of 65.59%, 48.86% and 55.77% respectively.
coreference resolution	F-1	Performance on out-of-context questions was evaluated on Bench'It, a dataset containing 150 open ended questions about general knowledge in French) 4 . The system reached a macro precision, recall and F-1 of 64.14%, 64.33% and 63.46% respectively 5 . We also evaluated the coreference resolution model on the test-set of CALOR, obtaining an average precision, recall and F-1 of 65.59%, 48.86% and 55.77% respectively.
Lexical diversity	F.Inc	 Table 2: Lexical diversity and fact inclusion analysis results. Model names are abbreviated according to Table 1. F.Inc denotes
fact inclusion analysis	F.Inc	 Table 2: Lexical diversity and fact inclusion analysis results. Model names are abbreviated according to Table 1. F.Inc denotes
answer utterance selection	F1-score	BERT in particular depicts promising results, an accuracy of 74.2% for answer utterance selection and an F1-score of 64.2% for answer span selection, suggesting that the FriendsQA task is hard yet has a great potential of elevating QA research on multiparty dialogue to another level.
SDS	HERMIT	With respect to previous approaches to NLU for SDS, HERMIT stands out for being a cross-domain, multi-task architecture, capable of recognising multiple intents/frames in an utterance.
Classification	accuracy	 Table 1: Classification results, training set, average accuracy over 3 folds and corresponding loss for 1 and 2 s  segments. All datasets are balanced, the prior is 0.5.
Classification	accuracy	 Table 2: Classification results, validation set, average accuracy over 3 folds and corresponding loss for 1 and 2 s  segments. All datasets are balanced, the prior is 0.5.
book recommendation dialogue agent	B.	To investigate these issues, we created a book recommendation dialogue agent, B. Rex.
relation prediction	recall	 Table 3: Scores of the systems for relation prediction, using the full relation set of the PDTB. The predicted  relations are either inferred from the predicted primitives ("Primitives"), or directly predicted ("Relations").We  report hierarchical recall (h-R) and hierarchical precision (h-P), along with max-h-P max-h-R, and accuracy.
relation prediction	precision	 Table 3: Scores of the systems for relation prediction, using the full relation set of the PDTB. The predicted  relations are either inferred from the predicted primitives ("Primitives"), or directly predicted ("Relations").We  report hierarchical recall (h-R) and hierarchical precision (h-P), along with max-h-P max-h-R, and accuracy.
relation prediction	accuracy	 Table 3: Scores of the systems for relation prediction, using the full relation set of the PDTB. The predicted  relations are either inferred from the predicted primitives ("Primitives"), or directly predicted ("Relations").We  report hierarchical recall (h-R) and hierarchical precision (h-P), along with max-h-P max-h-R, and accuracy.
parsing	Estimate	We plot the learning curves for parsing: Estimate of the effort required in gf2ud.
parsing	accuracy	Despite the lower improvements with increasing treebank sizes, there is still a consistent improvement in parsing accuracies with the best accuracy of 65.4 LAS using 10K synthetic samples (shown in).
parsing	LAS	Despite the lower improvements with increasing treebank sizes, there is still a consistent improvement in parsing accuracies with the best accuracy of 65.4 LAS using 10K synthetic samples (shown in).
aspect classification	BERT	The aspect classification model, described in Section 3.4, uses sentence pair classification from BERT (.
CORE register detection	precision	Biber and Egbert (2016) evaluated automatic CORE register detection performance with stepwise discriminant analysis, achieving 34% precision and 40% recall.
CORE register detection	recall	Biber and Egbert (2016) evaluated automatic CORE register detection performance with stepwise discriminant analysis, achieving 34% precision and 40% recall.
Bagging	accuracy	Bagging, in general, does deliver improvements in model accuracy compared to a baseline of a single model but does not outperform plain majority voting.
NER problem	PER	Finally, it is important to note that it is a classical NER problem with four classes, nevertheless only the PER, LOC and DATE classes are useful for the de-identification problem.
zero-shot transfer	F1	It is noteworthy that zero-shot transfer benefits only to a limiting degree from more source data (F1 increases by 3% when training on all English CoNLL data).
semantic parsing	accuracy	Our empirical results show that only gold-standard syntactic information leads to consistent improvements in semantic parsing accuracy, and that the magnitude of these improvements varies with the specific combination of the syntactic and the semantic representation used.
Summarization evaluation	ROUGE	Of the three, Summarization evaluation is most closely related to SAG: When determining the quality of an automatic summary, the standard evaluation method ROUGE (derived from Translation evaluation's BLEU) compares candidate summaries against manually created references, with the goal of comparing the meaning of the two texts with string-level evaluation tools.
Summarization evaluation	BLEU	Of the three, Summarization evaluation is most closely related to SAG: When determining the quality of an automatic summary, the standard evaluation method ROUGE (derived from Translation evaluation's BLEU) compares candidate summaries against manually created references, with the goal of comparing the meaning of the two texts with string-level evaluation tools.
Summarization evaluation	Recall	We follow Summarization evaluation practice and experiment with Recall and F-Score, with different weightings of Precision and Recall.
Summarization evaluation	F-Score	We follow Summarization evaluation practice and experiment with Recall and F-Score, with different weightings of Precision and Recall.
Summarization evaluation	Precision	We follow Summarization evaluation practice and experiment with Recall and F-Score, with different weightings of Precision and Recall.
Summarization evaluation	Recall	We follow Summarization evaluation practice and experiment with Recall and F-Score, with different weightings of Precision and Recall.
parameter sweeping	ROUGE	During parameter sweeping, the largest drops in τ compared to the optima are observed (in order) by changing the ROUGE variant, the F weighting and the combination of stemming and stop words (for all three corpora).
parameter sweeping	F weighting	During parameter sweeping, the largest drops in τ compared to the optima are observed (in order) by changing the ROUGE variant, the F weighting and the combination of stemming and stop words (for all three corpora).
Summarization	ROUGE-1	This is inline with observations from the Summarization community, where the numerically highest scores are usually achieved using ROUGE-1 and the lowest using ROUGE-4.
SAG task	recall of n-gram overlap	This is plausible for the SAG task, since the recall of n-gram overlap between the student answer and reference answer shows how much of the reference answer content the student replicated.
SAG	F	8 shows evidence of the high unigram baseline for SAG at at least F=59.7 (RF on SEB; F=65.1 SVM) and up to F=86.7 (RF on ASAP).
SAG	RF	8 shows evidence of the high unigram baseline for SAG at at least F=59.7 (RF on SEB; F=65.1 SVM) and up to F=86.7 (RF on ASAP).
SAG	F=65.1 SVM)	8 shows evidence of the high unigram baseline for SAG at at least F=59.7 (RF on SEB; F=65.1 SVM) and up to F=86.7 (RF on ASAP).
SAG	F	8 shows evidence of the high unigram baseline for SAG at at least F=59.7 (RF on SEB; F=65.1 SVM) and up to F=86.7 (RF on ASAP).
title detection task	F1 score	The evaluation metric used for this title detection task is the weighted F1 score.
sentiment analysis	accuracy	We show on two sentiment analysis tasks that utilizing pre-trained word embed-dings improves the accuracy over the baseline method.
prediction	accuracy	We show empirically in Section 3 that the use of pre-trained word embeddings improves prediction accuracy and generates better classification lexicons.
SVM Directional	Accuracy	 Table 2. SVM Directional Accuracy Results
segmentation	accuracy	Experiments show that our proposed method improves the segmentation accuracy measured in F 1 by 21.1% while maintains approximately the same latency, and reduces the BLEU loss to the oracle segmenta-tion by 28.6%, when compared to a strong baseline of the RNN LM-based method.
segmentation	F 1	Experiments show that our proposed method improves the segmentation accuracy measured in F 1 by 21.1% while maintains approximately the same latency, and reduces the BLEU loss to the oracle segmenta-tion by 28.6%, when compared to a strong baseline of the RNN LM-based method.
segmentation	BLEU loss	Experiments show that our proposed method improves the segmentation accuracy measured in F 1 by 21.1% while maintains approximately the same latency, and reduces the BLEU loss to the oracle segmenta-tion by 28.6%, when compared to a strong baseline of the RNN LM-based method.
ASR/MT	accuracy	By comparing various ASR/MT systems with different error profiles , our results demonstrate that a richer document representation can consistently overcome issues in low translation accuracy for CLIR in low-resource settings.
MT	MAGMATic	The main contribution of this paper is to provide the MT community with MAGMATic (Multi-domain Academic Gold standard with Manual Annotation of Terminology), a novel Italian-English benchmark which allows MT evaluation focused on terminology translation.
terminology translation	Term Hit Rate (THR) metric	The evaluation focused on terminology translation is based on the Term Hit Rate (THR) metric.
terminology translation	perfect	Focusing on the evaluation of terminology translation, perfect and partial THR scores were computed on MAGMATic for GT and the two MMT systems.
terminology translation	THR	Focusing on the evaluation of terminology translation, perfect and partial THR scores were computed on MAGMATic for GT and the two MMT systems.
MT	TER	analyzed four MT systems for English into German by comparing different TER () scores and sub-scores, and applied the WER-based approach proposed by Popovi´cPopovi´c and Ney (2011) fora multilingual and multifaceted evaluation of eighteen MT systems for nine translation directions including six languages from four different families.
Translation	BLEU	Translation quality is measured in terms of case-sensitive 4-gram BLEU).
classification	accuracy	This paper discusses our work on developing and testing different techniques for automatically generating valid synthetic data and determining whether synthetic data increases classification accuracy.
oracle translation	BLEU	The dataset used for the oracle translation statistics and the BLEU evaluation comprised 2,083,576 English and Japanese parallel sentence pairs from Opensubtitles 2018 (Lison et al., 2018).
NMT	accuracy	However, it remains unclear if such techniques are well suited for NMT, where language-agnostic tokenizations, e.g. byte-pair encoding (BPE) (, are widely used. has looked into Arabic SMT and NMT, achieving the highest accuracy using the Penn Arabic Treebank (ATB) tokenization, with 51.2 and 49.7 BLEU points for SMT and NMT, respectively.
NMT	BLEU	However, it remains unclear if such techniques are well suited for NMT, where language-agnostic tokenizations, e.g. byte-pair encoding (BPE) (, are widely used. has looked into Arabic SMT and NMT, achieving the highest accuracy using the Penn Arabic Treebank (ATB) tokenization, with 51.2 and 49.7 BLEU points for SMT and NMT, respectively.
SMT	accuracy	However, it remains unclear if such techniques are well suited for NMT, where language-agnostic tokenizations, e.g. byte-pair encoding (BPE) (, are widely used. has looked into Arabic SMT and NMT, achieving the highest accuracy using the Penn Arabic Treebank (ATB) tokenization, with 51.2 and 49.7 BLEU points for SMT and NMT, respectively.
SMT	BLEU	However, it remains unclear if such techniques are well suited for NMT, where language-agnostic tokenizations, e.g. byte-pair encoding (BPE) (, are widely used. has looked into Arabic SMT and NMT, achieving the highest accuracy using the Penn Arabic Treebank (ATB) tokenization, with 51.2 and 49.7 BLEU points for SMT and NMT, respectively.
MT	TER	We report on BLEU because it is a centerpiece of the development of MT systems, and on TER because it is the primary evaluation metric at the WMT APE shared task.
Neural Machine Translation	Speed	Post-editing Productivity with Neural Machine Translation: An Empirical Assessment of Speed and Quality in the Banking and Finance Domain
MT	Acceptable' level	Although some score decreases were found in the pre-edited MT, most of the segments stayed above the 'Acceptable' level on the human evaluation scale.
MT	accuracy errors	Errors in MT output for Russian as a target language show almost equal shares of fluency and accuracy errors in PBSTM and prevalence of the accuracy errors in NMT.
MT	accuracy errors	Errors in MT output for Russian as a target language show almost equal shares of fluency and accuracy errors in PBSTM and prevalence of the accuracy errors in NMT.
machine translation	BLEU	The typical evaluation of machine translation, due to a requirement of fast, automatic metrics during the training phase, typically involves the comparison with a set of human translation in what is calculated as the BLEU or the NIST scores of the translation ().
Machine translation	exactness	Machine translation in general does not produce the exactness in forms required in term translation.
MT	precision	The same scheme is also applied for with context translation We recognize that, since both MT systems and human translation do not include an annotation as to the exact location of the term translation in the sentence, we are unable to verify the precision of the term translation or the F1 score.
MT	F1 score	The same scheme is also applied for with context translation We recognize that, since both MT systems and human translation do not include an annotation as to the exact location of the term translation in the sentence, we are unable to verify the precision of the term translation or the F1 score.
MT	METEOR	Our pivot MT system took the first place in terms of METEOR and translation edit rate (TER) in the shared task.
MT	translation edit rate (TER)	Our pivot MT system took the first place in terms of METEOR and translation edit rate (TER) in the shared task.
Qualitative analysis of a sentence translated	TER	 Table 2: Qualitative analysis of a sentence translated by all models for Spanish to English translation. Fragments in bold face  are translation mistakes, and fragments in italics are translation alternatives that, while being penalised by TER and BLEU, can  be considered correct.
Qualitative analysis of a sentence translated	BLEU	 Table 2: Qualitative analysis of a sentence translated by all models for Spanish to English translation. Fragments in bold face  are translation mistakes, and fragments in italics are translation alternatives that, while being penalised by TER and BLEU, can  be considered correct.
MT workflow	AT	More than 75% of all translation volume for Office content is now routed through a recycling and MT workflow (a process which we refer to internally as 'AT ' or 'Automatic Translation'), for up to 36 languages.
MT	GNMT	Due to space constraints, here we present average scores of the MT engines' AEM results grouped by partner (Engine) against average scores of GNMT (Baseline), pointing out particularly interesting aspects.
MT	HLT	To complement the article, §2 presents previous work on MT for Peruvian languages, §3 introduces more details about the target language, and finally, §7 concludes and proposes further steps.: Details of the parallel corpora for spa-shp per domain and in total: S = number of sentences; rshp-spa = average of the ratioshp-spa per sentence; T = number of tokens; |V| = vocabulary size; HLT = tokens with frequency equals to one.
SMT	BLEU	First steps were taken by to apply NMT methods to EN-GA MT, although the resulting NMT system performed significantly worse than SMT, scoring more than 6 BLEU lower on an in-domain test set.
misalignment detection	BLEU score	We show that applying misalignment detection on a synthetic corpus before adding it to the parallel training data results in small increases in BLEU score and could be a useful strategy in terms of data selection.
neural machine translation (NMT)	accuracy	Current neural machine translation (NMT) approaches achieve state-of-the-art accuracy in high-resource contexts.
MT evaluation	Assessment	This test data is supplemented by two additional MT evaluation resources: Assessment and HyTER.
MT evaluation	HyTER	This test data is supplemented by two additional MT evaluation resources: Assessment and HyTER.
POS tagging	F- measure	 Table 4: results of POS tagging and lemmatisation (F- measure
WTE	SE	The current research aims to predict WTE of a source text as well as its SE to a target text without having access to the actual translation products.
Neural machine translation (NMT)	accuracy	Neural machine translation (NMT) has been widely used owing to its high accuracy.
translation	accuracy	We evaluate training speed, convergence speed, and translation accuracy to compare the performance of the proposed approach as shown in (hereafter referred to as HybridNMT) with the baseline model shown in with/without data/model parallelism.
translation	BLEU	The translation models were evaluated by the BLEU and METEOR metrics, assessment of fluency and adequacy, and measurement of the post-editing effort.
translation	METEOR	The translation models were evaluated by the BLEU and METEOR metrics, assessment of fluency and adequacy, and measurement of the post-editing effort.
translation	PE time	 Table 4. Average difference between translation and PE time
MT	FID	Then I illustrate challenges that the developers of literary MT systems must address by discussing a particularly thorny question of literary translation: rendering free indirect discourse (henceforth FID, fora definition see below) in different languages.
MT	accuracy errors	We annotated and classified all MT errors in the first chapter of the novel making use of the SCATE error taxonomy, which differentiates between fluency (well-formedness of the target language) and accuracy errors (correct transfer of source content).
speech recognition	confidence score	In speech recognition, it is desirable to have a confidence score which has a strong correlation with the correctness of recognition.
parsing	accuracy	We demonstrate that the proposed two-stage framework is beneficial for improving the parsing accuracy in a standard dataset called GeoQuery for the task of generating logical forms from a set of questions about the US geography.
tagger	F-score	Concerning the tagger, we compare the F-score of the tagger trained on WB and on CB.
WB tagging	F-score	The following are the results of WB tagging (​ ) and CB tagging after the recombination (​ F-score of word level POS (XPOS) for our character-based tagger after the recombination As we can see from these two tables above, the training on a character base has greatly improved the performance of the tagger.
grammar induction	accuracy	NLP researchers working on grammar induction and unsupervised parsing have achieved substantial gains in recovering dependency trees on the basis of corpus statistics, but overall accuracy remains modest (;.
parsing	accuracy	It offers a good trade-off between parsing accuracy and speed.
parsing	speed	It offers a good trade-off between parsing accuracy and speed.
dependency parsing	speed	However, recent work on dependency parsing as sequence labeling ignore the pre-processing time of Part-of-Speech tagging-which is required for this task-in the evaluation of speed while other studies showed that Part-of-Speech tags are not essential to achieve state-of-the-art parsing scores.
tagger	F-score	Concerning the tagger, we compare the F-score of the tagger trained on WB and on CB.
parsing	accuracy	 Instead of aiming solely at reaching better parsing accuracy, the project also aims to offer a number of tangible deliverables.
UD parsers	accuracy	Adding to this the desired goal that UD parsers should have high accuracy on text which has not been tokenized, we have a situation where UD has to deal not just with common syntactic constructions, but with a multitude of genre-specific and stylistic varieties.
IE	accuracy	Our goal is to develop an IE system which scales up to extract as many types of relations and events as possible with a minimum amount of porting effort combined with high accuracy.
porting	accuracy	Our goal is to develop an IE system which scales up to extract as many types of relations and events as possible with a minimum amount of porting effort combined with high accuracy.
DP	misses	 Table 3: Performance measures for DP with respect to hits, false alarms, and misses.
SML	accuracy	 Table 1: Results of Experiments. Most SML tools deliver the best result only. SVM_Light produces ranked  results, allowing to measure the accuracy of the top five alternatives (Best5).
information seeking behavior	QUESTION	This track aims at doing exactly that fora special (and popular) class of information seeking behavior: QUESTION ANSWERING.
information seeking behavior	ANSWERING	This track aims at doing exactly that fora special (and popular) class of information seeking behavior: QUESTION ANSWERING.
IE	GE	The final goal of our IE effort is to further extract open-ended general events (GE, or level 3 IE) for information like who did what (to whom) when (or how often) and where.
IE	BR	The IE results are stored in a database which is the basis for IE-related applications like QA, BR (Browsing, threading and visualization) and AS (Automatic Summarization).
IE	AS	The IE results are stored in a database which is the basis for IE-related applications like QA, BR (Browsing, threading and visualization) and AS (Automatic Summarization).
segmentation	accuracy	We compared two methods: (1) using only the segmentation algorithm with default analysis which is a baseline of our system and so is needed to estimate the accuracy of the algorithm.
segmentation	accuracy	(2) using both the built-in dictionary and the segmentation algorithm which reflects system accuracy as a whole.
segmentation	accuracy	Also, the overall segmentation accuracy for the gold standard is about 97.29% which is a very good result for the application system.
segmentation	accurate	With our new method, the result for segmentation is as accurate as 97.29%.
Result	accuracy	 Table 4: Result 2: Segmentation accuracy for Compound Noun
Segmentation	accuracy	 Table 4: Result 2: Segmentation accuracy for Compound Noun
parsing	coverage	The thus established results constitute one data point in the trade-off between ambiguity reduction on one side, which is in turn related to parsing speed, and loss in coverage on the other.
parsing	coverage	For both languages, inhibiting pruning on the most variable symbol has the expected effect of increasing both parsing time and coverage.
tagging	accuracy	First of all, we determine the tagging accuracy averaged over ten iterations.
Part-of-speech tagging	accuracy	 Table 2: Part-of-speech tagging accuracy for the NEGRA corpus, averaged over 10 test runs, training and  test set are disjoint. The table shows the percentage of unknown tokens, separate accuracies and standard  deviations for known and unknown tokens, as well as the overall accuracy.
Part-of-speech tagging	accuracy	 Table 2: Part-of-speech tagging accuracy for the NEGRA corpus, averaged over 10 test runs, training and  test set are disjoint. The table shows the percentage of unknown tokens, separate accuracies and standard  deviations for known and unknown tokens, as well as the overall accuracy.
information extraction (IE)	accuracy	Current information extraction (IE) systems are quite successful in efficient processing of large free text collections due to the fact that they can provide a partial understanding of specific types of text with a certain degree of partial accuracy using fast and robust language processing strategies (basically finite state technology).
NE tagging	accuracy	Intense research has been focused on improving NE tagging accuracy using several different techniques.
dialogue processing	GROUNDING	This paper describes an implementation of some key aspects of a theory of dialogue processing whose main concerns are to provide models of GROUNDING and of the role of DISCOURSE OBLIGATIONS in an agent's deliberation processes.
Speech tagging	error	Part of Speech tagging for English seems to have reached the the human levels of error, but full morphological tagging for inflectionally rich languages, such as Romanian, Czech, or Hungarian, is still an open problem, and the results are far from being satisfactory.
parsing	precision	The grammars are evaluated on parsing speed, precision/coverage, and improvement of word and sentence accuracy of the integrated system.
parsing	accuracy	The grammars are evaluated on parsing speed, precision/coverage, and improvement of word and sentence accuracy of the integrated system.
recognition	accuracy	Sentence-derived CDGs significantly improve recognition accuracy over the conventional CDG but are less general.
parsing	precision	We present anew parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established [5,9,10,15,17] "stan-dard" sections of the Wall Street Journal tree-bank.
parsing	recall	We present anew parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established [5,9,10,15,17] "stan-dard" sections of the Wall Street Journal tree-bank.
summarization	accuracy	The summarization accuracy peaks at 0.47, a 9% absolute improvement over the a = 0.0-baseline and only about 5% absolute lower than for reference summaries and).
WER reduction	accuracy	It illustrates how WER reduction and summary accuracy improvement can be achieved by using our confidence boosting method.
summarization	accuracy	 Table 3: Reference summarization accuracy of MMR  o~  summaries
predict problematic dialogues	accuracy	The results show that we can learn to predict problematic dialogues using fully automatic features with an accuracy ranging from 72% to 87%, depending on how much of the dialogue the system has seen so far.
parsing	accuracy	Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.
parsing	TREEBANK	Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.
parsing	accuracy	Since 1995, a few statistical parsing algorithms) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.
parsing	TREEBANK	Since 1995, a few statistical parsing algorithms) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.
sentence breaking punctuation	error rate	On the sentence breaking punctuation the tagger performed extremely wellan error rate of 0.39% on the WSJ and 0.25% on the Brown Corpus.
cog-nate alignment	accuracy	The algorithm performs better on cog-nate alignment, in terms of accuracy and efficiency, than other algorithms reported in the literature.
semantic interpretation	recall	Our focus is on the empirical evaluation of this approach to semantic interpretation , i.e., its quality in terms of recall and precision.
semantic interpretation	precision	Our focus is on the empirical evaluation of this approach to semantic interpretation , i.e., its quality in terms of recall and precision.
parsing	accuracy	Despite the incomplete parsing strategy and the radical pruning , the initial evaluation results show that the loss of parsing accuracy is acceptable.
parsing	accuracy	Despite the incomplete parsing strategy and the radical pruning , the initial evaluation results show that the loss of parsing accuracy is acceptable.
parsing	accuracy	Despite the incomplete parsing strategy and the radical pruning, the initial evaluation results show that the loss of parsing accuracy is acceptable.
parsing	accuracy	Despite the incomplete parsing strategy and the radical pruning, the initial evaluation results show that the loss of parsing accuracy is acceptable.
word sense disambiguation (WSD)	precision	This research paper describes the early stages I of our efforts to develop a word sense disambiguation (WSD) algorithm aimed at improving the precision of our cross-language retrieval system.
Selectional Pattern Queries and Responses)	accuracy	This paper presents SPQR (Selectional Pattern Queries and Responses), a module of the PUNDIT text-processing system designed to facilitate the acquisition of domain-specific semantic information, and to improve the accuracy and efficiency of the parser.
handwriting selectional	consistency	Unfortunately, handwriting selectional restrictions is not an easy matter, because it is time consuming and it is hard to keep consistency among the data when the lexicon has several hundred or thousand words.
parsing	accuracy	We then describe several heuristics for improving parsing accuracy and coverage, such as closest attachment of mod-ifiers, statistical grammars, and fitted parses, and present a quantitative evaluation of the improvements obtained with each strategy.
parsing	coverage	We then describe several heuristics for improving parsing accuracy and coverage, such as closest attachment of mod-ifiers, statistical grammars, and fitted parses, and present a quantitative evaluation of the improvements obtained with each strategy.
parsing	precision	Most automatic parsing systems generate considerably more detailed bracketing than that produced by the Treebank; in such cases precision would be reduced, but the recall and crossing count would still accurately reflect the correctness of the system-generated parses.
Parsing	precision	 Table 2: Parsing evaluation over 206 sentences, showing average  and precision.
machine translation	precision	When considering application to machine translation, the former leads to problems in the precision of resolutions because it is restricted to using specified information.
translation	accuracy	A series of experiments to evaluate ENTS using economic news in the AP news stories showed the translation accuracy was about 50 % higher than with our conventional rule-based MT method.
translation	accuracy	With some news topics, troubles with fixed expressions lows the translation accuracy of the MT system.
translation	accuracy	Actually, these problems reduce the translation accuracy of our rule-based MT system to only 20%, which is too low for practical use.
MT	ENTS	This paper presents anew English-to-Japanese MT system for economic news stories, which is called ENTS (Economic News stories machine Translation System), to process fixed expressions effectively.
MT	accuracy	This model was practically implemented and incorporated into the English-Japanese MT system, and provided about 75% accuracy in the practical translation use.
terminology extraction	recall	When terminology extraction is applied to automatic indexing, two measures are important: recall and precision.
terminology extraction	precision	When terminology extraction is applied to automatic indexing, two measures are important: recall and precision.
tokenization of Chinese text	accuracy	We present empirical evidence for four points concerning tokenization of Chinese text: (I) More rigorous "blind" evaluation methodology is needed to avoid inflated accuracy measurements; we introduce the nk-blind method.
Statistical lexical acquisition	accuracy	(3) Statistical lexical acquisition is a practical means to greatly improve tokenization accuracy with unknown words, reducing error rates as much as 32.0%.
Statistical lexical acquisition	error rates	(3) Statistical lexical acquisition is a practical means to greatly improve tokenization accuracy with unknown words, reducing error rates as much as 32.0%.
tokenization	accuracy	(3) Statistical lexical acquisition is a practical means to greatly improve tokenization accuracy with unknown words, reducing error rates as much as 32.0%.
tokenization	error rates	(3) Statistical lexical acquisition is a practical means to greatly improve tokenization accuracy with unknown words, reducing error rates as much as 32.0%.
parsing	accuracy	We will also discuss the relationship between parsing accuracy and the size of training corpus.
segmentation	recall	The preliminary open tests show that the segmentation precision of CSeg&Tagl.0 is about 98.0%-99.3%, POS tagging precision about 91.0% 97.1%, and the recall and precision for unknown words are ranging from 95.0% to 99.0% and from 87.6% to 95.3% respectively.
segmentation	precision	The preliminary open tests show that the segmentation precision of CSeg&Tagl.0 is about 98.0%-99.3%, POS tagging precision about 91.0% 97.1%, and the recall and precision for unknown words are ranging from 95.0% to 99.0% and from 87.6% to 95.3% respectively.
IR	precision range	At the same time, while the gain in recall and precision has not been negligible (we recorded 10-20% increases in precision), no dramatic breakthrough has occurred either.l Currently, the state-of-the art statistical and probabilistic IR system perform at about 20-40% precision range for arbitrary ad-hoc retrieval tasks.
information extraction (IE)	accuracy	The system takes advantage of information extraction (IE) technology in novel ways to improve the accuracy of cross-linguistic retrieval and to provide innovative methods for browsing and exploring multilingual document collections.
information extraction (IE)	accuracy	The system takes advantage of information extraction (IE) technology in novel ways to improve the accuracy of cross-linguistic retrieval and to provide innovative methods for browsing and exploring multilingual document collections.
speech recognition and synthesis	VIEWER	The prototype system shown in this film uses off-the-shelf speech recognition and synthesis technology combined with the NAUTILUS system and VIEWER (Solan and Hill, 1993), a 3D tactical scenario playback system developed by NRL's Tactical Electronic Warfare Division fora separate project.
sentiment classification task	accuracy	We collect substring rationales fora sentiment classification task (Pang and Lee, 2004) and use them to obtain significant accuracy improvements for each annotator.
Pronoun resolution	accuracy	 Table 4: Pronoun resolution accuracy on nouns in current  or previous sentence in MUC.
SRL	accuracy	We suggested that the previous lack of progress in dependency-based SRL was due to low parsing accuracy.
parsing	accuracy	We suggested that the previous lack of progress in dependency-based SRL was due to low parsing accuracy.
parse	accuracy	Furthermore, such features significantly improve parse accuracy over exact first-order methods.
stacking MST 2O	LAS	 Table 3: Results of stacking MST 2O with itself at both level 0 and level 1. Column 2 enumerates LAS for MST 2O .  Columns 3-6 enumerate results for four different stacked feature subsets. Bold indicates best results for a particular  language.
parsing	accuracy	hand side is known to be the nth Catalan Number . All binarizations lead to the same parsing accuracy, but maybe different parsing efficiency, i.e. parsing speed.
parsing	accuracy	As discussed in Section 6, the parsing accuracy is not changed in this experiment.
SVM prediction	accuracy	 Table 7: SVM prediction accuracy, linear kernel
Extraction Constraints	Equation	3. Extraction Constraints -This condition selected the best paraphrase according to Equation 10.
translation	BLEU	Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30-best parses.
IWSLT 2005 Chinese to English translation task	MERT	We evaluate our model on the IWSLT 2005 Chinese to English translation task, using the 2004 test set as development data for tuning the hyperparameters and MERT training the benchmark systems.
summaries	ROUGE	We carried out automatic evaluation of our summaries using ROUGE) toolkit, which has been widely adopted by DUC for automatic summarization evaluation.
phrase alignment	consistency	Previous phrase alignment work has sacrificed consistency for efficiency, employing greedy hill-climbing algorithms and constraining inference with word alignments ().
classification	precision	With the novel method, the classification precision of our system is 94.68%, which outperforms the strong baseline significantly.
translation	BLEU	We show slightly improved translation quality in terms of BLEU and TER and address various constraints to speedup the training based on Expectation-Maximization and to lower the overall number of triplets without loss in translation performance .
translation	TER	We show slightly improved translation quality in terms of BLEU and TER and address various constraints to speedup the training based on Expectation-Maximization and to lower the overall number of triplets without loss in translation performance .
translation	Expectation-Maximization	We show slightly improved translation quality in terms of BLEU and TER and address various constraints to speedup the training based on Expectation-Maximization and to lower the overall number of triplets without loss in translation performance .
MT	TM	 Table 6. MT scores with updated TM and LM
parser	accuracy	This approach achieves parser accuracy comparable to that on newspaper data without the need for annotated parse trees in the new domain.
IMT	WSR	Finally, the word graphs obtained were used within the IMT procedure to produce the reference translation contained in the test set, measuring WSR and MAR.
IMT	MAR	Finally, the word graphs obtained were used within the IMT procedure to produce the reference translation contained in the test set, measuring WSR and MAR.
IMT	MA	As a baseline system, we report the traditional IMT framework, in which no MA is taken into account.
LTAG dependency parsing	accuracy	We apply this algorithm to LTAG dependency parsing, and achieve significant improvement on accuracy over the previous best result on the same data set.
alignment	accuracy	Experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as "Hansard".
predicate argument structure analysis	precision	Recently, predicate argument structure analysis has attracted the attention of researchers because this information can increase the precision of text processing tasks, such as machine translation, information extraction), question answering (), and summarization ().
parsing	accuracy	Like, we evaluate the parsing accuracy by the precision of lexical heads (the percentage of input words, excluding punctuation, that have been assigned the correct parent) and by the percentage of complete matches, in which all words excluding punctuation have been assigned the correct parent.
parsing	precision	Like, we evaluate the parsing accuracy by the precision of lexical heads (the percentage of input words, excluding punctuation, that have been assigned the correct parent) and by the percentage of complete matches, in which all words excluding punctuation have been assigned the correct parent.
parsing	accuracy	The parsing accuracy generally increases as the beam size increases, while the quantity of increase becomes very small when B becomes large enough.
RD	precision	As expected, RD alone does not match combined precision and recall of state-of-the-art supervised systems.
RD	recall	As expected, RD alone does not match combined precision and recall of state-of-the-art supervised systems.
mention detection system	BLEU	To show the effectiveness of cross-language mention propagation information in improving mention detection system performance in Arabic, Chinese and Spanish, we use three SMT systems with very competitive performance in terms of BLEU).
SMT	BLEU	To show the effectiveness of cross-language mention propagation information in improving mention detection system performance in Arabic, Chinese and Spanish, we use three SMT systems with very competitive performance in terms of BLEU).
mention detection	Precision (P)	 Table 2: Performance of Arabic, Chinese, English and  Spanish mention detection systems. Performance is pre- sented in terms of Precision (P), Recall (R), and F- measure (F). The column (N) displays the number of  mentions in the test set.
mention detection	Recall (R)	 Table 2: Performance of Arabic, Chinese, English and  Spanish mention detection systems. Performance is pre- sented in terms of Precision (P), Recall (R), and F- measure (F). The column (N) displays the number of  mentions in the test set.
mention detection	F- measure (F)	 Table 2: Performance of Arabic, Chinese, English and  Spanish mention detection systems. Performance is pre- sented in terms of Precision (P), Recall (R), and F- measure (F). The column (N) displays the number of  mentions in the test set.
speech recognition	Maximum A Posteriori (MAP) decision rule	Statistical language processing systems for speech recognition, machine translation or parsing typically employ the Maximum A Posteriori (MAP) decision rule which optimizes the 0-1 loss function.
MBR decoding	BLEU	We now present experiments to evaluate MBR decoding on lattices under the linear corpus BLEU  gain.
coreference	B 3 scores	 Table 4: Comparison of coreference results in B 3 scores  on the ACE-2 datasets.
coreference	precision	 Table 5: Our coreference results in precision, recall, and  F1 for pairwise resolution.
coreference	recall	 Table 5: Our coreference results in precision, recall, and  F1 for pairwise resolution.
coreference	F1	 Table 5: Our coreference results in precision, recall, and  F1 for pairwise resolution.
coreference resolution	accuracy	More recently, use the distinction between pronouns, nominals and proper nouns in their unsupervised, generative model for coreference resolution; for their model, this is absolutely critical for achieving better accuracy.
dependency parsing	accuracy	Experimental evaluation on dependency parsing and named entity recognition demonstrate the superiority of our approach over the baseline pipeline models , especially when upstream stages in the pipeline exhibit low accuracy.
dependency parsing	accuracy	Experimental results on dependency parsing and named entity recognition show useful improvements over the baseline pipeline models, especially when the basic pipeline components exhibit low accuracy.
NER	F-score	All results for NER and chunking are in terms of F-score; all results for POS tagging are accuracy.
NER	accuracy	All results for NER and chunking are in terms of F-score; all results for POS tagging are accuracy.
chunking	F-score	All results for NER and chunking are in terms of F-score; all results for POS tagging are accuracy.
chunking	accuracy	All results for NER and chunking are in terms of F-score; all results for POS tagging are accuracy.
POS tagging	accuracy	All results for NER and chunking are in terms of F-score; all results for POS tagging are accuracy.
event ordering tasks	accuracy	We present results on two event ordering tasks, showing a 3.6% absolute increase in the accuracy of before/after classification over a pairwise model.
information extraction from text	recall	Predicting the scope of negation is important in information extraction from text for obvious reasons; instead of simply flagging the sentences containing negation as not suited for extraction (which is currently the best that can be done), correct semantic relations can be extracted when the scope of negation is known, providing a better recall.
scope finding classifier	gold- standard (GS NEG)	 Table 4: Results of the scope finding classifier with gold- standard (GS NEG) and with predicted negation signals  (PR NEG).
parsing	BLEU score	Examples are the mean average precision for ranked retrieval, the F-measure for parsing, and the BLEU score for statistical machine translation (SMT).
Translation	BLEU score metric	Translation results were evaluated using the mixedcase BLEU score metric in the implementation as suggested by).
translation	BLEU	We evaluated the translation quality of the system using the BLEU metric ().
predicting RTE2 answers	accuracy	 Table 2: Performance of various aligners and complete  RTE systems in predicting RTE2 answers. The columns  show the data set used, accuracy, and average precision  (the recommended metric for RTE2).
predicting RTE2 answers	precision	 Table 2: Performance of various aligners and complete  RTE systems in predicting RTE2 answers. The columns  show the data set used, accuracy, and average precision  (the recommended metric for RTE2).
MT	BLEU score	Whereas this is easily achievable for simple metrics like the Word Error Rate (WER) as described by, current research in MT uses more sophisticated measures, like the BLEU score ().
parsing	accuracy	Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direction, outperforming top generative models in grammar size and in parsing accuracy.
MT	BLEU	In an experiment using a syntactic MT system, we find that rules extracted from joint parses results in an increase of 2.4 BLEU points over rules extracted from independent parses.
estimating parsing	accuracy	However, estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees.
Question classification	accuracy	 Table 1: Question classification accuracy of SVM and  ME using individual feature sets for 6 and 50 classes over  UIUC dataset
Question classification	accuracy	 Table 2: Question classification accuracy of SVM and ME using incremental feature sets for 6 and 50 classes
Question classification	ME	 Table 2: Question classification accuracy of SVM and ME using incremental feature sets for 6 and 50 classes
WSD	accuracy	As a result, we found that a standard supervised WSD method works well for the idiom identification and achieved an accuracy of 89.25% and 88.86% with/without idiom-specific features and that the most effective idiom-specific feature is the one involving the adjacency of idiom constituents.
idiom identification	accuracy	As you see, the adjacency flag (f13) contributes to idiom identification accuracy the most.
WSD	accuracy	We show that though WSD systems trained with a large number of examples can obtain a high level of accuracy, they nevertheless suffer a substantial drop inaccuracy when applied to a different domain.
WSD	accuracy	Finally, we propose that one can maximize the dual benefits of reducing the annotation effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types.
WSD	SENSEVAL	To provide a standardized test-bed for evaluation of WSD systems, a series of evaluation exercises called SENSEVAL were held.
WSD	accuracy	Current supervised WSD systems (which include all the top-performing systems in the English all-words task) usually rely on this relatively small manually annotated corpus for training examples, and this has inevitably affected the accuracy and scalability of current WSD systems.
relationship discovery and extraction	precision	We test our system on manually collected test examples 2 , and find that the (formal-informal) relationship discovery and extraction process using our method achieves an average precision of more than 60%.
Tagging	accuracy	 Table 4: Tagging accuracy for Bilingual models with re- duced dictionary: Lexicon entries are available for only  the 100 most frequent words, while all other words be- come fully ambiguous. The improvement over the mono- lingual Bayesian HMM trained under similar circum- stances is shown. The best result for each language is  shown in boldface.
POS tagging	accuracy	 Table 3: POS tagging accuracy of Spanglish text with  different Machine Learning algorithms. Oracle shows  the accuracy achieved when always selecting the right  POS tag from the output of both Tree Taggers. Language  Id shows accuracy of identifying the language and then  choosing the output of the corresponding tagger.
POS tagging	accuracy	 Table 3: POS tagging accuracy of Spanglish text with  different Machine Learning algorithms. Oracle shows  the accuracy achieved when always selecting the right  POS tag from the output of both Tree Taggers. Language  Id shows accuracy of identifying the language and then  choosing the output of the corresponding tagger.
POS tagging	accuracy	 Table 3: POS tagging accuracy of Spanglish text with  different Machine Learning algorithms. Oracle shows  the accuracy achieved when always selecting the right  POS tag from the output of both Tree Taggers. Language  Id shows accuracy of identifying the language and then  choosing the output of the corresponding tagger.
semantic role labeling	F 1	A variety of methods have been developed for semantic role labeling with reasonably good performance (F 1 measures in the low 80s on standard test collections for English; we refer the interested reader to the proceedings of the SemEval-2007 shared task ( for an overview of the state-of-the-art).
SVM classifiers	LIBLINEAR	SVM classifiers were trained 2 with the LIBLINEAR library) and learned to predict the frame name, role spans, and role labels.
machine translation (MT)	BLEU	Minimum error rate training (MERT) involves choosing parameter values fora machine translation (MT) system that maximize performance on a tuning set as measured by an automatic evaluation metric , such as BLEU.
MERT	RYPT	Although performing MERT with a human-based metric seems like a daunting task, we describe anew metric, RYPT, which takes human judgments into account, but only requires human input to build a database that can be reused over and over again, hence eliminating the need for human input at tuning time.
translation	BLEU	In this investigative study, we analyze the diversity (or lack thereof) of the candidates produced during MERT, we describe how this redundancy can be used to our advantage, and show that RYPT is a better predictor of translation quality than BLEU.
translation	BLEU	In this investigative study, we describe how this redundancy can be used to our advantage to eliminate the need to involve a human at anytime except when building a database of reusable judgments, and furthermore show that RYPT is a better predictor of translation quality than BLEU, making it an excellent candidate for MERT tuning.
translation	BLEU	We analyze a MERT run optimizing BLEU to quantify the level of redundancy in the candidate set, and also provide an extensive analysis of the collected judgments, before describing a set of experiments showing RYPT is a better predictor of translation quality than BLEU.
predicting human preference	BLEU	The results show that RYPT significantly outperforms BLEU when it comes to predicting human preference, with its choice prevailing in 46.1% of judgments vs. 36.0% for BLEU, with 17.9% judged to be of equal quality (left half of Table 1).
MT	LBL+LEN+CLM	For Arabic-to-English MT, the LBL+LEN+CLM system improved lower-cased BLEU by 2.0 on MT06 and 1.7 on MT08 on decoding output.
MT	BLEU	For Arabic-to-English MT, the LBL+LEN+CLM system improved lower-cased BLEU by 2.0 on MT06 and 1.7 on MT08 on decoding output.
MT	BLEU	For Chinese-to-English MT, the improvements in lower-cased BLEU were 1.0 on MT06 and 0.8 on MT08.
classification processing	accuracy	Our experimental results showed that our approach reduces learning and classification processing time leaving the accuracy unchanged.
token labeling	accuracy	Empirical results demonstrate a 20.6% error reduction in token labeling accuracy compared to a strong baseline method that employs a set of high-precision alignments.
sentiment analysis	accuracy	They can have a major impact on the sentiment analysis accuracy.
SWSD	accuracy	To compare SWSD with WSD, we re-ran the 10-fold cross validation experiments, but this time using the original sense labels, rather than Sand O. The (micro-averaged) accuracy is 67.9%, much lower than the overall accuracy for SWSD (88.3%).
SWSD	accuracy	To compare SWSD with WSD, we re-ran the 10-fold cross validation experiments, but this time using the original sense labels, rather than Sand O. The (micro-averaged) accuracy is 67.9%, much lower than the overall accuracy for SWSD (88.3%).
WSD	accuracy	The positive results provide evidence that SWSD is a feasible variant of WSD, and that the S/O sense groupings are natural ones, since the system is able to learn to distinguish between them with high accuracy.
SWSD	Acc	 Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,  SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute  improvement in Acc over Base; EB is percent error reduction in Acc.
SWSD	accuracy	 Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,  SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute  improvement in Acc over Base; EB is percent error reduction in Acc.
SWSD	SF	 Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,  SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute  improvement in Acc over Base; EB is percent error reduction in Acc.
SWSD	precision	 Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,  SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute  improvement in Acc over Base; EB is percent error reduction in Acc.
SWSD	recall	 Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,  SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute  improvement in Acc over Base; EB is percent error reduction in Acc.
SWSD	F-measure	 Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,  SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute  improvement in Acc over Base; EB is percent error reduction in Acc.
SWSD	OF	 Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,  SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute  improvement in Acc over Base; EB is percent error reduction in Acc.
SWSD	IB	 Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,  SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute  improvement in Acc over Base; EB is percent error reduction in Acc.
SWSD	Acc	 Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,  SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute  improvement in Acc over Base; EB is percent error reduction in Acc.
SWSD	EB	 Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,  SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute  improvement in Acc over Base; EB is percent error reduction in Acc.
SWSD	error reduction	 Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,  SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute  improvement in Acc over Base; EB is percent error reduction in Acc.
SWSD	Acc	 Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,  SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute  improvement in Acc over Base; EB is percent error reduction in Acc.
translation from German to English	BLEU scores	Experiments on translation from German to English show improvements over phrase-based systems, both in terms of BLEU scores and inhuman evaluations.
translation from German to English	BLEU score	Experiments on translation from German to English show a 0.5% improvement in BLEU score over a phrase-based system.
translation	BLEU	We show that the models improve translation quality by 1% in BLEU over a competitive baseline on a large-scale task.
word alignment	precision	Yet word alignment precision remains surprisingly low, under 80% for state-of-the-art aligners on not closely related language pairs.
word alignment	accuracy	Finally we present evaluations on word alignment accuracy as well as the impact on end-to-end machine translation quality.
Entity extraction	average precision (AP) statistic	Entity extraction performance is evaluated using the average precision (AP) statistic, a standard information retrieval measure for evaluating ranking algorithms, defined as: where L is a ranked list produced by an extractor, P (i) is the precision of Lat rank i, and corr(i) is 1 if the instance at rank i is correct, and 0 otherwise.
Entity extraction	precision	Entity extraction performance is evaluated using the average precision (AP) statistic, a standard information retrieval measure for evaluating ranking algorithms, defined as: where L is a ranked list produced by an extractor, P (i) is the precision of Lat rank i, and corr(i) is 1 if the instance at rank i is correct, and 0 otherwise.
credit attribution problem	Latent Dirichlet Allocation (LDA)	One promising approach to the credit attribution problem lies in the machinery of Latent Dirichlet Allocation (LDA) (), a recent model that has gained popularity among theoreticians and practitioners alike as a tool for automatic corpus summarization and visualization.
lexical acquisition tasks	precision	Patterns were shown to be very useful in all sorts of lexical acquisition tasks, giving high precision results at relatively low computational costs ( ).
MT	edit rate	 Table 2: HTER scores for five MT systems. The  edit rate decreases as the number of editors in- creases from zero (where HTER is simply the TER  score between the MT output and the reference  translation) and five.
MT	HTER	 Table 2: HTER scores for five MT systems. The  edit rate decreases as the number of editors in- creases from zero (where HTER is simply the TER  score between the MT output and the reference  translation) and five.
MT	TER  score	 Table 2: HTER scores for five MT systems. The  edit rate decreases as the number of editors in- creases from zero (where HTER is simply the TER  score between the MT output and the reference  translation) and five.
Classification	accuracy	 Table 3: Classification accuracy with all features  from each feature class. Rows 1 to 4: individual  feature class; Row 5: all feature classes.
Classification	accuracy	 Table 4:  Classification accuracy with top  rules/word pairs for each feature class. Rows 1  to 4: individual feature class; Row 5: all feature  classes.
SMT	MERT	There has been some previous work on accuracy-driven training techniques for SMT, such as MERT and the Simplex Armijo Downhill method (, which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set.
machine translation	BLEU	This problem plays a major part in reducing machine translation quality, as reflected by both automatic measures such as BLEU () and human judgment tests.
Alignment	accuracy	Alignment accuracy remains a challenge for them.
WSD classification	accuracy	For the prepositions in, of, and on, the SRL feature did not affect the WSD classification accuracy significantly.
SRL task	F 1 measure	For the SRL task, we report the classification accuracy overall annotated prepositional phrases in the test section and the F 1 measure for the semantic roles ARGM-LOC and ARGM-TMP.
Translation	accuracy	 Table 8. Translation accuracy rates of the systems.  (%)
question classification	accuracy	Section 4 presents the question classification accuracy over UIUC question dataset.
question classification	accuracy	The question classification performance is measured by accuracy, i.e., the proportion of the correctly classified questions among all test questions.
Question classification	accuracy	 Table 2: Question classification accuracy using in- cremental feature sets for 6 and 50 classes over  UIUC split.  6 class 50 class  wh-word  46.0  46.8  + head word  92.2  82.0  + hypernym  91.8  85.6  + unigram  93.0  88.4  + word shape  93.6  89.0
Question classification	accuracy	 Table 3: Question classification accuracy by re- moving one feature at a time for 6 and 50 classes  over UIUC split.  6 class 50 class  overall  93.6  89.0  -wh-word  93.6  89.0  -head word  92.8  88.2  -hypernym  90.8  84.2  -unigram  93.6  86.8  -word shape  93.0  88.4
Dependency parsing	MIRA	 Table 3: Dependency parsing results for the SS-SCM method with different amounts of labeled training  data. Supervised SCM (1od) and Supervised MIRA (2od) are the baseline first and second-order ap- proaches; SS-SCM (1od) and 2-stage SS-SCM(+MIRA) (2od) are the first and second-order approaches  described in this paper. "Baseline" refers to models without cluster-based features, "CL" refers to models  which make use of cluster-based features.
sentiment classification	BOO	We use five sentiment classification datasets, including the widely-used movie review dataset () as well as four datasets containing reviews of four different types of products from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (.
CRFs	accuracy	As expected, CRFs can perform reasonably well (accuracy = 63.9%) even without consulting the dictionary, by learning directly from the data.
Information extraction	accuracy	Information extraction methods, on the other hand, often have limited accuracy.
RLM	similarity	For both RLM and the baseline algorithms, it is impractical to compute the similarity of a review with each object in the database.
document boundary awareness	precision	In our evaluation, document boundary awareness greatly benefits precision for small datasets, blocking acquisition of spurious affixes.
POS tagging	accuracy	This system achieves a 20% error reduction for POS tagging over state-of-the-art unsuper-vised systems tested under the same conditions , and achieves comparable accuracy when trained with much less prior information .
Tagging	accuracy	 Table 3: Tagging accuracy with partial dictionaries over
Tagging	Accuracy	 Table 4: Tagging Accuracy of models trained over dataset
Phrase-based trigram decoding	TER	 Table 3: Phrase-based trigram decoding results on the Arabic text and audio development sets. Decoding  weights were optimized on the Tune set in order to directly minimize TER. Corpus weights were also  optimized on Tune set, but based on expected TER.
Phrase-based trigram decoding	TER	 Table 3: Phrase-based trigram decoding results on the Arabic text and audio development sets. Decoding  weights were optimized on the Tune set in order to directly minimize TER. Corpus weights were also  optimized on Tune set, but based on expected TER.
MT	BLEU	tn , where S 񮽙 is a one-level tree, and 2 · j helper rules for adjoining Run time: O(|S| + |T |) begin rules ← {}, lhs-state ← concat('q', get-root(S), get-root(T )) site-and-word-list-s ← get-sites-and-words-in-order(S) site-and-word-list-t ← get-sites-and-words-in-order(T ) if r is adjoining then lhs-state ← concat(lhs-state, get-adjoin-dir(S), get-adjoin-dir(T )) lhs ← construct-LHS(lhs-state, get-root(S), site-and-word-list-s) rhs ← construct-RHS(add-states(id(r), site-and-word-list-t)) add: End-to-end MT results show that the best adjoining model using a log-linear combination of joint and independent models (line 6) outperforms the baseline (line 1) by +0.7 and +0.8 BLEU, a statistically significant difference at the 95% confidence level.
MT	BLEU	 Table 1: End-to-end MT results show that the best adjoining model using a log-linear combination  of joint and independent models (line 6) outperforms the baseline (line 1) by +0.7 and +0.8 BLEU, a  statistically significant difference at the 95% confidence level.
parsing	accuracy	The parser implementing this model is evaluated on the standard Switchboard transcribed speech parsing task for overall parsing accuracy and edited word detection.
parsing	accuracy	Recent approaches to syntactic modeling of speech with repairs have shown that significant gains in parsing accuracy can be achieved by modeling the syntax of repairs ().
parsing	accuracy	constituents in fluent and disfluent speech can also improve parsing accuracy.
parsing	accuracy	shows overall parsing accuracy results, with the same set of systems, with the exception of the TAG system which did not report parsing results.
Baseline translation	BLEU	 Table 4: Baseline translation results in BLEU us- ing data from the first stream epoch with a lossless  LM (4.5GB RAM), the TB-LM and the O-RLM  (300MB RAM). All LMs are static.
edit detection	accuracy	We first evaluate edit detection accuracy on those test SUs predicted to be errorful on a per-word basis.
SU-error classifier	precision ratio	The most likely issue is that the automatic SU-error classifier filtered out some SUs with true RC errors which had previously been correctly identified, reducing the overall precision ratio as well as recall (i.e., we no longer receive accuracy credit for some easier errors once caught).
SU-error classifier	recall	The most likely issue is that the automatic SU-error classifier filtered out some SUs with true RC errors which had previously been correctly identified, reducing the overall precision ratio as well as recall (i.e., we no longer receive accuracy credit for some easier errors once caught).
SU-error classifier	accuracy	The most likely issue is that the automatic SU-error classifier filtered out some SUs with true RC errors which had previously been correctly identified, reducing the overall precision ratio as well as recall (i.e., we no longer receive accuracy credit for some easier errors once caught).
word-level identification	accuracy	As seen in word-level identification results (Table 2), automatically selecting a subset of testing data upon which to apply simple cleanup reconstruction does not perform at the accuracy shown to be possible given an oracle filtering.
information retrieval	recall	In information retrieval, similar or related words are used to expand user queries to improve recall ().
parsing	accuracy	effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set, but they used a two-stage parser comprised of Charniak's lexicalized probabilistic parser with n-best parsing and a discriminative reranking parser), and thus it would be better categorized as "co-training".
parsing	accuracy	The performance curve for parsing the WSJ was a steep one at first, as the incorporation of notions such as head, distance, subcategorization) brought about a dramatic increase in parsing accuracy to the level of F 1 88.
parsing	F 1 88	The performance curve for parsing the WSJ was a steep one at first, as the incorporation of notions such as head, distance, subcategorization) brought about a dramatic increase in parsing accuracy to the level of F 1 88.
translation	accuracy	With 1-to-1 precision evaluation we determine the translation accuracy of our method, compared with the two baseline methods.
classification or retrieval task	precision	This could be cast as a simple classification or retrieval task, where traditional measures of precision, recall and F metrics are used.
classification or retrieval task	recall	This could be cast as a simple classification or retrieval task, where traditional measures of precision, recall and F metrics are used.
classification or retrieval task	F metrics	This could be cast as a simple classification or retrieval task, where traditional measures of precision, recall and F metrics are used.
Classification	accuracy	 Table 1: Classification accuracy compared against a majority baseline
Classification	accuracy	 Table 2: Classification accuracy, without word n-gram features, compared against a majority baseline
anaphoricity determination	Acc	For anaphoricity determination, we report the performance in Acc + and Acc -, which measure the accuracies of identifying anaphoric NPs and non-anaphoric NPs, respectively.
anaphoricity determination	Acc -	For anaphoricity determination, we report the performance in Acc + and Acc -, which measure the accuracies of identifying anaphoric NPs and non-anaphoric NPs, respectively.
coreference resolution	recall	For coreference resolution, we report the performance in terms of recall, precision, and F 1-measure using the commonly-used model theoretic MUC scoring program ().
coreference resolution	precision	For coreference resolution, we report the performance in terms of recall, precision, and F 1-measure using the commonly-used model theoretic MUC scoring program ().
coreference resolution	F 1-measure	For coreference resolution, we report the performance in terms of recall, precision, and F 1-measure using the commonly-used model theoretic MUC scoring program ().
SRL	precision	In particular for SRL, we use a state-of-the-art inhouse toolkit, which achieved the precision of 87.07% for ARG0 identification and the precision of 78.97% for ARG1 identification, for easy integration.
SRL	precision	In particular for SRL, we use a state-of-the-art inhouse toolkit, which achieved the precision of 87.07% for ARG0 identification and the precision of 78.97% for ARG1 identification, for easy integration.
ARG0 identification	precision	In particular for SRL, we use a state-of-the-art inhouse toolkit, which achieved the precision of 87.07% for ARG0 identification and the precision of 78.97% for ARG1 identification, for easy integration.
pronoun resolution	F-measure	shows the contribution of the semantic role features and the relative pronominal ranking feature in pronoun resolution when the detailed pronominal subcategory features are included:: Contribution of the semantic role features (SR) and the relative pronominal ranking feature (PR) in pronoun resolution when the detailed pronominal subcategory features are included  1) The inclusion of the semantic role features improve the performance by 4.0(>>>), 4.6(>>>) and 7.3(>>>) in F-measure on the NWIRE, NPAPER and BNEWS domains, respectively.
coreference	precision	The experiment we carried on a news corpus proves that the prior probabilities of coreference are an important factor for maintaining a good balance between precision and recall for cross document coreference systems.
coreference	recall	The experiment we carried on a news corpus proves that the prior probabilities of coreference are an important factor for maintaining a good balance between precision and recall for cross document coreference systems.
Machine translation	BLEU	 Table 3: Machine translation performance of several systems, measured against a single English refer- ence translation. The results vary both the preprocessing-either none, or reordered using the learned  Linear Ordering Problems-and the reordering model used in Moses. Performance is measured using  BLEU, METEOR (Lavie et al., 2004), and TER (Snover et al., 2006). (For TER, smaller values are  better.)
Machine translation	METEOR	 Table 3: Machine translation performance of several systems, measured against a single English refer- ence translation. The results vary both the preprocessing-either none, or reordered using the learned  Linear Ordering Problems-and the reordering model used in Moses. Performance is measured using  BLEU, METEOR (Lavie et al., 2004), and TER (Snover et al., 2006). (For TER, smaller values are  better.)
Machine translation	TER	 Table 3: Machine translation performance of several systems, measured against a single English refer- ence translation. The results vary both the preprocessing-either none, or reordered using the learned  Linear Ordering Problems-and the reordering model used in Moses. Performance is measured using  BLEU, METEOR (Lavie et al., 2004), and TER (Snover et al., 2006). (For TER, smaller values are  better.)
Machine translation	TER	 Table 3: Machine translation performance of several systems, measured against a single English refer- ence translation. The results vary both the preprocessing-either none, or reordered using the learned  Linear Ordering Problems-and the reordering model used in Moses. Performance is measured using  BLEU, METEOR (Lavie et al., 2004), and TER (Snover et al., 2006). (For TER, smaller values are  better.)
SMT	BLEU	 Table 1: Left: The translation quality of the SMT systems as measured by the BLEU score. Translations  were detokenized but not recased before evaluating their quality against lowercased reference translations  by the mteval-v11b.pl script. Right: Average total translation time in seconds.
SMT	PE	Adding features inspired by SMT, such as PE (t|s), eliminates the gap between the two.
coreference resolution	F1	We will present evaluations on multiple coreference resolution metrics, as no single one is clearly superior: • Pairwise F1: precision, recall, and F1 overall pairs of mentions in the same entity cluster.
coreference resolution	precision	We will present evaluations on multiple coreference resolution metrics, as no single one is clearly superior: • Pairwise F1: precision, recall, and F1 overall pairs of mentions in the same entity cluster.
coreference resolution	recall	We will present evaluations on multiple coreference resolution metrics, as no single one is clearly superior: • Pairwise F1: precision, recall, and F1 overall pairs of mentions in the same entity cluster.
coreference resolution	F1	We will present evaluations on multiple coreference resolution metrics, as no single one is clearly superior: • Pairwise F1: precision, recall, and F1 overall pairs of mentions in the same entity cluster.
coreference resolution	MUC score	With these proposed features and other standard syntactic features (which are commonly employed in existing coreference systems), our coreference resolution system can obtain an increase of 10.4% for MUC score and 9.7% for anaphor accuracy from the baseline in ACE2 evaluation.
coreference resolution	accuracy	With these proposed features and other standard syntactic features (which are commonly employed in existing coreference systems), our coreference resolution system can obtain an increase of 10.4% for MUC score and 9.7% for anaphor accuracy from the baseline in ACE2 evaluation.
SC classification	accuracy	The SC classification performance is measured by accuracy, i.e., the proportion of the correctly classified instances among all test instances.
SC classification	accuracy	 Table 3: SC classification accuracy of ME using  individual feature sets for development and test  ACE2 datasets.
SC classification	accuracy	 Table 4: SC classification accuracy of ME using  incremental feature sets for training and test ACE2  datasets.
SC classification	accuracy	 Table 5: SC classification accuracy of ME by re- moving one feature at a time for training and test  ACE2 datasets.
opinion extraction	f-score	The performance of opinion extraction boosts to an f-score 0.80 and the performance of polarity detection an f-score 0.54.
SRL	SRL	According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short).
translation	accuracy	We find an improvement in translation accuracy through, first, using constraints to limit the number of new templates, second, using Bayesian methods to limit which of these new templates are favored when re-analyzing the training data with EM, and, third, experimenting with different renormalization techniques for the EM re-analysis.
verb classification	error	In the task of verb classification into all VerbNet classes, our best model achieved a 10.69% error reduction in the classification accuracy, over the previously proposed model.
verb classification	accuracy	In the task of verb classification into all VerbNet classes, our best model achieved a 10.69% error reduction in the classification accuracy, over the previously proposed model.
SMT	Bleu (in %s)	 Table 1: Cross-lingual SMT experiments (shown in bold). Columns 2-5 present the bi-texts used for  training, development and testing, and the monolingual data used to train the English language model.  The following columns show the resulting Bleu (in %s) for different numbers of training sentence pairs.
prediction	accuracy	The prediction accuracy monotonically decreases, but remains above the baseline until more than 6 letters are cut.
SRL	F-measure	To date, the best SRL performance reported on the Chinese Proposition Bank (CPB) corresponds to a F-measure is 92.0, when using the handcrafted parse trees from Chinese Penn Treebank (CTB).
SRL	F-measure	We present encouraging SRL results on CPB 2 . The best F-measure performance (74.12) with gold segmentation and POS tagging can be achieved by the first method.
term variation identification task	recall	The proposed method achieves 70% precision on the term variation identification task with the recall slightly higher than 60%, reducing the error rate of a naïve baseline by 38%.
term variation identification task	error rate	The proposed method achieves 70% precision on the term variation identification task with the recall slightly higher than 60%, reducing the error rate of a naïve baseline by 38%.
SEAL	mean average precision (MAP)	By using only three seeds and top one hundred documents returned by Google, SEAL achieved 90% in mean average precision (MAP), averaged over 36 datasets from three languages: English, Chinese, and Japanese.
relation classification	precision	The idea is that the relation classification system is more accurate than Maxent when it is applicable, and hence would improve precision on disambiguating the species with few or no training instances.
relation classification	precision	Nevertheless, it is encouraging that the relation classification systems obtained higher precision than RuleSpSent, which is important, considering the decisions will be transfered to the untagged entity mentions across the document.
named entity recognition (NER)	F-measure	For named entity recognition (NER), for example, reported that a system trained on a labeled Reuters corpus achieved an F-measure of 91% on a Reuters test set, but only 64% on a Wall Street Journal test set.
SVM classifiers	training error	SVM classifiers use a value of 10 for parameter C (trade-off between training error and margin).
SVM classifiers	margin	SVM classifiers use a value of 10 for parameter C (trade-off between training error and margin).
parsing	accuracy	The increased expressiveness of the model, combined with the more robust parameter estimates provided by the smoothing, results in a nice increase in parsing accuracy on a held-out set.
parsing	accuracy	They showed that self-training latent variable grammars on their own output can mitigate data sparsity issues and improve parsing accuracy.
parsing	accuracy	Using a linear-chain conditional random field, we improve parsing accuracy over the gen-erative baseline parser on the Penn Treebank WSJ corpus, rivalling a similar model that does not make use of context.
IR	BL-1	For each aspect, similar to the pooling strategy in IR, we pooled the top 20 opinion words identified by BL-1, BL-2 and ME-LDA.
clustering	accuracy	 Table 1: The clustering accuracy with TAM using a variety of feature sets. These results were averaged over 200 randomly-initialized Gibbs
domain adaptation	error	Our domain adaptation algorithm improves performance to 87%, which is still far below in-domain performance, but a significant reduction in error.
punctuation prediction task	precision	To assess the performance of the punctuation prediction task, we compute precision (prec.), recall (rec.), and F1-measure (F 1 ), as defined by the following equations: prec.
punctuation prediction task	recall (rec.)	To assess the performance of the punctuation prediction task, we compute precision (prec.), recall (rec.), and F1-measure (F 1 ), as defined by the following equations: prec.
punctuation prediction task	F1-measure (F 1 )	To assess the performance of the punctuation prediction task, we compute precision (prec.), recall (rec.), and F1-measure (F 1 ), as defined by the following equations: prec.
Translation	BLEU	 Table 7: Translation performance on punctuated ASR outputs using Moses (Averaged percentage scores of BLEU)
ASR outputs	BLEU	 Table 7: Translation performance on punctuated ASR outputs using Moses (Averaged percentage scores of BLEU)
segmentation state	BIES label set	. and other (O), the segmentation state space we used is BIES label set, since we find that it yields a little improvement over BIO set.
Viewing tagging	recall (R)	• Pairwise Precision and Recall: Viewing tagging as a clustering task over tokens, we evaluate pairwise precision (P ) and recall (R) between the model tag sequence (M ) and gold tag sequence (G) by counting the true positives (tp), false positives (f p) and false negatives (f n) between the two and setting P = tp/(tp + f p) and R = tp/(tp + f n).
classification	accuracy	Empirical evaluation using a real-life blog data set shows that these two techniques improve the classification accuracy of the current state-of-the-art methods significantly.
classification	accuracy	This section evaluates the proposed techniques and sees how they affect the classification accuracy.
classification	accuracy	Use of these features provided the best classification accuracy for this task prior to this work.
SRL	accuracy	The reason is that even the state-ofthe-art SRL systems do not have very high accuracy on both English text, and Chinese text ().
IE	accuracy	Lately, IE has improved to the point of being usable for some real-world tasks whose accuracy requirements are reachable with current technology.
IE	accuracy	Our approach to IE has been to use languageindependent algorithms, in order to facilitate reuse across languages, but we train them with languagespecific data, for the sake of accuracy.
classification	accuracy	Our experimental results on the RST Discourse Tree-bank corpus and Penn Discourse Treebank indicate that the proposed method brings a significant improvement in classification accuracy and macro-average F-score when small training datasets are used.
classification	F-score	Our experimental results on the RST Discourse Tree-bank corpus and Penn Discourse Treebank indicate that the proposed method brings a significant improvement in classification accuracy and macro-average F-score when small training datasets are used.
RST segmenter	F-score	The relatively high performance of this RST segmenter, which has an F-score of 0.95 compared to that of 0.98 between human annotators, is acceptable for this task.
classification	accuracy	For example, doubling the number of selected features, N to 10000 did not improve the classification accuracy, although it required 4GB of memory to store the feature co-occurrence matrix.
RSTDT relations	F-score	 Table 1: F-scores for RSTDT relations, using a training set containing #Tr instances of each relation. B. indicates  F-score for baseline, P.M. for the proposed method. A boldface indicates the best classifier for each relation.
phrase extraction	PT	 Table 1: The coverage of the test set by the phrase table and the parallel corpus based on different amount of the  training data. "PL" indicates the Phrase Length N , where {1 <= N <= 10}; "20K" and "200K" represent the sizes  of the parallel data for model training and phrase extraction; "Cov." indicates the coverage rate; "Tset" represents the  number of unique phrases with the length N in the Test Set; "PT" represents the number of phrases of the Test Set  occur in the Phrase
translation	accuracy	Our focus is to boost translation accuracy for long tails with non-trivial Web occurrences in each monolingual corpus, but not with much bilingual cooccurrences, e.g., researchers publishing actively in two languages but not famous enough to be featured in multi-lingual Wikipedia entries or news articles.
translation coverage	accuracy	This holistic approach complements existing approaches and enhances the translation coverage and accuracy.
Information retrieval (IR)	term frequency (tf)	Information retrieval (IR) typically makes use of simple features that count terms within/across documents such as term frequency (tf) and inverse document frequency (IDF).
Information retrieval (IR)	inverse document frequency (IDF)	Information retrieval (IR) typically makes use of simple features that count terms within/across documents such as term frequency (tf) and inverse document frequency (IDF).
summariser	ROUGE score	As described in section 4 we trained the summariser using the A* search decoder to maximise the ROUGE score of the best scoring summaries.
surface realization	BLEU score	Automatic Evaluation To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score () (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output.
surface realization	precision	Automatic Evaluation To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score () (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output.
surface realization	BLEU score	Automatic Evaluation To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score () (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output.
surface realization	precision	Automatic Evaluation To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score () (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output.
translation	accuracy	The second aspect is of course translation accuracy.
Translation	BLEU	 Table 3: Translation results under grammar G 2 with indi- vidual rule sets, merged rule sets, and rescoring and sys- tem combination with lattice-based MBR (lower-cased  BLEU shown)
MT evaluations	BLEU	The realizations were also evaluated using seven automatic metrics: • IBM's BLEU, which scores a hypothesis by counting n-gram matches with the reference sentence (), with smoothing as described in ( • The NIST n-gram evaluation metric, similar to BLEU, but rewarding rarer n-gram matches, and using a different length penalty • METEOR, which measures the harmonic mean of unigram precision and recall, with a higher weight for recall ( • TER (Translation Edit Rate), a measure of the number of edits required to transform a hypothesis sentence into the reference sentence () • TERP, an augmented version of TER which performs phrasal substitutions, stemming, and checks for synonyms, among other improvements ( • TERPA, an instantiation of TERP with edit weights optimized for correlation with adequacy in MT evaluations • GTM (General Text Matcher), a generalization of the F-measure that rewards contiguous matching spans ( Additionally, targeted versions of BLEU, ME-TEOR, TER, and GTM were computed by using the human-repaired outputs as the reference set.
classification	accuracy	We demonstrate that classification margins, which incorporate information about features that most impact system accuracy, can effectively solve the second subproblem.
SMT	BLEU	At each iteration, we decode the unseen test set T with the most current SMT configuration and evaluate translation performance in terms of BLEU as well as coverage (defined as the fraction of untranslatable source words in the target hypotheses).
SMT	coverage	At each iteration, we decode the unseen test set T with the most current SMT configuration and evaluate translation performance in terms of BLEU as well as coverage (defined as the fraction of untranslatable source words in the target hypotheses).
Tagger	accuracy	 Table 9: Tagger accuracy over the training data, using  both the initial counts and the EM trained models.
parsing	accuracy	With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance.
speculation scope identification	accuracy	It shows that given golden parse trees and golden cues, speculation scope identification achieves higher performance (e.g., ~3.3% higher in accuracy) than negation scope identification.
SRL	accuracy	 Table 4: SRL accuracy when the semantic model is constrained  by the baseline model
alignment classification	precisionrecall	Because the alignment classification has extremely unbalanced classes, we use precisionrecall of true alignments as evaluation metrics.
character tagging	F measurement	As their experiment showed, the approach successfully predicted 85.88% of the word breaks, which is much lower than that of the character tagging approach if in terms of F measurement.
POS taggers	accurate	While state-of-the-art supervised POS taggers are more than 97% accurate (), unsupervised POS taggers continue to lag far behind.
Tagging	accuracy	 Table 2: Tagging accuracy on WSJ
Baseline tagging	accuracy	 Table 4: Baseline tagging accuracy on automatically de- tected sentence boundaries
word segmentation	precision	For word segmentation, precision is defined as the number of correctly segmented words divided by the total number of words in the output, and recall is defined as the number of correctly segmented words divided by the total number of words in the gold-standard output.
word segmentation	recall	For word segmentation, precision is defined as the number of correctly segmented words divided by the total number of words in the output, and recall is defined as the number of correctly segmented words divided by the total number of words in the gold-standard output.
tagging	accuracy	Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy.
tagging assignment	accuracy	Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary.
classification	accuracy	All results are presented in terms of classification accuracy.
comma insertion	recall	We conducted an experiment on comma insertion using the Kyoto Text Corpus (, and obtained higher recall and precision than those of the baseline, leading us to confirm the effectiveness of our method.
comma insertion	precision	We conducted an experiment on comma insertion using the Kyoto Text Corpus (, and obtained higher recall and precision than those of the baseline, leading us to confirm the effectiveness of our method.
parsing	accuracy	Section 5 discusses the effect which the addition of the new entries to the lexicon has on the parsing coverage and accuracy.
translation between distant language	BLEU	However , when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do notwork well.
translation between distant language	PER	However , when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do notwork well.
translation between distant language	TER	However , when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do notwork well.
MT	BLEU	However, argued that the MT community is overly reliant on BLEU by showing examples of poor performance.
Binding events	Ap- proximate	 Table 3: Effects of trimming of CoNLL dependencies on the Shared Task development data for Binding events. Ap- proximate Span Matching/Approximate Recursive Matching. The data was processed by the MST parser.
mention detection task	F-measure	However, given that we are interested in the mention detection task only, we decided to use the more intuitive and popular (un-weighted) F-measure, the harmonic mean of precision and recall.
mention detection task	precision	However, given that we are interested in the mention detection task only, we decided to use the more intuitive and popular (un-weighted) F-measure, the harmonic mean of precision and recall.
mention detection task	recall	However, given that we are interested in the mention detection task only, we decided to use the more intuitive and popular (un-weighted) F-measure, the harmonic mean of precision and recall.
word matching	time sensitivity	Other generated features include word matching with recurrent event seed words and time sensitivity of search result set.
Classification	R	 Table 1: Classification performance of the semi- supervised model. R is the ratio of labeled documents.
name matching	accuracy	However, the best of the name matching techniques can at best work with a few thousand names to give acceptable response time and accuracy.
name similarity search	recall	Therefore, we need a name similarity search technique that can ensure very high recall without producing too many candidates.
text generation tasks	BLEU	In complex text generation tasks like SMT, the ability to optimize BLEU (), TER (), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequacy) that often do not correlate well with task-agnostic loss functions such as log-loss.
text generation tasks	TER	In complex text generation tasks like SMT, the ability to optimize BLEU (), TER (), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequacy) that often do not correlate well with task-agnostic loss functions such as log-loss.
SMT	BLEU	In complex text generation tasks like SMT, the ability to optimize BLEU (), TER (), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequacy) that often do not correlate well with task-agnostic loss functions such as log-loss.
SMT	TER	In complex text generation tasks like SMT, the ability to optimize BLEU (), TER (), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequacy) that often do not correlate well with task-agnostic loss functions such as log-loss.
dependency grammar induction	attachment accuracy	 Table 3: Results for dependency grammar induction given gold-standard POS tags, reported as attachment accuracy  (fraction of parents which are correct). The three existing methods are: our replication of EM with the initializer from
SMT	Minimum error rate training (MERT)	We built Chinese-to-English SMT systems based on Moses 3 . Minimum error rate training (MERT) with respect to BLEU score was used to tune the decoder's parameters.
SMT	BLEU score	We built Chinese-to-English SMT systems based on Moses 3 . Minimum error rate training (MERT) with respect to BLEU score was used to tune the decoder's parameters.
meaning preservation	diversity	lists the average grammaticality, meaning preservation, and diversity scores of the two methods.
meaning preservation	ZHAO-ENG	In meaning preservation, ZHAO-ENG was slightly better, but the difference was not statistically significant.
MERT	BLEU metric	We perform MERT training with the popular BLEU metric () on the development set of erroneous sentences and their corrections.
sentiment classification	F 1 -measure	Additionally, for sentiment classification, our SVM classifier achieves an average F 1 -measure of 0.787 in the 11 products.
translation	BLEU	STIR gives a translation improvement of 3.84 BLEU over a standard phrase-based system with an integrated reordering model.
Translation	BLEU	 Table 2: Translation quality, measured by BLEU, for En- glish to Japanese. STIR results use both manually anno- tated and learned alignments.
translation	accuracy	The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems;).
ML	accuracy	We evaluated the ML results in terms of accuracy, precision, recall, and F-measure against manual AZ annotations in the corpus: acc = no. of correctly classified sentences total no. of sentences in the corpus p = no. of sentences correctly identified as Class i total no. of sentences identified as Class i r = no. of sentences correctly identified as Class i total no. of sentences in Class if = 2 * p * r p+r We used 10-fold cross validation for all the methods to avoid the possible bias introduced by relying on any particular split of the data.
ML	precision	We evaluated the ML results in terms of accuracy, precision, recall, and F-measure against manual AZ annotations in the corpus: acc = no. of correctly classified sentences total no. of sentences in the corpus p = no. of sentences correctly identified as Class i total no. of sentences identified as Class i r = no. of sentences correctly identified as Class i total no. of sentences in Class if = 2 * p * r p+r We used 10-fold cross validation for all the methods to avoid the possible bias introduced by relying on any particular split of the data.
ML	recall	We evaluated the ML results in terms of accuracy, precision, recall, and F-measure against manual AZ annotations in the corpus: acc = no. of correctly classified sentences total no. of sentences in the corpus p = no. of sentences correctly identified as Class i total no. of sentences identified as Class i r = no. of sentences correctly identified as Class i total no. of sentences in Class if = 2 * p * r p+r We used 10-fold cross validation for all the methods to avoid the possible bias introduced by relying on any particular split of the data.
ML	F-measure	We evaluated the ML results in terms of accuracy, precision, recall, and F-measure against manual AZ annotations in the corpus: acc = no. of correctly classified sentences total no. of sentences in the corpus p = no. of sentences correctly identified as Class i total no. of sentences identified as Class i r = no. of sentences correctly identified as Class i total no. of sentences in Class if = 2 * p * r p+r We used 10-fold cross validation for all the methods to avoid the possible bias introduced by relying on any particular split of the data.
segmentation tasks	O	For the majority of realistic segmentation tasks, however, the upper bound is O(M N ) messages, where M is a constant.
translation	BLEU	We show that it is possible to use our data selection methods to subselect less than 1% (or discard 99%) of a large general training corpus and still increase translation performance by nearly 2 BLEU points.
machine translation evaluation met-rics	BLEU	Many machine translation evaluation met-rics have been proposed after the seminal BLEU metric, and many among them have been found to consistently outperform BLEU, demonstrated by their better correlations with human judgment.
machine translation evaluation met-rics	BLEU	Many machine translation evaluation met-rics have been proposed after the seminal BLEU metric, and many among them have been found to consistently outperform BLEU, demonstrated by their better correlations with human judgment.
machine translation	BLEU	It is our hope that this work will encourage the machine translation community to finally move away from BLEU as the unquestioned default and to consider the new generation metrics when tuning their systems.
MT evaluation	BLEU	The first automatic MT evaluation metric to show a high correlation with human judgment is BLEU ().
SMT	BLEU	In the SMT community, MT tuning still uses BLEU almost exclusively.
MT tuning	BLEU	In the SMT community, MT tuning still uses BLEU almost exclusively.
SMT	BLEU	compared tuning a phrase-based SMT system with BLEU, NIST, METEOR, and TER, and concluded that BLEU and NIST are still the best choices for MT tuning, despite the proven higher correlation of METEOR and TER with human judgment.
SMT	TER	compared tuning a phrase-based SMT system with BLEU, NIST, METEOR, and TER, and concluded that BLEU and NIST are still the best choices for MT tuning, despite the proven higher correlation of METEOR and TER with human judgment.
SMT	BLEU	compared tuning a phrase-based SMT system with BLEU, NIST, METEOR, and TER, and concluded that BLEU and NIST are still the best choices for MT tuning, despite the proven higher correlation of METEOR and TER with human judgment.
MST parser	LAS	 Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
MST parser	TEDEVAL global average metrics	 Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
recognition of actions	accuracy	Recent works that used poses for recognition of actions achieved 70% and 61% accuracy respectively under extremely limited testing conditions with only 5-6 action classes each.
SMT	BLEU	The field of SMT has benefited greatly from the existence of an automatic evaluation metric, BLEU, which grades an output candidate according to n-gram matches to one or more reference outputs.
Token-based analysis	accuracy	 Table 4: Token-based analysis: Whole-word accuracy re- sults split into different frequency bins. In the last two  rows, all predictions are included, weighted by the fre- quency of the form to predict. Last row is edit distance.
QA	accuracy	as a test bed for open-domain QA technology due to its broad domain, complex language, as well as the emphasis on accuracy, confidence, and speed during game play.
QA	confidence	as a test bed for open-domain QA technology due to its broad domain, complex language, as well as the emphasis on accuracy, confidence, and speed during game play.
MWE pre-grouping	accuracy	Our models are useful for several NLP tasks in which MWE pre-grouping has improved accuracy.
parsing	accuracy	We evaluate parsing accuracy of the Stanford and DP-TSG models).
parsing	accuracy	In terms of parsing accuracy, the Berkeley parser exceeds both Stanford and DP-TSG.
MWE identification	re- sults	 Table 7: MWE identification per category and overall re- sults (test set, sentences ≤ 40 words). MWI and MWCL  do not occur in the test set.
MWE identification	F1	 Table 8: MWE identification F1 of the best parsing model  vs. the mwetoolkit baseline (test set, sentences ≤ 40  words). PA-PCFG+Features includes the grammar fea- tures in
identification	accuracy	We demonstrate a significant improvement in identification accuracy, compared with less sophisticated baselines.
Parameter tuning	minimum error rate training (MERT)	Parameter tuning of the decoder was done with minimum error rate training (MERT), adapted to BLEU maximization.
Parameter tuning	BLEU	Parameter tuning of the decoder was done with minimum error rate training (MERT), adapted to BLEU maximization.
translation	BLEU	We also tested the impact on translation and found a 0.48 BLEU improvement on Chinese to English and a 1.26 BLEU improvement on English to Italian translation.
translation	BLEU	We also tested the impact on translation and found a 0.48 BLEU improvement on Chinese to English and a 1.26 BLEU improvement on English to Italian translation.
Translation	BLEU	 Table 6: Translation results, Zh to En. BLEU=BLEUr4n4
Translation	BLEUr4n4	 Table 6: Translation results, Zh to En. BLEU=BLEUr4n4
MT	BLEU	Following the theoretical development below, the empirical effectiveness of our approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems -learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs -with our unsupervised discriminative training using only y.
MT task	MIRA	One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training, the averaged Perceptron (), maximum conditional likelihood (, minimum risk, and MIRA (.
splitting	accuracy	Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web in our case, and then using that information for constructing splitting models.
question classification	error reduction	The experiments with such kernels for question classification show an unprecedented results, e.g. 41% of error reduction of the former state-of-the-art.
question classification (QC)	error reduction	The extensive experimentation on two datasets of question classification (QC) and semantic role labeling (SRL), shows that: (i) PTK applied to our dependency trees outperforms STK, demonstrating that dependency parsers are fully exploitable for feature engineering based on structural kernels; (ii) SPTK outperforms any previous kernels achieving an unprecedented result of 41% of error reduction with respect to the former state-of-the-art on QC; and (iii) the experiments on SRL confirm that the approach can be applied to different tasks without any tuning and again achieving state-of-the-art accuracy.
question classification (QC)	accuracy	The extensive experimentation on two datasets of question classification (QC) and semantic role labeling (SRL), shows that: (i) PTK applied to our dependency trees outperforms STK, demonstrating that dependency parsers are fully exploitable for feature engineering based on structural kernels; (ii) SPTK outperforms any previous kernels achieving an unprecedented result of 41% of error reduction with respect to the former state-of-the-art on QC; and (iii) the experiments on SRL confirm that the approach can be applied to different tasks without any tuning and again achieving state-of-the-art accuracy.
semantic role labeling (SRL)	error reduction	The extensive experimentation on two datasets of question classification (QC) and semantic role labeling (SRL), shows that: (i) PTK applied to our dependency trees outperforms STK, demonstrating that dependency parsers are fully exploitable for feature engineering based on structural kernels; (ii) SPTK outperforms any previous kernels achieving an unprecedented result of 41% of error reduction with respect to the former state-of-the-art on QC; and (iii) the experiments on SRL confirm that the approach can be applied to different tasks without any tuning and again achieving state-of-the-art accuracy.
semantic role labeling (SRL)	accuracy	The extensive experimentation on two datasets of question classification (QC) and semantic role labeling (SRL), shows that: (i) PTK applied to our dependency trees outperforms STK, demonstrating that dependency parsers are fully exploitable for feature engineering based on structural kernels; (ii) SPTK outperforms any previous kernels achieving an unprecedented result of 41% of error reduction with respect to the former state-of-the-art on QC; and (iii) the experiments on SRL confirm that the approach can be applied to different tasks without any tuning and again achieving state-of-the-art accuracy.
LCT	WL	Finally, on the fine grained experiments LCT still produces the most accurate outcome again exceeding the state-of-the-art (, where WL significantly improves on all models (CT included).
coreference resolvers	F-measure	Experimental results on corpus of Switchboard dialogues show that (1) adding our linguistic features to Nissim's feature set enables our system to outperform her system by 8.1% in F-measure, and (2) learned knowledge of information status can be used to improve coreference resolvers by 1.1-2.6% in F-measure.
sentiment classification	accuracy	The words are not stemmed since stemming is known to be detrimental to sentiment classification (  To evaluate the result, we use accuracy, F-score, recall and precision as the metrics.
sentiment classification	F-score	The words are not stemmed since stemming is known to be detrimental to sentiment classification (  To evaluate the result, we use accuracy, F-score, recall and precision as the metrics.
sentiment classification	recall	The words are not stemmed since stemming is known to be detrimental to sentiment classification (  To evaluate the result, we use accuracy, F-score, recall and precision as the metrics.
sentiment classification	precision	The words are not stemmed since stemming is known to be detrimental to sentiment classification (  To evaluate the result, we use accuracy, F-score, recall and precision as the metrics.
word ordering task	BLEU score	Ina standard word ordering task, our system gives a BLEU score of 40.1, higher than the previous result of 33.7 achieved by a dependency-based system.
parsing	accuracy	Our experiments show that parsing accuracy drops by about 6% when using automatic POS tags instead of gold ones.
parsing	accuracy	Experimental results on Chinese Penn Treebank 5 show that our joint models significantly improve the state-of-the-art parsing accuracy by about 1.5%.
parsing	accuracy	This is the fundamental reason of parsing accuracy improvement.
parsing	accuracy	Our experimental results show that parsing accuracy decreases by about 6% on Chinese when using automatic POS tagging results instead of gold ones (see in Section 5).
parsing	accuracy	Experimental results on Chinese Penn Treebank show that our joint models can significantly improve the state-ofthe-art parsing accuracy by about 1.5%.
POS tagging	error number	 Table 4: Error analysis of POS tagging. # means the  error number of the corresponding pattern made by the  baseline tagging model. ↓ and ↑ mean the error number  reduced or increased by the joint model.
constituency projection	RCA	In this paper we propose a relaxed correspondence assumption (RCA) for constituency Figure 1: An example for constituency projection based on the RCA assumption.
ASTs	F-score	The scope comparisons are used to compute ASTs, with an F-score of 90.6% on the set of ordering decisons.
Chart Inference	Brute Force (BF)	In the following experiments, we compare Chart Inference to the two baseline methods: Brute Force (BF), derived from Watkinson and Manandhar, and Rule-Based (RB), derived from Yao et al.
dependency parsing tasks	accuracy	An evaluation against a suite of unsu-pervised dependency parsing tasks, fora variety of languages, showed that lateen strategies significantly speedup training of both EM algorithms , and improve accuracy for hard EM.
Grammar induction	accuracy	Grammar induction with gold tags scores 50.7%, while the oracle skyline (an ideal, supervised instance of the DMV) could attain 78.0% accuracy.
MT	MIRA	Tellingly, in the entire proceedings of ACL 2010, only one paper describing a statistical MT system cited the use of MIRA for tuning, while 15 used MERT.
MT	MERT	Tellingly, in the entire proceedings of ACL 2010, only one paper describing a statistical MT system cited the use of MIRA for tuning, while 15 used MERT.
MT	MERT	For each language pair and each MT model we used MERT, MIRA, and PRO to tune with a standard set of baseline features, and used the latter two methods to tune with an extended set of features.
MT	MIRA	For each language pair and each MT model we used MERT, MIRA, and PRO to tune with a standard set of baseline features, and used the latter two methods to tune with an extended set of features.
Machine translation	IBM	 Table 1: Machine translation performance for the experiments listed in this paper. Scores are case-sensitive IBM  BLEU. For every choice of system, language pair, and feature set, PRO performs comparably with the other methods.
classification	false positive rate	In this work, we use α = 0.05 during classification corresponding to an expected 5% false positive rate.
translation	BLEU score	We find that translation is possible using the two-pass strategy with the PDA translation representation and that gains in BLEU score result from using the larger translation grammar.
character sequence mappings between source and target languages	recall	Many possible character sequence mappings between source and target languages may not be observed in training data, particularly when limited training data is available -hurting recall.
parsing task	accuracy	Those scores are, to our knowledge, the highest scores previously reported for this parsing task and establish our second main point: letting the model learn the language's word order in addition to learning the mapping from sentences to MR increases semantic parsing accuracy.
parsing task	F- measure	 Table 2: A summary of results for the parsing task, in F- measure. We also show the results of
relation extraction	error reduction	In relation extraction, we can achieve 12% error reduction in precision over a state-of-the-art weakly supervised baseline and we show that using features from our proposed models can find more facts fora relation without significant accuracy loss.
relation extraction	precision	In relation extraction, we can achieve 12% error reduction in precision over a state-of-the-art weakly supervised baseline and we show that using features from our proposed models can find more facts fora relation without significant accuracy loss.
relation extraction	accuracy	In relation extraction, we can achieve 12% error reduction in precision over a state-of-the-art weakly supervised baseline and we show that using features from our proposed models can find more facts fora relation without significant accuracy loss.
dependency parsing	accuracy	Their reranking model showed large improvements in dependency parsing accuracy.
Domain adaptation	labeled ac- curacy score (LAS)	 Table 2: Domain adaptation results. Table shows (for  both transition and graph-based parsers) the labeled ac- curacy score (LAS), unlabeled accuracy score (UAS)
Domain adaptation	accuracy score (UAS	 Table 2: Domain adaptation results. Table shows (for  both transition and graph-based parsers) the labeled ac- curacy score (LAS), unlabeled accuracy score (UAS)
POS tagging	accuracy	For example POS tagging accuracy drops from about 0.97 on news to 0.80 on tweets.
POS tagging	error	 Table 2: POS tagging performance on tweets. By training  on in-domain labeled data, in addition to annotated IRC  chat data, we obtain a 41% reduction in error over the  Stanford POS tagger.
predicting	PERSON	 Table 12: Performance at predicting both segmentation  and classification. Systems labeled with PLO are evalu- ated on the 3 MUC types PERSON, LOCATION, ORGA- NIZATION.
predicting	LOCATION	 Table 12: Performance at predicting both segmentation  and classification. Systems labeled with PLO are evalu- ated on the 3 MUC types PERSON, LOCATION, ORGA- NIZATION.
predicting	ORGA- NIZATION	 Table 12: Performance at predicting both segmentation  and classification. Systems labeled with PLO are evalu- ated on the 3 MUC types PERSON, LOCATION, ORGA- NIZATION.
NER	Accuracy	 Table 4: NER Accuracy on 2 test sets as the seed dictio- nary for brands grows. Results shown here are obtained  the same Men's clothing category dataset, as used to show  the supervised NER results in
Classification	accuracy	 Table 1: Classification results based on 5-fold cross vali- dation with parse rules as syntactic features (accuracy %)
name transliteration task	accuracy	Experimental results on the name transliteration task in five diverse languages show a maximum improvement of 29% accuracy and an average improvement of 17% accuracy compared to a state-of-the-art baseline system.
name transliteration task	accuracy	Experimental results on the name transliteration task in five diverse languages show a maximum improvement of 29% accuracy and an average improvement of 17% accuracy compared to a state-of-the-art baseline system.
Information Extraction (IE)	accuracy	Generic rule-based systems for Information Extraction (IE) have been shown to work reasonably well out-of-the-box, and achieve state-of-the-art accuracy with further domain customization.
NER extractor	accuracy	Since the generic NER extractor has to be manually customized, a major challenge is to ensure that the generated rules have good accuracy, and, at the same time, that they are not too complex, and consequently interpretable.
SO	strength	The SO consists of polarity (positive, negative, or other 1 ) and strength (degree to which a sentence is positive or negative).
polarity classification	Base BoO	For our experiments on polarity classification, we converted the predicted ratings of MEM, Base BoO , and Base SO-CAL into polarities by the method described in Sec.
SCFG induction	BLEU	Prior work on SCFG induction for SMT has validated modeling claims by reporting BLEU scores on real translation tasks.
SMT	BLEU	Prior work on SCFG induction for SMT has validated modeling claims by reporting BLEU scores on real translation tasks.
ASRL	ROUGE-1	Third, we evaluated ASRL taking into consideration sentence compression by using a very naive method, in terms of ROUGE-1, ROUGE-2, and ROUGE-3.
sentence compression	ROUGE-1	Third, we evaluated ASRL taking into consideration sentence compression by using a very naive method, in terms of ROUGE-1, ROUGE-2, and ROUGE-3.
word substitution ciphers	accuracy	Secondly, although their algorithm is able to handle word substitution ciphers with limited vocabulary, its deciphering accuracy is low.
word substitution cipher	accuracy	Furthermore, we also solve a very large word substitution cipher built from the English Gigaword corpus and achieve 92.2% deciphering accuracy on news text.
NIST Chinese-English translation task	BLUE	Experimental results on the NIST Chinese-English translation task show that our proposed tense models are very effective, contributing performance improvement by 0.62 BLUE points over a strong baseline.
SMT	Observations	Such model can be used to estimate the rationality of tense combination in a sentence and thus supervise SMT to reduce tense inconsistency errors against Observations (1) and (2) in the sentence-level.
parsing	accuracy	We evaluate the impact of non-projective buffer transitions on parsing accuracy by using the same baseticular subset of non-projective structures captured by each such parser.
Question Answering (QA)	accuracy	Question Answering (QA) research for factoid questions has recently achieved great success as demonstrated by IBM's Watson at Jeopardy: its accuracy has been reported to be around 85% on factoid questions.
IR	precision	MAP, widely used in evaluation of IR systems, measures the overall quality of the top-n answer candidates (n=20 in this experiment) using the formula: Here Q is a set of why-questions, A q is a set of correct answers to why-question q ∈ Q, P rec(k) is the precision at cut-off kin the top-n answer candidates, rel(k) is an indicator, 1 if the item at rank k is a correct answer in A q , 0 otherwise.
question analysis	ROUGE	We employed precision (P), recall (R) and F 1 -measure (F 1 ) as the evaluation metric for question analysis, and utilized ROUGE ( as the metric to evaluate the quality of answer generation.
aspect identification	T-Test	 Table 3: Performance of aspect identification for question  analysis. * denotes the results (i.e. P , R, F 1 ) are tested  for statistical significance using T-Test, p-values<0.05.
question  analysis	T-Test	 Table 3: Performance of aspect identification for question  analysis. * denotes the results (i.e. P , R, F 1 ) are tested  for statistical significance using T-Test, p-values<0.05.
question analysis	T-Test	 Table 4: Performance of implicit aspect identification for  question analysis. T-Test, p-values<0.05
answer generation	T-Test	 Table 5: Performance of answer generation. T-Test, p- values<0.05.
Parameter estimation	error rate	h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood, error rate), margin () and ranking (, and among which minimum error rate training (MERT) is the most popular one.
Parameter estimation	margin	h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood, error rate), margin () and ranking (, and among which minimum error rate training (MERT) is the most popular one.
Parameter estimation	minimum error rate training (MERT)	h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood, error rate), margin () and ranking (, and among which minimum error rate training (MERT) is the most popular one.
SMT	error rate	h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood, error rate), margin () and ranking (, and among which minimum error rate training (MERT) is the most popular one.
SMT	margin	h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood, error rate), margin () and ranking (, and among which minimum error rate training (MERT) is the most popular one.
SMT	minimum error rate training (MERT)	h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood, error rate), margin () and ranking (, and among which minimum error rate training (MERT) is the most popular one.
NIST Chinese-to-English translation tasks	MERT	Experiments on NIST Chinese-to-English translation tasks show that our local training method significantly gains over MERT, with the maximum improvements up to 2.0 BLEU, and its efficiency is comparable to that of the global training method.
NIST Chinese-to-English translation tasks	BLEU	Experiments on NIST Chinese-to-English translation tasks show that our local training method significantly gains over MERT, with the maximum improvements up to 2.0 BLEU, and its efficiency is comparable to that of the global training method.
annotation adaptation	accuracy	Compared to annotation adaptation, the optimized annotation transformation strategy leads to classifiers with significantly higher accuracy and several times faster processing on the same data sets.
SMT	BLEU	As the first step in this line of research, our approach is verified in a phrase-based SMT system on both English-to-Japanese and Chinese-to-English translation tasks . Significant improvements are reported on both translation quality (up to 1.31 BLEU) and word alignment quality (up to 3.15 F-score).
SMT	F-score	As the first step in this line of research, our approach is verified in a phrase-based SMT system on both English-to-Japanese and Chinese-to-English translation tasks . Significant improvements are reported on both translation quality (up to 1.31 BLEU) and word alignment quality (up to 3.15 F-score).
word alignment	F-score	As the first step in this line of research, our approach is verified in a phrase-based SMT system on both English-to-Japanese and Chinese-to-English translation tasks . Significant improvements are reported on both translation quality (up to 1.31 BLEU) and word alignment quality (up to 3.15 F-score).
classification	accuracy	It reaches a classification accuracy of 34%, which is about twice the random.
word segmentation	precision (P)	1 For word segmentation, three metrics are used for evaluation: precision (P), recall (R), and F-score (F) defined by 2PR/(P+R).
word segmentation	recall (R)	1 For word segmentation, three metrics are used for evaluation: precision (P), recall (R), and F-score (F) defined by 2PR/(P+R).
word segmentation	F-score (F)	1 For word segmentation, three metrics are used for evaluation: precision (P), recall (R), and F-score (F) defined by 2PR/(P+R).
parsing	recall	For parsing, we use the standard parseval evaluation metrics: bracketing precision, recall and F-score.
parsing	F-score	For parsing, we use the standard parseval evaluation metrics: bracketing precision, recall and F-score.
POS tagging error patterns	error  number	 Table 11: POS tagging error patterns. # means the error  number of the corresponding pattern made by the pipeline  tagging model. ↓ and ↑ mean the error number reduced  or increased by the joint model.
LM adaptation	TF-IDF	Furthermore, traditional approaches for LM adaptation are based on bag-of-words models and considered to be context independent, despite of their state-of-the-art performance, such as TF-IDF (, centroid similarity, and cross-lingual similarity (CLS) ().
SMT)	BLEU score	We carryout translation experiments on the test set by hierarchical phrase-based (HPB) SMT) system to demonstrate the utility of LM adaptation on improving SMT performance by BLEU score ().
LM adaptation	BLEU score	We carryout translation experiments on the test set by hierarchical phrase-based (HPB) SMT) system to demonstrate the utility of LM adaptation on improving SMT performance by BLEU score ().
SMT	BLEU score	We carryout translation experiments on the test set by hierarchical phrase-based (HPB) SMT) system to demonstrate the utility of LM adaptation on improving SMT performance by BLEU score ().
Paraphrase classification	accuracy	In order to encode which words appear in: Paraphrase classification accuracy in %.
Paraphrase classification	accuracy	 Table 4: Paraphrase classification accuracy in %. In- cluded features are in parentheses: "con" is sentence vec- tor concatenation, "sub" is sentence vector subtraction,  "other" stands for 4 other features (see Section 4)
parsing	490	(Although all the parsing results are much weaker as compared to the results presented in Section 3.2, they are all higher than the majority baseline of 14.29% i.e. 70/490).
SCENE	TEXT	For instance, SCENE in English is substantially negatively impacted by the use of the numerous examples of TEXT (-4 in F-measure) and even more when using all other training data.
SCENE	F-measure	For instance, SCENE in English is substantially negatively impacted by the use of the numerous examples of TEXT (-4 in F-measure) and even more when using all other training data.
parsing	accuracy	This yields up to an eightfold parsing speedup, while providing the same empirical accuracy and certificates of optimality as working with the full LP relaxation.
speech recognition	accuracy	The WER relates the speech recognition accuracy.
translation	accuracy	The BLEU score reflects the translation accuracy.
minimization	avoidance	More careful choosing of bigrams during minimization results in the avoidance of LS and FW (but not SYM) for "a" as well as FW and RP for "in".
minimization	FW	More careful choosing of bigrams during minimization results in the avoidance of LS and FW (but not SYM) for "a" as well as FW and RP for "in".
minimization	FW	More careful choosing of bigrams during minimization results in the avoidance of LS and FW (but not SYM) for "a" as well as FW and RP for "in".
SMT	accuracy	In this work, we present a method for inducing a parser for SMT by training a discriminative model to maximize reordering accuracy while treating the parse tree as a latent variable.
translation	accuracy	Experiments find that the proposed model improves both reordering and translation accuracy, leading to average gains of 1.2 BLEU points on English-Japanese and Japanese-English translation without linguistic analysis tools, or up to 1.5 BLEU points when these tools are incorporated.
translation	BLEU	Experiments find that the proposed model improves both reordering and translation accuracy, leading to average gains of 1.2 BLEU points on English-Japanese and Japanese-English translation without linguistic analysis tools, or up to 1.5 BLEU points when these tools are incorporated.
translation	BLEU	Experiments find that the proposed model improves both reordering and translation accuracy, leading to average gains of 1.2 BLEU points on English-Japanese and Japanese-English translation without linguistic analysis tools, or up to 1.5 BLEU points when these tools are incorporated.
translation	accuracy	Our experiments test the reordering and translation accuracy of translation systems using the proposed method.
parsing	accuracy	train a bilingual parsing model that uses bilingual agreement features to improve parsing accuracy.
constancy identification	precision	Experimental results confirmed that the time-series frequency distributions contributed much to the recall of constancy identification and the precision of the uniqueness identification.
classification	accuracy	We have thereby confirmed that the features induced from this time-series text contributed much to improve the classification accuracy.
segmentation	accuracy	We evaluate the segmentation accuracy with respect to the intra-sentential segment boundaries following.
MT metrics	BLEU	The early seminal work on automatic MT metrics (e.g., BLEU and NIST) is largely based on n-gram matches ().
MT	BLEU	We consider four widely used MT metrics (BLEU, NIST, METEOR (v0.7), and TER) as our baselines.
MT	METEOR	We consider four widely used MT metrics (BLEU, NIST, METEOR (v0.7), and TER) as our baselines.
MT	TER	We consider four widely used MT metrics (BLEU, NIST, METEOR (v0.7), and TER) as our baselines.
argument identification	precision	It shows that our baseline system outperforms Chen and Ji (2009b) by 1.8, 2.2, 3.9 and 2.3 in F1-measure on trigger identification, trigger type determination, argument identification and argument role determination, respectively, with both gains in precision and recall.
argument identification	recall	It shows that our baseline system outperforms Chen and Ji (2009b) by 1.8, 2.2, 3.9 and 2.3 in F1-measure on trigger identification, trigger type determination, argument identification and argument role determination, respectively, with both gains in precision and recall.
argument identification	F1-measure	While argument identification has the performance gap of 8.5 in F1-measure compared to trigger type determination (49.3 vs. 57.8), the former one, trigger identification, can only achieve the performance of 61.5 in F1-measure (in particular the recall with only 52.0).
argument identification	F1-measure	While argument identification has the performance gap of 8.5 in F1-measure compared to trigger type determination (49.3 vs. 57.8), the former one, trigger identification, can only achieve the performance of 61.5 in F1-measure (in particular the recall with only 52.0).
argument identification	recall	While argument identification has the performance gap of 8.5 in F1-measure compared to trigger type determination (49.3 vs. 57.8), the former one, trigger identification, can only achieve the performance of 61.5 in F1-measure (in particular the recall with only 52.0).
trigger identification	F1-measure	While argument identification has the performance gap of 8.5 in F1-measure compared to trigger type determination (49.3 vs. 57.8), the former one, trigger identification, can only achieve the performance of 61.5 in F1-measure (in particular the recall with only 52.0).
trigger identification	recall	While argument identification has the performance gap of 8.5 in F1-measure compared to trigger type determination (49.3 vs. 57.8), the former one, trigger identification, can only achieve the performance of 61.5 in F1-measure (in particular the recall with only 52.0).
trigger identification	recall	In this paper, we will focus on trigger identification to improve its performance, particularly for the recall, via compositional semantics inside Chinese triggers and discourse consistency between Chinese trigger mentions.
relation extraction	accuracy	However, the KB contains much more knowledge about other relations that could potentially be helpful in improving relation extraction accuracy and coverage, but that is not used in such purely text-based approaches.
PRA	precision	It can be seen that the PRA model is able to produce very high precision predications even when one considers the top 10,000 predictions.
SIM	precision	Among the 4 methods, SIM has the highest precision (0.964) when relation phrases for which it fails to generate any Type A relations are excluded, but its recall is low.
SIM	recall	Among the 4 methods, SIM has the highest precision (0.964) when relation phrases for which it fails to generate any Type A relations are excluded, but its recall is low.
SNE	recall	shows that WEBRE outperforms SNE significantly in pairwise recall while having similar precision.
SNE	precision	shows that WEBRE outperforms SNE significantly in pairwise recall while having similar precision.
Type B relation extraction	recall	 Table 2. Performance for Type B relation extraction. The first  column shows the range of the maximum sizes of Type A  relations in the Type B relation. The last column shows the  number of Type B relations that are in this range. The number  in parenthesis in the third column is the recall of phrases.
Constituency parser	F-score	Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors.
MT evaluation	BLEU	This is notably reflected in the representative MT evaluation metrics, such as BLEU (), METEOR (Banerjee and) and TER (), that adopt a sentence-by-sentence fashion to score MT outputs.
MT evaluation	METEOR	This is notably reflected in the representative MT evaluation metrics, such as BLEU (), METEOR (Banerjee and) and TER (), that adopt a sentence-by-sentence fashion to score MT outputs.
MT evaluation	TER	This is notably reflected in the representative MT evaluation metrics, such as BLEU (), METEOR (Banerjee and) and TER (), that adopt a sentence-by-sentence fashion to score MT outputs.
MT	BLEU	The two MT outputs have a similar number of matched n-grams and hence receive similar BLEU scores.
MT evaluation	consistency	A previous study comparing MT evaluation at the sentence versus document level) reports a poor consistency in the evaluation results at these two levels when the sentence-level scores of MT output are low.
Evaluating Semantic Orientation	accuracy	 Table 2: Evaluating Semantic Orientation on accuracy metric
parsing	F1	 Table 1: The effect of algorithm choice for training and  parsing on a product of two 2-state parsers on F1. Petrov  is the product parser of Petrov (2010), and Indep. refers  to independently trained models. For comparison, a four- state parser achieves a score of 83.2.
separating out coherent and incoherent news articles	accuracies	Results show that our method has high discriminating power for separating out coherent and incoherent news articles reaching accuracies of up to 90%.
coherence prediction	accuracy	We also present results on coherence prediction: our model can distinguish the introduction section of conference papers from its perturbed versions with over 70% accuracy.
predicting related work sections	accuracy	 Table 9. The proportion  of examples under each setting is also indicated.  When only examples above 0.6 confidence are ex- amined, the classifier has a higher accuracy of 63.8%  for abstracts and covers close to 70% of the exam- ples. Similarly, when a cutoff of 0.7 is applied to the  confidence for predicting related work sections, we  achieve 63.3% accuracy for 53% of examples. So  we can consider that 30 to 47% of the examples in  the two sections respectively are harder to tell apart.  Interestingly however even high confidence predic- tions on introductions remain incorrect.
coreference resolution	B-CUBED	For coreference resolution, MUC (, B-CUBED ( and CEAF-E () are used for evaluation.
POS tagging	accuracy	 Table 2: POS tagging accuracy: The P-HMM+D+E tagger outperforms the unbiased HMM tagger and the  Stanford tagger on all target domains. The 'avg' column includes source-domain development data results. Differ- ences between the P-HMM+D+E and the Stanford tagger are statistically significant at p < 0.01 on average and on 11  out of 12 target domain. We used the two-tailed Chi-square test with Yates' correction.
parsing	accuracy	In the Japanese example above, this difference in mapping yields a 6.7% difference in parsing accuracy.
parsing	accuracy	Following the standard evaluation practice in parsing, we use directed dependency accuracy as our measure of performance.
domain adaptation	error reduction	For domain adaptation, we show an error reduction of up to 7.7% when adapting the second-order projective MST parser) from newswire to the QuestionBank domain.
parsing	unlabeled attachment score (UAS)	To measure parsing performance, we use unlabeled attachment score (UAS) given by the CONLL-X dependency parsing shared task evaluation script ().
POS tagging	accuracy	We compare the accuracy of POS tagging with global constraints to the accuracy of the Stanford POS tagger 3 . Domain Adaptation Accuracy Results are presented in.
Dependency parsing UAS	Acc	 Table 3: Dependency parsing UAS by size of training set and language. English data is from the WSJ. Bulgarian,  German, Japanese, and Spanish data is from the CONLL-X data sets. Base is the second-order, projective dependency  parser of McDonald et al. (2005). ST is a self-training model based on Reichart and Rappoport (2007). Model is the  same parser augmented with inter-sentence constraints. ER is error reduction. Using the sign test with p ≤ 0.05, all  50, 100, and 200 results are significant, as are Eng and Ger 500.  50  100  200  500  Base  Model (ER)  Base Model (ER) Base Model (ER) Base Model (ER)  Acc 79.67 81.77 (10.33) 85.42 86.37 (6.52) 88.63 89.37 (6.51) 91.59 91.98 (4.64)  Unk 62.88 67.16 (11.53) 71.10 73.32 (7.68) 75.82 78.07 (9.31) 80.67 82.28 (8.33)
error reduction	Acc	 Table 3: Dependency parsing UAS by size of training set and language. English data is from the WSJ. Bulgarian,  German, Japanese, and Spanish data is from the CONLL-X data sets. Base is the second-order, projective dependency  parser of McDonald et al. (2005). ST is a self-training model based on Reichart and Rappoport (2007). Model is the  same parser augmented with inter-sentence constraints. ER is error reduction. Using the sign test with p ≤ 0.05, all  50, 100, and 200 results are significant, as are Eng and Ger 500.  50  100  200  500  Base  Model (ER)  Base Model (ER) Base Model (ER) Base Model (ER)  Acc 79.67 81.77 (10.33) 85.42 86.37 (6.52) 88.63 89.37 (6.51) 91.59 91.98 (4.64)  Unk 62.88 67.16 (11.53) 71.10 73.32 (7.68) 75.82 78.07 (9.31) 80.67 82.28 (8.33)
parsing	accuracy	Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-the-art results for all languages.
parsing	accuracy	This suggests that joint models for tagging and parsing might improve accuracy also in the case of dependency parsing.
tagging	accuracy	Exper-iments show that joint modeling improves both tagging and parsing accuracy, leading to state-of-the-art accuracy for richly inflected languages like Czech and German as well as more configurational languages like Chinese and English.
tagging	accuracy	Exper-iments show that joint modeling improves both tagging and parsing accuracy, leading to state-of-the-art accuracy for richly inflected languages like Czech and German as well as more configurational languages like Chinese and English.
parsing	accuracy	Exper-iments show that joint modeling improves both tagging and parsing accuracy, leading to state-of-the-art accuracy for richly inflected languages like Czech and German as well as more configurational languages like Chinese and English.
parsing	accuracy	Exper-iments show that joint modeling improves both tagging and parsing accuracy, leading to state-of-the-art accuracy for richly inflected languages like Czech and German as well as more configurational languages like Chinese and English.
prediction	accuracy	We also investigate ways of how the language-based and the social views can be combined to improve prediction accuracy.
query spelling correction	recall	Because query spelling correction is an online operation, only a small number of candidates can enter the ranker due to efficiency concerns, thus limiting the ability of the ranker to the ceiling of recall set by the suboptimal search phase.
Authorship attribution	train- ing data	 Table 12: Authorship attribution with 80% train- ing data.
automatic dating of documents	accuracy	When the size of the training set is 1,000, our BFS-based model and confidence boosting model combined with the MaxEnt classifier outperform Chambers's joint model which is considered the state-of-the-art model for the task of automatic dating of documents by 38.7% and 46.8% relative accuracy respectively.
segmentation	precision	For segmentation, we report precision, recall and F-score for word boundaries (bds), and for the positions of word tokens in the surface string (srf ; both boundaries must be correct).
segmentation	recall	For segmentation, we report precision, recall and F-score for word boundaries (bds), and for the positions of word tokens in the surface string (srf ; both boundaries must be correct).
segmentation	F-score	For segmentation, we report precision, recall and F-score for word boundaries (bds), and for the positions of word tokens in the surface string (srf ; both boundaries must be correct).
interpretability	accuracy	Ensemble methods are well known (see for example, Dietterich (2000)) but our focus here is on using them for interpretability while still maintaining accuracy.
MT	BLEU score	 Table 6: Normalization and MT Results. Rows denote different normalizations, and columns different translation  systems, except the first column (Norm), which denotes the normalization experiment. Cells display the BLEU score  of that experiment.  Moses  Moses  Condition  Norm (News) (News+Weibo) Online A Online B Online C  baseline  19.90  15.10  24.37  20.09  17.89  18.79  norm+phrase  21.96  15.69  24.29  20.50  18.13  18.93  norm+phrase+char  22.39  15.87  24.40  20.61  18.22  19.08  norm+phrase+char+mono 22.91  15.94  24.46  20.78  18.37  19.21
question answering thread	difficulty score	Our approach is based on two intuitive assumptions: given a question answering thread, the difficulty score of the question is higher than the expertise score of the asker, but lower than that of the best answerer; (2) the expertise score of the best answerer is higher than that of the asker as well as all other answerers.
classification	accuracy	The classification accuracy results on the three tasks are reported in.
domain adaptation	accuracy	Moreover, these results also clearly show that domain adaptation is not asymmetric process, as we can see it is easier to conduct domain adaptation from the source domain Books to the target domain Kitchen (with an accuracy around 82%), but it is more difficult to make domain adaptation from the source domain Kitchen to the target domain Books (with an ac-   curacy around 75%).
domain adaptation	ac-   curacy	Moreover, these results also clearly show that domain adaptation is not asymmetric process, as we can see it is easier to conduct domain adaptation from the source domain Books to the target domain Kitchen (with an accuracy around 82%), but it is more difficult to make domain adaptation from the source domain Kitchen to the target domain Books (with an ac-   curacy around 75%).
translation	exact	 Table 1: Experimental results for translation experiments. Column time is the mean time per sentence in seconds,  cert is the percentage of sentences solved with a certificate of optimality, exact is the percentage of sentences solved  exactly, i.e. θ x + τ = θ x  *  + τ . Results are grouped by sentence length (group 1-10 is omitted for space).
IMT	Key-stroke ratio (KSR)	The counterpart of PKSR in an IMT scenario is (: Key-stroke ratio (KSR): number of key strokes, divided by the number of reference characters.
MT	BLEU score	We also evaluate the quality of the automatic translations generated by the MT models with the widespread BLEU score ().
machine translation	BLEU	Alternatively, we propose a max-margin estimation approach to discrim-inatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU.
translation	margin	The more serious the translation errors, the larger the margin.
coreference errors	accuracy	Previous investigations of coreference errors have focused on quantifying the importance of subtasks such as named entity recognition and anaphoricity detection, typically by measuring accuracy improvements when partial gold annotations are provided ().
coreference resolution	precision	While the refined parser improves the recall of mention detection and coreference resolution, refined example generation contributes more to precision.
recall mention detection	precision	NECO extends the Stanford's sieve-based model, in which a high recall mention detection phase is followed by a sequence of cluster merging operations ordered by decreasing precision.
coreference resolution	automatic mention detection	• We present experiments showing improved performance at coreference resolution, given both gold and automatic mention detection: e.g., 6.2 point improvement in MUC recall on ACE 2004 newswire text and 3.1 point improvement in MUC precision the CoNLL 2011 test set.
coreference resolution	recall	• We present experiments showing improved performance at coreference resolution, given both gold and automatic mention detection: e.g., 6.2 point improvement in MUC recall on ACE 2004 newswire text and 3.1 point improvement in MUC precision the CoNLL 2011 test set.
coreference resolution	precision	• We present experiments showing improved performance at coreference resolution, given both gold and automatic mention detection: e.g., 6.2 point improvement in MUC recall on ACE 2004 newswire text and 3.1 point improvement in MUC precision the CoNLL 2011 test set.
POS tagging	training time in minutes (TT)	 Table 2: POS tagging experiments with pruned and unpruned CRFs with different orders n. For every language the  training time in minutes (TT) and the POS accuracy (ACC) are given. * indicates models significantly better than CRF  (first line).
POS tagging	POS accuracy (ACC)	 Table 2: POS tagging experiments with pruned and unpruned CRFs with different orders n. For every language the  training time in minutes (TT) and the POS accuracy (ACC) are given. * indicates models significantly better than CRF  (first line).
POS tagging	accuracies (ACC)	 Table 4: Development results for POS tagging. Given are training times in minutes (TT) and accuracies (ACC).  Best baseline results are underlined and the overall best results bold. * indicates a significant difference (positive or  negative) between the best baseline and a PCRF model.
identifying triliteral Hebrew roots	F1-score	We obtain 74% accuracy in identifying triliteral Hebrew roots, while performing morphological segmentation with an F1-score of 78.1.
Morpheme lexicon induction	F1-scores	 Table 2: Morpheme lexicon induction quality. F1-scores for lexicons induced from the most probable parse  of each different dataset under each models.  † 42.4 was obtained by taking the union of R3 and T3 items to  match the way the model used them (see  §6.4).
Dependency parsing	ORE	Dependency parsing gives unambiguous relations among each word in the sentence, and the ORE approaches in this category such as PATTY (), OLLIE (, and TreeKernel () identify whole subtrees connecting the relation predicate and its arguments.
Dependency parsing	PATTY	Dependency parsing gives unambiguous relations among each word in the sentence, and the ORE approaches in this category such as PATTY (), OLLIE (, and TreeKernel () identify whole subtrees connecting the relation predicate and its arguments.
Dependency parsing	OLLIE	Dependency parsing gives unambiguous relations among each word in the sentence, and the ORE approaches in this category such as PATTY (), OLLIE (, and TreeKernel () identify whole subtrees connecting the relation predicate and its arguments.
answer selection	F1	The results show that our methods greatly improve on both tasks yielding a large improvement in Mean Average Precision for answer selection and in F1 for answer extraction: up to 22% of relative improvement in F1, when small training data is used.
answer selection	F1	The results show that our methods greatly improve on both tasks yielding a large improvement in Mean Average Precision for answer selection and in F1 for answer extraction: up to 22% of relative improvement in F1, when small training data is used.
answer extraction	F1	The results show that our methods greatly improve on both tasks yielding a large improvement in Mean Average Precision for answer selection and in F1 for answer extraction: up to 22% of relative improvement in F1, when small training data is used.
answer extraction	precision	 Table 5: Results on answer extraction. P/R -precision  and recall; pairs -number of QA pairs with a correctly ex- tracted answer, q -number of questions with at least one  correct answer extracted, F1 sets an upper bound on the  performance assuming the selected best answer among  extracted candidates is always correct. *-marks the set- ting
answer extraction	recall	 Table 5: Results on answer extraction. P/R -precision  and recall; pairs -number of QA pairs with a correctly ex- tracted answer, q -number of questions with at least one  correct answer extracted, F1 sets an upper bound on the  performance assuming the selected best answer among  extracted candidates is always correct. *-marks the set- ting
answer extraction	F1	 Table 5: Results on answer extraction. P/R -precision  and recall; pairs -number of QA pairs with a correctly ex- tracted answer, q -number of questions with at least one  correct answer extracted, F1 sets an upper bound on the  performance assuming the selected best answer among  extracted candidates is always correct. *-marks the set- ting
summarization	ROUGE	We evaluate our proposed summarization approach on the TAC 2008 and 2011 data sets using the standard ROUGE metric).
word ordering	BLEU	For the romance languages (Spanish and French), word ordering depends highly on lexical choice which is captured by the lexical features in our classifiers.: BLEU scores on the WMT 2010 setup.
translation	BLEU	To evaluate the translation quality, we used BLEU () as our evaluation metric.
Document classification	F1 score	 Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
MIRA	BLEU	In contrast, we propose a remarkably simple but efficient batch MIRA approach which exploits the exact corpus-level BLEU to compute model losses.
Polarity classification	accuracies	 Table 2: Polarity classification accuracies using excellent  and poor as seed words
Classification	Accuracy	 Table 2: Classification Accuracy by Relation Type
Classification	Accuracy	 Table 3: Classification Accuracy by Feature Class
subjectivity classification	Precision	Evaluation metrics used for subjectivity classification and relation extraction throughout the experiments include: Precision, Recall, and F1-score.
subjectivity classification	Recall	Evaluation metrics used for subjectivity classification and relation extraction throughout the experiments include: Precision, Recall, and F1-score.
subjectivity classification	F1-score	Evaluation metrics used for subjectivity classification and relation extraction throughout the experiments include: Precision, Recall, and F1-score.
relation extraction	Precision	Evaluation metrics used for subjectivity classification and relation extraction throughout the experiments include: Precision, Recall, and F1-score.
relation extraction	Recall	Evaluation metrics used for subjectivity classification and relation extraction throughout the experiments include: Precision, Recall, and F1-score.
relation extraction	F1-score	Evaluation metrics used for subjectivity classification and relation extraction throughout the experiments include: Precision, Recall, and F1-score.
scope detection	accuracy	Compared with the state of the art scope detection systems, our system achieves the performance of accuracy 76.90% on negation and 84.21% on speculation (on Abstracts sub-corpus).
tweet classification	Time	 Table 5: Example of tweet classification using the Na¨ıveNa¨ıve Bayes model with the two different priors (E -empirical, P - GP forecast). Rank shows the rank in probability of the correct class (hashtag) under the model. Time is G.M.T.
tweet classification	G.M.T	 Table 5: Example of tweet classification using the Na¨ıveNa¨ıve Bayes model with the two different priors (E -empirical, P - GP forecast). Rank shows the rank in probability of the correct class (hashtag) under the model. Time is G.M.T.
opinion mining	accuracy	Direct quotations are used for opinion mining and information extraction as they have an easy to extract span and they can be attributed to a speaker with high accuracy.
information extraction	accuracy	Direct quotations are used for opinion mining and information extraction as they have an easy to extract span and they can be attributed to a speaker with high accuracy.
Speaker attribution	accuracy	 Table 7: Speaker attribution accuracy results for both cor- pora over gold standard quotations.
GER	BL	 Table 5: Learning curve comparing system performance  for GER on tokens on DEV corpus. BL=Baseline
Translation	accuracy	 Table 5: Translation accuracy of the joint model with various encodings of the foreign sentence measured on the  French-English task. Perplexity (PPL) is based on news2011.
Translation	. Perplexity (PPL)	 Table 5: Translation accuracy of the joint model with various encodings of the foreign sentence measured on the  French-English task. Perplexity (PPL) is based on news2011.
Translation	accuracy	 Table 8: Translation accuracy of the joint model with a source-target transform, measured on the French-English task.  Perplexity (PPL) is based on news2011; differences to target-only are significant at the p < 0.001 level.
Translation	.  Perplexity (PPL)	 Table 8: Translation accuracy of the joint model with a source-target transform, measured on the French-English task.  Perplexity (PPL) is based on news2011; differences to target-only are significant at the p < 0.001 level.
MT	BLEU	Adding learned translations to our French-English MT model results in gains of about 2 BLEU points over strong baselines.
translation	BLEU score	While translation quality suffers by only about 0.67 in BLEU score on average, across two different language pairs.
MT	BLEU	It has been used for MT with varying de-6 They reported +0.8 BLEU from system combination for AR→EN, and saw a further +0.5-0.7 from their new features.
parsing	accuracy	By retaining function labels during parsing, we have shown that LTH dependencies can be recovered with a high level of accuracy without having to resort to a post-parsing function labeling step.
parser	accuracy	Our model achieves an ubertagging accuracy that can lead to a four to eight fold speedup while improving parser accuracy.
Information Retrieval	Mean Average Precision (MAP)	Similar to Information Retrieval, we also consider using Mean Average Precision (MAP) as metrics to measure the overall quality of retrieved products.
prediction	accuracy	As expected, prediction accuracy increases as we approach the time of the action.
prediction	accuracy	These tend to be hard instances, and thus as expected, prediction accuracy drops for all systems.
semantic relation classification	accuracy	Our experimental results on semantic relation classification show that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model.
NIST08 Chinese-English translation task	BLEU	On NIST08 Chinese-English translation task, we obtain an improvement of 0.48 BLEU from a competitive baseline (30.01 BLEU to 30.49 BLEU) with the Stanford Phrasal MT system.
NIST08 Chinese-English translation task	BLEU	On NIST08 Chinese-English translation task, we obtain an improvement of 0.48 BLEU from a competitive baseline (30.01 BLEU to 30.49 BLEU) with the Stanford Phrasal MT system.
NIST08 Chinese-English translation task	BLEU	On NIST08 Chinese-English translation task, we obtain an improvement of 0.48 BLEU from a competitive baseline (30.01 BLEU to 30.49 BLEU) with the Stanford Phrasal MT system.
Vector Matching Alignment	AER	 Table 3: Vector Matching Alignment AER (lower is bet- ter)
Tokenization	accuracy	Tokenization is widely regarded as a solved problem due to the high accuracy that rule-based tokenizers achieve.
predicting health inspections	accuracy	In this work, we report the first empirical study demonstrating the utility of review analysis for predicting health inspections, achieving over 82% accuracy in discriminating severe offenders from places with no violation, and find predictive cues in reviews that correlate with the inspection results.
feature selection	accuracy	We aim to do feature selection and edge pruning dynamically, balancing speed and accuracy by using only as many features as needed.
dependency parsing	accuracy	In this paper, we first explore standard static feature selection methods for dependency parsing, and show that even a few feature templates can give decent accuracy (Section 3.2).
summarization	ROUGE score	This shows that word-based joint compression and summarization can improve ROUGE score; however, we need to keep in mind about linguistic quality and find a tradeoff between the ROUGE score and the linguistic quality.
summarization	ROUGE score	This shows that word-based joint compression and summarization can improve ROUGE score; however, we need to keep in mind about linguistic quality and find a tradeoff between the ROUGE score and the linguistic quality.
information retrieval	mean reciprocal rank (MRR)	From a system aspect, CET boosts systems' information retrieval substantially: the mean reciprocal rank (MRR) of vector space model (VSM) and query likelihood language model (QLLM) are improved by 9.3% and 8.2%, respectively.
translation	BLEU	Experiments on large-scale training data show that they can substantially improve translation quality in terms of BLEU and that the probability cohesion model outperforms previous models based on lexical cohesion devices.
alignment	accuracy	Our approach gives the same level of alignment accuracy as IBM Model 2.
predicting pronunciation variation	Recall	A noteworthy result is the apparent usefulness of stress modeling for predicting pronunciation variation using WFSTs with the direct method; this is: Recall for generating alternative pronunciations seen in the first two data columns of 1.
translation	BLEU score	11 We evaluate translation quality by translating and measuring the BLEU score of a 2000-3000 sentencelong evaluation corpus, averaging the results over 3 MIRA runs to control for optimizer instability).
translation	MIRA	11 We evaluate translation quality by translating and measuring the BLEU score of a 2000-3000 sentencelong evaluation corpus, averaging the results over 3 MIRA runs to control for optimizer instability).
Translation	BLEU	 Table 4: Translation quality (measured by BLEU) aver- aged over 3 MIRA runs.
Translation	MIRA	 Table 4: Translation quality (measured by BLEU) aver- aged over 3 MIRA runs.
largescale retrieval	PRES	We show in an experimental evaluation on largescale retrieval on patent abstracts that our boosting approach is comparable in MAP and improves significantly by 13-15 PRES points over very competitive translation-based CLIR systems that are trained on 1.8 million parallel sentence pairs from JapaneseEnglish patent documents.
predicting the success of novels written by previously unseen authors	accuracy	First, we tackle the hard task of predicting the success of novels written by previously unseen authors, avoiding incidental learning of authorship signature, since previous research demonstrated that one can achieve very high accuracy in authorship attribution (as high as 96% in some experimental setup) (e.g.,,).
Constituency parsing	accuracy	Constituency parsing with high accuracy (e.g. latent variable) grammars remains a computational challenge.
NP bracketing	accuracy	Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing, we test an approach exploiting PMI features, and show that plausibility features relying on composed representations can significantly boost accuracy over PMI.
direction finding	BLEU	To improve search in large parameter spaces, we also present anew direction finding algorithm that uses the gradient of expected BLEU to orient MERT's exact line searches.
connective identification	F-measure	For connective identification, achieved the performance of 95.76% and 93.62% in F-measure using gold-standard and automatic parse trees, respectively.
discourse relation classification	F-measure	For discourse relation classification, achieved the performance of 86.77% in F-measure on classifying discourse relations into 16 level 2 types.
in-cremental repair detection	accuracy	Results on the Switchboard dis-fluency tagged corpus show utterance-final accuracy on a par with state-of-the-art in-cremental repair detection methods, but with better incremental accuracy, faster time-to-detection and less computational overhead.
text processing pipeline	accuracy	Since these two tasks are positioned as the fundamental step in the text processing pipeline, their accuracy is vital for all downstream applications.
lexical normalization task	precision	 Table 7: Results of lexical normalization task in  terms of precision, recall, and F 1 -score.
lexical normalization task	recall	 Table 7: Results of lexical normalization task in  terms of precision, recall, and F 1 -score.
lexical normalization task	F 1 -score	 Table 7: Results of lexical normalization task in  terms of precision, recall, and F 1 -score.
NIST08 Chinese-English translation task	BLEU	On the NIST08 Chinese-English translation task, we obtained 0.68 BLEU improvement.
phrase translation	BLEU	Our system includes a phrase translation model, an n-gram language model, a lexicalized reordering model, a word penalty model and a phrase penalty model, which is similar to Moses (. The evaluation metric is BLEU).
translation	BLEU	In translation evaluations, we achieved statistically significant gains 153 in BLEU scores in the NTCIR10.
word alignment	AER	We measure the impact of our proposed methods on the quality of word alignment measured   by AER and F-measure.
word alignment	F-measure	We measure the impact of our proposed methods on the quality of word alignment measured   by AER and F-measure.
translation	BLEU	Next, we performed a translation evaluation, measured by BLEU ().
Translationese	accuracy	There are two stated motivations for the tasks above: first, empirical validation of linguistic theories about Translationese (, and second, improving statistical machine translation by leveraging the knowledge of the translation direction in training and test data (Lember-   Motivated by these limitations, in this work we focus on improving sentence-level classification accuracy by using non-domain-specific bilingual features at the sentence level.
LM evaluation	BLEU score	The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus.
SMT	MERT	The Moses phrase-based SMT system was applied ( , together with GIZA++ () for alignment and MERT for tuning on the development data.
alignment	MERT	The Moses phrase-based SMT system was applied ( , together with GIZA++ () for alignment and MERT for tuning on the development data.
SMT	distortion scores	Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score.
translation	BLEU	The translation performance was measured by the case-insensitive BLEU on the tokenized test data.
LM growing	BLEU	The results were shown in. indicated that our proposed LM growing method improved both PPL and BLEU in comparison with both BNLM and our previous CSLM converting method, so it was suitable for domain adaptation, which is one of focuses of the current SMT research.
MT	BLEU score	In this paper, we present: (a) the first human judgment dataset for Arabic MT (b) the Arabic Language BLEU (AL-BLEU), an extension of the BLEU score for Arabic MT evaluation.
MT outputs	BLEU	 Table 2: Example of ranked MT outputs in our gold-standard dataset. The first two rows specify the  English input and the Arabic reference, respectively. The third row of the table lists the different MT  system as ranked by annotators, using BLEU scores (column 4) and AL-BLEU (column 6). The differ- ent translation candidates are given here along with their associated Bucklwalter transliteration. 3 This  example, shows clearly that AL-BLEU correlates better with human decision.
MT outputs	AL-BLEU	 Table 2: Example of ranked MT outputs in our gold-standard dataset. The first two rows specify the  English input and the Arabic reference, respectively. The third row of the table lists the different MT  system as ranked by annotators, using BLEU scores (column 4) and AL-BLEU (column 6). The differ- ent translation candidates are given here along with their associated Bucklwalter transliteration. 3 This  example, shows clearly that AL-BLEU correlates better with human decision.
MT	BLEU	 Table 2: Example of ranked MT outputs in our gold-standard dataset. The first two rows specify the  English input and the Arabic reference, respectively. The third row of the table lists the different MT  system as ranked by annotators, using BLEU scores (column 4) and AL-BLEU (column 6). The differ- ent translation candidates are given here along with their associated Bucklwalter transliteration. 3 This  example, shows clearly that AL-BLEU correlates better with human decision.
MT	AL-BLEU	 Table 2: Example of ranked MT outputs in our gold-standard dataset. The first two rows specify the  English input and the Arabic reference, respectively. The third row of the table lists the different MT  system as ranked by annotators, using BLEU scores (column 4) and AL-BLEU (column 6). The differ- ent translation candidates are given here along with their associated Bucklwalter transliteration. 3 This  example, shows clearly that AL-BLEU correlates better with human decision.
MT evaluation	BLEU	This was only possible because of the use of automatic metrics for MT evaluation, such as BLEU (), which is the defacto standard; and more recently: TER () and METEOR (), among other emerging MT evaluation metrics.
MT evaluation	TER	This was only possible because of the use of automatic metrics for MT evaluation, such as BLEU (), which is the defacto standard; and more recently: TER () and METEOR (), among other emerging MT evaluation metrics.
MT evaluation	METEOR	This was only possible because of the use of automatic metrics for MT evaluation, such as BLEU (), which is the defacto standard; and more recently: TER () and METEOR (), among other emerging MT evaluation metrics.
MT evaluation	similarity score	Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score.
classifying single word waveforms	accuracy	Therefore, classifying single word waveforms yields a low accuracy, peaking at 60%, which might lead to false negatives when looking for correspondences between neural network features and brain data.
hierarchical classification	subdivision factor (SF)	For hierarchical classification, there are additional parameters: subdivision factor (SF) and beam size (BM) ( §4), and hierarchy depth (D) ( §6.4).
hierarchical classification	beam size (BM)	For hierarchical classification, there are additional parameters: subdivision factor (SF) and beam size (BM) ( §4), and hierarchy depth (D) ( §6.4).
word-sense disambiguation (WSD)	precision	Finally, we feed the so-obtained candidate senses into standard word-sense disambiguation (WSD) methods, and boost their precision and recall.
word-sense disambiguation (WSD)	recall	Finally, we feed the so-obtained candidate senses into standard word-sense disambiguation (WSD) methods, and boost their precision and recall.
utterance-level  alignments	Ψ affinity	 Table 6: Ablation studies for utterance-level  alignments by removing Ψ affinity and Φ seg  from our model by replacing them with uni- form function.
CRF parser	F 1 scores	Our results show that: (i) The basic CRF parser, which uses semi-Markov CRF, or semi-CRF (), is already very accurate; it achieves F 1 scores over 83%, making any further improvement very hard.
SK	LSA	We compute the lexical similarity for SK by applying LSA ( to Tripadvisor data.
Automatic Speech Recognition (ASR)	Word Error Rates (WER)	Automatic Speech Recognition (ASR) systems frequently fail on noisy conditions and high Word Error Rates (WER) make the analysis of the automatic transcriptions difficult.
Theme classification	accuracy	 Table 1: Theme classification accuracy (%) with different c-vectors and GMM-UBM sizes.
MIR	Lasso	Our model outperforms previous MIR models and two strong linear models for rating prediction, namely SVR and Lasso by more than 10% relative in terms of MSE.
rating prediction	Lasso	Our model outperforms previous MIR models and two strong linear models for rating prediction, namely SVR and Lasso by more than 10% relative in terms of MSE.
sentiment classification	SEG	CG represents the candidate generation model, SC means the sentiment classification model and SEG stands for the segmentation ranking model.
SMT	BLEU	When simultaneously integrating the three models into SMT, we can gain a further improvement, which outperforms the baseline by up to 1.16 BLEU points.
classification	accuracy	We then apply a simple modification to their scoring algorithms which improves the classification accuracy of all five of them, three quite dramatically.
MERT training	BLEU	Poems in QVALID (with autogenerated references) were used for MERT training and Poems in QTEST (with auto-generated references) were used for BLEU evaluation.
summarize document clusters	ROUGE	Learners are trained to summarize document clusters based on various algorithms and reward functions and then evaluated using ROUGE.
Sentence compression	Imp	 Table 5: Sentence compression results: effect of  lexical features and expanded parse tree. ILP(I)  represents the system using only bottom nodes in  constituent parse tree. ILP(II) is our system. Imp  means the content importance value.
paraphrase detection (MSR)	accuracy	 Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
dialog act tagging (SWDA) tasks	accuracy	 Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
parsing	accuracy	This results in a fast, compact classifier, which uses only 200 learned dense features while yielding good gains in parsing accuracy and speed on two languages (English and Chinese) and two different dependency representations (CoNLL and Stanford dependencies).
parsing task	accuracy	The main contributions of this work are: (i) showing the usefulness of dense representations that are learned within the parsing task, (ii) developing a neural network architecture that gives good accuracy and speed, and (iii) introducing a novel acti-vation function for the neural network that better captures higher-order interaction features.
AZP resolution	F-score	Note that the task of AZP resolution alone is by no means easy: even when gold-standard AZPs are given, state-of-the-art supervised resolvers can only achieve an F-score of 47.7% for resolving Chinese AZPs.
ZP resolution	recall (R)	We express the results of ZP resolution in terms of recall (R), precision (P) and F-score (F).
ZP resolution	precision (P)	We express the results of ZP resolution in terms of recall (R), precision (P) and F-score (F).
ZP resolution	F-score (F)	We express the results of ZP resolution in terms of recall (R), precision (P) and F-score (F).
SF	recall	Recall for SF systems is especially low, with many systems using precise extractors with low recall.
POS tagging	accuracy	Supervised POS tagging has achieved great success, reaching as high as 95% accuracy for many languages).
POS tagging	accuracy	Fully unsupervised POS tagging is not yet useful in practice due to low accuracy (.
POS tagging	accuracy	Supervised POS tagging has achieved great success, reaching as high as 95% accuracy for many languages).
POS tagging	accuracy	Fully unsupervised POS tagging is not yet useful in practice due to low accuracy (.
code switching	accuracy	Again, we propose that syntactic constraints of the code switching phenomenon can help improve performance and model accuracy.
parsing	accuracy	We show that training with this oracle improves parsing accuracy over a conventional (static) oracle on a wide range of datasets.
POS tagging	accuracy	Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy .
parsing	accuracy	We hypothesise that such decisions are relatively rare, and are challenging for any parsing model, so a weak model is unlikely to result in substantially lower accuracy.
negation	SAD-NESS	For example, negation can toggle polarity ("don't love life" may suggest SAD-NESS, not JOY) and the aspectual context may indicate that no emotion is being expressed (e.g., "I would love life if ...").
negation	JOY	For example, negation can toggle polarity ("don't love life" may suggest SAD-NESS, not JOY) and the aspectual context may indicate that no emotion is being expressed (e.g., "I would love life if ...").
Emotion Classification	R =	 Table 4: Emotion Classification Results (P = Precision, R = Recall, F = F-score)
Emotion Classification	F =	 Table 4: Emotion Classification Results (P = Precision, R = Recall, F = F-score)
MT	HTER	Now we investigate re-tuning the MT system to the corrections by simply re-starting the online learning algorithm from the baseline weight vector w, this time scoring with HTER instead of BLEU.
MT	BLEU	Now we investigate re-tuning the MT system to the corrections by simply re-starting the online learning algorithm from the baseline weight vector w, this time scoring with HTER instead of BLEU.
translation	BLEU	derivations in the vast majority of the cases (100% with a 3-gram LM) and translation quality in terms of BLEU is no different from OS * . However, with k < 10 4 both model scores and translation quality can be improved.
Translation	BLEU	 Table 4: Translation quality in terms of BLEU as  a function of beam size in cube pruning with lan- guage models of order 3 to 5. The bottom row  shows BLEU for our exact decoder.
Translation	BLEU	 Table 4: Translation quality in terms of BLEU as  a function of beam size in cube pruning with lan- guage models of order 3 to 5. The bottom row  shows BLEU for our exact decoder.
PRO	BLEU	For our PRO experiments, we tuned three hyper-parameters controlling 2 regularization, sentence-level BLEU smoothing, and length.
PR	accuracy	The PR system achieved the highest average 1-1 accuracy in the shared task.
vocabulary prediction	accuracy	In our experiments, we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small.
mining stock price related news events	accuracy	Our results are helpful for automatically mining stock price related news events, and for improving the accuracy of algorithm trading systems.
noun classification	F 1	For noun classification, using coreferencebased features in addition to text-based features improves results on development set (F 1 is .78 vs .73) and test set (.69 vs .60).
dialect detection	accuracy	We show that accounting for such can improve dialect detection accuracy by nearly 10% absolute.
Dialect identification	accuracy	 Table 1:  Dialect identification accuracy using  various classification settings: only word-based  (WRD), character-based (CHAR), and both features.  BEST+LEX is built on the best feature of that system  plus a feature built on the concatenation of all lists
Dialect identification	BEST+LEX	 Table 1:  Dialect identification accuracy using  various classification settings: only word-based  (WRD), character-based (CHAR), and both features.  BEST+LEX is built on the best feature of that system  plus a feature built on the concatenation of all lists
speech recognition	CER	 Table 1. The speech recognition results (in CER  (%)) achieved by various language models along  with different numbers of latent topics/pseudo- relevance feedback documents.
machine translation task	BLEU	We also include a comparison to the log-bilinear neural language model and evaluate performance on a downstream machine translation task ( §6) where our method achieves consistent improvements in BLEU.
word analogy task	accuracy	 Table 2: Results on the word analogy task, given  as percent accuracy. Underlined scores are best  within groups of similarly-sized models; bold  scores are best overall. HPCA vectors are publicly  available 2 ; (i)vLBL results are from (Mnih et al.,  2013); skip-gram (SG) and CBOW results are  from (Mikolov et al., 2013a,b); we trained SG  †  and CBOW  † using the word2vec tool 3 . See text  for details and a description of the SVD models.
relation extraction	TRESCAL	Second, to validate its value to relation extraction, we apply TRESCAL to extracting relations from a free text corpus along with a knowledge base, using the data provided in ().
information extraction	precision	Traditional approaches to information extraction relies on supervised learning, yielding high  precision and recall results (.
information extraction	recall	Traditional approaches to information extraction relies on supervised learning, yielding high  precision and recall results (.
Triplet classification	accuracy	 Table 4: Triplet classification: accuracy (%) over various types of triplets.
sentiment analysis	accuracy	Researchers have recently started using the discourse structure of text in sentiment analysis and have shown its advantage in improving sentiment classification accuracy (e.g., ().
sentiment classification	accuracy	Researchers have recently started using the discourse structure of text in sentiment analysis and have shown its advantage in improving sentiment classification accuracy (e.g., ().
misspelling detection	coverage	We present a novel algorithm for misspelling detection, which runs inconstant time and improves the coverage to more than 96%.
misspelling detection	accuracy	We performed a set of experiments on different corpora in order to evaluate: (1) the performances of the PM algorithm for misspelling detection, (2) the accuracy of proper name misspelling pattern acquisition from large corpora, and (3) the improvements of a CDC system, employing a correction module for proper name misspellings.
machine translation (MT)	BLEU	The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (), METEOR (Banerjee and), TER (), GTM (.
machine translation (MT)	METEOR	The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (), METEOR (Banerjee and), TER (), GTM (.
machine translation (MT)	TER	The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (), METEOR (Banerjee and), TER (), GTM (.
WS	BLEU	Experimental results show that the refined WS reduced word alignment error rate by 6.82% and achieved the highest BLEU improvement (0.63 on average) on the Chinese-English open machine translation (OpenMT) corpora compared to related work.
word splitter	BLEU	The major contributions of this paper include, • analyze the CTB WS scheme for ChineseEnglish SMT; • propose a lexical word splitter to refine the WS; • achieve a BLEU improvement over a baseline Stanford word segmenter, and a state-of-theart extension, on Chinese-English OpenMT corpora.
WS	accuracy	This section contains experiments designed to empirically evaluate the proposed lexical word splitter in three aspects: first, whether the WS accuracy is improved; second, whether the accuracy of the unsupervised WA during training SMT systems is improved; third, whether the end-to-end translation quality is improved.
WS	precision	The performance of WS was measured by precision, recall and F 1 of gold words, The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)).
WS	recall	The performance of WS was measured by precision, recall and F 1 of gold words, The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)).
WS	F 1	The performance of WS was measured by precision, recall and F 1 of gold words, The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)).
WS	alignment error rate (AER))	The performance of WS was measured by precision, recall and F 1 of gold words, The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)).
SMT training	alignment error rate (AER))	The performance of WS was measured by precision, recall and F 1 of gold words, The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)).
SMT	BLEU	The performance of SMT was measured using BLEU ().
translation prediction	accuracy	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy.
word translation prediction	accuracy	We report significant improvements in word translation prediction accuracy for three morphologically rich target languages.
prediction	accuracy	We also show that these improvements in prediction accuracy can be beneficial in an end-to-end machine translation scenario by integrating into a large-scale EnglishRussian PSMT system.
stem prediction	accuracy	Conversely, the ConvNet performs poorly on stem prediction because it captures the meaning of the whole source context instead of emphasizing the importance of the source word s i as the main predictor of the target translation t j .  While the main objective of this paper is to improve prediction accuracy of word translations, see Section 5, we are also interested in knowing to which extent these improvements carryover within an end-to-end machine translation task.
SMT	BLEU	Integrating our bilingual neural network approach into our SMT system yields small but statistically significant improvements of 0.4 BLEU over a competitive baseline.
SMT	recall	Looking at the 1-best SMT output, we observe a slight increase of reference/output recall (50.0% to 50.7%), which is less than the increase we observe for the top 1 translation candidates (57.6% to 59.0%: Target word coverage analysis of the English-Russian SMT system before and after adding the morphological BNN models.
translation	precision	The inclusion of translation data was found to improve all of precision, recall and F-score across the board for all of the proposed methods.
translation	recall	The inclusion of translation data was found to improve all of precision, recall and F-score across the board for all of the proposed methods.
translation	F-score	The inclusion of translation data was found to improve all of precision, recall and F-score across the board for all of the proposed methods.
text mining	accuracy	Recent advances in text mining demonstrate that political ideology can be predicted from textoften with great accuracy.
hashtag prediction	TAGSPACE	We find representations trained on hashtag prediction outperform representations from unsupervised learning, and that our convolu-: #TAGSPACE convolutional network f (w, t) for scoring a (document, hashtag) pair.
parsing	accuracy	We also investigate the way in which the parsing accuracy helps to improve the ROUGE scores.
parsing	ROUGE	We also investigate the way in which the parsing accuracy helps to improve the ROUGE scores.
alignment	precision	We evaluated alignment quality using precision, recall, and F1.
alignment	recall	We evaluated alignment quality using precision, recall, and F1.
alignment	F1	We evaluated alignment quality using precision, recall, and F1.
translation	BLEU	Then, we evaluated the translation quality using BLEU () and ME-TEOR, and performed significance testing using bootstrap resampling) with 1,000 samples.
translation	ME-TEOR	Then, we evaluated the translation quality using BLEU () and ME-TEOR, and performed significance testing using bootstrap resampling) with 1,000 samples.
entity classification	precision	 Table 7: Results of entity classification and relation extraction on the data set using the 5-fold cross  validation (precision / recall / F1 score).
entity classification	F1 score	 Table 7: Results of entity classification and relation extraction on the data set using the 5-fold cross  validation (precision / recall / F1 score).
relation extraction	precision	 Table 7: Results of entity classification and relation extraction on the data set using the 5-fold cross  validation (precision / recall / F1 score).
relation extraction	F1 score	 Table 7: Results of entity classification and relation extraction on the data set using the 5-fold cross  validation (precision / recall / F1 score).
selectional preference learning	ORE	Many NLP and IR applications, including selectional preference learning, commonsense knowledge and entailment rule mining, have benefited from ORE (.
pattern extraction	precision	To assess the accuracy of pattern extraction, we rank the extracted patterns by the size, and evaluated the precision of the top 100 and a random set of 100 pattern synsets.
NEL argument identification	precision	• We demonstrate that NEL argument identification boosts both precision and recall, and using type constraints with linked arguments further boosts precision, yielding a 43% increase in precision and 27% boost to recall.
NEL argument identification	recall	• We demonstrate that NEL argument identification boosts both precision and recall, and using type constraints with linked arguments further boosts precision, yielding a 43% increase in precision and 27% boost to recall.
NEL argument identification	precision	• We demonstrate that NEL argument identification boosts both precision and recall, and using type constraints with linked arguments further boosts precision, yielding a 43% increase in precision and 27% boost to recall.
NEL argument identification	precision	• We demonstrate that NEL argument identification boosts both precision and recall, and using type constraints with linked arguments further boosts precision, yielding a 43% increase in precision and 27% boost to recall.
NEL argument identification	recall	• We demonstrate that NEL argument identification boosts both precision and recall, and using type constraints with linked arguments further boosts precision, yielding a 43% increase in precision and 27% boost to recall.
tense inference	accuracy	We show both approaches improve the tense inference accuracy over a baseline where these two sources of information are not used.
tense inference	accuracy	This shows improvements on tense inference accuracy as well in some of the experiment settings.
Parameter tuning	MERT	Parameter tuning is an important problem in statistical machine translation, but surprisingly , most existing methods such as MERT, MIRA and PRO are agnostic about search, while search errors could severely degrade translation quality.
Parameter tuning	MIRA	Parameter tuning is an important problem in statistical machine translation, but surprisingly , most existing methods such as MERT, MIRA and PRO are agnostic about search, while search errors could severely degrade translation quality.
translations	BLEU	We score translations using BLEU ().
EM	MERT	For EM, we ran parameter estimation with 5 randomly initialized starting points for 50 iterations; we tuned the MERT parameters with EM parameters obtained after 25 th iterations.
SD level classification	F- measures	 Table 6: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the macroaveraged value of G F 1 , M F 1  and H F 1 . SDTM outperforms all other methods  compared. The difference between SDTM and  FirstP is statistically significant (p-value < 0.05  for accuracy, < 0.0001 for Avg F 1 ).
SD level classification	Acc	 Table 6: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the macroaveraged value of G F 1 , M F 1  and H F 1 . SDTM outperforms all other methods  compared. The difference between SDTM and  FirstP is statistically significant (p-value < 0.05  for accuracy, < 0.0001 for Avg F 1 ).
SD level classification	accuracy	 Table 6: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the macroaveraged value of G F 1 , M F 1  and H F 1 . SDTM outperforms all other methods  compared. The difference between SDTM and  FirstP is statistically significant (p-value < 0.05  for accuracy, < 0.0001 for Avg F 1 ).
SD level classification	F-measure	 Table 6: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the macroaveraged value of G F 1 , M F 1  and H F 1 . SDTM outperforms all other methods  compared. The difference between SDTM and  FirstP is statistically significant (p-value < 0.05  for accuracy, < 0.0001 for Avg F 1 ).
SD level classification	Avg F 1	 Table 6: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the macroaveraged value of G F 1 , M F 1  and H F 1 . SDTM outperforms all other methods  compared. The difference between SDTM and  FirstP is statistically significant (p-value < 0.05  for accuracy, < 0.0001 for Avg F 1 ).
SD level classification	accuracy	 Table 6: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the macroaveraged value of G F 1 , M F 1  and H F 1 . SDTM outperforms all other methods  compared. The difference between SDTM and  FirstP is statistically significant (p-value < 0.05  for accuracy, < 0.0001 for Avg F 1 ).
discourse parsing output	Span	Standard evaluation of discourse parsing output computes the ratio of the number of identical tree constituents shared in the generated RS-trees and the gold-standard trees against the total number of constituents in the generated discourse trees 2 , which is further divided to three matrices: Span (on the blank tree structure), nuclearity (on the tree structure with nuclearity indication), and relation (on the tree structure with rhetorical relation indication but no nuclearity indication).
discourse parsing output	relation	Standard evaluation of discourse parsing output computes the ratio of the number of identical tree constituents shared in the generated RS-trees and the gold-standard trees against the total number of constituents in the generated discourse trees 2 , which is further divided to three matrices: Span (on the blank tree structure), nuclearity (on the tree structure with nuclearity indication), and relation (on the tree structure with rhetorical relation indication but no nuclearity indication).
coreference resolution error analysis	recall	We present a novel method for coreference resolution error analysis which we apply to perform a recall error analysis of four state-of-the-art English coreference resolution systems.
coreference resolution	precision	This is also evidenced by the CoNLL shared tasks on coreference resolution, where most competitive systems had higher precision than recall.
coreference resolution	recall	This is also evidenced by the CoNLL shared tasks on coreference resolution, where most competitive systems had higher precision than recall.
EDU recognition	accuracy	With various features as adopted in and, shows the performance of EDU recognition on the CDTB corpus with 10-fold cross validation., MaxEnt performs best, with accuracy up to 90.6% on gold standard parse tree, close to human agreement of 91.7%, and with accuracy up to 89% on automatic parse tree.
EDU recognition	accuracy	With various features as adopted in and, shows the performance of EDU recognition on the CDTB corpus with 10-fold cross validation., MaxEnt performs best, with accuracy up to 90.6% on gold standard parse tree, close to human agreement of 91.7%, and with accuracy up to 89% on automatic parse tree.
Simultaneous interpretation	speed	Simultaneous interpretation is challenging because it demands both quality and speed.
summarization tasks	ROUGE	Similar to traditional summarization tasks, we use the ROUGE metrics for automatic evaluation of all systems in comparison.
summarization evaluation	ROUGE	Automatic metrics of summarization evaluation have their origins in machine translation (MT), with ROUGE (, the first and still most widely used automatic summarization metric, comprising an adaption of the BLEU score ().
summarization evaluation	BLEU score	Automatic metrics of summarization evaluation have their origins in machine translation (MT), with ROUGE (, the first and still most widely used automatic summarization metric, comprising an adaption of the BLEU score ().
machine translation (MT)	ROUGE	Automatic metrics of summarization evaluation have their origins in machine translation (MT), with ROUGE (, the first and still most widely used automatic summarization metric, comprising an adaption of the BLEU score ().
machine translation (MT)	BLEU score	Automatic metrics of summarization evaluation have their origins in machine translation (MT), with ROUGE (, the first and still most widely used automatic summarization metric, comprising an adaption of the BLEU score ().
summarization	accuracy	Evaluation of summarization metrics has included, for example, the ability of a metric/significance test combination to distinguish between sets of human and system-generated summaries (), or by accuracy of conclusions drawn from metrics when combined with a particular significance test, Wilcoxon ranksum (.
summarization evaluation	precision-based	In addition, we include in the evaluation of metrics, an evaluation of BLEU for the purpose of summarization evaluation, and contrary to common belief, precision-based BLEU is on-par with recall-based ROUGE for evaluation of summarization systems.
summarization evaluation	recall-based ROUGE	In addition, we include in the evaluation of metrics, an evaluation of BLEU for the purpose of summarization evaluation, and contrary to common belief, precision-based BLEU is on-par with recall-based ROUGE for evaluation of summarization systems.
summaries	prepare4rouge 2	Moses () multi-bleu 1 was used to compute BLEU () scores for summaries and prepare4rouge 2 applied to summaries before running ROUGE (.
summaries	ROUGE	Moses () multi-bleu 1 was used to compute BLEU () scores for summaries and prepare4rouge 2 applied to summaries before running ROUGE (.
MT	ROUGE	Somewhat surprisingly, BLEU MT evaluation metric achieves strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU, to as low as r = 0.293.
MT	BLEU	Somewhat surprisingly, BLEU MT evaluation metric achieves strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU, to as low as r = 0.293.
PC removal	BLESS	For the PC removal the optimal number of removed PCs is 379 for TOEFL, 15 for BLESS and 128 for SimLex-999, while the optimal number for the Caron ptransform is -1.4 for TOEFL, 0.5 for BLESS and -0.40 for SimLex-999.
link prediction	FB15K	Data For link prediction, FB15K from ( ) is used as the knowledge base.
Distributional initialization	IECW D log V	Distributional initialization adds a term I, the average number of entries in the distributional vectors, so that time complexity increases to O(IECW D log V ).
Stemming Classification	accuracy	We construct classifiers for Biased Text Detection, Stemming Classification, Recognizing Textual Entailment, Twitter POS Tagging, and Affect Recognition, and evaluate the effect of our different training strategies on the accuracy of each task.
Path query answering	percentage reduction in error	 Table 2: Path query answering and knowledge base completion. We compare the performance of  single-edge training (SINGLE) vs compositional training (COMP). MQ: mean quantile, H@10: hits at 10,  %red: percentage reduction in error.
transfer from a single source language (English)	accuracy	In experiments on transfer from a single source language (English) to a single target language (German, French, Spanish, Italian, Portuguese, and Swedish), our average dependency accuracy is 78.89%.
parsing	accuracy	 Table 5: Influence result of parsing accuracy.
summarization	Input	This approach to summarization, which we call Attention-Based Summarization (ABS), incorporates less linguistic structure than comparable abstractive summarization approaches, but can easily Input (x1, . .
Attention-Based Summarization (ABS)	Input	This approach to summarization, which we call Attention-Based Summarization (ABS), incorporates less linguistic structure than comparable abstractive summarization approaches, but can easily Input (x1, . .
summarization	ROUGE	We show that our proposed method effectively improves over existing summarization approaches (greater than 30% improvement over the best performing baseline) in terms of ROUGE scores on TAC2014 scientific summarization dataset.
translation	GIZA++ 1.07	For estimating the translation probability without topical information, we use GIZA++ 1.07 to do it.
RE	precision	We adopted a previously proposed framework for the evaluation of large-scale RE systems by, to estimate precision and recall, using FreebaseEasy as the knowledge base.
RE	recall	We adopted a previously proposed framework for the evaluation of large-scale RE systems by, to estimate precision and recall, using FreebaseEasy as the knowledge base.
NER	accuracy	KB tags help NER accuracy across all entity types, but provide relatively better supervision for organisation entities than wide-coverage gazetteers.
NELL	accuracy	By using ENTICE, we are able to increase NELL's knowledge density by a factor of 7.7 at 75.5% accuracy.
NELL	accuracy	By using ENTICE, we are able to increase NELL's knowledge density by a factor of 7.7 1 , while achieving 75.4% accuracy.
Sample extraction	accuracy	Sample extraction examples and accuracy perextraction class are presented in, respectively.
relation mapping	accuracy	We observed that relation mapping has lesser accuracy due to two reasons.
NELL	accuracy	 Table 1.  By using ENTICE, we are able to increase  NELL's knowledge density by a factor of 7.7 1 ,  while achieving 75.4% accuracy. Our goal  here is to draw attention to the effectiveness  of entity-centric approaches with bigger scope  (i.e., covering all four extraction classes in
recognizing person names	errors	in given names, this makes recognizing person names challenging and contributes to 9% of our errors.
SVM	F1-F13	For SVM, the thirteen features F1-F13 proposed in Section 3 was used.
Disambiguation	precision	 Table 2: Disambiguation precision for all KBs
name-error detection	F-score	Experiments show a 26% improvement in name-error detection F-score over a system using n-gram lexical features.
character detection	F1	Second, we propose anew technique for character detection based on inducing character prototypes, and in comparisons with three state-of-the-art methods, demonstrate superior performance, achieving significant improvements in F1 over the next-best method.
sentiment analysis	accuracy	Experiments on six standard datasets for sentiment analysis, subjectivity detection and topic spotting showed statistically significant higher accuracy for our proposed kernel over the bigram approaches.
topic spotting	accuracy	Experiments on six standard datasets for sentiment analysis, subjectivity detection and topic spotting showed statistically significant higher accuracy for our proposed kernel over the bigram approaches.
classification	accuracy	Moreover, tuning the hyperparameters to maximize the classification accuracy needs to be carried out on a validation set and therefore requires additional labeled data.
classification	accuracy	When the assumption does not hold, the classification accuracy is worse.
NET	recall	A key challenge in NET is to extract such implicit, context-aware types to improve recall.
NER	JERL	2. We propose the first completely joint NER and linking model, JERL, to train and inference the two tasks together.
NER	precision	We follow the CoNLL'03 metrics to evaluate NER performance by precision, recall, and F 1 scores, and follow Hoffart's (2011) experiment setting to evaluate linking performance by micro precision@1.
NER	recall	We follow the CoNLL'03 metrics to evaluate NER performance by precision, recall, and F 1 scores, and follow Hoffart's (2011) experiment setting to evaluate linking performance by micro precision@1.
NER	F 1 scores	We follow the CoNLL'03 metrics to evaluate NER performance by precision, recall, and F 1 scores, and follow Hoffart's (2011) experiment setting to evaluate linking performance by micro precision@1.
translation	BLEU-4	The translation performance index is the IBM version of BLEU-4 ().
SMT	I- WSLT	 Table 3: BLEU scores of SMT systems. The I- WSLT is a public baseline which issued by the or- ganizer of IWSLT 2014, as described in (Cettolo  et al., 2012).
machine translation decoder	BLEU point	In addition, we show that incorporating these models into a machine translation decoder still results in large BLEU point improvements.
translate a clue	accuracy	Our system, LO-GICIA, learns to automatically translate a clue with 81.11% accuracy and is able to solve 71% of the problems of a corpus.
SMT	BLEU	In any case, NN training is typically performed (a) in isolation from the other components of the SMT system and (b) using a criterion that is unrelated to the actual performance of the SMT system (as measured for instance by BLEU).
SMT	BLEU	In any case, NN training is typically performed (a) in isolation from the other components of the SMT system and (b) using a criterion that is unrelated to the actual performance of the SMT system (as measured for instance by BLEU).
MT	Bleu  score	 Table 5. The Bleu score of each MT system, the Bleu  score of paraphrasing each MT system using lattice decod- ing and the Bleu score of paraphrasing each MT system  using paraphrasing model.
MT	Bleu score	 Table 5. The Bleu score of each MT system, the Bleu  score of paraphrasing each MT system using lattice decod- ing and the Bleu score of paraphrasing each MT system  using paraphrasing model.
MT system-independent	METEOR	These new features, along with standard MT system-independent features, are benchmarked on a series of datasets with various quality labels, including post-editing effort, human translation edit rate, post-editing time and METEOR.
translation	BLEU	We measured the overall translation quality with 4-gram BLEU (), which was computed on tokenized and lowercased data for all systems.
puzzle solving task	Match	 Table 4: Results for the puzzle solving task. "Match" shows  the percentage of predicted action sequences that exactly  match the annotation. "Success" shows the percentage of  predicted action sequences that result in a winning game con- figuration, regardless of the action sequence performed. Fol- lowing Branavan et al. (2009), we average across five random  train / test folds. Our model achieves state-of-the-art results  on this task.
word segmentation	accuracy	We conducted word segmentation experiments on tweets from Twitter, and showed that our method improves accuracy in this domain.
word segmentation (WS)	accuracy	This task is called word segmentation (WS) and the accuracy of state-of-the-art methods based on machine learning techniques is more than 98% for Japanese and 95% for Chinese).
ma-chine translation of patents	accuracy	Examples include ma-chine translation of patents, text mining of medical texts, and marketing on the micro-blog site Twitter . Some papers have reported low accuracy on WS or the joint task of WS and part-of-speech (POS) tagging of Japanese or Chinese in these domains To cope with this problem, we propose away to collect information from people as they type Japanese or Chinese on computers.
text mining of medical texts	accuracy	Examples include ma-chine translation of patents, text mining of medical texts, and marketing on the micro-blog site Twitter . Some papers have reported low accuracy on WS or the joint task of WS and part-of-speech (POS) tagging of Japanese or Chinese in these domains To cope with this problem, we propose away to collect information from people as they type Japanese or Chinese on computers.
WS	accuracy	 Table 4: WS accuracy on the tweets.  Recall [%] Precision [%] F-measure  Baseline  90.31  94.05  92.14  + Log-as-is  90.33  93.77  92.02  + Log-chunk  91.04  94.29  92.64  + Log-mconv  90.62  94.09  92.32  + Log-chunk-mconv  91.40  94.45  92.90
WS	Recall	 Table 4: WS accuracy on the tweets.  Recall [%] Precision [%] F-measure  Baseline  90.31  94.05  92.14  + Log-as-is  90.33  93.77  92.02  + Log-chunk  91.04  94.29  92.64  + Log-mconv  90.62  94.09  92.32  + Log-chunk-mconv  91.40  94.45  92.90
WS	Precision	 Table 4: WS accuracy on the tweets.  Recall [%] Precision [%] F-measure  Baseline  90.31  94.05  92.14  + Log-as-is  90.33  93.77  92.02  + Log-chunk  91.04  94.29  92.64  + Log-mconv  90.62  94.09  92.32  + Log-chunk-mconv  91.40  94.45  92.90
WS	F-measure  Baseline	 Table 4: WS accuracy on the tweets.  Recall [%] Precision [%] F-measure  Baseline  90.31  94.05  92.14  + Log-as-is  90.33  93.77  92.02  + Log-chunk  91.04  94.29  92.64  + Log-mconv  90.62  94.09  92.32  + Log-chunk-mconv  91.40  94.45  92.90
WS	accuracy	 Table 5: WS accuracy on BCCWJ.  Recall [%] Precision [%] F-measure  Baseline  99.01  98.97  98.99  + Log-as-is  99.02  98.87  98.94  + Log-chunk  99.05  98.88  98.96  + Log-mconv  98.98  98.91  98.95  + Log-chunk-mconv  98.98  98.92  98.95
WS	Recall	 Table 5: WS accuracy on BCCWJ.  Recall [%] Precision [%] F-measure  Baseline  99.01  98.97  98.99  + Log-as-is  99.02  98.87  98.94  + Log-chunk  99.05  98.88  98.96  + Log-mconv  98.98  98.91  98.95  + Log-chunk-mconv  98.98  98.92  98.95
WS	Precision	 Table 5: WS accuracy on BCCWJ.  Recall [%] Precision [%] F-measure  Baseline  99.01  98.97  98.99  + Log-as-is  99.02  98.87  98.94  + Log-chunk  99.05  98.88  98.96  + Log-mconv  98.98  98.91  98.95  + Log-chunk-mconv  98.98  98.92  98.95
WS	F-measure  Baseline  99.01  98.97	 Table 5: WS accuracy on BCCWJ.  Recall [%] Precision [%] F-measure  Baseline  99.01  98.97  98.99  + Log-as-is  99.02  98.87  98.94  + Log-chunk  99.05  98.88  98.96  + Log-mconv  98.98  98.91  98.95  + Log-chunk-mconv  98.98  98.92  98.95
word alignment search	accuracy	As conventional word alignment search algorithms usually ignore the consistency constraint in translation rule extraction, improving alignment accuracy does not necessarily increase translation quality.
translation rule extraction	accuracy	As conventional word alignment search algorithms usually ignore the consistency constraint in translation rule extraction, improving alignment accuracy does not necessarily increase translation quality.
word alignment	accuracy	However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality.
translation rule extraction	accuracy	However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality.
alignment search	coverage	We introduce anew alignment search algorithm with an objective that maximizes both alignment model score and coverage while keeping the training algorithm unchanged.
alignment	alignment error rate (AER)	The evaluation metrics for alignment and translation are alignment error rate (AER) and case-insensitive BLEU (), respectively.
alignment	BLEU	The evaluation metrics for alignment and translation are alignment error rate (AER) and case-insensitive BLEU (), respectively.
translation	alignment error rate (AER)	The evaluation metrics for alignment and translation are alignment error rate (AER) and case-insensitive BLEU (), respectively.
translation	BLEU	The evaluation metrics for alignment and translation are alignment error rate (AER) and case-insensitive BLEU (), respectively.
MT	BLEU	 Table 6. The  MML-based baseline systems (B mml ) used 20%  selected data for training the MT system and the  NNJM. On Arabic-English, both MML-based se- lection and our model (S v1 ) gave similar gains on  top of the baseline system (B cat ). Further results  showed that both approaches are complementary.  We were able to obtain an average gain of +0.3  BLEU points by training an NDAM v1 model over  the selected data (see S v1+mml ).  However, on English-German, the MML-based  selection caused a drop in the performance (see
SMT	MERT	Efforts in parameter tuning algorithms for SMT, such as MERT), MIRA (, and PRO () have improved the translation quality considerably in the past decade.
SMT	MIRA	Efforts in parameter tuning algorithms for SMT, such as MERT), MIRA (, and PRO () have improved the translation quality considerably in the past decade.
MT	BLEU-like	One key advantage of search-aware tuning for previous phrase-based MT is the minimal change to existing parameter tuning algorithms, which is achieved by defining BLEU-like metrics for the intermediate decoding states with sequencestructured derivations.
tagging	accuracy	Ratnaparkhi's method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state-of-the-art tagging model.
parsing	accuracy	Other work used richer label features but increased systems' complexities significantly, while achieving better parsing accuracy.
parsing	accuracy	Yet, there are no previous work addressing the problem of good balance between parsing accuracy and computational costs for joint parsing models.
tagging	accuracy	1 Except for some minor variations in a few cases, both using more labeled data and using more unlabeled data improves tagging accuracy for both ONLINE and BATCH.
tagging	ONLINE	1 Except for some minor variations in a few cases, both using more labeled data and using more unlabeled data improves tagging accuracy for both ONLINE and BATCH.
tagging	†	 Table 3: Error rates (err) and standard deviations (std) for tagging.  † (resp.  * ): significantly different from ONLINE error rate  above&below (resp. from "u:0" error rate to the left).
tagging	ONLINE error rate	 Table 3: Error rates (err) and standard deviations (std) for tagging.  † (resp.  * ): significantly different from ONLINE error rate  above&below (resp. from "u:0" error rate to the left).
dependency parsing	accuracy	We present a semi-supervised approach to improve dependency parsing accuracy by using bilexical statistics derived from auto-parsed data.
dependency parsing	accuracy	We are concerned with semi-supervised dependency parsing, namely how to leverage large amounts of unannotated data, in addition to annotated Treebank data, to improve dependency parsing accuracy.
dependency parsing	accuracy	We are concerned with semi-supervised dependency parsing, namely how to leverage large amounts of unannotated data, in addition to annotated Treebank data, to improve dependency parsing accuracy.
POS tagging	accuracy	 Table 2: POS tagging results on the CoNLL '09 test set for in- tegrated POS tagging and parsing. We compare the accuracy  of our baseline CRF tagger, 'Linear' (our re-implementation  of Bohnet and Nivre (2012, BN'12)), 'Neural' (the neural  parser presented in this work), and results reported by BN'12.
parsing	accuracy	We demonstrate that applying constituency model combination techniques to n-best lists instead of n different parsers results in significant parsing accuracy improvements.
SMT pipeline	BLEU	We use CDEC 1 as an endto-end SMT pipeline with its standard features 2 . fast align) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA.
SMT pipeline	MIRA	We use CDEC 1 as an endto-end SMT pipeline with its standard features 2 . fast align) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA.
word alignment	BLEU	We use CDEC 1 as an endto-end SMT pipeline with its standard features 2 . fast align) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA.
word alignment	MIRA	We use CDEC 1 as an endto-end SMT pipeline with its standard features 2 . fast align) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA.
Translation	BLEU	Translation performances are reported in case-sensitive BLEU () on newstest2014 (2737 sentences) and newstest2015 (2169 sentences).
translation	BLEU	Following (, we report translation quality using two types of BLEU: (a) tokenized 12 BLEU to be comparable with existing NMT work and (b) NIST 13 BLEU to be comparable with WMT results.
translation	BLEU	Following (, we report translation quality using two types of BLEU: (a) tokenized 12 BLEU to be comparable with existing NMT work and (b) NIST 13 BLEU to be comparable with WMT results.
Sentiment classification	accuracy	 Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
Sentiment classification	MSE	 Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
Sentiment classification	accu- racy	 Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
Sentiment classification	MSE	 Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
paraphrase detection task	accuracy	 Table 3: Results with different error functions for  the paraphrase detection task (accuracy × 100).
paraphrase detection task	accuracy	 Table 4: Different disambiguation choices for the  paraphrase detection task (accuracy × 100).
SICK task	Pearson's r	Our results on the SICK task are summarized in, showing Pearson's r, Spearman's ρ, and mean squared error (MSE).
SICK task	mean squared error (MSE)	Our results on the SICK task are summarized in, showing Pearson's r, Spearman's ρ, and mean squared error (MSE).
SICK	Pearson's r	 Table 2: Test set results on SICK, as reported  by Tai et al. (2015), grouped as: (1) RNN vari- ants; (2) SemEval 2014 systems; (3) sequential  LSTM variants; (4) dependency and constituency  tree LSTMs (Tai et al., 2015). Evaluation metrics  are Pearson's r, Spearman's ρ, and mean squared  error (MSE).
SICK	mean squared  error (MSE)	 Table 2: Test set results on SICK, as reported  by Tai et al. (2015), grouped as: (1) RNN vari- ants; (2) SemEval 2014 systems; (3) sequential  LSTM variants; (4) dependency and constituency  tree LSTMs (Tai et al., 2015). Evaluation metrics  are Pearson's r, Spearman's ρ, and mean squared  error (MSE).
SemEval 2010 relation classification task	F 1-score	We test our model on the SemEval 2010 relation classification task, and achieve an F 1-score of 83.7%, higher than competing methods in the literature.
SemEval 2010 relation classification task	F 1 -score	We evaluate our proposed method on the SemEval 2010 relation classification task, and achieve an F 1 -score of 83.7%, higher than competing methods in the literature.
FFM	Agreeableness	The FFM defines personality along five bipolar scales: Extraversion (sociable vs. reserved), Emotional stability (secure vs. neurotic), Agreeableness (friendly vs. unsympathic), Conscientiousness (organized vs. careless) and Openness to experience (insightful vs. unimaginative).
alignment	accuracy	They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly.
translation	accuracy	This significantly limits the translation accuracy, especially for distantly-related language pairs such as Chinese-English (see Section 5).
word segmentation	error reduction	Experimental results show that our joint model can help improve the performance of word segmentation on microblogs, giving an error reduction in segmentation accuracy of 12.02%, compared to the traditional approach .
word segmentation	accuracy	Experimental results show that our joint model can help improve the performance of word segmentation on microblogs, giving an error reduction in segmentation accuracy of 12.02%, compared to the traditional approach .
parsing	accuracy	More specifically, we show that an efficient lexicalized phrase structure parser -modelling both dependencies and morphology -already significantly improves parsing accuracy.
summarization	maximal coverage	The problem of summarization is reduced to the maximal coverage, following the assumption that a summary that best describes the original text, should cover most of the word sequences describing the document.
storyline extraction	Precision	The evaluation results of our proposed approach in comparison to the three baselines are presented in: Performance comparison of the storyline extraction results in terms of Precision (%), Recall (%) and F-score (%).
storyline extraction	Recall	The evaluation results of our proposed approach in comparison to the three baselines are presented in: Performance comparison of the storyline extraction results in terms of Precision (%), Recall (%) and F-score (%).
storyline extraction	F-score	The evaluation results of our proposed approach in comparison to the three baselines are presented in: Performance comparison of the storyline extraction results in terms of Precision (%), Recall (%) and F-score (%).
storyline  extraction	Precision	 Table 1: Performance comparison of the storyline  extraction results in terms of Precision (%), Recall  (%) and F-score (%).
storyline  extraction	Recall	 Table 1: Performance comparison of the storyline  extraction results in terms of Precision (%), Recall  (%) and F-score (%).
storyline  extraction	F-score	 Table 1: Performance comparison of the storyline  extraction results in terms of Precision (%), Recall  (%) and F-score (%).
MMR	precision	Lastly, the proposed method outperforms the MMR based baselines on the precision and Fmeasure of all three ROUGE scores.
MMR	Fmeasure	Lastly, the proposed method outperforms the MMR based baselines on the precision and Fmeasure of all three ROUGE scores.
MMR	ROUGE	Lastly, the proposed method outperforms the MMR based baselines on the precision and Fmeasure of all three ROUGE scores.
machine translation	ROUGE-N	ROUGE-N is similar to the BLEU metric for machine translation, but ROUGE-N is a recall-based metric while BLEU is a precision-based metric.
machine translation	recall-based	ROUGE-N is similar to the BLEU metric for machine translation, but ROUGE-N is a recall-based metric while BLEU is a precision-based metric.
machine translation	BLEU	ROUGE-N is similar to the BLEU metric for machine translation, but ROUGE-N is a recall-based metric while BLEU is a precision-based metric.
machine translation	precision-based	ROUGE-N is similar to the BLEU metric for machine translation, but ROUGE-N is a recall-based metric while BLEU is a precision-based metric.
answer triggering	precision	We propose to evaluate answer triggering using question-level precision, recall and F 1 scores.
answer triggering	recall	We propose to evaluate answer triggering using question-level precision, recall and F 1 scores.
answer triggering	F 1 scores	We propose to evaluate answer triggering using question-level precision, recall and F 1 scores.
answer triggering	precision	 Table 5: Evaluation of answer triggering on the  WIKIQA dataset. Question-level precision, recall  and F 1 scores are reported.
answer triggering	recall	 Table 5: Evaluation of answer triggering on the  WIKIQA dataset. Question-level precision, recall  and F 1 scores are reported.
answer triggering	F 1 scores	 Table 5: Evaluation of answer triggering on the  WIKIQA dataset. Question-level precision, recall  and F 1 scores are reported.
TOEFL synonym selection	accuracy	 Table 2: TOEFL synonym selection and docu- ment classification accuracy (percentage of cor- rectly answered questions/correctly classified doc- uments).
classification	accuracy	We report the average classification accuracy across the four tasks.
sentiment analysis task (Senti	accuracy	Our next downstream semantic task is the sentiment analysis task (Senti)) which is a binary classification task between positive and negative movie reviews using the standard training/dev/test split and report accuracy on the test set.
Frame semantic parsing	precision	 Table 2: Frame semantic parsing results (precision and F 1 in %)
Frame semantic parsing	F 1	 Table 2: Frame semantic parsing results (precision and F 1 in %)
event coreference resolution	MUC	With respect to event coreference resolution, the scorer computes MUC (
WMT English-French translation	BLEU	Compared to a baseline that replaces all rare words with an unknown word symbol, our best variable-length encoding strategy improves WMT English-French translation performance by up to 1.7 BLEU.
EnglishFrench news translation	BLEU	Nonetheless, compared to a baseline system that replaces all rare words with an unknown word symbol, our encoding approach improves EnglishFrench news translation by up to 1.7 BLEU.
dialect classification	accuracy	We propose several methods to improve the dialect classification accuracy by training models with distant supervision: a weakly supervised model is trained with data whose labels are automati-cally assigned based on authors' geographical information.
classification	accuracy	More importantly, semi-supervised learning on large amount of unlabeled data effectively increases the classification accuracy.
information extraction	precision	Actually, in some of the potential applications of zero anaphora resolution, such as information extraction, methods with high precision and low recall are preferable to ones with low precision and high recall.
information extraction	recall	Actually, in some of the potential applications of zero anaphora resolution, such as information extraction, methods with high precision and low recall are preferable to ones with low precision and high recall.
information extraction	precision	Actually, in some of the potential applications of zero anaphora resolution, such as information extraction, methods with high precision and low recall are preferable to ones with low precision and high recall.
information extraction	recall	Actually, in some of the potential applications of zero anaphora resolution, such as information extraction, methods with high precision and low recall are preferable to ones with low precision and high recall.
Morphological Tagging	LEMMING	Joint Lemmatization and Morphological Tagging with LEMMING
Translation	BLEU	 Table 1: Translation scores in lower-case BLEU.
statistical machine translation (SMT)	accuracy	In this paper, we describe a method based on statistical machine translation (SMT) that is able to restore accents in Hungarian texts with high accuracy.
SMT	BL-SMT	The experiments were then performed for the baseline system using the most frequent form (BL-FREQ), for the baseline SMT system (BL-SMT) and for the one augmented by the morphology with the first-ranked candidate (RANK).
SMT	accuracy	Comparing our results to those we obtained using Charlifter (89.75% with most frequent accented form baseline, 90.00% with the lexiconlookup+bigram contextual model and 93.31% with lookup+bigram context+character-n-grambased model), the results reveal that both the contextual model in the SMT system improves accuracy better than the bigram context model of Charlifter, and the performance boost we get by incorporating morphology vastly exceeds the accuracy improvement yielded by the incorporation of the character-n-gram-based model used in Charlifter.
SMT	accuracy	Comparing our results to those we obtained using Charlifter (89.75% with most frequent accented form baseline, 90.00% with the lexiconlookup+bigram contextual model and 93.31% with lookup+bigram context+character-n-grambased model), the results reveal that both the contextual model in the SMT system improves accuracy better than the bigram context model of Charlifter, and the performance boost we get by incorporating morphology vastly exceeds the accuracy improvement yielded by the incorporation of the character-n-gram-based model used in Charlifter.
Morphological analysis	accuracy	Morphological analysis including word segmentation has been widely and actively studied, and for example, Japanese word segmentation accuracy is in the high 90s.
WS	accuracy	 Table 3: WS accuracy on Shogi commentaries.  Recall Prec. F-meas.  Baseline  90.12 91.43  90.77  + Sym.Gro. 90.60 91.66  91.13
identify deception	accuracy	Our experiments show that the multimodal system can identify deception with an accuracy in the range of 77-82%, significantly improving over the baseline.
text classification	accuracy	Thus, text classification is needed to make more sophisticated decisions to improve accuracy.
humor recognition classifier	accuracy	The above humor recognition classifier provides us with decent accuracy in identifying humor in the text.
SP	similarity score	This is because in the SP method, the similarity score of a pair of entities depends on not only their current status but also on the status of the other pairs.
analogy mining	precision	We propose a bootstrapping algorithm for analogy mining using dependency embeddings, and experiments on a large Chinese corpus show that the method can achieve a precision of 95.2% at a recall of 56.8%.
analogy mining	recall	We propose a bootstrapping algorithm for analogy mining using dependency embeddings, and experiments on a large Chinese corpus show that the method can achieve a precision of 95.2% at a recall of 56.8%.
sentiment labeling	Macroaveraged Mean Absolute Error (MAE M	For the sentiment labeling part we used simple Macroaveraged Mean Absolute Error (MAE M , not the symmetric version) as the true dataset labels are the reference ones in this case, while in the annotator agreement case the two sets of labels have equal importance.
classification	accuracy	While the visual and the audio modalities provide additional evidence that improves classification accuracy, we found the textual modality to have the greater impact on the result.
SLSA	ArSenL	We then conduct an intrinsic evaluation of SLSA where the performance is compared to that of ArSenL, which is the most similar state-of-theart lexicon (see Section 2).
SLSA	Obj	 Table 2: Examples of SLSA entries; Obj. = Objective. All  scores are rounded for readability.
SLSA	Objective	 Table 2: Examples of SLSA entries; Obj. = Objective. All  scores are rounded for readability.
topic discovery task	theta 0	While in topic discovery task, Apriori's minimum support and TPD's theta 0 is around 0.020 and 2.0 respectively.
FSD	O	The most accurate approaches to FSD involve a runtime of O(n 2 ) and cannot scale to unbounded high volume streams such as Twitter.
FSD	O(1)	We present a novel approach to FSD that operates in O(1) per tweet.
FSD	novelty score	The traditional approach to FSD ( computes the distance of each incoming document 1 e.g. a natural disaster or a scandal TDT by. http://www.itl.nist.gov/ iad/mig/tests/tdt/resources.html (Last Update: 2008) 5,700 tweets per second https://about.twitter .com/company (last updated: to all previously seen documents and the minimum distance determines the novelty score.
speech recognition	error rate	We now have speech recognition systems that achieve better than half that error rate for two languages, English and Czech.
machine translation evaluation	BLEU	• BLEU (bilingual evaluation understudy):) have proposed a method of automatic machine translation evaluation, which they call "BLEU".
utterance unit evaluation	BLEU	Consequently, for the utterance unit evaluation, we conducted several experiments while varying N from 1 to 4 for BLEU and from 1 to 5 for NIST.
split construction	TCR	In the case of split construction, we used the extracted corpus after filtering based on the group maximum, and phrases that were TCR > 0.8 were judged to be literal phrases.
learning phrase translations from parallel corpora	coverage	We develop anew approach to learning phrase translations from parallel corpora , and show that it performs with very high coverage and accuracy in choosing French translations of English named-entity phrases in a test corpus of software manuals.
learning phrase translations from parallel corpora	accuracy	We develop anew approach to learning phrase translations from parallel corpora , and show that it performs with very high coverage and accuracy in choosing French translations of English named-entity phrases in a test corpus of software manuals.
learning phrase translations from parallel corpora	coverage	In this paper, we develop anew approach to learning phrase translations from parallel corpora, and show that it performs with very high coverage and accuracy on a named-entity phrase translation task.
learning phrase translations from parallel corpora	accuracy	In this paper, we develop anew approach to learning phrase translations from parallel corpora, and show that it performs with very high coverage and accuracy on a named-entity phrase translation task.
MUC	ALEMBIC	Some MUC systems rely on data-driven approaches, such as Nymble () which uses Hidden Markov Models, or ALEMBIC (, based on Error Driven Transformation Based Learning.
MT translations	precision	(1) Average of Ranks2 : (2) BLEU score: The MT translations are scored based on the precision of N-grams in an entire set of multiple reference translations ().
noun attachment	IB 1	On noun attachment, IB 1 attains an F-score of 82; RIPPER an F-score of 78.
noun attachment	F-score	On noun attachment, IB 1 attains an F-score of 82; RIPPER an F-score of 78.
noun attachment	RIPPER	On noun attachment, IB 1 attains an F-score of 82; RIPPER an F-score of 78.
noun attachment	F-score	On noun attachment, IB 1 attains an F-score of 82; RIPPER an F-score of 78.
parsing	accuracy	Using a weak information fusion scheme based on constraint optimization techniques, a parsing accuracy has been achieved which is comparable to other (stochastic) parsers.
DT	accuracy	For DT, the best accuracy (69.1%) is obtained with windows of three or four words to the left of the candidate noun-noun sequence (see / = 4 and 1 = 3 in).
MT	BLEU	Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.
acronym acquisition	recall	 Table 2. The precision of  the acronym acquisition is around 98% at 74%  recall, and the ATR precision improved in  average by 2% (resulting in 98% for the top  ranked terms) by adding term variation  recognition.
acronym acquisition	ATR	 Table 2. The precision of  the acronym acquisition is around 98% at 74%  recall, and the ATR precision improved in  average by 2% (resulting in 98% for the top  ranked terms) by adding term variation  recognition.
noun attachments	recall	 Table 1.  As can be seen, the 4-way classification is a little  worse for noun attachments (due to worse recall  of noun adjuncts) but better for the verb attach- ments. The distribution of the errors reveals that  improvements can be observed in distinguishing  noun arguments and verb arguments -clearly an  instance in which the actual attachment site does  not subsume the assigned attachment. This result  confirms that it is misleading to formalise PP at- tachment as a binary problem, assuming that a dis- tinction between arguments and adjuncts can be  performed later, if necessary, without loss of accu- racy.
candidate extraction step	accuracy	This methodology is of little use for the evaluation of the candidate extraction step, though, for several reasons: • The accuracy of the extraction step influences the final results in two quite different ways: (a) by changing the set of candidate types; (b) by changing their frequency profiles.
parsing	speed	Another issue in parsing is speed, which can only be gained by deterministic processing.
translation	BLEU score	On all tasks and conditions, we achieved significant improvements in translation quality, increasing e. g. the BLEU score by as much as 15% relative.
Multi-source translation	WER	 Table 5: Multi-source translation: improvements  in translation quality when computing consen- sus translation using the output of two Chinese- English and two Japanese-English systems on the  IWSLT04 task.  BTEC Chinese-English WER PER BLEU  + Japanese-English  [%] [%]  [%]  worst single system  58.0 41.8 39.5  best single system  51.3 38.6 44.7  consensus of 4 systems 44.9 33.9 49.6
translations of unknown words	BLEU score	Since the BLEU score alone is often not a good indicator of successful translations of unknown words (the unigram or bigram precision maybe increased but may not have a strong effect on the overall BLEU score), position-independent word error rate (PER) rate was measured as well.
translations of unknown words	position-independent word error rate (PER) rate	Since the BLEU score alone is often not a good indicator of successful translations of unknown words (the unigram or bigram precision maybe increased but may not have a strong effect on the overall BLEU score), position-independent word error rate (PER) rate was measured as well.
classification	accuracy	The overall classification accuracy was 75.1%.
parsing	accuracy	The parsing experiments are performed with a variant of Covington's algorithm for dependency parsing), using the treebank as an oracle in order to establish an upper bound on accuracy.
dependency parsing	accuracy	The parsing experiments are performed with a variant of Covington's algorithm for dependency parsing), using the treebank as an oracle in order to establish an upper bound on accuracy.
SST	accuracy	On the other hand, if the SST space contains too many irrelevant features, overfitting may occur and decrease the classification accuracy.
classification	accuracy	On the other hand, if the SST space contains too many irrelevant features, overfitting may occur and decrease the classification accuracy.
predicate argument classification	accuracy	On the other hand, we study the impact of the different tree based kernels on the predicate argument classification accuracy.
classification of semantic orientations of phrases	accuracy	Through experiments , we show that the proposed latent variable models work well in the classification of semantic orientations of phrases and achieved nearly 82% classification accuracy .
Tagging	accuracy	 Table 2: Tagging accuracy results for non-punctu- ation tokens and phrases for two dictionaries
tagging	accuracy	 Table 3: Average tagging accuracy results with standard deviation for ten runs using different eight pages  for training, and six pages for testing
tagging	standard	 Table 3: Average tagging accuracy results with standard deviation for ten runs using different eight pages  for training, and six pages for testing
POS disambiguation task	error rate	After more thoroughly investigating how problematic tagging distinctions affect the POS disambiguation task, in section 5 we modify the tagging model in order to better account for these distinctions, and we show this to significantly reduce the error rate of a corpus.
LM	false alarm rate	LM tends to over-predict on the top-level, resulting in a higher false alarm rate.
Recognizing Textual Parallelisms	similarity	Recognizing Textual Parallelisms with edit distance and similarity degree
MT	BLEU	While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments (), we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated with human judgments; correlation studies have been made of individual components (), but not of systems.
labelling task	F	shows that for the labelling task, our model outperforms the labelling baseline and the SVM labeller on the FrameNet data by at least 16 points F score while the correlation with human data remains significant.
Sentence transduction	precision	 Table 1: Sentence transduction results on DEV (la- beled precision/recall/F-measure)
Sentence transduction	F-measure	 Table 1: Sentence transduction results on DEV (la- beled precision/recall/F-measure)
Sentence transduction	TEST	 Table 2: Sentence transduction results on TEST  (labeled F-measure)
Sentence transduction	F-measure	 Table 2: Sentence transduction results on TEST  (labeled F-measure)
Grammar transduction	precision	 Table 5: Grammar transduction results on  development corpus (labeled precision/recall/F- measure)
Grammar transduction	F- measure	 Table 5: Grammar transduction results on  development corpus (labeled precision/recall/F- measure)
Grammar transduction	TEST	 Table 6: Grammar transduction results on TEST  (labeled F-measure)
Sample queries	accuracy	 Table 2: Sample queries and accuracy values
SMT translations	word error rate (WER)	Our technique is similar to that of (Munteanu and Marcu, 2005) but we bypass the need of the bilingual dictionary by using proper SMT translations and instead of a maximum entropy classifier we use simple measures like the word error rate (WER) and the translation error rate (TER) to decide whether sentences are parallel or not.
SMT translations	translation error rate (TER)	Our technique is similar to that of (Munteanu and Marcu, 2005) but we bypass the need of the bilingual dictionary by using proper SMT translations and instead of a maximum entropy classifier we use simple measures like the word error rate (WER) and the translation error rate (TER) to decide whether sentences are parallel or not.
classification of subjective phrases	accuracy	We show that classification of subjective phrases using our approach yields better accuracy than two baselines, a majority class baseline and a more difficult baseline of lexical n-gram features.
WSD	error reduction	In this paper we show for the first time that our WSD system trained on a general source corpus (BNC) and the target corpus, obtains up to 22% error reduction when compared to a system trained on the target corpus alone.
Translation	BLEU	 Table 1: Translation Results for the News Domain  in terms of the BLEU Metric.
Translation	BLEU	 Table 4: Translation Results on segmentd UN data  in terms of the BLEU Metric.
Translation	BLEU	 Table 5: Translation Results for the Spoken Lan- guage Domain in the BLEU Metric.
syntactic parsing	recall	We use the labeled crossing bracket metric (typically used in the syntactic parsing literature (), which computes recall, precision and crossing brackets for the constituents (subtrees) in a hypothesized parse tree given the reference parse tree.
syntactic parsing	precision	We use the labeled crossing bracket metric (typically used in the syntactic parsing literature (), which computes recall, precision and crossing brackets for the constituents (subtrees) in a hypothesized parse tree given the reference parse tree.
POS tagging	precision	We examine the use of POS tagging to improve precision and recall of stemming and thereby the coverage of the lexicon.
POS tagging	recall	We examine the use of POS tagging to improve precision and recall of stemming and thereby the coverage of the lexicon.
machine translation evaluation	BLEU	For example, in machine translation evaluation, approaches such as BLEU () use n-gram overlap comparisons with a model to judge overall "goodness", with higher n-grams meant to capture fluency considerations.
Semantic role labeling	F 1	 Table 2: Semantic role labeling performance using  different amounts of training data; the seeds are  expanded with their k nearest neighbors;  *  : F 1 is  significantly different from 0-NN (p < 0.05).
ASR	accuracy	Both methods exploit multiple hypotheses from ASR, in the form of word confusion networks, in order to achieve tighter coupling between ASR and query parsing and improved accuracy of the query parser.
ASR	accuracy	Both methods exploit multiple hypotheses from ASR, in the form of word confusion networks, in order to achieve tighter coupling between ASR and query parsing and improved accuracy of the query parser.
parsing word confusion	accuracy	We demonstrate that by parsing word confusion networks, the accuracy of the query parser can be improved.
edit detection	accuracy	We first evaluate edit detection accuracy on a perword basis.
POS tagging	accuracy	While a deterministic approach to Chinese segmentation and POS tagging might be appropriate and necessary for certain tasks or applications, it has been shown to suffer from a problem of low accuracy.
parsing	accuracy	We show that reranking based on bitext projection features increases parsing accuracy significantly .
identification from topics	accuracy	For identification from topics (train on five topics, test on one), an average accuracy of 94% is achieved.
parsing coordination structures	error rate	More generally, experiment 4 suggests that for the notoriously difficult problem of parsing coordination structures, a hybrid approach that combines parse selection of n best analyses with pre-bracketed scope in the input results in a considerable reduction in error rate compared to each of these methods used in isolation.
summarization	ROUGE	This is due to the fact that it is geared towards a different task-as ours is not automatic summarization per se-and that ROUGE works best judging between a number of candidate and model summaries.
Translation	precision	 Table 7: Translation performances on TEST. P smt  stands for the precision and recall of the SMT en- gine. ∆B indicates the absolute gain in BLEU  score of the combined system.
Translation	recall	 Table 7: Translation performances on TEST. P smt  stands for the precision and recall of the SMT en- gine. ∆B indicates the absolute gain in BLEU  score of the combined system.
Translation	BLEU  score	 Table 7: Translation performances on TEST. P smt  stands for the precision and recall of the SMT en- gine. ∆B indicates the absolute gain in BLEU  score of the combined system.
SMT en- gine	BLEU  score	 Table 7: Translation performances on TEST. P smt  stands for the precision and recall of the SMT en- gine. ∆B indicates the absolute gain in BLEU  score of the combined system.
terminology extraction	accuracy	Comparisons with standard terminology extraction programs show an improvement of up to 20% for bilingual terminology extraction and competitive results (85% to 90% accuracy) for monolingual terminology extraction, and reveal that the linguistically based alignment module is particularly well suited for the extraction of complex multiword terms.
summarization	ROUGE	Furthermore, automated summarization metrics like ROUGE ( are non-trivial to adapt to this domain as they require human curated outputs.
tagging	accuracy	 Table 2: Average tagging accuracy (%) using the  original IFD corpus
tagging	accuracy	 Table 3: Average tagging accuracy (%) using the  corrected IFD corpus
question answering	accuracy	In question answering for example, a system maybe configured not to answer questions for which the confidence of producing a correct answer is low, and in this way increase the overall accuracy of the system whenever it does produce an answer (.
SMT translation	BLEU score	It has been shown that F-score has a very weak correlation with SMT translation quality in terms of BLEU score (.
question classification	accuracy	In particular, in question classification, tree kernels, e.g. (, have shown accuracy comparable to the best models, e.g. ().
Default unification	accuracy	Default unification is effective: it ran faster and achieved higher accuracy than deterministic parsing with normal unification.
Named entity recognition (NER)	BBN	Named entity recognition (NER) for En-glish typically involves one of three gold standards: MUC, CoNLL, or BBN, all created by costly manual annotation.
parsing	accuracy	We show that a treatment of animacy as a lexical semantic property of noun types enables generalization over distri-butional properties of these nouns which proves beneficial in automatic classification and furthermore gives significant improvements in terms of parsing accuracy for Swedish, compared to a state-of-the-art baseline parser with gold standard ani-macy information.
parsing	accuracy	Overall parsing accuracy will be reported using the standard metrics of labeled attachment score (LAS) and unlabeled attachment score (UAS).
parsing	labeled attachment score (LAS)	Overall parsing accuracy will be reported using the standard metrics of labeled attachment score (LAS) and unlabeled attachment score (UAS).
parsing	unlabeled attachment score (UAS)	Overall parsing accuracy will be reported using the standard metrics of labeled attachment score (LAS) and unlabeled attachment score (UAS).
parsing	accuracy	One of our motivations for this work was that our parsing model may achieve a better balance between accuracy and speed than existing parsers.
parsing	speed	One of our motivations for this work was that our parsing model may achieve a better balance between accuracy and speed than existing parsers.
Cross-Language Information Retrieval (CLIR)	accuracy	In Cross-Language Information Retrieval (CLIR) systems, they play an even more important role as the accuracy of their transliterations is shown to correlate highly with the performance of the CLIR systems).
coreference resolution	accuracy	We formulate language ID as a coreference resolution problem and apply it to a Web harvesting task fora specific linguistic datatype and achieve a much higher accuracy than long accepted language ID approaches.
word segmentation	F-score	Traditionally, word segmentation performance is measured by F-score ( F = 2RP/(R + P ) ), where the recall (R) and precision (P ) are the proportions of the correctly segmented words to all words in, respectively, the gold-standard segmentation and a segmenter's output.
word segmentation	F	Traditionally, word segmentation performance is measured by F-score ( F = 2RP/(R + P ) ), where the recall (R) and precision (P ) are the proportions of the correctly segmented words to all words in, respectively, the gold-standard segmentation and a segmenter's output.
word segmentation	recall (R)	Traditionally, word segmentation performance is measured by F-score ( F = 2RP/(R + P ) ), where the recall (R) and precision (P ) are the proportions of the correctly segmented words to all words in, respectively, the gold-standard segmentation and a segmenter's output.
word segmentation	precision (P )	Traditionally, word segmentation performance is measured by F-score ( F = 2RP/(R + P ) ), where the recall (R) and precision (P ) are the proportions of the correctly segmented words to all words in, respectively, the gold-standard segmentation and a segmenter's output.
recogniser	accuracy	The overall accuracy of MLP, the state of the art recogniser, is 73.9% and the overall accuracy of the TGI+ based recogniser is 78.58%, which is a 4.68% ± 3.45% in favour of TGI+.
Translations	Bleu	Translations are evaluated on two automatic metrics: Bleu () and PER, position independent error-rate ().
Translations	PER	Translations are evaluated on two automatic metrics: Bleu () and PER, position independent error-rate ().
Translation	POS	 Table 4: Translation results for Bleu. Baseline with POS: 20.19, without POS: 19.66. Results that are  better than the baseline are marked with bold face.
Translation	POS	 Table 4: Translation results for Bleu. Baseline with POS: 20.19, without POS: 19.66. Results that are  better than the baseline are marked with bold face.
Translation	POS	 Table 5: Translation results for PER. Baseline with POS: 27.22, without POS: 26.49. Results that are  better than the baseline are marked with bold face.
Translation	POS	 Table 5: Translation results for PER. Baseline with POS: 27.22, without POS: 26.49. Results that are  better than the baseline are marked with bold face.
definition extraction	precision	Although the recall is very important, because of the context in which we want to apply definition extraction the precision also cannot be too low.
Argument clustering	F1	 Table 1: Argument clustering performance with gold  argument identification. Bold-face is used to highlight  the best F1 scores.
Detecting lexical entailment	accuracy	 Table 2: Detecting lexical entailment. Results ranked  by accuracy and expressed as percentages. 95% con- fidence intervals around accuracy calculated by bino- mial exact tests.
Detecting lexical entailment	con- fidence intervals	 Table 2: Detecting lexical entailment. Results ranked  by accuracy and expressed as percentages. 95% con- fidence intervals around accuracy calculated by bino- mial exact tests.
Detecting lexical entailment	accuracy	 Table 2: Detecting lexical entailment. Results ranked  by accuracy and expressed as percentages. 95% con- fidence intervals around accuracy calculated by bino- mial exact tests.
Detecting quantifier entailment	accuracy	 Table 3: Detecting quantifier entailment. Results  ranked by accuracy and expressed as percentages.  95% confidence intervals around accuracy calculated  by binomial exact tests.
Detecting quantifier entailment	accuracy	 Table 3: Detecting quantifier entailment. Results  ranked by accuracy and expressed as percentages.  95% confidence intervals around accuracy calculated  by binomial exact tests.
parsing	accuracy	Another approach is to evaluate parsing accuracy, for example), which is really a formalism-specific approximation to argument structure analysis.
classification	accuracy	In these, we use the LIBLINEAR package) for classification, which reduces training time for these larger datasets; and feature models adapted to this system which, in the case of German, result in higher accuracy than published results using LIBSVM.
tagging	accuracy	Table 1 shows the tagging accuracy.
SMT	MERT (Minimum Error Rate Training) algorithm	This is achieved either directly in the SMT system using the MERT (Minimum Error Rate Training) algorithm and optimiz-ing according to the BLEU 2 () score, or via reranking the Nbest translation candidates generated by a baseline system based on new parameters (and possibly new features) that aim to optimize a retrieval metric.
SMT	BLEU 2 () score	This is achieved either directly in the SMT system using the MERT (Minimum Error Rate Training) algorithm and optimiz-ing according to the BLEU 2 () score, or via reranking the Nbest translation candidates generated by a baseline system based on new parameters (and possibly new features) that aim to optimize a retrieval metric.
NE analysis	recall	Our contributions are: • A small corpus of articles annotated in anew scheme that provides more freedom for annotators to adapt NE analysis to new domains; • An "arrogant" learning approach designed to boost recall in supervised training as well as self-training; and • An empirical evaluation of this technique as applied to a well-established discriminative NER model and feature set.
NER	recall	We investigate two questions in the context of NER for Arabic Wikipedia: • Loss function: Does integrating a cost function into our learning algorithm, as we have done in the recall-oriented perceptron ( §4.1), improve recall and overall performance on Wikipedia data?
AMT evaluation	BL	 Table 3: AMT evaluation results. Numbers are percentages or counts. BL = baseline, SY = system, N-D = no  decision, B=S = same sentence selected by baseline and system
coreference resolution	accuracy	 Table 4.  Even with such a coarse approximation of  coreference resolution, our model is able to  achieve around 85% accuracy in most test cases,  except for dataset Earthquakes, training on PS BL  gives poorer performance than the standard model  by a small margin. But such inferior perfor- mance should be expected, because as explained  above, coreference resolution is crucial to this  dataset, since entities tend to be realized through  pronouns; simple string matching introduces too  much noise into training, especially when our  model wants to train a more fine-grained discrim- inative system than B&L's. However, we can see  from the result of training on PS M , if the per- mutations used in training do not involve swap- ping sentences which are too far away, the result- ing noise is reduced, and our model outperforms  theirs. And for dataset Accidents, our model  consistently outperforms the baseline model by a  large margin (with significance test at p < .01).
Classification	accuracy	The tree correctly classifies 76% of the labeled user edits.: Classification accuracy using the baseline, each feature set added to the baseline, and all features combined.
Classification	accuracy	 Table 4: Classification accuracy using the base- line, each feature set added to the baseline, and  all features combined. Statistical significance at  p < 0.05 is indicated by  † w.r.t the baseline (us- ing the same classifier), and by ∧ w.r.t to another  classifier marked by ∨ (using the same features).  Highest accuracy per classifier is marked in bold.
Classification	Statistical significance	 Table 4: Classification accuracy using the base- line, each feature set added to the baseline, and  all features combined. Statistical significance at  p < 0.05 is indicated by  † w.r.t the baseline (us- ing the same classifier), and by ∧ w.r.t to another  classifier marked by ∨ (using the same features).  Highest accuracy per classifier is marked in bold.
Classification	accuracy	 Table 4: Classification accuracy using the base- line, each feature set added to the baseline, and  all features combined. Statistical significance at  p < 0.05 is indicated by  † w.r.t the baseline (us- ing the same classifier), and by ∧ w.r.t to another  classifier marked by ∨ (using the same features).  Highest accuracy per classifier is marked in bold.
Classification	accuracy	 Table 11: Classification accuracy using features  from unlabeled data. The first two rows are identi- cal to Table 4. Statistical significance at p < 0.05  is indicated by:  † w.r.t the baseline;  ‡ w.r.t all fea- tures excluding features from unlabeled data; and  ∧ w.r.t to another classifier marked by ∨ (using the  same features). The best result is marked in bold.
Classification	Statistical significance	 Table 11: Classification accuracy using features  from unlabeled data. The first two rows are identi- cal to Table 4. Statistical significance at p < 0.05  is indicated by:  † w.r.t the baseline;  ‡ w.r.t all fea- tures excluding features from unlabeled data; and  ∧ w.r.t to another classifier marked by ∨ (using the  same features). The best result is marked in bold.
parsing	accuracy	The goal of our experiments is to show that incorporating hidden states in a SHAG using operator models can consistently improve parsing accuracy.
Translation	BLEU	 Table 7: Comparison of various hybrid LM variants. Translation quality is measured with BLEU, METEOR and  TER (all in percentage form). The settings used for weight tuning are marked with  †. Best models according to  all metrics are highlighted in bold.
Translation	METEOR	 Table 7: Comparison of various hybrid LM variants. Translation quality is measured with BLEU, METEOR and  TER (all in percentage form). The settings used for weight tuning are marked with  †. Best models according to  all metrics are highlighted in bold.
Translation	TER	 Table 7: Comparison of various hybrid LM variants. Translation quality is measured with BLEU, METEOR and  TER (all in percentage form). The settings used for weight tuning are marked with  †. Best models according to  all metrics are highlighted in bold.
Evaluating language understanding	accuracy	Evaluating language understanding accuracy with respect to objective outcomes in a dialogue system
Domain adaptation	IN	 Table 5: Domain adaptation results DE-FR. Domain: Alpine texts. Full IN TM: Using the full in-domain parallel  corpus; small IN TM: using 10% of available in-domain parallel data.
WSI	Latent Dirichlet Allocation (LDA:)	Building on the work of and others, we approach WSI via topic modelling -using Latent Dirichlet Allocation (LDA:) and derivative approaches -and use the topic model to determine the appropriate sense granularity.
MLE	BW-with	In this subset, MLE explores the effect of using the CATIB-EX and BW-with other morphological features.
BW prediction	accuracy	BW prediction accuracy is low because it includes case endings.
SRL	Arg0	For example, most of the works on SRL have used PropBank's numerical role labels (Arg0 to Arg5).
SRL	Arg5	For example, most of the works on SRL have used PropBank's numerical role labels (Arg0 to Arg5).
IS determination	accuracy	In an evaluation on 147 Switchboard dialogues, our learningbased approach to fine-grained IS determination achieves an accuracy of 78.7%, substantially outperforming the rule-based approach by 21.3%.
IS subtype determination	accuracies	These results also suggest that coreference plays a crucial role in IS subtype determination: accuracies could increase by up to 7.7-8.6% if we solely improved coreference resolution performance.
CAT	CALL	The preliminary results show that the method has great potentials in CAT and CALL (significant boost in translation quality is observed).
parsing	accuracy	Experiments show that MaltOptimizer can improve parsing accuracy by up to 9 percent absolute (labeled attachment score) compared to default settings.
parsing	labeled attachment score)	Experiments show that MaltOptimizer can improve parsing accuracy by up to 9 percent absolute (labeled attachment score) compared to default settings.
parsing	accuracy	During the demo session, we will run MaltOptimizer on different data sets (user-supplied if possible) and show how the user can interact with the system and track the improvement in parsing accuracy.
parsing	accuracy	The first thing to note is that the optimization improves parsing accuracy for all languages without exception, although the amount of improvement varies considerably from about 1 percentage point for Chinese, Japanese and Swedish to 8-9 points for Dutch, Czech and Turkish.
symptom description extraction	f-score	In the separate phases of symptom description extraction the f-score goes up to 96%.
MT	BLEU score	In combination with a larger phrase-based MT system (), these two training procedures yield an MT system that achieves a BLEU score of 31.79 on an English-to-Chinese translation task, an improvement of 2.64 in BLEU score over a baseline MT system trained on only our parallel corpora.
MT	BLEU score	In combination with a larger phrase-based MT system (), these two training procedures yield an MT system that achieves a BLEU score of 31.79 on an English-to-Chinese translation task, an improvement of 2.64 in BLEU score over a baseline MT system trained on only our parallel corpora.
MT	BLEU score	In combination with a larger phrase-based MT system (), these two training procedures yield an MT system that achieves a BLEU score of 31.79 on an English-to-Chinese translation task, an improvement of 2.64 in BLEU score over a baseline MT system trained on only our parallel corpora.
MT	BLEU score	In combination with a larger phrase-based MT system (), these two training procedures yield an MT system that achieves a BLEU score of 31.79 on an English-to-Chinese translation task, an improvement of 2.64 in BLEU score over a baseline MT system trained on only our parallel corpora.
word alignment	End	Because IBM 3 model is a fertility based model which might also alleviate Algorithm 2: Co-training for word alignment and language modeling 1: Input: parallel data X p , LCS data X LCS , language model training data X l 2: Initialize translation table tb for IBM1 model 3: For iteration from 1 to MAX tb ← Train-IBM(X p ) tb ← Train-HMM(X p |tb) 4: For each sentence xi in X LCS : For each source word s j in xi : 1) find the translation t j of s j with with probability p j from tb 2) replace s j with t j and update sentence's probability Extract the tri-gram gram 3 from LM 7: For each sentence xi in X LCS : run Algorithm 1: finding t similar 8: update tb using (t m , s j ) where t m ∈ t similar and s j ∈ xi 9: End For 10: Output: word alignment for X p and LM some of the problems caused by LCS data.
MT	Minimum Error Rate Training (MERT)	Parameters for the final MT system are tuned with Minimum Error Rate Training (MERT).
event ordering task	F-score	While on the event paraphrase task the R10 system performs slightly better, our model outperforms the R10 system on the event ordering task by a substantial margin of 7 points average F-score.
signaling personal gang identification	accuracy	In this paper, we present a series of experiments in which we analyze the usage of graffiti style features for signaling personal gang identification in a large, online street gangs forum, with an accuracy as high as 83% at the gang alliance level and 72% for the specific gang.
Lexical category	accuracy	 Table 1: Lexical category accuracy on TEST-4SEC
classification	accuracy	 Table 2: Ablation tests showing crossing scores  and classification accuracy as features are re- moved. All models were trained on 8M samples.
event-event classification	accuracy	However, choosing the right feature vectors to build the classification model is still an open issue , especially for event-event classification , whose accuracy is still under 50%.
Translation	PRO	Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO ( shows BLEU scores of the baseline system as well as the performance of three in-domain models (IN) tuned under the same conditions.
Translation	BLEU	Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO ( shows BLEU scores of the baseline system as well as the performance of three in-domain models (IN) tuned under the same conditions.
sentiment classification	precision	The graph-based model improves over explicit sentiment classification by 10 points in precision and, in an evaluation of the model itself, we find it has an 89% chance of propagating sentiments correctly.
MT	MIRA	We propose and evaluate three computationally efficient online methods for updating statistical MT systems in a scenario where post-edited MT output is constantly being returned to the system: (1) adding new rules to the translation model from the post-edited content, (2) updating a Bayesian language model of the target language that is used by the MT system, and (3) updating the MT system's discriminative parameters with a MIRA step.
MT	MIRA	We propose and evaluate three computationally efficient online methods for updating statistical MT systems in a scenario where post-edited MT output is constantly being returned to the system: (1) adding new rules to the translation model from the post-edited content, (2) updating a Bayesian language model of the target language that is used by the MT system, and (3) updating the MT system's discriminative parameters with a MIRA step.
MT	BLEU	The employed MT system is trained on largely the same resources as our own English-Spanish system, granting the opportunity fora much closer approximation to an actual post-editing task; our system configurations score between 54 and 56 BLEU against the sample MT, indicating that humans post-edited translations similar but not identical to our own.
WMT evaluations	luckof-the-draw	This has the added advantage of eliminating the criticism made of WMT evaluations that systems sometimes gain advantage from luckof-the-draw comparison with low quality output, and vice-versa).
Named Entity Recognition (NER	PERSON	Named Entity Recognition (NER) It labels atomic elements in the sentence into categories such as "PERSON" or "LOCATION".
NER task	F1  score	 Table 2: Performance comparison on NER task  with different embeddings. The first column is  results with the original embeddings. The sec- ond column is results with embeddings after fine- tuning for this task. Results are reported in F1  score (mean ± standard deviation of ten training  runs with different initialization).
classification	accuracy	Our methods yield significant improvement in classification accuracy over a naive baseline.
ANOVA	Re- sults	 Table 6: Two way ANOVA with factors model (4  possibilities), and sub-route (94 possibilities). Re- sults show a model effect accounting for most of  the variance. Meaning that the scores assigned to  the units by human judges are significantly influ- enced by the model used to generate the units.
keyword extraction	F-measure 1	We compare different keyword extraction methods by the F-measure 1 they achieve against the gold-standard summary keywords.
summarization task	ROUGE	We only consider the words in the summary that also appear in the original input 2 , with stopwords excluded 3 . Mean |G i | 102 32 15 6: Average number of words in G i For the summarization task, we compare results using ROUGE).
Keyword identification	F-score	 Table 5: Keyword identification F-score (%) for different G i and different number of words selected.
summariser	ROUGE	We evaluate the resulting summariser against two commonly used extractive summaris-ers using ROUGE, with encouraging results .
summariser	MEAD	Our summariser controls word count precisely; we require MEAD to produce summaries close to the length (allowing variations), and for LexRank we allow it to go beyond the limit by less than one sentence and then discard the exceeding part in the sentence with the lowest salience.
summariser	MEAD	This experiment on a large dataset demonstrates that our summariser performs in the ballpark of typical results of extractive summarisers, although it is still statistically a little worse than the state-ofthe-art MEAD (whose F-measure 95%-confidence interval is 0.349 -0.367).
summariser	F-measure 95%-confidence interval	This experiment on a large dataset demonstrates that our summariser performs in the ballpark of typical results of extractive summarisers, although it is still statistically a little worse than the state-ofthe-art MEAD (whose F-measure 95%-confidence interval is 0.349 -0.367).
summariser	precision	Our summariser is good at precision because many summaries produced have not used up the 100-word limit, making the average summary length smaller than that of MEAD's.
chunking	F1 score	 Table 2: Downstream results on chunking. Overall  F1 score (All) as well as F1 for NP, VP and PP.
chunking	F1	 Table 2: Downstream results on chunking. Overall  F1 score (All) as well as F1 for NP, VP and PP.
article matching tasks	accuracy	We conducted an extensive user study that shows that our toolkit allows users to solve cross-lingual entity tracking and article matching tasks more efficiently and with higher accuracy compared to the baseline approach.
Statistical Machine Translation system combination shared task	BLEU	On the most recent Workshop on Statistical Machine Translation system combination shared task, we achieve improvements of up to 0.7 points in BLEU over the best system combination hypotheses which were submitted for the official evaluation.
Part-of-speech labelling	accuracy	 Table 1: Part-of-speech labelling accuracy in  DKIE
role description task	F1-measure	mF and MF columns for each role description task represent respectively the micro and macro average F1-measure over the test set.
SRC	F 1 -measure (F 1 )	Within the specific case of SRC, different metrics have been used such as F 1 -measure (F 1 ), kSSL 1 and F b 3 -measure (F b 3 ) over different standard datasets: ODP-239 and Moresque (.
SRC	kSSL 1	Within the specific case of SRC, different metrics have been used such as F 1 -measure (F 1 ), kSSL 1 and F b 3 -measure (F b 3 ) over different standard datasets: ODP-239 and Moresque (.
SLQS	BLESS	To assess SLQS we relied on a subset of BLESS (), a freelyavailable dataset that includes 200 distinct English concrete nouns as target concepts, equally divided between living and non-living entities (e.g. BIRD, FRUIT, etc.).
SLQS	BIRD	To assess SLQS we relied on a subset of BLESS (), a freelyavailable dataset that includes 200 distinct English concrete nouns as target concepts, equally divided between living and non-living entities (e.g. BIRD, FRUIT, etc.).
disfluency detection	speed	In this paper, we extend RT13 in two important ways: 1) we show that by adding a set of novel features selected specifically for disfluency detection we can outperform the current state of the art in disfluency detection in two evaluations 1 and 2) we show that by extending the architecture from two to six classifiers, we can drastically increase the speed and reduce the memory usage of the model without a loss in performance.
disfluency detection	speed	In this paper, we extend RT13 in two important ways: 1) we show that by adding a set of novel features selected specifically for disfluency detection we can outperform the current state of the art in disfluency detection in two evaluations 1 and 2) we show that by extending the architecture from two to six classifiers, we can drastically increase the speed and reduce the memory usage of the model without a loss in performance.
parsing	accuracy	We compare our new models against this prior work in terms of disfluency detection performance and parsing accuracy.
disfluency detection task	F-score	The joint model performs poorly on the disfluency detection task, with an F-score of 41.5, and the prior work performance which serves as our baseline (RT13) has a performance of 81.4.
Disfluency detection	F1 score)	 Table 2: Disfluency detection results (F1 score) on  JC04 split and with cross-validation (xval)
conversational spoken language translation (CSLT)	accuracy	Selecting a contextually incorrect translation of such a wordhere referred to as a word sense translation error (WSTE) -can lead to a critical failure in a conversational spoken language translation (CSLT) system, where accuracy of concept transfer is paramount.
translation	precision	However, these resources are often limited to well-known or large SEs, which leads to translation with near-perfect precision but low recall.
translation	recall	However, these resources are often limited to well-known or large SEs, which leads to translation with near-perfect precision but low recall.
entity translation	accuracy	Moreover, blindly applying existing entity translation methods to SE translation leads to extremely low accuracy.
SE translation	accuracy	Moreover, blindly applying existing entity translation methods to SE translation leads to extremely low accuracy.
person recognition	F1	We explore noise-tolerant methods for sequence labeling and apply discriminative post-editing to exceed state-of-the-art performance for person recognition in tweets, reaching an F1 of 84%.
Named Entity Recognition (NER)	F1	In particular, traditional approaches to Named Entity Recognition (NER) perform poorly on tweets, especially on person mentions -for example, the default model of a leading system reaches an F1 of less than 0.5 on person entities in a major tweet corpus.
entity recognition	recall	Firstly, it demonstrates that entity recognition using noise-resistant sequence labeling outperforms state-of-the-art Twitter NER, although we find that recall is consistently lower than precision.
entity recognition	precision	Firstly, it demonstrates that entity recognition using noise-resistant sequence labeling outperforms state-of-the-art Twitter NER, although we find that recall is consistently lower than precision.
segmentation	accuracy	Experiments on three diverse languages show that this straightforward semi-supervised extension greatly improves the segmentation accuracy of the purely supervised CRFs in a computation-ally efficient manner.
segmentation	accuracy	Recently, showed that the CRFs can yield competitive segmentation accuracy compared to more complex, previous state-of-theart techniques.
segmentation	accuracy	Experiments on three diverse languages show that the semi-supervised extension substantially improves the segmentation accuracy of the CRFs.
information extraction	accuracy	Work on information extraction typically uses accuracy and recall of the extracted information as an evaluation measure.
information extraction	recall	Work on information extraction typically uses accuracy and recall of the extracted information as an evaluation measure.
pattern expansion	MRR	In the end, the combination of pattern expansion and clustering achieves an MRR of 71.7% and recall of 15.4%.
pattern expansion	recall	In the end, the combination of pattern expansion and clustering achieves an MRR of 71.7% and recall of 15.4%.
parsing	accuracy	Notably, Model 1 achieves parsing improvements of 1.3% in unlabeled attachment, 2.07% root attachment, and 3.94% incomplete accuracy.
parsing	accuracy	We evaluate this scheme on the TüBa-D/Z treebank w.r.t. several metrics and show that it improves both parsing accuracy and parsing speed considerably.
parsing	accuracy	Since dimension is a meaningful measure of complexity and parse trees have low dimension, we conjectured that annotating nonterminals with the dimension of the subtree rooted at them could improve parsing accuracy (see for an illustration).
parsing	speed	At the same time, quite surprisingly, parsing speed more than doubles.
parsing	accuracy	This is especially relevant for comparing parsing accuracy over different treebanks (Rehbein and Van Genabith, 2007a; Rehbein and van Genabith, 2007b).
parsing	LA	 Table 2: Average grammar sizes, parsing speed, and parsing accuracies according to various metrics (for  the 70k samples only, i.e. on 7000 test trees). All numbers are averaged over 10 independent random  samples. |G| denotes the number of rules in the grammar, parsing speed is measured in sentences per  second. LA scores are reported as sentence-level (s) and corpus-level (c) averages, respectively. All  accuracies reported in % (except # CB -the average number of crossing brackets per sentence).
MT	PEs	We then train anew MT system using the updated training data (initial training data plus PEs of the first batch of sentences).
PE-based incremental MT retraining	TER score	show that AL for PE-based incremental MT retraining really works: all AL based methods (Ngram, CED, CEDN) show strong improvements over both baselines after the initial 8-9 iterations and best performance on the complete incremental data sets, resulting in a noticeable decrease of the overall TER score).
MT	ORACLE	 Table 4: MT results of ORACLE and REAL ex- periments. Highest score per metric is bolded.  { + / − } indicates statistically significant improve- ment/degradation, p < 0.05. (P: precision; R: re- call; B: BLEU; M: METEOR; T:TER)
MT	precision	 Table 4: MT results of ORACLE and REAL ex- periments. Highest score per metric is bolded.  { + / − } indicates statistically significant improve- ment/degradation, p < 0.05. (P: precision; R: re- call; B: BLEU; M: METEOR; T:TER)
MT	BLEU	 Table 4: MT results of ORACLE and REAL ex- periments. Highest score per metric is bolded.  { + / − } indicates statistically significant improve- ment/degradation, p < 0.05. (P: precision; R: re- call; B: BLEU; M: METEOR; T:TER)
MT	METEOR	 Table 4: MT results of ORACLE and REAL ex- periments. Highest score per metric is bolded.  { + / − } indicates statistically significant improve- ment/degradation, p < 0.05. (P: precision; R: re- call; B: BLEU; M: METEOR; T:TER)
IMS	Precision	Our evaluation shows that the IMS system achieves a higher score for pronouns than the Berkley system when immediate antecedents are measured and has a higher Precision for pronouns regarding the inferred antecedents.
Recognition	accuracy	 Table 1. Recognition accuracy without language model.
Recognition	accuracy	 Table 2. Recognition accuracy with language model
MT	Adequacy	2. The requirements on lin@uistic theory as a background for MT can be summarized as follows: (a~ Adequacy: The theory should underlie relatively complete descriptions reflecting the structure of language.
TAGs	O	Various parsing algorithms for TAGs have been proposed in the literature: the worst-case time complexity varies from O(n 4 log n) to O(n 6) and O(n 9).
transfer of tense and aspect	differS	In this paper we will discuss the transfer of tense and aspect, a problem which arises immediately in Machine Translation and differS from language pair to language pair.
Machine Translation	differS	In this paper we will discuss the transfer of tense and aspect, a problem which arises immediately in Machine Translation and differS from language pair to language pair.
parsing	accuracy	The table shows that there is a relatively rapid inc~'~ase in parsing accuracy when enlarging the maximum depth of the subUees to 3.
parsing	accuracy	In the following figure, parsing accuracy is plotted against the sample size Nfor three of our experiments: the experiments where the depth of the subtrees is constrained to 2 and 3, and the experiment where the depth is unconswained.
bracketing	accuracy	In, 90.36% bracketing accuracy was reported using a stochastic CFG trained on bracketings from the ATIS corpus.
parsing	accuracy	This brings us to the question as to how much parsing accuracy depends on the size of the corpus.
parsing	accuracy	In the next figure the parsing accuracy, for sample size N = 100, is plotted against the corpus size, using all corpus subtrees.
parsing	O	The solution given in to obtain a variant of the parsing technique which has a fixed time complexity of O(n3), independent of p, implies an overhead in computation costs which worsens instead of improves the time complexity in practical cases.
bracketing	accuracy	Surprisingly high bracketing accuracy is achieved with only 1042 sentences as train-• ing materi,'d: 94.4% for test sentences shorter th,-m 10 words ~md 90.2% for sentences shorter than 15 words.
bracketing	accuracy	Furthermore, the bracketing accuracy does not drop drastic~dly as longer sentences ,are considered.
parsing grammar	accuracy	While a parsing grammar should avoid introducing structurally unresolvable distinctions in order to optimise on the accuracy of the parser, it also is beneficial for the grammarian to have as expressive a structural representation available as possible.
Derivation	accuracy	 Table 2. Derivation accuracy vs. parse accuracy
WSS	precision	The results of this WSS procedure are checked against a testing-sample manually analyzed, and precision and recall ratios are calculated.
WSS	recall	The results of this WSS procedure are checked against a testing-sample manually analyzed, and precision and recall ratios are calculated.
parsing	accuracy	The experiments show that involving larger fragments in the parsing process leads to higher accuracy.
disambiguating Turkish	recall	Our results for disambiguating Turkish indicate that using about 500 constraint rules and some additional simple statistics, we can attain a recall of 95-96~ and a precision of 94-95~ with about 1.01 parses per token.
disambiguating Turkish	precision	Our results for disambiguating Turkish indicate that using about 500 constraint rules and some additional simple statistics, we can attain a recall of 95-96~ and a precision of 94-95~ with about 1.01 parses per token.
tagging	accuracy	Although the variable-memory length approach remarkably reduces the number of parameters, tagging accuracy is only as good as conventional methods.
machine translation (SMT)	translation probability (TP)	The statistical approach to machine translation (SMT) can be understood as a word-by-word model consisting of two sub-models: a language model for generating a source text segment Sand a translation model for mapping S to its translation T. also recommend using a bilingual corpus to train the parameters of Pr(S I 73, translation probability (TP) in the translation model.
SIMR	error	The evaluation in Section 5 shows that SIMR's error rates are lower than those of other bitext mapping algorithms by an order of magnitude.
SIMR	accuracy	 Table 2: SIMR accuracy on different text genres in three language pairs.  language  number of  number of  RMS Error  pair  training TPCs  genre  test TPCs  in characters  French / English  598  parliamentary debates  CITI technical reports  other technical reports  court transcripts  U.N. annual report  I.L.O. report
segmentation	accuracy	demonstrate that segmentation accuracy is significantly higher when the lexicon is constructed using the same type of corpus as the corpus on which it is tested.
tagging	accuracy	The accuracy 2Automatically derived rules require less work than manually written ones but are unlikely to yield better results because they would consider relatively limited context and simple relations only. of this data has an impact on the tagging accuracy of both the HMM itself and the derived transducer.
Translation	possi- ble	 Table 4: Translation evaluation results (best possi- ble = 1.00, worst possible = 6.00)
KA from medium-sized corpora	LEXTER	The main idea to make KA from medium-sized corpora a feasible and efficient task is to perform a robust syntactic analysis (using LEXTER, see section 2) followed by a semi-automatic semantic analysis where automatic clustering techniques are used interactively by the knowledge engineer (see sections 3 and 4).
term extraction and structuring	LEXTER	The contribution of this paper is to propose an integrated platform for computer-aided term extraction and structuring that results from the combination of LEXTER, a Term Extraction tool (Bouriganlt et al.
Translation	accuracy	Translation accuracy includes transpositions (i.e., movement) of words as well as insertions, deletions, and substitutions.
translation model	accuracy	Quantitative evaluation with respect to independent human judgments has shown that each of these three estimation biases significantly improves translation model accuracy over a baseline knowledge-free model.
speech recognition	accuracy	We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy.
dialogue modeling	accuracy	We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy.
speech recognition	accuracy	We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy.
DA classification	chance	 Table 7  DA classification using prosodic  decision trees (chance = 35%).
information retrieval	inverse document frequency (IDF)	In information retrieval, document frequencies are converted into inverse document frequency (IDF), which plays an important role in term weighting.
Knowledge Representation Language	EMAIL	The Theory and Practice of Discourse Parsing and Summarization Empirical Methods for Exploiting Parallel Texts Spotting and Discovering Terms through Natural Language Processing Natural Language Processing and Knowledge Representation Language for Knowledge and Knowledge for Language edited by Association for Computational Linguistics 2001 MEMBERSHIP, CHANGE OF ADDRESS AND EMAIL, AND ORDER FORM (also at www
identification of complex noun phrases	error rate	• There were some mistakes in the partial parsing with respect to the identification of complex noun phrases (causing an error rate of around 7%) ().
interpretation of nominalizations	accuracy	Section 4 describes the algorithm used for the interpretation of nominalizations, and Section 5 reports the results of several experiments that achieve a combined accuracy of 86.1% on the British National Corpus (BNC).
disambiguation task	K	The judges' agreement on the disambiguation task was K = .78 (N = 200, k = 2).
Document Understanding Conference (DUC)-01	precision	In the Document Understanding Conference (DUC)-01 and DUC-02 summarization competitions, NIST used the Summary Evaluation Environment (SEE) interface to record values for precision and recall.
Document Understanding Conference (DUC)-01	recall	In the Document Understanding Conference (DUC)-01 and DUC-02 summarization competitions, NIST used the Summary Evaluation Environment (SEE) interface to record values for precision and recall.
MMR	lead factor	We used the optimized baseline MMR parameters and varied only the emphasis parameters for (1) false starts, (2) lead factor, and (3) Q-A sentences, to optimize the CLEAN summaries further.
POS tagging	accuracy	 Table 6  POS tagging accuracy on five subcorpora (evaluated on 500-word samples).
Sentence boundary detection	accuracy	 Table 10  Sentence boundary detection accuracy (F 1 -score).
Sentence boundary detection	F 1 -score)	 Table 10  Sentence boundary detection accuracy (F 1 -score).
translation direction English to German	WER	 Table 12  Translation results for the translation direction English to German on the TEST-331 test set.  The results are given in terms of computing time, WER, and PER for three different reordering  constraints: MON, EG, and S3.
translation direction English to German	PER	 Table 12  Translation results for the translation direction English to German on the TEST-331 test set.  The results are given in terms of computing time, WER, and PER for three different reordering  constraints: MON, EG, and S3.
translation direction English to German	MON	 Table 12  Translation results for the translation direction English to German on the TEST-331 test set.  The results are given in terms of computing time, WER, and PER for three different reordering  constraints: MON, EG, and S3.
translation direction English to German	EG	 Table 12  Translation results for the translation direction English to German on the TEST-331 test set.  The results are given in terms of computing time, WER, and PER for three different reordering  constraints: MON, EG, and S3.
MT	AB	If the three on-line MT systems translate the phrases extracted from the Penn-II Treebank in different ways, then combining systems to obtain results for AB, AC, BC, and ABC always involves an increase in the number of translations produced, both for sentences and noun phrases.
MT	BC	If the three on-line MT systems translate the phrases extracted from the Penn-II Treebank in different ways, then combining systems to obtain results for AB, AC, BC, and ABC always involves an increase in the number of translations produced, both for sentences and noun phrases.
MT	ABC	If the three on-line MT systems translate the phrases extracted from the Penn-II Treebank in different ways, then combining systems to obtain results for AB, AC, BC, and ABC always involves an increase in the number of translations produced, both for sentences and noun phrases.
parsing	accuracy	We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples.
word translation disambiguation	BB	We have experimentally evaluated the performance of BB in word translation disambiguation, and all of our results indicate that BB consistently and significantly outperforms MB.
translation disambiguation	BB	We then used the data to conduct translation disambiguation with BB, MB-B, and MB-D, as described in Sections 4.1 and Section 4.2.
Mappings LAB	FINANCE	 Table 9  Mappings LAB → FINANCE.
IS	STRUBE-HAHN	 Table 15  Transition percentages for IS with STRUBE-HAHN ranking.
anaphora resolution	accuracy.	Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.,, for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.
anaphora resolution	accuracies	Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.,, for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.
anaphora resolution	F-measure	Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.,, for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.
Translation	BLEU	Translation performance was measured using the automatic BLEU evaluation metric ( on four reference translations.
automatic speech recognition (ASR)	accuracy	For example, automatic speech recognition (ASR) systems prefer longer "words" to achieve higher accuracy, whereas in-formation retrieval (IR) systems prefer shorter "words" to obtain higher recall rates.
automatic speech recognition (ASR)	recall	For example, automatic speech recognition (ASR) systems prefer longer "words" to achieve higher accuracy, whereas in-formation retrieval (IR) systems prefer shorter "words" to obtain higher recall rates.
NER	recall	r The Viterbi iterative training is effective in boosting the precision of NER without great sacrifices for recall (e.g., the recall remains almost the same when using the seed set of Row 5 in, or becomes a little worse when using the seed sets of Rows 6 to 8).
NER	recall	r The Viterbi iterative training is effective in boosting the precision of NER without great sacrifices for recall (e.g., the recall remains almost the same when using the seed set of Row 5 in, or becomes a little worse when using the seed sets of Rows 6 to 8).
corpus design	B ¨ ohmová	The largest is made up of papers on corpus design and methodology , such as Nelson Francis on the construction of the Brown corpus, Burnage and Dunlop on the British National Corpus, and B ¨ ohmová and Hajičová on the Prague Dependency Treebank.
ASR	accuracy	To date, attempts to improve system performance have largely focused on improving ASR accuracy or simplifying the task, either by further constraining the domain and functionality of the system or by further restricting the vocabulary the system must recognize.
ASR	accuracy	However, as ASR accuracy improves, dialogue systems will be called upon to handle evermore complex tasks and ever less restricted vocabularies.
ASR	prerejbool	However, when only those ASR features derived from the acoustic confidence score (i.e., conf, preconf, ppreconf, pmnconf, rejbool, prerejbool, pprerejbool, priorrejboolnum, priorrejboolpct) are used for prediction, then performance does significantly degrade (conf+rejbool, DIA = all, error = 21.23%).
ASR	DIA	However, when only those ASR features derived from the acoustic confidence score (i.e., conf, preconf, ppreconf, pmnconf, rejbool, prerejbool, pprerejbool, priorrejboolnum, priorrejboolpct) are used for prediction, then performance does significantly degrade (conf+rejbool, DIA = all, error = 21.23%).
ASR	error	However, when only those ASR features derived from the acoustic confidence score (i.e., conf, preconf, ppreconf, pmnconf, rejbool, prerejbool, pprerejbool, priorrejboolnum, priorrejboolpct) are used for prediction, then performance does significantly degrade (conf+rejbool, DIA = all, error = 21.23%).
predicting corrections	precision	With respect to precision and recall, although the absolute numbers for corrections are much lower than in, we again see that predicting corrections yields higher precision than recall, whereas predicting noncorrections yields higher recall than precision.
predicting corrections	recall	With respect to precision and recall, although the absolute numbers for corrections are much lower than in, we again see that predicting corrections yields higher precision than recall, whereas predicting noncorrections yields higher recall than precision.
predicting corrections	recall	With respect to precision and recall, although the absolute numbers for corrections are much lower than in, we again see that predicting corrections yields higher precision than recall, whereas predicting noncorrections yields higher recall than precision.
predicting corrections	precision	With respect to precision and recall, although the absolute numbers for corrections are much lower than in, we again see that predicting corrections yields higher precision than recall, whereas predicting noncorrections yields higher recall than precision.
sentence boundary detection	error	For the sentence boundary detection task, the error rates Punkt achieved on the eleven newspaper corpora range from 2.12% on the Estonian corpus to only 0.35% on the German corpus with an average error rate of 1.26%.
sentence boundary detection	error rate	For the sentence boundary detection task, the error rates Punkt achieved on the eleven newspaper corpora range from 2.12% on the Estonian corpus to only 0.35% on the German corpus with an average error rate of 1.26%.
abbreviation detection	error rates	For the task of abbreviation detection, the increase in the error rates is even lower: 0.14% on the lowercase corpora and 0.13% on the uppercase corpora.
Translation	BLEU	Translation results are evaluated in terms of mWER and BLEU by using the two references available for each language test set.
EPPS translation task	BLEU	 Table 9  The four best BLEU results for the EPPS translation task in TC-STAR's first evaluation campaign.  N-gram based system results are provided in brackets. All BLEU values presented here have  been taken from TC-STAR's SLT Progress Report, available at: http://www.tc-star.org/.
SMT	BLEU score	The weights for all these models and for the sentence probability assigned by the SMT system were optimized with respect to BLEU score on the development corpus.
rescoring	BLEU score	If only the confidence measures are used for rescoring, the BLEU score is increased by 0.5 points.
Translation	BLEU	 Table 12  Translation quality for rescoring with confidence measures. EPPS Spanish → English test set.  Optimized for BLEU.
problem extraction	error rate	Although problem extraction largely depends on disease coverage in UMLS and MetaMap performance, the error rate could be further reduced by more sophisticated recognition of implicitly stated problems.
information retrieval	precision	It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.
information retrieval	recall	It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.
parameter tuning	MRR	In the parameter tuning process, we could not find a weight where performance across all measures was higher; in the end, we settled on what we felt was a reasonable weight that improved P10 and MRR on the development set.
translation	accuracy	We describe our system's training and decoding methods in detail, and evaluate it for translation speed and translation accuracy.
translation	BLEU	Despite this high level of interest, none of these techniques has been shown to result in a large gain in translation performance as measured by BLEU () or any other metric.
SMT	BLEU	We find this lack of correlation between previous word alignment quality metrics and BLEU counterintuitive, because we and other researchers have measured this correlation in the context of building SMT systems that have benefited from using the BLEU metric in improving performance in open evaluations such as the NIST evaluations.
association sampling/estimation	D	We evaluated our two-way association sampling/estimation algorithm with a chunk of Web crawls (D = 2 16 ) produced by the crawler for MSN.com.
parsing or labeling	accuracy	This shows that any parsing or labeling accuracy improvement from conditional estimation of WCFGs or conditional random fields (CRFs) over joint estimation of PCFGs or hidden Markov models (HMMs) is due to the estimation procedure rather than the change in model class, because PCFGs and HMMs are exactly as expressive as WCFGs and chain-structured CRFs, respectively.
parsing	accuracy	This model and parsing algorithm, when combined with normal-form constraints, give state-of-the-art accuracy for the recovery of predicate-argument dependencies from CCGbank.
parsing	accuracy	The results for parsing accuracy were obtained using Section 00 as development data and Section 23 as the final test data.
WSD sc	recall	These measures relate to WSD sc , but again, recall includes words not in the thesaurus which are scored incorrect, and precision does not include items with a joint automatic ranking.
WSD sc	precision	These measures relate to WSD sc , but again, recall includes words not in the thesaurus which are scored incorrect, and precision does not include items with a joint automatic ranking.
identifying the first sense of a word	WSD	Looking at in more detail, it seems to be the case that although the BNC thesaurus does well in identifying the first sense of a word (the type results), the PROX and DEP thesauruses from the NEWSWIRE corpus return better WSD results when used with the jcn measure.
parsing	accuracy	This section presents experimental results on the parsing accuracy attained by the feature forest models.
parsing	accuracy	Following previous studies on parsing with PCFG-based models, accuracy is measured for sentences of less than 40 words and for those with less than 100 words.
parsing	accuracy	The measure for evaluating parsing accuracy is precision/recall of predicateargument dependencies output by the parser.
parsing	precision	The measure for evaluating parsing accuracy is precision/recall of predicateargument dependencies output by the parser.
parsing	recall	The measure for evaluating parsing accuracy is precision/recall of predicateargument dependencies output by the parser.
parsing	accuracy	 Table 5  Specification of test data for the evaluation of parsing accuracy.
SRL	Manner	In order to accomplish this, the role-bearing constituents in a clause must be identified and their correct semantic role labels assigned, as in: [The girl on the swing] [whispered] Pred to [the boy beside her] Recipient Typical roles used in SRL are labels such as Agent, Patient, and Location for the entities participating in an event, and Temporal and Manner for the characterization of other aspects of the event or participant relations.
boundary recognition	accuracy	The results for boundary recognition, classification, and re-ranking stages provide systematic evidence about the significant impact of tree kernels on the overall accuracy, especially when the amount of training data is small.
classification	accuracy	The results for boundary recognition, classification, and re-ranking stages provide systematic evidence about the significant impact of tree kernels on the overall accuracy, especially when the amount of training data is small.
semantic role labeling of nominalized predicates	F-scores	We were able to achieve a larger improvement on the semantic role labeling of nominalized predicates by using noun-specific features (F-scores of 0.70 and 0.57, respectively, for treebank and fully automatic parses), but our results still show that the semantic role labeling of nominalized predicates is a much more challenging task than that of verbs.
parsing	accuracy	In particular, we show that employing sublexical units called inflectional groups, rather than word forms, as the basic parsing units improves parsing accuracy.
parsing	accuracy	We examine the impact of morphological and lexical information in detail and show that, properly used, this kind of information can improve parsing accuracy substantially.
parsing	accuracy	Thus, for Turkish, it has previously been shown that parsing accuracy can be improved by taking morphologically defined units rather than word forms as the basic units of syntactic structure.
parsing	accuracy	We also study the impact of different morphological feature representations on parsing accuracy.
classification	accuracy (Acc)	For classification, we report accuracy (Acc), as well as the relative error rate reduction (ERR) over a random (chance) baseline, referred to as Rand.
classification	relative error rate reduction (ERR)	For classification, we report accuracy (Acc), as well as the relative error rate reduction (ERR) over a random (chance) baseline, referred to as Rand.
PMI	Dice coefficient	In our experiments, we also found that PMI consistently performs better than two other association measures, the Dice coefficient and the log-likelihood measure.
recognition of out-of-grammar utterances	accuracy	Although the use of an SLM enables recognition of out-of-grammar utterances, resulting in improved speech recognition accuracy, this may not help overall system performance unless the multimodal understanding component itself is made robust to unexpected inputs.
NLG evaluation	BLEU	We review previous work on NLG evaluation and on validation of automatic metrics in NLP, and then present the results of two studies of how well some metrics which are popular in other areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of computer-generated weather forecasts.
NLG evaluation	ROUGE	We review previous work on NLG evaluation and on validation of automatic metrics in NLP, and then present the results of two studies of how well some metrics which are popular in other areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of computer-generated weather forecasts.
machine translation	BLEU	Although a number of previous studies have analyzed correlations between human judgments and automatic evaluation metrics in machine translation and document summarization, much less is known about how well automatic metrics correlate with human judgments in In this article we present two empirical studies of how well BLEU and various other corpus-based metrics agree with human judgments, when evaluating the outputs of several NLG systems that generate texts which describe changes in the wind (for weather forecasts).
referring expression generation	accuracy	The only studies we are aware of which examined how well human judgments predict task-effectiveness of computer-generated texts occurred in the recent Generation Challenges evaluations of referring expression generation, which measured the correlations between human assessments of language quality and adequacy of content with task-performance measures (referent identification time and accuracy).
MT evaluation	BLEU	From an NLG perspective, the most surprising aspect of current MT evaluation is the dominance of BLEU and other automatic corpus-based metrics).
MT	BLEU-like	We are not aware of any studies in MT that have tried to correlate BLEU-like metrics with the results of task-effectiveness studies.
summarization	ROUGE	Although there are automatic corpus-based metrics for summarization such as ROUGE (Lin and Hovy 2003), they do not seem to dominate summarization evaluation in the same way that BLEU-type metrics dominate MT evaluation.
summarization	BLEU-type	Although there are automatic corpus-based metrics for summarization such as ROUGE (Lin and Hovy 2003), they do not seem to dominate summarization evaluation in the same way that BLEU-type metrics dominate MT evaluation.
summarization evaluation	BLEU-type	Although there are automatic corpus-based metrics for summarization such as ROUGE (Lin and Hovy 2003), they do not seem to dominate summarization evaluation in the same way that BLEU-type metrics dominate MT evaluation.
statistical parsing	accuracy	The remainder of this article is organized as follows: Section 2 describes some related models of human syntactic processing using a bounded memory store; Section 3 describes a Hierarchical Hidden Markov Model (HHMM) framework for statistical parsing using this bounded store of incomplete constituents; Section 4 describes the right-corner transform and how it relates conventional phrase structure to incomplete constituents in a bounded memory store; Section 5 describes an experiment to estimate the level of coverage of the Penn Treebank corpus that can be achieved using this transform with various memory limits, given a linguistically motivated binarization of this corpus; and Section 6 gives accuracy results of this bounded-memory model trained on this corpus, given that some amount of incremental prediction (as described earlier) must be involved.
parsing	F-score	Evaluating on this partially binarized data does not seem to unfairly increase parsing performance compared to other published results-quite the contrary: an evaluation using the state-of-the-art Charniak (2000) parser scores about half a point worse on labeled F-score (89.3% vs. 89.9%) when its hypotheses and gold standard trees are converted into this format.
parsing	accuracy	The CKY baseline results appear to be better than those fora baseline probabilistic context-free grammar (PCFG) system reported by using no modifications to the corpus, and no parent or sibling conditioning (see, top) because the binarization process allows the parser to avoid some sparse data effects due to large flat branching structures in the Treebank, resulting in improved parsing accuracy.
HHMM parsing	labeled precision (LP)	The results for HHMM parsing, training, and evaluating on these same binarized trees (modulo right-corner and variable-mapping transforms) were substantially better than binarized CKY, most likely due to the expanded HHMM dependencies on previous (q d t−1 ) and parent (q d−1 t ) variables at each q d t . For example, binarized PCFG probabilities maybe defined in terms of three category symbols A, B, and C: P(A 񮽙 BC | A); whereas some of the HHMM probabilities are defined in terms of five category Labeled recall (LR), labeled precision (LP), weighted average (F-score), and parse failure (% of sentences yielding no tree output) results for basic CKY parser and HHMM parser on unmodified and binarized WSJ Sections 22 (sentences 1-393: "devset") and 23-24 (all sentences).
HHMM parsing	weighted average (F-score)	The results for HHMM parsing, training, and evaluating on these same binarized trees (modulo right-corner and variable-mapping transforms) were substantially better than binarized CKY, most likely due to the expanded HHMM dependencies on previous (q d t−1 ) and parent (q d−1 t ) variables at each q d t . For example, binarized PCFG probabilities maybe defined in terms of three category symbols A, B, and C: P(A 񮽙 BC | A); whereas some of the HHMM probabilities are defined in terms of five category Labeled recall (LR), labeled precision (LP), weighted average (F-score), and parse failure (% of sentences yielding no tree output) results for basic CKY parser and HHMM parser on unmodified and binarized WSJ Sections 22 (sentences 1-393: "devset") and 23-24 (all sentences).
WSD	error	report that each successive doubling of the training data for WSD only leads to a 3-4% error reduction within their experimental range.
SVM	gamma	The SVM training was done using the parameters of cost = 0.1 and gamma = 0.00001 with a Gaussian kernel.
MT training	accuracy	We present three modifications to the MT training data to improve the accuracy of a state-of-the-art syntax MT system: restructuring changes the syntactic structure of training parse trees to enable reuse of substructures; re-labeling alters bracket labels to enrich rule application context; and realigning unifies word alignment across sentences to remove bad word alignments and refine good ones.
Translation	BLEU	 Table 1  Translation accuracy versus binarization algorithms. In this and all other tables reporting BLEU  performance, statistically significant improvements over the baseline are highlighted. p = the  paired bootstrap p-value computed between each system and the baseline, showing the level at  which the two systems are significantly different.
MT	accuracy	 Table 5  Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was  carried out on EM-binarized trees; thus, it already benefits from tree re-structuring. All p-values  are computed against Baseline1.
MT	BLEU	 Table 5  Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was  carried out on EM-binarized trees; thus, it already benefits from tree re-structuring. All p-values  are computed against Baseline1.
paraphrase generation	precision	It is posited that to evaluate any paraphrase generation method, one could simply have it produce its own set of alignments for the sentence pairs in the corpus and precision and recall could then be computed over alignments instead of phrase pairs.
paraphrase generation	recall	It is posited that to evaluate any paraphrase generation method, one could simply have it produce its own set of alignments for the sentence pairs in the corpus and precision and recall could then be computed over alignments instead of phrase pairs.
alignment	accuracy	We examine the influence of constraints on the resulting posterior distributions and find that they are especially effective for increasing alignment accuracy for rare words.
phrase reordering	BLEU score	When combined with BWR, LAR provides complementary information for phrase reordering, which collectively improves the BLEU score significantly.
MT	BLEU	For Arabic-to-English MT, the str-dep model decoder improved BLEU by 1.3 on MT06 and 1.2 on MT08 before 5-gram rescoring.
MT	BLEU	For Chinese-to-English MT, the improvements in BLEU were 1.0 on MT06 and 1.4 on MT08.
translation	accuracy	The results on METEOR and TER suggested that the new model did improve translation accuracy.
TOEFL synonym detection	binomial confidence intervals (CI)	 Table 5  Percentage accuracy in TOEFL synonym detection with 95% binomial confidence intervals (CI).
noun clustering	bootstrapped 95% confidence intervals (CI)	 Table 6  Purity in noun clustering with bootstrapped 95% confidence intervals (CI).
Relation classification	accuracy	 Table 9  Relation classification performance; all measures macro-averaged, except accuracy in the NS and  OC data sets, where we also report the accuracy 95% confidence intervals (CI).
Relation classification	accuracy 95% confidence intervals (CI)	 Table 9  Relation classification performance; all measures macro-averaged, except accuracy in the NS and  OC data sets, where we also report the accuracy 95% confidence intervals (CI).
segmentation task	accuracy	For the segmentation task, we also compare our beam-search framework with alternative decoding algorithms including an exact dynamic-programming method, showing that the beam-search method is significantly faster with comparable accuracy.
parsing	accuracy	For all experiments, we used the EVALB tool 5 for evaluation, and used labeled recall (LR), labeled precision (LP) and F1 score (which is the harmonic mean of LR and LP) to measure parsing accuracy.
segmentation	F-score	When using early update, the algorithm reached the best accuracy at the 30th training iteration, obtaining a segmentation F-score of 91.14% and a joint F-score of 84.06%.
segmentation	F-score	When using early update, the algorithm reached the best accuracy at the 30th training iteration, obtaining a segmentation F-score of 91.14% and a joint F-score of 84.06%.
parsing	accuracy	Like McDonald, Crammer, and , we evaluated the parsing accuracy by the precision of lexical heads (the percentage of input words, excluding punctuation, that have been assigned the correct parent) and by the percentage of complete matches, in which all words excluding punctuation have been assigned the correct parent.
parsing	precision	Like McDonald, Crammer, and , we evaluated the parsing accuracy by the precision of lexical heads (the percentage of input words, excluding punctuation, that have been assigned the correct parent) and by the percentage of complete matches, in which all words excluding punctuation have been assigned the correct parent.
parsing	accuracy	The parsing accuracy was evaluated by the percentage of non-root words that have been assigned the correct head, the percentage of correctly identified root words, and the percentage of complete matches, all excluding punctuation.
prediction	accuracy	In Section 6.2.2 we measure its prediction accuracy.
parsing	accuracy	 Table 1  Labeled parsing accuracy for top-scoring systems at CoNLL-X (Buchholz and Marsi 2006).
parsing	O	If the total number of instantiations is M, parsing is O(M) if there are no cyclic dependencies among instantiations, or, equivalently, if all instantiations can be sorted topologically.
dictionary creation	consistency	Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.
cross-lingual sentiment classification	accuracy	In this study, we first investigate several basic methods for cross-lingual sentiment classification, and then propose a bilingual co-training approach to improve the accuracy of corpus-based polarity classification of Chinese product reviews.
Levenshtein Distances Fail to Identify Language Relationships	Accurately	Levenshtein Distances Fail to Identify Language Relationships Accurately
parsing	F-score	These figures supply a more detailed picture of how performance has changed, showing that although the new brackets make parsing marginally more difficult overall (by about 0.5% in F-score), accuracy on the original structure is only negligibly worse.
parsing	accuracy	These figures supply a more detailed picture of how performance has changed, showing that although the new brackets make parsing marginally more difficult overall (by about 0.5% in F-score), accuracy on the original structure is only negligibly worse.
parsing	O	When a 2-LCFGs is splittable, parsing time can be asymptotically improved to O(|w| 3).
role labeling	exact match scores	We report frame labeling accuracy, role labeling performance, and exact match scores.
cue classification	accuracy	In the current article we present several important extensions to the initial system design of: First, in Section 5, we present a simplified approach to cue classification, greatly reducing the model size and complexity of our Support Vector Machine (SVM) classifier while at the same time giving better accuracy.
cue detection	precision	For the approaches presented for cue detection in this article (for both speculation and negation), we will be reporting precision, recall, and F 1 for three different levels of evaluation; the sentence-level, the token-level, and the cue-level.
cue detection	recall	For the approaches presented for cue detection in this article (for both speculation and negation), we will be reporting precision, recall, and F 1 for three different levels of evaluation; the sentence-level, the token-level, and the cue-level.
cue detection	F 1	For the approaches presented for cue detection in this article (for both speculation and negation), we will be reporting precision, recall, and F 1 for three different levels of evaluation; the sentence-level, the token-level, and the cue-level.
resolving negation scope	percentage of correct scopes (PCS)	A commonly reported measure in the literature on resolving negation scope is the percentage of correct scopes (PCS) as used by, among others.
MN tagger	precision	We describe how our MN lexicon was semi-automatically produced and we demonstrate that a structure-based MN tagger results in precision around 86% (depending on genre) for tagging of a standard LDC data set.
Translation	Bleu metric	Translation quality, as measured by the Bleu metric (, improved when the training process for the Joshua machine translation system () used in the SCALE workshop included MN annotation.
SLM	TAGGER	Similar to SLM, after the parse undergoes headword percolation and binarization, each model component of WORD-PREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences.
parsing	accuracy	These bounds on processing are achieved without reducing the parsing accuracy, and in some cases accuracy improves.
parsing	accuracy	These bounds on processing are achieved without reducing the parsing accuracy, and in some cases accuracy improves.
parsing	accuracy	In the following sections we formally define our approach to finite-state chart constraints and analyze the accuracy of each of the three taggers and their impact on parsing efficiency and accuracy when used to prune the search space of a constituent parser.
Tagging	accuracy	 Table 3  Tagging accuracy on the respective development sets (WSJ Section 24 for English and Penn  Chinese Treebank articles 301-325 for Chinese) for binary classes B, E, and U, for various  Markov orders.
SRL	accuracy	Using the evaluation data from this track, showed that SRL can improve the accuracy of a QA system; a traditional SRL system alone, however, is not enough to recover the implied answer to Question (3): SEC or the agency.
MT	METEOR	 Table 18  MT system performance as measured by METEOR and TER.
MT	TER	 Table 18  MT system performance as measured by METEOR and TER.
parsing	accuracy	In order to test the hypothesis that the existence and placement of the dummy root node can have an impact on parsing accuracy, we performed an experiment using two widely used data-driven dependency parsers, MaltParser.
parsing	accuracy	Indeed, carrying over successful model architectures from English to typologically different languages mostly leads to a substantial drop in parsing accuracy.
parsing	LAS	shows the parsing results for the ILP parsing models in terms of LAS and UAS in comparison to the results of the Bohnet parser (repeated from).
parsing	UAS	shows the parsing results for the ILP parsing models in terms of LAS and UAS in comparison to the results of the Bohnet parser (repeated from).
parsing	accuracy	A similar drop in parsing accuracy is also exhibited in English when moving from the news domain, on which parsers have traditionally been trained, to other genres such as prose, blogs, poetry, product reviews, or biomedical texts, which use different vocabularies and, to some extent, different syntactic rules.
parsing	accuracy	Can morphological information be used effectively in order to improve parsing accuracy?
parsing	accuracy	This method is very effective in improving parsing accuracy.
parsing	accuracy	Using morphological information to improve parsing accuracy.
CASE	accuracy	As we will see, the CASE feature is very relevant and not redundant, but it cannot be predicted with high accuracy and overall it is not useful.
parsing	accuracy	In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.
MSA	accuracy	Similar to previous results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition.
MWE identification	F1	With this representation, the TSG model yields the best MWE identification results for Arabic (81.9% F1) and competitive results for French (71.3%), even though its parsing results lag state-of-the-art probabilistic CFG (PCFG)-based parsers.
segmentation	accuracy	We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (.
segmentation	F1	We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (.
Morphological analysis	accuracy	Morphological analysis accuracy was another experimental resource asymmetry between the two languages.
NE recognition	precision (P)	The answer keys to NE recognition and alignment are annotated manually, and used as the gold standard to calculate the metrics of precision (P), recall (R), and F-score (F) for both NE recognition and alignment.
NE recognition	recall (R)	The answer keys to NE recognition and alignment are annotated manually, and used as the gold standard to calculate the metrics of precision (P), recall (R), and F-score (F) for both NE recognition and alignment.
NE recognition	F-score (F)	The answer keys to NE recognition and alignment are annotated manually, and used as the gold standard to calculate the metrics of precision (P), recall (R), and F-score (F) for both NE recognition and alignment.
holder extraction task	F-measure	(2) The recall was boosted by almost 10 points for the holder extraction task (over three points in F-measure) by modeling the interaction of opinion expressions with respect to holders.
classification	accuracy	It appears that bigrams contribute most to improvements in classification accuracy.
classification	accuracy	We found that extending a unigram representation with statistical and/or linguistic phrases gives a significant improvement in classification accuracy over the unigram baseline.
parsing	accuracy	We end with an experimental evaluation showing that our 2-planar parser gives significant improvements in parsing accuracy over the corresponding 1-planar and projective parsers for data sets with non-projective dependency trees and performs on a par with the widely used arc-eager pseudo-projective parser.
parsing	accuracy	Finally, we show that the 2-planar parser, when evaluated on data sets with a non-negligible proportion of non-projective trees, gives significant improvements in parsing accuracy over the corresponding 1-planar and projective parsers, and provides comparable accuracy to the widely used arc-eager pseudo-projective parser.
parsing	accuracy	Finally, we show that the 2-planar parser, when evaluated on data sets with a non-negligible proportion of non-projective trees, gives significant improvements in parsing accuracy over the corresponding 1-planar and projective parsers, and provides comparable accuracy to the widely used arc-eager pseudo-projective parser.
coreference resolution	precision	The coreference resolution stage is based on a succession often independent coreference models (or "sieves"), applied from highest to lowest precision.
parsing	accuracy	The training and parsing times for our models are reported in, using the same meta-parameters (discussed subsequently) as for the accuracies reported in the previous section, which optimize accuracy at the expense of speed.
parsing	accuracy	We found that parsing accuracy is higher when using a probability model that does not factor in the NONE-adjunction events (parsing accuracy decreases by about 1.5 percentage points in a model that takes these events into account).
parsing	accuracy	We found that parsing accuracy is higher when using a probability model that does not factor in the NONE-adjunction events (parsing accuracy decreases by about 1.5 percentage points in a model that takes these events into account).
parsing	F-score	Without gold-standard POS tags, parsing performance drops to an F-score of Parsing results for the PLTAG parser with gold standard POS tags.
parsing	Parsing	Without gold-standard POS tags, parsing performance drops to an F-score of Parsing results for the PLTAG parser with gold standard POS tags.
word alignment	SRILM	For word alignment, Giza++ (Och and Ney 2003) was used and for language modeling we used SRILM).
SemEval 2007 shared task	precision	The standard evaluation script from the SemEval 2007 shared task calculates precision, recall, and F 1 -measure for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one.
SemEval 2007 shared task	recall	The standard evaluation script from the SemEval 2007 shared task calculates precision, recall, and F 1 -measure for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one.
SemEval 2007 shared task	F 1 -measure	The standard evaluation script from the SemEval 2007 shared task calculates precision, recall, and F 1 -measure for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one.
Frame identification	Precision	 Table 5  Frame identification results on both the SemEval 2007 data set and the FrameNet 1.5 release.  Precision, recall, and F 1 were evaluated under exact and partial frame matching; see Section 3.3.  Bold indicates best results on the SemEval 2007 data, which are also statistically significant with  respect to the baseline (p < 0.05).
Frame identification	recall	 Table 5  Frame identification results on both the SemEval 2007 data set and the FrameNet 1.5 release.  Precision, recall, and F 1 were evaluated under exact and partial frame matching; see Section 3.3.  Bold indicates best results on the SemEval 2007 data, which are also statistically significant with  respect to the baseline (p < 0.05).
Frame identification	F 1	 Table 5  Frame identification results on both the SemEval 2007 data set and the FrameNet 1.5 release.  Precision, recall, and F 1 were evaluated under exact and partial frame matching; see Section 3.3.  Bold indicates best results on the SemEval 2007 data, which are also statistically significant with  respect to the baseline (p < 0.05).
POS tagging	accuracy	Evidence from POS tagging, parsing, and semantic role labeling (SRL), among other NLP tasks, shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training.
parsing	accuracy	Evidence from POS tagging, parsing, and semantic role labeling (SRL), among other NLP tasks, shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training.
semantic role labeling (SRL)	accuracy	Evidence from POS tagging, parsing, and semantic role labeling (SRL), among other NLP tasks, shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training.
POS tagging	accuracy	 Table 4  POS tagging accuracy: the LATTICE-TOKEN-R and other graphical model representations  outperform TRAD-R and state-of-the-art Chinese POS taggers on all target domains. For target  domains, * indicates the performance is statistically significantly better than the Stanford and  TRAD-R baselines at p < 0.05, using a two-tailed χ 2 test; ** indicates significance at p < 0.01.  On the news domain, the Stanford tagger is significantly different from all other systems  using a two-tailed χ 2 test with p < 0.01.
POS tagging	LATTICE-TOKEN-R	 Table 4  POS tagging accuracy: the LATTICE-TOKEN-R and other graphical model representations  outperform TRAD-R and state-of-the-art Chinese POS taggers on all target domains. For target  domains, * indicates the performance is statistically significantly better than the Stanford and  TRAD-R baselines at p < 0.05, using a two-tailed χ 2 test; ** indicates significance at p < 0.01.  On the news domain, the Stanford tagger is significantly different from all other systems  using a two-tailed χ 2 test with p < 0.01.
dialect identification	accuracy	We use the data to train and evaluate automatic classifiers for dialect identification, and establish that classifiers using dialectal data significantly and dramatically outperform baselines that use MSA-only data, achieving near-human classification accuracy.
parsing	accuracy	Although various heuristics have been proposed to deal with this problem, there has so far been no clean theoretical solution that also gives good parsing accuracy.
machine translation evaluation	BLEU	admitted that machine translation evaluation metrics are not sufficient for evaluating stegosystems; for example, BLEU relies on word sequences in the stego sentence matching those in the cover sentence and thus is not suitable for evaluating transformations that change the word order significantly.
steganography	precision	Because imperceptibility is an important issue for steganography, we would prefer a system with a higher precision value.
noun substitutions	precision	Although a higher precision can be achieved by using a higher threshold value-for example, noun substitutions reach almost 90% precision with threshold equal to 0.9-the large drop in recall means many applicable substitutes are being eliminated.
noun substitutions	recall	Although a higher precision can be achieved by using a higher threshold value-for example, noun substitutions reach almost 90% precision with threshold equal to 0.9-the large drop in recall means many applicable substitutes are being eliminated.
identification	accuracy	We report results on English, French, and Hebrew, and demonstrate a significant improvement in identification accuracy, compared with less sophisticated baselines.
classification	accuracy	They clearly demonstrate that the linguistically motivated features we define provide a significant improvement in classification accuracy over the baseline PMI measure.
word segmentation	accuracy	r On the application side, for the three well-known tasks, including named entity recognition, word segmentation, and phrase chunking, the proposed simple method achieves equal or even better accuracy than the existing gold-standard systems, which are complicated and use extra resources.
phrase chunking	accuracy	r On the application side, for the three well-known tasks, including named entity recognition, word segmentation, and phrase chunking, the proposed simple method achieves equal or even better accuracy than the existing gold-standard systems, which are complicated and use extra resources.
word segmentation	accuracy	 Table 4  Results for the Bio-NER, word segmentation, and phrase chunking tasks. The results and the  number of passes are decided based on empirical convergence (with score deviation of adjacent  five passes less than 0.01). For the non-convergent case, we simply report the results based on a  large enough number of training passes. As we can see, the ADF method achieves the best  accuracy with the fastest convergence speed.
phrase chunking tasks	accuracy	 Table 4  Results for the Bio-NER, word segmentation, and phrase chunking tasks. The results and the  number of passes are decided based on empirical convergence (with score deviation of adjacent  five passes less than 0.01). For the non-convergent case, we simply report the results based on a  large enough number of training passes. As we can see, the ADF method achieves the best  accuracy with the fastest convergence speed.
label propagation	significance level  q	 Table 10  Results of agglomerative partitioning and label propagation for cosine and avgmax similarity  on German. For comparison purposes results for English on the gold/gold data set are also  tabulated. All improvements over the baseline are statistically significant at significance level  q < 0.001.
Frame-Semantic Parsing	precision	The authors of the article "Frame-Semantic Parsing" and a graduate student discovered that in rows 7 and 8 of Table 8, at inference time for argument identification with gold frames, the described model included gold spans along with the candidate set of automatic spans (elaborated in Section 6.1), thus creating an oracle, and artificially bloating the precision, recall, and F1 metrics.
Frame-Semantic Parsing	recall	The authors of the article "Frame-Semantic Parsing" and a graduate student discovered that in rows 7 and 8 of Table 8, at inference time for argument identification with gold frames, the described model included gold spans along with the candidate set of automatic spans (elaborated in Section 6.1), thus creating an oracle, and artificially bloating the precision, recall, and F1 metrics.
Frame-Semantic Parsing	F1	The authors of the article "Frame-Semantic Parsing" and a graduate student discovered that in rows 7 and 8 of Table 8, at inference time for argument identification with gold frames, the described model included gold spans along with the candidate set of automatic spans (elaborated in Section 6.1), thus creating an oracle, and artificially bloating the precision, recall, and F1 metrics.
WSD	precision	It is customary in the WSD literature to evaluate the performance of a disambiguation system based on precision, recall, and F1 measure.
WSD	recall	It is customary in the WSD literature to evaluate the performance of a disambiguation system based on precision, recall, and F1 measure.
WSD	F1 measure	It is customary in the WSD literature to evaluate the performance of a disambiguation system based on precision, recall, and F1 measure.
Chinese sentiment lexicon learning	precision	The experimental results on Chinese sentiment lexicon learning show the effectiveness of the proposed approach in terms of both precision and recall.
Chinese sentiment lexicon learning	recall	The experimental results on Chinese sentiment lexicon learning show the effectiveness of the proposed approach in terms of both precision and recall.
Sentiment classification	RAE-pretrain	 Table 4  Sentiment classification results on different data sets; The top three methods are in bold and the  best is also underlined; SVM-m = Support Vector Machine; MNB-m = Multinomial Na¨ıveNa¨ıve Bayes;  LM-m = Language Model; Voting-w/Rev = Voting with negation rules; HardRule = Rule based  method on dependency tree; Tree-CRF = Dependency tree-based method employing conditional  random fields; RAE-pretrain = Recursive autoencoders with pre-trained word vectors;  MV-RNN = Matrix-vector recursive neural network; s.parser-LongMatch = The longest  matching rules are used; s.parser-w/oComb = Without using the combination rules; s.parser =  Our method. Some of the results are missing (indicated by "-") in the table as there is no publicly  available implementation or they are hard to scale up.
MT	ease	Whereas MT has traditionally used a Likert scale score for the criteria of adequacy and fluency, this meta-evaluation noted that these are "seemingly difficult things for judges to agree on"; consequently, asking judges to express a preference between alternative translations is increasingly used on the grounds of ease and intuitiveness.
segmentation	accuracy	Note that the high segmentation accuracy reported by is due to a less stringent evaluation metric.
relation labeling task	F-score	Especially, on the relation labeling task, which is the hardest among the three tasks, we achieve an absolute F-score improvement of 12.2 percentage points, which represents a relative error rate reduction of 37.7%.
relation labeling task	error rate reduction	Especially, on the relation labeling task, which is the hardest among the three tasks, we achieve an absolute F-score improvement of 12.2 percentage points, which represents a relative error rate reduction of 37.7%.
text realization task	BLEU	One advantage of this formulation of the reordering problem, which can perhaps bethought of as a "pure" text realization task, is that systems for solving it are easily evaluated, because all that is required is a set of sentences for reordering and a standard evaluation metric such as BLEU (.
word ordering problem	accuracies	r We present a novel method for solving the word ordering problem that gives the best reported accuracies to date on the standard Wall Street Journal data.
word alignment	accuracy	We propose improvements to existing machine translation-based systems for word alignment, including a novel method of word alignment relying on random walks on a graph that achieves alignment accuracy superior to that of standard expectation maximization-based techniques for word alignment in a fraction of the time required for expectation maximization.
word alignment	accuracy	We propose improvements to existing machine translation-based systems for word alignment, including a novel method of word alignment relying on random walks on a graph that achieves alignment accuracy superior to that of standard expectation maximization-based techniques for word alignment in a fraction of the time required for expectation maximization.
machine translation	BLEU	This similarity can be measured with techniques such as latent semantic analysis (LSA) cosine distance or the summary-level statistics that are widely used in evaluation of machine translation or automatic summarization, such as BLEU, Meteor, or ROUGE.
computational modeling of metaphor	accuracy	Recent years have seen a growing interest in computational modeling of metaphor, with many new statistical techniques opening routes for improving system accuracy and robustness.
metaphor identification	precision	The evaluations of metaphor identification tend to be conducted in terms of precision and recall, and occasionally, accuracy.
metaphor identification	recall	The evaluations of metaphor identification tend to be conducted in terms of precision and recall, and occasionally, accuracy.
metaphor identification	accuracy	The evaluations of metaphor identification tend to be conducted in terms of precision and recall, and occasionally, accuracy.
predicting argument realization	F 1 -score	 Table 9  Results for correctly predicting argument realization. Significant differences from our (full)  model in terms of micro-averaged F 1 -score are marked with asterisks (* p < 0.01).
MT	transparency	A general purpose MT system intended for commercial application should ideally have many features, such as robustness and transparency, in common with any large industrial software implementation.
parsing "	accuracy	However, we have postponed an investigation of parsing "accuracy" until we have begun to elicit actual inputs from realistic users.
parsing	accuracy	 Table 3. As  with the experiments discussed above, these measurements  have limited statistical significance due to the small  sample sizes of grammars and of inputs. However, the  preliminary indication is that checking for nonlocal  compatibility adds more time to parsing than it saves.  This suggests that the benefits of such nonlocal checking  will be primarily in improved accuracy of disambiguations,  rather than speed of parsing.
classification	accuracy	This classification process is difficult and tedious, but it is crucial that it be done with accuracy and consistency.
classification	consistency	This classification process is difficult and tedious, but it is crucial that it be done with accuracy and consistency.
MARCH MORNING Altman-The resolution of local syntactic ambuiguity	FTER	Bestougeff, Ligozat-Parametrised abstract objects for linguistic information processing Salton-On the representation of query term relations by soft Boolean operators 29 MARCH MORNING Altman-The resolution of local syntactic ambuiguity by the human sentence processing mechanism Pulman-A parser that doesn't Delmonte-Parsing difficulties and phonological processing in Italian Izumida et al.-A natural language interface using a world model Berry-Rogghe-Interpreting singular definite descriptions in database queries Bree, Smit-Non-standard uses of if Wehrli-Design and implementation of a lexical database Maistros, Kotsanis-Lexifamis: A lexical analyser of modern Greek Beale-Grammatical analysis by computer of the Lancaster-Oslo/Bergen corpus Fimbel et al.-Using a text model for analysis and generation Gillott-The simulation of stress patterns in synthetic speech-a two level problem Johnston, Altman-Automatic speech recognition: a framework for research .4 FTER NOON Garside-A probabflistic parser Boguraev, Briscoe-Toward a dictionary support environment for real time parsing Koktova-Towards anew type of morphemic analysis Fum et al.-A rule based approach to evaluating importance in descriptive texts Patten-A problem solving approach to generating text from systematic grammar Parisi, Giorgi-GEMS: a model of sentence production McDonald, Pustejovskh-SAMSON: A computational theory of prose style in generation Tait-An English generator fora case-labelled dependency representation Muraki et al.-Augmented dependency grammar Hajicova, Sgall-Towards an automatic identification of topic and focus Mofik-User modelling, dialog structure, and dialog strut-egy in HAM-ANS Narin'Yani, Simonova-Communicative context of dialogue interaction 44
Subject Raising verbs	turnout	The two Subject Raising verbs which were not so classified by the system were come about and turnout.
MT	breadth	But even those who have worked on MT could not have fully appreciated the breadth, depth and impact of MT activity around the globe before reading Hutchins's book Machine Translation: Past, Present, Future.
Computational Analysis of English	ANALYSIS	Book Reviews The Computational Analysis of English: A Corpus-based Approach THE COMPUTATIONAL ANALYSIS OF ENGLISH: A CORPUS-BASED APPROACH
tagging	accuracy	By using both the individual probabilities of different parts of speech fora single word, and the combined probability of sequences of two parts of speech, tagging can be done with 96-97% accuracy.
MT SYSTEMS	METHODOLOGY	MACHINE TRANSLATION: LINGUISTIC CHARACTERISTICS OF MT SYSTEMS AND GENERAL METHODOLOGY OF EVALUATION
dependency analysis	BOOKS RECEIVED	Detailed examples of dependency analysis are offered in several different languages, and the process of metataxis is explained in BOOKS RECEIVED Books listed below that are marked with at will be reviewed in a future issue.
translation	clarity	This approach provides us with an effective basis for improving translation quality by systematically enhancing the transfer rules without sacrificing the clarity and maintainability of the transfer component.
rule compilation	Antworth	1987, Bear 1986, inter alia), though with respect to rule compilation, Antworth is justified in noting (p.
detection theory	correct detection (CD)	Using terminology from detection theory, these are also referred to as correct detection (CD) and false detection (FD) in the following sections.
detection theory	false detection (FD)	Using terminology from detection theory, these are also referred to as correct detection (CD) and false detection (FD) in the following sections.
OSTIA learning  flapping	Error	 Table 2  Unmodified OSTIA learning  flapping on 49,280-word test  set. Error rates are the  percentage of incorrect  transductions.
sentence analysis	accuracy	Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques.
machine translation	coverage rate	For aligned corpora to be useful for NLP tasks such as machine translation and word-sense disambiguation, a coverage rate higher than 60% is desirable, even at the expense of a slightly lower precision rate.
machine translation	precision	For aligned corpora to be useful for NLP tasks such as machine translation and word-sense disambiguation, a coverage rate higher than 60% is desirable, even at the expense of a slightly lower precision rate.
word-sense disambiguation	coverage rate	For aligned corpora to be useful for NLP tasks such as machine translation and word-sense disambiguation, a coverage rate higher than 60% is desirable, even at the expense of a slightly lower precision rate.
word-sense disambiguation	precision	For aligned corpora to be useful for NLP tasks such as machine translation and word-sense disambiguation, a coverage rate higher than 60% is desirable, even at the expense of a slightly lower precision rate.
broadening coverage	precision rate	Experimental results indicate that classification based on existing thesauri is highly effective in broadening coverage while maintaining a high precision rate.
rule induction	precision	The rule induction process is guided by a thorough guessing-rule evaluation methodology that employs precision, recall, and coverage as evaluation metrics.
rule induction	recall	The rule induction process is guided by a thorough guessing-rule evaluation methodology that employs precision, recall, and coverage as evaluation metrics.
rule induction	coverage	The rule induction process is guided by a thorough guessing-rule evaluation methodology that employs precision, recall, and coverage as evaluation metrics.
guessing of proper nouns	error rate	Since, arguably, the guessing of proper nouns is easier than is the guessing of other categories, we also measured the error rate for the subcategory of capitalized unknown words separately.
recognition of phrase boundaries	coverage	ability, in principle, to identify particular types of construction, ranging from recognition of verbs and nouns, through recognition of phrase boundaries, to attachment of prepositional phrases and analysis of coordination and gapping, amongst others; coverage, expressed in terms of the percentage of utterances for which the parser is able to produce some analysis (whether corrector not); efficiency, giving the time taken to analyze the utterances, specifying the type of machine used; and accuracy, measuring the proportion of each type of construction (as in 1, above) that was identified correctly.
recognition of phrase boundaries	accuracy	ability, in principle, to identify particular types of construction, ranging from recognition of verbs and nouns, through recognition of phrase boundaries, to attachment of prepositional phrases and analysis of coordination and gapping, amongst others; coverage, expressed in terms of the percentage of utterances for which the parser is able to produce some analysis (whether corrector not); efficiency, giving the time taken to analyze the utterances, specifying the type of machine used; and accuracy, measuring the proportion of each type of construction (as in 1, above) that was identified correctly.
Subscription Rate	handling	Subscription Information: ISSN 0922-6567 1998, Volume 13 (4 issues) Subscription Rate: NLG 490.00/USD 251.50, including postage and handling.
decision making	accuracy	The time for decision making will be plotted against the accuracy of the answers provided by the judges from the two sets.
supertag disambiguation	accuracy	In this paper, we present vastly improved supertag disambiguation results--from previously published 68% accuracy to 92% accuracy using a larger training corpus and better smoothing techniques.
supertag disambiguation	accuracy	In this paper, we present vastly improved supertag disambiguation results--from previously published 68% accuracy to 92% accuracy using a larger training corpus and better smoothing techniques.
SPOT	BEST	A paired t-test of SPOT to BEST shows that there are significant differences in performance ( ).
WSD	recall	In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), as well as in Information Retrieval (IR), since the diierence in the correct sense assignments aaects recall, precision and other evaluation measures.
WSD	precision	In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), as well as in Information Retrieval (IR), since the diierence in the correct sense assignments aaects recall, precision and other evaluation measures.
Information Extraction (IE)	recall	In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), as well as in Information Retrieval (IR), since the diierence in the correct sense assignments aaects recall, precision and other evaluation measures.
Information Extraction (IE)	precision	In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), as well as in Information Retrieval (IR), since the diierence in the correct sense assignments aaects recall, precision and other evaluation measures.
Machine Translation (MT)	recall	In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), as well as in Information Retrieval (IR), since the diierence in the correct sense assignments aaects recall, precision and other evaluation measures.
Machine Translation (MT)	precision	In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), as well as in Information Retrieval (IR), since the diierence in the correct sense assignments aaects recall, precision and other evaluation measures.
Information Retrieval (IR)	recall	In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), as well as in Information Retrieval (IR), since the diierence in the correct sense assignments aaects recall, precision and other evaluation measures.
Information Retrieval (IR)	precision	In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), as well as in Information Retrieval (IR), since the diierence in the correct sense assignments aaects recall, precision and other evaluation measures.
classification	accuracy	As before, classification accuracy is given on a per-symbol basis; average accuracy per word is around 85%.
classification	accuracy	As before, classification accuracy is given on a per-symbol basis; average accuracy per word is around 85%.
recognition	accuracy	Software licensed from Entropic Laboratory was used for performing recognition, evaluating accuracy and acoustic adaptation.).
ASR	accuracy	For ASR applications where there are significant discrepancies between an utterance and its formal transcription, the inclusion of literal data in the language model can reduce language model perplexity and improve recognition accuracy.
baseNP identification task	precision	In the baseNP identification task, the performance of the systems is usually measured with three rates: precision, recall and . In this paper, we refer to as accuracy.
baseNP identification task	recall	In the baseNP identification task, the performance of the systems is usually measured with three rates: precision, recall and . In this paper, we refer to as accuracy.
baseNP identification task	accuracy	In the baseNP identification task, the performance of the systems is usually measured with three rates: precision, recall and . In this paper, we refer to as accuracy.
Phone recognition	accuracy	 Table 5: Phone recognition accuracy and phone string  classification accuracy (PhoneMM with no rejection) for  increasing values of N max for domain A.
Phone recognition	accuracy	 Table 5: Phone recognition accuracy and phone string  classification accuracy (PhoneMM with no rejection) for  increasing values of N max for domain A.
Phone recognition	accuracy	 Table 6: Phone recognition accuracy and phone string  classification accuracy (PhoneMM with no rejection) for  increasing values of N max for domain B.
Phone recognition	accuracy	 Table 6: Phone recognition accuracy and phone string  classification accuracy (PhoneMM with no rejection) for  increasing values of N max for domain B.
Phone recognition	accuracy	 Table 7: Phone recognition accuracy and phone string  classification accuracy (PhoneMM with no rejection) for  increasing values of N max for domain C.
Phone recognition	accuracy	 Table 7: Phone recognition accuracy and phone string  classification accuracy (PhoneMM with no rejection) for  increasing values of N max for domain C.
answer resolution	precision	Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered, and a 32.8% improvement according to the average precision metric.
parsing	accuracy	This allows use to trade off parsing accuracy for parsing speed, which is a much more important issue than training time.
MT system evaluation	BLEU score	Recently, have proposed an automatic MT system evaluation technique (the BLEU score).
classification	accuracy	For example, given an enormous number of features, the cumulative effect of uncommon features can still have an important effect on classification accuracy, even though infrequent features contribute less information than common features individually.
ASR	maximum a posteriori (MAP) estimation	One of the methods that has received much attention in the ASR literature is maximum a posteriori (MAP) estimation (.
speech recognition	error rate	1 Motivation The move from isolated word to connected speech recognition engendered a qualitative improvement in the naturalness of users' interactions with speech transcription systems, sufficient even to makeup in user satisfaction for some modest increase in error rate.
Tagging	accuracy	 Table 2: Tagging accuracy on the development set with different sequence feature templates.  †All models include the same vertical  word-tag features (t 0 , w 0 and various t 0 , σ (w 1n )), though the baseline uses a lower cutoff for these features.
Tagging	accuracy	 Table 3: Tagging accuracy with different lexical feature templates on the development set.
tagging	accuracy	 Table 4: Final tagging accuracy for the best model on the test set.
summarization and generation	precision	But when the satisfactory output can take many different forms, as in summarization and generation, evaluation by precision and recall is not sufficient.
summarization and generation	recall	But when the satisfactory output can take many different forms, as in summarization and generation, evaluation by precision and recall is not sufficient.
translation	BLEU	 Table 2. Experimental results of translation by BLEU scores
BoosTexter text classification	accuracy	The phone-string output of such recognizers has been used in classification tasks using the BoosTexter text classification algorithm, giving utterance classfication accuracy that is surprisingly close to that obtained using conventionally trained word trigram models requiring transcription.
ASR transcripts	WER	For ASR transcripts with over 45% WER, the system recovers nearly 80% of agree/disagree utterances with a confusion rate of only 3%.
ASR transcripts	confusion rate	For ASR transcripts with over 45% WER, the system recovers nearly 80% of agree/disagree utterances with a confusion rate of only 3%.
classifying speech acts	accuracy	Our work builds on (), which showed that prosodic features are useful for classifying speech acts and lead to increased accuracy when combined with word based cues.
Tagging	Forward-Backward	Tagging is carried out by the standard Forward-Backward algorithm (see e.g. Murphy).
classification	accuracy	We have also conducted the same classification experiments by only using jitter and average OQ two features, and we obtained a classification accuracy of 68.06%.
language understanding	accuracy	FSM provides two strategies for language understanding and have a high accuracy but little robustness and flexibility.
answer extraction	ONG-AE	We fixed the configurations of the other modules (segmented question for the Question2Query module, top 10 hits in the Filter module), except for the AnswerExtraction module, for which we tested the performance while using for answer extraction the NG-AE, M1e-AE, and ONG-AE algorithms.
parsing	accuracy	The purpose of this study to determine if prosodic cues improve parsing accuracy in the same way that punctuation does.
parsing	accuracy	Our starting point is the observation that the Penn treebank annotation of punctuation does significantly improve parsing accuracy.
parsing	accuracy	Thus, given the correlation between the two, and the fact that sentence-internal punctuation tends to be commas, we expected that pause duration, coded in away similar to punctuation, would improve parsing accuracy in the same way that punctuation does.
parsing	accuracy	While it maybe the case that the encoding of prosodic information used in the experiments below is perhaps not optimal and the parser has not been tuned to use this information, note that exactly the same objections could be made to the way that punctuation is encoded and used in modern statistical parsers, and punctuation does in fact dramatically improve parsing accuracy.
parsing	accuracy	We focus in this paper on parsing accuracy in a modern statistical parsing framework, but it is important to remember that prosodic cues might help parsing in other ways as well, even if they do not improve parsing accuracy.
parsing	accuracy	point out that prosodic cues reduce parsing time and increase recognition accuracy when parsing speech lattices with the hand-crafted Verbmobil grammar.
parse selection	accuracy	The performance metric used here is parse selection accuracy as described in section 2.4.
POS tagging	timing	Timing results are reported in seconds of CPU time . POS tagging of the input to the Collins parser took 6 seconds and this was added to the timing result of the Collins parser.
ASR	standard word error rate (WER)	For evaluating ASR performance we use the standard word error rate (WER) as our metric.
generalization	accuracy	Section 6 evaluates our model's generalization performance, accuracy on short passages, and sensitivity to the amount of training data.
Document-level evaluation	mean average precision	Document-level evaluation was done by the traditional IR metrics of mean average precision and precision at various document cutoff points.
Document-level evaluation	precision	Document-level evaluation was done by the traditional IR metrics of mean average precision and precision at various document cutoff points.
IR	mean average precision	Document-level evaluation was done by the traditional IR metrics of mean average precision and precision at various document cutoff points.
IR	precision	Document-level evaluation was done by the traditional IR metrics of mean average precision and precision at various document cutoff points.
coreference resolver	BABAR	We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions.
grammar checking	error rates	In Section 5 we will show that on two very different tasks -grammar checking and a collaborative filtering task -the exponential prior yields lower error rates than the Gaussian.
article generation	TRAINGEN	 Table 2: Contingency table for article generation using  TRAINGEN on DROP0
tokenization	BAccHANT	This paper will introduce the tasks of tokenization, normalization before introducing BAccHANT, a system built for bioscience text normalization.
tokenize	certainty	In order to tokenize, one must be able to tell with certainty when apiece of punctuation ends a token.
Casting tokenization	BAccHANT	Casting tokenization, and by extension normalization, as a classification problem motivates the creation of BAccHANT, a machine learning system designed to normalize bioscience text.
MMR-based feature selection	accuracy	Moreover, MMR-based feature selection sometimes produces some improvements of conventional machine learning algorithms over SVM which is known to give the best classification accuracy.
MMR-based feature selection	accuracy	Using MMR-based feature selection the best accuracy is 82.47%.
MAP adaptation	WER	In this paper, our best scenario, which uses MAP adaptation and the perceptron algorithm in combination, achieves an additional 0.7% reduction, to 19.6% WER.
MAP adaptation	WER	The benefit of MAP adaptation that leads to its superior performance in suggests a hybrid approach, that uses MAP estimation to ensure that good hypotheses are present in the lattices, and the perceptron algorithm to further reduce the WER.
NER	accuracy	Finally, we introduce a method of using n-best hypotheses that yields a small but nevertheless useful improvement NER accuracy.
NER	precision	From, we can conclude that it is possible to improve NER precision by using n-best hypothesis by finding the optimized combination of different acoustic, language model, NER, and other scores.
ASR	precision	In particular, since most errors in Chinese ASR seem to be for person names, using NER score on the n-best hypotheses can improve recognition results by a relative 6.7% in precision and 1.7% in F-measure..
ASR	F-measure.	In particular, since most errors in Chinese ASR seem to be for person names, using NER score on the n-best hypotheses can improve recognition results by a relative 6.7% in precision and 1.7% in F-measure..
Subjective Assessment of Speech System Interfaces (SASSI)	accuracy	This questionnaire was based on the Subjective Assessment of Speech System Interfaces (SASSI) project), which sorts a number of subjective user satisfaction statements (such as "I always knew what to say to the system" and "the system makes few errors") into six relevant factors: system response accuracy, habitability, cognitive demand, annoyance, likeability and speed.
Subjective Assessment of Speech System Interfaces (SASSI)	speed	This questionnaire was based on the Subjective Assessment of Speech System Interfaces (SASSI) project), which sorts a number of subjective user satisfaction statements (such as "I always knew what to say to the system" and "the system makes few errors") into six relevant factors: system response accuracy, habitability, cognitive demand, annoyance, likeability and speed.
Retrieving documents or passages relevant to a user query	accuracy	Retrieving documents or passages relevant to a user query is significantly easier when the words in the query are contained in the document; when a query word is misrecognized by the ASR system, retrieval accuracy declines.
SU detection	accuracy	By varying the SU segmentation of the test data for our system, we gain insight into how the performance of SU detection changes the overall accuracy of the parser.
SVM classifiers	precision	All SVM classifiers, for POS tagging, syntactic phrase chunking and semantic argument labeling, were realized using the TinySVM 3 with the polynomial kernel of degree 2 and the general purpose SVM based chunker YamCha . The results were evaluated using precision and recall numbers along with the F metric.
SVM classifiers	recall	All SVM classifiers, for POS tagging, syntactic phrase chunking and semantic argument labeling, were realized using the TinySVM 3 with the polynomial kernel of degree 2 and the general purpose SVM based chunker YamCha . The results were evaluated using precision and recall numbers along with the F metric.
SVM classifiers	F metric	All SVM classifiers, for POS tagging, syntactic phrase chunking and semantic argument labeling, were realized using the TinySVM 3 with the polynomial kernel of degree 2 and the general purpose SVM based chunker YamCha . The results were evaluated using precision and recall numbers along with the F metric.
POS tagging	precision	All SVM classifiers, for POS tagging, syntactic phrase chunking and semantic argument labeling, were realized using the TinySVM 3 with the polynomial kernel of degree 2 and the general purpose SVM based chunker YamCha . The results were evaluated using precision and recall numbers along with the F metric.
POS tagging	recall	All SVM classifiers, for POS tagging, syntactic phrase chunking and semantic argument labeling, were realized using the TinySVM 3 with the polynomial kernel of degree 2 and the general purpose SVM based chunker YamCha . The results were evaluated using precision and recall numbers along with the F metric.
POS tagging	F metric	All SVM classifiers, for POS tagging, syntactic phrase chunking and semantic argument labeling, were realized using the TinySVM 3 with the polynomial kernel of degree 2 and the general purpose SVM based chunker YamCha . The results were evaluated using precision and recall numbers along with the F metric.
SMT	precision	Experimental results are shown in terms of BLEU scores of a phrase-based SMT system with the capitalization model incorporated, and in terms of capitalization precision.
translations of single word paraphrases	accuracy	In the case of the translations of single word paraphrases for the Spanish accuracy ranged from just below 50% to just below 70%.
DTs	BLEU	The advantage of DTs as measured by difference between the score of the best DT system and the best DP system is 0.75 BLEU at 1x and 0.5 BLEU at 4x.
DTs	BLEU	The advantage of DTs as measured by difference between the score of the best DT system and the best DP system is 0.75 BLEU at 1x and 0.5 BLEU at 4x.
RTE task	agreement rate	Despite the informality of the problem definition, human judges exhibit very good agreement on the RTE task, with agreement rate of 91-96% (.
MT	BLEU	MT output is evaluated using the standard MT evaluation metric BLEU ().
MT evaluation	BLEU	MT output is evaluated using the standard MT evaluation metric BLEU ().
phrasebased translation	BLEU score	Finally, we show that word alignments from our system can be used in a phrasebased translation system to modestly improve BLEU score.
extracting dependencies from lexical category sequences	precision	A simple method is used for extracting dependencies from lexical category sequences, resulting in high precision, yet incomplete and noisy data.
parsing	accuracy	As we add more data, it becomes clear that the maximum benefit to parsing accuracy by strictly adding rerankerbest sentences is about 0.7% and that f -scores will asymptote around 91.0%.
parsing	f -scores	As we add more data, it becomes clear that the maximum benefit to parsing accuracy by strictly adding rerankerbest sentences is about 0.7% and that f -scores will asymptote around 91.0%.
parsing	accuracy	In, we see that the new NANC data contains some information orthogonal to the reranker and improves parsing accuracy of the reranking parser.
parsing	accuracy	We present experiments showing that with our algorithm the workload (as measured by the total number of constituents processed) is decreased by a factor often with no decrease in parsing accuracy compared to standard CKY parsing with the original PCFG.
SVM classifier	accuracy	This baseline SVM classifier with a very small training set achieves 81% accuracy on clean read speech, but only 35% on the broadcast news speech.
translations	BLEU	In the final step, we score our translations with 4-gram BLEU ().
dialog tagging	accuracy	We evaluate our methods on the ICSI meeting recorder dialog act (MRDA) () corpus, and find that our novel hidden backoff model can significantly improve dialog tagging accuracy.
parsing	accuracy	Grammar nonterminals can be split to encode richer dependencies in a stochastic model and improve parsing accuracy.
parsing disambiguation	AUX MD tag sequence	The fifth ranked rule presumably does not add much information to aid parsing disambiguation, since the AUX MD tag sequence is unlikely . The eighth ranked production is the first with a factored category, ruling out coordination between NN and NP.
summaries of meeting recordings	precision score	This paper also introduces anew evaluation scheme for automatic summaries of meeting recordings, using a weighted precision score based on multiple human annotations of each meeting transcript.
speech recognition transcripts	recall	Directly searching such inaccurate speech recognition transcripts suffers from a poor recall.
SRL	accuracy	We also discuss the impact of using publicly available manually annotated verb data to improve the SRL accuracy of nouns, exploiting a widely-held assumption that verbs and their nominalizations share the same predicate-argument structure.
SRL	accuracy	Finally, we discuss the results of applying reranking techniques to improve SRL accuracy for nominalized predicates, which showed insignificant improvement.
SRL	accuracy	We also discuss whether verb data can be used to improve the SRL accuracy of nominalized predicates.
WASP	accuracy	We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.
WASP	coverage	We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.
WASP	accuracy	In initial evaluation on several real-world data sets, we show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring the same amount of supervision, and shows better robustness to variations in task complexity and word order.
WASP	coverage	In initial evaluation on several real-world data sets, we show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring the same amount of supervision, and shows better robustness to variations in task complexity and word order.
WASP	recall	WASP's performance is consistent across these languages despite some slight differences, most probably due to factors other than word order (e.g. lower recall for Turkish due to a much larger vocabulary).
summary comparison	ROUGE-1	This tiered design for summary comparison guarantees at least a ROUGE-1 level of summary content matching if no paraphrases are found.
MT evaluation	BLEU	This strategy is commonly used in MT evaluation, because of BLEU's well-known problems with documents of small size ().
recognition	accuracy	Moreover, recognition accuracy is still around 30% on spontaneous speech tasks, in contrast to speech read from text such as broadcast news.
Example Based Machine Translation (EBMT)	accuracy	Prior work has shown that generalization of data in an Example Based Machine Translation (EBMT) system, reduces the amount of pre-translated text required to achieve a certain level of accuracy (Brown, 2000).
translation	accuracy	The hypothesis is that better con-textual clustering can lead to better translation accuracy with limited training data.
Spectral clustering	BLEU score	Spectral clustering is seen to be superior to Group Average Clustering (GAC)) both in terms of semantic similarity of words falling in a single cluster, and overall BLEU score () in a large scale EBMT system.
classification	accuracy	They all aim to improve classification accuracy by exploiting more readily available unlabeled data as well as labeled examples.
classification	accuracy	The algorithms achieve classification accuracy of about 80% (compared to the base-line of around 50%) using the human transcriptions and manually labeled speaker turns.
NER	F 1 measure	While performance of NER systems is often evaluated in terms of F 1 measure (a harmonic mean of precision and recall), this measure may not match user preferences regarding precision and recall.
NER	precision	While performance of NER systems is often evaluated in terms of F 1 measure (a harmonic mean of precision and recall), this measure may not match user preferences regarding precision and recall.
NER	recall	While performance of NER systems is often evaluated in terms of F 1 measure (a harmonic mean of precision and recall), this measure may not match user preferences regarding precision and recall.
NER	precision	While performance of NER systems is often evaluated in terms of F 1 measure (a harmonic mean of precision and recall), this measure may not match user preferences regarding precision and recall.
NER	recall	While performance of NER systems is often evaluated in terms of F 1 measure (a harmonic mean of precision and recall), this measure may not match user preferences regarding precision and recall.
NER	F1	Furthermore, learned NER models maybe sub-optimal also in terms of F1, as they are trained to optimize other measures (e.g., loglikelihood of the training data for CRFs).
NER	precision	Obviously, different applications of NER have different requirements for precision and recall.
NER	recall	Obviously, different applications of NER have different requirements for precision and recall.
anonymization of medical records	recall	In some domains, such as anonymization of medical records, high recall is essential.
parsing	accuracy	If the internal semantics of a predicate determines the syntactic expressions of constituents bearing a semantic role, it is then reasonable to expect that knowledge about semantic roles in a sentence will be informative of its syntactic structure, and that learning semantic role labels at the same time as parsing will be beneficial to parsing accuracy.
parsing task	accuracy	We achieve promising results both on the simple parsing task, where the accuracy of the parser is measured on the standard Parseval measures, and also on the parsing task where more complex labels comprising both syntactic labels and semantic roles are taken into account.
parsing	accuracy	() report a reduction in parsing accuracy of an unlexicalised PCFG from 77.8% to 72.9% in using Penn Treebank function labels in training.
parsing	recall	To evaluate the former parsing task, we compute the standard Parseval measures of labelled recall and precision of constituents, taking into account not only the 33 original labels, but also the newly introduced PropBank labels.
parsing	precision	To evaluate the former parsing task, we compute the standard Parseval measures of labelled recall and precision of constituents, taking into account not only the 33 original labels, but also the newly introduced PropBank labels.
PropBank parsing task	accuracy	Moreover, the results indicate that we can perform the more complex PropBank parsing task at levels of accuracy comparable to those achieved by the best semantic role labellers (PropBank column).
MSA	LH	For MSA, the only test of LH has been the work of, arguing for Middle and Unaccusative alternations in Arabic.
OOV recognition	IV rate	While OOV recognition is very important in word segmentation, a higher IV rate is also desired.
word segmentation	IV rate	While OOV recognition is very important in word segmentation, a higher IV rate is also desired.
segmentation	recall(R)	Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv).
segmentation	precision(P)	Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv).
segmentation	F-score(F)	Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv).
segmentation	OOV rate(R-oov)	Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv).
segmentation	IV rate(R-iv)	Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv).
segmentation	R-oov rates	 Table 1: Our segmentation results by the dictionary- based approach for the closed test of Bakeoff 2005, very  low R-oov rates due to no OOV recognition applied.
phrase extraction	accuracy	Lexical relationships under the standard IBM models () do not account for many-to-many mappings, and phrase extraction relies heavily on the accuracy of the IBM word-toword alignment.
MT input	speed	This would allow bilingual speakers to correct MT input and get rewards for making good corrections, and compare their scores and speed with other users.
recognition of protein names	F	Through these broad linguistic features and the nature of CRF, our system outperforms state-of-the-art machine-learning-based systems, especially in the recognition of protein names (F=78.5%).
semantic role labeling (SRL)	extent	This problem can be tackled by using semantic role labeling (SRL) because it not only recognizes main roles, such as agents and objects, but also extracts adjunct roles such as location, manner, timing, condition, and extent.
SRL	F-score increase	() has demonstrated that full-parsing and SRL can improve the performance of relation extraction, resulting in an F-score increase of 15% (from 67% to 82%).
relation extraction	F-score increase	() has demonstrated that full-parsing and SRL can improve the performance of relation extraction, resulting in an F-score increase of 15% (from 67% to 82%).
MT	BLEU	For every MT hypothesis, BLEU computes the fraction of n-grams which also appear in the reference sentences, as well as a brevity penalty.
SIA	BLEU	 Table 1. As a comparison, we also show the re- sults of BLEU, NIST, METEOR, ROUGE, WER,  and HWCM. For METEOR and ROUGE, WORD- NET and PORTER-STEMMER are enabled, and for  SIA, the decay factor is set to 0.6. The number  in brackets, for BLEU, shows the n-gram length it  counts up to, and for SSCN, shows the length of the  n-gram it uses. In the table, the top 3 results in each  column are marked bold and the best result is also  underlined. The results show that the SSCN2 met- rics are better than the SSCN1 metrics in adequacy  and overall score. This is understandable since what  SSCN metrics need is which words in the source  sentence are aligned to an n-gram in the MT hy- pothesis/references. This is directly modeled in the  alignment used in SSCN2. Though we could also  get such information from the reverse alignment, as  in SSCN1, it is rather an indirect way and could con- tain more noise. It is interesting that SSCN1 gets  better fluency evaluation results than SSCN2. The  SSCN metrics with the unioned constraint, SSCN u,  by combining the strength of SSCN1 and SSCN2,  get even better results in all three aspects. We can  see that SSCN metrics, even without stochastic word  mapping, get significantly better results than their  relatives, BLEU, which indicates the source sen- tence constraints do make a difference. SSCN2 and  SSCN u are also competitive to the state-of-art MT  metrics such as METEOR and SIA. The best SSCN  metric, pSSCN u(2), achieves the best performance  among all the testing metrics in overall and ade- quacy, and the second best performance in fluency,  which is just a little bit worse than the best fluency  metric SIA.
SIA	BLEU	 Table 1. As a comparison, we also show the re- sults of BLEU, NIST, METEOR, ROUGE, WER,  and HWCM. For METEOR and ROUGE, WORD- NET and PORTER-STEMMER are enabled, and for  SIA, the decay factor is set to 0.6. The number  in brackets, for BLEU, shows the n-gram length it  counts up to, and for SSCN, shows the length of the  n-gram it uses. In the table, the top 3 results in each  column are marked bold and the best result is also  underlined. The results show that the SSCN2 met- rics are better than the SSCN1 metrics in adequacy  and overall score. This is understandable since what  SSCN metrics need is which words in the source  sentence are aligned to an n-gram in the MT hy- pothesis/references. This is directly modeled in the  alignment used in SSCN2. Though we could also  get such information from the reverse alignment, as  in SSCN1, it is rather an indirect way and could con- tain more noise. It is interesting that SSCN1 gets  better fluency evaluation results than SSCN2. The  SSCN metrics with the unioned constraint, SSCN u,  by combining the strength of SSCN1 and SSCN2,  get even better results in all three aspects. We can  see that SSCN metrics, even without stochastic word  mapping, get significantly better results than their  relatives, BLEU, which indicates the source sen- tence constraints do make a difference. SSCN2 and  SSCN u are also competitive to the state-of-art MT  metrics such as METEOR and SIA. The best SSCN  metric, pSSCN u(2), achieves the best performance  among all the testing metrics in overall and ade- quacy, and the second best performance in fluency,  which is just a little bit worse than the best fluency  metric SIA.
MT	BLEU	Our best performing model achieves a statistically significant improvement over the baseline MT system according to the BLEU metric.
translation	BLEU	These results demonstrate that the proposed model is effective at improving the translation quality according to the BLEU score.
coreference evaluation	precision	B 3 is common in coreference evaluation and is similar to the precision and recall of coreferent links, except that systems are rewarded for singleton clusters.
coreference evaluation	recall	B 3 is common in coreference evaluation and is similar to the precision and recall of coreferent links, except that systems are rewarded for singleton clusters.
SRM	mean average	In contrast, the SRM approach achieves a mean average precision of over twenty percent.
SR	O	The paper presents a scalable, fully-implemented system for SR that runs in O(KN log N) time in the number of extractions N and the maximum number of synonyms per word, K. The system, called RESOLVER, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them.
SR	RESOLVER	The paper presents a scalable, fully-implemented system for SR that runs in O(KN log N) time in the number of extractions N and the maximum number of synonyms per word, K. The system, called RESOLVER, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them.
relation extraction	precision	We apply this hypothesis to define a post-processing module for the output of Espresso, a state of the art relation extraction system, showing that irrelevant and erroneous relations can be filtered out by our module, increasing the precision of the final output.
SMT	PHARAOH	In this paper we present results on using a recent phrase-based SMT system, PHARAOH (, for NLG.
MT	precision	Both MT metrics measure the precision of a translation in terms of the proportion of n-grams that it shares with the reference translations, with the NIST score focusing more on n-grams that are less frequent and more informative.
word sense disambiguation	accuracy	Through word sense disambiguation experiments performed on the Wikipedia-based sense tagged corpus generated fora subset of the SENSE-VAL ambiguous words, we show that the Wikipedia annotations are reliable, and the quality of a sense tagging classifier built on this data set exceeds by a large margin the accuracy of an informed baseline that selects the most frequent word sense by default.
label propagation	LP	For both tasks we compare the performance of a supervised classifier, label propagation using the standard input features and either Euclidean or cosine distance, and LP using the output from a first-pass supervised classifier.
translation	accuracy	These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge.
machine translation	reliability	Whether the task is machine translation or named-entity recognition, the amount of data one has to train or test with can greatly impact the reliability and robustness of one's models, results and conclusions.
Morphology modeling	outof-vocabulary (OOV) rate	Morphology modeling aims to reduce the outof-vocabulary (OOV) rate as well as data sparsity, thereby producing more effective language models.
parsing	accuracy	First, we want to investigate whether allowing non-projective structures to be derived incrementally can improve parsing accuracy compared to a strictly projective baseline.
parsing	TR	 Table 3: Parsing time; PT = parsing time (s); TR = time reduction w.r.t. non-projective baseline (%)
parsing	accuracy	In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy.
parses	accuracy	Thresholds are automatically tuned on heldout data, and the final system parses up to 100 times faster than the baseline PCFG parser, with no loss in test set accuracy.
relation classification	accuracy	Using human-annotated and automatically-extracted test sets, we find that each of these techniques results in improved relation classification accuracy.
MT evaluation	Bleu metric	Automatic MT evaluation methods, as represented by the well-known Bleu metric (), assume the availability of human reference translations.
translation	Input type BLEU[%] Time [sec]  Single best	 Table 5: EPPS task: translation quality and time for  different input conditions (CN=confusion network,  time in seconds per sentence).  Input type BLEU[%] Time [sec]  Single best  37.6  2.7  CN pruned  38.5  4.8  full  38.9  9.2
QA	recall	If other stages of QA rely on a large number of candidates, a high recall value maybe desired so no potential answers are missed.
answer typing	precision	If answer typing is used as a means of boosting already-likely answers, high precision may instead be favoured.
Answer selection	accuracy	Answer selection performance was measured by average accuracy: the number of correct top answers divided by the number of questions whereat least one correct answer exists in the candidate list provided by an extractor.
Information extraction of entities and events within these documents	recall	Information extraction of entities and events within these documents is then used to pinpoint highly relevant sentences and associated words are selected to revise the query fora second pass of retrieval, improving recall.
answer assessment	TREC-QA track	For the answer assessment, we followed the TREC-QA track) and NTCIR to annotate answers in the pool that collected from the outputs of different passage retrieval methods.
ASR	word error rate (WER)	The aim of system combination for ASR is to minimize the expected word error rate (WER) given multiple system outputs, which are ideally annotated with word confidence information.
speech recognition	accuracy	We examine the effect of leveraging one particular type of external information, namely the written agendas and meeting minutes, and we demonstrate that, by using off-line language model adaptation techniques, these can significantly (p < 0.01) improve language modeling and speech recognition accuracy.
VAD post-processing	cross-channel correlation (XC)	A feature which has attracted attention since its use in VAD post-processing in () is the maximum cross-channel correlation (XC), max τ φ jk (τ ), between channels j and k, where τ is the lag.
VAD	WERs	 Table 2: VAD errors, measured at three points in our  system, and first-pass WERs for rt05s eval (05),  as well as first-pass WERs for rt05s eval* (05*)  and rt06s eval (06). Results are shown for 3 con- trastive VAD systems (ILAave(3), ILAave(2) and  ILAmin
tagging	accuracy	Evaluation shows that the average tagging accuracy is 91.54% and 90.44%, obtained by IceTagger and TnT, respectively.
tagging	accuracy	The average tagging accuracy of IceTagger is 91.54%, compared to 90.44% achieved by the TnT tagger, a state-of-the-art statistical tagger).
tagging	accuracy	 Table 2: Average tagging accuracy of the various  taggers.
speaker adaptation	ROUNDED	This paper presents away to perform speaker adaptation for automatic speech recognition using the stream weights in a multi-stream setup, which included acoustic models for "Articulatory Features" such as ROUNDED or VOICED.
morphological segmentation	OOV rate	Hence, morphological segmentation reduces the OOV rate by 1.3% (15% relative), which is not as large reduction as compared to training data (about 40% relative reduction).
translation	BLEU score	We measure translation performance by the BLEU score () with one reference for each hypothesis.
IE	F-measure	Stevenson's and Greenwood's approach provides some of the best available results in weakly supervised IE to date, with 0.58 F-measure.
phrase translation table	BLEU	This method not only reduces the size of the phrase translation table, but also improves translation quality as measured by the BLEU metric.
Translation	BLEU	Translation quality is evaluated according to the BLEU metric (with one reference translation).
summarize speech	accuracy	These findings enable us to summarize speech without placing astringent demand on speech recognition accuracy.
summarize speech	accuracy	This finding makes it possible to summarize speech without placing astringent demand on the speech recognition accuracy.
Disambiguation of	accuracy	Disambiguation of a limited number of words is not hard, and necessary context information can be carefully collected and hand-crafted to achieve high disambiguation accuracy as shown in.
segmentation	recall	Four metrics were used to evaluate segmentation results: recall (R, the percentage of gold standard output words that are correctly segmented by the decoder), precision (P , the percentage of words in the decoder output that are segmented correctly), balanced F-score (F ) defined by 2P R/(P + R), recall of OOV words (R-oov).
segmentation	precision	Four metrics were used to evaluate segmentation results: recall (R, the percentage of gold standard output words that are correctly segmented by the decoder), precision (P , the percentage of words in the decoder output that are segmented correctly), balanced F-score (F ) defined by 2P R/(P + R), recall of OOV words (R-oov).
segmentation	F-score (F )	Four metrics were used to evaluate segmentation results: recall (R, the percentage of gold standard output words that are correctly segmented by the decoder), precision (P , the percentage of words in the decoder output that are segmented correctly), balanced F-score (F ) defined by 2P R/(P + R), recall of OOV words (R-oov).
segmentation	recall	Four metrics were used to evaluate segmentation results: recall (R, the percentage of gold standard output words that are correctly segmented by the decoder), precision (P , the percentage of words in the decoder output that are segmented correctly), balanced F-score (F ) defined by 2P R/(P + R), recall of OOV words (R-oov).
Alignment	Alignment Error Rate (AER))	Alignment quality was evaluated by computing Alignment Error Rate (AER)) relative to the manual alignments.
translation	accuracy	 Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM  B scores.  *  or  *  *  = significantly better than MERT baseline (p < 0.05 or 0.01, respectively).
translation	IBM  B scores	 Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM  B scores.  *  or  *  *  = significantly better than MERT baseline (p < 0.05 or 0.01, respectively).
translation	MERT baseline	 Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM  B scores.  *  or  *  *  = significantly better than MERT baseline (p < 0.05 or 0.01, respectively).
NER classifiers	accuracy	Trained NER classifiers in the source domain usually lose accuracy in anew target domain when the data distribution is different between both domains.
LaSA-based adaptation	accuracy	Experimental results on Chinese corpus also show that LaSA-based adaptation effectively increases the accuracy in all the tests (see).
word segmentation	accuracy	Cumulatively these changes make a significant difference in word segmentation accuracy: our final adaptor grammar performs unsupervised word segmentation with an 87% token f-score on the standard Brent version of the Bernstein-Ratner corpus, which is an error reduction of over 35% compared to the best previously reported results on this corpus.
word segmentation	error reduction	Cumulatively these changes make a significant difference in word segmentation accuracy: our final adaptor grammar performs unsupervised word segmentation with an 87% token f-score on the standard Brent version of the Bernstein-Ratner corpus, which is an error reduction of over 35% compared to the best previously reported results on this corpus.
parsing	F1	The joint representation also allows the information from each type of annotation to improve performance on the other, and, in experiments with the OntoNotes corpus, we found improvements of up to 1.36% absolute F1 for parsing, and up to 9.0% F1 for named entity recognition.
parsing	F-score	Looking at the parsing improvements on a per-label basis, the largest gains came from improved identication of NML consituents, from an F-score of 45.9% to 57.0% (on all the data combined, fora total of 420 NML constituents).
ASR	word error rate (WER)	Most closely related, show results for an ASR which fixes its results after a given time ∆ and report the corresponding word error rate (WER).
recognition	accuracy	This gives relatively high recognition accuracy.
phone recognition	accuracy	We obtain a significant improvement in absolute accuracy in phone recognition of 3.77%-7.29% and a significant improvement of 4.1% in absolute accuracy in ASR.
phone recognition	accuracy	We hypothesize that improved pronunciation rules will have a profound impact on phone recognition accuracy.
phone recognition (XPR)	BASEPR	To compare our phone recognition (XPR) system with the baseline (BASEPR), we train two phone recognizers using HTK.
XPR recognizer	BASEPR	Unsurprisingly, we find that the XPR recognizer significantly (p-value <2.2e − 16) outperforms BASEPR when using these two variants under both conditions (see, third and fourth lines).
ASR	BASEWR	Using these dictionaries, we build the baseline ASR system (BASEWR).
SMT	PORTAGE	The SMT system we applied in our experiments is PORTAGE ( model which assigns a penalty based on the number of source words which are skipped when generating anew target phrase, and (d) a word penalty.
Phrase-based utility selection	BLEU	 Table 2: Phrase-based utility selection is compared  with random sentence selection baseline with respect to  BLEU, wer (word error rate), and per (position indepen- dent word error rate) across three language pairs.
Phrase-based utility selection	wer (word error rate)	 Table 2: Phrase-based utility selection is compared  with random sentence selection baseline with respect to  BLEU, wer (word error rate), and per (position indepen- dent word error rate) across three language pairs.
speech recognition	accuracy	Not surprisingly, we have not observed improvements in speech recognition accuracy.
SMT	BLEU	The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (), NIST) and METEOR () scores.
SMT	METEOR	The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (), NIST) and METEOR () scores.
SMT	BLEU	 Table 1: Effect of count-based pruning on SMT per- formance using EAN corpus. Results are according to  BLEU, NIST and METEOR (MET) metrics. Bold #s are  not statistically significant worse than exact model.
SMT	METEOR (MET)	 Table 1: Effect of count-based pruning on SMT per- formance using EAN corpus. Results are according to  BLEU, NIST and METEOR (MET) metrics. Bold #s are  not statistically significant worse than exact model.
SMT	BLEU	 Table 9: Evaluating SMT with different LMs on EAN.  Results are according to BLEU, NIST and MET metrics.  Bold #s are not statistically significant worse than exact.
Case Frame Construction	Time	 Table 6: Corpus Sizes for Case Frame Construction and  Time for Zero Anaphora Resolution.
Classification	F-Score	 Table 3: Classification Performance (F-Score) by  Relation: ILP on Set A
Dependency parsing	accuracy	 Table 3: Dependency parsing results for each of the domain adaptation models. Performance is measured as unlabeled  attachment accuracy.
EM	accuracies	In the results to follow, we first demonstrate that online EM is faster (Section 4.1) and sometimes leads to higher accuracies (Section 4.2).
parsing	accuracy	These worst-case bounds on processing are demonstrated to be achieved without reducing the parsing accuracy, in fact in some cases improving the accuracy.
SMT	accuracy	Over the past decade, we have seen an accumulation of evidence that SMT accuracy can be improved via tree-structured and syntactic models (e.g.,, and more recently, work from lexical semantics has also at long last been successfully applied to increasing SMT accuracy, in the form of techniques adapted from word sense disambiguation models.
SMT	BLEU score	The phrase-based SMT model used for the first pass achieves a BLEU score of 42.99, establishing a fairly strong baseline to begin with.
statistical machine translation (SMT)	BLEU	We propose a variation of simplex-downhill algorithm specifically customized for optimizing parameters in statistical machine translation (SMT) de-coder for better end-user automatic evaluation metric scores for translations, such as versions of BLEU, TER and mixtures of them.
statistical machine translation (SMT)	TER	We propose a variation of simplex-downhill algorithm specifically customized for optimizing parameters in statistical machine translation (SMT) de-coder for better end-user automatic evaluation metric scores for translations, such as versions of BLEU, TER and mixtures of them.
MT	BLEU	Due to limited variations in the N-Best list, the nature of ranking, and more importantly, the non-differentiable objective functions used for MT (such as BLEU ()), one often found only local optimal solutions to λ, with no clue to walkout of the riddles.
query expansion	recall	An accurate metric of query similarities is useful for query expansion, to improve recall in Information Retrieval systems; for query suggestion, to propose to the user related queries that might help reach the desired information more quickly; and for sponsored search, where advertisers bid for keywords that maybe different but semantically equivalent to user queries.
classification	accuracy	We report average classification accuracy and average Cohen's Kappa using 10-fold cross-validation.
IR	accuracy	While IR techniques might be useful to improve the selection accuracy, the current paper demonstrates that they are not necessary to obtain parallel sentence pairs.
Document classification	F 2  scores	 Table 2: Document classification results for the Tech-TC- 300 data set. The column F 2 shows the average of F 2  scores for each method of classification.
prediction	accuracy	We first computed the prediction accuracy over five classes (the regression output was rounded to the nearest integer).
speech understanding	accuracy	Instead, we have designed and developed anew framework that uses multiple LMs and LUMs to improve speech understanding accuracy under various situations.
speech understanding	accuracy	We empirically show that our method improves speech understanding accuracy.
ASR	accuracy	The work is different from our study in the following two points: it does not deal with speech understanding, and it assumes that each ASR is welldeveloped and achieves high accuracy fora variety of speech inputs.
speech understanding	accuracy	These results show that using multiple LMs and multiple LUMs can potentially improve speech understanding accuracy.
Speech recognition (ASR)	accuracy	Speech recognition (ASR) accuracy in limited input systems is better than in flexible input systems (.
ASR	accuracy	With user adaptation, in flexible input dialog systems prompts can be formulated to maximize ASR accuracy and reduce the number of ASR timeouts.
translation	Translation Error Rate (TER)	The automatic assessment of the translation quality has been carried out using the BiLingual Evaluation Understudy (BLEU) (), and the Translation Error Rate (TER) ().
adverbial positioning	accuracy	We find that: (a), one-and two-stage classification-based approaches can achieve almost 86% accuracy in determining the absolute position of adverbials; (b) a classifier trained with only syntactic features gives performance close to that of a classifier trained with all features; and (c) a surface realizer incorporating a two-stage classifier for adverbial positioning as the second stage gives improvements of at least 10% in simple string accuracy over a baseline real-izer for sentences containing adverbials.
statistical significance	length	For statistical significance, a different random sample is used for length histogram in each replication of experiment.
speech recognition	accuracy	We demonstrate that by incorporating constraints from the information repository that is being searched not only improves the speech recognition accuracy but also results in higher search accuracy.
speech recognition	accuracy	We demonstrate that by incorporating constraints from the information repository that is being searched not only improves the speech recognition accuracy but also results in higher search accuracy.
ASR problem	accuracy	However, in contrast to most of these systems that treat speech-driven search to be largely an ASR problem followed by a Search problem, in this paper, we show the benefits of tightly coupling ASR and Search tasks and illustrate techniques to improve the accuracy of both components by exploiting the co-constraints between the two components.
IR	accuracy	Traditional IR approaches, on the other hand, may naively use the terms in a document which can significantly hamper accuracy.
IQA	FU Q	Therefore, a first task in IQA is to detect whether a FU Q is a topic shift or a topic continuation ().
SMT	accuracy	At 95% confidence level, MIRA/PTEM outperform the SMT model in 1-best accuracy and MIRA outperforms PTEM/SMT in 5-best accuracy.
SMT	MIRA	At 95% confidence level, MIRA/PTEM outperform the SMT model in 1-best accuracy and MIRA outperforms PTEM/SMT in 5-best accuracy.
SMT	accuracy	At 95% confidence level, MIRA/PTEM outperform the SMT model in 1-best accuracy and MIRA outperforms PTEM/SMT in 5-best accuracy.
SMT	accuracy	Both MIRA and PTEM algorithms outperform the SMT model in terms of 1-best accuracy.
SMT	PTEM	At 5-best, the MIRA model outperforms both SMT and PTEM model.
SENSEVAL-2	F-measure	For SENSEVAL-2 it 2 http://www.cse.unt.edu/˜rada/downloads.html results in an F-measure of 59%, and for SENSEVAL-3 it results in an F-measure of 54%.
SENSEVAL-2	F-measure	For SENSEVAL-2 it 2 http://www.cse.unt.edu/˜rada/downloads.html results in an F-measure of 59%, and for SENSEVAL-3 it results in an F-measure of 54%.
SENSEVAL-3	F-measure	For SENSEVAL-2 it 2 http://www.cse.unt.edu/˜rada/downloads.html results in an F-measure of 59%, and for SENSEVAL-3 it results in an F-measure of 54%.
parsing	accuracy	Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German.
parsing	accuracy	Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher parsing accuracy.
parsing	accuracy	A product of eight latent variable grammars, learned on the same data, and only differing in the seed used in the random number generator that initialized EM, improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German.
parsing	accuracy	It learns not only statistics of those domains but quantitative measures of domain differences and how those differences affect parsing accuracy.
parsing	accuracy	Broadly put, we model how domain differences influence parsing accuracy.
summarization	ROUGE	To evaluate summarization performance, we use ROUGE), which has been widely used in previous studies of speech summarization (;).
word association task	correlation coefficient	On the word association task the best correlation coefficient was achieved with 750 topics and P(w 2 |w 1 ) (r = 0.139).
word association task	P	On the word association task the best correlation coefficient was achieved with 750 topics and P(w 2 |w 1 ) (r = 0.139).
word alignment	BLEU	Finally, our model improves word alignment in the context of translation, leading to a 1.2 BLEU increase over using HMM word alignments.
translation	BLEU	Finally, our model improves word alignment in the context of translation, leading to a 1.2 BLEU increase over using HMM word alignments.
Word alignment	F 1	 Table 2: Word alignment results. Our joint model has the  highest reported F 1 for English-Chinese word alignment.
summarization	similarity	Following work from the summarization community (), we employ Kendall's τ to measure the similarity of the max-probability permutation to the original order.
topic classification	accuracy	In topic classification, 90% accuracy is possible on conversational data even with 80%+ word error rate (WER) (.
topic classification	word error rate (WER)	In topic classification, 90% accuracy is possible on conversational data even with 80%+ word error rate (WER) (.
OOV detection	error	Our results show that such information improves OOV detection and we obtain large reductions in error compared to the best previously reported results.
OOV detection	accuracy	Previous research reported OOV detection accuracy on all test data.
SLM	accuracy	In this section, we present experiments showing that SLM accuracy correlates strongly with ADS performance.
SLM	accuracy	SLM performance is measured using the standard perplexity metric, and assessment accuracy is measured using area under the precision-recall curve (AUC), a standard metric for ranked lists of extractions.
SLM	precision-recall curve (AUC)	SLM performance is measured using the standard perplexity metric, and assessment accuracy is measured using area under the precision-recall curve (AUC), a standard metric for ranked lists of extractions.
SMT	BLEUr1n4	The first English reference translation is used as the input to our SMT system, and the single Arabic translation is used as the unique reference 2 . Translation quality is evaluated using two automatic evaluation metrics: (1) BLEUr1n4 (), which is based on n-gram precisions for n = 1..4, and (2) Translation Edit Rate (TER) (), which generalizes edit distance beyond single-word edits.
SMT	Translation Edit Rate (TER)	The first English reference translation is used as the input to our SMT system, and the single Arabic translation is used as the unique reference 2 . Translation quality is evaluated using two automatic evaluation metrics: (1) BLEUr1n4 (), which is based on n-gram precisions for n = 1..4, and (2) Translation Edit Rate (TER) (), which generalizes edit distance beyond single-word edits.
Polarity classification	accuracy	 Table 1: Polarity classification results (accuracy in %) using  different features for blogs and reviews.
Word Sense Disambiguation (WSD)	accuracy	In contrast to standard multi-class Word Sense Disambiguation (WSD), it uses a coarse-grained sense inventory that allows to achieve higher accuracy than WSD and therefore introduces less noise when embedded in another task such as word translation.
GALE	HTER metric	Teams on the GALE project, a DARPA-sponsored MT research program, are evaluated using the HTER metric, which is aversion of TER whereby the output is scored against a post-edited version of itself, instead of a preexisting reference.
GALE	TER	Teams on the GALE project, a DARPA-sponsored MT research program, are evaluated using the HTER metric, which is aversion of TER whereby the output is scored against a post-edited version of itself, instead of a preexisting reference.
MT research	HTER metric	Teams on the GALE project, a DARPA-sponsored MT research program, are evaluated using the HTER metric, which is aversion of TER whereby the output is scored against a post-edited version of itself, instead of a preexisting reference.
MT research	TER	Teams on the GALE project, a DARPA-sponsored MT research program, are evaluated using the HTER metric, which is aversion of TER whereby the output is scored against a post-edited version of itself, instead of a preexisting reference.
MT decoder	accuracy	Experiments are reported in Section 5 that in-vestigate the trade-off between complexity of the extended MT decoder versus translation accuracy.
MT	accuracy	Further, we show that in many cases the bridge language can be suitably selected to ensure optimal MT accuracy.
Translation	BLEU	Translation systems are generally trained to optimize BLEU, but many alternative metrics are available.
MT	BLEU	The most popular metric for both comparing systems and tuning MT models has been BLEU.
dependency grammar induction	accuracy	 Table 2: Attachment accuracy for different models for  dependency grammar induction. Bold marks best overall  accuracy per evaluation set, and  † marks figures that are  not significantly worse (binomial sign test, p < 0.05).
parsing	accuracy	multiword expressions with a single token improves parsing accuracy, we are not interested in instances that remain unchanged.
Supertagging	accuracy	 Table 2: Supertagging accuracy on section 23. ( †)  Dependencies are given by MSTParser evaluated with  labeled accuracy. PW-AP is the baseline point-wise  averaged perceptron model. PW-DEP is point-wise  dependency-informed model. The automatically tagged  POS tags were given by a maximum entropy tagger with  97.39% accuracy.
Supertagging	accuracy	 Table 2: Supertagging accuracy on section 23. ( †)  Dependencies are given by MSTParser evaluated with  labeled accuracy. PW-AP is the baseline point-wise  averaged perceptron model. PW-DEP is point-wise  dependency-informed model. The automatically tagged  POS tags were given by a maximum entropy tagger with  97.39% accuracy.
parsing	accuracy	Combining the output of multiple parsers in other more complex ways has been previously demonstrated to improve overall accuracy, so it is interesting to see if the relatively simple method used here improves parsing accuracy as well.
Tagging	precision	 Table 1: Tagging precision on all users in the test set
POS tagging	accuracy	Surprisingly, word-based POS tagging yields the best results , with a word accuracy of 94.74%.
role classification	accuracy	In experiments on English and Mandarin talk shows, the use of phrase patterns results in an increase of role classification accuracy over n-gram lexical features, and more compact phrase pattern lists are obtained due to the redundancy removal.
MST parser	accuracy	The globally optimized MST parser is better in rootprediction, and slightly better in terms of accuracy.
Translation	BLEU	 Table 2: Translation results (BLEU)
SemEval'07 shared task	precision	The standard evaluation script from the SemEval'07 shared task calculates precision, recall, and F 1 -measure for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one.
SemEval'07 shared task	recall	The standard evaluation script from the SemEval'07 shared task calculates precision, recall, and F 1 -measure for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one.
SemEval'07 shared task	F 1 -measure	The standard evaluation script from the SemEval'07 shared task calculates precision, recall, and F 1 -measure for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one.
Frame identification	Precision	 Table 4. Frame identification results. Precision, recall, and F 1 were evaluated under exact and partial frame matching;  see  §2.3. Bold indicates statistically significant results with respect to the baseline (p < 0.05).
Frame identification	recall	 Table 4. Frame identification results. Precision, recall, and F 1 were evaluated under exact and partial frame matching;  see  §2.3. Bold indicates statistically significant results with respect to the baseline (p < 0.05).
Frame identification	F 1	 Table 4. Frame identification results. Precision, recall, and F 1 were evaluated under exact and partial frame matching;  see  §2.3. Bold indicates statistically significant results with respect to the baseline (p < 0.05).
Parameter tuning	IBM	Parameter tuning was done with minimum error rate training, which was used to maximize IBM BLEU-4 ().
Parameter tuning	BLEU-4	Parameter tuning was done with minimum error rate training, which was used to maximize IBM BLEU-4 ().
Paraphrase identification	precision	 Table 4: Paraphrase identification results, with precision  and recall measures for true (positive) paraphrases. Wan  et al. (2006) report precision and recall values with only  two significant digits.
Paraphrase identification	recall	 Table 4: Paraphrase identification results, with precision  and recall measures for true (positive) paraphrases. Wan  et al. (2006) report precision and recall values with only  two significant digits.
Paraphrase identification	precision	 Table 4: Paraphrase identification results, with precision  and recall measures for true (positive) paraphrases. Wan  et al. (2006) report precision and recall values with only  two significant digits.
Paraphrase identification	recall	 Table 4: Paraphrase identification results, with precision  and recall measures for true (positive) paraphrases. Wan  et al. (2006) report precision and recall values with only  two significant digits.
RTE	accuracy	The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.
Coordination resolution	F-Measure	 Table 2: Coordination resolution results at the conjunct  and conjunction levels as F-Measure.
Distinguishing Use	Mention	Distinguishing Use and Mention in Natural Language
classification	precision	We report on an extensive series of experiments on a large meeting corpus which leads to classification improvement in weighted precision and f-score.
HAC	stop threshold γ	For HAC, we manually tuned the stop threshold γ, the Jaro-Winkler threshold η, and the ELM smoothing parameter ρ on the development set.
HAC	ELM smoothing parameter ρ	For HAC, we manually tuned the stop threshold γ, the Jaro-Winkler threshold η, and the ELM smoothing parameter ρ on the development set.
Cross-lingual mention matching	accuracy	 Table 4: Cross-lingual mention matching accuracy [%].  The training data contains names from three genres: broad- cast news (bn), newswire (nw), and weblog (wb). We used  the full training corpus (all) for the cross-lingual clustering  experiments, but the model achieved high accuracy with  significantly fewer training examples (e.g., bn).
Cross-lingual mention matching	accuracy	 Table 4: Cross-lingual mention matching accuracy [%].  The training data contains names from three genres: broad- cast news (bn), newswire (nw), and weblog (wb). We used  the full training corpus (all) for the cross-lingual clustering  experiments, but the model achieved high accuracy with  significantly fewer training examples (e.g., bn).
reference grouping	accuracy	Our reference grouping module achieved perfect accuracy for all the correctly tagged references.
reference removal	precision	The non-syntactic reference removal module achieved 90.08% precision and 90.1% recall.
reference removal	recall	The non-syntactic reference removal module achieved 90.08% precision and 90.1% recall.
ASR	re-liability	The recognition accuracy of state-of-the-art ASR systems on non-native spontaneous speech is still relatively low, which will sequentially impact the re-liability and accuracy of automatic scoring systems using these noisy transcripts.
ASR	accuracy	The recognition accuracy of state-of-the-art ASR systems on non-native spontaneous speech is still relatively low, which will sequentially impact the re-liability and accuracy of automatic scoring systems using these noisy transcripts.
automatic speech recognition (ASR)	accuracy	Lastly, automatic speech recognition (ASR) systems can now transcribe telephone conversations with sufficient accuracy for useful automated analysis.
SemEval task	accuracy	In contrast to the original SemEval task, where systems can Best normal measures the accuracy for each system annotation a j as the number of times a j appears in the R j , the multi-set union of human tags, and averages overall the test images.
MT	ablation stud- ies	 Table 3: The top 3 performing MT metrics for both  MSRP and PAN datasets as identified by ablation stud- ies. BLEU(1-4), NIST(1-5) and TER were used as the 10  base features in the classifiers.
MT	BLEU	 Table 3: The top 3 performing MT metrics for both  MSRP and PAN datasets as identified by ablation stud- ies. BLEU(1-4), NIST(1-5) and TER were used as the 10  base features in the classifiers.
MT	TER	 Table 3: The top 3 performing MT metrics for both  MSRP and PAN datasets as identified by ablation stud- ies. BLEU(1-4), NIST(1-5) and TER were used as the 10  base features in the classifiers.
Transliteration Mining (TM)	recall	Much previous work on Transliteration Mining (TM) was conducted on short parallel snippets using limited training data, and successful methods tended to favor recall.
Segment extraction	precision	Segment extraction produced significantly higher precision for three different TM methods.
TM	precision	2. For large comparable texts, we use contextual clues, namely translations of neighboring words, to constrain TM and to preserve precision.
extracting sub-sentence alignments	precision	Though there have been some papers on extracting sub-sentence alignments from comparable text (), extracting related (as opposed to parallel) text segments maybe preferable because: 1) transliterations may not occur in parallel contexts; 2) using simple lexical overlap is efficient; and as we will show 3) simultaneous use of phonetic and contextual evidences maybe sufficient to produce high TM precision.
translation evaluation	BLEU	One of the standards for such tuning is minimum error rate training (MERT), which directly minimize the loss of translation evaluation measures, i.e. BLEU ().
Translation	BLEU	Translation results are measured by case sensitive BLEU.
Translation	BLEU	 Table 1: Translation results by BLEU. Results with- out significant differences from the MERT baseline  are marked  †. The numbers in boldface are signif- icantly better than the MERT baseline (both mea- sured by the bootstrap resampling
parsing	accuracy	Decomposing the category in-    Supertagging and parsing accuracy are not entirely correlated between the parsers -in corpora A and B,  supertagging is comparable or better than I-3, but F -score is substantially worse.
parsing	F -score	Decomposing the category in-    Supertagging and parsing accuracy are not entirely correlated between the parsers -in corpora A and B,  supertagging is comparable or better than I-3, but F -score is substantially worse.
ASR	accuracy	Here we build on that work by using the output of such classifiers to adapt language models for ASR and thereby improve recognition accuracy.
ASR	accuracy	Section 4 uses an oracle simulation to show how increasing EEG classifier accuracy will affect ASR accuracy.
MTL	BASE	 Table 2: Word accuracies and error rate reductions (ERR)  in percentages for English-to-Japanese MTL augmented  by corresponding transliterations from other languages.  BASE is the base system while RERANKED represents the  same system with its output reranked using supplemental  transliterations. ORACLE represents an oracle reranker.
MTL	RERANKED	 Table 2: Word accuracies and error rate reductions (ERR)  in percentages for English-to-Japanese MTL augmented  by corresponding transliterations from other languages.  BASE is the base system while RERANKED represents the  same system with its output reranked using supplemental  transliterations. ORACLE represents an oracle reranker.
MTL	ORACLE	 Table 2: Word accuracies and error rate reductions (ERR)  in percentages for English-to-Japanese MTL augmented  by corresponding transliterations from other languages.  BASE is the base system while RERANKED represents the  same system with its output reranked using supplemental  transliterations. ORACLE represents an oracle reranker.
MT	Translation Edit Rate (TER))	The MT performance in terms of Translation Edit Rate (TER)) and BLEU () is shown in.
MT	BLEU	The MT performance in terms of Translation Edit Rate (TER)) and BLEU () is shown in.
taxonomy induction	TABLE	Data We evaluated our taxonomy induction algorithm using dataset which consists of for 541 basic level nouns (e.g., and TABLE).
NER	error reduction	Further, we show that by applying the same method to direct-transfer NER, we achieve a relative error reduction of 26%.
NER	F 1 -score	 Table 4: Supervised NER results measured with F 1 -score  on the CoNLL 2002/2003 development and test sets.
parsing	UAS	 Table 1: Results comparing pruning methods on PTB Section 22. Oracle is the max achievable UAS after pruning.  Pruning efficiency (PE) is the percentage of non-gold first-order dependency arcs pruned. Speed is parsing time relative  to the unpruned first-order model (around 2000 tokens/sec). UAS is the unlabeled attachment score of the final parses.
PMI	memory	2.5 of the top 3 true PMI items appear in the top 50 when the memory is about 35%.
MT	aver-age output length	These traits are simple properties of the MT output such as "aver-age output length" and "average rule length."
MT	average rule length	These traits are simple properties of the MT output such as "aver-age output length" and "average rule length."
MT	MIRA	In all experiments, our MT system learned asynchronous context-free grammar, using GIZA++ for word alignments, MIRA for parameter tuning), cdec for decoding ( ), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation.
MT	BLEU	In all experiments, our MT system learned asynchronous context-free grammar, using GIZA++ for word alignments, MIRA for parameter tuning), cdec for decoding ( ), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation.
synonym discovery	recall	An application that focuses on synonym discovery would favor recall, while an application portraying highly granular sense distinctions would favor precision.
synonym discovery	precision	An application that focuses on synonym discovery would favor recall, while an application portraying highly granular sense distinctions would favor precision.
negation	accuracy	The negation at phase 4 was recognised with high accuracy.
Illinois Entity Tagger	Orthomatcher	The system uses the Illinois Entity Tagger ( and Orthomatcher from the GATE framework 2 for withina-document co-reference resolution.
Translation	BLEU	Translation quality is measured through BLEU ().
MTU MMs	BLEU	Amongst the decomposition orders tested (L2RT, R2LT, L2RS, and R2LS), each of the individual MTU MMs was able to achieve significant improvement over the baseline, around 1 BLEU point.
segmentation	accuracy	We show that this model improves segmentation accuracy over purely segmental input representations, and recovers the dominant stress pattern of the data.
modelling	accuracy	This gives a substantial improvement in modelling accuracy.
ASR	accuracy	This approach has been shown to dramatically outperform ASR in terms of both accuracy and Word Error Rate (WER), even when using captionists drawn from Amazon's Mechanical Turk.
ASR	Word Error Rate (WER)	This approach has been shown to dramatically outperform ASR in terms of both accuracy and Word Error Rate (WER), even when using captionists drawn from Amazon's Mechanical Turk.
ASR	error rate	We also adapt the ASR models to the domain, and evaluate the impact of error rate on performance.
translation	accuracy	We address the goal of achieving a system that balances translation accuracy and la-tency.
S2S translation	latency	Another key parameter in designing a S2S translation system for any task is latency.
speech-to-speech translation	accuracy	On the other hand, realtime speech-to-text or speech-to-speech translation demand the best possible accuracy at low latencies such that communication is not hindered due to potential delay in processing.
sentence compression	F1 measure	Prior work in sentence compression use the F1 measure over grammatical relations to evaluate candidate compressions (.
MT	BLEU	Our experiments , involving Urdu-English, show that the proposed approach outperforms a state-of-the-art PBSMT system which uses the TSP model for reordering by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hi-ero, by 3 BLEU points.
MT	BLEU	Additionally, we evaluate the effect of reordering on MT performance using BLEU (extrinsic evaluation).
SMT tuning	Pairwise Ranked Optimisation (PRO)	We do this by incorporating these weights into SMT tuning using a modified version of Pairwise Ranked Optimisation (PRO) ().
part-of-speech tagging	accuracy	In part-of-speech tagging, the accuracy of the Stanford tagger () falls from 97% on Wall Street Journal text to 85% accuracy on Twitter ().
part-of-speech tagging	accuracy	In part-of-speech tagging, the accuracy of the Stanford tagger () falls from 97% on Wall Street Journal text to 85% accuracy on Twitter ().
IRC POS tagging	accuracy	With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute).
parsing	accuracy	There Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging), named entity recognition (), and parsing), but accuracy rates are still significantly lower than traditional well-edited genres like newswire.
MT	BLEU metric	We evaluate the MT system with the BLEU metric ().
SMT	BLEU score	Second, because the phrase model has to work with other component models in an SMT system in order to produce good translations and the quality of translation is measured via BLEU score, it is desirable to optimize the parameters of the phrase model jointly with other component models with respect to an objective function that is closely related to the evaluation metric under consideration, i.e., BLEU in this paper.
SMT	BLEU	Second, because the phrase model has to work with other component models in an SMT system in order to produce good translations and the quality of translation is measured via BLEU score, it is desirable to optimize the parameters of the phrase model jointly with other component models with respect to an objective function that is closely related to the evaluation metric under consideration, i.e., BLEU in this paper.
MRF	BLEU	The statistics for these features are given in. shows that all the MRF models lead to a substantial improvement over the baseline system across all test sets, with a statistically significant margin from 0.8 to 1.3 BLEU points.
normalization of Chinese/English social media text	BLEU	We achieved statistically significant improvements over two strong baselines: an improvement of 9.98%/7.35% in BLEU scores for normalization of Chinese/English social media text, and an improvement of 1.38%/1.35% in BLEU scores for translation of Chinese/English social media text.
MT	BLEU	In the corresponding MT experiments, as the normalization BLEU scores increase, the MT BLEU scores also increase.
MT	BLEU	In the Chinese-English MT experiments, the normalized texts output by our normalization decoder lead to improved translation quality compared to normalization by the PBMT baseline, by 1.38% in BLEU score.
parsing	accuracy	Several strategies have been proposed, including beam-search, best-first and A * . In this paper we focus on the standard approach of approximating the source PCFG in such away that parsing accuracy is traded for efficiency.
PCFG parsing	error upper	We obtain improved time upper bounds with respect to the input grammar size for PCFG parsing, and provide error upper bounds on the PCFG approximation, in contrast with existing heuristic methods.
Parsing	F 1 measure	Parsing with tensor decomposition for r = 280 takes 0.62 seconds per sentence (on average) with an F 1 measure of 64.05.
Deception detection	F)1-score	 Table 1: Deception detection performance, incl. (P)recision, (R)ecall, and (F)1-score, for three human judges and two  meta-judges on a set of 160 negative reviews. The largest value in each column is indicated with boldface.
IAA	Likert and Rate)	For measuring IAA, we selected Krippendorff's α, which is an agreement coefficient that handles missing data, as well as different levels of measurement, e.g., nominal data (Select and MaxDiff) and interval data (Likert and Rate).
IQ recognition	UAR	 Table 5: Recognition performance and variance of confi- dence distributions for IQ recognition  Classifier  σ 2  UAR  κ  ρ  SVM (cubic Kernel) 0.03 0.38 0.54 0.69  SVM (RBF-Kernel)  0.05 0.48 0.65 0.77  Naive Bayes  0.13 0.49 0.57 0.71  Naive Bayes (Kernel) 0.12 0.52 0.59 0.73  Rule Induction  0.13 0.55 0.68 0.79
Morphological Analysis	POS	Morphological Analysis We tested the effect of codafication on morphological tagging, specifically full POS and lemma determination in context by the morphological tagger MADA ARZ . Here, we are evaluating MADA ARZ not on its conversion to CODA (as above), but on its core functionality, namely morphological tagging.
translation	BLEU	We report translation results in terms of lower-case BLEU scores ().
TED 2011 mixture modeling	IN	 Table 3:  TED 2011 mixture modeling results.  Heuristics best is the best heuristics based system, IN for  Arabic-English and ALL for German-English. X,Y de- notes linear interpolation between X and Y phrase tables.
translation	accuracy	The experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors.
translation	accuracy	This confirms our claim that using synonyms for smoothing can lead to better translation accuracy than using nearest neighbors.
POS tagging	accuracy	Our algorithm is used for POS tagging and evaluated on the English Web Treebank and the Danish Dependency Treebank with an average 4.4% error reduction in tagging accuracy.
classification	accuracy	Second, multiple metadata attributes could impact the classification decision, and picking a single one might reduce classification accuracy.
conflict management	safety	Several relevant motives have been found to be associated with deleting posted information, including conflict management, safety, fear of retribution, impression management, and emotional regulation.
FP detection	miss (failed detection) rates	We evaluated FP detection performance in terms of both false alarm (incorrect detection) and miss (failed detection) rates, shown in Table 2.
DLM training	WERs	When we use ASR 50-best from t 1 for DLM training, WERs drop to 22.2% and 21.8% on the held-out and the test sets, respectively.
Word Sense Disambiguation (WSD)	accuracies	Word Sense Disambiguation (WSD) approaches have reported good accuracies in recent years.
WSD	accuracy	According to the classical definition, a strong AI based WSD system should perform the task of sense disambiguation in the same manner and with similar accuracy as human beings.
WSD	accuracy	Knowledge based approaches, which can be considered to be closest form of WSD conforming to the principles of strong AI, typically achieve low accuracy.
DDI extraction	PER	We used The target entities, for example, for DDI extraction and for EMP-ORG relation extraction would be {DRUG} and {PER, GPE, ORG} respectively.
segmentation	accuracy	Experiments show that using RSI significantly improves the segmentation accuracy compared to TF-IDF, a traditional content-based feature weighting scheme.
detecting	REQUEST-ACTION	As we can see, detecting REQUEST-ACTION is much harder than detecting the other DAs.
ODP cross validation	BAS	At each ODP cross validation step, we trained a BAS or DAC CMP tagger using ODP's training folds for that step and used tags produced by that tagger for both training and testing the ODP tagger for that step.
disfluency detection	F-score	Our method for disfluency detection achieves the best reported F-score 0.841 without the use of additional resources.
Relation extraction	OLLIE	 Table 5: Relation extraction results on OLLIE set  (Triple).
Relation extraction	OLLIE	 Table 5: Relation extraction results on OLLIE set  (Triple).
MMPs	accuracy	Our proposed model can predict MMPs with high accuracy.
information structure analysis	AZ	SVM and MaxEnt have proved successful in information structure analysis (e.g. () but, to the best of our knowledge, their semi-supervised versions have not been used for AZ of full articles.
SMT	BLEU	The tuning process improves the performance of an SMT system as measured by this metric; with BLEU () being the most popular choice.
MT	BLEU	MT system developers typically use BLEU and ignore all the other metrics.
MMO	BLEU-TER	MMO points use a series of single letters referring to the metrics used, e.g. BT for BLEU-TER.
MMO	BLEU	All our MMO approaches, except for the union method, show gains on both BLEU and RIBES axes.
MMO	RIBES	All our MMO approaches, except for the union method, show gains on both BLEU and RIBES axes.
MMO	BLEU-only tuning	We again see in these figures that the MMO approaches can improve the BLEU-only tuning by 0.3 BLEU points, without much drop in other metrics.
MMO	BLEU	We again see in these figures that the MMO approaches can improve the BLEU-only tuning by 0.3 BLEU points, without much drop in other metrics.
MMO	BLEU	The results are similar to the multiple references set indicating that MMO approaches are equally effective for single references .  shows the BLEU scores for our ensemble tuning method (for various combinations) and we again see improvements over the baseline BLEU-only tuning.
MMO	BLEU-only	The results are similar to the multiple references set indicating that MMO approaches are equally effective for single references .  shows the BLEU scores for our ensemble tuning method (for various combinations) and we again see improvements over the baseline BLEU-only tuning.
SMT	MERT	Moses () was used for the training of the SMT system and the symmetrisation (using the grow-diag-final procedure), with MERT ( used for tuning of the weights, and SRILM to build the language model (5-grams based).
focus discovery	BOOK	However, focus discovery identifies BOOK as the top candidate focus, which matches our intuition.
ASR	precision	For an utterance u that generates partial ASR results r 1 , ..., rm , we denote the hand-annotated subframe corresponding to partial ASR result r j by G SUB j . In the lines marked "(annotated subframe)", we show the precision, recall, and F-score of the explicit subframe for each ASR result r j in relation to the annotated subframe G SUB j . As a first observation, note that at any threshold level, the explicit subframes do better at recalling the handannotated subframe elements than they do at recalling the complete frame elements.
ASR	recall	For an utterance u that generates partial ASR results r 1 , ..., rm , we denote the hand-annotated subframe corresponding to partial ASR result r j by G SUB j . In the lines marked "(annotated subframe)", we show the precision, recall, and F-score of the explicit subframe for each ASR result r j in relation to the annotated subframe G SUB j . As a first observation, note that at any threshold level, the explicit subframes do better at recalling the handannotated subframe elements than they do at recalling the complete frame elements.
ASR	F-score	For an utterance u that generates partial ASR results r 1 , ..., rm , we denote the hand-annotated subframe corresponding to partial ASR result r j by G SUB j . In the lines marked "(annotated subframe)", we show the precision, recall, and F-score of the explicit subframe for each ASR result r j in relation to the annotated subframe G SUB j . As a first observation, note that at any threshold level, the explicit subframes do better at recalling the handannotated subframe elements than they do at recalling the complete frame elements.
ASR result r j	precision	For an utterance u that generates partial ASR results r 1 , ..., rm , we denote the hand-annotated subframe corresponding to partial ASR result r j by G SUB j . In the lines marked "(annotated subframe)", we show the precision, recall, and F-score of the explicit subframe for each ASR result r j in relation to the annotated subframe G SUB j . As a first observation, note that at any threshold level, the explicit subframes do better at recalling the handannotated subframe elements than they do at recalling the complete frame elements.
ASR result r j	recall	For an utterance u that generates partial ASR results r 1 , ..., rm , we denote the hand-annotated subframe corresponding to partial ASR result r j by G SUB j . In the lines marked "(annotated subframe)", we show the precision, recall, and F-score of the explicit subframe for each ASR result r j in relation to the annotated subframe G SUB j . As a first observation, note that at any threshold level, the explicit subframes do better at recalling the handannotated subframe elements than they do at recalling the complete frame elements.
ASR result r j	F-score	For an utterance u that generates partial ASR results r 1 , ..., rm , we denote the hand-annotated subframe corresponding to partial ASR result r j by G SUB j . In the lines marked "(annotated subframe)", we show the precision, recall, and F-score of the explicit subframe for each ASR result r j in relation to the annotated subframe G SUB j . As a first observation, note that at any threshold level, the explicit subframes do better at recalling the handannotated subframe elements than they do at recalling the complete frame elements.
ASR	F-score	For example, when evaluating the explicit subframes overall partial ASR results, an F-score of 0.75 is attained at thresholds in the range 0.5-0.55.
Logistic regression	accuracy	 Table 4: Logistic regression accuracy when trained  using varying features.
REG	accuracy	4. An evaluation method for non-deterministic REG that aligns generated and observed data and calculates accuracy over alignments.
REG	Minimality	Previous evaluation of REG algorithms have used measurements such as Uniqueness, Minimality, Dice , and Accuracy (.
REG	Dice	Previous evaluation of REG algorithms have used measurements such as Uniqueness, Minimality, Dice , and Accuracy (.
REG	Accuracy	Previous evaluation of REG algorithms have used measurements such as Uniqueness, Minimality, Dice , and Accuracy (.
predicting inflections	accuracy	Hand-engineered, rulebased methods for predicting inflections can offer extremely high accuracy, but they are laborious to construct and do not exist with full lexical coverage in all languages.
word prediction	accuracy	plots the word prediction accuracy for all three methods across the eight languages with varying training sizes, while provides corresponding numerical results.
ATE	precision	Our proposed ATE approach uses machine learning (ML), since it has been achieving high precision values (.
Europarl base- line translation	OOV	 Table 6: Evaluation results for the filtered Europarl base- line translation model (OOV -out of vocabulary)
WSD	accuracy	The majority of supervised WSD systems are limited to resolving a small number of ambiguous terms and, despite their accuracy, are not suitable for use within applications.
MT evaluation	BLEU	On the other hand, standard automatic MT evaluation metrics such as BLEU () and METEOR () are considerably cheaper and provide faster results, but return rather crude scores that are difficult to interpret for MT users and developers alike.
MT evaluation	METEOR	On the other hand, standard automatic MT evaluation metrics such as BLEU () and METEOR () are considerably cheaper and provide faster results, but return rather crude scores that are difficult to interpret for MT users and developers alike.
character recognition	accuracy	We conducted the evaluations on character recognition and Kana-Kanji conversion accuracy to measure KooSHO's performance.
Kana-Kanji conversion	accuracy	We conducted the evaluations on character recognition and Kana-Kanji conversion accuracy to measure KooSHO's performance.
Character Recognition	accuracy	3) EC query log (2000 most frequent query fragments issued in 2011 at Rakuten Ichiba) As the dictionary, we used UniDic 6 . Character Recognition Firstly, we evaluate the accuracy of the character recognition model.
character recognition	accuracy	The overall character recognition accuracy was 0.76.
SRL	purity	As inmost previous work on unsupervised SRL, we evaluate our model using purity, collocation and their harmonic mean F1.
Parsing	accuracy	Parsing accuracy is greatly impacted by the quality of preprocessing steps such as tagging and word segmentation.
AA	accuracy	We know from state of the art research in AA that the length of the documents and the number of potential candidate authors have an important effect on the accuracy of AA approaches.
alignment	accuracy	The results show that discriminative models outperform the generative models in terms of alignment accuracy.
translation	accuracy	Furthermore, the translation accuracy of the resulting SMT systems is significantly improved across four different translation tasks.
Alignment	accuracy	 Table 1: Alignment accuracy over heterogeneous corpora.
AMT-based	accuracy	 Table 2: AMT-based evaluations of cluster accuracy and pairwise ranking accuracy of systems that vary in the source  of clustering data, source of strength counts, and part of speech. For comparison, the approach used by de Melo and  Bansal (2013) achieves a pairwise ranking accuracy of 76.1% on the non-polysemous WordNet clusters.
relation extraction	F1	For example, adding entity type information improves relation extraction by 3% ( ) and entity linking by 4.2 F1 points (.
classification	accuracy	 Table 1: Comparing classification accuracy with 10% labeled data. The LCCT model performs significantly better
MIR	F-scores	We evaluate MIR on two tasks: (1) On word alignment , applying MIR on fertility based models we attain higher F-scores than ABA and PostCAT.
MIR	ABA	We evaluate MIR on two tasks: (1) On word alignment , applying MIR on fertility based models we attain higher F-scores than ABA and PostCAT.
word alignment	F-scores	We evaluate MIR on two tasks: (1) On word alignment , applying MIR on fertility based models we attain higher F-scores than ABA and PostCAT.
word alignment	ABA	We evaluate MIR on two tasks: (1) On word alignment , applying MIR on fertility based models we attain higher F-scores than ABA and PostCAT.
MIR	F-scores	We evaluate MIR on two tasks: (1) On word alignment , applying MIR on fertility based models we attain higher F-scores than ABA and PostCAT.
MIR	ABA	We evaluate MIR on two tasks: (1) On word alignment , applying MIR on fertility based models we attain higher F-scores than ABA and PostCAT.
MIR	F-and Bleu score	On Czech-English and Chinese-English word alignment (Section 5), restricted to the HMM model, MIR attains F-and Bleu score improvements that are comparable to those of ABA and PostCAT.
MIR	ABA	On Czech-English and Chinese-English word alignment (Section 5), restricted to the HMM model, MIR attains F-and Bleu score improvements that are comparable to those of ABA and PostCAT.
MIR	ABA	In this section, we compare MIR against standard EM training and ABA on Czech-English and Chinese-English word alignment and translation.
MIR	error rates	 Table 3: MIR reduces error rates (WNER, NED) and  learns sparser models (number of t 1 parameters greater  than 0.01) compared to the other models.
slot induction	accuracy	First, we examine the slot induction accuracy by comparing the ranked list of induced slots with the reference slots created by system developers.
paraphrase acquisition	coverage	This study tackles the problem of paraphrase acquisition: achieving high coverage as well as accuracy.
paraphrase acquisition	accuracy	This study tackles the problem of paraphrase acquisition: achieving high coverage as well as accuracy.
SCL	dimensionality K	The best parameters for SCL are dimensionality K = 50 and rescale factor α = 5.
SCL	rescale factor α	The best parameters for SCL are dimensionality K = 50 and rescale factor α = 5.
SCL	rescale factor α	Optimizing on a source-domain development set, we find that the best parameters for SCL are dimensionality K = 25 and rescale factor α = 5.
money laundering	similarity score	For example an item involving the words "bank" and "money", gives the words in their respective contexts, "along the east bank of the Des Moines River" and "the basis of all money laundering" with a low averaged similarity score of 2.5 (on a scale of 1.0 to 10.0).
semantic parsing of test sentences	recall	The learned model was then applied to semantic parsing of test sentences and evaluated on event precision, recall, and F1.
semantic parsing of test sentences	F1	The learned model was then applied to semantic parsing of test sentences and evaluated on event precision, recall, and F1.
sentiment prediction	accuracy	What sentiment prediction accuracy is expected when Arabic blog posts and tweets are translated into English (using the current stateof-art techniques), and then run through a stateof-the-art English sentiment analysis system?
phrase-structure parsing	accuracy	Our algorithm is faster than traditional phrase-structure parsing and achieves 90.4% English parsing accuracy and 82.4% Chinese parsing accuracy, near to the state of the art on both benchmarks.
phrase-structure parsing	accuracy	Our algorithm is faster than traditional phrase-structure parsing and achieves 90.4% English parsing accuracy and 82.4% Chinese parsing accuracy, near to the state of the art on both benchmarks.
Classification of discourse connectives	omission rate (OR)	 Table 1: Classification of discourse connectives based on omission rate (OR) and Jensen-Shannon Divergence context  differential (JSD).
coreference resolution	precision	As an indication to the difficulty of these instances, we note that a state-ofthe-art coreference resolution system () achieves precision of 53.26% on it.
parsing utterances	surprise	Nine-month olds show evidence of parsing utterances into prosodic units, and show 'surprise' when a pause is inappropriately inserted inside as opposed to between these units.
translation	BLEU score	In addition to these two key indicators, we evaluated the translation quality using an automatic measure, namely BLEU score ().
Translation	BLEU score	 Table 1: Translation performance for various language pairs using no preordering (baseline), and three alternative  preordering systems. Average test BLEU score and standard deviation across 3 independent tuning runs. Speed ratio  is calculated with respect to the speed of the slower baseline that uses a d = 10. Stack size is 1000. For eng-jpn and  eng-chi, character-based BLEU is used.
Translation	Speed ratio	 Table 1: Translation performance for various language pairs using no preordering (baseline), and three alternative  preordering systems. Average test BLEU score and standard deviation across 3 independent tuning runs. Speed ratio  is calculated with respect to the speed of the slower baseline that uses a d = 10. Stack size is 1000. For eng-jpn and  eng-chi, character-based BLEU is used.
Translation	BLEU	 Table 1: Translation performance for various language pairs using no preordering (baseline), and three alternative  preordering systems. Average test BLEU score and standard deviation across 3 independent tuning runs. Speed ratio  is calculated with respect to the speed of the slower baseline that uses a d = 10. Stack size is 1000. For eng-jpn and  eng-chi, character-based BLEU is used.
Summarization	ROUGE-1	Summarization is evaluated by comparing system summaries against reference summaries, using ROUGE-1 scores . System summaries are generated using the heuristic approach presented in §5: given a predicted subgraph, the approach finds the most frequently aligned word span for each concept node, and then puts them together as a bag of words.
sentence ordering	accuracy	The evaluation is conducted on the two tasks, sentence ordering and summary coherence rating, and the accuracy is the fraction of correct pairwise rankings.
SRL	F-score	Evaluation Measures Following standard practice in the SRL evaluation, we measure the performance using labeled F-score.
SMT	MIRA	SMT Model parameters were optimized using MIRA () on the WMT2011 news test set (3003 sentences).
SMT	MIRA	Its SMT features are trained on 1.8M parallel sentences of NTCIR-7 data () and weights were tuned on the NTCIR-8 test collection (2,000 sentences) using MIRA (.
WSD	precision (P)	The accuracy of WSD experiments was measured in terms of precision (P), recall (R) and F-Score (F-1).
WSD	recall (R)	The accuracy of WSD experiments was measured in terms of precision (P), recall (R) and F-Score (F-1).
WSD	F-Score (F-1)	The accuracy of WSD experiments was measured in terms of precision (P), recall (R) and F-Score (F-1).
tagging	accuracy	The induced models significantly improve tagging accuracy on held-out test sets across three domains (Twitter, spoken language, and search queries).
classification	accuracy	We perform classification, contributing a classifier that discriminates between the two hashtags exceptionally well at 82% accuracy with a substantial error reduction over its baseline.
classification	error	We perform classification, contributing a classifier that discriminates between the two hashtags exceptionally well at 82% accuracy with a substantial error reduction over its baseline.
part-of-speech tagging	accuracy	 Table 2: Results for part-of-speech tagging using differ- ent word embeddings (rows) on different datasets (PTB  and Twitter). Cells indicate the part-of-speech accuracy  of each experiment.
dependency parsing on PTB	UAS	 Table 3: Results for dependency parsing on PTB using  different word embeddings (rows). Columns UAS and  LAS indicate the labelled attachment score and the unla- belled parsing scores, respectively.
dependency parsing on PTB	LAS	 Table 3: Results for dependency parsing on PTB using  different word embeddings (rows). Columns UAS and  LAS indicate the labelled attachment score and the unla- belled parsing scores, respectively.
voice conversion	accuracy	We present a GPU-friendly local regression model for voice conversion that is capable of converting speech in real-time and achieves state-of-the-art accuracy on this task.
LLR	accuracy	In practice, LLR depends most strongly on nearby points, so a standard CPU implementation will skip distant points, with limited loss of accuracy.
parsing	F1-score	However, improved parsing performance does not correspond to improved F1-score in answer retrieval when using the respective parser in a response-based learning framework.
SMT	SEMPRE	Response-based learning for SMT uses the code described in . For semantic parsing we use the SEMPRE and PARASEMPRE tools of and Berant and Liang (2014) which were trained on the training portion of the FREE917 corpus . Further models use the training data enhanced with synonyms from WordNet as described in Section 4.
SMT	PARASEMPRE	Response-based learning for SMT uses the code described in . For semantic parsing we use the SEMPRE and PARASEMPRE tools of and Berant and Liang (2014) which were trained on the training portion of the FREE917 corpus . Further models use the training data enhanced with synonyms from WordNet as described in Section 4.
parse	IAA-weighted loss	Learning to parse with IAA-weighted loss
meaning preservation	simplicity	Our method improves upon the other methods in the detection of complex words, in meaning preservation, and in simplicity.
disfluency detection	F1	Experiments show improvement in disfluency detection on Supreme Court oral arguments , nearly 23% improvement in F1.
Disfluency detection	ANNOT	 Table 2: Disfluency detection of ANNOT OYEZ with  different training sets.
Medication Use Categorization	Precision	 Table 2: Medication Use Categorization Results on detected medications (each cell shows Precision, Recall, F)
Medication Use Categorization	Recall	 Table 2: Medication Use Categorization Results on detected medications (each cell shows Precision, Recall, F)
Medication Use Categorization	Precision	 Table 3: Medication Use Categorization Results on gold medications (each cell shows Precision, Recall, and F)
Medication Use Categorization	Recall	 Table 3: Medication Use Categorization Results on gold medications (each cell shows Precision, Recall, and F)
Medication Use Categorization	F	 Table 3: Medication Use Categorization Results on gold medications (each cell shows Precision, Recall, and F)
classification	accuracy	Therefore, classification performance is measured in terms of accuracy (i.e., the percentage of pairs of which labels were correctly predicted).
image description	METEOR	BLEU is the metric that is seen more commonly in image description literature, but a more recent study) has shown METEOR to be a better evaluation metric.
SMT	accu- rate	 Table 6: Proportion of SMT descriptions deemed accu- rate and relevant. System output evaluated for rank place- ments 1. . . 6.
WMT German→English task	BLEU	 Table 4: Results for the WMT German→English task in  BLEU [%]. The baseline contains a recurrent neural LM.  We compare with the best single system that is reported  on matrix.statmt.org, which was submitted by the  Unversity of Edinburgh.
Translation	BLEU	Translation quality is measured with BLEU ().
translation	BLEU	Moreover, our variant of SLP even improves translation quality by 0.2-0.3 BLEU.
MWE identification	recall	shows full supersense tagging results, separating the MWE identification performance (measured by link-based precision, recall, and F 1 ; see) from the precision, recall, and F 1 of class labels on the first token of each expression (segments with no class label are ignored).
MWE identification	F 1	shows full supersense tagging results, separating the MWE identification performance (measured by link-based precision, recall, and F 1 ; see) from the precision, recall, and F 1 of class labels on the first token of each expression (segments with no class label are ignored).
MWE identification	precision	shows full supersense tagging results, separating the MWE identification performance (measured by link-based precision, recall, and F 1 ; see) from the precision, recall, and F 1 of class labels on the first token of each expression (segments with no class label are ignored).
MWE identification	recall	shows full supersense tagging results, separating the MWE identification performance (measured by link-based precision, recall, and F 1 ; see) from the precision, recall, and F 1 of class labels on the first token of each expression (segments with no class label are ignored).
MWE identification	F 1	shows full supersense tagging results, separating the MWE identification performance (measured by link-based precision, recall, and F 1 ; see) from the precision, recall, and F 1 of class labels on the first token of each expression (segments with no class label are ignored).
semantic enrichment	accuracy	 Table 4: Comparison of retrofitting for semantic enrichment against Yu and Dredze (2014), Xu et al. (2014). Spear- man's correlation (3 left columns) and accuracy (3 right columns) on different tasks.
SA	accuracy	While previous work on SA for English tweets reports an overall accuracy of 65-71% on average (), recent studies investigating Arabic tweets only report accuracy scores ranging between 49-65% (.
SA	accuracy	While previous work on SA for English tweets reports an overall accuracy of 65-71% on average (), recent studies investigating Arabic tweets only report accuracy scores ranging between 49-65% (.
MT-based SA	recall	• MT-based SA approaches in general have a problem of identifying positive tweets (low recall and precision), often misclassifying them as negative.
MT-based SA	precision	• MT-based SA approaches in general have a problem of identifying positive tweets (low recall and precision), often misclassifying them as negative.
translation direction detection	accuracy	Existing statistical methods for translation direction detection have low accuracy when applied to the realistic out-of-domain setting, especially when the input texts are short.
translation direction detection	precision	Our contributions in this work are threefold: 1) We develop a multi-corpus parallel dataset with translation direction labels at the sentence level, 2) we perform a comparative evaluation of previously introduced features for translation direction detection in a cross-domain setting and 3) we generalize a previously introduced type of features to out-perform the best previously proposed features in detecting translation direction and achieve 0.80 precision with 0.85 recall.
translation direction detection	recall	Our contributions in this work are threefold: 1) We develop a multi-corpus parallel dataset with translation direction labels at the sentence level, 2) we perform a comparative evaluation of previously introduced features for translation direction detection in a cross-domain setting and 3) we generalize a previously introduced type of features to out-perform the best previously proposed features in detecting translation direction and achieve 0.80 precision with 0.85 recall.
detecting translation direction	precision	Our contributions in this work are threefold: 1) We develop a multi-corpus parallel dataset with translation direction labels at the sentence level, 2) we perform a comparative evaluation of previously introduced features for translation direction detection in a cross-domain setting and 3) we generalize a previously introduced type of features to out-perform the best previously proposed features in detecting translation direction and achieve 0.80 precision with 0.85 recall.
detecting translation direction	recall	Our contributions in this work are threefold: 1) We develop a multi-corpus parallel dataset with translation direction labels at the sentence level, 2) we perform a comparative evaluation of previously introduced features for translation direction detection in a cross-domain setting and 3) we generalize a previously introduced type of features to out-perform the best previously proposed features in detecting translation direction and achieve 0.80 precision with 0.85 recall.
entity extraction	Precision	We list it separately for entity extraction, where Precision = 57.4%, Recall = 91.7%, and relation extraction, where P = 80.6%, R = 63.2%.
entity extraction	Recall	We list it separately for entity extraction, where Precision = 57.4%, Recall = 91.7%, and relation extraction, where P = 80.6%, R = 63.2%.
coreference resolution	recall	After entity tagging and coreference resolution, the recall of entity mentions is 0.76 in our experiments.
Parsing	accuracy	Parsing accuracy has been shown to have a significant effect on downstream applications such as textual entailment) and machine translation, and most work on parsing evaluates accuracy to some extent.
Parsing	accuracy	Parsing accuracy has been shown to have a significant effect on downstream applications such as textual entailment) and machine translation, and most work on parsing evaluates accuracy to some extent.
machine translation	accuracy	Parsing accuracy has been shown to have a significant effect on downstream applications such as textual entailment) and machine translation, and most work on parsing evaluates accuracy to some extent.
parsing	accuracy	Parsing accuracy has been shown to have a significant effect on downstream applications such as textual entailment) and machine translation, and most work on parsing evaluates accuracy to some extent.
parsing	accuracy	We show 2 results for Ckylark with pruning threshold ϵ as 10 −5 and 10 −7 . These tables show that the result of Ckylark with ϵ = 10 −7 achieves nearly the same parsing accuracy as the Berkeley Parser.
parsing	accuracy	When the pruning threshold ϵ is smaller, parsing takes longer, but in all cases Ckylark is faster than Egret while achieving higher accuracy.
speech recognition	reward	As the speech recognition degrades, the POMDP policy acquires reward more slowly, but makes fewer mistakes and blind guesses compared to a conventional MDP policy.
parsing	accuracy	In Sections 4 and 5 we describe an experiment to test the parsing accuracy of a probabilistic TAG extracted automatically from the Penn Treebank.
TR	accuracy	A key assumption surrounding the bulk of past TR research has been that the greater the match stringency/linguistic awareness of the retrieval mechanism, the greater the final retrieval accuracy will become.
Information Retrieval (IR)	precision	Recent results from the TREC evaluations (() () show that Information Retrieval (IR) techniques alone are not sufficient for finding answers with high precision.
parsing	accuracy	We compare the results of the parsing step with our test corpus (annotated with syllable boundaries) and compute the accuracy.
DATE	SPEECH-ACT dimension	Thus, a central aspect of DATE is that it makes distinctions within three orthogonal dimensions of utterance classification: (1) a SPEECH-ACT dimension; (2) a TASK-SUBTASK dimension; and (3) a CONVERSATIONAL-DOMAIN dimension.
DATE	TASK-SUBTASK dimension	Thus, a central aspect of DATE is that it makes distinctions within three orthogonal dimensions of utterance classification: (1) a SPEECH-ACT dimension; (2) a TASK-SUBTASK dimension; and (3) a CONVERSATIONAL-DOMAIN dimension.
utterance classification	SPEECH-ACT dimension	Thus, a central aspect of DATE is that it makes distinctions within three orthogonal dimensions of utterance classification: (1) a SPEECH-ACT dimension; (2) a TASK-SUBTASK dimension; and (3) a CONVERSATIONAL-DOMAIN dimension.
utterance classification	TASK-SUBTASK dimension	Thus, a central aspect of DATE is that it makes distinctions within three orthogonal dimensions of utterance classification: (1) a SPEECH-ACT dimension; (2) a TASK-SUBTASK dimension; and (3) a CONVERSATIONAL-DOMAIN dimension.
coreference resolution	precision	First, we propose and evaluate three extra-linguistic modifications to the machine learning framework, which together provide substantial and statistically significant gains in coreference resolution precision.
parsing	accuracy	Our result shows that for about the same parsing accuracy, we only need to annotate a third of the samples as compared to the usual random selection method.
normalize abbreviations and acronyms	accuracy	I report on the results of an experiment involving training a number of ME models used to normalize abbreviations and acronyms on a sample of 10,000 rheumatology notes with ~89% accuracy.
parsing	precision	In order to seethe correspondence between parsing accuracy and PPL/WER performance, we also evaluated the labeled precision and recall statistics (LP/LR, the standard parsing accuracy measures) on the UPenn Treebank corpus.
parsing	recall statistics (LP/LR	In order to seethe correspondence between parsing accuracy and PPL/WER performance, we also evaluated the labeled precision and recall statistics (LP/LR, the standard parsing accuracy measures) on the UPenn Treebank corpus.
parsing	error rate	In parsing Wall Street Journal text, the method gives a 5.1% relative reduction in error rate over the model of).
translation disambiguation	BB	We then used the data to conduct translation disambiguation with BB, MB-B, and MB-D, as described in Section 5.
translation disambiguation	accuracy	We did not, however, conduct translation disambiguation on the words 'crane', 'sake', 'poach', 'axes', and 'motion', because the first four words do not frequently occur in the Encarta corpus, and the accuracy of choosing the major translation for the last word has already exceeded 98%.
ASR	accuracy	Although ASR accuracy was low, overall task completion was high, suggesting that the multimodal aspects of the system helped users to complete tasks.
ASR	completion	Although ASR accuracy was low, overall task completion was high, suggesting that the multimodal aspects of the system helped users to complete tasks.
translation	accuracy	shows a detailed comparison of the translation accuracy between our system, the commercial system, and the human translators.
generative parser	POS	notes that having his generative parser generate the POS of a constituent's head before the head itself increases performance by 2 points.
information extractions (IE)	speed	Current information extractions (IE) systems therefore do not attempt an exhaustive DNLP analysis of all aspects of a text, but rather try to analyse or "understand" only those text passages that contain relevant information, thereby warranting speed and robustness wrt.
parsing	accuracy	Ina previous paper), a boosting algorithm was used to rerank the output from an existing statistical parser, giving significant improvements in parsing accuracy on Wall Street Journal data.
Information Retrieval	exactness	Further, while Information Retrieval techniques are relatively successful at managing the vast quantity of text available on the Web, the exactness required of Question Answering systems makes them too slow and impractical for ordinary users.
IE	Fscore	In this paper we describe a domain-independent IE paradigm that is based on predicate-argument structures identified automatically by two different methods: (1) the statistical method reported in (); and (2) anew method based on inductive learning which obtains 17% higher Fscore over the first method when tested on the same data.
IE	accuracy	These results enforce our claim that predicate argument information for IE needs to be recognized with high accuracy.
role assignment	accuracy	shows that the new features increase the argument identification F-measure by 3.61%, and the role assignment accuracy with 4.29%.
argument identification task	precision	For the argument identification task, the head and content word features have a significant contribution for the task precision, whereas NE features contribute significantly to the task recall.
argument identification task	recall	For the argument identification task, the head and content word features have a significant contribution for the task precision, whereas NE features contribute significantly to the task recall.
parsing	accuracy	Treebank-based probabilistic parsing has been the subject of intensive research over the past few years, resulting in parsing models that achieve both broad coverage and high parsing accuracy (e.g.,.
Translation	WER	 Table 5: Translation results on the Verbmobil task.  type  automatic  human  System WER [%] PER [%] mWER [%] BLEU [%] SSER [%]  IBM  46.2  33.3  40.0  42.5  40.8  ITG  45.6  33.9  40.0  37.1  42.0
Translation	PER	 Table 5: Translation results on the Verbmobil task.  type  automatic  human  System WER [%] PER [%] mWER [%] BLEU [%] SSER [%]  IBM  46.2  33.3  40.0  42.5  40.8  ITG  45.6  33.9  40.0  37.1  42.0
Translation	BLEU	 Table 5: Translation results on the Verbmobil task.  type  automatic  human  System WER [%] PER [%] mWER [%] BLEU [%] SSER [%]  IBM  46.2  33.3  40.0  42.5  40.8  ITG  45.6  33.9  40.0  37.1  42.0
machine translation output legibility	BLEU score	Truecasing also enhances machine translation output legibility and yields a BLEU score improvement of 80.2%.
parsing	mean average precision	Some often used criteria are, for example, F-Measure for parsing, mean average precision for ranked retrieval, and BLEU or multi-reference word error rate for statistical machine translation.
parsing	BLEU	Some often used criteria are, for example, F-Measure for parsing, mean average precision for ranked retrieval, and BLEU or multi-reference word error rate for statistical machine translation.
translation	BLEU score	It was observed that the translation quality improved from 46.5% to 52.1% in BLEU score and from 59.2% to 65.1% in subjective evaluation.
translation	accuracy	We achieved 65.5% translation accuracy in a German-English translation task vs. 53.2% with IBM Model 4.
parsing-based NE	precision	First, some parsing-based NE rules are learned with high precision but limited recall.
parsing-based NE	recall	First, some parsing-based NE rules are learned with high precision but limited recall.
segmentation	accuracy	To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus.
MT	accurate	Source Pattern VP Note that a sizeable set of MT results is necessary in order to calculate an accurate BLEU score.
MT	BLEU score	Source Pattern VP Note that a sizeable set of MT results is necessary in order to calculate an accurate BLEU score.
MT	BLEU score	Therefore, we need all of the MT results from the evaluation corpus in order to calculate an accurate BLEU score.
feedback cleaning	BLEU	The idea of feedback cleaning is independent of BLEU.
MT	BLEU	Some automatic evaluation methods of MT quality other than BLEU have been proposed.
noun extraction	precision	According to, HanTag, which is a POS tagger, is an optimal tool in performing noun extraction in terms of the precision and the F-measure.
noun extraction	F-measure	According to, HanTag, which is a POS tagger, is an optimal tool in performing noun extraction in terms of the precision and the F-measure.
transformation	accuracy	The fourth line in transformation rules significantly contributed to improving the accuracy.
parsing	F-score	The use of parsing results contributed ¥ ¤ £ absolute increase in F-score.
NP Chunking	A	 Table 4: Results on NP Chunking. Training data is  WSJ section 15-18 of PTB. Test data is WSJ section  20. A = Accuracy of IOB tagging. P = NP chunk  Precision. R = NP chunk Recall. F = F-score. Brill- POS = fast TBL with Brill's POS tags. Tri-STAG =  fast TBL with supertags given by Srinivas' trigram- based supertagger. SNoW-STAG = fast TBL with  supertags given by our SNoW supertagger. SNoW- STAG2 = fast TBL with augmented supertags given  by our SNoW supertagger. GOLD-POS = fast TBL  with gold standard POS tags. GOLD-STAG = fast  TBL with gold standard supertags.
NP Chunking	Accuracy	 Table 4: Results on NP Chunking. Training data is  WSJ section 15-18 of PTB. Test data is WSJ section  20. A = Accuracy of IOB tagging. P = NP chunk  Precision. R = NP chunk Recall. F = F-score. Brill- POS = fast TBL with Brill's POS tags. Tri-STAG =  fast TBL with supertags given by Srinivas' trigram- based supertagger. SNoW-STAG = fast TBL with  supertags given by our SNoW supertagger. SNoW- STAG2 = fast TBL with augmented supertags given  by our SNoW supertagger. GOLD-POS = fast TBL  with gold standard POS tags. GOLD-STAG = fast  TBL with gold standard supertags.
NP Chunking	Recall	 Table 4: Results on NP Chunking. Training data is  WSJ section 15-18 of PTB. Test data is WSJ section  20. A = Accuracy of IOB tagging. P = NP chunk  Precision. R = NP chunk Recall. F = F-score. Brill- POS = fast TBL with Brill's POS tags. Tri-STAG =  fast TBL with supertags given by Srinivas' trigram- based supertagger. SNoW-STAG = fast TBL with  supertags given by our SNoW supertagger. SNoW- STAG2 = fast TBL with augmented supertags given  by our SNoW supertagger. GOLD-POS = fast TBL  with gold standard POS tags. GOLD-STAG = fast  TBL with gold standard supertags.
NP Chunking	F-score	 Table 4: Results on NP Chunking. Training data is  WSJ section 15-18 of PTB. Test data is WSJ section  20. A = Accuracy of IOB tagging. P = NP chunk  Precision. R = NP chunk Recall. F = F-score. Brill- POS = fast TBL with Brill's POS tags. Tri-STAG =  fast TBL with supertags given by Srinivas' trigram- based supertagger. SNoW-STAG = fast TBL with  supertags given by our SNoW supertagger. SNoW- STAG2 = fast TBL with augmented supertags given  by our SNoW supertagger. GOLD-POS = fast TBL  with gold standard POS tags. GOLD-STAG = fast  TBL with gold standard supertags.
NP Chunking	Brill	 Table 4: Results on NP Chunking. Training data is  WSJ section 15-18 of PTB. Test data is WSJ section  20. A = Accuracy of IOB tagging. P = NP chunk  Precision. R = NP chunk Recall. F = F-score. Brill- POS = fast TBL with Brill's POS tags. Tri-STAG =  fast TBL with supertags given by Srinivas' trigram- based supertagger. SNoW-STAG = fast TBL with  supertags given by our SNoW supertagger. SNoW- STAG2 = fast TBL with augmented supertags given  by our SNoW supertagger. GOLD-POS = fast TBL  with gold standard POS tags. GOLD-STAG = fast  TBL with gold standard supertags.
detecting grammatical and lexical errors made by Japanese learners of English	accuracy	This paper describes a method of detecting grammatical and lexical errors made by Japanese learners of English and other techniques that improve the accuracy of error detection with a limited amount of training data.
detection of unknown words	recall	This can ensure a high precision for the detection of unknown words, but unfortunately the recall is not quite satisfactory.
organization name extraction	Recall	 Table 3: Results for organization name extraction  Recall Precision F-measure  For  54.66  70.85  61.71  Back 63.25  79.36  70.40
organization name extraction	Back	 Table 3: Results for organization name extraction  Recall Precision F-measure  For  54.66  70.85  61.71  Back 63.25  79.36  70.40
English question classification	Accuracy	 Table 4: Results of English question classification (Accuracy)
automatic speech recognition (ASR)	accuracy	The question of how to integrate high-level knowledge representations of language with automatic speech recognition (ASR) is becoming more important as (1) speech recognition technology matures, (2) the rate of improvement of recognition accuracy decreases, and (3) the need for additional information (beyond simple transcriptions) becomes evident.
speech recognition	accuracy	The question of how to integrate high-level knowledge representations of language with automatic speech recognition (ASR) is becoming more important as (1) speech recognition technology matures, (2) the rate of improvement of recognition accuracy decreases, and (3) the need for additional information (beyond simple transcriptions) becomes evident.
parsing	accuracy	The parsing accuracy for parsing word lattices was not directly evaluated as we did not have annotated parse trees for comparison.
parsing HUB-1 n-best word lattices	OP	 Table 2: Results for parsing HUB-1 n-best word lattices and lists: OP = overparsing, S = substutitions (%),  D = deletions (%), I = insertions (%), T = total WER (%
parsing HUB-1 n-best word lattices	T	 Table 2: Results for parsing HUB-1 n-best word lattices and lists: OP = overparsing, S = substutitions (%),  D = deletions (%), I = insertions (%), T = total WER (%
parsing HUB-1 n-best word lattices	WER	 Table 2: Results for parsing HUB-1 n-best word lattices and lists: OP = overparsing, S = substutitions (%),  D = deletions (%), I = insertions (%), T = total WER (%
parsing HUB-1 words lattices	SER	 Table 3: Comparison of WER for parsing HUB-1 words lattices with best results of other works. SER =  sentence error rate. WER = word error rate. "Speech-like" transformations were applied to all training  corpora.
parsing HUB-1 words lattices	WER	 Table 3: Comparison of WER for parsing HUB-1 words lattices with best results of other works. SER =  sentence error rate. WER = word error rate. "Speech-like" transformations were applied to all training  corpora.
WSD	accuracy	However, the most accurate WSD systems are those which require manually sense tagged data in the first place, and their accuracy depends on the quantity of training examples () available.
classification	accuracy	It can be interpreted as classification accuracy when for each cluster we treat the majority of instances that have the same sense as correctly classified.
Mutual information (MI)	purity	Mutual information (MI) is more theoretically well-founded than purity.
Random clustering	MI	Random clustering also has a zero MI in the limit.
EM clustering	purity	The performance of the EM clustering model on 12 Chinese verbs measured by purity and normalized mutual information (NMI) that the EM clustering model has indeed learned such combinatory patterns from the training data.
EM clustering	normalized mutual information (NMI)	The performance of the EM clustering model on 12 Chinese verbs measured by purity and normalized mutual information (NMI) that the EM clustering model has indeed learned such combinatory patterns from the training data.
EM clustering	purity	 Table 2. The performance of the EM clustering model on 12 Chinese verbs measured  by purity and normalized mutual information (NMI)
Entity tagging	F1	Entity tagging has been thoroughly addressed by many statistical machine learning techniques, obtaining greater than 90% F1 on many datasets.
Role Extraction	TREATMENT	A related task is Role Extraction (also called, in the literature, "information extraction" or "named entity recognition"), defined as: given a sentence such as "The fluoroquinolones for urinary tract infections: a review", extract all and only the strings of text that correspond to the roles TREATMENT (fluoroquinolones) and DISEASE (urinary tract infections).
speech recognition	accuracy	For example, speech recognition systems prefer "longer words" to achieve higher accuracy whereas information retrieval systems prefer "shorter words" to obtain higher recall rates, etc.
speech recognition	recall rates	For example, speech recognition systems prefer "longer words" to achieve higher accuracy whereas information retrieval systems prefer "shorter words" to obtain higher recall rates, etc.
speech recognition	accuracy	For example, speech recognition systems prefer "longer words" to achieve higher accuracy whereas information retrieval systems prefer "shorter words" to obtain higher recall rates, etc.
speech recognition	recall rates	For example, speech recognition systems prefer "longer words" to achieve higher accuracy whereas information retrieval systems prefer "shorter words" to obtain higher recall rates, etc.
dependency structure	accuracy	In the next section, we discuss several models of dependency structure, and throughout this paper we report the accuracy of various methods at recovering gold-standard dependency parses from various corpora, detailed here.
MEMT	HFA	We evaluate the MEMT performance by running HFA and BLEU on MEMT selected translations block by block, and giving average performance over the blocks.
MEMT	BLEU	We evaluate the MEMT performance by running HFA and BLEU on MEMT selected translations block by block, and giving average performance over the blocks.
Translation	WER PER 100-BLEU Memory Time/Sentence	 Table 4: Translation results for different tasks compared to similar systems using the alignment template  (AT) approach (Tests were performed on a 1.2GHz AMD Athlon).  Task  System  Translation  WER PER 100-BLEU Memory Time/Sentence  [%]  [%]  [MB]  [ms]  Eutrans  FSA  Spanish → English 8.12 7.64  10.7  6-8  20  AT  8.25  - - - - FUB  FSA  Italian → English  27.0 21.5  37.7  3-5  22  AT  23.7 18.1  36.0  - - Verbmobil  FSA  German → English 48.3 41.6  69.8  65-90  460  AT  40.5 30.1  62.2  - - PF-Star  FSA  Italian → English  39.8 34.1  58.4  12-15  35  AT  36.8 29.1  54.3  - -
question splitting	precision	For the task of question splitting our system has performed, in terms of precision and recall, 85% and 71%, respectively.
question splitting	recall	For the task of question splitting our system has performed, in terms of precision and recall, 85% and 71%, respectively.
deep parsing of questions and texts	accuracy	Such methods include redundancy (getting the same answer from multiple documents, sources, or algorithms), deep parsing of questions and texts (hence improving the accuracy of confidence measures), inferencing (proving the answer from information in texts plus background knowledge) and sanity-checking (verifying that answers are consistent with known facts).
MT	recall	For example, evaluation guidelines for adequacy evaluation of MT have a low p/r ratio, because of the high emphasis on recall (i.e., content is rewarded) and low emphasis on precision (i.e., verbosity is not penalized); on the other hand, evaluation guidelines for fluency of MT have a high p/r ratio, because of the low emphasis on recall (i.e., content is not rewarded) and high emphasis on wording (i.e., extraneous words are penalized).
MT	precision	For example, evaluation guidelines for adequacy evaluation of MT have a low p/r ratio, because of the high emphasis on recall (i.e., content is rewarded) and low emphasis on precision (i.e., verbosity is not penalized); on the other hand, evaluation guidelines for fluency of MT have a high p/r ratio, because of the low emphasis on recall (i.e., content is not rewarded) and high emphasis on wording (i.e., extraneous words are penalized).
MT	recall	For example, evaluation guidelines for adequacy evaluation of MT have a low p/r ratio, because of the high emphasis on recall (i.e., content is rewarded) and low emphasis on precision (i.e., verbosity is not penalized); on the other hand, evaluation guidelines for fluency of MT have a high p/r ratio, because of the low emphasis on recall (i.e., content is not rewarded) and high emphasis on wording (i.e., extraneous words are penalized).
MT	recall	For example, evaluation guidelines for adequacy evaluation of MT have a low p/r ratio, because of the high emphasis on recall (i.e., content is rewarded) and low emphasis on precision (i.e., verbosity is not penalized); on the other hand, evaluation guidelines for fluency of MT have a high p/r ratio, because of the low emphasis on recall (i.e., content is not rewarded) and high emphasis on wording (i.e., extraneous words are penalized).
Machine Translation	BLEU	Since metric AEv(1,4) is almost the same as the BLEU metric (modulo stemming and stop word elimination for unigrams), our results confirm the current practice in the Machine Translation community, which commonly uses BLEU for automatic evaluation.
coverage	recall score	A final evaluation score for coverage was obtained using a coverage score computed as a weighted recall score (see (Lin and Hovy 2003) for more information on the human summary evaluation).
coverage	precision	As mentioned in Section 2, the evaluation guidelines for coverage have a low precision/recall ratio, whereas AS is an application with low faithfulness/compactness ratio.
coverage	recall ratio	As mentioned in Section 2, the evaluation guidelines for coverage have a low precision/recall ratio, whereas AS is an application with low faithfulness/compactness ratio.
coverage	AS	As mentioned in Section 2, the evaluation guidelines for coverage have a low precision/recall ratio, whereas AS is an application with low faithfulness/compactness ratio.
QA evaluation	precision	On the guideline side, the guideline package used in this first QA evaluation has a low precision/recall ratio, because the human judge is asked to evaluate based on the content provided by a given answer (high recall), but is asked to disregard the conciseness (or lack thereof) of the answer (low precision); consequently, systems that focus on   giving correct and concise answers are not distinguished from systems that give correct answers, but have no regard for concision.
QA evaluation	recall ratio	On the guideline side, the guideline package used in this first QA evaluation has a low precision/recall ratio, because the human judge is asked to evaluate based on the content provided by a given answer (high recall), but is asked to disregard the conciseness (or lack thereof) of the answer (low precision); consequently, systems that focus on   giving correct and concise answers are not distinguished from systems that give correct answers, but have no regard for concision.
QA evaluation	recall	On the guideline side, the guideline package used in this first QA evaluation has a low precision/recall ratio, because the human judge is asked to evaluate based on the content provided by a given answer (high recall), but is asked to disregard the conciseness (or lack thereof) of the answer (low precision); consequently, systems that focus on   giving correct and concise answers are not distinguished from systems that give correct answers, but have no regard for concision.
QA evaluation	precision	On the guideline side, the guideline package used in this first QA evaluation has a low precision/recall ratio, because the human judge is asked to evaluate based on the content provided by a given answer (high recall), but is asked to disregard the conciseness (or lack thereof) of the answer (low precision); consequently, systems that focus on   giving correct and concise answers are not distinguished from systems that give correct answers, but have no regard for concision.
translation	Adequacy	We show that this extension gives additional information about evaluated texts; in particular it allows us to measure translation Adequacy, which, for statistical MT systems, is often overestimated by the baseline BLEU method.
translation	BLEU	We show that this extension gives additional information about evaluated texts; in particular it allows us to measure translation Adequacy, which, for statistical MT systems, is often overestimated by the baseline BLEU method.
MT	BLEU	We show that this extension gives additional information about evaluated texts; in particular it allows us to measure translation Adequacy, which, for statistical MT systems, is often overestimated by the baseline BLEU method.
MT	Adequacy	Automatic methods for evaluating different aspects of MT quality -such as Adequacy, Fluency and Informativeness -provide an alternative to an expensive and time-consuming process of human MT evaluation.
MT	Fluency	Automatic methods for evaluating different aspects of MT quality -such as Adequacy, Fluency and Informativeness -provide an alternative to an expensive and time-consuming process of human MT evaluation.
MT	Informativeness	Automatic methods for evaluating different aspects of MT quality -such as Adequacy, Fluency and Informativeness -provide an alternative to an expensive and time-consuming process of human MT evaluation.
MT evaluation	BLEU	The proposed weighted N-gram model for MT evaluation is tested on a set of translations by four different MT systems available in the DARPA corpus, and is compared with the results of the baseline BLEU method with respect to their correlation with human evaluation scores.
MT	BLEU	The proposed weighted N-gram model for MT evaluation is tested on a set of translations by four different MT systems available in the DARPA corpus, and is compared with the results of the baseline BLEU method with respect to their correlation with human evaluation scores.
translation	Fluency	It computes evaluation scores with tf.idf and S-score weights for translation Adequacy and Fluency.
MT evaluation	Precision	In the second stage we carried out N-gram based MT evaluation, measuring Precision and Recall of N-grams in MT output using a single human reference translation.
MT evaluation	Recall	In the second stage we carried out N-gram based MT evaluation, measuring Precision and Recall of N-grams in MT output using a single human reference translation.
MT	Adequacy	With respect to evaluating MT systems, the correlation for the weighted N-gram model was found to be stronger, for both Adequacy and Fluency, the improvement being highest for Adequacy.
MT	Fluency	With respect to evaluating MT systems, the correlation for the weighted N-gram model was found to be stronger, for both Adequacy and Fluency, the improvement being highest for Adequacy.
MT	BLEU	These results are due to the fact that the weighted N-gram model gives much more accurate predictions about the statistical MT system "Candide", whereas the standard BLEU approach tends to over-estimate its performance for translation Adequacy.
Speaker ranking	accuracy	 Table 2. Speaker ranking accuracy
TTS	timing	In the third experiment we wanted to know if TTS applications that made use of purely textual input could be aided by the addition of timing and rhythm variables that can be gleaned from a text string.
Minimizing	Length	Minimizing the Length of Non-Mixed Initiative Dialogs
TFM	accuracy	 Table 4: Top parameter combinations for TFM by improvement in classification accuracy. Vocab fre- quency min. is the minimum number of times a term must appear in the corpus in order to be included.
TFM	Vocab fre- quency min.	 Table 4: Top parameter combinations for TFM by improvement in classification accuracy. Vocab fre- quency min. is the minimum number of times a term must appear in the corpus in order to be included.
verb classification	Adjusted Rand measure	(Schulte im reports that the evaluation method that is most appropriate to the task of unsupervised verb classification is the Adjusted Rand measure.
Improving	Accuracy	Improving the Accuracy of Subcategorizations Acquired from Corpora
alignment	precision	Experimental results show a significant improvement in terms of both alignment precision and recall.
alignment	recall	Experimental results show a significant improvement in terms of both alignment precision and recall.
text classification	KL-divergence	We define anew feature selection score for text classification based on the KL-divergence between the distribution of words in training documents and their classes.
classification	accuracy	compares classification accuracy for the three scoring functions.
POS tagging	accuracy	 Table 2: POS tagging accuracy.
SVM-shiftreduce parsing	MIRA	Y&M2003 is the SVM-shiftreduce parsing model of, N&S2004 is the memory-based learner of and MIRA is the the system we have described.
Dependency parsing	Accuracy	 Table 2: Dependency parsing results for English and Czech. Accuracy is the number of words that correctly  identified their parent in the tree. Root is the number of trees in which the root word was correctly identified.  For Czech this is f-measure since a sentence may have multiple roots. Complete is the number of sentences  for which the entire dependency tree was correct.
Dependency parsing	Complete	 Table 2: Dependency parsing results for English and Czech. Accuracy is the number of words that correctly  identified their parent in the tree. Root is the number of trees in which the root word was correctly identified.  For Czech this is f-measure since a sentence may have multiple roots. Complete is the number of sentences  for which the entire dependency tree was correct.
parsing	accuracy	Experiments using data from the Prague Dependency Treebank show that the combined system can handle non-projective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.
parsing	accuracy	From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.
projective parsing of non-projective structures	accuracy	Still, from a theoretical point of view, projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.
parsing	attachment score	The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.
recognition of NEs	F-scores	The recognition of NEs as well as their semantic categories was done by a HMM based NER, which was trained for the MUC NE task and obtained high F-scores of 96.9% (MUC-6) and 94.3% (MUC-7) ().
sequence labeling problem-POS tagging	EM	Applied to a sequence labeling problem-POS tagging given a tagging dictionary and unlabeled text-contrastive estimation outper-forms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.
SMT	BLEU	We show that the unsupervised SMT model, trained on parallel data without any manual sense annotation, yields higher BLEU scores than the case where the SMT model makes use of the lexical choice predictions from the supervised WSD model, which are more expensive to create.
relation detection	F-measure	It also shows that our fea-ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.
relation extraction	NONE	In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a "NONE" class for the case where the two mentions are not related.
ASR	accuracy	Experiments evaluating ASR accuracy on iCampus, highlighting empirical aspects of PSPL lattices as well as search accuracy results are reported in Section 6.
Breakdown of true positive relations	Predicted	 Table 4: Breakdown of true positive relations by  type that were returned by each system. Each cell  contains three numbers, Actual:Predicted:Correct,  which represents for each arity the actual, predicted  and correct number of relations for each system.
Breakdown of true positive relations	Correct	 Table 4: Breakdown of true positive relations by  type that were returned by each system. Each cell  contains three numbers, Actual:Predicted:Correct,  which represents for each arity the actual, predicted  and correct number of relations for each system.
Extracting information from resumes	precision	Extracting information from resumes with high precision and recall is not an easy task.
Extracting information from resumes	recall	Extracting information from resumes with high precision and recall is not an easy task.
ASR	GCLM	presents the word-error rates on rt02 and rt03 of the baseline ASR system, 1000-best perceptron and GCLM results from under this condition, and our 1000-best perceptron results.
translation from German to English	Bleu score	We describe experiments on translation from German to English, showing an improvement from 25.2% Bleu score fora baseline system to 26.8% Bleu score for the system with reordering, a statistically significant improvement.
translation from German to English	Bleu score	We describe experiments on translation from German to English, showing an improvement from 25.2% Bleu score fora baseline system to 26.8% Bleu score for the system with reordering, a statistically significant improvement.
translation	accuracy	The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrase-based system, which we will show leads to significant improvements in translation accuracy.
identification task	recall	On the other hand, the system's performance on the identification task is quite a bit lower, achieving only 80% recall with 86% precision.
identification task	precision	On the other hand, the system's performance on the identification task is quite a bit lower, achieving only 80% recall with 86% precision.
SCF types	precision	The experiments show that the system is able to detect SCF types with 70% precision and 66% recall rate.
SCF types	recall rate	The experiments show that the system is able to detect SCF types with 70% precision and 66% recall rate.
detecting comprehensive sets of SCFs	accuracy	Subsequent research has yielded systems for English) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (), besides systems fora number of other languages (e.g. ().
Hamming search	accuracy	 Table 3: Hamming search accuracy (Beam B = 25)
Hamming search	Beam B	 Table 3: Hamming search accuracy (Beam B = 25)
Hamming search	accuracy	 Table 4: Hamming search accuracy (Beam B = 100)
Hamming search	Beam B	 Table 4: Hamming search accuracy (Beam B = 100)
prediction	accuracy	We show that centrality measures increase the prediction accuracy.
classifying English inclusions	accuracy	When classifying English inclusions in the EU data, accuracy decreased slightly by 0.3%.
classification	accuracy	We find that a significant improvement in classification accuracy can be achieved when we combine both dependency and constituency extraction methods.
dependency parsing	accuracy	Our results suggest that dependency parsing features are reasonable extraction patterns, as their accuracy rates are competitive against the model based solely on constituency rules.
Classification	Accuracy	 Table 1: Classification Accuracy, labeled training &  testing data size
Name Discrimination	F-measure	 Table 1: Name Discrimination (F-measure)
bracketing	precision	Recall is the rate that bracketing given by hand are also given by machine, and precision is the rate that bracketing given by machine are also given by hand.
MT	BLEU	Other metrics assess the impact of alignments externally, e.g., different alignments are tested by comparing the corresponding MT outputs using automated evaluation metrics (e.g., BLEU () or METEOR ().
MT	METEOR	Other metrics assess the impact of alignments externally, e.g., different alignments are tested by comparing the corresponding MT outputs using automated evaluation metrics (e.g., BLEU () or METEOR ().
segmentation	accuracy	It is surprising to notice that the segmentation accuracy in this experiment was actually slightly higher than achieved in Experiment 1 (especially given that ASR word error rates were generally above 20%).
segmentation	accuracy	Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.
WSD	accuracy	By using well calibrated probabilities, we are able to estimate the sense priors effectively to achieve significant improvements in WSD accuracy.
WSD	accuracy	They found that training a WSD system on one part (BC or WSJ) of the DSO corpus and applying it to the other part can result in an accuracy drop of 12% to 19%.
WSD	accuracy	For instance, given the noun interest and the WSJ part of the DSO corpus, we attempt to estimate the proportion of each sense of interest occurring in WSJ and showed that these estimates help to improve WSD accuracy.
WSD	accuracy	Using these estimates improves WSD accuracy and we achieve results that are significantly better than using our earlier approach described in).
parsing	accuracy	Therefore, fora monologue sentence the parsing time would increase and the parsing accuracy would decrease.
parsing	accuracy	shows the parsing accuracy of both methods.
parsing	accuracy	As mentioned above, it is clear that our method is more effective than the conventional method in shortening parsing time and increasing parsing accuracy.
parsing	accuracy	As mentioned above, it is clear that our method is more effective than the conventional method in shortening parsing time and increasing parsing accuracy.
Parsing	accuracy	 Table 8: Parsing accuracy for dependency rela- tions over clause boundaries
parsing	accuracy	We ran a series of evaluations on held-out data in order to determine the impact of the different features which we described in section 2 on the parsing accuracy.
EC prediction	coindexation (CI)	The results are shown in: Differences between the baseline f-scores for labeled bracketing, EC prediction, and coindexation (CI) and the f-scores without the specified feature.
EC prediction	co- indexation (CI)	 Table 4: Differences between the baseline f-scores  for labeled bracketing, EC prediction, and co- indexation (CI) and the f-scores without the spec- ified feature.
generative	accuracy	However, generative models generally do not achieve the same accuracy as discriminatively trained models, and therefore it is preferable to focus on discriminative approaches.
sequential segmentation tasks (SSTs)	F-score	For example, sequential segmentation tasks (SSTs), such as text chunking and named entity recognition, are generally evaluated with the segmentation F-score.
text chunking	F-score	For example, sequential segmentation tasks (SSTs), such as text chunking and named entity recognition, are generally evaluated with the segmentation F-score.
SSTs	F-score loss function	After describing the new framework, as an example of optimizing multivariate evaluation measures, we focus on SSTs and introduce a segmentation F-score loss function for CRFs.
text classification	BLEU-score	Several non-linear objective functions, such as F-score for text classification (, and BLEU-score and some other evaluation measures for statistical machine translation, have been introduced with reference to the framework of MCE criterion training.
parsing	accuracy	Transforming syntactic representations in order to improve parsing accuracy has been exploited successfully in statistical parsing systems using constituency-based representations.
syntactic parsing	accuracy	It has become increasingly clear that the choice of suitable internal representations can be a very important factor in data-driven approaches to syntactic parsing, and that accuracy can often be improved by internal transformations of a given kind of representation.
parsing	accuracy	While the experiments reported in section 4.1 deal with pure treebank transformations, in order to establish an upper bound on what can be achieved in parsing, the experiments presented in section 4.2 examine the effects of different transformations on parsing accuracy.
parsing	accuracy	While the experiments reported in section 4.1 deal with pure treebank transformations, in order to establish an upper bound on what can be achieved in parsing, the experiments presented in section 4.2 examine the effects of different transformations on parsing accuracy.
parsing	accuracy	Finally, in section 4.3, we combine these transformations with previously proposed techniques in order to optimize overall parsing accuracy.
parsing	accuracy	In order to estimate the expected reduction in parsing accuracy due to this distortion, we first consider a pure treebank transformation experiment, where we compare τ −1 (τ (∆ t )) to ∆ t , for all the different transformations τ defined in the previous section.
parsing	accuracy	Rows 2-5 show that transforming coordinate structures to MS improves parsing accuracy compared to the baseline, regardless of which transformation and inverse transformation are used.
parsing	accuracy	The paper reports on a series of experiments using varying models of su-pertags that significantly increase the parsing accuracy.
parsing	accuracy	estimated that perfect supertag information already provides fora parsing accuracy of 98% if a correct supertag assignment were available.
supertag prediction	accuracy	Firstly, all types of supertag prediction, even the very basic model A which predicts only edge labels, improve the overall accuracy of parsing, although the baseline is already quite high.
parsing	accuracy	As expected, an improved accuracy of supertagging would lead to improved parsing accuracy in each case.
parsing	accuracy	As shows, pruning supertags that are wrong more often than they are right results in a further small improvement in parsing accuracy: unlabelled syntax accuracy rises up to 92.1% against the 91.8% if all supertags of model J are used.
parsing	accuracy	As shows, pruning supertags that are wrong more often than they are right results in a further small improvement in parsing accuracy: unlabelled syntax accuracy rises up to 92.1% against the 91.8% if all supertags of model J are used.
Parsing	accuracy	However, the effect is not very noticeable, so that it would be almost certainly more useful to: Parsing accuracy with empirically pruned supertag predictions.
parsing	accuracy	 Table 5: Unlabelled and labelled parsing accuracy  with a simulated perfect supertagger.
Parsing	accuracy	 Table 6: Parsing accuracy with empirically pruned  supertag predictions.
parsing	accuracy	This explains why their addition increases parsing accuracy even when their own accuracy is markedly lower than even the baseline (line 1).
parsing	accuracy	This explains why their addition increases parsing accuracy even when their own accuracy is markedly lower than even the baseline (line 1).
Structural/labelled parsing	accuracy	 Table 1: Structural/labelled parsing accuracy with  various predictor components.
parsing	f -scores	 Table 2: Effects of adding NANC sentences to WSJ  training data on parsing performance. f -scores  for the parser with and without the WSJ reranker  are shown when evaluating on BROWN develop- ment. For this experiment, we use the WSJ-trained  reranker.
parsing	BROWN	 Table 2: Effects of adding NANC sentences to WSJ  training data on parsing performance. f -scores  for the parser with and without the WSJ reranker  are shown when evaluating on BROWN develop- ment. For this experiment, we use the WSJ-trained  reranker.
parsing	accuracy	These models simulate the reading time advantage for parallel structures found inhuman data, and also yield a small increase in overall parsing accuracy.
parsing	accuracy	In Section 4, we test the engineering aspects of our model by demonstrating that a small increase in parsing accuracy can be obtained with a parallelism-based model.
parsing	accuracy	However, in this section we explore what effects our modifications have to overall coverage, and, perhaps more interestingly, to parsing accuracy.
classification task	accuracy	Results showed that breaking down the classification task into two stages increased overall accuracy, and the number of failures was reduced to 30.
lexical substitution	recall	This scenario, which is generally referred here as lexical substitution, is a common technique for increasing recall in Natural Language Processing (NLP) applications.
binary classification task	precision	However, addressing directly the binary classification task has practical advantages and can yield high precision values, as desired in precision-oriented applications such as IR and QA.
sentence alignment	accuracy	We have compared sentence alignment accuracy with and without DOM tree alignment support.
question answering (QA) tasks	accuracy	Either scenario will be useful in training parsers for use in question answering (QA) tasks, and it also provides a suitable resource to evaluate the accuracy of these parsers on questions.
MT	BLEU	This new model leads to significant improvements in MT quality as measured by BLEU ().
word order restoration task	BLEU	 Table 3: BLEU scores for the word order restoration task. The BLEU scores reported here are with 1 reference.  The input is the reordered English in the reference. The 95% Confidence σ ranges from 0.011 to 0.016
word order restoration task	Confidence σ	 Table 3: BLEU scores for the word order restoration task. The BLEU scores reported here are with 1 reference.  The input is the reordered English in the reference. The 95% Confidence σ ranges from 0.011 to 0.016
machine translation task	Confidence σ	 Table 5: BLEU scores for the Arabic-English machine translation task. The 95% Confidence σ ranges from 0.0158  to 0.0176. s is the number of words temporarily skipped, and w is the word permutation window size.
contrastive estimation	accuracy	We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1-17% (absolute) over CE (and 8-30% over EM), achieving to our knowledge the best results on this task to date.
EM-trained dependency  parsing	F1	 Table 1: Baseline performance of EM-trained dependency  parsing models: F1 on non-$ attachments in test data, with  various model selection conditions (3 initializers × 6 smooth- ing values). The languages are listed in decreasing order by  the training set size. Experimental details can be found in the  appendix.
Relevance Feedback (G.)	accuracy	Relevance Feedback (G.) is the main post-query method for automatically improving a system's accuracy of a user's individual need.
ASR	precision	In experiments using support vector machines (SVMs) and speech data from Japanese newspaper articles, the proposed method outperformed a simple application of text-based NER to ASR results in NER F-measure by improving precision.
ASR	accuracy	Such rejection is especially effective when ASR accuracy is relatively low because many misrecognized words maybe extracted as NEs, which would decrease NER precision.
ASR	precision	Such rejection is especially effective when ASR accuracy is relatively low because many misrecognized words maybe extracted as NEs, which would decrease NER precision.
ASR	ASR confidence scores	In testing, ASR errors are identified by ASR confidence scores and are used for the NER.
NER	recall	Note that NER recall with ASR results could not exceed the rate of the remaining NEs after ASR (about 82%) because NEs containing ASR errors were always lost.
ASR word	accuracy	 Table 4: NER results in averaged NER F-measure, precision, and recall without considering NER-level  rejection (t o = 0). ASR word accuracy was 79.45%, and 82.00% of NEs remained in ASR results.
decision tree induction	ÎÅ ÐÐØ	We use C4.5 release 8 for decision tree induction and Ë ÎÅ ÐÐØ for support vector machines.
tagging	accuracy	However, for grammar formalisms which use more fine-grained grammatical categories , for example TAG and CCG, tagging accuracy is much lower.
POS tagging	accuracy	Although POS tagging accuracy seems high, maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging.
POS tagging	accuracy	Our results first demonstrate that a surprising in-crease in POS tagging accuracy can be achieved with only a tiny increase in ambiguity; and second that maintaining some POS ambiguity can significantly improve the accuracy of the supertagger.
POS tagging	accuracy	Our results first demonstrate that a surprising in-crease in POS tagging accuracy can be achieved with only a tiny increase in ambiguity; and second that maintaining some POS ambiguity can significantly improve the accuracy of the supertagger.
POS tagging	accuracy	We performed several sets of experiments for POS tagging and CCG supertagging to explore the trade-off between ambiguity and tagging accuracy.
POS tagging	accuracy	For both POS tagging and supertagging we varied the average number of tags assigned to each word, to see whether it is possible to significantly increase tagging accuracy with only a small increase in ambiguity.
Supertagging	accuracy	 Table 2: Supertagging accuracy on Section 00 us- ing different approaches with multi-tagger ambi- guity fixed at 1.4 categories per word.
translation	accuracy	Through experiments, we show, that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task.
Phoneme-to-text transcription	accuracy	 Table 3: Phoneme-to-text transcription accuracy.
word alignment	accuracy	In this paper, we improve word alignment and, subsequently, MT accuracy by developing a range of increasingly sophisticated methods: 1.
MT	accuracy	In this paper, we improve word alignment and, subsequently, MT accuracy by developing a range of increasingly sophisticated methods: 1.
generative	accuracy	Successful discriminative parsers have relied on generative models to reduce training time and raise accuracy above generative baselines).
answer ranking	re-ranking	Section 4 and 5 discuss how to incorporate the correlations for answer ranking and re-ranking.
question answering	Mean Reciprocal Rank (MRR).	The performance of the question answering system was measured using Mean Reciprocal Rank (MRR).: The impact of the algorithm for propagating predicate arguments over the question answering system
classification	F 1 measure	The classification performance was evaluated using the F 1 measure for the individual role and ILC classifiers and the accuracy for the multiclassifiers.
classification	accuracy	The classification performance was evaluated using the F 1 measure for the individual role and ILC classifiers and the accuracy for the multiclassifiers.
MT	Bleu score	Corrected MT achieved a Bleu score of 0.82 when compared to the human-generated reference translations.
collocation extraction	precision	We compare our hybrid method (based on syntactic processing of texts) against the window method classically used in collocation extraction, from the point of view of their precision with respect to grammaticality and collocability.
SMT lexicon	BLEU	The standard SMT lexicon model is not optimal, as measured by BLEU, for any of the languages or training set sizes considered.
normalization	accuracy (a)	For normalization, we use accuracy (a), which is commonly accepted by machine translation researchers as a standard evaluation criterion.
Novel Association Measures	Double Checking	Novel Association Measures Using Web Search with Double Checking
retrieval of biomedical correlations	precision	Experiments on the retrieval of biomedical correlations revealed that the cost is sufficiently small for real-time applications and that the retrieval precision is significantly improved.
spelling correction	accuracy	The key contribution of our work is identifying that we can successfully use the evidence of distributional similarity to achieve better spelling correction accuracy.
annotation projection of semantic roles conveyed by sentential constituents	AGENT	In previous work) we considered the annotation projection of semantic roles conveyed by sentential constituents such as AGENT, PATIENT, or INSTRUMENT.
annotation projection of semantic roles conveyed by sentential constituents	PATIENT	In previous work) we considered the annotation projection of semantic roles conveyed by sentential constituents such as AGENT, PATIENT, or INSTRUMENT.
Rote extractors	accuracy	In this paper, we describe some contributions to the training of Rote extractors, including a procedure for generalizing the patterns, and a more complex way of calculating their accuracy.
Machine Translation Evaluation	Acceptability	We present a comparative study on Machine Translation Evaluation according to two different criteria: Human Likeness and Human Acceptability.
SMS translation	BLEU score	Another experiment of translating SMS texts from English to Chinese on a separate SMS text corpus shows that, using SMS normalization as MT preprocessing can largely boost SMS translation performance from 0.1926 to 0.3770 in BLEU score.
parse selection	accuracy	1 shows that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute less than 1% to parse selection accuracy when test data is in the same domain, and yield no improvement for test data selected from the Brown Corpus.
dependency parsing	accuracy	Using these as guidelines we devise an algo-rithm for dependency parsing, prove that it satisfies these principles, and show experimentally that this improves the accuracy of the resulting tree.
PRO	precision	These tests show that the PRO algorithm can identify the correct relations for compounds, and the correct senses of those relations, with high precision.
Detecting event nominals	similarity	Detecting event nominals is also an important step in detecting relations between event mentions, as in the causal relation between the demonstrations and the withdrawal and the similarity relation between the bombing and the blast in (1).
Translation	accuracy	Translation candidates are mined from these effective pages, whose accuracy curves are depicted in.
semantic prediction	accuracy	Experiments demonstrate the method with semantic prediction distinctly improves the accuracy, about 36.8%.
PP attachment	accuracy	We therefore integrate a data-driven classifier for the special task of PP attachment into an existing rulebased parser and measure the effect that the additional information has on the overall accuracy.
Translation disambiguation	accuracy	Translation disambiguation accuracy using frame semantics is 75%, compared to 15% by using dictionary glossing only.
GRE	Brevity	Much of the GRE literature has focused on developing efficient content determination strategies that output the best available description according to some interpretation of the Gricean maxims (, especially Brevity.
parsing	O	As discussed above, in this way we achieve an improvement of the parsing time for SCFGs, obtaining an upper bound of O(|G| N k+4 ) by using a parsing strategy that maintains continuous Figure 1: Two permutation trees.
MT	FE	 Table 5: MT output analysis of the 'EU', 'WNG' and 'EU+WNG' systems. FE, FW and FEW refer to the GTM (e = 1)
MT	FEW	 Table 5: MT output analysis of the 'EU', 'WNG' and 'EU+WNG' systems. FE, FW and FEW refer to the GTM (e = 1)
parsing	accuracy	The results also confirm that classifier-based deterministic parsing can achieve parsing accuracy very close to the best results reported for more complex parsing models.
parsing	accuracy	shows the parsing accuracy for the combination of three languages (Swedish, English and Chinese), two learning methods (MBL and SVM) and five feature models (Φ 1 -Φ 5 ), with algorithm parameters optimized as described in section 4.3.
SCF acquisition	recall	For the convenience of comparison between performances of different SCF acquisition methods, we define absolute and relative recall in this paper.
SCF acquisition	precision	As for English, reported that semantically motivated SCF acquisition achieved a precision of 87.1%, an absolute recall of 71.2% and a relative recall of 85.27%, thus making the acquired lexicon much more accurate and useful.
SCF acquisition	recall	As for English, reported that semantically motivated SCF acquisition achieved a precision of 87.1%, an absolute recall of 71.2% and a relative recall of 85.27%, thus making the acquired lexicon much more accurate and useful.
SCF acquisition	recall	As for English, reported that semantically motivated SCF acquisition achieved a precision of 87.1%, an absolute recall of 71.2% and a relative recall of 85.27%, thus making the acquired lexicon much more accurate and useful.
identification	accuracy	The identification accuracy was found to be as high as approximately 36%.
NE	error rates	NE errors adversely affect subsequent stages, and error rates are often compounded by later stages.
Statistical machine translation (MT)	accuracy	Statistical machine translation (MT) models are employed to take into account the source text for increasing the accuracy of automatic speech recognition (ASR) models.
parsing written language	accuracy	Although Bikel's parser achieves a higher accuracy for parsing written language, it achieves a higher accuracy when extracting subcategorization cues from spoken language.
SCF extraction	accuracy	3. Apply our SCF extraction system () to spoken and written language separately and compare the accuracy achieved for the acquired SCFs from spoken and written language.
Question answering	POURPRE	 Table 2: Question answering performance at different answer length cutoffs, as measured by POURPRE.
MT evaluation	BLEU	In the 2003 Johns Hopkins Workshop on Speech and Language Engineering, experiments on MT evaluation showed that BLEU and NIST do not correlate well with human judgments at the sentence level, even when they correlate well overlarge test sets.
MT output	BLEU	also pointed out that due to the limited references for every MT output, using the overlapping ratio of n-grams longer than 2 did not improve sentence level evaluation performance of BLEU.
MT	Pearson's correlation coefficient	Our experiments are carried out as follows: automatic metrics are used to evaluate the MT outputs based on the four sets of references, and the Pearson's correlation coefficient of the automatic scores and the human scores is computed to see how well they agree.
segmentation	accuracy	The results demonstrate that segmentation accuracy is really important in obtaining an accurate annotation of the dialogue, and consequently in obtaining quality strategy models.
segmentation	accuracy	The segmentation accuracy in must be compared with the turn accuracy in shows, the accuracy of the labelling decreased dramatically.
Dihana	WIP	 Table 6: Dihana results for the unsegmented  model (WIP=50, only two-level labelling for user  turns)
review identification	accuracy	We first demonstrate that review identification can be performed with high accuracy using only unigrams as features.
semantic parsing	Fmeasure score	Applications of SSVM ensemble in the semantic parsing problem show that the proposed SSVM ensemble is better than the SSVM in term of the Fmeasure score and accuracy measurements.
semantic parsing	accuracy	Applications of SSVM ensemble in the semantic parsing problem show that the proposed SSVM ensemble is better than the SSVM in term of the Fmeasure score and accuracy measurements.
letter matching	precision	The proposed method combined with a letter matching algorithm achieved 78% precision and 85% recall on an evaluation corpus with 4,212 acronym-definition pairs.
letter matching	recall	The proposed method combined with a letter matching algorithm achieved 78% precision and 85% recall on an evaluation corpus with 4,212 acronym-definition pairs.
information extraction	URES	In order to minimize the huge manual effort involved with building information extraction systems, we have designed and developed URES (Unsupervised Relation Extraction System) which learns a set of patterns to extract relations from the web in a totally unsupervised way.
parsing	precision	Perhaps more interestingly, the parsing model is significantly different from the generative models used by other well-known accurate parsers, allowing fora simple combination that produces precision and recall of 90.9% and 90.7%, respectively .
parsing	recall	Perhaps more interestingly, the parsing model is significantly different from the generative models used by other well-known accurate parsers, allowing fora simple combination that produces precision and recall of 90.9% and 90.7%, respectively .
generative parsing	accuracy	Furthermore, a simple combination of the shift-reduce parsing model with an existing generative parsing model produces results with accuracy that surpasses any that of any single (nonreranked) parser tested on the WSJ Penn Treebank, and comes close to the best results obtained with discriminative reranking).
parsing	accuracy	For example, parsing accuracy could be increased if parsers were tested on different text types or genres, ascertain constructions may occur only in certain types of texts.
SMT	BLEU score	In most SMT systems the use of a 4-gram back-off language model usually achieves improvements in the BLEU score in comparison to the 3-gram LM used during decoding.
machine translation	BLEU	In machine translation, annealed minimum risk training achieves significant improvements in BLEU over standard minimum error training.
standardization	PAROLE	The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EA-GLES 1 , PAROLE/SIMPLE (), ISLE/MILE (  and LIRICS 2 . These continuous efforts has been crystallized as activities in ISO-TC37/SC4 which aims to make an international standard for language resources.
standardization	LIRICS	The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EA-GLES 1 , PAROLE/SIMPLE (), ISLE/MILE (  and LIRICS 2 . These continuous efforts has been crystallized as activities in ISO-TC37/SC4 which aims to make an international standard for language resources.
WSM	accuracy	To evaluate the STW performance of our WSM, we define the STW accuracy, identified character ratio (ICR) and STW improvement, by the following equations: STW accuracy = # of correct characters / # of total characters.
WSM	identified character ratio (ICR)	To evaluate the STW performance of our WSM, we define the STW accuracy, identified character ratio (ICR) and STW improvement, by the following equations: STW accuracy = # of correct characters / # of total characters.
WSM	STW improvement	To evaluate the STW performance of our WSM, we define the STW accuracy, identified character ratio (ICR) and STW improvement, by the following equations: STW accuracy = # of correct characters / # of total characters.
word alignment	relative error rate reduction	And the interpolated model further improves the word alignment results, achieving a relative error rate reduction of 21.30% as compared with results produced by the original model.
SLU	accuracy	The resulting Conditional Random Fields (CRFs)) yielded a 21% relative improvement in SLU accuracy.
SLU	accuracy	Coverage for both preamble words and slot boundaries help improve the SLU accuracy.
semantic interpretation	accuracy	We aim to show that a fairly useful baseline level of semantic interpretation accuracy can already be achieved, even with relatively little lexical and ontological knowledge.
WSD	accuracy	In Section 3, we present our experimental results and show that although rich linguistic features and features derived from a Chinese Semantic Role Labeling improve the WSD accuracy, the improvement is not uniform across all verbs.
Word Sense Disambiguation (WSD)	precision	Word Sense Disambiguation (WSD) is generally taken as an intermediate task like part-of-speech (POS) tagging in natural language processing, but it has not so far achieved the sufficient precision for application as POS tagging (for the history of WSD, cf.).
WSD	precision	2 Some previous work on WSD using semantic similarity utilized the semantic network of nouns in WordNet to disambiguate term senses to improve the precision of SMART information retrieval at the stage of indexing, in which he assigned two different weights for both directions of edges in the network to compute the similarity of two nodes.
estimation	Expectation Maximization (EM)	We describe the application of the hook trick to estimation with Expectation Maximization (EM).
EM  training	AER	We chose the number of iterations for EM  training as the turning point of AER on the development data set.
Bilingual alignment	LITG	 Table 1: Bilingual alignment and English dependency results on Chinese-English corpus (≤ 25 words on  both sides). LITG stands for the cross-language Lexicalized ITG. BLITG is the within-English Bilexical  ITG. ITG-lh is ITG with left-head assumption on English. ITG-rh is with right-head assumption.
Bilingual alignment	BLITG	 Table 1: Bilingual alignment and English dependency results on Chinese-English corpus (≤ 25 words on  both sides). LITG stands for the cross-language Lexicalized ITG. BLITG is the within-English Bilexical  ITG. ITG-lh is ITG with left-head assumption on English. ITG-rh is with right-head assumption.
IOB tagging	in-vocabulary rate (R-iv)	There exists a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary rate (R-iv) in return fora higher out-of-vocabulary (OOV) rate (R-oov).
OOV recognition	IV rate	While OOV recognition is very important in word segmentation, a higher IV rate is also desired.
word segmentation	IV rate	While OOV recognition is very important in word segmentation, a higher IV rate is also desired.
word alignment	accuracy	Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.
word alignment	accuracy	We evaluate BiTAM models on the word alignment accuracy and the translation quality.
word alignment	accuracy	For word alignment accuracy, F-measure is reported, i.e., the harmonic mean of precision and recall against a gold-standard reference set; for translation quality,) and its variation of NIST scores are reported.
word alignment	F-measure	For word alignment accuracy, F-measure is reported, i.e., the harmonic mean of precision and recall against a gold-standard reference set; for translation quality,) and its variation of NIST scores are reported.
word alignment	precision	For word alignment accuracy, F-measure is reported, i.e., the harmonic mean of precision and recall against a gold-standard reference set; for translation quality,) and its variation of NIST scores are reported.
word alignment	recall	For word alignment accuracy, F-measure is reported, i.e., the harmonic mean of precision and recall against a gold-standard reference set; for translation quality,) and its variation of NIST scores are reported.
break prediction	adjusted score	To emphasize the effectiveness of break prediction, we define the adjusted score, S a , as follows.
parsing	accuracy	Our integrated model shows that percolating morphological ambiguity to the lowest level of non-terminals in the syntactic parse tree improves parsing accuracy.
parsing written language	accuracy	Although Bikel's parser achieves a higher accuracy for parsing written language, it achieves a higher accuracy when extracting subcategorization cues from spoken language.
MT	BLEU	Integrating this word order model in a baseline MT system results in a 2.4 points improvement in BLEU for English to Japanese translation.
WSD	accuracy	They found that training a WSD system on one part (BC or WSJ) of the DSO corpus, and applying it to the other, can result in an accuracy drop of more than 10%, highlighting the need to perform domain adaptation of WSD systems to new domains.
Speech recognition	out-of-vocabulary (OOV) ratio	Speech recognition in many morphologically rich languages suffers from a very high out-of-vocabulary (OOV) ratio.
feature selection	accuracy	The feature selection algorithm performs a greedy forward stepwise feature selection on the feature templates so as to maximize development set accuracy.
translation	mBLEU score	For translation of ASR 1-best, we see a systematic degradation of about 3% in mBLEU score compared to translating the transcription.
SRL	accuracy	We explore the effect of different auxiliary problems, and show that learning predictive structures with ASO results in significantly improved SRL accuracy.
parsing	accuracy	In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy.
POS tagging	accuracy	 Table 1: Results of part unsupervised POS tagging  on the different models, using a greedy accuracy  measure.
MT evaluation	BLEU-4	Currently, the most widely used automatic MT evaluation metric is the NIST BLEU-4 ( . By default, METEOR script counts the words that match exactly, and words that match after a simple Porter stemmer.
MT evaluation	METEOR	Currently, the most widely used automatic MT evaluation metric is the NIST BLEU-4 ( . By default, METEOR script counts the words that match exactly, and words that match after a simple Porter stemmer.
MST algorithm	accuracy	Although recent work suggests that the edge-factored constraints of the MST algorithm significantly inhibit parsing accuracy , we show that generating the 50-best parses according to an edge-factored model has an oracle performance well above the 1-best performance of the best dependency parsers.
parsing	accuracy	Although recent work suggests that the edge-factored constraints of the MST algorithm significantly inhibit parsing accuracy , we show that generating the 50-best parses according to an edge-factored model has an oracle performance well above the 1-best performance of the best dependency parsers.
translation	accuracy	The Bleu score was used to measure translation accuracy, calculated by the NIST script with its default settings.
classification	accuracy	The Adistance can be measured from unlabeled data, and it was designed to take into account only divergences which affect classification accuracy.
segmentation	accuracy	For both of these metrics, lower scores indicate better segmentation accuracy.
automatic speech recognition (ASR))	Latent Dirichlet Allocation (LDA)	This approach has been successfully applied in automatic speech recognition (ASR)) using the Latent Dirichlet Allocation (LDA) ().
LM adaptation	BLEU	In Section 3, we present the effect of LM adaptation on word perplexity, followed by SMT experiments reported in BLEU on both speech and text input in Section 3.3.
SMT	BLEU	In Section 3, we present the effect of LM adaptation on word perplexity, followed by SMT experiments reported in BLEU on both speech and text input in Section 3.3.
coreference resolution	Accuracy	The manually selected patterns, nevertheless, are not necessarily the most effective ones for coreference resolution from the following two concerns: • Accuracy.
IE	F 1 -measure	We define the measures of precision, recall and F 1 as follows: Since the performance in IE is measured in F 1 -measure, an appropriate threshold to be used for the most prominent candidate templates is: The value Score T O is used as a training model.
IE	precision	The usual measures of performance of regular IE systems are precision, recall, and their combinations -the breakeven point and F-measure.
IE	recall	The usual measures of performance of regular IE systems are precision, recall, and their combinations -the breakeven point and F-measure.
IE	breakeven point	The usual measures of performance of regular IE systems are precision, recall, and their combinations -the breakeven point and F-measure.
IE	F-measure	The usual measures of performance of regular IE systems are precision, recall, and their combinations -the breakeven point and F-measure.
WE	recall	Consequently, for evaluating the performance of WE systems, the recall is substituted by the number of extracted instances.
NER validators	recall	First, all of the NER validators improve over the baseline SRES, sometimes as much as doubling the recall at the same level of precision.
NER validators	precision	First, all of the NER validators improve over the baseline SRES, sometimes as much as doubling the recall at the same level of precision.
NER	F 1 measure	A classical NER system is set to maximize the F 1 measure of all mentions of all entities in the corpus.
Parsing	precision	Parsing performance is measured by f-score, f = 2×P ×R P +R , where P, R are labeled precision and recall.
Parsing	recall	Parsing performance is measured by f-score, f = 2×P ×R P +R , where P, R are labeled precision and recall.
parsing	accuracy	By constructing a more accurate but still tractable approximation , we significantly improve parsing accuracy, suggesting that ISBNs provide a good idealization for parsing.
parsing	error reduction	This gener-ative model of parsing achieves state-of-the-art results on WSJ text and 8% error reduction over the baseline neural network parser.
parsing	accuracy	Increasing the beam size beyond this value did not significantly effect parsing accuracy.
MF	error reduction	The MF model improves over the baseline NN approximation, with an error reduction in F-measure exceeding 8%.
MF	F-measure	The MF model improves over the baseline NN approximation, with an error reduction in F-measure exceeding 8%.
transliteration evaluation	character accuracy (CA)	In general, there are two commonly used metrics for transliteration evaluation: word accuracy (WA) and character accuracy (CA).
type checking	precision	The results of our type checking experiments are shown insured in missing area under the precision/recall curve) by 46%.
Type Checking	precision	 Table 1: Type Checking Performance. Listed is area  under the precision/recall curve. HMM-T outper- forms N-GRAMS for all relations, and reduces the  error in terms of missing area under the curve by  46% on average.
Type Checking	recall	 Table 1: Type Checking Performance. Listed is area  under the precision/recall curve. HMM-T outper- forms N-GRAMS for all relations, and reduces the  error in terms of missing area under the curve by  46% on average.
NP chunking	accuracy	 Table 1: NP chunking accuracy on test data us- ing different training methods. The effects of dis- criminative training (CRF) and extended feature sets  (lower section) are more than additive.
NP chunking	accuracy	 Table 2: NP chunking accuracy on test data using  different base models for the M-estimator. The "se- lection" column shows which accuracy measure was  optimized when selecting the hyperparameter c.
NP chunking	accuracy	 Table 2: NP chunking accuracy on test data using  different base models for the M-estimator. The "se- lection" column shows which accuracy measure was  optimized when selecting the hyperparameter c.
answer ranking	accuracy	Performance of the answer ranking framework was measured by average answer accuracy: the number of correct top answers divided by the number of questions whereat least one correct answer exists in the candidate list provided by an extractor.
ALL	precision	 Table 3. Precision, recall and F- measure are provided for ALL and for NP and VP  antecedents individually. The parameter tipster  is not available for the baseline system. The best  baseline performance is precision 4.88, recall 20.06  and F-measure 7.85 in the setting with it-filter  on. As expected, this filter yields an increase in pre- cision and a decrease in recall. The negative effect  is outweighed by the positive effect, leading to a  small but insignificant
ALL	recall	 Table 3. Precision, recall and F- measure are provided for ALL and for NP and VP  antecedents individually. The parameter tipster  is not available for the baseline system. The best  baseline performance is precision 4.88, recall 20.06  and F-measure 7.85 in the setting with it-filter  on. As expected, this filter yields an increase in pre- cision and a decrease in recall. The negative effect  is outweighed by the positive effect, leading to a  small but insignificant
ALL	F-measure	 Table 3. Precision, recall and F- measure are provided for ALL and for NP and VP  antecedents individually. The parameter tipster  is not available for the baseline system. The best  baseline performance is precision 4.88, recall 20.06  and F-measure 7.85 in the setting with it-filter  on. As expected, this filter yields an increase in pre- cision and a decrease in recall. The negative effect  is outweighed by the positive effect, leading to a  small but insignificant
ALL	recall	 Table 3. Precision, recall and F- measure are provided for ALL and for NP and VP  antecedents individually. The parameter tipster  is not available for the baseline system. The best  baseline performance is precision 4.88, recall 20.06  and F-measure 7.85 in the setting with it-filter  on. As expected, this filter yields an increase in pre- cision and a decrease in recall. The negative effect  is outweighed by the positive effect, leading to a  small but insignificant
ME estimation	BLasso	We also investigate ME estimation with L 1 regularization using a novel optimization algorithm, and BLasso, which is aversion of Boosting with Lasso (L 1) regularization.
translation	BLEU score	The translation quality was evaluated using a well-established automatic measure: BLEU score ().
transliteration extraction	precision	This is because for many of the tasks in which transliteration extraction would be useful (such as building a lexicon), precision is deemed more important.
cross-language retrieval	precision	Our results indicate that, far from adding more noise, more linguistic parallelism is better when it comes to cross-language retrieval precision, in addition to the self-evident benefit that CLIR can be performed on more languages.
SE classification	accuracy	 Table 3: SE classification results with sequencing  on Brown test set. Bold cell indicates accuracy at- tained by model parameters that performed best on  development data.
detecting comprehensive sets of SCFs	accuracy	Subsequent research has yielded systems for English) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. ().
morphological segmentations	accuracy	For the latter task, morphological segmentations from the unsupervised systems presented here have been shown to improve accuracy ().
Type checking	precision	Type checking also significantly improves precision and recall.
Type checking	recall	Type checking also significantly improves precision and recall.
Pseudo-projective parsing	AS U )	 Table 5: Pseudo-projective parsing results (AS U ) for  Alpino with MSTParser.
IR	irrele- vant)	 Table 3: IR is sufficient for  answering 14.4% of the IM2 queries, and 20% of  the MS-set queries. In 50% and 25.7% of the cases,  respectively, it simply cannot be applied (irrele- vant). Finally, IR alone is not enough in 35.6% of  the queries from the IM2-set, and in 54.3% of the  MS-set; it has to be complemented with other  techniques.
speech recognition	accuracy	The survey results were analyzed in terms of speech recognition accuracy and users' blog making experience to improve the system.
Motivation	Oscar	In the Motivation section, we introduce Oscar as a system for chemical NER and recognition of ontology terms.
correcting word spacing	spelling error	As in the case of general literary sentences, correcting word spacing error and spelling error is the very essential problem with colloquial style sentences.
classification	accuracy	Both approaches are designed to leverage context for the purpose of increasing classification accuracy on a classification task where the codes refer to the role a span of text plays in context.
parsing	accuracy	Experimental results indicate that minimal or low lexicalization is sufficient for parsing accuracy.
Tagging	accuracy	 Table 4: Tagging accuracy for Hungarian of HunPos  with and without morphological lexicon and with  first and second order emission/lexicon probabili- ties.
subjectivity detection	accuracy	It is worth noting that we observed the same relation between subjectivity detection and polarity classification accuracy as described by and.
sentiment detection of opinionated texts	Accuracy	The accuracy of the sentiment detection of opinionated texts (excluding erroneously detected unopinionated texts) in Experiment 2 has increased by 13% for positive reviews and by 6% for negative reviews (see).: Accuracy of sentiment polarity detection of opinionated texts (in percent).
single-relation extraction	recall	We find that while single-relation extraction, as embodied by R1-CRF, achieves comparatively higher levels of recall, it takes hundreds, and sometimes thousands, of labeled examples per relation, for R1-CRF to approach the precision obtained by O-CRF, which is self-trained without any relation-specific input.
single-relation extraction	precision	We find that while single-relation extraction, as embodied by R1-CRF, achieves comparatively higher levels of recall, it takes hundreds, and sometimes thousands, of labeled examples per relation, for R1-CRF to approach the precision obtained by O-CRF, which is self-trained without any relation-specific input.
PPI identification	accuracy	We run a PPI system with several combinations of parser and parse representation , and examine their impact on PPI identification accuracy.
parser comparison	accuracy	The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy.
parser comparison	recall	The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy.
parser evaluation	accuracy	Our approach to parser evaluation is to measure accuracy improvement in the task of identifying protein-protein interaction (PPI) information in biomedical papers, by incorporating the output of different parsers as statistical features in a machine learning classifier (.
parser evaluation	accuracy	In our approach to parser evaluation, we measure the accuracy of a PPI extraction system, in which This study demonstrates that IL-8 recognizes and activates CXCR1, CXCR2, and the Duffy antigen by distinct mechanisms.
SVMs	precision	A threshold for SVMs is moved to adjust the balance of precision and recall, and the maximum f-scores are reported for each setting.
SVMs	recall	A threshold for SVMs is moved to adjust the balance of precision and recall, and the maximum f-scores are reported for each setting.
machine translation (MT) evaluation	precision	We propose an automatic machine translation (MT) evaluation metric that calculates a similarity score (based on precision and recall) of a pair of sentences.
machine translation (MT) evaluation	recall	We propose an automatic machine translation (MT) evaluation metric that calculates a similarity score (based on precision and recall) of a pair of sentences.
MT evaluation	BLEU	Among all the automatic MT evaluation metrics, BLEU) is the most widely used.
MT research	BLEU	Although BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores.
MT submissions	Adequacy	For human evaluation of the MT submissions, four different criteria were used in the workshop: Adequacy (how much of the original meaning is expressed in a system translation), Fluency (the translation's fluency), Rank (different translations of a single source sentence are compared and ranked from best to worst), and Constituent (some constituents from the parse tree of the source sentence are translated, and human judges have to rank these translations).
MT submissions	Fluency	For human evaluation of the MT submissions, four different criteria were used in the workshop: Adequacy (how much of the original meaning is expressed in a system translation), Fluency (the translation's fluency), Rank (different translations of a single source sentence are compared and ranked from best to worst), and Constituent (some constituents from the parse tree of the source sentence are translated, and human judges have to rank these translations).
MT submissions	Rank	For human evaluation of the MT submissions, four different criteria were used in the workshop: Adequacy (how much of the original meaning is expressed in a system translation), Fluency (the translation's fluency), Rank (different translations of a single source sentence are compared and ranked from best to worst), and Constituent (some constituents from the parse tree of the source sentence are translated, and human judges have to rank these translations).
MT submissions	Constituent	For human evaluation of the MT submissions, four different criteria were used in the workshop: Adequacy (how much of the original meaning is expressed in a system translation), Fluency (the translation's fluency), Rank (different translations of a single source sentence are compared and ranked from best to worst), and Constituent (some constituents from the parse tree of the source sentence are translated, and human judges have to rank these translations).
extracting phrase translation	precision	In this work, the problem of extracting phrase translation is formulated as an information retrieval process implemented with a log-linear model aiming fora balanced precision and recall.
extracting phrase translation	recall	In this work, the problem of extracting phrase translation is formulated as an information retrieval process implemented with a log-linear model aiming fora balanced precision and recall.
phrase translation	accuracy	We would like to improve phrase translation accuracy and at the same time extract as many as possible valid phrase pairs that are missed due to incorrect word alignments.
Translation	BLEU	 Table 4: Translation Results (BLEU) of discriminative  phrase training approach using different features
Translation	BLEU	 Table 5: Translation Results (BLEU) of Different Phrase  Pair Combination
SMT	recall	Recall over SMT output We can see that the Bi-ME model can achieve better results than the Mo-ME model in both recall and precision metrics although only a small sized bilingual corpus is used for Bi-ME model training.
SMT	precision	Recall over SMT output We can see that the Bi-ME model can achieve better results than the Mo-ME model in both recall and precision metrics although only a small sized bilingual corpus is used for Bi-ME model training.
text transformation	Precision	 Table 1: Experimental evaluation of different text transformation techniques with different amounts of user-specific  data. Precision, recall, deletion, insertion and error rate values are given in percent and represent the average of 51  users, where the results for each user are the ratios of sums over 30 reports.
text transformation	recall	 Table 1: Experimental evaluation of different text transformation techniques with different amounts of user-specific  data. Precision, recall, deletion, insertion and error rate values are given in percent and represent the average of 51  users, where the results for each user are the ratios of sums over 30 reports.
stemming	recall	Another drawback of stemming is that it usually enhances recall, but may hurt precision (.
stemming	precision	Another drawback of stemming is that it usually enhances recall, but may hurt precision (.
NR classification	f-score	Our NR classification evaluation strictly follows the ACL SemEval-07 Task 4 datasets and protocol, obtaining an f-score of 70.6, as opposed to 64.8 of the best previous work that did not use the manually provided WordNet sense disambiguation tags.
Parsing	recall	 Table 2: Parsing results with gold-standard senses (  *  in- dicates that the recall or precision is significantly better  than baseline; the best performing method in each col- umn is shown in bold)
Parsing	precision	 Table 2: Parsing results with gold-standard senses (  *  in- dicates that the recall or precision is significantly better  than baseline; the best performing method in each col- umn is shown in bold)
PP attachment	recall	 Table 3: PP attachment results with gold-standard senses  (  *  indicates that the recall or precision is significantly bet- ter than baseline; the best performing method in each col- umn is shown in bold)
PP attachment	precision	 Table 3: PP attachment results with gold-standard senses  (  *  indicates that the recall or precision is significantly bet- ter than baseline; the best performing method in each col- umn is shown in bold)
Parsing	recall	 Table 4: Parsing results with 1ST (  *  indicates that the  recall or precision is significantly better than baseline; the  best performing method in each column is shown in bold)
Parsing	precision	 Table 4: Parsing results with 1ST (  *  indicates that the  recall or precision is significantly better than baseline; the  best performing method in each column is shown in bold)
PP attachment	1ST	 Table 5: PP attachment results with 1ST (  *  indicates that  the recall or precision is significantly better than baseline;  the best performing method in each column is shown in  bold)
PP attachment	recall	 Table 5: PP attachment results with 1ST (  *  indicates that  the recall or precision is significantly better than baseline;  the best performing method in each column is shown in  bold)
PP attachment	precision	 Table 5: PP attachment results with 1ST (  *  indicates that  the recall or precision is significantly better than baseline;  the best performing method in each column is shown in  bold)
Parsing	ASR	 Table 6: Parsing results with ASR (  *  indicates that the  recall or precision is significantly better than baseline; the  best performing method in each column is shown in bold)
Parsing	recall	 Table 6: Parsing results with ASR (  *  indicates that the  recall or precision is significantly better than baseline; the  best performing method in each column is shown in bold)
Parsing	precision	 Table 6: Parsing results with ASR (  *  indicates that the  recall or precision is significantly better than baseline; the  best performing method in each column is shown in bold)
PP attachment	ASR	 Table 7: PP attachment results with ASR (  *  indicates that  the recall or precision is significantly better than baseline;  the best performance in each column is shown in bold)
PP attachment	recall	 Table 7: PP attachment results with ASR (  *  indicates that  the recall or precision is significantly better than baseline;  the best performance in each column is shown in bold)
PP attachment	precision	 Table 7: PP attachment results with ASR (  *  indicates that  the recall or precision is significantly better than baseline;  the best performance in each column is shown in bold)
ASR	duration	Many factors are thought to increase the chances of misrecognizing a word in ASR, including low frequency, nearby disfluencies, short duration, and being at the start of a turn.
recognition of spontaneous monologues and dialogues	error rates	Previous work on recognition of spontaneous monologues and dialogues has shown that infrequent words are more likely to be misrecognized) and that fast speech increases error rates). and also found higher error rates in very slow speech.
recognition of spontaneous monologues and dialogues	error	Previous work on recognition of spontaneous monologues and dialogues has shown that infrequent words are more likely to be misrecognized) and that fast speech increases error rates). and also found higher error rates in very slow speech.
MT	BLEU	First, although names are important to human readers, automatic MT scoring metrics (such as BLEU) do not encourage researchers to improve name translation in the context of MT.
SMT	accuracy	񮽙 We evaluate both the base SMT system and the augmented system in terms of entity translation accuracy and BLEU (Sections 2 and 6).
SMT	BLEU	񮽙 We evaluate both the base SMT system and the augmented system in terms of entity translation accuracy and BLEU (Sections 2 and 6).
MT	BLEU	General MT metrics such as BLEU, TER, METEOR are not suitable for evaluating named entity translation and transliteration, because they are not focused on named entities (NEs).
MT	TER	General MT metrics such as BLEU, TER, METEOR are not suitable for evaluating named entity translation and transliteration, because they are not focused on named entities (NEs).
MT	METEOR	General MT metrics such as BLEU, TER, METEOR are not suitable for evaluating named entity translation and transliteration, because they are not focused on named entities (NEs).
Name translation	accuracy	 Table 2: Name translation accuracy with respect to BBN  and re-annotated Gold Standard on 1730 named entities  in 637 sentences.
Name translation	accuracy	 Table 3: Name translation accuracy in end-to-end statistical machine translation (SMT) system for different named  entity (NE) types: Person (PER), Geopolitical Entity, which includes countries, provinces and towns (GPE), Organi- zation (ORG), Facility (FAC), Nominal Person, e.g. Swede (PER.Nom), other location (LOC).
NER	accuracy	Moreover, we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases.
SMT	BLEU	We carryout experiments on a state-of-the-art SMT system, i.e.,, and show that the abbreviation translations consistently improve the translation performance (in terms of BLEU ()) on various NIST MT test sets.
MT	BLEU	 Table 7: MT Performance measured by BLEU Score
SMT	BLEU	We applied our inflection generation models in translating English into two morphologically complex languages, Russian and Arabic, and show that our model improves the quality of SMT over both phrasal and syntax-based SMT systems according to BLEU and human judgements .
SMT	BLEU	We applied our inflection generation models in translating English into two morphologically complex languages, Russian and Arabic, and show that our model improves the quality of SMT over both phrasal and syntax-based SMT systems according to BLEU and human judgements .
dependency parsing	accuracy	Finally, we apply this algorithm to dependency parsing and show improved dependency parsing accuracy for both Chinese and English.
MT	accuracy	In some language pairs, i.e. Chinese-to-English translation, state-ofthe-art hierarchical systems show significant advantage over phrasal systems in MT accuracy.
Document recovery from BOW	accuracy	3. Document recovery from BOW: With the bigram LMs, we show improved accuracy in recovering ordered documents from BOWs.
Syntactic chunking	F β=1 score	 Table 8: Syntactic chunking results of the previous top  systems for CoNLL'00 shared task data (F β=1 score)
answer ranking	accuracy	These results provide robust evidence that: (a) we can use publicly available online QA collections to investigate features for answer ranking without the need for costly human evaluation, (b) we can exploit large and noisy online QA collections to improve the accuracy of answer ranking systems and (c) readily available and scalable NLP technology can be used: Summary of the model selection process.
morphological disambiguation	error reduction	For full morphological disambiguation, our method achieves an error reduction of 30% (57% to 70%).
summarization	ROUGE	Following the current practice in evaluating summarization, particularly DUC 3 , we use the ROUGE evaluation package ().
Logistic regression classification	accu- racy	 Table 6: Logistic regression classification results (accu- racy, precision, recall and f-measure) for balanced data of  100-word summaries from DUC'02 through DUC'04.
Logistic regression classification	precision	 Table 6: Logistic regression classification results (accu- racy, precision, recall and f-measure) for balanced data of  100-word summaries from DUC'02 through DUC'04.
Logistic regression classification	recall	 Table 6: Logistic regression classification results (accu- racy, precision, recall and f-measure) for balanced data of  100-word summaries from DUC'02 through DUC'04.
Logistic regression classification	f-measure	 Table 6: Logistic regression classification results (accu- racy, precision, recall and f-measure) for balanced data of  100-word summaries from DUC'02 through DUC'04.
coreference resolution	accuracy	To accommodate for coreference resolution, we loosened the restrictions to allow rules that have above 50% accuracy and contain up to ten predicates.
segmentation	accuracy	The joint model gives an error reduction in segmentation accuracy of 14.6% and an error reduction in tagging accuracy of 12.2%, compared to the traditional pipeline approach.
segmentation	accuracy	In experiments with the Chinese Treebank data, the joint model gave an error reduction of 14.6% in segmentation accuracy and 12.2% in the overall segmentation and tagging accuracy, compared to the traditional pipeline approach.
tagging	accuracy	The overall tagging accuracy also increased slightly, consistent with observations from the pure POS tagger.
segmentation	accuracy	Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging ().
POS tagging	accuracy	Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging ().
segmentation	accuracy	We find that Joint S&T can also improve the segmentation accuracy.
segmentation	error	Experimental results show that, it achieves obvious improvement over the perceptron-only model, about from 0.973 to 0.978 on segmentation, and from 0.925 to 0.934 on Joint S&T, with error reductions of 18.5% and 12% respectively.
parsing	accuracy	In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model.
parsing	accuracy	In this paper, we consider a simple way of integrating graph-based and transition-based models in order to exploit their complementary strengths and thereby improve parsing accuracy beyond what is possible by either model in isolation.
word alignment	accuracy	Despite much recent work on word alignment methods, alignment accuracy increases often produce little or no improvements in machine translation quality.
MT	BLEU score	We propose and extensively evaluate a simple method for using alignment models to produce alignments better-suited for phrase-based MT systems, and show significant gains (as measured by BLEU score) in end-to-end translation systems for six languages pairs used in recent MT competitions.
MT evaluation	Len	 Table 1: Statistics of the corpora used in MT evaluation.  The training size is measured in thousands of sentences  and Len refers to average (English) sentence length. Test  is the number of sentences in the test set. Rare and Unk  are the percentage of tokens in the test set that are rare  and unknown in the training data, for each language.
machine translation	precision	While this is often beneficial for machine translation systems, it is not very suitable for creating bilingual dictionaries, where precision is of paramount importance.
MT06	OOV items	Even if the results are not statistically significant for MT06, there is a high decrease in OOV items when using word-lattices.
reck- less bootstrapping	accuracy	 Table 1 shows the results for 4 iterations of reck- less bootstrapping for four semantic categories: U.S.  states, countries, singers, and fish. The first two  categories are relatively small, closed sets (our gold  standard contains 50 U.S. states and 194 countries).  The singers and fish categories are much larger, open  sets (see Section 4 for details).  Table 1 reveals that the doubly-anchored pattern  achieves high accuracy during the first iteration, but
summaries	ROUGE	We carried out automatic evaluation of our summaries using ROUGE) toolkit, which has been widely adopted by DUC for automatic summarization evaluation.
MT evaluation	BLEU	Much work has focused on developing better algorithms to tackle the optimization problem (e.g. MERT), since MT evaluation metrics such as BLEU and PER are riddled with local minima and are difficult to differentiate with respect to re-ranker parameters.
MT evaluation	PER	Much work has focused on developing better algorithms to tackle the optimization problem (e.g. MERT), since MT evaluation metrics such as BLEU and PER are riddled with local minima and are difficult to differentiate with respect to re-ranker parameters.
translation	Translation Error Rate (TER)	The procedure described by has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.
ASP classifier	accuracy	Also note that baseline ASP classifier is notable to achieve higher accuracy even for users with large amount of past history.
parsing	accuracy	The results in show that this system performs comparably to the state of the art in overall parsing accuracy and reasonably well in edit detection.) achieves a higher EDIT-F score, largely as a result of its explicit tracking of overlapping words
parsing	EDIT-F score	The results in show that this system performs comparably to the state of the art in overall parsing accuracy and reasonably well in edit detection.) achieves a higher EDIT-F score, largely as a result of its explicit tracking of overlapping words
edit detection.	EDIT-F score	The results in show that this system performs comparably to the state of the art in overall parsing accuracy and reasonably well in edit detection.) achieves a higher EDIT-F score, largely as a result of its explicit tracking of overlapping words
polarity classification tasks	accuracies	Tested on 2000 cases, the methods mirror human judgements closely in three-and two-way polarity classification tasks, and reach accuracies above 63% and 81%, respectively.
text classification	precision (p)	We adopt the standard evaluation criteria in text classification, namely precision (p), recall (r), f-1 measure (f) and accuracy (a)).
text classification	recall (r)	We adopt the standard evaluation criteria in text classification, namely precision (p), recall (r), f-1 measure (f) and accuracy (a)).
text classification	f-1 measure (f)	We adopt the standard evaluation criteria in text classification, namely precision (p), recall (r), f-1 measure (f) and accuracy (a)).
text classification	accuracy	We adopt the standard evaluation criteria in text classification, namely precision (p), recall (r), f-1 measure (f) and accuracy (a)).
Tuning	BLEU	Tuning is done using Och's algorithm to optimize weights for the distortion model, language model, phrase translation model and word penalty over the BLEU metric ().
phrase translation	BLEU	Tuning is done using Och's algorithm to optimize weights for the distortion model, language model, phrase translation model and word penalty over the BLEU metric ().
translation	BLEU-4	The translation quality is evaluated by BLEU-4 (case-sensitive).
phrase translation	BLEU score	The baseline uses full matching (α=1.0) for phrase translation and achieves a BLEU score of 24.44.
Phone recognition	accuracy	 Table 2: Phone recognition accuracy for different HMM  sizes (N ), and with different amounts of speech used to  learn the HMM labeler and the label-to-phone transducer.
MRS	accuracy	Lexical semantic roles enriching the MRS structures are inferred, which are useful to improve the accuracy of deep semantic parsing.
summaries	ROUGE	For human summaries, after removing disfluencies, the correlation between ROUGE and human evaluation improves on the whole, but degrades on information structure (IS) and information coverage (IC) categories.
phrase translation	BLEU	The quality of phrase translation is typically measured using n-gram precision based metrics such as BLEU () and NIST scores.
MT	F-measure	In our experiments with Chinese-to-English MT output, we first used a simple merge of the outputs from an ET (Entity Translation) system and an English NER system, getting an absolute gain of 7.15% in F-measure, from 73.53% to 80.68%.
MT	F-measure	From our experiments on NIST 05 Chineseto-English MT evaluation data, when we used the same English NER to tag the reference translation and the MT output, the F-measure was 81.38% for the reference but only 73.53% for the MT output.
word alignment	AER (alignment error rate)	However, using word alignment to map the source language information into the English text is problematic, for two reasons: First, the word alignment produced by machine translation is typically not very good, with a Chinese-English AER (alignment error rate) of about 40%.
MT evaluation metrics'	precision	The introduction of machine learning methods aimed at the improvement of MT evaluation metrics' precision is a recent trend.
Pronoun resolution	recall	Pronoun resolution using the extended feature set gives 73.4% recall, coming near specialized pronoun resolution systems such as.
role classification	error reduction rate	The experimental result of the role classification shows 19.16% and 7.42% improvements in error reduction rate and macro-averaged F1 score, respectively.
role classification	F1 score	The experimental result of the role classification shows 19.16% and 7.42% improvements in error reduction rate and macro-averaged F1 score, respectively.
role classification	error reduction rate	Using the proposed method, the experimental result of the role classification shows 19.16% and 7.42% improvements in error reduction rate and macro-averaged F1, respectively.
role classification	F1	Using the proposed method, the experimental result of the role classification shows 19.16% and 7.42% improvements in error reduction rate and macro-averaged F1, respectively.
SRL	F-score	Precision reflects the algorithm's applicability for creating training data to be used by supervised SRL models, while the standard SRL F-score measures the model's performance when used by itself.
semantic role prediction	MALT	 Table 2: Accuracy of semantic role prediction using  CCG, CFG, and MALT based features.
constituency parsing	error reduction	Evaluation on the Penn Chinese Treebank indicates that a converted dependency treebank helps constituency parsing and the use of unlabeled data by self-training further increases parsing f-score to 85.2%, resulting in 6% error reduction over the previous best result.
parsing	accuracy	The experimental results in  show a negative impact on the parsing accuracy from too long dependency relation.
prediction	accuracy	We show that the addition of a feature for the entire output sequence improves prediction accuracy.
Translation	BLEU	Translation quality was evaluated using both the BLEU score proposed by and also the modified BLEU (BLEU-Fix) score 3 used in the IWSLT 2008 evaluation campaign, where the brevity calculation is modified to use closest reference length instead of shortest reference length.
Translation	BLEU (BLEU-Fix) score 3	Translation quality was evaluated using both the BLEU score proposed by and also the modified BLEU (BLEU-Fix) score 3 used in the IWSLT 2008 evaluation campaign, where the brevity calculation is modified to use closest reference length instead of shortest reference length.
CRR translation	BLEU	 Table 6: CRR translation results (BLEU scores)  by using different RBMT systems
CRR translation	BLEU	 Table 7: CRR translation results by using multilin- gual corpus. "/" separates the BLEU and BLEU- fix scores.
CRR translation	BLEU- fix scores	 Table 7: CRR translation results by using multilin- gual corpus. "/" separates the BLEU and BLEU- fix scores.
MT	BLEU	Since human evaluation is costly and difficult to do reliably, a major focus of research has been on automatic measures of MT quality, pioneered by BLEU () and NIST).
MT	BLEU	However, studies such as Callison- have identified a number of problems with BLEU and related n-gram-based scores: (1) BLEUlike metrics are unreliable at the level of individual sentences due to data sparsity; (2) BLEU metrics can be "gamed" by permuting word order; (3) for some corpora and languages, the correlation to human ratings is very low even at the system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences.
Tie awareness	correlation	"Tie awareness" makes a considerable practical difference, improving correlation figures by 5-10 points.
parsing	accuracy	But allowing non-projective dependency trees also makes parsing empirically harder, because it requires that we model relations between nonadjacent structures over potentially unbounded distances, which often has a negative impact on parsing accuracy.
parsing	accuracy	But allowing non-projective dependency trees also makes parsing empirically harder, because it requires that we model relations between nonadjacent structures over potentially unbounded distances, which often has a negative impact on parsing accuracy.
CRF training	accuracy	the number of parsed sentences required for supervised CRF training (averaged over 5 random splits) to match the accuracy of GE training using the possible parents + sequence constraint set.
extracting biomedical semantic lexicons from raw text	BASILISK	In our experiments we consider the task of extracting biomedical semantic lexicons from raw text using BASILISK and WMEB.
information retrieval (IR)	Precision@10	A couple of metrics in the information retrieval (IR) community like Precision@10, MAP (mean average precision), and nDCG (normalized discounted cumulative gain) are available for evaluating a single ranked list of items per query.
information retrieval (IR)	MAP (mean average precision)	A couple of metrics in the information retrieval (IR) community like Precision@10, MAP (mean average precision), and nDCG (normalized discounted cumulative gain) are available for evaluating a single ranked list of items per query.
information retrieval (IR)	nDCG (normalized discounted cumulative gain)	A couple of metrics in the information retrieval (IR) community like Precision@10, MAP (mean average precision), and nDCG (normalized discounted cumulative gain) are available for evaluating a single ranked list of items per query.
NLPBA 2004 Named entity recognition task	accuracy	 Table 2: NLPBA 2004 Named entity recognition task. Training time and accuracy of the trained model  on the test data.
chunking	F1	We found that including such features does improve chunking F1 by approximately 2%, but it also significantly slows down CRF training.
tagging	accuracy	They show considerable improvements in tagging accuracy when using a coarser-grained version (with 17-tags) of the tag set from the Penn Treebank.
segmentation	error reductions	Our experiments show that adaptation from PD to CTB results in a significant improvement in segmentation and POS tagging, with error reductions of 30.2% and 14%, respectively.
POS tagging	error reductions	Our experiments show that adaptation from PD to CTB results in a significant improvement in segmentation and POS tagging, with error reductions of 30.2% and 14%, respectively.
parsing	accuracy	In addition, the improved accuracies from segmentation and tagging also lead to an improved parsing accuracy on CTB, reducing 38% of the error propagation from word segmentation to parsing.
word segmentation	F-measure	The performance measurement indicators for word segmentation and Joint S&T are balanced F-measure, F = 2P R/(P + R), a function of Precision P and Recall R. For word segmentation, P indicates the percentage of words in segmentation result that are segmented correctly, and R indicates the percentage of correctly segmented words in gold standard words.
word segmentation	2P R/(P + R)	The performance measurement indicators for word segmentation and Joint S&T are balanced F-measure, F = 2P R/(P + R), a function of Precision P and Recall R. For word segmentation, P indicates the percentage of words in segmentation result that are segmented correctly, and R indicates the percentage of correctly segmented words in gold standard words.
word segmentation	Precision P	The performance measurement indicators for word segmentation and Joint S&T are balanced F-measure, F = 2P R/(P + R), a function of Precision P and Recall R. For word segmentation, P indicates the percentage of words in segmentation result that are segmented correctly, and R indicates the percentage of correctly segmented words in gold standard words.
word segmentation	Recall R.	The performance measurement indicators for word segmentation and Joint S&T are balanced F-measure, F = 2P R/(P + R), a function of Precision P and Recall R. For word segmentation, P indicates the percentage of words in segmentation result that are segmented correctly, and R indicates the percentage of correctly segmented words in gold standard words.
prosodic event detection	F-measure	Most of the previous work for prosodic event detection reported their results using classification accuracy instead of F-measure.
ASR	accuracy	We also run a custom ASR system designed to produce transcripts at various degrees of accuracy in order to simulate the type of performance one might expect given languages with sparser training corpora.
semantic parsing	precision	We measured the performance of semantic parsing using precision (percentage of returned MRs that were correct), recall (percentage of test examples with correct MRs returned), and F-measure (harmonic mean of precision and recall).
semantic parsing	recall	We measured the performance of semantic parsing using precision (percentage of returned MRs that were correct), recall (percentage of test examples with correct MRs returned), and F-measure (harmonic mean of precision and recall).
semantic parsing	F-measure	We measured the performance of semantic parsing using precision (percentage of returned MRs that were correct), recall (percentage of test examples with correct MRs returned), and F-measure (harmonic mean of precision and recall).
semantic parsing	precision	We measured the performance of semantic parsing using precision (percentage of returned MRs that were correct), recall (percentage of test examples with correct MRs returned), and F-measure (harmonic mean of precision and recall).
semantic parsing	recall	We measured the performance of semantic parsing using precision (percentage of returned MRs that were correct), recall (percentage of test examples with correct MRs returned), and F-measure (harmonic mean of precision and recall).
FS	ratio	In this paper, we propose a theoretic framework of FS methods based on two basic measurements: frequency measurement and ratio measurement.
FS	χ -test (CHI)	Various FS methods, such as document frequency (DF), information gain (IG), mutual information (MI), χ -test (CHI), Bi-Normal Separation (BNS), and weighted log-likelihood ratio (WLLR), have been proposed for the tasks and make text classification more efficient and accurate.
FS	weighted log-likelihood ratio (WLLR)	Various FS methods, such as document frequency (DF), information gain (IG), mutual information (MI), χ -test (CHI), Bi-Normal Separation (BNS), and weighted log-likelihood ratio (WLLR), have been proposed for the tasks and make text classification more efficient and accurate.
FS	DF	In addition to the "principled" FS methods, terms occurring in less than three documents ( 3 DF ≤ ) in the training set are removed.
character recognition	accuracy	In we seethe differences in character recognition accuracy by using only 5.8K characters and a full set of 61.5K lexicon.
Character recognition	accuracy	 Table 1: Character recognition accuracy under dif- ferent lexicons and the order of language model.
ASR	Word Error Rates (WERs)	Most ASR systems achieve Word Error Rates (WERs) of about 40-45% in realistic and uncontrolled lecture conditions ().
language modelling	WER	In contrast to some unsupervised approaches to language modelling that require large amounts of manual transcription, either from the same instructor or on the same topic (), the solution proposed by uses half of the lectures in a semester course to train an ASR system for the other half or for when the course is next offered, and still results in significant WER reductions.
ASR	WER reductions	As argued by, any ASR improvements that rely on manual transcripts need to offer a balance between the cost of producing those transcripts and the amount of improvement (i.e. WER reductions).
ASR	relative error reduction (RER)	Others combine the transcripts or word lattices (from which transcripts are extracted) of two complementary ASR systems, a technique first proposed in the context of NIST's ROVER system (Fiscus, 1997) with a 12% relative error reduction (RER), and subsequently widely employed in many ASR systems.
Parsing	accuracy	In the Parsing setting, we use its best configuration, which reaches a tagging accuracy of 97.25% on standard WSJ test data.
tagging uncased texts	accuracy	Additional experiments reveal two main contributing factors to this drop on WSJ: tagging uncased texts reduces tagging accuracy by about 1%, and using only wordbased features further reduces it by 0.6%.
Parameter tuning	BLEU	Parameter tuning was done with minimum error rate training, which was used to maximize BLEU ().
MT evaluation	BLEU	To evaluate the translations produced using the various source and target models and the different rule-sets, we rely mostly on manual assessment, since automatic MT evaluation metrics like BLEU do not capture well the type of semantic variations: Translation acceptance when using only paraphrases and when using all entailment rules.
SMT evaluation	TE	For different models, the agreement rate varied from 67% to 78% (72% overall), and the Kappa value ranged from 0.34 to 0.55, which is comparable to figures reported for other standard SMT evaluation metrics . Translation with TE For each model m, we measured P recision m , the percentage of acceptable translations out of all sampled translations.
SMT evaluation	P recision m	For different models, the agreement rate varied from 67% to 78% (72% overall), and the Kappa value ranged from 0.34 to 0.55, which is comparable to figures reported for other standard SMT evaluation metrics . Translation with TE For each model m, we measured P recision m , the percentage of acceptable translations out of all sampled translations.
Translation	TE	For different models, the agreement rate varied from 67% to 78% (72% overall), and the Kappa value ranged from 0.34 to 0.55, which is comparable to figures reported for other standard SMT evaluation metrics . Translation with TE For each model m, we measured P recision m , the percentage of acceptable translations out of all sampled translations.
Translation	P recision m	For different models, the agreement rate varied from 67% to 78% (72% overall), and the Kappa value ranged from 0.34 to 0.55, which is comparable to figures reported for other standard SMT evaluation metrics . Translation with TE For each model m, we measured P recision m , the percentage of acceptable translations out of all sampled translations.
MT evaluation	BLEU	Although automatic MT evaluation metrics are less appropriate for capturing the variations generated by our method, to ensure that there was no degradation in the system-level scores according to such metrics we also measured the models' performance using BLEU and METEOR.
MT evaluation	METEOR	Although automatic MT evaluation metrics are less appropriate for capturing the variations generated by our method, to ensure that there was no degradation in the system-level scores according to such metrics we also measured the models' performance using BLEU and METEOR.
SMT	BLEU	Our results on 400 test sentences, translated using an SMT system trained on around 13000 parallel sentences , show that suffix + semantic relation → case marker/suffix is a very useful translation factor, in the sense of making a significant difference to output quality as indicated by subjective evaluation as well as BLEU scores.
Subjective Evaluation	Fluency	 Table 3: Subjective Evaluation: Fluency Scale
sentence realization	accuracy	Our approach to sentence realization provides simplicity , efficiency and competitive accuracy.
relative position determination	BLEU score	On the foundation of relative position determination method, the combination of dependency relation and bigram word model achieves a BLEU score of 0.8615, and the combination of dependency relation and trigram word model achieves a BLEU score of 0.8772.
relative position determination	BLEU score	On the foundation of relative position determination method, the combination of dependency relation and bigram word model achieves a BLEU score of 0.8615, and the combination of dependency relation and trigram word model achieves a BLEU score of 0.8772.
MCE learning	BLEU score	For MCE learning, we selected the reference compression that maximize the BLEU score) (= argmax r∈R BLEU(r, R\r)) from the set of reference compressions and used it as correct data for training.
machine translation	ROUGE	Automated evaluation metrics that rate system behaviour based on automatically computable properties have been developed in a number of other fields: widely used measures include BLEU () for machine translation and ROUGE) for summarisation, for example.
Word alignment	BITG-S	 Table 2: Word alignment results on Chinese-English. Each column is a learning objective paired with an alignment  family. The first row represents our best model without external alignment models and the second row includes  features from the jointly trained HMM. Under likelihood, BITG-S uses the simple grammar (Section 2.2). BITG-N  uses the normal form grammar (Section 4.1).
alignment	accuracy	In section 3 we demonstrate two approaches to improve alignment accuracy through alignment combination.
word alignment	TER	In this paper, we compare four commonly used word alignment methods, namely GIZA++, TER, CLA and IHMM, for hypothesis alignment.
word alignment	IHMM	In this paper, we compare four commonly used word alignment methods, namely GIZA++, TER, CLA and IHMM, for hypothesis alignment.
word alignment	TER	Based on this decoder, we compare four commonly used word alignment methods (GIZA++, TER, CLA and IHMM) for hypothesis alignment using the same experimental data and the same multiple MT system outputs with similar features in terms of translation performance.
SeSAL	delay	For SeSAL with t = 0.99, the delay  has no particularly beneficial effect.
WS	precision	One state-of-the-art Korean WS model () is known to achieve a performance of 90.31% word-unit precision, which is comparable with other WS models for the Chinese or Japanese language.
parsing	accuracy	Overall parsing accuracy will be reported using the standard metrics of labeled attachment score (LAS) and unlabeled attachment score (UAS).Statistical significance is checked using Dan Bikel's randomized parsing evaluation comparator.
parsing	labeled attachment score (LAS)	Overall parsing accuracy will be reported using the standard metrics of labeled attachment score (LAS) and unlabeled attachment score (UAS).Statistical significance is checked using Dan Bikel's randomized parsing evaluation comparator.
parsing	unlabeled attachment score (UAS).Statistical significance	Overall parsing accuracy will be reported using the standard metrics of labeled attachment score (LAS) and unlabeled attachment score (UAS).Statistical significance is checked using Dan Bikel's randomized parsing evaluation comparator.
parsing	accuracy	We set the beam-width N to 300 and the beam factor γ to 10 −11 . We evaluated the parsing accuracy by using section 23.
parsing	accuracy	The result means that our proposed constraint of auxiliary trees improves parsing accuracy.
PTB parsing task	F-scores	The Berkeley parser) provides performance close to the state-of-the-art for the PTB parsing task, with reported F-scores of around 90%.
relation recognition	recall	Existing approaches to relation recognition do not address well problems with an open set of relations and a need for high recall: supervised methods are not easily scaled, while unsupervised and semi-supervised methods address a limited aspect of the problem, as they are restricted to frequent, explicit, highly localized patterns.
sentiment or reputation analysis	accuracy	In Web mining for sentiment or reputation analysis, it is important for reliable analysis to extract large amount of texts about certain products, shops, or persons with high accuracy.
ASR	accuracy	We combine the estimated ASR accuracy with the user's barge-in rate, which represents how well the user is accustomed to using the system, to predict interpretation errors in barge-in utterances.
ASR	accuracy	Experimental results showed that the estimated ASR accuracy improved prediction performance.
ASR	accuracy	Since this ASR accuracy and the barge-in rate are obtainable at runtime, they improve prediction performance without the need for manual labeling .
ASR	accuracy	The estimated ASR accuracy is combined with the user's barge-in rate to predict the interpretation error in the current barge-in utterance.
ASR	accuracy	Because the estimated ASR accuracy and the barge-in rate per user are obtainable at runtime, it is possible to improve prediction performance without any manual transcription or labeling.
ASR	accuracy	 Table 1: ASR accuracy by response type
machine translation (MT) evaluation	BLEU	Automatic tools for machine translation (MT) evaluation such as BLEU are well established, but have the drawbacks that they do not perform well at the sentence level and that they presuppose manually translated reference texts.
parsing	accuracy	However, as the parsing accuracy usually goes down dramatically with the increase of sentence length, translating long sentences often takes longtime and only produces degenerate translations.
parsing	accuracy	On one hand, parsing accuracy will be lower as the length of sentence grows.
class splitting	accuracies	Evaluation results of the system with the inclusion of this class splitting technique have shown the accuracies of 64.65% and 66.74% on the development and test data respectively.
SETs	CWS	The other SETs have been identified through SWS as the CWS for these SETs are significantly less than their corresponding SWS as shown in.
tokenize query words into several semantic segments	precision	It aims to tokenize query words into several semantic segments and help the search engine to improve the precision of retrieval.
paraphrase extraction	precision	Second, taking into consideration the special characteristics of our noisy data, we proposed several improvements to an existing general paraphrase extraction method, resulting in a significant performance gain -up to 58% relative improvement in precision.
classification	accuracy	shows the classification accuracy of NB against different threshold values.
identification of travel blogs	Recall	For the identification of travel blogs, we obtained scores of 38.1% for Recall and 86.7% for Precision.
identification of travel blogs	Precision	For the identification of travel blogs, we obtained scores of 38.1% for Recall and 86.7% for Precision.
Translation	BLEU score	 Table 2: Translation results (BLEU score) with  phrase tables trained with different word align- ment combination methods
SRC	accuracy	 Table 4: Overall SRC accuracy.
summarization	MEAD	Typical existing summarization methods include centroid-based methods (e.g., MEAD ( )), graph-ranking based methods (e.g.,), non-negative matrix factorization (NMF) based methods (e.g.,), Conditional random field (CRF) based summarization (, and LSA based methods).
SVM	ROUGE	Supervised Systems For SVM we use second order polynomial kernel for the ROUGE and ESSK labeled training.
Sem labeling	ROUGE	ESSK works as the best for HMM and Sem labeling performs the worst for all ROUGE scores.
N-gram compression tasks	compression rate	Experimental results for three large-scale N-gram compression tasks achieved a significant compression rate without any loss.
query refinement	precision	The hypothesis underlying most research on Web People Search is that query refinement is risky, because it can enhance precision but it will usually harm recall.
query refinement	recall	The hypothesis underlying most research on Web People Search is that query refinement is risky, because it can enhance precision but it will usually harm recall.
MT	ESEN	 Table 3: Performance of word-based MT system  in different alignment methods. The above is be- tween ENFR and ESEN.
MST parser	precision increment	For the 2nd-order MST parser trained on Penn Chinese Treebank (CTB) 5.0, the classifier give an precision increment of 0.5 points.
parsing	accuracy	We evaluate the parsing accuracy by the precision of lexical heads, which is the percentage of the words that have found their correct parents.
parsing	precision	We evaluate the parsing accuracy by the precision of lexical heads, which is the percentage of the words that have found their correct parents.
summarization	ROUGE measure	For the assessment of summarization performance, we adopted the widely used ROUGE measure) because of its higher correlation with human judgments.
word similarity estimation	accuracies	For the task of word similarity estimation using a large amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures.
document retrieval	precision at top T (MP@T )	This setting could be seen as document retrieval, and we can use an evaluation measure such as the mean of the precision at top T (MP@T ) or the mean average precision (MAP).
document retrieval	mean average precision (MAP)	This setting could be seen as document retrieval, and we can use an evaluation measure such as the mean of the precision at top T (MP@T ) or the mean average precision (MAP).
IE	F1 score	This paper presents LUCHS, a self-supervised, relation-specific IE system which learns 5025 relations-more than an order of magnitude greater than any previous approach-with an average F1 score of 61%.
Open extraction	precision	Open extraction is more scalable, but has lower precision and recall.
Open extraction	recall	Open extraction is more scalable, but has lower precision and recall.
Inversion Transduction Grammar (ITG)	speed	While Inversion Transduction Grammar (ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed.
TTT	F-score	To enable TTT achieving similar F-score or Fscore upper bound, the beam size has to be doubled and the time cost is more than twice the original (c.f.
TTT	Fscore upper bound	To enable TTT achieving similar F-score or Fscore upper bound, the beam size has to be doubled and the time cost is more than twice the original (c.f.
parsing	accuracy	To avoid a tradeoff between these two, we need to increase parsing speed, but without losing accuracy.
parsing	accuracy	Our goal is to increase parsing speed without loss of accuracy.
parsing	accuracy	By increasing the ambiguity level of the adaptive models to match the baseline system, we can also slightly increase supertagging accuracy, which can lead to higher parsing accuracy.
parsing	accuracy	The self-training method of adapting the supertagger to suit the parser increased parsing speed by more than 50% across all three domains, without loss of accuracy.
HMM tag transitions	accuracy	uses grammar-informed initialization for HMM tag transitions based on the universal combinatory rules of the CCG formalism to obtain 56.1% accuracy on ambiguous word tokens, a large improvement over the 33.0% accuracy obtained with uniform initialization for tag transitions.
HMM tag transitions	accuracy	uses grammar-informed initialization for HMM tag transitions based on the universal combinatory rules of the CCG formalism to obtain 56.1% accuracy on ambiguous word tokens, a large improvement over the 33.0% accuracy obtained with uniform initialization for tag transitions.
summarization	ROUGE	Evaluation We evaluated summarization quality using ROUGE (.
summarization	ROUGE-1	With regard to the summarization task, following, we used ROUGE-1 and ROUGE-2 to evaluate our system's output.
summarization	ROUGE-2	With regard to the summarization task, following, we used ROUGE-1 and ROUGE-2 to evaluate our system's output.
MaxEnt based error detection	CER	Using discrete word posterior probabilities as features in the MaxEnt based error detection model is marginally better than word posterior probability thresholding in terms of CER, but obtains a 13.79% relative improvement in F measure.
MaxEnt based error detection	F measure	Using discrete word posterior probabilities as features in the MaxEnt based error detection model is marginally better than word posterior probability thresholding in terms of CER, but obtains a 13.79% relative improvement in F measure.
translation recommendation	precision	In our framework, we recast translation recommendation as a binary classification (rather than regression) problem using SVMs, perform RBF kernel parameter optimization, employ posterior probability-based confidence estimation to support user-based tuning for precision and recall, experiment with feature sets involving MT-, TM-and system-independent features, and use automatic MT evaluation metrics to simulate post-editing effort.
translation recommendation	recall	In our framework, we recast translation recommendation as a binary classification (rather than regression) problem using SVMs, perform RBF kernel parameter optimization, employ posterior probability-based confidence estimation to support user-based tuning for precision and recall, experiment with feature sets involving MT-, TM-and system-independent features, and use automatic MT evaluation metrics to simulate post-editing effort.
MT	TER	Let Abe the set of recommended MT outputs, and B be the set of MT outputs that have lower TER than TM hits.
MT	confidence	 Table 5: Edit Statistics when Recommending MT Outputs in Classification, confidence=0.5  Insertion  Substitution  Deletion  Shift  MT 0.9849 ± 0.0408 2.2881 ± 0.0672 0.8686 ± 0.0370 1.2500 ± 0.0598  TM 0.7762 ± 0.0408 4.5841 ± 0.1036 3.1567 ± 0.1120 1.2096 ± 0.0554
MT	confidence	 Table 6: Edit Statistics when NOT Recommending MT Outputs in Classification, confidence=0.5  Insertion  Substitution  Deletion  Shift  MT 1.0830 ± 0.1167 2.2885 ± 0.1376 1.0964 ± 0.1137 1.5381 ± 0.1962  TM 0.7554 ± 0.0376 1.5527 ± 0.1584 1.0090 ± 0.1850 0.4731 ± 0.1083
MT	confidence	 Table 7: Edit Statistics when Recommending MT Outputs in Classification, confidence=0.85  Insertion  Substitution  Deletion  Shift  MT 1.1665 ± 0.0615 2.7334 ± 0.0969 1.0277 ± 0.0544 1.5549 ± 0.0899  TM 0.8894 ± 0.0594 6.0085 ± 0.1501 4.1770 ± 0.1719 1.6727 ± 0.0846
NE recognition	precision (P)	Afterwards, the answer keys for NE recognition and alignment were annotated manually, and used as the gold standard to calculate metrics of precision (P), recall (R), and F-score (F) for both NE recognition (NER) and NE alignment (NEA).
NE recognition	recall (R)	Afterwards, the answer keys for NE recognition and alignment were annotated manually, and used as the gold standard to calculate metrics of precision (P), recall (R), and F-score (F) for both NE recognition (NER) and NE alignment (NEA).
NE recognition	F-score (F)	Afterwards, the answer keys for NE recognition and alignment were annotated manually, and used as the gold standard to calculate metrics of precision (P), recall (R), and F-score (F) for both NE recognition (NER) and NE alignment (NEA).
comparator extraction	LSRs	For the comparator extraction, LSRs were learned from SET-B and applied for comparator extraction.
comparative question identification	precision	As evaluation measures for comparative question identification and comparator extraction, we used precision, recall, and F1-measure.
comparative question identification	recall	As evaluation measures for comparative question identification and comparator extraction, we used precision, recall, and F1-measure.
comparative question identification	F1-measure	As evaluation measures for comparative question identification and comparator extraction, we used precision, recall, and F1-measure.
classification	accuracy	Our classification accuracy results are 79.3% on our data and 63.6% on the Ó Séaghdha data.
parsing	errors	We applied our hierarchical joint model to parsing and named entity recognition, and it reduced errors by over 20% on both tasks when compared to a joint model trained on only the jointly annotated data.
translation	accuracy	The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems.
translation	accuracy	Experimental results show that our method leads to significant improvements in translation accuracy over the baseline systems.
translation	BLEU	The translation quality is evaluated in terms of case-insensitive NIST version BLEU metric.
classification	accuracy	In this paper we present a method of using the hierarchy of labels to improve the classification accuracy.
alignment	precision	We report the alignment quality in terms of precision, recall and Fscore.
alignment	recall	We report the alignment quality in terms of precision, recall and Fscore.
alignment	Fscore	We report the alignment quality in terms of precision, recall and Fscore.
alignment entropy	accuracy	It appears that there is only weak correlation between alignment entropy and L2P accuracy.
SMT	BLEU score	As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system.
SMT	BLEU	The alignment improvement results in an improvement of 2.16 BLEU score on phrase-based SMT system and an improvement of 1.76 BLEU score on parsing-based SMT system.
SMT	BLEU	If we use phrase collocation probabilities as additional features, the phrase-based SMT performance is further improved by 0.24 BLEU score.
Adjective ordering	accuracy	 Table 1: Adjective ordering accuracy (%). SVM  and Malouf (2000) trained on BNC, tested on  BNC (IN), Gutenberg (O1), and Medline (O2).
Spelling correction	accuracy	 Table 2: Spelling correction accuracy (%). SVM  trained on NYT, tested on NYT (IN) and out-of- domain Gutenberg (O1) and Medline (O2).
automated classification and filtering of documents	precision	In addition, TextRank can be considered languageindependent as long as it does not perform any morphological analysis. with on a daily basis (, assist in the automated classification and filtering of documents, and increase search engines precision.
SRL	F1	 Table 5: SRL results (F1) on the Brown test corpus broken down by role type. BL is the Base- line+HMM+Paths model, MSH is the Multi-Span-HMM model. Column 8 to 16 are all adjuncts (AM-).  We omit roles with ten or fewer examples.
SRL	BL	 Table 5: SRL results (F1) on the Brown test corpus broken down by role type. BL is the Base- line+HMM+Paths model, MSH is the Multi-Span-HMM model. Column 8 to 16 are all adjuncts (AM-).  We omit roles with ten or fewer examples.
parsing	accuracy	The results show that proposed shallow representations of sentence structure are robust to reductions in parsing accuracy, and that the contribution of alternative representations of sentence structure to successful semantic role labeling varies with the integrity of the parsing and argument-identification stages.
parsing	accuracy	By using the HMM part-of-speech tagger in this way, we can ask how the simple structural features that we propose children start withstand up to reductions in parsing accuracy.
reconstructing incomplete groups	precision	 Table 2: Accuracies for reconstructing incomplete groups.  Scores reported are precision, recall, and F1, averaged over  all word pairs.
reconstructing incomplete groups	recall	 Table 2: Accuracies for reconstructing incomplete groups.  Scores reported are precision, recall, and F1, averaged over  all word pairs.
reconstructing incomplete groups	F1	 Table 2: Accuracies for reconstructing incomplete groups.  Scores reported are precision, recall, and F1, averaged over  all word pairs.
Morphological segmentation	accuracy	In practice for our model, we use a high cognate prior, thus only ruling out precision recall f-measure Morfessor 88.87% 67.48% 76.71% Our Model 86.62% 90.53% 88.53%: Morphological segmentation accuracy fora standard unsupervised baseline and our model.
Morphological segmentation	accuracy	Morphological segmentation Finally, we evaluate the accuracy of our model's morphological segmentation for Ugaritic words.
parsing raw text	recall	Because of the strong case syncretism in German, traditional classification models using local information only run the risk of predicting multiple occurences of the same function (subject, object etc.) at the same level, causing feature clashes in the constraint solver with no f-structure being produced. and identify this as a major problem resulting in a considerable loss in coverage of the German annotation algorithm compared to English, in particular for parsing raw text, where TiGer function labels have to be supplied by a machine-learning-based method and where the coverage of the LFG annotation algorithm drops to 93.62% with corresponding drops in recall and f-scores for the f-structure evaluations.
parsing	F1	In the parsing case, the central result is that accuracies in the range of state-of-the-art parsers (i.e., over 88% F1 on English WSJ) can be obtained with no sampling, no latent-variable modeling, no smoothing, and even no explicit lexicon (hence negligible training overall).
SRL	SRL	According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short).
syntactic parsing	recall	For the evaluation measurement on syntactic parsing, we report labeled recall, labeled precision, and their F1-measure.
syntactic parsing	precision	For the evaluation measurement on syntactic parsing, we report labeled recall, labeled precision, and their F1-measure.
syntactic parsing	F1-measure	For the evaluation measurement on syntactic parsing, we report labeled recall, labeled precision, and their F1-measure.
Cross-language sentiment classification	Accuracy	 Table 1: Cross-language sentiment classification results. For each task, the number of unlabeled docu- ments from S and T is given. Accuracy scores (mean µ and standard deviation σ of 10 repetitions of  SGD) on the test set of the target language T are reported. ∆ gives the difference in accuracy to the  upper bound. CL-SCL uses m = 450, k = 100, and φ = 30.
Cross-language sentiment classification	accuracy	 Table 1: Cross-language sentiment classification results. For each task, the number of unlabeled docu- ments from S and T is given. Accuracy scores (mean µ and standard deviation σ of 10 repetitions of  SGD) on the test set of the target language T are reported. ∆ gives the difference in accuracy to the  upper bound. CL-SCL uses m = 450, k = 100, and φ = 30.
Cross-language sentiment classification	φ	 Table 1: Cross-language sentiment classification results. For each task, the number of unlabeled docu- ments from S and T is given. Accuracy scores (mean µ and standard deviation σ of 10 repetitions of  SGD) on the test set of the target language T are reported. ∆ gives the difference in accuracy to the  upper bound. CL-SCL uses m = 450, k = 100, and φ = 30.
summaries	ROUGE scores	Our results show that summaries biased by dependency pattern models lead to significantly higher ROUGE scores than both n-gram language models reported in previous work and also Wikipedia base-line summaries.
interpretation task	accuracy	In general, the sequential nature of the interpretation task makes it difficult to achieve high action accuracy.
grammar induction	accuracy	We demonstrate that derived constraints aid grammar induction by training Klein and Manning's Dependency Model with Valence (DMV) on this data set: parsing accuracy on Section 23 (all sentences) of the Wall Street Journal corpus jumps to 50.4%, beating previous state-of-the-art by more than 5%.
Information Retrieval (IR)	accuracy	In this paper, we examine whether term weighting functions adopted from Information Retrieval (IR) based on the standard tf.idf formula and adapted to the particular setting of sentiment analysis can help classification accuracy.
sentiment analysis	accuracy	In this paper, we examine whether term weighting functions adopted from Information Retrieval (IR) based on the standard tf.idf formula and adapted to the particular setting of sentiment analysis can help classification accuracy.
classification	accuracy	In this paper, we examine whether term weighting functions adopted from Information Retrieval (IR) based on the standard tf.idf formula and adapted to the particular setting of sentiment analysis can help classification accuracy.
solving event coreference	BUSI-NESS	However, the utilization of the ACE corpus for the task of solving event coreference is limited because this resource provides only withindocument event coreference annotations using a restricted set of event types such as LIFE, BUSI-NESS, CONFLICT, and JUSTICE.
coreference resolution	B 3	This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B 3 , and CEAF.
translation	BLEU	 Table 1: Experimental results demonstrate that the full extraction set model outperforms supervised and  unsupervised baselines in evaluations of word alignment quality, extraction set quality, and translation.  In word and bispan evaluations, GIZA++ did not have access to a dictionary while all other methods  did. In the BLEU evaluation, all systems used a bilingual dictionary included in the training corpus. The  BLEU evaluation of supervised systems also included rule counts from the Joint HMM to compensate  for parse failures.
translation	BLEU	 Table 1: Experimental results demonstrate that the full extraction set model outperforms supervised and  unsupervised baselines in evaluations of word alignment quality, extraction set quality, and translation.  In word and bispan evaluations, GIZA++ did not have access to a dictionary while all other methods  did. In the BLEU evaluation, all systems used a bilingual dictionary included in the training corpus. The  BLEU evaluation of supervised systems also included rule counts from the Joint HMM to compensate  for parse failures.
WSD	accuracy	It is a common observation that domain specific WSD exhibits high level of accuracy even for the all-words scenario () -provided training and testing are on the same domain.
translation	BLEU-4	The translation quality is evaluated by case-insensitive BLEU-4 metric ().
MT evaluation	BLEU	NCD is a general information theoretic measure of string similarity, whereas most MT evaluation measures, e.g., BLEU and METEOR, are specifically constructed for the task.
MT evaluation	METEOR	NCD is a general information theoretic measure of string similarity, whereas most MT evaluation measures, e.g., BLEU and METEOR, are specifically constructed for the task.
MT evaluation	BLEU	The NCD measure did not match the performance of the state-of-the-art MT evaluation measures in English, but it presented a viable alternative to de facto standard BLEU), which is simple and effective but has been shown to have a number of drawbacks).
n-grams-based machine translation (MT)	BLEU	We illustrate and explain problems of n-grams-based machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech.
SRL	F-measure	With semantics-driven shallow parsing, our SRL system achieves 76.10 F-measure, with gold segmentation and POS tagging.
collocation extraction field	likelihoodratio test statistic	PMI was introduced into the collocation extraction field by. proposed the use of the likelihoodratio test statistic, which is equivalent to MI up to a constant factor.
collocation extraction field	MI	PMI was introduced into the collocation extraction field by. proposed the use of the likelihoodratio test statistic, which is equivalent to MI up to a constant factor.
word alignment	accuracy	Therefore, it is easy to imagine that simply replacing all occurrences of 'chief' and 'forefront' with 'head' do sometimes harm with word alignment accuracy, and we have to model either the context or senses of words.
POS tagging	accuracies	Applying our approach to the POS tagging problem, we obtain higher accuracies than both EM and Bayesian inference as reported by.
tagging	accuracy	 Table 1: MAP-EM with a L0 norm achieves higher  tagging accuracy on English than (2007) and much  higher than standard EM.
tagging	accuracy	As implemented here, it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods.
tagging	accuracy	First, it achieves state-of-the-art tagging accuracy.
Tagging	accuracy	 Table 1. Tagging accuracy under the best M-to-1 map, the greedy 1-to-1 map, and  VI, for the full PTB45 tagset and the reduced PTB17 tagset. HMM-EM, HMM-VB  and HMM-GS show the best results from
parsing	accuracy	Both algorithms are shown to improve parsing accuracy.
generalisation	accuracy	Our models were all sampled for 5k iterations with hyperparameter inference for α c and s c ∀ c ∈ N , but in contrast to our previous approach we did not use annealing which we did not find to help generalisation accuracy.
MH	acceptance	The MH acceptance rates were in excess of 99% across both training and parsing.
QA	accuracy	In Section 5 we evaluate the QA accuracy by training the QA system with the resulting clusters.
Classification	accuracy	 Table 2: Classification accuracy. Scores sig- nificantly different from the best performance  (p 2t <0.05 on paired t-test) are given an asterisk.
SEQ	precision	We show that the novel, domain-independent sequence frame in SEQ substantially boosts the precision and recall of the system and yields coherent sequences filtered from low-precision extractions.
SEQ	recall	We show that the novel, domain-independent sequence frame in SEQ substantially boosts the precision and recall of the system and yields coherent sequences filtered from low-precision extractions.
preposition selection	precision	Results show a significant improvement in the preposition selection task on native speaker text and a modest increment in precision and recall in an ESL error detection task.
preposition selection	recall	Results show a significant improvement in the preposition selection task on native speaker text and a modest increment in precision and recall in an ESL error detection task.
SMT task	BLEU	Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics.
SMT task	BLEU	Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics.
MT	BLEU	Evaluation: All the MT systems are run on the Spanish test data and the quality of the resulting English translations are evaluated using two different measures-(1) Normalized edit distance score), and (2) BLEU (Papineni et When computing edit distance, we account for substitutions, insertions, deletions as well as local-swap edit operations required to convert a given English string into the (gold) reference translation.
MT	edit distance scores	Results: compares the results of various MT systems (using parallel versus decipherment training) on the two test corpora in terms of edit distance scores (a lower score indicates closer match to the gold translation).
MT	BLEU score	Better LMs yield better MT results for both parallel and decipherment training-for example, using a segment-based English LM instead of a 2-gram LM yields a 24% reduction in edit distance and a 9% improvement in BLEU score for EM decipherment.
MT	accuracy	In other words, "how much non-parallel data is worth how much parallel data in order to achieve the same MT accuracy?" provides a reasonable answer to this question for the Spanish/English MT task described here.
translation	DT	 Table 3: Statistics and translation results for four types of  tree-to-string rules. With the exception of '# nodes/tree',  the numbers in the table are in millions and the time is in  hours. Here, fw denotes function word, and DT denotes  the decoding time, and the BLEU scores were computed  on the test set.
translation	BLEU	 Table 3: Statistics and translation results for four types of  tree-to-string rules. With the exception of '# nodes/tree',  the numbers in the table are in millions and the time is in  hours. Here, fw denotes function word, and DT denotes  the decoding time, and the BLEU scores were computed  on the test set.
MT	BLEU	Automatic evaluation measures for MT, BLEU (Papineni et al., 2002), WER (Word Error Rate) and PER (Position Independent Word Error Rate) use the word as the basic unit rather than morphemes.
MT	WER (Word Error Rate)	Automatic evaluation measures for MT, BLEU (Papineni et al., 2002), WER (Word Error Rate) and PER (Position Independent Word Error Rate) use the word as the basic unit rather than morphemes.
MT	PER (Position Independent Word Error Rate	Automatic evaluation measures for MT, BLEU (Papineni et al., 2002), WER (Word Error Rate) and PER (Position Independent Word Error Rate) use the word as the basic unit rather than morphemes.
POS tagging	accuracy	In case of POS tagging, the decisions are ternary, and hence we report the classification accuracy.
MAP	♯	 Table 2: Results of MAP for HP/NP to TD adaptation.  †,  ‡, ♯ and boldface indicate significantly better than no-weight,  doc-pair, doc-avg and doc-comb, respectively. Confidence level is set at 95%
MAP	Confidence	 Table 2: Results of MAP for HP/NP to TD adaptation.  †,  ‡, ♯ and boldface indicate significantly better than no-weight,  doc-pair, doc-avg and doc-comb, respectively. Confidence level is set at 95%
MAP	Confidence level	 Table 3: Results of MAP for TD to HP/NP adaptation.  †,  ‡, ♯ and boldface indicate significantly better than no-weight,  doc-pair, doc-avg and doc-comb, respectively. Confidence level is set as 95%.
Supervised sentiment classification	accuracy	 Table 2: Supervised sentiment classification accuracy.
classification	accuracy	The evaluation metric is classification accuracy on a target domain, computed as the percentage of correctly classified target domain reviews out of the total number of reviews in the target domain.
Cross-domain sentiment classification	accuracy	 Table 3: Cross-domain sentiment classification accuracy.
Classification	accuracy	 Table 2: Classification accuracy on three tasks. From left to right the datasets are: A collection of 2,000 movie reviews  often used as a benchmark of sentiment classification (Pang
machine paraphrasing	BLEU	One of the limitations to the development of machine paraphrasing is the lack of standard metrics like BLEU, which has played a crucial role in driving progress in MT.
query expansion	recall	On the other hand, a query expansion algorithm might be less concerned with preserving the precise meaning so long as additional relevant terms are added to improve search recall.
SLM	TAGGER	Similar to SLM), after the parses undergo headword percolation and binarization, each model component of WORD-PREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences.
machine translation	BLEU score	Chiang (2007) studied the performance of machine translation on Hiero, the BLEU score is 33.31% when n-gram is used to re-rank the N -best list, however, the BLEU score becomes significantly higher 37.09% when the n-gram is embedded directly into Hiero's one pass decoder, this is because there is not much diversity in the N -best list.
machine translation	BLEU score	Chiang (2007) studied the performance of machine translation on Hiero, the BLEU score is 33.31% when n-gram is used to re-rank the N -best list, however, the BLEU score becomes significantly higher 37.09% when the n-gram is embedded directly into Hiero's one pass decoder, this is because there is not much diversity in the N -best list.
MT error prediction	accuracy	Experimental results show that the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score.
MT error prediction	F-score	Experimental results show that the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score.
MT evaluation	BLEU	As machine translation systems improve in lexical choice and fluency , the shortcomings of widespread n-gram based, fluency-oriented MT evaluation metrics such as BLEU, which fail to properly evaluate adequacy, become more apparent.
MT output	HMEANT	We first show that when using untrained monolingual readers to annotate semantic roles in MT output, the non-automatic version of the metric HMEANT achieves a 0.43 correlation coefficient with human adequacy judgments at the sentence level, far superior to BLEU at only 0.20, and equal to the far more expensive HTER.
MT output	BLEU	We first show that when using untrained monolingual readers to annotate semantic roles in MT output, the non-automatic version of the metric HMEANT achieves a 0.43 correlation coefficient with human adequacy judgments at the sentence level, far superior to BLEU at only 0.20, and equal to the far more expensive HTER.
machine translation	accuracy	In this paper we show that evaluating machine translation by assessing the translation accuracy of each argument in the semantic role framework correlates with human judgment on translation adequacy as well as HTER, at a significantly lower labor cost.
translation fluency	BLEU	While BLEU score performs well in capturing the translation fluency, and report cases where BLEU strongly disagree with human judgment on translation quality.
translation	accuracy	We present the results of evaluating translation utility by measuring the accuracy within a semantic role labeling (SRL) framework.
SRL	BLEU	We show empirically that our proposed SRL based evaluation metric, which uses untrained monolingual humans to annotate semantic frames in MT output, correlates with human adequacy judgments as well as HTER, and far better than BLEU and other commonly used metrics.
MT output	BLEU	We show empirically that our proposed SRL based evaluation metric, which uses untrained monolingual humans to annotate semantic frames in MT output, correlates with human adequacy judgments as well as HTER, and far better than BLEU and other commonly used metrics.
SRL annotation	ASSERT	The correlation with human judgment on adequacy of the fully automated SRL annotation version, i.e., applying ASSERT on both the reference translation and the MT output, of the SRL based evaluation metric is about 80% of that of HTER.
role filler	accuracy	However, we believe the evaluation of role filler accuracy will also be automatable.
Authorship attribution	accuracy	 Table 3: Authorship attribution accuracy when using bags  of local histograms and different kernels for word-based  and character-based representations. The BC data set is  used. Settings 1, 2 and 3 correspond to k = 2, 5 and 20,  respectively.
topic description	average length	 Table 1: Corpus statistics: topic description, number of  conversations in each topic, average length (number of  dialog acts), and standard deviation.
topic description	standard	 Table 1: Corpus statistics: topic description, number of  conversations in each topic, average length (number of  dialog acts), and standard deviation.
cluster  prediction task	precision	 Table 2: Results using the MUC metric on the cluster  prediction task. Note that while the precision of the base- line is higher, the recall and overall F1 of our model out- weighs that. While MUC has a deficiency in that putting  everything into a single cluster will artificially inflate the  score, parameters on our model are set so that the model  uses the same number of clusters as the baseline system.
cluster  prediction task	recall	 Table 2: Results using the MUC metric on the cluster  prediction task. Note that while the precision of the base- line is higher, the recall and overall F1 of our model out- weighs that. While MUC has a deficiency in that putting  everything into a single cluster will artificially inflate the  score, parameters on our model are set so that the model  uses the same number of clusters as the baseline system.
cluster  prediction task	F1	 Table 2: Results using the MUC metric on the cluster  prediction task. Note that while the precision of the base- line is higher, the recall and overall F1 of our model out- weighs that. While MUC has a deficiency in that putting  everything into a single cluster will artificially inflate the  score, parameters on our model are set so that the model  uses the same number of clusters as the baseline system.
candidate selection	precision	For candidate selection, we once again evaluate using token-level precision, recall and F-score.
candidate selection	recall	For candidate selection, we once again evaluate using token-level precision, recall and F-score.
candidate selection	F-score	For candidate selection, we once again evaluate using token-level precision, recall and F-score.
SMT	precision	Additionally, we evaluate using the BLEU score over the normalised form of each message, as the SMT method can lead to perturbations of the token stream, vexing standard precision, recall and F-score evaluation.
SMT	recall	Additionally, we evaluate using the BLEU score over the normalised form of each message, as the SMT method can lead to perturbations of the token stream, vexing standard precision, recall and F-score evaluation.
SMT	F-score	Additionally, we evaluate using the BLEU score over the normalised form of each message, as the SMT method can lead to perturbations of the token stream, vexing standard precision, recall and F-score evaluation.
keyphrase extraction	precision	Traditionally keyphrase extraction is evaluated using precision and recall on all the extracted keyphrases.
keyphrase extraction	recall	Traditionally keyphrase extraction is evaluated using precision and recall on all the extracted keyphrases.
information retrieval	IdealScore (K,t)	Inspired by the popular nDCG metric in information retrieval), we define the following normalized keyphrase quality measure (nKQM) fora method M: where T is the set of topics, M t,j is the jth keyphrase generated by method M for topic t, score(·) is the average score from the two human judges, and IdealScore (K,t) is the normalization factor-score of the top K keyphrases of topic t under the ideal ranking.
Phrase pair extraction	accuracy	 Table 3: Phrase pair extraction accuracy for phrase pairs  up to length 5. "grow-diag" denotes the grow-diag-final  heuristic.
word alignment	precision	We evaluate our word alignment system on two language pairs using gold standard word alignments and achieve improvements of 10% and 13.5% in precision and 3.5% and 13.5% in recall.
word alignment	recall	We evaluate our word alignment system on two language pairs using gold standard word alignments and achieve improvements of 10% and 13.5% in precision and 3.5% and 13.5% in recall.
Word alignment	precision	 Table 4: Word alignment results on the test data of En- glish/Hindi (EH) and English/Arabic (EA) where P b is  the precision of baseline GIZA++ and P ti is the precision  of our word alignment system
Word alignment	precision	 Table 4: Word alignment results on the test data of En- glish/Hindi (EH) and English/Arabic (EA) where P b is  the precision of baseline GIZA++ and P ti is the precision  of our word alignment system
parsing	accuracy	We show experimentally that this pipeline significantly lowers the upper bound on parsing accuracy ( §3).
AST	Parameter β	 Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
scope identification	precision	Our tagging  Our scope identification component extracted the scope of target references with good precision (86.4%) but low recall (35.2%).
scope identification	recall	Our tagging  Our scope identification component extracted the scope of target references with good precision (86.4%) but low recall (35.2%).
summarization tasks	ROUGE	Section 5 presents results on both generic and query-focused summarization tasks, showing as far as we know the best known ROUGE results for DUC-04 through DUC-06, and the best known precision results for DUC-07, and the best recall DUC-07 results among those that do not use a web search engine.
summarization tasks	precision	Section 5 presents results on both generic and query-focused summarization tasks, showing as far as we know the best known ROUGE results for DUC-04 through DUC-06, and the best known precision results for DUC-07, and the best recall DUC-07 results among those that do not use a web search engine.
summarization tasks	recall	Section 5 presents results on both generic and query-focused summarization tasks, showing as far as we know the best known ROUGE results for DUC-04 through DUC-06, and the best known precision results for DUC-07, and the best recall DUC-07 results among those that do not use a web search engine.
summarization evaluation	ROUGE-N	ROUGE) is widely used for summarization evaluation and it has been shown that ROUGE-N scores are highly correlated with human evaluation).
summation	ROUGE-N	Since summation preserves submodularity, and the denominator is constant, we see that F ROUGE-N is monotone submodular.
IE	precision	Most approaches to IE use supervised learning of relation-specific examples, which can achieve high precision and recall.
IE	recall	Most approaches to IE use supervised learning of relation-specific examples, which can achieve high precision and recall.
sentential extraction	accuracy	We manually compute sentential extraction accuracy by sampling a set of 1000 sentences from Se ∪ SF and manually labeling the correct extraction decision, either a relation r ∈ R or none.
RE	LIBLINEAR	To build our RE system, we use the LIBLINEAR () package, with its default settings of L2-loss SVM (dual) as the solver, and we use an epsilon of 0.1.
WSD	accuracy	Recent work by in this direction has shown that it is possible to perform cost effective WSD in a target language (L 2 ) without compromising much on accuracy by leveraging on the annotation work done in another language ).
word sense induction	recall	The word sense induction process produces only one of these mappings, which limits maximum possible recall in this experiment.
semantic parsing	accuracies	On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms.
POS tagging	accuracy	Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.'s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).
POS tagging	accuracy	Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.'s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).
phrase extraction	accuracy	As a solution to this, they proposed a supervised discriminative model that performs joint word alignment and phrase extraction, and found that joint estimation of word alignments and extraction sets improves both word alignment accuracy and translation results.
word alignment	accuracy	As a solution to this, they proposed a supervised discriminative model that performs joint word alignment and phrase extraction, and found that joint estimation of word alignments and extraction sets improves both word alignment accuracy and translation results.
phrase alignment and extraction	FLAT	We compare the accuracy of our proposed method of joint phrase alignment and extraction using the FLAT, HIER and HLEN models, with a baseline of using word alignments from GIZA++ and heuristic phrase extraction.
phrase alignment and extraction	HLEN	We compare the accuracy of our proposed method of joint phrase alignment and extraction using the FLAT, HIER and HLEN models, with a baseline of using word alignments from GIZA++ and heuristic phrase extraction.
WSJ dependency parsing	Dev	 Table 1: UAS results for English WSJ dependency parsing. Dev  is WSJ section 22 (all sentences) and Test is WSJ section 23  (all sentences). The order 2 baseline represents McDonald and  Pereira (2006).
OOV detection	OOV rate	In addition we report OOV detection results on a MIT lectures data set () consisting of 3 Hrs from two speakers with a 1.5% OOV rate.
OOV detection	accuracy	We report OOV detection accuracy using standard detection error tradeoff (DET) curves ().
OOV detection	standard detection error tradeoff (DET)	We report OOV detection accuracy using standard detection error tradeoff (DET) curves ().
entity linking	accuracy	Experimental results show that our method can significantly improve the entity linking accuracy.
Shooting	General Injury	Rather than one broad Attack type, we learn several: Shooting, Murder, Coup, General Injury, and Pipeline Attack.
SMT	reordering score	In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper).
crossevent inference	Precision (P)	To compare with the reported work on crossevent inference and its sentence-level baseline system, we cross-validate our method on 10 separate sets of 40 ACE texts, and report the optimum, worst and mean performances (see) on the data by using Precision (P), Recall (R) and F-measure (F).
crossevent inference	Recall (R)	To compare with the reported work on crossevent inference and its sentence-level baseline system, we cross-validate our method on 10 separate sets of 40 ACE texts, and report the optimum, worst and mean performances (see) on the data by using Precision (P), Recall (R) and F-measure (F).
crossevent inference	F-measure (F)	To compare with the reported work on crossevent inference and its sentence-level baseline system, we cross-validate our method on 10 separate sets of 40 ACE texts, and report the optimum, worst and mean performances (see) on the data by using Precision (P), Recall (R) and F-measure (F).
event extraction	precision	This multi-layered approach creates an event extraction system that can discover role fillers in a variety of different contexts, while maintaining good precision.
Entity Linking task	NIL	In the Entity Linking task, given a person (PER), organization (ORG) or geo-political entity (GPE, a location with a government) query that consists of a name string and a background document containing that name string, the system is required to provide the ID of the KB entry to which the name refers; or NIL if there is no such KB entry.
evidence fusion	Term	 Table 5. Performance comparison among various  evidence fusion methods (Term set: Ext100; p=2  for PNorm)
entity detection and tracking	RDC	The first stage is called EDT (entity detection and tracking) while the second stage is called RDC (relation detection and characterization).
Classification	accuracy	 Table 4. Classification accuracy and kappa for spe- cialized DA classifiers. Statistically significant  differences (across ten folds, one-tailed t-test) are  shown in bold.
Classification	kappa	 Table 4. Classification accuracy and kappa for spe- cialized DA classifiers. Statistically significant  differences (across ten folds, one-tailed t-test) are  shown in bold.
MT	BLEU	Specifically, we measure the correlation (using Pearson's r) between BLEU scores of MT systems measured against nonprofessional translations, and BLEU scores measured against professional translations.
predicting function tags	error rate	Third, the uniform treatment not only simplifies the model building process, but also affords us to concentrate on discovering most useful features fora particular application which often leads to improved performances, e.g, we find some features are very effective in predicting function tags and our system 1230 has significant lower error rate than.
translation errors	consistency	Moreover, even in the case of translation errors, consistency in the errors (e.g. repetitive error patterns) are easier to diagnose and subsequently correct by translators.
SMT	BLEU score	The performance of the phrase-based SMT system is measured by BLEU score () and TER ().
SMT	TER	The performance of the phrase-based SMT system is measured by BLEU score () and TER ().
Translation	BLEU	 Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
Translation	CF	 Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
SMT	BLEU	Experimental results show that our approach outperforms the best component SMT system by up to 2.11 BLEU points.
SMT	BLEU	The key contributions of this paper are three: the usage of a linear combination of distributions within the MBR decoding, which allows multiple SMT models to be involved in, and makes the computation of n-grams statistics to be more accurate; the decoding in an extended search space, which allows to find better hypotheses than the evidences provided by the component models; and the use of an expected BLEU score instead of the sentence-wise BLEU, which allows to efficiently apply MBR decoding in the huge search space under consideration.
SMT	BLEU	The key contributions of this paper are three: the usage of a linear combination of distributions within the MBR decoding, which allows multiple SMT models to be involved in, and makes the computation of n-grams statistics to be more accurate; the decoding in an extended search space, which allows to find better hypotheses than the evidences provided by the component models; and the use of an expected BLEU score instead of the sentence-wise BLEU, which allows to efficiently apply MBR decoding in the huge search space under consideration.
translation	BLEU	We evaluated translation quality using the BLEU metric, as calculated by mteval-v11b.pl with case-insensitive matching of n-grams.
translation	BLEU	Our experimental results show that both models are able to significantly improve translation quality and collectively achieve up to 1 BLEU point over a competitive baseline.
word segmentation task	f-score	Our system achieves an f-score of 98.17 for the word segmentation task and an f-score of 94.02 for the whole task, resulting in relative error reductions of 14.1% and 5.5% respectively over the best system reported in the literature.
word segmentation task	error	Our system achieves an f-score of 98.17 for the word segmentation task and an f-score of 94.02 for the whole task, resulting in relative error reductions of 14.1% and 5.5% respectively over the best system reported in the literature.
partof-speech tagging	accuracy	Another observation is we can still evaluate Chinese word segmentation and partof-speech tagging accuracy, by reading off the corresponding result from parse trees.
constituent parsing	F 1 measure	For constituent parsing, the best result on CTB 5.0 is reported to be 78% F 1 measure for unlimited sentences with automatically assigned POS tags.
SemEval'07 task	precision	The standard evaluation script from the SemEval'07 task calculates precision, recall, and F 1 -score for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one in the FrameNet lexicon.
SemEval'07 task	recall	The standard evaluation script from the SemEval'07 task calculates precision, recall, and F 1 -score for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one in the FrameNet lexicon.
SemEval'07 task	F 1 -score	The standard evaluation script from the SemEval'07 task calculates precision, recall, and F 1 -score for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one in the FrameNet lexicon.
WSD task	recall	The systems are then evaluated as in a standard WSD task, using recall.
aspect identification	F 1 -measure	The details of our product review data set is given in  To examine the performance on aspect identification and sentiment classification, we employed F 1 -measure, which was the combination of precision and recall, as the evaluation metric.
aspect identification	precision	The details of our product review data set is given in  To examine the performance on aspect identification and sentiment classification, we employed F 1 -measure, which was the combination of precision and recall, as the evaluation metric.
aspect identification	recall	The details of our product review data set is given in  To examine the performance on aspect identification and sentiment classification, we employed F 1 -measure, which was the combination of precision and recall, as the evaluation metric.
sentiment classification	F 1 -measure	The details of our product review data set is given in  To examine the performance on aspect identification and sentiment classification, we employed F 1 -measure, which was the combination of precision and recall, as the evaluation metric.
sentiment classification	precision	The details of our product review data set is given in  To examine the performance on aspect identification and sentiment classification, we employed F 1 -measure, which was the combination of precision and recall, as the evaluation metric.
sentiment classification	recall	The details of our product review data set is given in  To examine the performance on aspect identification and sentiment classification, we employed F 1 -measure, which was the combination of precision and recall, as the evaluation metric.
Document-level Review Sentiment Classification	IA	 Table 6: Evaluations on Term Weighting methods for Document-level Review Sentiment Classification. IA denotes  the term weighing based on the important aspects. * significant t-test, p-values<0.05.
parsing	accuracy	It is well known that parsing accuracy suffers when a model is applied to out-of-domain data.
parsing	Labeled Attachment Score (LAS)	In all experiments, parsing performance is measured as Labeled Attachment Score (LAS), the percentage of tokens with correct dependency edge and label.
Parsing	accuracy	Parsing accuracy is measured using labelled and unlabelled predicate argument structure recovery); we evaluate on all sentences and thus penalise lower coverage.
transliteration task of Western names to Japanese	accuracy	The experimental results of the transliteration task of Western names to Japanese show that the proposed model can achieve higher accuracy compared to the conventional models without latent classes.
Rhyme scheme	accuracy	 Table 1: Rhyme scheme accuracy and F-Score (computed from average precision and recall over all lines) using our algorithm
Rhyme scheme	F-Score	 Table 1: Rhyme scheme accuracy and F-Score (computed from average precision and recall over all lines) using our algorithm
Rhyme scheme	recall	 Table 1: Rhyme scheme accuracy and F-Score (computed from average precision and recall over all lines) using our algorithm
Natural language understanding (NLU)	accuracy	Natural language understanding (NLU) at the level of speech acts for conversational dialogue systems can be performed with high accuracy in limited domains using data-driven techniques (, for example), provided that enough training material is available.
MT evaluation	BLEU	A variety of automatic MT evaluation metrics have been developed over the years, including BLEU (), NIST), METEOR (exact) (), GTM (, and TER).
MT evaluation	METEOR (exact)	A variety of automatic MT evaluation metrics have been developed over the years, including BLEU (), NIST), METEOR (exact) (), GTM (, and TER).
MT evaluation	TER	A variety of automatic MT evaluation metrics have been developed over the years, including BLEU (), NIST), METEOR (exact) (), GTM (, and TER).
MT	BLEU	To our surprise, the gain in MT performance using human alignment is very small, less than 1 point in BLEU.
SMT	BLEU	For each language pair, we trained standard phrase-based SMT systems in both directions (including alignment symmetrization and log-linear model tuning) using Moses (, SRILM, and ZMERT (Zaidan, 2009) tools and evaluated using BLEU (  GIZA++ (Och and Ney, 2003) for EM.
parsing	accuracy	Representing the type of information a statistical system uses to make predictions, feature templates can be one of the most important factors determining parsing accuracy.
parsing	accuracy	We explore considerably richer feature representations and show that they improve parsing accuracy significantly.
parsing	accuracy	Potential arcs are scored using rich linear models that are discriminatively trained to maximize parsing accuracy).
Parsing	Accounts	 Table 2: Parsing with jointly-trained filters outperforms independently-trained filters (R+L), as well as a more complex  cascade (R+L+Q). *Accounts for total time spent parsing and applying filters, averaged over five runs.
image description evaluation	Fluency	Fluency/Readability: Both the weather forecast and image description evaluation experiments used a quality criterion intended to capture 'how well apiece of text reads', called Fluency in the latter, Readability in the former.
monologue-to-dialogue generation	accuracy	After briefly motivating the task of monologue-to-dialogue generation, we describe the system and present an evaluation in terms of fluency and accuracy.
machine translation tasks	BLEU	Experimental results on Chinese-English machine translation tasks show an average improvement of 0.45 BLEU and 1.22 TER points across 5 different NIST test sets.
machine translation tasks	TER	Experimental results on Chinese-English machine translation tasks show an average improvement of 0.45 BLEU and 1.22 TER points across 5 different NIST test sets.
SemEval	precision	 Table 1: SemEval precision scores averaged over all twenty test words
agreement detection	precision	Overall, our approach achieves 79.2% (precision), 50.5% (recall), 61.7% (F1) for agreement detection and 69.2% (precision), 46.9% (recall), and 55.9% (F1) for disagreement detection, on the English broadcast conversation data.
agreement detection	recall	Overall, our approach achieves 79.2% (precision), 50.5% (recall), 61.7% (F1) for agreement detection and 69.2% (precision), 46.9% (recall), and 55.9% (F1) for disagreement detection, on the English broadcast conversation data.
agreement detection	F1	Overall, our approach achieves 79.2% (precision), 50.5% (recall), 61.7% (F1) for agreement detection and 69.2% (precision), 46.9% (recall), and 55.9% (F1) for disagreement detection, on the English broadcast conversation data.
agreement detection	F1	We noticed that both sampling approaches degraded slightly in precision but improved significantly in recall, resulting in 4.5% absolute gain on F1 for agreement detection and 4.7% absolute gain on F1 for disagreement detection.
agreement detection	precision	Overall, we achieved 79.2% (precision), 50.5% (recall), 61.7% (F1) for agreement detection and 69.2% (precision), 46.9% (recall), and 55.9% (F1) for disagreement detection, on English broadcast conversation data.
agreement detection	recall	Overall, we achieved 79.2% (precision), 50.5% (recall), 61.7% (F1) for agreement detection and 69.2% (precision), 46.9% (recall), and 55.9% (F1) for disagreement detection, on English broadcast conversation data.
agreement detection	F1	Overall, we achieved 79.2% (precision), 50.5% (recall), 61.7% (F1) for agreement detection and 69.2% (precision), 46.9% (recall), and 55.9% (F1) for disagreement detection, on English broadcast conversation data.
parsing	accuracy	These grammars have different behaviors in parsing efficiency and accuracy, but so far no detailed comparison between them has been done.
paraphrase alignment	TER-PLUS (Translation Edit Rate Plus))	We present our experiments and discuss their results in section 4 and conclude in section 5. 2 Edit rate for paraphrase alignment TER-PLUS (Translation Edit Rate Plus)) is a score designed for evaluation of Machine Translation (MT) output.
MT	F1	 Table 1: MT results with individual mixture component  (F1 to F8), baseline, or mixture model.
MT	accuracy degradation	First, we claim that domain mismatch is not caused by MT errors, and accuracy degradation will occur even in the case of perfect MT.
patent translation tasks	BLEU	Experiment results for patent translation tasks show a significant improvement of 1.20% BLEU points in Japanese-English translation and 1.41% BLEU points in English-Japanese translation.
patent translation tasks	BLEU	Experiment results for patent translation tasks show a significant improvement of 1.20% BLEU points in Japanese-English translation and 1.41% BLEU points in English-Japanese translation.
NTCIR-8 patent translation tasks	BLEU	Experiment results for the NTCIR-8 patent translation tasks show a significant improvement of 1.20% BLEU points in Japanese-English translation and 1.41% BLEU points in English-Japanese translation.
NTCIR-8 patent translation tasks	BLEU	Experiment results for the NTCIR-8 patent translation tasks show a significant improvement of 1.20% BLEU points in Japanese-English translation and 1.41% BLEU points in English-Japanese translation.
SMT	LM probability	The SMT decoder uses a log-linear model that combines numerous features, including but not limited to phrase translation probability, LM probability, and distortion penalty, to estimate the posterior probability of target hypotheses.
SMT	distortion penalty	The SMT decoder uses a log-linear model that combines numerous features, including but not limited to phrase translation probability, LM probability, and distortion penalty, to estimate the posterior probability of target hypotheses.
SMT	BLEU	Finally, we evaluated SMT performance on the test set in terms of BLEU and TER ().
SMT	TER	Finally, we evaluated SMT performance on the test set in terms of BLEU and TER ().
MA	accuracy	In order to test the effectiveness of pointwise MA, we did an experiment measuring accuracy both on in-domain data, and in a domain-adaptation situation.
word segmentation	F-score	For word segmentation, this is typically done by computing the F-score, where F = (2 * Precision * Recall)/(Precision + Recall), for both boundaries (BF) and words (WF) found by the algorithm.
word segmentation	F	For word segmentation, this is typically done by computing the F-score, where F = (2 * Precision * Recall)/(Precision + Recall), for both boundaries (BF) and words (WF) found by the algorithm.
ASR recognition	accuracy	Combined with scores derived from the ASR recognition probability and the dialogue history, the proposed approach achieves 84.3% detection accuracy, an absolute improvement of 34.7% over the base-line of the semantic slot-based method with 49.6% detection accuracy.
dialogue management (DM)	A	The dialogue management (DM) module determines the user's dialogue act A * t and accordingly decides the current act of the system.
prediction of verbal feedback	accuracy	This approach improves the prediction of verbal feedback , up to 6-fold, while maintaining a high overall accuracy.
REG	Dice	For the sake of comparison, we also follow the evaluation methodology of the REG challenges, training and testing on two domains (a furniture and a people domain), and using two automatic metrics (Dice and accuracy) to measure human-likeness.
parsing	accuracy	We see that in all cases, unary constraints improve the efficiency of parsing without significant accuracy loss.
parsing	accuracy	The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-the-art performance with respect to other parsing approaches evaluated on the same data set.
parse	accuracy	This parse information helps to improve parsing accuracy without hurting parsing complexity).
parsing	accuracy	This parse information helps to improve parsing accuracy without hurting parsing complexity).
parsing	accuracy	The bootstrapping technique gives a significant improvement to parsing accuracy.
parsing	accuracy	In all the experiments, the use of the morphological analyzer in producing the lattice was crucial for parsing accuracy.
parsing from unsegmented text	F-score	Results Our final configuration (marking verbal forms and subject-NPs, using the analyzer to construct the lattice and training the parser for 5 iterations) produces remarkable parsing accuracy when parsing from unsegmented text: an F-score of 79.9% (prec: 82.3 rec: 77.6) and seg+tagging F of 93.8%.
parsing Hebrew	F	For comparison, the previous best results for parsing Hebrew are 84.1%F assuming gold segmentation and tagging  The strengths of the system can be attributed to three factors: (1) performing segmentation, tagging and parsing jointly using lattice parsing, (2) relying on an external resource (lexicon / morphological analyzer) instead of on the Treebank to provide lexical coverage and (3) using a strong syntactic model.
conversion	accuracy	Experiments on two Chinese treebanks show that our approach improves conversion accuracy by 1.31% over a strong baseline.
conversion	accuracy	The results show that our approach achieves a 1.31% absolute improvement in conversion accuracy over the approach used in Zhu and Zhu (2010).
SDP	F1	In its pure form, with no pruning or approximation, SDP is neither fast nor accurate, achieving less than 70% F1 on the English WSJ task.
Machine translation	Bleu	Machine translation systems are often only evaluated quantitatively by using automatic metrics, such as Bleu (), which compares the system output to one or more human reference translations.
extracting latent topics from a document	accuracy	To examine our proposed method's performance on extracting latent topics from a document, we compare the accuracy of our method to that of the conventional methods through a common document retrieval task.
dependency parsing	Bleu score	This paper examines this noun phrase structure's effect on dependency parsing, in English, with a maximum spanning tree parser and shows a 2.43%, 0.23 Bleu score, improvement for English to Czech machine translation.
parsing	accuracy	One would expect parsing accuracy to be reduced when the complexity of the parse is increased, such as adding noun phrase structure.
MT evaluation	BLAST	Since we believe that error analysis is a vital complement to MT evaluation, we think that BLAST can be useful for many other MT researchers and developers.
Quantitative evaluations	Bleu	Quantitative evaluations can be automatic, using metrics such as Bleu () or Meteor (, where the MT output is compared to one or more human reference translations.
MT-related tasks	2	Graphical tools have also successfully been used to aid humans in other MT-related tasks, such as human MT evaluation of adequacy, fluency and 2 Though it does, at least in principle, seem possible to mine HTER annotations for more information system comparison, and word alignment ().
MT evaluation	2	Graphical tools have also successfully been used to aid humans in other MT-related tasks, such as human MT evaluation of adequacy, fluency and 2 Though it does, at least in principle, seem possible to mine HTER annotations for more information system comparison, and word alignment ().
SMT	BLEU	An extensive study across six parametric function families, empirically establishing that a certain three-parameter power-law family is well suited for modeling learning curves for the Moses SMT system when the evaluation score is BLEU.
segmentation task	F-score	For the segmentation task, our model outperforms the state-of-the-art unsupervised method and improves the relative F-score by 18.8 points ().
ASR	AES	The impact of the first problem can be reduced by either perfecting the results of the ASR system or building the AES system which is not sensitive to the ASR errors.
ASR	BOW	Ina word, the FST model cannot only be insensitive to the recognition error in the ASR system, but also remedy the weakness of BOW methods in ASR result scoring.
document dating	consistency	Most previous work on document dating evaluates on the news genre, so we maintain the pattern for consistency.
relation extraction	F-measure	When applied to a complex virtual world and text describing that world, our relation extraction technique performs on par with a supervised baseline, yielding an F-measure of 66% compared to the baseline's 65%.
relation extraction	accuracy	Our results demonstrate the strength of our relation extraction technique -while using planning feedback as its only source of supervision, it achieves a precondition relation extraction accuracy on par with that of a supervised SVM baseline.
segmentation	accuracy	For the segmentation evaluation, we report percharacter labeling accuracy.
tagging	accuracy	Nonetheless, tagging accuracy only decreases by 0.1%.
SMT	accuracy	An evaluation on four language pairs with differing morphological properties shows that for distant language pairs, character-based SMT can achieve translation accuracy comparable to word-based systems.
Translation	BLEU	For both tasks, we selected as training data all sentences for which both: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant difference from the best system according to the bootstrap resampling method at p = 0.05).
Translation	BLEU	For both tasks, we selected as training data all sentences for which both: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant difference from the best system according to the bootstrap resampling method at p = 0.05).
Translation	METEOR	For both tasks, we selected as training data all sentences for which both: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant difference from the best system according to the bootstrap resampling method at p = 0.05).
translation	accuracy	Interestingly, for translation into English, character-based translation achieves higher accuracy compared to word-based translation on Japanese and Finnish input, followed by German, fi-en ja-en ITG-word 2.851 2.085 ITG-char 2.826 2.154  and finally French.
Translation	BLEU	 Table 2: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal  ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant differ- ence from the best system according to the bootstrap resampling method at p = 0.05
Translation	BLEU	 Table 2: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal  ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant differ- ence from the best system according to the bootstrap resampling method at p = 0.05
Translation	METEOR	 Table 2: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal  ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant differ- ence from the best system according to the bootstrap resampling method at p = 0.05
speech recognition	WER reduction	When using these improved tools in a language model for speech recognition, we obtain significant speed improvements with both N-best and hill climbing rescoring, and show that up-training leads to WER reduction.
multiword recognition	accuracy	Firstly, we show that pre-grouping multiword expressions before parsing with a state-of-the-art recognizer improves multiword recognition accuracy and unlabeled attachment score.
multiword recognition	attachment score	Firstly, we show that pre-grouping multiword expressions before parsing with a state-of-the-art recognizer improves multiword recognition accuracy and unlabeled attachment score.
parsing	accuracy	Although experiments always relied on a corpus where the MWEs were perfectly pre-identified, they showed that pre-grouping such expressions could significantly improve parsing accuracy.
MWE identification	F 1 score	The quality of MWE identification was evaluated by computing the F 1 score on MWE nodes.
MWE segmentation	F 1 score (U)	We also evaluated the MWE segmentation by using the unlabeled F 1 score (U).
Rescoring	UAS	shows the UAS curves on the development set, where K is beam size for Intersect and K-best for Rescoring, the X-axis represents K, and the Y-axis represents the UAS scores.
parsing	K	The parsing performance generally increased as the K increased.
POS tagging	accuracy	We implement a discriminative sequential classification model for POS tagging which achieves the state-of-the-art accuracy.
word detection	ADF	 Table 2: Incremental evaluations, by incrementally adding new features (word features and high dimensional edge  features), new word detection, and ADF training (replacing SGD training with ADF training). Number of passes is  decided by empirical convergence of the training methods.
word detection	Number of passes	 Table 2: Incremental evaluations, by incrementally adding new features (word features and high dimensional edge  features), new word detection, and ADF training (replacing SGD training with ADF training). Number of passes is  decided by empirical convergence of the training methods.
PRF	CE	For PRF, we follow the implementation of Indri's PRF method and further apply the CE technique as described in Section 3.2.: Results on test set in MAP score.
translation	minimum error rate training (MERT)	proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric.
Translation consensus	Bayes risk (MBR	Translation consensus is employed in those minimum Bayes risk (MBR) approaches where the loss function of a translation is defined with respect to all other translation candidates.
word alignments generated	accuracy	We measured the accuracy of word alignments generated by GIZA++ with and without the 0 -norm, and also translation accuracy of systems trained using the word alignments.
word alignments generated	accuracy	We measured the accuracy of word alignments generated by GIZA++ with and without the 0 -norm, and also translation accuracy of systems trained using the word alignments.
slot detection	F1 pairwise measure	For domain and dialog act detection performance we present results inaccuracy, and for slot detection we use the F1 pairwise measure.
sentiment lexicon extraction	F 1 Prec	 Table 2: Results on sentiment lexicon extraction. Num- bers in boldface denote significant improvement.  product vs. movie  movie vs. product  Prec. Rec. F 1 Prec. Rec.
Sentiment classification	accuracy	 Table 5: Sentiment classification results (accuracy in %).  Numbers in boldface denotes significant improvement.
parsing	accuracy	 Table 2: Comparison of parsing accuracy with the  small and full training sets. *Our reimplementation  of (Cohn et al., 2010).
translation	BLEU-4	The translation quality is evaluated by case-insensitive BLEU-4 metric ().
parsing	accuracy	We also present a comprehensive analysis of errors made by the C&C CCG parser, providing the first breakdown of the impact of implementation decisions, such as supertagging, on parsing accuracy.
segment refinement identification	accuracy	 Table 1: Unsupervised experiments comparing models for review and segment refinement identification on the recipe  data. Bold indicates the best result, and a  † next to an accuracy or F 1 value indicates that the improvements obtained  by RS-MixMix, RS-MixHMM, and RSA-MixHMM are significant (p = 0.05 according to a bootstrap test).
segment refinement identification	F 1 value	 Table 1: Unsupervised experiments comparing models for review and segment refinement identification on the recipe  data. Bold indicates the best result, and a  † next to an accuracy or F 1 value indicates that the improvements obtained  by RS-MixMix, RS-MixHMM, and RSA-MixHMM are significant (p = 0.05 according to a bootstrap test).
summarization	precision	When taking the summarization as a sequential biclassification problem, we can make use of the usual precision, recall and F1 measures for classification accuracy evaluation.
summarization	recall	When taking the summarization as a sequential biclassification problem, we can make use of the usual precision, recall and F1 measures for classification accuracy evaluation.
summarization	F1	When taking the summarization as a sequential biclassification problem, we can make use of the usual precision, recall and F1 measures for classification accuracy evaluation.
parsing	accuracy	Our approach can significantly advance the state-of-the-art parsing accuracy on two widely used target tree-banks (Penn Chinese Treebank 5.1 and 6.0) using the Chinese Dependency Treebank as the source treebank.
parsing	accuracy	At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.
parsing	accuracy	1 * Correspondence author: tliu@ir.hit.edu.cn Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy ( Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (), and bilingual texts or cross-lingual treebanks).
parsing	accuracy	Therefore, exploiting multiple treebanks is very attractive for boosting parsing accuracy.
parsing	accuracy	Results show that our approach can significantly boost state-of-the-art parsing accuracy.
SME	accuracy	In the first experiment, we compare the prediction accuracy of our SME model to a widely used discriminative learner in NLP -the linear kernel support vector machine (SVM) 3 . In the second experiment, in addition to the linear kernel SVM, we also compare our SME model to a state-of-the-art sparse generative model of text, and vary the size of input vocabulary W exponentially from 2 9 to the full size of our training vocabulary . In the third experiment, we examine the robustness of our model by examining how the number of topics influences the prediction accuracy when varying the K from 10 to 50.
SME	accuracy	In the first experiment, we compare the prediction accuracy of our SME model to a widely used discriminative learner in NLP -the linear kernel support vector machine (SVM) 3 . In the second experiment, in addition to the linear kernel SVM, we also compare our SME model to a state-of-the-art sparse generative model of text, and vary the size of input vocabulary W exponentially from 2 9 to the full size of our training vocabulary . In the third experiment, we examine the robustness of our model by examining how the number of topics influences the prediction accuracy when varying the K from 10 to 50.
translation	BLEU	Is our topic similarity model able to improve translation quality in terms of BLEU?
Attacking Parsing Bottlenecks	Relevant Factorizations	Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations
parsing	accuracy	In this section, we investigate the impact of unlabeled data on parsing accuracy using the two conversions and using each of the factorizations described in Section 3.1-3.4.
Event extraction	F1 percentage scores	 Table 3: Event extraction performance with automatic mention  identifier and typer. We report F1 percentage scores for pref- erence modeling (PM) as well as two baseline approaches. We  also report performance of the supervised approach trained with  the semi-CRF model for comparison.
Seminar extraction	Field-level F1	 Table 2: Seminar extraction results (5-fold CV, trained on 50% of corpus): Field-level F1
WMT shared tasks	METEOR	In the WMT shared tasks, many new generation metrics, such as METEOR (), TER), and TESLA ( ) have consistently outperformed BLEU as judged by the correlations with human judgments.
WMT shared tasks	TER	In the WMT shared tasks, many new generation metrics, such as METEOR (), TER), and TESLA ( ) have consistently outperformed BLEU as judged by the correlations with human judgments.
WMT shared tasks	TESLA	In the WMT shared tasks, many new generation metrics, such as METEOR (), TER), and TESLA ( ) have consistently outperformed BLEU as judged by the correlations with human judgments.
WMT shared tasks	BLEU	In the WMT shared tasks, many new generation metrics, such as METEOR (), TER), and TESLA ( ) have consistently outperformed BLEU as judged by the correlations with human judgments.
machine translation (MT) evaluation	BLEU	Many machine translation (MT) evaluation metrics have been shown to correlate better with human judgment than BLEU.
MT evaluation	precision	This paper presents PORT 1 , anew MT evaluation metric which combines precision, recall and an ordering metric and which is primarily designed for tuning MT systems.
MT evaluation	recall	This paper presents PORT 1 , anew MT evaluation metric which combines precision, recall and an ordering metric and which is primarily designed for tuning MT systems.
MT evaluation	BLEU	MT evaluation metrics fall into three groups: • BLEU (), NIST), WER, PER, TER), and LRscore (Birch and Osborne, 2011) do not use external linguistic information; they are fast to compute (except TER).
MT evaluation	WER	MT evaluation metrics fall into three groups: • BLEU (), NIST), WER, PER, TER), and LRscore (Birch and Osborne, 2011) do not use external linguistic information; they are fast to compute (except TER).
MT evaluation	PER	MT evaluation metrics fall into three groups: • BLEU (), NIST), WER, PER, TER), and LRscore (Birch and Osborne, 2011) do not use external linguistic information; they are fast to compute (except TER).
MT evaluation	LRscore	MT evaluation metrics fall into three groups: • BLEU (), NIST), WER, PER, TER), and LRscore (Birch and Osborne, 2011) do not use external linguistic information; they are fast to compute (except TER).
MT	BLEU	In this work, our goal is to devise a metric that, like BLEU, is computationally cheap and language-independent, but that yields better MT systems than BLEU when used for tuning.
Tuning	MERT	Tuning was done with n-best MERT, which is available in MOSES.
summarizer	AE- SOP metrics	 Table 1: Content correlation with human judgment  on summarizer level. Top three scores among AE- SOP metrics are underlined. The TESLA-S score is  bolded when it outperforms all others. ROUGE-2 is  shortened to R-2 and ROUGE-SU4 to R-SU4.
summarizer	TESLA-S score	 Table 1: Content correlation with human judgment  on summarizer level. Top three scores among AE- SOP metrics are underlined. The TESLA-S score is  bolded when it outperforms all others. ROUGE-2 is  shortened to R-2 and ROUGE-SU4 to R-SU4.
summarizer	ROUGE-2	 Table 1: Content correlation with human judgment  on summarizer level. Top three scores among AE- SOP metrics are underlined. The TESLA-S score is  bolded when it outperforms all others. ROUGE-2 is  shortened to R-2 and ROUGE-SU4 to R-SU4.
summarizer	ROUGE-SU4	 Table 1: Content correlation with human judgment  on summarizer level. Top three scores among AE- SOP metrics are underlined. The TESLA-S score is  bolded when it outperforms all others. ROUGE-2 is  shortened to R-2 and ROUGE-SU4 to R-SU4.
POS tagging	accuracy	With the characteristics of a language) and informative features for POS tagging), the state-of-the-art supervised POS tagging achieves over 97% of accuracy.
POS tagging	accuracy	With the characteristics of a language) and informative features for POS tagging), the state-of-the-art supervised POS tagging achieves over 97% of accuracy.
word segmentation	accuracies	The joint approach to word segmentation and POS tagging has been reported to improve word segmentation and POS tagging accuracies by more than 1% in Chinese (.
POS tagging	accuracies	The joint approach to word segmentation and POS tagging has been reported to improve word segmentation and POS tagging accuracies by more than 1% in Chinese (.
POS tagging	accuracies	The joint approach to word segmentation and POS tagging has been reported to improve word segmentation and POS tagging accuracies by more than 1% in Chinese (.
Tagging	accuracy	Tagging accuracy is moderately improved as well.
segmentation	F-measure	By adopting this ILP formulation, segmentation F-measure is increased from 0.968 to 0.974, as compared to Viterbi decoding with the same feature set.
translation	speed	We compare our approach with Moses and observe the same performance , but a substantially better trade-off between translation quality and speed.
phrase translation candidates	speed	We show that taking a heuristic LM score estimate for pre-sorting the phrase translation candidates has a positive effect on both translation quality and speed.
translation	BLEU	lists the translation performance with BLEU scores.
machine translation	MBR (Minimum Bayes Risk)	To reduce the noise introduced by machine translation,  propose combining the results of multiple machine translation engines' by performing MBR (Minimum Bayes Risk) () decoding on the N-best translation candidates.
SMT	BLEU score	The jointly-learned dual SMT system: (1) Adapts the SMT systems so that they are tuned specifically for paraphrase generation purposes, e.g., to increase the dissimilarity; (2) Employs a revised BLEU score (named iBLEU , as it's an input-aware BLEU metric) that measures adequacy and dissimilarity of the paraphrase results at the same time.
SMT	BLEU score	The jointly-learned dual SMT system: (1) Adapts the SMT systems so that they are tuned specifically for paraphrase generation purposes, e.g., to increase the dissimilarity; (2) Employs a revised BLEU score (named iBLEU , as it's an input-aware BLEU metric) that measures adequacy and dissimilarity of the paraphrase results at the same time.
translation	BLEU	Incorporating these features into our hierarchical phrase-based translation system significantly improved translation performance, by up to 1 BLEU and 3 TER over a strong Chinese to English baseline.
translation	TER	Incorporating these features into our hierarchical phrase-based translation system significantly improved translation performance, by up to 1 BLEU and 3 TER over a strong Chinese to English baseline.
alignment	accuracy	shows the alignment accuracy results.
SNA-based	degree centrality)	We call this a lower bound for SNA-based systems because we are only using a single simple metric (degree centrality) to establish dominance.
POS tagging	accuracy	When used in POS tagging, this process improves the accuracy from 73.24% to 86.84% on unseen CEA text, and reduces the percentage of out-of-vocabulary words from 28.98% to 16.66%.
POS tagging	Standard	We ran five experiments to test the effect of MSA to CEA conversion on POS tagging: (a) Standard, where we train the tagger on the ATB MSA data, (b) 3-gram LM, where for each MSA sentence we generate all transformed sentences (see Section 2.1 and) and pick the most probable sentence according to a trigram language model built from an 11.5 million words of user contributed comments.
tagging	accuracy	We also notice that the conversion alone improves tagging accuracy from 75.77% to 79.25% on the development set, and from 73.24% to 79.67% on the test set.
word segmentation	F-score	It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (.
Translation	BLEU-SBP	Translation evaluation of WordSys and proposed system using BLEU-SBP ( 4 compares WordSys to our proposed system.
word lattice decoding	recall	BS underperformed both two baselines, one reason is that Ma and Way (2009) also employed word lattice decoding techniques ( to tackle the low recall of BS, which was removed from our experiments for fair comparison.
translation performance evaluation	BLEU-4	Our main metric for translation performance evaluation is case- insensitive BLEU-4 scores ().
translation	consistency	The results show that the translation performance is highly co-related with the consistency.
phrase reduction task	BLEU score	For the phrase reduction task, the dependency method outperforms the non-dependency method in terms of BLEU score.
phrase reduction task	BLEU score	When the cross reduction technique was used for the phrase reduction task, BLEU score is not deteriorated even when more than half of phrase entries are pruned.
translation	accuracy	We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.
parsing	accuracy	We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.
parsing	accuracy	 Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases  parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.  (c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing  accuracy, but only slightly.
parsing	accuracy	 Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases  parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.  (c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing  accuracy, but only slightly.
parsing	accuracy	 Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases  parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.  (c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing  accuracy, but only slightly.
parsing	accuracy	 Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases  parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.  (c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing  accuracy, but only slightly.
parsing	accuracy	 Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases  parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.  (c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing  accuracy, but only slightly.
parsing	accuracy	 Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases  parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.  (c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing  accuracy, but only slightly.
translation	accuracy	We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.
parsing	accuracy	We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.
parsing	accuracy	 Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases  parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.  (c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing  accuracy, but only slightly.
parsing	accuracy	 Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases  parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.  (c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing  accuracy, but only slightly.
parsing	accuracy	 Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases  parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.  (c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing  accuracy, but only slightly.
parsing	accuracy	 Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases  parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.  (c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing  accuracy, but only slightly.
parsing	accuracy	 Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases  parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.  (c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing  accuracy, but only slightly.
parsing	accuracy	 Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases  parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.  (c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing  accuracy, but only slightly.
MT	BLEU score	We evaluate MT with BLEU score.
parsing-based	BLEU score	We see that the parsing-based method achieves the best BLEU score.
translation spotting	precision-recall	The performances of the translation spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively.
translation spotting	coverage rate	The performances of the translation spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively.
translation spotting	Recall	We evaluate the translation spotting in terms of the Recall and Precision metrics defined as follows: shows the evaluation of translation spotting for normalized correlation, NC, compared with the intersection and union of GIZA++ word alignment.
translation spotting	Precision	We evaluate the translation spotting in terms of the Recall and Precision metrics defined as follows: shows the evaluation of translation spotting for normalized correlation, NC, compared with the intersection and union of GIZA++ word alignment.
chunking task	F-score	The gap to the best performing solution for the chunking task is about 1.3% points in F-score, ahead of the baseline by 15.7% points.
SMT	BLEU	Compared with the pure SMT system, the proposed integrated Model-III achieves 3.48 BLEU points improvement and 2.62 TER points reduction overall.
SMT	TER	Compared with the pure SMT system, the proposed integrated Model-III achieves 3.48 BLEU points improvement and 2.62 TER points reduction overall.
translation	BLEU-4 score	In this work, the translation performance is measured with case-insensitive BLEU-4 score () and TER score).
translation	TER score	In this work, the translation performance is measured with case-insensitive BLEU-4 score () and TER score).
Translation	BLEU	 Table 3: Translation Results (BLEU%). Scores marked by "*" are significantly better (p < 0.05) than both TM  and SMT systems, and those marked by "#" are significantly better (p < 0.05) than Koehn-10.
Translation	TER	 Table 4: Translation Results (TER%). Scores marked by "*" are significantly better (p < 0.05) than both TM and  SMT systems, and those marked by "#" are significantly better (p < 0.05) than
machine translation quality estimation	accuracy	Our experiments on two machine translation quality estimation datasets show uniform significant accuracy gains from multi-task learning, and consistently out-perform strong baselines.
TempEval-2 temporal normalization task	accuracy	We achieve state-of-the-art accuracy on all languages in the TempEval-2 temporal normalization task, reporting a 4% improvement in both English and Spanish accuracy, and to our knowledge the first results for four other languages.
parsing	accuracy	Our experiments show that these advantages lead to significant improvements in parsing accuracy, compared to a baseline parser that uses the arc-eager transition system of, which is one of the most widely used static transition systems.
word alignment task	F-score	Experiments on a large scale English-Chinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.
text recognition	accuracy	We evaluate our system by comparing our text recognition accuracy to that of two state-of-the-art systems.
sentiment analysis	accuracy	In the case of sentiment analysis, while people are able to achieve 87.5% accuracy) on a movie review dataset (), the performance drops to 75% ( ) on a sentence level movie review dataset).
parsing	accuracy	Our results clearly demonstrate that for all three formalisms, parsing accuracy can be improved by training with additional coarse annotations.
parsing	accuracy	As shows, CFG data boosts parsing accuracy for all the target formalisms.
Machine translation	BLEU	 Table 3: Machine translation performance in  BLEU % on the IWSLT 2005 Chinese-English test  set. The Gibbs samplers were initialized with three  different alignments, shown as columns.
Translation	BLEU	 Table 4: Translation performance on Chinese to  English translation, showing BLEU% for models  trained on the FBIS data set.
translation	BLEU score	To evaluate translation quality, we use BLEU score (), a standard evaluation measure used in machine translation.
MT tasks	BLEU	For both the MT tasks, we also report BLEU scores fora baseline system using identity translations for common words (words appearing in both source/target vocabularies) and random translations for other words.
MT	BLEU	 Table 2: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours)  on the Spanish/English OPUS corpus using only non-parallel corpora for training. For the Bayesian  methods 4a and 4b, the samplers were run for 1000 iterations each on a single machine (1.8GHz Intel  processor). For 1a, 2a, 2b, we list the training times as reported by Nuhn et al. (2012) based on their EM  implementation for different settings.
classication	accuracy	We show that our automatic classication system is highly accurate, achieving 87.4% accuracy on a held-out test set.
Cross Lingual Sentiment Analysis (CLSA)	accuracy	Similar idea is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA.
parsing	accuracy	This allows different composition functions when combining different types of phrases and is shown to result in a large improvement in parsing accuracy.
parsing	precision	To evaluate the parsing performance, we use the standard unlabeled (i.e., hierarchical spans) and labeled (i.e., nuclearity and relation) precision, recall and F-score as described in.
parsing	recall	To evaluate the parsing performance, we use the standard unlabeled (i.e., hierarchical spans) and labeled (i.e., nuclearity and relation) precision, recall and F-score as described in.
parsing	F-score	To evaluate the parsing performance, we use the standard unlabeled (i.e., hierarchical spans) and labeled (i.e., nuclearity and relation) precision, recall and F-score as described in.
IR	Significance	 Table 4: IR results using filtered catenae consistently improve over non-linguistic methods.  Significance(p < .05) shown compared to QL ( †) and SD ( ‡).
type annotation	accuracy	With just four hours of type annotation, our system obtains good accuracy across the three languages: 89.8% on English, 81.9% on Kinyarwanda, and 81.2% on Malagasy.
tagger	accuracy	To better understand the effect that each type of supervision has on tagger accuracy, we perform a series of experiments, with KIN and MLG as true low-resource languages.
MT	accuracy	Traditional MT approaches focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content words including critical information, especially names.
MT evaluation	BLEU	Traditional MT evaluation metrics such as BLEU () and Translation Edit Rate (TER)) assign the same weights to all tokens equally.
MT evaluation	Translation Edit Rate (TER))	Traditional MT evaluation metrics such as BLEU () and Translation Edit Rate (TER)) assign the same weights to all tokens equally.
NE translation	accuracy	Knowing document similarity improves NE translation, and improved NE translation can boost the accuracy of document and relationship similarity.
NE translation	accuracy	Knowing document similarity improves NE translation, and improved NE translation can boost the accuracy of document and relationship similarity.
CWS evaluation	recall (OOVR)	For CWS evaluation, we employ the conventional scoring script provided in SIGHAN-5, which also provides out-of-vocabulary recall (OOVR).
segmentation	ErrDec	 Table 5: The statistics of segmentation error for  named entities (NE) and Chinese numbers (CN)  in test data. #baErr and #gbErr denote the count  of segmentations by Baseline II and our model;  ErrDec% denotes the error reduction.
Prediction	accuracy	 Table 5: Prediction accuracy with system- generated parse trees.
Translation	EN-DE	 Table 4: Translation experiments EN-DE. BLEU scores reported.
Translation	BLEU	 Table 4: Translation experiments EN-DE. BLEU scores reported.
Translation	BLEU	 Table 6: Translation experiments CZ-EN. BLEU scores reported.
Tagging	Accuracy	 Table 3: Tagging and Dependency Accuracy (%)
question retrieval	Mean Average Precision (MAP)	Evaluation Metrics: We evaluate the performance of question retrieval using the following metrics: Mean Average Precision (MAP) and.
SP task	recall	We therefore propose to measure both aspects of the SP task by computing both the recall and the precision between the list of possible arguments a verb can take according to the model and the corresponding test corpus list . We evaluate the value of our clustering for SP acquisition in the particularly challenging scenario of domain adaptation.
SP task	precision	We therefore propose to measure both aspects of the SP task by computing both the recall and the precision between the list of possible arguments a verb can take according to the model and the corresponding test corpus list . We evaluate the value of our clustering for SP acquisition in the particularly challenging scenario of domain adaptation.
Verb clustering	F-score	 Table 1: Verb clustering evaluation for the last five iterations of our DPP-cluster model and the baseline  agglomerative clustering algorithm (AC, see text for its description), and for the spectral clustering (SC)  algorithm of (Sun and Korhonen, 2009) with the same number of clusters induced by DPP-cluster. |C| is  the number of clusters for DPP-cluster and SC (first number) and for AC (second number). The F-score  performance of DPP-cluster is superior in 4 out of 5 cases.
WSD	accuracy	The method achieved successful WSD accuracy.
semantic parsing	accuracy	We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries.
multimodal sentiment analysis	error rate	Using anew multimodal dataset consisting of sentiment annotated utterances extracted from video reviews, we show that multimodal sentiment analysis can be effectively performed, and that the joint use of visual, acoustic, and linguistic modalities can lead to error rate reductions of up to 10.5% as compared to the best performing individual modality.
SO prediction task	similarity function (A)	We evaluate the performance of our PSSS models in the SO prediction task using the algorithm explained in by setting our PSSS as similarity function (A).
summarization task	ROUGE score	For each summarization task, we compare the system output (i.e., summaries automatically produced by the algorithm) against the humangenerated summaries and evaluate the performance in terms of ROUGE score), a standard recall-based evaluation measure used in summarization.
summarization task	recall-based	For each summarization task, we compare the system output (i.e., summaries automatically produced by the algorithm) against the humangenerated summaries and evaluate the performance in terms of ROUGE score), a standard recall-based evaluation measure used in summarization.
summarization task	ROUGE-1 5 scores	We use the following evaluation settings in our experiments for each summarization task: (1) For multi-document summarization, we compute the ROUGE-1 5 scores that was the main evaluation criterion for DUC 2004 evaluations.
summaries	precision	To evaluate the summaries, we followed the practices of the TAC summarization tasks and NTCIR ACLIA tasks, and computed pyramid-based precision with the allowance parameter, recall, and F β (where β is 1 or 3) scores.
summaries	allowance	To evaluate the summaries, we followed the practices of the TAC summarization tasks and NTCIR ACLIA tasks, and computed pyramid-based precision with the allowance parameter, recall, and F β (where β is 1 or 3) scores.
summaries	recall	To evaluate the summaries, we followed the practices of the TAC summarization tasks and NTCIR ACLIA tasks, and computed pyramid-based precision with the allowance parameter, recall, and F β (where β is 1 or 3) scores.
summaries	F β	To evaluate the summaries, we followed the practices of the TAC summarization tasks and NTCIR ACLIA tasks, and computed pyramid-based precision with the allowance parameter, recall, and F β (where β is 1 or 3) scores.
parsing	accuracy	Finally, we evaluated the parsing accuracy.
parsing	accuracy	shows the parsing accuracy on the development and the test sets.
parsing	accuracy	Coupled with dynamic programming, transition-based dependency parsing with beam search can be done very efficiently and gives significant improvement to parsing accuracy.
parsing	accuracy	With our new approach, we achieve a higher parsing accuracy than the current state-of-the-art transition-based parser that uses beam search and a much faster speed.
POS tagging	accuracy	Bohnet and Nivre (2012)'s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours.
dependency parsing	accuracy	Bohnet and Nivre (2012)'s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours.
parsing	accuracy	As the evaluation metric, we use parsing accuracy which is the percentage of the words which have found their correct parents.
NER	entity precision (P)	We report standard NER measures (entity precision (P), recall (R) and F 1 score) on the test set.
NER	recall (R)	We report standard NER measures (entity precision (P), recall (R) and F 1 score) on the test set.
NER	F 1 score	We report standard NER measures (entity precision (P), recall (R) and F 1 score) on the test set.
NER	NC	 Table 2: Joint alignment and NER test results. +NC means incorporating additional neighbor constraints  from DeNero and Macherey (2011) to the model. Best number in each column is highlighted in bold.
RM	BLEU	Interestingly, RM achieved substantially higher BLEU precision scores in all tests for both language pairs.
translation	BLEU-4	The translation quality is evaluated by case-insensitive BLEU-4 with shortest length penalty.
argument identification	accuracy	We use the F 1 measure as a metric for the argument identification stage and accuracy as an aggregate measure of argument classification performance.
Argument identification	F	 Table 3: Argument identification, transferred  model vs. projection baseline, F 1 .
Argument classification	accuracy	 Table 5: Argument classification, transferred  model vs. projection baseline, accuracy.
classification	accuracy	In column 2, we report the classification accuracy on a subset of training data.
word alignments	BLEU	The data generated allows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and again of 5.2 BLEU points over a standard phrase-based baseline.
machine translation	BLEU	Additionally, we evaluate the effect of reordering on our final systems for machine translation measured using BLEU.
VSM adaptation	Bhattacharyya similarity	They all enabled VSM adaptation to beat the non-adaptive baseline, but Bhattacharyya similarity worked best, so we adopted it for the remaining experiments.
Speaker identification	accuracy	 Table 5: Speaker identification accuracy (in %) on  Pride & Prejudice, Emma, and The Steppe.
Speaker identification	accuracy	 Table 6: Speaker identification accuracy (in %) on  Austen's Emma by the type of utterance.
SemEval-2012 Semantic Textual Similarity task	Pear- son correlation r	 Table 2: Performance of our system (ADW) and the 3 top-ranking participating systems (out of 88) in  the SemEval-2012 Semantic Textual Similarity task. Rightmost columns report the corresponding Pear- son correlation r for individual datasets, i.e., MSRpar (Mpar), MSRvid (Mvid), SMTeuroparl (SMTe),  OnWN (OnWN) and SMTnews (SMTn). We also provide scores according to the three official evalua- tion metrics (i.e., ALL, ALLnrm, and Mean). Rankings are also presented based on the three metrics.
SemEval-2012 Semantic Textual Similarity task	Mean	 Table 2: Performance of our system (ADW) and the 3 top-ranking participating systems (out of 88) in  the SemEval-2012 Semantic Textual Similarity task. Rightmost columns report the corresponding Pear- son correlation r for individual datasets, i.e., MSRpar (Mpar), MSRvid (Mvid), SMTeuroparl (SMTe),  OnWN (OnWN) and SMTnews (SMTn). We also provide scores according to the three official evalua- tion metrics (i.e., ALL, ALLnrm, and Mean). Rankings are also presented based on the three metrics.
Sentence compression	c rate	 Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
Domain adaptation evaluation	OUT	 Table 3: Domain adaptation evaluation. Systems trained on out-of-domain data are denoted with "(OUT)", oth-
translation	BLEU	When it is enhanced with the in-domain language model, it can further improve the translation performance by more than 2.5 BLEU points.
phrase pair induction	BLEU	When using the first 100k sentences for phrase pair induction, it obtains a significant improvement over the BestConfig by 0.65 BLEU points and can outperform the transductive learning method.
sentence generation	BRAIN-SUP	The objective of the evaluation is twofold: we wanted to demonstrate 1) the effectiveness of our approach for creative sentence generation, in general, and 2) the potential of BRAIN-SUP to support the brainstorming process behind slogan generation.
relation extraction	accuracy	Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy; * The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper.).
text normalization	precision	Third, text normalization as a preprocessing step should have very high precision; in other words, it should provide conservative and confident normalization and not overcorrect.
text normalization	recall	Moreover, the text normalization should have high recall, as well, to have a good impact on the NLP applications.
SMT	accuracy	Third, our method computes features using both human-generated text and SMT results to capture a phrase salad by contrasting these features, which significantly improves detection accuracy.
MT detection	accuracy	We evaluate the performance of MT detection based on accuracy 6 that is a broadly used evaluation metric for classification problems: where n T P and n TN are the numbers of truepositives and true-negatives, respectively, and n is the total number of exemplars.
joint inference	GLPK	For joint inference, we used GLPK 9 to provide the optimal ILP solution.
dialog management	accuracy	We evaluate the dialog tree construction and dialog management components in isolation, demonstrating high accuracy (in the 80-90% range).
segmentation evaluation	boundary similarity (B)	This work proposes anew segmentation evaluation metric, named boundary similarity (B), an inter-coder agreement coefficient adaptation, and a confusion-matrix for segmentation that are all based upon an adaptation of the boundary edit distance in Fournier and Inkpen (2012).
segmentation as a classification problem	precision	A confusion matrix to interpret segmentation as a classification problem is also proposed, allowing for the computation of information retrieval (IR) metrics such as precision and recall.
segmentation as a classification problem	recall	A confusion matrix to interpret segmentation as a classification problem is also proposed, allowing for the computation of information retrieval (IR) metrics such as precision and recall.
extraction	accuracy	This could potentially improve the extraction accuracy.
phrasal Machine Translation (MT)	BLEU	-We built a phrasal Machine Translation (MT) system on adapted Egyptian/English parallel data, which outperformed a non-adapted baseline by 1.87 BLEU points.
Phrase merging	BLEU	Phrase merging that preferred phrases learnt from EG data over AR data performed the best with a BLEU score of 16.96.
PRO	BLEU	The best of these fixes not only slay and protect against monsters, but also yield higher stability for PRO as well as improved test-time BLEU scores.
reconstruction sampling	reconstruction rate	For reconstruction sampling, we set the reconstruction rate to 0.01.
reconstruction	finetuning	Thanks to reconstruction sampling and refined mini-batch arrangement, it takes about 1 day to converge for pre-training and 3 days for finetuning, which is fast given our training set size.
tagging	accuracy	Our analysis shows that improvements in tagging accuracy can only address a subset of the challenges of Chinese syntax.
Tagging	accuracy	 Table 2: Tagging accuracy vs beam width vs. Speed is  evaluated using the number of sentences that can be  processed in one second
Tagging	Speed	 Table 2: Tagging accuracy vs beam width vs. Speed is  evaluated using the number of sentences that can be  processed in one second
Text Analysis Conference	accuracy	Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy , precision and recall in finding significantly different systems.
Text Analysis Conference	precision	Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy , precision and recall in finding significantly different systems.
Text Analysis Conference	recall	Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy , precision and recall in finding significantly different systems.
Document retrieval	Mean Average Precision (MAP)	Document retrieval (for relevant docs from corpus), measured by Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR)..
Document retrieval	Mean Reciprocal Rank (MRR).	Document retrieval (for relevant docs from corpus), measured by Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR)..
IR	coverage	 Table 2. The test sentences were ob- tained with answer-bearing queries. This is as- suming almost perfect IR. The gap between the  top two and other lines signals more room for im- provements for IR in terms of better coverage and  better rank for answer-bearing sentences.
WS	accuracy	The results show that we achieved a significant improvement in WS accuracy in both languages.
segmentation	LE	To further improve the segmentation performance, using latent topic distributions and LE instead of term frequencies to represent text blocks is studied in this paper.
prediction	accuracy	We did so in order to study the effect of training data size on the prediction accuracy.
translation	accuracy	Our key contribution is building an automatic classifier to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 6.1%.
classification	accuracy	The results show that, with more source side documents, our method achieved the highest classification accuracy.
classification	accuracy	From the table, classification accuracy of all methods significantly outperformed random classifier (accuracy=0.2).
classification	accuracy	From the table, classification accuracy of all methods significantly outperformed random classifier (accuracy=0.2).
classification	accuracy	This result implies not only the effectiveness of the latent topic matching but also increasing the number of source side documents (labeled training data) contributes to improving classification accuracy.
predicting relation pair equivalency	precision	Early experiments have shown encouraging results with an average of 0.75~0.87 precision in predicting relation pair equivalency and 0.78~0.98 precision in relation clustering.
translation	BLEU	Experimental results show that our approach significantly improves translation performance by up to 1.3 BLEU points over 1-best alignments ( § 4.3).
Stem-granularity translation	OOV rate	Stem-granularity translation rules have much larger coverage and can lower the OOV rate.
RST-based	BLEU	Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively.
SMT	BLEU	We compare the performance of our system with that of two baseline SMT systems tuned against BLEU and TER, the commonly used n-gram and edit distance based metrics.
SMT	TER	We compare the performance of our system with that of two baseline SMT systems tuned against BLEU and TER, the commonly used n-gram and edit distance based metrics.
MT	BLEU	Glaring errors caused by semantic role confusion that plague the state-of-the-art MT systems area consequence of using fast and cheap lexical n-gram based objective functions like BLEU to drive their development.
Translation	MEANT	 Table 1: Translation quality of MT system tuned against MEANT, BLEU and TER on newswire data
Translation	BLEU	 Table 1: Translation quality of MT system tuned against MEANT, BLEU and TER on newswire data
Translation	TER	 Table 1: Translation quality of MT system tuned against MEANT, BLEU and TER on newswire data
MT	MEANT	 Table 1: Translation quality of MT system tuned against MEANT, BLEU and TER on newswire data
MT	BLEU	 Table 1: Translation quality of MT system tuned against MEANT, BLEU and TER on newswire data
MT	TER	 Table 1: Translation quality of MT system tuned against MEANT, BLEU and TER on newswire data
Translation	MEANT	 Table 2: Translation quality of MT system tuned against MEANT, BLEU and TER on forum data
Translation	BLEU	 Table 2: Translation quality of MT system tuned against MEANT, BLEU and TER on forum data
Translation	TER	 Table 2: Translation quality of MT system tuned against MEANT, BLEU and TER on forum data
MT	MEANT	 Table 2: Translation quality of MT system tuned against MEANT, BLEU and TER on forum data
MT	BLEU	 Table 2: Translation quality of MT system tuned against MEANT, BLEU and TER on forum data
MT	TER	 Table 2: Translation quality of MT system tuned against MEANT, BLEU and TER on forum data
translation	BLEU score	The translation quality is measured by BLEU score).
Question classification	precision	 Table 1: Question classification precision for both  levels of the hierarchy (features = word n-grams,  classifier = libsvm)
Question classification	precision	 Table 2: Question classification precision for both  levels of the hierarchy (features = word n-grams  with abbreviations, classifier = libsvm)
Question classification	precision	 Table 3: Question classification precision for both  levels of the hierarchy (features = word n-grams  with abbreviations, classifier = libsvm)
recognizing textual entailment challenge	F 1 score	For instance, in the previous recognizing textual entailment challenge), the best system yielded an F 1 score of 0.48, while the baseline scored 0.374.
matching task	accuracy	Most notably, performance for the matching task using only domain independent information (PRED-ARGS) was surprisingly good, with an accuracy of 0.69.
classification	accuracy	Several authors have attempted to improve classification accuracy using only positive and unlabeled data ().
classification	accuracy	Our goal is to eliminate the need for manually collecting training documents, and hopefully achieve classification accuracy from positive and unlabeled data as high as that from labeled positive and labeled negative data.
Polarity Classification Experimental	BNB	 Table 3: Task 1: Polarity Classification Experimental Results. 1g means using the unigram model,  1g+2g is using unigrams + bigrams, and 1g+2g+3g is using trigrams. Tf-Idf indicates whether tf-idf  weighting was used or not. MNB is Multinomial Naive Bayes, BNB is Bernoulli Naive Bayes, and SVM  is the Support Vector Machine. The numbers represent total accuracy / weighted F1 measure. See Sec.  5.
Classification	accuracy	 Table 1: Classification accuracy of different systems using unigram features
Classification	accuracy	 Table 2: Classification accuracy of different systems using both unigram and bigram features
classification of  definitional sentences	Precision (P )	 Table 1: Evaluation results for the classification of  definitional sentences, in terms of Precision (P ),  Recall (R), F-Measure (F ), and Accuracy (Acc),  using 10-folds cross validation. For the WCL-3  approach and the Star Patterns see
classification of  definitional sentences	Recall (R)	 Table 1: Evaluation results for the classification of  definitional sentences, in terms of Precision (P ),  Recall (R), F-Measure (F ), and Accuracy (Acc),  using 10-folds cross validation. For the WCL-3  approach and the Star Patterns see
classification of  definitional sentences	F-Measure (F )	 Table 1: Evaluation results for the classification of  definitional sentences, in terms of Precision (P ),  Recall (R), F-Measure (F ), and Accuracy (Acc),  using 10-folds cross validation. For the WCL-3  approach and the Star Patterns see
classification of  definitional sentences	Accuracy (Acc)	 Table 1: Evaluation results for the classification of  definitional sentences, in terms of Precision (P ),  Recall (R), F-Measure (F ), and Accuracy (Acc),  using 10-folds cross validation. For the WCL-3  approach and the Star Patterns see
POS tagging	accuracy	This approach works well for languages like English where automatic tokenization and POS tagging can be performed with high accuracy without the guidance of the highlevel syntactic structure.
beam search	O	Thus, beam search implementations that copy the entire state are in fact quadratic O(kn 2 ) and not linear, with a slowdown factor of O(kn) with respect to greedy parsers, which is confirmed empirically in.
beam search	O	Thus, beam search implementations that copy the entire state are in fact quadratic O(kn 2 ) and not linear, with a slowdown factor of O(kn) with respect to greedy parsers, which is confirmed empirically in.
revision	accuracy	Self training and revision improve the accuracy for every language over the seed model, and gives an average improvement of roughly two percentage points.
POS tagging	NOUN	This paper considers the POS tagging problem, i.e. where we have training and test data consisting of sentences in which all words are assigned a label y chosen from a finite set of class labels {NOUN, VERB, DET,.
POS tagging	VERB	This paper considers the POS tagging problem, i.e. where we have training and test data consisting of sentences in which all words are assigned a label y chosen from a finite set of class labels {NOUN, VERB, DET,.
POS tagging	DET	This paper considers the POS tagging problem, i.e. where we have training and test data consisting of sentences in which all words are assigned a label y chosen from a finite set of class labels {NOUN, VERB, DET,.
POS tagging	accuracy	POS tagging accuracy is known to be very sensitive to domain shifts.
passage retrieval	recall	We use coarse features for our passage retrieval model to aggressively expand the knowledge base for maximum recall; at the same time, we exploit a multi-instance learning model with fine features for relation extraction to handle the newly introduced false positives and maintain high precision.
passage retrieval	precision	We use coarse features for our passage retrieval model to aggressively expand the knowledge base for maximum recall; at the same time, we exploit a multi-instance learning model with fine features for relation extraction to handle the newly introduced false positives and maintain high precision.
relation extraction	precision	We use coarse features for our passage retrieval model to aggressively expand the knowledge base for maximum recall; at the same time, we exploit a multi-instance learning model with fine features for relation extraction to handle the newly introduced false positives and maintain high precision.
information extraction	precision	The key to this success is the combination of two different views as in co-training: an information extraction technique with fine features for high precision and an information retrieval technique with coarse features for high recall.
information extraction	recall	The key to this success is the combination of two different views as in co-training: an information extraction technique with fine features for high precision and an information retrieval technique with coarse features for high recall.
extraction	accuracy	For evaluating extraction accuracy, we follow the experimental setup of, and use their implementation of MULTIR 4 with 50 training iterations as our baseline.
passage retrieval	MULTIR	Our complete system, which we call IRMIE, combines our passage retrieval component with MULTIR.
sentential extraction	recall	The sentential extraction evaluation is performed on a small amount of manually annotated sentences, sampled from the union of matched sentences and: Overall sentential extraction performance evaluated on the original test set of and our corrected test set: Our proposed relevance feedback technique yields a substantial increase in recall.
translation	BLEU	shows translation results in terms of BLEU (), RIBES (, and TER ().
translation	RIBES	shows translation results in terms of BLEU (), RIBES (, and TER ().
translation	TER	shows translation results in terms of BLEU (), RIBES (, and TER ().
Machine translation	BLEU point	Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs.
SMT	BLEU	With this approach, we demonstrate significant improvements over a baseline phrase-based SMT system as measured by BLEU, TER and NIST scores on an English-to-Iraqi CSLT task.
SMT	TER	With this approach, we demonstrate significant improvements over a baseline phrase-based SMT system as measured by BLEU, TER and NIST scores on an English-to-Iraqi CSLT task.
ASR transcriptions	word error rate (WER)	ASR transcriptions were generated using a high-performance two-pass HMM-based system, which delivered a word error rate (WER) of 10.6% on the test set utterances.
ASR	BLEU	In the ASR setting, which simulates a realworld deployment scenario, this system achieves improvements of 0.39 (BLEU), -0.6 (TER) and 0.08 (NIST).
ASR	TER	In the ASR setting, which simulates a realworld deployment scenario, this system achieves improvements of 0.39 (BLEU), -0.6 (TER) and 0.08 (NIST).
synonym choice	accuracy	For synonym choice, we follow the method established by, measuring accuracy over covered items, with partial credit for ties.
synonym choice	accuracy	Thus, the results for synonym choice are less clear-cut: derivational smoothing can trade accuracy against  coverage but does not lead to a clear improvement.
synonym choice	coverage	Thus, the results for synonym choice are less clear-cut: derivational smoothing can trade accuracy against  coverage but does not lead to a clear improvement.
synonym choice task	Acc: Accuracy	 Table 2: Results on the synonym choice task  (Acc: Accuracy, Cov: Coverage)
Annotating frame information	FEs	Annotating frame information is a complex task, usually modeled in two steps: first annotators are asked to choose the situation (or frame) evoked by a given predicate (the lexical unit, LU) in a sentence, and then they assign the semantic roles (or frame elements, FEs) that describe the participants typically involved in the chosen frame.
Image Description Transfer	BLEU	 Table 3: Image Description Transfer: performance in BLEU and F1 with strict & semantic matching.
Image Description Transfer	F1	 Table 3: Image Description Transfer: performance in BLEU and F1 with strict & semantic matching.
DS	accuracy	In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles.
DS	accuracy	In this paper, we point out and analyze some critical factors in DS which have great impact on the accuracy but has not been touched or well handled before.
Sentiment	Efficacy	 Table 7: Sentiment and Efficacy of Medications
classification	latent Dirichlet allocation (LDA)	3) Information used for classification -we use latent information estimated by latent Dirichlet allocation (LDA) () to classify documents, and compare the results of the cases using both surface and latent information.
stacking	accuracy	We demonstrate that the stacking method substantially outperforms benchmark methods, achieving 49% accuracy on a benchmark dataset.
stacking	accuracy	Experimental results demonstrate that our stacking model outperforms benchmark methods by a large margin, achieving 49% accuracy on the test data.
Chinese natural language processing (NLP)	soon	FudanNLP is an open source toolkit for Chinese natural language processing (NLP), which uses statistics-based and rule-based methods to deal with Chinese NLP tasks, such as word segmentation, part-of-speech tagging, named entity recognition , dependency parsing, time phrase recognition, anaphora resolution and soon.
word segmentation	soon	FudanNLP is an open source toolkit for Chinese natural language processing (NLP), which uses statistics-based and rule-based methods to deal with Chinese NLP tasks, such as word segmentation, part-of-speech tagging, named entity recognition , dependency parsing, time phrase recognition, anaphora resolution and soon.
dependency parsing	soon	FudanNLP is an open source toolkit for Chinese natural language processing (NLP), which uses statistics-based and rule-based methods to deal with Chinese NLP tasks, such as word segmentation, part-of-speech tagging, named entity recognition , dependency parsing, time phrase recognition, anaphora resolution and soon.
word segmentation (CWS)	soon	Similar to English, the main tasks in Chinese NLP include word segmentation (CWS), part-of-speech (POS) tagging, named entity recognition (NER), syntactic parsing, anaphora resolution (AR), and soon.
named entity recognition (NER)	soon	Similar to English, the main tasks in Chinese NLP include word segmentation (CWS), part-of-speech (POS) tagging, named entity recognition (NER), syntactic parsing, anaphora resolution (AR), and soon.
Tuning	error rate	Tuning was performed with minimum error rate training to maximize BLEU over 200-best lists.
Tuning	BLEU	Tuning was performed with minimum error rate training to maximize BLEU over 200-best lists.
Translation	BLEU	 Table 2: Translation results (BLEU, RIBES), for  several translation models (PBMT, Hiero, T2S,  F2S), aligners (GIZA++, Nile), and parsers (Stan- ford, Egret).
Translation	RIBES	 Table 2: Translation results (BLEU, RIBES), for  several translation models (PBMT, Hiero, T2S,  F2S), aligners (GIZA++, Nile), and parsers (Stan- ford, Egret).
translation	accuracy	We also compared two expert translations using METEOR to establish a skyline for the translation accuracy.
MT	BLEU	Standard automatic evaluation metrics for MT, such as BLEU () and TER), have already been implemented.
MT	TER	Standard automatic evaluation metrics for MT, such as BLEU () and TER), have already been implemented.
coreference resolution	early update	We show that for the task of coreference resolution the straightforward combination of beam search and early update () falls short of more limited feature sets that allow for exact search.
Unsup L-match segmentation	Morfessor	For Finnish, we adopt the Unsup L-match segmentation technique of, which uses Morfessor () to analyze the 5,000 most frequent Finnish words.
translation	BLEU-4	The translation quality is evaluated by case-insensitive IBM BLEU-4 metric.
translation	accuracy	Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline.
translation	accuracy	Experimental results demonstrate that our model significantly improves translation accuracy over a state-of-the-art baseline.
tagging	accuracy	Experiment on the SANCL 2012 shared task show that our approach achieves 93.15% average tagging accuracy , which is the best accuracy reported so far on this data set, higher than those given by ensembled syntactic parsers.
tagging	accuracy	Experiment on the SANCL 2012 shared task show that our approach achieves 93.15% average tagging accuracy , which is the best accuracy reported so far on this data set, higher than those given by ensembled syntactic parsers.
solution identification	F-Measure	In this study, we compare the performance of our method under varying settings of λ against the only unsupervised approach for solution identification from literature, that from ( . We use an independent implementation of the technique using Kullback-Leibler Divergence as the similarity measure between posts; KL-Divergence was seen to perform best in the experiments reported in ( .   on various quality metrics, of which F-Measure is typically considered most important.
solution identification	Fmeasure	The comparison with the approach from (  illustrates that our method is very clearly the superior method for solution identification outperforming the former by large margins on all the evaluation measures, with the improvement on Fmeasure being more than 25 points.
ANS CT	F-measure	Our technique is seen to outperform ANS CT by a respectable margin (8.6 F-measure points) while trailing behind the enhanced ANS-ACK PCT method with a reasonably narrow 3.8 F-measure point margin.
ANS CT	F-measure point margin	Our technique is seen to outperform ANS CT by a respectable margin (8.6 F-measure points) while trailing behind the enhanced ANS-ACK PCT method with a reasonably narrow 3.8 F-measure point margin.
Spouse prediction	LOCAL	Spouse prediction also benefits from neighboring effect and the improvement is about 12% for LOCAL(ENTITY) setting.
NELL	Recall score	It is also worth noting that most of education mentions that NELL fails to retrieve are those involve irregular spellings, such as HarvardUniv and Cornell U, which means Recall score for NELL baseline would be even higher if these irregular spellings are recognized in a more sophisticated system.
parsing	UAS	 Table 3: Results for parsing and corrective tagging  on the CATiB dataset. The upper part shows UAS  of our model with gold/predicted information or  POS correction. Bottom part shows UAS of the  best systems in the SPMRL shared task. IMS- Single (Björkelund et al., 2013) is the best single  parsing system, while IMS-Ensemble (Björkelund  et al., 2013) is the best ensemble parsing system.  We also show results for CADIM (Marton et al.,  2013), the second best system, because we use  their predicted features.
parsing	POS correction	 Table 3: Results for parsing and corrective tagging  on the CATiB dataset. The upper part shows UAS  of our model with gold/predicted information or  POS correction. Bottom part shows UAS of the  best systems in the SPMRL shared task. IMS- Single (Björkelund et al., 2013) is the best single  parsing system, while IMS-Ensemble (Björkelund  et al., 2013) is the best ensemble parsing system.  We also show results for CADIM (Marton et al.,  2013), the second best system, because we use  their predicted features.
parsing	UAS	 Table 3: Results for parsing and corrective tagging  on the CATiB dataset. The upper part shows UAS  of our model with gold/predicted information or  POS correction. Bottom part shows UAS of the  best systems in the SPMRL shared task. IMS- Single (Björkelund et al., 2013) is the best single  parsing system, while IMS-Ensemble (Björkelund  et al., 2013) is the best ensemble parsing system.  We also show results for CADIM (Marton et al.,  2013), the second best system, because we use  their predicted features.
corrective tagging	UAS	 Table 3: Results for parsing and corrective tagging  on the CATiB dataset. The upper part shows UAS  of our model with gold/predicted information or  POS correction. Bottom part shows UAS of the  best systems in the SPMRL shared task. IMS- Single (Björkelund et al., 2013) is the best single  parsing system, while IMS-Ensemble (Björkelund  et al., 2013) is the best ensemble parsing system.  We also show results for CADIM (Marton et al.,  2013), the second best system, because we use  their predicted features.
corrective tagging	POS correction	 Table 3: Results for parsing and corrective tagging  on the CATiB dataset. The upper part shows UAS  of our model with gold/predicted information or  POS correction. Bottom part shows UAS of the  best systems in the SPMRL shared task. IMS- Single (Björkelund et al., 2013) is the best single  parsing system, while IMS-Ensemble (Björkelund  et al., 2013) is the best ensemble parsing system.  We also show results for CADIM (Marton et al.,  2013), the second best system, because we use  their predicted features.
corrective tagging	UAS	 Table 3: Results for parsing and corrective tagging  on the CATiB dataset. The upper part shows UAS  of our model with gold/predicted information or  POS correction. Bottom part shows UAS of the  best systems in the SPMRL shared task. IMS- Single (Björkelund et al., 2013) is the best single  parsing system, while IMS-Ensemble (Björkelund  et al., 2013) is the best ensemble parsing system.  We also show results for CADIM (Marton et al.,  2013), the second best system, because we use  their predicted features.
root classification	accuracy	 Table 5: Fine-grained sentiment analysis results  on the Stanford Sentiment Treebank of Socher et  al. (2013). We compare against the printed num- bers in Socher et al. (2013) as well as the per- formance of the corresponding release, namely  the sentiment component in the latest version of  the Stanford CoreNLP at the time of this writ- ing. Our model handily outperforms the results  from Socher et al. (2013) at root classification and  edges out the performance of the latest version of  the Stanford system. On all spans of the tree, our  model has comparable accuracy to the others.
Synonym detection	TOEFL	Synonym detection The classic TOEFL (toefl) set was introduced by.
AN task	accuracy	For the AN task, the cross validation accuracy is better by 8% than the result of Turney et al.
AN metaphor detection	accuracy	 Table 4: Comparing AN metaphor detection  method to the baselines: accuracy of the 10- fold cross validation on annotations of five human  judges.
WSD	Acc UB	In addition to the WSD accuracy based on the predominant sense inferred from a particular corpus, we additionally compute: (1) Acc UB , the upper bound for the first sense-based WSD accuracy (using the gold standard predominant sense for disambiguation); 7 and (2) ERR, the error rate reduction between the accuracy fora given system (Acc) and the upper bound (Acc UB ), calculated as follows: Looking at the results in   both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies.
WSD	ERR	In addition to the WSD accuracy based on the predominant sense inferred from a particular corpus, we additionally compute: (1) Acc UB , the upper bound for the first sense-based WSD accuracy (using the gold standard predominant sense for disambiguation); 7 and (2) ERR, the error rate reduction between the accuracy fora given system (Acc) and the upper bound (Acc UB ), calculated as follows: Looking at the results in   both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies.
WSD	error rate reduction	In addition to the WSD accuracy based on the predominant sense inferred from a particular corpus, we additionally compute: (1) Acc UB , the upper bound for the first sense-based WSD accuracy (using the gold standard predominant sense for disambiguation); 7 and (2) ERR, the error rate reduction between the accuracy fora given system (Acc) and the upper bound (Acc UB ), calculated as follows: Looking at the results in   both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies.
WSD	accuracy	In addition to the WSD accuracy based on the predominant sense inferred from a particular corpus, we additionally compute: (1) Acc UB , the upper bound for the first sense-based WSD accuracy (using the gold standard predominant sense for disambiguation); 7 and (2) ERR, the error rate reduction between the accuracy fora given system (Acc) and the upper bound (Acc UB ), calculated as follows: Looking at the results in   both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies.
WSD	accuracy	 Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
WSD	accuracy	 Table 4: WSD accuracy for HDP-WSI on the  Macmillan-annotated datasets, as compared to the  upper-bound based on actual first sense in the cor- pus (higher values indicate better performance; the  best system in each row [other than the FS CORPUS  upper bound] is indicated in boldface).
reversing and shifting heuristics	MAE	The table shows that the basic reversing and shifting heuristics do capture negators' behavior to some degree, as their MAE scores are higher than that of the baseline.
GR extraction	precision	Manual evaluation highlights the reliability of our linguistically-motivated GR extraction algorithm: The overall dependency-based precision and recall are 99.17 and 98.87.
GR extraction	recall	Manual evaluation highlights the reliability of our linguistically-motivated GR extraction algorithm: The overall dependency-based precision and recall are 99.17 and 98.87.
parsing	accuracy	For example, and show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy.
parsing	accuracy	All above work leads to significant improvement on parsing accuracy.
parsing	accuracy	combine tri-training and parser ensemble to boost parsing accuracy.
alignment	precision	We followed previous work and evaluated the alignment performance in terms of four measures: precision, recall, F1, and accuracy.
alignment	recall	We followed previous work and evaluated the alignment performance in terms of four measures: precision, recall, F1, and accuracy.
alignment	F1	We followed previous work and evaluated the alignment performance in terms of four measures: precision, recall, F1, and accuracy.
alignment	accuracy	We followed previous work and evaluated the alignment performance in terms of four measures: precision, recall, F1, and accuracy.
sentiment classification	accuracy	Our sentiment classification model achieves approximately 1% greater accuracy than a state-of-the-art approach based on elementary discourse units.
sentiment classification	accuracy	Additionally, our sentiment classification model achieves approximately 1% greater accuracy than a state-of-theart approach based on elementary discourse units (.
relation extraction	F-score	The performance of relation extraction is still unsatisfactory with a F-score of 67.5% for English (23 subtypes) (.
POS tagging	accuracy	By limiting the evaluation to unseen words instead of all words, we can evaluate the gain in POS tagging accuracy solely due to DA.
sentiment prediction	BINB	 Table 1: Accuracy of sentiment prediction in the  movie reviews dataset. The first four results are  reported from Socher et al. (2013b). The baselines  NB and BINB are Naive Bayes classifiers with,  respectively, unigram features and unigram and bi- gram features. SVM is a support vector machine  with unigram and bigram features. RECNTN is a  recursive neural network with a tensor-based fea- ture function, which relies on external structural  features given by a parse tree and performs best  among the RecNNs.
MT tuning	MIRA	Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO) and MIRA (, which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses.
WMT12 metrics task	BLEU	To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (), NIST), TER), ROUGE-W (, and three METEOR variants (Denkowski and Lavie, 2011): METEOR-ex (exact match), METEOR-st (+stem-ming) and METEOR-sy (+synonyms).
WMT12 metrics task	TER	To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (), NIST), TER), ROUGE-W (, and three METEOR variants (Denkowski and Lavie, 2011): METEOR-ex (exact match), METEOR-st (+stem-ming) and METEOR-sy (+synonyms).
WMT12 metrics task	ROUGE-W	To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (), NIST), TER), ROUGE-W (, and three METEOR variants (Denkowski and Lavie, 2011): METEOR-ex (exact match), METEOR-st (+stem-ming) and METEOR-sy (+synonyms).
POS tag determination	accuracy	Because non-local syntactic information is now available to POS tag determination, the accuracy of POS tagging improves, and this will in turn improve parsing accuracy.
POS tag determination	accuracy	Because non-local syntactic information is now available to POS tag determination, the accuracy of POS tagging improves, and this will in turn improve parsing accuracy.
parsing	accuracy	Because non-local syntactic information is now available to POS tag determination, the accuracy of POS tagging improves, and this will in turn improve parsing accuracy.
parsing	accuracy	After integrating semi-supervised word cluster features, the parsing accuracy is further improved to 86.3% when trained on CTB 5.1 and 87.1% when trained on CTB 6.0, and this is the best reported performance for Chinese.
parsing	accuracy	Compared with the JointParsing system which does not employ any alignment strategy, the Padding system only achieved a slight improvement on parsing F 1 score, but no improvement on POS tagging accuracy.
parsing	accuracy	In contrast, our StateAlign system achieved an improvement of 0.6% on parsing F 1 score and 0.4% on POS tagging accuracy.
parsing F	accuracy	Compared with the StateAlign system which takes only the baseline features, the non-local features improved parsing F 1 by 0.8%, while the semi-supervised word cluster features result in an improvement of 2.3% in parsing F 1 and an 1.1% improvement on POS tagging accuracy.
parsing F	accuracy	Compared with the StateAlign system which takes only the baseline features, the non-local features improved parsing F 1 by 0.8%, while the semi-supervised word cluster features result in an improvement of 2.3% in parsing F 1 and an 1.1% improvement on POS tagging accuracy.
KN smoothing	Bleu	shows that, although the relationship between alignment F1 and Bleu is not very consistent, expected KN smoothing achieves the best Bleu among all these methods and is significantly better than the baseline (p < 0.01).
MT	consistency	By dynamically training the QE model for the document-specific MT model, we are able to achieve consistency and prediction quality across multiple documents, demonstrated by the higher correlation coefficient and F-scores in finding Good sentences.
MT	correlation coefficient	By dynamically training the QE model for the document-specific MT model, we are able to achieve consistency and prediction quality across multiple documents, demonstrated by the higher correlation coefficient and F-scores in finding Good sentences.
MT	F-scores	By dynamically training the QE model for the document-specific MT model, we are able to achieve consistency and prediction quality across multiple documents, demonstrated by the higher correlation coefficient and F-scores in finding Good sentences.
MT quality estimation	accuracy	In contrast to traditional static MT quality estimation methods, our approach not only trains the MT quality estimator dynamically for each document-specific MT model to obtain higher prediction accuracy, but also achieves consistency over different document-specific MT models.
MT	TERs-to	We would like to provide translators with some guidance on reasonably good MT proposals-the sentences with low TERs-to help them increase the leverage on MT proposals to achieve improved productivity.
summarization	ROUGE	We derive three features from these timelines, and show that their use in supervised summarization lead to a significant 4.1% improvement in ROUGE performance over a state-of-the-art base-line.
Summarization evaluation	ROUGE-2	Summarization evaluation is done using ROUGE-2 (R-2) (, as it has previously been shown to correlate well with human assessment) and is often used to evaluate automatic text summarization.
semantic parsing	F 1	While making semantic parsing more robust is a laudable goal, here we provide a more rigorous IE baseline against which those efforts should be compared: we show that "traditional" IE methodology can significantly outperform prior state-of-the-art as reported in the semantic parsing literature, with a relative gain of 34% F 1 as compared to.
MRR	overlap score	For MRR, we used the rank of the candidate with the highest overlap score, weighed by the inverse of the rank.
objectextraction labelling	accuracy	This slight, though significant in Eve, deficit is counter-balanced by a very substantial and significant improvement in objectextraction labelling accuracy.
classification	accuracy	F1 is a widely-used measure of classification accuracy.
Domain Adaptation	TER	Domain Adaptation using Topic Models We examine the effectiveness of using topic models for domain adaptation on standard SMT evaluation metrics-BLEU () and TER).
SMT evaluation	TER	Domain Adaptation using Topic Models We examine the effectiveness of using topic models for domain adaptation on standard SMT evaluation metrics-BLEU () and TER).
machine translation	BLEU	LDA marginally improves machine translation (less than half a BLEU point).
SRL	F1	 Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
SRL	FT	 Table 6: Subtractive experiments. Each row con- tains the F1 for SRL only (without sense disam- biguation) where the supervision type of that row  and all above it have been removed. Removed su- pervision types (Rem) are: syntactic dependencies  (Dep), morphology (Mor), POS tags (POS), and  lemmas (Lem). #FT indicates the number of fea- ture templates used (unigrams+bigrams).
entity mention detection	F-scores	This performance loss appears to be largely due to poor entity mention detection, as we found that not using entity mention lexicon entries attest time improves ASP's labeled and unlabeled F-scores by 0.3% on Section 00.
machine translation	F 1 metrics	Following evaluations in machine translation as well as previous work in sentence compression (, we evaluate system performance using F 1 metrics over n-grams and dependency edges produced by parsing system output with RASP () and the Stanford parser.
sentence compression	F 1 metrics	Following evaluations in machine translation as well as previous work in sentence compression (, we evaluate system performance using F 1 metrics over n-grams and dependency edges produced by parsing system output with RASP () and the Stanford parser.
predicting negative sentiment	TABLETS	Considering per-class performance, correctly predicting negative sentiment is most difficult for both AUTO and TABLETS, which is probably caused by the smaller proportion of the negative comments in the training set.
keyphrase extraction	precision (P)	To score the output of a keyphrase extraction system, the typical approach, which is also adopted by the SemEval-2010 shared task on keyphrase extraction, is (1) to create a mapping between the keyphrases in the gold standard and those in the system output using exact match, and then (2) score the output using evaluation metrics such as precision (P), recall (R), and F-score (F).
keyphrase extraction	recall (R)	To score the output of a keyphrase extraction system, the typical approach, which is also adopted by the SemEval-2010 shared task on keyphrase extraction, is (1) to create a mapping between the keyphrases in the gold standard and those in the system output using exact match, and then (2) score the output using evaluation metrics such as precision (P), recall (R), and F-score (F).
keyphrase extraction	F-score (F)	To score the output of a keyphrase extraction system, the typical approach, which is also adopted by the SemEval-2010 shared task on keyphrase extraction, is (1) to create a mapping between the keyphrases in the gold standard and those in the system output using exact match, and then (2) score the output using evaluation metrics such as precision (P), recall (R), and F-score (F).
IR	precision	R-precision (R p ) is an IR metric that focuses on ranking: given a document with n gold keyphrases, it computes the precision of a system over its n highest-ranked candidates (.
SemEval-2010 shared task	F-score	For example, KP-Miner (El-Beltagy and Rafea, 2010), an unsupervised system, ranked third in the SemEval-2010 shared task with an F-score of 25.2, which is comparable to the best supervised system scoring 27.5.
ASR	mescore	In, noticing that the correlations decrease going along a row, we can say that the errors in the ASR system caused both mescore and cos 4 to under-perform.
ASR	accuracy	Confidence scores from an ASR system (which incorporate N-gram probabilities) are optimized in order to produce the most likely sequence of words rather than the accuracy of individual word detections.
OOV reduction	FP	 Table 2: Type-based expansion results for the 50k-best list for different models. Tr. WFST stands for  trigraph WFST, NRR for no reranking, W•Tr for trigraph reweighting, TRR for trigraph-based rereank- ing, BRR for reranking morpheme boundary, and ∞ for the upper bound of OOV reduction via lexicon  expansion if we produce all words. FP (full-pack data) shows the effect of using bigger data with the size  of about seven times larger than our data set, instead of using our unsupervised approach.
Token-based expansion	Abbreviations	 Table 3: Token-based expansion results for the 50k-best list for different models. Abbreviations are the  same as
MT	accuracy	When used in conjunction with a pre-computed hidden layer, these techniques speedup NNJM computation by a factor of 10,000x, with only a small reduction on MT accuracy.
MT	BOLT	We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions.
concept identification	precision	Statistics about this corpus and our train/dev./test splits are given in   For the performance of concept identification, we report precision, recall, and F 1 of labeled spans using the induced labels on the training and test data as a gold standard.
concept identification	recall	Statistics about this corpus and our train/dev./test splits are given in   For the performance of concept identification, we report precision, recall, and F 1 of labeled spans using the induced labels on the training and test data as a gold standard.
concept identification	F 1	Statistics about this corpus and our train/dev./test splits are given in   For the performance of concept identification, we report precision, recall, and F 1 of labeled spans using the induced labels on the training and test data as a gold standard.
relation identification	F 1	Using gold concepts with the relation identification stage yields a much higher Smatch score of 80% F 1 . As a comparison, AMR Bank annotators have a consensus inter-annotator agreement Smatch score of 83% F 1 . The runtime of our system is given in.
relation identification	F	Using gold concepts with the relation identification stage yields a much higher Smatch score of 80% F 1 . As a comparison, AMR Bank annotators have a consensus inter-annotator agreement Smatch score of 83% F 1 . The runtime of our system is given in.
resolution	accuracy	For resolution, we report value accuracy, measuring correctness of time expressions detected according to the relaxed metric.
WSD	accuracy	They show that such a reformulated WSD can improve the accuracy of a simplified word translation task.
translation tasks	BLEU	For the translation tasks, our model achieves up to 0.74% gain in BLEU as compared to the FFNN-based model, which matches the translation qualities of the IBM Model 4.
Translation	BLEU4	 Table 3: Translation performance (BLEU4(%))
Word alignment	F1-measure	 Table 5: Word alignment performance of various  FFNN-based models (F1-measure)
alignment	accuracy	Results show that the new relaxation is much more effective at finding exact solutions and is able to produce comparable alignment accuracy.
text classification	Latent Dirichlet Allocation (LDA)	In this paper, we propose a text classification algorithm based on Latent Dirichlet Allocation (LDA) () which does not need labeled documents.
Decoder Integration	BLEU	Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models
parsing	accuracy	To do so, we perform an empirical study of the effect of parsing accuracy, packed forest input, alignment accuracy, and search.
parsing	accuracy	To do so, we perform an empirical study of the effect of parsing accuracy, packed forest input, alignment accuracy, and search.
alignment	accuracy	To do so, we perform an empirical study of the effect of parsing accuracy, packed forest input, alignment accuracy, and search.
spelling correction	Precision	For evaluation of spelling correction quality, we use the following metrics: • Precision: The number of correct spelling corrections for misspelled words generated by the system divided by the total number of corrections generated by the system; • Recall: The number of correct spelling corrections for misspelled words generated by the system divided by the total number of misspelled words in the test set; For hypotheses generator, K = 30 was fixed: recall of 91.8% was considered big enough.
spelling correction	Recall	For evaluation of spelling correction quality, we use the following metrics: • Precision: The number of correct spelling corrections for misspelled words generated by the system divided by the total number of corrections generated by the system; • Recall: The number of correct spelling corrections for misspelled words generated by the system divided by the total number of misspelled words in the test set; For hypotheses generator, K = 30 was fixed: recall of 91.8% was considered big enough.
spelling correction	K	For evaluation of spelling correction quality, we use the following metrics: • Precision: The number of correct spelling corrections for misspelled words generated by the system divided by the total number of corrections generated by the system; • Recall: The number of correct spelling corrections for misspelled words generated by the system divided by the total number of misspelled words in the test set; For hypotheses generator, K = 30 was fixed: recall of 91.8% was considered big enough.
spelling correction	recall	For evaluation of spelling correction quality, we use the following metrics: • Precision: The number of correct spelling corrections for misspelled words generated by the system divided by the total number of corrections generated by the system; • Recall: The number of correct spelling corrections for misspelled words generated by the system divided by the total number of misspelled words in the test set; For hypotheses generator, K = 30 was fixed: recall of 91.8% was considered big enough.
CWS	F-scores	State-of-the-art performance in CWS is high, with F-scores in the upper 90s.
dual decomposition	F 1 score	 Table 2: Performance of dual decomposition in  comparison to past published results on SIGHAN  2003 and 2005 datasets. Best reported F 1 score  for each dataset is highlighted in bold. Z&C 07  refers to
segmentation	accuracy	Second, we add new features that improve segmentation accuracy.
segmentation	accuracy	Second, we add new features that improve segmentation accuracy.
Document classification	accuracy	 Table 2: Document classification accuracy when  trained on 1,000 training examples of the RCV1/2  corpus (train→test). Baselines are the majority  class, glossed, and MT (Klementiev et al., 2012).  Further, we are comparing to Klementiev et al.  (2012), BiCVM ADD (
Document classification	MT	 Table 2: Document classification accuracy when  trained on 1,000 training examples of the RCV1/2  corpus (train→test). Baselines are the majority  class, glossed, and MT (Klementiev et al., 2012).  Further, we are comparing to Klementiev et al.  (2012), BiCVM ADD (
Document classification	BiCVM ADD	 Table 2: Document classification accuracy when  trained on 1,000 training examples of the RCV1/2  corpus (train→test). Baselines are the majority  class, glossed, and MT (Klementiev et al., 2012).  Further, we are comparing to Klementiev et al.  (2012), BiCVM ADD (
ASR	accuracy	This approach has been shown to dramatically outperform ASR in terms of both accuracy and Word Error Rate (WER) ().
ASR	Word Error Rate (WER)	This approach has been shown to dramatically outperform ASR in terms of both accuracy and Word Error Rate (WER) ().
alignment	accuracy	Furthermore, smaller weights allow faster alignment, but provide lower accuracy.
LDA	Moving Average Convergence Divergence	For the results of LDA, we identified topic words by using Moving Average Convergence Divergence.
tagging	accuracy	We propose improving tagging accuracy by utilizing dependencies within sub-components of the fine-grained labels.
tagging	accuracy	Experiments on five languages show that the approach can yield significant improvement in tagging accuracy in case the labels have sufficiently rich inner structure.
tagging	accuracy	We propose improving tagging accuracy by utilizing dependencies within the sub-labels (PRON, 1SG, V, NON3SG, N, and SG in the above example) of the compound labels.
POS induction	accuracy	In unsupervised POS induction it is standard to report accuracy on tokens even when the model itself works on types.
MT system	BLEU	In fact, the correlation is strong enough that we propose that this accuracy measure itself can be used as a measure of MT system quality, obviating the need fora reference corpus, as for example is necessary for BLEU ().
abbreviation expansion	precision	In this paper, we focus on abbreviation expansion for TTS, which requires a "do no harm", high precision approach yielding few expansion errors at the cost of leaving relatively many abbreviations un-expanded.
translation	accuracy	For some applications, where the normalized string is an intermediate stage in a larger application such as translation or information retrieval, overgeneration of normalized alternatives is often a beneficial strategy, to the extent that it may improve the accuracy of what is eventually being presented to the user.
emotion classification	F1	 Table 2: Experiment results for emotion classification in term of F1 score
Sentiment Analysis	F	The proposed sentiment model outperforms the top system in the task of Sentiment Analysis in Twit-ter in SemEval-2013 in terms of averaged F scores.
sentiment classification	accuracy	Such a mixture model results in further improvement on the sentiment classification accuracy.
PSQ	BM25	For PSQ, BM25 is computed on expected term and document frequencies.
WSD	F-measure	 Table 2: Lexical knowledge sources and WSD performance (F-measure) on the Senseval-2 (fine-and  coarse-grained) and the SemEval-2007 dataset.
SCL	dimensionality K	The best parameters for SCL are dimensionality K = 25 and rescale factor α = 5, which are the same as in the original paper.
SCL	rescale factor α	The best parameters for SCL are dimensionality K = 25 and rescale factor α = 5, which are the same as in the original paper.
Division	accuracy	Division into these sets is based on an automatically derived accuracy metric.
speech translation task	accuracy	We evaluate our methods on a speech translation task, and we confirm that our approaches can achieve translation units two to three times as finegrained as other methods, while maintaining the same accuracy.
MT	BLEU+1	A phrasebased machine translation (PBMT) system learned by Moses () is used as the translation system MT . We use BLEU+1 as the evaluation measure EV in the proposed method.
MT task	BLEU	When the selected sentence pairs are evaluated on an end-to-end MT task, our methods can increase the translation performance by 3 BLEU points.
Counting substitutions between similar  words	Dissimilarity	 Table 4: Counting substitutions between similar  words as half an error. Dissimilarity is measured  as letter edit distance
summarisation	ROUGE-1	We evaluated these summarisation approaches with the ROUGE-1 method), a widely used summarisation evaluation metric that correlates well with human evaluation (.
SRL	labeled semantic F1-score	In this paper, we describe a Korean SRL system which achieves 81% labeled semantic F1-score.
information extraction	breadth	On the other hand, modelling the semantics for information extraction purposes does not seem feasible given the breadth and diversity of the material.
SMT	BLEU	In this section, the proposed method is first validated on monolingual segmentation tasks, and then evaluated in the context of SMT to study whether the translation quality, measured by BLEU, can be improved.
translation	BLEU	Here we report translation quality in BLEU.
MT	BLEU scores)	 Table 5: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours) on  the Spanish/English OPUS corpus using only non-parallel corpora for training.
translation	BLEU	In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (.
translation	TER	In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (.
translation	MEANT	In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (.
MT evaluation	BITGs	We therefore propose XMEANT, a cross-lingual MT evaluation metric, that modifies MEANT using (1) simple translation probabilities (in our experiments, from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs).
MT adequacy	MEANT	We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language.
MT	BLEU	Our best result improves over the best single MT system baseline by 1.0% BLEU and over a strong system selection baseline by 0.6% BLEU on a blind test set.
MT	BLEU	Our best result improves over the best single MT system baseline by 1.0% BLEU and over a strong system selection baseline by 0.6% BLEU on a blind test set.
MT	BLEU	Our best system selection approach improves over our best baseline single MT system by 1.0% absolute BLEU point on a blind test set.
translation	BLEU-4	The translation quality is evaluated by case-insensitive BLEU-4 ().
MT	coverage	words phrase-based MT 32% 12% HIERO 35% 30% HIERO (all rules) 65% 55% See text below for "loose" and "tight". between overfitting and coverage.
Punctuation parsing	accuracy	Punctuation parsing errors lead to low parsing accuracy on words.
parsing	accuracy	* This work was done while the first author was visiting SUTD Moreover, experimental results showed that parsing accuracy of content words drops on sentences which contain higher ratios of punctuations.
parsing	accuracy	Take valency features for example, previous work () has shown that such features are important to parsing accuracy, e.g., it may inform the parser that a verb already has two objects attached to it.
parse hedge constituents	accuracy	In this paper, we propose several methods to parse hedge constituents and examine their accuracy/efficiency tradeoffs.
parsing	Fmeasure absolute	We find an order of magnitude speedup of parsing, but at the cost of 3 percent Fmeasure absolute.
tagging	accuracy	Initializing EM with the relative frequency of these unambiguous pairs improves tagging accuracy dramatically over uniform initialization, reducing errors by 56% in English and 29% in German.
tagging	errors	Initializing EM with the relative frequency of these unambiguous pairs improves tagging accuracy dramatically over uniform initialization, reducing errors by 56% in English and 29% in German.
tagging	accuracy	This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of, the Bayesian LDA-based model of, and an HMM trained with language-specific initialization described by.
Tagging	accuracy	 Table 2: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k  train describe models trained on the subset of 973k tokens used by
relation translation	F1-score	Our proposed hybrid approach of using both explicit and latent features improves relation translation by 0.16 F1-score, and in turn improves entity translation by 0.02.
relation translation	F1-score	Our new hybrid approach significantly improves the relation translation (0.16 higher F1-score than EF), and in turn improves the entity translation (0.02 higher F1-score).
relation translation	EF	Our new hybrid approach significantly improves the relation translation (0.16 higher F1-score than EF), and in turn improves the entity translation (0.02 higher F1-score).
relation translation	F1-score	Our new hybrid approach significantly improves the relation translation (0.16 higher F1-score than EF), and in turn improves the entity translation (0.02 higher F1-score).
MT	BLEU	Comparable evaluation is essential for MT research, yet conventional MT metrics, such as BLEU, is not effective in detecting improvement in discourse relation translation.
summarizing	F1 measure	The performance of the summarizing system is measured with Rouge-1 Recall, Rouge-1 Precision and F1 measure).
Sentence recall	precision	Sentence recall and sentence precision are defined as follows: where M is the number of the sentences included  As shown, the proposed system performs better than the best systems of DUC-2002 in terms of sentence recall.
NE recognizer tagger	RAM	 Table 4: Evaluation of the NE recognizer tagger  throughput, RAM and model size.
Word recognition	accuracy	 Table 1: Word recognition accuracy for Yoruba us- ing old (slower) and new (faster) implementations,  with p-values from t-tests for significance of dif- ference in means. Bold indicates highest accuracy.
Word recognition	accuracy	 Table 1: Word recognition accuracy for Yoruba us- ing old (slower) and new (faster) implementations,  with p-values from t-tests for significance of dif- ference in means. Bold indicates highest accuracy.
NIST Chinese-English translation tasks	BLEU	Experiments on two NIST Chinese-English translation tasks show that the proposed model can achieve significant improvements over the previous NNJM by up to +1.08 BLEU points on average.
NIST Chinese-English translation tasks	BLEU	Experiments on NIST Chinese-English translation tasks show that our model is able to achieve significant improvements of +2.0 BLEU points on average over the baseline.
MT	BLEU	 Table 1: MT results of various model combination  in BLEU and in TER.
MT	TER	 Table 1: MT results of various model combination  in BLEU and in TER.
classification	accuracy	In the experiments, classification accuracy was estimated via five repeats of 5-fold crossvalidation.
Named entity recognition	speed	Named entity recognition also shows significant speed increases.
Document classification	recall	Document classification is expected to reduce false positives in irrelevant documents while not dramatically reducing recall.
SMT	accuracy	In this paper, we perform the first experiments applying syntax-based SMT to simultaneous translation , and propose two methods to prevent degradations in accuracy: a method to predict unseen syntactic constituents that help generate complete parse trees, and a method that waits for more input when the current utterance is not enough to generate a fluent translation.
translation	BLEU	As an evaluation metric for translation quality, BLEU () is used.
MT	RTE	The answer-entailing structures in our model are closely related to the inference procedure often used in various models for MT), RTE (), paraphrase (), QA (, etc. and correspond to the best (latent) alignment between a hypothesis (formed from the question and a candidate answer) with appropriate snippets in the text that are required to answer the question.
MT	paraphrase	The answer-entailing structures in our model are closely related to the inference procedure often used in various models for MT), RTE (), paraphrase (), QA (, etc. and correspond to the best (latent) alignment between a hypothesis (formed from the question and a candidate answer) with appropriate snippets in the text that are required to answer the question.
MT	QA	The answer-entailing structures in our model are closely related to the inference procedure often used in various models for MT), RTE (), paraphrase (), QA (, etc. and correspond to the best (latent) alignment between a hypothesis (formed from the question and a candidate answer) with appropriate snippets in the text that are required to answer the question.
image labeling	accuracy	In image labeling, when a search space of realistic size is considered, accuracy is just above 1% (which is still well above chance for large search spaces).
parsing	accuracy	To further strengthen our parsing model, we incorporate phrase embeddings into the model, which significantly improves the parsing accuracy.
parsing	accuracy	To further strengthen our parsing model, we incorporate phrase embeddings into the model, which significantly improves the parsing accuracy.
parsing	accuracy	On the Penn Treebank, this structured learning approach significantly improves parsing accuracy by 0.8%.
parsing	accuracy	Structured learning reduces bias and significantly improves parsing accuracy by 0.6%.
parsing	accuracy	 Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
predicate labels	precision	For predicate labels, the precision is over 95% and the recall is over 85% for all languages except for Hindi.
predicate labels	recall	For predicate labels, the precision is over 95% and the recall is over 85% for all languages except for Hindi.
classification	accuracy	To improve the classification accuracy, multiview approaches have been proposed.
classification	accuracy	 Table 1: The classification accuracy with the  Boolean and TF-IDF methods.
disfluency detection	recall (Rec.)	The evaluation metrics of disfluency detection are precision (Prec.), recall (Rec.) and f-score (F1).
disfluency detection	f-score (F1)	The evaluation metrics of disfluency detection are precision (Prec.), recall (Rec.) and f-score (F1).
parsing	accuracy	For parsing accuracy metrics, we use unlabeled attachment score (UAS) and labeled attachment score (LAS).
parsing	labeled attachment score (LAS)	For parsing accuracy metrics, we use unlabeled attachment score (UAS) and labeled attachment score (LAS).
Disfluency detection	accuracy	 Table 2: Disfluency detection and parsing accuracies on English Switchboard data. The accuracy of  M 3 N refers to the result reported in (Qian and Liu, 2013). H&J is the L2R parsing based joint model in  (Honnibal and Johnson, 2014). The results of M 3 N  † come from the experiments with toolkit released by  Qian and Liu (2013) on our pre-processed corpus.
parsing	accuracy	 Table 2: Disfluency detection and parsing accuracies on English Switchboard data. The accuracy of  M 3 N refers to the result reported in (Qian and Liu, 2013). H&J is the L2R parsing based joint model in  (Honnibal and Johnson, 2014). The results of M 3 N  † come from the experiments with toolkit released by  Qian and Liu (2013) on our pre-processed corpus.
relevance evaluation	Latent Dirichlet Allocation (LDA)	For better comparison, we implement three methods as baselines, which have been proved effective in relevance evaluation: (1) Vector Space Model (VSM), (2) Word Embedding (WE), and (3) Latent Dirichlet Allocation (LDA).
Keyphrase selection	citation	Keyphrase selection is often motivated by increasing article findability within a domain (thereby increasing citation).
scope resolution	accuracy	In addition, for scope resolution, we also report the accuracy in PCS (Percentage of Correct Scopes), within which a scope is fully correct if the output of scope resolution system and the correct scope have been matched exactly.
scope resolution	PCS (Percentage of Correct Scopes)	In addition, for scope resolution, we also report the accuracy in PCS (Percentage of Correct Scopes), within which a scope is fully correct if the output of scope resolution system and the correct scope have been matched exactly.
preposition correction task	precision	While Tetrault and Chodorow (2008) and reported a difference of 10% precision and 5% recall between their two individual annotators in their simplified preposition correction task, shows this difference can actually be as much as almost 15% precision (A1 vs A7) and 6% recall (A1 vs A3) in a more realistic full scale correction task.
preposition correction task	recall	While Tetrault and Chodorow (2008) and reported a difference of 10% precision and 5% recall between their two individual annotators in their simplified preposition correction task, shows this difference can actually be as much as almost 15% precision (A1 vs A7) and 6% recall (A1 vs A3) in a more realistic full scale correction task.
Distributed word representations	accuracy	Distributed word representations have gained much popularity lately because of their accuracy as semantic representations for words ().
MT evaluation	BLEU	Finally, we empirically show that syntacticallyand semantically-oriented embeddings can be incorporated to produce sizeable and cumulative gains in performance over a strong combination of pre-existing MT evaluation measures (BLEU, NIST, METEOR, and TER).
MT evaluation	METEOR	Finally, we empirically show that syntacticallyand semantically-oriented embeddings can be incorporated to produce sizeable and cumulative gains in performance over a strong combination of pre-existing MT evaluation measures (BLEU, NIST, METEOR, and TER).
MT evaluation	TER	Finally, we empirically show that syntacticallyand semantically-oriented embeddings can be incorporated to produce sizeable and cumulative gains in performance over a strong combination of pre-existing MT evaluation measures (BLEU, NIST, METEOR, and TER).
MT evaluation	BLEU	Section I of shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and METEOR ().
MT evaluation	TER	Section I of shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and METEOR ().
MT evaluation	METEOR	Section I of shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and METEOR ().
relevance classification	precision	If we forgo relevance classification entirely, we get a constant precision of 0.74 (green bars) as mentioned in Section 5.2; it is clear that the relevance classifier results in a significant advantage.
tweet normalization	precision	When we perform the tweet normalization methods, every error is both a false positive and false negative, so in the task, precision equals to recall.
tweet normalization	recall	When we perform the tweet normalization methods, every error is both a false positive and false negative, so in the task, precision equals to recall.
AMR parsing	accuracy	We observe that SRL++ is not the hard part of AMR parsing; rather, much of the difficulty in AMR is generating high accuracy concept subgraphs from the NER++ component.
sentiment classification of documents	accuracy	Experimental results show that: (1) UPNN outperforms baseline methods for sentiment classification of documents; (2) incorporating representations of users and products significantly improves classification accuracy.
sentiment classification	M AE	We use standard accuracy () to measure the overall sentiment classification performance, and use M AE and RM SE to measure the divergences between predicted sentiment ratings (pr) and ground truth ratings (gd).
sentiment classification	RM SE	We use standard accuracy () to measure the overall sentiment classification performance, and use M AE and RM SE to measure the divergences between predicted sentiment ratings (pr) and ground truth ratings (gd).
Sentiment classification	accuracy	 Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
Sentiment classification	Acc	 Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
Sentiment classification	MAE	 Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
Sentiment classification	RMSE	 Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
classification	accuracy	We report the averaged results of 10-fold cross-validation in terms of classification accuracy.
sentiment prediction task	FIX)	Using the setup that led to the best results in the sentiment prediction task (FIX), that is, fixing E and updating S, leads to lower accuracies than the baseline (TRAIN-ALL, s = 0).
sentiment prediction task	accuracies	Using the setup that led to the best results in the sentiment prediction task (FIX), that is, fixing E and updating S, leads to lower accuracies than the baseline (TRAIN-ALL, s = 0).
sentiment prediction task	TRAIN-ALL	Using the setup that led to the best results in the sentiment prediction task (FIX), that is, fixing E and updating S, leads to lower accuracies than the baseline (TRAIN-ALL, s = 0).
automatic speech recognition (ASR)	word error rate (WER)	In automatic speech recognition (ASR), the combination of transcription hypotheses produced by multiple systems usually leads to significant word error rate (WER) reductions compared to the output of each individual system.
ASR evaluation	WER	As usually done in ASR evaluation, performance results are measured in terms of WER.
prediction	recall	This metric is also relevant to the overall performance of prediction and recall: an irrelevant output will be aligned to a random input, thus being heavily penalized.
parsing	precision	We evaluate parsing precision and MWE identification on a test treebank and, more importantly, on a dataset built specifically to study the representation of our target constructions.
semantic role labeling	F 1 score	The proposed algorithm for semantic role labeling was mainly evaluated on CoNLL-2005 shared task and achieved F 1 score of 81.07.
parsing	accuracy	Compared with the Berkeley Parser, on average our "Pretrain-Finetune" model is 3.4 percentage points better in terms of parsing accuracy, and 3.2 percentage points better in terms of POS tagging accuracy.
Dependency parsers	accuracy	Dependency parsers are usually evaluated on attachment accuracy.
parsing	accuracy	• When applied to the re-ranking model for parsing, RCNN improve the accuracy of base parser to make accurate parsing decisions.
parsing	F-score	We show a substantial improvement in parsing performance compared to the baseline parser, both in full-sentence F-score and in incre-mental F-score.
SRL	accuracy	Although not the main focus of this paper, we also report full-sentence combined SRL accuracy (counting verb-predicates only).
parsing	accuracy	With a batch size of 5000, the parsing accuracy is about 25% higher than with a batch size of 1 (i.e. SGD).
Cross-lingual transfer dependency parsing	labeled attachment score  (LAS)	 Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
Null Instantiation Detection	precision	Null Instantiation Detection gives the performance of NI detection, which achieves 72.71%, 86.12% and 78.84% in precision, recall and F-score, respectively.
Null Instantiation Detection	recall	Null Instantiation Detection gives the performance of NI detection, which achieves 72.71%, 86.12% and 78.84% in precision, recall and F-score, respectively.
Null Instantiation Detection	F-score	Null Instantiation Detection gives the performance of NI detection, which achieves 72.71%, 86.12% and 78.84% in precision, recall and F-score, respectively.
NI detection	precision	Null Instantiation Detection gives the performance of NI detection, which achieves 72.71%, 86.12% and 78.84% in precision, recall and F-score, respectively.
NI detection	recall	Null Instantiation Detection gives the performance of NI detection, which achieves 72.71%, 86.12% and 78.84% in precision, recall and F-score, respectively.
NI detection	F-score	Null Instantiation Detection gives the performance of NI detection, which achieves 72.71%, 86.12% and 78.84% in precision, recall and F-score, respectively.
DNI identification	precision	It shows that DNI identification based on maximum entropy model achieves the performance of 67.86%, 69.93% and 68.88% in terms of precision, recall and F-score respectively, which are better than the results using SVM classifier, as well as the results employing Lei et al.'s method on our data.
DNI identification	recall	It shows that DNI identification based on maximum entropy model achieves the performance of 67.86%, 69.93% and 68.88% in terms of precision, recall and F-score respectively, which are better than the results using SVM classifier, as well as the results employing Lei et al.'s method on our data.
DNI identification	F-score	It shows that DNI identification based on maximum entropy model achieves the performance of 67.86%, 69.93% and 68.88% in terms of precision, recall and F-score respectively, which are better than the results using SVM classifier, as well as the results employing Lei et al.'s method on our data.
entity linking task	F1 score	In the entity linking task, our approach improves the F1 score by 10% over state-of-the-art results.
sentiment analysis	accuracy	Furthermore, when comparing to other sentiment analysis methods, the accuracy of our method was also better than LDA and JST based methods by 6.43% and 6.07%.
coreference resolution	B 3	The models are evaluated using three of the most popular metrics for coreference resolution: MUC, B 3 , and Entity-based CEAFE (CEAF φ 4 ).
coreference resolution	Entity-based CEAFE (CEAF φ 4 )	The models are evaluated using three of the most popular metrics for coreference resolution: MUC, B 3 , and Entity-based CEAFE (CEAF φ 4 ).
coreference analysis	precision	 Table 5: Absolute error counts from the coreference analysis  tool of Kummerfeld and Klein (2013). The upper set roughly  corresponds to the precision and the lower to the recall of the  coreference clusters produced by the model.
coreference analysis	recall	 Table 5: Absolute error counts from the coreference analysis  tool of Kummerfeld and Klein (2013). The upper set roughly  corresponds to the precision and the lower to the recall of the  coreference clusters produced by the model.
classification	accuracy	The first column of shows the classification accuracy of those models.
classification	accuracy	shows the classification accuracy of those three models on the larger datasets, i.e., the 20 Newsgroups dataset, and the Reuters-21578 dataset.
question answering	breadth	Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality.
parse	accuracy	In the following, we find a practical solution for improving both parse accuracy and search efficiency in our system.
NONE prediction	label ratio	One of the reasons for the improved accuracy of NONE prediction is the imbalanced label ratio of each disease/symptom.
Document classification	F 1 - scores	 Table 3: Document classification results (F 1 - scores)
segmentation	accuracy	Character types 5 × 10 3 5 × 10 3 Character tokens 1.8 × 10 6 4.1 × 10 6 The segmentation accuracy is evaluated by precision (P ), recall (R), F-score and R oov , the recall for out-of-vocabulary words.
segmentation	precision (P )	Character types 5 × 10 3 5 × 10 3 Character tokens 1.8 × 10 6 4.1 × 10 6 The segmentation accuracy is evaluated by precision (P ), recall (R), F-score and R oov , the recall for out-of-vocabulary words.
segmentation	recall (R)	Character types 5 × 10 3 5 × 10 3 Character tokens 1.8 × 10 6 4.1 × 10 6 The segmentation accuracy is evaluated by precision (P ), recall (R), F-score and R oov , the recall for out-of-vocabulary words.
segmentation	F-score	Character types 5 × 10 3 5 × 10 3 Character tokens 1.8 × 10 6 4.1 × 10 6 The segmentation accuracy is evaluated by precision (P ), recall (R), F-score and R oov , the recall for out-of-vocabulary words.
segmentation	R oov	Character types 5 × 10 3 5 × 10 3 Character tokens 1.8 × 10 6 4.1 × 10 6 The segmentation accuracy is evaluated by precision (P ), recall (R), F-score and R oov , the recall for out-of-vocabulary words.
segmentation	recall	Character types 5 × 10 3 5 × 10 3 Character tokens 1.8 × 10 6 4.1 × 10 6 The segmentation accuracy is evaluated by precision (P ), recall (R), F-score and R oov , the recall for out-of-vocabulary words.
segmentation	accuracies	Results are shown in: segmentation accuracies came close to 90% but do not go beyond.
POS  tagging	POS	 Table 6: Semi-supervised segmentation and POS  tagging accuracies. POS is measured by precision.
POS  tagging	precision	 Table 6: Semi-supervised segmentation and POS  tagging accuracies. POS is measured by precision.
POS tagging	accuracy	Søgaard (2011) apply tri-training to English POS tagging, boosting accuracy from 97.27% to 97.50%.
POS tagging	accuracy	Experiments show our approach can significantly improve POS tagging accuracy from 94.10% to 95.00% on CTB.
annotation conversion task	accuracy	Experiments on the newly annotated data show that our coupled model also works effectively on the annotation conversion task, improving conversion accuracy from 90.59% to 93.90% (+3.31%).
conversion	accuracy	Experiments on the newly annotated data show that our coupled model also works effectively on the annotation conversion task, improving conversion accuracy from 90.59% to 93.90% (+3.31%).
WSD	accuracy	 Table 3: WSD accuracy for different feature sets and systems.  Best result (excluding line 16) in each column in bold.
sentiment analysis task.	accuracy	The words about time, numeral words, pronoun and punctuation are removed as they are unrelated to the sentiment analysis task.: Three-way classification accuracy  In our HMMs component, the number of hidden states is 80.
relation classification	accuracy	By incorporating a mixture of labeled and unla-beled data, we are able to improve relation classification accuracy, reduce the need for annotated data, while still retaining the capacity to use labeled data to ensure that specific desired relations are learned.
SMT	OOVs	All SMT systems, even when trained on billionsentence-size parallel corpora, are prone to OOVs.
MT	MERT	In all the MT experiments, we use the cdec 9 toolkit (, and optimize parameters with MERT.
MT	BLEU	We train three systems for each MT setup; reported BLEU scores are averaged over systems.
translation	accuracy	Experiments in Japanese-to-English translation revealed that our simple method is comparable with, or superior to, state-of-the-art methods in translation accuracy.
machine translation	BLEU	Many machine translation metrics have been proposed in recent years, such as BLEU (), NIST), TER (), Meteor (Banerjee and) and its extensions, and the MEANT family (Lo and Wu, 2011), amongst others.
machine translation	TER	Many machine translation metrics have been proposed in recent years, such as BLEU (), NIST), TER (), Meteor (Banerjee and) and its extensions, and the MEANT family (Lo and Wu, 2011), amongst others.
classification	accuracy	For classification, we trained a softmax regression on the training set, and checked the accuracy on the testing set.
classification	accuracy	 Table 1: Comparison of classification accuracy on  the 20 Newsgroups dataset using RSMs.
sentiment and question classification tasks	accuracy	Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC.
word clustering	accuracy	We also propose a word clustering technique based on canonical correlation analysis (CCA) that is sensitive to multiple word senses, to further improve the accuracy within the proposed framework.
NER task	Accuracy	 Table 2: F1 Score for NER task and Accuracy for  POS task.
parsing	F1	In this paper, we show how directly capturing sequence information using a recurrent neural network leads to further accuracy improvements for both su-pertagging (up to 1.9%) and parsing (up to 1% F1), on CCGBank, Wikipedia and biomedical text.
parsing	accuracy	In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (, since the derivation space of the parser is determined by the supertagger, at both train-*All work was completed before the author joined ing and test time.
parsing	accuracy	In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (, since the derivation space of the parser is determined by the supertagger, at both train-*All work was completed before the author joined ing and test time.
tagging	accuracy	 Table 1: 1-best tagging accuracy and speed com- parison on CCGBank Section 00 with a single  CPU core (1,913 sentences), tagging time in secs.
tagging	speed com- parison	 Table 1: 1-best tagging accuracy and speed com- parison on CCGBank Section 00 with a single  CPU core (1,913 sentences), tagging time in secs.
parsing	accuracy	Experiments show that training with the dynamic oracle significantly improves parsing accuracy over the static oracle baseline on a wide range of treebanks.
parsing	accuracy	This extra robustness in training provides higher parsing accuracy.
parsing	accuracy	First, we are interested in how much parsing accuracy is needed for good results.
normalizing gold disorder mentions	accuracy	Results on normalizing gold disorder mentions are shown in, where performance is reported in terms of accuracy (i.e., the percentage of gold disorder mentions correctly normalized).
classification	accuracy	 Table 2: The classification accuracy of proposed  method against other models
generation of conversational responses	IBM BLEU	In tasks involving generation of conversational responses, ∆BLEU correlates reasonably with human judgments and outperforms sentence-level and IBM BLEU in terms of both Spearman's ρ and Kendall's τ .
Statistical Machine Translation (SMT)	BLEU	In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as BLEU () and METEOR () Although BLEU is not immune from criticism (e.g.,), its properties are well understood, BLEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research.
Statistical Machine Translation (SMT)	METEOR	In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as BLEU () and METEOR () Although BLEU is not immune from criticism (e.g.,), its properties are well understood, BLEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research.
Statistical Machine Translation (SMT)	BLEU	In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as BLEU () and METEOR () Although BLEU is not immune from criticism (e.g.,), its properties are well understood, BLEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research.
Statistical Machine Translation (SMT)	BLEU	In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as BLEU () and METEOR () Although BLEU is not immune from criticism (e.g.,), its properties are well understood, BLEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research.
paraphrase generation	BLEU score	To foster diversity in paraphrase generation, propose a metric called iBLEU in which the BLEU score is discounted by a BLEU score computed between the source and paraphrase.
paraphrase generation	BLEU score	To foster diversity in paraphrase generation, propose a metric called iBLEU in which the BLEU score is discounted by a BLEU score computed between the source and paraphrase.
image captioning tasks	BLEU	In image captioning tasks,, employ a variant of BLEU in which n-grams are weighted by tf ·idf.
translation	BLEU	No matter from which curriculum it is trained, the CDCM model significantly improves the translation quality on the overall test data (with gains of 1.0 BLEU points).
statistical machine translation (SMT)	accuracy	In statistical machine translation (SMT), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy.
Sentence	recall	Sentence fragments fared hardly better: of the thirteen teams, two scored a recall of 0.25 for correction and another scored 0.2; the rest did not achieve any recall.
Sentence	correction	Sentence fragments fared hardly better: of the thirteen teams, two scored a recall of 0.25 for correction and another scored 0.2; the rest did not achieve any recall.
Sentence	recall	Sentence fragments fared hardly better: of the thirteen teams, two scored a recall of 0.25 for correction and another scored 0.2; the rest did not achieve any recall.
Tense prediction	accuracy	 Table 2: Tense prediction accuracy.
answer  sentence selection task	Gra- dient boosted regression tree (GBDT)	 Table 2: Overview of our results on the answer  sentence selection task. Features are keywords  matching baseline score (BM25), and pooling val- ues of single-layer unidirectional LSTM (Single- Layer LSTM), single-Layer bidirectional LSTM  (Single-Layer BLSTM) and three-Layer stacked  BLSTM's (Three-Layer BLSTM) outputs. Gra- dient boosted regression tree (GBDT) method is  used to combine features.
classification task	F-score	For example, in the classification task, the best performing DM achieves an averaged F-score of only 0.37, contrasted with an average F-score of 0.73 achieved by the same model for taxonomic properties.
classification task	F-score	For example, in the classification task, the best performing DM achieves an averaged F-score of only 0.37, contrasted with an average F-score of 0.73 achieved by the same model for taxonomic properties.
text simplification	BLEU	Additionally, we point out several important differences between cross-lingual MT and monolingual MT used in text simplification , and show that BLEU is not a good measure of system performance in text simplification task.
AMR parsing	absolute	We report final AMR parsing results that show an improvement of 7% absolute in F 1 score over the best previously reported result.
AMR parsing	F 1 score	We report final AMR parsing results that show an improvement of 7% absolute in F 1 score over the best previously reported result.
generative dependency parsers	UAS	The model surpasses the accuracy and speed of previous generative dependency parsers, reaching 91.1% UAS.
generative transitionbased dependency parser	accuracy	A generative transitionbased dependency parser based on recurrent neural networks obtains high accuracy, but training and decoding is prohibitively expensive.
Tagging VM	Accuracy	 Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
CP resolution tasks	accuracy	Our extensive experiments over our two million clue dataset show that our approach highly improves the quality of the answer list, enabling the achievement of unprecedented results on the complete CP resolution tasks, i.e., accuracy of 99.17%.
CP resolution	accuracy	To measure the complete CP resolution task, we use the accuracy over the entire words filling a CP grid (one wrong letter causes the entire definition to be incorrect).
solver	accuracy	This means that the lists of candidate answers generated by our models help the solver, which in turn fills the grid with higher accuracy.
summarization	coverage	Because the summarization task is to generate summary scripts of multiple documents, the essential basis of its evaluation is coverage, that is, how much content in the original documents is included in the generated scripts.
KDS	FRAGMENT-AND-COMPOSE	SYSTEM DESIGN The KDS organization reflects our novel paradigm: FRAGMENT-AND-COMPOSE.
MT	METAL	The LRC's MT system, METAL (for Mechanical Translation and Analysis of Languages), is an advanced, 'third generation' system incorporating proven Natural Language Processing (NLP) techniques, both syntactic and semantic, and stands at the forefront of the MT research Frontier.
discourse generation	ease	The task of discourse generation is to produce multisentential text in natural language which (when heard or read) produces effects (informing, motivating, etc.) and impressions (conciseness, correctness, ease of reading, etc.) which are appropriate to a need or goal held by the creator of the text.
SOME COMPUTATIONAL ASPECTS	21ANTICS	SOME COMPUTATIONAL ASPECTS OF SITUATION S~21ANTICS
Minimising kinds of entities	simplicity	(Ibid., p. 17.) Minimising kinds of entities is not the only way to achieve simplicity in a theory.
universal recognition	Exp-Poly	Next I prove that the universal recognition problem for current GPSG theory is Exp-Poly hard, and assuredly intractable.
GPSG theory	Exp-Poly	Next I prove that the universal recognition problem for current GPSG theory is Exp-Poly hard, and assuredly intractable.
Morphological decomposition	coverage	Morphological decomposition is used to reduce the size of the dictionary and to increase coverage.
ASPECTS	JAPANESE	ASPECTS OF CLAUSE POLITENESS IN JAPANESE: AN EXTENDED INQUIRY SEMANTICS TREATMENT
attitude revision	BeI	If the right conditions for attitude revision obtained, the conclusion BeI(H,P) would follow from the above assumption.
AUTOMATED	INVERSION	AUTOMATED INVERSION OF LOGIC GRAMMARS FOR GENERATION
Tree Adjoining Grammars	O	An efficient parser for Tree Adjoining Grammars with a worst case time complexity of O(n 4 log n) is discussed.
parsing	accuracy	For further market extension or new market creation for natural language applications, parsing speed-up as well as improving parmng accuracy is required.
machine translation	soon	Second, it also increases the advantage of such applications as machine translation, document proofreading, automatic indexing, and soon, which are used to treat a large amount of documents.
parsing	accuracy	Third, it realizes parsing methods based on larger scale dictionary or knowledge database, which are necessary to improve parsing accuracy.
parsing	accuracy	Third, it realizes parsing methods based on larger scale dictionary or knowledge database, which are necessary to improve parsing accuracy.
Machine Translation	handcmt	Machine Translation requires handcmt~ and complicated large-scale knowledge.
parsing	precision level	By combining parsing and statistical techniques the addition of this third stage has raised the overall precision level of Xtract from 40% to 80% With a precision of 94%.
parsing	precision	By combining parsing and statistical techniques the addition of this third stage has raised the overall precision level of Xtract from 40% to 80% With a precision of 94%.
parsing	accuracy	The purpose of these experiments is to explore the impact of varying of Picky's parsing algorithm on parsing accuracy, efficiency, and robustness.
parsing	accuracy	The purpose of these experiments is to explore the impact of varying of Picky's parsing algorithm on parsing accuracy, efficiency, and robustness.
parsing	accuracy	 Table 2: 7~icky's parsing accuracy, categorized by the  phase which the parser reached in processing the test  sentences.
parsing	accuracy	The procedure that we have described is called constrained training, and it significantly improves the effectiveness of the parser, providing a dramatic reduction in computational requirements for parameter estimation as well as a modest improvement in parsing accuracy.
conversion	reduction ratio(RR)	The measure for accuracy of conversion was a reduction ratio(RR) of the homonym choice operations of a user.
parsing	accuracy rate	In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.
parsing	error	In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.
ASSIGNING A SEMANTIC SCOPE	OPERATORS	ASSIGNING A SEMANTIC SCOPE TO OPERATORS
name identification	precision	Now, on a set of 11 sentence fragments where they reported 100% recall and precision for name identification, we had 80% precision and 73% recall.
name identification	recall	Now, on a set of 11 sentence fragments where they reported 100% recall and precision for name identification, we had 80% precision and 73% recall.
machine translation	precision	 Table 2. For reference, results reported  for English/French generally fall between 96% and  98%. However, all of these numbers should be in- terpreted as highly domain dependent, with very  small sample size.  The above rates are for Type I errors. The  alternative measure of accuracy on Type II er- rors is useful for machine translation applications,  where the objective is to extract only 1-for-1 sen- tence pairs, and to discard all others. In this case,  we are interested in the proportion of 1-for-1 out- put pairs that are true 1-for-1 pairs. (In informa- tion retrieval terminology, this measures precision  whereas the above measures recall.) In the test  session, 438 1-for-1 pairs were output, of which  377, or 86.1%, were true matches. Again, how- ever, by discarding the introduction, the accuracy  rises to a surprising 96.3%.
machine translation	recall	 Table 2. For reference, results reported  for English/French generally fall between 96% and  98%. However, all of these numbers should be in- terpreted as highly domain dependent, with very  small sample size.  The above rates are for Type I errors. The  alternative measure of accuracy on Type II er- rors is useful for machine translation applications,  where the objective is to extract only 1-for-1 sen- tence pairs, and to discard all others. In this case,  we are interested in the proportion of 1-for-1 out- put pairs that are true 1-for-1 pairs. (In informa- tion retrieval terminology, this measures precision  whereas the above measures recall.) In the test  session, 438 1-for-1 pairs were output, of which  377, or 86.1%, were true matches. Again, how- ever, by discarding the introduction, the accuracy  rises to a surprising 96.3%.
machine translation	accuracy	 Table 2. For reference, results reported  for English/French generally fall between 96% and  98%. However, all of these numbers should be in- terpreted as highly domain dependent, with very  small sample size.  The above rates are for Type I errors. The  alternative measure of accuracy on Type II er- rors is useful for machine translation applications,  where the objective is to extract only 1-for-1 sen- tence pairs, and to discard all others. In this case,  we are interested in the proportion of 1-for-1 out- put pairs that are true 1-for-1 pairs. (In informa- tion retrieval terminology, this measures precision  whereas the above measures recall.) In the test  session, 438 1-for-1 pairs were output, of which  377, or 86.1%, were true matches. Again, how- ever, by discarding the introduction, the accuracy  rises to a surprising 96.3%.
parsing	error rate	This allows very much faster parsing and gives a lower error rate, at the price of a small loss in coverage.
parsing	coverage	This allows very much faster parsing and gives a lower error rate, at the price of a small loss in coverage.
TAL parsing	O	The first polynomial time algorithm for TAL parsing was proposed in 1986 and had a run time of O(n6).
TALs	O	The first polynomial time parsing algorithm for TALs was given by, which had a run time of O(n6), for an input of size n.
TALs	M	In this paper, we propose an O(M(n2)) time recognition algorithm for TALs, where M(k) is the time needed to multiply two k x k boolean matrices.
translation	accuracy	The translation accuracy is imperfect (about 86% percent weighted precision), which turns out to cause many of the bracketing errors.
translation	precision	The translation accuracy is imperfect (about 86% percent weighted precision), which turns out to cause many of the bracketing errors.
tagging	accuracy	Additionally, there is a slight but not significant improvement of tagging accuracy.
PES processing	recall	To quantify the effects of PES processing, we used the standard IR evaluation measures of recall and precision.
PES processing	precision	To quantify the effects of PES processing, we used the standard IR evaluation measures of recall and precision.
WSD	LEXAS	We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed.
WSD	LEXAS	To evaluate our WSD program, named LEXAS (LEXical Ambiguity-resolving _System), we tested it on a common data set involving the noun "interest" used by.
segmentation	accuracy	Our preliminary experiments show that the iterative procedure is able to improve the segmentation accuracy and more importantly, it can detect unseen words automatically.
segmentation	accuracy	Our preliminary experiments show that the iterative procedure is able to improve the segmentation accuracy and more importantly, it can detect unseen words automatically.
segmentation	accuracy	The results indicate that the impact on segmentation accuracy would be small.
Segmentation	accuracy	 Table 4: Segmentation of accuracy after one itera- tion
MT	accuracy	Two open questions, however, have yet to be satisfactorily answered before we can confidently build commercial MT systems based on these approaches: • Can the system be used for various domains without showing severe degradation of translation accuracy?
MT	O(IGIn6) 2 worst case time complexity	TAG-based MT (Abeill~, Schabes, and Joshi, 1990) 1 and pattern-based translation share many important properties for successful implementation in practical MT systems, namely: • The existence of a polynomial-time parsing algorithm • A capability for describing a larger domain of locality • Synchronization of the source and target language structures ing algorithm for TAGs has O(IGIn6) 2 worst case time complexity, and that the "patterns" in Maruyama's approach are merely context-free grammar (CFG) rules.
parsing	Viterbi	Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others.
classification	accuracy	Hence, once batch size increases pasta point, the input distribution has too little influence on which examples are selected, and hence classification accuracy decreases.
parsing	accuracy	The experiments show that involving larger fragments in the parsing process leads to higher accuracy.
disambiguating Turkish	recall	Our results for disambiguating Turkish indicate that using about 500 constraint rules and some additional simple statistics, we can attain a recall of 95-96~ and a precision of 94-95~ with about 1.01 parses per token.
disambiguating Turkish	precision	Our results for disambiguating Turkish indicate that using about 500 constraint rules and some additional simple statistics, we can attain a recall of 95-96~ and a precision of 94-95~ with about 1.01 parses per token.
tagging	accuracy	Although the variable-memory length approach remarkably reduces the number of parameters, tagging accuracy is only as good as conventional methods.
machine translation (SMT)	translation probability (TP)	The statistical approach to machine translation (SMT) can be understood as a word-by-word model consisting of two sub-models: a language model for generating a source text segment Sand a translation model for mapping S to its translation T. also recommend using a bilingual corpus to train the parameters of Pr(S I 73, translation probability (TP) in the translation model.
SIMR	error	The evaluation in Section 5 shows that SIMR's error rates are lower than those of other bitext mapping algorithms by an order of magnitude.
SIMR	accuracy	 Table 2: SIMR accuracy on different text genres in three language pairs.  language  number of  number of  RMS Error  pair  training TPCs  genre  test TPCs  in characters  French / English  598  parliamentary debates  CITI technical reports  other technical reports  court transcripts  U.N. annual report  I.L.O. report
segmentation	accuracy	demonstrate that segmentation accuracy is significantly higher when the lexicon is constructed using the same type of corpus as the corpus on which it is tested.
tagging	accuracy	The accuracy 2Automatically derived rules require less work than manually written ones but are unlikely to yield better results because they would consider relatively limited context and simple relations only. of this data has an impact on the tagging accuracy of both the HMM itself and the derived transducer.
Translation	possi- ble	 Table 4: Translation evaluation results (best possi- ble = 1.00, worst possible = 6.00)
KA from medium-sized corpora	LEXTER	The main idea to make KA from medium-sized corpora a feasible and efficient task is to perform a robust syntactic analysis (using LEXTER, see section 2) followed by a semi-automatic semantic analysis where automatic clustering techniques are used interactively by the knowledge engineer (see sections 3 and 4).
spelling correction	accuracy	Existing spelling correction techniques are limited in their scope and accuracy.
translational equivalence	precision	For instance, reports that his word-to-word model for translational equivalence produced lexicon entries with 99% precision and 46% recall when trained on 13 million words of the Hansard corpus, where recall was measured as the fraction of words from the bitext that were assigned some translation.
translational equivalence	recall	For instance, reports that his word-to-word model for translational equivalence produced lexicon entries with 99% precision and 46% recall when trained on 13 million words of the Hansard corpus, where recall was measured as the fraction of words from the bitext that were assigned some translation.
translational equivalence	recall	For instance, reports that his word-to-word model for translational equivalence produced lexicon entries with 99% precision and 46% recall when trained on 13 million words of the Hansard corpus, where recall was measured as the fraction of words from the bitext that were assigned some translation.
MT	recall	The results show the trade-off between improved lexical disambiguation provided by machine translation and extended synonym choice provided by dictionary term lookup and indicate that MT is superior to DTL only at medium and low recall levels.
Sentence alignment	recall	 Table 1: Sentence alignment results as recall and  precision.
Sentence alignment	precision	 Table 1: Sentence alignment results as recall and  precision.
name recognition	recall	In one of our retrieval tests, the combination of noun phrase syntax and name recognition improved recall by 18% at a fixed precision point.
name recognition	precision	In one of our retrieval tests, the combination of noun phrase syntax and name recognition improved recall by 18% at a fixed precision point.
parsing	accuracy	Moreover, the boosting version is shown to have significant advantages; 1) better parsing accuracy than its single-tree counterpart for any amount of training data and 2) no over-fitting to data for various iterations.
parsing	accuracy	We can use arbitrary numbers of the attributes which potentially increase parsing accuracy.
parsing	accuracy	In tile rest of this section, parsing accuracy refers only to precision; how many of tile system's output are correct in terms of the annotated corpus.
parsing	precision	In tile rest of this section, parsing accuracy refers only to precision; how many of tile system's output are correct in terms of the annotated corpus.
parsing	accuracy	• Tree pruning and parsing accuracy  This section reports experimental results on the boosting version of our parser.
parsing	accuracy	to 55%. and show the parsing accuracy when the number of training examples was increased.
parsing	accuracy	Next, we discuss how the number of iterations influences the parsing accuracy.
SSN	precision	Although only a small training set was used, an SSN achieved 63% precision and 69% recall on unlabeled constituents for previously unseen sentences.
SSN	recall	Although only a small training set was used, an SSN achieved 63% precision and 69% recall on unlabeled constituents for previously unseen sentences.
parsing	precision	From current corpus based statistical work on parsing, we know that sequences of part of speech tags contain enough information to achieve around 75% precision and recall on constituents (Charniak, forthcoming).
parsing	recall	From current corpus based statistical work on parsing, we know that sequences of part of speech tags contain enough information to achieve around 75% precision and recall on constituents (Charniak, forthcoming).
term extraction	TERMINO	Past studies have focused on building term extraction tools : TERMINO (David S.
segmentation	precision	Discussion: The segmentation algorithm using word repetition and relation weights in combination achieved mean precision and recall rates of 0.80 and 0.69, respectively.
segmentation	recall	Discussion: The segmentation algorithm using word repetition and relation weights in combination achieved mean precision and recall rates of 0.80 and 0.69, respectively.
topic recognition	accuracy	The average precision of the Predict-Support algorithm is also calculated (  The results of the topic recognition show that the model performs well, and we notice a considerable improvement in the accuracy rates compared to accuracy rates in speech act recognition cited in section 2 (modulo perplexity).
topic recognition	accuracy	The average precision of the Predict-Support algorithm is also calculated (  The results of the topic recognition show that the model performs well, and we notice a considerable improvement in the accuracy rates compared to accuracy rates in speech act recognition cited in section 2 (modulo perplexity).
detection of errors in speech recognition	correct	In integrating recognition and translation into a speech translation system, the development of the following processes is therefore important: (1) detection of errors in speech recognition results; (2) sorting of speech recognition results by means of error detection; (3) providing feedback to the recognition process and/or making the user speak again; (4) correct errors, etc.
sorting of speech recognition	correct	In integrating recognition and translation into a speech translation system, the development of the following processes is therefore important: (1) detection of errors in speech recognition results; (2) sorting of speech recognition results by means of error detection; (3) providing feedback to the recognition process and/or making the user speak again; (4) correct errors, etc.
translation	accuracy	This shows our method is effective in improving translation accuracy when syntactic information is not available.
Kana-to-Kanji conversion	accuracy	The key factor of Kana-to-Kanji conversion technology is how to raise the accuracy of the conversion through the homophone processing, since we have so many homophonic Kanjis.
parsing	recall	Combined with simple thresholding, this method can be used to give a 58% reduction in grammar size without significant change in parsing performance, and can produce a 69% reduction with some gain in recall , but a loss in precision.
parsing	precision	Combined with simple thresholding, this method can be used to give a 58% reduction in grammar size without significant change in parsing performance, and can produce a 69% reduction with some gain in recall , but a loss in precision.
parsing	recall	The other is that a 58% reduction can be achieved with no loss in parsing performance, whereas a 69% reduction yields again in recall, but a loss in precision.
parsing	precision	The other is that a 58% reduction can be achieved with no loss in parsing performance, whereas a 69% reduction yields again in recall, but a loss in precision.
DEP	BI	It is not surprising that DEP performs better than BI.
POS tagging	POS	POS tagging disambiguates them, i.e., it assigns to each word the correct POS in the context of the sentence.
sentence alignment	accuracy rates	We present a sentence alignment algorithm that, by taking advantage of previously annotated texts, obtains accuracy rates close to 100%.
word clustering	accuracy	This result tells us our word clustering method increases parsing accuracy considerably.
parsing	accuracy	This result tells us our word clustering method increases parsing accuracy considerably.
character recognition	accuracy	When the baseline character recognition accuracy is 90%, it achieves 97.4% character recognition accuracy.
character recognition	accuracy	When the baseline character recognition accuracy is 90%, it achieves 97.4% character recognition accuracy.
character recognition	accuracy	When the baseline character recognition accuracy is 90%, it achieves 97.4% character recognition accuracy.
MRDs	accuracy	Our aim is to use conventional MRDs, with no explicit semantic coding, to obtain a comparable accuracy.
parsing	accuracy	This first stage of our parsing procedure has a high degree of accuracy.
segmentation	accuracy	The preliminary experiment shows that the segmentation accuracy of our algorithm is acceptable.
SI	MI	It is clear that the SI strategy is better because it has a higher utility: at the end of 108 training sessions (dialogues), the utility of SI is estimated at .249 and the utility of MI is estimated at -0.174.
word sense disambiguation (WSD) of content words in general text	accuracy	This paper describes a system that integrates a number of partial sources of information to perform word sense disambiguation (WSD) of content words in general text at a high level of accuracy.
Translation	accuracy	Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model word-order variation (between English and Chinese) even though it eliminates large portions of the space of 1408 word alignments.
tagging	accuracy	Much research has been done to improve tagging accuracy using several different models and methods, including: hidden Markov models (HMMs),; rule-based systems,; memory-based systems (; maximum-entropy systems; path voting constraint systems; linear separator systems; and majority voting systems.
HMM tagger	accuracy	This paper describes various modifications to an HMM tagger that improve the performance to an accuracy comparable to or better than the best current single classifier taggers.
verb frame evaluation	precision	The most notable observation in verb frame evaluation is the decrease of precision of frame recognition in unlexicalized training from the second iteration onward.
tagging	accuracy	The model can achieve 96.6% tagging accuracy if unknown words are correctly segmented .
word segmentation	accuracy	To improve word segmentation accuracy,) used a single general purpose unknown word model, while ) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.
parsing	coverage	As in parsing, there is a tradeoff between coverage and spurious ambiguity in these systems: the more sophisticated the rules become, the more needless ambiguity they introduce.
information retrieval	precision	In information retrieval, expansion of words in a non-compositional expression can lead to dramatic decrease in precision without any gain in recall.
information retrieval	recall	In information retrieval, expansion of words in a non-compositional expression can lead to dramatic decrease in precision without any gain in recall.
Transfers	standard deviation	Conversely, the precision of Transfers is higher (83.8% on average) with a smaller standard deviation (between 69.0% and 100%).
parser	accuracy	In addition, we find that enhancement with non-local information not only improves parser accuracy, but also substantially improves the search efficiency.
chart parsing	O	The rationale is that soft selectional restrictions play a crucial role in disambiguation, i The chart parsing algorithms used by most of the above authors run in time O(nS), because bilexical grammars are enormous (the part of the grammar relevant to a length-n input has size O(n 2) in practice).
parsing	accuracy	This paper first describes a baseline approach, based on the parsing model of (Collins 97), which recovers dependencies with 72% accuracy.
parsing	accuracy (UAS)	 Table 3: Trade off between parsing accuracy (UAS)  and speed (words per second) with different pre- pruning settings. k denotes the number of candi- date heads of each word preserved for B&B parsing.   † Their speed is not directly comparable as they per- forms labeled parsing without pruning.
parsing	speed	 Table 3: Trade off between parsing accuracy (UAS)  and speed (words per second) with different pre- pruning settings. k denotes the number of candi- date heads of each word preserved for B&B parsing.   † Their speed is not directly comparable as they per- forms labeled parsing without pruning.
summarization	ROUGE	Stop-words in both documents and queries are removed using a stop-word list of 598 words, and the remaining words are stemmed by Porter Stemmer . As for the automatic evaluation of summarization, ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures, including ROUGE-1, ROUGE-2, and ROUGE-SU4 and their corresponding 95% confidence intervals, are used to evaluate the performance of the summaries.
topic modeling	soon	This is because, in S-sLDA model, topic modeling is determined simultaneously by various features including terms and other ones such as sentence length, sentence position and soon, which can contribute to summary quality.
classification	accuracy	Overall, we observe an improvement of up to 25% in classification accuracy over the seed lexicon without profiles.
TSG parsing	F-score	We propose an efficient Earley-based algorithm for incremental TSG parsing and report an F-score competitive with other incre-mental parsers.
TSG parsing	F-score	We propose an efficient Earley-based algorithm for incremental TSG parsing and report an F-score competitive with other incremental parsers.
parsing	MRP	When we compare the different parsing objectives of the ITSG parser, MRP is the best one, followed by MPP and MPD.
FST noise	EM	Like our system, this baseline trains FST noise models using EM in the V-expectation semiring.
MT	accuracy	In real MT systems, these problems are highly interdependent, a point we emphasized in class and at the end of each assignment-for example, that alignment is an exercise in parameter estimation for translation models, that model choice is a tradeoff between expressivity and efficient inference, and that optimal search does not guarantee optimal accuracy.
relation clustering	precision	The relation clustering uses only proper nouns, to improve precision (sparsity problems are partly offset by the large input corpus).
scene understanding	accuracy	In scene understanding, accuracy similarly improves by 16%.
POS tagging	accuracy	When evaluated on several NLP tasks, including POS tagging, NER and dependency parsing, this optimization method also outperforms other approaches in terms of prediction accuracy.
dependency parsing	accuracy	When evaluated on several NLP tasks, including POS tagging, NER and dependency parsing, this optimization method also outperforms other approaches in terms of prediction accuracy.
translation	BLEU-4	The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.
dependency parsing	accuracy	Much of the recent work on dependency parsing has been aimed at finding a good balance between accuracy and efficiency.
parsing of projective trees	O.n 5 / time complexity	Standard context-free parsing methods, when adapted to the parsing of projective trees, provide O.n 5 / time complexity.
POS tagging	UAS	Based on automatic POS tagging, our final model achieves a UAS of 87.23%, resulting in a significant improvement of the state of the art.
POS tagging	UAS	Based on automatic POS tagging, our final model achieves a UAS of 87.23% on the CoNLL data and 84.65% on CTB5, which yield relative error reductions of 18-24% over the best published results in the literature.
POS tagging	error	Based on automatic POS tagging, our final model achieves a UAS of 87.23% on the CoNLL data and 84.65% on CTB5, which yield relative error reductions of 18-24% over the best published results in the literature.
parsing	accuracy	First, developing features has been shown crucial to advancing parsing accuracy and a very rich feature set is carefully evaluated by.
jump prediction	accuracy	However, jump prediction becomes much harder: Top-3 accuracy of long jumps by distortion drops from 50.7% to 18.9% (backward) and from 66.0% to 52.3% (forward).
WaW reordering modeling	BLEU	 Table 5: Effects of WaW reordering modeling and early reordering pruning on translation quality, measured with  % BLEU, METEOR, and Kendall Reordering Score: regular (KRS) and verb-specific (KRS-V). Statistically significant  differences with respect to the baseline [B] are marked with 񮽙񮽙 at the p ≤ .05 level and 񮽙񮽙 at the p ≤ .10 level.  Decoding time is measured in milliseconds per input word.
WaW reordering modeling	METEOR	 Table 5: Effects of WaW reordering modeling and early reordering pruning on translation quality, measured with  % BLEU, METEOR, and Kendall Reordering Score: regular (KRS) and verb-specific (KRS-V). Statistically significant  differences with respect to the baseline [B] are marked with 񮽙񮽙 at the p ≤ .05 level and 񮽙񮽙 at the p ≤ .10 level.  Decoding time is measured in milliseconds per input word.
WaW reordering modeling	Kendall Reordering Score	 Table 5: Effects of WaW reordering modeling and early reordering pruning on translation quality, measured with  % BLEU, METEOR, and Kendall Reordering Score: regular (KRS) and verb-specific (KRS-V). Statistically significant  differences with respect to the baseline [B] are marked with 񮽙񮽙 at the p ≤ .05 level and 񮽙񮽙 at the p ≤ .10 level.  Decoding time is measured in milliseconds per input word.
metaphor recognition	accuracy	The metaphor recognition accuracy significantly outperforms the state-of-theart methods (Section 5).
metaphor recognition	Our	 Table 2: Type 1 metaphor recognition  Precision Recall  F1  KZ  13%  30% 18%  Our Approach  73%  66% 69%
metaphor recognition  Precision Recall	VH	 Table 3: Type 2/3 metaphor recognition  Precision Recall  F1  SA  23%  20% 21%  CP  50%  20% 26%  VH  11%  86% 20%  Our Approach  65%  52% 58%
parsing	accuracy	We showed experimentally that, using a dynamic oracle for the arc-eager transition system, a greedy parser can be trained to perform well also after incurring a mistake, thus alleviating the effect of error propagation and resulting in consistently better parsing accuracy.
parsing	accuracy	One striking demonstration of this tendency can be found in the CoNLL shared tasks on multilingual dependency parsing, organized in 2006 and 2007, where richly inflected languages clustered at the lower end of the scale with respect to parsing accuracy).
parsing	accuracy	Thus, show that integrating an external wide-coverage lexicon with a treebank-trained PCFG parser improves parsing accuracy for Modern Hebrew, which is inline with earlier studies of part-of-speech tagging for morphologically rich languages).
parsing	accuracy	The experiments show that joint prediction of morphology and syntax, rule-based morphological analyzers, and word clusters all contribute to improved parsing accuracy, leading to new state-ofthe-art results for all languages.
ALL	accuracy	The baselines are slightly better on ALL accuracy because they were designed for tagging in-domain data and use feature sets that have been found to work well on the source domain.
ALL	accuracy	However, ALL accuracy consistently drops in all six domains.
tagging	accuracy	The different behavior of unknown and known words suggests that training and optimizing two separate models -an approach used by SVMToolwould further increase tagging accuracy.
Tagging	accuracy	 Table 3: Tagging accuracy of four baselines and FLORS on the test sets.
Tagging	FLORS	 Table 3: Tagging accuracy of four baselines and FLORS on the test sets.
Tagging	accuracy	 Table 6: Tagging accuracy on bio dev. NNP→NN results  were obtained by replacing NNPs with NNs.
Tagging	accuracy	 Table 7: Tagging accuracy of different word representations on the dev sets. Line 1 corresponds to FLORS basic. n:  number of indicator words. A column's best result is bold.
Tagging	FLORS	 Table 7: Tagging accuracy of different word representations on the dev sets. Line 1 corresponds to FLORS basic. n:  number of indicator words. A column's best result is bold.
language identification	accuracy	Our main contributions are: (1) we present a method for identifying multilingual documents, the languages contained therein and the relative proportion of the document in each language; (2) we show that our method outperforms state-of-the-art methods for language identification in multilingual documents; (3) we show that our method is able to estimate the proportion of the document in each language to a high degree of accuracy; and (4) we show that our method is able to identify multilingual documents in real-world data.
parsing	accuracy	Conditioning on wider syntactic contexts than simply individual head-modifier relationships improves parsing accuracy in a wide variety of parsers and frameworks.
POS tagging	accuracy	For POS tagging, this method () yields the best performance to date; 91.6% tagging accuracy on a standard test dataset from the English Penn Treebank.
Word segmentation task	accuracy	 Table 2: Word segmentation task results, for our ex- pert and 3 non-expert participants. For each participant,  the resulting classifier accuracy [%] after supervision is  shown, along with the time [min] they needed. The unsu- pervised accuracy was 95.14%.
MWE identification	precision	Given that most tokens do not belong to an MWE, to evaluate MWE identification we adopt a precision/recall-based measure from the coreference resolution literature.
MWE identification	recall-based	Given that most tokens do not belong to an MWE, to evaluate MWE identification we adopt a precision/recall-based measure from the coreference resolution literature.
alignment	F 1 scores	In two intrinsic evaluations on alignment test data, our system achieves F 1 scores of 88-92%, demonstrating 1-3% absolute improvement over the previous best system.
tagging	accuracy	A disadvantage of this approach is that larger sets of lexical categories mean increased sparsity, decreasing tagging accuracy.
Adapting AP	error inflation	 Table 9: Adapting AP using error inflation. Models are
WSD	precision	Game players were drawn from a small group of  WSD performance is measured using the traditional precision and recall definitions and the F1 measure of the two; because all items are annotated, precision and recall are equivalent and we report performance as accuracy.
WSD	recall	Game players were drawn from a small group of  WSD performance is measured using the traditional precision and recall definitions and the F1 measure of the two; because all items are annotated, precision and recall are equivalent and we report performance as accuracy.
WSD	F1 measure	Game players were drawn from a small group of  WSD performance is measured using the traditional precision and recall definitions and the F1 measure of the two; because all items are annotated, precision and recall are equivalent and we report performance as accuracy.
WSD	precision	Game players were drawn from a small group of  WSD performance is measured using the traditional precision and recall definitions and the F1 measure of the two; because all items are annotated, precision and recall are equivalent and we report performance as accuracy.
WSD	recall	Game players were drawn from a small group of  WSD performance is measured using the traditional precision and recall definitions and the F1 measure of the two; because all items are annotated, precision and recall are equivalent and we report performance as accuracy.
WSD	accuracy	Game players were drawn from a small group of  WSD performance is measured using the traditional precision and recall definitions and the F1 measure of the two; because all items are annotated, precision and recall are equivalent and we report performance as accuracy.
classification prediction	precision (P)	 Table 3: Results for leave-one-out crossvalidation on the train- ing set for regression and classification prediction (both trained  on support vector machines). Classification results are the  weighted average of precision (P), recall (R) and F1-measure  over all four classes.
classification prediction	recall (R)	 Table 3: Results for leave-one-out crossvalidation on the train- ing set for regression and classification prediction (both trained  on support vector machines). Classification results are the  weighted average of precision (P), recall (R) and F1-measure  over all four classes.
classification prediction	F1-measure	 Table 3: Results for leave-one-out crossvalidation on the train- ing set for regression and classification prediction (both trained  on support vector machines). Classification results are the  weighted average of precision (P), recall (R) and F1-measure  over all four classes.
PP attachment	accuracy	However , state-of-the-art systems that employ such representations yield modest gains in PP attachment accuracy.
PP attachment	accuracy	This is achieved via a non-linear architecture that is discriminatively trained to maximize PP attachment accuracy.
PP attachment	accuracy	 Table 7: PP attachment accuracy when enriching  word vectors with part-of-speech tags of the candi- date head (POS) and the following word (NextPOS),  and with WordNet and VerbNet features.
PP attachment	accuracy	 Table 8: PP attachment accuracy of linear (standard)  and syntactic (dependency-based) word vectors.
Semantic role labeling	Statistical significance	 Table 3: Semantic role labeling results on the CoNLL 2005 data set. The method labels are training/inference. For  example, Local/DP means training with the local model, but inference with the dynamic program. Bold font indicates  the best system using a single model and a single parse, while the best scores among all systems are underlined.  Statistical significance was assessed for F1 and Comp. on the WSJ and Brown test sets with p < 0.01  *  and p < 0.05  *  *  .
Semantic role labeling	F1	 Table 3: Semantic role labeling results on the CoNLL 2005 data set. The method labels are training/inference. For  example, Local/DP means training with the local model, but inference with the dynamic program. Bold font indicates  the best system using a single model and a single parse, while the best scores among all systems are underlined.  Statistical significance was assessed for F1 and Comp. on the WSJ and Brown test sets with p < 0.01  *  and p < 0.05  *  *  .
Semantic role labeling	Comp	 Table 3: Semantic role labeling results on the CoNLL 2005 data set. The method labels are training/inference. For  example, Local/DP means training with the local model, but inference with the dynamic program. Bold font indicates  the best system using a single model and a single parse, while the best scores among all systems are underlined.  Statistical significance was assessed for F1 and Comp. on the WSJ and Brown test sets with p < 0.01  *  and p < 0.05  *  *  .
SPRITE	feature value α (P ) m	When estimating document-specific SPRITE parameters for held-out documents, we fix the feature value α (P ) m = 0 for that document.
grammar extraction	speed	Our experiments examine both grammar extraction and end-to-end translation, comparing quality, speed, and memory use.
translation	BLEU	While language models that operate on words linked through a dependency chain -called syntactic n-grams () -can improve translation, some of the improvement is invisible to an n-gram metric such as BLEU.
SMT	BLEU	 Table 3: Translation quality of English→German string-to-tree SMT system with different language models, with k- best batch MIRA optimization on BLEU and BLEU+HWCM f . Average of 3 optimization runs. bold: no other system  in same block is significantly better (p < 0.05); *: significantly better than same model with other MIRA objective  (p < 0.05). Higher scores are better for BLEU, HWCM f and METEOR; lower scores are better for TER.
SMT	BLEU	 Table 3: Translation quality of English→German string-to-tree SMT system with different language models, with k- best batch MIRA optimization on BLEU and BLEU+HWCM f . Average of 3 optimization runs. bold: no other system  in same block is significantly better (p < 0.05); *: significantly better than same model with other MIRA objective  (p < 0.05). Higher scores are better for BLEU, HWCM f and METEOR; lower scores are better for TER.
SMT	BLEU	 Table 3: Translation quality of English→German string-to-tree SMT system with different language models, with k- best batch MIRA optimization on BLEU and BLEU+HWCM f . Average of 3 optimization runs. bold: no other system  in same block is significantly better (p < 0.05); *: significantly better than same model with other MIRA objective  (p < 0.05). Higher scores are better for BLEU, HWCM f and METEOR; lower scores are better for TER.
SMT	METEOR	 Table 3: Translation quality of English→German string-to-tree SMT system with different language models, with k- best batch MIRA optimization on BLEU and BLEU+HWCM f . Average of 3 optimization runs. bold: no other system  in same block is significantly better (p < 0.05); *: significantly better than same model with other MIRA objective  (p < 0.05). Higher scores are better for BLEU, HWCM f and METEOR; lower scores are better for TER.
SMT	TER	 Table 3: Translation quality of English→German string-to-tree SMT system with different language models, with k- best batch MIRA optimization on BLEU and BLEU+HWCM f . Average of 3 optimization runs. bold: no other system  in same block is significantly better (p < 0.05); *: significantly better than same model with other MIRA objective  (p < 0.05). Higher scores are better for BLEU, HWCM f and METEOR; lower scores are better for TER.
SMT	RDLM	 Table 4: SMT output of baseline system and best system (RDLM tuned on BLEU+HWCM f ).
Division	accuracy	Division into these sets is based on an automatically derived accuracy metric.
FCT	ablation	Finally, we demonstrate the efficacy of different features in FCT) with an ablation study (Table 9).
question  answering task	MAP	 Table 2: Mean average precision for our question  answering task. The difference in MAP between  each pair of adjacent models is statistically signifi- cant (p < .05) via the sign test.
text classification	accuracy	Translated texts are distinctively different from original ones, to the extent that supervised text classification methods can distinguish between them with high accuracy.
clustering	accuracy	We also show that clustering accuracy remains stable even when the number of available chunks decreases dramatically and remains satisfactory when the chunk size is reduced.
dependency parsing	accuracy	Recent improvements to dependency parsing accuracy have been driven by higher-order features.
Candidate generation	recall	 Table 1: Candidate generation recall on the three evalu- ation datasets: the percentage of linkable gold mentions  for which the correct entity was in the set of candidates  generated by Plato. This is an upper bound on our in-KB  accuracy.
Candidate generation	accuracy	 Table 1: Candidate generation recall on the three evalu- ation datasets: the percentage of linkable gold mentions  for which the correct entity was in the set of candidates  generated by Plato. This is an upper bound on our in-KB  accuracy.
coreference resolution	MUC	We consider three widely used coreference resolution metrics: (1) MUC (, which measures how many gold (predicted) cluster merging operations are needed to recover each predicted (gold) cluster; (2) B 3 (Bagga and Baldwin, 1998), which measures the proportion of overlap between the predicted and gold clusters for each mention and computes the average scores; and (3) CEAF () (CEAF e ), which measures the best alignment of the gold-standard and predicted clusters.
coreference resolution	B 3	We consider three widely used coreference resolution metrics: (1) MUC (, which measures how many gold (predicted) cluster merging operations are needed to recover each predicted (gold) cluster; (2) B 3 (Bagga and Baldwin, 1998), which measures the proportion of overlap between the predicted and gold clusters for each mention and computes the average scores; and (3) CEAF () (CEAF e ), which measures the best alignment of the gold-standard and predicted clusters.
coreference resolution	CEAF	We consider three widely used coreference resolution metrics: (1) MUC (, which measures how many gold (predicted) cluster merging operations are needed to recover each predicted (gold) cluster; (2) B 3 (Bagga and Baldwin, 1998), which measures the proportion of overlap between the predicted and gold clusters for each mention and computes the average scores; and (3) CEAF () (CEAF e ), which measures the best alignment of the gold-standard and predicted clusters.
parsing	accuracy	 Table 1: Test set results for the standard fixed-order  parser (FIXEDORDER) and our new agenda-based parser  (AGENDAIL), which substantially reduces parsing time and the  number of parsing actions at no cost to accuracy.
SCFG parser	accuracy	First we note that baseline SCFG parser achieves reasonable accuracy on regular questions but when the same method is used with underspecified input, the system accuracy decreases significantly.
SCFG parser	accuracy	First we note that baseline SCFG parser achieves reasonable accuracy on regular questions but when the same method is used with underspecified input, the system accuracy decreases significantly.
parsing	accuracy	Of the paraphrases that were assigned "2" score, 91% corresponded to correct MRs, indicating that the subjective fluency of the paraphrase is a good indicator of parsing accuracy.
Basque	precision	We also presented two systems for Basque: one using all the features and another selecting good features (those above a threshold of 85% precision).
WSD	accuracy	For WSD, involving lexicographers tackles the twin obstacles to high accuracy: paucity of training data and insufficiently explicit dictionaries.
Information Retrieval	precision	The FScore is used in a similar fashion to Information Retrieval exercises, with precision and recall defined as the percentage of correctly "retrieved" examples fora cluster (divided by total cluster size), and recall as the percentage of correctly "retrieved" examples fora cluster (divided by total class size).
Information Retrieval	recall	The FScore is used in a similar fashion to Information Retrieval exercises, with precision and recall defined as the percentage of correctly "retrieved" examples fora cluster (divided by total cluster size), and recall as the percentage of correctly "retrieved" examples fora cluster (divided by total class size).
Information Retrieval	recall	The FScore is used in a similar fashion to Information Retrieval exercises, with precision and recall defined as the percentage of correctly "retrieved" examples fora cluster (divided by total cluster size), and recall as the percentage of correctly "retrieved" examples fora cluster (divided by total class size).
segmentation	POS	For Task 5, we made use of the segmentation and POS information provided by the task organiser.
metonymy resolution shared task at SemEval-2007	accuracy	Though the GYDER system has achieved the highest accuracy scores for the metonymy resolution shared task at SemEval-2007 in all six subtasks, we don't consider the results (72.80% accuracy for org, 84.36% for loc) particularly impressive , and argue that metonymy resolution needs more features.
WSD	accuracy	Optimal ensembling is a novel method for combining WSD systems and obtaining higher classification accuracy (presented more fully in).
classification	accuracy	Optimal ensembling is a novel method for combining WSD systems and obtaining higher classification accuracy (presented more fully in).
SRL	accuracy	In this section we discuss the setup and the results of the experiments carried out on the dataset of the SemEval2007 closed task on SRL.: SRL accuracy on the development test for the boundary detection (BD) and the complete SRL task (BD+RC) using the polynomial kernel alone (poly) or combined with a tree kernel function (poly + TK).
WSD	accuracy	It shows that considering all single words in the context as features did not improve the performance of WSD, while word category information of Chinese thesaurus improved the accuracy obviously.
WSI	recall	Recently, graph-based methods were employed in WSI to isolate highly infrequent senses of a target word.) and the adaptation of PageRank () have been shown to outperform the most frequent sense (MFS) baseline in terms of supervised recall, but they still fall short of supervised WSD systems.
role recognition	accuracy	For role recognition and labeling we used a standard evaluation set-up, i.e., accuracy for role labeling and precision, recall, F-Score for role recognition.
role recognition	precision	For role recognition and labeling we used a standard evaluation set-up, i.e., accuracy for role labeling and precision, recall, F-Score for role recognition.
role recognition	recall	For role recognition and labeling we used a standard evaluation set-up, i.e., accuracy for role labeling and precision, recall, F-Score for role recognition.
role recognition	F-Score	For role recognition and labeling we used a standard evaluation set-up, i.e., accuracy for role labeling and precision, recall, F-Score for role recognition.
coreference resolution	BART	There are various publicly available systems that perform coreference resolution, such as BART ( and GUITAR.
SemEval task	BLANC	Three runs have been submitted for the SemEval task 1 on Coreference Resolution (Recasens et al., 2010), optimizing Corry's performance for BLANC (Recasens and Hovy, in prep), MUC (Vilain et al., 1995) and CEAF (Luo, 2005).
Coreference Resolution	BLANC	Three runs have been submitted for the SemEval task 1 on Coreference Resolution (Recasens et al., 2010), optimizing Corry's performance for BLANC (Recasens and Hovy, in prep), MUC (Vilain et al., 1995) and CEAF (Luo, 2005).
SemEval task	BART	For the SemEval task 1 on Coreference Resolution , BART runs have been submitted for Ger-man, English, and Italian.
Coreference Resolution	BART	For the SemEval task 1 on Coreference Resolution , BART runs have been submitted for Ger-man, English, and Italian.
keyphrase extraction	SEER-LAB	First, We describe our keyphrase extraction system, SEER-LAB.
relation classification	F1	The system obtained good results in the estimation of en-tities' position (F1=98.3%) but a critically poor performance in relation classification (F1=26.6%), indicating that lexical and semantic information is essential in relation extraction.
relation classification	Precision	 Table 3: Official results (upper part), and results of the three relation classification methods when used in  a 10-fold cross-validation experiment on training data (lower part). Precision, recall, and F 1 are reported  as percentages for more convenience.
relation classification	recall	 Table 3: Official results (upper part), and results of the three relation classification methods when used in  a 10-fold cross-validation experiment on training data (lower part). Precision, recall, and F 1 are reported  as percentages for more convenience.
relation classification	F 1	 Table 3: Official results (upper part), and results of the three relation classification methods when used in  a 10-fold cross-validation experiment on training data (lower part). Precision, recall, and F 1 are reported  as percentages for more convenience.
SemEval-2 task 9	Spearman correlation	Our best system for SemEval-2 task 9 combines all three components and achieves a Spearman correlation of 0.432 with human rankings.
Semeval 2010 task 5	GROBID	This article describes the system realized for the Semeval 2010 task 5, based on GROBID's (GeneRation Of BIbilographic Data) module dedicated to key term extraction.
DNI identification	recall	but DNI identification suffers from low recall and INI identification from low precision.
DNI identification	precision	but DNI identification suffers from low recall and INI identification from low precision.
normalization	TIPSem	Finally, in normalization, which is the only attribute that is not annotated by a purely data-driven process, best system surpassed TIPSem in 0.20.
role recognition and labeling task	recall	The implementation of this system was quite minimal at the time of submission, allowing only an initial completion of the role recognition and labeling task, with recall of 0.112, precision of 0.670, and F-score of 0.192.
role recognition and labeling task	precision	The implementation of this system was quite minimal at the time of submission, allowing only an initial completion of the role recognition and labeling task, with recall of 0.112, precision of 0.670, and F-score of 0.192.
role recognition and labeling task	F-score	The implementation of this system was quite minimal at the time of submission, allowing only an initial completion of the role recognition and labeling task, with recall of 0.112, precision of 0.670, and F-score of 0.192.
crossframework parser evaluation	F-score	However, crossframework parser evaluation is a difficult problem: previous attempts to evaluate the C&C parser on grammatical relations and Penn Treebank-trees have also produced upper bounds between 80 and 90% F-score.
RACAI	accuracy	 Table 3: RACAI systems results (accuracy) on the  RACAI test set
RACAI	accuracy	 Table 4: RACAI systems results (accuracy) on the  SEMEVAL test set
WSD tasks	precision rate	Among various WSD tasks, the lexical sample task can achieve a precision rate more than 70% in Chinese, so can the all-words task in English, but currently no Chinese all-words WSD system is available.
WSD	precision rate	This study proposes an all-words WSD system conducted on a specific domain which can achieve a 55.9% precision rate.
SSA	correlation	Furthermore, we notice that SSA is the best performing system under these settings, with a correlation improvement of approximately 15%.
SSA	µ	In AG400, SSA reports a µ of 0.53 which represents a 4% improvement over the English SSA model (µ = 0.51) and a 16% improvement over the best knowledge-based system J&C (µ = 0.457).
Semantic role classification	accuracy	Semantic role classification accuracy for most languages other than English is constrained by the small amount of annotated data.
dependency parse-based	accuracies	We find that dependency parse-based features are the most effective, achieving accuracies similar to the leading semi-manual approaches and higher than any published fora corpus-based model.
identification of synonyms among close associates	TOEFL	The cognitive plausibility of computational models of word meaning has typically been tested using behavioural benchmarks, such as identification of synonyms among close associates (the TOEFL task for language learners, see e.g.; emulating elicited judgments of pairwise similarity (such as; judgments of category membership (e.g.; and word priming effects.
parsing	accuracy	These are important to achieve optimal parsing accuracy, and yet these are also the features which by their nature suffer from data-sparseness problems in the training data.
prediction of negation cues subtask	F-measure	It maybe seen from these tables that our system behaves quite well in the prediction of negation cues subtask, achieving around 90% F-measure in all data sets, and the second position in the competition.
scope prediction task	F-1	Performance in the scope prediction task, however, is around 60% F-1, and the same results are obtained if the correct prediction of cues is required (Scope (cue match)).
negation prediction task	F-1	These results affect the performance of the full negation prediction task, where we get 32.18% and 32.96% F-1, respectively.
negation cues	scope	In Section 3 we explain how we detect negation cues and scope.
negated event identification	precision	However, there is a sharp drop (almost 4 points lower F 1 score) in negated event identification, primarily due to lower precision.
Lexical Substitution	simplicity	It is similar in many respects to the task of Lexical Substitution in that it involves determining adequate substitutes in context, but in this case on the basis of a predefined criterion: simplicity.
STS	PARA	Additionally, STS differs from both TE and PARA in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), STS incorporates the notion of graded semantic similarity (e.g. a vehicle and a car are more similar than a wave and a car).
semantic role labeling (SRL)	TRAJEC-TOR	Examples of this type of spatial relationship include: (1) SpRL is a type of semantic role labeling (SRL), where the spatial INDICA-TOR is the predicate (or trigger) and the TRAJEC-TOR and LANDMARK are its two arguments.
SRL pipeline	TRA-JECTOR	Previous approaches to) have largely followed the commonly employed SRL pipeline: (1) find predicates (i.e., the INDICATOR), (2) recognize the predicate's syntactic constituents, and (3) classify the constituent's role (i.e., TRA-JECTOR, LANDMARK, or neither).
Consistency checking	TRAJECTOR	Consistency checking is not possible under a pipeline approach that classifies whether wall as the TRAJECTOR without any knowledge of its LANDMARK.
Lexical Substitution	simplicity	It is similar in many respects to the task of Lexical Substitution in that it involves elements of selectional preference on the basis of a central predefined criterion (simplicity in the current case), as well as sensitivity to context.
machine translation evaluation	BLEU	Measures from machine translation evaluation are often used to evaluate lexical level approaches (), including BLEU (), a metric based on word ngram hit rates.
SMTeuroparl	LO	Finally, in the SMTeuroparl, longer sentences are the norm and length seems to compromise the performance of LO.
Machine translation evaluation	BLEU	Machine translation evaluation metrics can be used to estimate lexical level similarity (), including BLEU (), a metric using word n-gram hit rates.
Semantic Textual Similarity task (STS)	ARE	Our participation to the 2013 Semantic Textual Similarity task (STS) ( 1 was focused on the CORE problem: GIVEN TWO SENTENCES, s 1 AND s 2 , QUANTIFIABLY INFORM ON HOW SIMI-LAR s 1 AND s 2 ARE.
metaphor identification	accuracy	It outperforms the previous approaches to metaphor identification both in terms of accuracy and coverage, as well as providing an interpretation for each identified expression.
metaphor identification	coverage	It outperforms the previous approaches to metaphor identification both in terms of accuracy and coverage, as well as providing an interpretation for each identified expression.
classification	accuracy	The most promising approach is the first one, results for which show that textual features improve classification accuracy.
classification	accuracy	While the other two models do not improve classification accuracy , they do investigate the role of the text and suggest possibilities for developing automatic answer scoring systems with less supervision needed from instructors.
semantic parsing	accuracy	Existing semantic parsing research has steadily improved accuracy on a few domains and their corresponding databases.
semantic parsing	accuracy	A growing body of research on semantic parsing has yielded consistent improvements in parsing accuracy.
parsing	accuracy	A growing body of research on semantic parsing has yielded consistent improvements in parsing accuracy.
noun categorization	TOEFL test	Examples of the former are noun categorization and the TOEFL test, examples of the latter are word sense disambiguation, metonymy resolution, and lexical substitution.
machine translation	precision score	Using only local context information and no linguistic analysis beyond lemmatization, our machine translation system surprisingly yields top precision score based on the best predictions.
WSD evaluation	Jaccard Index (JI)	For WSD evaluation, three measures are used: (1) Jaccard Index (JI), which measures the degree of overlap between the induced senses and the gold senses; (2) positionally-weighted Kendall's tau (KT:), which measures the correlation between the ranking of the induced senses and that of the gold senses; and (3) normalised discounted cumulative gain (NDCG), which
WSD evaluation	normalised discounted cumulative gain (NDCG)	For WSD evaluation, three measures are used: (1) Jaccard Index (JI), which measures the degree of overlap between the induced senses and the gold senses; (2) positionally-weighted Kendall's tau (KT:), which measures the correlation between the ranking of the induced senses and that of the gold senses; and (3) normalised discounted cumulative gain (NDCG), which
Modeling negation	Fscore	Modeling negation improves the Fscore by 0.72 points on the tweets set and 1.57 points on the SMS set.
DDI detection	F-score	The official results of the task show that our approach yields an F-score of 0.80 for DDI detection and an F-score of 0.65 for DDI detection and classification.
DDI detection	Parameter tun- ing	 Table 1: Comparison of results for DDI detection on the  training data using 5-fold cross validation. Parameter tun- ing is not done during these experiments.
SwatCS	accuracy	SwatCS: Combining simple classifiers with estimated accuracy
TSA	accuracy	Early research on TSA showed that the challenging vocabulary made it harder to accurately tag tweets; however, report on using a POS tagger for marking tweets, performing with almost 90% accuracy.
sentiment classification	F-Measure	Assessing the effectiveness of our application in sentiment classification, we obtained a 69% F-Measure for neutral and an average of 43% F-Measure for positive and negative using Tweets and SMS messages.
sentiment classification	F-Measure	Assessing the effectiveness of our application in sentiment classification, we obtained a 69% F-Measure for neutral and an average of 43% F-Measure for positive and negative using Tweets and SMS messages.
sentiment classification	accuracy	Our work harness the importance of discourse connectives like conjunctions, connectives, modals, conditionals etc and show that along with bag-ofwords model, it gives better sentiment classification accuracy.
sentiment polarity classification	F-measure	We tested our model for sentiment polarity classification on Twitter as well as SMS chat expressions, analyzed their F-measure scores and drew some interesting conclusions from them.
classification	accuracy	In future work, it will be interesting to see how balancing out the proportions of the three classes affects the classification accuracy.
Spatial Role Labeling (SpRL)	TRAJECTOR	In Computational Linguistics, the task of recognizing spatial information is known as Spatial Role Labeling (SpRL), as discussed in ( where three roles are labeled: the phrase "A man" refers to a TRAJECTOR, "a chair" to a LAND-MARK and they are related by the spatial expression "on" denoted as SPATIAL INDICATOR.
SemEval	8th	This paper presents CELI's participation in the SemEval The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge (Task7) and Cross-lingual Textual Entailment for Content Synchronization task (Task 8).
extract DDI from biomedical texts	F1 measure	proposed an approach to extract DDI from biomedical texts based on Shallow Linguistic (SL)) methods obtaining an F1 measure of 60,01%.
interaction detection	F 1	For interaction detection we achieved F 1 measures ranging from 73 % to almost 76 % depending on the run.
knowledge extraction	F 1 score	The knowledge extraction was performed on the sentence level and the best system achieved 65.74% F 1 score ().
SEM	acceptance rate	As in the previous editions of SEM, the acceptance rate was very competitive.
feature filtering	accuracy	We ran our feature filtering trials using the first development set, then obtained preliminary accuracy figures from our second development set.
SRL	precision (P)	To evaluate SRL performance, we use the CoNLL 2009 shared task scoring script 1 , which assumes a semantic dependency between the argument and predicate and the predicate and a dummy root node and then calculates the precision (P), recall (R) and F 1 of identification of these dependencies and classification (labelling) of them.
SRL	recall (R)	To evaluate SRL performance, we use the CoNLL 2009 shared task scoring script 1 , which assumes a semantic dependency between the argument and predicate and the predicate and a dummy root node and then calculates the precision (P), recall (R) and F 1 of identification of these dependencies and classification (labelling) of them.
SRL	F 1	To evaluate SRL performance, we use the CoNLL 2009 shared task scoring script 1 , which assumes a semantic dependency between the argument and predicate and the predicate and a dummy root node and then calculates the precision (P), recall (R) and F 1 of identification of these dependencies and classification (labelling) of them.
MT	BLEU	In addition to these task-specific metrics, standard MT metrics such as BLEU, NIST, METEOR and error rates such as WER, PER and TER, are included in the evaluation script as well.
MT	NIST	In addition to these task-specific metrics, standard MT metrics such as BLEU, NIST, METEOR and error rates such as WER, PER and TER, are included in the evaluation script as well.
MT	METEOR	In addition to these task-specific metrics, standard MT metrics such as BLEU, NIST, METEOR and error rates such as WER, PER and TER, are included in the evaluation script as well.
MT	error rates	In addition to these task-specific metrics, standard MT metrics such as BLEU, NIST, METEOR and error rates such as WER, PER and TER, are included in the evaluation script as well.
MT	WER	In addition to these task-specific metrics, standard MT metrics such as BLEU, NIST, METEOR and error rates such as WER, PER and TER, are included in the evaluation script as well.
MT	PER	In addition to these task-specific metrics, standard MT metrics such as BLEU, NIST, METEOR and error rates such as WER, PER and TER, are included in the evaluation script as well.
MT	TER	In addition to these task-specific metrics, standard MT metrics such as BLEU, NIST, METEOR and error rates such as WER, PER and TER, are included in the evaluation script as well.
STS	Pearson product-moment correlation	STS experiments have traditionally used Pearson product-moment correlation between the system scores and the GS scores, or, alternatively, Spearman rank order correlation.
Tagging	accuracy	Tagging accuracy is evaluated on 1-best and on max-5 best tagger outputs.
Sentiment Analysis in Twitter	accuracy	The SemEval-2014 task 9: Sentiment Analysis in Twitter ( provides a public dataset to be used to compare the accuracy of different approaches.
Sentiment Analysis in Twitter	accuracy	The SemEval-2014 task 9: Sentiment Analysis in Twitter ( provides a public dataset to be used to compare the accuracy of different approaches.
entity recognition task	precision overall submitted runs	We obtained an F-measure of 69.4% in the entity recognition task, achieving the second best precision overall submitted runs (81.3%), with above average recall (60.5%).
entity recognition task	recall	We obtained an F-measure of 69.4% in the entity recognition task, achieving the second best precision overall submitted runs (81.3%), with above average recall (60.5%).
normalization task	accuracy	In the normalization task, we achieved a strict accuracy of 53.1% and a relaxed accuracy of 87.0%.
normalization task	accuracy	In the normalization task, we achieved a strict accuracy of 53.1% and a relaxed accuracy of 87.0%.
entity recognition task	Recall	The common evaluation metrics were used to evaluate the entity recognition task, namely P recision = T P/(T P + F P ) and Recall = T P/(T P +F N ), where TP, FP and FN are respectively the number of true positive, false positive, and false negative annotations, and F measure = 2 × P recision × Recall/(P recision + Recall), the harmonic mean of precision and recall.
entity recognition task	FP	The common evaluation metrics were used to evaluate the entity recognition task, namely P recision = T P/(T P + F P ) and Recall = T P/(T P +F N ), where TP, FP and FN are respectively the number of true positive, false positive, and false negative annotations, and F measure = 2 × P recision × Recall/(P recision + Recall), the harmonic mean of precision and recall.
entity recognition task	F measure	The common evaluation metrics were used to evaluate the entity recognition task, namely P recision = T P/(T P + F P ) and Recall = T P/(T P +F N ), where TP, FP and FN are respectively the number of true positive, false positive, and false negative annotations, and F measure = 2 × P recision × Recall/(P recision + Recall), the harmonic mean of precision and recall.
entity recognition task	precision	The common evaluation metrics were used to evaluate the entity recognition task, namely P recision = T P/(T P + F P ) and Recall = T P/(T P +F N ), where TP, FP and FN are respectively the number of true positive, false positive, and false negative annotations, and F measure = 2 × P recision × Recall/(P recision + Recall), the harmonic mean of precision and recall.
entity recognition task	recall	The common evaluation metrics were used to evaluate the entity recognition task, namely P recision = T P/(T P + F P ) and Recall = T P/(T P +F N ), where TP, FP and FN are respectively the number of true positive, false positive, and false negative annotations, and F measure = 2 × P recision × Recall/(P recision + Recall), the harmonic mean of precision and recall.
normalization task	accuracy	For the normalization task, the metric used to evaluate performance was accuracy.
Aspect term extraction	F-measure	 Table 3: Aspect term extraction results  (F-measure).  Laptop  Restaurant  Best  0.7455  0.8401  Blinov  0.5207  0.7121  Baseline  0.3564  0.4715
Aspect term polarity detection	Accuracy	 Table 5: Aspect term polarity detection results  (Accuracy).  Laptop  Restaurant  Best  0.7049  0.8095  Blinov  0.5229  0.6358  Baseline  0.5107  0.6428
translation	accuracy	We show that translation systems can address the L2 writing assistant task, reaching out-of-five word-based accuracy above 80 percent for 3 out of 4 language pairs.
STS	soon	Examples of tasks that benefit from STS include text summarization, machine translation, question answering, short answer scoring, and soon.
machine translation	soon	Examples of tasks that benefit from STS include text summarization, machine translation, question answering, short answer scoring, and soon.
question answering	soon	Examples of tasks that benefit from STS include text summarization, machine translation, question answering, short answer scoring, and soon.
SVM	accuracy	However we report the results of SVM as it produced the highest accuracy with respect to this particular feature set.
message polarity disambiguation	precision	Evaluation shows that for message polarity disambiguation we obtain the average precision, recall and F-score values of 52.19%, 55.46% and 53.77%, respectively.
message polarity disambiguation	recall	Evaluation shows that for message polarity disambiguation we obtain the average precision, recall and F-score values of 52.19%, 55.46% and 53.77%, respectively.
message polarity disambiguation	F-score	Evaluation shows that for message polarity disambiguation we obtain the average precision, recall and F-score values of 52.19%, 55.46% and 53.77%, respectively.
message polarity classification	precision	For message polarity classification we obtain the precision, recall and FScore values of 50.68%, 49.73% and 66.39%, respectively.
message polarity classification	recall	For message polarity classification we obtain the precision, recall and FScore values of 50.68%, 49.73% and 66.39%, respectively.
message polarity classification	FScore	For message polarity classification we obtain the precision, recall and FScore values of 50.68%, 49.73% and 66.39%, respectively.
message polarity disambiguation	F-score	For message polarity disambiguation we obtain the highest F-score of 77.04% for the SMS datatype in Task-A. The system shows the F-scores of 76.03%, 70.91%, 72.25% and 66.35% for LiveJournal2014, Twitter2013, Twitter2014 and Twitter2014sarcasm, respectively.
message polarity disambiguation	F-scores	For message polarity disambiguation we obtain the highest F-score of 77.04% for the SMS datatype in Task-A. The system shows the F-scores of 76.03%, 70.91%, 72.25% and 66.35% for LiveJournal2014, Twitter2013, Twitter2014 and Twitter2014sarcasm, respectively.
classifying negative sentiment	precision	They showed that semantic features produce better recall and F-score when classifying negative sentiment, and better precision with lower recall and F-score in positive sentiment classification.
classifying negative sentiment	recall	They showed that semantic features produce better recall and F-score when classifying negative sentiment, and better precision with lower recall and F-score in positive sentiment classification.
classifying negative sentiment	F-score	They showed that semantic features produce better recall and F-score when classifying negative sentiment, and better precision with lower recall and F-score in positive sentiment classification.
parsing	accuracy	• Parser ensemble is very useful to boost the parsing accuracy.
SemEval 2014	echo	In this context, Task 7 of SemEval 2014, which is a continuation of the ShARe/CLEF eHealth 2013 task (, provides a testbed to evaluate systems that automatically tag and normalize mentions of diseases, signs and symptoms in clinical records, which include discharge summaries and echo, radiology and ECG reports.
message polarity classification	accuracy	Our submission was developed in less than two days and achieved an F 1 score of 77.26% for contextual polarity disambiguation and 55.47% for message polarity classification, which shows that rapid prototyping of sentiment analysis systems with reasonable accuracy is possible.
sentiment analysis	accuracy	Our submission was developed in less than two days and achieved an F 1 score of 77.26% for contextual polarity disambiguation and 55.47% for message polarity classification, which shows that rapid prototyping of sentiment analysis systems with reasonable accuracy is possible.
sentiment analysis	accuracy	Although our scores are about 10-20% behind the top-scoring systems, we show that it is possible to develop sentiment analysis systems via rapid prototyping with reasonable accuracy in a very short amount of time.
aspect category detection	Boosting	The sub-task of aspect category detection obtains the best result when applying the Boosting method on the Maximum Entropy model, with the precision of 0.869 for Restaurants.
aspect category detection	precision	The sub-task of aspect category detection obtains the best result when applying the Boosting method on the Maximum Entropy model, with the precision of 0.869 for Restaurants.
semantic similarity	similarity score	A general system for semantic similarity aiming at being applicable in such abroad scope has to be able to adapt to the use case at hand, because different use cases might, for example, require different similarity scales: For one application, two texts dealing roughly with the same topic should get a high similarity score, whereas for another application being able to distinguish between subtle differences in meaning might be important.
SemEval Task 3	Pearson (0.58 average correlation)	Considering the overall system performance, SimCompass is remarkably versatile, ranking among the top at each lexical level, and taking the first place in the SemEval Task 3 overall evaluation with respect to both Pearson (0.58 average correlation) and Spearman correlations.
SA task	accuracy	These two approaches are used for SA task but the third one is specific for Twitter or social content, the social approach exploits social network properties and data for enhancing the accuracy of the classification (Speriosu,.
predicting semantic similarity	accuracy	In particular, we were interested in finding out whether paraphrasing techniques could increase the accuracy of our system, whether meaning representations used for textual entailment are useful for predicting semantic similarity, and conversely, whether similarity features could be used to boost accuracy of recognizing textual entailment.
SemEval-2014 Shared Task	F 1-score	The system was used to participate in the SemEval-2014 Shared Task on broad-coverage semantic dependency parsing and it was ranked third with an overall F 1-score of 80.49%.
RTE task	Pearson's correlation	Our best system achieves 73% accuracy on the RTE task, and a Pearson's correlation of 0.71 on the STS task.
entity recognition	accuracy	Our approaches achieved top rank in both subtasks, with the best F measure of 0.813 for entity recognition and the best accuracy of 74.1% for encoding, indicating the proposed approaches are promising.
entity encoding	accuracy	For entity encoding, all participating systems were evaluated using accuracy, in "strict" and "relaxed" modes, as defined in).
SemEval 2014 Task 7	precision	show the best performance of our systems in the SemEval 2014 Task 7 as reported by the organizers, where "P", "R", "F" denote precision, recall and F-measure respectively.
SemEval 2014 Task 7	recall	show the best performance of our systems in the SemEval 2014 Task 7 as reported by the organizers, where "P", "R", "F" denote precision, recall and F-measure respectively.
SemEval 2014 Task 7	F-measure	show the best performance of our systems in the SemEval 2014 Task 7 as reported by the organizers, where "P", "R", "F" denote precision, recall and F-measure respectively.
encoding	accuracy	For encoding, our system achieved an accuracy of 0.741 by ensemble DM under "strict" criterion and was again ranked first in the challenge..
SemEval 2014 Task 7	Precision (P)	 Table 3: SemEval 2014 Task 7 evaluation results for our system. Precision (P), recall (R) and F-measure (F) were measured
SemEval 2014 Task 7	recall (R)	 Table 3: SemEval 2014 Task 7 evaluation results for our system. Precision (P), recall (R) and F-measure (F) were measured
SemEval 2014 Task 7	F-measure (F)	 Table 3: SemEval 2014 Task 7 evaluation results for our system. Precision (P), recall (R) and F-measure (F) were measured
SRL	recall	Implicit SRL systems typically trade off recall against precision by restricting the search space.
SRL	precision	Implicit SRL systems typically trade off recall against precision by restricting the search space.
Word Sense Disambiguation (WSD)	accuracy	The utility of Word Sense Disambiguation (WSD) depends on the accuracy and on how useful the sense distinctions are.
IMS	Most Frequent Sense (MFS)	We compare results obtained with IMS against the Most Frequent Sense (MFS), which was estimated using the training corpus.
negation scope span detection	Percentage of Correct Scopes (PCS)	The most commonly used metric to evaluate negation scope span detection is Percentage of Correct Scopes (PCS).
document classification	accuracy	The major contribution of this paper is in showing that document classification accuracy can be improved over a range of datasets using automaticallyinduced implicit semantic inter-document links, using collective classification.
TE recognition	accuracy	The task is binary TE recognition, with baseline of 50% accuracy (balanced classes).
SRL annotation	precision	This is very important, because when we ease SRL annotation, we increase the likelihood of obtaining a high inter-annotator agreement and, consequently, the likelihood of obtaining a good precision for machine learning classifiers for the task.
entity recognition task	F-measure	The second stage was evaluated as a named entity recognition task () for which we have obtained an F-measure of 79.44.
mention detection	PERCENT	Thus, we have modified the mention detection module, improving the treatment of coordinations and eliminating numeric named entities (PERCENT, MONEY etc).
mention detection	MONEY	Thus, we have modified the mention detection module, improving the treatment of coordinations and eliminating numeric named entities (PERCENT, MONEY etc).
resolution	precision	For resolution, precision is computed as the fraction of predicted antecedents that are correct, and recall as the fraction of gold antecedents that are correctly predicted.
resolution	recall	For resolution, precision is computed as the fraction of predicted antecedents that are correct, and recall as the fraction of gold antecedents that are correctly predicted.
Classification evaluation	TWOSTAGE	 Table 3: Classification evaluation (TWOSTAGE corresponds to our system).
Resolution evaluation	TWOSTAGE	 Table 4: Resolution evaluation with oracle classification (TWOSTAGE corresponds to our system).
Resolution evaluation	TWOSTAGE	 Table 5: Resolution evaluation with system classification (TWOSTAGE corresponds to our system).
coreference resolution evaluation	TWOSTAGE	 Table 6: End-to-end coreference resolution evaluation (TWOSTAGE corresponds to our system). All differences be- tween the baseline system and TWOSTAGE are significant at the 1% level except for the B 3 F1 differences.
coreference resolution evaluation	B 3 F1	 Table 6: End-to-end coreference resolution evaluation (TWOSTAGE corresponds to our system). All differences be- tween the baseline system and TWOSTAGE are significant at the 1% level except for the B 3 F1 differences.
SVM prediction	accuracy	 Table 6: SVM prediction accuracy.
Semantic Similarity	Pearson's r	This effort placed first in Semantic Similarity and second in Paraphrase Identification with scores of Pearson's r of 61.9%, F1 of 66.7%, and maxF1 of 72.4%.
Semantic Similarity	F1	This effort placed first in Semantic Similarity and second in Paraphrase Identification with scores of Pearson's r of 61.9%, F1 of 66.7%, and maxF1 of 72.4%.
Semantic Similarity	maxF1	This effort placed first in Semantic Similarity and second in Paraphrase Identification with scores of Pearson's r of 61.9%, F1 of 66.7%, and maxF1 of 72.4%.
Paraphrase Identification	Pearson's r	This effort placed first in Semantic Similarity and second in Paraphrase Identification with scores of Pearson's r of 61.9%, F1 of 66.7%, and maxF1 of 72.4%.
Paraphrase Identification	F1	This effort placed first in Semantic Similarity and second in Paraphrase Identification with scores of Pearson's r of 61.9%, F1 of 66.7%, and maxF1 of 72.4%.
Paraphrase Identification	maxF1	This effort placed first in Semantic Similarity and second in Paraphrase Identification with scores of Pearson's r of 61.9%, F1 of 66.7%, and maxF1 of 72.4%.
machine translation evaluation	accuracy	We are interested in finding out whether semantic similarity, textual entailment and machine translation evaluation techniques could increase the accuracy of our system.
word alignment	METEOR	Thus, we use two metrics for word alignment in our system, the METEOR and BLEU.
word alignment	BLEU	Thus, we use two metrics for word alignment in our system, the METEOR and BLEU.
paraphrase identification	Pear-son correlation	We achieve an F1 score of 0.45 on paraphrase identification and a Pear-son correlation of 0.303 on computing semantic similarity.
paraphrase detection	recall	sion value for paraphrase detection increases significantly while the recall value is actually falling.
SemEval-2015 Task 1	F1-score	The official evaluation metrics for SemEval-2015 Task 1 are F1-score for paraphrase identification and Pearson correlation for the semantic similarity scores.
SemEval-2015 Task 1	Pearson correlation	The official evaluation metrics for SemEval-2015 Task 1 are F1-score for paraphrase identification and Pearson correlation for the semantic similarity scores.
paraphrase identification	Pearson correlation	The official evaluation metrics for SemEval-2015 Task 1 are F1-score for paraphrase identification and Pearson correlation for the semantic similarity scores.
PI task	accuracy	Although the PI task aims to identify sentences that are semantically equivalent, a number of researchers have shown that classifiers trained on lexical overlap features may achieve relatively high accuracy.
interpretable task	F1	For the interpretable task, we submitted a modified version of Run1 achieving mean F1 0.846, 0.461, 0.722, and 0.44 for alignment, type, score, and score with type respectively.
SemEval 2015 Shared Task 2	STS	We present in this paper our system developed for SemEval 2015 Shared Task 2 (2a-En-glish Semantic Textual Similarity, STS, and 2c-Interpretable Similarity) and the results of the submitted runs.
machine translation evaluation	accuracy	We are interested in finding out whether similarity, machine translation evaluation metrics and task specific techniques could increase the accuracy of our system.
Answer Selection	YES	In this context, we organized SemEval-2015 Task 3 on Answer Selection in cQA, which included two subtasks: (a) classifying answers as good, bad, or potentially relevant with respect to the question, and (b) answering a YES/NO question with yes, no, or unsure, based on the list of all answers.
summarization task	accuracy	For the extreme summarization task, the best accuracy is 72%.
answer selection	YES	We evaluate our approach on the answer selection and YES/NO answer inference tasks.
span detection	accuracy	The best system yielded a combined relaxed F (for span detection) and overall weighted accuracy of 80.8.
CUI normalization	accuracy	Note that unlike in Task 1, this F score does not consider CUI normalization, as this is captured through the accuracy in the template filling task.
entity recognition	F-measure	Evaluation on the test data set showed that our system achieved the F-measure of 0.898 for entity recognition and the F-measure of 0.794 for UMLS CUI.
disorder identification)	F Measure	It was ranked 12th in Task 1 (disorder identification) and 6th in Task 2b (disorder identification and slot filling) with a weighted F Measure of 0.711.
sentiment polarity	accuracy	To evaluate sentiment polarity (Slot 3) in Phase B, we used accuracy.
sentiment polarity detection	accuracy	Slot 3: To evaluate sentiment polarity detection in Phase B, we calculated the accuracy of each system, defined as the number of correctly predicted polarity labels of aspect categories, divided by the total number of aspect categories.
sentiment analysis	POSITIVE	One of the most popular settings for carrying out sentiment analysis is at the sentence level or over individual micro-blog posts, using the simple threelabel class set of POSITIVE, NEGATIVE and NEU-TRAL ().
SA task	accuracy	These two approaches are used for SA task but the third one is specific for Twitter or social content, the social approach exploits social network properties and data for enhancing the accuracy of the classification.
sentiment analysis of tweets	F-score	Due to this, sentiment analysis of tweets is gaining importance across a number of domains such as ecommerce (), politics (; Wang et We average the positive and negative F-measures to get the F-score, which is the evaluation metric for this al., 2012), health and psychology; Harman, ; Harman, ), multimodality (, crowd validation (, and even intelligence and surveillance.) is an international shared-task competition that aims to promote research in sentiment analysis of tweets by providing annotated tweets for training, development and testing.
sentiment analysis	accuracy	This is especially the casein the field of sentiment analysis where the presence of figurative language in subjective text can significantly undermine the classification accuracy.
sentiment analysis	accuracy	Ina sentiment analysis setting, the presence in a text of figurative language devices, such as for instance irony, can work as an unexpected polarity reverser, by undermining the accuracy of the systems (.
sentiment polarity	Z score	A Logistic Regression model with a weighting schema of positive and negative labels have been used for sentiment polarity; several groups of features (lexical, syntactic , semantic, lexicon and Z score) are extracted.
sentiment polarity	accuracy	In sentiment polarity, sentiue's result accuracy was approximately 79%, reaching the best score in 2 of the 3 domains.
time anchors assignment	accurracy	In we also provide the evaluation of time anchors assignment in terms of accurracy.
time anchors assignment	HEI-DELTOUL	In evaluating the time anchors assignment (see), we observed that HEI-DELTOUL and GPLSIUA systems performed better on the "Airbus" and "Stock" corpora than on "GM".
time anchors assignment	GPLSIUA	In evaluating the time anchors assignment (see), we observed that HEI-DELTOUL and GPLSIUA systems performed better on the "Airbus" and "Stock" corpora than on "GM".
TIMEX3 span (TS) and attribute (TA) classification	EVENT span (ES) and attribute (EA) classification	The 2015 Clinical TempEval consisted of six subtasks related to these core concepts: TIMEX3 span (TS) and attribute (TA) classification, EVENT span (ES) and attribute (EA) classification, document creation time (DR) and narrative container (CR) rela-tions.
SVM classification	accuracy	illustrates how SVM classification accuracy varies with feature set size.
classification	accuracy	 Table 2: Effect of feature set size (|F |) on classification  accuracy. (Char+POS+Google features)
Classification	accuracy	 Table 3: Classification accuracy of various feature sets,  using 10-fold cross-validiation on the training data set.
IR	recall	Also in IR and IE tasks, difference in the correct sense assignment will surely degrade recall and precision of the systems.
IR	precision	Also in IR and IE tasks, difference in the correct sense assignment will surely degrade recall and precision of the systems.
IE tasks	recall	Also in IR and IE tasks, difference in the correct sense assignment will surely degrade recall and precision of the systems.
IE tasks	precision	Also in IR and IE tasks, difference in the correct sense assignment will surely degrade recall and precision of the systems.
summarization	accuracy	We have developed an improved task-based evaluation method of summarization, the accuracy of which is increased by specifying the details of the task including background stories, and by assigning ten subjects per summary sample.
parsing	accuracy	This approach reports experimental results, using the SRI Core Language Engine,, in the ATIS domain, of more than a 3-fold speedup at a cost of 5% in grammatical coverage, the latter which is compensated by an increase in parsing accuracy.
TRI-GRAM tagging	BRILL	This paper expands on an empirical comparison (van Halteren et al., 1998) in which TRI-GRAM tagging, BRILL tagging, MAXIMUM EN-TROPY and MEMORY BASED tagging were compared on the LOB corpus.
WSD	accuracy	Although some published works include the comparison between some alternative algorithms), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not.
generalization	accuracy	The effects of a distributed class representation on generalization accuracy were measured using an experimental matrix based on 5 linguistic datasets, and 8 experimental conditions, addressing feature selection-based ECOC vs. voting-based ECOC, MVDM, values of k larger than 1, and dichotomizer weighting.
Chunking	Accuracy	 Table 1: Chunking results using specialized ILM  (Accuracy= 93.79%)
information retrieval	accuracy	For example, in information retrieval applications, where large corpora of texts need to be searched efficiently, it is useful to have information about the language style used in each text, to improve the accuracy of the search).
IR	precision	The effectiveness of IR systems is usually measured in terms of precision, the percentage of retrieved documents that are relevant, and recall, the percentage of relevant documents that are retrieved.
IR	recall	The effectiveness of IR systems is usually measured in terms of precision, the percentage of retrieved documents that are relevant, and recall, the percentage of relevant documents that are retrieved.
IE	precision	A domain expert and a linguist with knowledge of the IE system will generally be required to specify the task, manually mark-up or categorise domain data, perhaps build a domain ontology, write extraction patterns and fine-tune them for the best compromise of precision and recall.
IE	recall	A domain expert and a linguist with knowledge of the IE system will generally be required to specify the task, manually mark-up or categorise domain data, perhaps build a domain ontology, write extraction patterns and fine-tune them for the best compromise of precision and recall.
classification	accuracy	In this paper we demonstrate that classification accuracy can be improved by invoking a more descriptive feature set than what is typically used.
negation	accuracy	We found that for the 174 correctly determined finite verbs (out of the total 184), the heuristics for negation worked without any errors (100% accuracy).
parsing	accuracy	shows the result of parsing accuracy under the condition k = 5 (beam width), and d = 3 (dimension of the polynomial functions used for the kernel function).
parsing	accuracy	shows the relationship between the size of the training data and the parsing accuracy.
parsing	recall	On anew data set we have constructed for the task, while we were disappointed not to find parsing improvement over a traditional parsing model, our model achieves a recall of 84.0% and a precision of 67.3% of exact synset matches on our test corpus, where the gold standard has a reported inter-annotator agreement of 78.6%.
parsing	precision	On anew data set we have constructed for the task, while we were disappointed not to find parsing improvement over a traditional parsing model, our model achieves a recall of 84.0% and a precision of 67.3% of exact synset matches on our test corpus, where the gold standard has a reported inter-annotator agreement of 78.6%.
WSD	accuracy	Although this methodology could be valid for certain NLP problems, such as English Part-of-Speech tagging, we think that there exists reasonable evidence to say that, in WSD, accuracy results cannot be simply extrapolated to other domains (contrary to the opinion of other authors): On the aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus.
WSD	accuracy	one hand, WSD is very dependant to the domain of application () --see also, in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora.
portability	accuracy	Regarding portability, we observe a significant accuracy decrease of 7 and 5 points from A-A to B-A, and from B-B to A-B, respectively 9.
acquisition	AV	Avoiding this pattern would make NL more suitable for acquisition with a productivity of 23.2 (only 2 times lower than AV).
SCF acquisition	binomial log-likelihood ratio (LLR) test	Adopting the SCF acquisition system of Briscoe and Carroll, we have experimented with an alternative hypothesis test, the binomial log-likelihood ratio (LLR) test.
classify relationships between two-word noun compounds	accuracy	We have found that we can use such algorithms to classify relationships between two-word noun compounds with a surprising degree of accuracy.
POS tagging	Recall  Precision  F-measure	 Table 2: Results of Experiments (Segmentation and major POS tagging).  Recall  Precision  F-measure  Our method 95.80% (29,986/31,302) 95.09% (29,986/31,467)  95.44  JUMAN  95.25% (29,814/31,302) 94.90% (29,814/31,417)  95.07  JUMAN+KNP 98.49% (30,830/31,302) 98.13% (30,830/31,417)  98.31
WSD	accuracy rate	The evaluation of the WSD module has revealed a accuracy rate of 64% in a preliminary test.
statistical learning of full parsers	accuracy	However, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time.
indexing	recall	Given the three tasks to be performed (indexing, systematic terminology and translation), the usual notions of recall and precision can be used to evaluate the quality of results when matched with a manually-produced reference list.
indexing	precision	Given the three tasks to be performed (indexing, systematic terminology and translation), the usual notions of recall and precision can be used to evaluate the quality of results when matched with a manually-produced reference list.
translation	recall	Given the three tasks to be performed (indexing, systematic terminology and translation), the usual notions of recall and precision can be used to evaluate the quality of results when matched with a manually-produced reference list.
translation	precision	Given the three tasks to be performed (indexing, systematic terminology and translation), the usual notions of recall and precision can be used to evaluate the quality of results when matched with a manually-produced reference list.
ontology design	ORL	We describe the principles of ontology design and an ORL used to represent our LIP ontology in the next section.
phrase extraction	TF*IDF	In this paper, we show: (a) the efficiency of the linguistic approach for phrase extraction in comparing results with and without filtering techniques, (b) the usefulness of vector representation in determining proper features to identify contentful information, (c) the benefit of using anew measure of TF*IDF for the noun phrase and its constituents, (d) the power of machine learning systems in evaluating several classifiers in order to select the one performing the best for this task.
parsing	accuracy	With an additional 1153 treebanked questions as training input, parsing accuracy levels improve considerably for questions.
alignment	precision	The alignment and transfer-mapping acquisition procedure must acquire rules with very high precision.
topic identification	recall	Our results show that even with apparently incomprehensible system output, humans without any knowledge of Tamil can achieve performance rates as high as 86% accuracy for topic identification, 93% recall for document retrieval, and 64% recall on question answering (plus an additional 14% partially correct answers).
topic identification	recall	Our results show that even with apparently incomprehensible system output, humans without any knowledge of Tamil can achieve performance rates as high as 86% accuracy for topic identification, 93% recall for document retrieval, and 64% recall on question answering (plus an additional 14% partially correct answers).
Term translation extraction	accuracy	 Table 3: Term translation extraction accuracy  tested by 34 Japanese terms
call routing	accuracy	More speciically, we measure the call routing accuracy for unconstrained caller responses to the initial context prompt AT&T.
recognition	accuracy	The recognition accuracy affects the efficiency in other cases.
confirmation	accuracy	The expected number of turns can be derived from the required vocabulary for confirmation and base recognition accuracy under certain vocabulary size.
Identifying DATE Speech-Act Tags	Acc. =	 Table 3: Results for Identifying DATE Speech-Act Tags in the CMU Human-Human Corpus (Maj. Cl. =  Majority Class, Acc. = Accuracy, SE = Standard Error)
Identifying DATE Speech-Act Tags	SE = Standard Error	 Table 3: Results for Identifying DATE Speech-Act Tags in the CMU Human-Human Corpus (Maj. Cl. =  Majority Class, Acc. = Accuracy, SE = Standard Error)
recognition	accuracy	However, no work has achieved sufficient recognition accuracy.
class splitting	SOS	 Table 2: Training time and accuracy with/without  the class splitting technique. The number of training  samples includes SOS and EOS (special words for  the start/end of a sentence).
class splitting	EOS	 Table 2: Training time and accuracy with/without  the class splitting technique. The number of training  samples includes SOS and EOS (special words for  the start/end of a sentence).
entity identification	precision	Our findings also underscored the importance of good information retrieval and of the ability to disambiguate between genes, proteins, RNA, and a variety of other referents for performing entity identification with high precision.
WS	recall	It is comparable to WS at low recall levels (f y g i q s r ), but at high ones its precision decreases almost dramatically.
WS	precision	It is comparable to WS at low recall levels (f y g i q s r ), but at high ones its precision decreases almost dramatically.
IE	accuracy	While current work on IE in biology has concentrated by and large on the refinement of IE techniques and improving their accuracy, the incorporation of an IE system's output into effective interfaces that genuinely assist the biological researcher in his/her work is equally important, and has been neglected to date.
IE	accuracy	Of course improving IE techniques, their accuracy and cross-domain portability are important research objectives for language technology researchers.
Detecting topics	accuracy	Detecting topics also helps improve accuracy of the automatic interpretation system by disambiguating polysemy.
Topic detection	accuracy	Topic detection can enhance speech recognition accuracy by selecting the correct word dictionary and resources, which are organized according to the topic.
speech recognition	accuracy	Topic detection can enhance speech recognition accuracy by selecting the correct word dictionary and resources, which are organized according to the topic.
classification	accuracy	In this paper, we propose and evaluate some improvements, comparing classification accuracy as well as realtime performance in our framework.
MPM-pp classification	SID rate	 Table 5. Comparison of MPM-pp classification using base- line and improved phone recognizers on matched conditions  for 60 seconds of audio (SID rate in %)
translation	accuracy	While this level of translation accuracy cannot be considered impressive, our user studies and system demonstrations indicate that it is already sufficient for achieving effective communication with real users.
tagging	accuracy	We demonstrate that using richer linguistic contextual features significantly improves tagging accuracy, and compare the system's performance with human annotator performance in light of both fine-grained and coarse-grained sense distinctions made by the sense inventory .
WSD	BR	For an extensive evaluation of factors influencing the WSD performance (including representational features), we refer the readers to.  classifier combination methods for 5 classifiers, NB (Naïve Bayes), BR (BayesRatio), TBL, DL and MMVC, including the average classifier accuracy and the best classification accuracy.
WSD	accuracy	For an extensive evaluation of factors influencing the WSD performance (including representational features), we refer the readers to.  classifier combination methods for 5 classifiers, NB (Naïve Bayes), BR (BayesRatio), TBL, DL and MMVC, including the average classifier accuracy and the best classification accuracy.
WSD	accuracy	For an extensive evaluation of factors influencing the WSD performance (including representational features), we refer the readers to.  classifier combination methods for 5 classifiers, NB (Naïve Bayes), BR (BayesRatio), TBL, DL and MMVC, including the average classifier accuracy and the best classification accuracy.
SVMs	accuracy	The training data for SVMs are extracted using NB classifiers according to the category hierarchies , which makes it possible to reduce the amount of computation necessary for classification without sacrificing accuracy.
classification task	F1 score	It is generally agreed that these methods using statistical and machine learning techniques are effective for classification task, since most of them showed significant improvement (the performance over 0.85 F1 score) for Reuters-21578,,).
classification	accuracy	However, it is not clear whether 'works well' means that it exponentially reduces the amount of computation necessary for classification, while sacrificing only a small amount of accuracy, or whether it is statistically significantly better than other methods.
AskMSR Question Answering	accuracy	In this paper, we describe the architecture of the AskMSR Question Answering System and evaluate contributions of different system components to accuracy.
term recognition task.	FGM	As an activity of TMREC, they have provided us with a Japanese test collection of a term recognition task., FGM(CN,k) and MC-value(CN) in descending order.
translation knowledge acquisition	LOGICAL FORM (LF)	Unlike the systems discussed in, MSR-MT places the locus of translation knowledge acquisition at a greater level of abstraction than surface relations, pushing it into a semanticallymotivated layer called LOGICAL FORM (LF).
text classification task	FIFA algorithm	In this paper the basic process of text classification task and FIFA algorithm are described in detail.
text classification task	FIFA algorithm	In this paper the basic process of text classification task and FIFA algorithm are described in detail.
IR	precision	Moreover, IR systems usually employ query expansion techniques that frequently improve their precision.
discourse segmentation	ORG	The major modifications besides the seed handling include a different method of smoothing the distributions along the paths in the tries, anew 'soft' discourse segmentation method, and use of a different labeling methodology, as required by the current task i.e. no overlapping entities are allowed (for example, the correct labeling of colegio San Juan Bosco de Mérida is considered to be ORG(colegio San Juan Bosco) de LOC(Mérida) rather than ORG(colegio PER(San Juan Bosco) de LOC(Mérida))).
discourse segmentation	ORG	The major modifications besides the seed handling include a different method of smoothing the distributions along the paths in the tries, anew 'soft' discourse segmentation method, and use of a different labeling methodology, as required by the current task i.e. no overlapping entities are allowed (for example, the correct labeling of colegio San Juan Bosco de Mérida is considered to be ORG(colegio San Juan Bosco) de LOC(Mérida) rather than ORG(colegio PER(San Juan Bosco) de LOC(Mérida))).
prediction	accuracy	Third, the prediction accuracy is, inmost cases, more or less the same for all of the algorithms.
parse ranking	accuracy	The first row shows parse ranking accuracy using derivation trees of generative and log linear models over the same features.
summarization	length	Any summarization system must balance length of summary against informational and grammatical quality.
ML	accuracy	Since we were interested in evaluating the usefulness of the ML approach with respect to encoding the information in the form of a rule, we also measured the accuracy of a not-toocomplex-but-not-too-dumb hand-coded rule that uses some of the linguistic insights revealed by the inspection of the models.
text structuring	continuity	This paper explores the feasibility of implementing an evolutionary algorithm for text structuring using the heuristic of continuity as a fitness function, chosen over other more complicated metrics of text coherence.
Question answering	F-measure	Question answering, on the other hand, can benefit from more precise identification of the question type, and the average unweighted F-measure for the significant questions is .48.
phoneme recognitionthe	accuracy	One reason is the relatively poor performance of phoneme recognitionthe best phoneme recognition accuracy is about 75% for the TIMIT corpus.
word alignment	accuracy	We provide a rather informal presentation of a prototype system for word alignment based on our previous translation equivalence approach, discuss the problems encountered in the shared-task on word-aligning of a parallel Romanian-English text, present the preliminary evaluation results and suggest further ways of improving the alignment accuracy.
translation	accuracy	One is that the translation accuracy drastically drops as input sentences become long.
translation selection	accuracy	Our experiments on translation selection showed the accuracy of 85% demonstrating the basic feasibility of our approach.
extraction of transliterations	precision	As for the second experiment, performance on the extraction of transliterations is evaluated based on precision and recall rates on the word and character level.
extraction of transliterations	recall	As for the second experiment, performance on the extraction of transliterations is evaluated based on precision and recall rates on the word and character level.
MT	D 3	The following sections describe the two methods, the MT system, D 3 that the methods are applied to, and experiments.
sentence alignment	precision	Our sentence alignment technique achieves a precision of 91.4% and a recall of 92.3%.
sentence alignment	recall	Our sentence alignment technique achieves a precision of 91.4% and a recall of 92.3%.
parsing	accuracy	The differences between traditional and our model were insignificant and the results proved that a consistent probability model of parsing can be built without the independence assumption, and attains performance that rivals the traditional models in terms of parsing accuracy.
parsing	accuracy	The differences between traditional and our model were insignificant and the results proved that a consistent probability model of parsing can be built without the independence assumption, and attains performance that rivals the traditional models in terms of parsing accuracy.
parse reranking problem	recall	We have applied this algorithm to the parse reranking problem, and achieved labeled recall and precision of 89.4%/89.8% on WSJ section 23 of Penn Treebank.
parse reranking problem	precision	We have applied this algorithm to the parse reranking problem, and achieved labeled recall and precision of 89.4%/89.8% on WSJ section 23 of Penn Treebank.
Disambiguation	Accuracy	 Table 5: Disambiguation Accuracy of different  Clustering Methods over 28 pseudonames
POS tagger	accuracy	Our results show that, when using very small amounts of manually labelled seed data and a much larger amount of unlabelled material, agreement-based co-training can significantly improve POS tagger accuracy.
Naive co-training	accuracy	 Table 2: Naive co-training accuracy results when varying  the amount added after each round (50 seed sentences)
Naive co-training	accuracy	 Table 3: Naive co-training accuracy results when varying  the amount added after each round (500 seed sentences)
Preposition Semantic Classification	FRAMENET	Preposition Semantic Classification via PENN TREEBANK and FRAMENET
preposition disambiguation	Freq	 Table 7: Per-word results for preposition disambiguation with TREEBANK semantic roles. Freq gives the  frequency for the prepositions. Entropy measures non-uniformity of the role distributions. The Baseline  experiment selects the most-frequent role. The Word Only experiment just uses word collocations, whereas  Combined uses both word and hypernym collocations. Both columns show averages for percent correct over  ten trials. Total averages the values of the individual experiments (except for F req).
preposition disambiguation	Entropy	 Table 7: Per-word results for preposition disambiguation with TREEBANK semantic roles. Freq gives the  frequency for the prepositions. Entropy measures non-uniformity of the role distributions. The Baseline  experiment selects the most-frequent role. The Word Only experiment just uses word collocations, whereas  Combined uses both word and hypernym collocations. Both columns show averages for percent correct over  ten trials. Total averages the values of the individual experiments (except for F req).
preposition disambiguation	F req	 Table 7: Per-word results for preposition disambiguation with TREEBANK semantic roles. Freq gives the  frequency for the prepositions. Entropy measures non-uniformity of the role distributions. The Baseline  experiment selects the most-frequent role. The Word Only experiment just uses word collocations, whereas  Combined uses both word and hypernym collocations. Both columns show averages for percent correct over  ten trials. Total averages the values of the individual experiments (except for F req).
preposition disambiguation	FRAMENET	 Table 8: Per-word results for preposition disambiguation with FRAMENET semantic roles. See
SMT	incorrect rejection rate (CR)	Depending on the underlying SMT model, we obtained a relative improvement incorrect rejection rate (CR) ranging from 3.90% to 33.09% at a fixed 0.80 correct acceptance rate (CA) for prediction lengths of up to four words.
SMT	correct acceptance rate (CA)	Depending on the underlying SMT model, we obtained a relative improvement incorrect rejection rate (CR) ranging from 3.90% to 33.09% at a fixed 0.80 correct acceptance rate (CA) for prediction lengths of up to four words.
NEC	accuracy	 Table 2: NEC accuracy on the development set assuming  a perfect recognition of named entities
NER	miscellaneous (MISC)	In this year's CoNLL, the NER task is to tag noun phrases with the following four classes: person (PER), organization (ORG), location (LOC), and miscellaneous (MISC).
clause identification	fscore	LSTM was applied to an earlier CoNLL shared task, namely clause identification) although the performance was significantly below the performance of other methods, e.g. LSTM achieved an fscore of 50.42 on the test data where other systems' fscores ranged from 62.77 to 80.44.
CoNLL-2003 named entity recognition (NER) shared task	PERSON	We present results on the CoNLL-2003 named entity recognition (NER) shared task, consisting of news articles with tagged entities PERSON, LOCATION, ORGANI-ZATION and MISC.
CoNLL-2003 named entity recognition (NER) shared task	ORGANI-ZATION	We present results on the CoNLL-2003 named entity recognition (NER) shared task, consisting of news articles with tagged entities PERSON, LOCATION, ORGANI-ZATION and MISC.
Sum of	compression	The Sum of All Scores manual algorithm produces the best summaries at the twenty percent compression rate.
summarization	MEAD	To test the benefit of gzip in the summarization process, extracts were created using a combination of MEAD and gzip.
summarizer	precision	The summarizer produce the same number of sentences as are in the corresponding manual summary, as in (, therefore, precision and recall are the same for summaries sentences comparison.
summarizer	recall	The summarizer produce the same number of sentences as are in the corresponding manual summary, as in (, therefore, precision and recall are the same for summaries sentences comparison.
summarization	annotation weight β	It is obvious that context can help to improve the summarization performance than no context consideration, so in our later experiments, we set the context weight γ=1, and annotation weight β=1..
information discovery	breadth	Thus, information discovery applications demand breadth and depth in IE technology.
GATE	REES	Among these, the most widely used are the GATE system from the University of Sheffield [, the IE components from Clearforest (www.clearforest.com), SIFT from BBN [, REES from SRA] and various tools provided by Inxight (www.inxight.com).
parsing	accuracy	In addition to antecedent recovery, we also report parsing accuracy, using the bracketing F-Score, the combined measure of PARSEVAL-style labeled bracketing precision and recall.
parsing	F-Score	In addition to antecedent recovery, we also report parsing accuracy, using the bracketing F-Score, the combined measure of PARSEVAL-style labeled bracketing precision and recall.
parsing	recall	In addition to antecedent recovery, we also report parsing accuracy, using the bracketing F-Score, the combined measure of PARSEVAL-style labeled bracketing precision and recall.
parsing	recov- ery	 Table 3: F-Scores for parsing and antecedent recov- ery on Section 23.
parse reranking	recall	We use LTAG based features for the parse reranking task and obtain labeled recall and precision of 89.7%/90.0% on WSJ section 23 of Penn Treebank for sentences of length ≤ 100 words.
parse reranking	precision	We use LTAG based features for the parse reranking task and obtain labeled recall and precision of 89.7%/90.0% on WSJ section 23 of Penn Treebank for sentences of length ≤ 100 words.
ME	equality constraint	In spite of these advantages, the ME model still suffers from alack of data as long as it imposes the equality constraint (1), since the empirical expectation calculated from the training data of limited size is inevitably unreliable.
IFS	precision	Past work, including and, has shown that the IFS algorithm utilizes much fewer features than the count cutoff method, while maintaining the similar precision and recall on tasks, such as prepositional phrase attachment, text categorization and base NP chunking.
IFS	recall	Past work, including and, has shown that the IFS algorithm utilizes much fewer features than the count cutoff method, while maintaining the similar precision and recall on tasks, such as prepositional phrase attachment, text categorization and base NP chunking.
parsing	accuracy	A word-dictionary helps to improve both word-segmentation and parsing accuracy.
summarization	precision	We evaluate results of summarization by using the standard precision, recall, and F 1 . Let J be the number of extracts in the summary, K be the number of selected paragraphs in the summary, and M be the number of extracts in the test document.
summarization	recall	We evaluate results of summarization by using the standard precision, recall, and F 1 . Let J be the number of extracts in the summary, K be the number of selected paragraphs in the summary, and M be the number of extracts in the test document.
summarization	F	We evaluate results of summarization by using the standard precision, recall, and F 1 . Let J be the number of extracts in the summary, K be the number of selected paragraphs in the summary, and M be the number of extracts in the test document.
SRA	precision	Preliminary evaluations suggest that SRA can classify text fragments with very high precision, and that it is useful for efficient information access.
IR	precision	Current IR systems especially focus on improving precision the result rather than recall.
IR	recall	Current IR systems especially focus on improving precision the result rather than recall.
IR	accuracy	Our system is superior to vector-space ranking techniques from IR, and its accuracy approaches that of the top contenders at the TREC QA tasks in recent years.
word selection	recall	The results of a recall based evaluation comparing three different strategies to word selection, indicate that thematic information does help improve recall.
summaries	continuity	This paper is structured as follows: In Section 2 we present our hypothesis: it is possible to produce better summaries by enforcing the continuity principle (see next section fora definition of this principle) . A corpus of scientific abstracts is analysed in Section 3 to learn whether this principle holds inhuman produced summaries.
IR	precision	Hence, we evaluate our IR by only the precision measure at top N segments.
Question Answering	IE	Examples of using NLP and IE in Question Answering include shallow parsing], deep parsing [ [, and IE [.
semantic classification	F β = 1	The experimental results on the GENIA corpus show that the proposed method is effective not only in the reduction of training cost but also in performance improvement: the identification performance is about 79.9(F β = 1), the semantic classification accuracy is about 66.5(F β = 1).
recognition	accuracy	However, no work has achieved sufficient recognition accuracy.
Text Retrieval Conference	Mean Reciprocal Rank (MRR)	The Text Retrieval Conference uses the Mean Reciprocal Rank (MRR) as an evaluation metric.
classification of names	precision	Secondly, classification of names can help improve the precision of the methods.
classification	accuracy	However, classification was not the primary focus of these papers and hence the details and accuracy of the classification methods are not described in much detail.
word segmentation ambiguity	accuracy	First, the word segmentation ambiguity has a worse affect on accuracy than expected.
synonym extraction	precisions	Experimental results prove that the three resources are complementary to each other on synonym extraction, and that the ensemble method we used is very effective to improve both precisions and recalls of extracted synonyms.
synonym extraction	recalls	Experimental results prove that the three resources are complementary to each other on synonym extraction, and that the ensemble method we used is very effective to improve both precisions and recalls of extracted synonyms.
synonym extraction	precisions	Experimental results prove that the three resources are complementary to each other on synonym extraction, and that the ensemble method we used is very effective to improve both precisions and recalls of extracted synonyms.
synonym extraction	recalls	Experimental results prove that the three resources are complementary to each other on synonym extraction, and that the ensemble method we used is very effective to improve both precisions and recalls of extracted synonyms.
parsing	accuracy	Secondly, we want to test the influence of new knowledge on parsing accuracy and speed.
parsing	speed	Secondly, we want to test the influence of new knowledge on parsing accuracy and speed.
parsing	accuracy	Then the parsing accuracy and speed is tested against the same training corpus (q (recall,precision,f-score,time)).
parsing	speed	Then the parsing accuracy and speed is tested against the same training corpus (q (recall,precision,f-score,time)).
parsing	recall,precision,f-score,time	Then the parsing accuracy and speed is tested against the same training corpus (q (recall,precision,f-score,time)).
SCL recognition	F-score	We achieve an F-score of 81.01% for SCL recognition and an F-score of 68.02% for SCP recognition.
estimating	recall	Another difficulty we experienced relates to estimating recall.
USAS semantic tagger	precisions	As shown in this table, the USAS semantic tagger obtained precisions between 91.23% to 100.00% for each semantic field except for the field of "names and grammatical words" denoted by Z. As Z was the biggest field (containing 45.39% of the total MWEs and 43.12% of the accepted MWEs), we examined these MWEs more closely.
information analysis	accuracy	In the procedure of information analysis, like in the domain of patent analysis, the complexity of the studied topics and the accuracy of the question to be answered may often lead the analyst to partition his reasoning into viewpoints.
patent analysis	accuracy	In the procedure of information analysis, like in the domain of patent analysis, the complexity of the studied topics and the accuracy of the question to be answered may often lead the analyst to partition his reasoning into viewpoints.
IR	Recall R	In IR, the Recall R represents the ratio between the number of relevant documents which have been returned by an IR system fora given query and the total number of relevant documents which should have been found in the documentary database.
IR	F-measure	Similarly to IR, the F-measure (described by Eq. 1) could be used to combine Recall and Precision results.
IR	Precision	Similarly to IR, the F-measure (described by Eq. 1) could be used to combine Recall and Precision results.
information retrieval	TREC	In the field of information retrieval, there have been held successive evaluation workshops, such as TREC, CREF, and NTCIR, to build and utilize various kinds of test collections.
dialogue management	BIRDQUEST	Some empirical findings on dialogue management and domain ontologies in dialogue systems -Implications from an evaluation of BIRDQUEST
Machine Translation evaluation	BLEU	Given the relatively recent success in achieving high correlations with human judgement for Machine Translation evaluation, using the IBM content-based evaluation metric, BLEU (), we attempt to run this same metric on system generated extracts; this way we explore whether BLEU can be used reliably in this research area and if so, which testing parameters need to betaken into consideration.
Machine Translation evaluation	BLEU	Given the relatively recent success in achieving high correlations with human judgement for Machine Translation evaluation, using the IBM content-based evaluation metric, BLEU (), we attempt to run this same metric on system generated extracts; this way we explore whether BLEU can be used reliably in this research area and if so, which testing parameters need to betaken into consideration.
parsing	accuracy	It is to be expected that the performance of an NLP system quickly degrades if the parsing system returns incorrect syntactic structures, and therefore an evaluation of parsing coverage and accuracy is important.
classification	accuracy	The aim was to explore the contribution of each type of information, in particular the logical structure features, to the classification accuracy.
parsing search	accuracy	Finally, we will discuss the potential for effective use of fast, finite-state processing, e.g. part-of-speech tagging, to reduce the parsing search space without accuracy loss.
parsing	accuracy	In experiments reported in, a parsing accuracy of 85.7% (unlabeled attachment score) was achieved, using data from a small treebank of Swedish, divided into a training set of 5054 sentences and a test set of 631 sentences.
named-entity extraction	9 re-call	Since we are currently using a rather naive named-entity extraction scheme, 9 re-call is rather low as there are quite a number of foreign multi-word named-entities (persons and organizations mostly) that do not exist in our database of named-entities.
answer completeness	accuracy	This evaluation should include measures for answer completeness, accuracy, and relevancy.
Detection Costs	Initial Threshold	 Table 4. Detection Costs with Various Dynamic  Threshold Functions (Initial Threshold = 0.05)
Detection Costs	Initial	 Table 5. Detection Costs with Various Window  Sizes Using Formula (3) (Initial Threshold = 0.05)
anaphora resolution	DISCOURSE-NEW	Although many theories of definiteness and many anaphora resolution algorithms are based on the assumption that definite descriptions are anaphoric, in fact inmost corpora at least half of definite descriptions are DISCOURSE-NEW, as shown by the following examples, both of which are the first sentences of texts from the Penn Treebank.
SENSEVAL-3  English lexical sample Word Sense Disambiguation task	Precision	 Table 2: Performance and short description of the supervised systems participating in the SENSEVAL-3  English lexical sample Word Sense Disambiguation task. Precision and recall figures are provided for both  fine grained and coarse grained scoring. Corresponding team and reference to system description (in this  volume) are indicated for the first system for each team.
SENSEVAL-3  English lexical sample Word Sense Disambiguation task	recall	 Table 2: Performance and short description of the supervised systems participating in the SENSEVAL-3  English lexical sample Word Sense Disambiguation task. Precision and recall figures are provided for both  fine grained and coarse grained scoring. Corresponding team and reference to system description (in this  volume) are indicated for the first system for each team.
SR task	precision	The evaluation of the SR task is based on precision and recall.
SR task	recall	The evaluation of the SR task is based on precision and recall.
Bag of words	precision-recall	 Table 1: Bag of words versus bag of contexts, precision-recall
Classification	accuracy	 Table 2: Classification accuracy per POS in the En- glish lexical sample task.
Classification	accuracy	 Table 3: Classification accuracy in the English all  words task.
WSD	accuracy	However, the most accurate WSD systems are those which require manually sense tagged data in the first place, and their accuracy depends on the quantity of training examples () available.
SENSEVAL-3 English all words task	accuracy	SENSELEARNER participated in the SENSEVAL-3 English all words task, and achieved an average accuracy of 64.6%.
SSI	precision	A second problem of SSI, when applied to unrestricted WSD tasks, is that it is designed to disambiguate with high precision, possibly low recall.
SSI	recall	A second problem of SSI, when applied to unrestricted WSD tasks, is that it is designed to disambiguate with high precision, possibly low recall.
WSD tasks	precision	A second problem of SSI, when applied to unrestricted WSD tasks, is that it is designed to disambiguate with high precision, possibly low recall.
WSD tasks	recall	A second problem of SSI, when applied to unrestricted WSD tasks, is that it is designed to disambiguate with high precision, possibly low recall.
WSD	precision	In many interesting applications of WSD, especially in information retrieval, improved document access maybe obtained even when only few words in a query are disambiguated, but the disambiguation precision needs to be well over the 70% threshold.
information retrieval	precision	In many interesting applications of WSD, especially in information retrieval, improved document access maybe obtained even when only few words in a query are disambiguated, but the disambiguation precision needs to be well over the 70% threshold.
phrase head determination	standard (m/n) measure	For phrase head determination, the standard (m/n) measure is used.
classification	accuracy	To determine if the 90.2% upper bound on classification accuracy for the given experimental setup is due to limitations in the particular resources we are using or an inherent bound on the RCC interpretation task as defined herein, we performed a manual annotation task involving 4 annotators and 100 randomly-selected RCCs, taken from the 5143 RCCs used in this research.
Summarization	coverage	Summarization and Question Answering need precise linguistic information with a much higher coverage than what is being offered by currently available statistically based systems.
Question Answering	coverage	Summarization and Question Answering need precise linguistic information with a much higher coverage than what is being offered by currently available statistically based systems.
text understanding	GETARUNS	In this paper we present the system for text understanding called GETARUNS, General Text and Reference Understanding System (Delmonte, 2003a).
machine translation evaluation	BLEU	Following the successful application of automatic evaluation methods, such as BLEU (), in machine translation evaluation, showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.
Acquiring human judgments	CASE set)	Acquiring human judgments are usually very expensive; fortunately, we have  manual summaries (CASE set), stemmed 4 version of the summaries (STEM set), and stopped version of the summaries (STOP set).
summaries of text generated through sentence or word extraction	precision	Automatic summaries of text generated through sentence or word extraction has been evaluated by comparing them with manual summaries generated by humans by using numerical evaluation measures based on precision or accuracy.
summaries of text generated through sentence or word extraction	accuracy	Automatic summaries of text generated through sentence or word extraction has been evaluated by comparing them with manual summaries generated by humans by using numerical evaluation measures based on precision or accuracy.
sentence extraction	precision	Although sentence extraction has previously been evaluated based only on precision of a single sentence, sentence concate-nations in the summaries should be evaluated as well.
summarization	ROUGE	To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU () for machine translation.
summarization	BLEU	To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU () for machine translation.
coverage of correct answers	reliability	In addition, to solve the problems for the coverage of correct answers and the reliability of manual summaries as correct answers, weighted summarization accuracy (WSumACCY) in which SumACCY is weighted by the majority of the humans' selections, has been proposed).
coverage of correct answers	accuracy	In addition, to solve the problems for the coverage of correct answers and the reliability of manual summaries as correct answers, weighted summarization accuracy (WSumACCY) in which SumACCY is weighted by the majority of the humans' selections, has been proposed).
Sentence extraction	reliability	Sentence extraction should also be evaluated using measures that take into account sentence concatenations, the coverage of correct answers, and the reliability of manual summaries.
summarization	accuracy	This paper presents evaluation results of automatic summarization through sentence or word extraction using the above mentioned metrics based on n-gram precision and sentence/word accuracy and examines how well these measures reflect the judgments of humans as well.
summaries	NrstACCY	The automatic summaries were also evaluated by using the numerical metrics SumACCY, WSumACCY, NrstACCY, and n-gram precision (1 ≤ n ≤ 5) in comparison with reference summaries generated by humans.
summarization redundancy	RE	According to this, some important notations are defined as follows:   The value of RE () is calculated as follows: RE=- The evaluation principles of the summarization redundancy based on RE are demonstrated in.
comma classification	accuracy	Experimental results show that accuracy for the comma classification reaches 87.1 percent, and with our segmentation model, our parser's dependency parsing accuracy improves by 9.6 percent .
classification	accuracy	 Table 1: Reduction in classification accuracy with removal of features. Each row is labeled with the feature  that is newly removed from the set of available features.
translation	WER	The translation performance of the three systems is summarized in for the three evaluation criteria WER, PER and BLEU.
translation	PER	The translation performance of the three systems is summarized in for the three evaluation criteria WER, PER and BLEU.
translation	BLEU	The translation performance of the three systems is summarized in for the three evaluation criteria WER, PER and BLEU.
relational parsing	recall rate	A partial parsing example is the relational parsing for the inhibition relation), with a comparatively low recall rate of 57%.
Sample	precision	 Table 3: Sample of low precision patterns.
POS taggers	accuracy	State-of-the-art automated POS taggers achieve accuracy of 93% -98% and the most successful implementations are based on statistical approaches to POS tagging.
POS tagger	accuracy	Currently, most of the POS tagger accuracy reports are based on the experiments involving Penn Treebank data.
SVM	speed	However, notable limitation of SVM is its low speed both for training and recognition.
induction of morphology	accuracy	Approaches to the induction of morphology as presented in e.g. or show that the morphological properties of a small subset of languages can be induced with high accuracy, most of the existing approaches are motivated by applied or engineering concerns, and thus make assumptions that are less cognitively plausible: a.
word segmentation task	precision	We will show the results obtained fora word segmentation task on a phonetically transcribed and child-oriented French corpus, focusing on the effect of the preprocessing step on precision and recall, as well as its impact on memory load and processing time.
word segmentation task	recall	We will show the results obtained fora word segmentation task on a phonetically transcribed and child-oriented French corpus, focusing on the effect of the preprocessing step on precision and recall, as well as its impact on memory load and processing time.
Distribution	ANNAHAR	 Table 4. Distribution of items with no solution,  ANNAHAR
Classification	accuracy	 Table 7. Classification accuracy on the evaluation set  using Leave-one-out and TF-IDF with 2,000 roots/terms
recognition	accuracy	Both of these factors may lead to a loss in recognition accuracy.
summative assessment	accuracy	Finally, it focuses on summative assessment where the accuracy of results is paramount.
formative assessment	precision	Whereas formative assessment can be general and informative, summative assessment requires a high degree of precision.
summative assessment	precision	Whereas formative assessment can be general and informative, summative assessment requires a high degree of precision.
definition extraction	precision	The markers and patterns showed to be comparable to the other experiments mentioned in terms of definition extraction: the precision reached from 61 to 66%.
Frame Semantic Processing Automated Induction of Sense in Context Mining Linguistically Interpreted Texts Word Order Variation	Main	A POS Tagged and Syntactically Annotated Hungarian Natural Language Corpus Dora Csendes, Janos Csirik, and Tibor Gyimothy Mining Linguistically Interpreted Texts The Hinoki Treebank. Working Toward Text Understanding A POS Tagged and Syntactically Annotated Hungarian Natural Language Corpus Towards User-Adaptive Annotation Guidelines Towards a Dependency-Based Gold Standard for German Parsers. The TIGER Dependency Bank Corpus-based Induction of an LFG Syntax -Semantics Interface for Frame Semantic Processing Automated Induction of Sense in Context Mining Linguistically Interpreted Texts Word Order Variation in German Main Clauses
WSD task	ONTOSCORE	To perform the WSD task, ONTOSCORE calculates a coherence score for each of these concept sets in . The concepts in the highest ranked set are considered to be the ones representing the correct word meaning in this context.
classification	accuracy	Furthermore, for classification accuracy we only use the portion of the test set whose call-types are covered by the model and the call-types in this model are definitely easier than the specific ones.
unification	precision	A novel method for unification that minimizes conflicts among an-notators outperforms methods that require consensus among a majority for the and precision metrics, while capturing much of the structure of the discourse.
predictive	accuracy	In Section 5 we analyze our scheme with respect to interannotator agreement and predictive accuracy, using a corpus of human tutoring dialogues.
WSD	Accuracy	 Table 1: Supervised WSD Accuracy by Feature Type
parsing	accuracy	More precisely, parsing accuracy is measured by the attachment score, which is a standard measure used in studies of dependency parsing).
parsing	attachment score	More precisely, parsing accuracy is measured by the attachment score, which is a standard measure used in studies of dependency parsing).
parsing	accuracy	For parsing accuracy, we use a paired t-test for statistical significance.
classification	accuracy	Our evaluation using the 1996 Reuters corpus which consists of 806,791 documents shows that automatically constructing hierarchy improves classification accuracy.
argument classification task	accuracy (A)	For the argument classification task, the our system obtains a classification accuracy (A) of 85.45.
Question Answering	precision	Progress in Question Answering can be achieved by (1) combining multiple strategies that optimally resolve different question classes of various degrees of complexity; (2) enhancing the precision of question interpretation and answer extraction; and (3) question decomposition and answer fusion.
question categorization	MRAR score	This figure also shows that question categorization per se does not greatly impact the MRAR score of Q/A. illustrates the SRAR curves by considering the answer elimination policy.
question categorization	Q/A.	This figure also shows that question categorization per se does not greatly impact the MRAR score of Q/A. illustrates the SRAR curves by considering the answer elimination policy.
question categorization	SRAR	This figure also shows that question categorization per se does not greatly impact the MRAR score of Q/A. illustrates the SRAR curves by considering the answer elimination policy.
question categorization	SRAR score	The figure clearly shows that the QSVM and PRTC models for question categorization determine a higher SRAR score, thus indicating that fewer irrelevant answers are left.
SCF acquisition	accuracy	This method for SCF acquisition is highly sensitive to the accuracy of the lexical-semantic classes.
PDT	consistency	Within the massive manual annotation in PDT, the problem of consistency of assigning the valency structure increased.
classification	accuracy	 Table 1: Reduction in classification accuracy with removal of features. Each row is labeled with the feature that is  newly removed from the set of available features.
phone recognition	accuracy	Our own experiments, using data different from that used by AT&T, showed that this technique gave only a small benefit in phone recognition accuracy, but was useful for finding salient phoneme strings.
Phone recognition	accuracy	 Table 1. Phone recognition accuracy and  call routing accuracy
ASR	precision	Some authors investigated the relation of ASR errors and precision of IR ().
ASR error correction	accuracy	ASR error correction can be one of the domain adaptation techniques to improve the recognition accuracy, and the primary advan-: Adaptation via Post Error Correction tage of the error correction approach is its independence of the specific speech recognizer.
classification	accuracy	These features may undergo transformations such as weighting or dimension reduction with the goal of improving classification accuracy, improving clustering quality, or data reduction.
classification	accuracy	Recent work has shown that discriminative techniques frequently achieve classification accuracy that is superior to generative techniques, over a wide range of tasks.
parse selection	accuracy	The performance metric used here is parse selection accuracy as described in section 2.3.
ASE	PHRASEMAXF	Concerning the ASE algorithm, threshold parameters 3 were set as PHRASEMAXF=10 7 , SET-MINF=10 2 , SETMAXF=10 5 , SETMINP=0.066, and SETMAXP=0.666.
ASE	SET-MINF	Concerning the ASE algorithm, threshold parameters 3 were set as PHRASEMAXF=10 7 , SET-MINF=10 2 , SETMAXF=10 5 , SETMINP=0.066, and SETMAXP=0.666.
ASE	SETMAXF	Concerning the ASE algorithm, threshold parameters 3 were set as PHRASEMAXF=10 7 , SET-MINF=10 2 , SETMAXF=10 5 , SETMINP=0.066, and SETMAXP=0.666.
ASE	SETMINP	Concerning the ASE algorithm, threshold parameters 3 were set as PHRASEMAXF=10 7 , SET-MINF=10 2 , SETMAXF=10 5 , SETMINP=0.066, and SETMAXP=0.666.
ASE	SETMAXP	Concerning the ASE algorithm, threshold parameters 3 were set as PHRASEMAXF=10 7 , SET-MINF=10 2 , SETMAXF=10 5 , SETMINP=0.066, and SETMAXP=0.666.
SU detection	error rate	 Table 1: SU detection results (error rate in %) using
classification	accuracy	We also present new features which result in a 1.1% gain in classification accuracy and describe a technique that results in a 97% reduction in the feature space with no significant degradation inaccuracy.
model selection	accuracy	This maybe especially helpful during model selection or feature evaluation, after which, one could revert to the full dimensionality for final training to improve classification accuracy.
classification	accuracy	This maybe especially helpful during model selection or feature evaluation, after which, one could revert to the full dimensionality for final training to improve classification accuracy.
parsing	accuracy	We find that using the original parsing model with the new supertagger model dramatically increases parsing accuracy on TREC questions, producing a parser suitable for use in a QA system.
paraphrase generation	coverage	Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scal-ability than the current best-of-breed paraphrasing approaches.
derivation tree node annotation	Annotated	In the present experiments, we have limited the derivation tree node annotation to the features listed in: Annotated features of derivation tree nodes.
word segmentation	recall (R)	The accuracy of word segmentation is measured by recall (R), precision (P), and Fmeasure ( Recall is the proportion of correctly segmented words in the gold-standard segmentation, and precision is the proportion of correctly segmented words in word segmenter's output.
word segmentation	precision (P)	The accuracy of word segmentation is measured by recall (R), precision (P), and Fmeasure ( Recall is the proportion of correctly segmented words in the gold-standard segmentation, and precision is the proportion of correctly segmented words in word segmenter's output.
word segmentation	Fmeasure	The accuracy of word segmentation is measured by recall (R), precision (P), and Fmeasure ( Recall is the proportion of correctly segmented words in the gold-standard segmentation, and precision is the proportion of correctly segmented words in word segmenter's output.
word segmentation	precision	The accuracy of word segmentation is measured by recall (R), precision (P), and Fmeasure ( Recall is the proportion of correctly segmented words in the gold-standard segmentation, and precision is the proportion of correctly segmented words in word segmenter's output.
word segmentation	accuracy	There is a slight improvement in word segmentation and POS tagging accuracy using this approach, compared to the one-at-a-time, character-based approach.
POS tagging	accuracy	There is a slight improvement in word segmentation and POS tagging accuracy using this approach, compared to the one-at-a-time, character-based approach.
POS tagging	accuracy	When a paired t-test was carried out at the level of significance 0.01, the all-at-once approach was found to be significantly better than the one-at-a-time approach for POS tagging accuracy, although the difference was insignificant for word segmentation.
classification	accuracy	The most sensitive parameter is the prior variance σ 2 , as shown in; its value is chosen to maximize classification accuracy on development data.
machine translation evaluation.	BLEU score	These two goals, adequacy and fluency, are the main criteria in machine translation evaluation.: Translation quality of three systems, measured by the BLEU score and n-gram precision Human judges maybe asked to evaluate the adequacy and fluency of translation output, but this is a laborious and expensive task.
Translation	BLEU score	These two goals, adequacy and fluency, are the main criteria in machine translation evaluation.: Translation quality of three systems, measured by the BLEU score and n-gram precision Human judges maybe asked to evaluate the adequacy and fluency of translation output, but this is a laborious and expensive task.
Translation	mea- sured	 Table 1: Translation quality of three systems, mea- sured by the BLEU score and n-gram precision
Translation	BLEU score	 Table 1: Translation quality of three systems, mea- sured by the BLEU score and n-gram precision
Translation	precision	 Table 1: Translation quality of three systems, mea- sured by the BLEU score and n-gram precision
MT	BLEU metric	Similarly, in MT, the recent BLEU metric () also uses the idea that one gold standard is not enough.
proficiency estimation	TOEIC	In this restricted experiment, we do not claim that our proficiency estimation method covers the full range of TOEIC scores.
NERs	F- measure	 Table 3.  This shows the performances for half of NERs are  improved due to the adoption of both positive and  negative cases. Moreover, the total average F- measure is enhanced from 63.61% to 70.46% as a  whole.
TFM	accuracy	 Table 4: Top parameter combinations for TFM by improvement in classification accuracy. Vocab frequency min. is  the minimum number of times a term must appear in the corpus in order to be included.
TFM	Vocab frequency min.	 Table 4: Top parameter combinations for TFM by improvement in classification accuracy. Vocab frequency min. is  the minimum number of times a term must appear in the corpus in order to be included.
predicate argument labeling	recall	In this paper, we designed and experimented a boundary classifier for predicate argument labeling based on two phases: (1) a first annotation of potential arguments by using a high recall tbc and (2) a P AST classification step aiming to select the correct substructures associated with potential arguments.
WSD	accuracy	However, even the state-of-the-art methods for WSD did not improve the accuracy because of the inherent noise introduced by the disambiguation mistakes.
binary classification	precision-recall	For binary classification, a specific score threshold could be applied, but we defer the decision on the precision-recall trade-off to downstream applications.
identification	F β=1	Finally, for identification, we use 24 features except gov and pos+clau, and obtain an F β=1 of 80.59%, as shown in the bottom of.
classification	Accuracy	Also, for classification, we use 23 features except pred type, cont POS, and pos+clau, and obtain an Accuracy of 87.16%.
segmentation	accuracy	Unfortunately, we found that adding word boundary information in this way did not improve segmentation accuracy.
tagging	accuracy	 Table 6: Improvements in tagging accuracy from  adding affix features and constraining lexicon.
Tagging	accuracy	 Table 7: Tagging accuracy for various data sharing  methods.
alignment	accuracy	In the end we obtain alignment algorithms that are much faster, and in some ways simpler, whose accuracy comes surprisingly close to the established probabilistic generative approach.
parsing	accuracy	At the present stage, we are interested in quantitative results on parsing time, rather than qualitative results of parsing accuracy (for which a more extensive training of the rule parameters would be required).
parsing	accuracy	At the present stage, we are interested in quantitative results on parsing time, rather than qualitative results of parsing accuracy (for which a more extensive training of the rule parameters would be required).
word alignment	accuracy	Improved per-plexity, word alignment accuracy, and translation quality are observed in our experiments .
word translation	accuracy	A later study by) showed improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model.
translation equivalence extraction	TREQ	In) we described a translation equivalence extraction program called TREQ the development of which was twofold motivated: to help enriching the synsets of the Romanian wordnet () with new literals based on bilingual corpora evidence and to check the interlingual alignment of our wordnet against the Princeton Wordnet.
translation	BLEU	 Table 2: Our translation performance  (measured with BLEU)
SMT	SER	 Table 1.  Essentially, all the automatic evaluation metrics bar  one (Precision) suggest that EBMT can outperform  SMT from English-French. Surprisingly, however,  apart from SER, all evaluation scores are higher us- ing 100K sentence pairs as training data rather than  the full 203K sentences. It is generally assumed that  increasing the size of the training data for corpus- based MT systems will improve the quality of the  output translations. (
summaries	accuracy	Although researchers have demonstrated that users can read summaries faster than full text () with some loss of accuracy, researchers have found it difficult to draw strong conclusions about the usefulness of summarization due to the low level of interannotator agreement in the gold standards that they have used.
MT evaluation	BLEU	In the 2003 Johns Hopkins Workshop on Speech and Language Engineering, experiments on MT evaluation showed that BLEU and NIST do not correlate well with human judgments at the sentence level, even when they correlate well overlarge test sets ().
MT evaluation	accuracy	We investigate some pitfalls regarding the discriminatory power of MT evaluation metrics and the accuracy of statistical significance tests.
SMT	BLEU	Ina discriminative rerank-ing experiment for phrase-based SMT we show that the NIST metric is more sensitive than BLEU or F-score despite their incorporation of aspects of fluency or meaning adequacy into MT evaluation.
SMT	F-score	Ina discriminative rerank-ing experiment for phrase-based SMT we show that the NIST metric is more sensitive than BLEU or F-score despite their incorporation of aspects of fluency or meaning adequacy into MT evaluation.
statistical machine translation (SMT)	BLEU evaluation metric	In the area of statistical machine translation (SMT), recently a combination of the BLEU evaluation metric () and the bootstrap method for statistical significance testing has become popular).
machine translation (MT) evaluation	BLEU	Automatic Metrics for machine translation (MT) evaluation have been receiving significant attention in the past two years, since IBM's BLEU metric was proposed and made available.
MT evaluation	BLEU	We then computed the Pearson correlation between these system level human judgments and the system level scores for each algorithm; these numbers are presented in Observe that simply using Recall as the MT evaluation metric results in a significant improvement in correlation with human judgment over both the BLEU and the NIST algorithms.
bracketing	precision	After the TextTreebank was created, the bracketing script described in section 5.1 was applied both to the original TextTree file and to the TextTreebank, and the results were submitted to the EVALB program, which reported a bracketing recall of 71%, a bracketing precision of 84%, and an average crossing bracket count of 1.15.
MT	BLEU	Consequently, here we employ multiple references to evaluate MT systems like BLEU () and NIST).
SMT	accuracy	Therefore, if we can restrain or modify the training corpus, the SMT system might achieve high accuracy.
translation	accuracy	With regard to translation rather than alignment accuracy, show that decoding under ITG constraints yields significantly lower word error rates and BLEU scores than the IBM constraints.
translation	BLEU scores	With regard to translation rather than alignment accuracy, show that decoding under ITG constraints yields significantly lower word error rates and BLEU scores than the IBM constraints.
recognizing acronymdefinition pairs from running text	accuracy	As for the methods for recognizing acronymdefinition pairs from running text, there are many studies reporting high performance (e.g. over 96% accuracy and 82% recall) (;).
recognizing acronymdefinition pairs from running text	recall	As for the methods for recognizing acronymdefinition pairs from running text, there are many studies reporting high performance (e.g. over 96% accuracy and 82% recall) (;).
parsing	accuracy	We show that a crude way to use dependency length as a parsing feature can substantially improve parsing speed and accuracy in English and Chinese, with more mixed results on German.
Dependency parsing	accuracy	 Table 1: Dependency parsing of POS tag sequences with simple probabilistic split bilexical grammars. The models differ only  in how they weight the same candidate parse trees. Length-sensitive models are larger but can improve dependency accuracy  and speed. (Recall is measured as the fraction of non-punctuation tags whose correct parent (if not the $ symbol) was correctly  recovered by the parser; it equals precision, unless the parser left some sentences unparsed (or incompletely parsed, as in  §4), in
Dependency parsing	Recall	 Table 1: Dependency parsing of POS tag sequences with simple probabilistic split bilexical grammars. The models differ only  in how they weight the same candidate parse trees. Length-sensitive models are larger but can improve dependency accuracy  and speed. (Recall is measured as the fraction of non-punctuation tags whose correct parent (if not the $ symbol) was correctly  recovered by the parser; it equals precision, unless the parser left some sentences unparsed (or incompletely parsed, as in  §4), in
Dependency parsing	precision	 Table 1: Dependency parsing of POS tag sequences with simple probabilistic split bilexical grammars. The models differ only  in how they weight the same candidate parse trees. Length-sensitive models are larger but can improve dependency accuracy  and speed. (Recall is measured as the fraction of non-punctuation tags whose correct parent (if not the $ symbol) was correctly  recovered by the parser; it equals precision, unless the parser left some sentences unparsed (or incompletely parsed, as in  §4), in
parsing	accuracy	(, for instance report a reduction in parsing accuracy of an unlexicalised PCFG from 77.8% to 72.9% if using function labels in training.) also reports a decrease in performance when attempting to integrate his function labelling system with a full parser.
parsing	FLABEL	Both parsing results taking function labels into account in the evaluation (FLABEL) and results not taking them into account in the evaluation (FLABEL-less) are reported in, which shows results on the test set, section 23 of the PTB.
parsing	FLABEL-less	Both parsing results taking function labels into account in the evaluation (FLABEL) and results not taking them into account in the evaluation (FLABEL-less) are reported in, which shows results on the test set, section 23 of the PTB.
parsing	precision	The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank.
parsing	recall	The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank.
parsing	speed	The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank.
beam searching	recall	However, the beam searching reduced the recall from 87.9% to 82.4%.
parsing	accuracy	Our evaluation data is an excerpt from the Monroe corpus that has been used in previous TRIPS research on parsing speed and accuracy ).
bracketing	accuracy	We tested bracketing accuracy on the Monroe corpus, which contains collaborative emergency-management dialogues.
Sentence-level	accuracy	 Table 4: Sentence-level accuracy on 54 TRIPS error  sentences
dialogue generation	speed	The three factors that are most important in evaluating dialogue generation is portability, coverage, and speed.
NLP	coverage	Thesauruses are useful resources for NLP; however, manual construction of thesaurus is time consuming and suffers low coverage.
coreference resolution	precision	Evaluation of coreference resolution systems has traditionally been performed with precision and recall.
coreference resolution	recall	Evaluation of coreference resolution systems has traditionally been performed with precision and recall.
CRFs tense classifier	precision	To evaluate the performance of the CRFs tense classifier, we compute the precision, recall, general accuracy and F, which are defined as follow.
CRFs tense classifier	recall	To evaluate the performance of the CRFs tense classifier, we compute the precision, recall, general accuracy and F, which are defined as follow.
CRFs tense classifier	general	To evaluate the performance of the CRFs tense classifier, we compute the precision, recall, general accuracy and F, which are defined as follow.
CRFs tense classifier	accuracy	To evaluate the performance of the CRFs tense classifier, we compute the precision, recall, general accuracy and F, which are defined as follow.
CRFs tense classifier	F	To evaluate the performance of the CRFs tense classifier, we compute the precision, recall, general accuracy and F, which are defined as follow.
question answering	mean reciprocal rank (MRR)	Chinese question answering system is to return a ranked list of five answer sentences per question and will be strictly evaluated (unsupported answers counted as wrong) using mean reciprocal rank (MRR).
SVM chunker	tolerance	In our experiments, the SVM chunker uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and tolerance of the termination criterion, 0.01 In the base NPs chunking task, the evaluation metrics for base NP chunking include precision P, recall Rand the F β . Usually we refer to the F β as the creditable metric.
SVM chunker	precision	In our experiments, the SVM chunker uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and tolerance of the termination criterion, 0.01 In the base NPs chunking task, the evaluation metrics for base NP chunking include precision P, recall Rand the F β . Usually we refer to the F β as the creditable metric.
Parsing	accuracy	Parsing is a basic task in natural language processing; however, it has not been successful in achieving the accuracy and efficiency required by real world applications.
WSD	Overlap Ambiguity (OA)	To develop WSD, there are two major types of word segmentation ambiguities while there are no unknown word problems with them: (1) Overlap Ambiguity (OA).
word segmentation ambiguities	Overlap Ambiguity (OA)	To develop WSD, there are two major types of word segmentation ambiguities while there are no unknown word problems with them: (1) Overlap Ambiguity (OA).
word segmentation	F-score	Evaluation results show that our word segmentation system achieved 93.3% and 94.7% F-score in UPUC and MSRA open tests, and our NER system got 70.84% and 81.32% F-score in LDC and MSRA open tests.
word segmentation	F-score	Evaluation results show that our word segmentation system achieved 93.3% and 94.7% F-score in UPUC and MSRA open tests, and our NER system got 70.84% and 81.32% F-score in LDC and MSRA open tests.
NER	PER  LOC  ORG  GPE  Overall  Pre.  83.29	 Table 5 Results of NER Task for LDC corpus (in percentage %)  PER  LOC  ORG  GPE  Overall  Pre.  83.29  58.52  61.48  78.66  76.16  Rec.  66.93  18.87  45.19  79.94  66.21  F-score  74.22  28.57  52.09  79.30  70.84
NER	F-score	 Table 5 Results of NER Task for LDC corpus (in percentage %)  PER  LOC  ORG  GPE  Overall  Pre.  83.29  58.52  61.48  78.66  76.16  Rec.  66.93  18.87  45.19  79.94  66.21  F-score  74.22  28.57  52.09  79.30  70.84
NER	accuracy	Much of the NER research was pioneered in the MUC/DUC and Multilingual Entity Task (MET) evaluations, as a result of which significant progress has been made and many NER systems of fairly high accuracy have been constructed.
NER	PERSON	We evaluated the system using data from the third SIGHAN Chinese language processing bakeoff, the goal of which was to perform NER on three types of named entities: PERSON, LO-CATION and ORGANIZATION.
NER	LO-CATION	We evaluated the system using data from the third SIGHAN Chinese language processing bakeoff, the goal of which was to perform NER on three types of named entities: PERSON, LO-CATION and ORGANIZATION.
NER	ORGANIZATION	We evaluated the system using data from the third SIGHAN Chinese language processing bakeoff, the goal of which was to perform NER on three types of named entities: PERSON, LO-CATION and ORGANIZATION.
NER	LOC	 Table 6 The NER performance in MSRA Open test  MSRA NER  Precision Recall  F Score  PER  93.68%  86.37%  89.87  LOC  85.50%  59.67%  70.29  ORG  75.87%  47.48%  58.41  Overall  86.97%  65.56%  74.76
NER	ORG	 Table 6 The NER performance in MSRA Open test  MSRA NER  Precision Recall  F Score  PER  93.68%  86.37%  89.87  LOC  85.50%  59.67%  70.29  ORG  75.87%  47.48%  58.41  Overall  86.97%  65.56%  74.76
WS	precision	Evaluation result shows our WS system has a passable precision in word segmen-tation except for the unknown words recognition .
word segmentation task	F rate	The preliminary experimental result show that in the word segmentation task, our method can achieve 91.00 in F rate for the UPUC Chinese Treebank data, while it at-tends 78.76 F rate for the Microsoft Chinese named entity recognition task.
word segmentation task	F	The preliminary experimental result show that in the word segmentation task, our method can achieve 91.00 in F rate for the UPUC Chinese Treebank data, while it at-tends 78.76 F rate for the Microsoft Chinese named entity recognition task.
NER	recall	After analyzing the result of our NER based on CRFs model, we noticed that it presents a high recall on out-of-vocabulary.
IE	recall	Experiments on the MUC-4 test set show that these new IE patterns improved recall with only a small precision loss.
IE	precision	Experiments on the MUC-4 test set show that these new IE patterns improved recall with only a small precision loss.
topic identification	recall	The system achieved higher precision for topic identification, whereas it achieved higher recall for holder identification.
named entity recognition (NER)	Agirre	This task is closely related to both named entity recognition (NER), which traditionally assigns nouns to a small number of categories and word sense disambiguation (Agirre and 1 http://class.inrialpes.fr/, where the sense fora word is chosen from a much larger inventory of word senses.
Answer Processing (AnsSet	accuracy	When CE was used to rank passages for Answer Processing (AnsSet 2 ), accuracy increased by nearly 9% over the baseline (AnsSet 1 ), while accuracy increased by almost 14% overall when CE was used to select answers directly (AnsSet 3 ).
Answer Processing (AnsSet	accuracy	When CE was used to rank passages for Answer Processing (AnsSet 2 ), accuracy increased by nearly 9% over the baseline (AnsSet 1 ), while accuracy increased by almost 14% overall when CE was used to select answers directly (AnsSet 3 ).
ToggleText Kataku Indonesian-English machine translation Excite English-Japanese machine translation Babelfish English-Japanese machine translation	IR	Indonesian-Japanese dictionary, 14,823 words ToggleText Kataku Indonesian-English machine translation Excite English-Japanese machine translation Babelfish English-Japanese machine translation  In the experiments, we compare the IR score of each translation method.
syntactic and semantic analysis of opendomain natural language text	recall	Although full syntactic and semantic analysis of opendomain natural language text is beyond current technology, a number of papers have been recently published showing that, by using probabilistic or symbolic methods, it is possible to obtain dependencybased representations of unlimited texts with good recall and precision.
syntactic and semantic analysis of opendomain natural language text	precision	Although full syntactic and semantic analysis of opendomain natural language text is beyond current technology, a number of papers have been recently published showing that, by using probabilistic or symbolic methods, it is possible to obtain dependencybased representations of unlimited texts with good recall and precision.
information retrieval (IR)	similarity	In information retrieval (IR), also the focus of Lebart & Rajman's work, similarity is at heart of most techniques seeking an optimal match between query and document.
classification	accuracy	We also calculated the classification accuracy of our method.
answer selection	accuracy	 Table 2: Comparison of three different algorithms for answer selection on 7 additional QA characters.  The table shows the number of answers and the number of questions collected for each character. The  accuracy and the improvement over the baseline numbers are given in percentages.
semantic currently	f-measure)	Our system which relies on a symbolic approach using deep parsing and description logics for semantic currently scores 64% (f-measure) for identifying and describing accurately the referents.
Classification	accuracy	 Table 3: Classification accuracy with each fea- ture class removed from the union of all feature  classes.
argument interpretation	BIAS	Our argument interpretation mechanism has been implemented in a system called BIAS (Bayesian Interactive Argumentation System).
IA	FB	1 We decided to put Dale and Reiter's hypothesis to the test by an evaluation of the output of dif-1 A separate argument for IA involves tractability, but although some alternatives (such as FB) are intractable, others (such as GA) are only polynomial, and can therefore not easily be dismissed on purely computational grounds.
referent identification	TYPE	Their results show that referent identification in MAPTASK often requires no more than a TYPE attribute, so that none of the algorithms performed better than a baseline.
machine translation	BLEU	One of the best-known comparative studies of evaluation techniques was by who proposed the BLEU metric for machine translation and showed that BLEU correlated well with human judgements when comparing several machine translation systems.
pseudoknot prediction	accuracy	Finally, we show some experimental results on pseudoknot prediction for three RNA families using SMCFG algorithm, which show good prediction accuracy.
machine translation	accuracy	During the last four years, various implementations and extentions to phrase-based statistical models () have led to significant increases in machine translation accuracy.
SPMT	Bleu	The results show that the SPMT models clearly outperform the phrase-based systems -the 95% confidence intervals computed via bootstrap resampling in all cases are around 1 Bleu point.
machine translation (MT)	ROUGE	The introduction of automated evaluation procedures, such as BLEU () for machine translation (MT) and ROUGE ( for summarization, have prompted much progress and development in both of these areas of research in Natural Language Processing (NLP).
MT	BLEU-esque	In this paper, we focus on the intricacies involved in evaluating MT results and address two prominent problems associated with the BLEU-esque metrics, namely their lack of support for paraphrase matching and the absence of recall scoring.
MT	recall scoring	In this paper, we focus on the intricacies involved in evaluating MT results and address two prominent problems associated with the BLEU-esque metrics, namely their lack of support for paraphrase matching and the absence of recall scoring.
paraphrase matching	recall scoring	In this paper, we focus on the intricacies involved in evaluating MT results and address two prominent problems associated with the BLEU-esque metrics, namely their lack of support for paraphrase matching and the absence of recall scoring.
parsing	accuracy	A common theme that has emerged from this research is the claim that lexicalization of PCFGs, which has been proven highly beneficial for other languages 1 , is detrimental for parsing accuracy of German.
parsing	F-score	The results, summarized in, show that parsing results for all unlexicalized experiments show roughly the same 20 point difference in F-score that were obtained for the lexicalized models in Experiment I. We can therefore conclude that the difference in parsing performance is robust across two parsers with different parameter settings, such as lexicalization and markovization.
parsing	accuracy	Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures.
parsing	accuracy	In the experiments, we observed that the hybrid model significantly improved the parsing speed, by around three to four times speed-ups, and accuracy, by around two points in both precision and recall, over the previous model.
parsing	precision	In the experiments, we observed that the hybrid model significantly improved the parsing speed, by around three to four times speed-ups, and accuracy, by around two points in both precision and recall, over the previous model.
parsing	recall	In the experiments, we observed that the hybrid model significantly improved the parsing speed, by around three to four times speed-ups, and accuracy, by around two points in both precision and recall, over the previous model.
parsing	precision	3 When parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly.
parsing	recall	3 When parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly.
parsing	recall	3 When parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly.
parsing	accuracy	Our models significantly increased not only the parsing speed but also the parsing accuracy.
parsing	accuracy	Our models significantly increased not only the parsing speed but also the parsing accuracy.
SRL	PARA	Training times for IBL algorithms are very much faster than for other widely used techniques for SRL (e.g. parsing, support vector machines, perceptrons, etc); and the feature reduction effects of PARA yield testing and processing speeds of around 1.0 second per sentence for kNN and 0.9 second per sentence for PML respectively, suggesting that IBL could be a more practical way to perform SRL for NLP applications where it is employed; such as real-time Machine Translation or Automatic Speech Recognition.
topic classification	slot error rate	The performance of topic classification and semantic classification are measured in terms of topic error rate and slot error rate respectively.
semantic classification	slot error rate	The performance of topic classification and semantic classification are measured in terms of topic error rate and slot error rate respectively.
translation	accuracy	We use BLEU scores to measure the translation accuracy.
coreference resolution	BESTCUT	In experiments , the graph cutting algorithm for coreference resolution, called BESTCUT, achieves state-of-the-art performance.
coreference resolution	BESTCUT	We have devised a graph partitioning method for coreference resolution, called BESTCUT, which is inspired from the well-known graph-partitioning algorithm Min-Cut (.
coreference resolution	BESTCUT	In Section 2 we describe the coreference resolution method that uses the BESTCUT clusterization; Section 3 describes the approach we have implemented for detecting mentions in texts; Section 4 reports on the experimental results; Section 5 discusses related work; finally, Section 6 summarizes the conclusions.
IE task	precision	This range of precision is practical for the IE task, because precision is more important than recall for significant interactions that tend to be described in many abstracts (as shown by the next experiment), and too-low recall accompanying too-high precision requires an excessively large source text.
IE task	recall	This range of precision is practical for the IE task, because precision is more important than recall for significant interactions that tend to be described in many abstracts (as shown by the next experiment), and too-low recall accompanying too-high precision requires an excessively large source text.
IE task	recall	This range of precision is practical for the IE task, because precision is more important than recall for significant interactions that tend to be described in many abstracts (as shown by the next experiment), and too-low recall accompanying too-high precision requires an excessively large source text.
parsing	accuracy	We show here that there is a natural analogy between parsing and the protein folding problem, and demonstrate that CKY can find the native structures of a simplified lattice model of proteins with high accuracy.
protein folding	accuracy	Models of protein folding additionally aim to explain the process by which this structure formation takes place, and their validity depends not only on the accuracy of the predicted structures, but also on their physical plausibility.
parsing	accuracy	Using these methods, we can maintain high parsing accuracy while dramatically reducing the model size.
sentiment classification	accuracy	A number of studies have investigated sentiment classification at document level, e.g.,, and at sentence level, e.g., (); however, the accuracy is still less than desirable.
tagging	accuracy	The main contribution of this work are: (1) a comprehensive evaluation of three transductive algorithms (Transductive SVM, Spectral Graph Transducer, and anew technique called Transductive Clustering) as well as an inductive SVM on this task; and (2) a demonstration that lexicon learning is a worthwhile investment and leads to significant improvements in the tagging accuracy for dialectal Arabic.
source extraction	F-measure	For source extraction in particular, our system achieves an F-measure of 78.1, significantly outperforming previous results in this area (), which obtained an F-measure of 69.4 on the same corpus.
source extraction	F-measure	For source extraction in particular, our system achieves an F-measure of 78.1, significantly outperforming previous results in this area (), which obtained an F-measure of 69.4 on the same corpus.
spelling correction	soon	Aside from counting bigrams, various tasks are attainable using webbased models: spelling correction, adjective ordering, compound noun bracketing, countability detection, and soon ().
parsing	accuracy	Standard measures of parsing accuracy, plus complete match accuracy, are shown in table 1.
estimation	significance	It is also interesting to note that the best result on the validation set for estimation We measured significance of all the experiments in this paper with the randomized significance test of the loss with data-defined kernels (12) and was achieved when the parameter A is close to the inverse of the first component of the learned decision vector, which confirms the motivation for these kernels.
parsing	precision (P)	We believe that this method can be applied to most parsing models to achieve a significant improvement.: Percentage labeled constituent recall (R), precision (P), combination of both (F 1 ) on the testing set.
PP attachment task	Att	Thus for the PP attachment task we have binary labels Att, and four input variables -v, n 1 , p, n 2 . We work with the standard dataset previously used for this task by other researchers (Ratna-.
Information Retrieval	precision	The Fscore is used in a similar fashion to Information Retrieval exercises, with precision and recall defined as the percentage of correctly "retrieved" examples fora cluster (divided by total cluster size), and recall as the percentage of correctly "retrieved" examples fora cluster (divided by total class size).
Information Retrieval	recall	The Fscore is used in a similar fashion to Information Retrieval exercises, with precision and recall defined as the percentage of correctly "retrieved" examples fora cluster (divided by total cluster size), and recall as the percentage of correctly "retrieved" examples fora cluster (divided by total class size).
Information Retrieval	recall	The Fscore is used in a similar fashion to Information Retrieval exercises, with precision and recall defined as the percentage of correctly "retrieved" examples fora cluster (divided by total cluster size), and recall as the percentage of correctly "retrieved" examples fora cluster (divided by total class size).
record extraction task	standard error	 Table 2: Precision, recall, and F1 performance for  the record extraction task. The standard error is  calculated over 10 cross-validation trials.
entailment recognition	accuracy	Given that the overall system accuracies hovered between 50 and 60 percent with a baseline of 50 % 1 , this suggests that a better integration of syntax, compositional and lexical semantics might improve entailment recognition accuracy.
summarize	precision	It is a commonly used metric to summarize precision and recall in one measure.
summarize	recall	It is a commonly used metric to summarize precision and recall in one measure.
information extraction	precision	The precision of the information extraction stage is essential to the success of a QA system, because it places an upper bound on the precision of the entire system.
WSD	precision	Throughout the paper we will use the concepts of precision and recall to measure the performance of WSD systems, where precision refers to the ratio of correct answers to the total number of answers given by the system, and recall indicates the ratio of correct answers to the total number of instances.
WSD	recall	Throughout the paper we will use the concepts of precision and recall to measure the performance of WSD systems, where precision refers to the ratio of correct answers to the total number of answers given by the system, and recall indicates the ratio of correct answers to the total number of instances.
MT-based	recall	We can see that our MT-based approach has achieved significantly better recall than the other two automatic methods.
preposition interpretation	correctness	The preposition interpretation method achieves between 84% and 89% correctness for the six prepositions supported by the hand-tagged PP corpus; for prepositions without annotated corpus data, the performance seems to drop by around 10 to 20 percent points.
VPC identification	precision	 Table 5: Results for VPC identification only (P =  precision, R = recall, F = F-score)
VPC identification	recall	 Table 5: Results for VPC identification only (P =  precision, R = recall, F = F-score)
VPC identification	F	 Table 5: Results for VPC identification only (P =  precision, R = recall, F = F-score)
VPC identification	F-score	 Table 5: Results for VPC identification only (P =  precision, R = recall, F = F-score)
PP attachment	accuracy	In the PP attachment research for other languages there is often a comparison of the disambiguation accuracy with the English results.
classification	accuracy	In some machine learning algorithms the unbalanced distribution of examples can yield a significant loss in classification accuracy.
pattern acquisition	Collins criterion	For both setups we used the best pattern selection criterion for pattern acquisition, i.e. the Collins criterion.
SPL	LRPL	Section 2 and 3 describe SPL and LRPL respectively.
anaphora resolution	MARS	We present anew evaluation of three state-of-the-art algorithms for anaphora resolution -GuiTAR, JavaRAP, MARS -on the basis of a portion of Susan Corpus (derived from Brown Corpus) a much richer testbed than the ones previously used for evaluation, and in any case a much more comparable source with such texts as newspaper articles and stories.
parsing task	accuracy	We achieve promising results both on the simple parsing task, where the accuracy of the parser is measured on the standard Parseval measures, and also on the parsing task where the more complex labels of PropBank are taken into account.
PTB parsing task 88.8 88.6 88.9 PTB	Percentage F-measure (F)	To reduce variability, we add some of the tag-verb pairs licensing these argumental labels to the vocabu-F RP PropBank training and PropBank parsing task 82.3 82.1 82.4 PropBank training and PTB parsing task 88.8 88.6 88.9 PTB training and PTB parsing task 88.6 88.3 88.9: Percentage F-measure (F), recall (R), and precision (P) of our SSN parser on two different tasks and the original SSN parser.
PTB parsing task 88.8 88.6 88.9 PTB	recall (R)	To reduce variability, we add some of the tag-verb pairs licensing these argumental labels to the vocabu-F RP PropBank training and PropBank parsing task 82.3 82.1 82.4 PropBank training and PTB parsing task 88.8 88.6 88.9 PTB training and PTB parsing task 88.6 88.3 88.9: Percentage F-measure (F), recall (R), and precision (P) of our SSN parser on two different tasks and the original SSN parser.
PTB parsing task 88.8 88.6 88.9 PTB	precision (P)	To reduce variability, we add some of the tag-verb pairs licensing these argumental labels to the vocabu-F RP PropBank training and PropBank parsing task 82.3 82.1 82.4 PropBank training and PTB parsing task 88.8 88.6 88.9 PTB training and PTB parsing task 88.6 88.3 88.9: Percentage F-measure (F), recall (R), and precision (P) of our SSN parser on two different tasks and the original SSN parser.
PTB parsing task 88.6 88.3 88.9	Percentage F-measure (F)	To reduce variability, we add some of the tag-verb pairs licensing these argumental labels to the vocabu-F RP PropBank training and PropBank parsing task 82.3 82.1 82.4 PropBank training and PTB parsing task 88.8 88.6 88.9 PTB training and PTB parsing task 88.6 88.3 88.9: Percentage F-measure (F), recall (R), and precision (P) of our SSN parser on two different tasks and the original SSN parser.
PTB parsing task 88.6 88.3 88.9	recall (R)	To reduce variability, we add some of the tag-verb pairs licensing these argumental labels to the vocabu-F RP PropBank training and PropBank parsing task 82.3 82.1 82.4 PropBank training and PTB parsing task 88.8 88.6 88.9 PTB training and PTB parsing task 88.6 88.3 88.9: Percentage F-measure (F), recall (R), and precision (P) of our SSN parser on two different tasks and the original SSN parser.
PTB parsing task 88.6 88.3 88.9	precision (P)	To reduce variability, we add some of the tag-verb pairs licensing these argumental labels to the vocabu-F RP PropBank training and PropBank parsing task 82.3 82.1 82.4 PropBank training and PTB parsing task 88.8 88.6 88.9 PTB training and PTB parsing task 88.6 88.3 88.9: Percentage F-measure (F), recall (R), and precision (P) of our SSN parser on two different tasks and the original SSN parser.
parsing task	Parseval	To evaluate the first parsing task, we compute the standard Parseval measures of labelled recall and precision of constituents, taking into account not only the 33 original labels but also the 580 newly introduced PropBank labels.
parsing task	recall	To evaluate the first parsing task, we compute the standard Parseval measures of labelled recall and precision of constituents, taking into account not only the 33 original labels but also the 580 newly introduced PropBank labels.
parsing task	precision	To evaluate the first parsing task, we compute the standard Parseval measures of labelled recall and precision of constituents, taking into account not only the 33 original labels but also the 580 newly introduced PropBank labels.
dependency parsers	accuracy	As usual for dependency parsers, we measure the parsing quality by computing the structural accuracy (the ratio of correct subordinations to all subordinations) and labelled accuracy (the ratio of all correct subordinations that also bear the correct label to all subordinations).
dependency parsers	labelled accuracy	As usual for dependency parsers, we measure the parsing quality by computing the structural accuracy (the ratio of correct subordinations to all subordinations) and labelled accuracy (the ratio of all correct subordinations that also bear the correct label to all subordinations).
parsing	accuracy	shows that the proportion of sentences with rare phenomena is somewhat higher in the NEGRA sentences, and consequently the net gain in parsing accuracy is smaller; apparently the advantage of reducing the problem size is almost cancelled by the disadvantage of losing necessary coverage.
Parsing	coverage	Phenomena structural labelled removed accuracy accuracy none 90.5% 88.6% = 0 90.5% 88.7% < 10 90.6% 88.8% < 100 90.7% 88.6% >= 100 90.5% 88.6% >= 10 90.4% 88.5% > 0 90.5% 88.6% all 90.6% 88.7%: Parsing with coverage reduced stepwise.
Parsing	coverage	Phenomena structural labelled removed accuracy accuracy none 90.5% 88.6% A (1,3,4,6-10,13,16,18-21) 90.9% 89.0% B 90.4% 88.5% C 90.4% 88.6% 1-21 90.6% 88.7%: Parsing with coverage reduced by increasing leakage.
parsing  the same text	coverage	 Table 3: Structural and labelled accuracy when parsing  the same text with reduced coverage.
alignment	accuracy	As a result, both alignment quality and translation accuracy were slightly improved.
translation	accuracy	As a result, both alignment quality and translation accuracy were slightly improved.
translation evaluation	WER	As for translation evaluation, we used the following measures: WER (word error rate) or mWER (multireference word error rate) The WER is the minimum number of substitution, insertion and deletion operations that must be performed to convert the generated sentence into the reference target sentence.
translation evaluation	WER	As for translation evaluation, we used the following measures: WER (word error rate) or mWER (multireference word error rate) The WER is the minimum number of substitution, insertion and deletion operations that must be performed to convert the generated sentence into the reference target sentence.
sense disambiguation (WSD)	accuracy	It is likely that accurate word-level semantic disambiguation would benefit a number of different types of NLP application; however it is generally acknowledged byword sense disambiguation (WSD) researchers that current levels of accuracy need to be improved before WSD technology can usefully be integrated into applications.
tagging tasks	complexity	Besides, performance on the tagging tasks, complexity and training time by each model are reported.
NER task	precision	 Table 4: Performance of baseline models on the  NER task. Number of parameters, precision, re- call, and F-score are reported for each model.
NER task	re- call	 Table 4: Performance of baseline models on the  NER task. Number of parameters, precision, re- call, and F-score are reported for each model.
NER task	F-score	 Table 4: Performance of baseline models on the  NER task. Number of parameters, precision, re- call, and F-score are reported for each model.
Generalisation	accuracy	Generalisation performance is measured on the word accuracy level: if the entire phonological transcription of the word is predicted correctly, or if all three aspects of the morphological analysis are predicted correctly, the word is counted correct.
NER	D	 Table 2: NER experiment with D=6
parsing	accuracy	A mixture grammar fit with the EM algorithm shows improvement over a single PCFG, both in parsing accuracy and in test data likelihood.
statistical estimation	accuracy	While such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the accuracy of a parser.
parsing	accuracy	We demonstrate that a mixture fit with the EM algorithm gives improved parsing accuracy and test data likelihood.
parsing	accuracy	We evaluate parsing accuracy by comparing the undirected dependency links in the parser outputs against the undirected links in the treebank.
parsing	accuracy-and	Notably, the pointwise mutual information and distance features significantly improve parsing accuracy-and yet we know of no other research that has investigated these features in this context.
argument recognition	accuracy	In this study, we first evaluated the performance of the MOLT kernel for argument recognition, and found that the MOLT kernel cannot achieve a high accuracy if used in its original form.
Semantic Role Labeling (SRL)	accuracy	Recent work on Semantic Role Labeling (SRL) has shown that to achieve high accuracy a joint inference on the whole predicate argument structure should be applied.
Semantic Role Labeling (SRL))	accuracy	Recent work on Semantic Role Labeling (SRL)) has shown that to achieve high labeling accuracy a joint inference on the whole predicate argument structure should be applied.
classification	accuracy	The results show that this classification problem can be learned with high accuracy.
parsing	accuracy	Experimental evaluation shows that the proposed extension improves the parsing accuracy of our base parser in 9 languages.
parsing	F	At a given step of the parsing process, let T OP be the top of the stack and F IRST , the first token of the input list, and arc, the relation holding between ahead and a dependent.
parsing	IRST	At a given step of the parsing process, let T OP be the top of the stack and F IRST , the first token of the input list, and arc, the relation holding between ahead and a dependent.
parsing	arc	At a given step of the parsing process, let T OP be the top of the stack and F IRST , the first token of the input list, and arc, the relation holding between ahead and a dependent.
machine translation community	WER (word error rate)	The commonly used criteria to evaluate the translation results in the machine translation community are: WER (word error rate), PER (positionindependent word error rate), BLEU (), and NIST).
machine translation community	PER (positionindependent word error rate)	The commonly used criteria to evaluate the translation results in the machine translation community are: WER (word error rate), PER (positionindependent word error rate), BLEU (), and NIST).
machine translation community	BLEU	The commonly used criteria to evaluate the translation results in the machine translation community are: WER (word error rate), PER (positionindependent word error rate), BLEU (), and NIST).
translation	BLEU	The outcome was that the confidence measures did not result in improvements of the translation quality measured with the BLEU and NIST scores.
translation	BLEU score	In the experiments on the Chinese-English FBIS corpus the translation performance is improved by 0.4% of the BLEU score compared to the performance only with Champollion.
FBIS  Segmentation  Champollion  Chinese  English  Chinese  English  Train  Sentences  739 899  177 798  Running Words	Average Sentence Length	 Table 2: Corpus Statistics: FBIS  Segmentation  Champollion  Chinese  English  Chinese  English  Train  Sentences  739 899  177 798  Running Words 8 588 477 10 111 752 7 659 776 9 801 257  Average Sentence Length  11.6  13.7  43.1  55.1  Vocabulary  34 896  56 573  34 377  55 775  Singletons  4 775  19 283  4 588  19 004  Evaluation  Sentences  878  3 513  878  3 513  Running Words  24 111  105 516  24 111  105 516  Vocabulary  4 095  6 802  4 095  6 802  OOVs (Running Words)  109  2 257  119  2 309
word substitution	METEOR	Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (), which employs aversion of edit distance for word substitution and reordering; METEOR (, which uses stemming and WordNet synonymy; and a linear regression model developed by), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching.
Translation	BLEU score	Translation performance was measured in terms of BLEU score, NIST score, word-error rate (WER), and position independent error rate (PER).
Translation	word-error rate (WER)	Translation performance was measured in terms of BLEU score, NIST score, word-error rate (WER), and position independent error rate (PER).
Translation	position independent error rate (PER)	Translation performance was measured in terms of BLEU score, NIST score, word-error rate (WER), and position independent error rate (PER).
translation	BLEU	In general, none of the above measures is alone sufficiently informative about translation quality, however, in the community there seems to be a preference toward reporting results with BLEU.
Translation	IBM	 Table 1: Translation results (IBM BLEU) for each system on the Fr-En '06 Shared Task 'Development Set' (used for MER
information extraction-based information retrieval	accuracy	However, for applications such as information extraction-based information retrieval (e.g., the identification of documents mentioning a specific gene), document-level accuracy is a relevant gauge of system performance.
normalization	precision	For normalization alone, at the mention level our optimized normalization system achieved 0.882 precision, 0.704 recall, and 0.783 F-measure.
normalization	recall	For normalization alone, at the mention level our optimized normalization system achieved 0.882 precision, 0.704 recall, and 0.783 F-measure.
normalization	F-measure	For normalization alone, at the mention level our optimized normalization system achieved 0.882 precision, 0.704 recall, and 0.783 F-measure.
NER	precision	For the combined NER system, the performance was 0.718 precision, 0.626 recall, and 0.669 Fmeasure at the mention level.
NER	recall	For the combined NER system, the performance was 0.718 precision, 0.626 recall, and 0.669 Fmeasure at the mention level.
NER	Fmeasure	For the combined NER system, the performance was 0.718 precision, 0.626 recall, and 0.669 Fmeasure at the mention level.
NER	mention	For the combined NER system, the performance was 0.718 precision, 0.626 recall, and 0.669 Fmeasure at the mention level.
relation extraction	accuracy	We believe that a combination of the two approaches can inherit the advantages of each method and lead to improved relation extraction accuracy.
PMI	BIND	The comparative results for PMI and HG are shown in, together with the scores for three human curated databases: HPRD, BIND and Reactome.
semantic role labeling (SRL)	extent	The above problem can be tackled by using semantic role labeling (SRL) because it not only recognizes main roles, such as agents and objects, but also extracts adjunct roles such as location, manner, timing, condition, and extent.
SRL	F-score	In the newswire domain, have demonstrated that full-parsing and SRL can improve the performance of relation extraction, resulting in an F-score increase of 15% (from 67% to 82%).
relation extraction	F-score	In the newswire domain, have demonstrated that full-parsing and SRL can improve the performance of relation extraction, resulting in an F-score increase of 15% (from 67% to 82%).
Suffix	POS	Suffix rules were composed based on information from Michigan State University's Suffixes and Parts of Speech web page for Graduate Record Exams Most suffixes did well in determining the actual POS assigned to the word.
role detection	accuracy	We show that this algorithm results in a role detection accuracy of 83% on unseen test data, where the random baseline is 33.3%.
role detection	accuracy	We show that this aggre-gation mechanism improves the role detection accuracy from 66.7% (when ag-gregating over a single meeting) to 83% (when aggregating over 5 meetings).
role detection	accuracy	Over this test set we obtained a role detection accuracy of 83%.
parse	coverage	The purpose of our evaluation is to explore the extent to which we can achieve a better balance between parse time and coverage using backbone parsing with pruning compared to the original best-first algorithm.
parsing	accuracy	For our comparison we used an excerpt from the Monroe corpus that has been used in previous TRIPS research on parsing speed and accuracy) consisting of dialogues s2, s4, s16 and s17.
problem solving or procedural knowledge	BEETLE	Most natural language tutorial applications have focused on coaching either problem solving or procedural knowledge (e.g. Steve,),), BEETLE (),), inter alia).
generative	accuracy	Successful discriminative parsers have used generative models to reduce training time and raise accuracy above generative baselines).
prediction	accuracy	Through a first series of experiments, in which we train on Reuters newswire text and test either on the same type of data or on general or fictional text, we demonstrate that the system exhibits log-linear increases in prediction accuracy with increasing numbers of training examples.
word prediction task	TRAIN-REUTERS	We create incrementally-sized training sets for the word prediction task on the basis of the TRAIN-REUTERS set.
word prediction	TRAIN-REUTERS	The word prediction accuracy learning curves computed on the three test sets, and trained on increasing portions of TRAIN-REUTERS, are displayed in.
information  extraction	ACE value 6	 Table 2. Overall Performance  The improved NE results brought better per- formance for the subsequent stages of information  extraction too. We use the NE outputs from Max- Ent-Ranker as inputs for coreference resolver and  relation tagger. The ACE value 6 of entity detec- tion (mention detection + coreference resolution)  is increased from 73.2 to 76.5; the ACE value of  relation detection is increased from 34.2 to 34.8.
information  extraction	ACE	 Table 2. Overall Performance  The improved NE results brought better per- formance for the subsequent stages of information  extraction too. We use the NE outputs from Max- Ent-Ranker as inputs for coreference resolver and  relation tagger. The ACE value 6 of entity detec- tion (mention detection + coreference resolution)  is increased from 73.2 to 76.5; the ACE value of  relation detection is increased from 34.2 to 34.8.
coreference resolver	ACE value 6	 Table 2. Overall Performance  The improved NE results brought better per- formance for the subsequent stages of information  extraction too. We use the NE outputs from Max- Ent-Ranker as inputs for coreference resolver and  relation tagger. The ACE value 6 of entity detec- tion (mention detection + coreference resolution)  is increased from 73.2 to 76.5; the ACE value of  relation detection is increased from 34.2 to 34.8.
coreference resolver	ACE	 Table 2. Overall Performance  The improved NE results brought better per- formance for the subsequent stages of information  extraction too. We use the NE outputs from Max- Ent-Ranker as inputs for coreference resolver and  relation tagger. The ACE value 6 of entity detec- tion (mention detection + coreference resolution)  is increased from 73.2 to 76.5; the ACE value of  relation detection is increased from 34.2 to 34.8.
Translation Shortcuts	accuracy	We present our Translation Shortcuts facility, which minimizes the need for interactive verification of sentences after they have been vetted once, considerably speeds throughput while maintaining accuracy , and allows use by minimally literate patients for whom any mode of text entry might be difficult.
SLT	accuracy	In general, SLT applications have been able to achieve acceptable accuracy only by staying within restricted topics, in which fixed phrases could be used (e.g., www.phraselator.com), or in which grammars for automatic speech recognition (ASR) and machine translation (MT) could be optimized.
speech translation	accuracy	Sehda's 2-way speech translation system, S-MINDS, interprets between provider and patient in routine medical interactions with very high accuracy.
interprets between provider and patient in routine medical interactions	accuracy	Sehda's 2-way speech translation system, S-MINDS, interprets between provider and patient in routine medical interactions with very high accuracy.
translating	accuracy	Sehda's speech translation system, S-MINDS, focuses on translating in such situations with extremely high accuracy.
Name disambiguation	accuracy	 Table 1: Name disambiguation results (accuracy/F-measure) at a glance. The baseline is the relative frequency of the majority  name. "200" and "100" give the averaged results (over five different runs) using 200 and 100 randomly selected training instances  per ambiguous name. The weighted average is calculated based on the number of test instances per task. "Full" and "Trans" refer  to the results using the full network (pre-transformation) or the pared-down network (with transformation), respectively.
Name disambiguation	F-measure	 Table 1: Name disambiguation results (accuracy/F-measure) at a glance. The baseline is the relative frequency of the majority  name. "200" and "100" give the averaged results (over five different runs) using 200 and 100 randomly selected training instances  per ambiguous name. The weighted average is calculated based on the number of test instances per task. "Full" and "Trans" refer  to the results using the full network (pre-transformation) or the pared-down network (with transformation), respectively.
Lexical Chaining	Importance	Some of them correspond to full summarization methods by themselves: Lexical Chaining (, Relationship Mapping (, and Importance of Topics (Larocca).
Relationship Mapping	Importance	Some of them correspond to full summarization methods by themselves: Lexical Chaining (, Relationship Mapping (, and Importance of Topics (Larocca).
Statistical classification	accuracy	Statistical classification techniques for natural-language call routing systems have matured to the point where it is possible to distinguish between several hundreds of semantic categories with an accuracy that is sufficient for commercial deployments.
Steering application design	accuracy	Steering application design and research based on in-coverage accuracy is not suitable for these types of applications because a large fraction of the users are na¨ıvesna¨ıves and tend to use more natural and unconstrained speech inputs.
Speech recognition word	accuracy	Speech recognition word accuracy is 79%, however, Report Accuracy, which is after the speech has been processed by the NL, is 84%.
Speech recognition word	Report	Speech recognition word accuracy is 79%, however, Report Accuracy, which is after the speech has been processed by the NL, is 84%.
Translation	WER	 Table 5: Translation performance for the Chinese-English IWSLT task  WER[%] PER[%] NIST BLEU[%]  IWSLT04  baseline  47.3  38.2  7.78  39.1  source reordering  46.3  37.2  7.70  40.9  IWSLT05  baseline  45.0  37.3  7.40  41.8  source reordering  44.6  36.8  7.51  42.3  IWSLT06  baseline  67.4  50.0  6.65  22.4  source reordering  65.6  50.4  6.46  23.3  source reordering+non-monotone decoder  66.5  50.3  6.52  22.4
Translation	PER	 Table 5: Translation performance for the Chinese-English IWSLT task  WER[%] PER[%] NIST BLEU[%]  IWSLT04  baseline  47.3  38.2  7.78  39.1  source reordering  46.3  37.2  7.70  40.9  IWSLT05  baseline  45.0  37.3  7.40  41.8  source reordering  44.6  36.8  7.51  42.3  IWSLT06  baseline  67.4  50.0  6.65  22.4  source reordering  65.6  50.4  6.46  23.3  source reordering+non-monotone decoder  66.5  50.3  6.52  22.4
Translation	BLEU	 Table 5: Translation performance for the Chinese-English IWSLT task  WER[%] PER[%] NIST BLEU[%]  IWSLT04  baseline  47.3  38.2  7.78  39.1  source reordering  46.3  37.2  7.70  40.9  IWSLT05  baseline  45.0  37.3  7.40  41.8  source reordering  44.6  36.8  7.51  42.3  IWSLT06  baseline  67.4  50.0  6.65  22.4  source reordering  65.6  50.4  6.46  23.3  source reordering+non-monotone decoder  66.5  50.3  6.52  22.4
translation	BLEU score	In terms of translation quality, the final BLEU score at the largest beam setting is 0.2614, significantly higher than Pharaoh's 0.2354 as reported in ).
MT	BLEU	Scores for sentences with reordered adjuncts in an ideal situation  In the first experiment, we attempted to determine whether the dependency-based measure is biased towards statistical MT output, a problem that has been observed for n-gram-based metrics like BLEU and NIST.
translation	BLEU	As in the previous experiment, the translation was scored using BLEU, NIST, GTM, TER, METEOR, and the dependency-based method.
translation	TER	As in the previous experiment, the translation was scored using BLEU, NIST, GTM, TER, METEOR, and the dependency-based method.
translation	METEOR	As in the previous experiment, the translation was scored using BLEU, NIST, GTM, TER, METEOR, and the dependency-based method.
statistical parsing	accuracy	Yet have recently demonstrated that existing statistical parsing techniques can be usefully modified to analyse with promising accuracy.
word learning task	Aver-age Precision (AP)	As an evaluation measure of the model performance in the word learning task, we adopted Aver-age Precision (AP), recently used by.
translation	BLEU	We evaluated the translation quality of the system using the BLEU metric ().
MT	BLEU	The importance of NEs is not yet reflected in the evaluation methods used in the MT community, the most common of which is the BLEU metric.
MT	BLEU	However, we believe that such integration is more important for practical uses of MT than BLEU indicates.
NE translation	BLEU	Although there are metrics that directly address NE translation performance , we chose to use BLEU because our purpose is to assess NE translation within MT, and BLEU is currently the standard metric for MT.
NE translation	BLEU	Although there are metrics that directly address NE translation performance , we chose to use BLEU because our purpose is to assess NE translation within MT, and BLEU is currently the standard metric for MT.
NE translation	BLEU	Although there are metrics that directly address NE translation performance , we chose to use BLEU because our purpose is to assess NE translation within MT, and BLEU is currently the standard metric for MT.
translation	BLEU metric	To evaluate the result of the translation, the BLEU metric ( was used.
word alignment	BLEU score	MT results show that word alignment based on this method can be used in a phrase-based machine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment.
MT	BLEU	Our method follows and substantially extends the earlier work of, who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement.
translation	BLEU	As in the previous experiment, the translation was scored using BLEU, NIST, GTM, TER, METEOR, and our labelled dependencybased method.
translation	TER	As in the previous experiment, the translation was scored using BLEU, NIST, GTM, TER, METEOR, and our labelled dependencybased method.
translation	METEOR	As in the previous experiment, the translation was scored using BLEU, NIST, GTM, TER, METEOR, and our labelled dependencybased method.
translation	Bleu score	The 2005 workshop evaluated translation quality only in terms of Bleu score.
machine translation	Bleu	We examined three different ways of manually evaluating machine translation quality: • Assigning scores based on five point adequacy and fluency scales • Ranking translated sentences relative to each other • Ranking the translations of syntactic constituents drawn from the source sentence   The past two ACL workshops on machine translation used Bleu as the sole automatic measure of translation quality.
phrase translation"	accuracy	2. With the purpose of evaluating the changes related only to this small set of very promising phrases, we introduce anew measure, Apt, which computes "phrase translation" accuracy fora given list of source phrases.
Phrase Translation	Accuracy	 Table 1: "Phrase Translation" Accuracy (test set).
MT	BLEU	This approach has been shown experimentally to produce large improvements in performance not only over the baseline rule-based system that it corrects, but also over a similar statistical phrase-based MT system used in standalone mode, i.e. translating the "real" source text directly: Simard et al. report a reduction in post-editing effort of up to a third when compared to the input rule-based translation, and as much as 5 BLEU points improvement over the direct SMT approach.
SMT	BLEU	For the News Commentary domain, the APE strategy (SYSTRAN+PORTAGE lines) clearly outperforms the direct SMT strategy (PORTAGE lines): translating into English, the gain exceeds 1.5 BLEU points, while for French, it is close to 3 BLEU points.
SMT	BLEU	For the News Commentary domain, the APE strategy (SYSTRAN+PORTAGE lines) clearly outperforms the direct SMT strategy (PORTAGE lines): translating into English, the gain exceeds 1.5 BLEU points, while for French, it is close to 3 BLEU points.
SMT	APE	Instead, SMT eventually catches up with APE (anywhere between 100K and 1M sentence pairs), beyond which point both approaches appear to be more or less equivalent.
MT evaluation	Bleu metric	The most commonly used MT evaluation metric in recent years has been IBM's Bleu metric ().
translation	BLEU	Experimental results demonstrate significant improvement of translation quality in terms of BLEU.
Sentence Level Machine Translation Evaluation	BLEU	Sentence Level Machine Translation Evaluation as a Ranking Problem: one step aside from BLEU
MT evaluation	BLEU	The state-of-the-art automatic MT evaluation is an n-gram based metric represented by BLEU () and its variants.
MT	BLEU	We further assume that the degree of difficulty of a phrase is directly correlated with the quality of the translation produced by the MT system, which can be approximated using an automatic evaluation metric, such as BLEU ().
MT quality	BLEU	The reason is that, while MT quality aspects are diverse, BLEU limits its scope to the lexical dimension.
ASG	coverage	In Section 6, we discuss current performance of ASG (coverage and speed), and in Section 7, related work.
POS tagging	accuracy	The POS tagging results using ERTS are comparable to state of the art POS tagging using RTS with an accuracy of 96.13% for ERTS and 96.15% for RTS.
tagger	accuracy	This obviously sets an upper bound on tagger accuracy using methods that are purely based on a manually constructed lexicon.
SUMMARY field search	RUN	 Table 2: Results for SUMMARY field search. (RUN mt+t run provides the best results in all cases.)
Narrative retellings	mean pause duration	Narrative retellings provide a natural, conversational speech sample that can be analyzed for many of the characteristics of speech and language that have been shown to discriminate between healthy and impaired subjects, including syntactic complexity ( and mean pause duration ().
named entity recognition (NER)	accuracy	Therefore we consider that the use of advanced natural language processing (NLP) techniques like named entity recognition (NER) and anaphora resolution are needed in order to achieve high classification accuracy.
text classification	accuracy	In this paper, we focus on the task of text classification, proceeding under the simplifying assumption that given enough annotated training data for NEs and their Roles both can be automatically tagged with high accuracy.
NEs	accuracy	In this paper, we focus on the task of text classification, proceeding under the simplifying assumption that given enough annotated training data for NEs and their Roles both can be automatically tagged with high accuracy.
conversion	precision	In the evaluation of the conversion rules against the gold standard SF BioInfer annotation, we find a precision of 98.0% and a recall of 96.2%.
conversion	recall	In the evaluation of the conversion rules against the gold standard SF BioInfer annotation, we find a precision of 98.0% and a recall of 96.2%.
classification	accuracy	It is seen that based on all three features the system achieves a classification accuracy of 87.5%, which is comparable to the results of the original ABTA system.
classification	accuracy	However, the size of the feature set of the system is significantly reduced, and the classification accuracy of 87.5% is achieved based on only 18 parameters, which is 1/2 of the size of the original feature set.
classification	accuracy	It is comparable with the classification accuracy achieved by using only POS information in the system.
classification	accuracy	It is expected that larger training set size leads to better classification accuracy of experiments.
Temporality	recall	We then calculated recall and precision using the following formulas: For the Temporality feature, we calculated recall and precision separately for the values historical and hypothetical.
Temporality	precision	We then calculated recall and precision using the following formulas: For the Temporality feature, we calculated recall and precision separately for the values historical and hypothetical.
pattern extraction	BNE	The abstracts used for pattern extraction were POS tagged using the Schmid tagger and BNE tagging was done using ABNER.
SVM classification	recall	 Table 6: (1) Classification using CRFs into 4 ma- jor semantic classes with combined Train Set A and  B as training data. (2) Binary SVM classification  of a subset of test set sentences. (3) Classification  into 5 classes as described in Section 3.3. All results  (recall, precision and F -score) are reported on the  unseen test set.
SVM classification	precision	 Table 6: (1) Classification using CRFs into 4 ma- jor semantic classes with combined Train Set A and  B as training data. (2) Binary SVM classification  of a subset of test set sentences. (3) Classification  into 5 classes as described in Section 3.3. All results  (recall, precision and F -score) are reported on the  unseen test set.
SVM classification	F -score)	 Table 6: (1) Classification using CRFs into 4 ma- jor semantic classes with combined Train Set A and  B as training data. (2) Binary SVM classification  of a subset of test set sentences. (3) Classification  into 5 classes as described in Section 3.3. All results  (recall, precision and F -score) are reported on the  unseen test set.
identification of comparatives	recall	To understand how the overall identification of comparatives is influenced by the components of the construction, we also computed recall and precision separately for drug names, scale, and position on scale (SAME_AS, HIGHER_THAN and LOWER_THAN taken together).
identification of comparatives	precision	To understand how the overall identification of comparatives is influenced by the components of the construction, we also computed recall and precision separately for drug names, scale, and position on scale (SAME_AS, HIGHER_THAN and LOWER_THAN taken together).
identification of comparatives	SAME_AS	To understand how the overall identification of comparatives is influenced by the components of the construction, we also computed recall and precision separately for drug names, scale, and position on scale (SAME_AS, HIGHER_THAN and LOWER_THAN taken together).
identification of comparatives	HIGHER_THAN	To understand how the overall identification of comparatives is influenced by the components of the construction, we also computed recall and precision separately for drug names, scale, and position on scale (SAME_AS, HIGHER_THAN and LOWER_THAN taken together).
identification of comparatives	LOWER	To understand how the overall identification of comparatives is influenced by the components of the construction, we also computed recall and precision separately for drug names, scale, and position on scale (SAME_AS, HIGHER_THAN and LOWER_THAN taken together).
identification	ID	For identification, it was sufficient in BC2 to report the ID, regardless of number of occurrences or name variations.
classification-in	accuracy	presents the results of classification-in terms of average accuracy (%Acc) and relative error reduction (%RER)-for the individual feature groups, as well as for all groups combined.
classification-in	Acc)	presents the results of classification-in terms of average accuracy (%Acc) and relative error reduction (%RER)-for the individual feature groups, as well as for all groups combined.
classification-in	relative error reduction (%RER)-	presents the results of classification-in terms of average accuracy (%Acc) and relative error reduction (%RER)-for the individual feature groups, as well as for all groups combined.
INST	accuracy	As can be seen, INST features yield the lowest overall accuracy, around 36%, with a relative error reduction of only 14% over the baseline.
INST	error	As can be seen, INST features yield the lowest overall accuracy, around 36%, with a relative error reduction of only 14% over the baseline.
Maximising	likelihood	Maximising the likelihood involves calculating feature expectations, which is computationally expensive.
parsing	accuracy	In this paper we reduce the memory requirements from 20 GB of RAM to only a few hundred MB, but without greatly increasing the training time or reducing parsing accuracy.
parse	accuracy	We evaluated against the PARC 700 Dependency Bank and show that it is possible to decrease the time taken to parse by ∼18% while maintaining accuracy.
parsing	Chart	A typical breakdown of parsing time of XLE components is Morphology (1.6%), Chart (5.8%) and Unifier (92.6%).
parsing	coverage	 Table 8. The hybrid system achieves an  18% decrease in parsing time, a slight improvement  in coverage of 0.9%, and a 1.12% improvement in  overall f-structure quality.
bracketing	accuracy	Therefore, although the SP receives reasonable bracketing accuracy, it has less idea of the goodness of different edges with the same span.
IR	Run #1	For the IR and QA tasks, Run #1 results are better when compared to Run #2's.
IE pairs	coverage	Performances of our method For IE pairs, we find good coverage, whereas for IR and QA pairs the coverage is low, though it achieves good accuracy.
IE pairs	coverage	Performances of our method For IE pairs, we find good coverage, whereas for IR and QA pairs the coverage is low, though it achieves good accuracy.
IE pairs	accuracy	Performances of our method For IE pairs, we find good coverage, whereas for IR and QA pairs the coverage is low, though it achieves good accuracy.
IE task	BS	 Table 2 Results on RTE-3 Data  For the IE task, Mi+SK+BS obtained the highest  improvement over the baseline systems, suggesting  that the kernel method seems to be more appropri- ate if the underlying task conveys a more "rela- tional nature." Improvements in the other tasks are  less convincing as compared to the baselines. Nev- ertheless, the overall result obtained in experiment  B would have been among the top 3 of the RTE-2  challenge. We utilize the system description table  of (
Named Entity Recognition (NER	accuracy	While Named Entity Recognition (NER) provides remarkable results (accuracy over 70%) for RTE on QA task, IE task requires more sophisticated treatment of named entities such as the identification of relations between them.
Textual Entailment	BoLI	We compare two approaches to the problem of Textual Entailment: SLIM, a composi-tional approach modeling the task based on identifying relations in the entailment pair, and BoLI, a lexical matching algorithm.
PAS-CAL RTE textual inference task	precision	Most current approaches to the PAS-CAL RTE textual inference task achieve ro-bustness by sacrificing semantic precision; while broadly effective, they are easily confounded by ubiquitous inferences involving monotonicity.
SRL	XARA  Label  Precision  Recall F β	 Table 1: Results of SRL with XARA  Label  Precision  Recall F β=1  Overall  65,11% 45,83% 53,80  Arg0  98.97% 94.95% 96.92  Arg1  70.08% 64.83% 67.35  Arg2  47.41% 36.07% 40.97  Arg3  13.89%  6.85%  9.17  Arg4  1.56%  1.35%  1.45  ArgM-LOC  83.49% 13.75% 23.61  ArgM-NEG  72.79% 58.79% 65.05  ArgM-PNC  91.94% 39.31% 55.07  ArgM-PRD  63.64% 26.25% 37.17  ArgM-REC  85.19% 69.70% 76.67
SRL	Arg0	 Table 1: Results of SRL with XARA  Label  Precision  Recall F β=1  Overall  65,11% 45,83% 53,80  Arg0  98.97% 94.95% 96.92  Arg1  70.08% 64.83% 67.35  Arg2  47.41% 36.07% 40.97  Arg3  13.89%  6.85%  9.17  Arg4  1.56%  1.35%  1.45  ArgM-LOC  83.49% 13.75% 23.61  ArgM-NEG  72.79% 58.79% 65.05  ArgM-PNC  91.94% 39.31% 55.07  ArgM-PRD  63.64% 26.25% 37.17  ArgM-REC  85.19% 69.70% 76.67
SRL	Arg4	 Table 1: Results of SRL with XARA  Label  Precision  Recall F β=1  Overall  65,11% 45,83% 53,80  Arg0  98.97% 94.95% 96.92  Arg1  70.08% 64.83% 67.35  Arg2  47.41% 36.07% 40.97  Arg3  13.89%  6.85%  9.17  Arg4  1.56%  1.35%  1.45  ArgM-LOC  83.49% 13.75% 23.61  ArgM-NEG  72.79% 58.79% 65.05  ArgM-PNC  91.94% 39.31% 55.07  ArgM-PRD  63.64% 26.25% 37.17  ArgM-REC  85.19% 69.70% 76.67
SRL	ArgM-LOC	 Table 1: Results of SRL with XARA  Label  Precision  Recall F β=1  Overall  65,11% 45,83% 53,80  Arg0  98.97% 94.95% 96.92  Arg1  70.08% 64.83% 67.35  Arg2  47.41% 36.07% 40.97  Arg3  13.89%  6.85%  9.17  Arg4  1.56%  1.35%  1.45  ArgM-LOC  83.49% 13.75% 23.61  ArgM-NEG  72.79% 58.79% 65.05  ArgM-PNC  91.94% 39.31% 55.07  ArgM-PRD  63.64% 26.25% 37.17  ArgM-REC  85.19% 69.70% 76.67
SRL	ArgM-NEG	 Table 1: Results of SRL with XARA  Label  Precision  Recall F β=1  Overall  65,11% 45,83% 53,80  Arg0  98.97% 94.95% 96.92  Arg1  70.08% 64.83% 67.35  Arg2  47.41% 36.07% 40.97  Arg3  13.89%  6.85%  9.17  Arg4  1.56%  1.35%  1.45  ArgM-LOC  83.49% 13.75% 23.61  ArgM-NEG  72.79% 58.79% 65.05  ArgM-PNC  91.94% 39.31% 55.07  ArgM-PRD  63.64% 26.25% 37.17  ArgM-REC  85.19% 69.70% 76.67
SRL	ArgM-PNC	 Table 1: Results of SRL with XARA  Label  Precision  Recall F β=1  Overall  65,11% 45,83% 53,80  Arg0  98.97% 94.95% 96.92  Arg1  70.08% 64.83% 67.35  Arg2  47.41% 36.07% 40.97  Arg3  13.89%  6.85%  9.17  Arg4  1.56%  1.35%  1.45  ArgM-LOC  83.49% 13.75% 23.61  ArgM-NEG  72.79% 58.79% 65.05  ArgM-PNC  91.94% 39.31% 55.07  ArgM-PRD  63.64% 26.25% 37.17  ArgM-REC  85.19% 69.70% 76.67
SRL	ArgM-REC	 Table 1: Results of SRL with XARA  Label  Precision  Recall F β=1  Overall  65,11% 45,83% 53,80  Arg0  98.97% 94.95% 96.92  Arg1  70.08% 64.83% 67.35  Arg2  47.41% 36.07% 40.97  Arg3  13.89%  6.85%  9.17  Arg4  1.56%  1.35%  1.45  ArgM-LOC  83.49% 13.75% 23.61  ArgM-NEG  72.79% 58.79% 65.05  ArgM-PNC  91.94% 39.31% 55.07  ArgM-PRD  63.64% 26.25% 37.17  ArgM-REC  85.19% 69.70% 76.67
Part-of-Speech Tagging	Accelerating	Active Learning for Part-of-Speech Tagging: Accelerating Corpus Annotation
Semantic Role Labeling	F-measure	For example, we previously showed () that, forest from the computational linguistics community: Workshop on Multiword Expressions at COLING; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Knowledge Discovery from Text at ACL 2003; Shared task on Semantic Role Labeling at the task of automatic detection of part-whole relations, our system's learning curve reached a plateau at 74% F-measure when trained on approximatively 10,000 positive and negative examples.
recognition	precision	Evaluation of recognition's precision and recall yielded 88.6% and 82.6%, respectively.
recognition	recall	Evaluation of recognition's precision and recall yielded 88.6% and 82.6%, respectively.
morphological annotation	precision	Statistical methods have enabled to develop the automatic tool of morphological annotation for Lithuanian, with the disambiguation precision of 94%.
parsing	accuracy	We show that the incorporation of such self-trained preferences improves parsing accuracy significantly.
parsing	accuracy	Experimental results show that this method can obtain higher parsing accuracy than previous work on domain adaptation for parsing the same data.
parsing	accuracy	Moreover, the results show that the combination of the proposed method and the existing method achieves parsing accuracy that is as high as that of an HPSG parser retrained from scratch, but with much lower training cost.
parsing	accuracy	When we focus on the "ALL" domain, the approaches other than the baseline succeeded to give higher parsing accuracy than the baseline.
Mixture	accuracy	The "Mixture" method gave the highest accuracy which was 3.41 point higher than the baseline.
parsing	accuracy	When we focus on the individual domains, our method could successfully obtain higher parsing accuracy than the baseline for all the domains.
parsing	accuracy	Moreover, for the "CP" domain, our method could give the highest parsing accuracy among the methods.
Parsing	accuracy	 Table 2: Parsing accuracy and time for various methods
Parsing	time	 Table 2: Parsing accuracy and time for various methods
parse selection	accuracy	For instance, reports that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus.
parser	accuracy	shows that parser accuracy increases with the size of the in-domain selftraining material.
parsing	accuracy	Overall, the parsing speed is improved by over 35% with negligible loss of accuracy or coverage.
parsing	coverage	Overall, the parsing speed is improved by over 35% with negligible loss of accuracy or coverage.
parsing	accuracy	This paper describes several modifications to the C&C parser which improve parsing efficiency without reducing accuracy or coverage by reducing the impact of the longer sentences.
parsing	coverage	This paper describes several modifications to the C&C parser which improve parsing efficiency without reducing accuracy or coverage by reducing the impact of the longer sentences.
parsing	coverage	The new algorithm empirically exhibits a linear relationship between processing time and the number of analyses unpacked at all degrees of ME feature non-locality; in addition, compared with agenda-driven best-first parsing and exhaustive parsing with post-hoc parse selection it leads to improved parsing speed, coverage, and accuracy.
parsing	accuracy	The new algorithm empirically exhibits a linear relationship between processing time and the number of analyses unpacked at all degrees of ME feature non-locality; in addition, compared with agenda-driven best-first parsing and exhaustive parsing with post-hoc parse selection it leads to improved parsing speed, coverage, and accuracy.
parse selection	accuracy	Extending the context size of ME features, within the bounds of available training data, enables increased parse selection accuracy.
parsing	accuracy	Recently, supertagging becomes well known to drastically improve the parsing accuracy and speed, but su-pertagging techniques were heuristically introduced , and hence the probabilistic models for parse trees were not well defined.
parsing	accuracy	showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures.
parsing	accuracy	Our models increased the parsing accuracy.
parsing	accuracy	Although this approach is very simple, it achieved the state-of-art parsing accuracy.
RH parser	speed	While the most important factor in RH parser speed is the enormous speed of the shallow parser, the preference and pruning approach of the overlay parser make contributions to both speed and coverage.
RH parser	coverage	While the most important factor in RH parser speed is the enormous speed of the shallow parser, the preference and pruning approach of the overlay parser make contributions to both speed and coverage.
CF parsers	O	A well-known result is that CF parsers may reach a worst-case running time of O(|G|× n 3 ) where |G| is the size of the CFG and n is the length of the source text.
generative parsing	ISBN	Then we will define the generative parsing model, based on the algorithm of (), and propose an ISBN for this model.
Inform speech act	B&L	For the Inform speech act, subjects judged 10 example utterances for each situation, friend and stranger, with 5 B&L strategies, used to inform the hearer of some potentially face­ threatening event.
summarization	ROUGE	Different methods have been used in the above summarization research to compare system generated summaries with human annotation, such as Fmeasure, ROUGE, Pyramid, sumACCY (.
machine translation	ROUGE	e.g. BLEU () for machine translation, ROUGE) for summarization.
translation	BLEU score	This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score ().
SMT	accuracy	We describe two methods to improve SMT accuracy using shallow syntax information.
POS tagging	PATB	For POS tagging, we use the collapsed tagset for PATB (24 tags).
TTS transducer	BLEU-4	Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU-4 metric.
TTS templates from parallel corpora	accuracy	A systematic method for extracting TTS templates from parallel corpora was proposed by, and later binarized by for high efficiency and accuracy.
MT	BLEU-4	The Chinese sentence from the selected pair is used as the single reference to tune and evaluate the MT system with word-based BLEU-4 ().
MT evaluation	Bleu metric	The most commonly used MT evaluation metric in recent years has been IBM's Bleu metric ().
statistical machine translation (SMT)	BLEU score	Based on an architecture that allows to combine statistical machine translation (SMT) with rule-based machine translation (RBMT) in a multi-engine setup, we present new results that show that this type of system combination can actually increase the lexical coverage of the resulting hybrid system, at least as far as this can be measured via BLEU score.
rule-based machine translation (RBMT)	BLEU score	Based on an architecture that allows to combine statistical machine translation (SMT) with rule-based machine translation (RBMT) in a multi-engine setup, we present new results that show that this type of system combination can actually increase the lexical coverage of the resulting hybrid system, at least as far as this can be measured via BLEU score.
MT	BLEU score	This setup provides an elegant solution to the fairly complex task of integrating multiple MT results that may differ in word order using only standard software modules, in particular GIZA++ ( for the identification of building blocks and Moses for the recombination, but the authors were notable to observe improvements in 1 see http://www.statmt.org/moses/ terms of BLEU score.
SMT	BLEU	 Table 1: Performance of baseline SMT system, our system and RBMT systems (BLEU scores)
OOV recognition	F-score	The CRFbased CWS that supports OOV recognition produces word segmentations with a higher F-score, but a huge number of new words recognized correctly and incorrectly that can incur data sparseness in training the SMT models.
OOV recognition	F-score	On the other hand, the dictionarybased approach that does not support OOV recognition produced a lower F-score, but with a relatively weak data spareness problem.
SMT	BLEU	Which approach pro-  The performance of CWS is usually measured by the F-score, while that of SMT is measured using the BLEU score.
MT task	BLEU	Based on these findings , we implement methods inside a conditional random field segmenter that directly optimize seg-mentation granularity with respect to the MT task, providing an improvement of 0.73 BLEU.
MT training	BLEU point gain	Having lexicon-based features reduced the MT training lexicon by 29.5%, reduced the MT test data OOV rate by 34.1%, and led to a 0.38 BLEU point gain on the test data (MT05).
Translation	BLEU evaluation metric	Translation results are given in terms of the automatic BLEU evaluation metric () as well as the TER metric ().
Translation	TER metric	Translation results are given in terms of the automatic BLEU evaluation metric () as well as the TER metric ().
translation	BLEU	The final three columns report translation results in terms of BLEU , BLEU precision score (PREC), and TER.
translation	BLEU precision score (PREC)	The final three columns report translation results in terms of BLEU , BLEU precision score (PREC), and TER.
translation	TER	The final three columns report translation results in terms of BLEU , BLEU precision score (PREC), and TER.
parsing	accuracy	Recent research shows that parsing accuracy of biomedical corpora is now between 80% and 90% (.
PPI extraction	accuracy	Although it may appear that further increasing the training data for the parser may not improve the PPI extraction accuracy (since only small and inconsistent variations in F-score are observed in), when we plot the curves shown in in a single graph, we see that the two curves match each other to a large extent.
PPI extraction	F-score	Although it may appear that further increasing the training data for the parser may not improve the PPI extraction accuracy (since only small and inconsistent variations in F-score are observed in), when we plot the curves shown in in a single graph, we see that the two curves match each other to a large extent.
PPI extraction	accuracy	While this suggests that training the parser with a larger treebank may result in improved accuracy in PPI extraction, we observe that a 1% absolute improvement in parser accuracy corresponds roughly to a 0.25 improvement in PPI extraction F-score.
parser	accuracy	indicates that to obtain even a 1% improvement in parser accuracy by using more training data, the size of the treebank would have to increase significantly.
PPI extraction	accuracy	Although the results presented so far seem to suggest the need fora large data annotation effort to achieve a meaningful improvement in PPI extraction accuracy, there are other ways to improve the overall accuracy of the system without an improvement in parser accuracy.
PPI extraction	accuracy	Although the results presented so far seem to suggest the need fora large data annotation effort to achieve a meaningful improvement in PPI extraction accuracy, there are other ways to improve the overall accuracy of the system without an improvement in parser accuracy.
PPI extraction	accuracy	When the parser trained on WSJ sentences is used, PPI extraction accuracy is about 55, compared to over 57 when sentences from biomedical papers are used.
PPI extraction	accuracy	This corresponds fairly closely to the differences in parser accuracy: the accuracy of the parser trained on 500 sentences from GENIA is about the same as the accuracy of the parser trained on the entire WSJ Penn Treebank, and when these parsers are used in the PPI extraction system, they result in similar overall task accuracy.
parser	accuracy	Figures 5, 6 and 7 also suggest that additional efforts in improving parser accuracy (through the use of feature engineering, other machine learning techniques, or an increase in the size of its training set) could improve PPI extraction accuracy, but a large improvement in parser accuracy maybe required.
parser	accuracy	Figures 5, 6 and 7 also suggest that additional efforts in improving parser accuracy (through the use of feature engineering, other machine learning techniques, or an increase in the size of its training set) could improve PPI extraction accuracy, but a large improvement in parser accuracy maybe required.
parser	accuracy	Figures 5, 6 and 7 also suggest that additional efforts in improving parser accuracy (through the use of feature engineering, other machine learning techniques, or an increase in the size of its training set) could improve PPI extraction accuracy, but a large improvement in parser accuracy maybe required.
PPI extraction	accuracy	Figures 5, 6 and 7 also suggest that additional efforts in improving parser accuracy (through the use of feature engineering, other machine learning techniques, or an increase in the size of its training set) could improve PPI extraction accuracy, but a large improvement in parser accuracy maybe required.
PPI extraction	accuracy	Figures 5, 6 and 7 also suggest that additional efforts in improving parser accuracy (through the use of feature engineering, other machine learning techniques, or an increase in the size of its training set) could improve PPI extraction accuracy, but a large improvement in parser accuracy maybe required.
PPI extraction system evaluations	F-score measure	The majority of PPI extraction system evaluations use the balanced F-score measure for quantifying the performance of the systems.
recognition of biomedical named entities	precision	Systems for the recognition of biomedical named entities have traditionally worked on a 'first-best' approach, where all of the entities recognised have equal status, and precision and recall are given roughly equal importance.
recognition of biomedical named entities	recall	Systems for the recognition of biomedical named entities have traditionally worked on a 'first-best' approach, where all of the entities recognised have equal status, and precision and recall are given roughly equal importance.
TI	Avg. Rank'	 Table 4: Results of TI on the EPPI dataset. All figures, except 'Avg. Rank', are percentages. This evaluation was  carried out on protein entities only.
TI	Avg. Rank'	 Table 5: Results of TI on the TE dataset. All figures, except 'Avg. Rank', are percentages. There are four entity types  in the TE data, i.e., protein, gene, mRNAcDNA and GOMOP. The evaluation was carried out on all entity types.
segmentation	accuracy	it showed a small but significant improvement to segmentation accuracy by learning the possible syllable structures of the language together with the lexicon, and 3.
IR	precision	In the three languages of the IR evaluation, our enhanced Pa-raMor significantly outperforms, at average precision over newswire queries, a morphologically naïve baseline; scoring just behind the leading system from Morpho Challenge 2007 in English and ahead of the first place system in German.
Tag comparison	accuracy	Tag comparison operates on a word-by-word basis and provides binary measures of accuracy (tag identity or difference).
parsing	accuracy	Earlier studies by and using the Negra treebank ( reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negra's flat constituent structures.
parsing German	TIGER	The shared task on parsing German was to parse both the constituency version and the dependency version of the two German treebanks: TIGER () and TüBa-D/Z ().
summarizing graphics	PROP ALL)	Based on our experience with summarizing graphics, we first identified the set of all propositions (PROP ALL) that reflect information that we envisioned someone might ask about a simple bar chart.
coreference resolution	B-CUBED	There does not appear to be a single standard eval-uation metric in the coreference resolution community, so we opted to use three: MUC-6 (, CEAF, and B-CUBED (, which seem to be the most widely accepted metrics.
sense tagging	accibility	WordNet, which has been the default choice of NLP researchers for sense tagging because of its broad coverage and easy accibility, does not have hierarchical entries.
preposition selection	accuracy	For comparison purposes, using a majority baseline (always selecting the preposition of) in this domain results in an accuracy of) used perceptron classifiers for preposition selection in BNC News Text at 85% accuracy.
error detection tasks	precision	In addition, in error detection tasks, high precision (and thus low recall) is favored since one wants to minimize the number of false positives a student may see.
error detection tasks	recall	In addition, in error detection tasks, high precision (and thus low recall) is favored since one wants to minimize the number of false positives a student may see.
parsing	accuracy	However, our empirical results exhibit that the auxiliary distribution does not help: even when very little target training data is available the incorporation of the out-of-domain model does not contribute to parsing accuracy on the target domain; instead, better results are achieved either without adaptation or by simple model combination.
conversion	accuracy	Not to mention that the 80% success rate of conversion is not meaningful for parsers that boast 90% accuracy.
classification	accuracy	Therefore, the available training data are usually sparse and cannot produce a classification accuracy to the degree possible.
classification	accuracy	Furthermore, the effect of the background model on classification accuracy is investigated.
Classification	accuracy	 Table 1: Classification accuracy for the conventional method  and the proposed method with different lengths of n-best list
ASR channel	SLU	To assess the ASR channel simulation quality, we compared how SLU of utterances was affected by WER.
speech translation loop	accuracy	With users thus integrated into the speech translation loop, automatically translated spoken conversations can range widely with acceptable accuracy.
MT	accuracy	While our system's facilities for monitoring and correction of ASR and MT are vital for accuracy and confidence in wide-ranging conversations, they can be time consuming.
Recognizing Textual Entailment (RTE) task 1	error	A recent success story in language processing is the Recognizing Textual Entailment (RTE) task 1 . Since its inception in 2004, this has become extremely popular; the yearly RTE workshop now attracts around 40 submissions, and error rates on the task have more than halved.
parsing	accuracy	carried out a preliminary experiment to test the theory that if fewer c-structures were passed to the unifier, overall parsing times would improve, while the accuracy of parsing would remain stable.
parsing	accuracy	We carried out a number of parsing experiments to test the effect of c-structure pruning, both in terms of time and accuracy.
parsing	f-score	There is a 67% speedup in parsing the 560 sentences, and the most probable f-score increases significantly from 79.93 to 82.83.
answer extraction module	accuracy	Conventional wisdom suggests that, the higher the quality of the retrieval results used as input to the answer extraction module, the better the extracted answers, and hence system accuracy , will be.
IR system	recall	IR system recall is very important for question answering.
IR system	precision	IR system precision and ranking of candidate passages can also affect question answering performance.
sentence retrieval	MRR	For sentence retrieval we report 1) average sentence MRR, 2) overall sentence recall, 3) average precision of the first sentence, 4) number of correct candidate sentences in the top 10 results, and 5) number of correct candidate sentences in the top 50 results . shows our experimental results.
sentence retrieval	recall	For sentence retrieval we report 1) average sentence MRR, 2) overall sentence recall, 3) average precision of the first sentence, 4) number of correct candidate sentences in the top 10 results, and 5) number of correct candidate sentences in the top 50 results . shows our experimental results.
sentence retrieval	precision	For sentence retrieval we report 1) average sentence MRR, 2) overall sentence recall, 3) average precision of the first sentence, 4) number of correct candidate sentences in the top 10 results, and 5) number of correct candidate sentences in the top 50 results . shows our experimental results.
passage retrieval	precision	Commonly it is argued that passage retrieval for QA is merely a filtering task and ranking (precision) is less important than recall.
passage retrieval	recall	Commonly it is argued that passage retrieval for QA is merely a filtering task and ranking (precision) is less important than recall.
MRR	recall	This way, TDRR extends MRR with a notion of recall.
passage retrieval	MRR	 Table 3: Results for passage retrieval on why-questions against Wikipedia using disjoint windows (DW)  and sliding windows (SW)  Success@10  Success@150  MRR@150  Retrieval model  DW  SW  DW  SW  DW  SW  Baseline: Wumpus/QAP  40.9%  72.0%  0.229  Lemur/TFIDF  43.0% 45.2% 71.1% 81.7% 0.247 0.338  Lemur/Okapi  41.9% 44.1% 67.7% 79.6% 0.243 0.320  Lemur/KL  48.9% 50.0% 72.8% 77.2% 0.263 0.324
Answer Validation	accurate confidence score	Answer Validation is to decide whether the candidate answers are corrector not, or even to determine the accurate confidence score to them.
IR	recall	Hence, high performance of IR especially in terms of recall is essential.
SRL	precision	The SRL tool SwiRL (Surdeanu and) has a good precision and coverage, however it is slow and quite unstable when parsing large amounts of data.
SRL	coverage	The SRL tool SwiRL (Surdeanu and) has a good precision and coverage, however it is slow and quite unstable when parsing large amounts of data.
extraction of semantic role features	Fmeasure	This level of performance is still comparable to other syntactic parsers often used for extraction of semantic role features (88.2% Fmeasure)).
parsing	accuracy	There area number of studies that investigate the influence of different features or representational choices on overall parsing accuracy,.
parsing	accuracy	Overall parsing accuracy will be reported using the standard metrics of labeled attachment score (LAS) and unlabeled attachment score (UAS).
parsing	labeled attachment score (LAS)	Overall parsing accuracy will be reported using the standard metrics of labeled attachment score (LAS) and unlabeled attachment score (UAS).
parsing	unlabeled attachment score (UAS)	Overall parsing accuracy will be reported using the standard metrics of labeled attachment score (LAS) and unlabeled attachment score (UAS).
semantic parsing learning	MRG	A semantic parsing learning system typically exploits the given MRG of the MRL to learn a semantic parser (;).
automatic extraction of VPCs	Entropy	In this work we investigate the automatic extraction of VPCs, looking into a variety of methods, combining linguistic with statistical information, ranging from frequencies to association measures: Mutual Information (MI), χ 2 and Entropy.
segmentation	accuracy	We use this insight, also suggested by and recently utilized by in a different manner, to enhance Brent's (1999) model MBDP-1, and significantly increase segmentation accuracy.
SRL	accuracy	Learning ability is measured by the level of SRL accuracy and, more importantly, the types of errors made by the system on sentences containing novel verbs.
classification	accuracy	We used two measures of success, one to assess classification accuracy, and the other to assess the predicted error.
classification	accuracy	We used a per argument F1 for classification accuracy, with F1 based on correct identification of individual nouns rather than full phrases.
classification	F1	We used a per argument F1 for classification accuracy, with F1 based on correct identification of individual nouns rather than full phrases.
SRL	F1	 Table 2: Testing NPattern features on full SRL task  of heldout section 8 of Eve when trained on sec- tions 9 through 20. Each result column reflects a  per argument F1.
SMT phrase-based translation	BLEU	Our experiments showed that an SMT phrase-based translation using 4 words distance reordering could gain four BLEU points over monotone decoding.
MT evaluation	BLEU measure	For MT evaluation, we used BLEU measure () calculated by the NIST script version 11b.
syntactic parsing	accuracy	This result suggests that, though the latent variables associated with syntactic states in the joint model were trained to be useful in semantic role labelling, this did not have a negative effect on syntactic parsing accuracy, and may even have helped.
SRL	F-score	We use Markov Logic to define a joint SRL model and achieve a semantic F-score of 74.59%, the second best in the Open Track.
syntactic dependency parsing	F1 score	 Table 2: Labeled and unlabeled attachment scores  in syntactic dependency parsing and F1 score for  semantic role labeling.
parsing	UAL	Using both models during parsing interleaved, we obtained UAL=65.17% and LAL=28.47% on the development set.
parsing	LAL	Using both models during parsing interleaved, we obtained UAL=65.17% and LAL=28.47% on the development set.
WSD	agreement rate	An important concern for the evaluation of WSD systems is the agreement rate between human annotators on word sense assignment.
Boxer	coverage	Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts.
recognition	recall	So far, only recognition has been carried out by who computed recall and precision on three kinds of corpora.
recognition	precision	So far, only recognition has been carried out by who computed recall and precision on three kinds of corpora.
Automatic Context Extraction (ACE)	GALE	Recent efforts to create resources to support large evaluation initiatives in the USA such as Automatic Context Extraction (ACE), Translingual Information Detection, Extraction and Summarization (TIDES), and GALE are beginning to change this, but just at a point when the community is beginning to realize that even the 1M word annotated corpora created in substantial efforts such as Prop-Bank () and the OntoNotes initiative) are likely to be too small.
Translingual Information Detection, Extraction and Summarization (TIDES)	GALE	Recent efforts to create resources to support large evaluation initiatives in the USA such as Automatic Context Extraction (ACE), Translingual Information Detection, Extraction and Summarization (TIDES), and GALE are beginning to change this, but just at a point when the community is beginning to realize that even the 1M word annotated corpora created in substantial efforts such as Prop-Bank () and the OntoNotes initiative) are likely to be too small.
CTT	Score	In our experiments for CTT method each sentence is scored as following: where nuw i is the number of words "forced" to get the first WN sense in the sentence Si . If nuw i = 0 then Score(S i ) = 2.
Question Classification(QC) task.	accuracy	In this set of experiment, we evaluate different types of kernels for Question Classification(QC) task.: Classification accuracy of different kernels on different data sets this paper we use the same dataset as introduced in().
machine translation output	BLEU score	We explored novel automatic evaluation measures for machine translation output oriented to the syntactic structure of the sentence: the BLEU score on the detailed Part-of-Speech (POS) tags as well as the precision, recall and F-measure obtained on POS n-grams.
machine translation output	precision	We explored novel automatic evaluation measures for machine translation output oriented to the syntactic structure of the sentence: the BLEU score on the detailed Part-of-Speech (POS) tags as well as the precision, recall and F-measure obtained on POS n-grams.
machine translation output	recall	We explored novel automatic evaluation measures for machine translation output oriented to the syntactic structure of the sentence: the BLEU score on the detailed Part-of-Speech (POS) tags as well as the precision, recall and F-measure obtained on POS n-grams.
machine translation output	F-measure	We explored novel automatic evaluation measures for machine translation output oriented to the syntactic structure of the sentence: the BLEU score on the detailed Part-of-Speech (POS) tags as well as the precision, recall and F-measure obtained on POS n-grams.
MT	BLEU	This paper describes a simple evaluation metric for MT which attempts to overcome the well-known deficits of the standard BLEU metric from a slightly different angle.
MT hypotheses	BLEU	Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf. BLEU (), NIST).
Translation	BLEU Score	 Table 1: Translation results for English-German  (BLEU Score)
Translation	BLEU Score	 Table 2: Translation results for German-English  (BLEU Score)
Translation	BLEU Score	 Table 3: Translation results for English-French  (BLEU Score)
Translation	BLEU Score	 Table 4: Translation results for French-English  (BLEU Score)
translation	Bleu	Both methods gave some improvements to translation quality as measured by Bleu and Meteor scores, though not consistently.
SMT	Minimum Error Training	Our baseline system is a popular phrase-based SMT system, Moses, with 5-gram SRILM language model, tuned with Minimum Error Training.
Translation	BLEU	 Table 3: Translation results for the German- English task using different rule types (BLEU)
Translation	BLEU	 Table 5: Translation results for the German- French translation task (BLEU)
Translation	BLEU	 Table 6: Translation results for the English- German translation task (BLEU)
Machine translation (MT) from Chinese to English	BLEU	Machine translation (MT) from Chinese to English has been a difficult problem: structural differences between Chinese and English, such as the different orderings of head nouns and relative clauses, cause BLEU scores to be consistently lower than for other difficult language pairs like Arabic-English.
MT pipeline	MERT	After this preprocessing, we restart the whole MT pipeline -align the preprocessed data, extract phrases, run MERT and evaluate.
MT	BLEU	 Table 5: MT experiments of different settings on various NIST MT evaluation datasets. We used both the BLEU and TER  metrics for evaluation. All differences between DE-Annotated and BASELINE are significant at the level of 0.05 with the  approximate randomization test in (
MT	TER	 Table 5: MT experiments of different settings on various NIST MT evaluation datasets. We used both the BLEU and TER  metrics for evaluation. All differences between DE-Annotated and BASELINE are significant at the level of 0.05 with the  approximate randomization test in (
MT	BASELINE	 Table 5: MT experiments of different settings on various NIST MT evaluation datasets. We used both the BLEU and TER  metrics for evaluation. All differences between DE-Annotated and BASELINE are significant at the level of 0.05 with the  approximate randomization test in (
translation	BLEU	Most empirical work in translation analyzes models and algorithms using BLEU () and related metrics.
MT	TER-Plus	We explore these differences through the use of anew tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tun-able parameters and the incorporation of morphology, synonymy and paraphrases.
MT	Fluency	Numerous methods of judging MT output by humans have been used, including Fluency, Adequacy, and, more recently, Human-mediated Translation Edit Rate (HTER)).
MT	Adequacy	Numerous methods of judging MT output by humans have been used, including Fluency, Adequacy, and, more recently, Human-mediated Translation Edit Rate (HTER)).
MT	Translation Edit Rate (TER))	This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER)) or when used with other automatic metrics such as BLEU or ME-TEOR ().
MT	BLEU	This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER)) or when used with other automatic metrics such as BLEU or ME-TEOR ().
Classifier stacking	accuracy	Classifier stacking is then used fora posteriori error correction , yielding a further improvement in classification accuracy.
classification	accuracy	Classifier stacking is then used fora posteriori error correction , yielding a further improvement in classification accuracy.
classification tasks	precision	In the context of classification tasks, precision and recall measurements provide useful information of system accuracy.
classification tasks	recall	In the context of classification tasks, precision and recall measurements provide useful information of system accuracy.
classification tasks	accuracy	In the context of classification tasks, precision and recall measurements provide useful information of system accuracy.
realisation task	REALPRO	Over the past several years, a significant consensus has emerged over the definition of the realisation task, through the development of realisers such as REALPRO), ALETH-GEN,, FUF/SURGE (, HALO-GEN), YAG (), and OPENCCG).
realisation task	ALETH-GEN	Over the past several years, a significant consensus has emerged over the definition of the realisation task, through the development of realisers such as REALPRO), ALETH-GEN,, FUF/SURGE (, HALO-GEN), YAG (), and OPENCCG).
realisation task	FUF	Over the past several years, a significant consensus has emerged over the definition of the realisation task, through the development of realisers such as REALPRO), ALETH-GEN,, FUF/SURGE (, HALO-GEN), YAG (), and OPENCCG).
realisation task	YAG	Over the past several years, a significant consensus has emerged over the definition of the realisation task, through the development of realisers such as REALPRO), ALETH-GEN,, FUF/SURGE (, HALO-GEN), YAG (), and OPENCCG).
POS tagging (POS2)	accuracy	We made another POS tagging (POS2) task by adding the last two characters as an extra feature for enhancing the accuracy of some tags such as plural or dual noun (NNS) and singular noun (NN).
segmentation	accuracy	From comparison of Arabic and Hebrew performance we can also see that segmentation decisions for the task in Arabic are likely to be easier, since the accuracy for T=1 is very high.
segmentation	T	From comparison of Arabic and Hebrew performance we can also see that segmentation decisions for the task in Arabic are likely to be easier, since the accuracy for T=1 is very high.
ASR	accuracy	Being able to identify dialect vs. MSA as well as to identify which dialect is spoken during the recognition process will enable ASR engines to adapt their acoustic, pronunciation, morphological, and language models appropriately and thus improve recognition accuracy.
Mapping	precision	 Table 3: Mapping precision (freq. ≥ 2)
Mapping	precision	 Table 4: Mapping precision (freq. ≥ 200)
negation finding	inter-annotator agreement (IAA)	 Table 2: Baseline results of the negation finding system  and inter-annotator agreement (IAA) in %.
RM	RM	As for the efficiency, total training time for RM is 37% of RM, i.e. 1,190 vs. 2,596 minutes, while test time is reduced to 60%, i.e. 16 vs 27 minutes.
concept discovery	AS	 Table 1: Results for concept discovery with (P+) and  without (P) utilization of parsing data. V is the total num- ber (millions) of different words in the corpus. W is the  number (thousands) of words belonging to at least one of  the terms for one of the concepts. C is the number (thou- sands) of concepts (after merging and windowing). AS  is the average(words) category size.
token classification	Random baseline	 Table 1: Results: token classification for monosemous  verbs. Random baseline: Prec 0.8, Rec 49.8, F 1.6.
token classification	Prec	 Table 1: Results: token classification for monosemous  verbs. Random baseline: Prec 0.8, Rec 49.8, F 1.6.
token classification	F 1.6	 Table 1: Results: token classification for monosemous  verbs. Random baseline: Prec 0.8, Rec 49.8, F 1.6.
Token classification	F	 Table 3: Results: Token classification for polysemous  verbs, avg token computation. Random baseline: Prec  8.2, Rec 50.4, F 14.0.
named entity recognition (NER)	ORG	Consider the following named entity recognition (NER) example: His father was rushed to ORG , an arm of ORG , in west suburban LOC . For such tasks, it is helpful to know that west is a member of the SRWL and other such designations.
pattern discovery	accuracy	The pattern discovery procedure yields an accuracy of 97%, while the clustering procedures indicate accuracies of 91% and 82%.
pattern discovery	accuracies	The pattern discovery procedure yields an accuracy of 97%, while the clustering procedures indicate accuracies of 91% and 82%.
pattern discovery	accuracy	The pattern discovery procedure yields an accuracy of 97%, while the clustering procedures indicate accuracies of 91% and 82%.
pattern discovery	accuracies	The pattern discovery procedure yields an accuracy of 97%, while the clustering procedures indicate accuracies of 91% and 82%.
partof-speech clustering	accuracy	We further show that an extension based on partof-speech clustering can give similar accuracy gains for learning translations of all word-types, deepening the findings of previous literature which mainly focused on translating nouns).
labeling induction	accuracy	This makes sense, because the overall quality of the labeling induction algorithm is indeed not that high: using oneto-one mapping (the more forgiving mapping), the accuracy of the labels induced by MDL+SC is only 45-72%.
automatic extraction of hypernym information from text	precision	We evaluate automatic extraction of hypernym information from text and conclude that the application of dependency patterns does not lead to substantially higher precision and recall scores than using lexical patterns.
automatic extraction of hypernym information from text	recall scores	We evaluate automatic extraction of hypernym information from text and conclude that the application of dependency patterns does not lead to substantially higher precision and recall scores than using lexical patterns.
topic identification	annotation consistency	The system is found to exceed the performance of previously proposed machine learning algorithms for topic identification, with an annotation consistency comparable to human annotations .
parsing	accuracy	Our system combines and implements efficient parsing techniques to get a high accuracy as well as very good parsing and training time.
argument identification	recall	The system seems too conservative for argument identification, which makes the recall very lower.
hedge finding	prepro	 Table 4: Results of the hedge finding system with prepro- cessing.
Mutation extraction	recall	We evaluated our systems in two ways: • Header classification: performance of different systems on predicting the classes of each column/row of the tables; • Mutation extraction: recall of our system over the subset of the hand-curated MMR database.
header classification step	precision	Evaluation for the header classification step was performed using precision, recall and f-score, microaveraged amongst the classes.
header classification step	recall	Evaluation for the header classification step was performed using precision, recall and f-score, microaveraged amongst the classes.
header classification step	f-score	Evaluation for the header classification step was performed using precision, recall and f-score, microaveraged amongst the classes.
mutation extraction	recall	For mutation extraction we focus on a single class, and produce recall and a lower-bound on precision.
mutation extraction	precision	For mutation extraction we focus on a single class, and produce recall and a lower-bound on precision.
Mutation detection	TP	 Table 6: Results for Mutation detection. TP indicates the  number of true positives, "% in MMR" shows the per- centage of positives found in the database.
IE	recall	In previous applications of IE techniques for biomedical literature  we found that simple techniques for the generation of variants of the known names significantly benefited the recall of the application.
summaries	ROUGE	For the intrinsic evaluation of a large number of summaries, we made use of the ROUGE metrics that has been widely used in automatic evaluation of summarization systems.
CRA	detail	Expanding on our preliminary experiments ( ), we present a taxonomy which specifies the scientific evidence needed for CRA at the level of detail required for TM.
IE	accuracy	Since the ultimate goal of IE is to extract knowledge and accuracy is the most important aspect in evaluating the performance of such systems, it makes sense to focus the effort in seeking interaction sentences rather than passages or abstracts.
passage retrieval	ROUGE	In our system, we used a density-based passage retrieval strategy () and a sequence sensitive ranking strategy similar to ROUGE (F. Liu and Y.).
Offical Event Extraction	Approximate	 Table 3: Offical Event Extraction results on the shared task test data of the JULIELab Team. Approximate  Span Matching/Approximate Recursive Matching (columns 3-5). Event decomposition, Approximate Span Match- ing/Approximate Recursive Matching (columns 7-9).
Binding	precisions	For the non-regulation event classes excluding Binding, the precisions achieved range from 75% to 92% in both development and test data, with the exception of Transcription in the test data.
Binding events	precision	Our approach extracts Binding events with a single theme, more suitably evaluated by the Event Decomposition evaluation mode in which a similar high precision/low recall trend is observed, albeit with lower scores.
Binding events	recall	Our approach extracts Binding events with a single theme, more suitably evaluated by the Event Decomposition evaluation mode in which a similar high precision/low recall trend is observed, albeit with lower scores.
anaphora resolution	precision	The contribution of anaphora resolution to our system is limited as it relies on the argument extraction stage which, apart from introducing noise, is geared towards maintaining high precision.
entity detection classifier	precision	 Table 1: Results of the entity detection classifier.  Entities that are not in the table have a precision and  recall of 0.
entity detection classifier	recall	 Table 1: Results of the entity detection classifier.  Entities that are not in the table have a precision and  recall of 0.
BioCreative II protein-protein interaction (PPI) task	f-measure	For example, the BioCreative II protein-protein interaction (PPI) task consists of four sub-tasks, including the extraction of the protein interaction pairs in full-text documents, achieving an f-measure of up to 0.30.
event extraction shared task	recall	We submitted the output of our approach to the event extraction shared task at BioNLP 2009, where our methods suffered from low recall, although we were one of the few teams to provide answers for task 3.
negation context identification	recall	 Table 4: Performance for negation context identification  on the development set. The last row indicates the im- portance of fine tuning (F): when event class-trigger pair  exceptions and NTE exceptions are not applied, the pre- cision decreases considerably with only a small increase  in recall. See text for details in each method.
event extraction	recall	All of these previous literatures showed that one main bottleneck of event extraction lies in low recall.
WSD	Most Frequent Sense (MFS) baseline	Moreover, the evaluation in a WSD setting shows that all GCM estimate a set of parameters which are above the Most Frequent Sense (MFS) baseline by a statistically significant amount.
predicting annotation	RMS	Thus, for evaluating how accurate our model is in predicting annotation times, RMS is a more appropriate metric.
sentiment classification	accuracy	We conduct an empirical study to examine the effect of noisy annotations on the performance of sentiment classification models, and evaluate the utility of annotation selection on classification accuracy and efficiency.
classification	accuracy	 Table 4: Effect of annotation selection on classification accuracy.
metaphor classification	accuracy	Training on only 400 sentences, our models are able to achieve 61.3% accuracy on metaphor classification and 77.8% accuracy on HIGH vs. LOW metaphorical frequency estimation.
Syllable identification	accuracy	Based on the limited experiments performed on the trained CRF model, we observe that the feature set presented for Syllable identification seem to per­ form reasonably and identifies the syllables with 70% accuracy for manually tagged melodies.
VAT	reliability	We then describe the structure and development of the VAT in Section 4 and present evidence for its reliability and validity in Section 5.
VAT	validity	We then describe the structure and development of the VAT in Section 4 and present evidence for its reliability and validity in Section 5.
ASR	word accuracies	 Table 3. ASR experiment results (word accuracies in percent)
SSL	accuracy	While CV is often dismissed as unreliable for SSL due to the small amount of labeled data, we show that it is in fact effective for accuracy even when the labeled dataset size is as small as 10.
SCL	φ score	 Table 3: Results of SCL and self-training (single itera- tion, no selection). Entries marked with ⋆ are statistically  significant at p < 0.05. The φ score incorporates upper- and lower-bounds.
tagging named entities (NE)	precision	Automatically tagging named entities (NE) with high precision and recall requires a large amount of hand-annotated data, which is expensive to obtain.
tagging named entities (NE)	recall	Automatically tagging named entities (NE) with high precision and recall requires a large amount of hand-annotated data, which is expensive to obtain.
classification	accuracy	Especially, even when the domain from which the original training data is sampled is different from the domain of the testing data, our algorithm still provides significant gains in classification accuracy.
NER	accuracy	This indicates that data from different domains can adversely affect NER accuracy for supervised learning.
classification	accuracy	In addition, there is a trend that more unlabeled data results in more improvement in classification accuracy over the supervised model.
translation	AER	In the context of many-to-many alignments, this measure may tell us more about translation quality than AER.
Translation	accuracy	Translation accuracy is reported in terms of BLEU, NIST, and METEOR metrics.
Translation	BLEU	Translation accuracy is reported in terms of BLEU, NIST, and METEOR metrics.
Translation	METEOR	Translation accuracy is reported in terms of BLEU, NIST, and METEOR metrics.
phrase orientation classifier	BLEU point	We then apply the log probability of the phrase orientation classifier as an extra feature in a phrase-based MT system, and get significant BLEU point gains on three test sets: MT02 (+0.59), MT03 (+1.00) and MT05 (+0.77).
MT	BLEU point	We then apply the log probability of the phrase orientation classifier as an extra feature in a phrase-based MT system, and get significant BLEU point gains on three test sets: MT02 (+0.59), MT03 (+1.00) and MT05 (+0.77).
alignment search	BRCGs	This may come as a surprise, as ITGs restrict the alignment search space considerably, while (2,2)-BRCGs do not.
MFS refining	recall	An MFS refining system can therefore benefit from answers suggested by a very low recall (but high precision) WSD system.
MFS refining	precision	An MFS refining system can therefore benefit from answers suggested by a very low recall (but high precision) WSD system.
WSD	similarity measure	Our novel high precision WSD algorithms include a reranking algorithm (Section 4), and a Wikipedia-based similarity measure (Section 5).
role recognition	accuracy	For role recognition and labelling we use a standard evaluation set-up, i.e., for role recognition we will evaluate the accuracy with respect to the manually created gold standard, for role labelling we will evaluate precision, recall, and F-Score.
role recognition	precision	For role recognition and labelling we use a standard evaluation set-up, i.e., for role recognition we will evaluate the accuracy with respect to the manually created gold standard, for role labelling we will evaluate precision, recall, and F-Score.
role recognition	recall	For role recognition and labelling we use a standard evaluation set-up, i.e., for role recognition we will evaluate the accuracy with respect to the manually created gold standard, for role labelling we will evaluate precision, recall, and F-Score.
role recognition	F-Score	For role recognition and labelling we use a standard evaluation set-up, i.e., for role recognition we will evaluate the accuracy with respect to the manually created gold standard, for role labelling we will evaluate precision, recall, and F-Score.
role recognition	accuracy	For role recognition and labelling we use a standard evaluation set-up, i.e., for role recognition we will evaluate the accuracy with respect to the manually created gold standard, for role labelling we will evaluate precision, recall, and F-Score.
role recognition	precision	For role recognition and labelling we use a standard evaluation set-up, i.e., for role recognition we will evaluate the accuracy with respect to the manually created gold standard, for role labelling we will evaluate precision, recall, and F-Score.
role recognition	recall	For role recognition and labelling we use a standard evaluation set-up, i.e., for role recognition we will evaluate the accuracy with respect to the manually created gold standard, for role labelling we will evaluate precision, recall, and F-Score.
role recognition	F-Score	For role recognition and labelling we use a standard evaluation set-up, i.e., for role recognition we will evaluate the accuracy with respect to the manually created gold standard, for role labelling we will evaluate precision, recall, and F-Score.
domain adaptation	FINANCES	In domain adaptation experiments the BNC examples play the role of general corpora, and the FINANCES and SPORTS examples the role of two specific domain corpora.
relation classification tasks	F-score	The task of identifying the events and times described in a text and classifying the relations that hold among them has proven to be difficult, however, with reported results for relation classification tasks ranging in F-score from 0.52 to 0.60.
SemEval 2007 lexical substitution task	precision	In this paper, we propose a context-sensitive vector-space approach which draws some important ideas from Erk and Pado's paper ("E&P" in the following), but implements them in a different, more effective way: An evaluation on the SemEval 2007 lexical substitution task data shows that our model significantly outperforms E&P in terms of average precision.
hypernymy acquisition	accuracy	We investigate current work on hypernymy acquisition , and show that adapting one such approach leads to a marked improvement in entailment classification accuracy.
entailment classification	accuracy	We investigate current work on hypernymy acquisition , and show that adapting one such approach leads to a marked improvement in entailment classification accuracy.
SRL	precision	Experiments show that our LTAG-spinal based SRL system achieves very high precision on both goldstandard and automatic parses, and significantly outperforms the one using CCGbank.
Predicate Classification	Accuracy	 Table 6: Predicate Classification Accuracy
automatic recognition of three tags	precision	We have already studied problem 1) and showed that automatic recognition of three tags with a decision tree algorithm reached a precision over 92% ().
crisis management	PlusL	Test: Subjects were recruited to use the crisis management system in MinusL and PlusL condition, although they were not aware of the condition of the system and they were not involved with the project.
coreference resolution	B-CUBED	There does not appear to be a single standard evaluation metric in the coreference resolution community, so we opted to use three: MUC-6 (, CEAF, and B-CUBED (, which seem to be the most widely accepted metrics.
MSR task	REG08-Type Accuracy	For the MSR task, our system has a String Accuracy of 0.68 and a REG08-Type Accuracy of 0.76 and for the NEG task a String Accuracy of 0.79 and REG08-Type Accuracy of 0.83.
MSR task	REG08-Type Accuracy	For the MSR task, our system has a String Accuracy of 0.68 and a REG08-Type Accuracy of 0.76 and for the NEG task a String Accuracy of 0.79 and REG08-Type Accuracy of 0.83.
MWE classification	F-measure	Our approach yields the best results to date on MWE classification combining different linguistically motivated features, the overall performance yields an F-measure of 84.58% corresponding to an F-measure of 89.96% for idiomaticity identification and classification and 62.03% for literal identification and classification.
MWE classification	F-measure	Our approach yields the best results to date on MWE classification combining different linguistically motivated features, the overall performance yields an F-measure of 84.58% corresponding to an F-measure of 89.96% for idiomaticity identification and classification and 62.03% for literal identification and classification.
AMs	MI	Evert (2004) discussed a wide range of AMs, including exact hypothesis tests such as the binomial test and Fisher's exact tests, various coefficients such as evaluated MI, Pearson's 2 χ and Permutation Entropy.
AMs	precision	Next, we search to the right of the light verbs for the nearest noun,  To evaluate the performance of AMs, we can use the standard precision and recall measures, as in much past work.
AMs	recall	Next, we search to the right of the light verbs for the nearest noun,  To evaluate the performance of AMs, we can use the standard precision and recall measures, as in much past work.
alignment	accuracy	They reported that both alignment quality and translation accuracy were improved on a small corpus.
translation	accuracy	They reported that both alignment quality and translation accuracy were improved on a small corpus.
Tagging	accuracy	 Table 1: Tagging accuracy in % for Bangla and  Hindi
alignment refinement	TH	 Table 5: Results for alignment refinement (TH:  threshold, TC: term coverage, ATE: average term  expansion, AAA: average alignment accuracy in  percentage). The highest score for each measure- ment is marked as bold.
alignment refinement	TC: term coverage	 Table 5: Results for alignment refinement (TH:  threshold, TC: term coverage, ATE: average term  expansion, AAA: average alignment accuracy in  percentage). The highest score for each measure- ment is marked as bold.
alignment refinement	ATE: average term  expansion	 Table 5: Results for alignment refinement (TH:  threshold, TC: term coverage, ATE: average term  expansion, AAA: average alignment accuracy in  percentage). The highest score for each measure- ment is marked as bold.
alignment refinement	AAA: average alignment accuracy in  percentage	 Table 5: Results for alignment refinement (TH:  threshold, TC: term coverage, ATE: average term  expansion, AAA: average alignment accuracy in  percentage). The highest score for each measure- ment is marked as bold.
SMT	BLEU	According to the performance trajectories it seems that a reasonable increase in the contribution of the corpus of literal translations effectively improves the decoding performance of the SMT system since the BLEU scores with λ = 0.67 and 0.8 are higher than that of the baseline which are 0.0121 and 0.0105, and of the NIST scores which are 0.
SMT	OOV	Therefore, a proper selection of a corpus of literal translations as training data would contribute more to the improvement of SMT models should some heuristic pruning methods be employed to avoid a possible OOV increase.
word translation probabilities	accuracy	Chen (1993) developed a method based on optimizing word translation probabilities which he showed gave better accuracy than the sentence-length based approach.
SMT translations	word error rate (WER)	Our technique is similar to that of (Munteanu and Marcu, 2005) but we bypass the need of the bilingual dictionary by using proper SMT translations and instead of a maximum entropy classifier we use simple measures like the word error rate (WER) and the translation edit rate (TER) to decide whether sentences are parallel or not.
SMT translations	translation edit rate (TER)	Our technique is similar to that of (Munteanu and Marcu, 2005) but we bypass the need of the bilingual dictionary by using proper SMT translations and instead of a maximum entropy classifier we use simple measures like the word error rate (WER) and the translation edit rate (TER) to decide whether sentences are parallel or not.
MSR paraphrase detection test set	Accuracy	 Table 3: System performance on 1725 examples of  the MSR paraphrase detection test set. Accuracy  (micro-averaged F 1 ), F 1 for c 1 "paraphrase" and  c 0 "non-paraphrase" classes, and macro-averaged
MSR paraphrase detection test set	F 1	 Table 3: System performance on 1725 examples of  the MSR paraphrase detection test set. Accuracy  (micro-averaged F 1 ), F 1 for c 1 "paraphrase" and  c 0 "non-paraphrase" classes, and macro-averaged
entity extraction task	accuracy	Furthermore, applying the extended gazetteers to an entity extraction task in a scientific domain, we empirically observed a significant improvement in system accuracy when compared with those using seed gazetteers.
tagging	accuracy	The results show improvement in tagging accuracy for individual corpora to 94.2% and also for the merged corpus to 91%.
tagging	accuracy	The results show that T3 gives better tagging accuracy, as shown in.
Transliteration of Named-Entities (NEs)	accuracy	Transliteration of Named-Entities (NEs) is an important problem that affects the accuracy of many NLP applications like Cross Lingual Search and Machine Translation.
ML	ME	Some of the very effective ML approaches used in NER are ME), CRF () and SVM ().
NER	ME	Some of the very effective ML approaches used in NER are ME), CRF () and SVM ().
NER	Recall (R)	Evaluation results of the development set for the NER models are presented in in terms of percentages of Recall (R), Precision (P) and F-Score (FS).
NER	Precision (P)	Evaluation results of the development set for the NER models are presented in in terms of percentages of Recall (R), Precision (P) and F-Score (FS).
NER	F-Score (FS)	Evaluation results of the development set for the NER models are presented in in terms of percentages of Recall (R), Precision (P) and F-Score (FS).
Argumentative Zoning	F-measure	We also show how Argumentative Zoning can be applied to other domains by evaluating our system on a corpus of Astronomy journal articles, achieving an F-measure of 97.9%.
semantic construction	DUDES	In this paper we present a novel formalism for semantic construction called DUDES (Dependency-based Underspecified Discourse REpresentation Structures).
SRL	precision	As in the CoNLL (Conference on Natural Language Learning) shared task for SRL, the evaluation metrics used are precision, recall and F measure.
SRL	recall	As in the CoNLL (Conference on Natural Language Learning) shared task for SRL, the evaluation metrics used are precision, recall and F measure.
SRL	F measure	As in the CoNLL (Conference on Natural Language Learning) shared task for SRL, the evaluation metrics used are precision, recall and F measure.
coverage of ITGs	parse failure rates (PFRs)	All previous work on the coverage of ITGs, however, concerns parse failure rates (PFRs) or sentence level coverage, which is not directly related to any of the evaluation measures used in machine translation.
parsing	accuracy	generic parser model selection and its effect on parsing accuracy.
dependency parsing	accuracy	Recent work on dependency parsing has resulted in considerable progress with respect to both accuracy and efficiency, not least in the treatment of discontinuous syntactic constructions, usually modeled by non-projective dependency trees.
parsing	accuracy	The state of the art parsing accuracy for many MoR-FWO languages is still low compared to that of English.
RST parsing	speed	Our results improve on accuracy over existing approaches for data-driven RST parsing, while also improving on speed over Soricut and Marcu's chart parsing approach, which produces state-of-the-art results for RST discourse relations within sentences.
parsing	accuracy	Furthermore, any mistakes in the underlying token segmentations are sure to be reflected in the parsing accuracy.
Feature propagation in verb groups	LAS	Feature propagation in verb groups (P VG ) improves LAS in almost 0.5% (row 6 in).
case propagation	accuracy	While coordination and case propagation do not improve significantly the accuracy by themselves (rows 7 and 8), their combination with P VG (verb groups) significantly increases LAS (+0.86%, see row 10).
case propagation	LAS	While coordination and case propagation do not improve significantly the accuracy by themselves (rows 7 and 8), their combination with P VG (verb groups) significantly increases LAS (+0.86%, see row 10).
MT	BLEU	The overall MT system is evaluated both with and without function guessing on 500 held-out sentences, and the quality of the translation is measured using the BLEU metric ().
parsing	accuracy	In order to evaluate the relations between tagsets and parsing accuracy on a given treebank, we extract the optimal tagsets 13 from the FTB, the CC tagset and we convert the MFT POS tags to this tagset.
parsing	accuracy	For all experiments, we used the EVALB tool 1 for evaluation, and used labeled recall (LR), labeled precision (LP ) and F 1 score (which is the  harmonic mean of LR and LP ) to measure parsing accuracy.
parsing	accuracy	In this paper we explore anew way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure.
part-of-speech tagging	accuracy	For all experiments described here, part-of-speech tagging was done separately using a CRF tagger with accuracy of 97.3% on sections 22-24.
parsing	accuracy	Vine parsing is a parsing strategy that guarantees fast parsing and smaller models, but the accuracy of dependency-based vine parsers has been non-competitive).
Supertagging	accuracy	 Table 3: Supertagging accuracy and training&  testing speed on section 22. ( ‡) Test time was cal- culated on totally 1648 sentences.
Parsing	accuracy	 Table 1: Parsing accuracy scores.
parse selection	Exact (match) accuracy	To compare among the feature types, we build loglinear training models) for parse selection (which is standard for unificationbased grammars) for TDFC, local configurations, n-grams and active edges . For each model, we calculate the following evaluation metrics - • Exact (match) accuracy: it is simply the percentage of times that the top-ranked analysis for each test sentences is identical with the gold analysis of the same sentence.
prediction of semantic content from partial speech recognition hypotheses	accuracy	In, we presented our first results at prediction of semantic content from partial speech recognition hypotheses, looking at length of the speech hypothesis as a general indicator of semantic accuracy in understanding.
IR	F 1	We also provide results for the commonly-used IR measures F 1 , recall, and precision.
IR	recall	We also provide results for the commonly-used IR measures F 1 , recall, and precision.
IR	precision	We also provide results for the commonly-used IR measures F 1 , recall, and precision.
classification	accuracy	Depending on the task, non-acoustic features add 2.3% in classification accuracy compared to using only acoustic features.
IVR anger recognition	accuracies	While hot anger with studio quality conditions can be determined with over 90% () studies on IVR anger recognition report lower accuracies due to these limitations.
classification	accuracy	Obviously, adding non-acoustic features increases classification accuracy significantly, but only where the acoustic features are not expressive enough.
ASR	accuracy	This means that dialog systems can guide users' word choice and can adapt their own recognition models to gain improved ASR accuracy.
ASR	accuracy	Currently, much dialog systems research is devoted to improving ASR accuracy, because this is a significant contributor to task success rates and to dialog length.
ASR	length	Currently, much dialog systems research is devoted to improving ASR accuracy, because this is a significant contributor to task success rates and to dialog length.
ASR	accuracy	One way to improve ASR accuracy is to use targeted language models.
ASR	accuracy	Since users adapt to the system's choices of realization for task-related concepts, we can predict the user's choice of realization and use this to adjust the ASR's language model, improving ASR accuracy specifically on concept words.
ASR	accuracy	Since users adapt to the system's choices of realization for task-related concepts, we can predict the user's choice of realization and use this to adjust the ASR's language model, improving ASR accuracy specifically on concept words.
ASR	accuracy	Another way to improve ASR accuracy is to guide the user into using words that are likely to be recognized correctly).
ASR	accuracy	Our results imply that if the designer of a dialog system wants to improve ASR accuracy, system prompts should be designed to use word choices that are more recognizable; and, when, possible, to be adaptive to the user's choice of form for task-related concepts.
Resolutions	F1 score	The improvements for Resolutions are highly significant: with manual transcripts, the F1 score increases from .64 to .67 (p < 0.005), and with ASR, from .33 to .37 (p < 0.005).
Resolutions	ASR	The improvements for Resolutions are highly significant: with manual transcripts, the F1 score increases from .64 to .67 (p < 0.005), and with ASR, from .33 to .37 (p < 0.005).
per-frame classification task	ROC	The results on the per-frame classification task, including the ROC curves for the different models are presented and discussed in more detail in Appendix A. At runtime, the system uses these frame-based models to predict across time the likelihood that a given agent intends to engage (see).
Span ordering classification	accuracy	 Table 2: Span ordering classification accuracy.  For first-level data, n = 3147. For all data, n =  10170. Labels = {0 1, 1 0}.
Discourse cue classification	accuracy	 Table 4: Discourse cue classification accuracy.  For first-level data, n = 3147 and no. labels = 92.  For all data, n = 10170 and no. labels = 203.
ASR	betaken	That is, any type of ASR error in any position should betaken into consideration in ASR results of OOG utterances.
sentence splitter	F 1 score	The sentence splitter achieved an F 1 score of 92.54%.
MTT	BADELE	The MTT tools used and the database, BADELE 3000, are described in section 5.
definition extraction	f-measure	Once weights were learnt and applied to the definition extraction tool, this increased to 68% f-measure.
Definition extraction (DE) evaluation	precision	Definition extraction (DE) evaluation in terminology maybe seen as a task aimed at enhancing precision and reducing the noise generated, for example, by limited extraction algorithms, i.e. as a task consisting in separating information on a concept from other information (for example, on another concept).
word accumulation	precision	Consistent with the observations in, word accumulation secures the highest precision at the cost of the lowest recall, while LSA-based systems achieve high recall but significantly lower precision.
word accumulation	recall	Consistent with the observations in, word accumulation secures the highest precision at the cost of the lowest recall, while LSA-based systems achieve high recall but significantly lower precision.
word accumulation	recall	Consistent with the observations in, word accumulation secures the highest precision at the cost of the lowest recall, while LSA-based systems achieve high recall but significantly lower precision.
word accumulation	precision	Consistent with the observations in, word accumulation secures the highest precision at the cost of the lowest recall, while LSA-based systems achieve high recall but significantly lower precision.
identification of emotional email	accuracy	We cast the identification of emotional email as a text classification problem, and show that using salient features we can significantly improve the identification accuracy.
sentiment analysis	F score	• Each of the sentiment analysis resources alone is useful, and using them with the projection rules leads to improved performance over the baselines (10 points in F score for M and 6 points overall).
summarisation	accuracy	We setup a comparison framework to study the effect of different summarisation algorithms of various compression rates in this task and compare the classification accuracy of summaries and documents for associating documents to classes.
machine translation	BLEU score	In machine translation, the evaluation tool most commonly used for evaluating output, the BLEU score (), rates the "goodness" of output based on n-gram overlap with human-generated text.
Pinyin generation	accuracy	On the other hand, automatic Pinyin generation is considered a solved task, ( shows that using the most frequent Pinyin approach to assign Pinyin to each character can achieve 98% accuracy.
predicting the activation of a set of voxels	accuracy	Whereas () aimed at predicting the activation of a set of voxels, and judging how good that prediction is by its 2-way accuracy, this paper focuses on a different sort of experiment: prediction of semantic feature values fora test example, as schematized in.
interpretation	ROI	Furthermore, it facilitates interpretation since results can be analyzed at the ROI level instead of at the single voxel level.
Classification	accuracy	 Table 1: Classification accuracy, participant 1  Type (available signals) 60 ch. all ch. 60 cp.
Classification	accuracy	 Table 2: Classification accuracy, participant 2  Type (available signals) 60 ch. all ch. 60 cp.
Extracting	accuracy	Extracting such human-like features is difficult, and we do not anticipate a high level of accuracy in these early experiments.
MTurk	accuracy	This serves as an indicator of the effectiveness of the MTurk results: the accuracy can betaken as a general confidence measure for worker predictions; when five workers agree we can be 97.43% confident in the correctness of their prediction, when at least four workers agree we can be 94.63% confident, etc.
HIT design	accuracy	Our overarching hypothesis is that better addressing human factors in HIT design can yield significantly reduce cost, reduce time, and/or increase accuracy of the annotations obtained via crowdsourcing.
MTurk annotations	Precision	 Table 2. Quality measurement of MTurk annotations (k: Agreement level, P: Precision, R: Recall, F: F-measure;  the highest value for each column is in boldface)
MTurk annotations	Recall	 Table 2. Quality measurement of MTurk annotations (k: Agreement level, P: Precision, R: Recall, F: F-measure;  the highest value for each column is in boldface)
MTurk annotations	F-measure	 Table 2. Quality measurement of MTurk annotations (k: Agreement level, P: Precision, R: Recall, F: F-measure;  the highest value for each column is in boldface)
Knowledge Representation and Reasoning	scale	In Knowledge Representation and Reasoning, systems have achieved impressive performance and scale in far more complex problems than the past.
text interpretation	accuracy	We also showed that this representation could improve text interpretation accuracy considerably because the system could postpone resolving ambiguity until more evidence accumulates.
interpretation	accuracy	Our experiment showed that the approach could improve the interpretation accuracy considerably by delaying ambiguity resolution while avoiding combinatorial explosion.
SRL	recall	Second, SRL is trained on news corpora using a resource like Propbank, and so may face recall loss due to out of vocabulary verbs and precision loss due to different writing styles found on the Web.
SRL	precision	Second, SRL is trained on news corpora using a resource like Propbank, and so may face recall loss due to out of vocabulary verbs and precision loss due to different writing styles found on the Web.
entity classification	accuracy	We first compare our slot vector features with other features extracted from Wikipedia for entity classification task and then evaluate their accuracy.
system evaluation	precision	This low rater reliability has repercussions for system evaluation: Their experiments showed that system precision could vary as much as 10% depending on which rater's judgments they used as the gold standard.
preposition selection	Turkers	 Table 1. In the task of  preposition selection, only three Turkers are needed  to match the reliability of two trained raters; in the  more complicated task of error detection, up to 13  Turkers are needed. However, it should be noted  that these numbers can be viewed as upper bounds.  The error annotation scheme that was used is a very  simple one. We intend to experiment with different
second language acquisition (SLA)	accuracy	In the second language acquisition (SLA) and child language development research fields, the language development is measured according to fluency, accuracy, and complexity).
SVM prediction	accuracy	 Table 4: SVM prediction accuracy using linear kernel
SVM prediction	accuracy	 Table 5: SVM prediction accuracy using RBF kernel
Entity extraction	precision	Entity extraction performance is typically measured in terms of precision, recall and balanced f-measure at both the token (word) and phrase level.
Entity extraction	recall	Entity extraction performance is typically measured in terms of precision, recall and balanced f-measure at both the token (word) and phrase level.
IR	precision	One of the major obstacles in bridging the gap between IR and Natural Language Processing (NLP) is how to retain the flexibility and precision of working with text at the lexical level while gaining the greater descriptive precision that NLP provides.
MRR	Baseline MRR	 Table 1: The MRR results of the models presented in  §5 on testing dataset (TREC 2004) using different window sizes  of candidate passages. The statistically significant model results in each corresponding MRR category are bolded.  Baseline MRR=%67.6, Top1=%58, Top5=%82.2.
text normalization	accuracy	Experiment results suggest that the choice of text normalization technique should be made individually on each topic to enhance information retrieval accuracy.
ASR	accuracy	Traditional representations in ASR such as hidden Markov models (HMMs) trained for speaker independence that achieve 84.8% word-level accuracy for non-dysarthric speakers might achieve less than 4.5% accuracy given severely dysarthric speech on short sentences.
ASR	accuracy	Traditional representations in ASR such as hidden Markov models (HMMs) trained for speaker independence that achieve 84.8% word-level accuracy for non-dysarthric speakers might achieve less than 4.5% accuracy given severely dysarthric speech on short sentences.
parsing	accuracy	We examined a large space of settings including the following: (a) the contribution of POS tagsets to the parsing quality, as a function of the amount of information encoded in the tagset; (b) parsing performance on gold vs. predicted POS and morphological feature values for all models; (c) prediction accuracy of each POS tagset and morphological feature; (d) the contribution of numerous morphological features in a controlled fashion; and (e) the contribution of certain feature and POS tagset combinations.
parsing	accuracy	We examined a large space of settings including the following: (a) the contribution of POS tagsets to the parsing quality, as a function of the amount of information encoded in the tagset; (b) parsing performance on gold vs. predicted POS and morphological feature values for all models; (c) prediction accuracy of each POS tagset and morphological feature; (d) the contribution of numerous morphological features in a controlled fashion; and (e) the contribution of certain feature and POS tagset combinations.
parsing	accuracy	Among the three components (a-c, above), the parsing accuracy obtained using the POS feature is taken as baseline.
dependency parsing	accuracy	We follow this by experiments where we explore how each of morph and chunk features help in improving dependency parsing accuracy.
parsing	accuracy	Although many systems performed feature engineering on the BDT at CoNLL 2007, providing a strong baseline, we will take a step further to improve parsing accuracy taking into account the effect of specific morphosyntactic features.
parsing	accuracy	showed that they can increase parsing accuracy across languages/treebanks.
parsing Hebrew	F 1 84.13	We focus on parsing Hebrew and report the best result to date, F 1 84.13 for parsing off of gold-tagged text, 5% error reduction from previous results.
parsing Hebrew	error reduction	We focus on parsing Hebrew and report the best result to date, F 1 84.13 for parsing off of gold-tagged text, 5% error reduction from previous results.
parsing off of gold-tagged text	error reduction	We focus on parsing Hebrew and report the best result to date, F 1 84.13 for parsing off of gold-tagged text, 5% error reduction from previous results.
parsing gold PoS-tagged segments	error	The best result we report for the RR-AGR model, F 1 84.13, is the best result reported for Hebrew to date for parsing gold PoS-tagged segments, with 5% error reduction from previous results.
parsing	accuracy	We investigate parsing accuracy on the Korean Treebank 2.0 with a number of different grammars.
Tagging	accuracy	 Table 5: Tagging accuracy and UAS scores for modified terminal symbols in the dev set, grouped by ranges of fre- quencies in the modified training sets. The "replaced by UNKC*" line corresponds to the case where the desinflected  form or the POS+lemma pair does not appear more than 200 times in the L'est Républicain corpus.
Tagging	UAS	 Table 5: Tagging accuracy and UAS scores for modified terminal symbols in the dev set, grouped by ranges of fre- quencies in the modified training sets. The "replaced by UNKC*" line corresponds to the case where the desinflected  form or the POS+lemma pair does not appear more than 200 times in the L'est Républicain corpus.
parsing	f-score	Over the last 10 years, parsing performance on the PTB has hit a performance plateau of 90-92% f-score using the PARSEVAL evaluation metric.
parsing	accuracy	The primary purpose of this investigation is to study the role of different morphosyntactic features in Hindi dependency parsing, but we also want to improve the overall parsing accuracy.
parsing	accuracy	The overall effect on parsing accuracy is nevertheless very marginal, bringing LAS to 56.5% and UAS to 78.6%.
parsing	LAS	The overall effect on parsing accuracy is nevertheless very marginal, bringing LAS to 56.5% and UAS to 78.6%.
parsing	UAS	The overall effect on parsing accuracy is nevertheless very marginal, bringing LAS to 56.5% and UAS to 78.6%.
parsing	accuracy	However, incorporating these features did not improve parsing accuracy and hence these features were not used in the final setting.
parsing	accuracy	The addition of morphological-agreement feature improves the parsing accuracy, making it on-par with a second-order globally optimized MST parser.
dependency parsing	Easy	In recent work, we proposed another dependency parsing approach: Easy First, Non-Directional dependency * Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University 1 Strictly speaking, the traversal order is from start to end.
parsing	accuracy	In addition, adding morphological information to the default configurations of these parsers does not improve parsing accuracy.
Size of Corpora	Estimated Domain Counts	 Table 4: Size of Corpora and Range of Domains  (Estimated Domain Counts)
Translation	Fwd	 Table 2: Translation performances in BLEU-4(%)  over 1000-best lists for Chinese-to-English task: "res- coring" represents the results of rescoring; "three- pass", three-pass hypothesis regeneration with for- ward n-gram expansion; "FCD", fast consensus de- coding; "Fwd", the results of hypothesis regeneration  with forward n-gram expansion; "Bwd", backward n- gram expansion; and "Bid", bi-directional n-gram  expansion.
Translation	Bwd	 Table 2: Translation performances in BLEU-4(%)  over 1000-best lists for Chinese-to-English task: "res- coring" represents the results of rescoring; "three- pass", three-pass hypothesis regeneration with for- ward n-gram expansion; "FCD", fast consensus de- coding; "Fwd", the results of hypothesis regeneration  with forward n-gram expansion; "Bwd", backward n- gram expansion; and "Bid", bi-directional n-gram  expansion.
Translation	Bid	 Table 2: Translation performances in BLEU-4(%)  over 1000-best lists for Chinese-to-English task: "res- coring" represents the results of rescoring; "three- pass", three-pass hypothesis regeneration with for- ward n-gram expansion; "FCD", fast consensus de- coding; "Fwd", the results of hypothesis regeneration  with forward n-gram expansion; "Bwd", backward n- gram expansion; and "Bid", bi-directional n-gram  expansion.
Translation	BLEU-4	 Table 5: Translation performances in BLEU-4 (%)  over 1000-best lists for Chinese-to-English task:  "full" represents expectations over n-gram counts that  are computed on whole hypotheses; "partial"  represents expectations over n-gram counts that are  computed on partial hypotheses.
sentence ranking task	P (E)	 Table 4: Inter-and intra-annotator agreement for  the sentence ranking task. In this task, P (E) is  0.333.
sentence ranking task	P (E)	 Table 11: Inter-and intra-annotator agreement for  the MTurk workers on the sentence ranking task.  (As before, P (E) is 0.333.) For comparison, we  repeat here the kappa coefficients of the experts  (K  *  ), taken from
Data filtering	BLEU	 Table 2: Data filtering (E-F BLEU, no rescoring)
Translation	BLEU Score	 Table 1: Translation results for English-German  (BLEU Score)
Translation	BLEU Score	 Table 2: Translation results for German-English  (BLEU Score)
translation	OOV rate	In addition, we perform general compound splitting for German both before training and translation, which also reduces the OOV rate.
statistical machine translation (SMT)	BLEU score	applied morph-based models in statistical machine translation (SMT) between several language pairs without gaining improvement in BLEU score, but obtaining reductions in out-of-vocabulary rates.
translation between English and Japanese	distortion limit (dl)	Therefore, we should exclude dictionary entries from the calculation of τ .  In general, it is believed that translation between English and Japanese requires a large distortion limit (dl), which restricts how far a phrase can move.
Transductive regression	accuracy	Transductive regression is shown to achieve higher accuracy than L 2 regularized ridge regression on some machine learning benchmark datasets (.
MT	BLEU	have extended the work by showing that NCD can be used to rank translations of different MT systems so that the ranking order correlates with human rankings at the same level as BLEU ().
machine translation (MT)	BLEU	Since machine translation (MT) models are typically evaluated by BLEU (), a loss function which rewards partial matches, the MBR solution is to be preferred to the Maximum A Posteriori (MAP) solution.
machine translation (MT)	Maximum A Posteriori (MAP)	Since machine translation (MT) models are typically evaluated by BLEU (), a loss function which rewards partial matches, the MBR solution is to be preferred to the Maximum A Posteriori (MAP) solution.
Parameter tuning	BLEU	Parameter tuning is an important aspect of current data-driven machine translation systems, as an improper selection of feature weights can dramatically reduce scores on evaluation metrics such as BLEU () or METEOR ().
Parameter tuning	METEOR	Parameter tuning is an important aspect of current data-driven machine translation systems, as an improper selection of feature weights can dramatically reduce scores on evaluation metrics such as BLEU () or METEOR ().
translation	BLEU	We analyze the translation quality as measured by the BLEU score for the three methods: equal weights, LM weights and Condor weights and considering onetime resampling.
translation	BLEU	For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU) and METEOR).
translation	METEOR	For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU) and METEOR).
English-to-Japanese translation of multi-clause sentences	BLEU	In our experiment on the English-to-Japanese translation of multi-clause sentences, the proposed method improved the translation performance by 1.4% in BLEU and 1.3% in TER by using Moses, and by 2.2% in BLEU and 3.5% in TER by using our hierarchical phrase-based SMT.
English-to-Japanese translation of multi-clause sentences	TER	In our experiment on the English-to-Japanese translation of multi-clause sentences, the proposed method improved the translation performance by 1.4% in BLEU and 1.3% in TER by using Moses, and by 2.2% in BLEU and 3.5% in TER by using our hierarchical phrase-based SMT.
English-to-Japanese translation of multi-clause sentences	BLEU	In our experiment on the English-to-Japanese translation of multi-clause sentences, the proposed method improved the translation performance by 1.4% in BLEU and 1.3% in TER by using Moses, and by 2.2% in BLEU and 3.5% in TER by using our hierarchical phrase-based SMT.
SMT	BLEU	 Table 3: Experimental results obtained with Moses and our hierarchical phrase-based SMT, in BLEU,  TER, and PER.  Moses : BLEU (%) / TER (%) / PER (%)
SMT	TER	 Table 3: Experimental results obtained with Moses and our hierarchical phrase-based SMT, in BLEU,  TER, and PER.  Moses : BLEU (%) / TER (%) / PER (%)
SMT	PER	 Table 3: Experimental results obtained with Moses and our hierarchical phrase-based SMT, in BLEU,  TER, and PER.  Moses : BLEU (%) / TER (%) / PER (%)
SMT	BLEU	 Table 3: Experimental results obtained with Moses and our hierarchical phrase-based SMT, in BLEU,  TER, and PER.  Moses : BLEU (%) / TER (%) / PER (%)
SMT	TER	 Table 3: Experimental results obtained with Moses and our hierarchical phrase-based SMT, in BLEU,  TER, and PER.  Moses : BLEU (%) / TER (%) / PER (%)
SMT	PER	 Table 3: Experimental results obtained with Moses and our hierarchical phrase-based SMT, in BLEU,  TER, and PER.  Moses : BLEU (%) / TER (%) / PER (%)
NTCIR 8 Multilingual Analysis Task	precision	Compared to the NTCIR 8 Multilingual Analysis Task this year, we obtained significant improvements in precision, with a recall that is comparable to most of the participating systems.
NTCIR 8 Multilingual Analysis Task	recall	Compared to the NTCIR 8 Multilingual Analysis Task this year, we obtained significant improvements in precision, with a recall that is comparable to most of the participating systems.
emotion detection	precision	The best results for emotion detection were obtained for the "anger" category, where the precision was around 35 percent, fora recall of 19 percent.
emotion detection	recall	The best results for emotion detection were obtained for the "anger" category, where the precision was around 35 percent, fora recall of 19 percent.
NER taggers	recall	The following metrics were used to evaluate how good the silver standard(s) fit(s) the provided gold standard: • segment-level recall, precision, and F-score values with exact boundaries, the standard way to evaluate NER taggers, • segment-level recall, precision, and F-score, but with relaxed boundary constraints.
NER taggers	precision	The following metrics were used to evaluate how good the silver standard(s) fit(s) the provided gold standard: • segment-level recall, precision, and F-score values with exact boundaries, the standard way to evaluate NER taggers, • segment-level recall, precision, and F-score, but with relaxed boundary constraints.
NER taggers	F-score	The following metrics were used to evaluate how good the silver standard(s) fit(s) the provided gold standard: • segment-level recall, precision, and F-score values with exact boundaries, the standard way to evaluate NER taggers, • segment-level recall, precision, and F-score, but with relaxed boundary constraints.
NER tagger	Parameters	 Table 5: Performance of an NER tagger trained on an SSC, 10-fold cross validation, and all systems.  Parameters: threshold (confidence or cosine) and number of agreeing systems (agr. systems).
Turku Event Extraction	precision	We have previously evaluated the Turku Event Extraction System on a random 1% sample of PubMed citations, estimating a precision of 64% for event types and arguments pertaining to subtask 1 of the Shared Task (, which compares favorably to the 58% precision the system achieves on the Shared Task dataset itself.
Prevent relation	F-measure	We obtain 98.55% F-measure for the Cure relation, 100% F-measure for the Prevent relation, and 88.89% F-measure for the Side Effect relation.
event extraction	F-score	 Table 7: Performance of the event extraction  framework. First column: using the baseline fea- ture representation. Second column: using the  extended feature representation. All performance  rates indicate F-score, except for the last two rows.
parsing	accuracy	This paper examines the effects of varying beam width on parsing accuracy and speed in this model, showing that parsing accuracy degrades gracefully as beam width decreases dramatically (to 2% of the width used to achieve previous top results), without sacrificing gains over a baseline CKY parser.
parsing	accuracy	This paper examines the effects of varying beam width on parsing accuracy and speed in this model, showing that parsing accuracy degrades gracefully as beam width decreases dramatically (to 2% of the width used to achieve previous top results), without sacrificing gains over a baseline CKY parser.
parsing	accuracy	This paper examines the effects of varying beam width on parsing accuracy and speed in this model, showing that parsing accuracy degrades gracefully as beam width decreases dramatically (to 2% of the width used to achieve previous top results), without sacrificing gains over a baseline CKY parser.
parsing	accuracy	Thus, the first experiments in this paper evaluate the degradation of parsing accuracy depending on beam width of the HHMM parser.
parsing	accuracy	These results show fairly graceful decline in parsing accuracy with abeam width starting at 2000 elements down to about 50 beam elements.
parsing	accuracy	This experiment is intended to address two questions: Whether this framework is efficient enough to be considered a viable psycholinguistic model, and whether its parsing time and accuracy remain competitive with more standard cubic time parsing technologies at low beam widths.
parsing	accuracy	The results of this experiment show that the HHMM parser is indeed competitive with a probabilistic CKY parser, in terms of parsing efficiency, even while parsing with higher accuracy.
parsing	accuracy	The results of this experiment show that the HHMM parser is indeed competitive with a probabilistic CKY parser, in terms of parsing efficiency, even while parsing with higher accuracy.
Substitution acceptability	SOMEWHAT	 Table 3: Substitution acceptability in reduced cov- erage settings. SOMEWHAT class accounts for  percentage points missing to 100%.
SMT	accuracy (ACC) score	Interestingly, although an SMT based approach could not achieve a precise top-1 transliteration result, it is found in () that, in contrast to the ordinary top-1 accuracy (ACC) score, its recall rate, which is defined in terms of whether the correct answer is generated in the n-best output list, is rather high.
SMT	recall rate	Interestingly, although an SMT based approach could not achieve a precise top-1 transliteration result, it is found in () that, in contrast to the ordinary top-1 accuracy (ACC) score, its recall rate, which is defined in terms of whether the correct answer is generated in the n-best output list, is rather high.
SMT	minimum error rate training (MERT)	() modeled the phrase-based SMT system using minimum error rate training (MERT) for learning model weights.
classification	accuracy	Adding multilingual features helps boost classification accuracy and is shown to effectively classify multilingual pages in a language independent way.
Classification	recall	Classification is done using Support Vectors Machine (SVM) classifier at first, and then the threshold of SVM is adjusted in order to improve the recall scores of classification.
SVM classification	recall	To improve SVM classification beta-gamma threshold adjustment was used to improve recall of different NE classes and consequently overall F measure.
SVM classification	F measure	To improve SVM classification beta-gamma threshold adjustment was used to improve recall of different NE classes and consequently overall F measure.
SUB-E	F1	 Table 6. Results for SUB-E: Best F1 bolded and  italicized if significantly better than baseline.
SUB-EM	F1	 Table 7. Results for SUB-EM: Best F1 bolded and  italicized if significantly better than baseline.
SUB-EM+	F1	 Table 8. Results for SUB-EM+: Best F1 bolded and  italicized if significantly better than baseline.
SUB-M	F1	 Table 9. Results for SUB-M: Best F1 bolded and  italicized if significantly better than baseline.
SUB-M+	F1	 Table 10. Results for SUB-M+: Best F1 bolded and  italicized if significantly better than baseline.
Named Entity recognition (NER)	F1	In comparison, there are numerous Named Entity recognition (NER) systems, both general-purpose and specialized, and many of them achieve scores better than F1 = 0.95).
SMT	BLEU	The baseline score for applying a standard phrase-based SMT model yields an average score of 28.67 BLEU per document (28.60 per sentence) which is quite reasonable for an out-ofdomain test.
Sequence labeling	error rate	Sequence labeling systems like part-of-speech taggers are typically trained on newswire text, and in tests their error rate on, for example, biomedical data can triple, or worse.
parser domain adaptation	accuracy	We present experiments with a simple selftraining approach to semi-supervised parser domain adaptation that produce results that contradict the commonly held assumption that improved parser accuracy cannot be obtained by self-training a generative parser without reranking.
parsing	accuracy	This is, to our knowledge, the first attempt to quantify the benefits of semisupervised parser domain adaptation in semantic role labeling, a task in which parsing accuracy is crucial.
information retrieval	accuracy	Semantic spaces based on simple contextual units have been early used in information retrieval, showing dramatic impact on accuracy and scalability of many tasks.
WSD	accuracies	The use of coarse-grained sense groups ( ) has led to considerable advances in WSD performance, with accuracies of around 90% (.
grammar induction	f-score	Consequential a lot of effort has been put into unsupervised grammar induction during the last years and results and performance of unsupervised parsers improved steadily.'s constituent context model (CCM) obtains 51.2% f-score on ATIS part-of-speech strings.
parsing	recall	In order to evaluate the quality of the obtained grammar we have used the most common measures for parsing and grammar induction evaluation: recall, precision, and their harmonic mean (F-measure).
parsing	precision	In order to evaluate the quality of the obtained grammar we have used the most common measures for parsing and grammar induction evaluation: recall, precision, and their harmonic mean (F-measure).
parsing	F-measure	In order to evaluate the quality of the obtained grammar we have used the most common measures for parsing and grammar induction evaluation: recall, precision, and their harmonic mean (F-measure).
syntactic parsing	F1	For syntactic parsing, our bilingual predictor increases F1 by 2.1% absolute, and retraining a monolingual model on its output gives an improvement of 2.0%.
machine translation	BLEU	These results carryover to machine translation, where we can achieve slightly better BLEU improvements than the supervised model of Burkett and Klein (2008) since we are able to train our model directly on the parallel data where we perform rule extraction.
segmentations	Initial Sub- traction	 Table 2: Number of segmentations performed by  each operation: USC Segmentation, Initial Sub- traction, and Final Subtraction.
SASI	accuracy	We used two experimental frameworks to test SASI's accuracy.
Translation	BLEU4	Translation quality is measured by BLEU4 score ignoring the case.
initialization	recall	In the comparison of initialization strategies, the dividing strategy achieves a much higher recall of 0.7605, which is also the highest among all models.
alignment	AER	Thus, improving alignment quality according to AER or F 1 may not directly lead to an increase of BLEU scores.
alignment	F 1	Thus, improving alignment quality according to AER or F 1 may not directly lead to an increase of BLEU scores.
alignment	BLEU	Thus, improving alignment quality according to AER or F 1 may not directly lead to an increase of BLEU scores.
machine translation	BLEU4	 Table 4: Experiments on machine translation (BLEU4 scores in percentage)
Multiclass classification	accuracy	 Table 2: Multiclass classification accuracy on  three benchmarks.
hedge detection	F-measure	The experiments show that our system achieves 86.36% F-measure on biological corpus and 55.05% F-measure on Wikipedia corpus for hedge detection, and 49.95% F-measure on biological corpus for hedge scope detection.
hedge detection	F-measure	The experiments show that our system achieves 86.36% F-measure on biological corpus and 55.05% F-measure on Wikipedia corpus for hedge detection, and 49.95% F-measure on biological corpus for hedge scope detection.
hedge detection	F-score	By allowing an efficient handling of combinations of large-scale input features, the discriminative approach we adopted showed highly competitive empirical results for hedge detection on the Wikipedia dataset: our system is ranked as the first with an F-score of 60.17%.
hedge detection	precision	Performance for the hedge detection system was calculated at the word level while performance for the uncertainty classification stage was calculated at the sentence level using the classes of hedged and uncertain as the positive class for precision, recall and F1 statistics.
hedge detection	recall	Performance for the hedge detection system was calculated at the word level while performance for the uncertainty classification stage was calculated at the sentence level using the classes of hedged and uncertain as the positive class for precision, recall and F1 statistics.
hedge detection	F1	Performance for the hedge detection system was calculated at the word level while performance for the uncertainty classification stage was calculated at the sentence level using the classes of hedged and uncertain as the positive class for precision, recall and F1 statistics.
hedge cue detection	BIO	Similarly to previous work in hedge cue detection, we first convert the task into a sequential labeling task based on the BIO scheme, where each word in a hedge cue is labeled as B-CUE, I-CUE, or O, indicating respectively the labeled word is at the beginning of a cue, inside of a cue, or outside of a hedge cue; this is similar to the tagging scheme from the CoNLL-2001 shared task.
negation extraction	F1	A new negation corpus is presented that was constructed for the domain of English product reviews obtained from the open web, and the proposed negation extraction system is evaluated against the reviews corpus as well as the standard BioScope negation corpus, achieving 80.0% and 75.5% F1 scores, respectively.
ML	F-measure	On a test dataset, the ML approach showed significantly better results (51% F-measure) compared to the command-based rules (35-42% F-measure).
ML	F-measure	On a test dataset, the ML approach showed significantly better results (51% F-measure) compared to the command-based rules (35-42% F-measure).
RTE	accuracy	 Table 4: RTE systems' accuracy on phenomena
SRL task	BI	In this paper, the SRL task of CFN comprises two subtasks: BI and SRC.
SRL	BI	We summarized the experiment results of every stage of our SRL model, that is, BI, SRC and a combination of these two steps (BI+SRC).
SRL	BI	We summarized the experiment results of every stage of our SRL model, that is, BI, SRC and a combination of these two steps (BI+SRC).
SRL	BI  SRC  BI+SRC  p − value 0.77 0.166  0.228	 Table 6. Test values between two SRL models  BI  SRC  BI+SRC  p − value 0.77 0.166  0.228
word translation disambiguation	recall	In our word translation disambiguation experiment, the results show that our method achieved 42% recall and 49% precision for Japa-nese-English newspaper texts, and 45% recall and 76% precision for Chi-nese-Japanese technical documents.
word translation disambiguation	precision	In our word translation disambiguation experiment, the results show that our method achieved 42% recall and 49% precision for Japa-nese-English newspaper texts, and 45% recall and 76% precision for Chi-nese-Japanese technical documents.
word translation disambiguation	recall	In our word translation disambiguation experiment, the results show that our method achieved 42% recall and 49% precision for Japa-nese-English newspaper texts, and 45% recall and 76% precision for Chi-nese-Japanese technical documents.
word translation disambiguation	precision	In our word translation disambiguation experiment, the results show that our method achieved 42% recall and 49% precision for Japa-nese-English newspaper texts, and 45% recall and 76% precision for Chi-nese-Japanese technical documents.
segmentation	accuracy	Preliminary experimental results show percentage of segmentation accuracy.
SMT-MS	precision	We use the two measures precision and recall on discovered word boundaries to evaluate the effectiveness of SMT-MS and SMT 2 , where precision is the proportion of correctly discovered boundaries among all discovered boundaries by the algorithm, and recall is the proportion of correctly discovered boundaries among all correct boundaries.
SMT-MS	recall	We use the two measures precision and recall on discovered word boundaries to evaluate the effectiveness of SMT-MS and SMT 2 , where precision is the proportion of correctly discovered boundaries among all discovered boundaries by the algorithm, and recall is the proportion of correctly discovered boundaries among all correct boundaries.
SMT	precision	We use the two measures precision and recall on discovered word boundaries to evaluate the effectiveness of SMT-MS and SMT 2 , where precision is the proportion of correctly discovered boundaries among all discovered boundaries by the algorithm, and recall is the proportion of correctly discovered boundaries among all correct boundaries.
SMT	recall	We use the two measures precision and recall on discovered word boundaries to evaluate the effectiveness of SMT-MS and SMT 2 , where precision is the proportion of correctly discovered boundaries among all discovered boundaries by the algorithm, and recall is the proportion of correctly discovered boundaries among all correct boundaries.
SMT-based	Rand F	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT-based	precision	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT-based	recall	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT-based	F-measure	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT	Rand F	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT	precision	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT	recall	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT	F-measure	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT-MS	Rand F	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT-MS	precision	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT-MS	recall	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT-MS	F-measure	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT	Rand F	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT	precision	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT	recall	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
SMT	F-measure	Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.
hypernymy extraction	Precision	However the precision obtained by our system is quite competitive to other approaches for hypernymy extraction like the one of Erik Tjong and Kim Sang which extracts hypernyms in) (Precision: 0.48).
WPC extraction	precision	Experiments show that the accuracy of WPC extraction is 99.3% precision and 98.4% recall , while that of WPI extraction is 98.2% and 98.6%, respectively.
WPC extraction	recall	Experiments show that the accuracy of WPC extraction is 99.3% precision and 98.4% recall , while that of WPI extraction is 98.2% and 98.6%, respectively.
WPI extraction	accuracy	To evaluate WPI extraction accuracy, we used Wikipedia articles not listed on the Wikipedia categories used for training.
WPCs extraction	accuracy	shows the WPCs extraction accuracy.
clause identification	precision	The clause identification system and clause classification system demonstrated 73% and 78% precision values respectively.
clause classification	precision	The clause identification system and clause classification system demonstrated 73% and 78% precision values respectively.
Information Retrieval (IR)	recall	Stemming plays an important role in Information Retrieval (IR) systems by reducing the index size and increasing the recall by retrieving results containing any of the various possible forms of a word present in the query.
MT	BLEU	Instead we carryout extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (), METEOR (Banerjee and), NIST), WER, PER and TER ().
MT	METEOR	Instead we carryout extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (), METEOR (Banerjee and), NIST), WER, PER and TER ().
MT	WER	Instead we carryout extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (), METEOR (Banerjee and), NIST), WER, PER and TER ().
MT	PER	Instead we carryout extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (), METEOR (Banerjee and), NIST), WER, PER and TER ().
MT	TER	Instead we carryout extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (), METEOR (Banerjee and), NIST), WER, PER and TER ().
MT evaluation	BLEU	Instead we carryout extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (), METEOR (Banerjee and), NIST), WER, PER and TER ().
MT evaluation	METEOR	Instead we carryout extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (), METEOR (Banerjee and), NIST), WER, PER and TER ().
MT evaluation	WER	Instead we carryout extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (), METEOR (Banerjee and), NIST), WER, PER and TER ().
MT evaluation	PER	Instead we carryout extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (), METEOR (Banerjee and), NIST), WER, PER and TER ().
MT evaluation	TER	Instead we carryout extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (), METEOR (Banerjee and), NIST), WER, PER and TER ().
NE alignment (NEA)	BLEU	By contrast, automatic NE alignment (NEA) gives a huge impetus to system performance, the best of them (4.59 BLEU points absolute, 52.5% relative improvement) being the alignment of NEs of any length that produces the best scores across all metrics.
word alignment	precision	We will also explore whether discriminative approaches to word alignment can be employed to improve the precision of the NE alignment.
segmentation	accuracy	reported that segmentation methods achieving segmentation accuracy higher than 90% according to a manual segmentation standard yield no improvement in IR performance.
IR	mean average precision (MAP)	The performance of IR systems was measured by mean average precision (MAP) of the query set.
IR	Precision	Standard IR metrics like Precision, Recall and F-score are used to evaluate the system.
IR	Recall	Standard IR metrics like Precision, Recall and F-score are used to evaluate the system.
IR	F-score	Standard IR metrics like Precision, Recall and F-score are used to evaluate the system.
translation	accuracy	First, we measure the translation accuracy using LCS decoding.
extraction	accuracy	Proper handling of this extra information is likely to boost the extraction accuracy.
data acquisition	accuracy	This approach based on comparable corpora considerably relieves the data acquisition bottleneck, but has the disadvantage that the results tend to lack accuracy in practice.
BMWE extraction	MAP estimation	Section 4 describes our own algorithm based on the combination of BMWE extraction and the modified word alignment which incorporates the groupings of BMWEs and enforces their alignment links; we explain the EM algorithm with MAP estimation Although the code maybe similar in practice to our Prior Model I, his explanation to modify the E-step will not be applied to IBM Models 3 and 4.
NEs translation	MAP	This mechanism achieved 95% accuracy in NEs translation and 0.3756 MAP in English-Chinese cross-lingual information retrieval of QA.
Cross-lingual information retrieval (CLIR)	accuracy	Cross-lingual information retrieval (CLIR) plays a very important role in this process because the relevancy of retrieved documents (or passages) affects the accuracy of the answers.
machine translation (MT)	accuracy	Currently, machine translation (MT) can achieve very high accuracy when translating general text.
translation	accuracy	The evaluation of VMNET performance covers two main aspects: translation accuracy and CLIR performance.
CRF segmenter	F-score	Our CRF segmenter achieves an F-score of 83.34% and can be applied on a variety of data from different eras.
sentence segmentation	precision P	2 http://hanji.sinica.edu.tw  For Classical Chinese sentence segmentation, we define the precision P as the ratio of the boundaries of clauses which are correctly segmented to all segmented boundaries, the recall R as the ratio of correctly segmented boundaries to all reference boundaries, and the score F as the harmonic mean of precision and recall:
sentence segmentation	recall R	2 http://hanji.sinica.edu.tw  For Classical Chinese sentence segmentation, we define the precision P as the ratio of the boundaries of clauses which are correctly segmented to all segmented boundaries, the recall R as the ratio of correctly segmented boundaries to all reference boundaries, and the score F as the harmonic mean of precision and recall:
sentence segmentation	precision	2 http://hanji.sinica.edu.tw  For Classical Chinese sentence segmentation, we define the precision P as the ratio of the boundaries of clauses which are correctly segmented to all segmented boundaries, the recall R as the ratio of correctly segmented boundaries to all reference boundaries, and the score F as the harmonic mean of precision and recall:
sentence segmentation	recall	2 http://hanji.sinica.edu.tw  For Classical Chinese sentence segmentation, we define the precision P as the ratio of the boundaries of clauses which are correctly segmented to all segmented boundaries, the recall R as the ratio of correctly segmented boundaries to all reference boundaries, and the score F as the harmonic mean of precision and recall:
error detection and correction	precision mode (PM)	Since the correction is more important in an error detection and correction system, we define the corresponding f-score as: shows the initial results of the template module (TM), the translation module (LMM) and the combined results of the precision mode (PM) and detection mode (DM).
headdriven parsing	precision	For example, it enhances shallow parsing ( ) and headdriven parsing, and also improves the precision of sentence similarity computation.
association analysis	likelihood ratio test	We also propose an association analysis method based on likelihood ratio test to infer the polarity of opinion word.
relation detection	NULL	We first evaluate relation detection, where only two output classes are concerned, i.e. NULL (which means no relation recognized) and RELATION.
relation detection	RELATION	We first evaluate relation detection, where only two output classes are concerned, i.e. NULL (which means no relation recognized) and RELATION.
sentiment classification	accuracy	However, as will be discussed in Section 5.2 that the increasing number of the matched polarity words does not necessarily lead to the improvement of the sentiment classification accuracy.
segmentation	accuracy	Of course, segmentation accuracy does not imply parsing accuracy, though wrong segmentation necessarily implies a wrong parse.
segmentation	accuracy	Of course, segmentation accuracy does not imply parsing accuracy, though wrong segmentation necessarily implies a wrong parse.
parsing	accuracy	Of course, segmentation accuracy does not imply parsing accuracy, though wrong segmentation necessarily implies a wrong parse.
sentiment analysis	SP	In general, most current studies concerning sentiment analysis are about determining the SP of words, phrases or sentences.
opinioned elements identification	Fmeasure	This paper experimentally demonstrates the validity of active learning algorithm when used for opinioned elements identification and proposes a computational method for overall system performance evaluation which consists of Fmeasure, training time, and number of training instances.
CWS tagging	accuracy	This paper proposes anew approach to improve the CWS tagging accuracy by combining Self-Organizing Map (SOM) with structured support vector machine (SVM) for utilization of enormous unlabeled text corpus.
CWS tagging	accuracy	This paper proposes anew approach to improve the CWS tagging accuracy by structured support vector machine (SVM) utilization of unlabeled text corpus.
segmentation	accuracy	The resulting segmentation accuracy with unsupervised methods may not be very satisfying, but the human effort for creating the training data set is not absolutely required.
word segmentation	recall (R)	The performance of word segmentation is measured by test precision (P), test recall (R), F score (which is defined as 2PR/(P+R)) and the OOV recall rate.
word segmentation	F score	The performance of word segmentation is measured by test precision (P), test recall (R), F score (which is defined as 2PR/(P+R)) and the OOV recall rate.
word segmentation	OOV recall rate	The performance of word segmentation is measured by test precision (P), test recall (R), F score (which is defined as 2PR/(P+R)) and the OOV recall rate.
word segmentation competition	OOV	Subword list is augmented with newword list recognized by accessor variety method. whether punctuation e) T (C−1)T (C0)T (C+1) type of characters We participated in the close track of the word segmentation competition, on all the four test datasets, in two of which our system is ranked at the 1st position with respect to the metric of OOV recall.
BI" tagging	F1 measure score	The experiment result shows that TCB does improve "BI" tagging domain-independently about 1% of the F1 measure score.
Constituent parsing	precision	2) Constituent parsing evaluation We selected three commonly-used metrics to evaluation the performance of constituent parsing: labeled precision, recall, and F1-score.
Constituent parsing	recall	2) Constituent parsing evaluation We selected three commonly-used metrics to evaluation the performance of constituent parsing: labeled precision, recall, and F1-score.
Constituent parsing	F1-score	2) Constituent parsing evaluation We selected three commonly-used metrics to evaluation the performance of constituent parsing: labeled precision, recall, and F1-score.
Event recognition evaluation	recall	3) Event recognition evaluation We only considered the recognition recall of each event construction annotated in the event bank, due to the current parsing status of Task 2-1 output.
parsing	speed	However, parsing is a recognized research problem, and it is so difficult to meet the urgent needs of industrial applications inaccuracy, robustness, speed.
POS tagging	pos accuracy  80.40  94.82	 Table 2. Different POS tagging results  original  new  pos accuracy  80.40  94.82
parsing	Iter	Because mult-time iterations can't improve parsing performance tremendously but cost much time during our experiments, we take Iter=1 here.
CWSI task	FScore	The official performance metric for the CWSI task is FScore ().
NER task	CEAF	To measure accuracy in the NER task, we applied three commonly used performance measures for coreference resolution: MUC-6 (, CEAF, and B-CUBED (Bagga and Baldwin, 1998).
NER task	B-CUBED	To measure accuracy in the NER task, we applied three commonly used performance measures for coreference resolution: MUC-6 (, CEAF, and B-CUBED (Bagga and Baldwin, 1998).
coreference resolution	CEAF	To measure accuracy in the NER task, we applied three commonly used performance measures for coreference resolution: MUC-6 (, CEAF, and B-CUBED (Bagga and Baldwin, 1998).
coreference resolution	B-CUBED	To measure accuracy in the NER task, we applied three commonly used performance measures for coreference resolution: MUC-6 (, CEAF, and B-CUBED (Bagga and Baldwin, 1998).
ANOVAs	REG08-Type Recall	We carried out univariate ANOVAs with System as the fixed factor, and REG08-Type Recall as the dependent variable in one ANOVA, and REG08-Type Precision in the other.
ANOVAs	Recall	We performed univariate ANOVAs with System as the fixed factor, and Recall as the dependent variable in one, and Precision in the other.
ANOVAs	Precision	We performed univariate ANOVAs with System as the fixed factor, and Recall as the dependent variable in one, and Precision in the other.
coreference resolution	accuracy	The coreference resolution system with the SM2 tagger performs better, because a better coreference model is achieved from system mentions with higher accuracy.
RST-DT	F-score	Specifically, with 200 training instances, for RST-DT, the baseline method has a macro-averaged F-score of 0.079, whereas the the proposed method has a macro-averaged F-score of 0.159 (around 101% increase in F-score).
RST-DT	F-score	Specifically, with 200 training instances, for RST-DT, the baseline method has a macro-averaged F-score of 0.079, whereas the the proposed method has a macro-averaged F-score of 0.159 (around 101% increase in F-score).
RST-DT	F-score	Specifically, with 200 training instances, for RST-DT, the baseline method has a macro-averaged F-score of 0.079, whereas the the proposed method has a macro-averaged F-score of 0.159 (around 101% increase in F-score).
SVM classifier	LIBLIN-EAR 2	We train a linear SVM classifier (LIBLIN-EAR 2 ) for each task.
interpretation	precision	The results show that the interpretation side of RDT achieves a fair precision in identification (75.2%) but a low recall (44.7%).
interpretation	recall	The results show that the interpretation side of RDT achieves a fair precision in identification (75.2%) but a low recall (44.7%).
Classification of non-understandings	Inter-annotator agreement (κ)	 Table 3. Classification of non-understandings.  Inter-annotator agreement (κ) is substantial  over all classes.
ASR	accuracy	In our earlier report, we defined the estimated ASR accuracy and showed that it is helpful in improving the accuracy of classifying barge-in utterances into correctly and erroneously interpreted ones, by using it in conjunction with the user's barge-in rate.
ASR	accuracy	In our earlier report, we defined the estimated ASR accuracy and showed that it is helpful in improving the accuracy of classifying barge-in utterances into correctly and erroneously interpreted ones, by using it in conjunction with the user's barge-in rate.
classification	accuracy	Under Condition (6), the classification accuracy does not depend on the window width because the barge-in rate is not used.
classification	accuracy	By comparing Conditions (2) and (4), we can see that the classification accuracy improves after adding the estimated ASR accuracy to Condition (4).
ASR	accuracy	This shows that the estimated ASR accuracy also contributes to improving the classification accuracy.
ASR	accuracy	This shows that the estimated ASR accuracy also contributes to improving the classification accuracy.
classification	accuracy	This shows that the estimated ASR accuracy also contributes to improving the classification accuracy.
classification	accuracy	This suggests that the classification accuracy, whose upper limit is Condition (1), can be improved by making the ASR accuracy estimation shown in Section 3.2 more accurate.
classification	Condition	This suggests that the classification accuracy, whose upper limit is Condition (1), can be improved by making the ASR accuracy estimation shown in Section 3.2 more accurate.
classification	accuracy estimation	This suggests that the classification accuracy, whose upper limit is Condition (1), can be improved by making the ASR accuracy estimation shown in Section 3.2 more accurate.
ASR	accuracy estimation	This suggests that the classification accuracy, whose upper limit is Condition (1), can be improved by making the ASR accuracy estimation shown in Section 3.2 more accurate.
ASR	accuracy	 Table 1: ASR accuracy per barge-in
ASR	accuracy	 Table 2: ASR accuracy of barge-in utterances for  different barge-in rates
ASR	accuracy	 Table 4: ASR accuracy by user response type
frame matching	recall	In frame matching, the number of frame labels in the gold standard annotation that can also be found in the system annotation (recall) was counted.
RTE challenges	accuracy	The evaluation measure adopted in the RTE challenges is accuracy, i.e. the percentage of pairs correctly judged by a TE system.
rhetorical relations	accuracy	For rhetorical relations, Na¨ıveNa¨ıve Bayesian models achieve 84.90% and 57.87% accuracy in classifying NARRATION and BACKGROUND / ELABORATION relations respectively (16% and 23% above baseline).
Recognizing Textual Entailment (RTE) task	ENTAILMENT	In the Recognizing Textual Entailment (RTE) task, sentence pairs are classified into one of three semantic relations: ENTAILMENT, CONTRADICTION or UNKNOWN.
Recognizing Textual Entailment (RTE) task	CONTRADICTION	In the Recognizing Textual Entailment (RTE) task, sentence pairs are classified into one of three semantic relations: ENTAILMENT, CONTRADICTION or UNKNOWN.
frame matching	recall	In frame matching, the number of frame labels in the gold standard annotation that can also be found in the system annotation (recall) was counted.
parsing	accuracy	Since semantic information is used in syntactic disambiguation, we would expect practical improvements in parsing accuracy by accounting for the interactive interpretation process.
parsing	accuracyit	This pessimistically isolates the contribution of semantics on parsing accuracyit will only show parsing gains where semantic information does not overlap with distributional syntactic information.
RTE challenges	accuracy	The evaluation measure adopted in the RTE challenges is accuracy, i.e. the percentage of pairs correctly judged by a TE system.
MER	BIO-CRF	We presented three different methods for MER: MM+, TT-SVM, and BIO-CRF (with variant BIO-CRF-H).
Classification	accuracy	Classification accuracy varies between species, locations, and datasets.
relation extraction (RE) tasks	accuracy	Kernel methods are considered the most effective techniques for various relation extraction (RE) tasks as they provide higher accuracy than other approaches.
segmentation	accuracy	They evaluate segmentation accuracy on a per token basis, using recall, precision and F1-score computed on segmentation points.
segmentation	recall	They evaluate segmentation accuracy on a per token basis, using recall, precision and F1-score computed on segmentation points.
segmentation	precision	They evaluate segmentation accuracy on a per token basis, using recall, precision and F1-score computed on segmentation points.
segmentation	F1-score	They evaluate segmentation accuracy on a per token basis, using recall, precision and F1-score computed on segmentation points.
parsing	accuracy	This design helps in comparing results in away that enables us to measure the effect of automatic pre-processing on parsing accuracy.
dependency parsing	labeled attachment score (LAS)	The official evaluation metric in the CoNLL 2007 shared task on dependency parsing was the labeled attachment score (LAS), i.e., the percentage of tokens for which a system has predicted the correct HEAD and DEPREL, but results reported also included unlabeled attachment score (UAS), i.e., the percentage of tokens with correct HEAD, and the label accuracy (LA), i.e., the percentage of tokens with correct DEPREL.
dependency parsing	DEPREL	The official evaluation metric in the CoNLL 2007 shared task on dependency parsing was the labeled attachment score (LAS), i.e., the percentage of tokens for which a system has predicted the correct HEAD and DEPREL, but results reported also included unlabeled attachment score (UAS), i.e., the percentage of tokens with correct HEAD, and the label accuracy (LA), i.e., the percentage of tokens with correct DEPREL.
dependency parsing	label accuracy (LA)	The official evaluation metric in the CoNLL 2007 shared task on dependency parsing was the labeled attachment score (LAS), i.e., the percentage of tokens for which a system has predicted the correct HEAD and DEPREL, but results reported also included unlabeled attachment score (UAS), i.e., the percentage of tokens with correct HEAD, and the label accuracy (LA), i.e., the percentage of tokens with correct DEPREL.
CSC learning	C	In all our experiments, for CSC learning with PA, the C parameter is set by tuning on 10% of the training data and the number of rounds is fixed to 10.
syntactic parsing	F-score	For example, heuristically correct the syntactic parsing used and report that this improved their performance by four F-score points.
Urdu sentiment analysis	F-Score	To this end the best performance obtained for opinion entity detection for Urdu sentiment analysis is 58.06% F-Score using sequence kernels and 61.55% F-Score using a combination of sequence and linear kernels.
Urdu sentiment analysis	F-Score	To this end the best performance obtained for opinion entity detection for Urdu sentiment analysis is 58.06% F-Score using sequence kernels and 61.55% F-Score using a combination of sequence and linear kernels.
classification	accuracy	We also compare the classification accuracy of two standard semi-supervised classification algorithms on the mutual k-NN graphs and the k-NN graphs.
Classification	accuracy	 Table 1: Classification accuracy of vertices around hubs  in a k-NN graph, before ("original") and after ("hub- removed") hubs are removed. The value d represents the  shortest distance (number of hops) from a vertex to its  nearest hub vertex in the graph.
authorship attribution	Latent Dirichlet Allocation (LDA)	Our approach to authorship attribution consists of building models of authors and their texts using Latent Dirichlet Allocation (LDA) (.
domain transfer	accuracy	For domain transfer, we show that self-training gains an absolute improvement in labeling accuracy for blog data of 16% over the supervised approach with target domain training data.
classification	accuracy	It is evaluated in terms of classification accuracy.
SSL	accuracy	Overall, our results suggest that SSL improves accuracy for opinion detection although the contribution of SSL varies across data domains and different strategies need to be applied to achieve optimized performance.
machine translation from images to text	BLEU	Because our task can be viewed as machine translation from images to text, BLEU () may seem This limitation does not apply to TEMPLATE.
text chunking	accuracy	 Table 2: Performance of text chunking (training times and accuracy scores on test data).
WM	recall	To manipulate WM load, participants were asked to perform a combined task: memorizing a sequence of digits for later recall and performing a moving-window task.
summarization	coverage	For the different sources, we notice using linked web pages alone yields worse summarization performance, as well as lower reference summary coverage; however, when combined with the tweets, there is a slight increase in the coverage scores, and sometimes improved summarization results.
parsing	accuracy	Thus far, results show no improvement in parsing accuracy over the best base-line score; we identify possible problems and outline suggestions for future directions.
MWE detection	precisions	 Table 2: All the relevant numbers for the study. For purposes of comparison we recalculated the token counts for the  gold-annotated portion of the XWN corpus, and found discrepancies with Arranz's reported values. They reported  1300 fully-gold-annotated glosses containing 397 MWEs; we found 1307 glosses containing 382 MWEs. The table  contains our token counts, but Arranz's actual MWE detection and WSD f-measures, precisions, and recalls.
MWE detection	recalls	 Table 2: All the relevant numbers for the study. For purposes of comparison we recalculated the token counts for the  gold-annotated portion of the XWN corpus, and found discrepancies with Arranz's reported values. They reported  1300 fully-gold-annotated glosses containing 397 MWEs; we found 1307 glosses containing 382 MWEs. The table  contains our token counts, but Arranz's actual MWE detection and WSD f-measures, precisions, and recalls.
extrinsic evaluation	PERSON	There has been little work on any form of extrinsic evaluation, and how one tagger compares with another on the major classes: PERSON , ORGANIZATION, and LOCATION.
extrinsic evaluation	ORGANIZATION	There has been little work on any form of extrinsic evaluation, and how one tagger compares with another on the major classes: PERSON , ORGANIZATION, and LOCATION.
extrinsic evaluation	LOCATION	There has been little work on any form of extrinsic evaluation, and how one tagger compares with another on the major classes: PERSON , ORGANIZATION, and LOCATION.
IR evaluation	recall	IR evaluation is based on recall and precision.
IR evaluation	precision	IR evaluation is based on recall and precision.
labeling	accuracy	The clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks.
dependency parsing	accuracy	Furthermore, from our experience in dependency parsing, handling these steps together improves accuracy in identification as well as classification (unlabeled and labeled attachment scores in dependency parsing).
labeling	accuracy	Our experiments show that this technique improves labeling accuracy for both in-domain and out-of-domain tasks.
semantic relation classification	recall	However, Cluster ID features are found to be helpful to the overall goal of semantic relation classification, because they increase recall by much more (4.44%) than they decrease precision (-0.44%).
semantic relation classification	precision	However, Cluster ID features are found to be helpful to the overall goal of semantic relation classification, because they increase recall by much more (4.44%) than they decrease precision (-0.44%).
MT	BLEU	For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER ().
MT	METEOR	For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER ().
MT	PER	For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER ().
MT	WER	For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER ().
machine translation evaluation	BLEU	For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER ().
machine translation evaluation	METEOR	For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER ().
machine translation evaluation	PER	For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER ().
machine translation evaluation	WER	For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER ().
meaning preservation across translation	accuracy	A blueprint for more direct assessment of meaning preservation across translation was outlined by, in which translation utility is manually evaluated with respect to the accuracy of semantic role labels.
MT	accuracy	The human reader was instructed to order the sentences from the three MT systems according to the accuracy of meaning in the translations.
SRL	accuracy	While a comprehensive semantic mapping tool can supplement or improve the results of such techniques, there are many other exciting ideas we can explore: with automatic SRL, we can improve coverage (and possibly accuracy) of Chinese semantic class generation () by running the system on a large, unannotated parallel corpus.
SRL	arg  match	 Table 2: SRL results on triple-gold Xinhua corpus. "arg  match" is the standard CoNLL 2005 evaluation metric,  "oracle" is the oracle SRL based on automatic parser out- put, and "word match" is scoring based on length of ar- gument overlap with the reference
Predicate-argument mapping coverage	Predi- cate coverage	 Table 4: Predicate-argument mapping coverage. Predi- cate coverage denotes the number of mapped predicates  over all predicates in the corpus, word coverage denotes  the number of words in the mapped predicate-arguments  over all words in the corpus
SMT	depth factor (DF)	Syntax-based SMT overcomes the limitations of PBSMT because it finds discontinuous patterns along with the hier-: The maximum branching factor (BF) and depth factor (DF) in a dependency tree in our corpus archical structure.
SMT evaluation	BLEU	Standard SMT evaluation metrics such as BLEU) or edit-distance metrics (e.g. Word Error Rate) measure the global overlap of the translation with a reference, and are thus not very sensitive to WSD errors.
MT	BLEU	MT performance is usually measured by such metric as BLEU which measures the MT output as a whole including word choice and reordering.
MT	ME	 Table 3: MT results on MT08 Newswire set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;  Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).
MT	Prior:smoothed distortion prior	 Table 3: MT results on MT08 Newswire set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;  Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).
MT	ME	 Table 4: MT results on MT08 Weblog set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;  Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).
MT	Prior:smoothed distortion prior	 Table 4: MT results on MT08 Weblog set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;  Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).
ontology translation	BLEU	As such, for the taxonomy and ontology translation task we do not recommend using BLEU or NIST as an evaluation metric.
translation	accuracy	Our experiments demonstrate that our proposed method constantly improves translation accuracy.
translation	accuracy	The likelihood function fora translation matrix is defined by considering the expected achieved translation accuracy.
translation	accuracy	Our experiments show that the translation accuracy is constantly improved by the proposed method.
translation	accuracy	We can see that the overall translation accuracy varies across the test sets.
translation	accuracy	Finally, the value of this approach is demonstrated by the improvement of translation accuracy for medical words.
Sentence selection	Precision	 Table 1: Sentence selection results in terms of Precision, Recall and F 1 Score.
Sentence selection	Recall	 Table 1: Sentence selection results in terms of Precision, Recall and F 1 Score.
Sentence selection	F 1 Score	 Table 1: Sentence selection results in terms of Precision, Recall and F 1 Score.
Data selection	Dice coefficient	 Table 2: Data selection results in terms of Dice coefficient. Results shown for data selection methods preceded by  different sentence selection methods.
Automatic Speech Recognition (ASR)	accuracy	For example, most research that used Automatic Speech Recognition (ASR) technology to automatically score speaking proficiency () focused on wordlevel cues for fluency and accuracy.
CB detection	accuracy	For CB detection, this type of baseline model resulted in an accuracy of 91.6%; for IP detection, this type of baseline model resulted in an accuracy of 96.7%.
CB detection	accuracy	For CB detection, this type of baseline model resulted in an accuracy of 91.6%; for IP detection, this type of baseline model resulted in an accuracy of 96.7%.
IP detection	accuracy	For CB detection, this type of baseline model resulted in an accuracy of 91.6%; for IP detection, this type of baseline model resulted in an accuracy of 96.7%.
PRLM	accuracy	The model based on all PRLM features did not achieve a higher accuracy than the model based on only MaxLanguage.
tagger	accuracy	Ina minority of cases (305; 3%) normalisation has a negative effect on tagger accuracy.
PoS tagging	accuracy	These studies have mostly concentrated on mapping historical variants to modern words or evaluating PoS tagging accuracy and have dealt with Germanic and Romance languages.
topic modeling	accuracy	 Table 5: Accuracy of topic modeling: In parenthesis is  the term accuracy calculated using relevant topics only.
SSA	F1 measure	Clarke and Lapata showed a moderate correlation with human judgement for SSA and a strong correlation for the F1 measure.
classification	accuracy	Throughout these experiments, we are interested in the classification accuracy.
FS	accuracy	 Table 3: Results for FS Based on Syntactic-Semantic  set difference method. Each row represents the accuracy  achieved at a particular δ value.
bioevent extraction task	F-score	The final results enabled to observe the state-ofthe-art performance of the community on the bioevent extraction task, which showed that the automatic extraction of simple events -those with unary arguments, e.g. gene expression, localization, phosphorylation -could be achieved at the performance level of 70% in F-score, but the extraction of complex events, e.g. binding and regulation, was a lot more challenging, having achieved 40% of performance level.
ID task	F-score	By contrast, BINDING and regulation events, found challenging in ST'09 and GE, remain problematic also in the ID task, with best overall performance below 50% F-score.
event trigger  detection/argument detection/argument grouping/modification detection	BI	 Table 5: Participants and summary of system descriptions. Abbreviations: Trig./Arg./Group./Modif.=event trigger  detection/argument detection/argument grouping/modification detection, BI=Bioinformatician, NLP=Natural Lan- guage Processing researcher, CS=Computer scientist, CoreNLP=Stanford CoreNLP, Porter=Porter stemmer, Snow- ball=Snowball stemmer McCCJ=McClosky-Charniak-Johnson parser, LGP=Link Grammar Parser, SD=Stanford De- pendency conversion, UMLS=UMLS resources (e.g. lexicon, metamap)
domain adaptation	F-score	In particular, domain adaptation improved the performance on full papers by 1.22 points, thus reaching 51.22 in F-score.
Infectious Diseases task	accuracy	We also show that for the Infectious Diseases task using data from the Genia track is a very effective way to improve accuracy.
Genia Task 2	FAUST	Finally, for Genia Task 2 we rank 3rd-with the 1st rank achieved by the FAUST system.
stacking	FAUST	This seems to lead to more substantial improvements for stacking: FAUST+All obtains a f-score 2.2 points larger than the standalone UMass system.
REL task	precision	The evaluation of the REL task is relation-based and uses the standard precision/recall/F 1 -score metrics.
REL task	recall/F 1 -score metrics	The evaluation of the REL task is relation-based and uses the standard precision/recall/F 1 -score metrics.
REL task	recall	 Table 3: Primary evaluation results for the REL task. Results given as recall / precision / F-score.
REL task	precision	 Table 3: Primary evaluation results for the REL task. Results given as recall / precision / F-score.
REL task	F-score	 Table 3: Primary evaluation results for the REL task. Results given as recall / precision / F-score.
parsing	accuracy	Unlike for propositions, word sense and named entities, where it is simply a matter of counting the correct answers, or for parsing, where there are several established metrics, evaluating the accuracy of coreference continues to be contentious.
coreference resolution	F1-measures	And the performance of coreference resolution in the second step is measured using the average F1-measures of MUC, B-CUBED and CEAFE metrics).
coreference resolution	B-CUBED	And the performance of coreference resolution in the second step is measured using the average F1-measures of MUC, B-CUBED and CEAFE metrics).
mention detection	precision	These changes led to considerable improvement in mention detection precision.
classification	accuracy	reports mean classification accuracy using five-fold cross-validation.
parser evaluation	BLEU	PARSEVAL (, commonly used for parser evaluation, and BLEU (), commonly used in machine translation, are two examples of well-known imperfect metrics that have been the subject of much criticism, but that are widely agreed to have been necessary for much of the progress enjoyed by their respective fields.
speech recognition errors	accuracy	Past work in lab studies has showed that this distribution improves robustness to speech recognition errors; but to our surprise, we found the distribution yielded an increase inaccuracy for only two of the four slots, and actually decreased accuracy in the other two.
ASR	accuracy	ASR re-ranking helped and hurt ASR accuracy.
ASR	accuracy	We found that re-ranking degraded ASR accuracy for all slots, except DATE where it had a trivial positive impact.
ASR	accuracy	Where ASR re-ranking decreased ASR accuracy, we'd expect to see it also decrease belief state accuracy.
ASR	accuracy	Where ASR re-ranking decreased ASR accuracy, we'd expect to see it also decrease belief state accuracy.
ASR	accuracy	Where ASR re-ranking decreased ASR accuracy, we'd expect to see it also decrease belief state accuracy.
ASR	accuracy	Where ASR re-ranking decreased ASR accuracy, we'd expect to see it also decrease belief state accuracy.
ASR	accuracy	Indeed, for the TIME slot, ASR re-ranking causes a substantial decrease in belief state accuracy, highlighting the importance of an accurate confidence score to statistical techniques.
ASR	accuracy	In the previous section, for ROUTE, it was observed that ASR re-ranking degraded ASR accuracy, yet caused an improvement in belief accuracy.
ASR	accuracy	In the previous section, for ROUTE, it was observed that ASR re-ranking degraded ASR accuracy, yet caused an improvement in belief accuracy.
ASR	accuracy	In the previous section, for ROUTE, it was observed that ASR re-ranking degraded ASR accuracy, yet caused an improvement in belief accuracy.
ASR	accuracy	In the previous section, for ROUTE, it was observed that ASR re-ranking degraded ASR accuracy, yet caused an improvement in belief accuracy.
Emotion recognition	UAR	Emotion recognition itself is error-prone and a distinction of the emotional state of the caller with the employed annotation scheme can be expected with approximately 70%-80% UAR, see e.g..
AM adaptation	WER	For each experiment, indicates the acoustic model (AM) used, the number of hours of domain-specific spontaneous speech used for AM adaptation, the number of titles used to construct the language model (LM), the type of LM, the type of grammar rules in the Phoenix book title subgrammar, and average WER as measured by Levenstein word edit distance.
translation out of English	corre- lation	 Table 15: Segment-level Kendall's tau correlation of the  automatic evaluation metrics with the human judgments  for translation out of English, ordered by average corre- lation.
machine translation evaluation	AMBER	This paper proposes anew automatic machine translation evaluation metric: AMBER, which is based on the metric BLEU but incorporates recall, extra penalties , and some text processing variants.
machine translation evaluation	BLEU	This paper proposes anew automatic machine translation evaluation metric: AMBER, which is based on the metric BLEU but incorporates recall, extra penalties , and some text processing variants.
machine translation evaluation	recall	This paper proposes anew automatic machine translation evaluation metric: AMBER, which is based on the metric BLEU but incorporates recall, extra penalties , and some text processing variants.
MT	TESLA	Section 5 describes MT tuning with TESLA.
MT	aversion	To compare Meteor 1.3 against previous versions of the metric on the task of evaluating MT system outputs, we tune aversion for each language on 2009 WMT data and evaluate on 2010 data.
machine translation output	F score	We propose the use of morphemes for automatic evaluation of machine translation output , and systematically investigate a set of F score and BLEU score based metrics calculated on words, morphemes and POS tags along with all corresponding combinations.
machine translation output	BLEU score based metrics	We propose the use of morphemes for automatic evaluation of machine translation output , and systematically investigate a set of F score and BLEU score based metrics calculated on words, morphemes and POS tags along with all corresponding combinations.
MT evaluation	BLEU	An early MT evaluation metric, BLEU), is still the most commonly used metric in automatic machine translation evaluation.
gradient ascent optimization	BLEU	The gradient ascent optimization of the xBLEU appears to be more stable than the gradient-free direct 1-best BLEU tuning or N -best list based minimum error rate training, especially when tuning a large number of weights.
translation	BLEU-4	We evaluated translation quality using case-sensitive BLEU-4 (Papineni ) with a single reference.
machine translation	BLEU	Section 5 presents the results of a set of machine translation experiments using the automatic metrics BLEU () and METEOR, and a manual-evaluation of subject integrity.
machine translation	METEOR	Section 5 presents the results of a set of machine translation experiments using the automatic metrics BLEU () and METEOR, and a manual-evaluation of subject integrity.
SMT phrase	BLEU	 Table 1.  Augmenting SMT phrase tables with paraphrases of  OOV unigrams resulted in gains of 0.6-0.7 BLEU  points for both subset and full models, but TER  scores were worse (higher) for the full model. Aug- menting same models with same paraphrases filtered  for antonyms resulted in further gains of 1.6 and 1  BLEU points for both subset and full models, respec- tively, relative to the respective baselines. The TER  scores of the antonym filtered models were also as  good or better (lower) than those of the baselines.
SMT phrase	TER	 Table 1.  Augmenting SMT phrase tables with paraphrases of  OOV unigrams resulted in gains of 0.6-0.7 BLEU  points for both subset and full models, but TER  scores were worse (higher) for the full model. Aug- menting same models with same paraphrases filtered  for antonyms resulted in further gains of 1.6 and 1  BLEU points for both subset and full models, respec- tively, relative to the respective baselines. The TER  scores of the antonym filtered models were also as  good or better (lower) than those of the baselines.
SMT phrase	BLEU	 Table 1.  Augmenting SMT phrase tables with paraphrases of  OOV unigrams resulted in gains of 0.6-0.7 BLEU  points for both subset and full models, but TER  scores were worse (higher) for the full model. Aug- menting same models with same paraphrases filtered  for antonyms resulted in further gains of 1.6 and 1  BLEU points for both subset and full models, respec- tively, relative to the respective baselines. The TER  scores of the antonym filtered models were also as  good or better (lower) than those of the baselines.
SMT phrase	TER	 Table 1.  Augmenting SMT phrase tables with paraphrases of  OOV unigrams resulted in gains of 0.6-0.7 BLEU  points for both subset and full models, but TER  scores were worse (higher) for the full model. Aug- menting same models with same paraphrases filtered  for antonyms resulted in further gains of 1.6 and 1  BLEU points for both subset and full models, respec- tively, relative to the respective baselines. The TER  scores of the antonym filtered models were also as  good or better (lower) than those of the baselines.
MT tuning	BLEU	It is unlikely, however, that the difficulties in discriminative MT tuning are due solely to the use of BLEU as a metricbecause evaluation of translation is so difficult, any reasonable gain function is likely to have a complex relationship with the model parameters.
MT	BLEU	Margin-based techniques such as perceptron training () and MIRA ( have also been shown to be able to tune MT systems and scale to large numbers of features, but these generally involve repeatedly decoding the tuning set (and so are expensive) and require sentence-level approximations to the BLEU objective.
Translation	BLEU score	 Table 2: Translation results in terms of BLEU score  and translation edit rate (TER) estimated on newstest2010  with the NIST scoring script.
Translation	translation edit rate (TER)	 Table 2: Translation results in terms of BLEU score  and translation edit rate (TER) estimated on newstest2010  with the NIST scoring script.
Translation	rescoring	 Table 3: Translation results from English to French and  English to German measured on newstest2010 using a  100-best rescoring with SOUL LMs of different orders.
translation	F 1 measure	We present translation results using our training instance selection methods, translation results using graph decoding, system combination results with RegMT, and performance evaluation with the F 1 measure over target features as a metric for evaluating translation quality.
translation	F 1 measure	We present translation results using our training instance selection methods, translation results using graph decoding, system combination results with RegMT, and performance evaluation with the F 1 measure over target features as a metric for evaluating translation quality.
translation	BLEU score	All these techniques contributed to improving translation quality as measured by BLEU score over our baseline system.
translation	OOV rates	Expanded training data selection significantly increased translation scores and lowered OOV rates, while results on grammar filtering were mixed.
translation	BLEU	In terms of translation performance there seems to be a strong correlation between rule table size and translation quality as measured by BLEU.
Translation	BLEU	 Table 2: Translation results in BLEU for different corpora
Translation	BLEU	 Table 4: Translation results in BLEU with/without  spelling correction
Translation	BLEU	 Table 5: Translation results in BLEU with/without corpus  expansion
Translation	BLEU	 Table 7: Translation results in BLEU with/without ex- tracted data
parsing	accuracy	We also study the relationship between parsing accuracy in terms of un-labeled attachment score and machine translation quality in terms of BLEU.
parsing	BLEU	We also study the relationship between parsing accuracy in terms of un-labeled attachment score and machine translation quality in terms of BLEU.
parsing	accuracy	In both cases we document the relation between parsing accuracy (in terms of Unlabeled Attachment Score, UAS) and translation quality (estimated by the well known BLEU metric).
parsing	Unlabeled Attachment Score	In both cases we document the relation between parsing accuracy (in terms of Unlabeled Attachment Score, UAS) and translation quality (estimated by the well known BLEU metric).
parsing	BLEU	In both cases we document the relation between parsing accuracy (in terms of Unlabeled Attachment Score, UAS) and translation quality (estimated by the well known BLEU metric).
MT	BLEU	Section 4 summarizes the influence of the selected parsers on the MT quality in terms of BLEU.
translation	accuracy	Although some of the language pairs in this work clearly benefit from the factor augmentation , there is no consistent improvement in translation accuracy across the board.
MT	BLEU	Our hybrid system outperforms, in terms of BLEU, GTM and METEOR, a standard phrase-based statistical MT system trained on the same corpus, and received the second best BLEU score in the automatic evaluation.
translation from English to German	BLEU score	For the translation from English to German, the translation result of last year's submission was evaluated as 14.42% in the BLEU score, as shown in Table 4.
translation	BLEU	Comparing to last year's translation output, the improvement is over one percent absolutely (from 14.42% to 15.55%) in the BLEU score on the test10.
Translation	BLEU	 Table 1:  Translation performance BLEU[%] on  phrase/syntax-based system using various settings eval- uated on test10.
Translation	BLEU	 Table 2: Translation performance BLEU[%] on test2011  using hybrid system tuned on test10 with various settings  (DE-EN).
Translation	DE-EN	 Table 2: Translation performance BLEU[%] on test2011  using hybrid system tuned on test10 with various settings  (DE-EN).
Translation	BLEU	 Table 3: Translation performance BLEU[%] on three test  sets using different translation systems in 2011 submis- sion (DE-EN).
Translation	BLEU	 Table 4: Translation performance BLEU[%] on two test  sets using different translation systems in 2011 submis- sion (EN-DE).
rule extraction	BLEU score	Our objective in this paper is to provide a principled rule extraction method using a Bayesian framework that can extract the minimal SCFG rules without reducing the BLEU score.
name mining	accuracy	A side effect of the name mining algorithm is the unsuper-vised creation of a translation lexicon between the two languages, with an accuracy of 64%.
WSI evaluation	accuracy	Current WSI evaluation techniques have yet to analyze the specific impact of similarity on accuracy.
Classification	accuracy	 Table 4: Classification accuracy for the two data sets
paraphrase recognition task	Pearson correlation	We propose a metric which gives weight to lexical variety in paraphrase patterns; our proposed metric has a positive correlation with paraphrase recognition task performance, with a Pearson correlation of 0.5~0.7 (k=10, with "strict" judgment) in a statistically significant level (p-value<0.01).
machine translation	BLEU	In machine translation, n-gram-based evaluation measures like BLEU have been criticized exactly because they cannot cope sufficiently well with paraphrases), which play a central role in abstractive sentence compression ().
recruiting	safety	source: Constraints on recruiting are constraints on safety and have to be removed.
prediction	accuracy	The experiments also show that the improvements in prediction accuracy apply to cases in which the presence of a that-complementizer arguably makes a substantial difference to fluency or intelli-giblity.
surface realization	accuracy	In this paper, by contrast, we show that in the context of surface realization, using linguistically motivated features for English that-complementizer choice can improve upon the prediction accuracy of a state-ofthe-art realization ranking model, arguably in ways that make a substantial difference to fluency and intelligiblity.
parsing	accuracy	Across these experiments, the most crucial factor for parsing accuracy seems to be splitting/merging/smoothing.
rule extraction	accuracy	Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases.
generic parsing	accuracy	Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases.
rule learning	RE	In the baseline experiment, we keep the first n readings of all sentences and run DARE for rule learning and RE on top of these readings.
machine translation	latency	In this study, we are also interested in applications for which response time is of interest (e.g., realtime speech recognition and machine translation), and thus consider latency, the time to parse a single sentence, as a primary objective.
parsing	accuracy	With an unlexicalized prob-abilistic context-free grammar obtained by Maximum Likelihood Estimate on a large-scale automatically annotated corpus, we are able to achieve parsing accuracy higher than the original HPSG-based model.
parsing	accuracy	While HPSG framework is great for linguistic description, we show that when carefully designed, a much simpler approximating probabilistic context-free grammar can be extracted automatically, and is capable of achieving good parsing accuracy while maintaining high robustness and efficiency.
parsing	accuracy	The last row shows the parsing accuracy with predicted Part-ofSpeech.
Dependency parser	accuracy	 Table 1: Dependency parser accuracy. 1 Gold Part-of- Speech tags; 2 Predicted Part-of-Speech tags.
Information Retrieval	recall	Stemming plays a vital role in Information Retrieval systems by reducing the index size and increasing the recall by retrieving results that contain any of the possible forms of a word present in the query.
Translation	BLEU scoring	In general, BLEU is the most popular metric used for both comparison of Translation systems and tuning of machine translation models); most systems are trained to optimize BLEU scoring.
POS taggers	accuracy	Finally, we do the experiment of building POS taggers by using some source languages and evaluate the accuracy of those taggers.
transliteration alignment	GIZA	We have found that the most common word alignment tool for transliteration alignment is GIZA++; Sravana Reddy and Sonjia).
transliterating from English to Chinese	Accuracy	For transliterating from English to Chinese, our combined system achieved Accuracy in Top-1 0.312, compared with the best performance in NEWS2011, which was 0.348.
translation	accuracy	However, translation accuracy does not relate to quality of composite service or user satisfaction.
coverage	TF*IDF	For coverage, TF*IDF (X=1) and TF*IDF (X=2) were better than our method, while we obtained more appropriate travel information links provided as ad links than the baseline method.
coverage	TF*IDF (X=2)	For coverage, TF*IDF (X=1) and TF*IDF (X=2) were better than our method, while we obtained more appropriate travel information links provided as ad links than the baseline method.
generative	accuracy	Although generative models have some advantages, a number of studies on natural language tasks have shown that discriminative models tend to overcome generative models with respect to accuracy.
vocabulary acquisition	error ratio	have examined corpus for vocabulary acquisition for Japanese in terms of reused words and unused words; have proposed an error ratio corresponding to the number of newly introduced errors per each improvement after new training text was supplied.
classification	accuracy	Experimental results showed that the proposed method achieved 81.7% classification accuracy for the evaluation data.
classification	accuracy	The value in each cell indicates the classification accuracy.
Chinese sentiment analysis	accuracy	Segmentation is an initial and compulsory stage in Chinese sentiment analysis, without applying any other pre-processing stage for training and testing data, the overall accuracy for sentiment analysis is 81.65%.
classification	accuracy	The classification accuracy is defined as follows.
parsing longer sentences	coverage	For this reason we had to increase the value of h for parsing longer sentences, at the cost of decreased performance and coverage.
MWE recognizer	accuracy	Our extrinsic evaluation of an ideal MWE recognizer (for only extracting MWEs of type named entities, duplications, numbers, dates and some predefined list of compound prepositions) showed that the pre-processing of the test data would improve the labeled parsing accuracy by 1.5%.
parsing	accuracy	But the effect of different MWE types on parsing accuracy is still an open research topic and needs to be analyzed in detail.
parsing	accuracy	reports a 5% parsing accuracy increase for Swedish by using a depedency parser which uses a memory-based learner as its oracle.
MWE	accuracy	Our results showed that different MWE types have different impacts on the parsing accuracy.
parsing	accuracy	Our results showed that different MWE types have different impacts on the parsing accuracy.
parsing	accuracy	Preliminary results show that our method outperforms a self-training setting where instances are simply selected by order of occurrence in the corpus and argue that self-training is a cheap and effective method for improving parsing accuracy for morphologically rich languages.
parsing	accuracy	We present experiments for German, a language with rich morphology (relative to English) and semi-free word order, and show that self-training can improve parsing accuracy when only a small amount of labelled training data is available.
PHI detection	accuracy	Accuracy, Precision, Recall, Fscore are used to assess PHI detection . It is a common practice to evaluate detection as a binary classification of word categories (e.g., accuracy of name classification in a set of EHR).
RRE task	MIRA  87.081  81.881% 82.909%  82.339  MIRA-B  87.139  80.679%  84.59%  83.213	 Table 2: Experimental results with sequence learning models for RRE task.  Methods  Accuracy Precision  Recall  F-measure  CRF-LBFG  91.27  89.328% 87.039%  88.158  CRF-LBFG-B  91.45  89.708% 87.866%  88.807  CRF-SGD  91.39  90.046% 86.953%  88.023  CRF-SGD-B  92.041  90.011% 87.787%  88.584  MIRA  87.081  81.881% 82.909%  82.339  MIRA-B  87.139  80.679%  84.59%  83.213
POS tagging	accuracy	The aim of this work was to evaluate the presented approach for POS tagging regarding accuracy (number of correctly tagged tokens / total number of tokens).
text classification algorithms	precision	The usual measures for performance evaluation in text classification algorithms are precision and recall.
text classification algorithms	recall	The usual measures for performance evaluation in text classification algorithms are precision and recall.
Quotation Extraction task	F β=1 score value	For the whole Quotation Extraction task, the observed F β=1 score value is 66.03%.
Quotation Extraction task	F β=1 score value	For the whole Quotation Extraction task, the observed F β=1 score value is 66.03%.
QA systems	accuracy	We propose a method to increase QA systems accuracy by executing an answer appreciation phase over the extraction result that goes beyond the usual QA answer validation.
data crawling	recall	Unlike Bitextor, data crawling is fit to our type of text, resulting in very good recall and precision at document-level alignment with surface-based features.
data crawling	precision	Unlike Bitextor, data crawling is fit to our type of text, resulting in very good recall and precision at document-level alignment with surface-based features.
SMT	BR	 Table 5: System performance with different combina- tions of the SMT features used in decoding. BR is the  metric used to score the answers.
word prediction	accuracy	While the word prediction results exhibits 77% accuracy in predicting 40% of the most frequent words in the text, the perplexity reduction did not help to produce better translations.
word prediction	accuracy	The n-gram probabilities estimated were then used for word prediction, and we report the resulting prediction accuracy at different relaxation orders.
translation	accuracy	However, low translation accuracy has been observed for language pairs with limited training resources (.
MT	BLEU	In addition to assessing the MT systems using automatic evaluation metrics such as BLEU () and METEOR (), large-scale human evaluations are also carried out.
MT	METEOR	In addition to assessing the MT systems using automatic evaluation metrics such as BLEU () and METEOR (), large-scale human evaluations are also carried out.
WSD	BLEU	Results are evaluated in three ways: a manual evaluation of WSD performance from MT perspective, an analysis of agreement between the WSD-proposed equivalent and those suggested by the three systems, and finally by computing BLEU, NIST and METEOR scores for all translation versions.
WSD	METEOR	Results are evaluated in three ways: a manual evaluation of WSD performance from MT perspective, an analysis of agreement between the WSD-proposed equivalent and those suggested by the three systems, and finally by computing BLEU, NIST and METEOR scores for all translation versions.
WSD	BLEU	Results are evaluated in several ways: • By manually evaluating WSD performance from the MT perspective, • By analysing the agreement between each of the MT systems and the UKB/wordnet-derived translation, • By comparing BLEU, NIST and ME-TEOR scores achieved with each translation version.
WSD	ME-TEOR	Results are evaluated in several ways: • By manually evaluating WSD performance from the MT perspective, • By analysing the agreement between each of the MT systems and the UKB/wordnet-derived translation, • By comparing BLEU, NIST and ME-TEOR scores achieved with each translation version.
MT	BLEU	Finally, we wanted to see how the WSD/wordnet-based translation compares with the three MT systems using the BLEU, NIST and METEOR scores.
MT	METEOR	Finally, we wanted to see how the WSD/wordnet-based translation compares with the three MT systems using the BLEU, NIST and METEOR scores.
MT	Presis	We can see that our generated version using disambiguated equivalents does not outperform any of the MT systems on any metric, except once when the WSD-align version outperforms Presis on the NIST score and comes fairly close to the Bing score.
MT	BLEU	Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU () and NIST).
MT evaluation	BLEU	Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU () and NIST).
SETIMES	split	For SETIMES, the split is 100,000/500/1,000 and for EMEA, it is 700,000/500/1,000.
SMT	BLEU	The best modified SMT systems improve the translation of connectives without degrading BLEU scores.
SMT	BLEU	A threshold-based SMT system using only high-confidence labels improves BLEU scores by 0.2-0.4 points.
SMT	BLEU	On the one hand, we use the BLEU metric () with one reference translation as is most often done in current SMT research 2 . To improve confidence in the BLEU scores, especially when test sets are small, we also compute BLEU scores using bootstrapping of data sets; the test sets are re-sampled a thousand times and the average BLEU score is computed from individual sample scores.
SMT	BLEU	On the one hand, we use the BLEU metric () with one reference translation as is most often done in current SMT research 2 . To improve confidence in the BLEU scores, especially when test sets are small, we also compute BLEU scores using bootstrapping of data sets; the test sets are re-sampled a thousand times and the average BLEU score is computed from individual sample scores.
SMT	BLEU score	On the one hand, we use the BLEU metric () with one reference translation as is most often done in current SMT research 2 . To improve confidence in the BLEU scores, especially when test sets are small, we also compute BLEU scores using bootstrapping of data sets; the test sets are re-sampled a thousand times and the average BLEU score is computed from individual sample scores.
MT	BLEU	 Table 3: MT systems dealing with manually and automatically (PT, PT+, EU) sense-labeled connectives: BLEU  scores (including bootstrapped ones) and variation in the translation of individual connectives (∆Connectives,  as a percentage). The description of each condition and the baseline BLEU scores are in the text of the article.
MT	BLEU	 Table 3: MT systems dealing with manually and automatically (PT, PT+, EU) sense-labeled connectives: BLEU  scores (including bootstrapped ones) and variation in the translation of individual connectives (∆Connectives,  as a percentage). The description of each condition and the baseline BLEU scores are in the text of the article.
cognate detection	precision	Unfortunately, automatic approaches to cognate detection still lack the precision of trained linguists' judgments.
lie detection	accuracy	Likewise, meta-analyses of training programs designed to teach lie detection have shown a small to medium effect size in improving judges' detection accuracy (e.g.,.
lie detection	accuracy	Likewise, a recent meta-analysis of training programs designed to teach lie detection has shown only small to medium effect sizes in improving judges' detection accuracy (e.g.,.
detecting deception in text	accuracy	Previously suggested techniques for detecting deception in text reach modest accuracy rates at the level of lexico-semantic analysis.
POS induction	accuracy	Interest in unsupervised POS induction has been revived in recent years after Bayesian HMMs are shown to increase accuracy by up to 14 percentage points over basic maximum-likelihood estimation.
POS tagging	accuracy	We demonstrate the model's utility as a predictive language model by its low perplexity on held-out test data as compared to several related topic models, and most importantly, we show that this model achieves statistically significant and consistent improvements in unsupervised POS tagging accuracy over a Bayesian HMM.
TM	Interquartile Range (IQR)	To assess the robustness of the TM, we sweep over varying configurations of the LDA model, and plot the results using Box-and-Whiskers plots: the box indicates the quartiles and the whiskers are maximal 1.5 times of the Interquartile Range (IQR) or equal to the data point that is no greater to the 1.5 IQR.
Translation	BLEU score	 Table 5: Translation results for Haitian Creole- English. Statistically significant differences from  baseline BLEU score are marked ** (p < 0.01).
extracting relations of real-world objects	precision	Assuming co-mentioned entities to be related is an approach of extracting relations of real-world objects with limited precision.
noun classification	EVENT	In order to test this hypothesis we developed a series of tools for the task of noun classification into lexical semantic classes (such as EVENT, HUMAN, LOCATION, etc.).
noun classification	HUMAN	In order to test this hypothesis we developed a series of tools for the task of noun classification into lexical semantic classes (such as EVENT, HUMAN, LOCATION, etc.).
noun classification	LOCATION	In order to test this hypothesis we developed a series of tools for the task of noun classification into lexical semantic classes (such as EVENT, HUMAN, LOCATION, etc.).
Translation	Impo rtant --)	 Table 4: Translation evaluation. Total nb+ (-): total  number of imp rovements (decreases), not distinguish- ing whether it is slight or important; important ++ (--):  the number of important imp rovements (decreases).  Overall impact = (Total nb+) + (Importan++ ) -(Total  nb-) -(Impo rtant --)
MT	TER score	With this method, a human annotator produces a targeted reference sentence which is as close as possible to the MT hypothesis while being fully acceptable; from the targeted reference, the TER score then represents a normalized post-edit score, which has been shown to correlate with human ratings at least as well as more complex competing metrics.
interpretation	clarity	We therefore conducted a user-based evaluation which aims to assess the following points: interpretation quality, overall system quality, dialog clarity, game clarity and timing.
interpretation	clarity	We therefore conducted a user-based evaluation which aims to assess the following points: interpretation quality, overall system quality, dialog clarity, game clarity and timing.
interpretation	timing	We therefore conducted a user-based evaluation which aims to assess the following points: interpretation quality, overall system quality, dialog clarity, game clarity and timing.
interpretation	timing	shows the mean of the quantitative scores given by the 22 subjects for interpretation, overall system quality and timing.
summarization	ROUGE	To evaluate the performance of various summarization approaches, we use the widely accepted ROUGE ( metrics.
prediction	accuracy	Third, we show that using state-of-the-art lexicalized hierarchical models further improves prediction accuracy.
dependency grammar induction	accuracy	We gauged the suitability of capitalization-induced fragments for guiding dependency grammar induction by assessing accuracy, in WSJ, 1 of parsing constraints derived from their end-points.
parsing	accuracy	In addition to improving parsing accuracy, the automatically learned latent annotations of these latter approaches yield results that accord well with human intuitions, especially at the lexical or preterminal level (for example, separating demonstrative adjectives from definite articles under the DT tag).
PSD	Baseline	 Table 1: Accuracy of various PSD systems. Baseline is  most frequent sense.
POS induction research	standard	A second aim is to foster grammar and POS induction research across a wider variety of languages, and improving the standard of evaluation.
POS tagging	Pearson's correlation	Part of the story seems to be that our method tends to outperform the baseline by larger margins on datasets with smaller vocabularies 2 . The scatterplot in illustrates this tendency for coarsegrained POS tagging: Pearson's correlation is −0.6.
information retrieval	Mean Kendall's-τ measure	To evaluate our systems' performance in ranking, we use two measures commonly used in information retrieval: the Mean Kendall's-τ measure described in section 3.4.1 and Mean Reciprocal Rank (MRR).
information retrieval	Mean Reciprocal Rank (MRR)	To evaluate our systems' performance in ranking, we use two measures commonly used in information retrieval: the Mean Kendall's-τ measure described in section 3.4.1 and Mean Reciprocal Rank (MRR).
classification	accuracy	The resulting model results in similar classification accuracy while doing away with the complexity of Bayesian techniques.
read-ability classification	accuracy	The resulting classifiers significantly outperform the previous approaches on read-ability classification, reaching a classification accuracy of 93.3%.
error correction	precision	However, error correction is much harder and on this task precision remains low.
detection and recognition	F-score	Our system placed 11th out of 14 teams for the detection and recognition tasks and 11th out of 13 teams for the correction task based on F-score for both preposition and determiner errors.
recognition	correction	presents the results for recognition and Table 6 those for correction.
Article development	AP	 Table 5: Article development results: AP with inflation. The
Article development	inflation	 Table 5: Article development results: AP with inflation. The
spelling correction	F-score	In terms of the effect of pre-processing, spelling correction improved the F-score of Detection, Correction, and Recognition for preposition errors after revision, whereas there were fluctuations in other conditions.
spelling correction	Detection	In terms of the effect of pre-processing, spelling correction improved the F-score of Detection, Correction, and Recognition for preposition errors after revision, whereas there were fluctuations in other conditions.
determiner error correction	precision	For determiner error correction, the "mixed" model improved precision and F-score in the additional experiments.
determiner error correction	F-score	For determiner error correction, the "mixed" model improved precision and F-score in the additional experiments.
SVM	accuracy	In Experiment 3, we saw that SVM performs better than linear regression when the evaluation is done by accuracy but both demonstrate similar explanatory power in accounting for the variation.
OOV recognition	accuracy	As it is capable of capturing the morphological behaviors of characters, the character classification model has significantly better performance in OOV recognition and overall segmentation accuracy, and has been the state-of-art since its introduction, suggested by the leading performances of systems based on it in recent international Chinese word segmentation bakeoffs).
Riv	recall	And Riv is the recall of words that have occurred in the training corpus.
segmentation	accuracy	We decided to compare our algorithm with description length gain (DLG), for that it seems to deliver best segmentation accuracy among other unsupervised approaches ever reported on this benchmark (.
word alignment	accuracy	Using both earlier approaches and our novel method for word alignment, we then evaluate the accuracy of automated scoring and diagnostic classification for MCI.
Scoring	accuracy	 Table 3: Scoring accuracy results.
named-entity  recognition task	precision	 Table 5: Precision, total number of predicted genes (To- tal), and correct predictions (Correct), in a named-entity  recognition task using a complete lexicon, a filtered lex- icon, and lexicons learned with BioNELL and NELL.  BioNELL's lexicon achieves the highest precision, and  makes more correct predictions than NELL.
tagging	accuracy	We observe over 8% improvement in overall tagging accuracy with the inclusion of sequence information.
Sequence tagging	precision	Sequence tagging using CRFs gives us an average precision and recall of 79.6% and 69.7% respectively.
Sequence tagging	recall	Sequence tagging using CRFs gives us an average precision and recall of 79.6% and 69.7% respectively.
establishing when a patient experienced a condition	f-score	While for the task of establishing when a patient experienced a condition, our ML system significantly outperforms the ConText system (87% versus 69% f-score, respectively).
NER	Prec.	 Table 2. "Out-of-the-box" evaluation of existing de- identification and NER systems (Prec.=precision;  Rec.=recall; F 2 = F 2 -measure).
NER	precision	 Table 2. "Out-of-the-box" evaluation of existing de- identification and NER systems (Prec.=precision;  Rec.=recall; F 2 = F 2 -measure).
NER	Rec.	 Table 2. "Out-of-the-box" evaluation of existing de- identification and NER systems (Prec.=precision;  Rec.=recall; F 2 = F 2 -measure).
NER	recall	 Table 2. "Out-of-the-box" evaluation of existing de- identification and NER systems (Prec.=precision;  Rec.=recall; F 2 = F 2 -measure).
NER	F 2 = F 2 -measure	 Table 2. "Out-of-the-box" evaluation of existing de- identification and NER systems (Prec.=precision;  Rec.=recall; F 2 = F 2 -measure).
ID	F-score	For ID, we find a different effect also on F-score, with all but one system showing reduced performance under the new criteria, with some very clear drops in performance; the only system to benefit is UTurku.
extraction of PROCESS type events	precision	Conversely, the Stanford system, which showed the highest instance-level performance in the extraction of PROCESS type events (see ), shows a clear loss in precision.
phenotype mapping	accuracy	Active learning will make phenotype mapping more efficient and improve its accuracy.
phenotype mapping	accuracy	Application of effective active learning techniques will make phenotype mapping more efficient and improve its accuracy and, along with intuitive phenotype query tools, would provide a major resource for researchers utilizing these genomic data.
coreference resolution	accuracy	Unfortunately , the less successful story indicates that state-of-the-art coreference resolution systems fail to achieve high accuracy for this genre of discourse.
Segmentation	accuracy	 Table 1: Segmentation accuracy in artificial poems
Sentence alignment	recall	Sentence alignment tools are usually evaluated using standard recall and precision measures, combined in the F-measure, with respect to some manually defined gold alignment).
Sentence alignment	precision	Sentence alignment tools are usually evaluated using standard recall and precision measures, combined in the F-measure, with respect to some manually defined gold alignment).
Sentence alignment	F-measure	Sentence alignment tools are usually evaluated using standard recall and precision measures, combined in the F-measure, with respect to some manually defined gold alignment).
Authorship Attribution	AA	Function words are an important feature set for Authorship Attribution (hereafter "AA") because they are considered topic-independent or contextfree, and that they are largely used in an unconscious manner).
detecting famous quotes in literary or philosophical text	f-scores	Simple text classification algorithms perform remarkably well when used for detecting famous quotes in literary or philosophical text, with f-scores approaching 95%.
optical character recognition (OCR)	accuracy	We provide a detailed error analysis of transcription by optical character recognition (OCR), non-expert humans, and expert humans and weigh each technique based on accuracy, speed, cost and the need for scholarly overhead.
summarization	accuracy	These include task-based evaluations of summarization, assessments of the accuracy of current automatic evaluations, the benefits from using several automatic evaluation measures, case studies of differences between manual and automatic evaluation, cross-lingual summarization and steps towards abstractive summarization.
predicting abstractive sentence links	precision/recall/f-score	For evaluating Step 1, predicting abstractive sentence links, we present both precision/recall/f-score as well as the area under the receiver operator characteristic curve (AUROC).
predicting abstractive sentence links	receiver operator characteristic curve (AUROC)	For evaluating Step 1, predicting abstractive sentence links, we present both precision/recall/f-score as well as the area under the receiver operator characteristic curve (AUROC).
summaries	ROUGE	Using these context-free summaries, the original generic manual, primed manual, and automatic summaries were evaluated using ROUGE.
fact extraction	REVERB	Also, the elapsed time of the fact extraction process was more than one order of magnitude longer than REVERB, possibly limit- ing the ability of the system to scale to very large collections of documents.
resolver	accuracy	We evaluate two components of our system -the working context (C work ) and the resolver accuracy.
translation out of English	abso- lute value	 Table 8: System-level Spearman's rho correlation of the  automatic evaluation metrics with the human judgments  for translation out of English, ordered by average abso- lute value.
translation out of English	corre- lation	 Table 10: Segment-level Kendall's tau correlation of the  automatic evaluation metrics with the human judgments  for translation out of English, ordered by average corre- lation.
MT	STS score	In view of this, our claim is that the output of MT systems will be more strongly correlated with humans if we have a higher STS score between MT system output and the reference translation.
MT	BLEU	We consider four widely used MT metrics (BLEU, NIST, METEOR (Banerjee and) (v0.7), and TER) as our baselines.
MT	METEOR	We consider four widely used MT metrics (BLEU, NIST, METEOR (Banerjee and) (v0.7), and TER) as our baselines.
MT	TER	We consider four widely used MT metrics (BLEU, NIST, METEOR (Banerjee and) (v0.7), and TER) as our baselines.
MT	BLEU	Different from most classic approaches for measuring the progress of an MT system or comparing MT systems, which assess quality by contrasting system output to reference translations such as BLEU (), Quality Estimation (QE) is a more challenging task, aimed at MT systems in use, and therefore without access to reference translations.
Scoring task	Mean-Absolute-Error (MAE)	For the Scoring task the metrics were Mean-Absolute-Error (MAE) and Root Mean Squared Error (RMSE).
Scoring task	Root Mean Squared Error (RMSE)	For the Scoring task the metrics were Mean-Absolute-Error (MAE) and Root Mean Squared Error (RMSE).
scor- ing	DA	 Table 2: The experiment results on the ranking and scor- ing tasks. In this table, DA, SC, MAE and RMSE are  DeltaAvg, Spearman Correlation, Mean-Average-Error  and Root-Mean-Squared-Error respectively.
scor- ing	MAE	 Table 2: The experiment results on the ranking and scor- ing tasks. In this table, DA, SC, MAE and RMSE are  DeltaAvg, Spearman Correlation, Mean-Average-Error  and Root-Mean-Squared-Error respectively.
scor- ing	RMSE	 Table 2: The experiment results on the ranking and scor- ing tasks. In this table, DA, SC, MAE and RMSE are  DeltaAvg, Spearman Correlation, Mean-Average-Error  and Root-Mean-Squared-Error respectively.
scor- ing	DeltaAvg	 Table 2: The experiment results on the ranking and scor- ing tasks. In this table, DA, SC, MAE and RMSE are  DeltaAvg, Spearman Correlation, Mean-Average-Error  and Root-Mean-Squared-Error respectively.
translation	accuracy	These two approaches are: improved translation accuracy via system combination (, and automatic quality-estimation techniques used as an additional layer on top of MT systems, which present the user only with translations that are predicted as being accurate.
MT	BLEU	 Table 1: MT system performance and ranking performance using BLEU prediction at Doc-and Sent-level.
Translation	BLEU	Translation quality is assessed using case insensitive BLEU scores and bootstrapping is used to establish statistical significance of the score differences.
machine translation	BLEU metric	We measure machine translation performance using the BLEU metric ().
MT evaluation	MEANT	We introduce the first fully automatic, fully semantic frame based MT evaluation metric, MEANT, that outperforms all other commonly used automatic metrics in correlating with human judgment on translation adequacy.
Sentence level correlation	Kendall correlation	Sentence level correlation analysis, following standard NIST MetricsMATR protocol, shows that this fully automated version of HMEANT achieves significantly higher Kendall correlation with human adequacy judgments than BLEU, NIST, METEOR , PER, CDER, WER, or TER.
Sentence level correlation	BLEU	Sentence level correlation analysis, following standard NIST MetricsMATR protocol, shows that this fully automated version of HMEANT achieves significantly higher Kendall correlation with human adequacy judgments than BLEU, NIST, METEOR , PER, CDER, WER, or TER.
Sentence level correlation	PER	Sentence level correlation analysis, following standard NIST MetricsMATR protocol, shows that this fully automated version of HMEANT achieves significantly higher Kendall correlation with human adequacy judgments than BLEU, NIST, METEOR , PER, CDER, WER, or TER.
Sentence level correlation	WER	Sentence level correlation analysis, following standard NIST MetricsMATR protocol, shows that this fully automated version of HMEANT achieves significantly higher Kendall correlation with human adequacy judgments than BLEU, NIST, METEOR , PER, CDER, WER, or TER.
Sentence level correlation	TER	Sentence level correlation analysis, following standard NIST MetricsMATR protocol, shows that this fully automated version of HMEANT achieves significantly higher Kendall correlation with human adequacy judgments than BLEU, NIST, METEOR , PER, CDER, WER, or TER.
MT evaluation	BLEU	We introduce the first fully automatic semantic-framebased MT evaluation metric capable of outperforming all other commonly used automatic metrics like BLEU, NIST, METEOR, PER, CDER, WER, and TER for evaluating translation adequacy.
MT evaluation	METEOR	We introduce the first fully automatic semantic-framebased MT evaluation metric capable of outperforming all other commonly used automatic metrics like BLEU, NIST, METEOR, PER, CDER, WER, and TER for evaluating translation adequacy.
MT evaluation	PER	We introduce the first fully automatic semantic-framebased MT evaluation metric capable of outperforming all other commonly used automatic metrics like BLEU, NIST, METEOR, PER, CDER, WER, and TER for evaluating translation adequacy.
MT evaluation	WER	We introduce the first fully automatic semantic-framebased MT evaluation metric capable of outperforming all other commonly used automatic metrics like BLEU, NIST, METEOR, PER, CDER, WER, and TER for evaluating translation adequacy.
MT evaluation	TER	We introduce the first fully automatic semantic-framebased MT evaluation metric capable of outperforming all other commonly used automatic metrics like BLEU, NIST, METEOR, PER, CDER, WER, and TER for evaluating translation adequacy.
MT evaluation	BLEU	For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU), NIST), METEOR (Banerjee and), PER (), CDER (), WER (), and TER).
MT evaluation	METEOR	For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU), NIST), METEOR (Banerjee and), PER (), CDER (), WER (), and TER).
MT evaluation	PER	For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU), NIST), METEOR (Banerjee and), PER (), CDER (), WER (), and TER).
MT evaluation	WER	For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU), NIST), METEOR (Banerjee and), PER (), CDER (), WER (), and TER).
MT evaluation	TER	For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU), NIST), METEOR (Banerjee and), PER (), CDER (), WER (), and TER).
MT evaluation	BLEU	N-gram oriented automatic MT evaluation metrics like BLEU perform well at capturing translation fluency, and ranking overall systems with respect to each other when their scores are averaged over entire documents or corpora.
MT evaluation	PER	Other lexical similarity based automatic MT evaluation metrics, like NIST), ME-TEOR (), PER (), CDER (), WER (), and TER (), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumptionthat a good translation is one that shares the same lexical choices as the reference translationis not justified semantically.
MT evaluation	WER	Other lexical similarity based automatic MT evaluation metrics, like NIST), ME-TEOR (), PER (), CDER (), WER (), and TER (), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumptionthat a good translation is one that shares the same lexical choices as the reference translationis not justified semantically.
MT evaluation	TER	Other lexical similarity based automatic MT evaluation metrics, like NIST), ME-TEOR (), PER (), CDER (), WER (), and TER (), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumptionthat a good translation is one that shares the same lexical choices as the reference translationis not justified semantically.
MT evaluation	BLEU	 Table 3: Sentence-level correlation with human adequacy judgment on GALE-A (training) and GALE-B (testing) comparing all  commonly used MT evaluation metrics against our proposed new fully automatic semantic frame based MT evaluation metric  integrated with various lexical similarity scores between semantic role fillers: (a) BLEU, (b) METEOR, (c) cosine similarity and  (d) MinMax with mutual information.
MT evaluation	METEOR	 Table 3: Sentence-level correlation with human adequacy judgment on GALE-A (training) and GALE-B (testing) comparing all  commonly used MT evaluation metrics against our proposed new fully automatic semantic frame based MT evaluation metric  integrated with various lexical similarity scores between semantic role fillers: (a) BLEU, (b) METEOR, (c) cosine similarity and  (d) MinMax with mutual information.
MT evaluation	BLEU	 Table 3: Sentence-level correlation with human adequacy judgment on GALE-A (training) and GALE-B (testing) comparing all  commonly used MT evaluation metrics against our proposed new fully automatic semantic frame based MT evaluation metric  integrated with various lexical similarity scores between semantic role fillers: (a) BLEU, (b) METEOR, (c) cosine similarity and  (d) MinMax with mutual information.
MT evaluation	METEOR	 Table 3: Sentence-level correlation with human adequacy judgment on GALE-A (training) and GALE-B (testing) comparing all  commonly used MT evaluation metrics against our proposed new fully automatic semantic frame based MT evaluation metric  integrated with various lexical similarity scores between semantic role fillers: (a) BLEU, (b) METEOR, (c) cosine similarity and  (d) MinMax with mutual information.
SPE	PORTAGE	() report on good results both in terms of automatic evaluation metrics and human evaluation for the SPE systems based on PORTAGE () and Moses ().
Translation	BLEU score	 Table 3. Translation results in terms of BLEU score for  newstest2012.
SMT	Abstract	Forms Wanted: Training SMT on Monolingual Data. Abstract at Machine Translation and Morphologically-Rich Lan-guages
MT	TER	Any static automatic MT score, e.g. TER), BLEU (), can be used, provided that it is reliable on a block of usually relatively small size.
MT	BLEU	Any static automatic MT score, e.g. TER), BLEU (), can be used, provided that it is reliable on a block of usually relatively small size.
SMT	BLEU score	This is because the baseline SMT system was tuned with respect to the BLEU score on indomain data, differently to the adapting system.
SMT	TER	 Table 2: S values of 4 SMT systems (see text) for  the block-wise TER evaluation, corresponding to the U  model, and the incremental evaluation, corresponding to  the CA model.
translation	BLEU	When the layer features are applied for both training and translation, we observe improvements of up to 0.2% in BLEU and 0.5% in TER over the corresponding standard lattice system.
translation	TER	When the layer features are applied for both training and translation, we observe improvements of up to 0.2% in BLEU and 0.5% in TER over the corresponding standard lattice system.
translation	accuracy	The speed of direct search is generally comparable to MERT, and translation accuracy is generally superior.
parsing	accuracy	Typically, parsing accuracy drops from 90+% indomain to 80-84% in the target domain.
Information retrieval	accuracy	• Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).
MWE extraction	precision	Published results comparing MWE extraction techniques usually evaluate them on small controlled data sets using objective measures such as precision, recall and mean average precision ().
MWE extraction	recall	Published results comparing MWE extraction techniques usually evaluate them on small controlled data sets using objective measures such as precision, recall and mean average precision ().
MWE extraction	mean average precision	Published results comparing MWE extraction techniques usually evaluate them on small controlled data sets using objective measures such as precision, recall and mean average precision ().
parsing	accuracy	We obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the French Treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach.
pattern matching	accuracy	For the VNC, the pattern matching algorithm achieves an accuracy of 98%, for VPC, we get an accuracy of 77.6%, and NNC we achieve an accuracy of 99%.
pattern matching	accuracy	For the VNC, the pattern matching algorithm achieves an accuracy of 98%, for VPC, we get an accuracy of 77.6%, and NNC we achieve an accuracy of 99%.
pattern matching	accuracy	For the VNC, the pattern matching algorithm achieves an accuracy of 98%, for VPC, we get an accuracy of 77.6%, and NNC we achieve an accuracy of 99%.
SVM-TK classifier	accuracy	 Table 4: SVM-TK classifier accuracy over gold and predicted features
parsing	accuracy	Actually, ( also investigated parsing accuracy on the Penn Korean treebank; the direct comparison could be very difficult because parsing criteria is different.
classification	accuracy	Using supervised machine learning approach, we achieve classification accuracy of 88% on Stanford dataset.
SSA	LEM	 Table 3: SSA results with preprocessing TOK and LEM.
Polarity Classification	accuracy	Polarity Classification Using the Lexicon: High accuracy for prior polarity identification is very hard to achieve, as prior polarity values are approximations only.
polarity classification	accuracy	Several researches then tried syntacticstatistical techniques for polarity classification, reporting good accuracy (, making the two-step methodology (sentiment lexicon followed by further NLP techniques) the standard method for polarity classification.
differential diagnosis	accuracy	In the case of differential diagnosis, the lexicalstructural features performed best, matching the accuracy of the combined feature set (5% over the majority class baseline).
diagnostic process	count	 Table 3: Labels for various steps of the diagnostic process  as well as their count and ratios of the total narratives, af- ter eliminating those with no annotator agreement. These  labels are explained in section 4.3.
hedge cue detection	F-measure	Our classifier for hedge cue detection achieved an F-measure of 79.9, better than the third position in the Shared Task for hedge identification.
Scope detection	F-measure	Scope detection results (using learned hedge cues) achieved an F-measure of 54.7, performing better than the fifth result in the corresponding task, and five points below the best results obtained so far in the corpus (
information extraction (IE)	certainty	In recognition of this need, a number of recent information extraction (IE) resources involving structured representations of text statements have explicitly included some marking of certainty and polarity).
SMT translation	accuracy	Furthermore, SMT translation accuracy largely depends on the quality of the bilingual corpora as well as their relevance to the domain of text to be translated.
ontology creation	accuracy	However, ontology creation is labour intensive and error prone, and its maintenance is crucial for ensuring the accuracy and utility of a given resource.
entity recognition	F 1 score	(3) For entity recognition, we integrate the gazetteer with a simple, but effective machine learning classifier, and experimentally show that the extended gazetteers improve the F 1 score between 7% and 12% over our baseline approach and outperform) on all learned concepts (subject, location, temporal).
MFS	F-measure	The performance of the MFS baseline is remarkably high, i.e., 85.5% of F-measure).
MT outputs	RUR	We test these two techniques on English-Czech MT outputs using our own reimplementation of the MST parser () named RUR 1 parser. and evaluate their contribution to the SMT post-editing quality of the DEPFIX system), which we outline in Section 5.
SMT	UEDIN	We used the outputs of three SMT systems: GOOGLE, 9 UEDIN () and BOJAR.
SMT	BOJAR	We used the outputs of three SMT systems: GOOGLE, 9 UEDIN () and BOJAR.
SMT output parsing	BLEU	This leads us to believe that the two proposed methods are able to produce slightly better SMT output parsing results.: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and processed by DEPFIX.
SMT output parsing	BOJAR	This leads us to believe that the two proposed methods are able to produce slightly better SMT output parsing results.: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and processed by DEPFIX.
SMT	BOJAR	This leads us to believe that the two proposed methods are able to produce slightly better SMT output parsing results.: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and processed by DEPFIX.
SMT	BOJAR	 Table 2: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and  UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and  processed by DEPFIX. The score of RUR+WORS+PARA is significantly higher at 95% confidence level than the scores  marked with '*' on the same data.
MT	BLEU	For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER () because of their support on fast and inexpensive evaluation.
MT	METEOR	For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER () because of their support on fast and inexpensive evaluation.
MT	PER	For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER () because of their support on fast and inexpensive evaluation.
MT	WER	For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER () because of their support on fast and inexpensive evaluation.
MT evaluation	BLEU	For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER () because of their support on fast and inexpensive evaluation.
MT evaluation	METEOR	For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER () because of their support on fast and inexpensive evaluation.
MT evaluation	PER	For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER () because of their support on fast and inexpensive evaluation.
MT evaluation	WER	For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER () because of their support on fast and inexpensive evaluation.
MT	accuracy	The human reader was instructed to order the sentences from the three MT systems according to the accuracy of meaning in the translations.
translation	precision	As can be observed in, the translation quality, as measured by precision and error metrics, was consistently and significantly increased when the HFC reordering rule was used and was significantly improved further when the refinement proposed in this work was used.
translation	BLEU	 Table 7: Evaluation of translation quality of a test set when CWMT and CWMT extended corpus were used for  training. Results are given in terms of BLEU, RIBES, TER, and WER for baseline, head finalization, and proposed  refinement of head finalization reordering rules.
translation	RIBES	 Table 7: Evaluation of translation quality of a test set when CWMT and CWMT extended corpus were used for  training. Results are given in terms of BLEU, RIBES, TER, and WER for baseline, head finalization, and proposed  refinement of head finalization reordering rules.
translation	TER	 Table 7: Evaluation of translation quality of a test set when CWMT and CWMT extended corpus were used for  training. Results are given in terms of BLEU, RIBES, TER, and WER for baseline, head finalization, and proposed  refinement of head finalization reordering rules.
translation	WER	 Table 7: Evaluation of translation quality of a test set when CWMT and CWMT extended corpus were used for  training. Results are given in terms of BLEU, RIBES, TER, and WER for baseline, head finalization, and proposed  refinement of head finalization reordering rules.
translating xEBR ontology terms	BLEU	Tables 4 to 5 illustrate the final results for our experiments on translating xEBR ontology terms, using the NIST), BLEU (), and Meteor (Lavie and) algorithms.
clause alignment	precision	The evaluation of the clause alignment implementation of the Gale-Church algorithm on the same corpus shows overall precision of 0.902, recall -0.891 and F 1 measure -0.897.
clause alignment	recall -0.891	The evaluation of the clause alignment implementation of the Gale-Church algorithm on the same corpus shows overall precision of 0.902, recall -0.891 and F 1 measure -0.897.
clause alignment	F 1 measure -0.897	The evaluation of the clause alignment implementation of the Gale-Church algorithm on the same corpus shows overall precision of 0.902, recall -0.891 and F 1 measure -0.897.
empty node recovery evaluation	Precision	We were inspired empty node recovery evaluation by) and defined antecedent Precision (P), Recall (R) and F-measure (F) as follows, Here, S is the set of each pronoun in English translated by decoder, G is the set of the gold standard zero pronoun.
empty node recovery evaluation	Recall (R)	We were inspired empty node recovery evaluation by) and defined antecedent Precision (P), Recall (R) and F-measure (F) as follows, Here, S is the set of each pronoun in English translated by decoder, G is the set of the gold standard zero pronoun.
empty node recovery evaluation	F-measure (F)	We were inspired empty node recovery evaluation by) and defined antecedent Precision (P), Recall (R) and F-measure (F) as follows, Here, S is the set of each pronoun in English translated by decoder, G is the set of the gold standard zero pronoun.
MFS	MRR	Finally, MFS is defined as • MRR (mean reciprocal rank): Of the ranked candidates c n,1 , c n,2 , ..., let the highest ranked one which is also included in the reference set r n be c n,j . We then define reciprocal rank RR n = 1/j.
NER	accuracy	WP-derived corpora can also be used for improving NER accuracy in other ways.
parsing	accuracy	Unlike propositions, word sense and named entities, where it is simply a matter of counting the correct answers, or for parsing, where there is an established metric, evaluating the accuracy of coreference continues to be contentious.
Coreference Resolution	BCUB	 Table 5. Performance on Coreference Resolution - English  Precision Recall F 1  MUC  0.6892 0.6655 0.6771  BCUB  0.7547 0.7410 0.7478  CEAFE  0.4876 0.5105 0.4988  OF-Develop-Gold  0.6412  MUC  0.6535 0.5643 0.6056  BCUB  0.7812 0.6809 0.7276  CEAFE  0.4322 0.5101 0.4679  OF-Develop-Auto  0.6003  MUC  0.6928 0.6595 0.6758  BCUB  0.7765 0.7328 0.7540  CEAFE  0.5072 0.5390 0.6253
parsing	accuracy	For example, in parsing technologies, over 90% parsing accuracy has been achieved, yet some coordination structures or modifier dependencies are still analyzed incorrectly.
parsing	accuracy	For example, in parsing technologies, over 90% parsing accuracy has been achieved, yet some coordination structures or modifier dependencies are still analyzed incorrectly.
classification	accuracy	In our study, classification accuracy has been used to evaluate the results of the experiments.
classification	accuracy	Naïve Bayes (NB) has been used as Baseline system for our experiment with classification accuracy of 80.65%.
SMT	BASE	In both SMT and TC experiments, the BASE model, which is based on the manually-modified inconsistency of special characters, achieved better results than the ORG model.
SMT	VAR_	The SMT results show that three out of six augmented models, VAR_SPLIT, VAR_FREQ and BASE, performed better than the ORG configuration.
SMT	BASE	The SMT results show that three out of six augmented models, VAR_SPLIT, VAR_FREQ and BASE, performed better than the ORG configuration.
SMT	VAR	Comparing VAR_COMB and VAR_SPLIT in both the TC experiment and SMT experiment, we see that the VAR_SPLIT results are better in both cases.
SVM tagger	accuracy	We establish SVM tagger's (individual) accuracy as the baseline for our voting experiments.
Translation	BLEU	 Table 3: Translation results in term of BLEU and TER
Translation	TER	 Table 3: Translation results in term of BLEU and TER
parsing	accuracy	We outline here the parsing results on the six thesauri: DLR, completely parsed (175,000 entries; 15,000 pages; 37 volumes) at 98.01% accuracy, DAR completely parsed (25,000 entries, 3000 pages, 5 volumes), but no gold standard was available for automatic evaluation, while for TLF, DWB, GWB, and DMLRL, around 50 significant (including very large) entries have been parsed with very sound outcomes but not gold standard available for the parsing evaluation.
NER Tagger	precision	The unigram NER Tagger using gazetteer lists achieves up to 65.21% precision, 88.63% recall and 75.14% f-measure.
NER Tagger	recall	The unigram NER Tagger using gazetteer lists achieves up to 65.21% precision, 88.63% recall and 75.14% f-measure.
NER Tagger	f-measure	The unigram NER Tagger using gazetteer lists achieves up to 65.21% precision, 88.63% recall and 75.14% f-measure.
NER	Date	The objective of this NER system is to recognize five classes of NEs -Person, Location, Organization, Date and Time.
Massage Understanding Conference (MUC)	Precision (P) and Recall (R)	Massage Understanding Conference (MUC) and Multilingual Entity (MET) used the terms Precision (P) and Recall (R) from information retrieval research community which are now being used as evaluation metrics for performance of NER systems.
NER	precision	Our NER system is evaluated in terms of precision, recall and f-measure.
NER	recall	Our NER system is evaluated in terms of precision, recall and f-measure.
NER	f-measure	Our NER system is evaluated in terms of precision, recall and f-measure.
sentiment classification	Chi square (CHI)	Various feature selection methods has been proposed for selecting predominating features for sentiment classification, for example Information Gain (IG), Mutual Information (MI), Chi square (CHI), Gain Ratio (GR), Document Frequency (DF) etc.).
Negation handling	NOT	Documents are initially pre-processed as follows: (i) Negation handling, "NOT_" is added to every words occurring after the negation word (no, not, isn't, can't etc.) in the sentence.
document classification	Cosine similarity	Three classifiers, namely k-NN, Naïve Bayes and Decision Tree were used for document classification based on TF-IDF scores and Cosine similarity.
NER	Precision	The overall NER experimental result is good with Precision of 90.02%, Recall of 78.08%, and F-score of 83.60%.
NER	Recall	The overall NER experimental result is good with Precision of 90.02%, Recall of 78.08%, and F-score of 83.60%.
NER	F-score	The overall NER experimental result is good with Precision of 90.02%, Recall of 78.08%, and F-score of 83.60%.
NER	Accuracy	 Table 6: Overall NER experimental results  Stock market Accuracy with NER Accuracy without NER  AAPL  82.93%  73.17%  GOOG  80.49%  75.61%  MSFT  75.61%  68.29%  AMZN  75.00%  71.88%
NER	Accuracy	 Table 6: Overall NER experimental results  Stock market Accuracy with NER Accuracy without NER  AAPL  82.93%  73.17%  GOOG  80.49%  75.61%  MSFT  75.61%  68.29%  AMZN  75.00%  71.88%
MT	DELiC4MT	The effectiveness of the approach was tested on 5 English to Hindi MT systems and it was observed that the system-level DELiC4MT scores correlate well with the scores produced by the most commonly used automatic evaluation metrics (BLEU, NIST, METEOR and TER) while providing finer-grained information.
MT	BLEU	The effectiveness of the approach was tested on 5 English to Hindi MT systems and it was observed that the system-level DELiC4MT scores correlate well with the scores produced by the most commonly used automatic evaluation metrics (BLEU, NIST, METEOR and TER) while providing finer-grained information.
MT	TER	The effectiveness of the approach was tested on 5 English to Hindi MT systems and it was observed that the system-level DELiC4MT scores correlate well with the scores produced by the most commonly used automatic evaluation metrics (BLEU, NIST, METEOR and TER) while providing finer-grained information.
MT evaluation	BLEU	The state-of-the-art methods for automatic MT evaluation are represented by BLEU () and closely related NIST), METEOR (Banerjee and and TER ().
MT evaluation	METEOR	The state-of-the-art methods for automatic MT evaluation are represented by BLEU () and closely related NIST), METEOR (Banerjee and and TER ().
MT evaluation	TER	The state-of-the-art methods for automatic MT evaluation are represented by BLEU () and closely related NIST), METEOR (Banerjee and and TER ().
MT evaluation	BLEU	Globally, these automatic MT evaluation metrics (BLEU, NIST, TER, METEOR, etc.) are being studied with great interest for different language pairs.
MT evaluation	TER	Globally, these automatic MT evaluation metrics (BLEU, NIST, TER, METEOR, etc.) are being studied with great interest for different language pairs.
MT evaluation	METEOR	Globally, these automatic MT evaluation metrics (BLEU, NIST, TER, METEOR, etc.) are being studied with great interest for different language pairs.
parsing	accuracy	We identify the most prominent error types, which should help to improve the parsing accuracy in future.
parsing	accuracy	In our experiments we found that deepest has no effect in increasing the parsing accuracy rather there is a slight decrease in the accuracy as compared to shortest.
parsing	accuracy	In our experiments we found that deepest has no effect in increasing the parsing accuracy rather there is a slight decrease in the accuracy as compared to shortest.
parsing	accuracy	This is the major reason for the difference in the parsing accuracies between gold and auto data. has shown that nivre_eager algorithm gives the best accuracy for Hindi.
parsing	accuracy	We performed experiments with the above options and found that using branching there is an increase in the parsing accuracy.
parsing	accuracy	And then step-by-step we tried to optimize those features for which the parsing accuracy increases.
SMT	BLEU	Others report rather low (sometimes negative) impact of Named Entity integration with SMT (0.3 BLEU gain for French-English in), 0.2 BLEU gain for Arabic-English in (), 1 BLEU loss for Chinese-English in).
SMT	BLEU	Others report rather low (sometimes negative) impact of Named Entity integration with SMT (0.3 BLEU gain for French-English in), 0.2 BLEU gain for Arabic-English in (), 1 BLEU loss for Chinese-English in).
SMT	BLEU	Others report rather low (sometimes negative) impact of Named Entity integration with SMT (0.3 BLEU gain for French-English in), 0.2 BLEU gain for Arabic-English in (), 1 BLEU loss for Chinese-English in).
MT	APERTIUM	ML4HMT-2012 provides four translation outputs (s1 to s4) which are MT output by two RBMT systems, APERTIUM and LUCY, PB-SMT (MOSES) and HPB-SMT (MOSES), respectively.: The results of out-of-domain data cleaning compared with without cleaning.
MT	LUCY	ML4HMT-2012 provides four translation outputs (s1 to s4) which are MT output by two RBMT systems, APERTIUM and LUCY, PB-SMT (MOSES) and HPB-SMT (MOSES), respectively.: The results of out-of-domain data cleaning compared with without cleaning.
MT	BLEU	Since QE quantifies the confidence of the MT output (, this selection would roughly inline with BLEU, which selects the best performing hypothesis as a backbone.
MT	APERTIUM	ML4HMT-2012 provides four translation outputs (s1 to s4) which are MT outputs by two RBMT systems, APERTIUM and LUCY, PB-SMT (MOSES) and HPB-SMT (MOSES), respectively.
MT	LUCY	ML4HMT-2012 provides four translation outputs (s1 to s4) which are MT outputs by two RBMT systems, APERTIUM and LUCY, PB-SMT (MOSES) and HPB-SMT (MOSES), respectively.
Translation	BLEU	 Table 1: Translation quality of ML4HMT-12 submissions measured using Meteor, NIST, and BLEU  scores for language pair Spanish→English. Best system per metric printed in bold face.
Automatic speech recognition (ASR)	acoustic likelihood scores	Automatic speech recognition (ASR) technology would seem to provide the solution to automatic pronunciation error detection by its ability to decode speech into word and phone sequences and provide acoustic likelihood scores indicating the match with trained native speech models.
detecting nonnative pronunciation	F-ratios	The difference between the performances of MFCC and AP features in the task of detecting nonnative pronunciation can be understood from the values of F-ratios across the 10 speakers in.
reordering	BLEU	For reordering, we use the BLEU metric by comparing candidate reorderings with the reference reorderings that we create from the hand-alignments.
reordering	BLEU	For reordering, we use the BLEU metric by comparing candidate reorderings with the reference reorderings that we create from the hand-alignments.
POS Tagging Ana Paula	Ar l ind o Silva 1 Ir eneRod r i gues	BioPOS: Biologically Inspired Algorithms for POS Tagging Ana Paula Silva 1 Ar l ind o Silva 1 Ir eneRod r i gues
FS	precision	 Table 4: Performance of different FS machines in terms of the percentage of unclassified entries. All the  classified entries were correctly classified, yielding, as a result, a precision of 100%.
Statistical machine translation (SMT)	posterior probability Pr	Statistical machine translation (SMT) is a pattern recognition approach to machine translation which was defined by as follows: given a sentence s from a certain source language, a corresponding sentencê tin a given target language that maximises the posterior probability Pr(t|s) is to be found.
machine translation	posterior probability Pr	Statistical machine translation (SMT) is a pattern recognition approach to machine translation which was defined by as follows: given a sentence s from a certain source language, a corresponding sentencê tin a given target language that maximises the posterior probability Pr(t|s) is to be found.
segmentation	recall	Furthermore, after a close investigation of the segmentation results, we found that for the systems trained by statistical data, rule-based postprocessing is basically employed to increase recall and avert errors.
segmentation	Fmeasure	In order to solve the segmentation problems with cross-domain data, proposed novel steps for the first CIPS-SIGHAN segmentation task and achieved 0.9278 of Fmeasure based on CRFs approach.
CIPS-SIGHAN segmentation task	Fmeasure	In order to solve the segmentation problems with cross-domain data, proposed novel steps for the first CIPS-SIGHAN segmentation task and achieved 0.9278 of Fmeasure based on CRFs approach.
segmentation	OOV	However, the segmentation for microblog is difficult due to the noise problem and the OOV problem.
segmentation	accuracy	Its segmentation accuracy is depending on the quality of system dictionary.
parsing	accuracy	However, parsing accuracy on Chinese is generally significantly inferior.
Parsing TCT	Approach	Parsing TCT with a Coarse-to-fine Approach
parsing performances	precision	The parsing performances metrics convinced that the concept compound is detrimental to the parser performance even we only evaluate the phrasal constituent labels' precision and recall.
parsing performances	recall	The parsing performances metrics convinced that the concept compound is detrimental to the parser performance even we only evaluate the phrasal constituent labels' precision and recall.
parsing	F1 score	From table 2, we can see that the best parsing performance in terms of F1 score is 78.16, and the best tagging accuracy is 91.60.
parsing	accuracy	From table 2, we can see that the best parsing performance in terms of F1 score is 78.16, and the best tagging accuracy is 91.60.
tagging	accuracy	From table 2, we can see that the best parsing performance in terms of F1 score is 78.16, and the best tagging accuracy is 91.60.
parsing	F1 score	For these settings, the best parsing performances in terms of F1 score are all achieved on the 4-th split-merge round and we omit the performance achieved on other rounds.
parsing	F1	From table 3, we can see that on the one hand, 'RMS' improves parsing performance about 0.35 F1 points.
tagging	accuracy	The last issue we examine is tagging accuracy on parsing performance.
parsing	precision	The result is that parsing precision, recall and F1 boosted to 84.95, 84.44 and 84.69, respectively.
parsing	recall	The result is that parsing precision, recall and F1 boosted to 84.95, 84.44 and 84.69, respectively.
parsing	F1	The result is that parsing precision, recall and F1 boosted to 84.95, 84.44 and 84.69, respectively.
tagging	accuracy	These results illustrate that improving tagging accuracy can significantly boosting parsing performance on the Sinica Treebank.
parsing	accuracy	Our experiment gives the first result of adapting existing multilingual parsing models to the Sinica Treebank and shows that the parsing accuracy can be improved by our suggested approach.
Sentence Parsing task	precision	On Sentence Parsing task, our system performs at 44% precision, 53% recall, and F1 is 48% in the formal testing evaluation.
Sentence Parsing task	recall	On Sentence Parsing task, our system performs at 44% precision, 53% recall, and F1 is 48% in the formal testing evaluation.
Sentence Parsing task	F1	On Sentence Parsing task, our system performs at 44% precision, 53% recall, and F1 is 48% in the formal testing evaluation.
Cross Validation	Verb F1	 Table 8: Cross Validation and 12 Verb F1 results
coreference resolution	F-score	For instance, the performance of the state-of-the-art coreference resolution model still stays around 0.7 in F-score.
interpreting compound nouns	BE	For instance, some approaches to interpreting compound nouns use semantic primitives to represent the relationships between the elements in the compound (such as Levi's classes: BE, HAVE and so on).
interpreting compound nouns	HAVE	For instance, some approaches to interpreting compound nouns use semantic primitives to represent the relationships between the elements in the compound (such as Levi's classes: BE, HAVE and so on).
face detection	precision rate	Although the study of the face detection problem is out of purpose of the present paper, we can mention that the obtained general precision rate was 95% and that the undetected faces were manually added for the completeness of the dataset.
MT	BLEU	When developing complex MT systems, comparing BLEU or TER scores is not sufficient to understand what improved or what went wrong.
MT	TER scores	When developing complex MT systems, comparing BLEU or TER scores is not sufficient to understand what improved or what went wrong.
Translation	BLEU	Translation quality is measured in truecase with BLEU () on the MT08 test sets.
rule segmentation search	accuracy	We show that combining both bottom-up rule chunking and top-down rule segmentation search strategies in purely unsupervised learning of phrasal inversion transduction grammars yields significantly better translation accuracy than either strategy alone.
metaphor identification	F1-score	Our contributions in this paper are: • we annotate and release a corpus of 3872 instances for supervised metaphor classification • we are the first to use tree kernels for metaphor identification • our approach achieves an F1-score of 0.75, the best score of of all systems tested.
MWE treatment	acceptance rate	In the call for papers we solicited submissions about major challenges in the overall process of MWE treatment, both from the theoretical and the computational viewpoint, focusing on original research related to the following topics: • Manually and automatically constructed resources were long papers and 12 were short papers, and we accepted 7 long papers for oral presentation and 3 as posters: an acceptance rate of 66.6%.
IR	Precision	In IR, Precision is defined as the ratio of found relevant documents to all the retrieved documents with regards to a user's query.
identification of pronominal verbs	INDE-TERMINATION	Therefore, the identification of pronominal verbs requires linguistic knowledge to distinguish se as a CONSTITUTIVE PARTICLE from the other uses of the the pronoun se (SUBJECT INDE-TERMINATION, PASSIVE, REFLEXIVE, RECIPRO-CAL and INCHOATIVE.)
identification of pronominal verbs	PASSIVE	Therefore, the identification of pronominal verbs requires linguistic knowledge to distinguish se as a CONSTITUTIVE PARTICLE from the other uses of the the pronoun se (SUBJECT INDE-TERMINATION, PASSIVE, REFLEXIVE, RECIPRO-CAL and INCHOATIVE.)
identification of pronominal verbs	REFLEXIVE	Therefore, the identification of pronominal verbs requires linguistic knowledge to distinguish se as a CONSTITUTIVE PARTICLE from the other uses of the the pronoun se (SUBJECT INDE-TERMINATION, PASSIVE, REFLEXIVE, RECIPRO-CAL and INCHOATIVE.)
identification of pronominal verbs	RECIPRO-CAL	Therefore, the identification of pronominal verbs requires linguistic knowledge to distinguish se as a CONSTITUTIVE PARTICLE from the other uses of the the pronoun se (SUBJECT INDE-TERMINATION, PASSIVE, REFLEXIVE, RECIPRO-CAL and INCHOATIVE.)
identification of pronominal verbs	INCHOATIVE	Therefore, the identification of pronominal verbs requires linguistic knowledge to distinguish se as a CONSTITUTIVE PARTICLE from the other uses of the the pronoun se (SUBJECT INDE-TERMINATION, PASSIVE, REFLEXIVE, RECIPRO-CAL and INCHOATIVE.)
MWE	R-value	In the MWE rankings, measures of the R-value type only slightly outperform AMs.
MWE recognition	accuracy	We evaluate POS tagging accuracy and MWE recognition accuracy.
POS evaluation	accuracy	In POS evaluation, each token receives a tag in the cases of (a), (b) and (c), so the tagging accuracy is straightforwardly calculated.
MWE recognition	accuracy	MWE recognition accuracy is evaluated for the cases of (c) and (d).
MWE recognition	accuracy	Evaluation of MWE recognition accuracy is shown in precision, recall and F-measure.
MWE recognition	precision	Evaluation of MWE recognition accuracy is shown in precision, recall and F-measure.
MWE recognition	recall	Evaluation of MWE recognition accuracy is shown in precision, recall and F-measure.
MWE recognition	F-measure	Evaluation of MWE recognition accuracy is shown in precision, recall and F-measure.
Misrecognition	IN	Other examples of Misrecognition are due to zero or low frequency MWEs, whose substrings also matches shorter MWEs: "quite/RB, a few/PRP" while correct analysis is "quite a few/RB", and "the hell /RB, out of /IN" while the correct analysis is "the hell out of /RB".
Pattern Classification	precision	 Table 8: Pattern Classification Training; P: precision, R:  recall, F: F-measure, tp: true positive classifications
Pattern Classification	recall	 Table 8: Pattern Classification Training; P: precision, R:  recall, F: F-measure, tp: true positive classifications
translations	ease	As such, translations should present a similar degree of credibility, ease of understanding, and ability to engage the audience as in the source tweet-all while conforming to the 140 character limits.
coindexation	accuracy	However, coindexation is not always performed: when it is, its performance is computed separately because it is lower than accuracy for labeled/unlabeled tasks.
Sentence Class I identification	DU- sentences	 Table 3: Sentence Class I identification, regular and DU- sentences. Bold indicates a significant difference.
Emotion classification	F-measure	 Table 4: Emotion classification result (P = Precision, R = Recall, F = F-measure)
ICF Weighting	accuracy	We present a novel "Pruned ICF Weighting Coefficient," which improves the accuracy for subjectivity detection.
ICF Weighting	accuracy	We present a novel "Pruned ICF Weighting Coefficient," which improves the accuracy for subjectivity detection.
Negation handling	NOT	Documents are initially pre-processed as follows: (i) Negation handling is performed as, "NOT_" is added to every words occurring after the negation word (no, not, isn't, can't, never, couldn't, didn't, wouldn't, don't) and first punctuation mark in the sentence.
classification problem	accuracy	We also show that re-formulating the classification problem as a multi-stage cascaded classification improves the classification accuracy.
classification	accuracy	We also show that re-formulating the classification problem as a multi-stage cascaded classification improves the classification accuracy.
classification	accuracy	Finally, we also studied the effect of training data size on classification accuracy and found that more training data is beneficial in only some of the cases.
stacking ensemble	accuracy	We also studied the impact of a stacking ensemble on the overall classification accuracy and found out that it did not result in a significant improvement on the test set.
classification	accuracy	We also studied the impact of a stacking ensemble on the overall classification accuracy and found out that it did not result in a significant improvement on the test set.
classification	accuracy	We report the overall classification accuracy as our evaluation metric.
Maximizing Classification	Accuracy	Maximizing Classification Accuracy in Native Language Identification
Classification	accuracy	 Table 2: Classification accuracy results for POS n-grams  of size N using both the PTB and RASP tagset. The larger  RASP tagset performed significantly better for all N.
classification	accuracy	We describe the various features used for classification, as well as the settings of the classifier that yielded the highest accuracy.
LM adaptation	accuracy	Our experiments suggest that these LM adaptation methods can allow us to obtain considerable recognition accuracy gain with no or low human transcription cost.
Adaptation of language models (LMs)	accuracy	Adaptation of language models (LMs) to test responses is an effective method to improve recognition accuracy.
parsing	accuracy	As firstly demonstrated by, parsing systems have a drop of accuracy when tested against domain corpora outside of the data from which they were trained.
parser adaptation	accuracy	Three different methods of parser adaptation for the biomedical domain have been proposed by) who, starting from the results of unknown word rate experiments carried out on the Genia treebank, adapted a PTB-trained parser by improving the Part-Of-Speech tagging accuracy and by relying on an external domain-specific lexicon.
relation identification	Fscores	These results of TEES in the relation identification of the GRO task (Fscores between 50% and 87%) are much higher than the best results of relation identification (40% F-score) in the Bacteria Biotopes (BB) task, which is to extract relations of localization and part-of.
relation identification	F-score	These results of TEES in the relation identification of the GRO task (Fscores between 50% and 87%) are much higher than the best results of relation identification (40% F-score) in the Bacteria Biotopes (BB) task, which is to extract relations of localization and part-of.
relation identification	F-score	These results of TEES in the relation identification of the GRO task (Fscores between 50% and 87%) are much higher than the best results of relation identification (40% F-score) in the Bacteria Biotopes (BB) task, which is to extract relations of localization and part-of.
bio-event extraction task	F-score	After that,) compared different parsers and dependency representations on bio-event extraction task and obtained an F-score of 57.79% on development data sets and 56.00% on test data sets with parser ensemble.
sspG transcription	GerE	sspG transcription also requires the DNA binding protein GerE .
solver	F	For m = 4, the solver took 1 minute and 47 seconds on average per text; recall that |F | is also much larger now, compared to the experiments of the previous section.
solver	F	show the average solver times of ILPNLGAPPROX for different values of m and |F |; all the other settings are as in.
solver	F	The solver times of ILPNLGAPPROX grow approximately linearly tom and |F | and are under 0.3 seconds in all cases.
MT	BLEU	While the output of an MT system is not always perfectly grammatical, previous work has shown that secondary machine-generated references improve translation quality when only a single human reference is available when BLEU is used as an optimization criterion).
translation task	BLEUc	 Table 3: Best BLEUc results obtained on the translation task together with the LM order used when  obtaining the result compared with the best constrained Moses results in WMT12 and WMT13. The last  row compares the BLEUc result with respect to using a different LM order.
Tuning	MIRA	 Table 1: Tuning with k-best MIRA instead of MERT  (cased BLEU scores with length ratio)
Tuning	MERT	 Table 1: Tuning with k-best MIRA instead of MERT  (cased BLEU scores with length ratio)
Tuning	BLEU	 Table 1: Tuning with k-best MIRA instead of MERT  (cased BLEU scores with length ratio)
Tuning	length ratio	 Table 1: Tuning with k-best MIRA instead of MERT  (cased BLEU scores with length ratio)
SPE	BLEU score	As apparent from Table 2, marking either reliable phrases or identities is useful in our SPE setting in terms of BLEU score.
machine translation	consistency	SAIC's technology leverages hybrid machine translation, combining features of both rule-based machine and statistical machine translation for improved consistency, fluency, and accuracy of translation output.
machine translation	accuracy	SAIC's technology leverages hybrid machine translation, combining features of both rule-based machine and statistical machine translation for improved consistency, fluency, and accuracy of translation output.
MT	BLEU score	This allowed us to create a MT system that produced our primary evaluation submission with the translation speed of 18 words per second . This submission had a BLEU score of 24.2% on the Russian-toEnglish task 2 , and 27.3% on the English-to-French task.
translation	BLEU	Even if the single system with lowest BLEU are combined, the translation quality in terms of BLEU is comparable with the combination of the best single systems.
rescoring	MERT	For rescoring, we used the newstest2011 set as tuning set and re-optimized the parameters with MERT on 1000-best lists.
Translation	BLEU	 Table 3: Translation results, shown in lowercase NIST BLEU. Bold results correspond to submitted  systems.
MT	accuracy	In all cases, the effectiveness of the integration is conditioned by: i) the quality of MT, and ii) the accuracy in automatically predicting such quality.
MT evaluation	BLEU	While early works () exploited annotations obtained with automatic MT evaluation metrics like BLEU (), the current trend is to rely on human annotations, which seem to lead to more accurate models.
SMT	accuracy	We frame the shortcomings of SMT models trained on limited amounts of parallel text 1 in terms of accuracy and coverage.
SMT	coverage	We frame the shortcomings of SMT models trained on limited amounts of parallel text 1 in terms of accuracy and coverage.
SMT	accuracy	Given these deficiencies, we begin with a baseline SMT model learned from a small parallel corpus and supplement the model to improve its accuracy and coverage.
SMT	coverage	Given these deficiencies, we begin with a baseline SMT model learned from a small parallel corpus and supplement the model to improve its accuracy and coverage.
Positive Diversity	BLEU	We find that individual systems tuned on the same data to Positive Diversity can be even more diverse than systems built using different data sets, while still obtaining good BLEU scores.
feature scoring	ReliefF	Prior to training the models, feature scoring with ReliefF and Information Gain is used to choose feature sets of decent size and avoid computational complexity .
SVM regression	mean absolute error (MAE)	Parameter optimization for SVM regression models was performed with 1000 iterations of random search for which the process was set to minimize the mean absolute error (MAE) 2 . The parameters of SVR with RBF kernel (the penalty parameter C, the width of the insensitivity zone , and the RBF parameter γ) are sampled from an exponential distribution.
SVM regression	RBF parameter γ	Parameter optimization for SVM regression models was performed with 1000 iterations of random search for which the process was set to minimize the mean absolute error (MAE) 2 . The parameters of SVR with RBF kernel (the penalty parameter C, the width of the insensitivity zone , and the RBF parameter γ) are sampled from an exponential distribution.
MT evaluation	BLEU	The traditional MT evaluation metrics require reference translations in order to measure a score reflecting some aspects of its quality, e.g. the BLEU and NIST.
MT	QE	If the MT system had a QE component to mark translations as reliable or possibly erroneous, the reader would know to use information from passages marked as bad translations with caution, while still being able to trust other passages.
MT evaluation	LEPOR	In the Metrics task, we submitted two automatic MT evaluation systems nLEPOR_baseline and LEPOR_v3.1.
MT evaluation	precision	nLEPOR_baseline is an n-gram based language independent MT evaluation metric employing the factors of modified sentence length penalty, position difference penalty , n-gram precision and n-gram recall.
MT evaluation	recall	nLEPOR_baseline is an n-gram based language independent MT evaluation metric employing the factors of modified sentence length penalty, position difference penalty , n-gram precision and n-gram recall.
MT	MEANT	Recent studies (  show that tuning MT systems against MEANT more robustly improves translation adequacy, compared to tuning against BLEU or TER.
MT	BLEU	Recent studies (  show that tuning MT systems against MEANT more robustly improves translation adequacy, compared to tuning against BLEU or TER.
MT	TER	Recent studies (  show that tuning MT systems against MEANT more robustly improves translation adequacy, compared to tuning against BLEU or TER.
machine translation (MT)	BLEU	In the past decade, the progress of machine translation (MT) research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU (, which assume that a good translation is one that shares the same lexical choices as the reference translation.
MT evaluation	BLEU	In the past decade, the progress of machine translation (MT) research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU (, which assume that a good translation is one that shares the same lexical choices as the reference translation.
MT evaluation	BLEU	show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), CDER (), WER (), and TER).
MT evaluation	METEOR	show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), CDER (), WER (), and TER).
MT evaluation	WER	show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), CDER (), WER (), and TER).
MT evaluation	TER	show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), CDER (), WER (), and TER).
MT	MEANT	Recent studies ) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER.
MT	BLEU	Recent studies ) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER.
word reordering	BLEU	A large number of previous works on word reordering measured their success with generalpurpose metrics such as BLEU () or METEOR ().
word reordering	METEOR	A large number of previous works on word reordering measured their success with generalpurpose metrics such as BLEU () or METEOR ().
PSMT translation	BLEU	 Table 1: Effects of WaW reordering model and early reordering pruning on PSMT translation quality  and efficiency, compared against a hierarchical SMT baseline. Translation quality is measured with  % BLEU, METEOR, and Kendall Reordering Score: regular (KRS) and verb-specific (KRS-V). Statistically  significant differences with respect to the previous row are marked with 񮽙񮽙 at the p ≤ .05 level and 񮽙񮽙  at the p ≤ .10 level. Decoding time is measured in milliseconds per input word.
PSMT translation	METEOR	 Table 1: Effects of WaW reordering model and early reordering pruning on PSMT translation quality  and efficiency, compared against a hierarchical SMT baseline. Translation quality is measured with  % BLEU, METEOR, and Kendall Reordering Score: regular (KRS) and verb-specific (KRS-V). Statistically  significant differences with respect to the previous row are marked with 񮽙񮽙 at the p ≤ .05 level and 񮽙񮽙  at the p ≤ .10 level. Decoding time is measured in milliseconds per input word.
PSMT translation	Kendall Reordering Score	 Table 1: Effects of WaW reordering model and early reordering pruning on PSMT translation quality  and efficiency, compared against a hierarchical SMT baseline. Translation quality is measured with  % BLEU, METEOR, and Kendall Reordering Score: regular (KRS) and verb-specific (KRS-V). Statistically  significant differences with respect to the previous row are marked with 񮽙񮽙 at the p ≤ .05 level and 񮽙񮽙  at the p ≤ .10 level. Decoding time is measured in milliseconds per input word.
Sampling	acceptance rate	 Table 3: Sampling with a 4-gram LM and reaching  a 5% acceptance rate.
sentence alignment	recall	Therefore, it is critical to perform this sentence alignment step properly, ensuring both high recall (to have as much training data as possible) and high precision (to avoid noisy training data).
sentence alignment	precision	Therefore, it is critical to perform this sentence alignment step properly, ensuring both high recall (to have as much training data as possible) and high precision (to avoid noisy training data).
HMT alignment	alignment error rate 2 (AER)	We evaluate the performance of our HMT alignment model in terms of the standard alignment error rate 2 (AER) on a publicly available EnglishJapanese dataset, and compare it with the IBM Model 4 ( and HMM alignment with distance-based (HMM) and syntax-based (S-HMM) distortion models (.
translation	BLEU	By using these methods we are able to improve the translation performance by up to 0.8 BLEU points compared to a system that uses a standard DWL.
translation	BLEU	In this case we can improve the translation quality by additional 0.1 BLEU points by adding the target features.
Spelling normalization	accuracy	Spelling normalization with the Norma tool shows rather positive results even for small training samples: with only 100 tokens used for training, it achieves a normalization accuracy of 69% for the Anselm texts, and raises the score for the GerManC-GS texts by 5-10 percentage points.
Tagging	accuracy	 Table 3: Tagging accuracy on the gold-standard  normalizations (OrigP = original punctuation,  ModP = modern punctuation, NoP = no punctu- ation)
MT shared tasks	agreement	There has been a worrying trend in recent MT shared tasks -whether the evaluation was structured as ranking translations from best-to-worst, or by direct estimation of fluency and adequacy -of agreement between annotators decreasing.
translation fluency	consistency	We use human judgments of translation fluency as a test case and compare consistency levels when the conventional 5-point interval-level scale and a continuous visual analog scale (VAS) are used for human evaluation.
summarisation	accuracy	In order to carryout complex tasks such as automatic summarisation to a high degree of accuracy, it is important for systems to be able to analyse the discourse structure of texts automatically.
polarity classification	F1-scores	For polarity classification, we report F1-scores for positive and negative class.
rating prediction	Pearson correlation (r)	For rating prediction, we report Pearson correlation (r) and mean average error (MAE).
rating prediction	mean average error (MAE)	For rating prediction, we report Pearson correlation (r) and mean average error (MAE).
POS/MSD tagging	accuracy	State of the art in POS/MSD tagging and lemmatization across languages is generally achievedboth in terms of per token accuracy and speed and robustness -by statistical methods, which involve training annotation models on manually annotated corpora.
tagging	accuracy	 Table 4: Overall tagging accuracy with and without  the inflectional lexicon
alignment correctness	precision	Trying to predict alignment correctness with the help of Hun quality mark only for the whole our data set resulted in precision 0.727 and recall 0.548, which is much lower than our results presented below.
alignment correctness	recall	Trying to predict alignment correctness with the help of Hun quality mark only for the whole our data set resulted in precision 0.727 and recall 0.548, which is much lower than our results presented below.
NER	F 1 -score	also use Maximum Entropy based approach for NER in Czech achieving 79% F 1 -score.
NER	F 1 -score	KüçKüç¨Küçük and others (2009) describe a rule-based NER system for Turkish language which achieves F 1 -score of 79%.
PN recognition	precision	The accessible models for PN recognition for Polish obtain relatively good performance in terms of precision.
recognition of semantic relations between PNs	recall	However, in some NLP tasks like recognition of semantic relations between PNs, coreference resolution), machine translation) or sensitive data anonymization) the recall is much more important than the fine-grained categorization of PNs.
coreference resolution	recall	However, in some NLP tasks like recognition of semantic relations between PNs, coreference resolution), machine translation) or sensitive data anonymization) the recall is much more important than the fine-grained categorization of PNs.
MT	BLEU	We carried out evaluation of the MT quality using two automatic MT evaluation metrics: BLEU () and NIST.
MT	BLEU	We carried out evaluation of the MT quality using two automatic MT evaluation metrics: BLEU () and NIST.
classification	accuracy	 Table 6.  The metrics include classification accuracy, and
SMT	BLEU	The SMT systems are trained with the Moses toolkit (, according to the WMT 2011 guidelines . The translation performance was measured using the BLEU evaluation metric on a single reference translation.
translation	BLEU	The SMT systems are trained with the Moses toolkit (, according to the WMT 2011 guidelines . The translation performance was measured using the BLEU evaluation metric on a single reference translation.
TM including shuffled Wikipedia sentences	BLEU	On the other hand, the TM including shuffled Wikipedia sentences causes a performance drop of 0.34 BLEU points, which is statistically significant (p-value = 0.013).
DSM evaluation	accuracy or correlation)	Our approach to DSM evaluation constitutes a methodological contribution of this study: we use linear models with performance (accuracy or correlation) as a dependent variable and various model parameters as independent variables, instead of looking for optimal parameter combinations.
parse coverage	accuracy	 Table 1: Results: parse coverage & accuracy using  the top N hypotheses induced in training.
MT	accuracy	In practice, most statistical MT systems make use of manually designed rules in order to improve the MT accuracy.
MT	accuracy	In practice, most statistical MT systems make use of manually designed rules in order to improve the MT accuracy.
SMT	BLEU	They trained an SMT system Spanish-Quechua on translations of the Bible, resulting in 2.89 BLEU points.
Dependency parsers	accuracy	Dependency parsers are almost ubiquitously evaluated on their accuracy scores, these scores say nothing of the complexity and usefulness of the resulting structures.
Machine Translation	BLEU	For Machine Translation we report two automatic evaluation scores, BLEU and NIST.
Statistical Machine Translation (WMT)	BLEU	For this reason, automatic evaluation has fora longtime not been used to officially rank systems at Workshops on Statistical Machine Translation (WMT) . In our work, we presented results of automated evaluation using a single reference BLEU metrics, but we also investigated translations generated by each system using human evaluation, applying the ranking scheme used at WMT workshops to officially rank systems.
MT	BLEU	Eextrinsic evaluation was carried out on the MT quality using BLEU () and NIST).
SMT	BLEU score	The baseline SMT system, with no pre-editing, achieves an average BLEU score of 42.47 on this set.
translation	BLEU	To objectively evaluate the translation accuracy, four automatic evaluation metrics have been chosen, namely BLEU (), NIST (NIST 2002), Meteor (Denkowski and Lavie, 2011) and TER ().
translation	TER	To objectively evaluate the translation accuracy, four automatic evaluation metrics have been chosen, namely BLEU (), NIST (NIST 2002), Meteor (Denkowski and Lavie, 2011) and TER ().
translation	accuracy	The evolution of translation accuracy is depicted within.
content preservation	BLEU	The first group of approaches include human judges ratings of simplification, content preservation, and grammaticality, standard MT evaluation scores (BLEU and NIST), a variety of other automatic metrics (perplexity, precision/recall/F-measure, and edit distance).
content preservation	precision	The first group of approaches include human judges ratings of simplification, content preservation, and grammaticality, standard MT evaluation scores (BLEU and NIST), a variety of other automatic metrics (perplexity, precision/recall/F-measure, and edit distance).
MT evaluation	BLEU	The first group of approaches include human judges ratings of simplification, content preservation, and grammaticality, standard MT evaluation scores (BLEU and NIST), a variety of other automatic metrics (perplexity, precision/recall/F-measure, and edit distance).
MT evaluation	precision	The first group of approaches include human judges ratings of simplification, content preservation, and grammaticality, standard MT evaluation scores (BLEU and NIST), a variety of other automatic metrics (perplexity, precision/recall/F-measure, and edit distance).
TS	BLEU	The recent approaches considering TS as an MT task, such as Specia (2010),, and, apply standard MT evaluation techniques, such as BLEU (), NIST, and TERp ().
TS	TERp	The recent approaches considering TS as an MT task, such as Specia (2010),, and, apply standard MT evaluation techniques, such as BLEU (), NIST, and TERp ().
MT task	BLEU	The recent approaches considering TS as an MT task, such as Specia (2010),, and, apply standard MT evaluation techniques, such as BLEU (), NIST, and TERp ().
MT task	TERp	The recent approaches considering TS as an MT task, such as Specia (2010),, and, apply standard MT evaluation techniques, such as BLEU (), NIST, and TERp ().
MT evaluation	BLEU	The recent approaches considering TS as an MT task, such as Specia (2010),, and, apply standard MT evaluation techniques, such as BLEU (), NIST, and TERp ().
MT evaluation	TERp	The recent approaches considering TS as an MT task, such as Specia (2010),, and, apply standard MT evaluation techniques, such as BLEU (), NIST, and TERp ().
prediction	accuracy	to prediction accuracy, we adopted a linear kernel.
Text Analysis Conference	AESOP	The Text Analysis Conference has a separate track, named AESOP (e.g. see) aiming to test and evaluate different automatic evaluation methods of summarization systems.
summarization	coverage	Some authors reduce summarization to the maximum coverage problem) that, despite a great performance, is known as NPhard ().
reranking	F-score	The idea of reranking is motivated by analyses of the results of state-of-the-art symbolic parsers such as the Brown and Berkeley parsers, which have shown that there is still considerable room for improvement: oracle results on 50-best lists display a dramatic improvement inaccuracy (96.08% vs. 90.12% on F-score and 65.56% vs. 37.22% on exact match with the Berkeley parser).
subtree recognition	F1 score	If the autoencoder is trained only on reconstruction error and not subtree recognition, the F1 score for token classification drops from .370 (58.9% precision, 27.0% recall) to .269 (48.9% precision, 18.6% recall).
subtree recognition	precision	If the autoencoder is trained only on reconstruction error and not subtree recognition, the F1 score for token classification drops from .370 (58.9% precision, 27.0% recall) to .269 (48.9% precision, 18.6% recall).
subtree recognition	recall	If the autoencoder is trained only on reconstruction error and not subtree recognition, the F1 score for token classification drops from .370 (58.9% precision, 27.0% recall) to .269 (48.9% precision, 18.6% recall).
subtree recognition	precision	If the autoencoder is trained only on reconstruction error and not subtree recognition, the F1 score for token classification drops from .370 (58.9% precision, 27.0% recall) to .269 (48.9% precision, 18.6% recall).
subtree recognition	recall	If the autoencoder is trained only on reconstruction error and not subtree recognition, the F1 score for token classification drops from .370 (58.9% precision, 27.0% recall) to .269 (48.9% precision, 18.6% recall).
token classification	precision	If the autoencoder is trained only on reconstruction error and not subtree recognition, the F1 score for token classification drops from .370 (58.9% precision, 27.0% recall) to .269 (48.9% precision, 18.6% recall).
token classification	recall	If the autoencoder is trained only on reconstruction error and not subtree recognition, the F1 score for token classification drops from .370 (58.9% precision, 27.0% recall) to .269 (48.9% precision, 18.6% recall).
token classification	precision	If the autoencoder is trained only on reconstruction error and not subtree recognition, the F1 score for token classification drops from .370 (58.9% precision, 27.0% recall) to .269 (48.9% precision, 18.6% recall).
token classification	recall	If the autoencoder is trained only on reconstruction error and not subtree recognition, the F1 score for token classification drops from .370 (58.9% precision, 27.0% recall) to .269 (48.9% precision, 18.6% recall).
SMT	BLEU	The narrativity feature improves SMT by about 0.2 BLEU points when a factored SMT system is trained and tested on automatically labeled English-French data.
EN/FR translation	Imparfait	Narrativity is potentially relevant to EN/FR translation because three French past tenses can potentially translate an English Simple Past (SP), namely the Passé Composé (PC), Passé Simple (PS) or Imparfait (IMP).
SMT	precision	The narrativity feature can be of use to SMT only if it can be assigned with sufficient precision over a source text by entirely automatic methods.
SMT	BLEU	In testing mode, the narrativity classifier provides input to the SMT system, resulting (as we will show below) in improved tense and lexical choices for verbs, and a modest but statistically significant increase in BLEU and TER scores.
SMT	TER scores	In testing mode, the narrativity classifier provides input to the SMT system, resulting (as we will show below) in improved tense and lexical choices for verbs, and a modest but statistically significant increase in BLEU and TER scores.
SMT	TAGGED	A similar behavior was observed when using labeled connectives in combination with SMT . The lower scores of the TAGGED model maybe due to the scarcity of data (by a factor of 0.5) when verb word-forms are altered by concatenating them with the narrativity labels.
SMT.	BASELINE	In future work, we will therefore analyze in more detail which connectives and which sense labels from the PDTB should actually be included in the data to train SMT.: Translation outputs for the EN connective as, which was translated more correctly by SYSTEM2 thanks to the disambiguating sense tags compared to the BASELINE that often just produces the prepositional as -jako.
sentiment classification task	uncertainty measure	Dataset For the sentiment classification task, we use the dataset provided in), and then uses uncertainty measure to make query decision.
passage reranking	accuracy	This paper proposes passage reranking models that (i) do not require manual feature engineering and (ii) greatly preserve accuracy, when changing application domain.
classification	accuracy	Performance is measured in terms of classification accuracy, with a random baseline of 50%.
ASR outputs	Perfect ASR	For this experimentation we have considered three kinds of ASR outputs, namely, a Perfect ASR (text input), the 1-best output, and finally the nbest hypotheses (with n ranging from 1 to 20).
constituency parsing	accuracy	It has been shown that such an architecture improves constituency parsing accuracy both for Arabic (Green and Man- ning, 2010) and for Hebrew).
parsing	accuracy	Experimental results show that even a partial flexibility in predicting the morphological features helps improve the parsing accuracy statistically significantly.
parsing	LAS	A drop in such a scenario is of course expected, but the impact of no morphology in parsing is huge as compared to many other MRLs (e.g., Seeker and Kuhn (2013) report 6.3%, 2.4%, and only 0.4% absolute drops in LAS for Hungarian, Czech, and German respectively).
SFL	MOOD 2	In the remainder of this paper we briefly introduce key SFL concepts with focus on mood and transitivity along with simplified MOOD 2 and TRANSITIVITY systems in terms of which we currently consider parsing (Section 2).
SFL	TRANSITIVITY	In the remainder of this paper we briefly introduce key SFL concepts with focus on mood and transitivity along with simplified MOOD 2 and TRANSITIVITY systems in terms of which we currently consider parsing (Section 2).
Simile identification	LEXBL	 Table 2: Simile identification performance, with  respect to the 53 instances of mFlag=like annota- tion in VUAMC. LEXBL is the baseline that re- trieves all sentences that contain the preposition  like. The last column measures the number of re- trieved instances.
speaker adaptation	accuracy	We have further investigated the impact of several speaker adaptation techniques on the keyword detection accuracy.
keyword detection	accuracy	We have further investigated the impact of several speaker adaptation techniques on the keyword detection accuracy.
speaker adaptation	accuracy	Finally, several speaker adaptation techniques are applied to investigate the impact on the recognition accuracy which is the topic of Section 2.3.
recognition	accuracy	More context than ±2 frames degrades recognition accuracy on PHOENIX, capturing too much information of the following glosses.
ASR	accuracy	Despite major improvements in ASR technology over the past few decades, accuracy for unrestricted (i.e., 'dictation-style') speech remains decidedly imperfect, as described in the next section.
classification	accuracy	The work will serve as a starting point for further improvements of the classification accuracy and required annotation efforts for model estimation.
word segmentation	accuracy	However, because of word segmentation issues, online recognition accuracy can be significantly lower than offline classification accuracy.
word recognition	accuracy	The current project implemented the following three strategies for improving word recognition accuracy: (1) using symbolic aggregation approximation (SAX) representation to reduce the local variation in the original articulatory movement time-series data, (2) adding a look-back strategy to handle a situation in which two words are so close that the onset of the second word may not be accurately identified, and (3) using speaker-dependent thresholds to determine the word candidates during online recognition.
feature extraction	RP	In this paper, we introduce a robust feature extraction method based on PCA (Principal Component Analysis) and RP (Random Projection) for dysarthric speech recognition.
recognition	accuracy	The proposed method improved the recognition accuracy, but the performance was not sufficient when compared to that of persons with no disability.
UAS	statistical confidence interval	 Table 3: Experimental results for UAS on the ME- DIA database. The statistical confidence interval  at 95% with Gaussian approximation is reported.
stance classification task	accuracy	As mentioned earlier, we use the debate data collected from 'convinceme.net' to evaluate our approach on stance classification task and it beats baseline systems and a previous approach by significant margin and achieves overall accuracy of 74.4%.
CMs	estimation	Generally, CMs should be as highly scored as possible when the estimation is correct and as lowly scored as possible otherwise.
topic segmentation	Recall (R)	The performance of topic segmentation is usually measured using Recall (R), Precision (P), and F-measure (F) scores.
topic segmentation	Precision (P)	The performance of topic segmentation is usually measured using Recall (R), Precision (P), and F-measure (F) scores.
topic segmentation	F-measure (F) scores	The performance of topic segmentation is usually measured using Recall (R), Precision (P), and F-measure (F) scores.
IQ recognition	UAR	 Table 1: Results for IQ recognition: UAR and  RMSE for IQ recognition without stage two, with  error correction at stage two, and with a simple hi- erarchical approach.
IQ recognition	RMSE	 Table 1: Results for IQ recognition: UAR and  RMSE for IQ recognition without stage two, with  error correction at stage two, and with a simple hi- erarchical approach.
IQ recognition	error correction	 Table 1: Results for IQ recognition: UAR and  RMSE for IQ recognition without stage two, with  error correction at stage two, and with a simple hi- erarchical approach.
IQ recognition	error correction	 Table 1: Results for IQ recognition: UAR and  RMSE for IQ recognition without stage two, with  error correction at stage two, and with a simple hi- erarchical approach.
Spoken Dialogue Systems (SDS)	accuracy	To enable Spoken Dialogue Systems (SDS) to generate targeted clarification questions, we must first be able to identify mis-recognized words with high accuracy.
predicting individ- ual annotator's decision	POS-guess	 Table 3: Stop/Continue experiment predicting individ- ual annotator's decision with POS-guess. Accuracy, F- measure and Difference of f-measure from All feature.   †indicates statistically significant difference from the  majority baseline (p<.01)
predicting individ- ual annotator's decision	Accuracy	 Table 3: Stop/Continue experiment predicting individ- ual annotator's decision with POS-guess. Accuracy, F- measure and Difference of f-measure from All feature.   †indicates statistically significant difference from the  majority baseline (p<.01)
predicting individ- ual annotator's decision	F- measure	 Table 3: Stop/Continue experiment predicting individ- ual annotator's decision with POS-guess. Accuracy, F- measure and Difference of f-measure from All feature.   †indicates statistically significant difference from the  majority baseline (p<.01)
predicting individ- ual annotator's decision	Difference	 Table 3: Stop/Continue experiment predicting individ- ual annotator's decision with POS-guess. Accuracy, F- measure and Difference of f-measure from All feature.   †indicates statistically significant difference from the  majority baseline (p<.01)
predicting individ- ual annotator's decision	POS-guess	 Table 5: Targeted/not experiment predicting individ- ual annotator's decision with POS-guess. Accuracy, F- measure and Difference of f-measure from All feature.   †indicates statistically significant difference from the  majority baseline (p<.05)
predicting individ- ual annotator's decision	Accuracy	 Table 5: Targeted/not experiment predicting individ- ual annotator's decision with POS-guess. Accuracy, F- measure and Difference of f-measure from All feature.   †indicates statistically significant difference from the  majority baseline (p<.05)
predicting individ- ual annotator's decision	F- measure	 Table 5: Targeted/not experiment predicting individ- ual annotator's decision with POS-guess. Accuracy, F- measure and Difference of f-measure from All feature.   †indicates statistically significant difference from the  majority baseline (p<.05)
predicting individ- ual annotator's decision	Difference	 Table 5: Targeted/not experiment predicting individ- ual annotator's decision with POS-guess. Accuracy, F- measure and Difference of f-measure from All feature.   †indicates statistically significant difference from the  majority baseline (p<.05)
Segmental Context	F	The main effects concerned Reduction (F = 57.716, p<0.001, η p ²= 0.752), Rise Shape (F = 63.462, p<0.001, η p ²= 0.770), and Segmental Context (F = 10.991, p<0.001, η p ²= 0.366).
hedging	accuracy	Computational linguistics research in hedging, use of linguistic expressions whose contribution to sentence meaning is a modulation of the accuracy of the content they embed, and speculation detection has been done intensively in the domain of scholarly texts.
Dialog State Tracking Challenge	accuracy	The proposed method is evaluated in the Dialog State Tracking Challenge, where it achieves comparable performance in hypothesis accuracy to machine learning based systems.
classification	accuracy	The confusion matrix of the classification accuracy is given in Table 4.
Trust Evaluation	survival ratio	2) Trust Evaluation using Natural Language Processing Techniques: In several research articles primary focus is on survival ratio by counting the words to evaluate the quality.
NER	Recall	The performance of NER system in Malayalam is computed based on the parameters-Precision, Recall and F-Measure.
NER	F-Measure	The performance of NER system in Malayalam is computed based on the parameters-Precision, Recall and F-Measure.
UNL-ization	IAN	This paper illustrates UNL-ization of Punjabi language with the help of IAN (i.e., Interactive ANalysis) tool.
Spelling check	Fscore F	Spelling check performance is evaluated by Fscore F=2RP/(R + P).
information filtering	accuracy	This increase can be largely attributed to an increase in the information filtering accuracy, or the number of documents displayed to the user that have at least one piece of useful information.
extraction of complaints and diagnoses	F 1 score	The extraction of complaints and diagnoses is known to achieve a moderate performance (78.86 in F 1 score) by applying a simple conditional random field (CRF) based named entity recognition (NER) method.
translation	accuracy	To measure translation accuracy, we use the automatic evaluation measures of BLEU () and RIBES () measured overall sentences in the test corpus.
translation	BLEU	To measure translation accuracy, we use the automatic evaluation measures of BLEU () and RIBES () measured overall sentences in the test corpus.
translation	RIBES	To measure translation accuracy, we use the automatic evaluation measures of BLEU () and RIBES () measured overall sentences in the test corpus.
complaint and diagnosis task	recalls	For the de-identification task, the rule-based method achieved slightly higher results, while for the complaint and diagnosis task, the machine learning based method had much higher recalls and overall scores.
statistical translation	BLEU metric score	The quality of statistical translation (in terms of BLEU metric score) affects the APE module directly.
parsing	accuracy	Results show that using semi-supervised learning in the form of self-training and co-training yields only very modest improvements in parsing accuracy.
parsing	accuracy	These changes do not bring about an increase in parsing accuracy.
parsing	accuracy	We investigate the usefulness of different features and find that rich morphological features improve parsing accuracy significantly , by 7.5 percentage points with gold features and 5.6 points with predicted features.
parsing	accuracy	Using the newly developed Lithuanian Treebank, we train and evaluate a greedy transition-based parser and in particular investigate the impact of rich morphological features on parsing accuracy.
parsing	accuracy	We observe parsing accuracy on four test sets from two domains.
parsing	accuracy	We give insight into overall parser performance for Croatian and Serbian, impact of prepro-cessing for lemmas and morphosyntactic tags and influence of selected morphosyntactic features on parsing accuracy.
tokenization	accuracy	Our system achieves tokenization accuracy similar to state-of-the-art system fora standard split of the ATB part 3, and, in our experiments using ATB parts 1-3, our system achieves the highest labeled attachment, unlabeled attachment, and clitic separation figures (including pronomial clitics) for Arabic yet reported (although no other work can be compared directly).
dependency parsing	BASQUE	This paper presents a dependency parsing system, presented as BASQUE TEAM at the SPMRL'2013 Shared Task, based on the analysis of each morphological feature of the languages.
parsing	accuracy	Their experiments show that case and subordination type considerably increase parsing accuracy.
parsing	accuracy	Most data-driven dependency parsers do not use any information that is specific to the language being parsed, but it is shown that using language specific features has a crucial role in improving the overall parsing accuracy (.
SMA	accuracy	As far as Morfette is concerned, it performs on par with SMA in terms of overall accuracy but for OOV words, except for lemma prediction, SMA outperforms Morfette by significant margin.
PCFG-LA parsing	accuracy	More recently, advances in PCFG-LA parsing () and language-agnostic data-driven dependency parsing ( have made it possible to reach high accuracy with classical feature engineering techniques in addition to, or instead of, language-specific knowledge.
Dependency parsing	LAS	 Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold  show the best results per language and setting.
transitive closure merges records transitively	recall	Since transitive closure merges records transitively, it has very high recall but the precision is just 64%.
transitive closure merges records transitively	precision	Since transitive closure merges records transitively, it has very high recall but the precision is just 64%.
tagging	accuracy	We evaluate our approach on five language pairs of the Iberic peninsula, reaching up to 95% of precision on the lexicon induction task and up to 85% of tagging accuracy.
suffix analogy	accuracy	All methods except suffix analogy yield accuracy rates higher than 70%.
Token tagging	accuracy	 Table 6: Token tagging accuracy on the Catalan- Spanish datasets.
qualia extraction	PAROLE-SIMPLE-CLIPS	The qualia extraction has been addressed by means of a pattern-based approach and lexical match with an Italian generative lexicon based language resource, PAROLE-SIMPLE-CLIPS (PSC).
extracting semantic relations	precision	Pattern-based approaches for extracting semantic relations are well known in literature;;;;;, among others) and have proved highly reliable, namely in terms of precision, for extracting the different types of qualia.
Parse selection	SF	 Table 7: Parse selection results with SF.
Pattern-based extraction	precision rate	Pattern-based extraction has shown quite reasonable success characterized by a (relatively) high precision rate, but suffers from a very low recall resulting from the fact that the patterns are rare in corpora.
Pattern-based extraction	recall	Pattern-based extraction has shown quite reasonable success characterized by a (relatively) high precision rate, but suffers from a very low recall resulting from the fact that the patterns are rare in corpora.
assignment of marking labels	consistency	We present a decision tree for the assignment of marking labels, and examine the consistency of the editing decisions based on that tree.
speech recognition	accuracy	The performance of state-of-the-art speech recognition systems varies widely with domain and environment with word accuracy rates ranging from less than 70% to 98%, which often leads to misinterpretation of the user's intention.
MT evaluation	BLEU	Moreover , by applying well-established MT evaluation scores, namely BLEU and Meteor , to KOPTE learner translations that were graded by a human expert, we hope to shed light on properties (and potential shortcomings) of these scores.
MT evaluation	BLEU	In order to study the behaviour of automatic MT evaluation scores, we conducted three experiments by applying IBM BLEU () and Meteor) to a sample of KOPTE translations that were produced by translation students preparing for their final master's exams.
QE training	HTER	For QE training, we use the HTER scores between MT and its post-edited version as computed by the TERp tool.
normalisation	accuracy	We compare the results of the different methods in a multilingual evaluation including five languages, and we show that all three approaches have a positive impact on normalisation accuracy as compared to the baseline.
SW	GH similarities	The same situation occurs for SW and GH similarities on datasets of occupations and places.
optical character recognition (OCR)	error rate	State of the art optical character recognition (OCR) software currently achieve an error rate of around 1 to 10% depending on the age and the layout of the text.
translation	accuracy	Our results show improvements in translation accuracy on sentences containing either morphosyntactically constrained or unconstrained idioms.
identification of MWEs	accuracy	Within the NLP community, there is a growing interest in the identification of MWEs and their robust treatment, as this seems to improve parsing accuracy (.
parsing	accuracy	Within the NLP community, there is a growing interest in the identification of MWEs and their robust treatment, as this seems to improve parsing accuracy (.
SMT	Bleu score	In the case of SMT, German compounds are split into their constituents to decrease the number of unknown words and improve the results of evaluation measures like the Bleu score.
MWEs	prevalence	The importance of coverage for MWEs is underscored by their prevalence.
translation	BLEU score	The results indicate a slight improvement in translation of the paraphrased compound nouns, with a minor loss in overall BLEU score.
quote-tospeaker attribution	STORIES	For the first two tasks (quote-tospeaker attribution, and gender estimation) we used a dataset (STORIES) consisting of 17 children stories selected from Project Gutenberg 3 . This set of stories includes 98 unique speakers with 554 quotes assigned to them.
translation	accuracy	In both cases, the activation of the cut-off frequency improves the translation accuracy.
Translation	accuracy	 Table 2: Translation accuracy for EL→EN, using  PMG-simple with various criteria.
Translation	accuracy	 Table 4: Translation accuracy for EL→EN, using  PMG-simple with crit.4 and using CRF.
SMT	BLEU score	Our results show that on sentences containing idioms a standard SMT system achieves about half the BLEU score of the same system when applied to sentences that do not contain idioms.
translation	BLEU	The additional training data extracted from comparable corpora provided significant improvements in terms of translation quality over the baseline as measured by BLEU.
MT	BLEU	We carried out evaluation of the MT quality using four automatic MT evaluation metrics: BLEU (), METEOR (), NIST) and TER ().
MT	METEOR	We carried out evaluation of the MT quality using four automatic MT evaluation metrics: BLEU (), METEOR (), NIST) and TER ().
MT	TER	We carried out evaluation of the MT quality using four automatic MT evaluation metrics: BLEU (), METEOR (), NIST) and TER ().
MT	BLEU	We carried out evaluation of the MT quality using four automatic MT evaluation metrics: BLEU (), METEOR (), NIST) and TER ().
MT	METEOR	We carried out evaluation of the MT quality using four automatic MT evaluation metrics: BLEU (), METEOR (), NIST) and TER ().
MT	TER	We carried out evaluation of the MT quality using four automatic MT evaluation metrics: BLEU (), METEOR (), NIST) and TER ().
Improving	precision	Improving the precision of automatically constructed human-oriented translation dictionaries
Classification	accuracy	 Table 7: Classification accuracy w.r.t different size  of training set averaged over eight language pairs.
gender disambiguation task for Dutch nouns	coverage	The paper is organised as follows: Section 2 discusses linguistic aspects of the gender disambiguation task for Dutch nouns; Section 3 describes the set-up of our experiment on automatically deriving the lexicon for Dutch nouns enriched with gender information; Section 4 presents evaluation results for Precision and coverage for different frequency thresholds; Section 4 gives interpretation of the results; Section 6 discusses the development context, generalisation of our methodology for rule-based MT and some ideas for future work.
labeling	accuracy	Micro averaged f-scores for labeling accuracy were calculated based on the 1000 test folds for each model.
handling of spelling variation	precision	The most important functionality of the tool was handling of spelling variation, which greatly increased the number of phenotypes that could be identified in the records, without noticeably decreasing the precision.
abbreviation detection	accuracy	Both abbreviation detection and sentence delimitation showed an accuracy of about 93%.
meaning preservation	simplicity	The dataset contains 280 pairs of original sentences and their corresponding simplified versions annotated by humans for grammaticality, meaning preservation, and simplicity of the simplified version.
MT evaluation	TERp	Therefore, we cannot expect that any of the six MT evaluation metrics would have a significant correlation with this score (except maybe TERp and, in particular, one of its parts -'number of deletions'.
classification	accuracy	We used the WEKA toolkit () to perform our classification experiments and evaluated the classification accuracy using 10-fold cross validation.
classification	accuracy	We used the WEKA toolkit () to perform our classification experiments and evaluated the classification accuracy using 10-fold cross validation.
detecting the beginning and end of the preamble	accuracy	We evaluate segmentation performance statistically at both levels and in addition, we analyse errors in clause segmentation qualitatively; because our performance at the first level is almost perfect (i.e., for detecting the beginning and end of the preamble, the accuracy percentages are 100 and 97 and these numbers are 94 and 100 for the transition and 100 and 100 for the body text), we focus on the errors at the second level alone.
detecting abbreviations	precision	The first experiment (SCATM 1 and 1a) of detecting abbreviations achieved both high precision and recall.
detecting abbreviations	recall	The first experiment (SCATM 1 and 1a) of detecting abbreviations achieved both high precision and recall.
spelling correction	precision	The task of spelling correction (NoCM 2) performed poorly, reaching only 54.8% precision.
simplification	accuracy	Unfortunately, the evaluation of the actual simplification process is difficult, as there are no well established methods for measuring its accuracy.
candidate generation	accuracy	For candidate generation, we provide only the accuracy scores (the number of correctly normalized tokens over the total number of detected ill formed words).
parsing	coverage	The first evaluations show improvements in parsing speed, coverage, and robustness in comparison to earlier GF grammars.
translation	quality	In translation, this leads to decrease in quality, because dependencies between chunks are not detected.
tagging and parsing	FALLACY	For example, one such well-known bias from the tagging and parsing literature is what we may refer to as the WSJ FALLACY.
color selection task	Sel(ection accuracy)	 Table 2: Results for the color selection task.  Sel(ection accuracy) is frequency with which the  system was able to correctly identify the color de- scribed when paired with a random alternative.
navigation task	precision	 Table 5: Results for the navigation task. Higher is  better for all of precision, recall and F 1 .
navigation task	recall	 Table 5: Results for the navigation task. Higher is  better for all of precision, recall and F 1 .
navigation task	F 1	 Table 5: Results for the navigation task. Higher is  better for all of precision, recall and F 1 .
Summarisation	repetition	Summarisation systems can increase coherence and reduce repetition by correctly handling hyponymous words in the input text.
TSRF	End	Ina second stage, TSRF performs timestamp classification by employing models which learn "Start", "End" and "In" predictors of entities in a relationship; it computes the best 4-tuple timestamp [T1, T2, T3, T4] based on the confidence values associated to the top sentences extracted.
timestamp classification	End	Ina second stage, TSRF performs timestamp classification by employing models which learn "Start", "End" and "In" predictors of entities in a relationship; it computes the best 4-tuple timestamp [T1, T2, T3, T4] based on the confidence values associated to the top sentences extracted.
translations	precision	Its translations are low precision but high recall.
translations	recall	Its translations are low precision but high recall.
Motivating	BLEU	 Table 1: Motivating Experiment: BLEU results using the
SMT	CE: correct ed- its	 Table 1: Performance of language models on the  development set after ranking the SMT system's  10-best candidates per sentence. CE: correct ed- its, ME: missed edits, UE: unnecessary edits, P:  precision, R: recall.
SMT	UE	 Table 1: Performance of language models on the  development set after ranking the SMT system's  10-best candidates per sentence. CE: correct ed- its, ME: missed edits, UE: unnecessary edits, P:  precision, R: recall.
SMT	precision	 Table 1: Performance of language models on the  development set after ranking the SMT system's  10-best candidates per sentence. CE: correct ed- its, ME: missed edits, UE: unnecessary edits, P:  precision, R: recall.
SMT	recall	 Table 1: Performance of language models on the  development set after ranking the SMT system's  10-best candidates per sentence. CE: correct ed- its, ME: missed edits, UE: unnecessary edits, P:  precision, R: recall.
Tuning	BLEU	 Table 1: Tuning with BLEU and M 2
text correction	accuracy	The key reason that text correction is a difficult task is that even for non-native English speakers, writing accuracy is very high, as errors are very sparse.
SMT	F-β score)	First, we propose tuning the SMT systems to optimize a metric more suited to the grammar correction task (F-β score) rather than the traditional BLEU metric used for tuning language translation tasks.
SMT	BLEU	First, we propose tuning the SMT systems to optimize a metric more suited to the grammar correction task (F-β score) rather than the traditional BLEU metric used for tuning language translation tasks.
SMT	BLEU	Our system also builds upon the SMT methods and tries to address the above mentioned lacunae in two ways: • Tuning the SMT model to a metric suitable for grammar correction (i.e.F-β metric), instead of the BLEU metric.
SMT	BLEU	Our system also builds upon the SMT methods and tries to address the above mentioned lacunae in two ways: • Tuning the SMT model to a metric suitable for grammar correction (i.e.F-β metric), instead of the BLEU metric.
Repeat	Accuracy	Both human and machine holistic scores fora Repeat response are equal to: 50% · Accuracy + 25% · P ronunciation + 25% · F luency.
Repeat	F luency	Both human and machine holistic scores fora Repeat response are equal to: 50% · Accuracy + 25% · P ronunciation + 25% · F luency.
Magnetism	posttest	shows correlation of the running average of Magnetism's automatic scores with posttest.
summarization	ROUGE	The performance of automatic summarization systems is routinely evaluated using content metrics such as ROUGE (), which measures the n-gram overlap between the candidate summary and a set of reference summaries (see also for historical background).
Sentence alignment	accuracy	Sentence alignment We use accuracy as the evaluation metric.
readability classification	accuracy	We obtained a readability classification accuracy result of 71%, which approaches the performance of other models used in similar tasks.
ASR	accuracy	For the participants with AD, ASR accuracy is significantly higher in interviews (paired t(39) = 8.7, p < 0.0001, CI = [13.8, ∞]), which is expected due in large part to the closer proximity of the microphone.
ASR	paired t(39)	For the participants with AD, ASR accuracy is significantly higher in interviews (paired t(39) = 8.7, p < 0.0001, CI = [13.8, ∞]), which is expected due in large part to the closer proximity of the microphone.
ASR	accuracy	Surprisingly, ASR accuracy on participants with ASR was not significantly different than on caregivers (two-tailed heteroscedastic t(78) = −0.32, p = 0.75, CI = [−5.54, 4.0]).
ASR	accuracy	shows the mean ASR accuracy, with standard error (σ/ √ n), for each of the smallvocabulary and large-vocabulary ASR systems.: ASR accuracy (means, and std. dev.) across speakers, scenario (interviews vs. during the task), and presence of noise reduction for the small and large language models.
Phrase classification	accuracy	 Table 1. Phrase classification accuracy and  latency for all three participants.
Phrase classification	latency	 Table 1. Phrase classification accuracy and  latency for all three participants.
stimulus identification task	F-score	Finally, we show how the stimulus identification task can also be framed as a classification task, obtaining an F-score of 58.30.
emotion detection	accuracy	We develop a classifier for emotion detection that obtains an accuracy of 56.84.
POS	occurrence frequency	Therefore POS patterns will come in less variety but with higher occurrence frequency.
sentiment analysis	F-measures	It is not at all uncommon within the sentiment analysis community to evaluate a sentiment analyzer with a variety of classification accuracy or hypothesis testing scores such as F-measures, kappas or Krippendorff alphas derived from human-subject annotations, even when more extensional measures are available.
SVM	BM25	shows the performance of SVM with BM25 weighting on our Reuters evaluation set versus several baselines.
sentiment classification	accuracy	As can be observed, SVM-BM25 has the highest sentiment classification accuracy: 80.164% on average over the 10 folds.
Sentiment classification	accuracy	 Table 3: Sentiment classification accuracy (average 10-fold cross-validation), Scott's π, Krippendorff's  α, Cohen's κ and trade returns of different feature sets and term frequency weighting schemes in Exp. 3.  The same folds were used for the different representations. The non-annualized returns are presented in  columns 3-6.
Sentiment classification	trade returns	 Table 3: Sentiment classification accuracy (average 10-fold cross-validation), Scott's π, Krippendorff's  α, Cohen's κ and trade returns of different feature sets and term frequency weighting schemes in Exp. 3.  The same folds were used for the different representations. The non-annualized returns are presented in  columns 3-6.
sentiment orientation	accuracy	We did not apply stop-word removal, stemming and lemmatization to the dataset in any process in our system, because such stop-words as negation words might indicate sentiment orientation, and as pointed out by stemming and lemmatization processes could be detrimental to accuracy.
sentiment analysis	accuracy	The classical definition of strong AI suggests that a machine must be perform sentiment analysis in a manner and accuracy similar to human beings.
sentence-level sentiment analysis	accuracy	For sentence-level sentiment analysis, Moilanen and Pulman's algorithm obtained an accuracy of 65.6%.
Slang identification	precision	 Table 9: Slang identification precision. E: Emoti- con; T:TSM.
Slang identification	TSM	 Table 9: Slang identification precision. E: Emoti- con; T:TSM.
SD level classification	F- measures	 Table 4: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the average value of G F 1 , M F 1 and H  F 1 . SDTM outperforms all other methods com- pared. The difference between SDTM and FirstP  is statistically significant (p-value < 0.05 for ac- curacy, < 0.0001 for Avg F 1 ).
SD level classification	Acc	 Table 4: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the average value of G F 1 , M F 1 and H  F 1 . SDTM outperforms all other methods com- pared. The difference between SDTM and FirstP  is statistically significant (p-value < 0.05 for ac- curacy, < 0.0001 for Avg F 1 ).
SD level classification	accuracy	 Table 4: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the average value of G F 1 , M F 1 and H  F 1 . SDTM outperforms all other methods com- pared. The difference between SDTM and FirstP  is statistically significant (p-value < 0.05 for ac- curacy, < 0.0001 for Avg F 1 ).
SD level classification	F-measure	 Table 4: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the average value of G F 1 , M F 1 and H  F 1 . SDTM outperforms all other methods com- pared. The difference between SDTM and FirstP  is statistically significant (p-value < 0.05 for ac- curacy, < 0.0001 for Avg F 1 ).
SD level classification	Avg F 1	 Table 4: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the average value of G F 1 , M F 1 and H  F 1 . SDTM outperforms all other methods com- pared. The difference between SDTM and FirstP  is statistically significant (p-value < 0.05 for ac- curacy, < 0.0001 for Avg F 1 ).
Event recognition	Precision	 Table 8: Event recognition results showing Precision (%), Recall (%) and F 1 -Score (%), for the two  event types in English and Spanish.  † denotes statistical significance at p < 0.01 compared to the  baseline (User type-agnostic classifier)
Event recognition	Recall	 Table 8: Event recognition results showing Precision (%), Recall (%) and F 1 -Score (%), for the two  event types in English and Spanish.  † denotes statistical significance at p < 0.01 compared to the  baseline (User type-agnostic classifier)
Event recognition	F 1 -Score	 Table 8: Event recognition results showing Precision (%), Recall (%) and F 1 -Score (%), for the two  event types in English and Spanish.  † denotes statistical significance at p < 0.01 compared to the  baseline (User type-agnostic classifier)
DSWT	Precision	We evaluate DSWT performance using the standard evaluation metrics: Precision, Recall, and F-measure.
DSWT	Recall	We evaluate DSWT performance using the standard evaluation metrics: Precision, Recall, and F-measure.
DSWT	F-measure	We evaluate DSWT performance using the standard evaluation metrics: Precision, Recall, and F-measure.
IAA evaluation	accuracy	However, our emphasis for IAA evaluation is somewhat different than that of scoring annotation files for accuracy with regard to a gold standard.
ASD	accuracy	For example, find several significant differences in the pitch characteristics of ASD, and report that automatic classification utilizing these features achieves accuracy well above chance level.
ASD	chance	For example, find several significant differences in the pitch characteristics of ASD, and report that automatic classification utilizing these features achieves accuracy well above chance level.
WMT14 translation task	BLEU	We build Parallel FDA5 Moses SMT systems for all language pairs in the WMT14 translation task and obtain SMT performance close to the top Moses systems with an average of 3.49 BLEU points difference using significantly less resources for training and development.
SMT	BLEU	We build Parallel FDA5 Moses SMT systems for all language pairs in the WMT14 translation task and obtain SMT performance close to the top Moses systems with an average of 3.49 BLEU points difference using significantly less resources for training and development.
word alignment	BLEU	 Table 1: Comparison of word alignment tools:  MGIZA++ vs. fast_align. fast_align runs ten  times as fast and outperforms the IBM Model 4  from MGIZA++ in terms of BLEU scores.
Morphological Transforma- tions	BLEU	 Table 6: Results of Morphological Transforma- tions. We improved the statistical characteristics  of our models by reducing the number of distinct  words by 37% and managed to translate 25% of  previously untranslated words. BLEU scores were  improved by 0.14 and 0.64 points for WMT13 and  WMT14 test sets respectively.
Translation	BLEU	 Table 2: Translation results, as measured by  BLEU (Papineni et al., 2002).
query translation task	BLEU score	Our work fo-cuses on the query translation task and we achieved the highest BLEU score among the all submitted systems for the English-German intrinsic query translation evaluation.
translation	BLEU	In our experiment, the performance of translation system is measured by BLEU (%) and translation error rate -TER (%).
translation	translation error rate -	In our experiment, the performance of translation system is measured by BLEU (%) and translation error rate -TER (%).
terminology extraction	FR-EN	We deployed six different kinds of terminology extraction methods, and participated in three different tasks: FR-EN and EN-FR query tasks, and the CLIR task.
WMT2014 medical summary sentence translation constrained task	BLEU	Finally, we attend the WMT2014 medical summary sentence translation constrained task and our systems achieve the best BLEU scores for Czech-English, English-German, French-English language pairs and the second best BLEU scores for reminding pairs.
WMT2014 medical summary sentence translation constrained task	BLEU	Finally, we attend the WMT2014 medical summary sentence translation constrained task and our systems achieve the best BLEU scores for Czech-English, English-German, French-English language pairs and the second best BLEU scores for reminding pairs.
MT	BLEU scores	Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT, to assess fora pair of systems how likely a difference in BLEU scores occurred by chance.
MT	BLEU	We evaluate paired bootstrap resampling and bootstrap resampling as shown in and approximate randomization as shown in, each in combination with four automatic MT metrics: BLEU (), NIST), METEOR (Banerjee and) and TER ().
MT	METEOR	We evaluate paired bootstrap resampling and bootstrap resampling as shown in and approximate randomization as shown in, each in combination with four automatic MT metrics: BLEU (), NIST), METEOR (Banerjee and) and TER ().
MT	TER	We evaluate paired bootstrap resampling and bootstrap resampling as shown in and approximate randomization as shown in, each in combination with four automatic MT metrics: BLEU (), NIST), METEOR (Banerjee and) and TER ().
translation	recall	The main conclusion of our study is that alignments that are optimal for translation are not necessarily optimal for reordering, where pre-275 cision is of greater importance than recall.
SMT	Pearson correlations	Our SMT system is a standard PBSMT system trained on WMT13: Pearson correlations between gold standard word alignment evaluation on the link level and on translation unit level.
SMT	precision	These results confirm results from previous studies that link level measures, especially recall and weighted F-measure show some correlation with SMT quality whereas precision does not.
prediction	accuracy	Analyzing the results we observe that prediction accuracy is quite low.
MT	BLEU	By using this human translation as a reference for the other MT systems, we computed BLEU for each sentence.
SMT	BLEU	the metric task as evaluation metrics, then they were compared as metrics for tuning SMT systems to maximize the sum of expected sentence-level BLEU scores.
MT	BLEU	From the MT metrics that have been developed during the last decades, BLEU () is one of the most well-known and widely used, since it is fast and easy to use.
MT evaluation	BLEU	Various metrics exist for MT evaluation: BLEU (Papineni, 2002), METEOR (Alon Lavie, 2007), TER (Snover, 2006) etc., but are found inadequate in quite a few language settings like, for example , in case of free word order languages.
MT evaluation	METEOR	Various metrics exist for MT evaluation: BLEU (Papineni, 2002), METEOR (Alon Lavie, 2007), TER (Snover, 2006) etc., but are found inadequate in quite a few language settings like, for example , in case of free word order languages.
MT evaluation	TER	Various metrics exist for MT evaluation: BLEU (Papineni, 2002), METEOR (Alon Lavie, 2007), TER (Snover, 2006) etc., but are found inadequate in quite a few language settings like, for example , in case of free word order languages.
MT evaluation	METEOR	First, we calculated the Pearson's correlation with the human judgements for all the metrics in the current version of the ASIYA repository, including standard MT evaluation metrics, such as METEOR (), GTM (, -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER () and PER (Tillmann et al., 1997).
MT evaluation	TERp-A	First, we calculated the Pearson's correlation with the human judgements for all the metrics in the current version of the ASIYA repository, including standard MT evaluation metrics, such as METEOR (), GTM (, -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER () and PER (Tillmann et al., 1997).
MT evaluation	WER	First, we calculated the Pearson's correlation with the human judgements for all the metrics in the current version of the ASIYA repository, including standard MT evaluation metrics, such as METEOR (), GTM (, -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER () and PER (Tillmann et al., 1997).
MT evaluation	PER	First, we calculated the Pearson's correlation with the human judgements for all the metrics in the current version of the ASIYA repository, including standard MT evaluation metrics, such as METEOR (), GTM (, -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER () and PER (Tillmann et al., 1997).
SMT	BLEU	In particular, the development of BLEU () revolutionized the SMT field, allowing not only to compare two systems in away that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT, and later MIRA ( and PRO, to optimize BLEU, or an approximation thereof, directly.
MT	BLEU)	In this paper, we propose an MT metric called tBLEU (tolerant BLEU) that is based on the standard BLEU () and designed to suit better when translation into morphologically richer languages.
MT	BLEU	In this paper, we propose an MT metric called tBLEU (tolerant BLEU) that is based on the standard BLEU () and designed to suit better when translation into morphologically richer languages.
WMT tasks	METEOR	As baseline we used one of the best ranked metrics on the sentence level evaluations from previous WMT tasks -METEOR (  Kendall τ from the WMT12 (Callison-Burch et al., 2012) so the scores could be compared with other metrics on the same dataset that were reported in the proceedings of that year).
MERT tuning	BLEU	 Table 5: BLEU score comparison using different  corpora for MERT tuning. The Size row denotes  the number of sentences of each corpus, and the  EN-ZH and ZH-EN rows denote the BLEU scores  of the respective language pair and tuning dataset.
LM filtering	BLEU	 Table 4: German-English LM filtering results using different adaptation sets. The LM perplexity over  the blind test set nestest13, as well as BLEU and TER percentages are presented.
LM filtering	TER percentages	 Table 4: German-English LM filtering results using different adaptation sets. The LM perplexity over  the blind test set nestest13, as well as BLEU and TER percentages are presented.
TM filtering	BLEU	 Table 5: German-English TM filtering and weighting results using different adaptation sets. The results  are given in BLEU and TER percentages. Significance is measured over the full system (first row).
TM filtering	TER	 Table 5: German-English TM filtering and weighting results using different adaptation sets. The results  are given in BLEU and TER percentages. Significance is measured over the full system (first row).
Translation	BLEU	Translation quality is measured in truecase with BLEU and TER).
Translation	TER	Translation quality is measured in truecase with BLEU and TER).
translation	BLEU	 Table 3: Impact of mixture type on translation  quality as measured by BLEU.
coreference annotation	precision	Since coreference annotation was parsimonious in our dataset, we also manually examined a subset of the coreference relations extracted by the system for precision.
chunk representation	accuracy	Our results indicate that narrow context feature windows give the best results, but that chunk representation and minor differences in tagging quality do not have a significant impact on chunking accuracy.
classification	accuracy	Our models achieve a classification accuracy of 79% and a correlation of 0.85 when modeled as regression.
classification	correlation	Our models achieve a classification accuracy of 79% and a correlation of 0.85 when modeled as regression.
classification	exact error	After a comparison between them, we concluded that classification is more effective than regression in terms of exact error and the direction of error.
classification	accuracy	Our results show considerable improvement in classification accuracy over previously reported results and take us a step closer towards the automated assessment of Estonian learner text.
Binary Classification	Accuracy	 Table 5: Binary Classification Accuracy
Classification	Accuracy	 Table 7: Classification Accuracy with Feature Selection
answer extraction	accuracy	We also noticed that the results of the overall system is better than information retrieval results, which shows that answer extraction module improves the accuracy of the overall system.
Confused Subset Resolution (CSR)	accuracy	Hence, error analysis and diagnosis is run on the confusion matrix results, proposing the Confused Subset Resolution (CSR) method to train subclassifiers to resolve the identified confusions and automatically generate a deep network-ofnetworks composed of the main classifier and the sub-classifiers working together to offer improved accuracy system purified of the identified confusions, offering around 2% error enhancement.
NER	f-score	Although NER has been well studied in the literature, but the majority of the work primarily focuses on English in the newswire genre, with nearhuman performance (f-score≈ 93% in MUC-7).
query expansion	accuracy	This paper introduces a query expansion approach using an ontology built from Wikipedia pages in addition to other thesaurus to improve search accuracy for Arabic language.
error detection	PI	 Table 1: Pipeline precision, recall and F 1 scores.  ED: error detection, PI: punctuation insertion.
subjectivity classification	ArSenL	In subjectivity classification, ArSenL lexicons perform better than the majority baseline and outperform SIFAAT in terms of F1-score.
subjectivity classification	F1-score	In subjectivity classification, ArSenL lexicons perform better than the majority baseline and outperform SIFAAT in terms of F1-score.
sentiment classification	ArSenL	Similarly, sentiment classification experiment reveals that ArSenL lexicons produce results that are consistently better than SIFAAT and the majority baseline.
detecting subjectivity	accuracy	Results for emoticon-and lexicon-based DS show a significant performance gain over a fully supervised baseline, especially for detecting subjectivity, where we achieve 95.19% accuracy, which is a 48.47% absolute improvement over previous fully supervised results.
SSA	accuracy	It is important to mention the most prominent previous work on SSA of Arabic tweets like (AbdulMageed et al., 2012) who trained SVM classifiers on a nearly 3K manually labelled data-set to curry out two-stage binary classification attaining accuracy up to 65.87% for the sentiment classification task.
SVM classifiers	accuracy	It is important to mention the most prominent previous work on SSA of Arabic tweets like (AbdulMageed et al., 2012) who trained SVM classifiers on a nearly 3K manually labelled data-set to curry out two-stage binary classification attaining accuracy up to 65.87% for the sentiment classification task.
classification	accuracy	In the same manner as many previous NLI studies and also the NLI 2013 shared task, we report our results as classification accuracy under k-fold cross-validation, with k = 10.
SMT	B	shows the results for our SMT system when trained on the QCA parallel corpus, which was segmented using different training models of Morfessor with B = 40.
SMT	B	We achieved the best SMT score at B = 70.
Temporal relation classification	BEFORE	Temporal relation classification, which is one of the subtasks, aims to classify temporal relationships between pairs of temporal entities into one of the 14 relation types according to the TimeML specification (), e.g., BEFORE, AF-TER, DURING, and BEGINS.
Temporal relation classification	AF-TER	Temporal relation classification, which is one of the subtasks, aims to classify temporal relationships between pairs of temporal entities into one of the 14 relation types according to the TimeML specification (), e.g., BEFORE, AF-TER, DURING, and BEGINS.
Temporal relation classification	BEGINS	Temporal relation classification, which is one of the subtasks, aims to classify temporal relationships between pairs of temporal entities into one of the 14 relation types according to the TimeML specification (), e.g., BEFORE, AF-TER, DURING, and BEGINS.
genre classification	accuracy	The results show that our semi-supervised min-cut algorithm improves the overall genre classification accuracy.
genre classification	accuracy	The second question that we investigate in this paper is: could we exploit the graph structure of the web to increase genre classification accuracy?
Classification	accuracy	 Table 4: Classification accuracy of different features in genre classification. bin and nf refer to the use of  binary and normalized frequency representation of the features respectively.
Classification	bin	 Table 4: Classification accuracy of different features in genre classification. bin and nf refer to the use of  binary and normalized frequency representation of the features respectively.
Translation	GOD	Translation: Buddy you are GOD.
ASR	BioKIT	For the ASR experiments, we applied BioKIT, a dynamic one-pass decoder ().
CS prediction	accuracy	On the positive side, word embeddings and word lists have been shown to improve CS prediction accuracy, provided they have decent coverage of tokens in the test set.
detecting segments  containing French words	Precision	 Table 4: Confusion matrix for detecting segments  containing French words when English is the orig- inal language. It yields a Precision of 95.4% and a  Recall of 72.4%
detecting segments  containing French words	Recall	 Table 4: Confusion matrix for detecting segments  containing French words when English is the orig- inal language. It yields a Precision of 95.4% and a  Recall of 72.4%
detecting segments  containing English words	Precision	 Table 5: Confusion matrix for detecting segments  containing English words when French is the orig- inal language. It yields a Precision of 75% and a  Recall of 18.75%
detecting segments  containing English words	Recall	 Table 5: Confusion matrix for detecting segments  containing English words when French is the orig- inal language. It yields a Precision of 75% and a  Recall of 18.75%
translation	accuracy	In our experiments we explore the influence of three dimensions of bilingual reordering labels on translation accuracy.
MT	MEANT	Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese).
MT	BLEU	Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese).
MT	TER	Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese).
MT evaluation	BLEU	Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ).
Translation	Reordering	 Table 3: Translation quality by combination of HF-feature, Reordering, and Lexical Processing. Bold  indicates results that are not statistically significantly different from the best result (39.60 BLEU in line  4 and 79.47 RIBES in line 2).
Translation	BLEU	 Table 3: Translation quality by combination of HF-feature, Reordering, and Lexical Processing. Bold  indicates results that are not statistically significantly different from the best result (39.60 BLEU in line  4 and 79.47 RIBES in line 2).
Translation	RIBES	 Table 3: Translation quality by combination of HF-feature, Reordering, and Lexical Processing. Bold  indicates results that are not statistically significantly different from the best result (39.60 BLEU in line  4 and 79.47 RIBES in line 2).
MT	BLEU	Discriminatory Power -the correlation of rankings of four MT systems (by manual evaluation, BLEU and HMEANT) measured on a sentence and test set levels.
MT	HMEANT	Discriminatory Power -the correlation of rankings of four MT systems (by manual evaluation, BLEU and HMEANT) measured on a sentence and test set levels.
translation	accuracy	Section 6 describes our experiments on combining the model with synset ID's and baseline model to further improve the translation accuracy followed by results and observations sections.We conclude the paper with future work and conclusions.
translation	accuracy	Experiments indicate improvements in translation accuracy.
translation	accuracy	A number of different translation models of increasing complexity and translation accuracy have been developed.
translation	accuracy	A number of different translation models of increasing complexity and translation accuracy have been developed.
translation	accuracy	Thus, improvements in the translation accuracy will be more difficult to attain.
RAAM	accuracy	As with RAAM, the objective criteria for training can be adjusted to reflect accuracy on numerous different kinds of tasks, biasing the direction that vector representations evolve toward.
word alignment	accuracy	Similarly, for specific cross-lingual tasks such as word alignment, sense disambiguation, or machine translation, classifiers can simultaneously be trained in conjunction with evolving the vector representations to optimize task-specific accuracy.
machine translation	accuracy	Similarly, for specific cross-lingual tasks such as word alignment, sense disambiguation, or machine translation, classifiers can simultaneously be trained in conjunction with evolving the vector representations to optimize task-specific accuracy.
Translation	BLEU	Translation quality is measured in truecase with BLEU and TER).
Translation	TER	Translation quality is measured in truecase with BLEU and TER).
adjunct identification	accuracy	We evaluate adjunct identification accuracy using a set of 100 English and French sentences, drawn randomly from the Europarl corpus.
MOOCs	completion rates	According to the online data provided by, most MOOCs have completion rates of less than 13%.
parsing	accuracy	However, the accuracies of unsupervised methods are unacceptably low, and results from cross-lingual transfer learning show different outcomes for different pairs of languages, but, inmost cases, the parsing accuracy is still low for practical purposes.
translation	BLEU	For translation into English, a loss due to cross-translation is about 13% of BLEU and for the other translation direction about 15%.
dialect identification	MADAMIRA	We use two publicly available dialect identification tools: AIDA and MADAMIRA, to identify and replace dialectal Arabic OOV words with their modern standard Arabic (MSA) equivalents.
DST	accuracy	The DST accuracy and average reward are shown in.
classify L&W categories	accuracy	4 presents experiments on learning to automatically classify L&W categories, where we examine the the most predictive features, and the effect of annotator agreement on classification accuracy.
Interaction Quality estimation	Unweighted Average Recall (UAR)	Interaction Quality estimation is done by using three commonly used evaluation metrics: Unweighted Average Recall (UAR),.
IQ recognition	UAR	 Table 1: Results for IQ recognition of the statis- tical classifiers: UAR, κ and ρ for linear SVM,  Bayes classification and Rule Induction. σ 2 repre- sents the variances of the confidence scores.
classification	accuracy	In addition, classification accuracy was better for segments in which speakers accomplish goals with distinctive start and end points.
Dialog State Tracking Challenge	accuracy	We evaluate on the second Dialog State Tracking Challenge; together these two techniques yield highest accuracy in 2 of 3 tasks, including the most difficult and general task.
dialog state tracking	RL	In order to seethe relationship between the performance of dialog state tracking and that of the optimized policy, we applied the off-policy RL method presented in Section 3 to the outputs of each tracker for all four DSTC test datasets 2 . The summary statistics of the datasets are presented in.
Meaning Preservation	F	Meaning Preservation: The ANOVA conducted to evaluate meaning preservation for versions of simplified text also showed a highly significant effect of version on the meaning preservation score (F=17.22, p=4.55x10 -08 ).
meaning preservation	F	Meaning Preservation: The ANOVA conducted to evaluate meaning preservation for versions of simplified text also showed a highly significant effect of version on the meaning preservation score (F=17.22, p=4.55x10 -08 ).
summarization	F1 measure	 Table 2: An evaluation of summarization performance  using the F1 measure of ROUGE-1 2, and SU4
summarization	ROUGE-1	 Table 2: An evaluation of summarization performance  using the F1 measure of ROUGE-1 2, and SU4
entity filtering-relation extraction	Base	We ran experiments with three systems; the jointly learned entity filtering-relation extraction approach using imitation learning (henceforth 2stage), the one-stage classification approach using the features for both entity filtering and relation extraction (henceforth 1stage), and a baseline that for each question entity returns all candidate answers for the relation ranked by the number of times they appeared with the question entity and ignoring all other information (henceforth Base).
Word Alignment Condition	precision	Word Alignment Condition boosts both precision and purity significantly.
Word Alignment Condition	purity	Word Alignment Condition boosts both precision and purity significantly.
word alignment	precision	The word alignment condition on the other hand greatly increases both precision and purity.
word alignment	purity	The word alignment condition on the other hand greatly increases both precision and purity.
translation	WER	Of course the translation problem here is much easier, as illustrated by a baseline WER of 27.28% obtained by leaving the pre-standard texts unchanged.
predicting semantic translation adequacy	accuracy	Because of this, one of the most important factors in correctly predicting semantic translation adequacy is the accuracy of recognizing alternative lexical realizations of the same multiword expressions in semantic role fillers.
SMT	BLEU score	The small amount of high quality domain-specific terms is passed to the SMT system using the XML markup and the Fill-Up model methods, which produced a relative translation improvement up to 13% BLEU score points
machine translation	BLEU score	From the machine translation point of view, our experiments highlight the benefit of integrating bilingual terms into the SMT system, and the relative improvement in BLEU score of the Fill-Up model over the baseline and the XML markup approach.
SMT	BLEU score	From the machine translation point of view, our experiments highlight the benefit of integrating bilingual terms into the SMT system, and the relative improvement in BLEU score of the Fill-Up model over the baseline and the XML markup approach.
terminology extraction pipelines	precision	The results show the accuracy of the terminology extraction pipelines is not perfect, as their precision ranges from 27% on short texts to 83% on longer corpora for English, 24% to 31% on Chinese.
Answer classification	accuracy	All three settings were tried out for annotator 1 and 2.: Answer classification accuracy with the CoMiC system While this is an encouraging result already, the combination of basic givenness and focus performs substantially better, reaching 90.3% accuracy for annotator 1 and 89.3% for annotator 2.
Answer classification	accuracy	All three settings were tried out for annotator 1 and 2.: Answer classification accuracy with the CoMiC system While this is an encouraging result already, the combination of basic givenness and focus performs substantially better, reaching 90.3% accuracy for annotator 1 and 89.3% for annotator 2.
tagging	accuracy	We obtain a tagging accuracy of up to 89.08% using a Spanish tagger adapted to Catalan, which is 30.66% above the performance of an unadapted Spanish tagger, and 8.88% below the performance of a supervised tagger trained on annotated Catalan data.
classification	accuracy	Some improvements in terms of classification accuracy and 10-fold cross validation under the same data conditions as) are presented.
dialect prediction	accuracy	In Section 4.2, we present dialect prediction results in terms of accuracy and F-score on our two data sets.
dialect prediction	F-score	In Section 4.2, we present dialect prediction results in terms of accuracy and F-score on our two data sets.
dialect prediction	accuracy	Following previous work, we present dialect prediction results in terms of accuracy:.
dialect prediction	precision	In addition, we present dialect prediction results in terms of precision, recall, and F-score.
dialect prediction	recall	In addition, we present dialect prediction results in terms of precision, recall, and F-score.
dialect prediction	F-score	In addition, we present dialect prediction results in terms of precision, recall, and F-score.
generative classifier	accuracy	Language groups are predicted using a generative classifier with 99.99% accuracy on the five target groups.
classification of elementary classes	recall	The achieved average precision for classification of elementary classes is 32.6% and average recall is 27.5%.
SMT translation	BLEU score	Second, there is no strong correlation with SMT translation quality in terms of BLEU score.
SMT	BLEU score	Comparing to other SMT system still require running BLEU score based on the same segmentation guideline.
segmentation	precision	Compared to other similar techniques used for segmentation, there was an improvement in the precision and recall.
segmentation	recall	Compared to other similar techniques used for segmentation, there was an improvement in the precision and recall.
clause splitting	F1-score	Van used CRFs for clause splitting task with some linguistic information giving 84.09% F1-score.
NER task	percentage (PCT)	The NER task aims to identify and classify certain proper nouns into some pre-defined target entity classes such as person (PER), organization (ORG), location (LOC), temporal expressions (TIME), monetary values (MON), and percentage (PCT).
SMT translation	BLEU score	Second, there is no strong correlation with SMT translation quality in terms of BLEU score.
SMT	BLEU score	Comparing to other SMT system still require running BLEU score based on the same segmentation guideline.
syntactic parsing	accuracy	In the field of syntactic parsing, studies show that parsing accuracy is lower for longer sentences.
parsing	accuracy	In the field of syntactic parsing, studies show that parsing accuracy is lower for longer sentences.
Automatic evaluation	BLEU	For our Automatic evaluation we adopted the same technique as Specia (2010) using BLEU metric.
Splitting quality evaluation	precision	 Table 4: Splitting quality evaluation in terms of R(ecall), P(precision) and F(-measure).  Highlighting corresponds to the experiments in which our method outperforms (Koehn and Knight 2003).
Splitting quality evaluation	F(-measure)	 Table 4: Splitting quality evaluation in terms of R(ecall), P(precision) and F(-measure).  Highlighting corresponds to the experiments in which our method outperforms (Koehn and Knight 2003).
MT	accuracy	The benefits of paraphrasing are multiple: the unknown words will be reduced, the MT output will be better understandable, the accuracy of the meaning will be the same etc.
parsing	accuracy	We should note that parsing accuracy will significantly affect the result of event type classification.
classification	accuracy	Our finding shows that feature selection is capable of improving the classification accuracy only in balanced or slightly skewed situations.
aspect extraction	accuracy	Two popular review datasets were used for evaluating the system against state-of-the-art aspect extraction techniques, obtaining higher detection accuracy for both datasets.
IAC identification	IAC	A common approach for IAC identification is to assume that sentiments or polarity words are good candidates for IACs: for example, in "This MP3 player is really expensive," the word "expensive", which bears negative polarity, is also the IAC for the aspect price.
IACs	IAC	A common approach for IAC identification is to assume that sentiments or polarity words are good candidates for IACs: for example, in "This MP3 player is really expensive," the word "expensive", which bears negative polarity, is also the IAC for the aspect price.
Event detection	TP	 Table 1: Event detection as NER results. TP is for true positive, FP for false positive, and FN for false  negative.
Event detection	FP	 Table 1: Event detection as NER results. TP is for true positive, FP for false positive, and FN for false  negative.
Event detection	FN	 Table 1: Event detection as NER results. TP is for true positive, FP for false positive, and FN for false  negative.
NER	TP	 Table 1: Event detection as NER results. TP is for true positive, FP for false positive, and FN for false  negative.
NER	FP	 Table 1: Event detection as NER results. TP is for true positive, FP for false positive, and FN for false  negative.
NER	FN	 Table 1: Event detection as NER results. TP is for true positive, FP for false positive, and FN for false  negative.
POS tagging	accuracy	Our findings for POS tagging show that Morfette reaches the highest accuracy on UTS and overall on unknown words while TnT reaches the best performance for STTS and the RF-Tagger for STTSmorph.
parsing	accuracy	shows the parsing accuracy results when combining all models (1), only the models of the two best performing parsers,, and the best combination we have found by trying all possible combinations (around 32K) (3).
Tagger	accuracy	 Table 4: Tagger accuracy after voting and machine learning weighting.
parsing	accuracy	We examine the relative utility of these techniques as well as different ways to put them together to achieve maximum parsing accuracy.
parsing	accuracy	The results (see) indicate that depending on the application domain, the parsing accuracy can suffer an absolute drop of as much as 16%.
parsing	absolute	The results (see) indicate that depending on the application domain, the parsing accuracy can suffer an absolute drop of as much as 16%.
parsing	accuracy	 Table 9: The effect of the Wiktionary lexicon on parsing accuracy.
tagging	BIEOS	For the tagging scheme, we used BIEOS to mark the named entities to be extracted.
tagging	accuracy	Due to the co-occurrence constraint and transition constraint, COV model reduces the search space and improves tagging accuracy.
polarity classification	accuracy	Furthermore, in this paper the performance of polarity classification is reported in terms of accuracy.
sentence alignment	Pr	Since the alignment modes of most historical classics sentences are "one-to-many", sentence alignment can be regarded as finding the corresponding English sentences . Given a Chinese sentence, formula (1) is turned into: Wherein: Pr( ( , ) | ) M c e c is the mode probability, which denotes the probability that the alignment mode is
generative	OOV prediction	The system adopts a generative model with OOV prediction model.
parsing	accuracy	Due to the low parsing accuracy, plenty of "De" were wrongly replaced in our experiments.
WAT	accuracy	What makes WAT unique: • Open innovation platform The test data is fixed and open, so you can repeat evaluations on the same data and confirm changes in translation accuracy overtime.
WAT	accuracy	What makes WAT unique: • Open innovation platform The test data is fixed and open, so you can repeat evaluations on the same data and confirm changes in translation accuracy overtime.
parameter optimization	RIBES	We can also see that adding RIBES to the evaluation function used in parameter optimization leads to a significant increase in RIBES across all data sets.
dependency parsing	accuracy	In recent years, more and more pre-ordering research used dependency parsing for the translation of Arabic-English, English-SOV languages ( and ChineseEnglish (), because the accuracy of dependency parsing is greatly improved.
Translation	BLEU	We set distortion limits to default value 6 for all systems . Translation quality is evaluated in terms of BLEU () and RIBES (, as determined by the workshop organizers ().
Translation	RIBES	We set distortion limits to default value 6 for all systems . Translation quality is evaluated in terms of BLEU () and RIBES (, as determined by the workshop organizers ().
phrase extraction	max-phrase-length	In phrase extraction, the max-phrase-length was 7 with GoodTuring option in scoring.
WSD	accuracy	The situation of WSD is in stark contrast to the progresses made on Named-Entity Disambiguation, where performance over 80% accuracy is routinely reported.
Statistical Machine Translation evaluation	BLEU	They have found that several standard Statistical Machine Translation evaluation metrics, including BLEU, correlate moderately well with human judgment of adequacy and fluency for the string realization task.
relation interpretation	continuity	Asr and Demberg observe that discourse relations which are predictable given general cognitive biases in relation interpretation, such as continuity and causality are more likely to be expressed without an explicit discourse connective, while unpredictable (discontinuous, adversative or temporally backward) relations tend to be marked with an explicit connective.
Classification	accuracy	 Table 5: Classification accuracy for synonymy and antonymy for individual lexical and affective features.
parsing	accuracy	This system implements a tree approximation that strikes a good and linguistically plausible empirical balance between loss minimization and parsing accuracy.
Graph parsing	F 1 score (LF)	 Table 3: Graph parsing results. The radical and conservative, triangle () and square () trimmings are  compared with DFS in closed and open track evaluation scenarios. We evaluate for labeled F 1 score (LF),  and for labeled exact match (LM). The tree parsing labeled attachment score (LAS) is also provided.
Graph parsing	tree parsing labeled attachment score (LAS)	 Table 3: Graph parsing results. The radical and conservative, triangle () and square () trimmings are  compared with DFS in closed and open track evaluation scenarios. We evaluate for labeled F 1 score (LF),  and for labeled exact match (LM). The tree parsing labeled attachment score (LAS) is also provided.
text classification	precision	Like similar researches in text classification, precision, recall and f1-score are used as evaluation metrics.
text classification	recall	Like similar researches in text classification, precision, recall and f1-score are used as evaluation metrics.
text classification	f1-score	Like similar researches in text classification, precision, recall and f1-score are used as evaluation metrics.
sentiment detection	FEELING	Successful models of these tasks have many possible applications in sentiment detection, automatic summarization, argumentative agents (, and in systems that support human argumentative behavior . Our research examines FACTUAL versus FEELING argument styles, drawing on annotations provided in the Internet Argument Corpus (IAC) (.
classification	accuracy	We report classification accuracy under 10-fold cross-validation using the TOEFL11 training data and also on the test set from the 2013 shared task, shown in.
Amati	speed	In this paper we present the Amati system , which aims to help human markers improve the speed and accuracy of their marking.
Amati	accuracy	In this paper we present the Amati system , which aims to help human markers improve the speed and accuracy of their marking.
event coreference resolution	BLANC	 Table 5: Sentence template approach to event coreference resolution evaluated on the ECB+ corpus in MUC, B3,  mention-based CEAF, BLANC and CoNLL F in comparison to the singleton baseline BL.
Event Detection and Recognition (VDR)	accuracy	The Event Detection and Recognition (VDR) task in the Automatic Content Extraction 2005 evaluation) evaluate the accuracy of event arguments and multiple other event attributes.
event mention recognition	F-1	However, event mention recognition is not directly evaluated ( §3.2).) evaluate event trigger detection using a mention-wise F-1 score.
event trigger detection	F-1	However, event mention recognition is not directly evaluated ( §3.2).) evaluate event trigger detection using a mention-wise F-1 score.
multiword expression identification	F1 absolute	Recent work) has incorporated full phrase-structure trees in the process of multiword expression identification, obtaining a 36.4% F1 absolute improvement in MWE identification using a Tree-Substitution Grammar over an n-gram surface statistics baseline ().
parsing	F1 score	 Table 3: Performance of the parsing models on the  French and Dutch treebanks, with respect to parsing  results (F1 score and exact match) and the MWE-F1  score, for sentences ≤ 40 words.
parsing	exact match)	 Table 3: Performance of the parsing models on the  French and Dutch treebanks, with respect to parsing  results (F1 score and exact match) and the MWE-F1  score, for sentences ≤ 40 words.
French MWE identification	F1 score	 Table 4: French MWE identification, F1 score per  category, for sentences ≤ 40 words.
MT	BLEU	In order to evaluate the translation quality of our systems in comparison to each other and also to a baseline without any markup, we performed a standard MT evaluation using the BLEU metric.
parsing	accuracy	Knowledge about collocations and multi-word expressions (MWEs) can be beneficial for parsing, ultimately improving a parser's accuracy (.
translations	recall	The average score of our translations is high; however, the recall is low.
EGY-to-EN SMT lexical choice	word error rate (WER)	We show that our approach improves EGY-to-EN SMT lexical choice and reachs 0.6% and 0.1% reduction in word error rate (WER) and position-independent error (PER) () over the baseline respectively.
EGY-to-EN SMT lexical choice	position-independent error (PER)	We show that our approach improves EGY-to-EN SMT lexical choice and reachs 0.6% and 0.1% reduction in word error rate (WER) and position-independent error (PER) () over the baseline respectively.
SMT	BLEU score	In addition, we implement a purpose-build NE translator and integrate it in the SMT system, yielding a small but significant improvement in BLEU score.
NE translation	accuracy	 Table 2: NE translation accuracy in the development set for the baseline HSMT system.
predicate alignment	F	By collecting Chinese-English predicate-to-predicate and argument type-to-argument type alignment probabilities and iteratively improving the alignment output using these probabilities on a large unannotated parallel corpora, we improved the predicate alignment performance by 1 F point when using all automatic SRL and word alignment inputs.
sLDA	Depression (1)	 Table 4: Most extreme sLDA topics from Twitter training data (Depression (1) vs. PTSD (-1))
sLDA	PTSD	 Table 5: Most extreme sLDA topics from Twitter training data (PTSD (1) vs. Control (-1))
Dementia	Alzheimer	Current data estimate over 35.6 million people worldwide affected by dementia and this number will double by 2030 and more than triple by 2050 2 . Dementia is among the seven pri-ority mental and neurological impairments . Although dementia is a collective concept including different possible causes or diseases (vascular, Lewy bodies, frontotemporal degeneration, Alzheimer), there are broad similarities between the symptoms of all them.
suffix detection	accuracy	 Table 3: Intrinsic analysis results on suffix detection, suffix classification, and overall token accuracy.
suffix classification	accuracy	 Table 3: Intrinsic analysis results on suffix detection, suffix classification, and overall token accuracy.
ASD diagnostic	RRB	(. All major ASD diagnostic instruments require the evaluation of RRB (.
ASD	RRB	Individuals with ASD have significantly more RRB, stereotyped phrases, and idiosyncratic utterances in their conversations (.
negation detection	FSA	designed a negation detection algorithm based on syntactic patterns; similarly, implemented an FSA for automatic recognition of negation structures in Chinese medical texts, using a list of manually defined cues and the syntactic structures they appear in.
SMT	BLEU	Traditional error categories, such as the ones presented in), are mostly based on n-gram overlap between hypothesis and reference and so are the most widely used automatic evaluation metrics used in SMT (e.g. BLEU () and TER ().
SMT	TER	Traditional error categories, such as the ones presented in), are mostly based on n-gram overlap between hypothesis and reference and so are the most widely used automatic evaluation metrics used in SMT (e.g. BLEU () and TER ().
negation detection	precision	Using the above corpora we constructed five datasets: 1) The dataset available with the NegEx rule-based system, henceforth referred to as the NegCorp dataset; 2) We adapted the training set of the i2b2 assertion classification task for negation detection, the i2b2Train mod dataset; 3) The training subset of i2b2Train mod from Partners Health-  We implemented the kernels outlined in Section 3 and evaluated them within different datasets using precision, recall and F1 on ten-fold cross validation.
negation detection	recall	Using the above corpora we constructed five datasets: 1) The dataset available with the NegEx rule-based system, henceforth referred to as the NegCorp dataset; 2) We adapted the training set of the i2b2 assertion classification task for negation detection, the i2b2Train mod dataset; 3) The training subset of i2b2Train mod from Partners Health-  We implemented the kernels outlined in Section 3 and evaluated them within different datasets using precision, recall and F1 on ten-fold cross validation.
negation detection	F1	Using the above corpora we constructed five datasets: 1) The dataset available with the NegEx rule-based system, henceforth referred to as the NegCorp dataset; 2) We adapted the training set of the i2b2 assertion classification task for negation detection, the i2b2Train mod dataset; 3) The training subset of i2b2Train mod from Partners Health-  We implemented the kernels outlined in Section 3 and evaluated them within different datasets using precision, recall and F1 on ten-fold cross validation.
normalization	precision	We measure the fidelity of normalization using precision and recall.
normalization	recall	We measure the fidelity of normalization using precision and recall.
phrase induction	likelihood ratio test	Unsupervised phrase induction using likelihood ratio test, point-wise mutual information, etc., maybe used for such a task but they typically do not capture phrases formed from high frequency function words.
binary classification	precision	In binary classification problems precision and recall are very widely used.
binary classification	recall	In binary classification problems precision and recall are very widely used.
tagging	accuracy	We obtain tagging accuracy of 97.03% (about a half percent behind fully supervised models) with just 0.74% of the original training data.
crosslingual document classification (CLDC) tasks	accuracy	When tested on the often used crosslingual document classification (CLDC) tasks, our learned embeddings yield stateof-the-art performance with an accuracy of 92.7 for English to German and 91.5 for German to English.
POS tagging	accuracy	In tagging applications such as POS tagging, NER tagging and Semantic Role Labeling (SRL), this has proved quite effective in reaching state of art accuracy and reducing reliance on manually engineered feature selection ().
NER tagging	accuracy	In tagging applications such as POS tagging, NER tagging and Semantic Role Labeling (SRL), this has proved quite effective in reaching state of art accuracy and reducing reliance on manually engineered feature selection ().
Semantic Role Labeling (SRL)	accuracy	In tagging applications such as POS tagging, NER tagging and Semantic Role Labeling (SRL), this has proved quite effective in reaching state of art accuracy and reducing reliance on manually engineered feature selection ().
category detection	accuracy	Using simple vector based features, we achieve F1 scores of 79.91% for aspect term extraction , 86.75% for category detection, and the accuracy 72.39% for aspect sentiment prediction .
NER	accuracy	The majority of existing NER systems rely on the use of gazetteers to improve the system accuracy (, however, large external resources are correlated with higher performance cost.
Classification	accuracy	Classif: Classification accuracy for SDA-1 and other models shows the mean error distance for various models trained on the same dataset.
Classification	accuracy	 Table 1: Classification accuracy for SDA-1 and  other models
frame disambiguation	UI	To investigate how frame disambiguation can be accomplished at scale and with feedback, we used the frame-sorting design and UI described above in several annotation experiments.
summaries	Kappa coefficient	The results of these analyses indicate that agreement for human summaries, in terms of Kappa coefficient and ROUGE-1 measure, is low.
summaries	ROUGE-1 measure	The results of these analyses indicate that agreement for human summaries, in terms of Kappa coefficient and ROUGE-1 measure, is low.
Semantic Similarity	METEOR	In Semantic Similarity, though we only build simple model which averages the values of word alignment METEOR, BLEU and Edit Distance scores, our system still obtains better results than all three baselines and close to the top 9 http://alt.qcri.org/semeval2015/task1/data/uploads/semevalpit-2015-results.pdf three results.
Semantic Similarity	BLEU	In Semantic Similarity, though we only build simple model which averages the values of word alignment METEOR, BLEU and Edit Distance scores, our system still obtains better results than all three baselines and close to the top 9 http://alt.qcri.org/semeval2015/task1/data/uploads/semevalpit-2015-results.pdf three results.
Semantic Similarity	Edit Distance scores	In Semantic Similarity, though we only build simple model which averages the values of word alignment METEOR, BLEU and Edit Distance scores, our system still obtains better results than all three baselines and close to the top 9 http://alt.qcri.org/semeval2015/task1/data/uploads/semevalpit-2015-results.pdf three results.
Paraphrase Identification	METEOR	 Table 5: Paraphrase Identification F1-score obtained using  different classifiers on the best set of features (word/n- gram overlap + METEOR + BLEU + EditDistance).
Paraphrase Identification	BLEU	 Table 5: Paraphrase Identification F1-score obtained using  different classifiers on the best set of features (word/n- gram overlap + METEOR + BLEU + EditDistance).
LD	accuracy	LD on long messages is widely considered a solved problem as its accuracy is often found to be high with latest methods (.
prediction	accuracy	3. Taking advantage of powerful deep neural networks to increase prediction accuracy.
question classification	recall	That is why the precision for both question classification and answer detection tasks was more important than the recall.
answer detection tasks	recall	That is why the precision for both question classification and answer detection tasks was more important than the recall.
sentence formality classification task	precision	In the sentence formality classification task, we consider the informal class positive and the standard class negative for the purposes of calculating precision, recall and F-score.
sentence formality classification task	recall	In the sentence formality classification task, we consider the informal class positive and the standard class negative for the purposes of calculating precision, recall and F-score.
sentence formality classification task	F-score	In the sentence formality classification task, we consider the informal class positive and the standard class negative for the purposes of calculating precision, recall and F-score.
parsing	accuracy	The re-trained parser obtains improved parsing accuracy on a range of different data sets, including the five web domains of the English Web Treebank (EWT) () and the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB).
CRF	meaning	For the best performing CRF method, 75.4% and 44.9% (fluency and meaning) were assigned score of 2 or 3, i.e. usable as-is or with only minor corrections.
parsing	accuracy	As discussed by in the context of the Universal Stanford Dependencies which formed the basis on which UD was built, parsing accuracy has not been a major consideration in the definition of the scheme.
parsing	accuracy	In fact, a number of the design choices taken, such as the attachment of auxiliaries and prepositions as dependents rather than governors of their semantic head is known to result in a numerically worse parsing accuracy.
parsing	accuracy	Additionally, as the conversion is an automatic process, the resulting noise may have a detrimental effect on parsing accuracy as well.
tagging	accuracy	Using MIM-GOLD, tagging accuracy is considerably lower, 92.76% compared to 93.67% accuracy for IFD.
tagging	accuracy	Using MIM-GOLD, tagging accuracy is considerably lower, 92.76% compared to 93.67% accuracy for IFD.
PoS tagging	accuracy	State-of-the-art PoS tagging accuracy for Icelandic, 92.82%, was achieved by, using the Averaged Perceptron Tagger Stagger without an external lexicon.
Tagging	accuracy	 Table 1: Tagging accuracy when tagging IFD and  MIM-GOLD using 10-fold cross-validation.
classification	accuracy	 Table 5: Averaged classification accuracy when  training on datasets for individual L1s and on  mixed corpora
Disambiguation	accuracy	 Table 2: Disambiguation accuracy using a similar- ity measure.
Disambiguation	accuracy	 Table 3: Disambiguation accuracy by Part-of- Speech using a similarity measure. Overlapping  instances included in the training set.
Disambiguation	accuracy	 Table 4: Disambiguation accuracy using a classi- fier.
Disambiguation	accuracy	 Table 5: Disambiguation accuracy by Part-of- Speech using a classifier. Overlapping instances  included in the training data.
MFS prediction	accuracy	 Table 5. Accuracy of MFS prediction for words from  Set2 including accuracy of the best single features and  accuracy of the supervised algorithm trained on Set1
MFS prediction	accuracy	 Table 5. Accuracy of MFS prediction for words from  Set2 including accuracy of the best single features and  accuracy of the supervised algorithm trained on Set1
generative dependency parsing	accuracy	Although this performance is below state-of-theart discriminative models, it exceeds existing generative dependency parsing models in either accuracy, speed or both.
generative dependency parsing	speed	Although this performance is below state-of-theart discriminative models, it exceeds existing generative dependency parsing models in either accuracy, speed or both.
parsing	recall	The parsing evaluation revealed a labeled recall and precision of 73.52% and 73.79%, and an unlabeled recall and precision of 81.99% and 82.28%, respectively.
parsing	precision	The parsing evaluation revealed a labeled recall and precision of 73.52% and 73.79%, and an unlabeled recall and precision of 81.99% and 82.28%, respectively.
parsing	recall	The parsing evaluation revealed a labeled recall and precision of 73.52% and 73.79%, and an unlabeled recall and precision of 81.99% and 82.28%, respectively.
parsing	precision	The parsing evaluation revealed a labeled recall and precision of 73.52% and 73.79%, and an unlabeled recall and precision of 81.99% and 82.28%, respectively.
parsing	accuracy	Whereas a perfectly tokenized text with tagging errors degrades parsing results by less than 1%, errors in tokenization may decrease parsing accuracy as much as 5%.
tagging	accuracy	Our results reveal the strong impact of tagging accuracy especially with models trained on noisy projected data sets.
parsing	accuracy	Semi-supervised techniques gain popularity as they are able to improve parsing accuracy by exploiting unlabeled data which avoids the cost of labeling new data.
SPMRL shared task	accuracy	We have chosen the datasets as they provide smaller data sets of 5k sentences for each language of the SPMRL shared task which area good basis for our exploration for improving parsing accuracy of under-resourced languages and the shared task provides competitive results for these languages from the participants of the shared task that provides us strong accuracy scores against which we can compare our results.
parsing	accuracy	We have chosen the datasets as they provide smaller data sets of 5k sentences for each language of the SPMRL shared task which area good basis for our exploration for improving parsing accuracy of under-resourced languages and the shared task provides competitive results for these languages from the participants of the shared task that provides us strong accuracy scores against which we can compare our results.
parsing	accuracy	We improve parsing accuracy for out-of-domain texts with a self-training approach that uses confidence-based methods to select additional training samples.
parsing	accuracy	Self-training is one of these appealing techniques which improves parsing accuracy by using a parser's own annotations.
word identification	soon	series of sub-problems: word identification, partof-speech tagging, parsing, semantic analysis, and soon.
partof-speech tagging	soon	series of sub-problems: word identification, partof-speech tagging, parsing, semantic analysis, and soon.
semantic analysis	soon	series of sub-problems: word identification, partof-speech tagging, parsing, semantic analysis, and soon.
WS	accuracy	For the first process, WS, we adapt an existing tool to the target domain and achieve an enough high accuracy.
DAG estimation	accuracy	DAG estimation accuracy is measured by the F-measure of labeled arcs, which is the harmonic mean of precision and recall.
DAG estimation	F-measure	DAG estimation accuracy is measured by the F-measure of labeled arcs, which is the harmonic mean of precision and recall.
DAG estimation	precision	DAG estimation accuracy is measured by the F-measure of labeled arcs, which is the harmonic mean of precision and recall.
DAG estimation	recall	DAG estimation accuracy is measured by the F-measure of labeled arcs, which is the harmonic mean of precision and recall.
beam search parsers	accuracy	An evaluation on 10 treebanks reveals that the difference between static and non-deterministic oracles is generally insignificant for beam search parsers but that non-deterministic oracles can improve the accuracy of greedy parsers that use swap transitions.
parsing	accuracy	Since greedy parsers typically suffer from error propagation, dynamic oracles enable the parsers to learn to do "the next best thing" after having made a mistake, resulting in considerable improvements in parsing accuracy.
constituent parsing	F1 score	For constituent parsing, using a recursive neural network (RNN) got an F1 score close to the state-of-the-art on the Penn WSJ corpus.
parsing	accuracy	Methods for improving parsing accuracy typically increase the size of the grammar or even the exponent of n ().
parsing	Labeled Attachment Score (LAS)	We evaluate the parsing experiments using Labeled Attachment Score (LAS).
stacking	accuracy	Surprisingly, stacking with gold dependency trees does not reach 100% accuracy.
MT evaluation	BLEU	The standard automatic MT evaluation scores (BLEU, NIST, TER, METEOR;) do not offer specific insights about pronoun translation, but it is still useful to consider them first for an easy overview over the submitted systems.
MT evaluation	TER	The standard automatic MT evaluation scores (BLEU, NIST, TER, METEOR;) do not offer specific insights about pronoun translation, but it is still useful to consider them first for an easy overview over the submitted systems.
MT evaluation	METEOR	The standard automatic MT evaluation scores (BLEU, NIST, TER, METEOR;) do not offer specific insights about pronoun translation, but it is still useful to consider them first for an easy overview over the submitted systems.
pronoun translation	AUTO-POSTEDIT	There does not seem to be a correlation between pronoun translation quality and the choice of (a) a two-pass approach with automatic post-editing (IDIAP, AUTO-POSTEDIT) or (b) a single-pass SMT system with some form of integrated pronoun model (UU-TIEDEMANN, UU-HARDMEIER).
MT	BLEU score	In Section 5, the individual configurations of the MT system are evaluated using BLEU score and human evaluation.
MT evaluation	BLEU	For fair comparison and possible integration of our proposed document-level features, this section gives a brief introduction on two widely adopted MT evaluation metrics: BLEU () and METEOR ().
MT evaluation	METEOR	For fair comparison and possible integration of our proposed document-level features, this section gives a brief introduction on two widely adopted MT evaluation metrics: BLEU () and METEOR ().
MT	BLEU	METEOR is explicitly designed to improve the correlation with human judgments of MT quality at the sentence level and the performance of METEOR outperforms BLEU at sentence level.
pronoun translation	AR	Indeed, inaccurate pronoun translation is the result of non-existent AR when passing from the source to the target language.
pronoun translation	Anaphora Resolution (AR) step	For improving pronoun translation , an Anaphora Resolution (AR) step based on Chomsky's Binding Theory and Hobbs' algorithm has been implemented.
MT	TER	The proposed features have been observed to achieve substantial improvement of MT performance on a variety of standard test sets in terms of TER/BLEU score.
MT	BLEU score	The proposed features have been observed to achieve substantial improvement of MT performance on a variety of standard test sets in terms of TER/BLEU score.
MT task.	TER-BLEU	Our system is primarily built for an Arabic dialect to English MT task.) to minimize the score of (TER-BLEU).
translation	TER	shows that substantial improvements of translation quality, measured by both TER and BLEU, are achieved for most of the test sets.
translation	BLEU	shows that substantial improvements of translation quality, measured by both TER and BLEU, are achieved for most of the test sets.
MT	TER	 Table 2: MT performance on MT03-MT09 in terms of TER and BLEU.
MT	BLEU	 Table 2: MT performance on MT03-MT09 in terms of TER and BLEU.
MT	TER	 Table 3: MT performance on MT09 newswire and  weblog in terms of TER and BLEU.
MT	BLEU	 Table 3: MT performance on MT09 newswire and  weblog in terms of TER and BLEU.
PHI identification	IAA	Performance of PHI identification and IAA were assessed both overall for the entire corpus as well as for each of the four sub-sections.
Patient mobility	continuity	(2) Patient mobility: When patients move between services in healthcare facilities, there is increased risk during "handoffs" of problems with test result follow-up and continuity of care).
MAP	Equations	We compute MAP by Equations (2) and (3).
classification	accuracy	The main experiment involves a comparison of the 42 representations and their impact on classification accuracy.
Confusion analysis	F R&R	 Table 5: Confusion analysis: CL +b  H using F R&R
regression	mean absolute error (MAE)	shows the results for regression in terms of mean absolute error (MAE).
Agreement prediction	error reduction	 Table 3: Agreement prediction as classification  compared against the most-frequent, stratified and  uniform baseline. Datasets where the system out- performs the hardest baseline are marked in bold,  error reduction in parentheses.
alignment	accuracy	In this paper we discuss the parameters of our framework and their effects on alignment accuracy.
prediction	accuracy	shows the effect of increased data size on prediction accuracy for the two best dimensions.
prediction	accuracy	More data leads to better prediction accuracy.
NSD	precision (P)	For NSD experiments, we report precision (P), recall (R), F 1 score, and the percentage of correctly classified scopes (PCS): For classification tasks where the output is a sequence, metrics that only consider individual units regardless of their order are often insufficient.
NSD	recall (R)	For NSD experiments, we report precision (P), recall (R), F 1 score, and the percentage of correctly classified scopes (PCS): For classification tasks where the output is a sequence, metrics that only consider individual units regardless of their order are often insufficient.
NSD	F 1 score	For NSD experiments, we report precision (P), recall (R), F 1 score, and the percentage of correctly classified scopes (PCS): For classification tasks where the output is a sequence, metrics that only consider individual units regardless of their order are often insufficient.
Sentiment classifier ablation	F 1 scores	 Table 8: Sentiment classifier ablation (F 1 scores)
resolution of a word's syntactic ambiguity	accuracy	The resolution of a word's syntactic ambiguity has largely been solved in language processing by part-of-speech taggers which predict the syntactic category of words in text with high levels of accuracy.
Classification tasks	F1-scores	Classification tasks used F1-scores to measure performance.
machine translation	BLEU	We validate our approach using data selection for machine translation, and show that it maintains or improves BLEU and TER translation scores while substantially improving vocabulary coverage and reducing data selection model size.
machine translation	TER translation scores	We validate our approach using data selection for machine translation, and show that it maintains or improves BLEU and TER translation scores while substantially improving vocabulary coverage and reducing data selection model size.
SMT	MIRA	All SMT systems were tuned using MIRA () on the dev2010 data from, and then evaluated on the test2010 IWSLT test set using both BLEU () and TER ().
SMT	BLEU	All SMT systems were tuned using MIRA () on the dev2010 data from, and then evaluated on the test2010 IWSLT test set using both BLEU () and TER ().
SMT	TER	All SMT systems were tuned using MIRA () on the dev2010 data from, and then evaluated on the test2010 IWSLT test set using both BLEU () and TER ().
MT	BLEU	shows our MT results using both BLEU and TER.
MT	TER	shows our MT results using both BLEU and TER.
statistical machine translation (SMT)	BLEU	We build parallel FDA5 (ParFDA) Moses statistical machine translation (SMT) systems for all language pairs in the workshop on statistical machine translation (Bojar et al., 2015) (WMT15) translation task and obtain results close to the top with an average of 3.176 BLEU points difference using significantly less resources for building SMT systems.
WMT15) translation task	BLEU	We build parallel FDA5 (ParFDA) Moses statistical machine translation (SMT) systems for all language pairs in the workshop on statistical machine translation (Bojar et al., 2015) (WMT15) translation task and obtain results close to the top with an average of 3.176 BLEU points difference using significantly less resources for building SMT systems.
Submission system	BLEU	 Table 4: Submission system similarity measured  in uncased BLEU
translation memory	EBMT	Section 2 details the components of our system, in particular named entity extraction, translation memory, and EBMT, followed by description of 3 types of Hybrid systems and the system combination module.
Morphological Segmentation	OPUS	Morphological Segmentation and OPUS for Finnish-English Machine Translation
MT	BLEU	Several metrics have been proposed so far comparing the MT outputs to human translations (references) in terms of ngrams matches (such as BLEU)) or error rates (such as TER ().
MT	TER	Several metrics have been proposed so far comparing the MT outputs to human translations (references) in terms of ngrams matches (such as BLEU)) or error rates (such as TER ().
Tuning	BEER	 Table 1: Tuning results with BEER without bias  on WMT14 as tuning and WMT13 as test set
MT evaluation	accuracy	Current state of art MT evaluation metrics are generally able to identify problems with grammaticality of the translation but less evidently accuracy of translated semantics, e.g. incorrect translation of ambiguous words or wrong assignment of semantic roles.
MT evaluation	BLEU	The most popular MT evaluation metrics are IBM BLEU () and NIST, used not only for tuning MT systems, but also as evaluation metrics for translation shared tasks, such as the Workshop on Statistical Machine Translation (WMT).
WMT metrics shared tasks	BLEU	 Table 2: Performance of LeBLEU in recent WMT metrics shared tasks. Pearson's correlation coefficients  (system-level data) and average Kendall's tau correlation coefficients (segment-level data) for LeBLEU  with default parameters (def.), LeBLEU with optimized parameters (opt.), and topline method for the  shared task (top). For WMT 2014 data, also two reference methods are included: BLEU (ref-B) and  AMBER (ref-A).
MT evaluation	RED	We integrate a dependency-based MT evaluation metric, RED, to Moses and compare it with BLEU and METEOR in conjunction with two tuning methods: MERT and MIRA.
MT evaluation	BLEU	We integrate a dependency-based MT evaluation metric, RED, to Moses and compare it with BLEU and METEOR in conjunction with two tuning methods: MERT and MIRA.
MT evaluation	METEOR	We integrate a dependency-based MT evaluation metric, RED, to Moses and compare it with BLEU and METEOR in conjunction with two tuning methods: MERT and MIRA.
MT evaluation	MERT	We integrate a dependency-based MT evaluation metric, RED, to Moses and compare it with BLEU and METEOR in conjunction with two tuning methods: MERT and MIRA.
MT evaluation	MIRA	We integrate a dependency-based MT evaluation metric, RED, to Moses and compare it with BLEU and METEOR in conjunction with two tuning methods: MERT and MIRA.
MT evaluation	RED 1	In this paper, we integrate a reference dependency-based MT evaluation metric, RED 1 (), into the hierarchical phrasebased model) in Moses (.
MT	BLEU-tuned	We show that, consistent with MEANT-tuned systems that translate into Chinese, MEANT-tuned MT systems that translate into English also outperforms BLEU-tuned systems across commonly used MT evaluation metrics, even in BLEU.
MT	BLEU	We show that, consistent with MEANT-tuned systems that translate into Chinese, MEANT-tuned MT systems that translate into English also outperforms BLEU-tuned systems across commonly used MT evaluation metrics, even in BLEU.
MT evaluation	BLEU	We show that, consistent with MEANT-tuned systems that translate into Chinese, MEANT-tuned MT systems that translate into English also outperforms BLEU-tuned systems across commonly used MT evaluation metrics, even in BLEU.
translating into Chinese	BLEU-tuned	showed that MEANT-tuned system for translating into Chinese outperforms BLEU-tuned system across commonly used MT evaluation metrics, even in BLEU.
translating into Chinese	BLEU	showed that MEANT-tuned system for translating into Chinese outperforms BLEU-tuned system across commonly used MT evaluation metrics, even in BLEU.
MT evaluation	BLEU	showed that MEANT-tuned system for translating into Chinese outperforms BLEU-tuned system across commonly used MT evaluation metrics, even in BLEU.
MT	MEANT	In this paper, for the first time, we present MT systems for translating into English, which is tuned to a improved version of MEANT, also outperforms BLEU-tuned system across commonly used MT evaluation metrics, even in BLEU.
MT	BLEU-tuned	In this paper, for the first time, we present MT systems for translating into English, which is tuned to a improved version of MEANT, also outperforms BLEU-tuned system across commonly used MT evaluation metrics, even in BLEU.
MT	BLEU	In this paper, for the first time, we present MT systems for translating into English, which is tuned to a improved version of MEANT, also outperforms BLEU-tuned system across commonly used MT evaluation metrics, even in BLEU.
translating into English	MEANT	In this paper, for the first time, we present MT systems for translating into English, which is tuned to a improved version of MEANT, also outperforms BLEU-tuned system across commonly used MT evaluation metrics, even in BLEU.
translating into English	BLEU-tuned	In this paper, for the first time, we present MT systems for translating into English, which is tuned to a improved version of MEANT, also outperforms BLEU-tuned system across commonly used MT evaluation metrics, even in BLEU.
translating into English	BLEU	In this paper, for the first time, we present MT systems for translating into English, which is tuned to a improved version of MEANT, also outperforms BLEU-tuned system across commonly used MT evaluation metrics, even in BLEU.
MT evaluation	BLEU	In this paper, for the first time, we present MT systems for translating into English, which is tuned to a improved version of MEANT, also outperforms BLEU-tuned system across commonly used MT evaluation metrics, even in BLEU.
MT	MEANT	Subsequently, tuning MT system against the improved version of MEANT produce more adequate translations than tuning against BLEU.
MT	BLEU	Subsequently, tuning MT system against the improved version of MEANT produce more adequate translations than tuning against BLEU.
Translation	MEANT	 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
Translation	BLEU	 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
MT	MEANT	 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
MT	BLEU	 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
Translation	MEANT	 Table 4: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  test set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
Translation	BLEU	 Table 4: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  test set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
MT	MEANT	 Table 4: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  test set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
MT	BLEU	 Table 4: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  test set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
Machine translation (MT)	accuracy	Machine translation (MT) is one tool to achieve CLQA, and it is intuitively clear that a better MT system improves QA accuracy.
translation	accuracy	There is also work on optimizing translation to improve CLQA accuracy (, but these methods require a large set of translated questionanswer pairs, which may not be available in many languages.
MT	accuracy	Correspondingly, it is of interest to investigate which factors of translation output affect CLQA accuracy, which is the first step towards designing MT systems that achieve better accuracy on the task.
translation	BLEU+1	First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (), WER (), NIST) and RIBES ( and manual evaluation of acceptability ().
translation	WER	First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (), WER (), NIST) and RIBES ( and manual evaluation of acceptability ().
translation	NIST	First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (), WER (), NIST) and RIBES ( and manual evaluation of acceptability ().
translation	RIBES	First, we evaluate translation quality of each system using 4 automatic evaluation measures BLEU+1 (), WER (), NIST) and RIBES ( and manual evaluation of acceptability ().
translation among European languages	BLEU	For translation among European languages, BLEU () has strong correlation with human judgments and almost all MT papers use BLEU for evaluation of translation quality.
translation among European languages	BLEU	For translation among European languages, BLEU () has strong correlation with human judgments and almost all MT papers use BLEU for evaluation of translation quality.
tagging	accuracy	The experiment shows that the unlabeled corpus can enhance the state-of-the-art conditional random field (CRF) learning model and has potential to improve the tagging accuracy even though the margin is a little weak and not satisfying in current experiments .
translation	BLEU	Let us address what is perhaps the most frequent question: translation quality with the usual metrics BLEU and TER.
translation	TER	Let us address what is perhaps the most frequent question: translation quality with the usual metrics BLEU and TER.
ASR	WER	Most ASR systems are trained and tuned by minimizing WER, which counts word errors at the surface level.
SMT error correction	MADAMIRA	 Table 6: Character level SMT error correction  with MADAMIRA pre-process.
POS tagging	accuracy	In MD-AMIRA, POS tagging is implemented for MSA using linear classification with lexical features, and it results in reasonable accuracy within familiar contexts.
parsing	accuracy	Experimental results show a statistically significant improvement in parsing accuracy with this categorial grammar.
parsing	accuracy	Experimental results show a statistically significant gain in parsing accuracy from this moderately lexicalized grammar over parsing with a strongly lexicalized CCG.
parsing	accuracy	In order to evaluate the Chinese GCG annotations in terms of parsing accuracy, we compare the parsing performance of a latent-variable parser trained on Chinese GCG annotations with that of the same parser trained on Chinese CCG annotations.
phrase extraction	BLEU	Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with Moses (   against the BLEU evaluation metric ().
information retrieval	precision	We compute two metrics to compare them: precision at rank 1 (P@1) which indicates the percentage of terms for which the best ranked candidate is the reference one, and Mean Average Precision at rank 20 (MAP-20), a measure commonly used in information retrieval ( which averages precision at various recall rates.
information retrieval	recall	We compute two metrics to compare them: precision at rank 1 (P@1) which indicates the percentage of terms for which the best ranked candidate is the reference one, and Mean Average Precision at rank 20 (MAP-20), a measure commonly used in information retrieval ( which averages precision at various recall rates.
POS tagging	accuracies	POS tagging is generally thought of as a solved task for many languages, with per-token accuracies reaching 97%.
MT	BLEU score	All the MT systems were evaluated by singlereference, case-insensitive BLEU score using the Moses package.
Classification	accuracy	 Table 7: Classification accuracy, including a cat- egory breakdown for SICK test data. Categories  are shown with their frequencies.
SMT	BLEU  score obtained	 Table 5: BLEU scores obtained by a baseline SMT (without and with an CSLM) and a project-adapted  SMT with baseline (unadapted) CSLM and adapted CSLM. The first value in every cell is the BLEU  score obtained with respect to the reference translation of the human translator; the second one is cal- culated with respect to all the 3 references created by the professional translators (i.e. obtained by  post-edition) and an independent reference.
MT	BLEU	The system received a few months of development, but its performance is already similar to that of baseline phrase-based MT, when evaluated using BLEU, and surpasses the base-line under human qualitative assessment.
translation	accuracy	The present article reports on efforts to improve the translation accuracy of a corpus-based hybrid MT system developed using the PRESEMT methodology.
MT	BLEU	MT setups are evaluated regarding the translation quality, based on a selection of widely-used MT metrics: BLEU (), NIST),) and TER ().
MT	TER	MT setups are evaluated regarding the translation quality, based on a selection of widely-used MT metrics: BLEU (), NIST),) and TER ().
word alignment	Minimum Error Rate Training (MERT	For all experiments we used the phrase-based SMT implemented in the Moses toolkit () with the following experimental settings: • MGIZA++ implementation of IBM word alignment model 4 with grow-diagonalfinal-and heuristics for word alignment and phrase-extraction ( • Bi-directional lexicalized reordering model that considers monotone, swap and discontinuous orientations ( • Language modeling is trained using KenLM with maximum phrase length of 5 with Kneser-Ney smoothing • Minimum Error Rate Training (MERT) to tune the decoding parameters.
Machine Translation (MT)	BLEU	Machine Translation (MT) quality is typically assessed using automatic evaluation metrics such as BLEU and TER.
Machine Translation (MT)	TER	Machine Translation (MT) quality is typically assessed using automatic evaluation metrics such as BLEU and TER.
tagging	accuracy	Section 5 reports our tagging accuracy results on three state-of-the-art statistical taggers.
POS tagging	F1-score	The use of UGCNormal was also validated in a task of POS tagging, which improved from 91.35% to 93.15% inaccuracy and in a task of opinion classification, which improved the average of F1-score measures (F1-score positive and F1-score negative) from 0.736 to 0.758.
POS tagging	F1-score	The use of UGCNormal was also validated in a task of POS tagging, which improved from 91.35% to 93.15% inaccuracy and in a task of opinion classification, which improved the average of F1-score measures (F1-score positive and F1-score negative) from 0.736 to 0.758.
POS tagging	F1-score negative)	The use of UGCNormal was also validated in a task of POS tagging, which improved from 91.35% to 93.15% inaccuracy and in a task of opinion classification, which improved the average of F1-score measures (F1-score positive and F1-score negative) from 0.736 to 0.758.
Entity Linking (EL)	precision	Entity Linking (EL) refers to the task of detecting textual entity mentions and linking them to corresponding entries within knowledge bases (e.g.,, The harmonic mean of precision and recall.).
Entity Linking (EL)	recall.	Entity Linking (EL) refers to the task of detecting textual entity mentions and linking them to corresponding entries within knowledge bases (e.g.,, The harmonic mean of precision and recall.).
predicting NER tags	accuracy	 Table 1: Evaluation of the influence of the context  window size of the word embeddings on the ac- curacy of predicting NER tags using a neural net- work with an input window of five words, 500 hid- den Leaky ReLU units and dropout. All word em- beddings are inferred using negative sampling and  a Skip-gram architecture, and have a vector size  of 400. The baseline accuracy is achieved when  tagging all words of a micropost with the O-tag.
predicting NE tags	ReLUs	 Table 3: Evaluation of the influence of the input layer and hidden layer size on the accuracy/error reduc- tion when predicting NE tags. The fixed neural network is trained with dropout, word embeddings of  size 400 and ReLUs. The error reduction values are calculated using the baseline which tags all words  with an O-tag.
translation	coverage	For the first, we start with Mi-crosoft's Bing translation API but employ additional dictionary-based heuristics that significantly improve translation in both coverage and accuracy.
translation	accuracy	For the first, we start with Mi-crosoft's Bing translation API but employ additional dictionary-based heuristics that significantly improve translation in both coverage and accuracy.
RTE	TA	RTE is the task to decide whether there is an inference relation between two texts; in the case of SAS, these texts are the learner answer (LA), given by a student, and a teacher-specified target answer (TA, i.e. a sample solution).
SAS	TA	RTE is the task to decide whether there is an inference relation between two texts; in the case of SAS, these texts are the learner answer (LA), given by a student, and a teacher-specified target answer (TA, i.e. a sample solution).
Scoring short-answer questions	consistency	Scoring short-answer questions has disadvantages that may take longtime to grade and maybe an issue on consistency in scoring.
parsing	accuracy	And Li, Z., proposed to integrate the POS (part-of-speech) tagging and parsing can reduce the complexity and improve the accuracy of parsing. divided.
Grammatical error correction	F 0.5	Grammatical error correction using a phrasebased SMT system can be improved by tuning using evaluation metrics such as F 0.5 () or even a combination of different tuning algorithms).
Tuning	precision	 Table 4: Tuning result suitable to an evalua- tion score but unacceptable for its low precision  and recall.
Tuning	recall	 Table 4: Tuning result suitable to an evalua- tion score but unacceptable for its low precision  and recall.
RL	Average scores	The RL policy learned to exploit tradeoffs that while not being optimal for the SU, they are good enough for the SU to accept (the SU is: Average scores as a function of the number of episodes during training (10 runs).
argument extraction	RRSE	We focus hereon the argument extraction task, and show that we can train regressors to predict the quality of extracted arguments with RRSE values as low as .73 for some topics.
argument extraction task	IMPLICIT MARKUP hypothesis	Our approach to the argument extraction task is driven by a novel hypothesis, the IMPLICIT MARKUP hypothesis.
MMS tasks	recall score	Hebrew: 3 rd place out of 7 and out of 9 partici-  pants in MSS and MMS tasks, respectively; and the highest recall score in MMS task.
MSS task	recall score	Arabic: 5 th place out of 7 systems in MSS task, and 4 th place out of 9 participants and the highest recall score in MMS task.
summarization tasks	ROUGE	Furthermore, we summarize and discuss some unexpected negative experimental results, particularly in light of the problems posed by summarization tasks and their evaluation using ROUGE).
MMS	ROUGE	Since the dataset of MMS is composed of news articles, just selecting the headlines and first sentences will produce a strong baseline with very high ROUGE scores.
RL	WER	For the training of the RL strategy, the simulated speech recognition WER was fixed at 0.15.
IQ	IQ	Furthermore, this measure is also used to investigate if adapting the course of the dialogue to IQ also results in higher IQ values.
classification	accuracy	We investigated whether the user adaptation contributes to improving the classification accuracy.
Semantics QNR	dismissal rate	 Table 3: Semantics QNR with its dismissal rate.
Sentence (2)	INTRACELLULAR-DIGESTION	For instance, in Sentence (2), the event (INTRACELLULAR-DIGESTION) is verbalised as a nominalisation and the OBJECT role as a verb (produces).
Sentence (2)	OBJECT	For instance, in Sentence (2), the event (INTRACELLULAR-DIGESTION) is verbalised as a nominalisation and the OBJECT role as a verb (produces).
dependency parsing	accuracy	In the evaluation of dependency parsing, we obtained the dependency accuracy (the percentage of correctly analyzed dependencies out of all dependencies) and sentency accuracy (the percentage of the sentences in which all the dependencies are analyzed correctly), which were defined by.
response generation	BLEU scores	We deployed our method in response generation fora Chi-nese spoken dialogue system, obtaining results comparable to a strong baseline both in terms of BLEU scores and human evaluation.
text selection	clarity	The text selection was performed based on the text size, complexity of the ski touring sortie; clarity of the description of the sortie, linguistic quality, and finally the number of protagonists of the sortie and the level of expertise shown in the narration of the sortie.
Generating Image Descriptions	Evaluation	Generating Image Descriptions with Gold Standard Visual Inputs: Motivation, Evaluation and Baselines
concept selection	precision	We can infer from the results that (i) using prior knowledge on the ordering of concepts (i.e. bigrams) is helpful for concept selection; (ii) frequency of concepts (i.e. unigrams) are helpful when there are only one or two instances to be described, possibly because the remaining objects are not mentioned as frequently as the main actors; (iii) visual cues are helpful for concept selection, although the precision is reduced ask increases.
concept selection	precision	We can infer from the results that (i) using prior knowledge on the ordering of concepts (i.e. bigrams) is helpful for concept selection; (ii) frequency of concepts (i.e. unigrams) are helpful when there are only one or two instances to be described, possibly because the remaining objects are not mentioned as frequently as the main actors; (iii) visual cues are helpful for concept selection, although the precision is reduced ask increases.
translation recommendation	translation error rate (TER	proposed a simple approach called β-combination, which simply selects machine translation when there is no translation memory proposal with a fuzzy match score above a given threshold β, which can be tuned. and approach this problem, which they call translation recommendation, by training a classifier which selects which of the two, TM(s i ) or MT(s i ), gets the lowest value for an approximate indicator of effort, called translation error rate (TER, ().
SMT	BLEU	Our model out-performs a baseline (standard hierarchical SMT) by 0.78 BLEU points absolute, statistically significant at p = 0.01.
SMT	ULC	The final document-level translator incorporating the semantic model outperforms the basic Docent (without semantics) and also performs slightly over a standard sentence-level SMT system in terms of ULC (the average of a set of standard automatic evaluation metrics for MT).
document-level prediction	BLEU	Recent work has looked into document-level prediction) using automatic metrics such as BLEU () and TER () as quality labels.
document-level prediction	TER	Recent work has looked into document-level prediction) using automatic metrics such as BLEU () and TER () as quality labels.
MT output	O	While these authors ask informants to fill gaps in MT output, and O' ask informants to fill gaps in the reference (human) translation.
Translation memories (TM)	consistency	Translation memories (TM) are widely used in the localization industry to improve consistency and speed of human translation.
translation memories	repetition	Since translation memories are most effective on text that has a certain amount of repetition, we evaluate our approach on typical localization data, from the IT, legal and intellectual property domains 3).
MT	BLEU	Systems 5a/b are based on the same MT output; however, 5a fares better in this evaluation even though 5b had a higher BLEU score.
MT	BLEU	The method allows us to get cleaner parallel corpora, smaller statistical models, and faster MT training, but this does not always guarantee higher BLEU scores.
MT	BLEU score	For the MT evaluation we trained an SMT system with the original EU Bookshop corpus and noted the BLEU score.
SMT	BLEU score	For the MT evaluation we trained an SMT system with the original EU Bookshop corpus and noted the BLEU score.
MT	BLEU score	The BLEU score for both the original and cleaned MT systems was nearly identical with the cleaned corpus having a slightly lower BLEU score than the original.
SMT	BLEU	However it is known that BLEU is biased towards SMT systems (.: BLEU scores for SMT and RMT  Three testers were used, all of them good speakers of English with translation background.
SMT	RMT	It can be seen that testers evaluate the SMT somewhat between 'mostly' and 'partially' fluent / comprehensible, and the RMT close to 'mostly' fluent / comprehensible.
SMT	BLEU	For SMT improvement, () report improvements between 8.6 and 16.8 BLEU (relative) for domain adaptation; results here are inline with these findings.
translation	BLEU	We calculated automatic evaluation scores for the translation results by applying two popular metrics: BLEU () and RIBES (.
translation	RIBES	We calculated automatic evaluation scores for the translation results by applying two popular metrics: BLEU () and RIBES (.
MT reranking	BLEU	Experiments reconfirmed results from previous work stating that neural MT reranking provides a large gain in objective evaluation measures such as BLEU, and also confirmed for the first time that these results also carryover to manual evaluation.
MT reranking	BLEU score	While neural MT reranking has been noted to improve traditional systems with respect to BLEU score in previous work), to our knowledge this is the first work that notes that these gains also carryover convincingly to human evaluation scores.
JE translation	DEP-REO	On JE translation, the DEP-REO has a more obvious improvement than REV-REO, although the BLEU scores of the two approaches are nearly the same.
JE translation	REV-REO	On JE translation, the DEP-REO has a more obvious improvement than REV-REO, although the BLEU scores of the two approaches are nearly the same.
JE translation	BLEU	On JE translation, the DEP-REO has a more obvious improvement than REV-REO, although the BLEU scores of the two approaches are nearly the same.
SMT	SPE	In JPOko-ja, SMT, SPE and COMB show very high performances which are close to 70 BLEU, and SMT with reranking achieves the highest BLEU and RIBES scores.
SMT	COMB	In JPOko-ja, SMT, SPE and COMB show very high performances which are close to 70 BLEU, and SMT with reranking achieves the highest BLEU and RIBES scores.
SMT	BLEU	In JPOko-ja, SMT, SPE and COMB show very high performances which are close to 70 BLEU, and SMT with reranking achieves the highest BLEU and RIBES scores.
SMT	BLEU	In JPOko-ja, SMT, SPE and COMB show very high performances which are close to 70 BLEU, and SMT with reranking achieves the highest BLEU and RIBES scores.
SMT	RIBES	In JPOko-ja, SMT, SPE and COMB show very high performances which are close to 70 BLEU, and SMT with reranking achieves the highest BLEU and RIBES scores.
SMT	BLEU	On the other hand, more than half of translations selected from SMT in JPOzh-ja and JPOko-ja. shows the translation examples that COMB achieves better results than SPE with reranking in sentence-level BLEU.
SPE	BLEU	In all systems including SPE, hierarchical phrase-based model improves about 0.4 BLEU.
MT	BLEU	Previous studies on the schism between BLEU and manual evaluation highlighted the poor correlation between MT systems with low BLEU scores and high manual evaluation scores.
Asian Translation 2015 (WAT 2015)	BLEU	In this paper we demonstrate, via our submission to the Workshop on Asian Translation 2015 (WAT 2015), a patent translation system with very high BLEU and RIBES scores and very poor human judgement scores.
Asian Translation 2015 (WAT 2015)	RIBES	In this paper we demonstrate, via our submission to the Workshop on Asian Translation 2015 (WAT 2015), a patent translation system with very high BLEU and RIBES scores and very poor human judgement scores.
phrase extraction	MERT	Later we rectified the encoding problem by using KoNLPy and re-ran the alignment, phrase extraction, MERT and decoding, hence the submission name, Unicode2String, i.e. the system reported in  • Minor MT evaluation metric differences not reflecting major translation inadequacy Each of these failures contributes to an increased amount of disparity between the automatic translation metric improvements and human judgement scores.
translation	BLEU score	In terms of translation performance, the BLEU score and RIBES score on the test data achieved by dep2str system are higher than the baseline system by 0.62 and 0.31 respectively.
translation	RIBES score	In terms of translation performance, the BLEU score and RIBES score on the test data achieved by dep2str system are higher than the baseline system by 0.62 and 0.31 respectively.
Translations	BLEU	Translations were evaluated using BLEU) and RIBES (.
Translations	RIBES	Translations were evaluated using BLEU) and RIBES (.
MeCab Japanese word segmentation	BLEU	 Table 1: Official evaluation results in Pair- wise Cloudsourcing Evaluation scores (Human),  RIBES, and BLEU. RIBES and BLEU are based  on MeCab Japanese word segmentation. Scores in  bold are the best ones.  System  Human RIBES BLEU  Organizer PBMT  n/a  0.781 0.382  Organizer T2S  20.75  0.814 0.394  Ours rule-based  16.25  0.822 0.406  Ours data-driven  8.00  0.812 0.399
CALL	accuracy	Recent algorithm refinements for CALL applications suggest that the accuracy of GOP results can be greatly improved, as in Forced-aligned GOP measurements (F-GOP).
stuttering detection	accuracies	Chee et al. provided an overview of automatic stuttering detection, emphasizing its difficulty across a number of classification methods., e.g., implemented artificial neural networks (ANNs) and 'rough sets' to detect three types of 'stuttering': stop-gaps, vowel prolongations, and syllable repetitions, obtaining accuracies up to 73.25% with ANNs and 91% with rough sets.
speech recognition	accuracy	First, articulatory data have been successfully used to improve the speech recognition accuracy.
diagnosis of AD	Verbal Fluency test	Another type of test also commonly applied by therapists in the diagnosis of AD is the Verbal Fluency test.
MMSE	mean absolute error (MAE)	We use a bivari-ate dynamic Bayes net to represent the longitudinal progression of observed linguistic features and MMSE scores overtime, and obtain a mean absolute error (MAE) of 3.83 in predicting MMSE, comparable to within-subject interrater standard deviation of 3.9 to 4.8 [1].
translations	errors	The quality of the translations using the first method is high and the errors are relatively few.
CAT	penaltizing	To help the translator, CAT tools suggest or highlight all the differences or similarities between the sentences, penaltizing as well the match percent in some cases.
CAT	match percent	To help the translator, CAT tools suggest or highlight all the differences or similarities between the sentences, penaltizing as well the match percent in some cases.
TM expansion	METEOR metric	The asset of the TM expansion methods were evaluated by the pre-translation analysis of widely used MemoQ CAT system and the METEOR metric was used for measuring the quality of fully expanded new translation segments.
Terminology and computer-aided translation tools (CAT)	consistency	Terminology and computer-aided translation tools (CAT) are among the most widely used software that professional translators use on a regular basis to increase their productivity and also improve consistency in translation.
coreference resolution	MUC score	We evaluate all models in terms of two standard evaluation measures for coreference resolution -MUC score and B 3 score.
coreference resolution	B 3 score	We evaluate all models in terms of two standard evaluation measures for coreference resolution -MUC score and B 3 score.
coreference evaluation	BLANC	We show that the widely used met-rics for coreference evaluation (B 3 , MUC, CEAF, BLANC) do not reflect the real performance when dealing with the task of semantic relations recognition between named entities.
fuzzy extraction task	SREP-PAT-FUZZY-2	presents the precision-recall curves for the fuzzy extraction task, where SREP-PAT-FUZZY-2 curve corresponds to a variant of fuzzy matching, in which relation phrase may also slightly differ from the relation phrase in the test corpus.
AL	accuracy	The purpose of AL is to reduce the labeling effort, i.e., to achieve a satisfactory level of accuracy with a smaller number of training instances.
labeling	accuracy	The purpose of AL is to reduce the labeling effort, i.e., to achieve a satisfactory level of accuracy with a smaller number of training instances.
tagging	accuracy	The results show that the tagging accuracy increases from 88% to 94%.
classification	accuracy	They analysed in depth how the cost parameter influenced the classification results, and reported an overall accuracy over 95% after fixing a bug.
Language prediction test	accuracy	 Table 3: Language prediction test accuracy for the closed task, both test sets and all runs. Our best  results are in bold (tied for second overall). Overall #err is larger than column sum due to "Other".
Language prediction test	err	 Table 3: Language prediction test accuracy for the closed task, both test sets and all runs. Our best  results are in bold (tied for second overall). Overall #err is larger than column sum due to "Other".
accession of new contracts	closing date	(1) For the accession of new contracts, the closing date was kept on the 30th, as previously informed.
Salience	Relevance	Selection: Salience, Relevance and the Coupling between Domain-Level Tasks and Text Planning
classification errors	success	The number of classification errors is halved, yielding a success rate of 92%.
information retrieval	precision	Although precision has always been a concern in information retrieval, the problem assumes new significance when low precision translates into thousands of non-relevant documents that each user must peruse.
Text recognition	accuracy	Text recognition systems require the use of contextual information in order to maximise the accuracy of their output.
MK1	precision	For MK1 alone, 31 of 46 combinations are collocations, a precision of 67.4% (recall is set to 100%).
MK1	recall	For MK1 alone, 31 of 46 combinations are collocations, a precision of 67.4% (recall is set to 100%).
Tagging	bringen	Tagging possibly be improved by determining syntactic relations as done by Smadja (1991a,b) for English, we conducted another test with bringen, where we manually excluded those uninteresting extracted combinations in which the nouns were in fact used in subject position of the verb.
classification	agreement	The human evaluators reported that the task of classification was easier for the third set, and their models exhibited about the same degree of agreement for the second and third sets although the third set is significantly larger.
Translation lexicon	precision	Translation lexicon quality has traditionally been measured on two axes: precision and recall.
Translation lexicon	recall	Translation lexicon quality has traditionally been measured on two axes: precision and recall.
case identification	accuracy	Compared with the baseline system; 17.4% error reduction rate for sense discrimination, 50.7% for case identification, and 47.4% for parsing accuracy are obtained.
parsing	accuracy	Compared with the baseline system; 17.4% error reduction rate for sense discrimination, 50.7% for case identification, and 47.4% for parsing accuracy are obtained.
tagging	accuracy	They pointed out limitation of such methods revealed by their experiments and said that the optimization of likelihood didn't necessarily improve tagging accuracy.
divergence	BPP	As the result, we halt up the process at the 22nd step and the 27th step for the cases of divergence and BPP, respectively.
divergence	recall	Using divergence as a similarity measure, we get, on average, 84 % positive recall and 67 ~ positive precision and up to 90 ~ and 82 % when considering both positive and negative measures.
divergence	precision	Using divergence as a similarity measure, we get, on average, 84 % positive recall and 67 ~ positive precision and up to 90 ~ and 82 % when considering both positive and negative measures.
divergence	BPP	Moreover, the 22nd an 25th merge steps were the most suitable points to terminate the merging process for divergence and BPP, respectively.
SIMR	RMS error	The lowest estimates for SIMR without the translation lexicon are an RMS error of 6.1 for the "easy" bitext and 5.4 for the "hard" bitext.
Information Retrieval	precision	F-measure is used in Information Retrieval, and is calculated by where P is precision, R is recall, and/3 is the relative importance given to recall over precision.
Information Retrieval	recall	F-measure is used in Information Retrieval, and is calculated by where P is precision, R is recall, and/3 is the relative importance given to recall over precision.
Information Retrieval	recall	F-measure is used in Information Retrieval, and is calculated by where P is precision, R is recall, and/3 is the relative importance given to recall over precision.
Information Retrieval	precision	F-measure is used in Information Retrieval, and is calculated by where P is precision, R is recall, and/3 is the relative importance given to recall over precision.
Restricted Memory Bias Representation	significance	 Table 5: Results for the Restricted Memory Bias Representation. (% correct, *'s indicate significance  with respect to the original baseline result shown in boldface, * ~ p = 0.05)
Subject Accessibility Bias Representation	RM	 Table 7: Additional Results for the Subject Accessibility Bias Representation. (% correct, *'s  indicate significance with respect to the original baseline result shown in boldface, • ~ p = 0.05, ** --, p =  0.01; RM refers to the memory limit).
tagging	accuracy	8, our experience indicates that we can expect a roughly 10~o improvement in this score when we compare performance against "golden--standard" test data in which all correct answers are indicated; this would bring our tagging accuracy into the 80-percent area.
tagging	accuracy	Note that tagging accuracy is quoted on a per-word basis, as is customary.
bracketing	recall	The measures used for this evaJuation are bracketing recall, precision and crossing.
bracketing	precision	The measures used for this evaJuation are bracketing recall, precision and crossing.
bracketing	crossing	The measures used for this evaJuation are bracketing recall, precision and crossing.
Parsing	accuracy	 Table 1: Parsing accuracy using the WSJ corpus
parsing	reestimated	We are planning to evaluate the parsing model based on the reestimated PDG and the PDG-based language model.
Semantic classification	accuracy	Semantic classification has proved useful in a range of application areas, such as information extraction, acquim'tion of domain knowledge and improvement of parsing accuracy through the speci~cation of selectional restrictions (.
information extraction	accuracy	Semantic classification has proved useful in a range of application areas, such as information extraction, acquim'tion of domain knowledge and improvement of parsing accuracy through the speci~cation of selectional restrictions (.
spell checking	ring	The proposed model of Tb.ai morphological analysis consists of three steps: sentence segmenting, spell checking and word ill, ring.
parsing	accuracy	The parsing accuracy--roughly 87% precision and 86% recall--surpasses the best previously published results on the Wall St. Journal domain.
parsing	precision	The parsing accuracy--roughly 87% precision and 86% recall--surpasses the best previously published results on the Wall St. Journal domain.
parsing	recall	The parsing accuracy--roughly 87% precision and 86% recall--surpasses the best previously published results on the Wall St. Journal domain.
parsing	accuracy	For this reason, research into reranking schemes appears to be a promising step towards the goal of improving parsing accuracy.
Tagging	accuracy	 Table 2: Tagging accuracy for assigning grammatical  functions depending on the category of the mother  node. For each category, the first row shows the per- centage of branches that occur within this category  and the overall accuracy, the following rows show the  relative percentage and accuracy for different levels  of reliability.
Tagging	accuracy	 Table 2: Tagging accuracy for assigning grammatical  functions depending on the category of the mother  node. For each category, the first row shows the per- centage of branches that occur within this category  and the overall accuracy, the following rows show the  relative percentage and accuracy for different levels  of reliability.
Tagging	accuracy	 Table 2: Tagging accuracy for assigning grammatical  functions depending on the category of the mother  node. For each category, the first row shows the per- centage of branches that occur within this category  and the overall accuracy, the following rows show the  relative percentage and accuracy for different levels  of reliability.
tagging	accuracy	We find the tagging accuracy is very low.
WSD	accuracy	In this paper, we report recent improvements to the exemplar-based learning approach for WSD that have achieved higher disambiguation accuracy.
translation	WER	shows that good translation results (a WER of 6.4%) can be achieved with a Real Time Factor (RTF) of just 2.2.
translation	Real Time Factor (RTF)	shows that good translation results (a WER of 6.4%) can be achieved with a Real Time Factor (RTF) of just 2.2.
translation	accuracy	When translation accuracy is the main concern, a more detailed acoustic model and a wider beam in the search can be used to achieve a WER of 1.9%, but with a RTF of 11.3.
translation	WER	When translation accuracy is the main concern, a more detailed acoustic model and a wider beam in the search can be used to achieve a WER of 1.9%, but with a RTF of 11.3.
translation	RTF	When translation accuracy is the main concern, a more detailed acoustic model and a wider beam in the search can be used to achieve a WER of 1.9%, but with a RTF of 11.3.
Sentence Extraction	Accuracy	 Table 1: Sentence Extraction Accuracy
grammatical analysis	accuracy	Whereas some (e.g. () argue that grammatical analysis may improve recognition accuracy, our current experiments have as yet not been able to reveal a clear advantage in this respect.
speech recognition	safety	During the evaluation, attention will be paid to (i) the speech recognition performance, and (ii) the user-system interface, with the emphasis on security, safety, acceptability and effectiveness.
selectional preference acquisition	FREQ	For the selectional preference acquisition experiments 4 and 6 described below it was decided to use the criteria FREQ 3, RATIO 2 and D (ignore difficult nouns).
selectional preference acquisition	RATIO 2	For the selectional preference acquisition experiments 4 and 6 described below it was decided to use the criteria FREQ 3, RATIO 2 and D (ignore difficult nouns).
Model Switching	accuracy	The Model Switching method maybe preferable to other methods because of its generality (i.e., wide range of applicability), and its competitive accuracy in prediction.
SCG estimation	coverage	As a vehicle for demonstrating SCG estimation, we show, in terms of crossing rates and in coverage, that when training material is limited, SCG estimation using the Minimum Description Length Principle is preferable to SCG estimation using an indifferent prior.
anaphora resolution	reliability	This paper assumes that currently, anaphora resolution at a desired level of reliability has to remain partial.
parsing	accuracy	Finally, the effectiveness of our approach is shown through some experiments investigating the correctness of selected hypotheses and parsing accuracy.
parsing	accuracy	Another experiment is also done for evaluating the parsing accuracy.
Annotation of speech	consistency	Annotation of speech by hand is a tedious task, subject to errors and consistency problems.
recognition	accuracy	Also, additional alternative pronunciations to standard forms have shown to improve recognition accuracy.
parse	accuracy	Our results show conclusively that this information can improve parse accuracy.
parsing	precision	Furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing .
parsing	recall	Furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing .
Iongish sentences	O	i04 rules) and Iongish sentences (say, 40 words and punctuation), even O(n 3) looks pretty bleak.
topical segmentation	precision	In this paper, we present a topical segmentation program that achieves a 10% increase in both precision and recall over comparable previous work.
topical segmentation	recall	In this paper, we present a topical segmentation program that achieves a 10% increase in both precision and recall over comparable previous work.
tagging	accuracy	The accuracy of this data has an impact on the tagging accuracy of both the HMM itself and the derived transducer.
FST	length	The size of a b-type FST increases with the size of the tag set and with the length of look-back plus look-ahead, ~+c~.
Tagging	accuracy	 Table 2: Tagging accuracy and agreement of the FST tagging results with those  of the underlying HMM, for tag sets of different sizes
Tagging	agreement	 Table 2: Tagging accuracy and agreement of the FST tagging results with those  of the underlying HMM, for tag sets of different sizes
AUTOMATIC GENERATION	SALIENCE	AUTOMATIC GENERATION OF SUBWAY DIRECTIONS: SALIENCE GRADATION AS A FACTOR FOR \ DETERMINING MESSAGE AND FORM
AUTOMATIC GENERATION	FORM	AUTOMATIC GENERATION OF SUBWAY DIRECTIONS: SALIENCE GRADATION AS A FACTOR FOR \ DETERMINING MESSAGE AND FORM
parsing	accuracy	The results of full parsing accuracy show that model-1 under the LEX model outperforms other models.
dialogue game detection	UNDEFINED	We have conducted experiments for dialogue game detection where we assumed that the boundaries of the games were known 3 We used the seven primary game tags info, quest, seek_conf, opinion, direct, express and UNDEFINED.
Tagging	accuracy	 Table 3: Tagging accuracy, speed, and storage requirement of RTT and STT taggers
Tagging	accuracy	 Table 5: Tagging accuracy, speed, and storage requirements of enriched RTT and 5TT taggers
coreference resolution	RESOLVE	More importantly, the clustering approach outperforms the only MUC-6 system to view coreference resolution as a learning problem: The RESOLVE system employs decision tree induction and achieves an Fmeasure of 47% on the MUC-6 data set.
coreference resolution	Fmeasure	More importantly, the clustering approach outperforms the only MUC-6 system to view coreference resolution as a learning problem: The RESOLVE system employs decision tree induction and achieves an Fmeasure of 47% on the MUC-6 data set.
entity recognition task	accuracy	For the 1995 Message Understanding Conference (MUC-6), a separate named entity recognition task was developed and the best systems achieved impressive accuracy (with an F-measure approaching 95%).
entity recognition task	F-measure	For the 1995 Message Understanding Conference (MUC-6), a separate named entity recognition task was developed and the best systems achieved impressive accuracy (with an F-measure approaching 95%).
tagging	accuracy	By using the IDIOMTAG, CLAWS system improved tagging accuracy from 94% to 96-97%.
tagging	accuracy	By using these additional states, the tagging system improved the accuracy from 95.7% to 96.0%.
parsing	accuracy	We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.
IR processes	recall	The effect of lexical ambiguity on IR processes is discussed in Section 3, and the sensitivity of recall/precision to Word Sense Disambiguation errors in Section 4.
POS mappings	precision	Similar to the evaluation criteria outlined for POS mappings, we define the precision and recall on lexicon merging as the following: where • E L is the set of lexemes generated by the lexicon insertor.
POS mappings	recall	Similar to the evaluation criteria outlined for POS mappings, we define the precision and recall on lexicon merging as the following: where • E L is the set of lexemes generated by the lexicon insertor.
tagging English texts	accuracy	The tagger has been trained for tagging English texts with an accuracy of 97%.
pluralization	accuracy	Theron and Cloete have experimented with pluralization in Afrikaans, and the resulting system has shown about 94% accuracy on unseen words.
generalization	accuracy	We study the effect of the Cartesian product operator on memory-based language learning, and demonstrate its effect on generalization accuracy and data compression fora number of linguistic classification tasks, using k-nearest neighbor learning algorithms.
generalization	accuracy	The effects of forming Cartesian product attributes on generalization accuracy and reduction of dimensionality (compression) were compared with those of backward sequential elimination of attributes.
attribute joining or elimination step	significance	Generalization accuracy for every attribute joining or elimination step was measured using 10-fold crossvalidation, and significance was measured using a twotailed paired t-test at the .05 level.
NP identification	accuracy	We varied the information content of the training set and measured the effect this had upon NP identification accuracy.
crossing	recall	In terms of crossing rates, recall and precision, no clear story has emerged.
crossing	precision	In terms of crossing rates, recall and precision, no clear story has emerged.
Keyword matching	accuracy	Keyword matching alone provides 45% accuracy.