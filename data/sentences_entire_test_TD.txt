sentences1	sentences2	sentences3
Submission	EMNLP 2016	 Table 1: Submission statistics of EMNLP 2016
predicting coordination boundaries	PTB	We show improvements on predicting coordination boundaries on the PTB compared to two state-of-the-art parsers; as well as improvement over previous coordination boundary prediction systems on the Genia corpus.
predicting coordination boundaries	Genia corpus	We show improvements on predicting coordination boundaries on the PTB compared to two state-of-the-art parsers; as well as improvement over previous coordination boundary prediction systems on the Genia corpus.
event extraction	TIMEBANK	This task bridges event extraction research and temporal research in the tradition of TIMEBANK ( and.
Skipping	Dundee corpus	Skipping is a particularly intriguing phenomenon: about 40% of all words are skipped (in the Dundee corpus, see below), without apparent detriment to text understanding.
Sentence similarity evaluation	GS2013	 Table 5: Sentence similarity evaluation scores on GS2013
SMT outputs	IWSLT data	To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural vs. phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data.
Tagging	Malagasy dataset	 Table 3: Tagging accuracies for the Malagasy dataset.
Extractive Summarisation	FSC	 Table 1: Extractive Summarisation Performance. (1) The extractive summaries of these models are decoded  by the pointer network (i.e the shared component of the ASC and FSC models). (2) R-1, R-2 and R-L  represent the Rouge-1, Rouge-2 and Rouge-L score respectively.
abstractive compression of sentences and short paragraphs	Open American National Corpus (OANC 1 )	In the first half, we introduce a manually-created multi-reference dataset for abstractive compression of sentences and short paragraphs, with the following features: • It contains approximately 6,000 source texts with multiple compressions (about 26,000 pairs of source and compressed texts), representing business letters, newswire, journals, and technical documents sampled from the Open American National Corpus (OANC 1 ).
Sentence-pair classification	SNLI	• Experiment II: Sentence-pair classification − SNLI.
Factoid question answering (QA)	DBpedia	Factoid question answering (QA) has gained great attention recently, owing to the fast growth of large knowledge bases (KBs) such as DBpedia () and Freebase (, which avail QA systems of comprehensive and precise knowledge of encyclopedic scope (.
Factoid question answering (QA)	Freebase	Factoid question answering (QA) has gained great attention recently, owing to the fast growth of large knowledge bases (KBs) such as DBpedia () and Freebase (, which avail QA systems of comprehensive and precise knowledge of encyclopedic scope (.
QA evaluation	Freebase	To demonstrate the usefulness of question characteristics in QA evaluation, we construct anew dataset with over 5,000 questions based on Freebase using the proposed framework, and extensively evaluate several QA systems.
Label Distribution Learning (LDL)	Geng	EDL is first compared with four existing Label Distribution Learning (LDL) methods (Geng,.2.
MLL	ML-KNN	For the MLL methods, the value of k is set to 8 in ML-KNN, ratio is 0.02 and µ is 2 in ML-RBF.
MLL	ML-RBF	For the MLL methods, the value of k is set to 8 in ML-KNN, ratio is 0.02 and µ is 2 in ML-RBF.
AMR parser	JAMR	built the first AMR parser, JAMR, based on a pipelined approach, which breaks down the whole task into two separate subtasks: concept identification and relation identification.
AMR parsing	newswire sections of LDC2013E117	Following previous studies on AMR parsing, our experiments were performed on the newswire sections of LDC2013E117 and LDC2014T12, and we also follow the official split for training, development and evaluation.
AMR parsing	LDC2014T12	Following previous studies on AMR parsing, our experiments were performed on the newswire sections of LDC2013E117 and LDC2014T12, and we also follow the official split for training, development and evaluation.
scope detection	Abstracts	summarizes the performances of scope detection on Abstracts.
negation scope detection	PCRB	Much of this is due to the better ability of our CNN-based models in identifying the right boundaries of scopes than the left ones on negation scope detection, with the huge gains of 29.44% and 25.25% on PCRB using CNN_C and CNN_D, respectively.
negation scope detection	CNN_C	The PCSs of speculation and negation scope detection are 74.73% (CNN_C) and 91.03% (CNN_C) on Clinical Records, which are both higher than the ones trained on Abstracts.
negation scope detection	Clinical Records	The PCSs of speculation and negation scope detection are 74.73% (CNN_C) and 91.03% (CNN_C) on Clinical Records, which are both higher than the ones trained on Abstracts.
negation scope detection	Abstracts	The PCSs of speculation and negation scope detection are 74.73% (CNN_C) and 91.03% (CNN_C) on Clinical Records, which are both higher than the ones trained on Abstracts.
negation scope detection	CNN_C	However, we get lower PCSs on Full Papers (49.54% for speculation scope detection using CNN_C, and 44.67% for negation scope detection using CNN_C).
negation scope detection	Abstracts	It shows that our CNNbased models can achieve higher PCSs (+1.54%) than those of the state-of-the-art systems for speculation scope detection and the second highest PCS for negation scope detection on Abstracts, and can get comparable PCSs on Clinical Records (73.92% vs 78.69% for speculation scopes, 89.66% vs 90.74% for negation scopes).
stance detection	Section 6	TaskB Unlab is an unlabelled corpus containing Donald Trump tweets supplied by the task organisers, and TaskB Auto-lab* is an automatically labelled version of a small portion of the corpus for the weakly supervised stance detection experiments reported in Section 6.
structured prediction	ACE-2005 dataset	For structured prediction, we use the named-entity recognition (NER) ACE-2005 dataset with 7 classes and 6 domains.
named-entity recognition (NER)	ACE-2005 dataset	For structured prediction, we use the named-entity recognition (NER) ACE-2005 dataset with 7 classes and 6 domains.
sentiment classification	Amazon review data set	The second task is sentiment classification on the Amazon review data set) from 4 domains, labeled positive or negative.
parsing	ASR output texts	We conducted experiments using both the proposed parsing method and the tree-annotated corpus based on the ASR output texts.
Sentiment analysis	Movie Review dataset	We implemented our CNN with the THEANO framework: Sentiment analysis result on human-machine dialogue when trained from Twitter and Movie Review dataset.
Sentiment analysis	Movie Review dataset	 Table 3: Sentiment analysis result on human-machine dialogue when trained from Twitter and Movie Review dataset
Machine Translation shared task	Europarl data	Europarl-NC is a 64-million word corpus that was developed fora Machine Translation shared task (, combining Europarl data (from parliamentary debates in the European Union) and News Commentary data.
Machine Translation shared task	News Commentary data	Europarl-NC is a 64-million word corpus that was developed fora Machine Translation shared task (, combining Europarl data (from parliamentary debates in the European Union) and News Commentary data.
RG	TUNA corpus	The abstract scenes dataset provides a more challenging version of RG than anything we are aware of in the existing computational pragmatics literature, which has largely used the TUNA corpus of isolated object descriptions ( or small synthetic datasets (.
PoS tagging	NAIST Text Corpus	Word segmentation, PoS tagging and dependency parsing of the sentences in the NAIST Text Corpus were performed by a Japanese morphological analyzer, MeCab 8 (), and a depentwo sets.dency parser, J.DepP 9).
Word ordering	BSO	 Table 1: Word ordering. BLEU Scores of seq2seq, BSO,  constrained BSO, and a vanilla LSTM language model  (from Schmaltz et al, 2016). All experiments above have  K tr = 6.
Word ordering	BSO	 Table 1: Word ordering. BLEU Scores of seq2seq, BSO,  constrained BSO, and a vanilla LSTM language model  (from Schmaltz et al, 2016). All experiments above have  K tr = 6.
Machine translation	BSO	 Table 4: Machine translation experiments on test set; re- sults below middle line are from MIXER model of Ran- zato et al. (2016). SB-∆ indicates sentence BLEU costs  are used in defining ∆. XENT is similar to our seq2seq  model but with a convolutional encoder and simpler at- tention. DAD trains seq2seq with scheduled sampling  (Bengio et al., 2015). BSO, SB-∆ experiments above  have K tr = 6.
Binary classification	ROC Curve (AUC)	Binary classification tasks were also measured by Area Under the ROC Curve (AUC).
link prediction	WN18	• The experiments on link prediction task prove that our approach has a high efficiency on mining formulas and has a good performance on both WN18 and FB15K datasets.
link prediction	FB15K datasets	• The experiments on link prediction task prove that our approach has a high efficiency on mining formulas and has a good performance on both WN18 and FB15K datasets.
semantic segmentation	Stanford Parser Ablative Study	For semantic segmentation this is the output of DeepLab-CRF ( and for the PPAR module this is the 1-best output of the Stanford Parser Ablative Study.
SemEval textual similarity	Pearson's r × 100	 Table 3: Results on SemEval textual similarity datasets (Pearson's r × 100). The highest score in each row is in boldface (omitting
parse structure	E2E	In practice, one could also predict parse structure for the E2E in two steps: (1) use E2E's decoder to recover the original English sentence, and (2) parse that sentence with the CJ parser.
SMT	CoNLL-2014 test set	We find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning out-performs all previously published results for the CoNLL-2014 test set by a large margin (46.37% M 2 over previously 41.75%, by an SMT system with neural features) while being trained on the same, publicly available data.
SMT-specific	GEC	When we further investigate the influence of well-known SMT-specific features and introduce new features adapted to the problem of GEC, our final systems outperform the best reported results by 8% M 2 , moving the state-of-the-art results for the CoNLL-2014 test set from 41.75% M 2 to 49.49%.
SMT-specific	CoNLL-2014 test set	When we further investigate the influence of well-known SMT-specific features and introduce new features adapted to the problem of GEC, our final systems outperform the best reported results by 8% M 2 , moving the state-of-the-art results for the CoNLL-2014 test set from 41.75% M 2 to 49.49%.
SBMT	NMT baselines	Table 1 also shows the SBMT system scores along with the NMT baselines that do not use transfer.
clustering	CQA datasets	In this paper, we consider using the two-part structure in QAs for clustering CQA datasets.
Question Answering (cQA) forums	Qatar Living	In recent years, community Question Answering (cQA) forums, such as StackOverflow, Quora, Qatar Living, etc., have gained a lot of popularity as a source of knowledge and information.
Distribution	POS	 Table 4: Distribution of the POS Tag
sentiment classification	IMDB	We evaluate the effectiveness of our NSC model on three sentiment classification datasets with user and product information: IMDB, Yelp 2013 and Yelp 2014, which are built by).
sentiment classification	Stanford Sentiment Treebank	Most existing datasets for sentiment classification such as Stanford Sentiment Treebank () are composed of short paragraphs with several sentences, which cannot evaluate the effectiveness of the model under the circumstance of encoding long texts.
Sentiment classification	IMDB	 Table 2: Sentiment classification results of our model against competitor models on IMDB, Yelp 2014 and  Yelp 2013. Evaluation metrics are classification accuracy (Acc.) and MSE. Models with * use user and  product information as additional features. Best results in each group are in bold.
Sentiment classification	Yelp	 Table 2: Sentiment classification results of our model against competitor models on IMDB, Yelp 2014 and  Yelp 2013. Evaluation metrics are classification accuracy (Acc.) and MSE. Models with * use user and  product information as additional features. Best results in each group are in bold.
Sentiment classification	Yelp 2013	 Table 2: Sentiment classification results of our model against competitor models on IMDB, Yelp 2014 and  Yelp 2013. Evaluation metrics are classification accuracy (Acc.) and MSE. Models with * use user and  product information as additional features. Best results in each group are in bold.
Classification	CR dataset	 Table 2: Classification performance on the CR dataset. We
WSD	WordNet	However, the non-optimal WSD techniques and the shallow utilization of knowledge from WordNet do not allow these techniques to learn accurate and high-coverage semantic representations for all senses in the inventory.
AMR parsing	CCG	Task and Data We evaluate on AMR parsing with CCG.
SMTL-LLR	MTL-LR	plots SMTL-LLR accuracy versus unlabeled data sizes from 0 to 10,000, where 0 corresponds to using only labeled data to build the model, i.e., MTL-LR.
Adaptation	NNJM	Adaptation is done by using the unadapted NNJM trained on general domain data (i.e., not L1-specific) using a log likelihood objective function with selfnormalization () as the starting point, and training for subsequent iterations using the smaller L1-specific in-domain data with a modified objective function which includes a KullbackLeibler (KL) divergence regularization term.
SMT-based	GEC baseline	Our baseline system which is SMT-based, achieves the best F 0.5 score compared to other systems using the SMT approach alone, making it a competitive SMT-based GEC baseline.
SMT	GEC baseline	Our baseline system which is SMT-based, achieves the best F 0.5 score compared to other systems using the SMT approach alone, making it a competitive SMT-based GEC baseline.
SMT-based	GEC baseline	Our baseline system which is SMT-based, achieves the best F 0.5 score compared to other systems using the SMT approach alone, making it a competitive SMT-based GEC baseline.
answer selection	TREC answer selection dataset	The framework aims to unify various sentence matching tasks, including answer selection, and provides implementations for variants of sentencematching models that achieve state-of-the-art results on the TREC answer selection dataset (.
translation evaluation	IWSLT 2007 corpus	Compared to the HMM (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), and the baseline agreement method (Ganchev et al., 2010), the experimental results show that the proposed method significantly improves alignment performance regarding the Japanese-English KFTT and BTEC corpus, and in translation evaluation, the proposed method shows comparable or statistical significantly better performance on the Japanese-English KFTT and IWSLT 2007 corpus.
translation evaluations	KFTT	In translation evaluations, we used the KFTT and Ja-En IWSLT 2007 translation tasks . shows each corpus size.
translation evaluations	Ja-En IWSLT 2007 translation	In translation evaluations, we used the KFTT and Ja-En IWSLT 2007 translation tasks . shows each corpus size.
slot filling task	ATIS corpus	In the experiments of a slot filling task, which is an essential component of natural language understanding , with using the standard ATIS corpus , we achieved the state-of-the-art F 1-score of 95.66%.
slot filling task	ATIS corpus	2. Achieved the state-of-the-art F 1 -score of 95.66% in the slot filling task of the standard ATIS corpus.
machine translation (MT) output	GEC	While grammaticality-based, reference-less metrics have been effective in estimating the quality of machine translation (MT) output, the utility of such metrics has not been investigated previously for GEC.
Slot estimation	Japanese weather cor-	 Table 3: Slot estimation accuracy for the Japanese weather cor-
tokenize English	Stanford CoreNLP toolkit	We tokenize English data and segment Chinese data using the Stanford CoreNLP toolkit.
parsing	CONLL-X	Parsing Quality The parsing quality was evaluated using the CONLL-X () standard.
relation extraction task	TACRED	Specifically, we run two sets of experiments: (1) we evaluate model performance on the relation extraction task using TACRED, and (2) we evaluate model performance on the TAC KBP 2015 cold start slot filling task, by training the models on TACRED.
TAC KBP 2015 cold start slot filling task	TACRED	Specifically, we run two sets of experiments: (1) we evaluate model performance on the relation extraction task using TACRED, and (2) we evaluate model performance on the TAC KBP 2015 cold start slot filling task, by training the models on TACRED.
slot filling	TAC KBP 2015	Second, we evaluate the slot filling performance of all models using the TAC KBP 2015 cold start slot filling task (.
WSD	WordNet	Baseline As a traditional baseline in WSD, we used the Most Frequent Sense (MFS) baseline given by the first sense in WordNet.
IMS	OMSTI	 Table 3: F1 of IMS trained on Train-O-Matic, OMSTI and SemCor, and MFS for the Senseval-2,  Senseval-3, SemEval-07, SemEval-13 and SemEval-15 datasets.
IMS	SemEval-15 datasets	 Table 3: F1 of IMS trained on Train-O-Matic, OMSTI and SemCor, and MFS for the Senseval-2,  Senseval-3, SemEval-07, SemEval-13 and SemEval-15 datasets.
question answering	WebQuestions and GraphQuestions datasets	We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation.
predicting words from location	DARE dataset	We also show the effectiveness of the representation for predicting words from location in lexical dialectology, and evaluate it using the DARE dataset.
coreference resolution	CoNLL-2012 shared task	We use the English coreference resolution data from the CoNLL-2012 shared task () in our experiments.
Diversity filtering	STS datasets	 Table 12: Diversity filtering test results after tun- ing filtering hyperparameters on development data  (averaged over languages and data sources for the  NMT rows). Results are on STS datasets (Pear- son's r × 100).
Diversity filtering	Pear- son's r	 Table 12: Diversity filtering test results after tun- ing filtering hyperparameters on development data  (averaged over languages and data sources for the  NMT rows). Results are on STS datasets (Pear- son's r × 100).
Embedding selection	VecShare	Embedding selection was performed using both the VocabRk signature method on VecShare and the conventional method of selecting embeddings, which trains models for each embedding set and then evaluates those models on the test corpus.
synonym selection	ESL-50	Three standard synonym selection datasets, ESL-50, RD-300 (), and TOEFL-80, are performed.
NER	CoNLL 2002 evaluation script	 Table 2: F 1 scores for NER, computed by the  CoNLL 2002 evaluation script.
Sentiment Analysis	Amazon reviews dataset	Sentiment Analysis For sentiment analysis, we evaluate on the Amazon reviews dataset).
sentiment analysis	Amazon reviews dataset	Sentiment Analysis For sentiment analysis, we evaluate on the Amazon reviews dataset).
POS tagging	SANCL 2012 shared task	POS tagging For POS tagging and parsing, we evaluate on the coarse-grained POS data (12 universal POS) of the SANCL 2012 shared task.
POS tagging	SANCL 2012 shared task	We use the same domains as used for POS tagging, i.e., the dependency parsing data with gold POS as made available in the SANCL 2012 shared task.
parsing domain adaptation	SANCL 2012 shared task dataset	 Table 3: Results for data selection for part-of-speech tagging and parsing domain adaptation on the  SANCL 2012 shared task dataset (Petrov and McDonald, 2012). POS: Part-of-speech tagging. Pars:  Parsing. POS tagging models: Structured Perceptron (P); Bi-LSTM tagger (B) (Plank et al., 2016). Pars- ing model: Bi-LSTM parser (BIST) (Kiperwasser and Goldberg, 2016). Evaluation metrics: Accuracy  (POS tagging); Labeled Attachment Score (parsing). Best: bold; second-best: underlined.
image classification task	MNIST dataset	We experiment with an image classification task based on MNIST dataset ( and Romanian→English neural machine translation system.
Sentence segmentation	WEBNLG corpus	Sentence segmentation We first preprocess all 13,308 distinct verbalisations contained in the WEBNLG corpus using the Stanford CoreNLP pipeline () to segment each verbalisation Ti into sentences.
Sentence segmentation	Stanford CoreNLP pipeline	Sentence segmentation We first preprocess all 13,308 distinct verbalisations contained in the WEBNLG corpus using the Stanford CoreNLP pipeline () to segment each verbalisation Ti into sentences.
Sentence segmentation	WEBNLG corpus	Sentence segmentation allows us to associate each text T in the WEBNLG corpus with the number of sentences it contains.
CoreNLP sentence segmentation	WEBNLG	As the CoreNLP sentence segmentation often fails on complex/rare named entities thereby producing unwarranted splits, we verified the sentence segmentations produced by the CoreNLP sentence segmentation module for each WEBNLG verbalisation and manually corrected the incorrect ones.
word segmentation (WS)	Microsoft Research Corpus (MSR)	As the first processing step of Chinese language processing, word segmentation (WS) has been extensively studied and made great progress during the past decades, thanks to the annotation of large-scale benchmark datasets, among which the most widelyused are Microsoft Research Corpus (MSR) (), Peking University * Correspondence author MSR PPD CTB: An example of annotation heterogeneity: (all) (country) (every) (place) (medical science) (field) (experts) (walk) (out) (people) (great hall).
word segmentation (WS)	MSR PPD CTB	As the first processing step of Chinese language processing, word segmentation (WS) has been extensively studied and made great progress during the past decades, thanks to the annotation of large-scale benchmark datasets, among which the most widelyused are Microsoft Research Corpus (MSR) (), Peking University * Correspondence author MSR PPD CTB: An example of annotation heterogeneity: (all) (country) (every) (place) (medical science) (field) (experts) (walk) (out) (people) (great hall).
MWS	SWS	Finally, using bichar embeddings turns out very helpful for MWS, and leads to 0.97 ∼ 1.18% F-score improvement on dev data and 0.62 ∼ 0.85% on test data, which is consistent with the SWS results in.
LSTM taggers	DyNet	We implement the LSTM taggers in DyNet ( and use the hyper-parameter settings by, i.e. we train 20 epochs using Statistical-Gradient-Descent with a learning rate of 0.1 and adding Gaussian noise of 0.2 to the embedding layer.
noun category prediction	Weibo	In this section, we evaluate our method by the noun category prediction task on Twitter, Weibo, and Wikipedia corpora.
noun category prediction	Wikipedia corpora	In this section, we evaluate our method by the noun category prediction task on Twitter, Weibo, and Wikipedia corpora.
solving geometry questions	SAT practice	 Table 7: Scores for solving geometry questions on the SAT practice and
answer synthesis	840 Billion Words Common Crawl corpus	We initialize word-embeddings for the BIDAF model, answer synthesis module, and question synthesis module with 300-dimensional-GloVe vectors () trained on the 840 Billion Words Common Crawl corpus.
answer synthesis	SQuAD development set	To stop training of the answer synthesis module, we similarly monitor predictions on the SQuAD development set.
answer synthesis	NER system 6	However, to train the answer synthesis module, we further augment the human-annotated labels of each paragraph with tags from a simple NER system 6 because labels of answers provided in the train set are underspecified, i.e., many words in the paragraph that could be potential answers are not labeled.
answer sentence selection	MS MARCO	To achieve the 3 rd goal, we integrate our question generation approach into an end-to-end QA task, i.e., answer sentence selection, and evaluate its impact on three popular benchmark datasets, SQuAD, MS MARCO, and WikiQA.
answer sentence selection	WikiQA	To achieve the 3 rd goal, we integrate our question generation approach into an end-to-end QA task, i.e., answer sentence selection, and evaluate its impact on three popular benchmark datasets, SQuAD, MS MARCO, and WikiQA.
conflict resolution	NO-LINK	We did use double-checking in the final conflict resolution, but without giving positive cases the veto power over NO-LINK.
relation extraction	TAC KBP competition	As a pilot of the service, we evaluated three relation extraction systems that also participated in the official 2016 TAC KBP competition.
answer selection	WikiQA	For answer selection task, we experiment on two datasets: TrecQA and WikiQA.
automatic extraction of semantic frames	FrameNet	Here we consider the task of automatic extraction of semantic frames as defined in FrameNet.
frame identification	FrameNet 1.5 data	We evaluate our models for frame identification, SRL, and full structure extraction on the FrameNet 1.5 data.
SRL	FrameNet 1.5 data	We evaluate our models for frame identification, SRL, and full structure extraction on the FrameNet 1.5 data.
POS tagging	Stanford: CIDER performance	Tokenization, POS tagging and sentence splitting were performed using the Stanford: CIDER performance using different constituent representations (RST-DT test set).
POS tagging	RST-DT test set	Tokenization, POS tagging and sentence splitting were performed using the Stanford: CIDER performance using different constituent representations (RST-DT test set).
discourse relation recognition evaluation	BLLIP corpus	We adopted three corpora: PDTB 2.0 and CoNLL-2016 datasets are annotated for discourse relation recognition evaluation, and the BLLIP corpus is unlabeled and used as auxiliary task.
predicting discourse relations and structure	BPS16	The distributed representations of DUs computed and used in JE14 DPLP () and possibly BCS17 cross+dev ( plausibly capture semantic information that helps with predicting discourse relations and structure, but the current experimental results do not provide a similarly strong support for BPS16 (), LLC16 () and BCS17 mono ().
coreference resolution	CoNLL dataset	We show that the incorporation of our model improves coreference resolution performance on the CoNLL dataset, matching the state-of-the-art results of a more complex system.
SMT	NIST dataset	Firstly, it can be observed that with the small IWSLT05 dataset, the SMT outperforms the baseline NMT, but with the large NIST dataset, NMT outperforms SMT.
translation of real product/item descriptions from an e-commerce site)	WMT 2016 German→English task	We present results on two tasks: an inhouse English→Russian e-commerce task (translation of real product/item descriptions from an e-commerce site), and the WMT 2016 German→English task (news domain).
WMT task	English News Crawl data	For the WMT task, we use the English News Crawl data containing 3.8B words for additional language model data.
SMT	KenLM	For Moses, we used the full bilingual training data to train the phrase-based SMT model and the target portion of the bilingual training data to train a 4-gram language model using KenLM . We ran Giza++ on the training data in both Chinese-to-English and English-to-Chinese directions and applied the "grow-diag-final" refinement rule () to obtain word alignments.
Translation	BPE data	 Table 3: Translation results for the various models. The first column shows the models; the second  column indicates whether the corresponding experiment uses BPE data. The number of parameters (M  = millions) in each model is given in the third column. The remaining columns are the translation  accuracies for the test sets and development set, evaluated using BLEU scores (%). "↑ / ⇑": indicates  that the hierarchical encoder is significantly better than the vanilla tree-based encoder (p < 0.05/p <  0.01).
SRL	English and Chinese CoNLL-2009 dataset	We tested the proposed SRL model on the English and Chinese CoNLL-2009 dataset with standard splits into training, test and development sets.
SRL	English development set	 Table 1: SRL results without predicate disam- biguation on the English development set.
SRL	Chinese development set	 Table 2: SRL results without predicate disam- biguation on the Chinese development set.
generative	National Natural Science Foundation of China (61503248)	Besides generative approaches, proposed an unsupervised discrim- * This work was supported by the National Natural Science Foundation of China (61503248).
parsing	English and Chinese datasets	In this section, we evaluate our parsing model on the English and Chinese datasets.
POS tagging task	UD (Universal Dependencies) 1.4 dataset	Dataset We evaluated our model on the POS tagging task, in both the supervised and semisupervised learning settings, over eight different languages from the UD (Universal Dependencies) 1.4 dataset ().
TAG parsing	WSJ Penn Tree Bank	In order to ensure comparability with past work on TAG parsing, we follow the protocol of and, and use the grammar and the TAG-annotated WSJ Penn Tree Bank described in Section 2.
Supertagging	Syn- taxnet	 Table 5: Supertagging and Parsing Results on Section 23. For  the NN parser, k=5 and B=16 throughout. We trained Syn- taxnet (Andor et al., 2016) with global normalization beam  size 16 using the TensorFlow toolkit.
relation extraction	NYT dataset (NYT	To measure the effectiveness of adversarial training on relation extraction, we evaluate both the CNN (PCNN) and RNN (bi-GRU) models on two different datasets, the NYT dataset (NYT) developed by and the UW dataset (UW) by . All code is implemented in Tensorflow () and available at https://github.
relation extraction	UW dataset (UW	To measure the effectiveness of adversarial training on relation extraction, we evaluate both the CNN (PCNN) and RNN (bi-GRU) models on two different datasets, the NYT dataset (NYT) developed by and the UW dataset (UW) by . All code is implemented in Tensorflow () and available at https://github.
Temporal relation classification	TimeBank corpus	 Table 2: Temporal relation classification result on  TimeBank corpus.
relation extraction	PCNN+ATT	 Table 3: P@N for relation extraction with different models  where N is small. We get the result of PCNN+ATT using  their public source code.
coreference resolution	English benchmark data from the CoNLL 2012 shared task	For language modeling and coreference resolution, we use the English benchmark data from the CoNLL 2012 shared task on coreference resolution ().
topical classification	Ama- zon reviews	 Table 4: Accuracies of topical classification on  20NewsGroups and sentiment analysis on Ama- zon reviews. Although not significantly improving  the performance, tLDA topics at least do not hurt.
sentiment analysis	Ama- zon reviews	 Table 4: Accuracies of topical classification on  20NewsGroups and sentiment analysis on Ama- zon reviews. Although not significantly improving  the performance, tLDA topics at least do not hurt.
SRL	IBM Research -Almaden	A major bottleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to * The work was done while the author was at IBM Research -Almaden.
POS tagging	Wall Street Journal (WSJ) portion of Penn Treebank	POS tagging: To train the POS tagging layer, we used the Wall Street Journal (WSJ) portion of Penn Treebank, and followed the standard split for the training (Section 0-18), development (Section 19-21), and test (Section 22-24) sets.
chunking	WSJ corpus	Chunking: For chunking, we also used the WSJ corpus, and followed the standard split for the training (Section 15-18) and test (Section 20) sets as in the CoNLL 2000 shared task.
chunking	CoNLL 2000 shared task	Chunking: For chunking, we also used the WSJ corpus, and followed the standard split for the training (Section 15-18) and test (Section 20) sets as in the CoNLL 2000 shared task.
Dependency parsing	WSJ corpus	Dependency parsing: We also used the WSJ corpus for dependency parsing, and followed the standard split for the training (Section 2-21), development (Section 22), and test (Section 23) sets.
neural translation	WMT'15	We train underlying neural translation systems using the parallel corpora made available from WMT'15.
SWEAR	WikiReading dataset	In Section 3.5 we presented results for the supervised SWEAR on the full WikiReading dataset, establishing it as the highest-scoring method so far developed for WikiReading.
SWEAR	WikiReading test set	 Table 1: Results for SWEAR compared to top published re- sults on the WikiReading test set.
summarization	NIKKEI	We evaluate our proposed summarization method on two datasets: the NIKKEI, the leading financial news publisher in Japan and a financial report corpus; and the New York Times Annotated Corpus.
summarization	New York Times Annotated Corpus	We evaluate our proposed summarization method on two datasets: the NIKKEI, the leading financial news publisher in Japan and a financial report corpus; and the New York Times Annotated Corpus.
Dialog Act Classification	DNN Framework	Using Context Information for Dialog Act Classification in DNN Framework
IE	MUC	IE systems have along history, and became popular after evaluations such as MUC () and ACE ().
IAA	Grexit dataset	More specifically, to compute IAA, we sampled the data applying the same strategy: for the first task, we randomly selected 10% of the tweets of the Grexit dataset (our training set); for task 2, again we randomly selected 10% of the tweets annotated as argument in the previous annotation step; for task 3, given the small size of the dataset, both annotators annotated the whole corpus.
segmentation	Japanese Wikipedia dump data	The segmentation model was constructed using three    months of tweets (from July 1 to Sept. and Japanese Wikipedia dump data (as of Oct. 1, 2016).
Paraphrase detection	MSRP dataset	Paraphrase detection Now we consider paraphrase detection on the MSRP dataset ().
paraphrase detection	MSRP dataset	Paraphrase detection Now we consider paraphrase detection on the MSRP dataset ().
proper nouns recognition	Moby Words list of US Locations	The name list is useful for proper nouns recognition, which fires on names from many sources, such as Freebase lists of celebrities, the Moby Words list of US Locations, proper names from Mark Kantrowitz's name corpus and soon.
Tagging	DAILY547	 Table 4: Tagging accuracies on DAILY547.
MT	GNMT	The 108-sentence English-French challenge set presented in Appendix 7 was submitted to the four MT systems described in section 4: PBMT-1, PBMT-2, NMT, and GNMT.
telicity classification	MASC data	 Table 2: Results for telicity classification on  MASC data (1863 instances), 10-fold CV.
SMT	Europarl corpus	We conduct two SMT tasks with hypergraph re-decoding: The first is German-to-English and is trained using a concatenation of the Europarl corpus (, the Common Crawl corpus and the News Commentary corpus.
SMT	Common Crawl corpus	We conduct two SMT tasks with hypergraph re-decoding: The first is German-to-English and is trained using a concatenation of the Europarl corpus (, the Common Crawl corpus and the News Commentary corpus.
SMT	News Commentary corpus	We conduct two SMT tasks with hypergraph re-decoding: The first is German-to-English and is trained using a concatenation of the Europarl corpus (, the Common Crawl corpus and the News Commentary corpus.
relation extraction	SemEval-2010 dataset	We also evaluate the relation extraction component (Sec. 5) on Cause-Effect subset of SemEval-2010 dataset.
SMT	Werther data	Baselines: the original text coming from the OCR system and the character-level SMT system trained on the Werther data.
SMT	DTA data	The more general modules prove useful for test unk . The number of corrected words increases for the SMT module trained on DTA data on character-level.
SMT	Werther data	 Table 4: WER and CER for both test sets be- fore and after automatic post-correction for the  system trained with the small training set (train)  and the larger training set (train ext ). Baselines:  the original text coming from the OCR system  and the character-level SMT system trained on the  Werther data.
SMT	CAMB16 SMT	Compared to just doing SMT with CAMB16 SMT , re-ranking improves F 0.5 from 52.90 to 55.60 on FCE (performance increases further even though CAMB16 SMT 's training set includes a large set of FCE data), from 37.33 to 42.44 on CoNLL, and from 52.44 to 54.66 on JFLEG.
SMT	FCE	Compared to just doing SMT with CAMB16 SMT , re-ranking improves F 0.5 from 52.90 to 55.60 on FCE (performance increases further even though CAMB16 SMT 's training set includes a large set of FCE data), from 37.33 to 42.44 on CoNLL, and from 52.44 to 54.66 on JFLEG.
SMT	FCE data	Compared to just doing SMT with CAMB16 SMT , re-ranking improves F 0.5 from 52.90 to 55.60 on FCE (performance increases further even though CAMB16 SMT 's training set includes a large set of FCE data), from 37.33 to 42.44 on CoNLL, and from 52.44 to 54.66 on JFLEG.
SMT	CoNLL	Compared to just doing SMT with CAMB16 SMT , re-ranking improves F 0.5 from 52.90 to 55.60 on FCE (performance increases further even though CAMB16 SMT 's training set includes a large set of FCE data), from 37.33 to 42.44 on CoNLL, and from 52.44 to 54.66 on JFLEG.
SMT	JFLEG	Compared to just doing SMT with CAMB16 SMT , re-ranking improves F 0.5 from 52.90 to 55.60 on FCE (performance increases further even though CAMB16 SMT 's training set includes a large set of FCE data), from 37.33 to 42.44 on CoNLL, and from 52.44 to 54.66 on JFLEG.
correction task	Automated Evaluation of Scientific Writing (AESW) 2016 dataset	We revisit the comparison across translation approaches for the correction task in light of the Automated Evaluation of Scientific Writing (AESW) 2016 dataset, a correction dataset containing over 1 million sentences, holding constant the training data across approaches.
SMT	CoNLL-2014 shared task data	The SMT model follows the training, parameters, and dense and sparse task-specific features that generate state-of-the-art results for CoNLL-2014 shared task data, as implemented in publicly available code.
Classification	1ep set	 Table 1: Classification performance on 1ep set.
SMT	TED-talks corpus	To evaluate the quality, we train a Moses ( SMT system on selected data, and evaluate each trained SMT system on 3 test corpora: newstest2011 which contains 3003 sentence pairs, and a random subset of the TED-talks corpus and the movie-subtitle corpus from OPUS (, each of which contains 3000 sentence pairs.
SMT	OPUS	To evaluate the quality, we train a Moses ( SMT system on selected data, and evaluate each trained SMT system on 3 test corpora: newstest2011 which contains 3003 sentence pairs, and a random subset of the TED-talks corpus and the movie-subtitle corpus from OPUS (, each of which contains 3000 sentence pairs.
WSD	TWSI (Biemann, 2012) dataset	WSD performance is measured using the accuracy with respect to the sentences labeled with the direct hypernyms (Hypers) or an extended set of hypernym including hypernyms of hypernyms (Hy-11 Most of the nouns come from the TWSI (Biemann, 2012) dataset, while the remaining nouns were manually selected.  perHypers).
SMT	Europarl training corpora	 Table 7  BLEU, WER, and KSMR for the EMEA test corpora using conventional (batch learning without  retraining), online, and online from scratch SMT systems, with models estimated by means of  the Europarl training corpora. Log-linear weights were trained via MERT. The average online  learning time (LT) in seconds is shown for the online system.
WSD	KORE50	 Table 6  Comparison with state-of-the-art algorithms on WSD and entity linking. The results are  provided as F1 for S13 and as accuracy for KORE50. The first result with a statistically significant  difference from the best (bold result) is marked with * (χ 2 , p < 0.05).
sentiment analysis	Pang and Lee survey	This is clearly an important topic: A Google Scholar search for "sentiment analysis" yields about 31,000 publications, and the Pang and Lee survey alone has more than 4,700 citations.
MT	AMR Bank	The development of the AMR Bank () is the latest attempt in that direction, although it is important to point out that the significance of such semantically annotated corpora goes far beyond the narrow purpose of MT, and that the AMR Bank is neither the first nor the only attempt to develop semantically annotated resources that can be used for MT purposes.
MT evaluation	NIST (Doddington 2002)	Group III contains other important individual evaluation metrics that are commonly used in MT evaluation: BLEU (, NIST (Doddington 2002), ROUGE (Lin 2004), and TER ().
MT evaluation	WMT14	In this section we show how the simple combination of DR-based metrics with a selection of other existing strong MT evaluation metrics can lead to a very competitive evaluation metric, DISCOTK party (), which we presented at the metrics task of WMT14.
MT	FEMTI	The suitability requirement of MT system in the FEMTI comprises discourse aspects including readability, comprehensibility, coherence, and cohesion.
Rhetorical Structure Theory (RST)	RST Discourse Treebank (RST-DT)	Computational text-level discourse analysis mostly happens within Rhetorical Structure Theory (RST), whose structures have classically been presented as constituency trees, and relies on data from the RST Discourse Treebank (RST-DT); as a result, the RST discourse parsing community has largely borrowed from the syntactic constituency parsing community.
Discourse analysis	Penn Discourse Treebank	Discourse analysis takes various forms in the NLP literature, from mostly local relations between propositions in the Penn Discourse Treebank () to structures covering a whole document, for instance, constituent structures (trees) in Rhetorical Structure Theory (RST) (, or graphs between discourse elements in Segmented Discourse Representation Theory (SDRT) or.
RST structures	RST-DT treebank	Our article is organized as follows: In Section 2 we present the traditional view of RST structures as constituency trees and point out specificities of the RST-DT treebank.
RST parsing	Marcu 2000)	Early work on RST parsing (Marcu 2000) introduced a modified version of Parseval to accommodate RST c-trees encoded as outlined above.
MWE detection	French SPMRL corpus	 Table 7  Results for MWE detection on the French SPMRL corpus. Both the generation of the gold  standard and the computations of the measures have been performed on this corpus.
MT	OPUS data set	 Table 6  Comparison of MT performance on the OPUS data set.
HIT	Turkers	Each HIT was done by five Turkers (a total of 160 HITs).
irony detection	Riloff et al.'s (2013) irony data set	 Table 7  Comparison of our approach to three state-of-the-art irony detection methods. The results are  obtained on Riloff et al.'s (2013) irony data set. The best results per column are in bold.
topic modeling	RbAM	Indeed, the best results are obtained using more advanced techniques for topic modeling rather than simple associations of topics ∼ nouns, and LSTMs for RbAM, with 0.38% improvement compared with using syntactic features and tf-idf features.
matching prediction	Ubuntu corpus	The three approaches to matching prediction do not show much difference in both SCN and SAN, but dynamic average and static average are better than the last state on the Ubuntu corpus and worse than it on the Douban corpus.
matching prediction	Douban corpus	The three approaches to matching prediction do not show much difference in both SCN and SAN, but dynamic average and static average are better than the last state on the Ubuntu corpus and worse than it on the Douban corpus.
MISC tags	CoNLL documents	This process of changing MISC tags is only done when we train on CoNLL documents and test on low-resource languages.
coreference resolution	WordNet	Previous approaches for modeling event mentions in context of coreference resolution) make either use of external feature sources with limited cross-domain availability like WordNet) and FrameNet (), or show low performance.
MT training sentence	CNN	Fixing the number of MT training sentence pairs to 300K that will be selected by the CNN, we reduce the CNN training data from 6,000 down to 100 sentence pairs in steps.
MT training sentence	CNN training data	Fixing the number of MT training sentence pairs to 300K that will be selected by the CNN, we reduce the CNN training data from 6,000 down to 100 sentence pairs in steps.
OPT	WSJ and 'blind' test sets	Overall Results summarizes OPT system performance in terms of the metrics computed by the official scorer for the Shared Task, against both the WSJ and 'blind' test sets.
connectives detection	CoNLL-2015	For other components such as connectives detection and arguments extraction, we just follow the top ranked system () in CoNLL-2015, which is as the baseline system in this paper.
dis-course parsing	Penn Discourse TreeBank (PDTB)	During the past few years, English shallow discourse parsing has dominated the research on dis-course parsing, thanks to the availability of Penn Discourse TreeBank (PDTB) (.
Discourse Relation Sense Classification	CoNLL-2016 Shared Task	Discourse Relation Sense Classification Systems for CoNLL-2016 Shared Task
NED	CoNLL-YAGO	Current research on NED is mostly driven by a number of standard datasets, such as CoNLL-YAGO), TAC KBP () and ACE ().
NED	TAC KBP	Current research on NED is mostly driven by a number of standard datasets, such as CoNLL-YAGO), TAC KBP () and ACE ().
NED	ACE	Current research on NED is mostly driven by a number of standard datasets, such as CoNLL-YAGO), TAC KBP () and ACE ().
sentiment classification	IMDB review data set	The cosine scores are computed between the vectors of each pair of words in the datasets 8 . The measures adopted are Pearson's coefficient of product-moment correlation (γ) and Spearman's rank correlation (ρ), which reflect how  We perform sentiment classification on the IMDB review data set), which has 50K labeled samples with equal number of positive and negative reviews.
Sentiment classification	IMDB  data set	 Table 3: Sentiment classification results on IMDB  data set (Maas et al., 2011). Bold indicates the  highest score for each embedding type. * indi- cates t-test significance at p < 0.05 level when  compared with the baseline.
Semantic parsing	Prolog	Semantic parsing is the task of mapping a natural language query to a logical form (LF) such as Prolog or lambda calculus, which can be executed directly through database query.
sentiment classifier	MSDA baselines	The sentiment classifier we employ, in this case as well as with our methods and with the SCL-MI and MSDA baselines, is a standard logistic regression classifier.
tagging	Penn Arabic Treebank (PATB) data set	We report tagging accuracy on two data sets: the Penn Arabic Treebank (PATB) data set and the Arabic Universal Dependencies Treebank (UD Arabic) data set.
tagging	Arabic Universal Dependencies Treebank (UD Arabic) data set	We report tagging accuracy on two data sets: the Penn Arabic Treebank (PATB) data set and the Arabic Universal Dependencies Treebank (UD Arabic) data set.
SVM	MGR	When lookup is used, SVM yields better results across the board except in three cases, namely when training and testing on Egyptian with DA+MSA lookup, when training with Egyptian and testing on MGR, and when training with GLF and testing on MGR with DA+MSA lookup.
SVM	GLF	When lookup is used, SVM yields better results across the board except in three cases, namely when training and testing on Egyptian with DA+MSA lookup, when training with Egyptian and testing on MGR, and when training with GLF and testing on MGR with DA+MSA lookup.
SVM	MGR	When lookup is used, SVM yields better results across the board except in three cases, namely when training and testing on Egyptian with DA+MSA lookup, when training with Egyptian and testing on MGR, and when training with GLF and testing on MGR with DA+MSA lookup.
Morphological Reinflection	CoNLL-2017	Experiments on Morphological Reinflection: CoNLL-2017 Shared Task
IMS	CoNLL 2017 UD Shared Task	IMS at the CoNLL 2017 UD Shared Task: CRFs and Perceptrons Meet Neural Networks
IMS	CoNLL 2017 Shared Task	This paper presents the IMS contribution to the CoNLL 2017 Shared Task.
IMS	CoNLL 2017 UD Shared Task	This paper presents the IMS contribution to the CoNLL 2017 UD Shared Task (.
dependency parsing task	UDPipe	Because we wanted to focus on the dependency parsing task, we used automatically annotated corpora for testing and also trained all models with the annotated corpora provided by UDPipe (.
parsing	UD- Pipe 1.2	 Table 5: Joint segmentation and parsing in UD- Pipe 1.2 Participant System, optimized to maxi- mize parsing likelihood, in comparison with se- quential segmentation and parsing.
parsing	CoNLL 2017	Universal Dependencies release 2.0 2 (Nivre et al., 2017b) includes rich languages and treebanks resources and the parsing task in CoNLL 2017 is * Correspondence author.
CoNLL 2017 shared task Multilingual Parsing from Raw Text to Universal Dependencies	CLCL (Geneva) entry	This paper describes the University of Geneva's submission to the CoNLL 2017 shared task Multilingual Parsing from Raw Text to Universal Dependencies (listed as the CLCL (Geneva) entry).
citation extraction	UMass Citations dataset	We examine the performance of the EmbeddedState Latent CRF on two datasets: citation extraction on the UMass Citations dataset and medical record field extraction on the CLEF dataset.
citation extraction	CLEF dataset	We examine the performance of the EmbeddedState Latent CRF on two datasets: citation extraction on the UMass Citations dataset and medical record field extraction on the CLEF dataset.
entity navigation	clickstream dataset generated from the Wikipedia webserver logs	For entity navigation, we use the clickstream dataset generated from the Wikipedia webserver logs from February until September, 2016.
tokenization	Hindi Shallow Parser	The tokenization and PoS tagging are performed using the publicly available Hindi Shallow Parser  the performance of CMQG in terms of accuracy, BLEU () and ROUGE) score.
churn detection	German dataset	Since there is no existing dataset for churn detection except for English, we create a novel German dataset.
AL evaluation	WMT2018	For AL evaluation, we randomly sample 500K sentence pairs from the parallel corpora in WMT2018 for each of the three language pairs, and take 100K as the initially available bitext and the rest of 400K as the pool of untranslated sentences, pretending the translation is not available.
Morphological tagging	UDv2.1 test sets	 Table 3: Morphological tagging accuracies on UDv2.1 test sets for MarMot (MMT) and MC baselines as well as for MCML,
Morphological tagging	MarMot (MMT) and MC baselines	 Table 3: Morphological tagging accuracies on UDv2.1 test sets for MarMot (MMT) and MC baselines as well as for MCML,
imagesentence ranking	University of baseline	Their results show that bilingual training improves imagesentence ranking performance over a monolingual * Work carried out at the University of baseline, and it improves performance on semantic textual similarity benchmarks (.
Sequence Labeling Tasks	PTB	 Table 2: Performance on Sequence Labeling Tasks. % accuracy shown for PTB, and % F-measure otherwise
NLI classification	SNLI train set	 Table 2: Results on NLI classification with sentence-to-vector encoders. Params. is the approximate number of  model parameters, and the numbers in parentheses indicate the parameters contributed by word embeddings. S tr.,  and S te. are the class accuracies (%) on SNLI train set and test set, respectively. M tr., M te. mat., and M te.  mism. are the class accuracies (%) on MultiNLI train set, matched test set, and mismatched test set, respectively.  Underlining marks the best result among tree-structured models.
NLI classification	MultiNLI train set	 Table 2: Results on NLI classification with sentence-to-vector encoders. Params. is the approximate number of  model parameters, and the numbers in parentheses indicate the parameters contributed by word embeddings. S tr.,  and S te. are the class accuracies (%) on SNLI train set and test set, respectively. M tr., M te. mat., and M te.  mism. are the class accuracies (%) on MultiNLI train set, matched test set, and mismatched test set, respectively.  Underlining marks the best result among tree-structured models.
sentiment analysis	Movie reviews sentiment datasets	We use the same seven datasets with, including both sentiment analysis and topic classification tasks: MR: Movie reviews sentiment datasets 2 . SST-1: Stanford Sentiment Treebank with 5 sentiment labels . The data consists of phrases-level and sentence-level instances.
sentiment analysis	Stanford Sentiment Treebank	We use the same seven datasets with, including both sentiment analysis and topic classification tasks: MR: Movie reviews sentiment datasets 2 . SST-1: Stanford Sentiment Treebank with 5 sentiment labels . The data consists of phrases-level and sentence-level instances.
extrinsic evaluation	Uppsala	In the extrinsic evaluation, the Uppsala system ranked second for event extraction, first for opinion analysis, and 16th (out of 16 systems) for negation resolution.
SLT-Interactions Parsing	CoNLL 2018 Shared Task	The SLT-Interactions Parsing System at the CoNLL 2018 Shared Task
tokenization	UDPipe 1.2 base-line	Sentence seg-mentation, tokenization and dependency parsing were handled by UDPipe 1.2 base-line.
dependency parsing	UDPipe 1.2 base-line	Sentence seg-mentation, tokenization and dependency parsing were handled by UDPipe 1.2 base-line.
dependency parsing	CoNLL 2017 UD Shared	The state-of-the-art in dependency parsing is a network with deep biaffine attention module, which won CoNLL 2017 UD Shared.
Parsing under-resourced languages	CoNLL 2018 UD Shared Task	CUNI x-ling: Parsing under-resourced languages in CoNLL 2018 UD Shared Task
parsing	MLAS	For a full description of the metrics, see (Zeman et al., 2018) or the shared task website; here, we only note that while LAS only evaluates parsing accuracy, MLAS also includes evaluation of tagging (UPOS and morphological features), while BLEX also includes lemmatization.
POS tagging	CoNLL-U training data	The prototype utilizes an artificial neu-ral network with a single joint model for POS tagging, lemmatization and dependency parsing, and is trained only using the CoNLL-U training data and pretrained word embeddings, contrary to both systems surpassing the prototype in the LAS and BLEX ranking in the shared task.
Morphological Reinflection	CoNLL 2018 Shared Task	Experiments on Morphological Reinflection: CoNLL 2018 Shared Task
Morphological Reinflection in Context	CU Boulder's Submission to CoNLL-SIGMORPHON 2018 Shared Task	Morphological Reinflection in Context: CU Boulder's Submission to CoNLL-SIGMORPHON 2018 Shared Task
MT evaluation	WMT-14 Czech-toEnglish metrics shared task	For example, although WMT constitutes a main authority on MT evaluation, and have made the best attempt to provide confidence intervals for metric correlations we could find, when we closely examine results of WMT-14 Czech-toEnglish metrics shared task, reproduced herein, a discrepancy can be identified.
IE	tourism dataset	For the time consuming manual seed list setting of our IE system, we only use the tourism dataset.
Rhetorical Structure Theory (RST)	RST Discourse Tree Bank	Rhetorical Structure Theory (RST) () does take into account the global structure of the document, and the RST Discourse Tree Bank has texts annotated according to RST with full discourse structures.
MT	Fr-En	The main reason for this is that over the years MT has matured enough for language pairs such as Fr-En and De-En and it can generate almost perfect translations for short sentences (such as captions).
sense prediction	VerSe dataset	We trained logistic regression classifiers for sense prediction by dividing the images in VerSe dataset into train and test splits.
partof-speech tagging	Penn treebank	We also show that a baseline feed-forward neural network (NN) architecture significantly outperforms previous feed-forward NN baselines, with slightly fewer features, achieving better accuracy than the RNN model from ( . Recently, bi-LSTMs have achieved high accuracies in a simpler sequence labeling task: partof-speech tagging () on the Penn treebank, with small improvements over local models.
intent recognition	SIGHAN Chinese language processing Bakeoff	As described above, intent recognition is the first step in SLU, and the availability of which is assumed in this research  For NER experiments, we use the benchmark NER data from the third SIGHAN Chinese language processing Bakeoff ().
SLU	SIGHAN Chinese language processing Bakeoff	As described above, intent recognition is the first step in SLU, and the availability of which is assumed in this research  For NER experiments, we use the benchmark NER data from the third SIGHAN Chinese language processing Bakeoff ().
NER	SIGHAN Chinese language processing Bakeoff	As described above, intent recognition is the first step in SLU, and the availability of which is assumed in this research  For NER experiments, we use the benchmark NER data from the third SIGHAN Chinese language processing Bakeoff ().
discourse relation prediction	PDTB	Our main evaluation is discourse relation prediction, using the PDTB and SWDA corpora.
discourse relation prediction	SWDA corpora	Our main evaluation is discourse relation prediction, using the PDTB and SWDA corpora.
SMT-based	FCE test set	Our best NMT-based system trained on the CLC outperforms our SMT-based system when testing on the publicly available FCE test set.
slot tagging	ATIS	 Table 2: F1 score (in %) for slot tagging on ATIS achieved by different
chunking	CoNLL 2000 shared task	 Table 3: F1 score (in %) for chunking on CoNLL 2000 shared task
link prediction evaluation	STransE model	For link prediction evaluation, we conduct experiments and compare the performance of our STransE model with published results on the benchmark WN18 and FB15k datasets ().
link prediction evaluation	WN18	For link prediction evaluation, we conduct experiments and compare the performance of our STransE model with published results on the benchmark WN18 and FB15k datasets ().
link prediction evaluation	FB15k datasets	For link prediction evaluation, we conduct experiments and compare the performance of our STransE model with published results on the benchmark WN18 and FB15k datasets ().
Link prediction	NTN	 Table 3: Link prediction results. MR and H10 denote  evaluation metrics of mean rank and Hits@10 (in %), re- spectively. "NLFeat" abbreviates Node+LinkFeat. The  results for NTN (Socher et al., 2013) listed in this table  are taken from Yang et al. (2015) since NTN was origi- nally evaluated on different datasets. The results marked  with + are obtained using the optimal hyper-parameters  chosen to optimize Hits@10 on the validation set; trained  in this manner, STransE obtains a mean rank of 244 and  Hits@10 of 94.7% on WN18, while producing the same  results on FB15k.
Link prediction	WN18	 Table 3: Link prediction results. MR and H10 denote  evaluation metrics of mean rank and Hits@10 (in %), re- spectively. "NLFeat" abbreviates Node+LinkFeat. The  results for NTN (Socher et al., 2013) listed in this table  are taken from Yang et al. (2015) since NTN was origi- nally evaluated on different datasets. The results marked  with + are obtained using the optimal hyper-parameters  chosen to optimize Hits@10 on the validation set; trained  in this manner, STransE obtains a mean rank of 244 and  Hits@10 of 94.7% on WN18, while producing the same  results on FB15k.
Link prediction	FB15k	 Table 3: Link prediction results. MR and H10 denote  evaluation metrics of mean rank and Hits@10 (in %), re- spectively. "NLFeat" abbreviates Node+LinkFeat. The  results for NTN (Socher et al., 2013) listed in this table  are taken from Yang et al. (2015) since NTN was origi- nally evaluated on different datasets. The results marked  with + are obtained using the optimal hyper-parameters  chosen to optimize Hits@10 on the validation set; trained  in this manner, STransE obtains a mean rank of 244 and  Hits@10 of 94.7% on WN18, while producing the same  results on FB15k.
SP contexts	DepAll	Moreover, the number of SP contexts is substantially smaller than the alternatives, making it extremely fast to train: 11 minutes only on an 8G word corpus using a 32 CPU core machine, compared to 5 and 11 hours for BOW and DepAll, respectively.
dialog act classification task	DSTC 4	We evaluate our model on the dialog act classification task using the following datasets: • DSTC 4: Dialog State Tracking Challenge 4 ().
tagging	BLSTM-RNN	Then the tagging task for training BLSTM-RNN is described in section 3.
POS tagging	Wall Street Journal data from Penn Treebank III	• The POS tagging experiment is conducted on the Wall Street Journal data from Penn Treebank III (.
CNN training	RNN	For CNN training, we use mini batches of 25 training examples while we perform stochastic gradient descent for the RNN.
Semantic Parsing	NLMAPS	 Table 2: Semantic Parsing results on NLMAPS (split 1500/880 for train/test set) using different settings. Tuning was carried out
SMT	NLMAPS	 Table 3: SMT results on NLMAPS, reporting Precision (P), Re-
slot filling relation classification	TAC Shared Task data	To start with one of the most important components, we have created a benchmark for slot filling relation classification, based on 2012 -2014 TAC Shared Task data.
Sentences Involving Compositional Knowledge (SICK)	TrecQA data	Sentences Involving Compositional Knowledge (SICK) is from Task 1 of the 2014 SemEval competition () and consists of 9,927 annotated sentence pairs, with 4,500 for training, 500 as a development set, and 4,927 for  The TrecQA data consists of 1,229 questions with 53,417 question-answer pairs in the TRAIN-ALL training set, 82 questions with 1,148 pairs in the development set, and 100 questions with 1,517 pairs in the test set.
Sentences Involving Compositional Knowledge (SICK)	TRAIN-ALL training set	Sentences Involving Compositional Knowledge (SICK) is from Task 1 of the 2014 SemEval competition () and consists of 9,927 annotated sentence pairs, with 4,500 for training, 500 as a development set, and 4,927 for  The TrecQA data consists of 1,229 questions with 53,417 question-answer pairs in the TRAIN-ALL training set, 82 questions with 1,148 pairs in the development set, and 100 questions with 1,517 pairs in the test set.
answer selection task	WikiQA	We used a hinge loss for the answer selection task on WikiQA and TrecQA data.
answer selection task	TrecQA data	We used a hinge loss for the answer selection task on WikiQA and TrecQA data.
answer selection task	Wiki-QA	For the answer selection task (Wiki-QA and TrecQA), we used the official trec eval scorer to compute the metrics Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) and  selected the best development model based on MRR for final testing.
translation	2.13TB Chinese Web Page Collection Corpus 5	 Table 3. As far as the DP  training corpus is concerned, we annotate the Chi- nese side of the parallel data using the approach de- scribed in Section 2.1. There are two different lan- guage models for the DP annotation (Section 2.1)  and translation tasks, respectively: one is trained on  the 2.13TB Chinese Web Page Collection Corpus 5  while the other one is trained on all extracted 7M  English subtitle data (
translation	7M  English subtitle data	 Table 3. As far as the DP  training corpus is concerned, we annotate the Chi- nese side of the parallel data using the approach de- scribed in Section 2.1. There are two different lan- guage models for the DP annotation (Section 2.1)  and translation tasks, respectively: one is trained on  the 2.13TB Chinese Web Page Collection Corpus 5  while the other one is trained on all extracted 7M  English subtitle data (
coreference resolution	CoNLL 2012 English test set	By incorporating our pruning method in one of the state-of-the-art coreference resolution systems, we achieve the best reported overall score on the CoNLL 2012 English test set.
relation extraction	English data sets	We include detailed feature templates in PP-attachment and relation extraction are two fundamental NLP tasks, and we test our models on the largest English data sets.
predicting human judgments of semantic relations between words	WordSim-353	Intrinsic tasks mostly include predicting human judgments of semantic relations between words, e.g., as in WordSim-353 (), while extrinsic tasks include various 'real' downstream NLP tasks, such as coreference resolution and sentiment analysis.
alias matching	genomes dataset	(2) alias matching: we create a list of aliases for each of the 2983 cities from the genomes dataset, which includes the smaller cities clustered together by.
MT	WMT test sets	MT systems were evaluated with the standard BLEU metric) on two official WMT test sets that cover different domains: News (WMT'11) and Europarl (WMT'08).
MT	News (WMT'11)	MT systems were evaluated with the standard BLEU metric) on two official WMT test sets that cover different domains: News (WMT'11) and Europarl (WMT'08).
MT	Europarl (WMT'08)	MT systems were evaluated with the standard BLEU metric) on two official WMT test sets that cover different domains: News (WMT'11) and Europarl (WMT'08).
MT	Europarl test	MT systems were trained using test-domain specific language models (LM) -English News Commentary for News test and English Europarl for the Europarl test.
MT	WMT'11 baseline parallel collections	shows the performance comparison, on the News test set (WMT'11), of the MT system trained on extracted parallel sentences from four years of Gigaword data (GW) with a MT system trained on two WMT'11 baseline parallel collections: Europarl (EP) and News Commentary (NC).
MT	Europarl (EP)	shows the performance comparison, on the News test set (WMT'11), of the MT system trained on extracted parallel sentences from four years of Gigaword data (GW) with a MT system trained on two WMT'11 baseline parallel collections: Europarl (EP) and News Commentary (NC).
MT	News Commentary (NC)	shows the performance comparison, on the News test set (WMT'11), of the MT system trained on extracted parallel sentences from four years of Gigaword data (GW) with a MT system trained on two WMT'11 baseline parallel collections: Europarl (EP) and News Commentary (NC).
MT	News test set	In order to determine statistically significant differences between the results of different MT systems we ran the randomization test on the News test set with 10k iterations.
MT	OCD	 Table 3: Summary of ablation experiments: BLEU score  values of MT systems trained on extracted bitext by OCD  alone and with PLTM reestimation along with the dedu- plication (dedup.) effect. * denotes statistical significance  level (p-value≤0.001) above NC.  ‡ denotes statistical sig- nificance level (p-value≤0.05) above NC.  † denotes sta- tistical significance level (p-value≤0.001) above OCD.
SMTbased error correction	CoNLL2014 Shared Task	SMTbased error correction systems have achieved rankings first and third in the CoNLL2014 Shared Task ( ).
translation	Lang-8 Learner Corpora v2.0	The translation model was trained on the Lang-8 Learner Corpora v2.0.
spelling normalization	Early Modern English texts	Feature Embeddings also give better performance than spelling normalization, but the combination of the two methods is better still, yielding a 5% raw improvement in tagging accuracy on Early Modern English texts.
POS tagging	Wall Street Journal (WSJ) text from the PTB	The second setting is the standard and well-studied evaluation scenario for POS tagging, where we train on the Wall Street Journal (WSJ) text from the PTB and test on historical texts.
VARD normalization	PPCEME corpus	In addition, we evaluate the effectiveness of the VARD normalization tool () for improving POS tagging performance on the PPCEME corpus.
POS tagging	PPCEME corpus	In addition, we evaluate the effectiveness of the VARD normalization tool () for improving POS tagging performance on the PPCEME corpus.
clustering paraphrases byword sense	Paraphrase Database (PPDB)	We present anew method for clustering paraphrases byword sense, and apply it to the Paraphrase Database (PPDB).
TREC question type classification	Stanford Sentiment Treebank	We consider the usage of word and dependency context features for three common sentence classification tasks: TREC question type classification, binary sentiment prediction on Stanford Sentiment Treebank, and SemEval 2010 relation identification.
sentence sentiment classification	Stanford Sentiment Treebank	For sentence sentiment classification, the Stanford Sentiment Treebank () is used.
query intent detection	ATIS (airline travel information system) dataset	For query intent detection, ATIS (airline travel information system) dataset () is used.
RMN	NUBBI	For the RMN, we initialize the word embedding matrix L with 300-dimensional GloVe embeddings trained on the Common Crawl (Pennington et al., NUBBI requires additional spans that mention only a single character to differentiate character topics from relationship topics.
clustering	MTDSEM	BOB90 4 used a supervised approach to tackle the clustering problem ( and get the best score on MTDSEM.
Rhetorical Structure Theory	RST Discourse Treebank; RSTDT	Since its introduction by Rhetorical Structure Theory has enjoyed continuing interest as a framework for the analysis of discourse relations, including the development of large scale corpora (especially the RST Discourse Treebank; RSTDT,) and automatic parsers ().
Sentiment categorization	Xerox Research Centre India	Sentiment categorization, one of the common social media analytics task, segregates a collection of UGC into different buckets with positive, negative or neutral orientation (; * Work done at Xerox Research Centre India.
SIGHAN Workshop on Chinese language processing)	WeiboNER	WeiboNER Transfer Following;, we transfer knowledge from SighanNER (MSR corpus of the sixth SIGHAN Workshop on Chinese language processing) to WeiboNER (a social media NER corpus) (.
parsing	Switchboard corpus	Most previous work on integrating prosody and parsing has used the Switchboard corpus, but it is still difficult to compare results because of differences in constraints, objectives and the use of constituent vs. dependency structure, as discussed further in Section 6.
Binary Classification	PDTB	 Table 4: Binary Classification Results on PDTB. We report F1-scores for implicit discourse relations.
benchmarking	TV dataset	For benchmarking we use the TV dataset and the Laptop dataset) with 7K and 13K samples, respectively.
benchmarking	Laptop dataset	For benchmarking we use the TV dataset and the Laptop dataset) with 7K and 13K samples, respectively.
citation recommendation	OpenCorpus	We release an online portal for citation recommendation based on our method, 1 and anew dataset OpenCorpus of 7 million research articles to facilitate future research on this task.
paraphrasing	WMT'14 fr-en	For paraphrasing, we compare our model trained on WMT'14 fr-en to the model of) on the MTC dataset () following their setup.
paraphrasing	MTC dataset	For paraphrasing, we compare our model trained on WMT'14 fr-en to the model of) on the MTC dataset () following their setup.
sentence-level classification task	CoNLL 2010	Performance on the sentence-level classification task is similar for the different architectures on the CoNLL 2010 and FCE datasets, whereas the composition method based on attention obtains an advantage on the SemEval datasets.
sentence-level classification task	FCE datasets	Performance on the sentence-level classification task is similar for the different architectures on the CoNLL 2010 and FCE datasets, whereas the composition method based on attention obtains an advantage on the SemEval datasets.
sentence-level classification task	SemEval datasets	Performance on the sentence-level classification task is similar for the different architectures on the CoNLL 2010 and FCE datasets, whereas the composition method based on attention obtains an advantage on the SemEval datasets.
Transfer Learning	Accenture Technology Labs	Although learning paradigms like Transfer Learning attempt to incorporate * equal contribution † Main work done during internship at Accenture Technology Labs knowledge from one task into another, these techniques are limited in scalability and are specific to the task at hand.
paraphrase detection	Microsoft Research Paraphrase Corpus	We show that na¨ıvena¨ıve use of AMR in paraphrase detection is not necessarily useful, and turn to describe a technique based on latent semantic analysis in combination with AMR parsing that significantly advances state-of-the-art results in paraphrase detection for the Microsoft Research Paraphrase Corpus.
paraphrase detection	Microsoft Research Paraphrase Corpus	We show that na¨ıvena¨ıve use of AMR in paraphrase detection is not necessarily useful, and turn to describe a technique based on latent semantic analysis in combination with AMR parsing that significantly advances state-of-the-art results in paraphrase detection for the Microsoft Research Paraphrase Corpus.
AMR parsing	LDC2015E86	For AMR parsing, we used the JAMR 2 version published for SemEval, reporting 0.67 Smatch score on LDC2015E86 and the first and only version available for AMREager, 3 obtaining 0.64 Smatch score on the same dataset.
AMR parsing	AMREager	For AMR parsing, we used the JAMR 2 version published for SemEval, reporting 0.67 Smatch score on LDC2015E86 and the first and only version available for AMREager, 3 obtaining 0.64 Smatch score on the same dataset.
text classification task	Reuters-21578 dataset	Here, we conduct an experimental study to assess whether the proposed embedding model can lead to better performance fora text classification task on the Reuters-21578 dataset, using trained vectors as input to a convolutional neural network.
DST task	Wizard-of-Oz (WOZ) v2.0 dataset	The importance of word vector specialisation for the DST task (e.g., distinguishing between synonyms and antonyms by pulling northern and north closer in   the vector space while pushing north and south away) has been established . Again, as in prior work the DST evaluation is based on the Wizard-of-Oz (WOZ) v2.0 dataset , comprising 1,200 dialogues split into training (600 dialogues), development (200), and test data (400).
DST evaluation	Wizard-of-Oz (WOZ) v2.0 dataset	The importance of word vector specialisation for the DST task (e.g., distinguishing between synonyms and antonyms by pulling northern and north closer in   the vector space while pushing north and south away) has been established . Again, as in prior work the DST evaluation is based on the Wizard-of-Oz (WOZ) v2.0 dataset , comprising 1,200 dialogues split into training (600 dialogues), development (200), and test data (400).
paraphrase identification (MSRP)	Pang	We evaluate paraphrase identification (MSRP) (), classification of movie review sentiment (MR) (Pang and), product reviews (CR) (), subjectivity classification (SUBJ) (), opinion polarity (MPQA) () and question type classification (TREC).
classification of movie review sentiment (MR)	Pang	We evaluate paraphrase identification (MSRP) (), classification of movie review sentiment (MR) (Pang and), product reviews (CR) (), subjectivity classification (SUBJ) (), opinion polarity (MPQA) () and question type classification (TREC).
QA	SQUAD	shows the results of the QA evaluations on the SQUAD and MS MARCO datasets.
QA	MS MARCO datasets	shows the results of the QA evaluations on the SQUAD and MS MARCO datasets.
OSS	Illinois Math Solver	Recall that OSS is created by selecting some MWPs which both Illinois Math Solver (Roy and Roth, 2016) and our system 16 can correctly solve.
solver	IMS	The solver sums all quantities and gives the wrong answer 22, which reveals that IMS cannot understand that the quantity "5 yellow flowers" is irrelevant to the question "How many yellow balloons?".
Sentence segmentation	NLTK package	Sentence segmentation of the outputs was carried out using the NLTK package).
meaning preservation	SAMSA abl	The highest correlation for meaning preservation is obtained by SAMSA abl which provides further evidence that the retainment of semantic structures is a strong predictor of meaning preservation (.
summarization	DUC	shows results for summarization systems on DUC, CNN / Daily Mail, and NEWS-ROOM.
summarization	CNN / Daily Mail	shows results for summarization systems on DUC, CNN / Daily Mail, and NEWS-ROOM.
summarization	NEWS-ROOM	shows results for summarization systems on DUC, CNN / Daily Mail, and NEWS-ROOM.
factuality prediction	UDS-IH2	We train and evaluate our factuality prediction models on this new dataset, UDS-IH2, as well as the unified versions of UW, FactBank, and MEANTIME.
factuality prediction	UW	We train and evaluate our factuality prediction models on this new dataset, UDS-IH2, as well as the unified versions of UW, FactBank, and MEANTIME.
factuality prediction	FactBank	We train and evaluate our factuality prediction models on this new dataset, UDS-IH2, as well as the unified versions of UW, FactBank, and MEANTIME.
Oracle classification	develop- ment set	 Table 3: Oracle classification on claims in the develop- ment set using gold sentences as evidence
NER	SnapCaptions dataset	 Table 1: NER performance on the SnapCaptions dataset with varying modalities (W: word, C: char, V: visual).  We report precision, recall, and F1 score for both entity types recognition (PER, LOC, ORG, MISC) and entity  segmentation (untyped recognition -named entity or not) tasks.
Language modelling perplexity	PTB parsing test set	 Table 5: Language modelling perplexity results on the  PTB parsing test set.
Dependency parser	TWEE- BANK V2 test set	 Table 6: Dependency parser comparison on TWEE- BANK V2 test set, with automatic POS tags. We use
POS tagging	Penn Treebank WSJ corpus (Englsih)	In order to demystify the effects of adversarial training in the context of NLP, we conduct POS tagging experiments on multiple languages using the Penn Treebank WSJ corpus (Englsih) and the Universal Dependencies dataset, with thorough analyses of the following points: • Effects on different target languages • Vocabulary statistics and tagging accuracy • Influence on downstream tasks • Representation learning of words In our experiments, we find that our adversarial training model consistently outperforms the baseline POS tagger, and even achieves state-of-the-art results on 22 languages.
POS tagging	Universal Dependencies dataset	In order to demystify the effects of adversarial training in the context of NLP, we conduct POS tagging experiments on multiple languages using the Penn Treebank WSJ corpus (Englsih) and the Universal Dependencies dataset, with thorough analyses of the following points: • Effects on different target languages • Vocabulary statistics and tagging accuracy • Influence on downstream tasks • Representation learning of words In our experiments, we find that our adversarial training model consistently outperforms the baseline POS tagger, and even achieves state-of-the-art results on 22 languages.
Chunking	CoNLL-2000 task	 Table 8: Chunking F1 scores on the CoNLL-2000 task,  with other top performing models.
SST fined-grained labelling of movie reviews	Microsoft paraphrase corpus	We assess our induced representations using their framework on the following benchmarks evaluated on classification ↑accuracy (MRPC is further evaluated on ↑F1) MR classification of positive or negative movie reviews; SST fined-grained labelling of movie reviews from the Stanford sentiment treebank (SST); TREC classification of questions into k-classes; CR classification of positive or negative product reviews; SUBJ classification of a sentence into subjective or objective; MPQA classification of opinion polarity; SICK-E textual entailment classification; MRPC paraphrase identification in the Microsoft paraphrase corpus; as well as the following benchmarks evaluated on the indicated correlation metric(s) SICK-R semantic relatedness between two sentences (↑Pearson); SST-14 semantic textual similarity (↑Pearson/Spearman).
TREC classification of questions	Microsoft paraphrase corpus	We assess our induced representations using their framework on the following benchmarks evaluated on classification ↑accuracy (MRPC is further evaluated on ↑F1) MR classification of positive or negative movie reviews; SST fined-grained labelling of movie reviews from the Stanford sentiment treebank (SST); TREC classification of questions into k-classes; CR classification of positive or negative product reviews; SUBJ classification of a sentence into subjective or objective; MPQA classification of opinion polarity; SICK-E textual entailment classification; MRPC paraphrase identification in the Microsoft paraphrase corpus; as well as the following benchmarks evaluated on the indicated correlation metric(s) SICK-R semantic relatedness between two sentences (↑Pearson); SST-14 semantic textual similarity (↑Pearson/Spearman).
SUBJ classification of a sentence	Microsoft paraphrase corpus	We assess our induced representations using their framework on the following benchmarks evaluated on classification ↑accuracy (MRPC is further evaluated on ↑F1) MR classification of positive or negative movie reviews; SST fined-grained labelling of movie reviews from the Stanford sentiment treebank (SST); TREC classification of questions into k-classes; CR classification of positive or negative product reviews; SUBJ classification of a sentence into subjective or objective; MPQA classification of opinion polarity; SICK-E textual entailment classification; MRPC paraphrase identification in the Microsoft paraphrase corpus; as well as the following benchmarks evaluated on the indicated correlation metric(s) SICK-R semantic relatedness between two sentences (↑Pearson); SST-14 semantic textual similarity (↑Pearson/Spearman).
Forward prediction	movie dataset	 Table 7: Forward prediction task with noisy explanations on the movie dataset and the saliency method
RSM	DTM	Individual 19 different RSM and LDA models are trained for each year, while DTM 2 and RNN-RSM are trained over the years with 19 time steps, where paper collections fora year is input at each time step.
RSM	RNN-RSM	Individual 19 different RSM and LDA models are trained for each year, while DTM 2 and RNN-RSM are trained over the years with 19 time steps, where paper collections fora year is input at each time step.
parsing of gapping constructions	UD framework	Here, we provide the first careful analysis of parsing of gapping constructions, and we present two methods for reconstructing elided predicates in sentences with gapping within the UD framework.
parsing	TAG: Bangalore and Joshi (1999)	In TAG and CCG, the task of parsing can be decomposed into two phases (e.g. TAG: Bangalore and Joshi (1999); CCG:): supertagging, where elementary units or supertags are assigned to each lexical item and parsing where these supertags are combined together.
parsing	BiLSTMs	Specif-ically, we perform POS tagging, supertagging, and parsing using the same feature representations from the BiLSTMs.
MAN	MDTC data sets	We then validate the effectiveness of MAN in experiments on two MDTC data sets.
MAN	FDU-MTL dataset	To provide more insights on how well MAN works with other feature extractor architectures, we provide a third set of experiments on the FDU-MTL dataset (.
question answering	IWAN	 Table 3: MAP and MRR for question answering. *  indicates statistical significance with α < 0.05 in t-test  compared to IWAN (our implementation).
Translation	NMT vocabulary	 Table 3: Translation results for homographs and all words in our NMT vocabulary. We compare scores for baseline  and our best proposed model on three different language pairs. Improvements are in italic. We performed bootstrap  resampling for 1000 times: our best model improved more on homographs than all words in terms of either f1,  precision, or recall with p < 0.05, indicating statistical significance across all measures.
link prediction task	FB15k-237	We evaluate the performances of KBGAN on the link prediction task, using three knowledge base completion datasets: FB15k-237, WN18 and WN18RR.
link prediction task	WN18	We evaluate the performances of KBGAN on the link prediction task, using three knowledge base completion datasets: FB15k-237, WN18 and WN18RR.
link prediction task	WN18RR	We evaluate the performances of KBGAN on the link prediction task, using three knowledge base completion datasets: FB15k-237, WN18 and WN18RR.
FN 1.7 full structure extraction	FN 1.7 test set	 Table 4: FN 1.7 full structure extraction and frame  identification test results. Bold font indicates best  performance. FN 1.7 test set is an extension of FN  1.5 test, hence the results here are not comparable  to those reported in Table 2.
FN 1.7 full structure extraction	FN  1.5 test	 Table 4: FN 1.7 full structure extraction and frame  identification test results. Bold font indicates best  performance. FN 1.7 test set is an extension of FN  1.5 test, hence the results here are not comparable  to those reported in Table 2.
DM semantic dependencies	WSJ test data	 Table 5: Labeled parsing performance in F 1 score  for DM semantic dependencies. id denotes in- domain WSJ test data, and ood denotes out-of- domain brown corpus test data. Bold font indi- cates best performance.
headline generation	Gigaword dataset	More precisely, we train and evaluate headline generation models using the publicly-available Gigaword dataset ().
regularization	HRDE	The dropout () is applied for the purpose of regularization with the ratio of: 0.2 for the RNN in the RDE and the RDE-LTC, 0.3 for the word-level RNN part in the HRDE and the HRDE-LTC, 0.8 for the latent topic memory in the RDE-LTC and the HRDE-LTC.
regularization	HRDE-LTC	The dropout () is applied for the purpose of regularization with the ratio of: 0.2 for the RNN in the RDE and the RDE-LTC, 0.3 for the word-level RNN part in the HRDE and the HRDE-LTC, 0.8 for the latent topic memory in the RDE-LTC and the HRDE-LTC.
relation extractors	ClausIE	We measure these two relation extractors because Stanford OpenIE is included with the popular CoreNLP software and ClausIE achieves the highest recall in two systematic studies of relation extractors).
KL divergence decomposition	VHRED	 Table 2: KL divergence decomposition. VHRED  and VHCR are trained with word drop and utter- ance drop respectively.
KL divergence decomposition	VHCR	 Table 2: KL divergence decomposition. VHRED  and VHCR are trained with word drop and utter- ance drop respectively.
Instruction generation	SAIL dataset	 Table 3: Instruction generation results. We report the  accuracies of human evaluators at following the outputs  of the speaker systems (as well as other humans) on 50- instance samples from the SAIL dataset and SCONE  domains. DBW is the system of Daniele et al. (2017).  Bold numbers are new state-of-the-art results.
Natural Language Inference (NLI)	Stanford NLI (SNLI) dataset	The task of Natural Language Inference (NLI) has received a lot of attention and has elicited models which have achieved impressive results on the Stanford NLI (SNLI) dataset.
DST	movie booking dataset	 Table 2: DST results on movie booking dataset
IC	Meteor	With the advent of sequence-to-sequence approaches to IC, e.g. (, coupled with the availability of large image description datasets, the performance of IC systems showed marked improvement, at least according to automatic evaluation metrics like Meteor) and CIDEr ().
speaker naming	MovieQA dataset	To demonstrate the effectiveness of our speaker naming approach, we design a model based on an end-to-end memory network (, namely Speaker-based Convolutional Memory Network (SC-MemN2N), which relies on the MovieQA dataset, and integrates the speaker naming approach as a component in the network.
stacking	VQA test-standard server	All scores for the stacking models were obtained using the VQA test-standard server.
vanilla stacking"	SWAF	The voting and the "vanilla stacking" ensembles do not perform as well as SWAF.
Question answering	Stanford Question Answering Dataset (SQuAD)	Question answering The Stanford Question Answering Dataset (SQuAD) ( contains 100K+ crowd sourced questionanswer pairs where the answer is a span in a given Wikipedia paragraph.
Sentiment analysis	Stanford Sentiment Treebank (SST-5	Sentiment analysis The fine-grained sentiment classification task in the Stanford Sentiment Treebank (SST-5;) involves selecting one of five labels (from very negative to very positive) to describe a sentence from a movie review.
stance detection	FNC	We believe that this is because we used a realistic information retrieval approach (see Section 3), whereas the FNC corpus contains a significant number of totally unrelated document-claim pairs, e.g., about 40% of the unrelated examples have no word overlap with the claim (even after stemming!), which makes it much easier to correctly predict the unrelated class (and this class is also by far the largest).: Performance of some stance detection models from FNC when applied to our Arabic corpus.
stance detection	FNC	 Table 2: Performance of some stance detection models from FNC when applied to our Arabic corpus.
machine translation (MT)	Simple English Wikipedia (SEW)	Prior work has explored monolingual machine translation (MT) approaches, utilizing corpora of simplified texts, e.g., Simple English Wikipedia (SEW), and making use of statistical MT models, such as phrase-based MT (PBMT), tree-based MT (TBMT) (, or syntax-based MT (SBMT) ().
SARI	NSELSTM-S	With regard to SARI, NSELSTM-S scored best among neural models   trained on a huge corpus of 106M sentence pairs and 2B words -scored highest on SARI with 39.96, followed by DRESS-LS (37.27), DRESS (37.08), and NSELSTM-S (36.88).
AMR parsing	AMR-COVINGTON	This paper explores unrestricted non-projective AMR parsing and introduces AMR-COVINGTON, inspired by.
STM	CogALex-V dataset	Our results show that, despite its simplicity, STM outperforms more complex models on the benchmarking CogALex-V dataset (.
STM	WN-LS dataset	 Table 2: STM performance for three languages on (re- spective translations of) the WN-LS dataset.
AMR parser	Chinese AMR Bank	In this paper, we present the first AMR parser built using the Chinese AMR Bank ( . We adopt the transition-based parsing framework first proposed for English (, where AMR parsing is modeled as a dependency tree to AMR graph transformation using a set of linguistically motivated actions.
sentiment analysis	SemEval-2017 shared task 5	We evaluate our proposed approach for sentiment analysis on the benchmark datasets of SemEval-2017 shared task 5.
SMT	CoNLL-2014 test set	Systems that rely on neural machine translation are not yet able to achieve as high performance as SMT systems according to automatic evaluation metrics (see for comparison on the CoNLL-2014 test set).
SMT output	JFLEG benchmark	Such a GEC system preserves the accuracy of SMT output and at the same time generates more fluent sentences achieving new state-of-the-art results on two different benchmarks: the annotationbased CoNLL-2014 and the fluency-based JFLEG benchmark.
SMT	NMT	Using consistent training data and preprocessing ( § 2), we first create strong SMT ( § 3) and NMT ( § 4) baseline systems.
SMT	CoNLL-2014 (M 2 ) and JFLEG Test (GLEU) sets	 Table 2: Results for SMT baseline systems on the  CoNLL-2014 (M 2 ) and JFLEG Test (GLEU) sets.
SMT-NMT	CoNLL-2014 (M 2 )	 Table 4: Results for hybrid SMT-NMT systems on the  CoNLL-2014 (M 2 ) and JFLEG Test (GLEU) sets.
SMT-NMT	JFLEG Test (GLEU) sets	 Table 4: Results for hybrid SMT-NMT systems on the  CoNLL-2014 (M 2 ) and JFLEG Test (GLEU) sets.
CRF	Stanford NER tagger	For the CRF implementation, we used the Stanford NER tagger (Finkel et al.,
paraphrase generation	Paraphrase Database (PPDB 2.0) corpus	For training the paraphrase generation models, we use a subset of the Paraphrase Database (PPDB 2.0) corpus.
link prediction	WN18RR	Experiments show that ConvKB achieves better link prediction performance than previous state-of-the-art embedding models on two benchmark datasets WN18RR and FB15k-237.
link prediction	FB15k-237	Experiments show that ConvKB achieves better link prediction performance than previous state-of-the-art embedding models on two benchmark datasets WN18RR and FB15k-237.
trigger identification	ACE data	In particular, it achieves 1.1 and 1.3 point F1 improvements over a state-of-the-art system in trigger identification on TAC-KBP and ACE data respectively.
Scalability	TED corpus	The syntactical divergence and rich morphology of German posits a Scalability: The preliminary results were obtained using models trained on the TED corpus only.
summarization	English datasets	Finally, our approach aims at improving summarization systems based on human judgments, therefore we also setup a manual evaluation for the two English datasets.
coreference resolution	English OntoNotes benchmark	Our experiments show that both of the above contributions improve the performance of coreference resolution on the English OntoNotes benchmark.
coreference resolution	CoNLL-2012 shared task	We use the English coreference resolution data from the CoNLL-2012 shared task () in our experiments.
Caption manipulation	CONCAT	Caption manipulation: Our last evaluation measures how much influence the caption has on the CONCAT and FILM models.
Generalized Knowledge Hunting	Winograd Schema	A Generalized Knowledge Hunting Framework for the Winograd Schema Challenge
NER	CoNLL NER dataset	We evaluate NER using the F1 score on the CoNLL NER dataset).
sentiment classification task	SWESA	 Table 1: This table shows results from a standard  sentiment classification task on all four data sets.  Results from SWESA are in boldface and results  from pre-trained RNTN are in blue.
sentiment classification task	KCCA  (GlvCC, LSA)	 Table 3: This table shows results obtained by us- ing sentence embeddings from the InferSent en- coder in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard devi- ations (STD). Best results are obtained by KCCA  (GlvCC, LSA) and are highlighted in boldface.
MTE	WMT-2017 Metrics task	However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference translation based on character N-grams or word N-grams, such as SentBLEU (), which is a smoothed version of BLEU (), Blend (, MEANT 2.0 (, and chrF++, which achieved excellent results in the WMT-2017 Metrics task).
MT hypothesis	WMT-2017 Metrics task	However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference translation based on character N-grams or word N-grams, such as SentBLEU (), which is a smoothed version of BLEU (), Blend (, MEANT 2.0 (, and chrF++, which achieved excellent results in the WMT-2017 Metrics task).
SM CNN	TrecQA dataset	For SM CNN, we used input sentence pairs from the validation set of the TrecQA dataset (1148 sentences).
provisioning	DynamoDB	Note that these costs do not include provisioning DynamoDB, which costs 0.013 cents per Read Capacity Unit per hour.
Generation	NAACL HLT 2018	Recently, he was nominated as an area co-chair for Generation at NAACL HLT 2018.
Complex Word Identification (CWI) shared task	NAACL-HLT'2018	We report the findings of the second Complex Word Identification (CWI) shared task organized as part of the BEA workshop co-located with NAACL-HLT'2018.
Complex Word Identification (CWI) shared task	NAACL-HLT'2018	In this paper we present the findings of the second Complex Word Identification (CWI) shared task organized as part of the thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA) co-located with NAACL-HLT'2018.
AES	CEFR scale	We study AES for multiple languages for the first time using CEFR scale.
predicting a student's mistake	Duolingo application	SLAM 2018 focuses on predicting a student's mistake while using the Duolingo application.
SLAM	Duolingo	The 2018 Shared Task on SLAM provides student trace data from users on the online educational platform Duolingo (.
SLA modeling-the	Duolingo challenge datasets	It remains an open question whether the existing knowledge tracing techniques can be directly applied to SLA modeling-the release of the Duolingo challenge datasets now enables us to investigate this very question.
SIB	UMLS	In terms of ontology relations, SIB includes only candidates that are considered to be the siblings of the correct answer (according to UMLS).
Predicting Psychological Health from Childhood Essays	CLPsych 2018 Shared Task (Team UKNLP)	Predicting Psychological Health from Childhood Essays with Convolutional Neural Networks for the CLPsych 2018 Shared Task (Team UKNLP)
Predicting Psychological Health	UGent-IDLab CLPsych 2018 Shared Task System	Predicting Psychological Health from Childhood Essays The UGent-IDLab CLPsych 2018 Shared Task System
Bridging Resolution	ARRAU	Rule-and Learning-based Methods for Bridging Resolution in the ARRAU Corpus
bridging resolution	ARRAU corpus	We present two systems for bridging resolution , which we submitted to the CRAC shared task on bridging anaphora resolution in the ARRAU corpus (track 2): a rule-based approach following Hou et al.
bridging resolution	ARRAU corpus	The first shared task on bridging resolution, co-located with the workshop on computational models of reference, anaphora and coreference (CRAC), deals with the task of bridging anaphora resolution in the RST domain of the ARRAU corpus (.
coreference resolution	OntoNotes corpus	Data We based our experiments on the benchmark dataset for coreference resolution, the OntoNotes corpus).
semantic relation prediction	OntoNotes corpus	In order to obtain candidate pairs for semantic relation prediction, we considered all heads of noun phrases in the OntoNotes corpus) and combined them with preceding heads of noun phrases in the same document.
metaphor identification	VU Amsterdam Metaphor Corpus	We report on the shared task on metaphor identification on the VU Amsterdam Metaphor Corpus conducted at the NAACL 2018 Workshop on Figurative Language Processing .
metaphor detection	Vrije University Amsterdam Metaphor Corpus (VUA)	This article describes the system that participated in the shared task (ST) on metaphor detection (Leong et al., 2018) on the Vrije University Amsterdam Metaphor Corpus (VUA).
WEs	Vienna-Oxford International Corpus of English (VOICE)	The WEs were trained (cf. Section 4.2) on different large corpora (BNC, Wikipedia, enTenTen13, ukWaC) and on the Vienna-Oxford International Corpus of English (VOICE) as well as on the TOEFL11 Corpus of Non-Native English.
WEs	TOEFL11 Corpus of Non-Native English	The WEs were trained (cf. Section 4.2) on different large corpora (BNC, Wikipedia, enTenTen13, ukWaC) and on the Vienna-Oxford International Corpus of English (VOICE) as well as on the TOEFL11 Corpus of Non-Native English.
personality prediction	MBTI	Most work on personality prediction rests on one of the two widely used personality models: Big Five and MBTI.
parsing	GKR	Section 4 evaluates the current parsing into GKR, while section 5 discusses our future additions to the system.
translation studies	LCMC	ZCTC was created specifically for translation studies "as a comparable counterpart of translated Chinese" to LCMC, with the same genre distribution and also one million words in total.
Word sense induction (WSI)	WordNet	Word sense induction (WSI) is a challenging task of natural language processing whose goal is to categorize and identify multiple senses of polysemous words from raw text without the help of predefined sense inventory like WordNet.
ASR	French Globalphone dev set	As a consequence, our ASR results are far from state-of-the-art: current end-to-end Kaldi systems obtain 16% WER on Switchboard train-dev, and 22.7% WER on the French Globalphone dev set.
ASR	CMU Wilderness Multilingual Speech Dataset	In this paper we explore pretraining multilingual ASR models using speech from as many as voxforge.org 100 languages from the CMU Wilderness Multilingual Speech Dataset.
copying	GEC task	The copying mechanism is for the first time used on the GEC task, which was used on text summarization tasks.
copying	UNK	In the experiment section of this paper, we show that copying does more than just solving the "UNK problem", and it can also recall more edits for the GEC problem.
SMT	CNN + FB Learning	In, "SMT (with LM)" refers to  "SMT Rule-Based Hybird" refers to); "SMT Classification Hybird" refers to; "Neural Hybird MT" refers to (; "CNN + EO" refers to and "EO" means rerank with edit-operation features; "Transformer + MIMs" refers to  and "MIMs" means model indepent methods; "NMT SMT Hybrid" refers to (Grundkiewicz and Junczys-Dowmunt, 2018); "CNN + FB Learning" refers to (Ge et al., 2018).
SMT Rule-Based Hybird	CNN + FB Learning	In, "SMT (with LM)" refers to  "SMT Rule-Based Hybird" refers to); "SMT Classification Hybird" refers to; "Neural Hybird MT" refers to (; "CNN + EO" refers to and "EO" means rerank with edit-operation features; "Transformer + MIMs" refers to  and "MIMs" means model indepent methods; "NMT SMT Hybrid" refers to (Grundkiewicz and Junczys-Dowmunt, 2018); "CNN + FB Learning" refers to (Ge et al., 2018).
PoS Taggers	UD Project	How Bad are PoS Taggers in Cross-Corpora Settings? Evaluating Annotation Divergence in the UD Project
Evaluating Annotation Divergence	UD Project	How Bad are PoS Taggers in Cross-Corpora Settings? Evaluating Annotation Divergence in the UD Project
Part-of-Speech tagging	Universal Dependencies project	The performance of Part-of-Speech tagging varies significantly across the treebanks of the Universal Dependencies project.
dialog generation	CLEVR-Dialog	In what follows, we will first describe these primitives, discuss how they are used to generate a caption or a question at each round, and tie everything together to explain dialog generation in CLEVR-Dialog.
satire classification	RQ1	Hence, we chose λ = 0.2 (the best performing model on satire classification) and λ = 0.7 (the worst performing model on publication identification) to investigate RQ1.
publication identification	RQ1	Hence, we chose λ = 0.2 (the best performing model on satire classification) and λ = 0.7 (the worst performing model on publication identification) to investigate RQ1.
dependency parsing	CoNNL 09 corpus	Following, who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus.
parsing	PTB-SD	Apart from increasing the parsing speed twofold (while keeping the same quadratic time complexity), it achieves the best accuracy to date among fully-supervised single-model dependency parsers on the PTB-SD, and obtains competitive accuracies on twelve different languages in comparison to the original top-down version.
common named entity recognition (NER) tasks	CoNLL-03	We evaluate these pooled contextualized embeddings on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER.
common named entity recognition (NER) tasks	WNUT	We evaluate these pooled contextualized embeddings on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER.
entity recognition (NER)	CONLL-03 shared task	We verify our proposed approach in four named entity recognition (NER) tasks: We use the English, German and Dutch evaluation setups of the CONLL-03 shared task) to evaluate our approach on classic newswire data, and the WNUT-17 task on emerging entity detection to evaluate our approach in a noisy user-generated data setting with few repeated entity mentions.
IE task	CONLL03 dataset	Specifically, in the textual IE task, we obtain an improvement of 0.5% over SeqIE on the CONLL03 dataset, and an improvement of 1.4% on the chemical entity extraction ().
NER	CONLL03	 Table 4: NER accuracy on the CONLL03 and the  CHEMDNER datasets (Task 1). Scores for our methods  are the average of 5 runs. * indicates statistical signifi- cance of the improvement over SeqIE (p < 0.01).
NER	CHEMDNER datasets	 Table 4: NER accuracy on the CONLL03 and the  CHEMDNER datasets (Task 1). Scores for our methods  are the average of 5 runs. * indicates statistical signifi- cance of the improvement over SeqIE (p < 0.01).
Event coreference resolution	biLSTM baseline model	 Table 3: Event coreference resolution results of our proposed system, compared with the biLSTM baseline model  and the current state-of-the-art system.
metaphor generation	Wiki corpus	In this paper, we propose a neural approach for metaphor generation trained with Wiki corpus rather than the limited annotated metaphor corpus, which assures the quality of the language model.
Coreference Resolution	WSC	Finally, we also compare to the pre-trained Coreference Resolution Tool , and we can see that it doesn't adapt to our commonsense reasoning tasks and can't tell https://github.com/huggingface/ neuralcoref the difference between each pair of sentences from WSC.
Document classification	20 Newsgroups  dataset	 Table 3: Document classification accuracy a five-way  classification on the comp subject of 20 Newsgroups  dataset and on the top five most frequent labels of  ohsumed dataset (no labels for NIPS dataset).
Document classification	NIPS dataset	 Table 3: Document classification accuracy a five-way  classification on the comp subject of 20 Newsgroups  dataset and on the top five most frequent labels of  ohsumed dataset (no labels for NIPS dataset).
link prediction task	WN18	(2) We evaluate ConvR in the link prediction task on KBs and achieve very promising results on multiple benchmark datasets, including not only the popular WN18 and FB15K (), but also the more difficult WN18RR () and FB15K-237 (   increase in MRR and a 6% increase in Hits@10, with the total parameter number only 88% as large as that of ConvE.
link prediction task	FB15K	(2) We evaluate ConvR in the link prediction task on KBs and achieve very promising results on multiple benchmark datasets, including not only the popular WN18 and FB15K (), but also the more difficult WN18RR () and FB15K-237 (   increase in MRR and a 6% increase in Hits@10, with the total parameter number only 88% as large as that of ConvE.
link prediction task	WN18RR	(2) We evaluate ConvR in the link prediction task on KBs and achieve very promising results on multiple benchmark datasets, including not only the popular WN18 and FB15K (), but also the more difficult WN18RR () and FB15K-237 (   increase in MRR and a 6% increase in Hits@10, with the total parameter number only 88% as large as that of ConvE.
link prediction task	FB15K-237	(2) We evaluate ConvR in the link prediction task on KBs and achieve very promising results on multiple benchmark datasets, including not only the popular WN18 and FB15K (), but also the more difficult WN18RR () and FB15K-237 (   increase in MRR and a 6% increase in Hits@10, with the total parameter number only 88% as large as that of ConvE.
link prediction	WN18	Therefore, link prediction models should perform well on WN18.
Segment recall	PRPN-LM	 Table 4: Segment recall from WSJ separated by phrase  type. The 10 most frequent phrase types are shown  above, and the highest value in each row is bolded. P- UP=PRNP-UP, P-LM=PRPN-LM
LM evaluation	CoNLL 2003	We use two datasets: Recipe used only for LM evaluation and CoNLL 2003 used for both the LM and NER evaluations.
SMD	KA datasets	 Table 5: AMT Evaluations on CamRest and SMD (50%  unseen) KA datasets
MTSA	SNLI	 Table 3: An ablation study of MTSA on SNLI.
SRL	CoNLL-05	 Table 4: Experimental Results of SRL for single models on CoNLL-05 with gold predicates.  *  Multi-head baseline  is equivalent to the model in Tan et al. (2017). For fair comparisons, first, we use the hyper-parameters provided  by Tan et al. (2017) instead of tuning them; second, all listed models are independent of external linguistics  information, e.g., PoS, dependency parsing.
Genia event extraction task	Genia event extraction 2011 dataset	 Table 1: Predefined event types with accepted argu- ment roles in Genia event extraction task, and data  statistics of Genia event extraction 2011 dataset. P:  protein; E: event.
Cognitive impairments	MoCA	Cognitive impairments are traditionally diagnosed only with standard clinical tests like MoCA () and the ReyAuditory Verbal learning Test, but hiring clinicians to administer these tests and analyze their results is costly.
Cookie Theft picture description task	Boston Diagnostic Aphasia Examination	It includes verbal descriptions (and associated transcripts) of the Cookie Theft picture description task from the Boston Diagnostic Aphasia Examination.
MT	NEWS	We trained the MT models using a concatenation of the NEWS and the TED training datasets, and we tested on official TED test sets (testsets-11-13) to perform the evaluation using BLEU ().
MT	TED training datasets	We trained the MT models using a concatenation of the NEWS and the TED training datasets, and we tested on official TED test sets (testsets-11-13) to perform the evaluation using BLEU ().
MT	TED test sets (testsets-11-13	We trained the MT models using a concatenation of the NEWS and the TED training datasets, and we tested on official TED test sets (testsets-11-13) to perform the evaluation using BLEU ().
semantic tagging	Groningen Parallel Meaning Bank	For semantic tagging, we used the gold-annotated semantic tags from the Groningen Parallel Meaning Bank (, which were made available by for more detailed statistics about the train/dev/test datasets we used.
tokenization	Weibo	For tokenization and word segmentation, we employed the tweet preprocessing toolkit released by for Twitter, and the Jieba toolkit 6 for Weibo.
keyphrase extraction	Tex-tRank	In the context of keyphrase extraction, incorporating global random walk scores obtained from Tex-tRank boosts performance significantly.
Navigation	Matterport	 Table 2: Navigation results for Matterport when trained  using student forcing. Best unimodal in bold; better  than reported baseline.
classify AD	DementiaBank (DB) dataset	Previous models have applied machine learning to automatic detection of AD, for example, extracted a wide variety of lexicosyntactic and acoustic features to classify AD and obtained 82% accuracy on the DementiaBank (DB) dataset.
ASR	En-De IWSLT18	 Table 4: Performance of ASR, MT and SLT systems  trained with En-De IWSLT18 and MuST-C data.
ASR	MuST-C data	 Table 4: Performance of ASR, MT and SLT systems  trained with En-De IWSLT18 and MuST-C data.
MT	En-De IWSLT18	 Table 4: Performance of ASR, MT and SLT systems  trained with En-De IWSLT18 and MuST-C data.
MT	MuST-C data	 Table 4: Performance of ASR, MT and SLT systems  trained with En-De IWSLT18 and MuST-C data.
negation	length mismatch NLI challenge datasets	The word overlap, negation, spelling errors and length mismatch NLI challenge datasets, as well as Adversarial SQuAD, include splits for training and evaluation.
reminding	Doc2hash	We compare the reminding methods with Doc2hash.
Document concatenation	Weaver	Document concatenation in this setting is also used in Weaver () and MHPGM ().
Document concatenation	MHPGM	Document concatenation in this setting is also used in Weaver () and MHPGM ().
RRC	laptop and restaurant reviews of SemEval 2016 Task 5	As there are no existing datasets for RRC and to be consistent with existing research on sentiment analysis, we adopt the laptop and restaurant reviews of SemEval 2016 Task 5 as the source to create datasets for RRC.
sentiment analysis	laptop and restaurant reviews of SemEval 2016 Task 5	As there are no existing datasets for RRC and to be consistent with existing research on sentiment analysis, we adopt the laptop and restaurant reviews of SemEval 2016 Task 5 as the source to create datasets for RRC.
sentiment analysis tasks	RRC	Note that the annotations for sentiment analysis tasks are not exposed to annotators to avoid biased annotation on RRC.
ASC	SemEval 2014 Task 4	For ASC, we use SemEval 2014 Task 4 for both laptop and restaurant as existing research frequently uses this version.
MRC	SQuAD 1.1	To be consistent with existing research on MRC, we use the same evaluation script from SQuAD 1.1 ( for RRC, which reports Exact Match (EM) and F1 scores.
MRC	RRC	To be consistent with existing research on MRC, we use the same evaluation script from SQuAD 1.1 ( for RRC, which reports Exact Match (EM) and F1 scores.
name tagging	LSTM-CNN baseline	In the name tagging task, our LSTM-CNN baseline obtains 78.76% and 70.76% F1 score for Kinyarwanda and Sinhalese, respectively.
Sentiment Classification	IMDB Bench- mark	 Table 3: Sentiment Classification on IMDB Bench- mark. *Results are collected from Chen (2017).
abstractive summarization	Reddit TIFU	Our new large-scale dataset for abstractive summarization named as Reddit TIFU contains 122,933 pairs of an online post as source text and its corresponding long or short summary sentence.
MRC task	MultiRC	We further fine-tune the resulting model on a target MRC task, leading to an absolute improvement of 6.2% in average accuracy over previous state-of-the-art approaches on six representative non-extractive MRC datasets from different domains (i.e., ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, ROCStories, and MultiRC).
MRC	MS MARCO	However, most public MRC datasets (e.g., SQuAD, MS MARCO, TriviaQA) are typically small (less than 100K) compared to the model size (such as SAN (,b) with around 10M parameters).
segmentation	NER	They can provide more useful character disambiguation evidence in segmentation than in NER where lattice LSTM works well in disambiguating characters.
segmentation	SIGHAN Bakeoff	For consistency, all segmentation results are automatically calculated with the script provided in the SIGHAN Bakeoff ( and are reported as word F-measures.
question classification	PubMed and PMC texts	For the embedding we use word2vec embeddings pre-trained on Google News () for the question classification and RTE tasks, and a pre-trained embedding () trained on a combination of English Wikipedia, PubMed and PMC texts for the sentence classification task.
relation extraction	NYT dataset	• We construct anew accurate dataset for relation extraction extended from the NYT dataset.
image captioning	MS COCO	For image captioning, we used two datasets: MS COCO () and Flickr8K.
image captioning	Flickr8K	For image captioning, we used two datasets: MS COCO () and Flickr8K.
SGM	AAPD dataset	The SGM results are taken directly from (, and are originally reported only on AAPD dataset in terms of hamming-loss and micro-F1.
regularization	RNNLM	However, the regularization approach in that work had tractability limitationsthe time cost of the regularization was sufficiently high that using it was inferior to simply training the RNNLM on more sequential text.
regularization	JS	From the table, we can see that regularization with Euclidean distance performs the best compared to KL and JS.
sequence tagging constituent parsers	English and Chi-nese Penn Treebanks	Combining these techniques, we clearly surpass the performance of sequence tagging constituent parsers on the English and Chi-nese Penn Treebanks, and reduce their parsing time even further.
transfer learning	DISCOFUSE	Last, we performed transfer learning by training on DISCOFUSE and then fine-tuning on a smaller dataset from a different distribution.
coreference resolution	Google Cloud Natural Language	Documents were annotated with dependency trees and coreference resolution using Google Cloud Natural Language.
splitting task	WEBSPLIT	For the last two models, we use the CopyNet architecture (, which is similar  to state-of-the-art models for the splitting task on WEBSPLIT (.
word description task	WordNet	Datasets To evaluate our model on the word description task on WordNet, we followed and extracted data from WordNet using the dict-definition 9 toolkit.
word description task	WordNet	Datasets To evaluate our model on the word description task on WordNet, we followed and extracted data from WordNet using the dict-definition 9 toolkit.
NER	CoNLL-2003 dataset	 Table 7: Results of the NER experiments on the  CoNLL-2003 dataset.
TE	SNLI  dataset	 Table 8: Results of the TE experiments on the SNLI  dataset.
Question Answering (QA)	CNN News Articles	For Question Answering (QA): CNN News Articles (.
translation	WMT14 English⇒German	For translation tasks, we validate our approach on top of the advanced TRANSFORMER model () on both WMT14 English⇒German and WMT17 Chinese⇒English data.
translation	WMT17 Chinese⇒English data	For translation tasks, we validate our approach on top of the advanced TRANSFORMER model () on both WMT14 English⇒German and WMT17 Chinese⇒English data.
generalization	SQuAD datasets	Our representations also aid in better generalization with gains of around 6-7% on adversarial SQuAD datasets, and 8.8% on the adversarial entail-ment test set by Glockner et al.
detection of dementia	French dataset	The two aims of this study are: (1) to identify a set of features that are both useful for the detection of dementia and that we expect to transfer across different languages, and (2) to improve classification results on the French dataset by augmenting the training set with English data.
Reiss Classification	MNPCSCS dataset	 Table 3: Model ablations for Reiss Classification on  MNPCSCS dataset w/o belonging.
parsing	Newswire text	Here we explore parsing issues that arose when running our system, a tool built on Newswire text, on clinical notes in the THYME corpus.
parsing	THYME corpus	Here we explore parsing issues that arose when running our system, a tool built on Newswire text, on clinical notes in the THYME corpus.
TAC-KBP 2015 English Entity Discovery and Linking () data set	Automatic Content Extraction (ACE2005) data set	We create the English annotated resource by combining the TAC-KBP 2015 English Entity Discovery and Linking () data set and the Automatic Content Extraction (ACE2005) data set.
Dependency parsing	Universal Dependencies v2 test sets	 Table 2: Dependency parsing results, in terms of unlabeled attachment accuracy (UAS) and labeled attachment  accuracy (LAS) after ignoring punctuations, on the Universal Dependencies v2 test sets (Nivre et al., 2017) using  supervised part-of-speech tags. The results are sorted by their "difference" between the ensemble model and the  baseline. The rows for non-European languages are highlighted with cyan. The rows that are highlighted by pink  are the ones that the transfer model outperforms the supervised model. For all of the non-European datasets except  "hi", our model outperforms significantly better in terms of UAS with p < 0.001 using McNemar's test.
Dependency parsing	McNemar's test	 Table 2: Dependency parsing results, in terms of unlabeled attachment accuracy (UAS) and labeled attachment  accuracy (LAS) after ignoring punctuations, on the Universal Dependencies v2 test sets (Nivre et al., 2017) using  supervised part-of-speech tags. The results are sorted by their "difference" between the ensemble model and the  baseline. The rows for non-European languages are highlighted with cyan. The rows that are highlighted by pink  are the ones that the transfer model outperforms the supervised model. For all of the non-European datasets except  "hi", our model outperforms significantly better in terms of UAS with p < 0.001 using McNemar's test.
Machine Translation	Europarl	In many NLP tasks such as Machine Translation there are large, curated datasets (e.g. Europarl) used by several research groups.
QA task	CNN/Daily Mail dataset	Concerning issue 2, we focus on a relatively easy type of questions: given source documents and associated questions, a QA system can be trained over fill-in-the-blank type questions as was shown in and . In their work,  achieve 'ceiling performance' for the QA task on the CNN/Daily Mail dataset.
summarization task	Gigaword corpus	The first is a short summarization task, where the summary is one sentence long, for which we use the Gigaword corpus ().
headline generation task	Japanese and English datasets	We conduct experiments on the headline generation task on Japanese and English datasets.
WMT'18 English-German (en-de) news translation task	WMT'18 EnglishTurkish (en-tr) news task	We consider two benchmarks: Most experiments are run on the WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 EnglishTurkish (en-tr) news task.
WAE	VAE	More importantly, WAE is robust to hyperparameters and much easier to train, without the need for KL annealing or word dropout as in VAE.
transfer learning	Overnight and NLmaps	With either flavor of transfer learning , we are able to improve performance on most domains; we experiment with public data sets such as Overnight and NLmaps as well as with commercial SLU data.
parsing	Overnight	• We report parsing baselines for Overnight for which exact match parsing scores have not been yet published.
Transfer learning	Overnight domains	 Table 3: Transfer learning results for the Overnight domains.
dialect identification of Arabic text	Modern Standard Arabic (MSA)	In this demo paper, we present ADIDA, 1 a public online interface for visualizing fine-grained dialect identification of Arabic text . The dialect identification system produces a vector of probabilities indicating the likelihood an input sentence is from 25 cities ( and Modern Standard Arabic (MSA).
fact checking process	FAKTA	While previous works separately investigated individual components of the fact checking process, in this work, we present a unified framework titled FAKTA that integrates these components to not only predict the factuality of given claims, but also provide evidence at the document and sentence level to explain its predictions.
OOV handling	WordNet	User input OOV handling is done via WordNet by recursively searching for hypernyms and hyponyms (in that order) until either an in-vocabulary word is found or until a maximum distance from the initial word is reached.
predicting stance	Baltimore datasets	 Table 2: Result of predicting stance (first 12 columns) and morality (last two columns) with SVM and RF for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each half  column) in bold, highest accuracy per each model (each column) in gray)
predicting stance	Baltimore datasets	 Table 3: Result of predicting stance (first 7 columns) and morality (last column) with LSTM model for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each  half column) in bold, highest accuracy per each model (each column) in gray)
LM adaptation	Chinese datasets	We modified our LM adaptation technique to be used with the NB classifier and this fared better than the adaptive HeLI 2.0 method on both of the Chinese datasets.
PMA	EMA	Second, we compared the best performance in PMA with the performance in EMA.
PMA	EMA data	For the 132 phrases in both PMA and EMA data, 110 phrases were used for training, 10 for validating, and 12 for testing.
Extractive Clinical Note Summarization	EHR	A Novel System for Extractive Clinical Note Summarization using EHR Data
hedging detection	Bioscience domain	Furthermore, inspired by the work on hedging detection on Bioscience domain (, we explore a list of "speculation cues" to filter out non-factual min/max age expressions.
compositionality prediction	RAMISCH dataset	 Table 2: Pearson correlation coefficient for compositionality prediction results on the RAMISCH dataset.
word relatedness/similarity tasks	WS-3533	Our proposed task is most similar to the word relatedness/similarity tasks, several of which have already been proposed in literature: WS-3533 (), WS-SIM and WS-REL (), RG-65,), Rare Words (, etc.
GPT	CODAH-only and SWAG+CODAH experiments	Unsurprisingly, GPT performance improves with more data on both the CODAH-only and SWAG+CODAH experiments, with the rate of improvement slowing down as data size increases.
NER task	BC2GM dataset	Data: For the NER task, we use the BC2GM dataset.
stance prediction	MFC baseline	 Table 3: Performance of stance prediction models trained only on user attributes, shown here for each of the  different stance targets. Bold indicates best in column for user attributes and inferred factors. The weighted F1  is shown for each target and the last column is the unweighted average across all targets.  † indicates statistical  significance at the 0.05 level compared to the MFC baseline.
Paraphrase Generation	NLU	Paraphrase Generation for Semi-Supervised Learning in NLU
validation	Europarl dataset	For the training, validation and test splits, we used 200k, after filtering, randomly chosen sentences from the Europarl dataset for training and 40k sentences for testing.
Computational analysis of genre	Brown corpus	Computational analysis of genre has most often involved material from a single source (e.g., a newspaper corpus, for which the goal is to distinguish between news articles and opinion pieces) or from standard, well-curated test corpora that contain primarily non-literary texts (e.g., the Brown corpus or equivalents in other languages) ().
normalizing	CEEC	In this paper, we will present different NMT models and evaluate their effectiveness in normalizing the CEEC.
coreference resolution	ACL anthology's paper identifier	Example (1) discusses the cons of previous technologies for coreference resolution: (1) While successful, these approaches require labeled training data, consisting of mention 1 Throughout the paper, an appended 8-character identifier indicates the ACL anthology's paper identifier.
Segmented Discourse Representation Theory (SDRT)	Penn Discourse Treebank (PDTB)	Several theoretical frameworks exist for discourse analysis, and automatic discourse analyzers (ADA) have been developed within each framework, but mostly for English texts: i) under Rhetorical Structure Theory (RST) (: see for example ( ii) under Segmented Discourse Representation Theory (SDRT), as the one developed by iii) or Penn Discourse Treebank (PDTB) style as the one described in ().
Rhetorical Structure Theory (RST)	Penn Discourse Treebank (PDTB)	Among the most popular approaches are Rhetorical Structure Theory (RST) (, the Penn Discourse Treebank (PDTB) (, Segmented Discourse Representation Theory (SDRT) ( and the Cognitive approach to Coherence Relations (CCR) (.
Segmented Discourse Representation Theory (SDRT)	Penn Discourse TreeBank (PDTB)	The task definition differs slightly across the various existing and competing formalisms: in Rhetorical Structure Theory (RST) (, all segments are adjacent while in Segmented Discourse Representation Theory (SDRT), segments can be embedded in one another; In the Penn Discourse TreeBank (PDTB), the task is expressed as finding the arguments of a discourse connective, whether this connective is implicit or explicit.
RST SS	RST Dis-  course Treebank	For comparison, we trained three different parsers on both RST DT and RST SS: (a) RST SS: using the training set from the corpus of non-native spontaneous speech, where 49 double-annotated responses were used as the development set; (b) RST DT: using the training set from the RST Dis-  course Treebank, where 40 samples from the training set were separated as the development set; and (c) RST SS + RST DT: using the training sets from both RST SS and RST DT, where the development set is the same one used in (a).
coreference resolution	OntoNotes benchmark	We propose an end-to-end coreference resolution system obtained by adapting neural models that have recently improved the state-of-the-art on the OntoNotes benchmark to make them applicable to other paradigms for this task.
coreference resolution	CoNLL-2012	For coreference resolution, we use the CoNLL-2012 metrics () including BLANC.
estimating suicide risk	Suicide Watch	Task C has the goal of estimating suicide risk by looking at a subject's posts on different subreddits, but excluding Suicide Watch.
Predicting Suicide Risk from Online Postings	CLPysch 2019 Shared Task	Predicting Suicide Risk from Online Postings in Reddit The UGent-IDLab submission to the CLPysch 2019 Shared Task A
name tagging	DARPA LORELEI program 1	Since Chinese name tagging is a well-studied problem, we choose Hausa instead of Chinese as the LL for name tagging experiment, because we can use the ground truth from the DARPA LORELEI program 1 for evaluation.
translation	GroundHog	 Table 1: Evaluation of translation quality. d denotes the dimension of NN-based coverages, and  † and  ‡  indicate statistically significant difference (p < 0.01) from GroundHog and Moses, respectively. "+" is  on top of the baseline system GroundHog.
translation	GroundHog	 Table 1: Evaluation of translation quality. d denotes the dimension of NN-based coverages, and  † and  ‡  indicate statistically significant difference (p < 0.01) from GroundHog and Moses, respectively. "+" is  on top of the baseline system GroundHog.
MWE identification	FMWE	MWE identification is evaluated with the F-score of the MWE segmentation, namely MWE for all MWEs and FMWE for fixed MWEs only.
metaphor detection	abreast cancer discussion forum dataset	To demonstrate the efficacy of our approach, we evaluate our system on the metaphor detection task presented by using abreast cancer discussion forum dataset.
WE spaces	Polyglot website	To induce monolingual WE spaces, two monolingual SGNS models were trained on the cleaned and tokenized Wikipedias from the Polyglot website (Al-Rfou et al., 2013) using SGD with a global learning rate of 0.025.
sentiment classifier	Book domain	Thus a sentiment classifier trained in Electronics domain cannot accurately predict their sentiments in Book domain.
Lucene	Waterloo corpus baseline	For Lucene, the improvements overall but the Waterloo corpus baseline are statistically significant.
MLN	Regents	For MLN, the improvements are statistically significant in the case of Regents and Regents+Monarch tables.
MLN	Regents+Monarch tables	For MLN, the improvements are statistically significant in the case of Regents and Regents+Monarch tables.
generat- ing MCQs	MTurk	 Table 2: Comparison of different ways of generat- ing MCQs with MTurk.
summarization	DailyMail website	Inspired by previous work on summarization and reading comprehension ( we retrieve hundreds of thousands of news articles and corresponding highlights from the DailyMail website.
text classification	NewsTile	For text classification task, we use three datasets: NewsTile, TREC and Twitter.
text classification	TREC	For text classification task, we use three datasets: NewsTile, TREC and Twitter.
answer selection	DocChat	Take into account response ranking task and answer selection task are similar, we first evaluate DocChat in a QA scenario as a simulation.
AS task	WikiQA	 Table 2: Evaluation of AS task on WikiQA.
REG	ReferIt data set	To the best of our knowledge, end-to-end REG performance has not been reported on the ReferIt data set before.
calculation	CCP	13 depicts the calculation of CCP.
POS tagging	TLE dataset	Finally, we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy.
Tagging	TLE corpus	The EWT training regime, which uses out of domain texts written in standard English, provides the lowest performance on all the evaluation met-: Tagging and parsing results on a test set of 500 sentences from the TLE corpus.
Tagging	TLE test set	This comparison indeed reveals a substantial difference between the two types of tokens, with an average gap of: Tagging and parsing results on the original version of the TLE test set for tokens marked with grammatical errors (Ungrammatical) and tokens not marked for errors (Grammatical).
Tagging	TLE corpus	 Table 3: Tagging and parsing results on a test set of  500 sentences from the TLE corpus. EWT is the  English UD treebank. TLE orig are original sen- tences from the TLE. TLE corr are the correspond- ing error corrected sentences.
Tagging	English UD treebank	 Table 3: Tagging and parsing results on a test set of  500 sentences from the TLE corpus. EWT is the  English UD treebank. TLE orig are original sen- tences from the TLE. TLE corr are the correspond- ing error corrected sentences.
Tagging	TLE test set	 Table 4: Tagging and parsing results on the origi- nal version of the TLE test set for tokens marked  with grammatical errors (Ungrammatical) and to- kens not marked for errors (Grammatical).
question answering (QA)	Quora	Community-driven question answering (QA) websites such as Quora, Yahoo-Answers, and Answers.com are accumulating millions of users and hundreds of millions of questions.
RTE	English RTE-3 data	We employed a simple classification-based approach to RTE and trained and evaluated a Naive Bayes classifier on the test sets of three RTE benchmarks, using 10-fold cross validation: the English RTE-3 data (  and their German translation 20 (the development sets and the test sets each consist of 800 pairs), and an expanded version of the English RTE-3 data from the Sagan Textual Entailment Test Suite consisting of 2974 pairs.
RTE	English RTE-3 data	We employed a simple classification-based approach to RTE and trained and evaluated a Naive Bayes classifier on the test sets of three RTE benchmarks, using 10-fold cross validation: the English RTE-3 data (  and their German translation 20 (the development sets and the test sets each consist of 800 pairs), and an expanded version of the English RTE-3 data from the Sagan Textual Entailment Test Suite consisting of 2974 pairs.
RTE	Sagan Textual Entailment Test Suite	We employed a simple classification-based approach to RTE and trained and evaluated a Naive Bayes classifier on the test sets of three RTE benchmarks, using 10-fold cross validation: the English RTE-3 data (  and their German translation 20 (the development sets and the test sets each consist of 800 pairs), and an expanded version of the English RTE-3 data from the Sagan Textual Entailment Test Suite consisting of 2974 pairs.
SRILM	CommonCrawl data	To prevent SRILM from running out of RAM, we excluded the large monolingual CommonCrawl data, but included English from the parallel CommonCrawl data.
SRILM	CommonCrawl data	To prevent SRILM from running out of RAM, we excluded the large monolingual CommonCrawl data, but included English from the parallel CommonCrawl data.
MT	IWSLT 2011 Romanian-English TED test	All MT system variants were optimized using IWSLT 2011 Romanian-English TED test as the development set, and were evaluated using the IWSLT 2012 Romanian-English TED test set.
MT	IWSLT 2012 Romanian-English TED test set	All MT system variants were optimized using IWSLT 2011 Romanian-English TED test as the development set, and were evaluated using the IWSLT 2012 Romanian-English TED test set.
WSD	Muffin	In addition, we performed experiments on the nouns subsets of the datasets in order to be able to provide comparisons against two other WSD approaches: Babelfy () and Muffin).
IMS	OMSTI corpus	Another point to be noted here is the difference between results of the IMS with the pre-trained models and those trained on the OMSTI corpus.
Multi-class classification	Flickr	 Table 3: Multi-class classification results in Flickr.
Multi-class classification	YouTube	 Table 4: Multi-class classification results in YouTube.
topic detection	DEV test set	Initial topic detection experiments were run using the DEV test set with both the reference transcriptions (REF) and recognition hypotheses (ASR) to compare different KNN and RNN systems.
ASR	DEV	 Table 2: ASR performance on DEV.
topic detection	DEV test set	 Table 3: % False rejection in topic detection using KNN classifier with 6 nearest neighbour and distance  weights and RNNLM classifier on the DEV test set. 280 dim. topic spaces for LDA and LSA, and 560  dim. for LDA+LSA.
relation classification	SemEval 2010 Task 8 dataset	3. We obtain the new state-of-the-art results for relation classification with an F1 score of 88.0% on the SemEval 2010 Task 8 dataset, outperforming methods relying on significantly richer prior knowledge.
NER	CONLL 2003 named entity recognition shared dataset 7	For NER, we use the CONLL 2003 named entity recognition shared dataset 7, in which segments are tagged with one of four entity types: person ("PER"), location ("LOC"), organization ("ORG") and miscellaneous("MISC"), or others ("O") which is used to denote non-entities.
question answering (QA)	WIKIQA	Many natural language datasets for question answering (QA), such as WIKIQA (, have only thousands of examples and are thus too small for training end-to-end models.
predicting entities in news summaries from the text of the original news articles	NEWS dataset	proposed a task similar to QA, predicting entities in news summaries from the text of the original news articles, and generated a NEWS dataset with 1M instances.
parsing	UD  treebanks	 Table 4: Averaged parsing and POS tagging results on the UD  treebanks for joint variants of stackprop. Given the window- based architecture, stackprop leads to higher parsing accura- cies than joint modeling (83.38% vs. 82.58%).
SMT	Europarl	For all SMT experiments we tokenized, lowercased and aligned words using cdec tools, trained 4-gram in-domain and out-of-domain language models (on the English sides of Europarl and in-domain NewsCommentary) For dense feature models, the out-of-domain baseline SMT model was trained on 1.6M parallel Europarl data and tuned with cdec's lattice MERT on out-of-domain Europarl dev2006 dev set (2,000 sent.).
SMT	Europarl data	For all SMT experiments we tokenized, lowercased and aligned words using cdec tools, trained 4-gram in-domain and out-of-domain language models (on the English sides of Europarl and in-domain NewsCommentary) For dense feature models, the out-of-domain baseline SMT model was trained on 1.6M parallel Europarl data and tuned with cdec's lattice MERT on out-of-domain Europarl dev2006 dev set (2,000 sent.).
SMT	Europarl dev2006 dev set	For all SMT experiments we tokenized, lowercased and aligned words using cdec tools, trained 4-gram in-domain and out-of-domain language models (on the English sides of Europarl and in-domain NewsCommentary) For dense feature models, the out-of-domain baseline SMT model was trained on 1.6M parallel Europarl data and tuned with cdec's lattice MERT on out-of-domain Europarl dev2006 dev set (2,000 sent.).
SMT	Europarl data	For all SMT experiments we tokenized, lowercased and aligned words using cdec tools, trained 4-gram in-domain and out-of-domain language models (on the English sides of Europarl and in-domain NewsCommentary) For dense feature models, the out-of-domain baseline SMT model was trained on 1.6M parallel Europarl data and tuned with cdec's lattice MERT on out-of-domain Europarl dev2006 dev set (2,000 sent.).
SMT	Europarl dev2006 dev set	For all SMT experiments we tokenized, lowercased and aligned words using cdec tools, trained 4-gram in-domain and out-of-domain language models (on the English sides of Europarl and in-domain NewsCommentary) For dense feature models, the out-of-domain baseline SMT model was trained on 1.6M parallel Europarl data and tuned with cdec's lattice MERT on out-of-domain Europarl dev2006 dev set (2,000 sent.).
SMT	MERT	The full-information in-domain SMT model tuned by MERT on news in-domain sets (nc-dev2007, 1,057 sent.) gives the range of possible improvements by the difference of its BLEU score to the one of the out-of-domain model (2.5 BLEU points).
copying	COPYNET	Despite the seemingly "hard" operation of copying, COPYNET can be trained in an end-toend fashion.
SE type classification	UT07 system	Here we present our experiments on SE type classification, beginning with a (near) replication of the UT07 system, and moving onto evaluate our new approach from multiple perspectives.
image caption generation	MS COCO caption dataset	To support the image caption generation task in Japanese, we have annotated images taken from the MS COCO caption dataset) with Japanese captions.
generative and retrieval	VQG	We train and test several generative and retrieval models to tackle the task of VQG.
parsing	Thai sub-corpus	Similarly, FRAG is a source of parsing errors in the Thai sub-corpus.
NED	TAC KBP DEL 2014 dataset	Our contributions are the following: (1) we introduce novel background information to provide additional disambiguation context for NED; we integrate this information in a Bayesian generative NED model; (3) we show that similar entities are useful when no textual context is present; (4) we show that selectional preferences are useful when limited context is present; (5) both kinds of background information help improve results of a NED system, yielding the state-of-the-art in the TAC KBP DEL 2014 dataset and getting the third best results in the CoNLL 2003 dataset; (6) we release both resources for free to facilitate reproducibility.
NED	CoNLL 2003 dataset	Our contributions are the following: (1) we introduce novel background information to provide additional disambiguation context for NED; we integrate this information in a Bayesian generative NED model; (3) we show that similar entities are useful when no textual context is present; (4) we show that selectional preferences are useful when limited context is present; (5) both kinds of background information help improve results of a NED system, yielding the state-of-the-art in the TAC KBP DEL 2014 dataset and getting the third best results in the CoNLL 2003 dataset; (6) we release both resources for free to facilitate reproducibility.
predicting sentence boundaries	Switchboard dataset	As can be seen, predicting sentence boundaries for the Switchboard dataset is a more difficult task than for well-formatted text like the WSJ.
predicting sentence boundaries	WSJ	As can be seen, predicting sentence boundaries for the Switchboard dataset is a more difficult task than for well-formatted text like the WSJ.
machine translation (MT)	SICK dataset	The success of high-level language generation tasks such as machine translation (MT), paraphrasing and image/video captioning depends on the existence of reliable and precise automatic evaluation metrics.: A few select entries from the SICK dataset.
sentiment classification task	Pang  and Lee (2005) movie review	 Table 7: 10-fold cross-validation accuracy and stan- dard error of our system and as reported in previous  work for the sentiment classification task on Pang  and Lee (2005) movie review data
parsing	GKT	Our new system achieves a 9x speedup for parsing the whole corpus, compared to GKT 15.
Sentiment classification	Exp	 Table 3: Sentiment classification accuracy (aver- age 10-fold cross-validation) and trade returns of  different feature sets and term frequency weight- ing schemes in Exp. 3. The same folds were  used for the different representations. The non- annualized returns are presented in columns 3-6.
Temporal Anchoring of Events	TimeBank	Temporal Anchoring of Events for the TimeBank Corpus
MT	CoNLL-2014	 Table 6: Performance of MT and classifier sys- tems from CoNLL-2014 on common errors.
EL	ACE and MSNBC	To evaluate EL accuracy on ACE and MSNBC, we report on a Bag-of-Titles (BOT) F1 evaluation as introduced by).
relation classification	SemEval 2010 Task 8 dataset	To examine the usefulness of the dataset and distributed representations fora different application, we address the task of relation classification on the SemEval 2010 Task 8 dataset (.
parsing	Penn-YM	Following previous work, UAS (unlabeled attachment scores) and LAS (labeled attachment scores) are calculated by excluding punctuation . The parsing speeds are measured on a workstation with Intel Xeon 3.4GHz CPU and 32GB RAM which is same to: Comparison with previous state-of-the-art models on Penn-YM, Penn-SD and CTB5.
parsing	Penn-SD	Following previous work, UAS (unlabeled attachment scores) and LAS (labeled attachment scores) are calculated by excluding punctuation . The parsing speeds are measured on a workstation with Intel Xeon 3.4GHz CPU and 32GB RAM which is same to: Comparison with previous state-of-the-art models on Penn-YM, Penn-SD and CTB5.
dependency parsing	Wall Street Journal	In particular for dependency parsing on the Wall Street Journal we achieve the best-ever published unlabeled attachment score of 94.61%.
Sentence compression	News data	 Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
Sentence compression	News training corpus	 Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
constituency parsing	Penn Treebank	 Table 5: Test F-scores for constituency parsing on  Penn Treebank and CTB-5. Dependency Constituency  Embeddings  Word (dims)  50  100  Tags (dims)  20  100  Nonterminals (dims)  - 100  Pretrained  No  No  Network details  LSTM units (each direction)  200  200  ReLU hidden units  200 / decision  1000  Training  Training epochs  10  10  Minibatch size (sentences)  10  10  Dropout (LSTM output only)  0.5  0.5  L2 penalty (all weights)  none  1 × 10 −8  ADADELTA ρ  0.99  0.99  ADADELTA  1 × 10 −7  1 × 10 −7
constituency parsing	CTB-5	 Table 5: Test F-scores for constituency parsing on  Penn Treebank and CTB-5. Dependency Constituency  Embeddings  Word (dims)  50  100  Tags (dims)  20  100  Nonterminals (dims)  - 100  Pretrained  No  No  Network details  LSTM units (each direction)  200  200  ReLU hidden units  200 / decision  1000  Training  Training epochs  10  10  Minibatch size (sentences)  10  10  Dropout (LSTM output only)  0.5  0.5  L2 penalty (all weights)  none  1 × 10 −8  ADADELTA ρ  0.99  0.99  ADADELTA  1 × 10 −7  1 × 10 −7
Translation	IBM model 2	 Table 1: Translation results from and into English. Alignments in the top (1a) and bottom (1b) tables  were obtained in the target-to-source direction and symmetrised, respectively. Differences are computed  with respect to the directional IBM model 2 in its original parameterisation
natural language processing (NLP) research	Ehime University	In the field of natural language processing (NLP) research, * This work was done when the first author was at Ehime University.
translations	BoW Features system	 Table 1: Experimental results of translations using exponentially decaying bag-of-words models with  different kinds of decay rates. Improvements by systems marked by  *  have a 95% statistical significance  from the baseline system, whereas  † denotes the 95% statistical significant improvements with respect to  the BoW Features system (without decay weights). We experimented with several values for the fixed  decay rate (DR) and 0.9 performed best. The applied RNN model is the LSTM bidirectional translation  model proposed in
NMT decoding	Hiero	We propose to guide NMT decoding using Hiero.
Transductive Prediction Adaptation (TPA)	AMT dataset	In, we show the performance of the Transductive Prediction Adaptation (TPA) on 12 adaptation tasks in the AMT dataset.
RT	Simple) English Wikipedia	Note that RT is trained on the (Simple) English Wikipedia, not on the Google compressions, and therefore the results may not be directly comparable.
slot filling	IBC-C dataset	The three tasks correspond to named entity recognition, slot filling, and In this work we introduce the report based IBC-C dataset.
dependency  parsing	CBOW Skipgram  Original 88.58	 Table 3: Evaluation results for dependency  parsing (in LAS).  Bits  CBOW Skipgram  Original 88.58% 88.15%  Binary 89.25% 88.41%  4-bits  87.56% 86.46%  6-bits  88.62% 87.98%  8-bits  88.63% 88.16%
negation heuristic	MCTest dataset	The negation heuristic helps significantly, especially for 'single' questions (majority of negation cases in the MCTest dataset are for the "single" questions).
prediction	McNemar Test	 Table 2: Accuracy of prediction by L 0 , L 1 and  L 2 . Improvements above the baseline are bolded.  * means significant at p < 0.02 by McNemar Test.
MED	SIGMORPHON16 baseline	We compare MED with the three models of as well as with two recently proposed models: (i) discriminative string transduction, the SIGMORPHON16 baseline, and (ii)'s encoder-decoder model.
pure partial cognate detection	LexStat	With a total of three different clustering algorithms (UPGMA, Markov Clustering, and Infomap), we thus carried out twelve tests on complete cognacy (three for each of our four approaches), and six additional tests on pure partial cognate detection, in which we compared the suitability of SCA and LexStat as string similarity measures.: General performance of the algorithms on all datasets.
caption generation	VGGNet	In this study, we conducted three experiments, under the conditions shown in, using the model of caption generation for images and the model to learn the corresponding relationships between the brain activity data and the features obtained from VGGNet.
Singleton detection	CoNLL-2012 train- ing set	 Table 1: Singleton detection performance using  the best-scoring model. The CoNLL-2012 train- ing set was used as training data. 'dM15' marks  the results by de Marneffe et al. (2015) 'BL' marks  the baseline performance.
MT-based	Chinese test data	For the traditional MT-based cross-lingual method, we use the state-of-the-art statistical MT system Moses 6 . Language model is trained on Chinese gigawords corpus with SRILM 8 . The parallel corpora used are from LDC 9 . We first translate English data into Chinese, and then apply the model trained on the translated dataset to the Chinese test data.
MT	Chinese test data	For the traditional MT-based cross-lingual method, we use the state-of-the-art statistical MT system Moses 6 . Language model is trained on Chinese gigawords corpus with SRILM 8 . The parallel corpora used are from LDC 9 . We first translate English data into Chinese, and then apply the model trained on the translated dataset to the Chinese test data.
Entity linking (EL) recognizes mentions in a text	Freebase	Entity linking (EL) recognizes mentions in a text and associates them to their corresponding entries in a knowledge base (KB), for example, Wikipedia 1 , Freebase (, and DBPedia (.
Disambiguation of Entities	Wordnet	Although, AIDA datasets are widely used for Disambiguation of Entities, AIDA uses YAGO, an unique Knowledge Base derived from Wikipedia, GeoNames and Wordnet, which makes it difficult to compare.
recognizing and disambiguating textual mentions of named entities	DBpedia	Advances in recognizing and disambiguating textual mentions of named entities and the linkage to comprehensive knowledge bases like DBpedia, Freebase, Wikidata and Yago have enabled powerful and user-friendly retrieval systems.
recognizing and disambiguating textual mentions of named entities	Freebase	Advances in recognizing and disambiguating textual mentions of named entities and the linkage to comprehensive knowledge bases like DBpedia, Freebase, Wikidata and Yago have enabled powerful and user-friendly retrieval systems.
recognizing and disambiguating textual mentions of named entities	Yago	Advances in recognizing and disambiguating textual mentions of named entities and the linkage to comprehensive knowledge bases like DBpedia, Freebase, Wikidata and Yago have enabled powerful and user-friendly retrieval systems.
Coreference Resolution	Ak- bik (2014)	 Table 3: Performance with type inference and  Coreference Resolution using Named Entities  and Nouns as entity markers, comparing to Ak- bik (2014), reporting Precision, Recall and F- measures.
Sequence Tagging	STag	(b) Sequence Tagging For these experiments, we use the BLCC tagger from and refer to the resulting system as STag BLCC . Again, we observe that paragraph level is considerably easier than essay level; e.g., for relations, there is ∼5% points increase from essay to paragraph level.
SRT	UCCA	Few SRT schemes place cross-linguistically applicability as one of their main criteria, examples include UCCA, and the), both of which draw on typological theory.
AMR Parsing	newswire portion of a  previous release of the corpus (LDC2014T12)	 Table 1: SMATCH scores for AMR Parsing. *Reported numbers are on the newswire portion of a  previous release of the corpus (LDC2014T12).
normalization	Anselm corpus) of Early New High German	Normalization For the normalization task, we use a total of 44 texts from the Anselm corpus) of Early New High German.
Aspect identification	beer  domain	 Table 5: Aspect identification results on the beer  domain.
SRL	PropBank-style, span-based SRL datasets	We measure the performance of our SRL system on two PropBank-style, span-based SRL datasets:) and CoNLL-2012 (Pradhan et al., 2013) 3 . Both datasets provide gold predicates (their index in the sentence) as part of the input.
SRL	CoNLL-2012	We measure the performance of our SRL system on two PropBank-style, span-based SRL datasets:) and CoNLL-2012 (Pradhan et al., 2013) 3 . Both datasets provide gold predicates (their index in the sentence) as part of the input.
image search	Places audio caption data	 Table 1: Results for image search and annotation  on the Places audio caption data (214k training  pairs, 1k testing pairs). Recall is shown for the  top 1, 5, and 10 hits. The model we use in this  paper is compared against the meanpool variant of  the model architecture presented in Harwath et al.  (2016). For both training and testing, the captions  were truncated/zero-padded to 10 seconds.
segmentation	Morpho Challenge (MC)	Our proposed segmentation algorithm is evaluated using benchmarking datasets from the Morpho Challenge (MC) for multiple languages and a newly introduced dataset for English which compensates for lack of discriminating capabilities in the MC dataset.
neural machine translation (NMT)	Huawei Noah's Ark Lab	Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) * Work done at Huawei Noah's Ark Lab, HongKong.
neural machine translation (NMT)	HongKong	Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) * Work done at Huawei Noah's Ark Lab, HongKong.
SMT	WAT 2016	The results in the first 3 rows are produced by SMT systems taken from the official WAT 2016.
stance markers	Switchboard Dialogue Act Corpus	We therefore first compile a lexicon of stance markers, combining prior lexicons from Biber and and the Switchboard Dialogue Act Corpus ().
Link prediction	STransE	 Table 2: Link prediction results on two datasets. Higher Hits@10 or lower Mean Rank indicates better  performance. Following Nguyen et al. (2016b) and Shen et al. (2016), we divide the models into two  groups. The first group contains intrinsic models without using extra information. The second group  make use of additional information. Results in the brackets are another set of results STransE reported.
argument mining	Marseille	To support argument mining research, we also release our Python implementation, Marseille.
Sentiment analysis	Yelp reviews	Sentiment analysis on Yelp reviews.
question answering	MCTest	Historically, many of the question answering datasets have only thousands of question answering pairs, such as WebQuestions (), MCTest (), WikiQA (, and TREC-QA).
MR	Wikipedia-based MR dataset	(2) We improve the annotation guidelines in MR and contribute with anew Wikipedia-based MR dataset called ReLocaR to address the training data shortage.
MR	ReLocaR	(2) We improve the annotation guidelines in MR and contribute with anew Wikipedia-based MR dataset called ReLocaR to address the training data shortage.
video captioning generation	YouTube2Text dataset	shows video captioning generation results on the YouTube2Text dataset where our final M-to-M multi-task model is compared with our strongest attention-based baseline model for three categories of videos: (a) complex examples where the multi-task model performs better than the baseline; (b) ambiguous examples (i.e., ground truth itself confusing) where multi-task model still correctly predicts one of the possible categories (c) complex examples where both models perform poorly.
Classification	Cookie Theft dataset	 Table 4: Classification accuracy achieved on Cookie Theft dataset.
Classification	ABCD dataset	 Table 6: Classification accuracy achieved on ABCD dataset.
Classification	Cin- derella dataset	 Table 7: Classification accuracy achieved on Cin- derella dataset manually processed to revise non- grammatical sentences.
summarization	TextRank	shows the ROUGE scores) of multiple existing summarization systems, namely TF*IDF,), TextRank (), LSA (Gong and), KL-Greedy (Haghighi and Vanderwende, 2009), provided by the sumy package and ICSI (, a strong state-of-the-art approach) in comparison to the extractive upper bound on DUC'04 and DBS.
summarization	DUC'04	shows the ROUGE scores) of multiple existing summarization systems, namely TF*IDF,), TextRank (), LSA (Gong and), KL-Greedy (Haghighi and Vanderwende, 2009), provided by the sumy package and ICSI (, a strong state-of-the-art approach) in comparison to the extractive upper bound on DUC'04 and DBS.
summarization	DBS	shows the ROUGE scores) of multiple existing summarization systems, namely TF*IDF,), TextRank (), LSA (Gong and), KL-Greedy (Haghighi and Vanderwende, 2009), provided by the sumy package and ICSI (, a strong state-of-the-art approach) in comparison to the extractive upper bound on DUC'04 and DBS.
RC problem	Stanford Question Answering Dataset (SQuAD)	In this paper, we study the RC problem on the Stanford Question Answering Dataset (SQuAD).
multiple-choice question answering (QA)	CNN/Daily Mail	Various RC tasks and datasets have been developed, including Machine Comprehension Test () for multiple-choice question answering (QA) (), Algebra () and Science  for passing standardized tests , CNN/Daily Mail ( and Children's Book Test) for cloze-style The most authoritative account at the time came from the medical faculty in Paris in a report to the king of France that blamed the heavens.
Entity extraction	ACE2005  test set	 Table 1: Entity extraction results on the ACE2005  test set with gold-standard mention boundaries.
NER	CoNLL NER test data	The results show that the combined systems outperform the state-of-the-art cross-lingual NER approaches proposed in, and  on the CoNLL NER test data.
representation projection	CoNLL English training data	For representation projection, the source (English) NER systems are neural network models (NN1 and NN2) trained with the CoNLL English training data.
translation	ChineseEnglish direction	We carry the translation task on the ChineseEnglish direction to evaluate the effectiveness of our models.
Entity Relatedness	NDCG	 Table 2: Entity Relatedness.  NDCG  MAP  @1  @5  @10  ALIGN  0.416 0.432 0.472 0.410  Entity2vec 0.593 0.595 0.636 0.566  SPME  0.593 0.594 0.636 0.566  MPME  0.613 0.613 0.654 0.582
ASDA	Book domain	The learning curve of our ASDA approach in Book domain is shown in.
Seamless Integration of Word Senses	Downstream NLP	Towards a Seamless Integration of Word Senses into Downstream NLP Applications
partof-speech tagging	AWY	The last few years have seen much success of applying neural networks to many important applications in natural language processing, e.g., partof-speech tagging, chunking, named entity recognition), sentiment analysis, document classification, machine translation; Sutskever * Most of work was done when AWY was with Google.
sentiment analysis	AWY	The last few years have seen much success of applying neural networks to many important applications in natural language processing, e.g., partof-speech tagging, chunking, named entity recognition), sentiment analysis, document classification, machine translation; Sutskever * Most of work was done when AWY was with Google.
Sutskever	AWY	The last few years have seen much success of applying neural networks to many important applications in natural language processing, e.g., partof-speech tagging, chunking, named entity recognition), sentiment analysis, document classification, machine translation; Sutskever * Most of work was done when AWY was with Google.
sentiment analysis	movie review datasets	So in this section, we conduct sentiment analysis on two movie review datasets, both containing equal numbers of positive and negative reviews.
name tagging	Wikipedia data	Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.
sentiment analysis	HowNet	HowNet has been widely utilized in word similarity computation () and sentiment analysis (), and in section 3.2 we will give a detailed introduction to sememes, senses and words in HowNet.
SemEval textual similarity	Pearson's r × 100	 Table 2: Results on SemEval textual similarity  datasets (Pearson's r × 100) when experimenting  with different regularization techniques.
PP attachment task	RRR dataset	As discussed in, this dataset is a more realistic PP attachment task than the RRR dataset (.
parse	TEST section	lists the reasons for the parse failures on 1/4 of the TEST section (the problems taken from exams on 2007).
coreference resolvers	CoNLL dataset	As reported by, state-of-the-art coreference resolvers trained on the CoNLL dataset perform poorly, i.e. worse than the rule-based system (), on the new dataset, WikiCoref (Ghaddar and Langlais, 2016b), even though WikiCoref is annotated with the same annotation guidelines as the CoNLL dataset.
coreference resolvers	CoNLL dataset	As reported by, state-of-the-art coreference resolvers trained on the CoNLL dataset perform poorly, i.e. worse than the rule-based system (), on the new dataset, WikiCoref (Ghaddar and Langlais, 2016b), even though WikiCoref is annotated with the same annotation guidelines as the CoNLL dataset.
RTE	INIT	 Table 1: Results on RTE performance without (INIT) and with prior compound splitting. : significant  difference of the performance in comparison to INIT
text simplification	EW-SEW corpus 2	It is one of the largest freely available resources for text simplification, and unlike the previously used EW-SEW corpus 2, which only contains full matches (167K pairs), the newer dataset also contains partial matches.
caption generation	Flickr30K	Our task is related to caption generation, which has been studied extensively (e.g.,) with MSCOCO () and Flickr30K (.
RST parsing	Penn Discourse Treebank (PDTB)	Another study compared features derived from deep hierarchical discourse relations based on RST parsing and features derived from shallow discourse relations based on Penn Discourse Treebank (PDTB) ( parsing in the task of essay scoring and demonstrated the effectiveness of deep discourse structure in better differentiation of text coherence . Related work has also been conducted to annotate discourse relations in spoken language, which is produced and processed differently from written texts, and often lacks explicit discourse connectives that are more frequent in written language.
coreference	Midwest High Speed Rail Association vs U.S. High Speed Rail Association	For coreference, mistakes usually happen when there is significant lexical overlap but some distinguishing feature that proves too subtle for our system to doubt the match, like Midwest High Speed Rail Association vs U.S. High Speed Rail Association or [English] Nationwide Building Society vs Irish Nationwide Building Society.
coreference	Nationwide Building Society vs Irish Nationwide Building Society	For coreference, mistakes usually happen when there is significant lexical overlap but some distinguishing feature that proves too subtle for our system to doubt the match, like Midwest High Speed Rail Association vs U.S. High Speed Rail Association or [English] Nationwide Building Society vs Irish Nationwide Building Society.
sentence classification tasks	DMV FAQ 1	First, different from the flat and coarse categories inmost sentence classification tasks (i.e. sentimental classification), question classes often have a hierarchical structure such as those from the New York State DMV FAQ 1 (see).
sentimental classification	DMV FAQ 1	First, different from the flat and coarse categories inmost sentence classification tasks (i.e. sentimental classification), question classes often have a hierarchical structure such as those from the New York State DMV FAQ 1 (see).
hyperlink prediction	Streusle corpus	Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following; (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in); (3) hyperlink prediction using the dataset from, (4) identification of multi-word expressions using the Streusle corpus (; and (5) semantic super-sense tagging using the Semcor dataset, following.
hyperlink prediction	Semcor dataset	Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following; (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in); (3) hyperlink prediction using the dataset from, (4) identification of multi-word expressions using the Streusle corpus (; and (5) semantic super-sense tagging using the Semcor dataset, following.
SemEval 2017 Task 10	ACL-RD-TEC corpora	 Table 1: Characteristics of SemEval 2017 Task 10 and ACL-RD-TEC corpora, statistics of training sets
keyphrase boundary classification	SemEval 2017 Task 10	 Table 2: Results for keyphrase boundary classification on the SemEval 2017 Task 10 corpus
keyphrase boundary classification	ACL RD-TEC	 Table 3: Results for keyphrase boundary classification on the ACL RD-TEC corpus
POS tagging	Stanford CoreNLP toolkit	Word segmentation and POS tagging are performed with the Stanford CoreNLP toolkit ( ).
image search	EMNLP	By improving the quality of image captioning, image search using natural sentences and image recognition support for 1In recent years it has been held as a joint workshop such as EMNLP and ACL; https://vision.cs.hacettepe.
image search	ACL	By improving the quality of image captioning, image search using natural sentences and image recognition support for 1In recent years it has been held as a joint workshop such as EMNLP and ACL; https://vision.cs.hacettepe.
sentiment analysis	TripAdvisor	A relatively early study by focuses on detecting deceptive review opinions in sentiment analysis, using a crowdsourcing approach to create training data for the positive class, and then combine with truthful opinions from TripAdvisor.
MWE recognition	CRF	To evaluate MWE recognition, we used the F-measure for untagged / tagged MWEs (FUM/FTM) . For the pipeline model, we compared the gold MWEs with predictions by CRF.
sentiment and emotion detection	Stanford Twitter Sentiment (STS)	As sentiment and emotion detection are widely studied in social media analysis (), we test model performance based on the Stanford Twitter Sentiment (STS) and the International Survey on Emotion Antecedents and Reactions (ISEAR) corpus.
RE	Gold Standard data	In summary our contributions are the following: • we introduce a self-training strategy for crowdsourcing; • we propose an iterative human-machine cotraining framework for the task of RE; and • we test our approach on a standard benchmark, obtaining a slightly lower performance compared to the state-of-the-art methods based on Gold Standard data.
MT	Europarl corpus	Apart from its prominent role in MT as a training set, the Europarl corpus has been used for cross-lingual WSD (, including, more recently, preposition sense disambiguation, and widely exploited to develop cross-lingual word embeddings () as well as multi-sense embeddings (.
SGMM adaptation	fMLLR	It is not clear why this was the case, though it might relate to some overlap between the SGMM adaptation method and fMLLR.
Automating Biomedical Evidence Synthesis	RobotReviewer	Automating Biomedical Evidence Synthesis: RobotReviewer
Classification	ChaLearn Validation dataset	 Table 1: Classification performance on ChaLearn Validation dataset using CNN on raw audio
Classification	WordNet test  set	 Table 3: Classification accuracy on WordNet test  set.
AMR generation	KIYCZ17	 Table 1: Results for AMR generation on the test  set. All score differences between our models and  the corresponding baselines are significantly dif- ferent (p<0.05). "(-s)" means input without scope  marking. KIYCZ17, PKH16, SPZWG17 and
AMR generation	PKH16	 Table 1: Results for AMR generation on the test  set. All score differences between our models and  the corresponding baselines are significantly dif- ferent (p<0.05). "(-s)" means input without scope  marking. KIYCZ17, PKH16, SPZWG17 and
topic classification	AG news and DBpedia ontology datasets	Topic classification For topic classification, we evaluate on the large-scale AG news and DBpedia ontology datasets created by .  Pre-processing We use the same pre-processing as in earlier work.
MTL	UCCA test sets	We here detail a range of experiments to assess the value of MTL to UCCA parsing, training the parser in single-task and multitask settings, and evaluating its performance on the UCCA test sets in both in-domain and out-of-domain settings.
AMR	AMR	For AMR, we use LDC2017T10, identical to the dataset targeted in) in our English experiments, henceforth referred to as UD ++ . We use only the AMR, DM and UD training sets from standard splits.
AMR	UD training sets	For AMR, we use LDC2017T10, identical to the dataset targeted in) in our English experiments, henceforth referred to as UD ++ . We use only the AMR, DM and UD training sets from standard splits.
event coreference resolution	KBP corpora	But as the goal of this study is to leverage discourse level topic structure in a document for improving event coreference resolution performance, we only evaluate the ILP system using regular documents (news articles) in the KBP corpora.
coreference resolution classifier	KBP 2015 corpus	Specifically, we train our event extraction system and local coreference resolution classifier on 310 documents from the KBP 2015 corpus that consists of both discussion forum documents and news articles, tune the hyper-parameters corresponding to ILP using 50 news articles 6 from the KBP 2015 corpus and evaluate our system on The ECB+ ( corpus is another commonly used dataset for evaluating event coreference resolution performance.
coreference resolution classifier	KBP 2015 corpus	Specifically, we train our event extraction system and local coreference resolution classifier on 310 documents from the KBP 2015 corpus that consists of both discussion forum documents and news articles, tune the hyper-parameters corresponding to ILP using 50 news articles 6 from the KBP 2015 corpus and evaluate our system on The ECB+ ( corpus is another commonly used dataset for evaluating event coreference resolution performance.
coreference resolution classifier	ECB+ ( corpus	Specifically, we train our event extraction system and local coreference resolution classifier on 310 documents from the KBP 2015 corpus that consists of both discussion forum documents and news articles, tune the hyper-parameters corresponding to ILP using 50 news articles 6 from the KBP 2015 corpus and evaluate our system on The ECB+ ( corpus is another commonly used dataset for evaluating event coreference resolution performance.
event coreference resolution	KBP 2016 and 2017 corpus	 Table 3: Results for event coreference resolution systems on the KBP 2016 and 2017 corpus. Joint learning
summarization	DUC challenge	Indeed, report that summarization systems were optimized for ROUGE during the DUC challenge) until they were indistinguishable from the ROUGE scores of human-generated summaries, but the systems The Direct Marketing Commission probing B2C Data and Data Bubble.
summarization	B2C Data	Indeed, report that summarization systems were optimized for ROUGE during the DUC challenge) until they were indistinguishable from the ROUGE scores of human-generated summaries, but the systems The Direct Marketing Commission probing B2C Data and Data Bubble.
summarization	CNN/Daily Mail (CDM) dataset	In automatic summarization, systems must generate a short (on average two or three sentence) summary of an article: for our study, we chose articles from the CNN/Daily Mail (CDM) dataset () which come paired with reference summaries in the form of story highlights.
question answering	MS MARCO question answering dataset	Next, we look at evaluating the correctness of system outputs in question answering using the MS MARCO question answering dataset.
ASR transcriptions	AMI	The word error rate of the ASR transcriptions is respectively of 36% and 37% for AMI and ICSI.
ASR transcriptions	ICSI	The word error rate of the ASR transcriptions is respectively of 36% and 37% for AMI and ICSI.
Generalization	DUC-2002 (F1)	 Table 3: Generalization to DUC-2002 (F1).
question generation task	SQuAD dataset	For the question generation task, we use the SQuAD dataset (, where we learn to generate a question given a sentence containing the answer, similar to the recent work by.
generative fusion	KNN	Our generative fusion model can produce many stories without degraded performance, while the KNN can only produce a limited number relevant stories.
word similarity	SimLex data	For word similarity, following a standard practice ( we tune all models on one half of the SimLex data and evaluate on the other half, and vice versa.
Machine Translation	IWSLT 2014 German-English task	 Table 9: Machine Translation results on the IWSLT 2014 German-English task.
relation detection	Jane joined Google"	In relation detection (, we may want to identify all instances of a specific relation, such as "Jane joined Google" for "Employment" relation.
POS tagging	SANCL domains	 Table 4: Accuracy for POS tagging on the dev and test sets of the SANCL domains, models trained on  full source data setup. Values for methods with * are from (
constituency parsing	Wall Street Journal (WSJ) test set	Along the way, we provide several other notable contributions: • We raise the state-of-the-art single-model F 1 -score for constituency parsing from 92.6% to 94.3% on the Wall Street Journal (WSJ) test set.
Parsing	WSJTEST	 Table 1: Parsing performance on WSJTEST,  along with the results of other recent single-model  parsers trained without external parse data.
RSP	QBANKDEV	 Table 4: Performance of RSP on QBANKDEV.
RSP	GENIADEV	 Table 5: Performance of RSP on GENIADEV.
sentiment classification	Word2Vec	In Section 6, the proposed word embeddings show evident improvements on sentiment classification, as compared to the base model Word2Vec and other baselines using the same lexical resource.
Topic Classification	Newsgroups dataset 6	Topic Classification Task We use the 20 Newsgroups dataset 6 ("bydate" version), whereby the newsgroups are organized into six subject matter groupings.
tracking of rare states	WoZ	We show that this significantly improves tracking of rare states and achieves state-of-the-art performance on the WoZ and DSTC2 state tracking tasks.
segmentation	MSRA test sections	On the other hand, no segmentation is available for the MSRA test sections, nor the Weibo / resume datasets.
segmentation	Weibo / resume datasets	On the other hand, no segmentation is available for the MSRA test sections, nor the Weibo / resume datasets.
event schema induction	Chambers-13	 Table 5: Higher-order RSI accuracies of various methods on the three datasets. Induced schemata for  each dataset and method are evaluated by three human evaluators, E1, E2, and E3. TFBA performs  better than HardClust for Shootings and NYT Sports datasets. Even though HardClust achieves better  accuracy on MUC dataset, it has several limitations, see Section 4 for more details. Chambers-13 solves  a slightly different problem called event schema induction, for more details about the comparison with  Chambers-13 see Section 4.1.
Machine Translation	IWSLT 2014 German-toEnglish dataset	For a more detailed description, we refer readers to Appendix B and the code 6 . Machine Translation Following, we evaluate on IWSLT 2014 German-toEnglish dataset ().
question answering	SQuAD	We evaluate our model on the task of question answering using recently released SQuAD and TriviaQA Wikipedia (, which have gained a huge attention over the past year.
question answering	TriviaQA Wikipedia	We evaluate our model on the task of question answering using recently released SQuAD and TriviaQA Wikipedia (, which have gained a huge attention over the past year.
word analogy task	Text8	(a) (b): Impact of different settings of negative sampling on skip-gram for the word analogy task on Text8.
sentiment classification	Stanford Sentiment Treebank	 Table 2: Test accuracy of sentiment classification  on Stanford Sentiment Treebank. Bold font indi- cates the best performance.
Protein task	OccamzRazor	The Protein task was completed in conjunction with OccamzRazor, a neuroscience company targeting biological pathways of Parkinson's disease.
Harvesting Paragraph-Level Question-Answer Pairs	Wikipedia	Harvesting Paragraph-Level Question-Answer Pairs from Wikipedia
MRC	SQuAD dataset 1	A significant milestone is that several MRC models have exceeded the performance of human annotators on the SQuAD dataset 1 ().
NED	SnapCaptionsKB dataset	 Table 1: NED performance on the SnapCaptionsKB dataset at Top-1, 3, 5, 10, 50 accuracies. The classi- fication is over 1M entities. Candidates generation methods: N/A, or over a fixed number of candidates  generated with methods: m→e hash list and kNN (lexical neighbors).
answer selection	NewsQA	We show that our model consistently outperforms strong baselines, in terms of both informativeness and fluency (for CNN document summarization) and achieves state-of-the-art results for answer selection on WikiQA and NewsQA.
answer selection	AP-CNN	 Table 4: Results (in percentage) for answer selection comparing our approaches (bottom part) to base- lines (top): AP-CNN (dos Santos et al., 2016), ABCNN (Yin et al., 2016), L.D.C (
answer selection	ABCNN	 Table 4: Results (in percentage) for answer selection comparing our approaches (bottom part) to base- lines (top): AP-CNN (dos Santos et al., 2016), ABCNN (Yin et al., 2016), L.D.C (
answer selection	L.D.C	 Table 4: Results (in percentage) for answer selection comparing our approaches (bottom part) to base- lines (top): AP-CNN (dos Santos et al., 2016), ABCNN (Yin et al., 2016), L.D.C (
PMI	Weibo	To estimate the probabilities in PMI, we collected about 9 million post-response pairs from Weibo.
Skill discovery	IPDAs	Skill discovery is difficult with a pure voice user interface, it is hard for users to know the capabilities of thousands of skills a priori, which may leads to limited user en-gagement with skills and potentially with IPDAs.
interpretability	CMU-MOSEI	Aside interpretability, DFG achieves superior performance compared to previously proposed models for multimodal sentiment and emotion recognition on CMU-MOSEI.
sentiment analysis	MOSEI dataset	 Table 3: Results for sentiment analysis and emotion recognition on the MOSEI dataset (reported results  are as of 5/11/2018. please check the CMU Multimodal Data SDK github for current state of the art and  new features for CMU-MOSEI and other datasets). SOTA1 and SOTA2 refer to the previous best and  second best state-of-the-art models (from Section 2) respectively. Compared to the baselines Graph-MFN  achieves superior performance in sentiment analysis and competitive performance in emotion recognition.  For all metrics, higher values indicate better performance except for MAE where lower values indicate  better performance.
sentiment analysis	CMU Multimodal Data SDK github	 Table 3: Results for sentiment analysis and emotion recognition on the MOSEI dataset (reported results  are as of 5/11/2018. please check the CMU Multimodal Data SDK github for current state of the art and  new features for CMU-MOSEI and other datasets). SOTA1 and SOTA2 refer to the previous best and  second best state-of-the-art models (from Section 2) respectively. Compared to the baselines Graph-MFN  achieves superior performance in sentiment analysis and competitive performance in emotion recognition.  For all metrics, higher values indicate better performance except for MAE where lower values indicate  better performance.
sentiment analysis	CMU-MOSI	 Table 2: Results for sentiment analysis on CMU-MOSI, emotion recognition on IEMOCAP and personality  trait recognition on POM. Best results are highlighted in bold.
sentiment analysis	IEMOCAP	 Table 2: Results for sentiment analysis on CMU-MOSI, emotion recognition on IEMOCAP and personality  trait recognition on POM. Best results are highlighted in bold.
Ontology classification	DBpedia 2014 (Wikipedia)	• DBPedia: Ontology classification over fourteen non-overlapping classes picked from DBpedia 2014 (Wikipedia).
citation recommendation	DBLP	We then discuss additional settings and present experimental results of the two tasks, i.e., document classification and citation recommendation, respectively.: F 1 on DBLP when newcomers are discarded.
Detailed Analysis	MultiNLI	 Table 6: Detailed Analysis on MultiNLI.
negate	SNLI	Notably, even a single handwritten rule, negate, improves the accuracy on the negation examples in SNLI by 6.1%.
word similarity and analogy tasks	WS-353	Our second major contribution is the Korean evaluation datasets for word similarity and analogy tasks, a translation of the WS-353 with annotations by 14 Korean native speakers, and 10,000 items for semantic and syntactic analogies, developed with Korean linguistic expertise.
sememe prediction	HowNet	In our experiments, we evaluate our models on the task of sememe prediction using HowNet.
cross-lingual projection	words from a 2017 dump of the English edition of Wiktionary	For cross-lingual projection, we extract links between words from a 2017 dump of the English edition of Wiktionary.
Modeling Deliberative Argumentation Strategies	Wikipedia	Modeling Deliberative Argumentation Strategies on Wikipedia
COCO Image Captions	COCO image captioning dataset	COCO Image Captions The COCO image captioning dataset is normally divided into 82K images for training, and 40K images for validation.
paragraph generation	IU X-Ray dataset	 Table 1: Main results for paragraph generation on the IU X-Ray dataset (upper part), and single sentence  generation on the PEIR Gross dataset (lower part). BLUE-n denotes the BLEU score that uses up to  n-grams.
paragraph generation	PEIR Gross dataset	 Table 1: Main results for paragraph generation on the IU X-Ray dataset (upper part), and single sentence  generation on the PEIR Gross dataset (lower part). BLUE-n denotes the BLEU score that uses up to  n-grams.
sentiment classification task	KCCA  (GlvCC, LSA)	 Table 2: This table shows results obtained by us- ing sentence embeddings from the InferSent en- coder in the sentiment classification task. Met- rics reported are average Precision, F-score and  AUC along with the corresponding standard devi- ations (STD). Best results are obtained by KCCA  (GlvCC, LSA) and are highlighted in boldface.
participant linking	MUC	For participant linking, we evaluate) the combined performance of participant mention identification and alias detection using the standard evaluation metrics, MUC, BCUB (, Entity-based CEAF (CEAFe) ( and their average.
Relation extraction	ACE  2005 test dataset	 Table 1: Relation extraction performance on ACE  2005 test dataset. * denotes significance at p <  0.05 compared to SPTree, denotes significance  at p < 0.05 compared to the Baseline.
Plagiarism Detection Systems evaluation	PAN Summary	In this paper, we study the performance of plagdet, the main measure for Plagiarism Detection Systems evaluation, on manually paraphrased plagiarism datasets (such as PAN Summary).
Summary Plagiarism Detection	NormPlagdet  Dataset  Model  Year  Precision Recall  Plagdet	 Table 2: Results of Summary Plagiarism Detection using NormPlagdet  Dataset  Model  Year  Precision Recall  Plagdet
Symptom Normalization	SNOMED CT 2	Symptom Normalization After symptom expression identification, medical experts manually link each symptom expression to the most relevant concept on SNOMED CT 2 for normalization.
NE recognition	CONLL2003 dataset	For NE recognition we used NeuroNER 3 , a tool designed by, trained on CONLL2003 dataset and recognizing four types of NE: person, location, organization and miscellaneous.
Textbook Question Answering (TQA)	SciQ dataset	We test our model on Textbook Question Answering (TQA) and SciQ dataset.
SRL	CoNLL 2005	 Table 1: End-to-end SRL results for CoNLL 2005 and CoNLL 2012, compared to previous systems.  CoNLL 05 contains two test sets: WSJ (in-domain) and Brown (out-of-domain).
SRL	CoNLL 2012	 Table 1: End-to-end SRL results for CoNLL 2005 and CoNLL 2012, compared to previous systems.  CoNLL 05 contains two test sets: WSJ (in-domain) and Brown (out-of-domain).
SRL	CoNLL 05	 Table 1: End-to-end SRL results for CoNLL 2005 and CoNLL 2012, compared to previous systems.  CoNLL 05 contains two test sets: WSJ (in-domain) and Brown (out-of-domain).
Tormenta	EEUU	Consider the following text from a Spanish news article: "Tormenta de nieve afecta a 100 millones de personas en EEUU.
slot filling	OOV	To address these challenges, we present a neural generative model for slot filling on unaligned dialog data, specifically for slot value prediction as it has more challenges caused by OOV.
SVM	KyTea toolkit	For the SVM experiments, we used the offthe-shelf LIBLINEAR library ( wrapped by the KyTea toolkit.
generative	DSTM	In Section 3, the generative model and the inference/learning procedures of DSTM are presented.
SNLI	SNLI	For SNLI, this is not surprising as the crowd-workers tend to construct the hypotheses in SNLI by some regular rules ().
PRET	Electronics dataset	We pair up an aspectlevel dataset and a document-level dataset when they are from a similar domain -the Yelp dataset is used by D1, D3, and D4 for PRET and MULT, and the Electronics dataset is only used by D2.
synonym substitution	WordNet	This is particularly important for synonym substitution, for which we relied on WordNet).
MT	NIST	There is an abundance of evaluation metrics available for MT including WER (Su et al.), BLEU (), NIST) and ME-TEOR (), all of which compare the similarity between reference translations and translations.
summarize drug and DDI	DrugBank	Several drug databases have been maintained to summarize drug and DDI information such as DrugBank (), Therapeutic Target database ( , and PharmGKB (.
summarize drug and DDI	Therapeutic Target database	Several drug databases have been maintained to summarize drug and DDI information such as DrugBank (), Therapeutic Target database ( , and PharmGKB (.
summarize drug and DDI	PharmGKB	Several drug databases have been maintained to summarize drug and DDI information such as DrugBank (), Therapeutic Target database ( , and PharmGKB (.
RACE	CNN/Daily Mail dataset	The key difference between RACE and previously released machine comprehension datasets (e.g., the CNN/Daily Mail dataset () and SQuAD () is that the answers in RACE often cannot be directly extracted from the given passages, as illustrated by the two example questions (Q1 & Q2) in.
Tackling the Story Ending Biases	Story Cloze	Tackling the Story Ending Biases in The Story Cloze Test
SCT	SCT dataset	Although the original goal behind the SCT was to require systems to perform deep language understanding and commonsense reasoning for successful narrative understanding, some recent models could perform significantly better than the initial baselines by leverag-ing human-authorship biases discovered in the SCT dataset.
irony detection	SemEval dataset	For performing the experiment of irony detection in Section 5, we reserve the other 1,072 tweets in the SemEval dataset that are annotated as real ironic as the test data.
Supertagging	German  TiGer treebank	 Table 4: Supertagging experiments with German  TiGer treebank.
Supertagging	French  Treebank (FTB)	 Table 6: Supertagging experiments with French  Treebank (FTB).
JSL learners	WWWJDIC	In recent years, certain online Japanese learning systems are developed to support JSL learners, such as Reading Tutor 2 , Asunaro 3 , Rikai , and WWWJDIC 5 . Some of these systems are particularly designed to enable JSL learners to read and write Japanese texts by offering the word information with their corresponding difficulty information or translation information (.
translate	NI	Finally we used the Chinese Room to translate and asked the NI to point out any errors.
VAs	International Classification of Diseases (ICD-10) code	Typically, VAs are collected by non-medical surveyors who record the information on a form that is later reviewed by physicians who assign the record an International Classification of Diseases (ICD-10) code.
EBM	PubMed	In practice, successful EBM applications rely on answering clinical questions via analysis of large medical literature databases such as PubMed.
parsing	GENIA+PubMed	For parsing the questions, we used BLLIP reranking parser) (Charniak-Johnson parser) and used the model GENIA+PubMed for biomedical text.
Navigational queries	Katanaev AND(1):111-22	Navigational queries, also called known-item queries, such as Katanaev AND(1):111-22, are intended to retrieve a specific publication.
PMC articles	PubMed and PMC score	 Table 3. The value of full text PMC articles  in the retrieval performance. In combined  retrieval, we assign each article the maxi- mum of its PubMed and PMC score and  evaluate based on that maximum.
NER	CRF	In this paper, we present a corpus for NER in Hindi-English Code-Mixed along with extensive experiments on our machine learning models which achieved the best f1-score of 0.95 with both CRF and LSTM.
MRC	DuReader	 Table 6: Performance of typical MRC systems on the DuReader.
MRC	Stanford Question Answering Dataset (SQuAD)	One of the first large MRC datasets (over 100k QA pairs) is the Stanford Question Answering Dataset (SQuAD) ().
summarization	CNN-Dailymail	Our study compares summarization with fixed value control variables on full text CNN-Dailymail with (See et al., 2017).
hyperparameter optimization	WMT data	For hyperparameter optimization, we did a complete grid search over a span of learning rates (0.1, 0.01, 0.001, 0.0001, 0.00001), train epochs, and optimizers (Adam, SGD) on WMT data and a partial search on EMEA data.
hyperparameter optimization	EMEA data	For hyperparameter optimization, we did a complete grid search over a span of learning rates (0.1, 0.01, 0.001, 0.0001, 0.00001), train epochs, and optimizers (Adam, SGD) on WMT data and a partial search on EMEA data.
Pronoun Resolution	fMRI	The Role of Syntax during Pronoun Resolution: Evidence from fMRI
tokenization	UD Japanese-GSD test set	 Table 5: F1 scores of tokenization and UAS on the UD Japanese-GSD test set. The top section shows the  systems which used their own tokenizers. The second section is a comparison with the systems relying  on the default settings of UDPipe, and the bottom section is the situation to ru parsers using the gold PoS  as input.
relation classification task	CoNLL	We investigate the use of different syntactic dependency representations in a neu-ral relation classification task and compare the CoNLL, Stanford Basic and Universal Dependencies schemes.
Topic classification	RCV1	 Table 3: Topic classification results on RCV1.
paraphrase detection	SNLI dataset	We also tested our approach on semantic textual similarity (STS 14 -), paraphrase detection (MRPC -Dolan et al.), entailment and semantic relatedness tasks (SICK-R and SICK-E -), though those tasks are more close in nature to the task of the SNLI dataset which the model was trained on.
SRL	PropBank data	We explore a sequence-to-sequence formulation of SRL that we apply, as a first step, in a classical monolingual setting on PropBank data, as illustrated in.
event extraction'	ACE/ERE	Unlike other supervised data-driven 'event extraction' tasks such as in the ACE/ERE programs (), we do not conceptualize events as structured schemata/frames, but more limited as textual mentions of real-world occurrences.
classification task	REPORTS	 Table 3: Results of the classification task for the  uncertain class of REPORTS. The best results are  boldfaced and significant performance increases  (α = 0.05) over Unc are marked with asterisks.
SPA-ENG	English Gigaword corpus	The embeddings for SPA-ENG are trained by combining a portion of English Gigaword corpus ( and Spanish Gigaword corpus), and a subset of tweets from.
LM	HindiEnglish code-mixed data (Hinglish)	Our goal is to improve LM for HindiEnglish code-mixed data (Hinglish) where similar challenges are apparent.
Tackling Code-Switched NER	CMU	Tackling Code-Switched NER: Participation of CMU
sentiment analysis	CMU-MOSEI	 Table 1: Performance of individual modality and multimodal fusion for sentiment analysis on the vali- dation set of CMU-MOSEI. MAE is the Mean Absolute Error.
Sentiment classification	MOSI  MOSEI  Acc. F1 Acc. F1	 Table 1: Sentiment classification performance us- ing a bi-directional LSTM-RNN classifier.  MOSI  MOSEI  Acc. F1 Acc. F1
ASR view	MOSI MOSEI  MT  71.1  67.5	 Table 2: Improvement in ASR view accuracy us- ing a non contextual classifier.  MOSI MOSEI  MT  71.1  67.5  AT  63.7  63.8  AT ↑  65.1  65.7
Unimodal sentiment analysis	CMU-MOSI test set	 Table 2: Unimodal sentiment analysis results on  the CMU-MOSI test set. Numbers in bold are the  best results on each modality.
sentiment prediction	MOSI dataset	We present our work on sentiment prediction using the benchmark MOSI dataset from the CMU-MultimodalDataSDK.
sentiment prediction	CMU-MultimodalDataSDK	We present our work on sentiment prediction using the benchmark MOSI dataset from the CMU-MultimodalDataSDK.
Domain Shift Experiments  Corpus  Type  Aim  Genre	NWJC  Training Nwjc2vec  Web pages  1,463,142,939  Mainichi Shimbun 1993-1999 Training Mai2vec  Newspaper  6,791,403  BCCWJ  Training Word2vec	 Table 5: Corpus Data for Domain Shift Experiments  Corpus  Type  Aim  Genre  Number of Sentences  NWJC  Training Nwjc2vec  Web pages  1,463,142,939  Mainichi Shimbun 1993-1999 Training Mai2vec  Newspaper  6,791,403  BCCWJ  Training Word2vec of base-lm-1  Blogs  7,226  Language model  And  BCCWJ  Test  For blogs and Q &A sites Q & A sites  104  Mainichi Shimbun 2007  Training Word2vec of base-lm-2  100,000  Language model  Newspaper  Mainichi Shimbun 2008  Test  For newspaper  10,000
Domain Shift Experiments  Corpus  Type  Aim  Genre	Mainichi Shimbun 2007  Training Word2vec	 Table 5: Corpus Data for Domain Shift Experiments  Corpus  Type  Aim  Genre  Number of Sentences  NWJC  Training Nwjc2vec  Web pages  1,463,142,939  Mainichi Shimbun 1993-1999 Training Mai2vec  Newspaper  6,791,403  BCCWJ  Training Word2vec of base-lm-1  Blogs  7,226  Language model  And  BCCWJ  Test  For blogs and Q &A sites Q & A sites  104  Mainichi Shimbun 2007  Training Word2vec of base-lm-2  100,000  Language model  Newspaper  Mainichi Shimbun 2008  Test  For newspaper  10,000
Domain Shift Experiments  Corpus  Type  Aim  Genre	Mainichi Shimbun 2008  Test	 Table 5: Corpus Data for Domain Shift Experiments  Corpus  Type  Aim  Genre  Number of Sentences  NWJC  Training Nwjc2vec  Web pages  1,463,142,939  Mainichi Shimbun 1993-1999 Training Mai2vec  Newspaper  6,791,403  BCCWJ  Training Word2vec of base-lm-1  Blogs  7,226  Language model  And  BCCWJ  Test  For blogs and Q &A sites Q & A sites  104  Mainichi Shimbun 2007  Training Word2vec of base-lm-2  100,000  Language model  Newspaper  Mainichi Shimbun 2008  Test  For newspaper  10,000
Tweet distribution	HEOT	 Table 1: Tweet distribution in dataset A and  HEOT.
SR training	WikiText corpus	To augment the SR training data, we used sentences from the WikiText corpus (.
SMT	Chinese Wikipedia corpus	For the SMT model, we trained the language model part on different corpora, including the Gigaword, the Chinese Wikipedia corpus), and the corpus consists of CGED as well as Lang-8 correct sentences which are constructed by ourselves.
Chinese Grammatical Error Diagnosis (CGED)	ACL2018	Chinese Grammatical Error Diagnosis (CGED) is a natural language processing task for the NLPTEA2018 workshop held during ACL2018.
Detecting Grammatical Errors	NTOU CGED System	Detecting Grammatical Errors in the NTOU CGED System by Identifying Frequent Subsentences
SMT	NMT	Section 3 then describes our principled unsupervised SMT method, while Section 4 discusses our hybridization method with NMT.
argument classification	UKP Senten-tial Argument Mining Corpus	For argument classification, we improve the state-of-the-art for the UKP Senten-tial Argument Mining Corpus by 20.8 percentage points and for the IBM Debater-Evidence Sentences dataset by 7.4 percentage points.
argument classification	IBM Debater-Evidence Sentences dataset	For argument classification, we improve the state-of-the-art for the UKP Senten-tial Argument Mining Corpus by 20.8 percentage points and for the IBM Debater-Evidence Sentences dataset by 7.4 percentage points.
natural language generation (NNLG)	E2E NLG challenge	With their end-to-end trainability, neural approaches to natural language generation (NNLG), particularly sequence-to-sequence (Seq2Seq) models, have been promoted with great fanfare in recent years (, and avenues like the recent E2E NLG challenge) have made available large datasets to promote the development of these models.
NER	2010 i2b2/VA challenge dataset	To overcome the problem of increased parameter size especially for low-resource settings, we propose the Conditional Softmax Shared Decoder architecture which achieves state-of-art results for NER and negation detection on the 2010 i2b2/VA challenge dataset and a proprietary de-identified clinical dataset.
negation detection	2010 i2b2/VA challenge dataset	To overcome the problem of increased parameter size especially for low-resource settings, we propose the Conditional Softmax Shared Decoder architecture which achieves state-of-art results for NER and negation detection on the 2010 i2b2/VA challenge dataset and a proprietary de-identified clinical dataset.
summarization	NVIDIA Tesla P100	We implemented summarization models with Chainer () and all summarization models were trained on NVIDIA Tesla P100.
sumeval	MeCab	Therefore, we used sumeval 4 with the MeCab on the ROUGE evaluation of the Mainichi dataset.
sumeval	Mainichi dataset	Therefore, we used sumeval 4 with the MeCab on the ROUGE evaluation of the Mainichi dataset.
MRT	GOLC	Although ROUGE scores of PG w/ LE trained with MRT showed better ROUGE scores than GOLC, %overs are higher than those of GOLC.
relation extraction	PCNN	For relation extraction, we use the embedding to augment PCNN+ATT () and improve the precision for top 1000 predictions from 83.9% to 89.8%.
relational reasoning	NUS	Besides graphs, relational reasoning is also of great importance in many natural language processing The original idea comes from several discussions between Hao Zhu and Jie Fu while Hao Zhu visiting NUS; Hao Zhu designed research, prepared datasets, and conducted experiments; Jie Fu, Yankai Lin, and Zhiyuan Liu also participated in discussion while planning experiments; Zhiyuan Liu, Tat-seng Chua and Maosong Sun proofread the paper.
sentiment analysis	Stanford Sentiment Treebank (SST) data sets	 Table 2: Experimental results on sentiment analysis on IMDb and Stanford Sentiment Treebank (SST) data sets.  Evaluation measure is accuracy.
neural machine translation (NMT)	EN-ET (WMT 2018)	 Table 3: Experimental results on neural machine translation (NMT). Results of Transformer Base on EN-VI  (IWSLT 2015), EN-RO (WMT 2016) and EN-ET (WMT 2018). Parameter size excludes word embeddings. Our  proposed Quaternion Transformer achieves comparable or higher performance with only 67.9% parameter costs  of the base Transformer model.
ASP	SNLI	For example, ASP with three-source tasks achieves 82.23% and 66.92% accuracy, respectively, in SNLI and MNLI, which are lower than 82.28% and 67.39% accuracy with its best performance with two-source tasks.
ASP	MNLI	For example, ASP with three-source tasks achieves 82.23% and 66.92% accuracy, respectively, in SNLI and MNLI, which are lower than 82.28% and 67.39% accuracy with its best performance with two-source tasks.
sentiment analysis	Stanford Sentiment Treebank dataset	Then we present the results on sentiment analysis using our sentence encoding on the Stanford Sentiment Treebank dataset ).
SST-5	Stanford Treebank paper (2013)	 Table 2: SST-5 and SST-2 performance on all and root  nodes respectively. Model results in the first section are  from the Stanford Treebank paper (2013). GenSen and  BERT BASE results are from (Subramanian et al., 2018)  and (Devlin et al., 2018) respectively.
MRC	MS	To facilitate the explorations and innovations in this area, many MRC datasets have been established, such as SQuAD (, MS MARCO (, and TriviaQA (.
HPSG building	PTB	For the purpose of accurate HPSG building, in this work, we construct a simplified HPSG only from annotations of PTB by combining constituent and dependency parse trees.
sentiment analysis	Se-mEval 2017 sentiment analysis dataset	Finally, we demonstrate that a deeper understanding of hash-tag semantics obtained through segmentation is useful for downstream applications such as sentiment analysis, for which we achieved a 2.6% increase in average recall on the Se-mEval 2017 sentiment analysis dataset.
Sentiment Analysis	Twitter shared task (subtask A) at SemEval 2017	We use the dataset from the Sentiment Analysis in Twitter shared task (subtask A) at SemEval 2017 (.
Sentiment analysis	SemEval 2017 test set	 Table 9: Sentiment analysis evaluation on the 3384  tweets from SemEval 2017 test set using the BiL- STM+Lex method
summarization task	WMS	As in the summarization task, SMS outperforms both ROUGE-L and WMS.
translation	NIST Zh-En dataset	 Table 2: Evaluation of translation performance on NIST Zh-En dataset. RNN and Lattice-RNN results are from  (Su et al., 2017). We highlight the highest BLEU score in bold for each set. ↑ indicates statistically significant  difference (p <0.01) from best baseline.
translation	IWSLT2016 En-De dataset	 Table 3: Evaluation of translation performance on  IWSLT2016 En-De dataset. RNN results are reported  from Morishita et al. (2018). ↑ indicates statistically  significant difference (p <0.01) from best baseline.
query translation	DBQT	For query translation, PSQ is better than DBQT because PSQ uses a weighted alternative to translate query terms and does not limit to the fixed translation from the dictionary as in DBQT.
query translation	DBQT	For query translation, PSQ is better than DBQT because PSQ uses a weighted alternative to translate query terms and does not limit to the fixed translation from the dictionary as in DBQT.
transfer learning	DEV document set	 Table 1: The MATERIAL dataset statistics. For SW and TL, we use the ANALYSIS document set with Q1 for  training, Q2 for dev, and Q3 for test. For transfer learning to SO, we use the DEV document set with Q1. Q1
sentiment classification	SST-5	shows the overall performances for sentiment classification on both SST-5 and SST-2.
sentiment classification	SST-2	shows the overall performances for sentiment classification on both SST-5 and SST-2.
Classification	NLPCC dataset	 Table 4: Classification accuracy on the NLPCC dataset.
MT-DNN	SNLI and SciTail datasets	Even with only 0.1% or 1.0% of the original training data, the performance of MT-DNN on both SNLI and SciTail datasets is better than many existing models.
MT-DNN	SNLI	We compare MT-DNN with existing state-of-the-art models including BERT and demonstrate the effectiveness of MTL with and without model fine-tuning using GLUE and domain adaptation using both SNLI and SciTail.
LE specialisation	WordNet	As in other work on LE specialisation, asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an-7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors.
extractive summarization	Google data set from Filippova and Altun (2013)	For extractive summarization, we use the Google data set from Filippova and Altun (2013).
knitting annotations	PTB corpus	We are publicly releasing the standoff annotations along with detailed annotation guidelines and scripts for knitting annotations onto the underlying PTB corpus.
ADR monitoring	ADR data sources	To overcome the limitation of a passive reporting system, active methods to ADR monitoring continuously explores frequently updated ADR data sources).
question duplicate detection	Quora question pairs dataset	For question duplicate detection, we use the Quora question pairs dataset as the source domain dataset and 5 datasets that are from different and diverse set of domains as our target domains.
RTE	Guardian Headlines RTE and SICK (SICK, 2014) datasets	For RTE, the Stanford Natural Language Inference (SNLI) has been used as source domain, and for target domains we used The Guardian Headlines RTE and SICK (SICK, 2014) datasets.
question duplicate  detection	RTE	 Table 1: Comparison of Accuracy for different domain adaptation methods; Source domain for question duplicate  detection: Quora (240k/ 80k/ 80k), Source domain for RTE: SNLI (550k/ 10k/ 10k); SF: shared features, DSF:  domain specific features, maxLoss: maximizing domain discriminator loss, GRL: gradient reversal layer
MWE similarity task	Sogou-T corpus	We We use pretrained word embeddings of MWEs (needed for training in the MWE similarity task) and constituents, which are trained using GloVe) on the Sogou-T corpus . We also utilize pretrained sememe embeddings obtained from the results of a sememe-based word representation learning model 5 (.
MMT	Multi30K dataset	We build and test our MMT models on the Multi30K dataset (.
regularization	EWC	We compare the regularization by EWC with the previous work that focuses on regularization by language modeling objectives.
Named Entity Recognition	Telugu-English Code-Mixed Social Media Data	Corpus Creation and Analysis for Named Entity Recognition in Telugu-English Code-Mixed Social Media Data
NER	Telugu-English code-mixed social media data	In this paper we present our work on NER in Telugu-English code-mixed social media data.
NER	BIO standard annotation	In NER using the BIO standard annotation, I-ORG cannot follow I-PER.
argument mining	ACL	The tutors have delivered tuto-rials on argument mining at ACL 2016, at IJCAI 2016 and at ESSLLI 2017; for ACL 2019, we have developed a tutorial that provides a synthesis of the major advances in the area over the past three years.
argument mining	IJCAI	The tutors have delivered tuto-rials on argument mining at ACL 2016, at IJCAI 2016 and at ESSLLI 2017; for ACL 2019, we have developed a tutorial that provides a synthesis of the major advances in the area over the past three years.
argument mining	ESSLLI	The tutors have delivered tuto-rials on argument mining at ACL 2016, at IJCAI 2016 and at ESSLLI 2017; for ACL 2019, we have developed a tutorial that provides a synthesis of the major advances in the area over the past three years.
Detection of Adverse Drug Reaction	ELMo	Detection of Adverse Drug Reaction mentions in tweets using ELMo
Node identification	SMATCH	 Table 2: Node identification and WSD results on MRS  in terms of noun (n), verb (v), quantifier (q), preposi- tion (p), adjective (a), conjunction (c), and others (x),  and on AMR in terms of predicate (pred). Both are  measured on the test set in terms of accuracy based on  SMATCH.
WSD	SMATCH	 Table 2: Node identification and WSD results on MRS  in terms of noun (n), verb (v), quantifier (q), preposi- tion (p), adjective (a), conjunction (c), and others (x),  and on AMR in terms of predicate (pred). Both are  measured on the test set in terms of accuracy based on  SMATCH.
SRL	MRS	 Table 6: Results on SRL. MRS's argument number be- gins at 1 so we just move all the argument to begin at 0  to make them comparable.
mention identification	UCCA	• A semantically-based framework for mention identification and coreference resolution as a layer of UCCA ( §3).
coreference resolution	UCCA	• A semantically-based framework for mention identification and coreference resolution as a layer of UCCA ( §3).
Size	Waseem and Hovy (2016) data set	 Table 1: Size of the Waseem and Hovy (2016) data set
Recognition task	BSNLP19	 Table 3: Official results on the Recognition task of BSNLP19, measured as F1 with Strict evaluation. The training  languages used are: Bulgarian (BG), Czech (CS), Polish (PL), Russian (RU), English (EN, CoNLL2003) and  the BSNLP17 languages (Croatian, Slovak, Slovene and Ukrainian). The top section of the table shows single- source experiments, in which each model is trained on a single language. The bottom section shows multi-source  experiments. The rightmost column, ALL, is a micro-average of the test results over the 4 test languages.
coreference resolution	CoNLL-2012 shared task dataset	Many works) related to coreference resolution have been published recently and all of them are evaluated with CoNLL-2012 shared task dataset).
segmentation	Sejong corpus	Previously, Na (2015) obtained 97.90% and 94.57% for segmentation and POS tagging respectively using the same Sejong corpus.
dialogue modeling	Reddit data 2	To enable the study of high-quality and large-scale dataset for dialogue modeling, we have collected a corpus of 35M conversations drawn from the Reddit data 2 , where each dialogue is composed of three turn exchanges.
dialogue utterance prediction	Reddit dataset	 Table 5: Side-by-side human evaluation along with 4-scale human evaluation of dialogue utterance prediction on  Reddit dataset (mean preferences ±90% confidence intervals).
referring expression  detection	CQA dataset	 Table 3: Results of POSNet-D for referring expression  detection on CQA dataset
Alation	CQA dataset	 Table 8: Alation study of the end-to-end contexual res- olution on the CQA dataset
paraphrase detection	Microsoft Research Paraphrase Corpus (MRPC	PD For paraphrase detection (i.e., decide whether two sentences are semantically equivalent), we use the Microsoft Research Paraphrase Corpus (MRPC;).
Sentence alignment recovery	newstest2018	 Table 3: Sentence alignment recovery results with dif- ferent similarity measures (newstest2018).
Sentiment Analysis	CMU-MOSI	 Table 2: Results for Sentiment Analysis on CMU-MOSI, emotion recognition on IEMOCAP and personality trait  recognition on POM. (CF, TF, and LMF stand for concat, tensor and low-rank fusion respectively).
Sentiment Analysis	IEMOCAP	 Table 2: Results for Sentiment Analysis on CMU-MOSI, emotion recognition on IEMOCAP and personality trait  recognition on POM. (CF, TF, and LMF stand for concat, tensor and low-rank fusion respectively).
SMDs	GER	This in combination with the unusual score distribution lead to higher absolute SMDs for GER and JPN speakers.
spell correction	QWK	With regard to Hypothesis 2.1, that character representations should improve performance as much as spell correction, the results demonstrate that adding character representations (w+c, -sp: mean MSE = 0.2218) can outperform spell correction of a word-only model (w, +sp: mean MSE = 0.2236) (although this is not reflected in the QWK results).
GEC	GEC test sets	In recent years, GEC performance has seen significant improvement in some public GEC test sets.
GEC translation	GEC parallel corpus	Learning a GEC translation model from noisy data is a worthy future direction as the GEC parallel corpus is expensive to obtain.
SED	Yahoo Japan Corporation	In order to improve the performance of SED, we propose an SED model taking the learner's proficiency into * Current affiliation: Yahoo Japan Corporation, hiroasan@yahoo-corp.jp † Current affiliation: Future Corporation, mizumoto.tomoya.mh7@is.naist.jp account.
Re-ranking	BEA 2019 Grammatical	TMU Transformer System Using BERT for Re-ranking at BEA 2019 Grammatical Error Correction on Restricted Track
GEC	GEC	There are three main types of neural network models for GEC, namely, recurrent neural networks (), a multi-layer convolutional model based on convolutional neural networks) and a transformer model based on self-attention . We follow the best practices to develop our system based on the transformer model, which has achieved better performance for GEC (.
ASAP	QWK	 Table 5: Results for the essay scoring task for ASAP  sets 1 and 2 reported in QWK.
MT	MSA	Therefore, special care was needed to train the MT system for the Gulf dialect because it has far fewer sentences than MSA we needed to duplicate the Gulf data 10 times in order to make the sizes of the data of the Gulf dialect and other dialects comparable.
segmentations	BPE	The segmentations obtained by BPE and SR were also relatively similar with an average edit distance of 5.03.
segmentations	SR	The segmentations obtained by BPE and SR were also relatively similar with an average edit distance of 5.03.
Cross-Country adaptation	ArSentD-LEV	 Table 1: Accuracies of linear SVM, NN, DANN and  the proposed approach for Cross-Country adaptation  on ArSentD-LEV. We can see that the proposed vari- ant outperforms other models in almost all DA tasks.
money lender)	French Historical Dictionary	For example, one of the meanings of the word usurier (i. e.; money lender), as reported by the French Historical Dictionary, refers to: the financial activities of the Jews [who since the Middle Ages were], the only ones authorised to lend on pawns.
IIC	PHMC dataset	It must be noted that IIC involves influenza while the PHMC dataset covers a set of illnesses as described later.
extracting lactation-specific drug information	NLM Drugs and Lactation Database (LactMed)	This paper describes a natural language processing (NLP) approach to extracting lactation-specific drug information from two sources: FDA-mandated drug labels and the NLM Drugs and Lactation Database (LactMed).
classify sentiment in tweets concerning current affairs	SemEval17-task4A corpus	Our system is firstly trained to classify sentiment in tweets concerning current affairs, using the SemEval17-task4A corpus.
ADR detection	SemEval17-task4A data	Our new ADR detection model firstly trains a classifier on the SemEval17-task4A data, which consists of Tweets on the subject of current affairs.
Transfer learning	MedNLI	Transfer learning: We conduct transfer learning on four different combinations of MedNLI, SNLI, and MNLI as it shown in the table 4 (line 4 to 7) and also add the results of general domain tasks (MNLI, SNLI) for comparison.
Transfer learning	MNLI	Transfer learning: We conduct transfer learning on four different combinations of MedNLI, SNLI, and MNLI as it shown in the table 4 (line 4 to 7) and also add the results of general domain tasks (MNLI, SNLI) for comparison.
Question Answering	SquAD dataset	Further, a closed-domain Question Answering technique that uses Bi-directional LSTMs trained on the SquAD dataset to determine relevant ranks of answers fora given question is also discussed.
phrase-based machine translation (MT)	WMT19	Parallel feature weight decay algorithms (parfda)) is an instance selection tool we use to select training and language model instances to build Moses ( phrase-based machine translation (MT) systems to translate the test sets in the news translation task at WMT19 (.
morphological segmentation	Apertium	Sections 4 and 5 describe respectively morphological segmentation and hybridization with Apertium.
MT	WMT19	This is precisely the motivation for the document-level MT track in this year's WMT19.
IITP-MT	WMT	IITP-MT System for Gujarati-English News Translation Task at WMT 2019
Robustness Task	WMT15 parallel data	The model that was used as a baseline for the Robustness Task was trained on the WMT15 parallel data.
transference	APE Task organized at WMT 2019	In this paper we present an English-German Automatic Post-Editing (APE) system called transference, submitted to the APE Task organized at WMT 2019.
transference4M	NMT training data	For transference4M, we first train on a training set called eScape4M combined with the first 12k of the provided NMT training data.
transferenceALL	eScape dataset (eScapeAll)	For transferenceALL, we initially train on the complete eScape dataset (eScapeAll) combined with the first 12k of the training data.
corpus filtering task	YiSi-1	At last year's edition of the corpus filtering task,'s supervised submissions were developed in the same philosophy using anew semantic MT evaluation metric, YiSi-1.
Answer extraction	Wang et al.  (2007) test set	 Table 4: Answer extraction results on the Wang et al.  (2007) test set.
SRL)	FrameNet	Previous approaches to generating training data for SRL) do not use lexical resources apart from FrameNet.
NERD	ACE	 Table 6: NERD results on ACE.
annotation projection	Watchtower Society	For annotation projection, we need a parallel corpus, and we therefore have to rely on resources such as the Bible (parts of which are available in 1,646 languages), and publications from the Watchtower Society (up to 583 languages).
UAS parsing	WTC data	 Table 3: POS tagging accuracies and UAS parsing scores for the models built using WTC data. The results are  split for source and target languages. All baselines and upper bounds use IBM1 POS taggers, while our MULTI-PROJ  systems use their respective IBM1 or IBM2 taggers.
SNM language	LDC English Gigaword corpus	We evaluate SNM language models on two corpora: the One Billion Word Benchmark and a subset of the LDC English Gigaword corpus.
dependency parsing	WordNet	For example, in dependency parsing, word embeddings could be tailored to capture similarity in terms of context within syntactic parses () or they could be refined using semantic lexicons such as WordNet, FrameNet () and the Paraphrase Database ( to improve various similarity tasks (.
NP bracketing	WordNet	 Table 1: Results for the word similarity datasets, geographic analogies and NP bracketing. The first upper blocks  (A-C) present the results with retrofitting. NPK stands for no prior knowledge (no retrofitting is used), WN for  WordNet, PD for PPDB and FN for FrameNet. Glove, Skip-Gram, Global Context, Multilingual and Eigen are the  word embeddings of Pennington et al. (2014), Mikolov et al. (2013b), Huang et al. (2012), Faruqui and Dyer
NP bracketing	WordNet	 Table 2: Results on word similarity dataset (average  over 11 datasets) and NP bracketing. The word embed- dings are derived by using SVD on the similarity graph  extracted from the prior knowledge source (WordNet,  PPDB and FrameNet).
taxonomy construction	MH370 domain	We evaluate our method for taxonomy construction based on the following four datasets of document collections obtained from different domains: • • MH370 domain: The corpus is about the root term 'Issues related to MH370 search'.
fMRI acquisition	Bruker MedSpec MRI scanner	fMRI acquisition and preprocessing recorded fMRI images on a 4T Bruker MedSpec MRI scanner.
translation	GRU	The translation performance is comparable with the standard NMT system with GRU, but the system enjoys a simpler structure (i.e., uses only a single gate and half of the parameters) and a faster decoding (i.e., requires only half the matrix computations for decoding).
POS tagging	UD v1. treebanks	1.  For POS tagging we also experiment with UD v1. treebanks.
POS tagging	CoNLL 2006/07 shared task	 Table 2: Treebanks used for POS tagging experiments  from the CoNLL 2006/07 shared task.
POS tagging	CoNLL-X datasets	 Table 3: Performances of sparse and dense word representations for POS tagging over the 12 CoNLL-X datasets.
SemEval 2015 Twitter sentiment analysis challenge	WE-BIS	We also compare against the three top-performing systems in the SemEval 2015 Twitter sentiment analysis challenge: WE-BIS (,, and LSISLIF (.
DST	NBT	We investigate the difference in DST performance for English, German and Italian when the NBT model employs the following word vector collections: 1) distributional word vectors; 2) monolingual semantically specialized vectors; and 3) monolingual subspaces of the cross-lingual semantically specialized EN-DE-IT-RU vectors.
dependency parsers	CoNLL 2006-2007 shared tasks	In contrast, modern dependency parsers are expected to excel on the 19 languages of the CoNLL 2006-2007 shared tasks on multilingual dependency parsing (, and additional challenges, such as the shared task on parsing multiple English Web domains, are continuously proposed.
sentence understanding task	Penn Treebank (PTB;)	These models are designed to learn grammars-strategies for assigning trees to sentences-that are suited to help solve the sentence understanding task at hand, rather than ones that approximate expert-designed grammars like that of the Penn Treebank (PTB;).
Qualitiative analysis	WIKIHOP samples	 Table 3: Qualitiative analysis of WIKIHOP samples.
SVM	GPPL medi.	This improvement is statistically significant (p .01 using two-tailed Wilcoxon signed-rank test) for SVM with all metrics except accuracy, for BiLSTM with AUC only, and for GPPL medi. with Pearson correlation only.
segmentation	Universal Dependencies datasets	Based on the analysis, we design a small set of language-specific settings and extensively evaluate the segmentation system on the Universal Dependencies datasets.
segmentation	UD datasets	3. Our segmentation system achieves state-of-theart accuracy on the UD datasets and improves on previous work especially for the most challenging languages.
tokenization	Stanford CoreNLP	For all data used in experiments, tokenization is done using Stanford CoreNLP ().
Coreference resolution	MUC	Coreference resolution involves linking referring expressions that evoke the same discourse entity, as defined in shared tasks such as) and MUC (.
sentiment analysis	Yelp	 Table 3: System comparison of sentiment analysis on  Yelp. Significant improvements over state of the art are  marked with  *  (test of equal proportions, p < 0.05).
answer selection	NewsQA	To further test the utility of explicit entailment rules, we evaluate the learned rules on an extrinsic task: answer selection for machine reading comprehension on NewsQA, a data set that 712 The board hailed Romney for his solid credentials.
grammar correction	CoNLL shared tasks	2 This is a standard metric used in grammar correction since the CoNLL shared tasks.
AMR-to-text generation task	LDC2015E86 dataset (AMR15	For the AMR-to-text generation task, we use two benchmarks-the LDC2015E86 dataset (AMR15) and the LDC2017T10 dataset (AMR17).
AMR-to-text generation task	LDC2017T10 dataset	For the AMR-to-text generation task, we use two benchmarks-the LDC2015E86 dataset (AMR15) and the LDC2017T10 dataset (AMR17).
AMR-to-text generation task	AMR17	For the AMR-to-text generation task, we use two benchmarks-the LDC2015E86 dataset (AMR15) and the LDC2017T10 dataset (AMR17).
Tweet classification	Tweet 2016 dataset	 Table 11: Results for Subtask B "Tweet classification according  to a two-point scale" on the Tweet 2016 dataset. The systems  are ordered by their ρ P N score (higher is better). The meaning  of "(*)" is as in
Size	SemEval 2016 datasets	 Table 1: Size of the data used for training and development purposes. We only relied on the SemEval 2016 datasets.
sentiment analysis	Twitter Task of SemEval-2016	This paper describes our sentiment analysis system which has been built for Sentiment Analysis in Twitter Task of SemEval-2016.
Sentiment Analysis	Twitter Task of SemEval-2016	This paper describes our sentiment analysis system which has been built for Sentiment Analysis in Twitter Task of SemEval-2016.
feature selection	Slot 1	We conducted several different approaches for feature selection to improve classification performance on both Slot 1 and Slot 3.
Detecting Stance in Tweets task	SemEval-2016 Task 6)	This paper describes our approach for the Detecting Stance in Tweets task (SemEval-2016 Task 6).
stance detection	NPOV Corpus from Wikipedia	More recent approaches to stance detection have been performed using linguistic rules on online debate dataset) and NPOV Corpus from Wikipedia (.
sentiment strength prediction	Tweet in SemEval 2015	This sentiment strength prediction task was severed as the subtask E of Sentiment Analysis on Tweet in SemEval 2015.
Sentiment Analysis	Tweet in SemEval 2015	This sentiment strength prediction task was severed as the subtask E of Sentiment Analysis on Tweet in SemEval 2015.
SemEval shared task	STS Core	In this paper, we describe our system DTSim and the submitted three different runs in this year's SemEval shared task on Semantic Textual Similarity English track (STS Core;).
words alignments between sentences	WordNet	In SemEval 2015, the most popular approach among the best systems was the use of words alignments between sentences combining resources such as WordNet, neural word embedding (Mikolov,) and the Paraphrase Database ().
SemEval 2016 Task 3	CQA	This section presents the evaluation of the SemEval 2016 Task 3 on CQA.
hyperparameter tuning	DiMSUM dataset	For hyperparameter tuning, we holdout 30% randomly selected training samples of the DiMSUM dataset as validation data.
SST of unambigu-ous nouns	WordNet	first trained and tested a discriminative model for SST of unambigu-ous nouns on data extracted from different versions of WordNet and achieved an accuracy of slightly over 52%.
SemEval Complex Word Identification challenge	English Wikipedia (NormalBag)	We submitted two systems to the SemEval Complex Word Identification challenge (Task 11), which used the same parameter settings and only differed in where the corpus frequencies were collected, English Wikipedia (NormalBag) and the Google Web Corpus (GoogleBag).
SemEval Complex Word Identification challenge	Google Web Corpus (GoogleBag)	We submitted two systems to the SemEval Complex Word Identification challenge (Task 11), which used the same parameter settings and only differed in where the corpus frequencies were collected, English Wikipedia (NormalBag) and the Google Web Corpus (GoogleBag).
translation	English gold standards	The translation of English gold standards revealed a number of issues.
Subtask	training	 Table 1: Results on Subtask A on a 10 fold CV on the training
AMR parser	JAMR	The first published AMR parser, JAMR), performs AMR parsing in two stages: concept identification and relation identification.
AMR parsing	SemEval development set (LDC2015E86).	 Table 2: AMR parsing performance on the official SemEval development set (LDC2015E86).VERB: ISI  verbalization list. BROWN: Brown cluster features.RNE: Rich (OntoNotes) named entities.SRL: semantic  role labeling features. WIKI: Addition of wikification of named entities in AMR.
AMR parsing	SemEval  Test Set	 Table 3: AMR parsing performance on full SemEval  Test Set and the Blind Test Set
AMR parsing	Blind Test Set	 Table 3: AMR parsing performance on full SemEval  Test Set and the Blind Test Set
SemEval 2016 Task 9 Chinese Semantic Dependency Parsing shared task	NEWS	In the SemEval 2016 Task 9 Chinese Semantic Dependency Parsing shared task, semantic dependency parsing for two text genres, TEXT, which includes sentences from conversations and primary school textbooks, and NEWS, which contains newswire text, is explored.
SemEval Clinical TempEval Tasks	TimeML	The setting of the 2015 and 2016 SemEval Clinical TempEval Tasks is similar to previous TempEval campaigns, with the two main differences: i.) the domain , i.e. (colon) cancer clinical notes; and ii.) the annotation scheme, i.e. the THYME annotation scheme), an extended version of TimeML ().
taxonomy induction	TExEval-2 task in SemEval-2016	Below, we will briefly (i) describe related work on different approaches to taxonomy induction, (ii) explain the linguistic phenomenon of endocentricity, (iii) present our endocentric hypo-hypernym identification system and the results of our submission to the TExEval-2 task in SemEval-2016.
MT	IWSLT 2007	 Table 1: Comparison of translation quality for three methods used to train Moses for Chinese-English  MT under small corpus IWSLT 2007 conditions  cased  uncased  System  BLEU TER BLEU TER  Giza++ based induction  19.23 63.94 19.83 63.40  ITG based induction  20.05 63.19 20.42 62.61  XMEANT outside probabilities based 27.59 59.48 28.54 58.81
RTE	RTE datasets	These types of reasoning are weak points of RTE systems as the above mentioned semantic phenomena are underrepresented in the RTE datasets.
Distribution of sentiment	Stance Train and Test sets	 Table 5: Distribution of sentiment in the Stance Train and Test sets.
role filler	PropBank	The left column of predicts the role filler, while the right column predicts PropBank annotations.
word analogy task	Word2Vec Skip-gram	 Table 4: Accuracies for the word analogy task. All  our results are significantly higher than the result  of Word2Vec Skip-gram (with two-tail p < 0.001  using McNemar's test).
word analogy task	McNemar's test	 Table 4: Accuracies for the word analogy task. All  our results are significantly higher than the result  of Word2Vec Skip-gram (with two-tail p < 0.001  using McNemar's test).
relation extraction task	FB15k-237	For evaluating our method on the relation extraction task, we conducted experiments on two freely available datasets: TEG-REP ( and FB15k-237 ( ).
SE-type classification	English test set	 Table 3: SE-type classification on English test set,  with context as predicted labels (pLab).
question answering	GEOS	In this paper, we introduce the novel task of question answering using natural language GEOS derives a weighted logical expression where each predicates also carry a weighted score but we do not show them here for clarity. demonstrations.
SemEval shared task	STS English track	In this paper, we describe our system DT Team and the three different runs that we submitted to this year's SemEval shared task on STS English track (Track 5;).
PE	SemEval STS data	We train PE using two datasets, PPDB 2.0 (Pavlick et al., 2015) and SemEval STS data.
SemEval-2017 Task	CQA	This section presents the evaluation of the SemEval-2017 Task 3 on CQA (.
SemEval Subtask2	WordNet	 Table 3: SemEval Subtask2 results using WordNet  as similarity measure.
Stance Classification	CNN	Mama Edha at SemEval-2017 Task 8: Stance Classification with CNN and Rules
SemEval-2017 Task 4	Yelp	Ordinal Classification As last year, SemEval-2017 Task 4 includes sentiment analysis on a fivepoint scale {HIGHLYPOSITIVE, POSITIVE, NEU-TRAL, NEGATIVE, HIGHLYNEGATIVE}, which is inline with product ratings occurring in the corporate world, e.g., Amazon, TripAdvisor, and Yelp.
replicability	BLSTM	For replicability purposes, with this paper we are releasing our source code 1 and the finance specific BLSTM word embedding model 2 .
Size	Se- mEval 2016 datasets	 Table 1: Size of the data used for training and de- velopment purposes. We only relied on the Se- mEval 2016 datasets.
SemEval Task	Financial Microblogs and News	This paper presents the details of our system IBA-Sys that participated in SemEval Task: Fine-grained sentiment analysis on Financial Microblogs and News.
SVM	microblog datasets	During validation phase we observed that models trained on SVM work better than that of SVR for the microblog datasets.
sentiment analysis	Emo-Int2017	Some existing tools and resources that enlarged the perspective and built the basis of sentiment analysis are: Emo-Int2017, NRC Emotion lexicon, Best-Worst Scaling resources; VADER-Sentiment-Analysis, SentiWordNet, NLTK Sentiment Analyze, and Affective Tweets.
sentiment analysis	NRC Emotion lexicon	Some existing tools and resources that enlarged the perspective and built the basis of sentiment analysis are: Emo-Int2017, NRC Emotion lexicon, Best-Worst Scaling resources; VADER-Sentiment-Analysis, SentiWordNet, NLTK Sentiment Analyze, and Affective Tweets.
sentiment analysis ordinal classification task	Random Forest Tree	For emotion intensity ordinal classification task (Task 2), sentiment analysis ordinal classification task (Task 4) and emotion classification task (Task 5) Doc2Vec size was varied from 10 to 1000 with an increment of 10 in each iteration, n estimator of Random Forest Tree was varied from 10 to 150 with an increment of 10 in each iteration and max depth of Random Forest Tree was varied from 2 to 20 with an increment of 1 in each iteration.
sentiment analysis ordinal classification task	Random Forest Tree	For emotion intensity ordinal classification task (Task 2), sentiment analysis ordinal classification task (Task 4) and emotion classification task (Task 5) Doc2Vec size was varied from 10 to 1000 with an increment of 10 in each iteration, n estimator of Random Forest Tree was varied from 10 to 150 with an increment of 10 in each iteration and max depth of Random Forest Tree was varied from 2 to 20 with an increment of 1 in each iteration.
emotion classification task	Random Forest Tree	For emotion intensity ordinal classification task (Task 2), sentiment analysis ordinal classification task (Task 4) and emotion classification task (Task 5) Doc2Vec size was varied from 10 to 1000 with an increment of 10 in each iteration, n estimator of Random Forest Tree was varied from 10 to 150 with an increment of 10 in each iteration and max depth of Random Forest Tree was varied from 2 to 20 with an increment of 1 in each iteration.
emotion classification task	Random Forest Tree	For emotion intensity ordinal classification task (Task 2), sentiment analysis ordinal classification task (Task 4) and emotion classification task (Task 5) Doc2Vec size was varied from 10 to 1000 with an increment of 10 in each iteration, n estimator of Random Forest Tree was varied from 10 to 150 with an increment of 10 in each iteration and max depth of Random Forest Tree was varied from 2 to 20 with an increment of 1 in each iteration.
classification tasks	Random Forest Tree algorithm	Variables used to estimate the ideal parameters for classification tasks was accuracy of the Random Forest Tree algorithm.
validation	Keras (TensorFlow backend)	All the training and validation procedure were carried outwith Keras (TensorFlow backend) ().
classification	GitHub link	For classification, data set is comprised of 4792 samples, taken from GitHub link provided by the SemEval 2018 organizers.
classification	SemEval 2018 organizers	For classification, data set is comprised of 4792 samples, taken from GitHub link provided by the SemEval 2018 organizers.
Near Document Detection	Thresh- old	 Table 9: Near Document Detection Results by Thresh- old
transfer learning	SNLI dataset	About transfer learning, showed a good precedent, using SNLI dataset.
relation extraction	USAGE	Given the concept pair [Unsupervised training] and [phone n-gram model], the relation extraction task is to identify whether there is a relation between the concepts, while the the relation classification task is to identity the relation as USAGE.
relation extraction	ACE	Most research on relation extraction has leveraged standard benchmark datasets from the ACE ( and.
SemEval-2018 Task 7-given an entity-tagged text	ACL Anthology corpus	This paper describes our approach to SemEval-2018 Task 7-given an entity-tagged text from the ACL Anthology corpus, identify and classify pairs of entities that have one of six possible semantic relationships.
Classifying the relationship between entities	ACL Anthology corpus	Classifying the relationship between entities is an important natural language processing (NLP) task that serves as a building block fora variety of NLP applications such as knowledge base construction and question-answering tasks.) provided entitytagged texts from the ACL Anthology corpus and asked participants to identify and classify entity pairs into one of six semantic relationships.
SemEval 2018	Advanced Persistent Threats Notes collection	Task 8 for SemEval 2018 asked participants to work on a set of related sub-tasks involving analyzing information from text about malware drawn from the Advanced Persistent Threats Notes collection () using the semantic framework found in the Malware Attribute Enumeration and Characterization language).
comparison of semantic similarity	WordNet	This corpus has been used in many different applications including estimating word-relatedness (), comparison of semantic similarity (Joubarne and Inkpen, 2011), information retrieval, lexical disambiguation (, improving general purpose NLP classifiers ( , and improving parsing performance . Knowledge-driven approaches to the detection of semantic relations rely on manually constructed lexical and encyclopedic resources, such as ConceptNet (, ImageNet (Russakovsky et al., 2015), WordNet), Wiktionary, Open Mind Common Sense () and DBpedia (.
information retrieval	WordNet	This corpus has been used in many different applications including estimating word-relatedness (), comparison of semantic similarity (Joubarne and Inkpen, 2011), information retrieval, lexical disambiguation (, improving general purpose NLP classifiers ( , and improving parsing performance . Knowledge-driven approaches to the detection of semantic relations rely on manually constructed lexical and encyclopedic resources, such as ConceptNet (, ImageNet (Russakovsky et al., 2015), WordNet), Wiktionary, Open Mind Common Sense () and DBpedia (.
Recognising Discriminative Attributes	WordNet	ABDN at SemEval-2018 Task 10: Recognising Discriminative Attributes using Context Embeddings and WordNet
argument reasoning comprehension	Debate section of the New York Times	The most central part of this task is how to find the warrant for the given Rand C. In the argument reasoning comprehension task, the organizer extracts the instances from Room for Debate section of the New York Times.
argument reasoning comprehension	Debate section of the New York Times	The argument reasoning comprehension task chooses the Room for Debate section of the New York Times as source data.
question answering	DBPedia Spotlight 1	for question answering (e.g. DBPedia Spotlight 1 , AIDA 2 ).
emotion recognition task	Se-mEval 2007 news headlines dataset	We also evaluate EmoWord-Net in an emotion recognition task using Se-mEval 2007 news headlines dataset and we achieve an improvement compared to the use of DepecheMood.
KGC task	WN9-IMG dataset	To gain initial insights into the potential benefits of external information for the KGC task, let us consider the embeddings produced by the translationbased TransE method () on the WN9-IMG dataset (.
KG representation learning	Freebase	The contributions of this paper can be summarized as follows: (1) We propose an approach for KG representation learning that incorporates multimodal (visual and linguistic) information in a translation-based framework and extends the definition of triple energy to consider the new multimodal representations; (2) we investigate different methods for combining multimodal representations and evaluate their performance; (3) we introduce anew large-scale dataset for multimodal KGC based on Freebase; (4) we experimentally demonstrate that our approach outperforms baseline approaches including the state-of-the-art method of on the link prediction and triple classification tasks.
DA	Amazon multi-domain sentiment dataset	For DA we use Amazon multi-domain sentiment dataset () containing product reviews from four categories: Books (B), DVDs (D), Electronics (E) and Kitchen Appliances (K).
MAP	Word2Vec	 Table 1: MAP scores on three categories. The first four  rows use various techniques with Word2Vec. The next  four demonstrate Category Builder built on the same  corpus, to show the effect of ρ and association measure  used. For all four Category Builder rows, we used n =  100. Both increasing ρ and switching to APPMI can be  seen to be individually and jointly beneficial.  † The last  line reports the score on a different corpus, the release  data, with APPMI and ρ = 3, n = 100.
cross-lingual alignment	WMT'12 test sets	We used WMT'12 Common Crawl data for cross-lingual alignment, and WMT'12 test sets for evaluations.
word similarity task	WordNet graph	We show the effectiveness of the proposed approach intrinsically on a word similarity task, by learning synset vectors of the WordNet graph based on several similarity measures.
negation type	Simple  Wikipedia	 Table 3: Distribution of negation type in Simple  Wikipedia.
SemEval-2019	UCCA	SemEval-2019 Task 1: Cross-lingual Semantic Parsing with UCCA
Criticism of futures COMES	Wall Street	(1) Criticism of futures COMES FROM Wall Street.
SemEval-2019 Task 6	Amrita School of Engineering -CSE	 Table 5: All the teams that participated in SemEval-2019 Task 6 with their ranks for each sub-task. The symbol '-'  indicates that the team did not participate in some of the subtasks. Please, refer to Table 4 to see the scores based  on a team's rank. The top team for each task is in bold, and the second-place team is underlined. Note: ASE -CSE  stands for Amrita School of Engineering -CSE, and BNU-HBKU stands for BNU-HKBU UIC NLP Team 2.
SemEval-2019 Task 6	BNU-HKBU UIC NLP Team 2	 Table 5: All the teams that participated in SemEval-2019 Task 6 with their ranks for each sub-task. The symbol '-'  indicates that the team did not participate in some of the subtasks. Please, refer to Table 4 to see the scores based  on a team's rank. The top team for each task is in bold, and the second-place team is underlined. Note: ASE -CSE  stands for Amrita School of Engineering -CSE, and BNU-HBKU stands for BNU-HKBU UIC NLP Team 2.
SemEval 2019 Task 1	UCCA	In SemEval 2019 Task 1: Cross-lingual Semantic Parsing with Universal Conceptual Cognitive Annotation (UCCA) 1 , the Committee focuses on parsing text according to the UCCA semantic annotation.
parsing English, German, and French	UCCA semantic tagset	The shared task, 'cross-lingual semantic parsing with UCCA' consists in parsing English, German, and French datasets using the UCCA semantic tagset.
semantic parsing	UCCA	We present a simple and accurate model for semantic parsing with UCCA as our submission for SemEval 2019 Task 1.
semantic parsing of texts into graphs	UCCA scheme	Universal Conceptual Cognitive Annotation (UCCA)) is designed to support semantic parsing with mappings between sentences and their corresponding meanings in a framework intended to be applicable across languages.) focuses on semantic parsing of texts into graphs consisting of terminal nodes that represent words, non-terminal nodes that represent internal structure, and labeled edges representing relationships between nodes (e.g. participant, center, linker, adverbial, elaborator), according to the UCCA scheme.
transfer learning	OpenAI GPT	We experiment with transfer learning using pre-trained language models (ULMFiT, OpenAI GPT, and BERT) and fine-tune them on this task.
Emotion Classification	RCNN	ntuer at SemEval-2019 Task 3: Emotion Classification with Word and Sentence Representations in RCNN
SemEval-2019 Task 3	EmoContext	In this paper we present our submission for SemEval-2019 Task 3: EmoContext.
Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter	hatEval)	LT3 at SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter (hatEval)
stacking	CNN-Bi-LSTM	The three best models that performed best on individual sub tasks are stacking of CNN-Bi-LSTM with Attention, BiLSTM with POS information added with word features and Bi-LSTM for third task.
Tackling Offensive Language Identification	ELMo	HHU at SemEval-2019 Task 6: Context Does Matter -Tackling Offensive Language Identification and Categorization with ELMo
Detection of offensive language	TRAC	Detection of offensive language has been investigated in recent years in various studies () and various workshops such as ALW (Abusive Language Online) and TRAC.
SemEval-2019 competition tasks	hatEval	The system was developed for two SemEval-2019 competition tasks: Task 5 hatEval "Multilingual detection of hate speech against immigrants and women in Twitter" (team name: LU Team) and Task 6 OffensEval "Identifying and categorizing offensive language in social media" () (team name: NLPR@SRPOL).
SemEval-2019 competition tasks	NLPR@SRPOL	The system was developed for two SemEval-2019 competition tasks: Task 5 hatEval "Multilingual detection of hate speech against immigrants and women in Twitter" (team name: LU Team) and Task 6 OffensEval "Identifying and categorizing offensive language in social media" () (team name: NLPR@SRPOL).
Multilingual detection of hate speech against immigrants and women in Twitter	NLPR@SRPOL	The system was developed for two SemEval-2019 competition tasks: Task 5 hatEval "Multilingual detection of hate speech against immigrants and women in Twitter" (team name: LU Team) and Task 6 OffensEval "Identifying and categorizing offensive language in social media" () (team name: NLPR@SRPOL).
Toponym disambiguation	Manchester, NH, USA vs. Manchester, UK	Toponym disambiguation tackles ambiguities between different toponyms, like Manchester, NH, USA vs. Manchester, UK, and between toponyms and other entities, such as names of people or daily life objects (Geo-NonGeo ambiguities).
Rumour stance classification	SemEval 2019 Task 7	Rumour stance classification and rumour veracity classification are two subtasks of and SemEval 2019 Task 7 (.
Subtask A	Qatar Living forum	The given training set consisted of 1,118 questions for Subtask A that were selected from the Qatar Living forum.
question answering (cQA)	Qatar Living forum (QLF)	With the rising popularity of online community question answering (cQA) systems such as Quora, StackOverflow, and Qatar Living forum (QLF), the amount of information shared over these platforms is also increasing rapidly with time.
SemEval 2019 Task 8	QatarLiving public forum	The SemEval 2019 Task 8 on Fact-Checking in community question answering forums aimed to classify questions into categories and verify the correctness of answers given on the QatarLiving public forum.
suggestion mining task	Universal Windows Platform	The dataset for suggestion mining task consists of feedback posts on Universal Windows Platform available on uservoice.com.
Suggestions	Oxford Dictionary	Suggestions in the Oxford Dictionary are defined as ideas or plans for consideration.
collection	IRB-approved	Patients were de-identified and the collection process was part of an IRB-approved study.
pathology tracking	Personal Health Questionnaire 9 (PHQ9)	However, most of the pathology tracking and improvement assessment is done through questionnaires, e.g. the Personal Health Questionnaire 9 (PHQ9) for depression (), and require a subjective comment by the patient, e.g. "How many days have you been bothered with little interest or pleasure in doing things in the past two weeks?".
classification	Naïve Bayes classifier	During the classification phase using the Naïve Bayes classifier, we have experimented under two conditions 1.
short-answer scoring	ASAP corpus	For short-answer scoring, Horbach & Palmer explore the suitability of active learning for automatic short-answer assessment on the ASAP corpus; Banjade et al present a corpus that contains student answers annotated for their correctness in context, in addition to a baseline for predicting the correctness label; and Rudzewitz explores the practical usefulness of the combination of features from three different fields -short answer scoring, authorship attribution, and plagiarism detection -for two tasks: semantic learner language classification, and plagiarism detection for evaluating short answers.
Classification	WeeBit cor-	 Table 3: Classification and ranking results on the WeeBit cor-
SMT	GEC	Since SMT was not originally designed for GEC, many standard features do not perform well on this task.
tokenisation	Wall Street Journal	Segmentation and tokenisation are performed using RASP (), which is expected to perform better on learner data than a system developed exclusively from high quality copy-edited text such as the Wall Street Journal.
SMT	FCE test set	 Table 2: SMT system performance on the FCE test set (in percentages). The best results are marked in bold.
coreference resolution	MUC	Many NLP researchers, especially those not working in the area of discourse processing, tend to equate coreference resolution with the sort of coreference that people did in MUC, ACE, and OntoNotes, having the impression that coreference is a well-worn task owing in part to the large number of papers reporting results on the MUC/ACE/OntoNotes corpora.
coreference resolution	MUC/ACE/OntoNotes corpora	Many NLP researchers, especially those not working in the area of discourse processing, tend to equate coreference resolution with the sort of coreference that people did in MUC, ACE, and OntoNotes, having the impression that coreference is a well-worn task owing in part to the large number of papers reporting results on the MUC/ACE/OntoNotes corpora.
information extraction (IE)	MUC-6 coreference in 1995	The decision to focus on entity coreference resolution was initially made by information extraction (IE) researchers when coreference was selected as one of the tasks in the MUC-6 coreference in 1995.
mention detection	CoNLL scorer	gives precision and recall for mention detection, while shows coreference resolution performance according to several measures calculated using the official CoNLL scorer (version 8.01, see).
Mention detection	GUM	 Table 5: Mention detection in GUM and WSJ.
Mention detection	WSJ	 Table 5: Mention detection in GUM and WSJ.
Cross-domain classification	Naïve Bayes classifier	 Table 5: Cross-domain classification results on  features using a Naïve Bayes classifier (%)
parsing	TIGER data set	choosing the appropriate terminal order is crucial to parsing success: We obtain state-of-the-art results for discontinuous shift-reduce constituency parsing, namely 80.02 on the TIGER data set of.
constituency parsing	TüBa-D/Z treebank	Hence, constituency parsing techniques that rely on a context-free backbone either have to do with a language-dependent approximation that puts topology at the center (e.g. the TüBa-D/Z treebank of, based on topological fields) or can only produce an approximation of the actual predicate-argument structures (as is the case with most parsing approaches targeting the Negra and Tiger treebanks, cf..
constituency parsing	Negra and Tiger treebanks	Hence, constituency parsing techniques that rely on a context-free backbone either have to do with a language-dependent approximation that puts topology at the center (e.g. the TüBa-D/Z treebank of, based on topological fields) or can only produce an approximation of the actual predicate-argument structures (as is the case with most parsing approaches targeting the Negra and Tiger treebanks, cf..
parsing	SMULTRON tree	 Table 3: Comparative results for parsing the SMULTRON tree-
metaphor detection	MRC Psycholinguistic Database (MRCPD)	We analyze CRF based classification model for metaphor detection using syntactic, conceptual, affective, and word embeddings based features which are extracted from MRC Psycholinguistic Database (MRCPD) and WordNet-Affect.
metaphor detection	WordNet-Affect	We analyze CRF based classification model for metaphor detection using syntactic, conceptual, affective, and word embeddings based features which are extracted from MRC Psycholinguistic Database (MRCPD) and WordNet-Affect.
alignment transfer	English-Swedish test set	Experimental setup We evaluate the proposed alignment transfer methods on the English-Swedish test set provided by, which consists of 192 word aligned sentence pairs extracted from the English-Swedish part of Europarl (Koehn, ).
alignment transfer	Europarl	Experimental setup We evaluate the proposed alignment transfer methods on the English-Swedish test set provided by, which consists of 192 word aligned sentence pairs extracted from the English-Swedish part of Europarl (Koehn, ).
WSD	SALDO lexicon	We used this method to build a WSD system for Swedish using the SALDO lexicon, and evaluated it on six different annotated test sets.
MIR	IMRaD structure	We analyze two major characteristics of MIR: their positions in the IMRaD structure of articles, and the number of in-text references that makeup a MIR in the di↵erent journals.
answer selection task	cQA	 Table 4: Results on (a) development and (b) test data for answer selection task in cQA.
question retrieval task	cQA	 Table 5: Results on (a) development and (b) test data for question retrieval task in cQA.
WSD	AdaGram	Finally, note that none of the unsupervised WSD methods discussed in this paper, including the top-ranked SemEval submissions and AdaGram, were able to beat the most frequent sense baselines of the respective datasets (with the exception of the balanced version of TWSI).
WSD	TWSI	Finally, note that none of the unsupervised WSD methods discussed in this paper, including the top-ranked SemEval submissions and AdaGram, were able to beat the most frequent sense baselines of the respective datasets (with the exception of the balanced version of TWSI).
WSD	TWSI dataset	 Table 3: Upper-bound and actual value of the WSD performance on the sense-balanced TWSI dataset,  function of sense inventory used for unweighted pooling of word vectors.
Blow Out"	Movie Dialog Dataset	Why "Blow Out"? A Structural Analysis of the Movie Dialog Dataset
transfer	MSRP (Section 6.1	Our results on transfer to the related task of paraphrase ranking which also uses MSRP (Section 6.1) are even more encouraging.
paraphrase ranking	MSRP (Section 6.1	Our results on transfer to the related task of paraphrase ranking which also uses MSRP (Section 6.1) are even more encouraging.
Domain adaptation	MSCOCO	 Table 5: Domain adaptation from MSCOCO to  Flickr30K dataset.
Domain adaptation	Flickr30K dataset	 Table 5: Domain adaptation from MSCOCO to  Flickr30K dataset.
Domain adaptation	TOEIC dataset	 Table 8: Domain adaptation to TOEIC dataset.
POS tagging	GUM	For each of the three tasks, POS tagging, dependency annotation, and coreference resolution, we first split the corpus into each of the four text types and collate responses from manual, automatic and gold annotation in GUM.
coreference resolution	GUM	For each of the three tasks, POS tagging, dependency annotation, and coreference resolution, we first split the corpus into each of the four text types and collate responses from manual, automatic and gold annotation in GUM.
POS tagging	UD framework	Therefore, our work also contributes to the discussions on POS tagging and segmentation of Turkish in the UD framework.
segmentation of Turkish	UD framework	Therefore, our work also contributes to the discussions on POS tagging and segmentation of Turkish in the UD framework.
MWE usage	1 Million English books corpus) published between 1520 and 2008	Specifically, we explored MWE usage using the 1 Million English subset of the dataset that was constructed from 1 Million English books corpus) published between 1520 and 2008 and originally contained 101.3 billion words.
extraction of synonym candidates	Europarl	For the extraction of synonym candidates, we use the German-English version of Europarl (1.5M parallel sentences) with GIZA++ word alignments for the extraction of synonym candidates.
detection of AD	Alzheimer's Association, 2015)	The first approach yields reliable results in the detection of AD in its moderate and advanced stages, albeit still performing insufficiently in the early stages of the disease (Alzheimer's Association, 2015).
morphological annotation of Old Hungarian texts	Humor analyzer	For the morphological annotation of Old Hungarian texts, the Humor analyzer was used, thus all of the morphologically annotated texts are in a special format, which is hard to be interpreted fora non-Hungarian researcher.
SMT	news test set from WMT 2015	The baseline SMT system is applied on a news test set from WMT 2015.
translation model	News14+parallel data	We used 4.592.139 parallel sentences aligned with GIZA++ for translation model training, and 45M sentences (News14+parallel data) to build a 5-gram language model.
translation	Jane toolkit	All translation experiments are performed with the Jane toolkit ().
translation	NTCIR-9	Tuning is performed on the NTCIR-7 dev sets, and translation is evaluated on the test set from NTCIR-9.
translation	WMT 2016 news translation data	English-German For translation into German, we built a machine translation system based on the WMT 2016 news translation data.
MT	WMT	Many different automatic MT quality metrics are available and the Metrics Shared Task 1 is held annually at WMT to assess their quality, starting with and following up to Stanojevi´c . http://www.statmt.org/wmt16/ metrics-task/ Metrics participating in the metrics task rely on the existence of reference translations with which MT outputs are compared, and the metrics task itself then needs manual judgments of translation quality in order to check the extent to which the automatic metrics can approximate the judgment.
MT	MOSES	For all our experiments, the MT systems are tuned using the kb-mira algorithm (Cherry and Foster, 2012) implemented in MOSES, including the reranking step.
SMT	WMT16	Similar bounds show that top constrained SMT results at WMT16 can be improved by 8 BLEU points on average while German to En-glish and Romanian to English translations results are already close to the bounds.
SMT submission	Meteor	 Table 2: BLEU, BLEU-Cased and Translation Er- ror Rate (TER) scores on newstest2016 of our  phrase-based SMT submission with and without  the use of n-best rescoring. The third line shows  the upper bound of our system with the n-best en- tries scored and sorted against the reference trans- lations using Meteor. The improvement in BLEU  for our n-best rescoring over the baseline MERT  is statistically significant with p ≤ 0.05.
2016 Conference on Machine Translation (WMT16) news-translation shared task	MITLL	As part of the 2016 Conference on Machine Translation (WMT16) news-translation shared task, the MITLL and AFRL human language techology teams participated in the Russian-English and English-Russian news translation tasks.
machine translation (MT)	IWSLT2015	Our machine translation (MT) systems represent improvements to both our systems from IWSLT2015 ( ) and WMT15 ( , the introduction of Neural Machine Translation rescoring, neural-net based recasing, unsupervised transliteration of out-of-vocabulary (OOV) words (, and an unique selection process for language modelling data.
machine translation (MT)	WMT15	Our machine translation (MT) systems represent improvements to both our systems from IWSLT2015 ( ) and WMT15 ( , the introduction of Neural Machine Translation rescoring, neural-net based recasing, unsupervised transliteration of out-of-vocabulary (OOV) words (, and an unique selection process for language modelling data.
MT Submission	newstest2016	 Table 7: Russian-English MT Submission Systems decoding newstest2016
machine translation from English into Romanian	HimL	In order to achieve high-quality machine translation from English into Romanian, members of the QT21 and HimL projects have jointly built a combined statistical machine translation system.
MT	Europarl	English-to-Finnish is a particularly challenging language pair for corpus-based MT because of the lack of in-domain parallel data (the only available parallel corpus in the shared task is Europarl) and the complex morphology of Finnish.
SMT	Omorfi +  BPE) on newstest2016	 Table 8: Results of the different reranking strate- gies applied to the best SMT system (Omorfi +  BPE) on newstest2016. The best score for each  metric is shown in bold, as is the system submit- ted. An arrow pointing upwards (↑) means that  the corresponding system outperforms the sys- tem without reranking by a statistically significant  margin.
SMT	develop- ment test data (newstest 2015)	 Table 3: Lower-cased BLEU scores for factored  SMT models for Finnish-to-English on develop- ment test data (newstest 2015).
SMT	Europarl	 Table 1. Each lan- guage pair was translated in both directions.  "BASE" in the tables represents the baseline  SMT system. "EXT" indicates results for the  baseline system, using the baseline settings but  extended with additional permissible data (lim- ited to parallel Europarl v7, Common Crawl,
SMT	Common Crawl	 Table 1. Each lan- guage pair was translated in both directions.  "BASE" in the tables represents the baseline  SMT system. "EXT" indicates results for the  baseline system, using the baseline settings but  extended with additional permissible data (lim- ited to parallel Europarl v7, Common Crawl,
Machine Translation (MT)	WMT	Machine Translation (MT) has been evolving in recent years achieving successful translations as shown by international evaluations such as WMT and increasing use of MT in commercial applications.
translation fluency	UPF-Cobalt	To this end, we develop a number of features representing translation fluency and integrate them with our reference-based metric UPF-Cobalt, which was originally presented at WMT15 (.
translation fluency	WMT15	To this end, we develop a number of features representing translation fluency and integrate them with our reference-based metric UPF-Cobalt, which was originally presented at WMT15 (.
Logistic Regression	WMT14 dataset	We use a standard implementation of Logistic Regression algorithm from the Python toolkit scikit-learn 5 . The model is trained on WMT14 dataset and tested on WMT15 dataset.
Logistic Regression	WMT15 dataset	We use a standard implementation of Logistic Regression algorithm from the Python toolkit scikit-learn 5 . The model is trained on WMT14 dataset and tested on WMT15 dataset.
Sentence-level	WMT15 dataset	 Table 2: Sentence-level evaluation results for WMT15 dataset in terms of Kendall rank correlation coef- ficient (τ )
cross-lingual pronoun prediction task	WMT 2016	In this paper we describe our system we designed and implemented for the cross-lingual pronoun prediction task as apart of WMT 2016.
Bilingual Document Alignment Task	WMT16	We apply cross-lingual Latent Semantic Indexing to the Bilingual Document Alignment Task at WMT16.
Bilingual document alignment	Kupiec1993	Bilingual document alignment has gained utmost importance these days, Kupiec1993,.
Bilingual Document Alignment	WMT 2016 Shared Task	Word Clustering Approach to Bilingual Document Alignment (WMT 2016 Shared Task)
Bilingual Document Alignment shared task	WMT16	Our participation in Bilingual Document Alignment shared task at WMT16 focuses on building a language-independent, scal-able system for aligning documents based on content as opposed to using webpage meta information.
MT	English-German evaluation set	According to the shared task results, our primary and con-trastive (which does not include the QE module) submissions have similar performance and achieved significant improvement of 3.31% TER and 4.25% BLEU (relative) over the baseline MT system on the English-German evaluation set.
APE	USAAR team	This paper presents anew approach to APE which was submitted by the USAAR team to the Automatic Post-editing (APE) shared task at WMT-2016.
MT	WMT-2016 APE task	The MT outputs are provided by WMT-2016 APE task (c.f) are considered as baseline system translation.
tweet normalization	Early Modern Finnish corpus of OCR processed newspaper text	In addition to experiments on tweet normalization, we present experiments on OCR post-processing using an Early Modern Finnish corpus of OCR processed newspaper text.
automatic speech recognition (ASR)	WFSTs	The hypothesis space for tasks like automatic speech recognition (ASR) and optical character recognition can be be represented as a compact, efficiently searchable cascade of WFSTs ().
tokenization	CMC data set	The two sub-tasks of tokenization and part-of-speech tagging were performed on two data sets: (i) a genuine CMC data set with samples from several CMC genres, and (ii) a Web corpora data set of CC-licensed Web pages which represents the type of data found in large corpora crawled from the Web.
tokenization	Web corpora data set of CC-licensed Web pages	The two sub-tasks of tokenization and part-of-speech tagging were performed on two data sets: (i) a genuine CMC data set with samples from several CMC genres, and (ii) a Web corpora data set of CC-licensed Web pages which represents the type of data found in large corpora crawled from the Web.
PoS tagging	CMC data subset	 Table 3: Agreement between annotators and gold  standard for PoS tagging of the CMC data subset  (training and test sets). Values are accuracy (acc)  percentages.
tokenization	Web corpora test  data	 Table 4: Agreement between annotators and gold  standard for tokenization of the Web corpora test  data. Values are F 1 scores given as percentages.
PoS tagging	Web corpora test  data	 Table 5: Agreement between annotators and gold  standard for PoS tagging of the Web corpora test  data. Values are accuracy (acc) percentages.
tokenization subtask	EmpiriST 2015 "podium" ranking	 Table 7: Results of the tokenization subtask including non-competitive submissions (marked with  *  ) and  baseline systems (marked with  † ). The last column gives the official EmpiriST 2015 "podium" ranking.  pr, rc, and F 1 are given as percentages for better readability.
PoS tagging subtask	EmpiriST 2015 "podium" ranking	 Table 8: Results of the PoS tagging subtask including non-competitive or late submissions (marked  with  *  ) and baseline systems (marked with  † ). If applicable, a subscript indicates the best run of the  respective system (based on overall accuracy), which is listed in the table. The last column gives the  official EmpiriST 2015 "podium" ranking. acc is given as a percentage for better readability.
tokenization	EmpiriST rankings	The results of our approaches for tokenization and PoS tagging are reported under the name LTL-UDE in the EmpiriST rankings.
PoS tagging	EmpiriST rankings	The results of our approaches for tokenization and PoS tagging are reported under the name LTL-UDE in the EmpiriST rankings.
Machine Transliteration Shared Task	Sixth Named Entities Workshop (NEWS 2016) held at ACL 2016in Berlin	This report presents the results from the Machine Transliteration Shared Task conducted as part of The Sixth Named Entities Workshop (NEWS 2016) held at ACL 2016in Berlin, Germany.
automated extraction of argument components from user content	German online participation project Tempelhofer Feld	This paper focuses on the automated extraction of argument components from user content in the German online participation project Tempelhofer Feld.
classification of positive and negative reviews of movies	Pang	showed that using pre-built word embeddings, induced from 100 billion words of Google News using word2vec, as inputs of a simple convolutional neural network (CNN) could achieve state-of-the-art performances on several sentence classification tasks, such as classification of positive and negative reviews of movies (Pang and) and consumer products, e.g. cameras ().
NER	JNLPBA corpus (PBA)	Given that the ultimate evaluation for word vectors is their performance in downstream applications, we also assess the quality of the vectors by performing NER using two well-established biomedical reference standards: the BioCreative II Gene Mention task corpus (BC2) ( and the JNLPBA corpus (PBA) ().
relation extraction between entities	MEDLINE R articles	proposed kernel methods for relation extraction between entities in MEDLINE R articles.
Bacteria Biotope task	BioNLP Shared Task 2016	This paper presents the Bacteria Biotope task of the BioNLP Shared Task 2016, which follows the previous 2013 and 2011 editions.
named entity recognition (NER) of organisms, tissues	BioNLP BB3	These competitions have resulted in the development of text-mining tools focusing on specific curation tasks (, one of which is the interactive EXTRACT tool that assists curators through automated named entity recognition (NER) of organisms, tissues, diseases and environments ( . The BioNLP BB3 focuses on the identification of bacteria and their habitats in text.
Bacteria Biotopes Task	BioNLP Shared Task 2016	These are the challenges set forth by the Bacteria Biotopes Task of the BioNLP Shared Task 2016.
Identification of Mentions and Relations between Bacteria and Biotope	PubMed Abstracts	Identification of Mentions and Relations between Bacteria and Biotope from PubMed Abstracts
relation extraction	NER	Similarly to the current BB3-event subtask, the 2013 subtask 2 concerned only relation extraction, and subtask 3 extended this with NER.
MTI	Histor- ical CheckTags	 Table 2: Performance of MTI with L2R on Histor- ical CheckTags.
BioASQ challenge semantic  indexing task	MLC  Micro-F Macro-F  Meta-Labeler  0.61936 0.57477  Vanilla SVMs  0.58422 0.50080  Tuned SVMs  0.61365 0.54444  Labeled LDA  0.47399 0.39084  Fast XML  0.38053 0.28899  HOMER-BR	 Table 1: Performance of the multi-label classifiers  used throughout the BioASQ challenge semantic  indexing task 4a, in terms of Micro-F and Macro- F. Training set size was 1,000,000 documents and  test set size 50,000 respectively.  MLC  Micro-F Macro-F  Meta-Labeler  0.61936 0.57477  Vanilla SVMs  0.58422 0.50080  Tuned SVMs  0.61365 0.54444  Labeled LDA  0.47399 0.39084  Fast XML  0.38053 0.28899  HOMER-BR (k=3) 0.59698 0.54972
TAG+ workshop	German Science Foundation (DFG) funded CRC 991	This TAG+ workshop is supported by the German Science Foundation (DFG) funded CRC 991 and DFG-funded project "BeyondCFG".
Argument linking	XMG	Argument linking in LTAG: A constraint-based implementation with XMG
translation task	WMT14 test data	To test our models on the translation task, we use WMT14 test data as corpus), considering submissions from all participating MT systems (including statistical, rule-based, hybrid) in the translation shared task for three language pairs, namely, 13 German-English (de-en) systems, 9 French-English (fr-en) systems, and 13 Russian-English (ru-en) systems.
translation of neuter English pronouns	DiscoMT 2015 baseline	In this paper, we present a simple yet effective approach to improve the translation of neuter English pronouns it and they into French, which outperforms the DiscoMT 2015 baseline by about 5% (relative improvement on an automatic metric).
SMT	WIT 3 corpus	We trained the Moses phrase-based SMT system () on the following parallel and monolingual datasets: aligned TED talks from the WIT 3 corpus (Cettolo et al., 2012), Europarl v.
SMT	Europarl	We trained the Moses phrase-based SMT system () on the following parallel and monolingual datasets: aligned TED talks from the WIT 3 corpus (Cettolo et al., 2012), Europarl v.
SMT	SETimes parallel corpus	Our SMT baseline was constructed by training a phrase-based Moses system on 200k segments from the SETimes parallel corpus, with an additional 2 thousand segments of development data, while we use hrWaC2.0 for building the language model.
MT evaluation	WMT15	We then derive a human score for each system with the TrueSkill method adapted to MT evaluation () following its usage at WMT15.
Summary	Corpus Source Texts	 Table 1: Summary Statistics for Corpus Source Texts
tokenisation convention adaptation	Wall-Street Journal text genre	Foster (2010) and noted that simple lexical and tokenisation convention adaptation to the Wall-Street Journal text genre could increase the parsing performance by a large margin.
statistical machine translation (SMT)	Modern Standard Arabic (MSA)	Almost all current state-of-the-art statistical machine translation (SMT) systems for Arabic-to-English translation are trained on data comprising Modern Standard Arabic (MSA).
PMS	Yellow Card Scheme	PMS is implemented in passive national reporting schemes such as the Yellow Card Scheme in the UK and MedWatch in the US; it is also implemented as active surveillance, e.g. organisations such as the MHRA in the UK and the FDA in the US conduct post-approval studies and postmarketing surveys.
PMS	MHRA	PMS is implemented in passive national reporting schemes such as the Yellow Card Scheme in the UK and MedWatch in the US; it is also implemented as active surveillance, e.g. organisations such as the MHRA in the UK and the FDA in the US conduct post-approval studies and postmarketing surveys.
segmentation of hashtags	WordNet	Our third contribution involves segmentation of hashtags and a semantic enrichment using a combination of relations from WordNet, which helps the performance of our classification system, including disambiguation of named entities, abbreviations and acronyms.
predicting geographical location	English Twitter text data	This shared task focuses on predicting geographical location (i.e., geotagging) using English Twitter text data.
SRL annotation	Penn Treebank	SRL annotation is typically based on syntactic structures obtained from treebanks, such as the constituent-based Penn Treebank (for PropBank annotation), or the German TIGER treebank for FrameNet-style annotation).
SRL annotation	German TIGER treebank	SRL annotation is typically based on syntactic structures obtained from treebanks, such as the constituent-based Penn Treebank (for PropBank annotation), or the German TIGER treebank for FrameNet-style annotation).
Semantic Indexing of Multilingual Corpora	History Domain	Semantic Indexing of Multilingual Corpora and its Application on the History Domain
Tracking Words in Chinese Poetry of Tang and Song Dynasties	China Biographical Database	Tracking Words in Chinese Poetry of Tang and Song Dynasties with the China Biographical Database
linguistic analysis	Pantera tagger	At the same time linguistic analysis is successfully tackled with state-of-the-art linguistic tools available for individual languages -see for the same example analysed with Pantera tagger available both as offline application and as Web service.
NER	Agerri	Many techniques have been developed in order to improve the NER results, such as the incorporation of additional information, in the form of lemmatization, POS tagging, dictionaries and ontologies, or the inclusion of knowledge acquired by unsupervised techniques like Brown clusters (, word2vec neural models (Agerri and or deep neural network architectures (dos) that yielded significant improvements.
Text Retrieval Conference (TREC)	Medical Reports Track	The Text Retrieval Conference (TREC), which addresses more diverse issues, also launched the Medical Reports Track.
MOO-based	AMOSA	Therefore, an alternative MOO-based approach is needed in order to verify whether we can improve the run-time complexity of AMOSA.
RNN-based concept recognition	i2b2 2010 corpus	 Table 6: Results for RNN-based concept recognition on the i2b2 2010 corpus, measured with precision  (P), recall (R), and F 1 -measure.
Text Retrieval Conference (TREC)	Medical Reports Track	The Text Retrieval Conference (TREC), which addresses more diverse issues, also launched the Medical Reports Track ().
Extracting ontological triples	Wordnet	Extracting ontological triples directly from the text needs many steps such as entity linking, disambiguation and predicate linking, and also needs many resources like Wordnet.
SRDF	Korean Wikipedia	The performance of SRDF system has been evaluated with randomly sampled sentences from featured article in Korean Wikipedia.
SP	FrameNet	In this paper, we propose SP approach based on the frame semantics in FrameNet () to interpret questions.
MT translations	Hugo and Yandex (HY) online MT	represents results from evaluating a combination of Google and Bing (BG) online MT translations (denoted with darker blue colours) and a combination of Hugo and Yandex (HY) online MT (brighter blue colours) on the general domain test dataset.
MT translations	general domain test dataset	represents results from evaluating a combination of Google and Bing (BG) online MT translations (denoted with darker blue colours) and a combination of Hugo and Yandex (HY) online MT (brighter blue colours) on the general domain test dataset.
MT	legal domain test dataset	Whereas shows results of combining the same MT on the legal domain test dataset.
predicting translation equivalents	WordNets	We present an algorithm for predicting translation equivalents between two languages, based on the corresponding WordNets.
translation dictionary	WordNets	To generate a translation dictionary, we need two linked WordNets.
translation	JPC-CJ	iterate the previous step 1000 times and count the number of wins (W ), losses (L) and ties (T )  The translation quality of JPC-CJ does not so much varied from the last year, but that of JPC-KJ is much worse.
translation	JPC-KJ	iterate the previous step 1000 times and count the number of wins (W ), losses (L) and ties (T )  The translation quality of JPC-CJ does not so much varied from the last year, but that of JPC-KJ is much worse.
SMT translations	NMT	We also use it to rerank the 1,000-best SMT translations on the basis of the average of the SMT score and that of the NMT rescoring of the translated sentences with technical term tokens.
SMT	NMT	We also use it to rerank the 1,000-best SMT translations on the basis of the average of the SMT score and that of the NMT rescoring of the translated sentences with technical term tokens.
SMT	NMT system of Section 4.3	When compared with the baseline SMT, the performance gain of the proposed system is approximately 3.1 BLEU points if translations are produced by the proposed NMT system of Section 4.3 or 2.3 RIBES points if translations are produced by the proposed NMT system of Section 4.2.
SMT	NMT system of Section 4.2	When compared with the baseline SMT, the performance gain of the proposed system is approximately 3.1 BLEU points if translations are produced by the proposed NMT system of Section 4.3 or 2.3 RIBES points if translations are produced by the proposed NMT system of Section 4.2.
translation	NRM	Consequently, we consider that there is little influence on the translation results, because the change in each label of reordering is small, although the reordering accuracy rate of the NRM and the proposed method differ by 1.67 points.
Translation	EHR group	Translation systems and experimental results of the EHR group for WAT2016 tasks
WAT2016	Japan Patent Office (JPO) Corpus	For WAT2016, the Japan Patent Office (JPO) Corpus can be regarded as multi-domain data because it includes chemistry, electricity, machine, and physics patents with their domain ID, and thus it is suitable for observing the effects of domain adaptation.
IITP English-Hindi Machine Translation	WAT 2016	IITP English-Hindi Machine Translation System at WAT 2016
translation task	WAT	This year English-Hindi language pair is adopted for translation task for the first time in WAT.
system combination	GMBR	For system combination, although we gained improvement by applying GMBR method 2 , the naive method of system combination based on the length of input sentence works better in the evaluation with test data.
MT	Hjerson (Popovi´cPopovi´c, 2011)	In order to better understand the overall evaluation scores and differences between the MT systems, five error classes, produced by the automatic error analysis tool Hjerson (Popovi´cPopovi´c, 2011), are reported.
SMT	Google translate 8 system	presents the automatic scores for standard test sets for all SMT systems together with the scores for translations by the publicly available Google translate 8 system.
TOCFL	TOCFL	For TOCFL, the training set is the union of the training sets in the NLP-TEA1, NLP-TEA2, and TOCFL in NLP-TEA3.
HSK	TOCFL	For HSK, the training set is the union of the one used for TOCFL and HSK in NLP-TEA3.
HSK	NLP-TEA3	For HSK, the training set is the union of the one used for TOCFL and HSK in NLP-TEA3.
POS tagging	Mainichi Newspaper	We use the Japanese morphological analyser MeCab to word segmentation and POS tagging of Mainichi Newspaper, BCCWJ, and the children's corpora.
POS tagging	BCCWJ	We use the Japanese morphological analyser MeCab to word segmentation and POS tagging of Mainichi Newspaper, BCCWJ, and the children's corpora.
negation	Spanish SFU Review Corpus (SFU Review SP -NEG)	In this work, we show the main problems found during the annotation of negation for the Spanish SFU Review Corpus (SFU Review SP -NEG).
diagnosis coding	International Classification of Diseases	Very few datasets have been released for the evaluation of diagnosis coding with the International Classification of Diseases, and only one so far in a language other than English.
ICD coding	ICD-10	A potential source of ICD coding datasets comes from death certificates, which are coded in countries around the world according to the World Health Organization (WHO) international standards, using ICD-10.
Detection of Text Reuse	French Medical Corpora	Detection of Text Reuse in French Medical Corpora
negation and speculation terms	gold standard	Section 3 presents the main contributions, by explaining the methods and the data sets used, by providing an analysis of length and types of negation and speculation terms and by describing the generation of our gold standard.
translation	Exp.1	 Table 5: The quality of translation of Exp.1
semantic relation classification task	CogALex 2016 shared task	Further, it was the best performing system in the semantic relation classification task of the CogALex 2016 shared task . We further assess the contribution of path-based information to semantic relation classification.
taxonomic relations	WN	The results show the importance of taxonomic relations, a fact well exploited by WN.
Discovering Potential Terminological Relationships	Twitter's Timed Content	Discovering Potential Terminological Relationships from Twitter's Timed Content
ambiguity resolution	UNL-jp dictionary	For the development of a dictionary aiming at ambiguity resolution for expressions including quantifiers and classifiers which maybe ambiguous with common nouns, we have annotated our corpus with UWs (interlin-gual lexemes) of UNL (Universal Networking Language) found on the UNL-jp dictionary.
Machine Translation (MT) evaluation	NIST	Recent Machine Translation (MT) evaluation tends to be conducted based on (1) Automatic evaluation metrics use reference translations for each segment such as BLEU, NIST, METEOR (;).
Parsing	Berkeley parser 6	Parsing was performed by the Berkeley parser 6 (.
MT	ASPEC-CJ corpus	For the Chinese-to-Japanese MT task, we trained a 5-gram language model for Japanese, on the training data of the ASPEC-CJ corpus using the KenLM toolkit 12 with interpolated Kneser-Ney discounting.
MT task	NTCIR-CE corpus	For the Chinese-to-English MT task, we trained a 5-gram language model for English, on the training data of the NTCIR-CE corpus using the same method.
sentiment analysis	CNN	 Table 7: Results of sentiment analysis with CNN and LSTM
sentiment analysis	LSTM	 Table 7: Results of sentiment analysis with CNN and LSTM
TPP	TPP	For TPP, this trend is likely due to the fact that all models incorporate party information and the issue of TPP is the most heavily divided within and across parties, with 8 Republicans and 4 Democrats in support of TPP and 8 Republicans and 12 Democrats opposed.
Querying	EURO 2016	Querying EURO 2016, although successfully expands the query with the relevant hashtag, retrieves noisy results.
Tweet Timeline Generation task	TREC 2014	We describe improvements to Bayesian inference that make the application of this model feasible, and present encouraging empirical results on the Tweet Timeline Generation task from TREC 2014 ().
OCR error detection and correction	French Clinical Texts	Low-resource OCR error detection and correction in French Clinical Texts
ICD-10 coding of death certificates	CLEF eHealth 2016 Task 2)	ICD-10 coding of death certificates has received renewed attention recently with the organization of the CLEF eHealth 2016 clinical information extraction task (CLEF eHealth 2016 Task 2).
CLEF eHealth 2016 clinical information extraction task	CLEF eHealth 2016 Task 2)	ICD-10 coding of death certificates has received renewed attention recently with the organization of the CLEF eHealth 2016 clinical information extraction task (CLEF eHealth 2016 Task 2).
SMT	RBMT	System architecture of SMT often complements RBMT, and the vice-versa.
SMT	RBMT	To the best of our knowledge, for such a domain, there is no work involving Indian languages.Below we describe SMT and RBMT very briefly.
Discourse classification	Hindi story corpus	 Table 5: Discourse classification results for the  Hindi story corpus using SVM.
bio-text mining evaluation	TREC Genomics track	Some of the very popular bio-text mining evaluation challenges include TREC Genomics track, JNLPBA 1 , LLL () and BioCreative.
bio-text mining evaluation	BioCreative	Some of the very popular bio-text mining evaluation challenges include TREC Genomics track, JNLPBA 1 , LLL () and BioCreative.
summarization	TAC 2011 conference	Our competitors consist of summarization methods that competed in the TAC 2011 conference (UBSummarizer and UoEssex), a widely used summarizer which is integrated with Microsoft-Word (autosummarize) and the recently proposed Association Mixture Text Summarization (AMTS) ().
identification	POS	The features are of course listed which will have a very good impact with the identification of POS.
translation	Hiero	The labelling scheme can be reduced to two adjunct/non-adjunct labels, and improves translation over Hiero by up to 0.6 BLEU points for English-Chinese.
Faking Intelligent CALL	Irish context	Faking Intelligent CALL: the Irish context and the road ahead
AMR-to-English generation	develop	 Table 3: Results for AMR-to-English generation on develop-
Collecting Reliable Human Judgements on Machine-Generated Language	QG-STEC Data	Collecting Reliable Human Judgements on Machine-Generated Language: The Case of the QG-STEC Data *
parsing	NDT	found the Mate dependency parser to have the best performance for parsing of NDT, and recent dependency parser comparisons () have also found Mate to perform very well for English.
NER tagging	FST	Our new evaluated tool for NER tagging is non-conventional: it is a rule-based Finnish Semantic Tagger, the FST (Löfberg et al., 2005), and its results are compared to those of a standard rule-based NE tagger, FiNER.
NER tagging	FiNER	Our new evaluated tool for NER tagging is non-conventional: it is a rule-based Finnish Semantic Tagger, the FST (Löfberg et al., 2005), and its results are compared to those of a standard rule-based NE tagger, FiNER.
NER evaluation	FiNER	Our present day Finnish evaluation collection is from a Finnish technology newspaper Digitoday 1 .  have reported NER evaluation results of the historical Finnish data with two tools, FiNER and ARPA.
FST	USAS Semantic Tagset	The FST assigns a semantic category to each word in text employing a comprehensive semantic category scheme (USAS Semantic Tagset, available in English 2 and also in Finnish 3 ).
speaker adaptation	GMM-HMM ASR	In this paper we introduce speaker adaptation for GMM-HMM ASR system which also reduces the model size.
OCR	ABBYY FineReader	The leading software frameworks for OCR are commercial ABBYY FineReader and two open source frameworks: Ocropy 2 (previously known as OCRopus) and Tesseract . experiment with these three and compare https://www.abbyy.com 2 https://github.com/tmbdev/ocropy 3 https://github.com/tesseract-ocr their performance on five pages of historical printings of Latin texts.
dependency parsing	Norwegian Bokmål	In this paper, we optimize a PoS tagset for the task of dependency parsing of Norwegian Bokmål.
Tagging	NDT	 Table 4: Tagging and parsing our development  section of NDT with the two initial tagsets. From  left to right, we report the tagger accuracy of the  most-frequent-tag baseline, the tagger accuracy of  TnT, and the labeled and unlabeled attachment  score for the Mate parser.
tagging	NDT  coarse tagset	 Table 12: Results of tagging and parsing with the  optimized tagset, compared to the original NDT  coarse tagset. The parser is both trained and tested  using automatically predicted tags from TnT.
parsing	NDT  coarse tagset	 Table 12: Results of tagging and parsing with the  optimized tagset, compared to the original NDT  coarse tagset. The parser is both trained and tested  using automatically predicted tags from TnT.
ML tagging	FrameNet	The data-driven approach of PropBank promises better coverage and statistical balance 1 , and therefore better automatic ML tagging, but its semantic role inventory and numbered arguments are highly predicate-dependent, and do not support semantic generalization and interpretation as well as FrameNet.
SMT	Meteor	There is a tendency that tuning in the same direction as the SMT system performs best, especially as measured by length ratio and Meteor.
Improving Optical Character Recognition	Finnish Historical Newspapers	Improving Optical Character Recognition of Finnish Historical Newspapers with a Combination of Fraktur & Antiqua Models and Image Preprocessing
POS tagging	MarMoT	For POS tagging, we use a state-of-theart CRF-based tagger MarMoT.
parsing	UD PoS tagset	They highlight that the parsing benefits from the disambiguation of PoS tags for main verbs and auxiliaries in UD PoS tagset even if the overall parsing accuracy decreases.
SMT	LemmData	The SMT system first reacts positively to the increase of the training data with LemmData.
SMT-based spelling normalisation	ARCHER corpus of historical English and American texts	In this paper, we compare the results of applying the SMT-based spelling normalisation approach to the ARCHER corpus of historical English and American texts, to the results achieved for VARD2 on the same corpus.
SMT-based spelling normalisation	VARD2	In this paper, we compare the results of applying the SMT-based spelling normalisation approach to the ARCHER corpus of historical English and American texts, to the results achieved for VARD2 on the same corpus.
SMT	VARD2	The comparison is interesting as the approaches are significantly different: SMT is a probablistic, languageindependent approach, whereas VARD2 combines lexicon-lookup with rules and non-probabilistic weights.
text reuse detection	OCR-recognized Finnish newspapers and journals from 1771	We present the results of text reuse detection , based on the corpus of scanned and OCR-recognized Finnish newspapers and journals from 1771 to 1910.
lexical identification task	NZE	The first lexical identification task was designed to train listeners to correctly identify the vowels of NZE.
SST	MASC* data set	 Table 2: Emotionality results for the SST* and the  MASC* data set.
SRL training	FrameNet	VerbNet provides larger semantic expressivity than PropBank, and we find that its generalization capacity approaches PropBank in SRL training, but it benefits less from training data expansion than the sparse-data affected FrameNet.
commonsense understanding	LSDSem 2017 shared task	As an effort to advance commonsense understanding, developed the story cloze task, which is the focus of the LSDSem 2017 shared task.
SCT	NCT	The SCT departs from the narrow focus of the NCT.
Sentence splitting	punkt	Sentence splitting was done using punkt().
clustering	English-language dataset	 Table 1: Results of clustering evaluation on the English-language dataset
LID classification	MIRA	We are the first to compare LID classification performance using the MIRA algorithm and langid.py.
Discriminating between Similar Languages (DSL)	VarDial Workshop series	Discriminating between Similar Languages (DSL) is a challenging task addressed at the VarDial Workshop series.
ASR	NCLR	Speaker Clustering using SOM has notably reduced the word error rate of ASR results over both clustering using NCLR and the baseline system (were no speaker clustering performed).
opinion mining	ASTD dataset	58.5% 53.6%: Performance of the different models for opinion mining, evaluated on the ASTD dataset.
opinion mining	ASTD dataset	 Table 5: Performance of the different models for  opinion mining, evaluated on the ASTD dataset.
ASR	ELRA	Our ASR system is built using several hours of standard Arabic news broadcasts from corpora distributed by ELRA.
parsing	NUDAR treebank	We conducted some experiments to benchmark the parsing scores in the NUDAR treebank.
parsing	NUDAR	For our parsing experiments, we used the MaltParser () to train an Arabic dependency parser in the space of both CATiB and NUDAR.
NER evaluations-covering Chinese	ACE Programme	The first non-English monolingual NER evaluations-covering Chinese, Japanese, Spanish, and Arabic-were carried out in the context of the Message Understanding Conferences (MUCs) and the ACE Programme ().
NE recognition	National Corpus of Polish	Prior work targeting NEs specifically for Slavic languages includes tools for NE recognition for Croatian (), a tool tailored for NE recognition in Croatian tweets (), a manually annotated NE corpus for Croatian, tools for NE recognition in Slovene (), a Czech corpus of 11,000 manually annotated NEs (, NER tools for Czech (, tools and resources for fine-grained annotation of NEs in the National Corpus of Polish () and a recent shared task on NE Recognition in Russian (.
NE recognition in Croatian tweets	National Corpus of Polish	Prior work targeting NEs specifically for Slavic languages includes tools for NE recognition for Croatian (), a tool tailored for NE recognition in Croatian tweets (), a manually annotated NE corpus for Croatian, tools for NE recognition in Slovene (), a Czech corpus of 11,000 manually annotated NEs (, NER tools for Czech (, tools and resources for fine-grained annotation of NEs in the National Corpus of Polish () and a recent shared task on NE Recognition in Russian (.
NE recognition	National Corpus of Polish	Prior work targeting NEs specifically for Slavic languages includes tools for NE recognition for Croatian (), a tool tailored for NE recognition in Croatian tweets (), a manually annotated NE corpus for Croatian, tools for NE recognition in Slovene (), a Czech corpus of 11,000 manually annotated NEs (, NER tools for Czech (, tools and resources for fine-grained annotation of NEs in the National Corpus of Polish () and a recent shared task on NE Recognition in Russian (.
coreference resolution	MUC	Many NLP researchers, especially those not working in the area of discourse processing, tend to equate coreference resolution with the sort of coreference that people did in MUC, ACE, and OntoNotes, having the impression that coreference is a well-worn task owing in part to the large number of papers reporting results on the MUC/ACE/OntoNotes corpora.
coreference resolution	MUC/ACE/OntoNotes corpora	Many NLP researchers, especially those not working in the area of discourse processing, tend to equate coreference resolution with the sort of coreference that people did in MUC, ACE, and OntoNotes, having the impression that coreference is a well-worn task owing in part to the large number of papers reporting results on the MUC/ACE/OntoNotes corpora.
information extraction (IE)	MUC-6 coreference in 1995	The decision to focus on entity coreference resolution was initially made by information extraction (IE) researchers when coreference was selected as one of the tasks in the MUC-6 coreference in 1995.
coreference resolution	EPEC corpus	In order to quantify the impact of using semantic knowledge sources in coreference resolution, we have tested the enriched coreference resolution system using the EPEC corpus and compared the results with the baseline system.
coreference resolution	EPEC corpus	In order to quantify the impact of using semantic knowledge sources in coreference resolution, we have tested the enriched coreference resolution system using the EPEC corpus and compared the results with the baseline system.
Mention detection	Polish Coreference Corpus	Mention detection is evaluated against the manual annotation of the Polish Coreference Corpus in two settings: taking into account only mention heads or exact borders.
MT	Google Translate 6 (GT)	able MT systems -Google Translate 6 (GT) and Moses 7 in the Czech to English setting.
MWEntity classification	NERC	In this paper we focus on MWEntity classification, thereby placing NERC in the context of the study of MWExpressions.
MWE Detection	PARSEME Shared Task	Parsing and MWE Detection: Fips at the PARSEME Shared Task
MWEs	Verb + Noun	This paper outlines investigation of MWEs of the class Verb + Noun in Italian.
Classifying literal and metaphorical  phrases	Saif M. Mohammad and Tur- ney (2016) dataset	 Table 2: AUC Score single features and com- binations. Classifying literal and metaphorical  phrases based on the Saif M. Mohammad and Tur- ney (2016) dataset.
PCZ induction	Gigaword	We experiment with two different corpora for PCZ induction as in, namely a 100 million sentence news corpus (news) from Gigaword (Parker et al., 2011) and LCC (), and a 35 million sentence Wikipedia corpus (wiki).
PCZ induction	LCC	We experiment with two different corpora for PCZ induction as in, namely a 100 million sentence news corpus (news) from Gigaword (Parker et al., 2011) and LCC (), and a 35 million sentence Wikipedia corpus (wiki).
Reinforcement Learning dialogue control	BURCHAK corpus	Finally, we train and evaluate a Reinforcement Learning dialogue control agent for learning visually grounded word meanings, trained from the BURCHAK corpus.
Tackling Biomedical Text Summarization	BioASQ 5B	Tackling Biomedical Text Summarization: OAQA at BioASQ 5B
Neural Question Answering	BioASQ 5B	Neural Question Answering at BioASQ 5B
NER	BB3 shared task datasets	As a result, we present a text mining pipeline which achieves state-of-theart results for the joint evaluation of NER and event extraction as well as for the categorization task using the official BB3 shared task datasets and evaluation tools.
Bacteria Biotope normalization task	BioNLP Shared Task 2016	We evaluate the performance of our normalization method on the Bacteria Biotope normalization task of the BioNLP Shared Task 2016.
normalization task	BioNLP-ST 2016	 Table 4: Results on the normalization task of  BioNLP-ST 2016
parsing a sentence	AMR	In the recent past, there have been several efforts towards parsing a sentence into its AMR (.
spelling correction	MIMIC-III test set	We greatly outperform two baseline off-the-shelf spelling correction tools on a manually annotated MIMIC-III test set, and counter the frequency bias of an optimized noisy channel model, showing that neural embeddings can be successfully exploited to include context-awareness in a spelling correction model.
relation extraction	SeeDev subtask	The recent 2016 relation extraction problems focused on two areas: bacteria biotopes (BB3 subtask) and seed development (SeeDev subtask).
relation extraction	Kindred	In order to overcome some of the challenges in the relation extraction community in terms of ease-of-use and integration, we present Kindred.
predication structure	CNN/Daily Mail dataset	To understand predication structure, it is helpful to review the anonymization performed in the CNN/Daily Mail dataset.
prediction	MemN2N	discusses prediction accuracy and attention accuracy with MemN2N and the modifications described in Section 3.1.
IRN prediction	FB15k	We provide some more IRN prediction examples at each step from FB15k as shown in Appendix A. In addition to the KBC tasks, we construct a synthetic task, shortest path synthesis, to evaluate the inference capability over a shared memory as shown in the Appendix B.
relation learning	FB15k	Most of these introduce novel architectures for relation learning that are evaluated on standard datasets such as FB15k and WN18.
relation learning	WN18	Most of these introduce novel architectures for relation learning that are evaluated on standard datasets such as FB15k and WN18.
information retrieval (IR)	Lucene	Extensive deployment of inverted-index-based information retrieval (IR) systems has led to the availability of robust open source IR systems such as Sphinx, Lucene or its popular, horizontally scalable extensions of Elasticsearch and Solr.
trimming	avg. diff.	However, trimming to as low as 6 values results in a significant increase in avg. diff.
prediction	DocTag2Vec	In terms of prediction, anew document will be first embedded using a Doc2Vec component inside the DocTag2Vec, and tags are then assigned by searching for the nearest tags embedded around the document.
relation prediction	FN	2) We transfer the relation prediction task from research on Knowledge Graph Completion to the case of FN and present our best-performing system for predicting F2F relations.
paraphrase detection	Microsoft Paraphrase Detection Corpus	The dataset we use for the paraphrase detection is the Microsoft Paraphrase Detection Corpus ().
character-level translation task	WMT'15	We show that our model outperforms strong baselines on character-level translation task from WMT'15 with less parameters and computes alignments that are qualitatively intuitive.
sentiment classification tasks	Cornell Sentence Polarity (CSP))	We also evaluate the method on sentiment classification tasks, using the Cornell Sentence Polarity (CSP)) and IMDb movie review datasets.
sentiment classification tasks	IMDb movie review datasets	We also evaluate the method on sentiment classification tasks, using the Cornell Sentence Polarity (CSP)) and IMDb movie review datasets.
classification	IMDb corpora	For classification, we perform both sentencelevel and document-level binary sentiment classification using logistic regression on the CSP and IMDb corpora respectively.
coreference resolution	Winograd Schema Challenge	These clues are particularly critical in cases in which selectional preference is not helpful, such as coreference resolution problems presented in Winograd Schema Challenge (.
predicting the dynamics of global armed conflicts	Gi-gaword news corpus	In this paper, we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year-to-year basis, using a dataset from the field of conflict research as the gold standard and the Gi-gaword news corpus as the training data.
slot-only mapping	MUC-4 templates	For slot-only mapping, we still restricted candidate slots to the four schemas that mapped to the MUC-4 templates.
paraphrase choice	Facebook data set	Building on previous work on demographic preferences, we quantify differences in paraphrase choice from a massive Facebook data set with posts from over 115,000 users.
recognition	International Social Survey Programme ISSP	The recognition, however, has been done just on study name level, in the Social Sciences typically a survey study, e.g. the International Social Survey Programme ISSP.
Surface Realization Shared Task (SR'11)	UD treebanks	Our objective is to setup a follow-up of the 2011 Surface Realization Shared Task (SR'11) at Generation Challenges; this time with an emphasis of multilingual surface generation from UD treebanks.
WMT	RNNLM	This model performs particularly strongly on WMT data and is complementary to an RNNLM: combining both yields large BLEU gains even for small beam sizes.
WMT task	German news2015 corpus	All monolingual models for the WMT task were trained on the German news2015 corpus (∼51.3M sentences).
RST	Deceptive Review (DeRev) corpus	A manual RST analysis was conducted on a forensic corpus (i.e. real review data with established ground-truth) of 25 known fake and 25 authentic Amazon book reviews drawn from the Deceptive Review (DeRev) corpus.
segmentation	PKU dataset	 Table 2: segmentation results on PKU dataset
segmentation	MSR dataset	 Table 3: segmentation results on MSR dataset
Tagging	WSJ test set	 Table 2: Tagging accuracy on the WSJ test set.
Existing Natural Language Generation (NLG)	SimpleNLG	Existing Natural Language Generation (NLG) approaches are usually applied to non morphological rich languages, such as English, where the morphological inflection of the word during the generation process can be addressed using simple handwritten rules or existing libraries such as SimpleNLG (.
SVM classifier	NYT Picks	They train an SVM classifier on a skewed dataset containing 94 NYT Picks and 5,174 nonpicks and achieve a cross-validation precision of 0.13 and recall of 0.60.
SumRepo	DUC2004 dataset	(2014) published SumRepo, a repository of summaries for the DUC2004 dataset generated by several baseline and state-of-the-art methods . We evaluate summaries generated by a selection of these methods on the same data that we use for testing.
ASR	Amharic corpus	In addition to ASR, a preliminary EnglishAmharic machine translation experiments was conducted using phonemic transcription on the Amharic corpus ().
Coreference resolution	CoNLL	Coreference resolution is an active NLP research area, with its own track at most NLP conferences and several shared tasks such as the CoNLL or SemEval shared tasks () or the CORBON shared task 2017 . Almost all work is based on text, although *The two first authors contributed equally to this work.
Coreference resolution	CORBON shared task 2017	Coreference resolution is an active NLP research area, with its own track at most NLP conferences and several shared tasks such as the CoNLL or SemEval shared tasks () or the CORBON shared task 2017 . Almost all work is based on text, although *The two first authors contributed equally to this work.
SMT	WordNet	We demonstrate that WSD systems can be adapted to help SMT, thanks to three key achievements: (1) we consider a larger context for WSD than SMT can afford to consider; (2) we adapt the number of senses per word to the ones observed in the training data using clustering-based WSD with K-means; and (3) we initialize sense-clustering with definitions or examples extracted from WordNet.
SMT	WordNet	We demonstrate that WSD systems can be adapted to help SMT, thanks to three key achievements: (1) we consider a larger context for WSD than SMT can afford to consider; (2) we adapt the number of senses per word to the ones observed in the training data using clustering-based WSD with K-means; and (3) we initialize sense-clustering with definitions or examples extracted from WordNet.
SMT	UN Corpus 7	We evaluate our sense-aware SMT on the UN Corpus 7) and on the Europarl Corpus 8 ().
SMT	Europarl Corpus 8	We evaluate our sense-aware SMT on the UN Corpus 7) and on the Europarl Corpus 8 ().
SMT	WSD	We thus focus on SMT in what follows, and leave WSD for NMT for future studies.
SMT	NMT	We thus focus on SMT in what follows, and leave WSD for NMT for future studies.
MT	WIT3 corpus	We select the optimal model configuration based on the MT performance, measured with the traditional BLEU score (), on the WIT3 corpus for EN/ZH and EN/DE.
MT	Europarl corpus	 Table 7: Detailed confusion matrix of our factored MT system and the baseline MT system with respect  to the reference on the EN/DE pair from Europarl corpus and the EN/ZH from UN corpus.
MT	UN corpus	 Table 7: Detailed confusion matrix of our factored MT system and the baseline MT system with respect  to the reference on the EN/DE pair from Europarl corpus and the EN/ZH from UN corpus.
MT	MRL	Our method for measuring the morphological competence of MT systems (detailed in § 2) is mainly based on the analysis of minimal pairs, each representing a contrast that is expressed syntactically in English and morphologically in the MRL.
phrase structure analysis	Hiero	We used English raw text without tokenization for phrase structure analysis and for training Hiero and T2S TMs on the pivot side.
Translation	WMT 2016 English→Romanian task	 Table 2: Translation results on the WMT 2016 English→Romanian task and the WMT 2017  German→English task.
Translation	WMT 2017  German→English task	 Table 2: Translation results on the WMT 2016 English→Romanian task and the WMT 2017  German→English task.
correcting English→German translations	Pharmaceutical domain	In this third round, the task focused on correcting English→German translations in the IT domain and German→English translations in the Pharmaceutical domain.
Translation	WMT15	 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16,  WMT17 EN-DE and WMT17 DE-EN data.
Translation	WMT16	 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16,  WMT17 EN-DE and WMT17 DE-EN data.
Translation	WMT17 DE-EN data	 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16,  WMT17 EN-DE and WMT17 DE-EN data.
Multimodal Translation task	English-German Ambiguous COCO dataset	 Table 5: Results for the Multimodal Translation task on the English-German Ambiguous COCO dataset.  Systems with grey background indicate use of resources that fall outside the constraints provided for the  shared task.
Multimodal Translation task	English-French Multi30K Test 2017 data	 Table 6: Results for the Multimodal Translation task on the English-French Multi30K Test 2017 data.
Multimodal Translation task	Ambiguous COCO dataset	 Table 7: Results for the Multimodal Translation task on the English-French Ambiguous COCO dataset.
Multilingual Image Description task	English-German Multi30K 2017 test  data	 Table 8: Results for the Multilingual Image Description task on the English-German Multi30K 2017 test  data.
validation	Appraise tool 15	The validation task was carried out using the Appraise tool 15.
news translations shared task	WMT 17	FBK's participation to the news translations shared task in WMT 17 focused this year on the English-German language direction.
JHU neural machine translation	Nematus () and Marian (Junczys-Dowmunt et al., 2016) toolkits	The JHU neural machine translation systems were built with the Nematus () and Marian (Junczys-Dowmunt et al., 2016) toolkits.
Machine Translation) news-translation shared task	MITLL	As part of the 2017 Conference on Machine Translation) news-translation shared task, the MITLL and AFRL human language technology teams participated in the Russian-English, English-Russian, Turkish-English and ChineseEnglish tasks.
SMT	NMT	When postedited MT was used as a reference, total TER/BLEU for the sample changed from 24.7/50.2 to 12.5/76.0 for SMT and from 48.4/25.0 to 18.3/70.5 for NMT.
SMT	NMT	While the score improved for both SMT and NMT, the improvement is clearly much larger for NMT.
SMT	NMT	While the score improved for both SMT and NMT, the improvement is clearly much larger for NMT.
SMT	NMT	With the second sample, total TER/BLEU changed from 58.9/22.1 to 42.5/39.3 for SMT and from 28.2/48.5 to 12.1/77.01 for NMT, so the result was even more favorable for NMT.
SMT	NMT	With the second sample, total TER/BLEU changed from 58.9/22.1 to 42.5/39.3 for SMT and from 28.2/48.5 to 12.1/77.01 for NMT, so the result was even more favorable for NMT.
news translation shared task	WMT conference	The year 2016 marked the first time when neural machine translation (NMT) systems achieved significantly better results than statistical machine translation (SMT) systems for most of the translation directions in the news translation shared task of the WMT conference.
NE enforcing	newstest2016 data	 Table 3:  Performance of NE enforcing on  newstest2016 data. The table shows how many  NEs were recognized, how many of those were  changed by our algorithm and how many of the  changes were positive, negative or neutral.
translating news	WMT 2016 and 2017 test sets	 Table 2: BLEU scores for translating news into English (WMT 2016 and 2017 test sets -WMT 2017 dev  set is used where there was no 2016 test)
translating news	WMT 2017 dev  set	 Table 2: BLEU scores for translating news into English (WMT 2016 and 2017 test sets -WMT 2017 dev  set is used where there was no 2016 test)
translating news out of English	WMT 2016 and 2017 test sets	 Table 3: BLEU scores for translating news out of English (WMT 2016 and 2017 test sets -WMT 2017  dev set is used where there was no 2016 test)
translating news out of English	WMT 2017  dev set	 Table 3: BLEU scores for translating news out of English (WMT 2016 and 2017 test sets -WMT 2017  dev set is used where there was no 2016 test)
news translation shared task	WMT17	We describe the JAIST phrase-based machine translation systems that participated in the news translation shared task of the WMT17.
MT community	WMT evaluation	First, we were able to focus on the pragmatical aspects of the teaching material; second, the study was comprehensive, since we covered all the components of a machine translation system; and third, student motivation was enhanced, because the results were directly available in the MT community through the WMT evaluation.
MMT	VGG-19 CNN	The winning submission for MMT) was a phrase-based MT system rescored using a language model enriched with FC 7 global visual features extracted from a pre-trained VGG-19 CNN).
MT	VGG-19 CNN	The winning submission for MMT) was a phrase-based MT system rescored using a language model enriched with FC 7 global visual features extracted from a pre-trained VGG-19 CNN).
MT	HimL	The underlying texts and MT systems come from two other WMT tasks, namely News Translation Task (, denoted as Findings 2017 in the following) and Neural MT training task, and from the EU project HimL, aiming at translation of healthrelated documents.
WMT17 News Translation Task	WMT	We designed training task so that it was in fact subsumed by the WMT17 News Translation Task (: the training data was a subset of the training data provided for English-to-Czech news task participants and the testset we used the official newstest2017 of WMT.
translation task	WMT evaluation	To train the word embedding and the RNNLM, the source side and the target side of the bilingual parallel corpus for the translation task, publicly released by the WMT evaluation campaign, are used; they include Europarl v7, Common Crawl corpus, News Commentary v8 and v11; Batch1 and Batch2, localization PO files, IT-related terms from Wikipedia 3 ; WMT16 and WMT17 QE task1 corpus.
translation task	Europarl	To train the word embedding and the RNNLM, the source side and the target side of the bilingual parallel corpus for the translation task, publicly released by the WMT evaluation campaign, are used; they include Europarl v7, Common Crawl corpus, News Commentary v8 and v11; Batch1 and Batch2, localization PO files, IT-related terms from Wikipedia 3 ; WMT16 and WMT17 QE task1 corpus.
translation task	Common Crawl corpus	To train the word embedding and the RNNLM, the source side and the target side of the bilingual parallel corpus for the translation task, publicly released by the WMT evaluation campaign, are used; they include Europarl v7, Common Crawl corpus, News Commentary v8 and v11; Batch1 and Batch2, localization PO files, IT-related terms from Wikipedia 3 ; WMT16 and WMT17 QE task1 corpus.
translation task	WMT16	To train the word embedding and the RNNLM, the source side and the target side of the bilingual parallel corpus for the translation task, publicly released by the WMT evaluation campaign, are used; they include Europarl v7, Common Crawl corpus, News Commentary v8 and v11; Batch1 and Batch2, localization PO files, IT-related terms from Wikipedia 3 ; WMT16 and WMT17 QE task1 corpus.
translation task	WMT17 QE task1 corpus	To train the word embedding and the RNNLM, the source side and the target side of the bilingual parallel corpus for the translation task, publicly released by the WMT evaluation campaign, are used; they include Europarl v7, Common Crawl corpus, News Commentary v8 and v11; Batch1 and Batch2, localization PO files, IT-related terms from Wikipedia 3 ; WMT16 and WMT17 QE task1 corpus.
Emotion Intensity Analysis	WMT 2017 conference	These include Quality Estimation and Emotion Intensity Analysis, which are the subjects of shared tasks held at the WMT 2017 conference and WASSA 2017 workshop 2, respectively.
Emotion Intensity Analysis	WASSA 2017 workshop 2	These include Quality Estimation and Emotion Intensity Analysis, which are the subjects of shared tasks held at the WMT 2017 conference and WASSA 2017 workshop 2, respectively.
translation quality	WMT 2016 metrics task	Two other kinds of human judgments of translation quality were collected in the WMT 2016 metrics task.
WMT'2017 news translation shared task	News Crawl corpora	data from the WMT'2017 news translation shared task: we took a random 50 million sentences from the News Crawl corpora for each language (except Chinese, where we used a portion of Common Crawl).
MT	APE system	In APE-QE, the original MT hypothesis is then aligned with the pseudo-postedit from the APE system using word level edit-distance, and words which correspond to Insert or Delete operations are labeled as incorrect.
machine translation outputs	WMT17 APE en-de task	We find that only a small number of edit operations are required for most machine translation outputs, through analysis of the training set of WMT17 APE en-de task.
APE	JXNU team	This paper presents anew approach for APE which was submitted by the JXNU team to WMT17 APE shared task.
SMT	News-Commentary corpus 5	With Moses decoder (, we train a phrase-based SMT model on another different version of News-Commentary corpus 5 provided respectively by OPUS (69,206 sentence pairs) and WMT2017 Shared Task 6 (235,724 pairs), and the model is tuned by MERT) with the development sets (2002 pairs) provided by WMT2017.
SMT	OPUS	With Moses decoder (, we train a phrase-based SMT model on another different version of News-Commentary corpus 5 provided respectively by OPUS (69,206 sentence pairs) and WMT2017 Shared Task 6 (235,724 pairs), and the model is tuned by MERT) with the development sets (2002 pairs) provided by WMT2017.
SMT	WMT2017 Shared Task 6	With Moses decoder (, we train a phrase-based SMT model on another different version of News-Commentary corpus 5 provided respectively by OPUS (69,206 sentence pairs) and WMT2017 Shared Task 6 (235,724 pairs), and the model is tuned by MERT) with the development sets (2002 pairs) provided by WMT2017.
SMT	WMT2017	With Moses decoder (, we train a phrase-based SMT model on another different version of News-Commentary corpus 5 provided respectively by OPUS (69,206 sentence pairs) and WMT2017 Shared Task 6 (235,724 pairs), and the model is tuned by MERT) with the development sets (2002 pairs) provided by WMT2017.
Error detection	FCE dataset	 Table 2: Error detection results on the FCE dataset using different auxiliary loss functions.
Error detection	CoNLL-14 test set	 Table 3: Error detection results on the CoNLL-14 test set using different auxiliary loss functions.
Humor recognition	TED  data sets	 Table 1: Humor recognition on both Pun and TED  data sets by using (a) random prediction (Chance),  conventional method (Base) and CNN method
GEC	NUCLE	The most frequently used corpus for GEC is NUCLE, which was the official dataset of the 2013 and 2014 shared tasks.
parsing	Penn Treebank	For well over a decade, the field was heavily focused on improving parsing accuracy on the Penn Treebank), but robustness was greatly improved with the advent of Ontonotes () and the Google Web Treebank (Petrov and: Metric scores of three artificially contrived systems (Game), input source sentences (Src), and top 3 system outputs (Sys) on CoNLL14 data.
parsing	Google Web Treebank	For well over a decade, the field was heavily focused on improving parsing accuracy on the Penn Treebank), but robustness was greatly improved with the advent of Ontonotes () and the Google Web Treebank (Petrov and: Metric scores of three artificially contrived systems (Game), input source sentences (Src), and top 3 system outputs (Sys) on CoNLL14 data.
parsing	CoNLL14 data	For well over a decade, the field was heavily focused on improving parsing accuracy on the Penn Treebank), but robustness was greatly improved with the advent of Ontonotes () and the Google Web Treebank (Petrov and: Metric scores of three artificially contrived systems (Game), input source sentences (Src), and top 3 system outputs (Sys) on CoNLL14 data.
SMT pipeline	GEC	These approaches make use of a great deal of resources, and in this work we propose a lighter-weight approach to GEC by methodically examining different aspects of the SMT pipeline, identifying and applying modifications tailored for GEC, introducing artificial data, and evaluating how each of these specializations contributes to the overall performance.
segmentation	StanfordParser	All data is preprocessed using a DKPro pipeline) consisting of segmentation, POS-tagging (both OpenNLP 2 ), lemmatization using the MateLemmatizer () and parsing using the StanfordParser (  This section presents our experimental results.
WASSA shared task	CodaLab competition website	Even though the 2017 WASSA shared task has concluded, the CodaLab competition website is kept open.
IMS	Test data	 Table 7: Overview IMS full and partial System  performance on Test data.
aspect extraction	SemEval ABSA corpus	Our model provides state-of-the-art performance on aspect extraction without requiring the usage of hand-crafted features on the SemEval ABSA corpus, while it outperforms the baseline on the joint task.
parsing	AESC	Motivated by the success of attention-based approaches in multiple NLP problems such as machine translation (), parsing (, slot-filling () and others (, we also introduce an attention-augmented RNN model for AE and AESC.
validation	Laptops dataset	We used exponential decay of ratio 0.9 and early stopping on the validation when there was no improvement in the F1-score after 1000 training steps.: Best results for our ARNN on AE for the Laptops dataset.
ATE	SemEval-2014 ABSA task datasets	We validate its suitability for ATE by achieving top-ranking results for supervised ATE using the SemEval-2014 ABSA task datasets . Then, we use it for unsupervised ATE.
Emotion Intensity	EmoInt-2017)	This paper describes the system that we submitted as part of our participation in the shared task on Emotion Intensity (EmoInt-2017).
sentiment analysis	WASSA-2017	Unlike most shared tasks on sentiment analysis, the EmoInt Shared Task at WASSA-2017) focused on sentiment intensity rather than classification.
SNLI	SNLI training set	For the SNLI test results in, we train on only the SNLI training set (and we also verify that the tuning decisions hold true on the SNLI dev set).
SNLI	SNLI dev set	For the SNLI test results in, we train on only the SNLI training set (and we also verify that the tuning decisions hold true on the SNLI dev set).
sentiment analysis	BLGNLP2017 workshop	The current paper covers several strategies we used to 'break' predictions of sentiment analysis systems participating in the BLGNLP2017 workshop.
ASR	Dragon Client SDK 12.5	A full dictation language model was used for ASR (provided via Dragon Client SDK 12.5) to realize the free-form entering of the appointment.
Discourse Parsing	Sentiment Treebank dataset	Discourse Parsing is such a critical task in NLP because previous work has shown that information: The Sentiment annotation (over Discourse Tree structure) of a sentence from Sentiment Treebank dataset contained in the resulting Discourse Tree can benefit many other NLP tasks including but not restricted to automatic summarization (e.g.,,,), machine translation (e.g.,,) and question answering (e.g.,).
RL	CDR baseline policy	One of our main contributions is that we provide a detailed comparison of the RL policy and the CDR baseline policy, including information about how much effort and time it took to develop each one of them.
ASR	NLU	For every ASR partial we obtain the highest assigned confidence score from the NLU, use the time consumed feature from the game, and obtain the action from the policy.
MT	QE/APE datasets	While the tuples of first two elements, i.e., source text and human translation, compose ordinary parallel corpus used to train (data-driven) MT systems, the remaining three are specific to this kind of QE/APE datasets.
MT	QE/APE datasets	grade: Quality grade of MT output B (∈ {S, A, B, C, D}) pe: Manually post-edited MT output Can I just buy a one way ticket?: Example record in our QE/APE datasets (see Section 2.4 for the definition of grade).
SMT	JPC-EJ/CJ	 Table 5: Errors of SMT and NMT for JPC-EJ/CJ
translation task	ASPEC dataset	This translation task contains several subtasks, but we focused on the ASPEC dataset, for the Japanese-English and Japanese-Chinese language pairs.
tokenization	ASPEC dataset	 Table 1: Comparison of various tokenization  methods measured on the ASPEC dataset.
beam search	NMT	We implemented beam search and ensemble decoding in the NMT system.
NE recognition	GENIA corpus	For example, the biomedical NE recognition system that trained with GENIA corpus (, only cover the gene and protein subdomain.
Engineering Communication I	NTU	To develop the system, we have tagged an LC of 180 written assignments fora course entitled Engineering Communication I, taught at NTU.
Scoring	ASAP	 Table 6: Scoring performance on ASAP with and  without spell checking
word segmentation domain adaptation	Chinese Treebank (CTB)	Following previous Chinese word segmentation domain adaptation methods, we employ the Chinese Treebank (CTB) () as the source domain data.
TAG	MICA	Recent efficient parsers for TAG, such as MICA ( and Alto (, do not support feature structures.
parsing	FTAG	In this paper, we offer a simple and modular approach to efficient parsing with FTAG.
parsing	Penn Treebank	Hence, parsing with weighted TAGs has received too little attention even though Chiang (2000) experimentally demonstrated their usefulness in the Penn Treebank parsing task.
POS taggers	UD treebanks	When the data for training POS taggers is small -as for the small languages scenario as well as for many upcoming UD treebanks 3 -the delexicalized methods might be affected by poor POS accuracy.
POS taggers	MarMoT	Tools To train POS taggers, we employ MarMoT ().
parsing argument scramblings	newswire treebanks	However, simple sampling techniques such as minority oversampling may not be a feasible solution for parsing argument scramblings which are almost non-existent in the newswire treebanks (see).
VDCs	Jinxi	Until now, research efforts concerning VDCs have centered on the origin and lexical properties of de (e.g., Jinxi, categorization and typology (e.g.,, and semantic, syntactic, and pragmatic properties of the construction (e.g.,.
dependency syntax	Tesnière (1959/2015: Book D)	Valency has, of course, been a central subtheory of dependency syntax since Tesnière (1959/2015: Book D).
parsing	Korean UD corpus	We report parsing results using the new dependency corpus for Korean and compare them with the previous Korean UD corpus.
UD conversion	Bosque (Linguateca version 7.5 of March 2016)	For that, between September 2015 and March 2016, a set of UD conversion rules for the CG input was written, as described in, and applied to the updated version of the dependency-style Bosque (Linguateca version 7.5 of March 2016).
sentiment analysis	EVALITA	Nevertheless, the annotation and exploitation of Twitter corpora are currently mainly referred to sentiment analysis and opinion mining or other semantic-oriented forms of processing, see e.g. tasks in SemEval 2017 and EVALITA (.
dependency parser training	COP POS tag	In the experiment, we applied the Bohnet parser this time for POS tagger, morphology tagger, and dependency parser training and evaluation, using tenfold cross validation on the original content head treebank and the new version with the COP POS tag.
Derivational Morphology	Brazilian Institute of Geography and Statistics (IBGE) website	As a theoretical basis, the concepts of Derivational Morphology are considered, and, concerning the methodology, we used data about cities and demonyms provided by the Brazilian Institute of Geography and Statistics (IBGE) website, for which we produced procedures to make this data tractable.
Sentiment Analysis	Election Brazilian News	A Comparative Study for Sentiment Analysis on Election Brazilian News
STS	MSRVID	 Table 3: STS performance on the MSRVID and NEWS-16 datasets (Pearson ρ).
STS	NEWS-16 datasets	 Table 3: STS performance on the MSRVID and NEWS-16 datasets (Pearson ρ).
semantic parsing	AMRs	A constrained graph algebra for semantic parsing with AMRs
AM algebra derivations	AMRBank	We explain how to obtain AM algebra derivations for graphs in the AMRBank in Section 5.
Annotation of temporal information	TimeML ISO standard	Annotation of temporal information as a whole has made steady progress in recent years following the TimeML ISO standard) and going beyond it.
ALM formalization	MAS discourse	The section concludes with the ALM formalization of the MAS discourse.
WSD evaluation	WordNet sense baseline	However, as is customary in WSD evaluation, we do compare our system to the most frequent WordNet sense baseline, which is notoriously difficult to beat due to the highly skewed distribution of word senses.
semantic tagging of corpora	WordNet	At this stage of the process, our taxonomies can be already used for several NLP tasks such as semantic tagging of corpora, population of other taxonomies such as WordNet or applications in terminology.
coreference annotation	Democrat corpus	We first give a rough description of the existing formats for coreference-annotated corpora, then goon to look at the tools available in the TEI for coreference annotation, and finally describe how we plan to implement coreference annotation in the Democrat corpus and in a TEI-compliant version of the ANCOR corpus.
coreference annotation	ANCOR corpus	We first give a rough description of the existing formats for coreference-annotated corpora, then goon to look at the tools available in the TEI for coreference annotation, and finally describe how we plan to implement coreference annotation in the Democrat corpus and in a TEI-compliant version of the ANCOR corpus.
Temporal annotation	ISO TC37/SC4	Temporal annotation has benefitted from the normalization efforts of the ISO TC37/SC4 committee which has led to the definition of the ISO-TimeML standard, following the seminal proposal of.
normalization	ISO TC37/SC4	Temporal annotation has benefitted from the normalization efforts of the ISO TC37/SC4 committee which has led to the definition of the ISO-TimeML standard, following the seminal proposal of.
SMT	NMT	 Table 3: Results of SMT and NMT on the  ILCI test set
SMT	ILCI test set	 Table 3: Results of SMT and NMT on the  ILCI test set
novelty detection	RTE-6	The novelty detection subtask was organized in conjunction with the main tasks of RTE-6 and RTE-7 (Bentivogli, 2011) tracks.
novelty detection	RTE-7 (Bentivogli, 2011)	The novelty detection subtask was organized in conjunction with the main tasks of RTE-6 and RTE-7 (Bentivogli, 2011) tracks.
normalizing noisy text that appear on social media platforms	Whatsapp	This paper sets out to investigate ways of normalizing noisy text that appear on social media platforms like Facebook, Twitter, Whatsapp, etc.
SVM Classification	DMOZ dataset	 Table 5: SVM Classification on DMOZ dataset
SVM Classification	WebKB dataset	 Table 8: SVM Classification on WebKB dataset
ML	GENIA corpus	Yang et al., (2004) developed a supervised ML approach for anaphora resolution and evaluated it on a portion of the GENIA corpus.
anaphora resolution	GENIA corpus	Yang et al., (2004) developed a supervised ML approach for anaphora resolution and evaluated it on a portion of the GENIA corpus.
Semantic Enrichment	Czech Bibliographic Databases	Semantic Enrichment Across Language: A Case Study of Czech Bibliographic Databases
POS tagging	UDPipe	We also report preliminary POS tagging and dependency parsing results using the treebank data and UDPipe (.
tagging	UDPipe	 Table 2: Preliminary tagging and parsing results with UDPipe.
MWE	English UD v2	 Table 2. Measures for MWE of the English UD v2
dependency parsing	CONLL-U Shared Task for 2017	The tool was developed in order to perform dependency parsing on considerably low-resource languages, and the work was originally carried out within the CONLL-U Shared Task for 2017.
dependency parsing	HFST	Here the Finnish language is a very interesting challenge for dependency parsing since the word form disambiguation (e.g. lasta/lapsi) will be made in the R statistical programming environment, after the possible lemmas are parsed with HFST, but before estimation of the dependency graph with UDPIPE which will benefit from the lemma disambiguation.
dependency parsing	UDPIPE	Here the Finnish language is a very interesting challenge for dependency parsing since the word form disambiguation (e.g. lasta/lapsi) will be made in the R statistical programming environment, after the possible lemmas are parsed with HFST, but before estimation of the dependency graph with UDPIPE which will benefit from the lemma disambiguation.
FST processing	Pite Saami	⁴This common ELAN structure can be found at https://github.com/langdoc/FRechdoc/wiki/ELAN-tiers. that occur during FST processing; however, as this project is still at an early stage in describing syntactic structures in Pite Saami, the constraint grammar implementation for Pite Saami is also only in an initial stage.
Statistical learning theory	OT's strict dominationÉmile	Statistical learning theory and linguistic typology: a learnability perspective on OT's strict dominationÉmile
MWEs	FWE	 Table 4: Significant clusters for MWEs after FWE voxel cor-
Complex Word Identification (CWI) shared task	NAACL-HLT'2018	We report the findings of the second Complex Word Identification (CWI) shared task organized as part of the BEA workshop co-located with NAACL-HLT'2018.
Complex Word Identification (CWI) shared task	NAACL-HLT'2018	In this paper we present the findings of the second Complex Word Identification (CWI) shared task organized as part of the thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA) co-located with NAACL-HLT'2018.
AES	CEFR scale	We study AES for multiple languages for the first time using CEFR scale.
predicting a student's mistake	Duolingo application	SLAM 2018 focuses on predicting a student's mistake while using the Duolingo application.
SLAM	Duolingo	The 2018 Shared Task on SLAM provides student trace data from users on the online educational platform Duolingo (.
SLA modeling-the	Duolingo challenge datasets	It remains an open question whether the existing knowledge tracing techniques can be directly applied to SLA modeling-the release of the Duolingo challenge datasets now enables us to investigate this very question.
SIB	UMLS	In terms of ontology relations, SIB includes only candidates that are considered to be the siblings of the correct answer (according to UMLS).
Predicting Psychological Health from Childhood Essays	CLPsych 2018 Shared Task (Team UKNLP)	Predicting Psychological Health from Childhood Essays with Convolutional Neural Networks for the CLPsych 2018 Shared Task (Team UKNLP)
Predicting Psychological Health	UGent-IDLab CLPsych 2018 Shared Task System	Predicting Psychological Health from Childhood Essays The UGent-IDLab CLPsych 2018 Shared Task System
Bridging Resolution	ARRAU	Rule-and Learning-based Methods for Bridging Resolution in the ARRAU Corpus
bridging resolution	ARRAU corpus	We present two systems for bridging resolution , which we submitted to the CRAC shared task on bridging anaphora resolution in the ARRAU corpus (track 2): a rule-based approach following Hou et al.
bridging resolution	ARRAU corpus	The first shared task on bridging resolution, co-located with the workshop on computational models of reference, anaphora and coreference (CRAC), deals with the task of bridging anaphora resolution in the RST domain of the ARRAU corpus (.
coreference resolution	OntoNotes corpus	Data We based our experiments on the benchmark dataset for coreference resolution, the OntoNotes corpus).
semantic relation prediction	OntoNotes corpus	In order to obtain candidate pairs for semantic relation prediction, we considered all heads of noun phrases in the OntoNotes corpus) and combined them with preceding heads of noun phrases in the same document.
metaphor identification	VU Amsterdam Metaphor Corpus	We report on the shared task on metaphor identification on the VU Amsterdam Metaphor Corpus conducted at the NAACL 2018 Workshop on Figurative Language Processing .
metaphor detection	Vrije University Amsterdam Metaphor Corpus (VUA)	This article describes the system that participated in the shared task (ST) on metaphor detection (Leong et al., 2018) on the Vrije University Amsterdam Metaphor Corpus (VUA).
WEs	Vienna-Oxford International Corpus of English (VOICE)	The WEs were trained (cf. Section 4.2) on different large corpora (BNC, Wikipedia, enTenTen13, ukWaC) and on the Vienna-Oxford International Corpus of English (VOICE) as well as on the TOEFL11 Corpus of Non-Native English.
WEs	TOEFL11 Corpus of Non-Native English	The WEs were trained (cf. Section 4.2) on different large corpora (BNC, Wikipedia, enTenTen13, ukWaC) and on the Vienna-Oxford International Corpus of English (VOICE) as well as on the TOEFL11 Corpus of Non-Native English.
personality prediction	MBTI	Most work on personality prediction rests on one of the two widely used personality models: Big Five and MBTI.
parsing	GKR	Section 4 evaluates the current parsing into GKR, while section 5 discusses our future additions to the system.
translation studies	LCMC	ZCTC was created specifically for translation studies "as a comparable counterpart of translated Chinese" to LCMC, with the same genre distribution and also one million words in total.
Word sense induction (WSI)	WordNet	Word sense induction (WSI) is a challenging task of natural language processing whose goal is to categorize and identify multiple senses of polysemous words from raw text without the help of predefined sense inventory like WordNet.
APE shared task	WMT	Three rounds of the APE shared task at WMT (2015-2017) followed a similar trend, with improvements that reflect a steady progress of the underlying technology developed by participants.
translation quality	High Register test set	To perform a more detailed analysis on translation quality and register appropriateness, we randomly selected 90 Mandarin sentences for human evaluation; 40% of these were drawn from the High Register test set, and 60% from the Low Register test set.
translation quality	Low Register test set	To perform a more detailed analysis on translation quality and register appropriateness, we randomly selected 90 Mandarin sentences for human evaluation; 40% of these were drawn from the High Register test set, and 60% from the Low Register test set.
caption translation	MS COCO	While caption translation in previous work has been conducted solely on clean, manually labeled captions based on MS COCO (),, or its multilingual variant, the goal of our work is to lift multimodal caption translation to a more realistic setup.
machine translation (MT)	CEF eTranslation service	Additionally to the custom NMT systems, the EU Council Presidency Translator provides access to all of the machine translation (MT) systems from the CEF eTranslation service, for translation between the 24 official languages of the EU and English.
MT	eBay	MT for L10n: How we build and evaluate MT systems at eBay
Translation Quality Estimation	AMTA 2018	Are we experiencing the Golden Age of Automatic Post-Editing? Translation Quality Estimation and Automatic Post-Editing AMTA 2018
NMT	Irish	We then present a preliminary NMT baseline, based on the same training and test data as previous SMT experiments, in order to investigate its strengths and weaknesses with respect to Irish.
SMT	Irish	We then present a preliminary NMT baseline, based on the same training and test data as previous SMT experiments, in order to investigate its strengths and weaknesses with respect to Irish.
translate from English to French	NVIDIA GeForce GTX 1080 Ti	In such away, we manage to build a system which learns to translate from English to French in less than 8 hours of training on NVIDIA GeForce GTX 1080 Ti by using only 20k parallel sentences and 300k monolingual corpora.
VAs	International Classification of Diseases (ICD-10) code	Typically, VAs are collected by non-medical surveyors who record the information on a form that is later reviewed by physicians who assign the record an International Classification of Diseases (ICD-10) code.
EBM	PubMed	In practice, successful EBM applications rely on answering clinical questions via analysis of large medical literature databases such as PubMed.
parsing	GENIA+PubMed	For parsing the questions, we used BLLIP reranking parser) (Charniak-Johnson parser) and used the model GENIA+PubMed for biomedical text.
Navigational queries	Katanaev AND(1):111-22	Navigational queries, also called known-item queries, such as Katanaev AND(1):111-22, are intended to retrieve a specific publication.
PMC articles	PubMed and PMC score	 Table 3. The value of full text PMC articles  in the retrieval performance. In combined  retrieval, we assign each article the maxi- mum of its PubMed and PMC score and  evaluate based on that maximum.
NER	CRF	In this paper, we present a corpus for NER in Hindi-English Code-Mixed along with extensive experiments on our machine learning models which achieved the best f1-score of 0.95 with both CRF and LSTM.
MRC	DuReader	 Table 6: Performance of typical MRC systems on the DuReader.
MRC	Stanford Question Answering Dataset (SQuAD)	One of the first large MRC datasets (over 100k QA pairs) is the Stanford Question Answering Dataset (SQuAD) ().
summarization	CNN-Dailymail	Our study compares summarization with fixed value control variables on full text CNN-Dailymail with (See et al., 2017).
hyperparameter optimization	WMT data	For hyperparameter optimization, we did a complete grid search over a span of learning rates (0.1, 0.01, 0.001, 0.0001, 0.00001), train epochs, and optimizers (Adam, SGD) on WMT data and a partial search on EMEA data.
hyperparameter optimization	EMEA data	For hyperparameter optimization, we did a complete grid search over a span of learning rates (0.1, 0.01, 0.001, 0.0001, 0.00001), train epochs, and optimizers (Adam, SGD) on WMT data and a partial search on EMEA data.
Pronoun Resolution	fMRI	The Role of Syntax during Pronoun Resolution: Evidence from fMRI
tokenization	UD Japanese-GSD test set	 Table 5: F1 scores of tokenization and UAS on the UD Japanese-GSD test set. The top section shows the  systems which used their own tokenizers. The second section is a comparison with the systems relying  on the default settings of UDPipe, and the bottom section is the situation to ru parsers using the gold PoS  as input.
relation classification task	CoNLL	We investigate the use of different syntactic dependency representations in a neu-ral relation classification task and compare the CoNLL, Stanford Basic and Universal Dependencies schemes.
Topic classification	RCV1	 Table 3: Topic classification results on RCV1.
paraphrase detection	SNLI dataset	We also tested our approach on semantic textual similarity (STS 14 -), paraphrase detection (MRPC -Dolan et al.), entailment and semantic relatedness tasks (SICK-R and SICK-E -), though those tasks are more close in nature to the task of the SNLI dataset which the model was trained on.
SRL	PropBank data	We explore a sequence-to-sequence formulation of SRL that we apply, as a first step, in a classical monolingual setting on PropBank data, as illustrated in.
event extraction'	ACE/ERE	Unlike other supervised data-driven 'event extraction' tasks such as in the ACE/ERE programs (), we do not conceptualize events as structured schemata/frames, but more limited as textual mentions of real-world occurrences.
classification task	REPORTS	 Table 3: Results of the classification task for the  uncertain class of REPORTS. The best results are  boldfaced and significant performance increases  (α = 0.05) over Unc are marked with asterisks.
SPA-ENG	English Gigaword corpus	The embeddings for SPA-ENG are trained by combining a portion of English Gigaword corpus ( and Spanish Gigaword corpus), and a subset of tweets from.
LM	HindiEnglish code-mixed data (Hinglish)	Our goal is to improve LM for HindiEnglish code-mixed data (Hinglish) where similar challenges are apparent.
Tackling Code-Switched NER	CMU	Tackling Code-Switched NER: Participation of CMU
sentiment analysis	CMU-MOSEI	 Table 1: Performance of individual modality and multimodal fusion for sentiment analysis on the vali- dation set of CMU-MOSEI. MAE is the Mean Absolute Error.
Sentiment classification	MOSI  MOSEI  Acc. F1 Acc. F1	 Table 1: Sentiment classification performance us- ing a bi-directional LSTM-RNN classifier.  MOSI  MOSEI  Acc. F1 Acc. F1
ASR view	MOSI MOSEI  MT  71.1  67.5	 Table 2: Improvement in ASR view accuracy us- ing a non contextual classifier.  MOSI MOSEI  MT  71.1  67.5  AT  63.7  63.8  AT ↑  65.1  65.7
Unimodal sentiment analysis	CMU-MOSI test set	 Table 2: Unimodal sentiment analysis results on  the CMU-MOSI test set. Numbers in bold are the  best results on each modality.
sentiment prediction	MOSI dataset	We present our work on sentiment prediction using the benchmark MOSI dataset from the CMU-MultimodalDataSDK.
sentiment prediction	CMU-MultimodalDataSDK	We present our work on sentiment prediction using the benchmark MOSI dataset from the CMU-MultimodalDataSDK.
Domain Shift Experiments  Corpus  Type  Aim  Genre	NWJC  Training Nwjc2vec  Web pages  1,463,142,939  Mainichi Shimbun 1993-1999 Training Mai2vec  Newspaper  6,791,403  BCCWJ  Training Word2vec	 Table 5: Corpus Data for Domain Shift Experiments  Corpus  Type  Aim  Genre  Number of Sentences  NWJC  Training Nwjc2vec  Web pages  1,463,142,939  Mainichi Shimbun 1993-1999 Training Mai2vec  Newspaper  6,791,403  BCCWJ  Training Word2vec of base-lm-1  Blogs  7,226  Language model  And  BCCWJ  Test  For blogs and Q &A sites Q & A sites  104  Mainichi Shimbun 2007  Training Word2vec of base-lm-2  100,000  Language model  Newspaper  Mainichi Shimbun 2008  Test  For newspaper  10,000
Domain Shift Experiments  Corpus  Type  Aim  Genre	Mainichi Shimbun 2007  Training Word2vec	 Table 5: Corpus Data for Domain Shift Experiments  Corpus  Type  Aim  Genre  Number of Sentences  NWJC  Training Nwjc2vec  Web pages  1,463,142,939  Mainichi Shimbun 1993-1999 Training Mai2vec  Newspaper  6,791,403  BCCWJ  Training Word2vec of base-lm-1  Blogs  7,226  Language model  And  BCCWJ  Test  For blogs and Q &A sites Q & A sites  104  Mainichi Shimbun 2007  Training Word2vec of base-lm-2  100,000  Language model  Newspaper  Mainichi Shimbun 2008  Test  For newspaper  10,000
Domain Shift Experiments  Corpus  Type  Aim  Genre	Mainichi Shimbun 2008  Test	 Table 5: Corpus Data for Domain Shift Experiments  Corpus  Type  Aim  Genre  Number of Sentences  NWJC  Training Nwjc2vec  Web pages  1,463,142,939  Mainichi Shimbun 1993-1999 Training Mai2vec  Newspaper  6,791,403  BCCWJ  Training Word2vec of base-lm-1  Blogs  7,226  Language model  And  BCCWJ  Test  For blogs and Q &A sites Q & A sites  104  Mainichi Shimbun 2007  Training Word2vec of base-lm-2  100,000  Language model  Newspaper  Mainichi Shimbun 2008  Test  For newspaper  10,000
Tweet distribution	HEOT	 Table 1: Tweet distribution in dataset A and  HEOT.
SR training	WikiText corpus	To augment the SR training data, we used sentences from the WikiText corpus (.
SMT	Chinese Wikipedia corpus	For the SMT model, we trained the language model part on different corpora, including the Gigaword, the Chinese Wikipedia corpus), and the corpus consists of CGED as well as Lang-8 correct sentences which are constructed by ourselves.
Chinese Grammatical Error Diagnosis (CGED)	ACL2018	Chinese Grammatical Error Diagnosis (CGED) is a natural language processing task for the NLPTEA2018 workshop held during ACL2018.
Detecting Grammatical Errors	NTOU CGED System	Detecting Grammatical Errors in the NTOU CGED System by Identifying Frequent Subsentences
alignment	English-Portuguese CLUE4Translation Alignment Collection	The alignment task was performed manually in a subset of the English-Portuguese CLUE4Translation Alignment Collection.
Indo-Aryan Language Identification shared task	VarDial 2018 Evaluation Campaign	This paper presents the experiments and results obtained by the SUKI team in the Indo-Aryan Language Identification shared task of the VarDial 2018 Evaluation Campaign.
variety adaptation	eSPERTo Paraphrase Aligned Corpus of EN-EP/BP Translations)	In order to enable variety adaptation, we have analyzed the contrastive pairs of paraphrastic units aligned and collected from the corpus e-PACT (eSPERTo Paraphrase Aligned Corpus of EN-EP/BP Translations), a parallel corpus of aligned paraphrases ( . One of the motivations behind the creation of this corpus was to contrast the European (EP) and Brazilian (BP) varieties This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Morphosyntactic Tagging of Tweets shared task	VarDial Evaluation Campaign	This paper presents two systems taking part in the Morphosyntactic Tagging of Tweets shared task on Slovene, Croatian and Serbian data, organized inside the VarDial Evaluation Campaign.
Discriminating between Dutch and Flemish in Subtitles (DFS)	VarDial evaluation campaign 2018	Discriminating between Dutch and Flemish in Subtitles (DFS) is a shared task at the VarDial evaluation campaign 2018.
Identification of Differences between Dutch Language Varieties	VarDial2018 Dutch-Flemish Subtitle Data	Identification of Differences between Dutch Language Varieties with the VarDial2018 Dutch-Flemish Subtitle Data
Distribution	AOC Train	 Table 2: Distribution of classes in our AOC Train split
MT	WMT	Even so, NMT systems have constantly ranked in the top positions in the competitions held in MT conferences and workshops such as WMT () and WAT (.
MT	WAT	Even so, NMT systems have constantly ranked in the top positions in the competitions held in MT conferences and workshops such as WMT () and WAT (.
SMT	NMT system	We evaluated the quality of the EP-BP translation direction asking a group of seven native speakers of BP to rank sentences produced by the SMT system and by the NMT system.
WSC	WSCL	Unlike most other research on WSC, we test our models on both data sets discussed above -WSCL, the smaller data set of higher quality (282 examples) with strict and mostly unambiguous WSC cases, which we exclusively use for testing and WSCR, which comes in a predefined split of 1322 training and 564 testing problems, but which is of slightly reduced quality for the reasons discussed in Section 3.
WS	WSCL	Given that the WS problems in WSCL and WSCR come in pairs with alternative resolutions to first vs. second antecedent candidate, we apply a random process as the baseline with 0.5 probability of achieving the correct guess.
WS	WSCR	Given that the WS problems in WSCL and WSCR come in pairs with alternative resolutions to first vs. second antecedent candidate, we apply a random process as the baseline with 0.5 probability of achieving the correct guess.
Classification	Bo Xilai subcorpus	 Table 2: Classification results for the Bo Xilai subcorpus.
Classification	Internet & Propaganda subcorpus	 Table 5: Classification results for the Internet & Propaganda subcorpus.
Aggression Identification Shared Task	TRAC -1	 Table 2: The teams that participated in the Aggression Identification Shared Task at TRAC -1.
tokenization	Penn Treebank style	The procedure consisted in tokenization and extraction of parts of speech (POS) with Penn Treebank style.
aggression detection shared task	TRAC1 workshop	We describe the participation of team TakeLab in the aggression detection shared task at the TRAC1 workshop for English.
normalizing historical English automatically	CEEC corpus (Corpora of Early English Correspondence)	The goal of this paper is to propose and compare methods of normalizing historical English automatically in the context of our CEEC corpus (Corpora of Early English Correspondence), which consists of letters ranging from the 15th to the 19th century.
Induction	Regesta Imperii	Induction of a Large-Scale Knowledge Graph from the Regesta Imperii
tagging	FG-SO	Indeed, the tagging task, although still lower than the segmentation task, is much improved in FG-SO compared to FG.
tagging	FG	Indeed, the tagging task, although still lower than the segmentation task, is much improved in FG-SO compared to FG.
identification task	BRNN	The model we employed for the identification task is a BRNN with 100 LSTM units.
Multiple-Personality Generation	DISAGREEABLE	 Table 10: Multiple-Personality Generation Out- put based on DISAGREEABLE
relation modelling	Cambridge	To evaluate the relation modelling capabilities of the CEDM, the task of finding a hotel and a restaurant in Cambridge has been selected (corresponding to the CamRestaurants and CamHotels domains of PyDial).
relation modelling	CamRestaurants	To evaluate the relation modelling capabilities of the CEDM, the task of finding a hotel and a restaurant in Cambridge has been selected (corresponding to the CamRestaurants and CamHotels domains of PyDial).
relation modelling	PyDial	To evaluate the relation modelling capabilities of the CEDM, the task of finding a hotel and a restaurant in Cambridge has been selected (corresponding to the CamRestaurants and CamHotels domains of PyDial).
SLU	Micro F1	Previous studies have evaluated SLU performances by metrics such as Micro F1.
dialogue modeling	OpenDial	In this category some of the more expressive approaches to dialogue modeling are based on the information state; notable toolkits include TrindiKit) and its open-source successor trindikit.py, and OpenDial (.
noun replacement	LSTM	In the third generated dataset, noun replacement, the highest performance relates to the LSTM and the second highest relates to the CNN with a very small margin.
noun replacement	CNN	In the third generated dataset, noun replacement, the highest performance relates to the LSTM and the second highest relates to the CNN with a very small margin.
Classification of Offensive Tweets	Hinglish Language	Did you offend me? Classification of Offensive Tweets in Hinglish Language
Link prediction	CDCP dataset	Link prediction is a particularly difficult task in the CDCP dataset, where only 3% of all the possible proposition pairs (more than 43,000) are linked.
AM	PCPA	While our dataset of discussion threads will make further advances in AM, the proposed PCPA will make end-to-end AM studies going forward.
SRL	BioASQ dataset	We are not aware of a comprehensive evaluation of available SRL tools on the BioASQ dataset, which is the most comprehensive dataset on biomedical QA ().
SRL	BioASQ datasets	Our contribution in this work is three-fold: (i) we provide a comprehensive overview on SRL for biomedicine and QA; (ii) we perform a comparison of selected tools regarding pre-defined criteria based on hands-on experiments; and (iii) we evaluated the selected SRL tools on the BioASQ datasets regarding their PAS coverage and performance in a QA system.
answer generation	BioASQ 5b batch 2 test	This paper presents a system for ideal answer generation (using ontology-based retrieval and a neu-ral learning-to-rank approach, combined with extractive and abstractive summarization techniques) which achieved the highest ROUGE score of 0.659 on the BioASQ 5b batch 2 test.
topic categorization	BBC news dataset	For the topic categorization task we used the BBC news dataset), 20News (Lang, 1995), Reuters 6 () and Ohsumed 7 . The code for this CNN implementation is the same as in (, which is available at https://github.
topic categorization	20News (Lang, 1995)	For the topic categorization task we used the BBC news dataset), 20News (Lang, 1995), Reuters 6 () and Ohsumed 7 . The code for this CNN implementation is the same as in (, which is available at https://github.
topic categorization	Reuters 6	For the topic categorization task we used the BBC news dataset), 20News (Lang, 1995), Reuters 6 () and Ohsumed 7 . The code for this CNN implementation is the same as in (, which is available at https://github.
tokenization	Stanford CoreNLP	For tokenization and lemmatization we relied on Stanford CoreNLP ().
sentiment classification	Amazon review dataset	b) Elec: electronic product reviews for sentiment classification introduced by, assembled from the Amazon review dataset For word embeddings, we use the pre-trained GloVe Wikipedia 2014-Gigaword 5 embeddings (), which we fine-tune with the model.
sentiment classification	GloVe Wikipedia 2014-Gigaword 5 embeddings	b) Elec: electronic product reviews for sentiment classification introduced by, assembled from the Amazon review dataset For word embeddings, we use the pre-trained GloVe Wikipedia 2014-Gigaword 5 embeddings (), which we fine-tune with the model.
SST	IMDb dataset	The word and character n-gram vectors are kept fixed for SST but are learned for the IMDb dataset.
PoS tagging	CoNLL2000 Chunking shared task	We used a standard benchmark for each task: the Universal Dependencies English Web Treebank v2.0 (Zeman et al., 2017) for PoS tagging, the CoNLL2000 Chunking shared task), the CoNLL2003 NER shared task, and the annotated data from the Parallel Meaning Bank (PMB) for Semantic tagging (.
PoS tagging	CoNLL2003 NER shared task	We used a standard benchmark for each task: the Universal Dependencies English Web Treebank v2.0 (Zeman et al., 2017) for PoS tagging, the CoNLL2000 Chunking shared task), the CoNLL2003 NER shared task, and the annotated data from the Parallel Meaning Bank (PMB) for Semantic tagging (.
Shooting	Warren Ave	The query "Shooting on Warren Ave.
Stance Detection	Fake News Challenge 1 (FNC-1)	Concerning FND, recently released a dataset for the Stance Detection step in the framework of the Fake News Challenge 1 (FNC-1).
Relation classification	NIST-organised TAC Knowledge Base Population (TAC-KBP) track	Relation classification is an essential part of many high-performing relation extraction systems in the NIST-organised TAC Knowledge Base Population (TAC-KBP) track (.
relation extraction	NIST-organised TAC Knowledge Base Population (TAC-KBP) track	Relation classification is an essential part of many high-performing relation extraction systems in the NIST-organised TAC Knowledge Base Population (TAC-KBP) track (.
Oracle classification	develop- ment set	 Table 2: Oracle classification on claims in the develop- ment set using gold sentences as evidence
RE tasks	EU-ADR	presents a comparison of performances obtained with our approach versus two state-ofthe-art systems applied to the RE tasks associated respectively with SNPPhenA (Bokharaeian et al., 2017) and EU-ADR ().
TL	Train corpus	 Table 2: Results of our TL strategy in terms of precision (P), recall (R) and f-measure (F). σ F is the standard  deviation of the f-measure. The + in the column Train corpus indicates that we trained our model using the target  corpus plus one additional source corpus.
sequence tagging tasks	CoNLL 2003 corpus	Furthermore, YASET supports the use of handcrafted features in combination with word and character embeddings; • an easy-to-use interface based on a central configuration file that is used to setup experiments, with default parameters that are suitable for most sequence tagging tasks; • an evaluation on various biomedical corpora and on the CoNLL 2003 corpus, studying the stability of our model and the effect of training data size.
SMM4H shared tasks	EMNLP-2018	The third execution of the SMM4H shared tasks, co-hosted with EMNLP-2018, comprised of four subtasks.
SMM4H shared tasks	AMIA Annual Symposium	The third execution of the SMM4H shared tasks built on the success of the two previous shared task workshops, which were held at the Pacific Symposium on Biocomputing (PSB) in 2016 and at the AMIA Annual Symposium in 2017.
SVM classifier	HO datasets	 Table 3: Results of SVM classifier on PDR, BDC and HO datasets
vaccination behaviour detection shared task	SMM4H workshop	We present our submission for vaccination behaviour detection shared task at the SMM4H workshop.
word vector training	Gensim package	Other parameters involved in word vector training were left to the default values of the Gensim package.
detection of drug mentions	Drug-Bank [Wishart et al., 2017]	2.1 Gazetteer Inspired by Task 1, detection of drug mentions, we scraped the name field of product fields in Drug-Bank [Wishart et al., 2017] to compile a gazetteer list for drugs.
chunking	Penn Treebank	Nowadays, studies) still compare chunking -as well as constituency parsing-performance on these same data from the Penn Treebank.
constituency parsing-performance	Penn Treebank	Nowadays, studies) still compare chunking -as well as constituency parsing-performance on these same data from the Penn Treebank.
parsing	UDv2New	Our experimental parsing results with UDv2New confirm that improving the coherence and internal consistency of the UD HTB indeed leads to improved parsing performance.
parsing	UD HTB	Our experimental parsing results with UDv2New confirm that improving the coherence and internal consistency of the UD HTB indeed leads to improved parsing performance.
predicting morphological analysis (MA)	HebLex	We use yap for predicting morphological analysis (MA) and morphological disambiguation, and we contrast the use of a data-driven lexicon baselinelex with an external broad-coverage lexicon HebLex.
reddit Assigning people to tasks identified in email	EPA dataset	Table of Contents Inducing a lexicon of sociolinguistic variables from code-mixed text Geocoding Without Geotags: A Text-based Approach for reddit Assigning people to tasks identified in email: The EPA dataset for addressee tagging for detected task intent
geolocation inference	reddit data set	We first examine geolocation inference performance within the domain of our reddit data set.
Normalization of Transliterated Words	Levenshtein Distance	Normalization of Transliterated Words in Code-Mixed Data Using Seq2Seq Model & Levenshtein Distance
sentiment analysis	ROC AUC	 Table 2: Results of the sentiment analysis and textual entailment tasks in terms of ROC AUC.
role labeling	PropBank	Research on role labeling maps text into frames of existing ontologies such as FrameNet () and PropBank ().
sentiment analysis	Word2Vec	In order to establish the importance of the SSWE for sentiment analysis, we trained multiple models using standard features (word ngrams, character ngrams) or more advanced features (Word2Vec, SSWE: average sentence embedding) with standard classifiers (logistic regression (LR), SVM, Random Forest) or deep learning frameworks (CNN or RNN with transfer learning on the data set Train), then evaluated them on Test1, Test2, and Test3.
Distribution	IEST data	 Table 2: Distribution of IEST data.
WSCs	Chinese microblog	The dataset for WSCs is collected from Chinese microblog by Lee's group.
ABSA real-world	Foursquare 1 comments	In order to assess ABSA real-world performances, we manually annotated a completely new dataset from Foursquare 1 comments.
Implicit Emotion Recognition	WASSA 2018 workshop	As it is, Implicit Emotion Recognition, as proposed by organizers of WASSA 2018 workshop (, can be seen both as a text reconstruction and as a sentiment analysis task.
recognition of emotions	IEST dataset on English nativespeakers	This makes recognition of emotions intricate even for human readers as evident from the noticeably low inter-annotator agreement reported by or the  "testing" of the IEST dataset on English nativespeakers which resulted in an F-score of 0.45 ().
sentiment classification	SemEval dataset	We train and evaluate our model on sentiment classification SemEval dataset obtained through shared task and affect emotion dataset from SemEval-2018.
sentiment classification	SemEval-2017 Task 4 Subtask A dataset	For sentiment classification dataset we use SemEval-2017 Task 4 Subtask A dataset.
sentiment analysis	Corpus_500	For sentiment analysis, we used Corpus_500.
MT evaluation	WMT15, 7	To this end we use the TrueSkill method adapted to MT evaluation () following its usage at WMT15, 7 i.e. we run 1,000 iterations of the rankings recorded with Appraise followed by clustering (significance level α = 0.05).
domain adaptation analysis	PBMT	Motivated by domain adaptation analysis in PBMT (, this work proposes a simple freezing subnetworks technique and uses it to gain insight into how the various components of an NMT system behave during continued training.
WMT	ParaCrawl data	 Table 6: BLEU scores on WMT testing sets of the  NMT models trained on different data: 1) WMT  training sets, 2) filtered ParaCrawl data, and 3)  combined data of WMT and filtered ParaCrawl.
WMT	ParaCrawl	 Table 6: BLEU scores on WMT testing sets of the  NMT models trained on different data: 1) WMT  training sets, 2) filtered ParaCrawl data, and 3)  combined data of WMT and filtered ParaCrawl.
Translation	WMT 2016 English→Romanian task	The  Translation results for the WMT 2016 English→Romanian task and the BOLT Chinese→English task.
MMT	2016 test set	 Table 1: Quantitative results of the MMT experiments on the 2016 test set. Column 'adv. BLEU' is an  adversarial evaluation with randomized image input.
Machine Translation (WMT)	EMNLP 2018 1	The Third Conference on Machine Translation (WMT) held at EMNLP 2018 1 host a number of shared tasks on various aspects of machine translation.
DA judgments	Amazon's Mechanical Turk	Where possible,  we collect DA judgments via the crowd-sourcing platform, Amazon's Mechanical Turk, and as in previous year's we ask participating teams to provide manual evaluation of system outputs via Appraise.
DA judgments	Appraise	Where possible,  we collect DA judgments via the crowd-sourcing platform, Amazon's Mechanical Turk, and as in previous year's we ask participating teams to provide manual evaluation of system outputs via Appraise.
MT	OpenNMT	However, in spite of the growing number of parallel corpora and the many open source tools for MT (e.g.,), OpenNMT ( and Marian), there is still no ready-to-use tool for automatic translation of biomedical publications for any language pair.
MT	UFRGS team	Once again the statistical MT system (Moses) from the UFRGS team obtained a slightly higher score than their neural MT system (Open-NMT).
validation	Appraise tool	The validation task was carried out using the 3-way ranking task in our installation of the Appraise tool.
SMT	WMT 2017 datasets	parfda obtains results close to the top constrained phrase-based SMT with an average of 2.252 BLEU points difference on WMT 2017 datasets using significantly less computation for building SMT systems than that would be spent using all available corpora.
SMT	WMT18	parfda is an instance selection tool based on feature decay algorithms) we use to select training and language model instances to build Moses phrase-based SMT systems to translate the test sets in the news translation task at WMT18.
translation tasks	WMT18	Building a language independent system that can perform well in translation tasks is a challenging task and SMT systems participating at WMT18 have been largely built dependent on the translation direction.
SMT	WMT18	Building a language independent system that can perform well in translation tasks is a challenging task and SMT systems participating at WMT18 have been largely built dependent on the translation direction.
SMT	WMT18	Apart from the SMT system itself, we also describe our work on parallel-corpus preprocessing and filtering, an aspect which has gained importance in WMT18 with the addition of the much larger and noisier parallel corpus ParaCrawl.
Size	WMT18 parallel dataset	 Table 1: Size by corpus of the WMT18 parallel dataset
JHU Machine Translation	WMT 2018 Anonymous EMNLP submission	The JHU Machine Translation Systems for WMT 2018 Anonymous EMNLP submission
MT	WMT18	This simple combination method, associated to the exploitation of large back-translated monolingual data, performed among the best MT systems at WMT18.
MT	Newstest2018 test set	 Table 6: Detokenized BLEU-cased scores for our MT systems on the Newstest2018 test set. "NMT-reranked"  denotes the reranking of the Moses's 100-best hypotheses using all our NMT models (left-to-right and right-to- left, for both translation directions, trained with back-translated data) as features. "backtr" denotes the use or not  of back-translated monolingual data. "Moses + Marian" denotes our combination of best NMT (#5) and SMT  (#1) systems described in Section 5.
WMT	Theano framework	In 2017, the University of Helsinki participated in WMT with an in-house implementation of an attentional encoder-decoder architecture based on the Theano framework, called HNMT ( ¨.
SMT	WMT17	The team has a strong track record at previous WMT shared tasks) working on SMT systems) and proposed atop scoring linguistically informed neural machine translation system) based on human evaluation at WMT17.
WMT18 news translation task	Europarl	Although this language pair has an abundance of parallel data, we are constrained to only using monolingual data provided for the WMT18 news translation task, excluding Europarl and News Commentary because of content overlap.
MT	WMT	We thus limit the comparison to overall MT system quality as provided by WMT.
MT shared task	WMT 2018 conference	In this paper, we present a semi-automatic evaluation of the systems participating in the EnglishGerman news translation track of the MT shared task at the WMT 2018 conference.
Biomedical Translation Task	Medline	The 2018 Biomedical Translation Task, held as part of the Third Conference on Machine Translation, aims at evaluating systems on scientific publications from Medline ().
Neural Machine Translation	WMT	LMU Munich's Neural Machine Translation Systems at WMT 2018
Biomedical translation	WMT'18	We conduct our experiments on Biomedical translation task of WMT'18.
SMT	NMT	For the English/Spanish pair, the SMT system presented slightly better results than the NMT one, probably due to the dictionary size used in the NMT.
SMT	NMT	For the English/Spanish pair, the SMT system presented slightly better results than the NMT one, probably due to the dictionary size used in the NMT.
DA task	NMT	The twelve human evaluators spent a total of 64 hours on the DA task with an average of 17.2 and 17.5 seconds per assessment for the NMT and PB-SMT subtasks respectively.
MT	MS UEdin	The human post-edited MT output reaches an averaged DA score of 96.13%, ranked above the first system (MS UEdin) with an averaged DA score of 91.11%.
MT	WMT16	 Table 2: Number of MT systems and system-level DA human evaluation datasets for to-English language pairs in  WMT16 (Bojar et al., 2016) and WMT17 (Bojar et al., 2017).
MT	WMT17	 Table 2: Number of MT systems and system-level DA human evaluation datasets for to-English language pairs in  WMT16 (Bojar et al., 2016) and WMT17 (Bojar et al., 2017).
WMT 2016 IT translation task	WMT 2017 QE task	For English-German, in the IT domain, we used the training data from the WMT 2016 IT translation task, the WMT 2017 QE task and the WMT 2018 PE task; given the low amounts of data in each individual corpus, we also merged the data from the technical manuals of OpenOffice and KDE4 available in the OPUS repository task.
WMT 2016 IT translation task	WMT 2018 PE task	For English-German, in the IT domain, we used the training data from the WMT 2016 IT translation task, the WMT 2017 QE task and the WMT 2018 PE task; given the low amounts of data in each individual corpus, we also merged the data from the technical manuals of OpenOffice and KDE4 available in the OPUS repository task.
WMT 2016 IT translation task	OPUS repository task	For English-German, in the IT domain, we used the training data from the WMT 2016 IT translation task, the WMT 2017 QE task and the WMT 2018 PE task; given the low amounts of data in each individual corpus, we also merged the data from the technical manuals of OpenOffice and KDE4 available in the OPUS repository task.
MT QE	WMT	Six datasets were provided for the shared task on MT QE at WMT 2018 (, covering four language pairs -English-German (EN-DE), German-English (DE-EN), English-Latvian (EN-LV), and English-Czech (EN-CS)-and two MT systems -statistical MT (SMT) and neural MT (NMT).
SMT post-editing	NMT sub-task	Our submissions decisively wins the SMT post-editing sub-task establishing the new state-of-the-art and is a very close second (or equal, 16.46 vs 16.50 TER) in the NMT sub-task.
SMT postediting	NMT sub-task	Our submissions decisively wins the SMT postediting sub-task establishing the new state-of-theart and is a very close second (or equal, 16.46 vs 16.50 TER) in the NMT sub-task.
MT	German side of the dirty corpus	Our supervised submissions used the given parallel data to train an MT system to translate the German side of the dirty corpus into English.
SMT	newstest2018	 Table 2: BLEU scores of SMT and NMT systems trained on the 10M-and 100M-word corpora subselected by the  scoring systems. "bicov" indicates that the final bigram coverage step ( §2.4) was performed. The development set  is newstest2017 and the test set is newstest2018.
SMT	Batch	10 SMT systems were built with Moses and tuned with Batch MIRA.
SMT	NMT	The development set (used for tuning the parameters of the log-linear model in SMT and for early stopping in NMT) was newstest2016, while the test set was newstest2017.
SMT	newstest2016	The development set (used for tuning the parameters of the log-linear model in SMT and for early stopping in NMT) was newstest2016, while the test set was newstest2017.
SMT	newstest2017	The development set (used for tuning the parameters of the log-linear model in SMT and for early stopping in NMT) was newstest2016, while the test set was newstest2017.
translation	100M dataset	The submissions that aimed at increasing the fluency of the training corpus brought alight improvement in translation quality for the NMT system trained on the 100M dataset.
Discourse ordering	WebNLG corpus	• Discourse ordering information of 20,370 instances of the WebNLG corpus.
SR	NLG	Section 5 assesses the quality of the obtained representations, while Section 6 discusses their suitability for SR and NLG more generally.
parsing	OpenCCG	Building on recent work using LSTMs to improve accuracy on supertagging for parsing, we develop an LSTM hypertagging method for OpenCCG, an open source NLP toolkit for CCG.
Answerability	FocusCR	Answerability is the category in which the FocusCR model has the greatest improvement over the Baseline.
parsing	ccg2lambda	The parsing and inference system of ccg2lambda achieved high precision in RTE tasks;  reported that the precision was nearly 100% for the SICK dataset).
parsing	SICK dataset	The parsing and inference system of ccg2lambda achieved high precision in RTE tasks;  reported that the precision was nearly 100% for the SICK dataset).
dialog generation	NCM	Seq2Seq was first proposed for dialog generation by   NCM being closed-source, nearly all the papers comparing against it have implemented their own versions, with widely varying performance.
Meaning Assessments	CREG-5K	 Table 1: Meaning Assessments in CREG-5K
SLA	GSL	By taking the perspective of SLA research into consideration, we also approach a broader group of learners, including GSL.
Parsing  mance	Brown corpus	Out-of-domain Parsing  mance in the Brown corpus.
Abstract Meaning Representation (AMR)	NLU	We explore the adequacy of Abstract Meaning Representation (AMR) as a conduit for NLU.
WSD	FinnWord-Net	While there has been some work on other languages, including Uralic languages, up until this point no work has been published providing a contrastive evaluation of WSD for Finnish, despite the requisite lexical resources, most notably FinnWord-Net, having long been in place.
WSD	SemEval data sets	For English,  present a recent comparison of different WSD systems across harmonised SensEval and SemEval data sets.
AMR graph semantics	CCG	After an overview of related work ( §2), we introduce our formalism for AMR graph semantics in CCG ( §3).
WSD task	English SenseEval datasets	We now turn to comparing the difficulty of the WSD task, when tested on English SenseEval datasets versus on FrenchSemEval.
WSD task	FrenchSemEval	We now turn to comparing the difficulty of the WSD task, when tested on English SenseEval datasets versus on FrenchSemEval.
WSD	FSE	 Table 4: WSD accuracies when training on Wiktionary examples, and testing on FSE.
Frame Identification	Das et al. (2014) benchmark dataset	 Table 1: Accuracy results for Frame Identification on Das et al. (2014) benchmark dataset (test partition)
Linguistics Theory and Studies in Probability (CLASP)	Swedish Research Council (VR project 2014-39)	IWCS 2019 has received general financial support (covering over a half of the costs) from the Centre for Linguistics Theory and Studies in Probability (CLASP) which in turn is financed by a grant from the Swedish Research Council (VR project 2014-39) and University of Gothenburg.
Linguistics Theory and Studies in Probability (CLASP)	Swedish Research Council (VR project 2014-39)	IWCS 2019 has received general financial support (covering over a half of the costs) from the Centre for Linguistics Theory and Studies in Probability (CLASP) which in turn is financed by a grant from the Swedish Research Council (VR project 2014-39) and University of Gothenburg.
predicting stance	Baltimore datasets	 Table 2: Result of predicting stance (first 12 columns) and morality (last two columns) with SVM and RF for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each half  column) in bold, highest accuracy per each model (each column) in gray)
predicting stance	Baltimore datasets	 Table 3: Result of predicting stance (first 7 columns) and morality (last column) with LSTM model for stance  and Baltimore datasets (Accuracy) (highest performance per set of experiments (OM, EM, and EMNP -each  half column) in bold, highest accuracy per each model (each column) in gray)
LM adaptation	Chinese datasets	We modified our LM adaptation technique to be used with the NB classifier and this fared better than the adaptive HeLI 2.0 method on both of the Chinese datasets.
PMA	EMA	Second, we compared the best performance in PMA with the performance in EMA.
PMA	EMA data	For the 132 phrases in both PMA and EMA data, 110 phrases were used for training, 10 for validating, and 12 for testing.
Extractive Clinical Note Summarization	EHR	A Novel System for Extractive Clinical Note Summarization using EHR Data
hedging detection	Bioscience domain	Furthermore, inspired by the work on hedging detection on Bioscience domain (, we explore a list of "speculation cues" to filter out non-factual min/max age expressions.
compositionality prediction	RAMISCH dataset	 Table 2: Pearson correlation coefficient for compositionality prediction results on the RAMISCH dataset.
word relatedness/similarity tasks	WS-3533	Our proposed task is most similar to the word relatedness/similarity tasks, several of which have already been proposed in literature: WS-3533 (), WS-SIM and WS-REL (), RG-65,), Rare Words (, etc.
GPT	CODAH-only and SWAG+CODAH experiments	Unsurprisingly, GPT performance improves with more data on both the CODAH-only and SWAG+CODAH experiments, with the rate of improvement slowing down as data size increases.
NER task	BC2GM dataset	Data: For the NER task, we use the BC2GM dataset.
stance prediction	MFC baseline	 Table 3: Performance of stance prediction models trained only on user attributes, shown here for each of the  different stance targets. Bold indicates best in column for user attributes and inferred factors. The weighted F1  is shown for each target and the last column is the unweighted average across all targets.  † indicates statistical  significance at the 0.05 level compared to the MFC baseline.
Paraphrase Generation	NLU	Paraphrase Generation for Semi-Supervised Learning in NLU
validation	Europarl dataset	For the training, validation and test splits, we used 200k, after filtering, randomly chosen sentences from the Europarl dataset for training and 40k sentences for testing.
Computational analysis of genre	Brown corpus	Computational analysis of genre has most often involved material from a single source (e.g., a newspaper corpus, for which the goal is to distinguish between news articles and opinion pieces) or from standard, well-curated test corpora that contain primarily non-literary texts (e.g., the Brown corpus or equivalents in other languages) ().
normalizing	CEEC	In this paper, we will present different NMT models and evaluate their effectiveness in normalizing the CEEC.
coreference resolution	ACL anthology's paper identifier	Example (1) discusses the cons of previous technologies for coreference resolution: (1) While successful, these approaches require labeled training data, consisting of mention 1 Throughout the paper, an appended 8-character identifier indicates the ACL anthology's paper identifier.
Segmented Discourse Representation Theory (SDRT)	Penn Discourse Treebank (PDTB)	Several theoretical frameworks exist for discourse analysis, and automatic discourse analyzers (ADA) have been developed within each framework, but mostly for English texts: i) under Rhetorical Structure Theory (RST) (: see for example ( ii) under Segmented Discourse Representation Theory (SDRT), as the one developed by iii) or Penn Discourse Treebank (PDTB) style as the one described in ().
Rhetorical Structure Theory (RST)	Penn Discourse Treebank (PDTB)	Among the most popular approaches are Rhetorical Structure Theory (RST) (, the Penn Discourse Treebank (PDTB) (, Segmented Discourse Representation Theory (SDRT) ( and the Cognitive approach to Coherence Relations (CCR) (.
Segmented Discourse Representation Theory (SDRT)	Penn Discourse TreeBank (PDTB)	The task definition differs slightly across the various existing and competing formalisms: in Rhetorical Structure Theory (RST) (, all segments are adjacent while in Segmented Discourse Representation Theory (SDRT), segments can be embedded in one another; In the Penn Discourse TreeBank (PDTB), the task is expressed as finding the arguments of a discourse connective, whether this connective is implicit or explicit.
coreference resolution	OntoNotes benchmark	We propose an end-to-end coreference resolution system obtained by adapting neural models that have recently improved the state-of-the-art on the OntoNotes benchmark to make them applicable to other paradigms for this task.
coreference resolution	CoNLL-2012	For coreference resolution, we use the CoNLL-2012 metrics () including BLANC.
estimating suicide risk	Suicide Watch	Task C has the goal of estimating suicide risk by looking at a subject's posts on different subreddits, but excluding Suicide Watch.
Predicting Suicide Risk from Online Postings	CLPysch 2019 Shared Task	Predicting Suicide Risk from Online Postings in Reddit The UGent-IDLab submission to the CLPysch 2019 Shared Task A
ADR normalization	SMM4H 2019 Task 3	The end-to-end model based on BERT for ADR normalization ranked first at the SMM4H 2019 Task 3 and obtained a relaxed F1 of 43.2%.
Text classification	Task 1 test  set	 Table 1: Text classification results on the Task 1 test  set.
Transfer learning	CADEC medical ADR dataset	Transfer learning is conducted on the CADEC medical ADR dataset () first in task 1.
Detection of Adverse Drug Reaction	ELMo	Detection of Adverse Drug Reaction mentions in tweets using ELMo
Node identification	SMATCH	 Table 2: Node identification and WSD results on MRS  in terms of noun (n), verb (v), quantifier (q), preposi- tion (p), adjective (a), conjunction (c), and others (x),  and on AMR in terms of predicate (pred). Both are  measured on the test set in terms of accuracy based on  SMATCH.
WSD	SMATCH	 Table 2: Node identification and WSD results on MRS  in terms of noun (n), verb (v), quantifier (q), preposi- tion (p), adjective (a), conjunction (c), and others (x),  and on AMR in terms of predicate (pred). Both are  measured on the test set in terms of accuracy based on  SMATCH.
SRL	MRS	 Table 6: Results on SRL. MRS's argument number be- gins at 1 so we just move all the argument to begin at 0  to make them comparable.
mention identification	UCCA	• A semantically-based framework for mention identification and coreference resolution as a layer of UCCA ( §3).
coreference resolution	UCCA	• A semantically-based framework for mention identification and coreference resolution as a layer of UCCA ( §3).
character extraction	BiLSTM	So are that on character extraction (except the difference between BiLSTM and IDCNN).
Narrative Generation in the Wild	NaNoGenMo	Narrative Generation in the Wild: Methods from NaNoGenMo
generation of long stories	NaNoGenMo	To gain more insight in how generation of long stories is done 'in the wild', we review a collection of story generation projects that were created as part of the online challenge NaNoGenMo.
Classification	Brown Corpus and Baby BNC	 Table 4: Classification accuracy for Brown Corpus and Baby BNC with different feature sets (most frequent class  i.e., non-fiction baseline results reported).
segmentation	Spinn3r personal stories corpora	Since the segmentation model is unsupervised, we can use all data from both MCScript and the Spinn3r personal stories corpora to build the LDA model.
sentiment analysis	CMU-MOSEI data set	For the task of sentiment analysis, ;; propose several models, namely contextual intermodal attention, dynamic fusion graph, and lowrank multimodal fusion, for integrating visual, audio, and text signals on the CMU-MOSEI data set.
classifiers analysis	RFDT	Next, for classifiers analysis, the ensemble method on RFDT relatively can give better accuracy compared to NB and SVM.
Size	Waseem and Hovy (2016) data set	 Table 1: Size of the Waseem and Hovy (2016) data set
MA	Revita	We examine MA in the context of our language learning system, Revita (.
Stance and Sentiment Analysis	Croatian News	Data Set for Stance and Sentiment Analysis from User Comments on Croatian News
JRC Text Mining and Analysis Competence	BSNLP-2019 Shared Task	We report on the participation of the JRC Text Mining and Analysis Competence Centre (TMA-CC) in the BSNLP-2019 Shared Task, which focuses on named-entity recognition , lemmatisation and cross-lingual linking.
translation of English IMDb user movie reviews	Ser-bian	We focus on translation of English IMDb user movie reviews into Ser-bian, in a low-resource scenario.
sentiment classification	dataset  Reviews3	 Table 3: Results of sentiment classification for dataset  Reviews3.
coreference resolution	CoNLL-2012 shared task dataset	Many works) related to coreference resolution have been published recently and all of them are evaluated with CoNLL-2012 shared task dataset).
parsing	PUD and TNC-UD treebanks	To seethe effect of re-annotation on the parsing accuracy, we trained a state-of-the-art graph-based neural parser on the previous and re-annotated versions of the PUD and TNC-UD treebanks.
segmentation	Sejong corpus	Previously, Na (2015) obtained 97.90% and 94.57% for segmentation and POS tagging respectively using the same Sejong corpus.
dialogue modeling	Reddit data 2	To enable the study of high-quality and large-scale dataset for dialogue modeling, we have collected a corpus of 35M conversations drawn from the Reddit data 2 , where each dialogue is composed of three turn exchanges.
dialogue utterance prediction	Reddit dataset	 Table 5: Side-by-side human evaluation along with 4-scale human evaluation of dialogue utterance prediction on  Reddit dataset (mean preferences ±90% confidence intervals).
referring expression  detection	CQA dataset	 Table 3: Results of POSNet-D for referring expression  detection on CQA dataset
Alation	CQA dataset	 Table 8: Alation study of the end-to-end contexual res- olution on the CQA dataset
dialogue modeling	Ubuntu technical support datasets	We demonstrate the effectiveness of hredGAN over the VHRED for dialogue modeling with evaluations on the Movie triples and Ubuntu technical support datasets.
MT	CORPUS6	Individual-individual trains six segmenters on relevant subsections of the monolingual data and six MT systems on the relevant partitions of CORPUS6.
paraphrase detection	Microsoft Research Paraphrase Corpus (MRPC	PD For paraphrase detection (i.e., decide whether two sentences are semantically equivalent), we use the Microsoft Research Paraphrase Corpus (MRPC;).
Sentence alignment recovery	newstest2018	 Table 3: Sentence alignment recovery results with dif- ferent similarity measures (newstest2018).
Synonymy detection	TOEFL examples	Synonymy detection This popular task, first applied on the TOEFL examples for word embeddings, is to select out of some candidate targets, the one with the highest similarity to the given word.
SMLM	ONB and LFT datasets	To use SMLM we extract all vocabulary (characters) from the ONB and LFT datasets.
PR	FB15K	For PR, we learned path rules of length 2 using a sampling size of 500 for FB15K and FB-237.
PR	FB-237	For PR, we learned path rules of length 2 using a sampling size of 500 for FB15K and FB-237.
Sentiment Analysis	CMU-MOSI	 Table 2: Results for Sentiment Analysis on CMU-MOSI, emotion recognition on IEMOCAP and personality trait  recognition on POM. (CF, TF, and LMF stand for concat, tensor and low-rank fusion respectively).
Sentiment Analysis	IEMOCAP	 Table 2: Results for Sentiment Analysis on CMU-MOSI, emotion recognition on IEMOCAP and personality trait  recognition on POM. (CF, TF, and LMF stand for concat, tensor and low-rank fusion respectively).
SMDs	GER	This in combination with the unusual score distribution lead to higher absolute SMDs for GER and JPN speakers.
neural machine translation (NMT)	GEC	Currently, neural machine translation (NMT) systems using sequence-to-sequence (seq2seq) learning () that "translate" incorrect sentences into correct ones, have shown to be promising in grammatical error correction, and several recent NMT approaches have obtained the state-of-the-art results in GEC (e.g.,.
spell correction	QWK	With regard to Hypothesis 2.1, that character representations should improve performance as much as spell correction, the results demonstrate that adding character representations (w+c, -sp: mean MSE = 0.2218) can outperform spell correction of a word-only model (w, +sp: mean MSE = 0.2236) (although this is not reflected in the QWK results).
GEC	GEC test sets	In recent years, GEC performance has seen significant improvement in some public GEC test sets.
GEC translation	GEC parallel corpus	Learning a GEC translation model from noisy data is a worthy future direction as the GEC parallel corpus is expensive to obtain.
SED	Yahoo Japan Corporation	In order to improve the performance of SED, we propose an SED model taking the learner's proficiency into * Current affiliation: Yahoo Japan Corporation, hiroasan@yahoo-corp.jp † Current affiliation: Future Corporation, mizumoto.tomoya.mh7@is.naist.jp account.
Re-ranking	BEA 2019 Grammatical	TMU Transformer System Using BERT for Re-ranking at BEA 2019 Grammatical Error Correction on Restricted Track
GEC	GEC	There are three main types of neural network models for GEC, namely, recurrent neural networks (), a multi-layer convolutional model based on convolutional neural networks) and a transformer model based on self-attention . We follow the best practices to develop our system based on the transformer model, which has achieved better performance for GEC (.
machine translation task	GEC	Due to the fundamental differences between a "true" machine translation task and the error correction task, previous work has investigated the adaptation of NMT for the task of GEC.
GPT-2 gap filling	WebText	 Table 7: Completion rates broken down per readability  level for the GPT-2 gap filling system pre-trained on  WebText and tested on the Weebit and OSE datasets.  Kendall tau-b correlation is reported as τ .
GPT-2 gap filling	Weebit and OSE datasets	 Table 7: Completion rates broken down per readability  level for the GPT-2 gap filling system pre-trained on  WebText and tested on the Weebit and OSE datasets.  Kendall tau-b correlation is reported as τ .
ASAP	QWK	 Table 5: Results for the essay scoring task for ASAP  sets 1 and 2 reported in QWK.
argument mining tasks	SciDTB corpus	The first set of experiments, described in this section, are aimed at exploring the potential of applying a transfer learning method to improve the performance of argument mining tasks trained with a small corpus of 60 abstracts by leveraging the discourse annotations available in the full SciDTB corpus.
RST	Penn Discourse Treebank, henceforth PDTB	We use common parsers for RST and for Shallow Discourse Parsing (specifically the Penn Discourse Treebank, henceforth PDTB), run them on the microtexts, and first compare the RST output to the gold annotations, in order to assess the prospects of the idea.
predicting argument convincingness	UKP dataset)	An example of our use case and the goal of the model we aim to construct are presented in. formally introduced the task of predicting argument convincingness to the language processing community by providing the first annotated corpus 1 (the UKP dataset), as well as providing initial experimental results on the dataset.
stance detection classifier	Fake News Challenge dataset	We show empirically, using a stance detection classifier built from the Fake News Challenge dataset ( and tested on the RumourEval dataset, that DF-QuAD performs competitively in comparison with a standard stance aggregation method using a credibility-weighted average of stance predictions.
stance detection classifier	RumourEval dataset	We show empirically, using a stance detection classifier built from the Fake News Challenge dataset ( and tested on the RumourEval dataset, that DF-QuAD performs competitively in comparison with a standard stance aggregation method using a credibility-weighted average of stance predictions.
stance detection classifiers	RumourEval dataset	Two datasets are employed as part of this study: the Fake News Challenge dataset 2 , used to train the stance detection classifiers, and the RumourEval dataset 3 , which we adapt for the problem of fake news detection to evaluate our argumentation-based stance aggregation methods.
stance detection classifiers	FNC-1 dataset	We train a number of stance detection classifiers on the FNC-1 dataset, the best of which we use to predict the labels for the RumourEval Task A dataset.
stance detection classifiers	RumourEval Task A dataset	We train a number of stance detection classifiers on the FNC-1 dataset, the best of which we use to predict the labels for the RumourEval Task A dataset.
stance detection classifiers	FNC-1 test set	 Table 3: Precision (P), recall (R), and F1-score (F1) of stance detection classifiers on FNC-1 test set and Ru- mourEval dataset (see Section 5).
stance detection classifiers	Ru- mourEval dataset	 Table 3: Precision (P), recall (R), and F1-score (F1) of stance detection classifiers on FNC-1 test set and Ru- mourEval dataset (see Section 5).
MT	MSA	Therefore, special care was needed to train the MT system for the Gulf dialect because it has far fewer sentences than MSA we needed to duplicate the Gulf data 10 times in order to make the sizes of the data of the Gulf dialect and other dialects comparable.
segmentations	BPE	The segmentations obtained by BPE and SR were also relatively similar with an average edit distance of 5.03.
segmentations	SR	The segmentations obtained by BPE and SR were also relatively similar with an average edit distance of 5.03.
Detecting Binary Semantic Textual Similarity	MSA	Neural Models for Detecting Binary Semantic Textual Similarity for Algerian and MSA
machine translation (MT)	Europarl	Even though machine translation (MT) is one of the popular topics in natural language processing, most of the existing parallel texts include English as one of the languages (e.g., Europarl), Multi-UN ().
Cross-Country adaptation	ArSentD-LEV	 Table 1: Accuracies of linear SVM, NN, DANN and  the proposed approach for Cross-Country adaptation  on ArSentD-LEV. We can see that the proposed vari- ant outperforms other models in almost all DA tasks.
SCAN	GASC-all	On the other hand, SCAN has a higher held-out log-likelihood than GASC-all.
money lender)	French Historical Dictionary	For example, one of the meanings of the word usurier (i. e.; money lender), as reported by the French Historical Dictionary, refers to: the financial activities of the Jews [who since the Middle Ages were], the only ones authorised to lend on pawns.
Question answering (QA)	EHRs	Question answering (QA) systems have the potential to reduce the time it takes for users to access information present in the EHRs.
SRL	ProcessBank dataset	For empirical evaluation, we explore the challenge of SRL in ProcessBank dataset, where the target domain (biological processes) is drastically different compared to the source domain (news).
domain adaptation	ProcessBank dataset	For the domain adaptation scenario, we use the ProcessBank dataset) 2 . We used 134 annotated paragraphs for training, 19 for development and 50 for testing.
IIC	PHMC dataset	It must be noted that IIC involves influenza while the PHMC dataset covers a set of illnesses as described later.
extracting lactation-specific drug information	NLM Drugs and Lactation Database (LactMed)	This paper describes a natural language processing (NLP) approach to extracting lactation-specific drug information from two sources: FDA-mandated drug labels and the NLM Drugs and Lactation Database (LactMed).
answer ranking task	Adopted flag	We are notable to use the corpus for the answer ranking task as we cannot infer the ranking label from the Adopted flag.
Dependency Parsing	GENIA 1.0  corpus	 Table 4: Dependency Parsing results on the GENIA 1.0  corpus converted to dependencies using the Stanford  Universal Dependency Converter. We additionally pro- vide evaluations using Stanford Dependencies(SD) in  order for comparison relative to the results reported in  (Nguyen and Verspoor, 2018).
Dependency Parsing	Stanford  Universal Dependency Converter	 Table 4: Dependency Parsing results on the GENIA 1.0  corpus converted to dependencies using the Stanford  Universal Dependency Converter. We additionally pro- vide evaluations using Stanford Dependencies(SD) in  order for comparison relative to the results reported in  (Nguyen and Verspoor, 2018).
classify sentiment in tweets concerning current affairs	SemEval17-task4A corpus	Our system is firstly trained to classify sentiment in tweets concerning current affairs, using the SemEval17-task4A corpus.
ADR detection	SemEval17-task4A data	Our new ADR detection model firstly trains a classifier on the SemEval17-task4A data, which consists of Tweets on the subject of current affairs.
outcome extraction	PMC article abstracts	In our previous work on outcome extraction, we manually annotated a corpus for reported outcomes comprising 1,940 sentences from the Results and Conclusions sections of PMC article abstracts.
Transfer learning	MedNLI	Transfer learning: We conduct transfer learning on four different combinations of MedNLI, SNLI, and MNLI as it shown in the table 4 (line 4 to 7) and also add the results of general domain tasks (MNLI, SNLI) for comparison.
Transfer learning	MNLI	Transfer learning: We conduct transfer learning on four different combinations of MedNLI, SNLI, and MNLI as it shown in the table 4 (line 4 to 7) and also add the results of general domain tasks (MNLI, SNLI) for comparison.
Statistic of sentence pairs	RQE and  QA datasets	 Table 1: Statistic of sentence pairs in RQE and  QA datasets.
question answering (QA) task	BioNLP 2019	The question answering (QA) task proposed by BioNLP 2019) aims to automatically extract answers to a medical question by using text mining technology.
answer ranking	TREC LiveQA 2017 challenge	employ an attentional encoder-decoder model based on long short-term memory (LSTM) for answer ranking, and their model achieves the best performance of 63.7% average score on the TREC LiveQA 2017 challenge (.
validation	NLM collections	For validation, two sources of questions were used: validated questions from the NLM collections and FAQs retrieved from the National Institutes of Health (NIH) website.
QA task	MER	For the QA task, we obtained higher Spearman's rank correlation values using the entities recognized by MER.
Question Answering	SquAD dataset	Further, a closed-domain Question Answering technique that uses Bi-directional LSTMs trained on the SquAD dataset to determine relevant ranks of answers fora given question is also discussed.
MT	UNdreaMT	The two methods based on just one contextualized word, CT(head) and CT(dep), obtain similar scores to the well-known baselines, mult and add, as well as to the unsupervised MT strategy implemented with UNdreaMT.
TRL	MWE dataset	In the TRL scenario, we overcome the scarcity of data by learning a model on a larger MWE dataset and transferring the knowledge to a resource-poor setting in another language.
Document matching	UN corpus eval- uated	 Table 3: Document matching on the UN corpus eval- uated using P@1. For methods that require sentence  splitting, we report results using both the UN sentence  annotations and an off-the-shelf sentence splitter.
translationese	WMT	If the answer to RQ1 is yes, does this effect of translationese have an impact on WMT's system rankings?
MT	Home documentary	 Table 1: Case-sensitive MT error measures on part 1 of  the Home documentary computed in 3 different modes:
MT	Lucy: The Bean Queen documentary	 Table 2: Case-sensitive MT error measures on part 1 of  the Lucy: The Bean Queen documentary computed as  in Table 1.
Machine Translation (WMT)	ACL 2019 1	The Fourth Conference on Machine Translation (WMT) held at ACL 2019 1 hosts a number of shared tasks on various aspects of machine translation.
neural machine translation (NMT)	Marian 1 toolkit	All our systems are neural machine translation (NMT) systems trained in constrained data conditions with the Marian 1 toolkit . The different language pairs pose very different challenges, due to the characteristics of the languages involved and arguably more importantly, due to the amount of training data available.
phrase-based machine translation (MT)	WMT19	Parallel feature weight decay algorithms (parfda)) is an instance selection tool we use to select training and language model instances to build Moses ( phrase-based machine translation (MT) systems to translate the test sets in the news translation task at WMT19 (.
Transfer learning	WMT	Transfer learning has been found effective in submissions to WMT in previous years:  reported improvements of +2.4 BLEU on the low-resource Estonian→English translation task by transfer learning from Finnish→English.
MT	WMT19	This simple combination method performed the best among unsupervised MT systems at WMT19 by BLEU 1 and human evaluation (.
morphological segmentation	Apertium	Sections 4 and 5 describe respectively morphological segmentation and hybridization with Apertium.
translation	PBSMT baseline	 Table 6: Using different kinds of language models for translation on news-test2018. The PBSMT baseline  gets 26.7 BLEU on English-German and 27.5 BLEU on German-English.
MT	WMT19	This is precisely the motivation for the document-level MT track in this year's WMT19.
IITP-MT	WMT	IITP-MT System for Gujarati-English News Translation Task at WMT 2019
translation	News test sets	Out-ofdomain training data can hurt the translation performance on News test sets () and also significantly increase training time.
MT	TQAutoTest	For the evaluation of the MT outputs the software TQAutoTest () was used.
Robustness Task	WMT15 parallel data	The model that was used as a baseline for the Robustness Task was trained on the WMT15 parallel data.
translation	fairseq 11 toolkit	We compute case-sensitive BLEU scores () for evaluating translation quality . All our implementations are based on the fairseq 11 toolkit.
WMT task	MT Automatic Post-Editing	We present the results from the 5 th round of the WMT task on MT Automatic Post-Editing.
translation	ARC team	However, the translation from the ARC team were considered better than the ones from the OOM team.
translation	OOM team	However, the translation from the ARC team were considered better than the ones from the OOM team.
transference	APE Task organized at WMT 2019	In this paper we present an English-German Automatic Post-Editing (APE) system called transference, submitted to the APE Task organized at WMT 2019.
transference4M	NMT training data	For transference4M, we first train on a training set called eScape4M combined with the first 12k of the provided NMT training data.
transferenceALL	eScape dataset (eScapeAll)	For transferenceALL, we initially train on the complete eScape dataset (eScapeAll) combined with the first 12k of the training data.
MT	MT	We denote the scores for the original (baseline) MT output with MT.
Translation	PBSMT	 Table 4: Translation results of PBSMT systems
Similar Language Translation Task	WMT 2019	In this paper, we present the submission for Similar Language Translation Task in WMT 2019.
Similar Language Translation	WMT	Panlingua-KMI MT System for Similar Language Translation Task at WMT 2019
Panlingua-KMI Machine Translation (MT)	WMT 2019 Shared Task	The present paper enumerates the development of Panlingua-KMI Machine Translation (MT) systems for Hindi ↔ Nepali language pair, designed as part of the Similar Language Translation Task at the WMT 2019 Shared Task.
Similar Language Translation	WMT 2019	In addition to that, the Similar Language Translation was organized for the first time at WMT 2019 with the purpose of evaluating the performance of MT systems on three pairs of similar languages from three different language families: Ibero-Romance, Indo-Aryan, and Slavic.
transference	BPE text	On the decoder side of the transference model we pass only BPE text.
corpus filtering task	YiSi-1	At last year's edition of the corpus filtering task,'s supervised submissions were developed in the same philosophy using anew semantic MT evaluation metric, YiSi-1.
SMT	NMT	The SMT system is implemented using Moses () and the NMT system is built using the FAIRseq (Ott et al., 2019) toolkit.
SMT	FAIRseq (Ott et al., 2019) toolkit	The SMT system is implemented using Moses () and the NMT system is built using the FAIRseq (Ott et al., 2019) toolkit.
SMT	NMT	The organizers publish the official evaluation using both SMT and NMT on the final official test set.
Shared Task	IJCAI-2019	The Shared Task was organized as part of The First Workshop on Financial Technology and Natural Language Processing (FinNLP), collocated with IJCAI-2019.
SBD	Penn Treebank (PTB)	Beyond traditional, well-formed, -edited, and -curated news data, the snowballing of noisy web and social media data since the late 1990s has made SBD much harder: when faced with unstructured user-generated content involving tweets, extremely complex graphemic devices (e.g. new emoji, abbreviations, and acronyms), mark-up, and (up to a point) machine-readable data, traditional (and most offthe-shelf) sentence breakers that were trained on "bare" ASCII data in the Penn Treebank (PTB) simply run out of steam.
Sentence Boundary Detection	FinSBD 2019 shared task	This paper presents the application technique 1 of the pointwise prediction to a shared task of Sentence Boundary Detection in PDF Noisy Text in the Financial Domain for the FinSBD 2019 shared task . We address the sentence boundary detection problem in PDF Noisy Text using a type of approach referred to as "pointwise" prediction.
SBD	WSJ corpus	Although SBD is being researched for almost 20 years, the majority of works focus on structured texts (e.g. WSJ corpus, Brown corpus) and little attention is given to SBD in PDFs.
SBD	Brown corpus	Although SBD is being researched for almost 20 years, the majority of works focus on structured texts (e.g. WSJ corpus, Brown corpus) and little attention is given to SBD in PDFs.
Sentence Boundary Detection (SBD)	FinSBD-2019 shared task	This paper presents two different approaches towards Sentence Boundary Detection (SBD) that were submitted to the FinSBD-2019 shared task.
knowledge enhancement	FinSBD's datasets	For knowledge enhancement , we implement a rule-based validation by extracting a keyword dictionary from the out-of-vocabulary sequences in FinSBD's datasets.
SBD	Brown corpus or Wall Street Journal	However, previous work in SBD mainly dealt with well-formed and clean data such as articles from the Brown corpus or Wall Street Journal.
Detecting sentence boundaries	China Light and Power (CLP) company listed in Hong Kong	Detecting sentence boundaries on the basis of periods/stops may also be less straightforward, for example the presence of company tickers in a document may introduce some difficulties in cleanly identifying sentence boundaries, especially if appended with exchange, for example the full ticker for China Light and Power (CLP) company listed in Hong Kong can be written as '0002.hk'.
Classifying Arabic dialect text	Social Media Arabic Dialect Corpus (SMADC)	Classifying Arabic dialect text in the Social Media Arabic Dialect Corpus (SMADC)
WAM	SMADC	 Table 6: Results of WAM using the dictionaries created  from SMADC.
POS tagging task	PTB	However, many datasets constructed for the POS tagging task are from carefully-edited newswire articles, such as PTB ( and CTB, which are greatly different from social media texts.
SIM	WoZ	 Table 3: Ablation study of SIM on WoZ. We pick the  model with highest joint goal score on development set  and report its performance on test set.
DA tagging	ICSI Meeting Recorder	We obtain competitive performance on two popular DA tagging tasks (SwitchBoard and ICSI Meeting Recorder) and an emotion labeling task (IEMO-CAP).
slot filling annotation	ATIS (Airline Travel Information System) dataset	Among these resources, we mention PropBank () and FrameNet (, which consist of annotated documents with verb and frame-based semantic roles, respectively; CoNLL 2003) and OntoNotes (, which provide named entity information; and Abstract Meaning Representation (AMR) (, which provides a graph-based seman-: An example of slot filling annotation from the ATIS (Airline Travel Information System) dataset and author-annotated NER and SemTag in IOB format.
MTL	NER	In our MTL configuration, the target task (T T ) is slot filling, and the auxiliary tasks (T S ) are set to NER or SemTag or both.
coreference resolution	CALOR	Performance on out-of-context questions was evaluated on Bench'It, a dataset containing 150 open ended questions about general knowledge in French) 4 . The system reached a macro precision, recall and F-1 of 64.14%, 64.33% and 63.46% respectively 5 . We also evaluated the coreference resolution model on the test-set of CALOR, obtaining an average precision, recall and F-1 of 65.59%, 48.86% and 55.77% respectively.
SFNs	MultiWOZ dataset	This paper will show that SFNs obtain strong results on the MultiWOZ dataset () both with and without the use of reinforcement learning.
answer utterance selection	FriendsQA	BERT in particular depicts promising results, an accuracy of 74.2% for answer utterance selection and an F1-score of 64.2% for answer span selection, suggesting that the FriendsQA task is hard yet has a great potential of elevating QA research on multiparty dialogue to another level.
answer utterance selection	FriendsQA dataset	Two tasks are experimented, answer utterance selection and answer span selection, with the FriendsQA dataset.
DA classification	FFNNs	 Table 5: DA classification results for FFNNs with dif- ferent types of embeddings
relation prediction	PDTB	 Table 3: Scores of the systems for relation prediction, using the full relation set of the PDTB. The predicted  relations are either inferred from the predicted primitives ("Primitives"), or directly predicted ("Relations").We  report hierarchical recall (h-R) and hierarchical precision (h-P), along with max-h-P max-h-R, and accuracy.
MT evaluation	UGC data	Because of the lack of manually translated UGC, we consider a out-domain scenario in which our systems are trained on the canonical corpora generally used in MT evaluation campaigns and tested on UGC data.
MT	MTNT	 Table 5: BLEU score results for our three models for the different train-test combinations. All the MT  predictions have been treated to replace UNK tokens according to Section 3.3.2. The best result for each  test set is marked in bold, best result for each system (row-wise) in blue color and score for in-domain  test sets with a dag. 'Crap', 'MTNT', 'News' and 'Open' stand, respectively, for the Cr#pbank, MTNT,  newstest'14 and OpenSubtitlesTest test sets.
MT	OpenSubtitlesTest test sets	 Table 5: BLEU score results for our three models for the different train-test combinations. All the MT  predictions have been treated to replace UNK tokens according to Section 3.3.2. The best result for each  test set is marked in bold, best result for each system (row-wise) in blue color and score for in-domain  test sets with a dag. 'Crap', 'MTNT', 'News' and 'Open' stand, respectively, for the Cr#pbank, MTNT,  newstest'14 and OpenSubtitlesTest test sets.
MT	Cr#pbank corpus	 Table 6: Noise added by the MT system estimated  with the TSNR metric for the Cr#pbank corpus,  the lower the better.
synonym retrieval	Word2Vec/CBOW	 Table 5: Results on synonym retrieval; Models  were trained using Word2Vec/CBOW on lemma- tized CNC
MWE identification task	PARSEME ST1.1 data sets	The contributions of our work are: • anew state-of-the art for the MWE identification task on the PARSEME ST1.1 data sets.
SMO	Weka	All the models built with SMO are based on Weka's standard parameters.
Veracity prediction	Danish-language DAST	 Table 6: Veracity prediction from stance only,  training on English/German PHEME rumour dis- cussions and testing on Danish-language DAST.
NER	Nynorsk	The study also presents the first NER for Nynorsk.
Named Entity Recognition	French Legal Texts	May I Check Again? -A simple but efficient way to generate and use contextual dictionaries for Named Entity Recognition. Application to French Legal Texts
zero-shot transfer	English CoNLL data	It is noteworthy that zero-shot transfer benefits only to a limiting degree from more source data (F1 increases by 3% when training on all English CoNLL data).
stemming	Beetle	In combination with stemming, they work well for SEB, but not at all for Beetle and not optimally for ASAP.
ASAP	Beetle	Incidentally, this is the optimal parametrization for ASAP, and causes only a small drop in τ for Beetle and SEB (see the bottom line in).
Summarization evaluation	DUC 2004	These parameters hardly differ from the most commonly used settings in Summarization evaluation (i.e. as used in DUC 2004).
classification tasks	SEB	We report F scores as the standard measure for classification tasks and in accordance with previous work for SEB, Beetle and CSSAG (.
classification tasks	Beetle	We report F scores as the standard measure for classification tasks and in accordance with previous work for SEB, Beetle and CSSAG (.
classification tasks	CSSAG	We report F scores as the standard measure for classification tasks and in accordance with previous work for SEB, Beetle and CSSAG (.
SAG	ASAP	8 shows evidence of the high unigram baseline for SAG at at least F=59.7 (RF on SEB; F=65.1 SVM) and up to F=86.7 (RF on ASAP).
SST-2 sentiment analysis	GLUE benchmark	For the experiments we use the popular SST-2 sentiment analysis dataset which is part of the GLUE benchmark and anew dataset of manually labeled financial newspaper headlines.
ASR	Kaldibased (Povey et al., 2011) system build	Our ASR system follows normal pattern for Kaldibased (Povey et al., 2011) system build.
translation	IWSLT	Despite the poor translation quality, exploring the Transformer is still interesting because of its reduced training time), which is reduced by a factor of 2 on IWSLT (67K vs 112K seconds) and even more on Librispeech (101K vs 248K seconds).
translation	Librispeech	Despite the poor translation quality, exploring the Transformer is still interesting because of its reduced training time), which is reduced by a factor of 2 on IWSLT (67K vs 112K seconds) and even more on Librispeech (101K vs 248K seconds).
MT	Word	What is the impact of raw MT on Japanese users of Word: preliminary results of a usability study using eye-tracking
MT	ModernMT	MAGMATic is released under a Creative Commons Attribution -Non CommercialShare Alike 4.0 International license (CC BY-NC-SA 4.0), and is freely downloadable at: https://ict.fbk.eu/magmatic/ In the remainder of this paper we describe MAGMATic and illustrate its potential by using it to evaluate two state-of-the-art MT systems (Google Translate and ModernMT), both in terms of overall performance and focusing on their ability to translate domain-specific terminology.
MT	PE2RR	The main differences between the two data sets are (i) post-edited MT hypotheses are available in PE2RR (and standard reference translations in TERRA), (ii) manual error annotation in PE2RR is based on correcting automatically assigned labels whereas in TERRA it is performed from scratch.
MT	TERRA	The main differences between the two data sets are (i) post-edited MT hypotheses are available in PE2RR (and standard reference translations in TERRA), (ii) manual error annotation in PE2RR is based on correcting automatically assigned labels whereas in TERRA it is performed from scratch.
MT	PE2RR	The main differences between the two data sets are (i) post-edited MT hypotheses are available in PE2RR (and standard reference translations in TERRA), (ii) manual error annotation in PE2RR is based on correcting automatically assigned labels whereas in TERRA it is performed from scratch.
MT	TERRA	The main differences between the two data sets are (i) post-edited MT hypotheses are available in PE2RR (and standard reference translations in TERRA), (ii) manual error annotation in PE2RR is based on correcting automatically assigned labels whereas in TERRA it is performed from scratch.
ASL	Lifeprint.com	ASL is a visually perceived language based on a naturally evolved system of articulated hand gestures and their placement relative to the body, along with non-manual markers such as facial expressions, head movements, shoulder raises, mouth morphemes, and movements of the body (ASL: A brief description -Lifeprint.com).
NMT	Penn Arabic Treebank (ATB)	However, it remains unclear if such techniques are well suited for NMT, where language-agnostic tokenizations, e.g. byte-pair encoding (BPE) (, are widely used. has looked into Arabic SMT and NMT, achieving the highest accuracy using the Penn Arabic Treebank (ATB) tokenization, with 51.2 and 49.7 BLEU points for SMT and NMT, respectively.
SMT	NMT	 Table 1: Comparing Raw, ATB and D3 Tokenized cases without/with BPE on in-domain test (MT05), in terms of BLEU  scores, where the Confidence Interval (CI) and P -value are reported. Bold font highlights best results by SMT and NMT.
SMT	MT05	 Table 2: BLEU score of the length-based system selection (using best models of SMT and NMT) when applied on in-domain  test (MT05).
SMT	MT12	 Table 3: BLEU score of the length-based system selection (using best models of SMT and NMT) when applied on out-of- domain test (MT12).
MT	Europarl	To test our hypothesis we built three types of MT systems and analysed their output for two language pairs on Europarl ( To draw more general conclusions on the effects of bias propagation and loss of lexical richness, we assessed output from seen (during training) and unseen data.
MT	TAUS DQF	The quality of the raw MT output was evaluated by the students based on the MQM error typology and the TAUS DQF.
translation	TED	In addition to translation quality, we have also examined the subtitling character limitation and verified that the number of segments in both raw MT and pre-edited MT output that violate the 21-CPS rule guideline by TED was almost none.
MT	PBSTM	Errors in MT output for Russian as a target language show almost equal shares of fluency and accuracy errors in PBSTM and prevalence of the accuracy errors in NMT.
MT	NMT	Errors in MT output for Russian as a target language show almost equal shares of fluency and accuracy errors in PBSTM and prevalence of the accuracy errors in NMT.
MT	Smartcat	The errors in the MT output are considered as a baseline for analysis of errors on Smartcat.
machine translation	NIST	The typical evaluation of machine translation, due to a requirement of fast, automatic metrics during the training phase, typically involves the comparison with a set of human translation in what is calculated as the BLEU or the NIST scores of the translation ().
MT	UN Parallel Corpus 1.0	We also compared pivot MT systems using UN Parallel Corpus 1.0 and participated in the China Workshop on Machine Translation (CWMT) shared task on pivot MT (Liu, Silva,.
RBMT	Lucy LT	Notable examples of RBMT systems are the original, rule-based Systran, Lucy LT ( and).
machine translation (MT) of software	Microsoft Office product range	We report on a model for machine translation (MT) of software, without review, for the Microsoft Office product range.
machine translation (MT)	Prompsit	The evaluation was performed by the ADAPT Centre at Dublin City University (DCU) on 34 state-of-theart domain-adapted machine translation (MT) systems built by four leading MT companies KantanMT, Pangeanic, Prompsit and Tilde.
machine translation (MT)	Tilde	The evaluation was performed by the ADAPT Centre at Dublin City University (DCU) on 34 state-of-theart domain-adapted machine translation (MT) systems built by four leading MT companies KantanMT, Pangeanic, Prompsit and Tilde.
MT	Tilde	The four MT companies involved in this study are presented in Sections 2 (KantanMT), 3 (Pangeanic), 4 (Prompsit), and 5 (Tilde), with a description of the systems that they developed, including the data that they used and how they customized their engines.
MT	GNMT	Tilde's MT systems score higher than GNMT inmost cases, with Latvian-English and English-Latvian being the only the engines that do not outperform the baseline (by a statistical significant amount for Latvian↔English).
MT	ParaCrawl pipeline	Collecting domain specific data for MT: an evaluation of the ParaCrawl pipeline
Translation and Interpreting	Geneva University Hospitals (HUG)	BabelDr 1 is a joint project between the Faculty of Translation and Interpreting of the University of Geneva and Geneva University Hospitals (HUG) (.
translation task	WMT'13	Since we lacked an in-domain corpus, we trained a general system with the data from the translation task from WMT'13 (, consisting in 15 million parallel segments.
MT	LoResMT	Shared Tasks on MT for Bhojpuri, Magahi, Sindhi and Latvian (to and from English) is also organized under LoResMT,
translation	NMT	In this paper we focus on the translation of the English-Irish language pair using NMT.
Knowledge Transfer Partnership (KTP)	Welsh Language Service Provider , Cymen Cyf 1	This paper reports on a Knowledge Transfer Partnership (KTP) project that aimed to implement machine translation technology at a Welsh Language Service Provider , Cymen Cyf 1.
Adapting Term Recognition	Irish	Adapting Term Recognition to an Under-Resourced Language: the Case of Irish
tokenizing the Würzburg	Thesaurus Palaeohibernicus (TPH)	Section two of this paper will detail the difficulties associated with tokenizing the Würzburg glosses as they appear in Thesaurus Palaeohibernicus (TPH), and of tokenizing Old Irish text more generally.
Machine Translation (MT) post-editing	CRITT Translation Process Research Database (CRITT TPR-DB	In this regard, this paper seeks to empirically study the cognitive effort of translating and Machine Translation (MT) post-editing in relation to different predictor variables including word frequency, word translation entropy, and syntactic choice entropy, making use of a large dataset from the CRITT Translation Process Research Database (CRITT TPR-DB, see Carl et al., 2016) which incorporates multiple languages and translation production modes.
PE	GNMT	In addition to the measuring of the PE effort and assessing fluency and adequacy, we also compared GNMT, the SPOOK and the Novel NMT models based on an error analysis.
PE	SPOOK	In addition to the measuring of the PE effort and assessing fluency and adequacy, we also compared GNMT, the SPOOK and the Novel NMT models based on an error analysis.
Detecting the fake news before they were even written	Qatar Computing Research Institute (QCRI)	The first one, entitled "Detecting the fake news before they were even written", will be presented by Dr. Preslav Nakov from Qatar Computing Research Institute (QCRI), Qatar.
summarizer	EASC corpus	The proposed summarizer is evaluated using EASC corpus and benchmarked against popular state of the art Arabic text summariza-tion systems.
predicting AV	Modern Standard Arabic (MSA)	In this paper I examine the effects of using a small corpus for training or testing documents on the accuracy of predicting AV in different domains in Modern Standard Arabic (MSA) through a number of AV experiments.
Voicing Contrast	Greek Children	Production of the Voicing Contrast by Greek Children with Cochlear Implants
summary generation process	SumSAT	The evaluation of the summary generation process by point of view consists of the evaluation of the discursive annotation task made by SumSAT.
WB tagger	Chinese SUD treebanks	Therefore, we have prepared our training data as following: for WB tagger and parser training, we extract the last 10% of the four former mentioned Chinese SUD treebanks to serve as the testing set and the developing set, and we combine the rest 90% to serve as a training set; for CB tagger and parser training, we carryout the exact same arrangement, except this time all the treebanks are converted from word level to character level.
tagger	WB	Concerning the tagger, we compare the F-score of the tagger trained on WB and on CB.
WB tagger	Chinese SUD treebanks	Therefore, we have prepared our training data as following: for WB tagger and parser training, we extract the last 10% of the four former mentioned Chinese SUD treebanks to serve as the testing set and the developing set, and we combine the rest 90% to serve as a training set; for CB tagger and parser training, we carryout the exact same arrangement, except this time all the treebanks are converted from word level to character level.
tagger	WB	Concerning the tagger, we compare the F-score of the tagger trained on WB and on CB.
parsing	UD data	The experiments on using a larger dataset for training the morphosyntactic tagging and lemmatization models, for which we expect to have a positive impact on the parsing quality, are split into two main parts: (1) training and evaluating morphosyntactic tagging and lemmatization models on the UD data and on all the available data, and (2) applying both models as pre-processing for training and evaluating models for dependency parsing.
parsing	IMST-UD Treebank	In order to observe the effect of the changes on the parsing performance of the IMST-UD Treebank, a transition-based LSTM dependency parser (, which is a morphologically enhanced version of  and a state-of-the-art graph-based neural parser () are trained on the previous and updated versions of the treebank separately.
parsing	UD data	The experiments on using a larger dataset for training the morphosyntactic tagging and lemmatization models, for which we expect to have a positive impact on the parsing quality, are split into two main parts: (1) training and evaluating morphosyntactic tagging and lemmatization models on the UD data and on all the available data, and (2) applying both models as pre-processing for training and evaluating models for dependency parsing.
Improving	Turkish Universal Dependency Treebank	Improving the Annotations in the Turkish Universal Dependency Treebank
parsing	IMST-UD Treebank	In order to observe the effect of the changes on the parsing performance of the IMST-UD Treebank, a transition-based LSTM dependency parser (, which is a morphologically enhanced version of and a state-of-the-art graph-based neural parser () are trained on the previous and updated versions of the treebank separately.
IE	MUC standards	Our proposal for three levels of IE is modelled after the MUC standards using MUC-style representation.
IE	MUC-style	Our proposal for three levels of IE is modelled after the MUC standards using MUC-style representation.
Bagging	Treebank	 Table 1: Bagging the Treebank
Simulating Speech Recognizer	Word	 Table 4: Simulating Speech Recognizer: Word
Simulating Speech Recognizer	Word	 Table 5: Simulating Speech Recognizer: Word
parsing	Penn tree-bank style parse trees	We present anew parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established [5,9,10,15,17] "stan-dard" sections of the Wall Street Journal tree-bank.
parsing	Wall Street Journal tree-bank	We present anew parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established [5,9,10,15,17] "stan-dard" sections of the Wall Street Journal tree-bank.
predicting problematic dialogues	HMIHY corpus	We train an automatic classifer for predicting problematic dialogues from features that can be automatically extracted from the HMIHY corpus.
parsing	UPenn	Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.
Information retrieval	MUC	Information retrieval applications specialising in describing events, as with a number of the MUC applications, could greatly benefit from some of these in determining the where-when-why of things.
SBD task	Brown Corpus	There are two corpora normally used for evaluation and development in a number of text processing tasks and in the SBD task in particular: the Brown Corpus and the Wall Street Journal (WSJ) corpus -both part of the Penn Treebank.
SBD task	Wall Street Journal (WSJ) corpus	There are two corpora normally used for evaluation and development in a number of text processing tasks and in the SBD task in particular: the Brown Corpus and the Wall Street Journal (WSJ) corpus -both part of the Penn Treebank.
SBD task	Penn Treebank	There are two corpora normally used for evaluation and development in a number of text processing tasks and in the SBD task in particular: the Brown Corpus and the Wall Street Journal (WSJ) corpus -both part of the Penn Treebank.
sentence breaking punctuation	WSJ	On the sentence breaking punctuation the tagger performed extremely wellan error rate of 0.39% on the WSJ and 0.25% on the Brown Corpus.
sentence breaking punctuation	Brown Corpus	On the sentence breaking punctuation the tagger performed extremely wellan error rate of 0.39% on the WSJ and 0.25% on the Brown Corpus.
WSD	Brown Corpus	Some WSD evaluations have been done using the Brown Corpus as training and testing resources and comparing the results against SemCor 3, the sense-tagged version of the Brown Corpus (Agirre and).
WSD	Brown Corpus	Some WSD evaluations have been done using the Brown Corpus as training and testing resources and comparing the results against SemCor 3, the sense-tagged version of the Brown Corpus (Agirre and).
DSMS	EUFID	PROBLEMS IN NATURAL-LANGUAGE INTERFACE TO DSMS WITH EXAMPLES FROM EUFID
parsing	EPISTLE	This paper first reviews parsing in EPISTLE, and then describes the fitting procedure, followed by several examples of its application.
MRDs	ACQUILEX project, ESPRIT BRA 3030	However, to maximize the utility of MRDs in *The research reported in this paper was carried out at the Computer Laboratory (University of Cambridge) as part of an ongoing study on the acquisition of lexical knowledge from machine-readable dictionaries within the ACQUILEX project, ESPRIT BRA 3030.
ontology building	EDR 1990	The word sense view of ontology building leads to proliferation of concepts whenever words in different languages do not "line-up" (see EDR 1990), while using a core set of "primitives" is limited for large-scale applications, if shades of meaning are to be captured.
clustering	ATIS domain	As evidence for these claims, I present results showing that clustering improves some models but not others for the ATIS domain.
MT	Pangloss MT project	Using this novel approach to MT in the latest version of the Pangloss MT project, we submit an input text to a battery of machine translation systems (engines), collect their (possibly, incomplete) results in a joint chart data structure and select the overall best translation using a set of simple heuristics.
translations	VERBMOBIL	The translations are as the deep processing line of VERBMOBIL provides them.
MT	KANT	Other MT systems, including the KANT system, seethe advantage of this.
Corpora size	ENAMEX phrases	 Table 4: Corpora size by ENAMEX phrases.
IR	TREC (Hatman 96)	A special NLP track emphasizing the evaluation of NLP techniques for IR is currently held in the context of TREC (Hatman 96).
translation lexicon acquisition	SABLE system	In the remainder of the paper we briefly outline translation lexicon acquisition in the SABLE system, describe its application to a corpus of technical documentation, and provide a quantitative assessment of its performance.
summarizes data extracted	NameTag TM	It organizes, analyzes, and summarizes data extracted by NameTag TM and any Full-Text search engine.
coreference resolution	MUC-7	The demo will feature aspects of the system currently being used to develop a coreference resolution engine in preparation for MUC-7, in addition to an information extraction task done over the summer of 1996.
CRF	ReLIE	Finally, we show how the accuracy of CRF can be improved by using features extracted by ReLIE.
Pronoun resolution	MUC	 Table 4: Pronoun resolution accuracy on nouns in current  or previous sentence in MUC.
TATs	NIST MT 2003	 Table 1: Statistical information of TATs filtered by test sets of NIST MT 2003 and 2005.
SMT	NIST Open MT Evaluation	Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation.
summaries	DUC	We carried out automatic evaluation of our summaries using ROUGE) toolkit, which has been widely adopted by DUC for automatic summarization evaluation.
citation analysis	ACL Anthology Network	This resource has already been the basis of citation analysis work, for example, in the ACL Anthology Network of.
Mention detection system	ACE 2007 data sets	Mention detection system experiments are conducted on the ACE 2007 data sets in Arabic and English.
LU induction	WordNet	The aim of our study is to pioneer in this field by proposing two unsupervised models for LU induction, one based on distributional techniques and one using WordNet as a support; and a combined model which mixes the two.
SMT	Europarl corpus	In addition, we performed an experimental comparison in regular SMT with the Europarl corpus, and found that the performance difference was negligible.
POS tagging	PTB dataset	The bidirectional tagger obtained the best results in literature on POS tagging on the standard PTB dataset.
alignment	Hansard	Experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as "Hansard".
dependency parsing	Penn Treebank	In line with previous work on dependency parsing using the Penn Treebank, we focus on projective dependency parsing.
Adsorption extractions	A8	 Table 4: Precision of top 100 Adsorption extractions (for  five classes) which were not present in A8.
SMT	NIST	As mentioned earlier, these three SMT systems have very competitive performance and are ranked among top 2 systems participating to NIST or TC-STAR evaluations.
Phrase Translation	ITG	Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective
coreference	MUC-6 dataset	 Table 1: Comparison of coreference results in MUC  scores on the MUC-6 dataset.
coreference	ACE-2004 (English) datasets	 Table 2: Comparison of coreference results in MUC  scores on the ACE-2004 (English) datasets.
coreference	ACE-2 datasets	 Table 3: Comparison of coreference results in MUC  scores on the ACE-2 datasets.
coreference	ACE-2 datasets	 Table 4: Comparison of coreference results in B 3 scores  on the ACE-2 datasets.
NLI alignment task	Recognizing Textual Entailment data	We compare the performance of MANLI to existing NLI and MT aligners on an NLI alignment task over the well-known Recognizing Textual Entailment data.
speech recognition	HUB-4 test set	Experimental results reveal that this unified framework is an effective approach which significantly improves the performance of speech recognition with a 9.9% relative reduction of character error rate on the HUB-4 test set, a widely used Mandarin speech recognition task.
adaption	HUB4 training set	The adaption data was drawn from the HUB4 training set, excluding the HUB-4 develop-  ing set, where only the cleaned male speech data (data under condition f0 defined as) was used.
adaption	HUB-4 develop-  ing set	The adaption data was drawn from the HUB4 training set, excluding the HUB-4 develop-  ing set, where only the cleaned male speech data (data under condition f0 defined as) was used.
speech recognition	People's Daily of China	In addition, the analyzer incorporated in speech recognition was trained with a larger corpus from People's Daily of China, including the data in 1998 from January to June and the data in 2000 from January to November (annotated by the Institute of Computational Linguistics of Peking University).
MT	Linguistic Data Consortium (LDC)	To examine this, we created confusion networks out of the translations of the four best MT systems of q q q q q q q q q q q q q q q the Chinese-English evaluation campaigns, as available from the Linguistic Data Consortium (LDC).
parsing	Chinese treebank	All the data used to train the joint parsing model and to evaluate parsing performance were taken from articles 1-325 of the Chinese treebank, which all have English translations with gold-standard parse trees.
parsing	Chinese treebank data	We also note that since all parsing evaluations were performed on Chinese treebank data, the Chinese test sentences were in-domain, whereas the English sentences were very far out-of-domain for the Penn Treebank-trained baseline English parser.
parsing	Penn Treebank-trained baseline English parser	We also note that since all parsing evaluations were performed on Chinese treebank data, the Chinese test sentences were in-domain, whereas the English sentences were very far out-of-domain for the Penn Treebank-trained baseline English parser.
Development	WSJ section 24)	2. Development (WSJ section 24).
Question classification	UIUC dataset	 Table 1: Question classification accuracy of SVM and  ME using individual feature sets for 6 and 50 classes over  UIUC dataset
SEAL	AQUAINT newswire corpus	In both phases, the answers found by SEAL were retrieved from the Web instead of the AQUAINT newswire corpus used in the TREC evaluations.
dialog management	RavenClaw	In recent dialog management frameworks, such as RavenClaw ( and Collagen (), domain-dependent components of a dialog manager are clearly separated from domain-independent components.
WSD	WordNet	In current WSD research, WordNet is usually used as the sense inventory.
WSD	SEMCOR	Through our experiments, we then highlight that domain adaptation for WSD is an important issue as it substantially affects the performance of a state-of-theart WSD system which is trained on SEMCOR but evaluated on sense-tagged examples in OntoNotes.
WSD	OntoNotes	Through our experiments, we then highlight that domain adaptation for WSD is an important issue as it substantially affects the performance of a state-of-theart WSD system which is trained on SEMCOR but evaluated on sense-tagged examples in OntoNotes.
WSD	OntoNotes evaluation data	In Section 4, we investigate the WSD performance when we train our system on examples that are gathered from a different domain as compared to the OntoNotes evaluation data.
SRL	PropBank	In the recent years, most works on SRL, including two CoNLL shared task in, focus on verbal predicates with the availability of PropBank ( ).
MT	NIST	For both Arabic-to-English and Chinese-toEnglish MT, we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets.
MT	MT02-05	For both Arabic-to-English and Chinese-toEnglish MT, we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets.
MT	MT06	For both Arabic-to-English and Chinese-toEnglish MT, we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets.
MT	MT08 newswire sets	For both Arabic-to-English and Chinese-toEnglish MT, we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets.
MT	MT06	For Arabic-to-English MT, the LBL+LEN+CLM system improved lower-cased BLEU by 2.0 on MT06 and 1.7 on MT08 on decoding output.
MT	MT08	For Arabic-to-English MT, the LBL+LEN+CLM system improved lower-cased BLEU by 2.0 on MT06 and 1.7 on MT08 on decoding output.
textual entailment recognition	HPSG	Yet, some NLP tasks such as textual entailment recognition () and some linguistic theories such as HPSG ( require more general graphs and, then, more general algorithms for computing similarity among graphs.
CoNLL 2005 shared task on Semantic Role Labeling (SRL)	TREC 10 QA competition	We evaluated the capability of our model to extract relevant features on two data sets: the CoNLL 2005 shared task on Semantic Role Labeling (SRL), and the Question Classification (QC) task based on data from the TREC 10 QA competition).
Question Classification (QC) task	TREC 10 QA competition	We evaluated the capability of our model to extract relevant features on two data sets: the CoNLL 2005 shared task on Semantic Role Labeling (SRL), and the Question Classification (QC) task based on data from the TREC 10 QA competition).
citation extraction task	DBLP database records	We evaluate our method on a citation extraction task in which alignments between DBLP database records and citation texts are used to train an extractor.
NER task	MUC workshops	The NER task arose in the context of the MUC workshops, as small chunks which could be identified by finite state models or gazetteers.
sentiment classification	Pang	There are many research directions, e.g., sentiment classification (classifying an opinion document as positive or negative) (e.g., Pang,), subjectivity classification (determining whether a sentence is subjective or objective, and its associated opinion) (), feature/topic-based sentiment analysis (assigning positive or negative sentiments to topics or product features) (;.
entity extraction	Wikipedia	In this paper, we show large gains in entity extraction by combining state-of-the-art distributional and patternbased systems with a large set of features from a 600 million document webcrawl, one year of query logs, and a snapshot of Wikipedia.
Recognizing Implicit Discourse Relations	Penn Discourse Treebank	Recognizing Implicit Discourse Relations in the Penn Discourse Treebank
question answering	TREC corpus	With our accurate question classifier and some standard question answer features , our question answering system performs close to the state of the art using TREC corpus.
question classification	UIUC question dataset	Section 4 presents the question classification accuracy over UIUC question dataset.
Question classification	UIUC split	 Table 3: Question classification accuracy by re- moving one feature at a time for 6 and 50 classes  over UIUC split.  6 class 50 class  overall  93.6  89.0  -wh-word  93.6  89.0  -head word  92.8  88.2  -hypernym  90.8  84.2  -unigram  93.6  86.8  -word shape  93.0  88.4
dependency parsing	Penn Treebank	We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for En-glish, and the Prague Dependency Tree-bank for Czech.
dependency parsing	Prague Dependency Tree-bank	We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for En-glish, and the Prague Dependency Tree-bank for Czech.
parsing	McNemar's Test	The improvements of parsing with subtree-based features were significant in McNemar's Test (p < 10 −6 ).
sentiment classification	movie review dataset	We use five sentiment classification datasets, including the widely-used movie review dataset () as well as four datasets containing reviews of four different types of products from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (.
SMT	United Nations and newswire data	For example, an SMT system applied to broadcast conversational data maybe trained on a corpus consisting mostly of United Nations and newswire data, with only a very small amount of in-domain broadcast news/conversational data.
semantic distance	WordNet	These automatic approaches to semantic distance rely on manually created lexical resources such as WordNet, large amounts of text corpora, or both.
parsing	WSJ training set	effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set, but they used a two-stage parser comprised of Charniak's lexicalized probabilistic parser with n-best parsing and a discriminative reranking parser), and thus it would be better categorized as "co-training".
parsing	Modern Hebrew treebank	Firstly, RR models significantly outperform HD models (about 2 points absolute improvement in F 1 ) in parsing the Modern Hebrew treebank.
metonymy resolution task	SemEval 2007	The method developed is applied to the metonymy resolution task from SemEval 2007.
coreference	John Smith"	The relevance of a context for coreference also depends on the corpus, not only on the specific relationship that exists between "John Smith" and "works for U.N.".
Machine Translation	MEDIA	In (, it is compared with other four models (Stochastic Finite State Transducers, Support Vector Machines, Machine Translation, PositionalBased Log-linear model) and it is by far the best on MEDIA.
parsing	HPSG parser Enju	In this sentence, the former Error: ARG1  We applied our approaches to parsing errors given by the HPSG parser Enju, which was trained on the Penn Treebank () section 2-21.
parsing	Penn Treebank () section 2-21	In this sentence, the former Error: ARG1  We applied our approaches to parsing errors given by the HPSG parser Enju, which was trained on the Penn Treebank () section 2-21.
entailment acquisition	Shinyama 1 Verb entailment	Note that previous researches on entailment acquisition focused on templates with variables or word-lattices (; Shinyama 1 Verb entailment pairs are described as v1 → v2 (v1 is the entailing verb and v2 is the entailed one) henceforth.
coreference resolution	WordNet	There have been considerable attempts to incorporate semantic knowledge into coreference resolution systems: different knowledge sources such as WordNet and Wikipedia have been used to boost the performance.
SC classification	ACE2 datasets	 Table 3: SC classification accuracy of ME using  individual feature sets for development and test  ACE2 datasets.
SC classification	ACE2  datasets	 Table 4: SC classification accuracy of ME using  incremental feature sets for training and test ACE2  datasets.
SC classification	ACE2 datasets	 Table 5: SC classification accuracy of ME by re- moving one feature at a time for training and test  ACE2 datasets.
SRL	PropBank	During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (), PropBank ( , and the consecutive CoNLL shared tasks () in English language.
SRL	PropBank	During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (), PropBank ( , and the consecutive CoNLL shared tasks () in English language.
tokenize newline breaks	CRR07	However, we did not tokenize newline breaks, as CRR07 did, which might be useful in determining sentence boundaries.
parsing	HowNet	The semantic resource we used to improve parsing was HowNet, which has been introduced in Subsection 3.1.
IR	WordNet	As the vocabulary coverage of knowledge bases is a crucial factor for being effective in IR, we compare the coverage of Wikipedia, Wiktionary and WordNet.
SRL	CoNLL-2004 shared task	In English SRL research, there have been some attempts at relaxing the necessity of using full syntactic parses; better understanding of SRL with shallow parsing is achieved by CoNLL-2004 shared task).
SRL	CoNLL-2004 shared task	In English SRL research, there have been some attempts at relaxing the necessity of using full syntactic parses; better understanding of SRL with shallow parsing is achieved by CoNLL-2004 shared task).
SRL	Chinese Proposition Bank (CPB)	To date, the best SRL performance reported on the Chinese Proposition Bank (CPB) corresponds to a F-measure is 92.0, when using the handcrafted parse trees from Chinese Penn Treebank (CTB).
SRL	Chinese Penn Treebank (CTB)	To date, the best SRL performance reported on the Chinese Proposition Bank (CPB) corresponds to a F-measure is 92.0, when using the handcrafted parse trees from Chinese Penn Treebank (CTB).
SRL	CPB 2	We present encouraging SRL results on CPB 2 . The best F-measure performance (74.12) with gold segmentation and POS tagging can be achieved by the first method.
parsing re-ranking	Collins 2000;)	In addition, parsing re-ranking (Collins 2000;) has also been shown to be another effective technique to improve parsing performance.
parsing	Penn Treebank WSJ corpus	Using a linear-chain conditional random field, we improve parsing accuracy over the gen-erative baseline parser on the Penn Treebank WSJ corpus, rivalling a similar model that does not make use of context.
SVM classification	SVMLight	For SVM classification and regression, we used SVMLight, and for NB we used.
text classification	Reuters-21578 collection	We used two well-known benchmark data collections for text classification, the Reuters-21578 collection and the 20 Newsgroup collection 4 . Reuters-21578 contains 21578 documents.
text classification	20 Newsgroup collection	We used two well-known benchmark data collections for text classification, the Reuters-21578 collection and the 20 Newsgroup collection 4 . Reuters-21578 contains 21578 documents.
tagger	WSJ PennTreebank () annotated	The tagger was trained on sections 2-21 of the WSJ PennTreebank () annotated with gold standard POS tags.
relation classification	ACE RDC 2004 corpus	For easy comparison with related work, we only evaluate the relation classification task on the 7 major relation types of the ACE RDC 2004 corpus.
headline generation task	DUC-03 (Task 1) corpus	For the headline generation task, the full DUC-03 (Task 1) corpus was used for training; it contains 500 documents and 4 headline-style summaries per document.
headline generation	DUC-04 corpus	Title generation For the headline generation task, we evaluated our model on a testing partition from the DUC-04 corpus (75 documents, Task 1).
SMT	NIST 2009 MT evaluation	To establish strong baselines, we used a string-totree SMT system, one of the top performing systems in the NIST 2009 MT evaluation, and trained it with very large amounts of parallel and language model data.
MT	Arabic weblog	 Table 3: MT results on Arabic weblog (see footnote 2).
MT	Chinese newswire	 Table 4: MT results on Chinese newswire (see footnote  2).
MT	Chinese weblog	 Table 5: MT results on Chinese weblog (see footnote 2).
parsing	Penn treebank test sets	It is a Penn treebank ( parser in that it is capable of parsing the Penn treebank test sets, and is trained on the now standard training set.
scope learning	BioScope corpus	However, scope learning has been largely ignored until the recent release of the BioScope corpus (, where negation/speculation cues and their scopes are annotated explicitly. and pioneered the research on scope learning by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a cue.
negation and speculation scope learning	BioScope corpus	We have evaluated our simplified shallow semantic parsing approach to negation and speculation scope learning on the BioScope corpus.
scope identification	GTB1.0	 Table 6: Accuracy (%) of scope identification on the  three subcorpora using automatic parser trained on  6,691 sentences in GTB1.0
SRL	Clark and Curran CCG  parser	 Table 2: SRL performance on the development set (section 00)  for various values of n. The final row indicates SRL perfor- mance on section 00 parses from the Clark and Curran CCG  parser.
Tagging	WSJ	 Table 2: Tagging accuracy on WSJ
comma insertion	Kyoto Text Corpus	We conducted an experiment on comma insertion using the Kyoto Text Corpus (, and obtained higher recall and precision than those of the baseline, leading us to confirm the effectiveness of our method.
PEM	Section 6	The correlation of PEM with human judgments is studied in Section 6.
dictionary creating	TEI consortium	Indeed, it has been some attempts for dictionary creating in accordance to the TEI consortium (Véronis and Ide 1996) but the problem was the fact that created textual views (corresponding to the surface structure) or database views (corresponding to the deep structure) were not customized.
event extraction task	JULIE Lab	In this paper, we explore these different representations of the dependency graphs and try, first, to pinpoint their effects on solving the overall event extraction task and, second, to further enhance the potential of JREX, a high-performance relation and event extractor developed at the JULIE Lab ().
trimming	CoNLL'X dependency graphs	Our second experiment focused on trimming operations on CoNLL'X dependency graphs.
Binding events	MST	 Table 3: Effects of trimming of CoNLL dependencies on the Shared Task development data for Binding events. Ap- proximate Span Matching/Approximate Recursive Matching. The data was processed by the MST parser.
relation extraction	Freebase	We use distant supervision to train a factor graph model for relation extraction based on an existing knowledge base (Freebase, derived in parts from Wikipedia).
question answering	CLEF	During along period of time, researches on question answering are mainly focused on finding short and concise answers from plain text for factoid questions driven by annual trackes such as CLEF, TREC and NTCIR.
question answering	NTCIR	During along period of time, researches on question answering are mainly focused on finding short and concise answers from plain text for factoid questions driven by annual trackes such as CLEF, TREC and NTCIR.
translation from German to English	MOSES	We describe experiments on translation from German to English, using phrase-based models trained by MOSES ().
translation from German to English	Europarl data	The experiments focus on translation from German to English, using the Europarl data).
Statistical parsing	Penn Treebank	Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (.
SMT	CTB2 tp	The development and test sets were from NIST MT08 evaluation campaign . We then used the SMT systems to translate the training data of CTB2 tp and CTB7.
SMT	CTB7	The development and test sets were from NIST MT08 evaluation campaign . We then used the SMT systems to translate the training data of CTB2 tp and CTB7.
semantic distance learning	WordNet	For semantic distance learning, we collected 50 hierarchies from WordNet and ODP, respectively.
semantic distance learning	ODP	For semantic distance learning, we collected 50 hierarchies from WordNet and ODP, respectively.
sentiment classification	MPQA dataset	 Table 4: Accuracy of sentiment classification on movie  review polarity (MR) and the MPQA dataset.
syntactic parsing	Penn Treebank	The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (.
parsing	Penn Treebank	We show that while there is a good correlation between those extrinsic metrics, parsing quality as measured on the Penn Treebank is not a good indicator of the final downstream application quality.
dependency parsing	Penn Treebank trees	We use the proposed procedure to compare dependency parsing results trained on Penn Treebank trees converted into dependency trees according to five different sets of linguistic assumptions.
predicting actions from still images	English Gi-gaword corpus	As predicting actions from still images directly is unreliable, we use a language model trained from the English Gi-gaword corpus to obtain their estimates; together with probabilities of co-located nouns, scenes and prepositions.
Sentence generation	hu- man gold standard	 Table 3: Sentence generation evaluation results with hu- man gold standard. Human R 1 scores are averaged over  the 5 sentences using a leave one out procedure. Values  in bold are the top scores.
RDM	2010 i2b2/VA Challenge	We evaluated the RDM using a corpus of electronic medical records provided by the 2010 i2b2/VA Challenge).
PRA	Amazon Mechanical Turk	To accurately compare PRA and N-FOIL's ability to reliably infer new beliefs from an imperfect knowledge base, we use human assessments obtained from Amazon Mechanical Turk.
PRA	RWR(no train) 0.271  0.456  RWR  0.280  3.7s 0.471  9.2s  PRA  0.307  5.7s 0.516 15.4s	 Table 2: Comparing PRA with RWR models. MRRs and  training times are averaged over 96 tasks.  =2  =3  MRR Time MRR Time  RWR(no train) 0.271  0.456  RWR  0.280  3.7s 0.471  9.2s  PRA  0.307  5.7s 0.516 15.4s
AAC language modeling	Amazon Mechanical Turk microtask market	In this paper we show that it is possible to significantly improve conversational AAC language modeling by first crowdsourcing the creation of a fictional collection of AAC messages on the Amazon Mechanical Turk microtask market.
parsing	DP-TSG	We evaluate parsing accuracy of the Stanford and DP-TSG models).
parsing	DP-TSG	In terms of parsing accuracy, the Berkeley parser exceeds both Stanford and DP-TSG.
summarization	Penn Discourse Treebank	The automatic detection of discourse relations, such as causal, contrast or temporal relations, is useful for many applications such as automatic summarization), question answering (, sentiment analysis ( and readability assessment . This task has recently seen renewed interest due to the growing availability of large-scale corpora annotated for discourse relations, such as the Penn Discourse Treebank ().
question answering	Penn Discourse Treebank	The automatic detection of discourse relations, such as causal, contrast or temporal relations, is useful for many applications such as automatic summarization), question answering (, sentiment analysis ( and readability assessment . This task has recently seen renewed interest due to the growing availability of large-scale corpora annotated for discourse relations, such as the Penn Discourse Treebank ().
sentiment analysis	Penn Discourse Treebank	The automatic detection of discourse relations, such as causal, contrast or temporal relations, is useful for many applications such as automatic summarization), question answering (, sentiment analysis ( and readability assessment . This task has recently seen renewed interest due to the growing availability of large-scale corpora annotated for discourse relations, such as the Penn Discourse Treebank ().
translation	IWSLT	Empirically, we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both IWSLT and NIST tasks.
document alignment task	Europarl	Later, we report our experimental results on the document alignment task on Europarl and Wikipedia data sets and on two language pairs.
document alignment task	Wikipedia data sets	Later, we report our experimental results on the document alignment task on Europarl and Wikipedia data sets and on two language pairs.
recognition	Broadcast News (BN) dev04f	We performed recognition on the Broadcast News (BN) dev04f, rt03 and rt04 task using the stateof-the-art acoustic models trained on the English Broadcast News (BN) corpus (430 hours of audio) provided to us by IBM (.
recognition	English Broadcast News (BN) corpus	We performed recognition on the Broadcast News (BN) dev04f, rt03 and rt04 task using the stateof-the-art acoustic models trained on the English Broadcast News (BN) corpus (430 hours of audio) provided to us by IBM (.
constituency projection	CTB-TEST-40	 Table 1: The performance of the Berkeley Parser trained on 160 thousand best projected trees, compared with previous  works on constituency projection and unsupervised parsing. CTB-TEST-40: sentences ≤ 40 words from CTB standard  test set (chapter 271-300); CTB1-ALL-10/CTB5-ALL-10: sentences ≤ 10 words from CTB 1.0/CTB 5.0 after the  removal of punctuation; CTB5-ALL-40: sentences ≤ 40 words from CTB 5.0 after the removal of punctuation.
constituency projection	CTB standard  test set	 Table 1: The performance of the Berkeley Parser trained on 160 thousand best projected trees, compared with previous  works on constituency projection and unsupervised parsing. CTB-TEST-40: sentences ≤ 40 words from CTB standard  test set (chapter 271-300); CTB1-ALL-10/CTB5-ALL-10: sentences ≤ 10 words from CTB 1.0/CTB 5.0 after the  removal of punctuation; CTB5-ALL-40: sentences ≤ 40 words from CTB 5.0 after the removal of punctuation.
constituency projection	CTB1-ALL-10	 Table 1: The performance of the Berkeley Parser trained on 160 thousand best projected trees, compared with previous  works on constituency projection and unsupervised parsing. CTB-TEST-40: sentences ≤ 40 words from CTB standard  test set (chapter 271-300); CTB1-ALL-10/CTB5-ALL-10: sentences ≤ 10 words from CTB 1.0/CTB 5.0 after the  removal of punctuation; CTB5-ALL-40: sentences ≤ 40 words from CTB 5.0 after the removal of punctuation.
constituency projection	CTB5-ALL-40	 Table 1: The performance of the Berkeley Parser trained on 160 thousand best projected trees, compared with previous  works on constituency projection and unsupervised parsing. CTB-TEST-40: sentences ≤ 40 words from CTB standard  test set (chapter 271-300); CTB1-ALL-10/CTB5-ALL-10: sentences ≤ 10 words from CTB 1.0/CTB 5.0 after the  removal of punctuation; CTB5-ALL-40: sentences ≤ 40 words from CTB 5.0 after the removal of punctuation.
parse correction	FTB training set	To train our parse correction models, we generated specialized training sets corresponding to each parser by doing 10-fold jackknifing on the FTB training set (cf. Section 5.1).
translation	FSA representation	We then investigate a translation grammar which is large enough that exact translation under the FSA representation is not possible.
translation model	GALE 2008 evaluation parallel text	For translation model training, we use a subset of the GALE 2008 evaluation parallel text; 3 this is 2.1M sentences and approximately 45M words per language.
parsing	PCFG	For parsing, the resulting PCFG is slightly modified by removing the context-identifiers.
parsing task	Fmeasure	In this specific case, precision is the fraction of correct parses out: A summary of results for the parsing task, in Fmeasure.
NER	MTurk	 Table 1: For NER, active learning consistently beats random sampling on MTurk. NER F 1 evaluated on  CoNLL test set A. #train = number of sentences in training set, S = single, 3-v = 3-voting, 5/4-voting = 5- and 4-voting for NER and sentiment resp., +f = using fragments; sentiment budget 1130 for run 1, sentiment  budget 1756 averaged over 2 runs.
NER	CoNLL test set A	 Table 1: For NER, active learning consistently beats random sampling on MTurk. NER F 1 evaluated on  CoNLL test set A. #train = number of sentences in training set, S = single, 3-v = 3-voting, 5/4-voting = 5- and 4-voting for NER and sentiment resp., +f = using fragments; sentiment budget 1130 for run 1, sentiment  budget 1756 averaged over 2 runs.
CRF	Mallet package	For CRF, we use the linear-chain CRF model available from the Mallet package 1 .  In the first experiment, a set of 72, 628 listings from the women's clothing category is partitioned into a training set of 39, 448 listings and test set of 33, 180 listings based on an initial seed list of known 6, 312 women's apparel brands manually prepared by our fashion experts.
NER	Men's clothing category dataset	 Table 4: NER Accuracy on 2 test sets as the seed dictio- nary for brands grows. Results shown here are obtained  the same Men's clothing category dataset, as used to show  the supervised NER results in
parse	German treebank	Consider the example English and German sentences shown in, and suppose that we wish to parse the German side without access to a German treebank.
NER	GATE	summarizes the quality of NER rules out-of-the-box and after domain customization in the GATE) and SystemT () systems, as reported in ( and () respectively.
induction	CoNLL03 test collection	We used the CoNLL03 training set for induction and report results on the CoNLL03 test collection.
sentiment classification	DVD	justifies that it is challenging to perform active learning in imbalanced sentiment classification: the approaches of margin-based, uncertainty-based and self-selecting perform no better than random selection while co-testing only outperforms random selection in two domains: DVD and Electronic with only a small improvement (about 1%).
predicte alignment	MTC	 Table 1: Results for sentence-based predicte alignment in the three benchmark settings MTC, Leagues and MSR (all  numbers in %); results that significantly differ from Full are marked with asterisks (* p<0.05; ** p<0.01).
metonymy resolution task	SemEval 2007	Our algorithm is tested on the data from the metonymy resolution task (Task 8) at SemEval 2007.
summarization	Sparck Jones (1999)	Of the many summarization paradigms that have been identified over the years (see Sparck Jones (1999) and for comprehensive overviews), multi-document summarization -the task of producing summaries from clusters of thematically related documents -has consistently attracted attention.
SMT	NIST MT data	To testify its impact on SMT in wider range, we design a special experiment based on the 2005 NIST MT data (see section 6.1).
parsing	English  test data	 Table 1: Comparing this work in terms of parsing accu- racy compared to state-of-the-art baselines on the English  test data. We also report results for a re-implementation  of exact first to third-order graph-based parsing and a re- implementation of
Question Answering (QA)	Watson at Jeopardy	Question Answering (QA) research for factoid questions has recently achieved great success as demonstrated by IBM's Watson at Jeopardy: its accuracy has been reported to be around 85% on factoid questions.
PRO tuning	HOO	 Table 3: PRO tuning of the full decoder model on HOO-
speech recognition search space ASR	WFST-based LVCSR speech decoder	Decoding of the speech recognition search space ASR is performed by T 3 Decoder (, which is a state-of-the-art WFST-based LVCSR speech decoder.
Relation Extraction	Chinese data set	 Table 1: Relation Extraction Results. Models using hidden constituency syntax provide significant gains over the  syntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated on  the smaller Chinese data set.
SRL	CoNLL data sets	 Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
MT	IWSLT data set	 Table 5. MT performance of symmetrization  methods on IWSLT data set. The results in bold  type are significantly better than the performance  of IDG.
MT	NIST data	 Table 6. MT performance of symmetrization  methods on NIST data. The results in bold type are  significantly better than the performance of IDG.
MT	IWSLT data set	 Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
MT	NIST data set	 Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
MT	NIST data set	 Table 8. MT performance of the new methods  on NIST data set. The results in bold type are  significantly better than the baseline.
MT metrics	NIST	The early seminal work on automatic MT metrics (e.g., BLEU and NIST) is largely based on n-gram matches ().
MT	NIST	We consider four widely used MT metrics (BLEU, NIST, METEOR (v0.7), and TER) as our baselines.
HMM sampling/optimization	ARPA format	In Section 2, we present our approach and describe our joint algorithm for HMM sampling/optimization, giving details about our extension of the ARPA format and refinement procedure.
SMT	WMT 2011 workshop	The SMT models for our experiments were created with a subset of the training data for the English-French shared task at the WMT 2011 workshop).
coreference resolution	CEAF-E	For coreference resolution, MUC (, B-CUBED ( and CEAF-E () are used for evaluation.
constraint selection	Section 22 of the Wall Street Journal	 Table 1: Exploratory statistics for constraint selection.  The table shows the percentage of context types for which  the probability of the most frequent head tag is at least p.  Head in Context refers to the subset of contexts where the  most frequent head is within the context itself. Numbers  are based on Section 22 of the Wall Street Journal and are  given for contexts that appear at least 10 times.
burst detection	Twitter dataset	We systematically evaluate three types of activities for burst detection on a large Twitter dataset and analyze different properties of these three streams for burst detection.
MT	News+Weibo) Online A Online B Online C  baseline	 Table 6: Normalization and MT Results. Rows denote different normalizations, and columns different translation  systems, except the first column (Norm), which denotes the normalization experiment. Cells display the BLEU score  of that experiment.  Moses  Moses  Condition  Norm (News) (News+Weibo) Online A Online B Online C  baseline  19.90  15.10  24.37  20.09  17.89  18.79  norm+phrase  21.96  15.69  24.29  20.50  18.13  18.93  norm+phrase+char  22.39  15.87  24.40  20.61  18.22  19.08  norm+phrase+char+mono 22.91  15.94  24.46  20.78  18.37  19.21
domain adaptation	Books	Moreover, these results also clearly show that domain adaptation is not asymmetric process, as we can see it is easier to conduct domain adaptation from the source domain Books to the target domain Kitchen (with an accuracy around 82%), but it is more difficult to make domain adaptation from the source domain Kitchen to the target domain Books (with an ac-   curacy around 75%).
coreference resolution	CoNLL 2011 shared task	First, we compare the error distributions on coreference resolution of all of the systems from the CoNLL 2011 shared task plus several publicly available systems.
coreference resolution	CoNLL-2012 test  set	 Table 5: Performance (F-measure) of the three best Chinese coreference resolution systems on the CoNLL-2012 test  set
coreference resolution	ACE 2004 newswire text	• We present experiments showing improved performance at coreference resolution, given both gold and automatic mention detection: e.g., 6.2 point improvement in MUC recall on ACE 2004 newswire text and 3.1 point improvement in MUC precision the CoNLL 2011 test set.
coreference resolution	CoNLL 2011 test set	• We present experiments showing improved performance at coreference resolution, given both gold and automatic mention detection: e.g., 6.2 point improvement in MUC recall on ACE 2004 newswire text and 3.1 point improvement in MUC precision the CoNLL 2011 test set.
coreference resolution	NEL	We first look at NECO's performance at coreference resolution and then evaluate its ability at NEL.
NEL	ACE-NWIRE-NEL	 Table 6: NEL performance of our system and the ensem- ble baseline linker on ACE-NWIRE-NEL.
summarization	TAC 2008 and 2011 data sets	We evaluate our proposed summarization approach on the TAC 2008 and 2011 data sets using the standard ROUGE metric).
translation	NIST MT12 evaluation	It achieves translation accuracies comparable to the top ranked systems in the NIST MT12 evaluation.
MT	GALE/BOLT data set	 Table 3: Effects of tag annotation, tag distribution, and  syntax mismatch features on MT performance on the  GALE/BOLT data set.
coreference resolution	Ontonotes-5.0	Overall, the main contribution of this paper is a principled machine learning model operating at mention-pair granularity, using easy to implement constraint-augmented inference and learning, that yields competitive results on coreference resolution on Ontonotes-5.0 (Pradhan et al., 2012) and ACE 2004).
coreference resolution	ACE 2004	Overall, the main contribution of this paper is a principled machine learning model operating at mention-pair granularity, using easy to implement constraint-augmented inference and learning, that yields competitive results on coreference resolution on Ontonotes-5.0 (Pradhan et al., 2012) and ACE 2004).
POS tagging task	International Chinese Language Processing Bakeoff (SIGHAN Bakeoff 2008)(Jin and Chen, 2008)	• Another is the CTB dataset from the POS tagging task of the Fourth International Chinese Language Processing Bakeoff (SIGHAN Bakeoff 2008)(Jin and Chen, 2008).
question answering (Q-A) task	CoreSC summaries	We compare the performance of the experts on a question answering (Q-A) task when given the CoreSC summaries and two other types of summary, amounting to a total of three experimental conditions (A,B,C).
definition sentence identification task	WCL (English Wikipedia) corpus	We not only benchmark DefMiner's performance over our own W00 collection, but also compare DefMiner against previous published work on the definition sentence identification task on the WCL (English Wikipedia) corpus.
paraphrase classification	Microsoft Research Paraphrase Corpus	Our experiments test the utility of the TF-KLD weighting towards paraphrase classification, using the Microsoft Research Paraphrase Corpus ().
relation extraction	PDTBRel	For evaluating the performance of relation extraction, we combine the results of RAE with PDTBRel and SVM-Rel.
GER	DEV corpus	 Table 5: Learning curve comparing system performance  for GER on tokens on DEV corpus. BL=Baseline
MT	M -best lists	In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation.
parsing	English Resource Grammar	However parsing with such a lexicon, as included in the English Resource Grammar, can be very slow.
Identifying Trend related	Microblogs	Mining New Business Opportunities: Identifying Trend related Products by Leveraging Commercial Intents from Microblogs
predicting pronunciation variation	WFSTs	A noteworthy result is the apparent usefulness of stress modeling for predicting pronunciation variation using WFSTs with the direct method; this is: Recall for generating alternative pronunciations seen in the first two data columns of 1.
translations	RCTM	Further, translations can be generated directly from the probability distribution of the RCTM without any external resources.
paraphrase	WordNet	For each target word to paraphrase, they first compute a set of substitution candidates using WordNet: all synonyms from all of the target word's WordNet synsets, together with the words from synsets in similar to, entailment and also see relation to these synsets are considered as potential substitutions.
text understanding	Penn Discourse Treebank (PDTB) corpus	It has attracted increasing attention in recent years due to its importance in text understanding, especially since the release of the Penn Discourse Treebank (PDTB) corpus, which adds a layer of discourse annotations on top of the Penn Treebank * The research reported in this paper was carried out while Fang Kong was a research fellow at the National University of Singapore.
text understanding	Penn Treebank	It has attracted increasing attention in recent years due to its importance in text understanding, especially since the release of the Penn Discourse Treebank (PDTB) corpus, which adds a layer of discourse annotations on top of the Penn Treebank * The research reported in this paper was carried out while Fang Kong was a research fellow at the National University of Singapore.
argument labeling	Section	Related work on argument labeling is reviewed in Section 3.
repair structural classification	Switchboard	Similarity metrics For direct comparison to previous approaches we use the standard measure of overall accuracy, the F-score over reparandum words, which we abbreviate F rm (see 7): We are also interested in repair structural classification, we also measure F-score overall repair components (rm words, ed words as interregna and rp words), a metric we abbreviate F s . This is not measured in standard repair detection on Switchboard.
repair detection	Switchboard	Similarity metrics For direct comparison to previous approaches we use the standard measure of overall accuracy, the F-score over reparandum words, which we abbreviate F rm (see 7): We are also interested in repair structural classification, we also measure F-score overall repair components (rm words, ed words as interregna and rp words), a metric we abbreviate F s . This is not measured in standard repair detection on Switchboard.
NER	CoNLL-2003 test data	 Table 3: The performance of semi-supervised  NER on the CoNLL-2003 test data, using vari- ous embedding features.  † DenseEmb refers to the  method used by Turian et al. (2010), i.e., the direct  use of the dense and continuous embeddings.
translation	NTCIR10	In translation evaluations, we achieved statistically significant gains 153 in BLEU scores in the NTCIR10.
word alignment	KFTT	The threshold for the filtering factor was set to 0.1 which was the best setting in the word alignment experiment in section 4.2 under KFTT.
translation direction detection task	Vowpal Wabbit	For the translation direction detection task explained in section 1, we use a fast linear classifier trained with online learning, Vowpal Wabbit (.
translation task	Moses HPB decoder	For each translation task, the recent version of Moses HPB decoder ( with the training scripts was used as the baseline (Base).
Translation	BLM	 Table 3: Translation results. The symbol ++ (--)  represents a significant difference at the p < 0.01  level and -represents a significant difference at the  p < 0.05 level against the BLM.
LM growing	NTCIR corpora	The same LM growing method was ap-7 https://wit3.fbk.eu/ plied on TED corpora as on NTCIR corpora.
LM growing	BNLM	The results were shown in. indicated that our proposed LM growing method improved both PPL and BLEU in comparison with both BNLM and our previous CSLM converting method, so it was suitable for domain adaptation, which is one of focuses of the current SMT research.
MT evaluation	NIST	While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task fora given set of outputs.
MT evaluation	WMT	While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task fora given set of outputs.
SRL	Chinese Prop-Bank	The experimental results show that our approach can significantly improve SRL performance, especially in Chinese Prop-Bank.
MIR	SVR	Our model outperforms previous MIR models and two strong linear models for rating prediction, namely SVR and Lasso by more than 10% relative in terms of MSE.
rating prediction	SVR	Our model outperforms previous MIR models and two strong linear models for rating prediction, namely SVR and Lasso by more than 10% relative in terms of MSE.
aspect prediction	BeerAdvocate	Five publicly available datasets were built for aspect prediction by -BeerAdvocate, Ratebeer (ES), RateBeer (FR), Audiobooks and Toys & Games -and have aspect ratings assigned by their creators on the respective websites.
Sentiment Analysis	People's Daily	Sentiment Analysis on the People's Daily
MT	NIST OpenMT 2012	State-of-the-art English-Arabic (EnAr) and English-Chinese (En-Ch) MT systems were trained on parallel corpora released in NIST OpenMT 2012, in addition to parallel forum data collected as part of the BOLT program (10m EnAr words; 30m En-Ch words).
QA evaluations	WebQ	We used two publicly released data sets for QA evaluations:) includes the annotated lambda calculus forms for each question, and covers 81 domains and 635 Freebase relations; WebQ.
Result Analysis	WebQ	Result Analysis Since the majority of questions in WebQ.
MERT training	QTEST	Poems in QVALID (with autogenerated references) were used for MERT training and Poems in QTEST (with auto-generated references) were used for BLEU evaluation.
summarization	DUC2004 dataset	Our experiments show a statistically significant improvement of 1.33%, 1.58%, and 2.25% for ROUGE-1, ROUGE-2 and ROUGE-L scores, respectively, when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset.
summarization	TAC data sets	Summarization Data For summarization experiments, we use the standard TAC data sets , which have been used in the NIST competitions.
summarization	NIST competitions	Summarization Data For summarization experiments, we use the standard TAC data sets , which have been used in the NIST competitions.
sentence compression module	TAC2010 data	The training data for the sentence compression module in the summarization system is summary guided compression corpus annotated by) using TAC2010 data.
summarization	TAC2010 data	The training data for the sentence compression module in the summarization system is summary guided compression corpus annotated by) using TAC2010 data.
paraphrase detection	Microsoft Research Paraphrase Corpus	Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of) and dialogue act tagging using the Switchboard Corpus (see e.g. ().
parsing	CoNLL	This results in a fast, compact classifier, which uses only 200 learned dense features while yielding good gains in parsing accuracy and speed on two languages (English and Chinese) and two different dependency representations (CoNLL and Stanford dependencies).
parsing	PTB +  CoNLL dependencies	 Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
parsing	PTB +  Stanford dependencies	 Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
parsing	CTB	 Table 6: Accuracy and parsing speed on CTB.
AZP resolution	Chinese portion of the OntoNotes 5.0 corpus	Our contribution lies in the proposal of the first unsupervised probabilistic model for AZP resolution that rivals its supervised counterparts in performance when evaluated on the Chinese portion of the OntoNotes 5.0 corpus.
taxonomy construction	IJCAI proceedings	We evaluated our methods for taxonomy construction against the following text collections of five domains: • Artificial Intelligence (AI) domain: 4,119 papers extracted from the IJCAI proceedings from 1969 to 2011 and the ACL archives from year 1979 to 2010.
taxonomy construction	ACL archives	We evaluated our methods for taxonomy construction against the following text collections of five domains: • Artificial Intelligence (AI) domain: 4,119 papers extracted from the IJCAI proceedings from 1969 to 2011 and the ACL archives from year 1979 to 2010.
extracting biomedical events	BioNLP 2013, 2011 and 2009 Genia shared task datasets	We evaluate our approach for the task of extracting biomedical events on the BioNLP 2013, 2011 and 2009 Genia shared task datasets.
tokenization	GENIA	Specifically, sentence split and tokenization are done using the GENIA tools, while part-of-speech information is provided by the BLLIP parser that uses the self-trained biomedical model.
binding event	Miwa12	Our system's F1 score for the binding event is 58.79, while those of Miwa12 and Riedel11 are 56.64 and 48.49 respectively.
segmentation	KWS	Data The segmentation algorithms described in Section 3 are tested using the setup of the KWS system described in Section 4.
phrase chunking task	CoNLL2003 dataset	We used the CoNLL2000 dataset) for the phrase chunking task, the CoNLL2003 dataset, for the named-entity recognition task, the NLPBA2004 dataset (), for the biomedical entity recognition task and the CoNLL2000POS dataset 2 for the part-of-speech labeling task.
phrase chunking task	NLPBA2004 dataset	We used the CoNLL2000 dataset) for the phrase chunking task, the CoNLL2003 dataset, for the named-entity recognition task, the NLPBA2004 dataset (), for the biomedical entity recognition task and the CoNLL2000POS dataset 2 for the part-of-speech labeling task.
phrase chunking task	CoNLL2000POS dataset	We used the CoNLL2000 dataset) for the phrase chunking task, the CoNLL2003 dataset, for the named-entity recognition task, the NLPBA2004 dataset (), for the biomedical entity recognition task and the CoNLL2000POS dataset 2 for the part-of-speech labeling task.
named-entity recognition task	NLPBA2004 dataset	We used the CoNLL2000 dataset) for the phrase chunking task, the CoNLL2003 dataset, for the named-entity recognition task, the NLPBA2004 dataset (), for the biomedical entity recognition task and the CoNLL2000POS dataset 2 for the part-of-speech labeling task.
named-entity recognition task	CoNLL2000POS dataset	We used the CoNLL2000 dataset) for the phrase chunking task, the CoNLL2003 dataset, for the named-entity recognition task, the NLPBA2004 dataset (), for the biomedical entity recognition task and the CoNLL2000POS dataset 2 for the part-of-speech labeling task.
SMT	CoNLL-2014 test  set	 Table 3: Performance of the pipeline, SMT,  and combined systems on the CoNLL-2014 test  set. All improvements of combined systems over  their component systems are statistically signifi- cant (p < 0.01). The differences between P1 and  S1 and between P2 and S2 are not statistically sig- nificant.
Tagging	SANCL test sets	 Table 1: Tagging accuracies and comparison to prior work on the SANCL test sets (fine-grained POS).
ERROR level detection	ENNI	 Table 4: ERROR level detection performance for  each code (system trained on ENNI; 30% error  utterances; ZHANG feature set; with mazes)
parsing	Penn Treebank-style annotations	We see this starting point as a necessity, given observations about the rapidly changing nature of tweets, the attested difficulties of domain adaptation for parsing (, and the expense of creating Penn Treebank-style annotations.
parsing	Penn Treebank data	Notably, a 2-3% gain is obtained by modifying the parsing algorithm, and our stackinginspired use of Penn Treebank data contributes in both cases, quite a lot on TEST-FOSTER (unsurprisingly given that test set's similarity to the Penn Treebank).
parsing	Penn Treebank	Notably, a 2-3% gain is obtained by modifying the parsing algorithm, and our stackinginspired use of Penn Treebank data contributes in both cases, quite a lot on TEST-FOSTER (unsurprisingly given that test set's similarity to the Penn Treebank).
question answering	QALD-1	We use the following three collections of questions from the QALD 13 task for question answering over linked data: QALD-1, QALD-3 and QALD-4.
question answering	QALD-3	We use the following three collections of questions from the QALD 13 task for question answering over linked data: QALD-1, QALD-3 and QALD-4.
question answering	QALD-4	We use the following three collections of questions from the QALD 13 task for question answering over linked data: QALD-1, QALD-3 and QALD-4.
phrase extraction	Europarl	For phrase extraction, we used both Europarl ( and News Commentaries (NC) totalling about 2.2M sentences.
phrase extraction	News Commentaries (NC)	For phrase extraction, we used both Europarl ( and News Commentaries (NC) totalling about 2.2M sentences.
parsing	Phone document	presents the results for parsing the Phone document, our development set, with the sentence-based model, varying the training data.
identifying	ARZ	Further, we show the positive impact of using the new features in identifying ARZ (Section 5).
dialect identification	ARZ	Dataset: We performed dialect identification experiment for ARZ and MSA.
MT	WMT 2013 shared task	For the MT evaluation, we used the parallel data from the WMT 2013 shared task, excluding the Common Crawl corpus data.
MT	Common Crawl corpus data	For the MT evaluation, we used the parallel data from the WMT 2013 shared task, excluding the Common Crawl corpus data.
word analogy task	CoNLL-2003 shared benchmark Word analogies	We conduct experiments on the word analogy task of, a variety of word similarity tasks, as described in (, and on the CoNLL-2003 shared benchmark Word analogies.
NER task	HPCA	 Table 4: F1 score on NER task with 50d vectors.  Discrete is the baseline without word vectors. We  use publicly-available vectors for HPCA, HSMN,  and CW. See text for details.  Model  Dev Test ACE MUC7  Discrete 91.0 85.4 77.4  73.4  SVD  90.8 85.7 77.3  73.7  SVD-S 91.0 85.5 77.6  74.3  SVD-L 90.5 84.8 73.6  71.5  HPCA 92.6 88.7 81.7  80.7  HSMN 90.5 85.7 78.7  74.7  CW  92.2 87.4 81.7  80.2  CBOW 93.1 88.2 82.2  81.1  GloVe  93.2 88.3 82.9  82.2
NER task	Model  Dev Test ACE MUC7  Discrete 91.0 85.4 77.4  73.4  SVD  90.8 85.7 77.3  73.7  SVD-S 91.0 85.5 77.6  74.3  SVD-L 90.5 84.8 73.6  71.5  HPCA 92.6 88.7 81.7  80.7  HSMN 90.5 85.7 78.7  74.7  CW  92.2 87.4 81.7  80.2  CBOW 93.1 88.2 82.2  81.1  GloVe  93.2 88.3 82.9  82.2	 Table 4: F1 score on NER task with 50d vectors.  Discrete is the baseline without word vectors. We  use publicly-available vectors for HPCA, HSMN,  and CW. See text for details.  Model  Dev Test ACE MUC7  Discrete 91.0 85.4 77.4  73.4  SVD  90.8 85.7 77.3  73.7  SVD-S 91.0 85.5 77.6  74.3  SVD-L 90.5 84.8 73.6  71.5  HPCA 92.6 88.7 81.7  80.7  HSMN 90.5 85.7 78.7  74.7  CW  92.2 87.4 81.7  80.2  CBOW 93.1 88.2 82.2  81.1  GloVe  93.2 88.3 82.9  82.2
WS	GALE manual WA corpus	The experimental corpus for WS was constructed by first segmenting 2000 held out sentences from the GALE manual WA corpus with the Stanford segmenter, and then refining the segmentation with the gold alignment annotation.
MERT tuning	NIST evaluation	The data set of the NIST evaluation 2002 was used as a development set for MERT tuning, and the remaining data sets of the NIST evaluation from 2003 to 2006 were used as test sets.
prediction	EnglishRussian PSMT	We also show that these improvements in prediction accuracy can be beneficial in an end-to-end machine translation scenario by integrating into a large-scale EnglishRussian PSMT system.
translation	MCMC sampler	We use the same SCFG decoder for translation with both the baseline and the grammars sampled using our type-based MCMC sampler.
Classification	Pang	Classification involves detecting positive/negative reviews (Pang and).
segmentation	FMM	From the segmentation results, we can see that the FMM and RMM methods are highly dependent on the compiled vocabulary and their identified OOV words are mainly the ones composed of a single Chinese character.
MT	LDC	The data that we used for training the MT system was a Chinese-English corpus derived from newswire text from LDC.
Detecting Non-compositional MWE	Wiktionary	Detecting Non-compositional MWE Components using Wiktionary
Compositionality prediction	ENC dataset	 Table 2: Compositionality prediction results over the ENC dataset, relative to the first component (the  modifier noun) and the second component (the head noun)
Compositionality prediction	EVPC dataset	 Table 3: Compositionality prediction results over the EVPC dataset, relative to the first component (the  head verb) and the second component (the particle)
summarization evaluation	RST-DT corpus	This is also the commonly used evaluation condition for single-document summarization evaluation on the RST-DT corpus.
MT	EIJIRO dictionary entries	For training the MT system, we also include the EIJIRO dictionary entries and the accompanying example sentences.
Cross Training	HindEnCorp) label	For Cross Training, we used the bilingual sentence-aligned data from HindEnCorp) label.
preposition prediction	Flickr30k	We base the preposition prediction task on two large-scale image datasets with human authored descriptions, namely MSCOCO () and Flickr30k (.
PC removal	TOEFL	For the PC removal the optimal number of removed PCs is 379 for TOEFL, 15 for BLESS and 128 for SimLex-999, while the optimal number for the Caron ptransform is -1.4 for TOEFL, 0.5 for BLESS and -0.40 for SimLex-999.
PC removal	TOEFL	For the PC removal the optimal number of removed PCs is 379 for TOEFL, 15 for BLESS and 128 for SimLex-999, while the optimal number for the Caron ptransform is -1.4 for TOEFL, 0.5 for BLESS and -0.40 for SimLex-999.
summarization	TAC2014 scientific summarization dataset	We show that our proposed method effectively improves over existing summarization approaches (greater than 30% improvement over the best performing baseline) in terms of ROUGE scores on TAC2014 scientific summarization dataset.
Machine Translation evaluation campaigns	GEC	Methods introduced for the Workshop of Machine Translation evaluation campaigns have been adapted to GEC and extended where necessary.
ternary sentiment analysis	SemEval 2013 Task B dataset	We considered four tasks: (1) binary sentiment analysis with a movie review dataset of 10,662 sentences (PL05) (Pang and) and a product review dataset (Amazon) of 2,000 multiline documents for 4 different product groups () (we will report the average effectiveness over the 4 sub-collections); (2) ternary sentiment analysis with the SemEval 2013 Task B dataset (Twitter) containing 12,348 tweets classified as positive, neutral or negative (Nakov et al., 2013); (3) binary subjectivity detection with a dataset of 10,000 sentences (PL04) () and another of 11,640 sentences (MPQA) (); and (4) seven-class topic spotting with a news dataset (News) of 32,602 one-line news summaries (Vitale et al., 2012).
Summarizing topical contents	PubMed documents	Summarizing topical contents from PubMed documents using a thematic analysis
NEL	ECB	 Table 6: NEL performance (%) comparison on ECB
detecting the types of named entities in short inputs-such as sentences or tweets-with	WordNet	We propose FINET, a system for detecting the types of named entities in short inputs-such as sentences or tweets-with respect to WordNet's super fine-grained type system.
type selection	WordNet	FINET combats data scarcity and noise from existing systems: It does not rely on supervision in its extractors and generates training data for type selection from WordNet and other resources.
SMT reranking	Chinese-English translation dataset	We also apply our HRNNLM to SMT reranking task in an open Chinese-English translation dataset.
SMT	IWSLT 2014	 Table 3: BLEU scores of SMT systems. The I- WSLT is a public baseline which issued by the or- ganizer of IWSLT 2014, as described in (Cettolo  et al., 2012).
Extrinsic evaluation	Bond & Marvel)	 Table 6: Extrinsic evaluation (Bond & Marvel)
taxonomy construction	IJCAI proceedings	We evaluate our method for taxonomy construction against the following collections of six domains: • Artificial Intelligence (AI) domain: The corpus consists of 4,976 papers extracted from the IJCAI proceedings from 1969 to 2014 and the ACL archives from year 1979 to 2014.
taxonomy construction	ACL archives	We evaluate our method for taxonomy construction against the following collections of six domains: • Artificial Intelligence (AI) domain: The corpus consists of 4,976 papers extracted from the IJCAI proceedings from 1969 to 2014 and the ACL archives from year 1979 to 2014.
MT	GALE ChiEng Dataset	lists distinguishing machine translation approaches of top five MT of GALE ChiEng Dataset.
MT	GALE Chi-Eng Da- taset	 Table 3: Techniques of top five MT of GALE Chi-Eng Da- taset
MT evaluation	WordNet	While dense vector space representations such as those obtained through Deep Neural Networks (DNNs) or RNNs are able to capture semantic similarity for words (), segments) and documents () naturally, traditional MT evaluation metrics can only achieve this using resources like WordNet and paraphrase databases.
MT	IWSLT 2014	Both ASR and MT outputs come from a system submission in IWSLT 2014 ().
CSLM training	Europarl	The dataset used for CSLM training consists of Europarl, News-commentary and News-crawl corpus.
CSLM training	News-commentary	The dataset used for CSLM training consists of Europarl, News-commentary and News-crawl corpus.
CSLM training	News-crawl corpus	The dataset used for CSLM training consists of Europarl, News-commentary and News-crawl corpus.
rule selection	VW	Our rule selection model was trained with VW.
AMR parsing	JAMR	 Table 3: AMR parsing Smatch scores for the experiments in this work. We provide a cross-reference to  the section of this paper that describes each of the evaluated systems. Entries in bold are improvements  over JAMR (
WS	BCCWJ	 Table 5: WS accuracy on BCCWJ.  Recall [%] Precision [%] F-measure  Baseline  99.01  98.97  98.99  + Log-as-is  99.02  98.87  98.94  + Log-chunk  99.05  98.88  98.96  + Log-mconv  98.98  98.91  98.95  + Log-chunk-mconv  98.98  98.92  98.95
alignment evaluation	Tsinghua Chinese-English word alignment evaluation data set	For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set (.
translation evaluation	NIST 2006 dataset	For translation evaluation, we used the NIST 2006 dataset as the development set and the and 2008 datasets as the test sets.
translation evaluation	news-test2012" dataset	For translation evaluation, we used the "news-test2012" dataset that contains 3,003 sentences as the development set and the "news-test2013" dataset that contains 3,000 sentences as the test set.: Comparison of different alignment methods on the Chinese-English dataset.
translation evaluation	news-test2013" dataset	For translation evaluation, we used the "news-test2012" dataset that contains 3,003 sentences as the development set and the "news-test2013" dataset that contains 3,000 sentences as the test set.: Comparison of different alignment methods on the Chinese-English dataset.
translation evaluation	Chinese-English dataset	For translation evaluation, we used the "news-test2012" dataset that contains 3,003 sentences as the development set and the "news-test2013" dataset that contains 3,000 sentences as the test set.: Comparison of different alignment methods on the Chinese-English dataset.
SMT	NNJM	Our aim in this paper is to advance the state-ofthe-art in SMT by extending NNJM for domain adaptation to leverage the huge amount of out-domain data coming from heterogeneous sources.
MT	NNJM	We also tried B cat,in variation, i.e. training an MT system on the entire data and using in-domain data to train the baseline NNJM.
MT	NNJM	The MML-based baseline systems (B mml ) used 20% selected data for training the MT system and the NNJM.
MT	NNJM	 Table 6. The  MML-based baseline systems (B mml ) used 20%  selected data for training the MT system and the  NNJM. On Arabic-English, both MML-based se- lection and our model (S v1 ) gave similar gains on  top of the baseline system (B cat ). Further results  showed that both approaches are complementary.  We were able to obtain an average gain of +0.3  BLEU points by training an NDAM v1 model over  the selected data (see S v1+mml ).  However, on English-German, the MML-based  selection caused a drop in the performance (see
tagging	BATCH	As tagging progresses, the distributional representations become increasingly specific to the target domain (TD), converging to the representations that BATCH uses at the end of the tagging process.
POS tagging	CoNLL '09 test set	 Table 2: POS tagging results on the CoNLL '09 test set for in- tegrated POS tagging and parsing. We compare the accuracy  of our baseline CRF tagger, 'Linear' (our re-implementation  of Bohnet and Nivre (2012, BN'12)), 'Neural' (the neural  parser presented in this work), and results reported by BN'12.
POS tagging	BN'12	 Table 2: POS tagging results on the CoNLL '09 test set for in- tegrated POS tagging and parsing. We compare the accuracy  of our baseline CRF tagger, 'Linear' (our re-implementation  of Bohnet and Nivre (2012, BN'12)), 'Neural' (the neural  parser presented in this work), and results reported by BN'12.
POS tagging	BN'12	 Table 2: POS tagging results on the CoNLL '09 test set for in- tegrated POS tagging and parsing. We compare the accuracy  of our baseline CRF tagger, 'Linear' (our re-implementation  of Bohnet and Nivre (2012, BN'12)), 'Neural' (the neural  parser presented in this work), and results reported by BN'12.
Parse fusion	British National Corpus (BNC)	Corpora: Parse fusion is evaluated on British National Corpus (BNC), Brown, GENIA, Question Bank (QB), Switchboard (SB) and Wall Street Journal (WSJ) Dependencies (basic dependencies, version 3.3.0) and provide dependency metrics (UAS, LAS) as well.
Parse fusion	Brown	Corpora: Parse fusion is evaluated on British National Corpus (BNC), Brown, GENIA, Question Bank (QB), Switchboard (SB) and Wall Street Journal (WSJ) Dependencies (basic dependencies, version 3.3.0) and provide dependency metrics (UAS, LAS) as well.
Parse fusion	GENIA	Corpora: Parse fusion is evaluated on British National Corpus (BNC), Brown, GENIA, Question Bank (QB), Switchboard (SB) and Wall Street Journal (WSJ) Dependencies (basic dependencies, version 3.3.0) and provide dependency metrics (UAS, LAS) as well.
Parse fusion	Question Bank (QB)	Corpora: Parse fusion is evaluated on British National Corpus (BNC), Brown, GENIA, Question Bank (QB), Switchboard (SB) and Wall Street Journal (WSJ) Dependencies (basic dependencies, version 3.3.0) and provide dependency metrics (UAS, LAS) as well.
Parse fusion	Wall Street Journal (WSJ) Dependencies	Corpora: Parse fusion is evaluated on British National Corpus (BNC), Brown, GENIA, Question Bank (QB), Switchboard (SB) and Wall Street Journal (WSJ) Dependencies (basic dependencies, version 3.3.0) and provide dependency metrics (UAS, LAS) as well.
parsing	WSJ	When parsing WSJ (in-domain), we tune parameters on WSJ section 24.
parsing	WSJ section	When parsing WSJ (in-domain), we tune parameters on WSJ section 24.
SMT	Europarl corpus(v7)	We trained a SMT system on 10K French-English sentences from the Europarl corpus(v7).
Translation	newstest2014	Translation performances are reported in case-sensitive BLEU () on newstest2014 (2737 sentences) and newstest2015 (2169 sentences).
Translation	newstest2015	Translation performances are reported in case-sensitive BLEU () on newstest2014 (2737 sentences) and newstest2015 (2169 sentences).
translation	WMT	Following (, we report translation quality using two types of BLEU: (a) tokenized 12 BLEU to be comparable with existing NMT work and (b) NIST 13 BLEU to be comparable with WMT results.
Sentiment classification	Yelp 2013/2014/2015	 Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
Sentiment classification	IMDB datasets	 Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
Sentiment classification	IMDB	 Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
Sentiment classification	Yelp 2013/2014/2015 datasets	 Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
paraphrase detection	Microsoft Research Paraphrase Corpus	In the context of paraphrase detection, we achieve a result very close to the current state-ofthe-art on the Microsoft Research Paraphrase Corpus).
sentiment classification	Stanford Sentiment Treebank benchmark	For sentiment classification, we use the Stanford Sentiment Treebank benchmark).
RTE	PASCAL RTE-2 3 and RTE-3 4 datasets	We evaluate the performance of our framework for RTE on the PASCAL RTE-2 3 and RTE-3 4 datasets, which has 1600 examples.
detection	NAVYTIME) classification detector	Baselines For detection, we include a baseline reimplementation of the NAVYTIME) classification detector, one of the top performers in the TempEval-3 event detection task.
tokenization	UW SPF	We use EasyCCG () trained with the re-banked CCGBank) to generate CCGBank categories, the Illinois Named Entity Tagger for NER, Stanford CoreNLP () for tokenization and part-of-speech tagging and UW SPF) to develop our system.
Initialization	JAMR	Initialization and Parameters We created the seed lexicon from the training data by sampling and annotating 50 sentences with lexical entries, adding entries for pronouns and adding lexemes for all alignments generated by JAMR ().
ASR	Fisher Spanish-English speech translation corpus	We demonstrate that several standard features utilized by ASR and SMT systems can be used in such a model at the speech-translation interface, and we provide empirical results on the Fisher Spanish-English speech translation corpus .
SMT	Fisher Spanish-English speech translation corpus	We demonstrate that several standard features utilized by ASR and SMT systems can be used in such a model at the speech-translation interface, and we provide empirical results on the Fisher Spanish-English speech translation corpus .
	BSU semantic link network	 Summary is built from sentence to sentence to a coherent body of information based on the BSU semantic link network by summary structure planning.
summarization	DUC 2002 and DUC 2004 documents	As we could conclude from our experiments, the best summarization results are obtained if this parameter is set to the maximal number of words in the summary, W . Consequently, we used 100 codes for summarizing DUC 2002 and DUC 2004 documents and 250 codes for DUC 2007 documents.
summarization	DUC 2007 documents	As we could conclude from our experiments, the best summarization results are obtained if this parameter is set to the maximal number of words in the summary, W . Consequently, we used 100 codes for summarizing DUC 2002 and DUC 2004 documents and 250 codes for DUC 2007 documents.
summarizing	DUC 2002 and DUC 2004 documents	As we could conclude from our experiments, the best summarization results are obtained if this parameter is set to the maximal number of words in the summary, W . Consequently, we used 100 codes for summarizing DUC 2002 and DUC 2004 documents and 250 codes for DUC 2007 documents.
summarizing	DUC 2007 documents	As we could conclude from our experiments, the best summarization results are obtained if this parameter is set to the maximal number of words in the summary, W . Consequently, we used 100 codes for summarizing DUC 2002 and DUC 2004 documents and 250 codes for DUC 2007 documents.
Robert E. Quinn	Rhode Island	Subsequently, the death place of Robert E. Quinn is Rhode Island.
summarization of user-generated content	DUC datasets	This dataset is a collection of user reviews in 51 different topics such as hotels, cars, and products; thus, it is more appropriate for evaluating summarization of user-generated content than wellknown DUC datasets, which consist of formal news articles.
answer sentence selection	TREC-QA data	Most previous work on answer sentence selection focuses on a dataset created using the TREC-QA data, which includes editor-generated questions and candidate answer sentences selected by matching content words in the question.
answer triggering	WIKIQA dataset	We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the WIKIQA dataset.
Translation	NNJM+UPD	 Table 2: Translation results. The symbol + and *  represent significant differences at the p < 0.01  level against Base and NNJM+UPD, respectively.  Significance tests were conducted using bootstrap  resampling
classification	Yelp dataset	For classification we use the Yelp dataset which is a customer review dataset.
summarization	microblog posts data collected from Sina Weibo	To evaluate the two modules in our repost tree summarization system, i.e., CRF-based model for leader detection and LeadSum model for summarization, we conducted two sets of experiments based on microblog posts data collected from Sina Weibo, which has a similar market penetration as Twitter (Rapoza, 2011) 2 . Microblog messages on Sina Weibo are in Chinese and we use FudanNLP () for text preprocessing including word segmentation and POS tagging.
Sentiment classification	movie review datasets	 Table 1: Sentiment classification accuracies on  two movie review datasets (Pang and Lee, 2004;  Socher et al., 2013), described in Section 2.3.
WS	Recall Prec. F-meas.  Baseline  90.12 91.43  90.77  + Sym.Gro. 90.60 91.66  91.13	 Table 3: WS accuracy on Shogi commentaries.  Recall Prec. F-meas.  Baseline  90.12 91.43  90.77  + Sym.Gro. 90.60 91.66  91.13
parsing	Stanford sentimentbank dataset	Recurrent models without parse structures have shown good results in sequenceto-sequence generation ) for machine translation (e.g.,), parsing ( , and sentiment, where for example recurrent-based paragraph vectors () outperform recursive models) on the Stanford sentimentbank dataset.
Phrase Matching	UMD-QA dataset	• Phrase Matching on the UMD-QA dataset) can help seethe difference between outputs from intermediate components from different models, i.e., representations for intermediate parse tree nodes and outputs from recurrent models at different time steps.
Sentiment Classification	Stanford Sentiment Treebank (Socher et al., 2013)	• Sentiment Classification on the Stanford Sentiment Treebank (Socher et al., 2013): comprehensive labels are found for words and phrases where local compositionally (such as from negation, mood, or others cued by phrase-structure) is to be learned.
ASTD	Arabic Sentiment Tweets Dataset	ASTD: Arabic Sentiment Tweets Dataset
sentiment analysis	TPD	In sentiment analysis task, Apriori's minimum support and TPD's theta 0 is respectively set as 0.004 and 1.4.
topic discovery task	TPD	While in topic discovery task, Apriori's minimum support and TPD's theta 0 is around 0.020 and 2.0 respectively.
neural network parsing	Penn Treebank	The resulting parser achieves performance far greater than previous approaches to neural network parsing (), and only marginally below the current state-of-the-art for parsing the Penn Treebank.
SSN parser	Penn Treebank	The generality and efficiency of the above parsing model makes it possible to test a SSN parser on the Penn Treebank (, and thereby compare its performance directly to other statistical parsing models in the literature.
translation	TOEIC	Consequently, the translation capability of the language translation system equals that of an examinee with a score of around 700 points on the TOEIC.
utterance unit evaluation	NIST	Consequently, for the utterance unit evaluation, we conducted several experiments while varying N from 1 to 4 for BLEU and from 1 to 5 for NIST.
translation knowledge acquisition	WWW news sites	Within the framework of translation knowledge acquisition from WWW news sites, this paper studies issues on the effect of cross-language retrieval of relevant texts in bilingual lexicon acquisition from comparable corpora.
translation knowledge acquisition	WWW news sites	illustrates the overview of our framework of translation knowledge acquisition from WWW news sites.
translation knowledge acquisition	WWW news sites	Within this framework of translation knowledge acquisition from WWW news sites, this paper studies issues on the effect of cross-language retrieval of relevant texts in bilingual lexicon acquisition from comparable corpora.
GR extraction	Penn Treebank style parses	We introduce GRs in Section 3 and briefly describe our GR extraction software for Penn Treebank style parses.
Sizes	Spanish and Catalan data sets	 Table 1: Sizes of Spanish and Catalan data sets
MT	TOEIC	(3) Estimated TOEIC score: It is important to interpret MT performance from the viewpoint of a language proficiency test such as TOEIC . A translator compared MT translations with human ones, then, MT's proficiency is estimated by regression analysis ().
Tagger	WSJ 23	 Table 7: Tagger performance on WSJ 23
corefer-ence resolution	GATE	We present a robust summarisation system developed within the GATE architecture that makes use of robust components for semantic tagging and corefer-ence resolution provided by GATE.
coreference resolution	GATE	Here, we present a summarisation system that makes use of robust components for semantic tagging and coreference resolution provided by GATE ().
coreference resolution	DIANA-Summ dialog summarization project	The system has been developed on the basis of meeting transcriptions from the ICSI Meeting Corpus (, and it is intended as a preprocessing component fora coreference resolution system in the DIANA-Summ dialog summarization project.
Dependency parsing	Czech	 Table 2: Dependency parsing results for Czech.
Dependency parsing	Danish	 Table 3: Dependency parsing results for Danish.
Fuzzy Sentiment	WordNet Glosses	Mining WordNet for Fuzzy Sentiment: Sentiment Tag Extraction from WordNet Glosses
Sentiment Tag Extraction	WordNet Glosses	Mining WordNet for Fuzzy Sentiment: Sentiment Tag Extraction from WordNet Glosses
labelling task	FrameNet data	shows that for the labelling task, our model outperforms the labelling baseline and the SVM labeller on the FrameNet data by at least 16 points F score while the correlation with human data remains significant.
relation extraction	AImed corpus	We evaluated our relation extraction algorithm on two biomedical data sets (i.e. the AImed corpus and the LLL challenge data set; see Section 4).
relation extraction	LLL challenge data set	We evaluated our relation extraction algorithm on two biomedical data sets (i.e. the AImed corpus and the LLL challenge data set; see Section 4).
Statistical Machine Translation (SMT)	NIST Evaluation Campaigns 1	At this time, Statistical Machine Translation (SMT) has empirically proven to be the most competitive approach in international competitions like the NIST Evaluation Campaigns 1 and the International Workshops on Spoken Language Translation.
Reinforcement Learning	TALK data collection format	For these reasons we have built a system which: contains an interface to a dialogue strategy learner module, covers a realistic domain of useful "in-car" conversation and a wide range of dialogue phenomena (e.g. confirmation, initiative, clarification, information presentation), can be used to complete measurable tasks (i.e. there is a measure of successful and unsuccessful dialogues usable as a reward signal for Reinforcement Learning), logs all interactions in the TALK data collection format ( ).
predicting pollen concentrations	Aerospace and Marine International (UK) Ltd (AMI)	The subtask of predicting pollen concentrations is carried out by our industrial collaborator, Aerospace and Marine International (UK) Ltd (AMI).
direction	EmEliza	It has not yet actually been used for direction, but instead to control a simple automated bit-part actor, EmEliza.
Domain adaptation	BNC	 Table 3: Domain adaptation results: Train on  BNC and SPORTS, test on SPORTS (same for FI-
Domain adaptation	FI	 Table 3: Domain adaptation results: Train on  BNC and SPORTS, test on SPORTS (same for FI-
Named entity recognition (NER)	MUC and ACE 2 conferences	Named entity recognition (NER) is not anew domain (see MUC and ACE 2 conferences) but some new needs appeared concerning NEs processing.
parsing	PMCFG	We present a parsing algorithm that is incremental, top-down and supports PMCFG directly.
Translation	NIST test set	 Table 2: Translation Results depending on sen- tence length for NIST test set.
MT evaluations	Wall Street Journal)	• Use the models trained on data from MT evaluations to predict potential fluency problems of human-written texts (from the Wall Street Journal).
Parsing	FST	 Table 4: Parsing performance using the FST approach
Disfluency detection	SSR test subcorpus	 Table 1: Disfluency detection performance on the SSR test subcorpus using JC04 system.
Disfluency detection	JC04	 Table 1: Disfluency detection performance on the SSR test subcorpus using JC04 system.
edit detection	SSR test subcorpus	 Table 2: Deeper analysis of edit detection performance on the SSR test subcorpus using JC04 system.
edit detection	JC04	 Table 2: Deeper analysis of edit detection performance on the SSR test subcorpus using JC04 system.
segmentation	Beijing University (PKU)	We present an experiment in improving the output of an off-the-shelf module that performs segmentation and tagging , the tokenizer-tagger from Beijing University (PKU).
disambiguating person names	Britannica	While biographic facts are certainly useful for disambiguating person names, they also allow for automatic extraction of encylopedic knowledge that has been limited to manual efforts such as Britannica, Wikipedia, etc.
MRS analyses	English Resource Grammar (ERG	MRS analyses () derived from deep grammars, such as the English Resource Grammar (ERG,) are special cases of RMRS.
Translation	TEST	 Table 7: Translation performances on TEST. P smt  stands for the precision and recall of the SMT en- gine. ∆B indicates the absolute gain in BLEU  score of the combined system.
PoS error detection	IFD corpus	In this paper, we experiment with three different methods of PoS error detection using the IFD corpus.
tagging	IFD	 Table 2: Average tagging accuracy (%) using the  original IFD corpus
BS	NIST task	 Table 10: Scalability of BS on NIST task
disambiguation of ambiguous words	WordNet	Experiments are performed on the disambiguation of ambiguous words in the glosses of WordNet and two machine-readable dictionaries.
Named entity recognition (NER)	MUC	Named entity recognition (NER) for En-glish typically involves one of three gold standards: MUC, CoNLL, or BBN, all created by costly manual annotation.
parsing	written part of Talbanken05	All parsing experiments are performed using 10-fold cross-validation for training and testing on the entire written part of Talbanken05.
parsing experiments	Penn Treebank)	We evaluate our parser on the standard data set for parsing experiments (i.e. the Penn Treebank) and compare it with existing approaches to full parsing.
Translation	Bleu	 Table 4: Translation results for Bleu. Baseline with POS: 20.19, without POS: 19.66. Results that are  better than the baseline are marked with bold face.
DS	FS	This result brings DS closer to the central concerns of FS.
dependency parsing of Hungarian	Szeged Dependency Treebank	Here, we introduce results on dependency parsing of Hungarian that employ a 80K, multi-domain, fully manually annotated corpus, the Szeged Dependency Treebank.
Named Entity Recognition (NER)	French broadcast data	In this paper we deal with Named Entity Recognition (NER) on transcriptions of French broadcast data.
parse structure	CHILDES corpus	We define a Bayesian model of parse structure with Dirichlet process priors and train this on a set of (utterance, meaning-candidates) pairs derived from the CHILDES corpus) using online variational Bayesian EM.
parsing	CFG-CTF	compares the parsing times of LCFRS with and without the new CFG-CTF method.
parsing	CFG-CTF	The graph shows a steep incline for parsing with LCFRS directly, which makes it infeasible to parse longer sentences, while the CFG-CTF method is faster for    from the PCFG chart.
parsing	PCFG chart	The graph shows a steep incline for parsing with LCFRS directly, which makes it infeasible to parse longer sentences, while the CFG-CTF method is faster for    from the PCFG chart.
Parsing	NEGRA-40	Parsing with NEGRA-40 took about 11 hours and 4 GB of memory.
WSI	SemEval datasets	We next demonstrate that our interpretation of HDP-based WSI is superior to other topic model-based approaches to WSI, and indeed, better than the bestpublished results for both SemEval datasets.
JSMs	SRILM Toolkit	Our JSMs are trained using the SRILM Toolkit.
classification task	WEKA	To determine the most accurate algorithm for our classification task, two comparisons of learning algorithms implemented in WEKA) were carried out.
SMT	Europarl 8	For our experiments we used the phrase-based, open-source SMT toolkit Moses 6 ( . For language modeling, we computed 5-gram models using IRSTLM) and queried the model with KenLM shows a first comparison of results of Moses models trained on 500,000 parallel sentences from patent text sections balanced over IPC classes, against Moses trained on 1.7 Million sentences of parliament proceedings from Europarl 8 ().
Mixture	IPC sections	 Table 11: Mixture and pooling on IPC sections.
pooling	IPC sections	 Table 11: Mixture and pooling on IPC sections.
rect identification of the English antecedent head noun	PCEDT 2.0 alignment file	An example of where the Annotated system correctly drops a pronoun is: rect identification of the English antecedent head noun, (2) incorrect identification of the Czech translation of the antecedent head noun in the Baseline output due to errors in the word alignments, and (3) errors in the PCEDT 2.0 alignment file (affecting training only).
MT	LATL	We use the Europarl corpus to evaluate two MT systems on their performance regarding null subject translation: Its-2, a rule-based system developed at LATL, and a statistical system built using the Moses toolkit.
MERT	NIST MT06 data set	The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences.
MT training	Hong Kong News (LDC2004T08)	Our MT training corpus contains 2,636,692 sentence pairs from two parallel corpora: Hong Kong News (LDC2004T08) and Chinese English News Magazine Parallel Text (LDC2005T10).
MT training	Chinese English News Magazine Parallel Text (LDC2005T10)	Our MT training corpus contains 2,636,692 sentence pairs from two parallel corpora: Hong Kong News (LDC2004T08) and Chinese English News Magazine Parallel Text (LDC2005T10).
Statistical Parsing of Morphologically Rich Languages	TedEval	From an empirical point of view, the organizers of the recent shared task on 'Statistical Parsing of Morphologically Rich Languages') provided datasets only for languages having treebanks in both dependency and constituency format and their cross-framework evaluation -employing the unlabeled TedEval () as evaluation procedure -revealed that at 4 out of 9 morphologically rich languages, the results of constituent parsers were higher than the scores achieved by the best dependency parsing system.
SMT domain adaption	Europarl	Many techniques for SMT domain adaption have focused on rather diverse domains such as using systems trained on Europarl or news to translate medical articles), blogs () and transcribed lectures).
POS tagging task	CTB5	 Table 4: Final results of the POS tagging task on  CTB5.
dependency parsing  task	CTB5	 Table 5: Final results of the dependency parsing  task on CTB5.
translating	MTBT	The sample would not be helpful at all or it might even be harmful for translating the MTBT.
translation grammar	MIRA	Compared to a competitive baseline, we show substantial improvement from updating the translation grammar or language model independently and super-additive gains from combining these techniques with a MIRA update ( §6).
paraphrase ranking	SEMEVAL lexical substitution data	We also identify significant differences to results for paraphrase ranking in context reported for the SEMEVAL lexical substitution data.
MT	tst2011 test set	 Table 5: Comparison of the baseline translation  systems with the systems augmented with syn- thetic phrases. We measure EN-FR MT perfor- mance on the tst2011 test set: reference tran- scripts and ASR outputs on from five systems  and their ROVER combination. Improvements in  translation of all ASR outputs are statistically sig- nificant. This confirms the claim that incorporat- ing simulated ASR errors via synthetic phrases ef- fectively adapts MT to SLT scenario.
passage retrieval	TREC QA corpus	We evaluated our different rerankers encoding several semantic structures on passage retrieval task, using a factoid open-domain TREC QA corpus.
syntactic parsing	Stanford Parser (Rafferty and Manning, 2008)	For syntactic parsing, Stanford Parser (Rafferty and Manning, 2008) was used.
generic summarization	DUC	These are the last two years in which generic summarization was evaluated at DUC workshops.
summariser	LexRank	Our summariser controls word count precisely; we require MEAD to produce summaries close to the length (allowing variations), and for LexRank we allow it to go beyond the limit by less than one sentence and then discard the exceeding part in the sentence with the lowest salience.
summariser	MEAD's	Our summariser is good at precision because many summaries produced have not used up the 100-word limit, making the average summary length smaller than that of MEAD's.
machine translation system combination	Jane toolkit	We present a novel machine translation system combination framework which has been implemented and released as part of the most recent version of the Jane toolkit.
SMT	Europarl ( corpus	In order to evaluate the quality of locating the wrong term translation, we applied the terminology verification service to an SMT model trained with Moses () on the Europarl ( corpus.
information extraction	GATE	We implement an information extraction architecture for Danish within GATE, including integrated third-party tools.
alignment	Bulgarian-English Sentence-and Clause-Aligned Corpus	Anaphora has been successfully applied for the annotation and the alignment of the Bulgarian-English Sentence-and Clause-Aligned Corpus (Koeva et al.
recognition of entailment by speakers	RTE 1-3	This allows us not only to indicate linguistic phenomena that are involved in the recognition of entailment by speakers, but also to provide formal proofs that substantiate the annotations and explain how the Pairs of sentences in RTE 1-3 are categorized in two classes: yes-or no-entailment; pairs in RTE 4-5 are categorized in three classes: entailment, contradiction and unknown.
recognition of entailment by speakers	RTE	This allows us not only to indicate linguistic phenomena that are involved in the recognition of entailment by speakers, but also to provide formal proofs that substantiate the annotations and explain how the Pairs of sentences in RTE 1-3 are categorized in two classes: yes-or no-entailment; pairs in RTE 4-5 are categorized in three classes: entailment, contradiction and unknown.
MRSs	PETE  development data	 Table 1: The results for 1-best MRSs for the PETE  development data.
NE tagger	WDC	Judging by the results, an NE tagger trained on WDC can compete with those trained on manually annotated corpora.
SRC	ODP-239	Within the specific case of SRC, different metrics have been used such as F 1 -measure (F 1 ), kSSL 1 and F b 3 -measure (F b 3 ) over different standard datasets: ODP-239 and Moresque (.
SRC	Moresque	Within the specific case of SRC, different metrics have been used such as F 1 -measure (F 1 ), kSSL 1 and F b 3 -measure (F b 3 ) over different standard datasets: ODP-239 and Moresque (.
SRC	AMBIENT + Moresque	 Table 2: State-of-the-art Results for SRC. (*) The  result of (Di Marco and Navigli, 2013) is based  on a reduced version of AMBIENT + Moresque.
MA	Viterbi search	However, computation cost of modern MA systems is mainly attributed to the Viterbi search as point out.
Spearman	hu- man gold standard	 Table 5: Spearman rank correlations with the hu- man gold standard (  *  : only the 3 most frequent  modifiers are used (see
SMT	Europarl Corpus	Parallel corpora used in SMT, for instance the Europarl Corpus (), tend to contain few (up to tens of) languages, but many (up to billions of) words in each language.
parsing	Penn Treebank	Supertagging and parsing experiments on the Penn Treebank () are shown in Section 4.
parse	English test data	 Table 2: Time taken to parse English test data.
parsing	PCFG baseline	Despite increasing the size of the grammar, our annotation allows for parsing more than twice as fast as the PCFG baseline.
rule extraction	MT08 test sets	Comparing to the results of PBMT, this suggests our method maybe most effective in improving systems where rule extraction is sen- We use the standard MT08 test sets; the training data includes (34M English words and 1.1M sentences).
Systemic Linguistics 3	Air Force Office of Scientific Research Contract NO	Here I will take a look at Systemic Linguistics 3 in the service of computational linguistics tasks, concentrating on a 1This research was Supported by the Air Force Office of Scientific Research Contract NO.
WP text-checking	AtweU 86]	The UNIX Writer's Workbench collection of programs (see [Cherry and Macdonald 83], [Cherry et ai 83]) is probably the most widely-used system for WP text-checking (and also one of the most widely-used NLP systems overall-see [AtweU 86], [Hubert 85]).
grammatical description and analysis	Atwell 83]	CLAWS can deal with Unrestricted English text input including "noisy" or ill-formed sentences, because it is based on Constituent Likelihood Grammar, a novel probabilistic approach to grammatical description and analysis described in [Atwell 83].
parsing	BLR/ELR	In the current version, the CM largely relies on the BEGS specialist for structudng the parsing process and for generating the BLR/ELR: each sentence in the text is processed one after the other, from left to right.
SS	CM	SS needs also morphological information contained in the Dictionary, that will be requested through an appropriate message to the CM.
recognition of spoken Italian	IBM Rome Scintific Center	This paper discusses theoretical and practical issues regarding an approach to building such a language model based on any equivalence criterion defined on incomplete sentences, and experimental results and measurements performed on such a model of the Italian language, which is apart of the prototype for the recognition of spoken Italian built at the IBM Rome Scintific Center.
MT	ALPAC report	The question whether linguistics is able to offer a reliable theoretical basis for MT cannot be answered in a qualified way without examining such linguistically based systems as Garvin~s'fulcrum" approach {which was abolished on external grounds, after the unfortunate ALPAC report~ or the systems formulated by Kulagina and Apresyan.
Ambiguity Resolution	DMTRANS PLUS	Ambiguity Resolution in the DMTRANS PLUS
rids article	EURO'rH.A-D accompanying research project K IT-FAST	Most of the research underlying rids article was accomplished within the EURO'rH.A-D accompanying research project K IT-FAST at the Technical University of Berlin and fimded by the BMFT trader contract 1013211.
rids article	BMFT trader contract 1013211	Most of the research underlying rids article was accomplished within the EURO'rH.A-D accompanying research project K IT-FAST at the Technical University of Berlin and fimded by the BMFT trader contract 1013211.
Parsing	MorP	Parsing without lexicon: the MorP system
bracketing	ATIS corpus	In, 90.36% bracketing accuracy was reported using a stochastic CFG trained on bracketings from the ATIS corpus.
translation	Oxford University Press	As far as translation is concerned, we no longer need a transfer rule mapping 'strong criticism' on Z We would hereby like to acknowledge the financial support by the Commission of the European Community , Association Suissetra (Geneva) and Oxford University Press.
SR learning task	WordNet	Evaluation of the SR learning task would provide grounds to compare different techniques that try to abstract SRs from corpus using WordNet (e.g, section 4.2).
anaphora resolution within sentence boundaries	GB's binding theory	Criteria for anaphora resolution within sentence boundaries rephrase major concepts from GB's binding theory, while those for text-level anaphora incorporate an adapted version of a Grosz-Sidner-style focus model.
binding theory	GB framework	As we conceive it, binding theory as developed within the GB framework offers one of the most sophisticated approaches for treating anaphora at the sentence level of description.
Splitting	DRT	Splitting the Reference Time: Temporal Anaphora and Quantification in DRT
Extraposition	HPSG	Towards an Account of Extraposition in HPSG*
CFG parsing	CFG	The standard methods for CFG parsing are the CKY algorithm and Earley's algorithm, both of which have a worst-case running time of O(gN 3) fora CFG (in Chomsky normal form) of size g and a string of length N. give a variant of Earley's algorithm which runs in time O(gN3/log N).
syntactic analysis	Penn Tree-bank	This approach has been successfully used for syntactic analysis , using corpora with syntactic annotations such as the Penn Tree-bank.
referential description	Donellan	Over the last decade,, and others 2 have contributed to this issue The term 'referential description' is due to Donellan.
SIMR	U.N. annual report  I.L.O. report	 Table 2: SIMR accuracy on different text genres in three language pairs.  language  number of  number of  RMS Error  pair  training TPCs  genre  test TPCs  in characters  French / English  598  parliamentary debates  CITI technical reports  other technical reports  court transcripts  U.N. annual report  I.L.O. report
segmentation	WSJ	In this paper we will report all scores as a balanced F-measure (precision and recall weighted equally) with/~ = 1, such that F = 2PR/(P + R)  For an initial experiment, segmentation was performed using the maximum matching algorithm, with a large lexicon of 34272 English words compiled from the WSJ.
parsing and translating natural language	Wall Street Journal	We present a knowledge and context-based system for parsing and translating natural language and evaluate it on sentences from the Wall Street Journal.
document retrieval	Harlequin Ltd	One such application is document retrieval or automated document forwarding: documents annoted with NE information can be searched more "Now also at Harlequin Ltd.
range concatenation grammar (RCG)	Boullier 98a]	On the other hand, range concatenation grammar (RCG), presented in [Boullier 98a], is a syntactic formalism which is a variant of simple literal movement grammar (LMG), described in [Groenink 97], and which is also related to the framework of LFP developed by [Rounds 88].
chunking	Penn Treebank data	Results for chunking Penn Treebank data were previously presented by several authors 'the dismissal of the federation from several areas that was intended by the government'   because they processed a different language and generated only one layer of structure (the chunk boundaries), while our algorithm also generates the internal structure of chunks.
Supertag disambiguation	GER-9354869	Supertag disambiguation is resolved "Supported by NSF grants ~SBR-9710411 and ~GER-9354869 by using statistical distributions of supertag cooccurrences collected from a corpus of parses.
parsing	EDOL	The argument, due to, is that benefits to parsing arise from lexicalization, and that lexicalization is only possible because of the EDOL.
IE	MUC-5	Unfortunately one of the current trends in IE is the progressive reduction of the size of training corpora: e.g., from the 1,000 texts of the MUC-5) to the 100 texts in.
IE	WordNet	In this paper we propose a methodology for semi-automatically developing the relevant part of a lexicon (foreground lexicon) for IE applications by using both a small corpus and WordNet.
distribution translation task	Bible bitext	To study how the benefits of the various biases vary with training corpus size, I evaluated Models A, B, C, and 1 on the whole distribution translation task, after training them on three different-size subsets of the Bible bitext.
parsing	MUC-6	As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.
parsing	MUC-7	As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.
coreference" annotations	MUC community	In this paper, it is argued that "coreference" annotations, as performed in the MUC community for example, go well beyond annotation of the relation of coreference proper.
IE tasks	MUC-6	We begin with an overview of IE tasks, systems, and evaluation, including the alignment procedure used for MUC-6.
pronoun resolution	BFP	The algorithm is tested against three other leading pronoun resolution algorithms: Hobbs's naive algorithm, S-list (Strube 1998), and BFP.
information extraction (IE)	DARPA Message Understanding Conferences	In particular, information extraction (IE) systems like those builtin the DARPA Message Understanding Conferences have revealed that coreference resolution is such a critical component of IE systems that a separate coreference subtask has been defined and evaluated since.
coreference resolution	MUC-6	In order to evaluate the performance of our learning approach to coreference resolution on common data sets, we utilized the annotated corpora and scoring programs from MUC-6 and MUC-7, which assembled a set of newswire documents annotated with coreference chains.
coreference resolution	MUC-7	In order to evaluate the performance of our learning approach to coreference resolution on common data sets, we utilized the annotated corpora and scoring programs from MUC-6 and MUC-7, which assembled a set of newswire documents annotated with coreference chains.
interpretation of nominalizations	British National Corpus (BNC)	Section 4 describes the algorithm used for the interpretation of nominalizations, and Section 5 reports the results of several experiments that achieve a combined accuracy of 86.1% on the British National Corpus (BNC).
Question detection	8E-CH corpus	 Table 14  Question detection on the 8E-CH corpus using two different classifiers.
Automatic Labeling of Semantic Roles Class-Based Probability Estimation	The Cem Bozsahin 28	Title Index Volume 28 Articles Automatic Labeling of Semantic Roles Class-Based Probability Estimation Using a Semantic Hierarchy Combinatory Morphemic Lexicon, The Cem Bozsahin 28(2):145-186 Critique and Improvement of an Evaluation Metric for Text Segmentation, A
beam search	Verbmobil	The beam search procedure has been successfully tested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary).
beam search	Canadian Hansards task	The beam search procedure has been successfully tested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary).
Translation	Canadian Hansards task	Translation experiments are carried out for the translation directions German to English and English to German (Verbmobil task) and for the translation directions French to English and English to French (Canadian Hansards task).
translation	Canadian Hansards task	In Section 4.3, translation results for the Canadian Hansards task are reported.
translation	Hansards corpus	For the translation experiments on the Hansards corpus, no word joining is carried out.
translation direction English to German	TEST-331 test set	 Table 12  Translation results for the translation direction English to German on the TEST-331 test set.  The results are given in terms of computing time, WER, and PER for three different reordering  constraints: MON, EG, and S3.
translating	NP	In translating (2), however, observations is a feminine plural noun, so the adjective mesquines is inserted to maintain agreement throughout the NP.
MT	Penn-II Treebank	If the three on-line MT systems translate the phrases extracted from the Penn-II Treebank in different ways, then combining systems to obtain results for AB, AC, BC, and ABC always involves an increase in the number of translations produced, both for sentences and noun phrases.
MT	NP test set	System Ranked 1 Ranked 2-5 Ranked 6-10 Ranked 10-20 Ranked  As we did with sentences, we seeded our EBMT system with fragments derived from the three different on-line MT systems and confronted it with the NP test set.
SENSEVAL-2 workshop	WordNet	At the time of the SENSEVAL-2 workshop, this was assumed to be due largely to the use of WordNet as the inventory, as opposed to HECTOR (Atkins 1993), but Palmer, Trang Dang, and Fellbaum (forthcoming) have subsequently shown that, at least for the lexical sample tasks, this was due to a harder selection of words, with a higher average level of polysemy.
translation	Nespole!	The improvement of the translation results is demonstrated on two German-English corpora taken from the Verbmobil task and the Nespole!
Resolution	Verbmobil corpus	 Table 3  Resolution of ambiguity on the Verbmobil corpus.
Translation	Hansards task	 Table 10  Translation results on the Hansards task.
parsing	Wall Street Journal treebank	We apply the boosting method to parsing the Wall Street Journal treebank.
parsing	Wall Street Journal (WSJ) treebank	We applied the boosting method to parsing the Wall Street Journal (WSJ) treebank.
semantic representation	Penn Treebank	The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank.
RST	DGT	I followed from afar the further developments in his life: his departure from ISI and move to Kenya with his family to take up a position with the Summer Institute of Linguistics (SIL) as a consultant responsible for work on discourse and computational matters, their return to the United States, and his new phase of research with initiatives in RST and DGT.
summarization	Newsblaster framework	In our previous work, we evaluated the overall summarization strategy of MultiGen in multiple experiments, including comparisons with human-written summaries in the Document Understanding Conference (DUC) 11 evaluation ( and quality assessment in the context of a particular information access task in the Newsblaster framework (.
FT detection	MSR gold test set	 Table 8  FT detection results on the MSR gold test set. The 'All' column shows the results of detecting all  10 types of factoids, as described in
machine translation	Cambridge Language Research Unit	In the late 1950s, as an assistant lecturer in Chinese at Cambridge, he worked in a project concerned with machine translation at the Cambridge Language Research Unit (directed by Margaret Masterman and which was later to host researchers such as Karen Spärck Jones and Yorick Wilks).
classifying semantic relations	VSM	On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM.
sentence boundary detection	Estonian corpus	For the sentence boundary detection task, the error rates Punkt achieved on the eleven newspaper corpora range from 2.12% on the Estonian corpus to only 0.35% on the German corpus with an average error rate of 1.26%.
sentence boundary detection	German corpus	For the sentence boundary detection task, the error rates Punkt achieved on the eleven newspaper corpora range from 2.12% on the Estonian corpus to only 0.35% on the German corpus with an average error rate of 1.26%.
abbreviation detection	Estonian corpus	The error rates for abbreviation detection are slightly lower, lying between 1.75% on the Estonian corpus and 0.26% on the German corpus with an average of 0.80% for all eleven corpora.
abbreviation detection	German corpus	The error rates for abbreviation detection are slightly lower, lying between 1.75% on the Estonian corpus and 0.26% on the German corpus with an average of 0.80% for all eleven corpora.
Translation	European Parliament Plenary Sessions (EPPS)	Translation performance, which happens to be in the state of the art, is demonstrated with Spanish-to-English and English-to-Spanish translations of the European Parliament Plenary Sessions (EPPS).
Translation	mWER	Translation results are evaluated in terms of mWER and BLEU by using the two references available for each language test set.
Grammar induction	Penn Treebank	Grammar induction based on annotation has been very successful for the Penn Treebank, where a corpus of English text was annotated with syntactic information.
translation	NIST test set	The table additionally shows the translation quality achieved by the system PBT on the NIST test set.
Translation	NIST test corpora	 Table 4  Translation quality of different MT systems on the TransType2 and the NIST test corpora.
MT	NIST test corpora	 Table 4  Translation quality of different MT systems on the TransType2 and the NIST test corpora.
Translation	EPPS Spanish → English test set	 Table 12  Translation quality for rescoring with confidence measures. EPPS Spanish → English test set.  Optimized for BLEU.
IR	TREC 1 in 1999	Currently we are witnessing a surge of activity in the area from the perspective of IR, initiated by the Question Answering track of TREC 1 in 1999.
summarizing the incidence of diseases and operations	CPT4	For example, in the medical domain alone there area large number of clinical terminologies and classifications, used for different purposes: Some classifications, such as ICD-9, ICD-10, and OPCS-4, are employed in summarizing the incidence of diseases and operations on a national or worldwide level; others, such as CPT4 or ICD-9CM, manage the process of billing patients.
summarizing the incidence of diseases and operations	ICD-9CM	For example, in the medical domain alone there area large number of clinical terminologies and classifications, used for different purposes: Some classifications, such as ICD-9, ICD-10, and OPCS-4, are employed in summarizing the incidence of diseases and operations on a national or worldwide level; others, such as CPT4 or ICD-9CM, manage the process of billing patients.
SMT	NIST evaluations	We find this lack of correlation between previous word alignment quality metrics and BLEU counterintuitive, because we and other researchers have measured this correlation in the context of building SMT systems that have benefited from using the BLEU metric in improving performance in open evaluations such as the NIST evaluations.
translation	Penn Treebank	Here we first examine briefly the coverage of the translation algorithm on the entire Penn Treebank.
parsing	CCGbank	This model and parsing algorithm, when combined with normal-form constraints, give state-of-the-art accuracy for the recovery of predicate-argument dependencies from CCGbank.
parsing	BFGS	These methods are either too slow or sacrifice parsing performance, and so we use a parallelized version of BFGS running on an 18-node Beowulf cluster to perform the estimation.
parsing	Beowulf cluster	These methods are either too slow or sacrifice parsing performance, and so we use a parallelized version of BFGS running on an 18-node Beowulf cluster to perform the estimation.
parsing	Section	The results for parsing speed were obtained using Section 23.
identifying the first sense of a word	NEWSWIRE corpus	Looking at in more detail, it seems to be the case that although the BNC thesaurus does well in identifying the first sense of a word (the type results), the PROX and DEP thesauruses from the NEWSWIRE corpus return better WSD results when used with the jcn measure.
SFC	WordNet	 Table 15  Most frequent SFC labels for all senses of polysemous words in WordNet, by part of speech.
SRL task	FrameNet	SemEval-2007 also included an SRL task based on FrameNet.
ASSERT	WSJ corpus	In this section, we present a series of experiments comparing the performance of ASSERT on the WSJ corpus to performance on the Brown corpus.
ASSERT	Brown corpus	In this section, we present a series of experiments comparing the performance of ASSERT on the WSJ corpus to performance on the Brown corpus.
ASSERT	WSJ test set	 Table 2  Performance of ASSERT on WSJ test set (Section 23) using correct Treebank parses as well as  Charniak parses.
ASSERT	WSJ	 Table 5  Performance on the entire PropBanked Brown corpus when ASSERT is trained on WSJ.
parsing	Turkish Treebank	Applying the techniques presented in this article, we achieve the highest reported accuracy for parsing the Turkish Treebank.
MT	IPMT assessment	The PBMs show better performance for both the off-line MT and for the IPMT assessment figures.
MT	Xerox corpus	As in the MT experiments, the results are comparable to those obtained on the Xerox corpus, with the exception of the English-Spanish pair.
beam search	Xerox corpus	In order to run the EU experiments within reasonable time limits, all the systems have required the use of beam search and/or other  suboptimal pruning techniques, although this was largely unnecessary for the Xerox corpus.
preposition disambiguation	PTB	In particular, here we specifically address preposition disambiguation using semantic role annotations from PTB, FrameNet, and Factotum.
preposition disambiguation	FrameNet	The second set of experiments perform preposition disambiguation using FrameNet.
preposition disambiguation	Factotum	The third set of experiments deals with preposition disambiguation using Factotum.
summarization evaluation	NIST TAC 2008 summarization track	The main summarization evaluation technique in the NIST TAC 2008 summarization track is the pyramid technique (Nenkova and Passonneau 2004), which is a structured human-based evaluation, based on asking human judges to identify 'summarization content units' (SCU) in model and system-generated summaries, and measuring how many SCUs from the model summaries occur in the system summary.
parsing complexities	GIZA-aligned Chinese-English data	In, we compare the approximate strategy which takes the minimum of CKY runs for The distribution of parsing complexities of non-binarizable rules extracted from the GIZA-aligned Chinese-English data in Section 5.
parsing complexities	GIZA-aligned Chinese-English data	 Table 4  The distribution of parsing complexities of non-binarizable rules extracted from the  GIZA-aligned Chinese-English data in Section 5. The first column denotes the exponent of the  time complexity-for example, 10 means O(|w| 10 ). opt denotes the optimal parsing strategy and  cky-min denotes the approximation strategy that takes the better of the CKY results on both sides.
HHMM parsing	WSJ Sections 22	The results for HHMM parsing, training, and evaluating on these same binarized trees (modulo right-corner and variable-mapping transforms) were substantially better than binarized CKY, most likely due to the expanded HHMM dependencies on previous (q d t−1 ) and parent (q d−1 t ) variables at each q d t . For example, binarized PCFG probabilities maybe defined in terms of three category symbols A, B, and C: P(A 񮽙 BC | A); whereas some of the HHMM probabilities are defined in terms of five category Labeled recall (LR), labeled precision (LP), weighted average (F-score), and parse failure (% of sentences yielding no tree output) results for basic CKY parser and HHMM parser on unmodified and binarized WSJ Sections 22 (sentences 1-393: "devset") and 23-24 (all sentences).
name disambiguation task	LIBSVM software package	As in the name disambiguation task, we use the LIBSVM software package (Chang and Lin 2001) and tune the classifier in the training phase for the best SVM parameters prior to the testing.
statistical classification modeling	Collins- and	The first is implemented by means of statistical classification modeling, as reported in Collins- and.
MERT optimization	mt08 set	 Table 3  Contrastive Chinese-to-English translation results (lower-cased IBM BLEU) after first-pass  decoding and subsequent rescoring steps. The MERT k-best column indicates which decoder  generated the k-best lists used in MERT optimization. The mt08 set contains 691 sentences of  newswire and 666 sentences of Web text.
MT	NIST MT06	Experiments show that the algorithm achieves significant improvement in MT performance over a state-of-the-art hierarchical string-to-string system on NIST MT06 and MT08 newswire evaluation sets.
MT	MT08 newswire evaluation sets	Experiments show that the algorithm achieves significant improvement in MT performance over a state-of-the-art hierarchical string-to-string system on NIST MT06 and MT08 newswire evaluation sets.
MT	NIST	For both Arabic-to-English and Chinese-to-English MT, we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets.
MT	MT02-05	For both Arabic-to-English and Chinese-to-English MT, we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets.
MT	MT06	For both Arabic-to-English and Chinese-to-English MT, we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets.
MT	MT08 newswire sets	For both Arabic-to-English and Chinese-to-English MT, we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets.
MT	MT06	For Arabic-to-English MT, the str-dep model decoder improved BLEU by 1.3 on MT06 and 1.2 on MT08 before 5-gram rescoring.
MT	MT08	For Arabic-to-English MT, the str-dep model decoder improved BLEU by 1.3 on MT06 and 1.2 on MT08 before 5-gram rescoring.
Relation classification	NS and  OC data sets	 Table 9  Relation classification performance; all measures macro-averaged, except accuracy in the NS and  OC data sets, where we also report the accuracy 95% confidence intervals (CI).
dialogue management	Queen's Communicator	This is followed by a discussion of two systems that employ software agents for dialogue management: the Queen's Communicator and the AthosMail system.
opinion lexicon expansion	KN06	For the comparison of our approach in opinion lexicon expansion, we implemented the approach in referred to as KN06 hereafter).
parsing	CoNLL-X	 Table 1  Labeled parsing accuracy for top-scoring systems at CoNLL-X (Buchholz and Marsi 2006).
PE	Big Five space	Here, we test whether PE models can convey personality in extreme regions of the Big Five space.
Parsing Noun Phrases	Penn Treebank	Parsing Noun Phrases in the Penn Treebank
negation	BioScope DTD	Note that our modified scorer for negation is available from our Web page of supplemental materials, 2 together with the system output (in XML following the BioScope DTD) for all end-to-end runs with our final model configurations.
detecting speculation CUES	BioScope training data (BSA and BSP)	 Table 3  Development results for detecting speculation CUES: Averaged 10-fold cross-validation results for  the cue classifiers on both the abstracts and full papers in the BioScope training data (BSA and BSP).
scope resolution	BSE	 Table 10  Final end-to-end results for scope resolution: Held-out testing on BSE, using the cue classifier  described in Section 5.2 while combining the dependency rules and the constituent ranker for  scope resolution. The results are compared to the system with the best end-to-end performance  in the
MN tagger	LDC data set	We describe how our MN lexicon was semi-automatically produced and we demonstrate that a structure-based MN tagger results in precision around 86% (depending on genre) for tagging of a standard LDC data set.
MT research	Yngve [2000].)	(For a retrospective survey of his MT research activities see Yngve [2000].)
translating from German	Locke and Booth (Yngve 1955b)	The results of translating from German were published in the collection edited by Locke and Booth (Yngve 1955b).
Tagging	WSJ Section 24	 Table 3  Tagging accuracy on the respective development sets (WSJ Section 24 for English and Penn  Chinese Treebank articles 301-325 for Chinese) for binary classes B, E, and U, for various  Markov orders.
Tagging	Penn  Chinese Treebank articles 301-325	 Table 3  Tagging accuracy on the respective development sets (WSJ Section 24 for English and Penn  Chinese Treebank articles 301-325 for Chinese) for binary classes B, E, and U, for various  Markov orders.
SRL	SEC	Using the evaluation data from this track, showed that SRL can improve the accuracy of a QA system; a traditional SRL system alone, however, is not enough to recover the implied answer to Question (3): SEC or the agency.
constituency parsing	TIGER Treebank	Our reranking parser is the new state of the art in constituency parsing of the TIGER Treebank.
constituency parsing	BerkeleyParser	We present a system which is based on a state-of-the-art model for constituency parsing, namely, the probabilistic context-free grammar (PCFG) with latent annotations (PCFG-LA) model of, as implemented in the BerkeleyParser.
dependency parsing	Modern Standard Arabic (MSA)	In this article we investigate morphological features for dependency parsing of Modern Standard Arabic (MSA).
feature selection	LDC2005T06 corpus	The development set for feature selection and weight training is composed of 200 sentence pairs selected from the LDC2005T06 corpus, which includes 482 manually tagged NE pairs.
pyramid evaluation	TAC	Four human summaries are normally used for pyramid evaluation at TAC.
TAC	NIST	Another important point is that for TAC, the gold standards are created by trained assessors at NIST.
metaphor identification	BNC	The evaluation data for metaphor identification was the BNC parsed by the RASP parser).
metaphor identification	RASP parser	The evaluation data for metaphor identification was the BNC parsed by the RASP parser).
parsing	NONE-adjunction	We found that parsing accuracy is higher when using a probability model that does not factor in the NONE-adjunction events (parsing accuracy decreases by about 1.5 percentage points in a model that takes these events into account).
parsing	NONE-adjunction	In what follows we will present results for parsing with the probability model that does not include NONE-adjunction.
CRF	Danish Automotive	 Table 22  Experimental results for CRF on Danish Automotive, with different training data and sizes.  SMT is the same data that was used to train the SMT system, and new is additional data.
Frame identification	SemEval 2007 data set	 Table 5  Frame identification results on both the SemEval 2007 data set and the FrameNet 1.5 release.  Precision, recall, and F 1 were evaluated under exact and partial frame matching; see Section 3.3.  Bold indicates best results on the SemEval 2007 data, which are also statistically significant with  respect to the baseline (p < 0.05).
Frame identification	FrameNet 1.5 release	 Table 5  Frame identification results on both the SemEval 2007 data set and the FrameNet 1.5 release.  Precision, recall, and F 1 were evaluated under exact and partial frame matching; see Section 3.3.  Bold indicates best results on the SemEval 2007 data, which are also statistically significant with  respect to the baseline (p < 0.05).
Frame identification	SemEval 2007 data	 Table 5  Frame identification results on both the SemEval 2007 data set and the FrameNet 1.5 release.  Precision, recall, and F 1 were evaluated under exact and partial frame matching; see Section 3.3.  Bold indicates best results on the SemEval 2007 data, which are also statistically significant with  respect to the baseline (p < 0.05).
WSD	PageRank	The algorithm for WSD was first presented in . In this article, we present further evaluation on two more recent data sets, analyze the parameters and options of the system, compare it to the state of the art, and discuss the relation of our algorithm with PageRank and the MFS heuristic.
WSD	MFS heuristic	The algorithm for WSD was first presented in . In this article, we present further evaluation on two more recent data sets, analyze the parameters and options of the system, compare it to the state of the art, and discuss the relation of our algorithm with PageRank and the MFS heuristic.
translation	Hiero baseline	We use our in-house SCFG decoder for translation with both the Hiero baseline and our sampled grammars.
WD coreference resolution	ACE data set	 Table 2  Results for WD coreference resolution on the ACE data set.
WD coreference resolution	OntoNotes data set	 Table 3  Results for WD coreference resolution on the OntoNotes data set.
WD coreference resolution	ECB data set	 Table 4  Results for WD coreference resolution on the ECB data set.
coreference resolution	ECB data set	 Table 5  Results for CD coreference resolution on the ECB data set.
WSI evaluation	SemEval 2010 systems	To gauge the effect of the bias problem on WSI evaluation, we computed how the ranking of the SemEval 2010 systems () were affected by different estimators.
FSA	HiFST	We assess our analysis for FSA and PDA representations by contrasting HiFST and HiPDT with large grammars for translation and alignment.
FSA	HiPDT	We assess our analysis for FSA and PDA representations by contrasting HiFST and HiPDT with large grammars for translation and alignment.
Coreference resolution	MUC-6	Coreference resolution systems have been evaluated for several decades, beginning with MUC-6 (.
WSD	IMS	As discussed in the experimental set-up, WSD experiments were carried outwith IMS and UKB while injecting an increasingly higher amount of supervision and knowledge, respectively, that is, from 0 to 800 training sentences (cf. Section 6.3).
WSD	UKB	As discussed in the experimental set-up, WSD experiments were carried outwith IMS and UKB while injecting an increasingly higher amount of supervision and knowledge, respectively, that is, from 0 to 800 training sentences (cf. Section 6.3).
dependency parsing	Penn Chinese Treebank	For dependency parsing, we perform annotation adaptation from the Penn Chinese Treebank to a semantics-oriented Dependency Treebank, which is annotated using significantly different annotation guidelines.
dependency parsing	CTB-derived syntactic dependency treebank (DCTB)	Annotation adaptation for dependency parsing is performed from the CTB-derived syntactic dependency treebank (DCTB) ( to the Semantic Dependency Treebank (SDT) ().
Phrase  translation	WMT-2013	 Table 9  Data sizes (in number of sentences) and memory usage (in giga-bytes). Columns: Phrase  translation and lexicalized reordering tables give overall model sizes/sizes when filtered on  WMT-2013.
word ordering problem	Wall Street Journal data	r We present a novel method for solving the word ordering problem that gives the best reported accuracies to date on the standard Wall Street Journal data.
machine translation	Harbin Institute of Technology (HIT)	In the same year, machine translation research at the Harbin Institute of Technology (HIT) was started by Prof. Zhen Wang (and later Prof. Kaizhu Wang), focusing on the Russian-Chinese MT group.
Parsing Ill-Formed Input	FINITE STRING Newsletter	On the Need for Parsing Ill-Formed Input The FINITE STRING Newsletter
Parsing	Eastman and McLean 1981	Parsing of ill-formed input maybe an important issue in the design of robust natural language systems, as pointed out by Eastman and McLean 1981.
understanding of "scruffy" texts	NOMAD	This method of using expectations to aid the understanding of "scruffy" texts has been incorporated into a working computer program called NOMAD, which understands scruffy texts in the domain of Navy ship-to-shore messages.
MT	ALPAC	Paradoxically, MT systems were still being used by various government agencies here and abroad, because there was simply no alternative means of gathering information from foreign sources so quickly; in addition, private companies were developing and selling MT systems based on the mid-60s technology so roundly castigated by ALPAC.
MT	ALPAC	Paradoxically, MT systems were still being used by various government agencies here and abroad, because there was simply no alternative means of gathering information from foreign sources so quickly; in addition, private companies were developing and selling MT systems based on the mid-60s technology so roundly castigated by ALPAC.
Size of dictionaries	PAHO Machine  Translation System, 1976-1984	 Table 1. Size of dictionaries, PAHO Machine  Translation System, 1976-1984.
Translation	SPANAM, 1979-1984	 Table 2. Translation speeds, SPANAM, 1979-1984.
encoding of specialized linguistic knowledge	Copyright1985	This structure has proven particularly effective in the encoding of specialized linguistic knowledge, i.e., knowledge about Copyright1985 by the Association for Computational Linguistics.
ABSTRACTS	FINITE STRING Abstracts of Current Literature ASK	ABSTRACTS OF CURRENT LITERATURE Transportable Natural Language Process- ing through Simplicity -The PRE System Transporting the Linguistic String Project System from a Medical to a Navy Domain Portability of Syntax and Semantics in Datalog Problems and Some Solutions in Customi- zation of Natural Language Database Front Ends The FINITE STRING Abstracts of Current Literature ASK Is Transportable in Half a Dozen Ways
RP	Linggard H&T	There are also some discrepancies between vowel symbols in his table and the table for RP presented in Hughes and Trudgill (H&T; 1979: 26): Linggard H&T bat a ~e bet e e load ov ou 72
Object Raising verbs	LDOCE	66% of the Object Raising verbs were misclassified as Object Equi verbs, because the cooccurrence of the T5 and V (2, 3, or 4) codes in the same code fields, as predicted by the Object Raising rule above, was not confirmed by LDOCE.
unification	FUG	In particular, unification has become the basic idea (Shieber 1986), even if with a large number of variations on the formal character (for instance, one may compare Prolog-based unification for DCG, specialized unification for particular data types in FUG, equation resolution for LFG), and on overall strategies.
representation and reasoning)	SRI International) San Mateo: Morgan Kaufmann Publishers, 1988	(The Morgan Kaufmann series in representation and reasoning) ISBN 0-934613-73-7; $39.95 (bb) Practical Planning: Extending the Classical AI Planning Paradigm by David E. Wilkins (SRI International) San Mateo: Morgan Kaufmann Publishers, 1988, xiii + 205 PP.
parsing	FCR set	In particular, the parsing system correctly implements feature co-occurrence restrictions, subject only to the restriction that the FCR set can be expressed in clausal form as a set of Horn clauses.
ABSTRACTS	Longman Dictionary of Contemporary English	ABSTRACTS OF CURRENT LITERATURE On Psychological Plausibility in Artificial Intelligence Neural-Net Implementation of Complex Symbol-Processing in a Mental Model Approach to Syllogistic Reasoning Constructing A Machine Tractable Dictionary From Longman Dictionary of Contemporary English
Neural-Net Implementation of Complex Symbol-Processing	Longman Dictionary of Contemporary English	ABSTRACTS OF CURRENT LITERATURE On Psychological Plausibility in Artificial Intelligence Neural-Net Implementation of Complex Symbol-Processing in a Mental Model Approach to Syllogistic Reasoning Constructing A Machine Tractable Dictionary From Longman Dictionary of Contemporary English
MT	ALPAC Report	Chapter 1, a short history of the field of MT in the world, cow;r.s developments in Japan, as might be expected, but also in the Soviet Union, and it ends with an optimistic asses,lment of the effects of the ALPAC Report, one of
Linguistic Exploitation of Syntactic Databases	Nijmegen Linguistic DataBase	Linguistic Exploitation of Syntactic Databases: The Use of the Nijmegen Linguistic DataBase Program
Situation Theory	CSLI Lecture Notes 26), 1991	Paperbound, ISBN 0-937073-74-1, $34.95 (distributed by the University of Chicago Press) Situation Theory and its Applications, Volume 2 Stanford: Center for the Study of Language and Information (CSLI Lecture Notes 26), 1991, xiii + 637 pp.; distributed by the University of Chicago Press.
parsing	CFG	This is reflected in common features of the parsing algorithms for HG and TAG and the CKY algorithm for CFG).
RE	Memorial University of Newfoundland	The "proto-projection" techniques used by RE were implemented earlier by John Hewson and others at the Memorial University of Newfoundland (.
referent resolution of deictic and deixis-related expressions	EDWARD	In this paper we will go into the semantic and pragmatic processes involved in the referent resolution of deictic and deixis-related expressions by EDWARD.
spelling correction	SPARCStation 10/41	These results indicate that such recognition works very efficiently for candidate generation in spelling correction for many European languages (English, Dutch, French, German, and Italian, among others) with very large word lists of root and inflected forms (some containing well over 200,000 forms), generating all candidate solutions within 10 to 45 milliseconds (with an edit distance of 1) on a SPARCStation 10/41.
explanation generation	Biology Knowledge Base	This paper is structured as follows: The task of explanation generation is characterized and the Biology Knowledge Base is described.
Word Alignment Squibs and Discussions	Bangla Book Review	Computational Linguistics Articles Floating Constraints in Lexical Choice Adaptive Multilingual Sentence Boundary Disambiguation Finite-State Transducers in Language and Speech Processing A Class-based Approach to Word Alignment Squibs and Discussions A Delayed Syntactic-Encoding-based LFG Parsing Strategy for an Indian Language---Bangla Book Review Dimensions of Register Variation: A Cross-Linguistic Comparison
MT	Hansards	For MT and other purposes, many methods have been proposed for sentence alignment of the Hansards, an English-French corpus of Canadian parliamentary debates, and for other language pairs, including English-German, EnglishChinese, and English-Japanese ().
clustering definition sentences	Longman Dictionary of Contemporary English (LDOCE)	An implementation of the method for clustering definition sentences in the Longman Dictionary of Contemporary English (LDOCE) is described.
classification task	Penn Treebank	A second reason was that we intended to use computer simulations of the classification task to supplement the results of our experiments, and we needed a parsed corpus for this purpose; the articles we chose were all part of the Penn Treebank.
NLP evaluation	materi-336	Section 2 of Chapter 2 presents a survey of NLP evaluation activities and materi-336
Distribution	European Language Resources Association	The project has sought to 1 Distribution and licensing will be carried out by the European Language Resources Association, http://www.icp.inpg.fr/ELRA.
Alignment	French-to- English Hansards Alignment Task	 Table 1: Alignment Performance on the French-to- English Hansards Alignment Task.
Topic detection	English 20 Newsgroup  data	 Table 3: Topic detection results on English 20 Newsgroup  data
MAP adaptation	Penn Wall St. Journal treebank	We will present empirical results for multiple MAP adaptation schema, both starting from the Penn Wall St. Journal treebank and adapting to the Brown corpus, and vice versa.
MAP adaptation	Brown corpus	We will present empirical results for multiple MAP adaptation schema, both starting from the Penn Wall St. Journal treebank and adapting to the Brown corpus, and vice versa.
question answering (QA)	TREC in 1999 (TREC-8)	A question answering (QA) track was started in TREC in 1999 (TREC-8) to address the problem of returning answers, rather than document lists, in response to a question.
Answering Questions Adaptation	EBMT	Automating XML markup of text documents Getting More Mileage from Web Text Sources for Conversational Speech Language Modeling using Class- Dependent Mixtures Exploiting Diversity for Answering Questions Adaptation Using Out-of-Domain Corpus within EBMT
Topic Detection and Tracking (TDT)	DARPA TIDES program	Topic Detection and Tracking (TDT) research is sponsored by the DARPA TIDES program.
ASR differences	TDT  2001 evaluation data	 Table 3: Effect of using an "ASR stoplist" and "enhanced  preprocessing" for handling ASR differences on the TDT  2001 evaluation data.  ASRstop  No  Yes  No  Preproc  Std  Std  Enh  LNK  0.312 0.299 (+4.4%) 0.301 (+3.3%)  NED  0.606 0.641 (-5.5%) 0.587 (+3.1%)
ASR differences	ASRstop  No  Yes  No  Preproc  Std  Std  Enh  LNK  0.312 0.299	 Table 3: Effect of using an "ASR stoplist" and "enhanced  preprocessing" for handling ASR differences on the TDT  2001 evaluation data.  ASRstop  No  Yes  No  Preproc  Std  Std  Enh  LNK  0.312 0.299 (+4.4%) 0.301 (+3.3%)  NED  0.606 0.641 (-5.5%) 0.587 (+3.1%)
ASR differences	NED  0.606 0.641	 Table 3: Effect of using an "ASR stoplist" and "enhanced  preprocessing" for handling ASR differences on the TDT  2001 evaluation data.  ASRstop  No  Yes  No  Preproc  Std  Std  Enh  LNK  0.312 0.299 (+4.4%) 0.301 (+3.3%)  NED  0.606 0.641 (-5.5%) 0.587 (+3.1%)
WSD	SENSEVAL1 corpus	When used to evaluate areal WSD system on the SENSEVAL1 corpus, pseudowords were found to be optimistic in their estimations compared to real ambiguous words with the same distribution.
Tagging	Murphy	Tagging is carried out by the standard Forward-Backward algorithm (see e.g. Murphy).
ASR	TIMIT database	In order to evaluate the use of the proposed features for ASR in noisy car environments, we repeated the same experiments performed in our previous study) using the subsets dr1 & dr2 of a noisy version of the TIMIT database at different values of SNR which varies from 16 dB to -4 dB.
link detection task	Edmonton, May-June 2003 Student Research Workshop , pp	In section 4, we present details of the link detection task and Edmonton, May-June 2003 Student Research Workshop , pp.
Generation and Summarization	Cornell	This year, the award goes to "Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization" by Regina Barzilay, MIT, and Lillian Lee, Cornell.
Entity Detection and Tracking task (EDT henceforth)	MUC-6	The Entity Detection and Tracking task (EDT henceforth) has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of attention of much investigation in the recent past (), and have been at the center of several evaluations: MUC-6, MUC-7, CoNLL'02 and CoNLL'03 shared tasks.
Entity Detection and Tracking task (EDT henceforth)	MUC-7	The Entity Detection and Tracking task (EDT henceforth) has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of attention of much investigation in the recent past (), and have been at the center of several evaluations: MUC-6, MUC-7, CoNLL'02 and CoNLL'03 shared tasks.
coreference resolution tasks	MUC-6	The Entity Detection and Tracking task (EDT henceforth) has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of attention of much investigation in the recent past (), and have been at the center of several evaluations: MUC-6, MUC-7, CoNLL'02 and CoNLL'03 shared tasks.
coreference resolution tasks	MUC-7	The Entity Detection and Tracking task (EDT henceforth) has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of attention of much investigation in the recent past (), and have been at the center of several evaluations: MUC-6, MUC-7, CoNLL'02 and CoNLL'03 shared tasks.
parse selection	HPSG	Even if it were possible to bootstrap from the Penn Treebank, it is still unlikely that there would be sufficient quantities of high quality material necessary to improve parse selection for detailed linguistic formalisms such as HPSG.
parsing	PARC 700 predicates	After parsing, the underscores were converted to spaces to match the PARC 700 predicates.
POS tagging	Collins	Timing results are reported in seconds of CPU time . POS tagging of the input to the Collins parser took 6 seconds and this was added to the timing result of the Collins parser.
POS tagging	Collins parser	Timing results are reported in seconds of CPU time . POS tagging of the input to the Collins parser took 6 seconds and this was added to the timing result of the Collins parser.
dependency extraction	Collins output	We did not include the time for dependency extraction or stemming the Collins output.
machine translation	DARPA contract F49620-00-1-0337	Recently, specific probabilistic tree-based models have been proposed not only for machine translation, but also for This work was supported by DARPA contract F49620-00-1-0337 and ARDA contract MDA904-02-C-0450.
machine translation	ARDA contract MDA904-02-C-0450	Recently, specific probabilistic tree-based models have been proposed not only for machine translation, but also for This work was supported by DARPA contract F49620-00-1-0337 and ARDA contract MDA904-02-C-0450.
noun countability detection	Bald- win and Bond 2003	 Table 12: Performance of Altavista counts and BNC  counts for noun countability detection (data from Bald- win and Bond 2003)
query expansion	UWAThard2 run	It is not clear why query expansion method 1 performed worse in the official UWAThard2 run compared to the training run, given very similar numbers of relevant sentences selected.
syntactic parsing	CTB Release 2	 Table 7 Results for syntactic parsing, trained on  CTB Release 2, tested on test set in semantic parsing  LP(%)  LR(%)  F1(%)  overall  81.6  82.1  81.0  len<=40  86.1  85.5  86.7
translation	Canadian Hansards task	The translation results for the Xerox and Canadian Hansards task are very promising.
Translation	German- English Verbmobil task	 Table 6: Translation performance [%] for the German- English Verbmobil task (251 sentences).
Translation	French- English Canadian Hansards task	 Table 8: Translation performance [%] for the French- English Canadian Hansards task (5432 sentences).
Translation	AMD Athlon  2.2GHz	 Table 9: Translation Speed for all tasks on a AMD Athlon  2.2GHz.
coreference resolver	MUC-4 or Reuters collections	For example, even if the contexts surrounding an anaphor and candidate match exactly, they are not coreferent if they have substantially different meanings We would be happy to make our manually annotated test data available to others who also want to evaluate their coreference resolver on the MUC-4 or Reuters collections.
coreference resolution	WordNet	Such rare senses make it difficult fora coreference resolution system to use WordNet to enforce the constraint that personal pronouns (e.g. he or she) must refer to a person.
Scoring	MUC scorer	Scoring was performed using the MUC scorer.
article generation	DROP0	 Table 2: Contingency table for article generation using  TRAINGEN on DROP0
summarization	Newsblaster	We participated in the DUC 2004 conference submitting the results of the summarization system used in Newsblaster, as well as an in-progress system described in Section 4.1 for multilingual cluster summarization.
Topic Detection and Tracking	LDC, 1999)	The definition of "story" in our experiments corresponds with the Topic Detection and Tracking definition: a segment of a news broadcast with a coherent news focus, containing at least two independent, declarative clauses (LDC, 1999).
parsing arguments of eventive nominalizations	FrameNet database	In this paper, we use a machine learning framework for semantic argument parsing, and apply it to the task of parsing arguments of eventive nominalizations in the FrameNet database.
MT	NIST MT05 Chinese-English evaluation	The training corpus for the MT system's phrase tables consists of all parallel text available for the NIST MT05 Chinese-English evaluation, except the Xinhua corpora and part 3 of LDC's "MultipleTranslation Chinese Corpus" (MTCCp3).
RTE problem	PASCAL RTE dataset	The RTE problem as presented in the PASCAL RTE dataset is particularly attractive in that it is a reasonably simple task for human annotators with high inter-annotator agreement (95.1% in one independent labeling), but an extremely challenging task for automated systems.
alignment combination	SAHMM	The alignment combination techniques are evaluated in this paper using data from three language pairs, as shown in.  and SAHMM ().
MT	MTEval'02 data	The parameters of the MT system were optimized on MTEval'02 data using minimum error rate training.
parsing event	Wall Street Journal	The Wall Street Journal training data is combined with the NANC data in the following way: The count of each parsing event is the (optionally weighted) sum of the counts of that event in Wall Street Journal and NANC.
parsing event	NANC	The Wall Street Journal training data is combined with the NANC data in the following way: The count of each parsing event is the (optionally weighted) sum of the counts of that event in Wall Street Journal and NANC.
parsing	PCFG	We present experiments showing that with our algorithm the workload (as measured by the total number of constituents processed) is decreased by a factor often with no decrease in parsing accuracy compared to standard CKY parsing with the original PCFG.
resolution	WordNet and Wikipedia taxonomies	The resolution requires an encyclopedia (i.e. Wikipedia) look-up and reasoning on the content relatedness holding between the different expressions (i.e. as a path measure along the links of the WordNet and Wikipedia taxonomies).
coreference resolution	Interfax news agency	Event representations seem also to be important for coreference resolution, as shown below: (2) A state commission of inquiry into the sinking of the Kursk will convene in Moscow on Wednesday, the Interfax news agency reported.
IR	PageRank	 Table 7. System Performance Comparison.  Another widely-used graph algorithm in IR is  PageRank
relation extraction	ACE corpus	But to our surprise, the sole two-reported dependency tree kernels for relation extraction on the ACE corpus () showed much lower performance than the feature-based methods.
relation characterization	ACE corpus	 Table 2. It shows  that 85.9% (587/684) of the errors result from rela- tion detection and only 14.1% (97/684) of the er- rors result from relation characterization. This is  mainly due to the imbalance of the posi- tive/negative instances and the sparseness of some  relation types on the ACE corpus.
parsing	Penn WSJ treebank	Finally, Section 4 reports the results of parsing experiments using our exhaustive k-best CYK parser with the concise PCFGs induced from the Penn WSJ treebank ().
question answering community	NIST's TREC QA mailing list	Assessors consisted of graduate students in library and information science and computer science at the University of Maryland as well as volunteers from the question answering community (obtained via a posting to NIST's TREC QA mailing list).
SRL	Chinese Nombank	In this paper we report results of SRL experiments on nominalized predicates in Chi-nese, using a newly completed corpus, the Chinese Nombank.
Detecting and classifying the arguments of predicates	Propbank	Detecting and classifying the arguments of predicates has been an active area of research in recent years, driven by the availability of large-scale semantically annotated corpora such as the FrameNet () and the Propbank ( . It is generally formulated as a semantic role labeling (SRL) task, where each argument of the predicate is assigned a label that represents the semantic role it plays with regard to its predicate ().
semantic parsing	ATIS (Air Travel Information Service)	Prior research in semantic parsing has mainly focused on relatively simple domains such as ATIS (Air Travel Information Service) (), in which a typcial MR is only a single semantic frame.
WASP	GEO-QUERY	We evaluated WASP in the ROBOCUP and GEO-QUERY domains (see Section 2).
WASP	COCKTAIL	However, WASP performs quite favorably compared to SILT and COCKTAIL, which use the same training data.
WASP	GEOQUERY data set	shows the performance of WASP on the multilingual GEOQUERY data set.
MT engines	Google MT	As for MT engines, we employed Babelfish and Google MT, 8 rule-based systems developed by SY ST R A N and Google, respectively.
coreference resolution	Switchboard Corpus	There are few other reported results for coreference resolution on spontaneous, unconstrained speech;) similarly finds low overall scores for pronoun resolution on the Switchboard Corpus, albeit by a different scoring metric.
Decoding weight optimization	NIST MT evaluation test set	Decoding weight optimization was done on 200 sentences from the 2003 NIST MT evaluation test set.
Sample optimized CRF	MUC-6	 Table 2: Sample optimized CRF results, for the MUC-6
parsing	Penn Treebank function labels	() report a reduction in parsing accuracy of an unlexicalised PCFG from 77.8% to 72.9% in using Penn Treebank function labels in training.
SSN parser	PropBank	Our extended semantic role SSN parser was trained on sections 2-21 and validated on section 24 from the PropBank.
parsing	PropBank labels	To evaluate the former parsing task, we compute the standard Parseval measures of labelled recall and precision of constituents, taking into account not only the 33 original labels, but also the newly introduced PropBank labels.
Broadcast News story segmentation	SRI NIGHTINGALE system	In this paper, we present results from a Broadcast News story segmentation system developed for the SRI NIGHTINGALE system operating on English, Ara-bic and Mandarin news shows to provide input to subsequent question-answering processes.
NE recognition	Remedia test set	For example, have augmented the BOW approach with stemming, NE recognition, NE filtering, semantic class identification and pronoun resolution to achieve 36% HumSent 1 accuracy in the Remedia test set.
NE filtering	Remedia test set	For example, have augmented the BOW approach with stemming, NE recognition, NE filtering, semantic class identification and pronoun resolution to achieve 36% HumSent 1 accuracy in the Remedia test set.
document classification	GLSA document vectors	We also conducted document classification experiments to demonstrate the advantage of the GLSA document vectors ().
MT evaluation	ACL05	The data for the experiments are from the MT evaluation workshop at ACL05.
MT	E09 E11 E12 E14 E15 E17 E22	There are seven sets of MT outputs (E09 E11 E12 E14 E15 E17 E22), each of which contains 919 English sentences translated from the same set of Chinese sentences.
case  prediction	5K-test set	 Table 4: Accuracy (%) and BLEU score for case  prediction when given correct context (reference  translations) on the 5K-test set
segmentation	PASCAL datasets	In this section, we will first evaluate our segmentation algorithm for English and Bengali, and then examine its performance on the PASCAL datasets.
MT	NIST	Both MT metrics measure the precision of a translation in terms of the proportion of n-grams that it shares with the reference translations, with the NIST score focusing more on n-grams that are less frequent and more informative.
question answering	TREC QA tracks	Most question answering systems are designed to pinpoint the exact named entity (person, date, organization, etc.) that answers a particular question-and the development of such technology has been encouraged by the setup of the TREC QA tracks.
parsing	Wall Street Journal corpus	For instance, if the task is "document clustering," a task instance could be "clustering of a set of scientific documents into particular fields"; or, if the task is "parsing," a task instance could be "parsing of English sentences from the Wall Street Journal corpus".
cross-instance tuning	20 Newsgroups corpus	We demonstrate the feasibility of cross-instance tuning with experiments on unsupervised document categorization from the 20 Newsgroups corpus; this corpus consists of roughly 20,000 news articles, evenly divided among 20 Usenet groups.
information extraction of genes and gene-protein interactions	TREC Genomics track	In the genetics domain, for instance, information extraction of genes and gene-protein interactions helps geneticists scan large amounts of information (e.g., as explored in the TREC Genomics track).
Single document information extraction of named entities and relationships	MUC evaluations 1	Single document information extraction of named entities and relationships has received much attention since the MUC evaluations 1 in the mid-90s.
IWSLT 2006 Chinese to English translation task	Full BTEC corpus	We present results on the IWSLT 2006 Chinese to English translation task, based on the Full BTEC corpus of travel expressions with 120K parallel sentences (906K source words and 1.2m target words).
SMT	Stanford Parser (D. pre-trained on the Penn Treebank	Grammar rules were induced with the syntaxbased SMT system "SAMT" described in), which requires initial phrase alignments that we generated with "GIZA++" (, and syntactic parse trees of the target training sentences, generated by the Stanford Parser (D. pre-trained on the Penn Treebank.
answer typing	TREC	Although our answer typing system is not capable of fully answering questions, we will make use of the how-adjective questions from TREC) as a set of test data.
answer type checking of location questions	WordNet	To use more than one resource for answer type checking of location questions, combined WordNet with geographical databases.
answer assessment	NTCIR	For the answer assessment, we followed the TREC-QA track) and NTCIR to annotate answers in the pool that collected from the outputs of different passage retrieval methods.
SRL system	Wall Street Journal corpora	A final difficulty with PropBank's current approach is that it limits SRL system robustness in the face of verb senses, verbs or verb constructions that were not included in the training data, and the training data is all Wall Street Journal corpora.
SRL	Brown corpus, during CoNLL 2005	This issue is reflected in the relatively poor performance of most state-of-the-art SRL systems when tested on a novel genre, the Brown corpus, during CoNLL 2005.
SRL	Brown corpus	describes the results of SRL overall performance tested on the WSJ corpus Section 23; Table 4 demonstrates the SRL overall system performance tested on the Brown corpus.
SRL	PropBank tag set	 Table 1: Overall SRL System performance using the  PropBank tag set ("Original") and the augmented  tag set ("Mapped")
SRL	Brown	 Table 2: Performance of the SRL system on Brown.
MT training/decoding	Europarl	We have implemented anew general-purpose MT training/decoding system in this framework , and have tested this on a variety of existing MT models (including the 4 IBM models), and some novel ones as well, all using Europarl as a test corpus.
MT ex- periments	Europarl corpus	We perform MT ex- periments on a English-French subset of the Europarl corpus used for the ACL 2005 SMT evaluations ().
MT ex- periments	ACL 2005	We perform MT ex- periments on a English-French subset of the Europarl corpus used for the ACL 2005 SMT evaluations ().
MT	ACL training set	We train an English language model on the whole training set using the SRILM toolkit) and train MT models mainly on a 10k sentence pair subset of the ACL training set.
MT	Rewrite	As an admissible heuristic for decoding, we compute, for each node V with Conditional Probability, the largest value of cover all possible configurations of V and its parents compares MT performance between (1) Pharaoh (which uses beam search), (2) our system, and (3) Rewrite (hill-climbing).
MT	GS	We observed that it takes 15% more time overall to read MT than GS.
MT	DARPA GALE 2006 evaluation data sets	To guarantee that the MT systems were up to the task of processing the documents, we used the DARPA GALE 2006 evaluation data sets, against which several research sites were testing MT algorithms.
VAD	ILAmin	 Table 2: VAD errors, measured at three points in our  system, and first-pass WERs for rt05s eval (05),  as well as first-pass WERs for rt05s eval* (05*)  and rt06s eval (06). Results are shown for 3 con- trastive VAD systems (ILAave(3), ILAave(2) and  ILAmin
tagging	IceTagger	Evaluation shows that the average tagging accuracy is 91.54% and 90.44%, obtained by IceTagger and TnT, respectively.
tagging	IceTagger	The average tagging accuracy of IceTagger is 91.54%, compared to 90.44% achieved by the TnT tagger, a state-of-the-art statistical tagger).
Speech Summarization	Mandarin Broadcast News	Speech Summarization Without Lexical Features for Mandarin Broadcast News
WSD	WordNet	Generally, WSD methods use the context of a word for its sense disambiguation, and the context information can come from either annotated/unannotated text or other knowledge resources, such as WordNet,, Open Mind Word Expert (), eXtended WordNet (), Wikipedia, parallel corpora (Ng,.
WSD	Wikipedia	Generally, WSD methods use the context of a word for its sense disambiguation, and the context information can come from either annotated/unannotated text or other knowledge resources, such as WordNet,, Open Mind Word Expert (), eXtended WordNet (), Wikipedia, parallel corpora (Ng,.
segmentation	S&B  dataset	 Table 1: Comparison of segmentation results on the S&B  dataset.
segmentation	S&B dataset	 Table 3: Comparison of segmentation results with super- vised and semi-supervised learning on the S&B dataset.
segmentation	Ara- bic Penn Treebank	 Table 4: Comparison of segmentation results on the Ara- bic Penn Treebank.
MT	NIST 2008 Chinese-English common-data track-by +1.1	We add more than 250 features to improve a syntaxbased MT system-already the highest-scoring single system in the NIST 2008 Chinese-English common-data track-by +1.1 B.
summarization	PYTHY	 Table 2: Example summarization output for systems compared in section 5.2. (a), (b), and (c) represent the first two  sentences output from PYTHY, HIERSUM, and reference summary respectively. In (d), we present the most frequent  non-stop unigrams appearing in the reference summary and their counts in the PYTHY and HIERSUM summaries.  Note that many content words in the reference summary absent from PYTHY's proposal are present in HIERSUM's.
summarization	PYTHY and HIERSUM summaries	 Table 2: Example summarization output for systems compared in section 5.2. (a), (b), and (c) represent the first two  sentences output from PYTHY, HIERSUM, and reference summary respectively. In (d), we present the most frequent  non-stop unigrams appearing in the reference summary and their counts in the PYTHY and HIERSUM summaries.  Note that many content words in the reference summary absent from PYTHY's proposal are present in HIERSUM's.
phone recognition (XPR)	HTK	To compare our phone recognition (XPR) system with the baseline (BASEPR), we train two phone recognizers using HTK.
SMT	EAN corpus.	If we compare this with an exact model of size 367.6 million n-grams, we see an increase of 0.8 points in Bleu (95% statistical significance level: Effect of entropy-based pruning on SMT performance using EAN corpus. Results are as in is ≈ 0.53 Bleu).
SMT	NIST	The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (), NIST) and METEOR () scores.
SMT	EAN corpus	 Table 1: Effect of count-based pruning on SMT per- formance using EAN corpus. Results are according to  BLEU, NIST and METEOR (MET) metrics. Bold #s are  not statistically significant worse than exact model.
SMT	NIST	 Table 1: Effect of count-based pruning on SMT per- formance using EAN corpus. Results are according to  BLEU, NIST and METEOR (MET) metrics. Bold #s are  not statistically significant worse than exact model.
SMT	EAN corpus	 Table 2: Effect of entropy-based pruning on SMT perfor- mance using EAN corpus. Results are as in
SMT	NIST	 Table 9: Evaluating SMT with different LMs on EAN.  Results are according to BLEU, NIST and MET metrics.  Bold #s are not statistically significant worse than exact.
Classification	ILP	 Table 3: Classification Performance (F-Score) by  Relation: ILP on Set A
summarizers	LexRank	Among the automatic summarizers, C-LexRank and LexRank perform best.
sequence modeling task	Penn Treebank	Firstly, we found that DAUME07, which had outperformed the ALL DATA baseline for the sequence modeling task, performed worse than the Specifically, all the other domains use the "new" Penn Treebank annotation style, whereas the WSJ data is still in the "traditional" annotation style, familiar from the past decade's work in Penn Treebank parsing.
sequence modeling task	WSJ data	Firstly, we found that DAUME07, which had outperformed the ALL DATA baseline for the sequence modeling task, performed worse than the Specifically, all the other domains use the "new" Penn Treebank annotation style, whereas the WSJ data is still in the "traditional" annotation style, familiar from the past decade's work in Penn Treebank parsing.
sequence modeling task	Penn Treebank	Firstly, we found that DAUME07, which had outperformed the ALL DATA baseline for the sequence modeling task, performed worse than the Specifically, all the other domains use the "new" Penn Treebank annotation style, whereas the WSJ data is still in the "traditional" annotation style, familiar from the past decade's work in Penn Treebank parsing.
phrase structure	Penn Treebank Wall Street Journal corpus	This corpus is annotated for phrase structure in much the same way as the Penn Treebank Wall Street Journal corpus, with the addition of several speech-specific categories as described in Section 2.1.
SMT	HKUST	Semantic Roles for SMT: A Hybrid Two-Pass Model Human Language Technology Center HKUST
SMT first pass translation	ARG0 National Development Bank	The SMT first pass translation has an ARG0 National Development Bank of Japan in the capital market which is badly mismatched to both the input sentence's ARG0 񮽙񮽙 开发 银񮽙 and ARGM-LOC 񮽙 񮽙񮽙 资񮽙 񮽙场.
automatic classification of comments as qualified or bald	MinorThird toolkit	For our supervised machine learning experiments on automatic classification of comments as qualified or bald, we used the Support Vector Machine classifier in the MinorThird toolkit) with the default linear kernel.
speaker clustering	DARPA contract HR0011-06-2-0001	The output of speaker clustering is the internal labels relative to a dataset rather than real * This work was funded in part by DARPA contract HR0011-06-2-0001.
document classification	Tech-TC-300 data set	For the second experiment, we performed the document classification task using the Tech-TC-300 data set ( . We used the tf-idf scores as feature values.
Syntactic Transformations	Gold Standard Corpora	Evaluating the Syntactic Transformations in Gold Standard Corpora for Statistical Sentence Compression
translation	RBMT output	On the other hand, statistical training and translation in both SMT and APE systems were carried out using the Moses toolkit ( . It should be noted that APE system was trained taking the RBMT output as source, instead of the original text.
SMT	RBMT output	On the other hand, statistical training and translation in both SMT and APE systems were carried out using the Moses toolkit ( . It should be noted that APE system was trained taking the RBMT output as source, instead of the original text.
Syntactic Tree-based Relation Extraction	Collins and Duffy Convolution Tree Kernel	Syntactic Tree-based Relation Extraction Using a Generalization of Collins and Duffy Convolution Tree Kernel
SVM classification	Stanford NLP package 1	We have used LIBSVM (Chang and Lin 2001) java source for the SVM classification and Stanford NLP package 1 for tokenization, sentence segmentation and parsing.
Demonstration Committee	Demo Session	I thank Carolyn Penstein Rosé, Chair of the Demonstration Committee, for soliciting candidates for the Demo Session and overseeing the rigorous process of review.
chart mining feature extraction	LOGON Treebank	The chart mining feature extraction is implemented as an extension to the PET parser (Callmeier,  Since the parser has no access to any of the verbs under investigation (due to the DUMMY-V substitution), those VPC types attested in the LOGON Treebank do not directly impact on the model's performance.
Fisher evaluation parsing	Fisher  training set	 Table 1: Fisher evaluation parsing results for the basic  PCFGs without latent annotations trained on the Fisher  training set.
preposition corrections	Cambridge University Press Learners' Corpus	The investigation of preposition corrections can even be narrowed further: amongst the more than 150 English prepositions, the usage of the ten most frequent prepositions accounts for 82% of preposition errors in the 20 million word Cambridge University Press Learners' Corpus.
speech recognition task	Broadcast news data	As an application, we demonstrate how this framework can be used for adjusting language model interpolation weight for speech recognition task to adapt from Broadcast news data to MIT lecture data.
speech recognition task	MIT lecture data	As an application, we demonstrate how this framework can be used for adjusting language model interpolation weight for speech recognition task to adapt from Broadcast news data to MIT lecture data.
LM adaptation	CSR Hub4 Language Model data	For LM adaptation experiments, the out-ofdomain LM (p B , Broadcast News LM) training text consists of 335M words from the following broadcast news (BN) data sources): 1996 CSR Hub4 Language Model data, EARS BN03 closed captions, GALE Phase 2 Distillation GNG Evaluation Supplemental Multilingual data, Hub4 acoustic model training transcripts, TDT4 closed captions, TDT4 newswire, and GALE Broadcast Conversations and GALE Broadcast News.
LM adaptation	GALE Phase 2 Distillation GNG Evaluation Supplemental Multilingual data	For LM adaptation experiments, the out-ofdomain LM (p B , Broadcast News LM) training text consists of 335M words from the following broadcast news (BN) data sources): 1996 CSR Hub4 Language Model data, EARS BN03 closed captions, GALE Phase 2 Distillation GNG Evaluation Supplemental Multilingual data, Hub4 acoustic model training transcripts, TDT4 closed captions, TDT4 newswire, and GALE Broadcast Conversations and GALE Broadcast News.
LM adaptation	Hub4 acoustic model training transcripts	For LM adaptation experiments, the out-ofdomain LM (p B , Broadcast News LM) training text consists of 335M words from the following broadcast news (BN) data sources): 1996 CSR Hub4 Language Model data, EARS BN03 closed captions, GALE Phase 2 Distillation GNG Evaluation Supplemental Multilingual data, Hub4 acoustic model training transcripts, TDT4 closed captions, TDT4 newswire, and GALE Broadcast Conversations and GALE Broadcast News.
LM adaptation	TDT4 newswire	For LM adaptation experiments, the out-ofdomain LM (p B , Broadcast News LM) training text consists of 335M words from the following broadcast news (BN) data sources): 1996 CSR Hub4 Language Model data, EARS BN03 closed captions, GALE Phase 2 Distillation GNG Evaluation Supplemental Multilingual data, Hub4 acoustic model training transcripts, TDT4 closed captions, TDT4 newswire, and GALE Broadcast Conversations and GALE Broadcast News.
LM adaptation	GALE Broadcast News	For LM adaptation experiments, the out-ofdomain LM (p B , Broadcast News LM) training text consists of 335M words from the following broadcast news (BN) data sources): 1996 CSR Hub4 Language Model data, EARS BN03 closed captions, GALE Phase 2 Distillation GNG Evaluation Supplemental Multilingual data, Hub4 acoustic model training transcripts, TDT4 closed captions, TDT4 newswire, and GALE Broadcast Conversations and GALE Broadcast News.
MWE integration	NIST  MT08	 Table 1: Impact of MWE integration measured on NIST  MT08
Document summarization	TAC	Document summarization, as captured in modern comparative evaluations such as TAC and DUC, is mostly conceived as a "one-shot" task.
Document summarization	DUC	Document summarization, as captured in modern comparative evaluations such as TAC and DUC, is mostly conceived as a "one-shot" task.
MT output	AMT	For each of the 2,153 MT output segments, we collected edits from 5 distinct workers on AMT, fora total of 10,765 post-edited segments by a total of about 500 distinct workers.
Annotation Noise	Benchmarked Dataset	Some Empirical Evidence for Annotation Noise in a Benchmarked Dataset
machine translation evaluation	NIST	The news stories were selected from the evaluation sets of the 2008 machine translation evaluation campaign organized by NIST.
Supertagging	MSTParser	 Table 2: Supertagging accuracy on section 23. ( †)  Dependencies are given by MSTParser evaluated with  labeled accuracy. PW-AP is the baseline point-wise  averaged perceptron model. PW-DEP is point-wise  dependency-informed model. The automatically tagged  POS tags were given by a maximum entropy tagger with  97.39% accuracy.
Reranking	Berkeley and Brown Parsers	Reranking the Berkeley and Brown Parsers *
parsing	Wall Street Journal (WSJ) section of the University of Pennsylvania treebank corpus	This paper focuses on parsing the Wall Street Journal (WSJ) section of the University of Pennsylvania treebank corpus ().
segmentation	ATB	Previous approaches () chose the segmentation approach but concentrated on POS tagging by using the segmentation provided by the ATB.
POS tagging	ATB	Previous approaches () chose the segmentation approach but concentrated on POS tagging by using the segmentation provided by the ATB.
named-entity recognition (NER)	CoNLL 2003 shared task	We consider the problem of named-entity recognition (NER) and use the English data from the CoNLL 2003 shared task.
Identifying Opinion Holders	Chinese News Texts	Identifying Opinion Holders and Targets with Dependency Parser in Chinese News Texts
crosslingual entity clustering of person and organization entities	NIST Automatic Content Extraction (ACE) 2008 Arabic-English evaluation set	Our work makes the following contributions: (1) introduction of the task, (2) novel models for crosslingual entity clustering of person and organization entities, (3) cross-lingual annotation of the NIST Automatic Content Extraction (ACE) 2008 Arabic-English evaluation set, and (4) experimental results using both gold and automatic within-document processing.
Sequence Labeling (SL-CRF)	WordNet	Sequence Labeling (SL-CRF) Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering (REF).
Segment Classification	WordNet	Segment Classification (SC-S2-R1) Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering (REF).: Two example outputs produced by the three methods tion 5.1.
paraphrase identification	Microsoft Research Paraphrase corpus	We show that a meta-classifier trained using nothing but recent MT metrics outperforms all previous paraphrase identification approaches on the Microsoft Research Paraphrase corpus.
MT	Microsoft Research paraphrase corpus (MSRP)	We employ 8 different MT metrics for identifying paraphrases across two different datasets -the well-known Microsoft Research paraphrase corpus (MSRP) () and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task).
MSRP	PAN	Our approach yields impressive results -the current state of the art for MSRP and extremely positive for PAN.
MT	PAN datasets	 Table 3: The top 3 performing MT metrics for both  MSRP and PAN datasets as identified by ablation stud- ies. BLEU(1-4), NIST(1-5) and TER were used as the 10  base features in the classifiers.
MT	NIST	 Table 3: The top 3 performing MT metrics for both  MSRP and PAN datasets as identified by ablation stud- ies. BLEU(1-4), NIST(1-5) and TER were used as the 10  base features in the classifiers.
TM	NEWS workshop dataset	In this paper, we focus on improving TM between Arabic and English in more realistic settings, compared to the NEWS workshop dataset.
TM	ACL 2010 NEWS workshop	Modifying the TM technique with the best reported results on the ACL 2010 NEWS workshop, namely graph reinforcement () to handle training sets of arbitrary sizes by introducing parameterized exponential penalty to the mapping induction process.
Translation	MERT baseline	 Table 1: Translation results by BLEU. Results with- out significant differences from the MERT baseline  are marked  †. The numbers in boldface are signif- icantly better than the MERT baseline (both mea- sured by the bootstrap resampling
Translation	MERT baseline	 Table 1: Translation results by BLEU. Results with- out significant differences from the MERT baseline  are marked  †. The numbers in boldface are signif- icantly better than the MERT baseline (both mea- sured by the bootstrap resampling
copying	XTOP	The author believes that this is a very sensible restriction that intuitively makes sense and at the same time suitably limits the copying power of XTOP.
MTL	Combilex G2P data	As with the MTL experiments, the reranker is trained on an intersection of the Combilex G2P data and the supplemental data.
Tagging	Estonian  Multext-East corpus	 Table 3: Tagging and segmentation results on Estonian  Multext-East corpus (Learned seg and Learned tag) com- pared to the semisupervised setting where segmentations  are fixed to gold standard (Fixed seg) and tags are fixed  to gold standard (Fixed tag). Finally the segmentatation  results from Morfessor system for comparison are pre- sented.
Tagging	Morfessor system	 Table 3: Tagging and segmentation results on Estonian  Multext-East corpus (Learned seg and Learned tag) com- pared to the semisupervised setting where segmentations  are fixed to gold standard (Fixed seg) and tags are fixed  to gold standard (Fixed tag). Finally the segmentatation  results from Morfessor system for comparison are pre- sented.
interpretation of negation	PropBank	The main contributions are: (1) interpretation of negation using fine-grained focus; (2) fine-grained focus of negation annotation over a subset of PropBank; (3) feature set to detect fine-grained focus of negation; and (4) model to retrieve precise positive implicit meaning from negated statements.
NER	DE	In our second set of experiments, we evaluate direct transfer of a NER system trained on EN to DE, ES and NL.
NER	CoNLL 2002/2003 development and test sets	 Table 4: Supervised NER results measured with F 1 -score  on the CoNLL 2002/2003 development and test sets.
Structured Event Retrieval	Microblog	Structured Event Retrieval over Microblog Archives
translation sense clustering	WordNet	In addition to describing our approach, we formalize the task of translation sense clustering and describe a procedure that leverages WordNet for evaluation.
dialect processing	EMNLP 2011	Furthermore, there was a workshop on dialect processing as part of EMNLP 2011.
parsing	Tiger treebank	Here, we see that all three adaptation methods give statistically significant 4 improvements over the baseline when parsing the Tiger treebank.
segmentation	ICSI meeting transcripts	The gold-standard segmentation is  Although TSM still performs the best on the debates, all the methods have relatively worse performance than on the ICSI meeting transcripts.
Size	French Treebank	 Table 1: Size and decomposition of the French Treebank
SDLM	Gigaword corpus	In terms of SDLM training, since the size of TreeBank-extracted elementary trees is much smaller compared to most practical n-gram LMs trained from the Gigaword corpus, we also extract elementary trees from automatically-generated parses of part of the Gigaword corpus (around oneyear newswire of "afp_eng" in Gigaword 4) in addition to TreeBank-extracted elementary trees.
SDLM	Gigaword corpus	In terms of SDLM training, since the size of TreeBank-extracted elementary trees is much smaller compared to most practical n-gram LMs trained from the Gigaword corpus, we also extract elementary trees from automatically-generated parses of part of the Gigaword corpus (around oneyear newswire of "afp_eng" in Gigaword 4) in addition to TreeBank-extracted elementary trees.
MADA classifier	MADA (v 3.2) training data	We train anew set of MADA classifier models using a combination of the original MADA (v 3.2) training data (578K words taken from PATBs 1, 2 and 3) and the enriched CATiB data (218K words).
MADA classifier	CATiB data	We train anew set of MADA classifier models using a combination of the original MADA (v 3.2) training data (578K words taken from PATBs 1, 2 and 3) and the enriched CATiB data (218K words).
MT	FBIS corpus (LDC2003E14)	The language model of the Chinese-English (English-Chinese) MT system is the English (Chinese) side of the FBIS corpus (LDC2003E14) and the English (Chinese) Gigaword corpus.
MT	English (Chinese) Gigaword corpus	The language model of the Chinese-English (English-Chinese) MT system is the English (Chinese) side of the FBIS corpus (LDC2003E14) and the English (Chinese) Gigaword corpus.
parsing	Arabic treebank	With the Penn treebank, we use sections 2-21 for training a maximum likelihood model and section 22 for parsing, while for the Arabic treebank we divide the data into two sets, of size 80% and 20%, one is used for training a maximum likelihood model and the other is used for parsing.
scraping	Lang-8	They kindly provided us with their scripts to carryout the scraping of Lang-8.
syntactic analysis	Stanford Parser	For syntactic analysis we use the Stanford Parser ().
Sentence alignment	GIZA++	Sentence alignment was done using GIZA++.
FP detection	English test set of conversational speech data	For evaluating constrained speech recognition for FP detection, the English test set of conversational speech data and word transcripts is derived from the CTS subset of the NIST 2002 Rich Transcription evaluation.
FP detection	NIST 2002 Rich Transcription evaluation	For evaluating constrained speech recognition for FP detection, the English test set of conversational speech data and word transcripts is derived from the CTS subset of the NIST 2002 Rich Transcription evaluation.
FP detection	English and Mandarin test set w.r.t	 Table 2: Probabilities of false alarms (FAs) and misses in  FP detection on the English and Mandarin test set w.r.t.
SS	Semeval-2012 dataset	SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (, resulting in inadequate evidence to generalize to robust sentential semantics.
ASR	KIT lectures	Word-Error-Rates (WER) of ASR output range from 24.37 to 30.80 for KIT lectures.
coherence modeling	J48 decision tree	For coherence modeling, we again use the J48 decision tree from the Weka machine learning toolkit () and run 4-fold crossvalidation on the 600 annotated responses.
coherence modeling	Weka machine learning toolkit	For coherence modeling, we again use the J48 decision tree from the Weka machine learning toolkit () and run 4-fold crossvalidation on the 600 annotated responses.
Relation extraction	Treebank set	 Table 3: Relation extraction results on Treebank set  (Triple)
Relation extraction	Treebank set	 Table 3: Relation extraction results on Treebank set  (Triple)
tuning	MT03	MT02 was used for tuning, and MT03 was used for evaluation.
clustering	BNC	Our noun dataset used for clustering contains 2000 most frequent nouns in the BNC.
detecting empty categories	CTB	In this paper we describe a novel approach to detecting empty categories in Chinese, using the CTB as training and test data.
EC detection	Chinese Treebank	We focus on EC detection here because most of the ECs in the Chinese Treebank are either not resolved to an overt element or linked to another EC.
EC detection	Chinese TreeBank 6.0	Our EC detection models are trained and evaluated on a subset of the Chinese TreeBank 6.0.
summarization	DUC 2 and TAC 3 evaluations	Most previous summarization studies focus on the well-formatted news documents, as driven by the annual DUC 2 and TAC 3 evaluations.
morphological analyzer	Xerox	The morphological analyzer is provided by Xerox.
coreference resolution	OntoNotes corpus	We review the most commonly used metrics (MUC, B 3 , CEAF and BLANC) on the basis of their evaluation of coreference resolution in five texts from the OntoNotes corpus.
Naive Bayes classifier	MER  (train set	 Table 3: Naive Bayes classifier performance on the MER  (train set) and Eng (test set) with only the words/phrases
Naive Bayes classifier	Eng (test set	 Table 3: Naive Bayes classifier performance on the MER  (train set) and Eng (test set) with only the words/phrases
translation	Europarl	We applied our approach to the translation of a financial ontology, translating from English to German, using Europarl as parallel corpus.
Equity	Europarl corpus	 Table 1: Contextual information for Equity with its target labels Gerechtigkeit and Eigenkapital extracted from the  Europarl corpus
segmentation	SPMRL dataset	However, we have the opposite observation on the segmentation and POS tagging on the SPMRL dataset (28% vs. 48%).
POS tagging	SPMRL dataset	However, we have the opposite observation on the segmentation and POS tagging on the SPMRL dataset (28% vs. 48%).
ASR	HUB4	We evaluate reference transcripts and output from two ASR systems run on our dataset (HUB4).
News translation task	News-test 2008	For the News translation task, we tune systems on the News-test 2008 of 2, 051 sentence pairs and test them on the News-test 2013 of 3, 000 sentence pairs from the WMT 2013 shared task (.
News translation task	News-test 2013	For the News translation task, we tune systems on the News-test 2008 of 2, 051 sentence pairs and test them on the News-test 2013 of 3, 000 sentence pairs from the WMT 2013 shared task (.
News translation task	WMT 2013 shared task	For the News translation task, we tune systems on the News-test 2008 of 2, 051 sentence pairs and test them on the News-test 2013 of 3, 000 sentence pairs from the WMT 2013 shared task (.
summarization	Mechanical Turk	The summarization task was run on Mechanical Turk.
summarization	RST Discourse Treebank (RST-DTB))	For the text summarization experiments, we use the test collection for summarization evaluation contained in the RST Discourse Treebank (RST-DTB)), which is used in the previous work.
sentence compression	British National Corpus	For sentence compression, we use the English compression corpus used in, which consists of 82 news stories selected from the British National Corpus and American News Text Corpus, and consists of more than 1,300 sentences.
sentence compression	American News Text Corpus	For sentence compression, we use the English compression corpus used in, which consists of 82 news stories selected from the British National Corpus and American News Text Corpus, and consists of more than 1,300 sentences.
AMT-based	WordNet clusters	 Table 2: AMT-based evaluations of cluster accuracy and pairwise ranking accuracy of systems that vary in the source  of clustering data, source of strength counts, and part of speech. For comparison, the approach used by de Melo and  Bansal (2013) achieves a pairwise ranking accuracy of 76.1% on the non-polysemous WordNet clusters.
translation as annotation"	bronze standard" data	Our results confirm that even though this "translation as annotation" process is noisy, it is still possible to learn on large amounts of "bronze standard" data.
Machine Translation task	DARPA BOLT project	For training our focus-tracking model, we use Chinese-English parallel data from the SMS/chat domain available as part of training data used in the Machine Translation task under the DARPA BOLT project.
MIR	PostCAT	We evaluate MIR on two tasks: (1) On word alignment , applying MIR on fertility based models we attain higher F-scores than ABA and PostCAT.
word alignment	PostCAT	We evaluate MIR on two tasks: (1) On word alignment , applying MIR on fertility based models we attain higher F-scores than ABA and PostCAT.
MIR	PostCAT	We evaluate MIR on two tasks: (1) On word alignment , applying MIR on fertility based models we attain higher F-scores than ABA and PostCAT.
pronoun resolution	Winograd dataset	Recently, Rahman and Ng (2012) gathered a dataset containing 1886 sentences of such challenging pronoun resolution problems (referred to later as the Winograd dataset, following and).
text regression or classification	ETS	This task is typically defined as a text regression or classification problem: we label student responses that consist of one or more sentences with scores on an * Work done when Keisuke Sakaguchi was an intern at ETS.
subgraph prediction	ILP decoder	Oracle results for the subgraph prediction stage are obtained using the ILP decoder to minimize the cost of the output graph, given the gold-standard.
Removing	Coreference Dataset	Removing the Training Wheels: A Coreference Dataset that Entertains Humans and Challenges Computers
coreference	AMR	Note that coreference here includes cross-sentence co-reference not based on AMR.
SRL	CoNLL-2009 shared task benchmark datasets	We evaluate our tensor-based approach for SRL on the CoNLL-2009 shared task benchmark datasets of five languages: English, German, Chinese, Catalan and Spanish (.
SMT	BOW-FD	Baseline SMT systems and BOW-FD share the hierarchical phrase-based SMT systems built with cdec ( ).
SMT	WMT2011 news test set	SMT Model parameters were optimized using MIRA () on the WMT2011 news test set (3003 sentences).
SMT	NTCIR-7 data	Its SMT features are trained on 1.8M parallel sentences of NTCIR-7 data () and weights were tuned on the NTCIR-8 test collection (2,000 sentences) using MIRA (.
SMT	NTCIR-8 test collection	Its SMT features are trained on 1.8M parallel sentences of NTCIR-7 data () and weights were tuned on the NTCIR-8 test collection (2,000 sentences) using MIRA (.
WSD	MFS baseline	It has been observed that supervised WSD approaches generally outperform the MFS baseline, whereas unsupervised WSD approaches fail to beat this baseline.
SMT adaptation	Freebase database	In this paper , we elicit response signals for SMT adaptation by executing semantic parses of translated queries against the Freebase database.
SMT	COMMON CRAWL) dataset	The SMT framework used is CDEC) with standard dense features and additional sparse features as described in . Training of the baseline SMT system was performed on the COMMON CRAWL) dataset consisting of 7.5M parallel English-German segments extracted from the web.
SMT	COMMON CRAWL) dataset	The SMT framework used is CDEC) with standard dense features and additional sparse features as described in . Training of the baseline SMT system was performed on the COMMON CRAWL) dataset consisting of 7.5M parallel English-German segments extracted from the web.
SMT	FREE917 corpus	Response-based learning for SMT uses the code described in . For semantic parsing we use the SEMPRE and PARASEMPRE tools of and Berant and Liang (2014) which were trained on the training portion of the FREE917 corpus . Further models use the training data enhanced with synonyms from WordNet as described in Section 4.
SMT	WordNet	Response-based learning for SMT uses the code described in . For semantic parsing we use the SEMPRE and PARASEMPRE tools of and Berant and Liang (2014) which were trained on the training portion of the FREE917 corpus . Further models use the training data enhanced with synonyms from WordNet as described in Section 4.
relation extraction	ACE 2005 dataset	We demonstrate these advantages on two relation extraction tasks: the well studied ACE 2005 dataset and the new ERE relation extraction task.
SuS tagging	Princeton WordNet	For SuS tagging, we only consider English-Danish and extract equivalence classes from Princeton WordNet and DanNet.
SuS tagging	DanNet	For SuS tagging, we only consider English-Danish and extract equivalence classes from Princeton WordNet and DanNet.
Reserating the awesometastic	WordNet taxonomy	Reserating the awesometastic: An automatic extension of the WordNet taxonomy for novel terms
WMT German→English task	Unversity of Edinburgh	 Table 4: Results for the WMT German→English task in  BLEU [%]. The baseline contains a recurrent neural LM.  We compare with the best single system that is reported  on matrix.statmt.org, which was submitted by the  Unversity of Edinburgh.
sentiment analysis	news headlines dataset	We evaluated the viability of using ACT in sentiment analysis on a news headlines dataset that we collected and annotated.
sentiment classification of news  headlines	SentiWordNet	 Table 3: Results for sentiment classification of news  headlines dataset using ACT and standard sentiment clas- sification method, ACT-PTQ = ACT using the parsing  tree quintet, ACT-UA = ACT using users' annotation,  ACT-PTT= ACT using the parsing tree triplet, STD- classifier= Standard classifier using SentiWordNet
Scope Resolution	Washington in "Washington Post"	(ii) Scope Resolution: Certain Entities such as Washington in "Washington Post" should not be labeled as a location name because the entire textual span is an organization name (iii) Type Disambiguation: In the sentence, " England beat Australia 2 -0".
SG	OnWN dataset	In contrast, the SG performs better than other two approaches to obtain better correlation with human judgment; yet it is still below the bag-of-word baseline (only better on OnWN dataset).
SG	STS and SICK datasets	• The SG improves the performance 2-12% on most of STS and SICK datasets.
relation extraction	Mariah Carey born? Mariah  Carey was born 27 March 1970	 Table 1: Examples of features used for relation extraction for "When was Mariah Carey born? Mariah  Carey was born 27 March 1970"
AMR parser	LDC data sets	While we demonstrate the performance of our AMR parser on data sets annotated by the LDC, we will focus attention in the demo on the following two areas: 1) we will make available AMR annotations for the data sets that were used to develop our parser, to serve as a supplement to the LDC data sets, and 2) we will demonstrate AMR parsers for German, French, Spanish and Japanese that make use of the same small set of LF-to-AMR conversion rules.
question answering (QA)	WEBQUESTIONS dataset	For the task of question answering (QA) over Freebase on the WEBQUESTIONS dataset (Berant et al., 2013), we found that 85% of all questions (in the training set) can be directly answered via a single binary relation.
synonym recognition	TOEFL dataset	We assessed the implementation of ADW on two evaluation benchmarks: similarity judgement correlation on the RG-65 dataset and synonym recognition on the TOEFL dataset.
QA	IRQA	We assert that a successful QA system will require appropriate cooperation between a KBQA and an IRQA.
AMR Formalism	AMR data	In Part I: The AMR Formalism, participants will be coached in the basics of annotation so that, when working with AMR data in the future, they will appreciate the benefits and limitations of the process by which it was created.
parsing	Penn Treebank	In Sections 4 and 5 we describe an experiment to test the parsing accuracy of a probabilistic TAG extracted automatically from the Penn Treebank.
parsing 36,000 newspaper sentences	German LFG	In our largest experiment, we used 250,000 parses which were produced by parsing 36,000 newspaper sentences with the German LFG.
MT parallel corpora	Canadian Hansards	This results in greater differences across translations than the differences in typical MT parallel corpora, such as the Canadian Hansards.
Parsing	Penn Treebank	Parsing with Treebank Grammars: Empirical Bounds, Theoretical Models, and the Structure of the Penn Treebank
segmentations	Choi's package	The segmentations were evaluated by applying an evaluation program in Choi's package.
Parameter search	ATIS corpus	Parameter search with EM produces higher quality analyses than previously exhibited by un-supervised systems, giving the best published un-supervised parsing results on the ATIS corpus.
parsing	UPenn Treebank corpus	In order to seethe correspondence between parsing accuracy and PPL/WER performance, we also evaluated the labeled precision and recall statistics (LP/LR, the standard parsing accuracy measures) on the UPenn Treebank corpus.
parsing	Wall Street Journal text	We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data.
parsing	Wall Street Journal text	In parsing Wall Street Journal text, the method gives a 5.1% relative reduction in error rate over the model of).
parsing	UPenn Wall Street Journal (WSJ) treebank	We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank.
translation disambiguation	Encarta corpus	We did not, however, conduct translation disambiguation on the words 'crane', 'sake', 'poach', 'axes', and 'motion', because the first four words do not frequently occur in the Encarta corpus, and the accuracy of choosing the major translation for the last word has already exceeded 98%.
data mining	Section 6	We discuss the application of the PDI to data mining in Section 6.
answer validation	NIST	A baseline for the answer validation experiment was defined by considering how often an answer occurs in the top 10 documents among those (1000 for each question) provided by NIST to participants.
parsing task	Penn Treebank	In Section 2 we describe the parsing task, its input representation, how this data was extracted from the Penn Treebank, and how we setup the learning curve experiments using a memory-based learner.
Text Summarization Challenge (TSC)	NTCIR (NII-NACSIS Test C ollection	The Text Summarization Challenge (TSC) task under the NTCIR (NII-NACSIS Test C ollection for IR Systems) project started in 2000 in Japan.
NER	NE tasks [Chinchor98a] in MUCs [MUC6]	During last decade, NER has drawn more and more attention from the NE tasks [Chinchor98a] in MUCs [MUC6], where person names, location names, organization names, dates, times, percentages and money amounts are to be delimited in text using SGML mark-ups.
parsing	Wall Street Journal data	Ina previous paper), a boosting algorithm was used to rerank the output from an existing statistical parser, giving significant improvements in parsing accuracy on Wall Street Journal data.
revision learning	POS bigram	When revision learning is used, all the measures are improved for both POS bigram and ChaSen.
IE paradigm	FSA-based IE	Section 4 describes the integration of predicate-argument parsers into the IE paradigm and compares the results against a FSA-based IE system.
role assignment task	FS2	For the role assignment task the best features from the feature set FS2 are the content word features (cw and cPos) and the Boolean NE flags, which show that semantic information, even if minimal, is important for role classification.
role assignment task	Boolean NE flags	For the role assignment task the best features from the feature set FS2 are the content word features (cw and cPos) and the Boolean NE flags, which show that semantic information, even if minimal, is important for role classification.
IE	PropBank	The that the new IE paradigm with the inductive learning model achieves about 90% of the performance of the FSA-based system for both domains, even though one of the domains uses mainly verbs rarely seen in training (e.g. "die" appears 5 times in PropBank).
Size	NAB	 Table 1: Size of models (in thousands) built from the  NAB and Switchboard corpora, with failure transitions
Size	Switchboard corpora	 Table 1: Size of models (in thousands) built from the  NAB and Switchboard corpora, with failure transitions
parsing	Negra	Section 4 presents a series of experiments that compare the parsing performance of these three models (and several variants) on Negra.
parsing	Negra	As detailed in Section 2.2, PPs lack an intermediate NP projection, which can be inserted straightforwardly using the following rule: In the present experiment, we investigated if parsing performance improves if we test and train on aversion of Negra on which the transformation in, but instead substitutes Pr (and, by analogy, Pl ) by: Here the head H is substituted by the sister R i−1 (and L i−1 ).
Translation	IBM  46.2  33.3  40.0  42.5  40.8  ITG  45.6  33.9  40.0  37.1  42.0	 Table 5: Translation results on the Verbmobil task.  type  automatic  human  System WER [%] PER [%] mWER [%] BLEU [%] SSER [%]  IBM  46.2  33.3  40.0  42.5  40.8  ITG  45.6  33.9  40.0  37.1  42.0
coreference resolution	MUC-6	Our coreference resolution approach is evaluated on the standard MUC-6 (1995) and MUC-7 (1998) data set.
coreference resolution	MUC-7 (1998) data set	Our coreference resolution approach is evaluated on the standard MUC-6 (1995) and MUC-7 (1998) data set.
Extracting񮽙Key񮽙Semantic񮽙Terms񮽙from񮽙Chinese񮽙Speech񮽙Query	񮽙WANG񮽙 񮽙 񮽙 Tat-Seng񮽙CHUA	Extracting񮽙Key񮽙Semantic񮽙Terms񮽙from񮽙Chinese񮽙Speech񮽙Query񮽙for񮽙Web񮽙 Searches񮽙 񮽙 Gang񮽙WANG񮽙 񮽙 񮽙 Tat-Seng񮽙CHUA񮽙񮽙񮽙
MT	Rule No. Syn. Cat.	BLEU measures the similarity between MT results and translation results made by humans (called Rule No. Syn. Cat.
Semantic annotation	Prague Treebank	Semantic annotation has predominantly concentrated on word senses, e.g. in the SENSEVAL initiative), a notable exception being the Prague Treebank . As a consequence, most recent work in corpus-based semantics has taken an unsupervised approach, relying on statistical methods to extract semantic regularities from raw corpora, often using information from ontologies like WordNet (.
Semantic annotation	WordNet	Semantic annotation has predominantly concentrated on word senses, e.g. in the SENSEVAL initiative), a notable exception being the Prague Treebank . As a consequence, most recent work in corpus-based semantics has taken an unsupervised approach, relying on statistical methods to extract semantic regularities from raw corpora, often using information from ontologies like WordNet (.
Recognition	WER	 Table 2: Recognition results grouped by WER
CFG filtering	HPSG	An empirical comparison of CFG filtering techniques for LTAG and HPSG is presented.
coreference resolution	MUC	However, our system was setup to perform coreference resolution according to the MUC rules, which are fairly different from the ACE guidelines in terms of the identification of markables as well as evaluation schemes.
coreference resolution	ACE	However, our system was setup to perform coreference resolution according to the MUC rules, which are fairly different from the ACE guidelines in terms of the identification of markables as well as evaluation schemes.
Chinese Transliteration of Foreign Personal Names	Xinhua News Agency	We use a database from the bilingual dictionary "Chinese Transliteration of Foreign Personal Names" which was edited by Xinhua News Agency and was considered the de facto standard of personal name transliteration in today's Chinese press.
Extracting Regulatory Gene Expression	PubMed	Extracting Regulatory Gene Expression Networks from PubMed
parse	ALE HPSG	This was found to improve parse times on the ALE HPSG by up to 33%.
MRS constraints	LKB system	To enumerate the solutions of MRS constraints and their translations, we use the MRS solver built into the LKB system and a solver for weakly normal dominance constraints), which is implemented in C ++ and uses LEDA, a class library for efficient data types and algorithms).
MRS solver	LKB system	To enumerate the solutions of MRS constraints and their translations, we use the MRS solver built into the LKB system and a solver for weakly normal dominance constraints), which is implemented in C ++ and uses LEDA, a class library for efficient data types and algorithms).
WSD task	SENSEVAL-2 all-words data	We calculate the accuracy of finding the predominant sense, when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ) . We also calculate the WSD accuracy that would be obtained on SemCor, when using our first sense in all contexts (  b  v ) .  In order to see how well the automatically acquired predominant sense performs on a WSD task from which the WordNet sense ordering has not been taken, we use the SENSEVAL-2 all-words data ().
parsing	WSJ part	Currently we achieve fstructure/dependency f-scores of 80.24 and 80.97 for parsing section 23 of the WSJ part of the Penn-II treebank, evaluating against the PARC 700 and DCU 105 respectively.
parsing	Penn-II treebank	Currently we achieve fstructure/dependency f-scores of 80.24 and 80.97 for parsing section 23 of the WSJ part of the Penn-II treebank, evaluating against the PARC 700 and DCU 105 respectively.
parsing	PARC 700	Currently we achieve fstructure/dependency f-scores of 80.24 and 80.97 for parsing section 23 of the WSJ part of the Penn-II treebank, evaluating against the PARC 700 and DCU 105 respectively.
parsing	DCU 105	Currently we achieve fstructure/dependency f-scores of 80.24 and 80.97 for parsing section 23 of the WSJ part of the Penn-II treebank, evaluating against the PARC 700 and DCU 105 respectively.
Translation	FUB  FSA  Italian	 Table 4: Translation results for different tasks compared to similar systems using the alignment template  (AT) approach (Tests were performed on a 1.2GHz AMD Athlon).  Task  System  Translation  WER PER 100-BLEU Memory Time/Sentence  [%]  [%]  [MB]  [ms]  Eutrans  FSA  Spanish → English 8.12 7.64  10.7  6-8  20  AT  8.25  - - - - FUB  FSA  Italian → English  27.0 21.5  37.7  3-5  22  AT  23.7 18.1  36.0  - - Verbmobil  FSA  German → English 48.3 41.6  69.8  65-90  460  AT  40.5 30.1  62.2  - - PF-Star  FSA  Italian → English  39.8 34.1  58.4  12-15  35  AT  36.8 29.1  54.3  - -
LR parsing	CFGs	In Section 2 we outline our formalization of LR parsing as a construction of PDTs from CFGs, making some superficial changes with respect to standard formulations.
PDT	CFG	 Table 1: Dimensions of PDT implementing LR  strategy for CFG derived from WSJ, sect. 02-21.
PDT	WSJ, sect. 02-21	 Table 1: Dimensions of PDT implementing LR  strategy for CFG derived from WSJ, sect. 02-21.
MT	NIST Chinese MT evaluation	Fortunately, we have access to eight MT systems' outputs, their human assessment data, and the reference translations from 2003 NIST Chinese MT evaluation).
MT evaluation	DARPA corpus	The proposed weighted N-gram model for MT evaluation is tested on a set of translations by four different MT systems available in the DARPA corpus, and is compared with the results of the baseline BLEU method with respect to their correlation with human evaluation scores.
MT	DARPA corpus	The proposed weighted N-gram model for MT evaluation is tested on a set of translations by four different MT systems available in the DARPA corpus, and is compared with the results of the baseline BLEU method with respect to their correlation with human evaluation scores.
WSD	KPCA	Based on this, we describe a full model for WSD built on KPCA.
Improving	Corpora	Improving the Accuracy of Subcategorizations Acquired from Corpora
Tagging	Alpino data	 Table 1: Tagging results on Alpino data
Question Answering (QA) field	TREC QA track	During recent years the Question Answering (QA) field has undergone considerable changes: question types have diversified, question complexity has increased, and evaluations have become more standardized -as reflected by the TREC QA track.
RU replacements	DIAG-orig	The trend on RU replacements is opposite what we would have hoped for: when repairing areal system, replacing parts that are working should clearly be kept to a minimum, and subjects replace fewer parts in DIAG-orig.
parsing	Wall Street Journal of Penn Treebank II	The aim of this paper is to report the development of log-linear models for the disambiguation in widecoverage HPSG parsing, and their empirical evaluation through the parsing of the Wall Street Journal of Penn Treebank II (.
parsing	Penn Treebank	To our knowledge, this work provides the first results of extensive experiments of parsing Penn Treebank with a probabilistic HPSG.
parsing	HPSG	To our knowledge, this work provides the first results of extensive experiments of parsing Penn Treebank with a probabilistic HPSG.
parsing	Prague Dependency Treebank	In section 5, we then evaluate the entire parsing system by training and evaluating on data from the Prague Dependency Treebank.
information extraction	MUC evaluations	The term information extraction was introduced in the MUC evaluations for the task of finding short pieces of relevant information within a broader text that is mainly irrelevant, and returning it in a structured form.
IE pattern induction	WordNet ontology	This paper presents a novel weakly supervised algorithm for IE pattern induction which makes use of the WordNet ontology.
relation detection	Centre College and Danville, KY in the phrase Centre College in Danville, KY	One such task, relation detection, finds instances of predefined relations between pairs of entities, such as a Located-In relation between the entities Centre College and Danville, KY in the phrase Centre College in Danville, KY.
ACE Relation Detection and Characterization (RDC) task	ACE training set	lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set.
relation extraction	ACE corpus	In this paper, we only measure the performance of relation extraction on "true" mentions with "true" chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.
Distribution of articles and words	Weekly Reader corpus	 Table 1: Distribution of articles and words in the  Weekly Reader corpus.
translation from German to English	Europarl corpus	In our approach we reorder the constituents in a parse of the German sentence to give the following word order, which is much closer to the target English word order (words which have been "moved" are underlined): We applied our approach to translation from German to English in the Europarl corpus.
MT	NIST	We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software.
identification of phrasal terms	TREC databases	By contrast, Schone and Jurafsky (2001) evaluate the identification of phrasal terms without grammatical filtering on a 6.7 million word extract from the TREC databases, applying both WordNet and online dictionaries as gold standards.
identification of phrasal terms	WordNet	By contrast, Schone and Jurafsky (2001) evaluate the identification of phrasal terms without grammatical filtering on a 6.7 million word extract from the TREC databases, applying both WordNet and online dictionaries as gold standards.
Switching	FHMM	Section 3 presents anew model, the Switching FHMM, which represents cross-sequence dependencies more effectively than FHMMs.
NEC	Spanish development set	 Table 4: NEC performance on the Spanish development set
relation extraction	PartEx system	Section 2 considers background and related work; Section 3 introduces an algorithm for relation extraction, and its implementation in the PartEx system; Section 4 considers materials and methods used for experiments with PartEx.
relation extraction	PartEx	Section 2 considers background and related work; Section 3 introduces an algorithm for relation extraction, and its implementation in the PartEx system; Section 4 considers materials and methods used for experiments with PartEx.
classifying English inclusions	EU data	When classifying English inclusions in the EU data, accuracy decreased slightly by 0.3%.
Demonstration	ACL	In addition, the Demonstration should address an application of broad interest in such away that it can be appreciated by the diverse audience that attends ACL.
Speech Analysis and Interpretation	HRL Laboratories	In this paper we give a brief description of a two-way speech-to-speech translation system, which was created under a collaborative effort between three organizations within USC (the Speech Analysis and Interpretation Lab of the Electrical Engineering department, the Information Sciences Institute, and the Institute for Creative Technologies) and the Information Sciences Lab of HRL Laboratories.
Decoding weight optimization	NIST MT evaluation test set (MT03)	Decoding weight optimization was done using a set of 200 sentences from the 2003 NIST MT evaluation test set (MT03).
segmentation	Physics ASR transcripts	We also include segmentation results on Physics ASR transcripts.
parsing	Minipar	All of the parsing, noun-phrase identification, and named-entity recognition are done automatically with Minipar.
noun-phrase identification	Minipar	All of the parsing, noun-phrase identification, and named-entity recognition are done automatically with Minipar.
WSD	WSJ) of the DSO corpus	They found that training a WSD system on one part (BC or WSJ) of the DSO corpus and applying it to the other part can result in an accuracy drop of 12% to 19%.
WSD	WordNet	Importantly, our WSD algorithms and combination methods do not make use of training material in anyway, nor do they use the first sense information available in WordNet.
WSD	WordNet sense inventory	It seems therefore that the major obstacle to effective WSD is the fine granularity of the WordNet sense inventory, rather than the performance of the best disambiguation systems.
relation extraction	ACE RDC 2003 corpus	As the largest annotated corpus in relation extraction, the ACE RDC 2003 corpus shows that different subtypes/types of relations are much unevenly distributed and a few relation subtypes, such as the subtype "Founder" under the type "ROLE", suffers from a small amount of annotated data.
relation extraction	ACE	It indicates that, although support vector machines and maximum entropy models always perform better than the simple perceptron algorithm inmost (if not all) applications, the hierarchical learning strategy using the perceptron algorithm can easily overcome the difference and outperforms the flat learning strategy using the overwhelming support vector machines and maximum entropy models in relation extraction, at least on the ACE: Comparison of the hierarchical and flat learning strategies on the relation subtypes of different training data sizes.
relation extraction	ACE Data	Supervised methods for relation extraction perform well on the ACE Data, but they require a large amount of manually labeled relation instances.
relation subtype detection	ACE 2003 corpus	We evaluated this label propagation based relation extraction method for relation subtype detection and characterization task on the official ACE 2003 corpus.
ACE Relation Detection and Characterization (RDC) task	ACE training set and test set	lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their  frequency of occurrence in the ACE training set and test set.
Saturation	HPSG	Saturation is ensured by additional mechanisms, such as the distinction of terminal and non-terminal symbols in rewriting systems or by requiring some features to have an empty list as a value in HPSG.
Parsing of German	NEGRA corpus	Parsing of German seems to be even harder and parsers trained on the NEGRA corpus or an enriched version of it still perform considerably worse.
parsing	FRMG	In order to validate our approach, we applied these principles to look for error causes in parsing results given by two deep parsing systems for French, FRMG and SXLFG, on large corpora.
parsing	Wall Street Journal (WSJ) section of the Penn Treebank	But the lack of corpora has led to a situation where much of the current work on parsing is performed on a single domain using training data from that domain -the Wall Street Journal (WSJ) section of the Penn Treebank (.
parsing	WSJ-trained  reranker	 Table 2: Effects of adding NANC sentences to WSJ  training data on parsing performance. f -scores  for the parser with and without the WSJ reranker  are shown when evaluating on BROWN develop- ment. For this experiment, we use the WSJ-trained  reranker.
semantics construction	USR	This simplifies semantics construction, and current algorithms support the efficient enumeration of the individual semantic representations from an USR (.
classification	DTree1	In our first approach, the classification is done in a single stage (DTree1).
classification process	DTree2	To alleviate this problem, we broke the classification process down to two stages (DTree2).
classifier stacking	Penn Chinese Treebank	In classifier stacking, we collect the outputs from Maxent, DTree and TiMBL, which are all trained on a separate dataset from the training set (section 400-650 of the Penn Chinese Treebank, smaller than the original training set).
sense matching task	Senseval dataset	Having source-target synonym pairs, a classification instance for the sense matching task is created from each example occurrence of the target word in the Senseval dataset.
WSD	Senseval-3 data set	Supervised WSD is first implemented using the Bayesian model on the Senseval-3 data set.
word alignment	Canadian Hansards data	Through a careful choice of features, and modest improvements in training procedures, we obtain the lowest error rate yet reported for word alignment of Canadian Hansards data.
parsing	UW Switchboard corpus	To see whether the improvements are translated into parsing results, we have conducted one more set of experiments on the UW Switchboard corpus.
MT	NIST score	We measure the performance of the MT system by translation quality and use NIST score as the evaluation measure).
Entity Detection and Tracking task	ACE program	In AR, the research trend has been shifting from rulebased approaches) to empirical, or corpus-based, approaches) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC 1 and the Entity Detection and Tracking task in the ACE program 2 ).
SVM chunking	YamCha 8 toolkit	For all the SVM chunking experiments, we use the YamCha 8 toolkit (.
POS tagging	Penn Treebank POS tags	CCG supertagging is much harder than POS tagging because the CCG tag set consists of finegrained lexical categories, resulting in a larger tag set -over 400 CCG lexical categories compared with 45 Penn Treebank POS tags.
keyword matching	WordNet	However, using this knowledge beyond simple keyword matching, for example in inferences, requires it to be linked into formal semantic repositories such as ontologies or term banks like WordNet.
question answering	DUC evaluation	The last few years have seen a convergence between the question answering and summarization communities), as highlighted by the shift from generic to queryfocused summaries in the 2005 DUC evaluation (.
question answering (QA)	Marcus Camby won for Massachusetts	A question answering (QA) system could find the answer in the snippet "Marcus Camby won for Massachusetts" as the question verb play is related to the verb win.
plot advice	Adventure Corpus	The plot advice algorithm is run using a randomly selected corpus of 20 stories, 5 from each plot rating level using the "Adventure Corpus."
Naive Bayes	Adventure	 Table 4: Naive Bayes Results: "Adventure"
recognizing entailment between a question and an answer	TREC test sets	In order to provide training data that replicated the task of recognizing entailment between a question and an answer, we assembled a corpus of 5000 question-answer pairs selected from answers that our baseline Q/A system returned in response to anew set of 1000 questions selected from the TREC test sets.
semantic parsing	ATIS (Air Travel Information Service))	Some of the previous work on semantic parsing has focused on fairly simple domains, primarily, ATIS (Air Travel Information Service)) whose semantic analysis is equivalent to filling a single semantic frame.
classification	ILC	The classification performance was evaluated using the F 1 measure for the individual role and ILC classifiers and the accuracy for the multiclassifiers.
PSM	SET1	To establish a baseline system, we first train a PSM using all 8,898 DQTPs in supervised manner and conduct a closed test on SET1 as in
parse selection	Brown Corpus	1 shows that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute less than 1% to parse selection accuracy when test data is in the same domain, and yield no improvement for test data selected from the Brown Corpus.
parse selection	WSJ PTB	Both Collins' Model 3 and the XLE Parser use lexicalized models for parse selection trained on the rest of the WSJ PTB.
Topic Tracking	TDT project	Topic Tracking defined by the TDT project is a research area to attack the problem.
Translation	FrameNet	 Table 2. Translation accuracies of the most am- biguous words in FrameNet
SMT	Europarl corpus 2	We start by empirically testing the performance of a previously developed English-Spanish SMT system, built from the large Europarl corpus 2.
MT	SYSTRAN	 Table 1: MT Results on development and test sets, for the two baseline systems compared to SYSTRAN and to the 'EU'
parsing	Penn Treebank	Both showed decent results on parsing the Penn Treebank, but in the decade since these papers were published, history-based parsers have been largely ignored by the research community in favor of PCFG-based approaches.
Recognition	Dev  Eval  ASR  19.3  21.3  ASR+MT IBM-3  17.4  18.6  IBM-4  17.4  18.5  IBM-5  17.6  18.7	 Table 4: Recognition WER [%] using fertility- based transducer to rescore ASR word graphs.  Models  Dev  Eval  ASR  19.3  21.3  ASR+MT IBM-3  17.4  18.6  IBM-4  17.4  18.5  IBM-5  17.6  18.7
ASR word graphs	Dev  Eval  ASR  19.3  21.3  ASR+MT IBM-3  17.4  18.6  IBM-4  17.4  18.5  IBM-5  17.6  18.7	 Table 4: Recognition WER [%] using fertility- based transducer to rescore ASR word graphs.  Models  Dev  Eval  ASR  19.3  21.3  ASR+MT IBM-3  17.4  18.6  IBM-4  17.4  18.5  IBM-5  17.6  18.7
answering "relationship" questions	TREC 2005	This paper explores the role of information retrieval in answering "relationship" questions, anew class complex information needs formally introduced in TREC 2005.
question answering	SIGIR 2004	Indeed, a number of works have specifically examined the effects of information retrieval on question answering, including a dedicated workshop at SIGIR 2004 ().
IR	WT10G test collection	In IR, term weighting schemes estimate the relevance for the WT10G test collection.
MT	E09 E11 E12 E14 E15 E17 E22	There are seven sets of MT outputs (E09 E11 E12 E14 E15 E17 E22), all of which contain 919 English sentences.
SIA	WordNet	 Table 8: Results of SIA working with Porter-Stem  and WordNet
case splitting	Overall corpus	For our ARE system with case splitting, we present the results on Overall corpus, as well as separate results on Simple, Average and Hard categories.
generative parsing	WSJ Penn Treebank	Furthermore, a simple combination of the shift-reduce parsing model with an existing generative parsing model produces results with accuracy that surpasses any that of any single (nonreranked) parser tested on the WSJ Penn Treebank, and comes close to the best results obtained with discriminative reranking).
translation of European Parliament Speeches from Spanish to English	European TC-STAR project in February 2006	In this work, we consider the translation of European Parliament Speeches from Spanish to English, in the framework of an international evaluation organized by the European TC-STAR project in February 2006.
standardization	ISO-TC37/SC4	The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EA-GLES 1 , PAROLE/SIMPLE (), ISLE/MILE (  and LIRICS 2 . These continuous efforts has been crystallized as activities in ISO-TC37/SC4 which aims to make an international standard for language resources.
model training	GIZA++ toolkit	For model training, we use the GIZA++ toolkit 3 .  The evaluation results on the testing data are shown in table 2.
EM training	GIZA++ toolkit	For EM training, we use the GIZA++ toolkit 4 . In this paper, we take English to Chinese word alignment as a case study.
SENSEVAL exercises	WordNet	The English data used for the SENSEVAL exercises, arguably the most widely used data to train and test WSD systems, are annotated based on very fine-grained distinctions defined in WordNet, with human inter-annotator agreement at a little over seventy percent and the top-ranked systems' performances falling between 60%~70%).
WSD	WordNet	The English data used for the SENSEVAL exercises, arguably the most widely used data to train and test WSD systems, are annotated based on very fine-grained distinctions defined in WordNet, with human inter-annotator agreement at a little over seventy percent and the top-ranked systems' performances falling between 60%~70%).
WSD	WordNet	2 Some previous work on WSD using semantic similarity utilized the semantic network of nouns in WordNet to disambiguate term senses to improve the precision of SMART information retrieval at the stage of indexing, in which he assigned two different weights for both directions of edges in the network to compute the similarity of two nodes.
EM  training	development data set	We chose the number of iterations for EM  training as the turning point of AER on the development data set.
Parsing	English RST treebank	Parsing through machine learning has encountered a bottleneck, due to limited resources--there is only one English RST treebank publicly available, and one RST-annotated German corpus on its way.
MT	MT-train dataset	The baseline MT system was trained on the MT-train dataset described in.
MT experiment	MT-test	The test set for the MT experiment is a 1K sentences set from the same domain (shown as MT-test in the table).
Translation	NIST  Chinese-English	 Table 4: Translation quality using an additional  adapted phrase table trained on the dev/test sets.  Different selection and scoring methods. NIST  Chinese-English, best results printed in boldface.
Translation	NIST Chinese-English	 Table 5: Translation quality using an additional  phrase table trained on monolingual Chinese news  data. Selection step using threshold on confidence  scores. NIST Chinese-English.
MT	Hiero	In this paper, we successfully integrate a state-of-the-art WSD system into a state-of-the-art hierarchical phrase-based MT system, Hiero.
WSD	WSJ) of the DSO corpus	They found that training a WSD system on one part (BC or WSJ) of the DSO corpus, and applying it to the other, can result in an accuracy drop of more than 10%, highlighting the need to perform domain adaptation of WSD systems to new domains.
MT	LDC data	This is based on the assumption that if the MT results can be accurately distinguished from human references  The experiment was conducted using 10-fold cross validation on two LDC data, low-ranked and high-ranked data . The results using SVM as classification model are given in.
SRL	CoNLL shared tasks	A large number of SRL systems have been evaluated and compared on the standard data set in the CoNLL shared tasks (, and many systems have performed reasonably well.
SRL task	NomBank	First, we present a novel application of ASO to the SRL task on NomBank.
MT	NIST MT Eval	Moreover, there is already a sizable collection of human assessed data fora range of MT systems through multiple years of the NIST MT Eval efforts.
MT	MT5	Using the two mid-quality MT systems as references (MT5 and MT6), regression metrics yield correlations that are only slightly lower than standard metrics with human references.
MT	MT6	Using the two mid-quality MT systems as references (MT5 and MT6), regression metrics yield correlations that are only slightly lower than standard metrics with human references.
MT evaluation	NIST	Currently, the most widely used automatic MT evaluation metric is the NIST BLEU-4 ( . By default, METEOR script counts the words that match exactly, and words that match after a simple Porter stemmer.
parsing	WSJ corpus	The effect of sentence length on parsing speed is small: the full WSJ corpus was parsed at 3900 words/sec.
MST oracle	CoNLL test set	 Table 1: k-best MST oracle results. The 1-best results represent the performance of the parser in isolation.  Results are reported for the CoNLL test set and for English, on Section 23 of the Penn WSJ Treebank.
MST oracle	Section 23 of the Penn WSJ Treebank	 Table 1: k-best MST oracle results. The 1-best results represent the performance of the parser in isolation.  Results are reported for the CoNLL test set and for English, on Section 23 of the Penn WSJ Treebank.
translation	NIST script	The Bleu score was used to measure translation accuracy, calculated by the NIST script with its default settings.
MT	Europarl corpus	Comparing U-DOP* and DOP* in syntaxbased MT on the German-English Europarl corpus against the Pharaoh system.
parser adaptation	Brown test section	For the parser adaptation scenario, they are tested on the Brown test section.
SMT	FAQ pages	Our first query expansion model trains an endto-end phrase-based SMT model on 10 million question-answer pairs extracted from FAQ pages.
reference resolution	WordNet	To obtain this semantic information, previous work on reference resolution usually leverages a semantic lexicon like WordNet (Vieira and Poesio,;).
coreference resolver	Penn Treebank	This paper examines whether a learning-based coreference resolver can be improved using semantic class knowledge that is automatically acquired from aversion of the Penn Treebank in which the noun phrases are labeled with their semantic classes.
SC induction	ACE Phase 2 coreference corpus	As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.
SC classification	ACE training set	 Table 3: SC classification accuracies of different methods for the ACE training set and test set.
IE	MUC4	In this paper, we follow the IE tasks as defined by the conferences MUC4, MUC6 and ACE RDC: slotbased extraction, template filling and relation extraction, respectively.
IE	MUC6	In this paper, we follow the IE tasks as defined by the conferences MUC4, MUC6 and ACE RDC: slotbased extraction, template filling and relation extraction, respectively.
IE	ACE RDC	In this paper, we follow the IE tasks as defined by the conferences MUC4, MUC6 and ACE RDC: slotbased extraction, template filling and relation extraction, respectively.
OI	OO	For OI and OO, the test data is the Brown test section (2424 sentences), and the self-training data is either the Brown training section (in OI) or WSJ sections 2-21 (in OO).
OI	Brown test section	For OI and OO, the test data is the Brown test section (2424 sentences), and the self-training data is either the Brown training section (in OI) or WSJ sections 2-21 (in OO).
OI	WSJ	For OI and OO, the test data is the Brown test section (2424 sentences), and the self-training data is either the Brown training section (in OI) or WSJ sections 2-21 (in OO).
parser adaptation	WSJ sections 2-21	To further demonstrate our results for parser adaptation, we also performed the OI experiment where seed data is taken from WSJ sections 2-21 and both self-training and test data are taken from the Switchboard corpus.
parser adaptation	Switchboard corpus	To further demonstrate our results for parser adaptation, we also performed the OI experiment where seed data is taken from WSJ sections 2-21 and both self-training and test data are taken from the Switchboard corpus.
answer selection	WordNet	To address the second question, several answer selection approaches have used external knowledge resources such as WordNet, CYC and gazetteers for answer validation or answer reranking.
answer selection	CYC	To address the second question, several answer selection approaches have used external knowledge resources such as WordNet, CYC and gazetteers for answer validation or answer reranking.
translation	Europarl multilingual corpus	In section 6, translation results on the Europarl multilingual corpus indicate the effectiveness of our method.
morphological segmentation	CELEX	 Table 3: Evaluating rule-based and data-based sys- tems for morphological segmentation with respect to  CELEX manual morphological annotation.
parsing	MSTParser	To study the influence of parsing methodology, we will compare two different parsers: MaltParser () and MSTParser ().
Information Retrieval	BERUFEnet dataset	 Table 5: Information Retrieval performance on the BERUFEnet dataset.
text mining	M. W..	This is the basic intelligent procedure, and is important in text mining systems (M. W..
normalize	TF-IDF	We normalize them using TF-IDF.
statistical machine translation (SMT)	NIST	In recent years, statistical machine translation (SMT) systems have achieved substantial progress regarding their perfomance in international translation tasks (TC-STAR, NIST, GALE).
Translation	IWSLT06 CSTAR	 Table 1: Translation results as increasing amount of training  data in IWSLT06 CSTAR track
Tagging	Hungarian of HunPos	 Table 4: Tagging accuracy for Hungarian of HunPos  with and without morphological lexicon and with  first and second order emission/lexicon probabili- ties.
parser retraining	GENIA	We also measure the accuracy improvements obtained by parser retraining with GENIA, to examine the domain portability, and to evaluate the effectiveness of domain adaptation.
parsing	AImed corpus	shows the time for parsing the entire AImed corpus, and shows the time required for 10-fold cross validation with GENIA-retrained parsers.
parsing	GENIA-retrained	shows the time for parsing the entire AImed corpus, and shows the time required for 10-fold cross validation with GENIA-retrained parsers.
MT	ACL-07 MT workshop	We note that this framework is not used by any of the 11 automatic MT metrics in the ACL-07 MT workshop.
query expansion	WordNet	By incorporating this similarity function into the axiomatic retrieval models, we show that query expansion using the information from only WordNet can lead to significant improvement of retrieval performance, which has not been shown in the previous studies).
Supertagging	CCG	Hypertagging: Supertagging for Surface Realization with CCG
Parsing	NER	 Table 5: Parsing results with NER features
SMT	QJ 񮽙 񮽙 KP 񮽙 J K . 񮽙 񮽙 Z	For example, our base SMT system translates QJ 񮽙 񮽙 KP 񮽙 J K . 񮽙 񮽙 Z @ PP P 񮽙 @ (as a whole phrase) to "Premier Li Peng", based on its bitext knowledge.
SMT	Premier Li Peng"	For example, our base SMT system translates QJ 񮽙 񮽙 KP 񮽙 J K . 񮽙 񮽙 Z @ PP P 񮽙 @ (as a whole phrase) to "Premier Li Peng", based on its bitext knowledge.
Name translation	BBN	 Table 2: Name translation accuracy with respect to BBN  and re-annotated Gold Standard on 1730 named entities  in 637 sentences.
Name translation	Gold Standard	 Table 2: Name translation accuracy with respect to BBN  and re-annotated Gold Standard on 1730 named entities  in 637 sentences.
NER	Tokyo Stock Exchange	Most studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN) to named entity categories such as "Tokyo Stock Exchange → {ORGANIZATION}".
representing the meaning of sentences or other short texts	WordNet	We also proposed anew method of representing the meaning of sentences or other short texts using either WordNet or Roget's Thesaurus, and tested it on the data set provided by.
SMT	NIST MT test sets	We carryout experiments on a state-of-the-art SMT system, i.e.,, and show that the abbreviation translations consistently improve the translation performance (in terms of BLEU ()) on various NIST MT test sets.
SRL	Penn Treebank corpus	PropBank () is the most widely used corpus for training SRL systems, probably because it contains running text from the Penn Treebank corpus with annotations on all verbal predicates.
Document recovery from BOW	bigram LMs	3. Document recovery from BOW: With the bigram LMs, we show improved accuracy in recovering ordered documents from BOWs.
POS tagging	CoNLL'00 shared task data	Then, we apply our model to widely used test collections, namely Penn Treebank (PTB) III data for POS tagging, CoNLL'00 shared task data for syntactic chunking, and CoNLL'03 shared task data for NER.
POS tagging	CoNLL'03 shared task data	Then, we apply our model to widely used test collections, namely Penn Treebank (PTB) III data for POS tagging, CoNLL'00 shared task data for syntactic chunking, and CoNLL'03 shared task data for NER.
Syntactic chunking	CoNLL'00 shared task data	 Table 8: Syntactic chunking results of the previous top  systems for CoNLL'00 shared task data (F β=1 score)
SRL	Modern Standard Arabic	In this paper, we present an SRL system for Modern Standard Arabic that exploits many aspects of the rich morphological features of the language.
summarization	DUC 3	Following the current practice in evaluating summarization, particularly DUC 3 , we use the ROUGE evaluation package ().
summarization	MEAD-Doc+Cite baseline	Since they are general evaluation measures for summarization, they are also applicable to evaluating the MEAD-Doc+Cite baseline method to be described below.
resolution	NP Chunker	For both training and resolution, an input raw document was processed by a pipeline of NLP modules including Tokenizer, Part-of-Speech tagger, NP Chunker and Named-Entity (NE) Recognizer.
segmentation	Joint S&T	We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T, over the perceptron-only model POS+.
segmentation	Joint S&T	Among other features, the 4-gram POS LM plays the most important role, removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.
segmentation	Joint S&T	Finally, the word count penalty gives improvement to the cascaded model, 0.13 points on segmentation and 0.16 points on Joint S&T.
segmentation	Joint S&T	Experimental results show that, it achieves obvious improvement over the perceptron-only model, about from 0.973 to 0.978 on segmentation, and from 0.925 to 0.934 on Joint S&T, with error reductions of 18.5% and 12% respectively.
expert search	TREC	Many approaches to expert search have been proposed by the participants of TREC and other researchers.
expert search task	TREC	They are also the standard measures used in the expert search task of TREC.
SMT	FBIS Chinese-English parallel text	To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system) with the FBIS Chinese-English parallel text.
MT	Chiang's Hiero system	We carried out MT experiments for translation from Chinese to English and from Arabic to English, using a descendant of Chiang's Hiero system.
translation from Chinese to English	Chiang's Hiero system	We carried out MT experiments for translation from Chinese to English and from Arabic to English, using a descendant of Chiang's Hiero system.
ASTRL	WSJ derived corpus	To evaluate ASTRL, we used the WSJ derived corpus.
summaries	DUC	We carried out automatic evaluation of our summaries using ROUGE) toolkit, which has been widely adopted by DUC for automatic summarization evaluation.
MT	Development4	We used a standard phrase-based statistical MT system to generated N-best lists (N=2000) on Development4, Development5, and Evaluation sub-sets.
NEs	NHK corpus	Table 3 shows the statistics of NEs of NHK corpus which were annotated by a graduate student except shows the results of chunkers trained from whole IREX corpus against NHK corpus.
NEs	IREX corpus	Table 3 shows the statistics of NEs of NHK corpus which were annotated by a graduate student except shows the results of chunkers trained from whole IREX corpus against NHK corpus.
NEs	NHK corpus	Table 3 shows the statistics of NEs of NHK corpus which were annotated by a graduate student except shows the results of chunkers trained from whole IREX corpus against NHK corpus.
NIST translation tasks	FBIS 1 corpus	Experiments on Chinese-to-English NIST translation tasks were carried out on the FBIS 1 corpus.
Construct State Modification	Arabic Treebank	Construct State Modification in the Arabic Treebank
phrase translation	NIST	The quality of phrase translation is typically measured using n-gram precision based metrics such as BLEU () and NIST scores.
NIST 05 machine translation evaluation	NIST04) data	The experiment was carried out on the Chinese part of the NIST 05 machine translation evaluation (NIST05) and NIST 04 machine translation evaluation (NIST04) data, where NISTT05 contains 100 documents and NIST04 contains 200 documents.
NIST 05 machine translation evaluation	NISTT05	The experiment was carried out on the Chinese part of the NIST 05 machine translation evaluation (NIST05) and NIST 04 machine translation evaluation (NIST04) data, where NISTT05 contains 100 documents and NIST04 contains 200 documents.
NIST 05 machine translation evaluation	NIST04	The experiment was carried out on the Chinese part of the NIST 05 machine translation evaluation (NIST05) and NIST 04 machine translation evaluation (NIST04) data, where NISTT05 contains 100 documents and NIST04 contains 200 documents.
NIST 04 machine translation evaluation	NIST04) data	The experiment was carried out on the Chinese part of the NIST 05 machine translation evaluation (NIST05) and NIST 04 machine translation evaluation (NIST04) data, where NISTT05 contains 100 documents and NIST04 contains 200 documents.
NIST 04 machine translation evaluation	NISTT05	The experiment was carried out on the Chinese part of the NIST 05 machine translation evaluation (NIST05) and NIST 04 machine translation evaluation (NIST04) data, where NISTT05 contains 100 documents and NIST04 contains 200 documents.
NIST 04 machine translation evaluation	NIST04	The experiment was carried out on the Chinese part of the NIST 05 machine translation evaluation (NIST05) and NIST 04 machine translation evaluation (NIST04) data, where NISTT05 contains 100 documents and NIST04 contains 200 documents.
Biomedical Term Disambiguation	Medline	An Unsupervised Vector Approach to Biomedical Term Disambiguation: Integrating UMLS and Medline
Subcategorization Acquisition	French Verbs	A Subcategorization Acquisition System for French Verbs
coreference resolution	JAVARAP	A number of systems that perform coreference resolution are publicly available, such as GUITAR (, which handles the full coreference task, and JAVARAP (), which only resolves pronouns.
information extraction	PropBank	, and information extraction (. In recent years, with the wide availability of corpora such as PropBank () and FrameNet (), a number of studies have presented statistical approaches to SRL).
SRL	PropBank	Most SRL studies on PropBank have used these tags in order to gather a sufficient amount of training data, and to generalize semantic-role classifiers across different frames.
SRL	SRL	Precision reflects the algorithm's applicability for creating training data to be used by supervised SRL models, while the standard SRL F-score measures the model's performance when used by itself.
clause detection	NANC (Graff, 1995) corpus	For the statistics gathering phase of the clause detection algorithm, we used 4.5M sentences of the NANC (Graff, 1995) corpus, bounding their length in the same manner.
parsing	CTB	more effective than instance pruning for the use of converted treebanks for parsing and converted CDT helps parsing on CTB.
parsing	CTB	more effective than instance pruning for the use of converted treebanks for parsing and converted CDT helps parsing on CTB.
generative parser	CTB training set	 Table 4: Results of the generative parser on the de- velopment set, when trained with various weight- ing of CTB training set and CDT P S .
generative parser	CTB training set	 Table 5: Results of the generative parser (GP) and  the reranking parser (RP) on the test set, when  trained on only CTB training set or an optimal  combination of CTB training set and CDT P S .
generative parser	CTB training set	 Table 5: Results of the generative parser (GP) and  the reranking parser (RP) on the test set, when  trained on only CTB training set or an optimal  combination of CTB training set and CDT P S .
generative parser	CTB training set	 Table 6: Results of the generative parser and the  reranking parser on the test set, when trained on  an optimal combination of CTB training set and  converted CDT.
Translation	IWSLT 2008 evaluation campaign	Translation quality was evaluated using both the BLEU score proposed by and also the modified BLEU (BLEU-Fix) score 3 used in the IWSLT 2008 evaluation campaign, where the brevity calculation is modified to use closest reference length instead of shortest reference length.
CRR translation	RBMT	 Table 6: CRR translation results (BLEU scores)  by using different RBMT systems
summarization	EDCL	The purposes of our experiment are two-fold: (i) evaluate the effects of wiki definition to the TREC-QA task; and (ii) examine the characteristics and summarization performance of EDCL.
Distribution of edits	Wikipedia	 Table 4: Distribution of edits on Wikipedia.
cross-lingual sentiment classification	Eng-lish corpus	This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chi-nese sentiment classification by using the Eng-lish corpus as training data.
opinion retrieval	TREC blog track	For the opinion retrieval task, we use the two datasets used by TREC blog track and NTCIR MOAT evaluation workshops.
MT	NIST	Since human evaluation is costly and difficult to do reliably, a major focus of research has been on automatic measures of MT quality, pioneered by BLEU () and NIST).
translation from English to French	Europarl	In this section we consider two real translation tasks, namely, translation from English to French, trained on Europarl () and translation from German to Spanish training on the NewsCommentary corpus.
translation from English to French	NewsCommentary corpus	In this section we consider two real translation tasks, namely, translation from English to French, trained on Europarl () and translation from German to Spanish training on the NewsCommentary corpus.
translation from German to Spanish	NewsCommentary corpus	In this section we consider two real translation tasks, namely, translation from English to French, trained on Europarl () and translation from German to Spanish training on the NewsCommentary corpus.
GE training	Spanish 60 data	 Table 3: Error analysis for GE training with possible parent + sequence constraints on Spanish 60 data. On the left, the
Query construction	China Huarong Asset Management Corporation	Step 3: Query construction: We select the words "资产" and "管理" to translate and a bilingual query is constructed as: " If we don't add some English words into the query, we may not obtain the web pages which contain the English phrase "China Huarong Asset Management Corporation".
tagging	Penn Treebank	They show considerable improvements in tagging accuracy when using a coarser-grained version (with 17-tags) of the tag set from the Penn Treebank.
characterize the meaning of a word	WordNet	Linguists have tried to characterize the meaning of a word with feature-based approaches, such as semantic roles), as well as word-relation approaches, such as WordNet.
NP coreference resolution	MUC and ACE task definitions	We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora.
sentiment text classification	Cornell movie-review dataset 2	And 20NG is a collection of approximately 20,000 20-category documents . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset 2 () and one dataset from product reviews of domain DVD.
sentiment classification	movie review dataset	For evaluation, we use five sentiment classification datasets, including the widely-used movie review dataset () as well as four datasets that contain reviews of four different types of product from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (.
SSL	RBF kernel SVM	We performed manual iterative parameter optimization during training based on prediction accuracy to find the best k-nearest parameter for SSL, i.e., k = {3, 5, 10, 20, 50} , and best C = 10 −2 , .., 10 2 and γ = 2 −2 , .., 2 3 for RBF kernel SVM.
ASR	NIST's ROVER system (Fiscus, 1997)	Others combine the transcripts or word lattices (from which transcripts are extracted) of two complementary ASR systems, a technique first proposed in the context of NIST's ROVER system (Fiscus, 1997) with a 12% relative error reduction (RER), and subsequently widely employed in many ASR systems.
WER calculation	TBL	What we show in this paper is that a true WER calculation is so valuable that a manual transcription of only about 10 minutes of a one-hour lecture is necessary to learn the TBL rules, and that this smaller amount of transcribed data in turn makes the true WER calculation computationally feasible.
Parsing	Penn treebank version 3	For Parsing, sentences are cased and tokenization abides to the PTB segmentation as used in the Penn treebank version 3.
MT parsing	CTB	We evaluate MT parsing models on CTB rather than on WSJ, since CTB contains newswire and is thus more representative of MT evaluation conditions.
MT parsing	WSJ	We evaluate MT parsing models on CTB rather than on WSJ, since CTB contains newswire and is thus more representative of MT evaluation conditions.
Parsing	WSJ test data	In the Parsing setting, we use its best configuration, which reaches a tagging accuracy of 97.25% on standard WSJ test data.
Cause Identification	Aviation Safety Reports	Semi-Supervised Cause Identification from Aviation Safety Reports
NER	CoNLL benchmark	Our NER system achieves the best current result on the widely used CoNLL benchmark.
parsing	Villemonte de la)	The more restricted class of LCFRS has received more attention concerning parsing (Villemonte de la).
SMT	NIST05 test set	For instance, in our investigations for SMT (Section 3.1), the Formally SCFG based hierarchical phrase-based model (hereinafter FSCFG) has a better generalization capability than a Linguistically motivated STSSG based model (hereinafter LSTSSG) (, with 5% rules of the former matched by NIST05 test set while only 3.5% rules of the latter matched by the same test set.
OOV translation	CLIR	Therefore, OOV translation has become a very important and challenging issue in CLIR.
paraphrase generation	MOSES	Most paraphrase generation tools use some standard SMT decoding algorithms) or some off-the-shelf decoding tools like MOSES (.
SMT decoding	MOSES	Most paraphrase generation tools use some standard SMT decoding algorithms) or some off-the-shelf decoding tools like MOSES (.
QA	OWL	They are of particular importance for QA over semi-structured data, as represented by Topic Maps, OWL or custom XML formats.
question answering problem	DUC-2007 main task	In this paper, we analyze the impact of different automatic annotation methods on the performance of supervised approaches to the complex question answering problem (defined in the DUC-2007 main task).
relation extraction	Joachim's (1999) SVM light	We implemented the tree-kernels for relation extraction in Java and used Joachim's (1999) SVM light with the JNI Kernel Extension using the implementation details from the original papers.
information credibility analysis	WISDOM	In this paper, we describe an information credibility analysis system called WISDOM, which automatically analyzes and organizes the above aspects on the basis of semantically oriented NLP techniques.
MST parser	Penn Chinese Treebank (CTB) 5.0	For the 2nd-order MST parser trained on Penn Chinese Treebank (CTB) 5.0, the classifier give an precision increment of 0.5 points.
MT outputs	NTCIR-7	Evaluation experiments were conducted to calculate the correlation among human judgments, along with the scores produced using automatic evaluation methods for MT outputs obtained from the 12 machine translation systems in NTCIR-7.
MT	NTCIR-7	Evaluation experiments using MT outputs obtained by 12 machine translation systems in NTCIR-7() demonstrate that the scores obtained using our system yield the highest correlation with the human judgments among the automatic evaluation methods in both sentence-level adequacy and fluency.
automatic extraction of structured information from Wikipedia	WordNet	But while a great deal of work has been recently devoted to the automatic extraction of structured information from Wikipedia (, inter alia), the knowledge extracted is organized in a looser way than in a computational lexicon such as WordNet.
SMT	NIST training set	In SMT experiments, the bilingual training dataset is the NIST training set excluding the Hong Kong Law and Hong Kong Hansard, and our 5-gram language model is trained from the Xinhua section of the Gigaword corpus.
SMT	Hong Kong Law and Hong Kong Hansard	In SMT experiments, the bilingual training dataset is the NIST training set excluding the Hong Kong Law and Hong Kong Hansard, and our 5-gram language model is trained from the Xinhua section of the Gigaword corpus.
SMT	Gigaword corpus	In SMT experiments, the bilingual training dataset is the NIST training set excluding the Hong Kong Law and Hong Kong Hansard, and our 5-gram language model is trained from the Xinhua section of the Gigaword corpus.
parsing evaluation	BioInfer corpus	For parsing evaluation, grammatical relations from the BioInfer corpus were used ( same post-processing process as to convert the C&C parser output to Stanford format grammatical relations).
sentiment analysis	Example 1	We argue that when performing sentiment analysis on reviews, such as in the Example 1, more attention is needed to distinguish between attributes that are mentioned with and without sentiment.
EM+IP IP minimization	EM	EM+IP IP minimization using initial grammar provided by EM.
highlight generation task	CNN highlights	For the highlight generation task, the original CNN highlights were used as the reference.
MERT weight training	LDC2005T06 corpus	Besides, the development-set for MERT weight training is composed of 200 sentence pairs selected from the LDC2005T06 corpus, which includes 482 manually tagged NE pairs.
Temporal information processing	TERN2004	Temporal information processing is a topic of natural language processing boosted by recent evaluation campaigns like TERN2004, 1 TempEval-1 () and the forthcoming TempEval-2 2.
classification	Ó Séaghdha data	Our classification accuracy results are 79.3% on our data and 63.6% on the Ó Séaghdha data.
translation	NIST version	The translation quality is evaluated in terms of case-insensitive NIST version BLEU metric.
Spelling correction	NYT	 Table 2: Spelling correction accuracy (%). SVM  trained on NYT, tested on NYT (IN) and out-of- domain Gutenberg (O1) and Medline (O2).
Spelling correction	NYT	 Table 2: Spelling correction accuracy (%). SVM  trained on NYT, tested on NYT (IN) and out-of- domain Gutenberg (O1) and Medline (O2).
semantic role labeling (SRL)	Wall Street Journal text	In recent semantic role labeling (SRL) competitions such as the shared tasks of, supervised SRL systems have been trained on newswire text, and then tested on both an in-domain test set (Wall Street Journal text) and an out-of-domain test set (fiction).
SRL	Wall Street Journal text	In recent semantic role labeling (SRL) competitions such as the shared tasks of, supervised SRL systems have been trained on newswire text, and then tested on both an in-domain test set (Wall Street Journal text) and an out-of-domain test set (fiction).
SRL	Brown corpus	These pipeline systems are important for generating features for SRL, and one key reason for the poor performance of SRL systems on the Brown corpus is that the pipeline systems themselves perform worse.
SRL	Brown test corpus	 Table 5: SRL results (F1) on the Brown test corpus broken down by role type. BL is the Base- line+HMM+Paths model, MSH is the Multi-Span-HMM model. Column 8 to 16 are all adjuncts (AM-).  We omit roles with ten or fewer examples.
attribute selection	WoZ data	For attribute selection we choose a majority baseline (randomly choosing between 3 or 4 attributes) since the attribute selection models learned by Supervised Learning on the WoZ data didn't show significant improvements.
parsing	English WSJ	In the parsing case, the central result is that accuracies in the range of state-of-the-art parsers (i.e., over 88% F1 on English WSJ) can be obtained with no sampling, no latent-variable modeling, no smoothing, and even no explicit lexicon (hence negligible training overall).
SRL	Chinese PropBank	We are not aware of any SRL system combing automatic predicate recognition, verbal SRL and nominal SRL on Chinese PropBank and NomBank.
SRL	NomBank	We are not aware of any SRL system combing automatic predicate recognition, verbal SRL and nominal SRL on Chinese PropBank and NomBank.
TE task	NIST TAC RTE 5 challenge corpus	This pilot study establishes the feasibility of an inference-motivated annotation effort, and its results offer a quantitative insight into the difficulty of the TE task, and the distribution of a number of entailment-relevant linguistic phenomena over a representative sample from the NIST TAC RTE 5 challenge corpus.
grammar induction	Wall Street Journal corpus	We demonstrate that derived constraints aid grammar induction by training Klein and Manning's Dependency Model with Valence (DMV) on this data set: parsing accuracy on Section 23 (all sentences) of the Wall Street Journal corpus jumps to 50.4%, beating previous state-of-the-art by more than 5%.
IE	ACE corpus	Annotated corpora and semantic dictionaries used in IE, such as the ACE corpus and WordNet, include examples of part-whole relations.
IE	WordNet	Annotated corpora and semantic dictionaries used in IE, such as the ACE corpus and WordNet, include examples of part-whole relations.
Query refinement	PRF	Query refinement is done by adding the terms obtained through PRF, along with their weights, to the actual query.
WD coreference resolution	ACE	 Table 3: Results for WD coreference resolution on ACE.
coreference resolution	MUC	This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B 3 , and CEAF.
HMM alignment	Berkeley Aligner	We trained and combined two HMM alignment models) using the Berkeley Aligner.
translation	Joint HMM	 Table 1: Experimental results demonstrate that the full extraction set model outperforms supervised and  unsupervised baselines in evaluations of word alignment quality, extraction set quality, and translation.  In word and bispan evaluations, GIZA++ did not have access to a dictionary while all other methods  did. In the BLEU evaluation, all systems used a bilingual dictionary included in the training corpus. The  BLEU evaluation of supervised systems also included rule counts from the Joint HMM to compensate  for parse failures.
WSD research	WordNets	Advances in WSD research in the current millennium can be attributed to several key factors: the availability of large scale computational lexical resources such as WordNets, the availability of large scale corpora, the existence and dissemination of standardized data sets over the past 10 years through different testbeds such as SENSEVAL and SEMEVAL competitions, 1 devising more robust computing algorithms to handle large scale data sets, and simply advancement in hardware machinery.
SRL	PropBank	To factor out errors from standard SRL analyses, the model used gold-standard argument labels provided by PropBank and NomBank.
SRL	NomBank	To factor out errors from standard SRL analyses, the model used gold-standard argument labels provided by PropBank and NomBank.
SMT decoders	Microsoft Research Asia	SMT decoders are forced to face the challenge of * This work was finished while the first author visited Microsoft Research Asia as an intern.
SMT	MOSES decoder	For the training of our SMT model, we use a modified training toolkit adapted from the 2 http://www.cs.huji.ac.il/~shais/code/index.html MOSES decoder.
authorship prediction	PCFG-I	 Table 2: Accuracy in % for authorship prediction on different datasets. Bigram-I refers to the bigram  language model with smoothing. PCFG-E refers to the ensemble based on MaxEnt, Bigram-I, and  PCFG-I. MaxEnt+Bigram-I refers to the ensemble based on MaxEnt and Bigram-I.
MT	WMT08	Many automatic metrics of MT quality have been proposed and evaluated in terms of correlation with human judgments while various techniques of manual judging are being examined as well, see e.g. 1 , WMT08 and WMT09 . The contribution of this paper is twofold.
MT	WMT09	Many automatic metrics of MT quality have been proposed and evaluated in terms of correlation with human judgments while various techniques of manual judging are being examined as well, see e.g. 1 , WMT08 and WMT09 . The contribution of this paper is twofold.
SRL	Chinese PropBank (CPB) data	We present encouraging SRL results on Chinese PropBank (CPB) data.
coreference resolution task	ACE NIST (2004))	Several formal evaluations have been conducted for the coreference resolution task (e.g.,, ACE NIST (2004)), and the data sets created for these evaluations have become standard benchmarks in the field (e.g., MUC and ACE data sets).
coreference resolution task	MUC	Several formal evaluations have been conducted for the coreference resolution task (e.g.,, ACE NIST (2004)), and the data sets created for these evaluations have become standard benchmarks in the field (e.g., MUC and ACE data sets).
coreference resolution task	ACE data sets	Several formal evaluations have been conducted for the coreference resolution task (e.g.,, ACE NIST (2004)), and the data sets created for these evaluations have become standard benchmarks in the field (e.g., MUC and ACE data sets).
predicate argument structure analysis	GDA Corpus	For predicate argument structure analysis, we have the following representative large corpora: FrameNet (),), and NomBank () in English, the Chinese PropBank in Chinese, the GDA Corpus (), Kyoto Text Corpus Ver.4.0 (), and the NAIST Text Corpus () in Japanese.
predicate argument structure analysis	Kyoto Text Corpus Ver.4.0	For predicate argument structure analysis, we have the following representative large corpora: FrameNet (),), and NomBank () in English, the Chinese PropBank in Chinese, the GDA Corpus (), Kyoto Text Corpus Ver.4.0 (), and the NAIST Text Corpus () in Japanese.
predicate argument structure analysis	NAIST Text Corpus	For predicate argument structure analysis, we have the following representative large corpora: FrameNet (),), and NomBank () in English, the Chinese PropBank in Chinese, the GDA Corpus (), Kyoto Text Corpus Ver.4.0 (), and the NAIST Text Corpus () in Japanese.
Tagging	PTB45 tagset	 Table 1. Tagging accuracy under the best M-to-1 map, the greedy 1-to-1 map, and  VI, for the full PTB45 tagset and the reduced PTB17 tagset. HMM-EM, HMM-VB  and HMM-GS show the best results from
Tagging	PTB17 tagset	 Table 1. Tagging accuracy under the best M-to-1 map, the greedy 1-to-1 map, and  VI, for the full PTB45 tagset and the reduced PTB17 tagset. HMM-EM, HMM-VB  and HMM-GS show the best results from
NIST Chinese-to-English translation task	MT04	We evaluate our approach by comparing translation quality, as evaluated by the IBM-BLEU () metric on the NIST Chinese-to-English translation task using MT04 as development set to train the model parameters λ, and MT05, MT06 and MT08 as test sets.
NIST Chinese-to-English translation task	MT05	We evaluate our approach by comparing translation quality, as evaluated by the IBM-BLEU () metric on the NIST Chinese-to-English translation task using MT04 as development set to train the model parameters λ, and MT05, MT06 and MT08 as test sets.
NIST Chinese-to-English translation task	MT06	We evaluate our approach by comparing translation quality, as evaluated by the IBM-BLEU () metric on the NIST Chinese-to-English translation task using MT04 as development set to train the model parameters λ, and MT05, MT06 and MT08 as test sets.
NIST Chinese-to-English translation task	MT08	We evaluate our approach by comparing translation quality, as evaluated by the IBM-BLEU () metric on the NIST Chinese-to-English translation task using MT04 as development set to train the model parameters λ, and MT05, MT06 and MT08 as test sets.
PSCFG rule extraction	Venugopal	We perform PSCFG rule extraction and decoding using the open-source "SAMT" system (Venugopal and, using the provided implementations for the hierarchical and syntax-augmented grammars.
MT	Spanish test data	Evaluation: All the MT systems are run on the Spanish test data and the quality of the resulting English translations are evaluated using two different measures-(1) Normalized edit distance score), and (2) BLEU (Papineni et When computing edit distance, we account for substitutions, insertions, deletions as well as local-swap edit operations required to convert a given English string into the (gold) reference translation.
MT	Time corpus	plots the BLEU scores versus training sizes for different MT systems on the Time corpus.
association mapping	JST model	Such an association mapping problem can be naturally solved by the posterior inference in the JST model.
Classification	Pang	 Table 2: Classification accuracy on three tasks. From left to right the datasets are: A collection of 2,000 movie reviews  often used as a benchmark of sentiment classification (Pang
sentiment classification	Pang	 Table 2: Classification accuracy on three tasks. From left to right the datasets are: A collection of 2,000 movie reviews  often used as a benchmark of sentiment classification (Pang
SRL annotation	HTER	The correlation with human judgment on adequacy of the fully automated SRL annotation version, i.e., applying ASSERT on both the reference translation and the MT output, of the SRL based evaluation metric is about 80% of that of HTER.
Authorship attribution	BC data set	 Table 3: Authorship attribution accuracy when using bags  of local histograms and different kernels for word-based  and character-based representations. The BC data set is  used. Settings 1, 2 and 3 correspond to k = 2, 5 and 20,  respectively.
cluster  prediction task	MUC	 Table 2: Results using the MUC metric on the cluster  prediction task. Note that while the precision of the base- line is higher, the recall and overall F1 of our model out- weighs that. While MUC has a deficiency in that putting  everything into a single cluster will artificially inflate the  score, parameters on our model are set so that the model  uses the same number of clusters as the baseline system.
Transliteration mining	WIL data sets	Transliteration mining on the WIL data sets is easier due to a higher percentage of transliterations than in parallel corpora.
parsing	Penn Treebank-style contextfree grammars	Even for practical CCG that are strongly context-free, parsing is much harder than with Penn Treebank-style contextfree grammars, with vast numbers of nonterminal categories leading to increased grammar constants.
relation extraction	ACE 2004 training data	For relation extraction, we used the benchmark ACE 2004 training data.
WSJ dependency parsing	WSJ section 22	 Table 1: UAS results for English WSJ dependency parsing. Dev  is WSJ section 22 (all sentences) and Test is WSJ section 23  (all sentences). The order 2 baseline represents McDonald and  Pereira (2006).
WSJ dependency parsing	WSJ	 Table 1: UAS results for English WSJ dependency parsing. Dev  is WSJ section 22 (all sentences) and Test is WSJ section 23  (all sentences). The order 2 baseline represents McDonald and  Pereira (2006).
OOV detection	MIT lectures data set	In addition we report OOV detection results on a MIT lectures data set () consisting of 3 Hrs from two speakers with a 1.5% OOV rate.
coreference	Japanese (MUC  score	 Table 6: Results for overall coreference: Japanese (MUC  score)
coreference resolution	FrameNet	Specifically, we (1) derive world knowledge from encyclopedic sources that are underinvestigated for coreference resolution, including FrameNet () and, in addition to coreference-annotated data and unannotated data; (2) incorporate such knowledge as features into a richer baseline feature set that we previously employed; and (3) evaluate their utility using two coreference models, the traditional mention-pair model () and the recently developed cluster-ranking model.
correcting preposition and article errors	ESL data set	Similarly, web-based models built on Google Web1T 5-gram Corpus () achieve better results when compared to a maximum entropy model that uses a corpus 10, 000 times smaller ( . In this work, we compare four popular learning methods applied to the problem of correcting preposition and article errors and evaluate on a common ESL data set.
Entity Linking task	TAC 2009	We adopted the standard performance metrics used in the Entity Linking task of the TAC 2009).
NER	CoNLL data	We train our NER system on an 80% sample of the CoNLL data.
Attack	MUC-4	Our learned templates for Attack have a different granularity than MUC-4.
crossevent inference	ACE texts	To compare with the reported work on crossevent inference and its sentence-level baseline system, we cross-validate our method on 10 separate sets of 40 ACE texts, and report the optimum, worst and mean performances (see) on the data by using Precision (P), Recall (R) and F-measure (F).
evidence propagation	Term set: Wiki100L	 Table 8. Combination of PB and DS graphs for  evidence propagation (Term set: Wiki100L)
SMT	Canadian Hansards	SMT owes its existence to data like the Canadian Hansards (which bylaw must be published in both French and English).
Turker translation	NIST MT evaluation	We also evaluate Turker translation quality by using them as reference sets to score various submissions to the NIST MT evaluation.
TAG	CFG	As a formal tree rewriting system, TAG provides a larger domain of locality than CFG to state linguistic dependencies that are far apart since the formalism treats trees as basic building blocks.
constituent parsing	CTB 5.0	For constituent parsing, the best result on CTB 5.0 is reported to be 78% F 1 measure for unlimited sentences with automatically assigned POS tags.
SemEval'07 task	FrameNet lexicon	The standard evaluation script from the SemEval'07 task calculates precision, recall, and F 1 -score for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one in the FrameNet lexicon.
WSD	WordNet	The bulk of WSD algorithms up till now use pre-defined sense inventories (such as WordNet) that often contain finegrained sense distinctions, which poses serious problems for computational semantic processing.
Sentence annotation	Ads data  set	 Table 3: Sentence annotation performance on Ads data  set
Sentence annotation	Review  data set	 Table 4: Sentence annotation performance on Review  data set
topic labelling task	MSZ	In this section we present our experimental results for the topic labelling task, based on both the unsupervised and supervised methods, and the methodology of, which we denote MSZ for the remainder of the paper.
LAS	CoNLL 2007 evaluation script 7	To compute LAS, we use the CoNLL 2007 evaluation script 7 with punctuation tokens excluded from scoring (as was the default setting in CoNLL 2006).
parsing	Penn Treebank-style context-free grammars	Even with practical CCG that are strongly context-free, parsing can be much harder than with Penn Treebank-style context-free grammars, since the number of nonterminal categories is generally much larger, leading to increased grammar constants.
parsing	HWDep	A*    makes modest CPU-time improvements in parsing the full space of the HWDep model.
AST	CKY	On the other hand, the AST tradeoff improves significantly: by combining AST with A* we observe a decrease in running time of 15% for the A* NULL parser of the HWDep model over CKY.
Breakdown	HWDep (AST) model	 Table 6: Breakdown of the number of sentences parsed  for the HWDep (AST) model (see
parsing	Modern Standard Arabic (MSA)	In this paper, we explore the role of morphological features in parsing Modern Standard Arabic (MSA).
POS taggers	Wall Street Journal corpus of the Penn Treebank (PTB;	Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB;.
question detection	Meeting Recorder Dialog Act corpus (MRDA)	In this work, we focus on question detection in the Meeting Recorder Dialog Act corpus (MRDA) ( ), using text sentences with question marks in Wikipedia "talk" pages.
machine translation evaluation	NIST	Although BLEU) has become a de facto standard for machine translation evaluation, other metrics such as NIST) and, more recently,), are commonly used too.
EM estimation	IBM Model 1	Problems with the standard EM estimation of IBM Model 1 was pointed out by and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported.
translation task	GS-N	For each translation task, we report two EM estimates, obtained after 5 and 80 iterations (EM-5 and EM-80), respectively; and three Gibbs sampling estimates, two of which were initialized with those two EM Viterbi alignments (GS-5 and GS-80) and a third was initialized naively 9 (GS-N).
split	PTB3	We follow the standard approach to split PTB3, using sections 2 -21 for training, section 22 for development and 23 for final testing.
Query	Snowball	We call this method Query Snowball.
DS	YAGO	In this paper, we extend DS by (i) considering relations from semantic repositories different from Wikipedia, i.e. YAGO, and (2) using training instances derived from any Wikipedia document.
WSD	Europarl parallel corpus	Most stateof-the-art WSD systems are supervised classifiers that are trained on manually sense-tagged corpora, which are very time-consuming and expensive to build) . In order to overcome this acquisition bottleneck (sense-tagged corpora are scarce for languages other than English), we decided to take a multilingual approach to WSD, that builds up the sense inventory on the basis of the Europarl parallel corpus (.
parsing	LiuG	The parsing times of LGFN and LiuG also slowly increased, but parsing LGFN consistently took less time than LiuG.
parsing	LiuG	The parsing times of LGFN and LiuG also slowly increased, but parsing LGFN consistently took less time than LiuG.
parsing LGFN	LiuG	The parsing times of LGFN and LiuG also slowly increased, but parsing LGFN consistently took less time than LiuG.
Sentiment Analysis	Modern Standard Arabic	Subjectivity and Sentiment Analysis of Modern Standard Arabic
dependency parsing	PTB-III	Our method matches the results of current state-of-the-art systems with very few features, i.e., F-score 90.72 with 344 features for CoNLL-2003 NER data, and UAS 93.55 with 12.5K features for dependency parsing data derived from PTB-III.
NER	Penn Treebank III (PTB) corpus	For the supervised datasets, we used CoNLL'03 shared task data for NER, and the Penn Treebank III (PTB) corpus for dependency parsing.
kmeans clustering	Dutch corpus data	In this section we describe our experiment with kmeans clustering to derive property costs from English and Dutch corpus data.
dependency parsing	Penn Tree-bank (PTB)	We present a set of experiments using semantic classes in dependency parsing of the Penn Tree-bank (PTB).
dependency parsing	Penn Treebank (PTB)	We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB).
parsing Hebrew	PCFG-LA Berkeley Parser	We experiment with extending a lattice parsing methodology for parsing Hebrew (Gold-berg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser.
SDP	English WSJ task	In its pure form, with no pruning or approximation, SDP is neither fast nor accurate, achieving less than 70% F1 on the English WSJ task.
MT shared tasks	WMT	I mainly take advantage of this type of evaluation as part of participating with my research group in MT shared tasks with large evaluation campaigns such as WMT (e.g.).
relation extraction	J.D.	This paper presents an analysis of a relation extraction system based on prior work but applied to the J.D.
Relation extraction	JDPA Corpus test set	 Table 1: Relation extraction results on the JDPA Corpus test set, broken down by document source.
Dependency parsing	CoNLL X shared task	Dependency parsing made many improvements due to the CoNLL X shared task).
Quantitative evaluations	Meteor	Quantitative evaluations can be automatic, using metrics such as Bleu () or Meteor (, where the MT output is compared to one or more human reference translations.
summarization	Microblog	Take summarization for example, a Microblog user usually has to browse through tensor even hundreds of posts together with their responses daily, therefore it can be beneficial if there is an intelligent tool assisting summarizing those information.
text mining	Microblog	We also found that the content features are not as useful as the temporal or positional features for text mining in Microblog.
Sharing posts	Microblog	The Sharing posts are very frequently observed in Microblog as Microbloggers like to share interesting websites, pictures, and videos with their friends.
SITS	Crossfire	Moreover, we provide evidence that SITS captures an individual's tendency to introduce new topics in political contexts, via analysis of the 2008 US presidential debates and the television program Crossfire.
topic segmentation problem	ICSI corpus and the 2008 debates	shows the performance of various models on the topic segmentation problem, using the ICSI corpus and the 2008 debates.
parsing	Penn Treebank	As the parsing community trains on sections 2-21 of the Penn Treebank () and tests on section 23, we create Gigaword sections by isolating specific months.
MWE identification	McNemar-s test	In order to establish the statistical significance of results between two parsing experiments in terms of F 1 and UAS, we used a unidirectional t-test for two independent samples 12 . The statistical significance between two MWE identification experiments was established by using the McNemar-s test.
translation	IWSLT 2011  MT_CE task	 Table 4. The translation results on IWSLT 2011  MT_CE task.
machine translation	IWSLT	Experimental results show that, our method can significantly improve machine translation performance on both IWSLT and NIST data, compared with a state-of-the-art baseline.
machine translation	NIST data	Experimental results show that, our method can significantly improve machine translation performance on both IWSLT and NIST data, compared with a state-of-the-art baseline.
coreference resolution	Wikipedia	There is also work on end-to-end coreference resolution that uses large noun-similarity lists) or structured knowledge bases such as Wikipedia () and).
coreference resolution	Reconcile DT baseline	Altogether, our final system produces the best numbers reported to date on end-to-end coreference resolution (with automatically detected system mentions) on multiple data sets and metrics (MUC and B 3 ), achieving significant improvements over the Reconcile DT baseline and over the state-of-the-art results of.
stemming	Iveonik English Stemmer	We further conducted stemming on the words with Iveonik English Stemmer (http://www.iveonik.com/ ).
transliteration mining	WIL	NEWS10 is a standard task on transliteration mining from WIL.
MSR Sentence  Completion Challenge	SAT test set	 Table 6: Performance of methods on the MSR Sentence  Completion Challenge, contrasted with SAT test set.
MIX	Hampshire College Summer Studies in Mathematics	According to, "MIX was originally described by Emmon Bach and was so-dubbed by students in the 1983 Hampshire College Summer Studies in Mathematics".
parsing	Penn Chinese Treebank 5.1	Our approach can significantly advance the state-of-the-art parsing accuracy on two widely used target tree-banks (Penn Chinese Treebank 5.1 and 6.0) using the Chinese Dependency Treebank as the source treebank.
parsing	Chinese Dependency Treebank	Our approach can significantly advance the state-of-the-art parsing accuracy on two widely used target tree-banks (Penn Chinese Treebank 5.1 and 6.0) using the Chinese Dependency Treebank as the source treebank.
Size	French Treebank	 Table 1: Size and decomposition of the French Treebank
Tuning	MOSES	Tuning was done with n-best MERT, which is available in MOSES.
MT evaluation	NIST 2002	For development, we used the Chinese-English MT evaluation sets of NIST 2002 and NIST 2005.
MT evaluation	NIST 2005	For development, we used the Chinese-English MT evaluation sets of NIST 2002 and NIST 2005.
parser combination	WSJ treebank	For parser combination, we follow the setting of, using Section 24 instead of Section 22 of WSJ treebank as development set.
temporal ordering problem in clinical text	Timebank	When we model the temporal ordering problem in clinical text as a ranking problem, we empirically show that it outperforms classification; we perform similar experiments with Timebank and observe the opposite conclusion (classification outperforms ranking).
fact extraction from Web contents	DBpedia	In recent years, automated fact extraction from Web contents has seen significant progress with the emergence of freely available knowledge bases, such as DBpedia (,), TextRunner (, or ReadTheWeb ().
Translation	WordSys	Translation evaluation of WordSys and proposed system using BLEU-SBP ( 4 compares WordSys to our proposed system.
Translation	WordSys	Translation evaluation of WordSys and proposed system using BLEU-SBP ( 4 compares WordSys to our proposed system.
Translation	WordSys	 Table 4 Translation evaluation of WordSys and pro-
translation	Chinese Treebank	We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.
parsing	Chinese Treebank	We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.
translation	Chinese Treebank	We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.
parsing	Chinese Treebank	We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.
dependency parsing	MST-parser	For preprocessing, we performed POS tagging by stanford-tagger. and dependency parsing by MST-parser ().
SR labeling	LTH	We performed SR labeling on gold data: Results of Sentence Compression and system outputs by LTH.
tokenizing	GE-NIA tagger	3 observe that tokenizing with the GE-NIA tagger yields mismatches in one of five sentences of the GENIA Treebank, although the GENIA guidelines refer to scripts that maybe available on request).
tokenizing	GENIA Treebank	3 observe that tokenizing with the GE-NIA tagger yields mismatches in one of five sentences of the GENIA Treebank, although the GENIA guidelines refer to scripts that maybe available on request).
tokenizing	GENIA	3 observe that tokenizing with the GE-NIA tagger yields mismatches in one of five sentences of the GENIA Treebank, although the GENIA guidelines refer to scripts that maybe available on request).
MT evaluations	NTCIR-9 Chinese-to-English PatentMT task	The toolkit has been used to build translation systems that have placed well at recent MT evaluations, such as the NTCIR-9 Chinese-to-English PatentMT task).
translation	IBM-version	The translation quality was evaluated with the case-insensitive IBM-version BLEU4.
translation spotting	GIZA++ toolkit	We also build two different translation spotting modules by using the GIZA++ toolkit) with the intersection/union of the bidirectional word alignment as baseline systems.
IE tasks	WizIE	In the pre-study survey, 10 of the participants reported no prior experience with IE tasks, two of them have seen demonstrations of IE systems, and two had brief involvement in IE development, but no experience with WizIE.
IE development	WizIE	In the pre-study survey, 10 of the participants reported no prior experience with IE tasks, two of them have seen demonstrations of IE systems, and two had brief involvement in IE development, but no experience with WizIE.
link prediction	WordNet	This paper explains how link prediction , information integration and taxonomy induction methods have been used to build UWN based on WordNet and extend it with millions of named entities from Wikipedia.
information integration	WordNet	This paper explains how link prediction , information integration and taxonomy induction methods have been used to build UWN based on WordNet and extend it with millions of named entities from Wikipedia.
taxonomy induction	WordNet	This paper explains how link prediction , information integration and taxonomy induction methods have been used to build UWN based on WordNet and extend it with millions of named entities from Wikipedia.
Writing in English	EFL (English as a Foreign Language) learners	Writing in English might be one of the most difficult tasks for EFL (English as a Foreign Language) learners.
SVM	WMT12 QE task	µ is a baseline which predicts the training mean, SVM uses the same system as the WMT12 QE task, and the remainder are GP regression models with different kernels (all include additive noise).
sentence compression	Simple English Wikipedia dataset of Woodsend and Lapata (2011)	For sentence compression, we adapted the Simple English Wikipedia dataset of Woodsend and Lapata (2011), containing aligned sentences for 15,000 articles from the English and Simple English Wikipedias.
sentence compression	Simple English Wikipedias	For sentence compression, we adapted the Simple English Wikipedia dataset of Woodsend and Lapata (2011), containing aligned sentences for 15,000 articles from the English and Simple English Wikipedias.
sentiment analysis	movie review dataset	In the case of sentiment analysis, while people are able to achieve 87.5% accuracy) on a movie review dataset (), the performance drops to 75% ( ) on a sentence level movie review dataset).
parsing constituency-based grammars	HPSG	We are interested in parsing constituency-based grammars such as HPSG and CCG using a small amount of data specific for the target formalism, and a large quantity of coarse CFG annotations from the Penn Treebank.
parsing constituency-based grammars	Penn Treebank	We are interested in parsing constituency-based grammars such as HPSG and CCG using a small amount of data specific for the target formalism, and a large quantity of coarse CFG annotations from the Penn Treebank.
parsing	Penn Treebank	We evaluate OSTAG on the familiar task of parsing the Penn Treebank.
Translation	FBIS data set	 Table 4: Translation performance on Chinese to  English translation, showing BLEU% for models  trained on the FBIS data set.
MT task	EMEA corpus	We also report for the first time-BLEU score results fora large-scale MT task using only non-parallel data (EMEA corpus).
MT	OPUS corpus	First, we present MT results on non-parallel Spanish/English data from the OPUS corpus which was used by and.
MT	Spanish/English OPUS corpus	 Table 2: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours)  on the Spanish/English OPUS corpus using only non-parallel corpora for training. For the Bayesian  methods 4a and 4b, the samplers were run for 1000 iterations each on a single machine (1.8GHz Intel  processor). For 1a, 2a, 2b, we list the training times as reported by Nuhn et al. (2012) based on their EM  implementation for different settings.
MT	French/Spanish EMEA  corpus	 Table 3: MT results on the French/Spanish EMEA  corpus using the new hash sampling method.  *  The  last row displays results when we sample target  translations from a pruned candidate set (most fre- quent 1k Spanish words + identity translation can- didates) which enables the sampler to run much  faster when using more complex models.
classification	EUROVOC thesaurus	For classification we use an SVM binary classifier and training data taken from the EUROVOC thesaurus.
alignment	FBIS data set	 Table 3: Minutes used for alignment and phase  pair extraction in the FBIS data set.
sentiment classification	MPQA datasets	 Table 3: Accuracy of sentiment classification on  the sentiment polarity (SP) and MPQA datasets.  For NB we only display the best result among a  larger group of models analysed in that paper.
domain adaptation (DA)	QuestionBank	To evaluate the domain adaptation (DA) approach and to compare with results reported by, we use the first and second half of QuestionBank () as our development and test sets (target).
sentiment classification	MOUD dataset	We run our sentiment classification experiments on the MOUD dataset introduced earlier.
parsing	Kyoto corpus	The widely used resources for parsing research are the Kyoto corpus () and the NAIST text corpus (, both of which are based on the dependency structures of chunks.
parsing	NAIST text corpus	The widely used resources for parsing research are the Kyoto corpus () and the NAIST text corpus (, both of which are based on the dependency structures of chunks.
Word alignment evaluation	OntoNotes	Word alignment evaluation is done over the sections of OntoNotes that have matching goldstandard word alignment annotations from GALE Y1Q4 dataset.
Word alignment evaluation	GALE Y1Q4 dataset	Word alignment evaluation is done over the sections of OntoNotes that have matching goldstandard word alignment annotations from GALE Y1Q4 dataset.
SRL evaluation	PropBank	This annotation was an extension of the standard training, development and testing sections of Penn TreeBank that have been typically used for SRL evaluation and were already annotated with PropBank and NomBank predicate structures.
Speaker identification	Emma	 Table 5: Speaker identification accuracy (in %) on  Pride & Prejudice, Emma, and The Steppe.
Speaker identification	The Steppe	 Table 5: Speaker identification accuracy (in %) on  Pride & Prejudice, Emma, and The Steppe.
TOEFL synonym selection task	RG-65 dataset	Third, we demonstrate that this single representation can achieve state-of-the-art performance on three similarity tasks, each operating at a different lexical level: (1) surpassing the highest scores on the SemEval-2012 task on textual similarity) that compares sentences, (2) achieving a near-perfect performance on the TOEFL synonym selection task proposed by, which measures word pair similarity, and also obtaining state-of-the-art performance in terms of the correlation with human judgments on the RG-65 dataset, and finally (3) surpassing the performance of in a sensecoarsening task that measures sense similarity.
synonym recognition	TOEFL dataset	For synonym recognition, we used the TOEFL dataset created by.
Sentence compressions	SRILM	Sentence compressions are evaluated by a 5-gram language model trained on Gigaword by SRILM).
navigational context	QA module	Using the navigational context, Spacebook pushes point-of-interest information which can then initiate tourist information tasks using the QA module.
query segmentation	Amazon Mechanical Turk (AMT)	In this paper we explore crowdsourcing as an option for query segmentation through experiments designed using Amazon Mechanical Turk (AMT) . We compare the results against gold datasets created by trained annotators.
NER	Penn Treebank (PTB) III corpus	We used CoNLL'03 data for NER, and the Penn Treebank (PTB) III corpus ( converted to dependency trees for DEPAR ().
NER	DEPAR	We used CoNLL'03 data for NER, and the Penn Treebank (PTB) III corpus ( converted to dependency trees for DEPAR ().
Coreference resolvers	ACE) and OntoNotes () data sets	Coreference resolvers are typically evaluated on collections of news articles that cover a wide range of topics, such as the ACE) and OntoNotes () data sets.
Dependency parsing	CTB4	 Table 1: Dependency parsing results on the test set with different joint inference features. Abbreviations:  dep1/dep2 = first-order parser and second-order parser; baseline = dep1 without considering any joint  inference features; +* = the baseline features conjoined with the joint inference features derived from the  heterogeneous treebanks; CTB4 + CDT = we simply concatenate the two corpora and train a dependency  parser, and then test on CTB4 and CDT using this single model. Improvements of joint models over  baseline models are shown in parentheses.
Tagging	McNemar's test	 Table 4: Tagging results on the test set. '  † ' denotes  statistically significant over the greedy baseline by  McNemar's test (  )
PRC	ODP-239	 Table 2: PRC comparative results for F β and F b 3 over the ODP-239 and MORESQUE datasets.
PRC	MORESQUE datasets	 Table 2: PRC comparative results for F β and F b 3 over the ODP-239 and MORESQUE datasets.
SMT	NIST data	All SMT models included a 5-gram language model built from the English side of the NIST data plus the English side of the Twitter corpus D trg . Word alignments were created using GIZA++.
RST-based	NIST04	Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively.
RST-based	NIST05/CWMT2008	Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively.
PRO tuning	MERT	In line with prior work, PRO tuning achieves a bit lower scores on the tuning set but higher scores on the test set, compared to MERT.
Retrieval	Yahoo! Answers	 Table 3: Retrieval Performance on Dataset  from Yahoo! Answers
ROUGE evaluation	Amazon Mechanical Turk 1	Reference timeline in ROUGE evaluation is manually generated by using Amazon Mechanical Turk 1 . Workers were asked to generate reference timeline for news at each epoch in less than 50 words and we collect 790 timelines in total.
extraction	MULTIR 4	For evaluating extraction accuracy, we follow the experimental setup of, and use their implementation of MULTIR 4 with 50 training iterations as our baseline.
ASR	NIST	In the ASR setting, which simulates a realworld deployment scenario, this system achieves improvements of 0.39 (BLEU), -0.6 (TER) and 0.08 (NIST).
synonym choice	German version of the Reader's Digest WordPower dataset	The second task is synonym choice on the German version of the Reader's Digest WordPower dataset ().
NER	McNemar's test	 Table 1: NER performance using different word clustering models. Bold indicates an improvement over  the monolingual (β = 0) baseline;  † indicates a significant improvement (McNemar's test, p < 0.01).
image caption generalization	Amazon Mechanical Turk (AMT)	Since there is no existing benchmark data for image caption generalization, we crowdsource evaluation using Amazon Mechanical Turk (AMT).
Predicting High-Quality Answers	cQA portals	Multimodal DBN for Predicting High-Quality Answers in cQA portals
parsing	MST	In this section we give a detailed analysis of parsing into SB, CD and DT dependencies with Malt, MST and the Bohnet and Nivre (2012) parser.
Narrative Generation	Satellite Tag Data	Tag2Blog: Narrative Generation from Satellite Tag Data
predicting	RST Treebank	The resulting shift-reduce discourse parser obtains substantial improvements over the previous state-of-the-art in predicting relations and nuclearity on the RST Treebank.
POS taggers	Penn Treebank (PTB)	However, state-of-the-art POS taggers in the literature are mainly optimized on the the Penn Treebank (PTB), and when shifted to web data, tagging accuracies drop significantly ().
NELL	HarvardUniv	It is also worth noting that most of education mentions that NELL fails to retrieve are those involve irregular spellings, such as HarvardUniv and Cornell U, which means Recall score for NELL baseline would be even higher if these irregular spellings are recognized in a more sophisticated system.
NELL	Cornell U	It is also worth noting that most of education mentions that NELL fails to retrieve are those involve irregular spellings, such as HarvardUniv and Cornell U, which means Recall score for NELL baseline would be even higher if these irregular spellings are recognized in a more sophisticated system.
parsing	CATiB dataset	 Table 3: Results for parsing and corrective tagging  on the CATiB dataset. The upper part shows UAS  of our model with gold/predicted information or  POS correction. Bottom part shows UAS of the  best systems in the SPMRL shared task. IMS- Single (Björkelund et al., 2013) is the best single  parsing system, while IMS-Ensemble (Björkelund  et al., 2013) is the best ensemble parsing system.  We also show results for CADIM (Marton et al.,  2013), the second best system, because we use  their predicted features.
parsing problem	PCFG	Some aspects of the parsing problem, such as the tree constraint, are clearly best captured by a PCFG.
WSD	MKWC	 Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
WSD	WordNet-annotated datasets	 Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
WSD	Macmillan-annotated datasets	 Table 4: WSD accuracy for HDP-WSI on the  Macmillan-annotated datasets, as compared to the  upper-bound based on actual first sense in the cor- pus (higher values indicate better performance; the  best system in each row [other than the FS CORPUS  upper bound] is indicated in boldface).
WSD	FS CORPUS	 Table 4: WSD accuracy for HDP-WSI on the  Macmillan-annotated datasets, as compared to the  upper-bound based on actual first sense in the cor- pus (higher values indicate better performance; the  best system in each row [other than the FS CORPUS  upper bound] is indicated in boldface).
translation	PWKP corpus	We trained our simplification and translation models on the PWKP corpus.
parsing	RST-DT	 Table 3: The parsing time (in seconds) for the 38  documents in the test set of RST-DT. Time cost of  any pre-processing is excluded from the analysis.
negation focus annotation	PropBank	In particular, negation focus annotation on this corpus is restricted to verbal negations (with corresponding mark MNEG in PropBank).
relation classification task	ACE RDC 2005 RDC Chinese and English corpora	We have systematically evaluated our BAL paradigm on the relation classification task using ACE RDC 2005 RDC Chinese and English corpora.
relation classification	ACE2005e	Comparison of overall deficiency compares the deficiency scores of relation classification on the Chinese (ACE2005c) and English corpora (ACE2005e) for various learning methods, i.e., SL-CR, AL-MO, AL-CR and AL-BI.
POS tagging	Wall Street Journal (WSJ)	To evaluate DA for POS tagging, following, we use sections 2 − 21 from Wall Street Journal (WSJ) as the source domain labeled data.
sentiment classification	Amazon product reviews collected	To evaluate DA for sentiment classification, we use the Amazon product reviews collected by for four different product categories: books (B), DVDs (D), electronic items (E), and kitchen appliances (K).
POS tagging	WSJ	In cross-domain POS tagging, WSJ is always the source domain, whereas the five domains in SANCL dataset are considered as the target domains.
POS tagging	SANCL dataset	In cross-domain POS tagging, WSJ is always the source domain, whereas the five domains in SANCL dataset are considered as the target domains.
sentiment prediction	movie reviews dataset	 Table 1: Accuracy of sentiment prediction in the  movie reviews dataset. The first four results are  reported from Socher et al. (2013b). The baselines  NB and BINB are Naive Bayes classifiers with,  respectively, unigram features and unigram and bi- gram features. SVM is a support vector machine  with unigram and bigram features. RECNTN is a  recursive neural network with a tensor-based fea- ture function, which relies on external structural  features given by a parse tree and performs best  among the RecNNs.
MT evaluation	WMT11	We first design two discourse-aware similarity measures, which use DTs generated by a publiclyavailable discourse parser (; then, we show that they can help improve a number of MT evaluation metrics at the segment-and at the system-level in the context of the WMT11 and the WMT12 metrics shared tasks).
MT evaluation	WMT12 metrics shared tasks	We first design two discourse-aware similarity measures, which use DTs generated by a publiclyavailable discourse parser (; then, we show that they can help improve a number of MT evaluation metrics at the segment-and at the system-level in the context of the WMT11 and the WMT12 metrics shared tasks).
parsing	CTB 5.1	After integrating semi-supervised word cluster features, the parsing accuracy is further improved to 86.3% when trained on CTB 5.1 and 87.1% when trained on CTB 6.0, and this is the best reported performance for Chinese.
parsing	CTB 6.0	After integrating semi-supervised word cluster features, the parsing accuracy is further improved to 86.3% when trained on CTB 5.1 and 87.1% when trained on CTB 6.0, and this is the best reported performance for Chinese.
Parsing	Chinese development set	We built two new systems to verify the effectiveness of our state alignment strategy proposed in: Parsing performance on Chinese development set.
Parsing	Chinese devel- opment set	 Table 3: Parsing performance on Chinese devel- opment set.
MWE recognition	French dataset of the SPMRL 2013 Shared Task	While the realistic scenario of syntactic parsing with automatic MWE recognition (either done jointly or in a pipeline) has already been investigated in constituency parsing, the French dataset of the SPMRL 2013 Shared Task (  only recently provided the opportunity to evaluate this scenario within the framework of dependency syntax.
constituency parsing	French dataset of the SPMRL 2013 Shared Task	While the realistic scenario of syntactic parsing with automatic MWE recognition (either done jointly or in a pipeline) has already been investigated in constituency parsing, the French dataset of the SPMRL 2013 Shared Task (  only recently provided the opportunity to evaluate this scenario within the framework of dependency syntax.
SMTbased	CoNLL 2013 shared task	The other was the SMTbased method () which was the best-performing system in preposition error correction in the CoNLL 2013 shared task ( ).
word alignment	IBM models	One is language model domain adaptation, and the other is word alignment using the IBM models).
sentiment analysis	Stanford sentiment treebank	Our sentiment analysis datasets consist of movie reviews from the Stanford sentiment treebank, and floor speeches by U.S. Congressmen alongside "yea"/"nay" votes on the bill under discussion ().
LDA learning	Reuters	shows computational time and memory size for LDA learning on the original corpus, (1/10)-reduced corpus, and (1/20)-reduced corpus of Reuters.
LDA learning	Reuters	 Table 2: Computational time and memory size  for LDA learning on the original corpus, (1/10)- reduced corpus, and (1/20)-reduced corpus of  Reuters.  corpus  time  memory perplexity  original 4m3.80s 71,548KB  500  (1/10) 3m55.70s 46,648KB  550  (1/20) 3m42.63s 34,024KB  611
translation	FBIS corpus	The bilingual training data for translation model and CSS-based transfer model is FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words.
CSS-based transfer	FBIS corpus	The bilingual training data for translation model and CSS-based transfer model is FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words.
SMT	SCFG decoder CDEC	The bilingual SMT system used in our experiments is the state-of-the-art SCFG decoder CDEC . We built grammars using its implementation of the suffix array extraction method described in.
SMT	COMMON CRAWL) dataset	We trained the SMT system on the English-German parallel web data provided in the COMMON CRAWL) dataset.
Answering natural language questions	Freebase knowledge base	Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing.
Objectivity Classifier	Mechanical Turk study	(3) Objectivity Classifier: Using labeled data from the Mechanical Turk study, we developed and trained an objectivity classifier which performed better than prior proposed lexicons from literature.
SRL	CoNLL'09	 Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
SRL	CoNLL 2005 span	 Table 5: F1 for SRL approaches (without sense  disambiguation) in matched and mismatched  train/test settings for CoNLL 2005 span and 2008  head supervision. We contrast low-resource ()  and high-resource settings ( ), where latter uses a  treebank. See  § 4.4 for caveats to this comparison.
projection learning	CilinE (Section 3.3.3)	The training data for projection learning is collected from CilinE (Section 3.3.3).
MT	Chinese treebank (CTB)	The practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the handannotated treebank data, e.g., Chinese treebank (CTB) ().
Translation	MT-05  testing data	 Table 1: Translation performances (%) on MT-05  testing data by using ten different CWS models.
MT	NIST OpenMT12	We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions.
MT	DARPA	We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions.
synonym extraction	TS68	 Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
relation identification	AMR Bank annotators	Using gold concepts with the relation identification stage yields a much higher Smatch score of 80% F 1 . As a comparison, AMR Bank annotators have a consensus inter-annotator agreement Smatch score of 83% F 1 . The runtime of our system is given in.
resolution	AdaGrad	For resolution, we set the learning rate to 0.25 and ran AdaGrad for 5 iterations.
Frame identification	FrameNet	 Table 2: Frame identification results for FrameNet. See  §5.6.
Frame identification	FrameNet	 Table 2: Frame identification results for FrameNet. See  §5.6.
WSD	HowNet	As they apply WSD to Chinese-to-English translation, they predict word senses from a Chinese ontology HowNet and project the predicted senses to English glosses provided by HowNet.
WSD	HowNet	As they apply WSD to Chinese-to-English translation, they predict word senses from a Chinese ontology HowNet and project the predicted senses to English glosses provided by HowNet.
alignment tasks	BT EC	Note that the development data was not used in the alignment tasks, i.e., BT EC and Hansards, because the hyperparameters of the alignment models were set by preliminary small-scale experiments.
alignment tasks	Hansards	Note that the development data was not used in the alignment tasks, i.e., BT EC and Hansards, because the hyperparameters of the alignment models were set by preliminary small-scale experiments.
word naming	CELEX	Specifically, we predict human data from three widely used psycholinguistic experimental paradigms-lexical decision, word naming, and picture naming-using unigram frequency estimates from Google n-grams (),), spoken and written English portions of CELEX (, and spoken and written portions of the British National Corpus).
word naming	British National Corpus	Specifically, we predict human data from three widely used psycholinguistic experimental paradigms-lexical decision, word naming, and picture naming-using unigram frequency estimates from Google n-grams (),), spoken and written English portions of CELEX (, and spoken and written portions of the British National Corpus).
coreference-The	OntoNotes corpus	The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference-The OntoNotes corpus) in the general domain, as well as the i2b2 () and THYME () corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative.
coreference resolution	OntoNotes corpus	The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them.
SVM training	FTK	For SVM training, the parameter C is set to 2.4 for all experiments, and the tree kernel parameter λ is tuned to 0.2 for FTK and 0.4 (the optimal parameter setting used in) for CTK.
Morphological segmentation	MeCab (MeCab, 2011)	Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (), respectively.
classification	YamCha	For classification, we used the SVM implementation in YamCha (, and trained with different variations of the features described above.
MLE	GSEC	To train the MLE model, we use the word pairs obtained from the original training data, rather than from the output of GSEC.
dual decomposition	SIGHAN  2003 and 2005 datasets	 Table 2: Performance of dual decomposition in  comparison to past published results on SIGHAN  2003 and 2005 datasets. Best reported F 1 score  for each dataset is highlighted in bold. Z&C 07  refers to
Document classification	RCV1/2  corpus	 Table 2: Document classification accuracy when  trained on 1,000 training examples of the RCV1/2  corpus (train→test). Baselines are the majority  class, glossed, and MT (Klementiev et al., 2012).  Further, we are comparing to Klementiev et al.  (2012), BiCVM ADD (
Sentence Extraction	NTCIR-3 test data	 Table 1: Sentence Extraction (NTCIR-3 test data)
information extraction	FactBank corpus	Detecting and reasoning about the certainty of propositional content has been identified as a key task for information extraction, and is now supported by the FactBank corpus of annotations for newstext.
Identifying Real-Life Complex Task Names	Microblogs	Identifying Real-Life Complex Task Names with Task-Intrinsic Enti- ties from Microblogs
DT	BM25	Patent retrieval for DT was done by sentencewise translation and subsequent re-joining to form one query per patent, which was ranked against the documents using BM25.
WSD	SemEval-2007 dataset	 Table 2: Lexical knowledge sources and WSD performance (F-measure) on the Senseval-2 (fine-and  coarse-grained) and the SemEval-2007 dataset.
LM	NYT data	 Table 2: LM evaluation on held out NYT data.
MT development	NEUCSS	For comparison, we also manually annotated the MT development and test data with skeleton information according to the annotation standard provided within NEUCSS.
correction detection	FCE corpus	Their correction detection algorithm relies on a set of heuristics developed from one single data collection (the FCE corpus).
parsing algo- rithms	Penn2Malt	 Table 1: LAS results with several parsing algo- rithms, Penn2Malt conversion ( †: p <0.05,  ‡: p  <0.005). In parenthesis, difference with baseline.
parsing	Penn-S-2.0.5	indicates that simply enlarging the beam -relative to parsing speed -does not recover the wins of structural diversity on Penn-S-2.0.5 and CTB-5, though it does reduce the gap on Penn-S-2.0.5.
parsing	CTB-5	indicates that simply enlarging the beam -relative to parsing speed -does not recover the wins of structural diversity on Penn-S-2.0.5 and CTB-5, though it does reduce the gap on Penn-S-2.0.5.
parsing	Penn-S-2.0.5	indicates that simply enlarging the beam -relative to parsing speed -does not recover the wins of structural diversity on Penn-S-2.0.5 and CTB-5, though it does reduce the gap on Penn-S-2.0.5.
parsing	PTB	Our goal was to determine whether the parsing results fell in the same general range as for the PTB by roughly compensating for the difference in annotation style.
Dispute detection	Wikipedia Talk pages	 Table 4: Dispute detection results on Wikipedia Talk pages.
generative	BTEC data	proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data.
MT	Spanish/English OPUS corpus	 Table 5: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours) on  the Spanish/English OPUS corpus using only non-parallel corpora for training.
MT	NIST MTEval	We work with eight standard MT test sets: three MSA sets from NIST MTEval with four references (MT06, MT08, and MT09), four Egyptian sets from LDC BOLT data with two references (EgyDevV1, EgyDevV2, EgyDevV3, and EgyTestV2), and one Levantine set from BBN () with one reference which we split into LevDev and LevTest.
MT	MT06	We work with eight standard MT test sets: three MSA sets from NIST MTEval with four references (MT06, MT08, and MT09), four Egyptian sets from LDC BOLT data with two references (EgyDevV1, EgyDevV2, EgyDevV3, and EgyTestV2), and one Levantine set from BBN () with one reference which we split into LevDev and LevTest.
MT	LDC BOLT data	We work with eight standard MT test sets: three MSA sets from NIST MTEval with four references (MT06, MT08, and MT09), four Egyptian sets from LDC BOLT data with two references (EgyDevV1, EgyDevV2, EgyDevV3, and EgyTestV2), and one Levantine set from BBN () with one reference which we split into LevDev and LevTest.
MT	BBN	We work with eight standard MT test sets: three MSA sets from NIST MTEval with four references (MT06, MT08, and MT09), four Egyptian sets from LDC BOLT data with two references (EgyDevV1, EgyDevV2, EgyDevV3, and EgyTestV2), and one Levantine set from BBN () with one reference which we split into LevDev and LevTest.
MT	LevDev	We work with eight standard MT test sets: three MSA sets from NIST MTEval with four references (MT06, MT08, and MT09), four Egyptian sets from LDC BOLT data with two references (EgyDevV1, EgyDevV2, EgyDevV3, and EgyTestV2), and one Levantine set from BBN () with one reference which we split into LevDev and LevTest.
MT	LevTest	We work with eight standard MT test sets: three MSA sets from NIST MTEval with four references (MT06, MT08, and MT09), four Egyptian sets from LDC BOLT data with two references (EgyDevV1, EgyDevV2, EgyDevV3, and EgyTestV2), and one Levantine set from BBN () with one reference which we split into LevDev and LevTest.
parsing	CBOW	For parsing experiments, we choose w = 2 for CBOW and w = 1 for SKIP.
parsing	SKIP	For parsing experiments, we choose w = 2 for CBOW and w = 1 for SKIP.
Tagging	English Penn Treebank	 Table 2: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k  train describe models trained on the subset of 973k tokens used by
summarizing	Rouge-1	The performance of the summarizing system is measured with Rouge-1 Recall, Rouge-1 Precision and F1 measure).
Sentence recall	DUC-2002	Sentence recall and sentence precision are defined as follows: where M is the number of the sentences included  As shown, the proposed system performs better than the best systems of DUC-2002 in terms of sentence recall.
clustering Web search results	Carrot	Another solution consists of clustering Web search results byway of clustering engines such as Carrot and Yippy and presenting them to the user grouped by topic.
clustering Web search results	Yippy	Another solution consists of clustering Web search results byway of clustering engines such as Carrot and Yippy and presenting them to the user grouped by topic.
SMT	BBN.	The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated * * Research conducted when the author was at BBN. by phrase-based or hierarchical translation rules.
SMT	BBN.	The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated * * Research conducted when the author was at BBN. by phrase-based or hierarchical translation rules.
MT	DARPA BOLT Web Forum	We demonstrate the impact of our work with extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions.
MT	NIST OpenMT12 conditions	We demonstrate the impact of our work with extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions.
SENSEMBED	RG-65	We evaluate SENSEMBED on standard word similarity and relatedness datasets: the RG-65 and the WordSim-353 (, WS-353) datasets.
SENSEMBED	WordSim-353	We evaluate SENSEMBED on standard word similarity and relatedness datasets: the RG-65 and the WordSim-353 (, WS-353) datasets.
SENSEMBED	WS-353) datasets	We evaluate SENSEMBED on standard word similarity and relatedness datasets: the RG-65 and the WordSim-353 (, WS-353) datasets.
negation heuristic	MCTest dataset	We can observe that the negation heuristic also helps, especially for "single" questions (majority of negation cases in the MCTest dataset are for the "single" questions).
Question retrieval	cQA archives	Question retrieval in cQA archives aims to find the existing questions that are semantically equivalent or relevant to the queried questions.
question retrieval	cQA	One big challenge for question retrieval in cQA is the lexical gap between the queried questions and the existing questions in the archives.
parsing	MSTParser	With regard to parsing speed, 1-order-atomic is the fastest while other two models have similar speeds as MSTParser.
IE extractions	Ollie system	Although this is effective at capturing many of these patterns, it visits; the US): Open IE extractions produced by the system, alongside extractions from the stateof-the-art Ollie system.
coreference	UW	For coreference, UW uses the Stanford coreference system (Lee et al., 2011); we employ a variant of the simple coref system described in).
SRL	MATE-SRL	To assess the impact of English SRL quality, we used two different English SRL systems: CLEARNLP and MATE-SRL.
opinion mining	MPQA English corpus	Ina first set of experiments, we evaluated the performance of our dependency-based model for opinion mining ( §3) in the MPQA English corpus.
classification	Boolean	 Table 1: The classification accuracy with the  Boolean and TF-IDF methods.
Sentence planner	Treex NLP toolkit	Sentence planner is based on A* search with a perceptron ranker that uses novel differing subtree updates and a simple future promise estimation; surface realization uses a rule-based pipeline from the Treex NLP toolkit.
Disfluency detection	English Switchboard data	 Table 2: Disfluency detection and parsing accuracies on English Switchboard data. The accuracy of  M 3 N refers to the result reported in (Qian and Liu, 2013). H&J is the L2R parsing based joint model in  (Honnibal and Johnson, 2014). The results of M 3 N  † come from the experiments with toolkit released by  Qian and Liu (2013) on our pre-processed corpus.
parsing	English Switchboard data	 Table 2: Disfluency detection and parsing accuracies on English Switchboard data. The accuracy of  M 3 N refers to the result reported in (Qian and Liu, 2013). H&J is the L2R parsing based joint model in  (Honnibal and Johnson, 2014). The results of M 3 N  † come from the experiments with toolkit released by  Qian and Liu (2013) on our pre-processed corpus.
Disfluency detection	Chinese annotated data	 Table 5: Disfluency detection performance on  Chinese annotated data.
NEs	Wikipedia annotated  datasets	 Table 4: Results for NEs on Wikipedia annotated  datasets.
SMT	IAA statistics	Section 2 contains an overview of some of the latest research in both GEC and SMT that makes use of IAA statistics.
SMT	JRC-Acquis	For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.0  We additionally compare our results to an SMT system built on an existing domain-specific parallel dataset, i.e. EMEA 12, which holds specific medical parallel data extracted from the European Medicines Agency documents and websites.
SMT	EMEA 12	For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.0  We additionally compare our results to an SMT system built on an existing domain-specific parallel dataset, i.e. EMEA 12, which holds specific medical parallel data extracted from the European Medicines Agency documents and websites.
SMT	EMEA 12	For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.0  We additionally compare our results to an SMT system built on an existing domain-specific parallel dataset, i.e. EMEA 12, which holds specific medical parallel data extracted from the European Medicines Agency documents and websites.
Lexical enhancement	IATE	Lexical enhancement for SMT Since the outof-vocabulary problem can be only mitigated with sentence selection, we accessed lexical resources IATE and DBpedia to further improve the translations of the medical labels.
SMT	IATE	Lexical enhancement for SMT Since the outof-vocabulary problem can be only mitigated with sentence selection, we accessed lexical resources IATE and DBpedia to further improve the translations of the medical labels.
SBTs	RCV1 Reuters corpus of newswire text	We evaluate SBTs on two data sets, the RCV1 Reuters corpus of newswire text (), and a distinct data set of Wikipedia links, WPL.
SBTs	WPL	We evaluate SBTs on two data sets, the RCV1 Reuters corpus of newswire text (), and a distinct data set of Wikipedia links, WPL.
speech recognition	Wall Street Journal data	We perform speech recognition experiments on Wall Street Journal data, and find that our TDRF models lead to performances as good as the recurrent neural network LMs but are computationally more efficient in computing sentence probability.
MT evaluation	NIST	Finally, we empirically show that syntacticallyand semantically-oriented embeddings can be incorporated to produce sizeable and cumulative gains in performance over a strong combination of pre-existing MT evaluation measures (BLEU, NIST, METEOR, and TER).
solving	WordNet	Moreover, a good solving method has to account for typically scarce labeled training data, by enriching its model with lexical semantic resources like WordNet to bridge gaps between surface forms (.
AMR parsing	LDC2014T12 datasets	We improve on the previous state-of-the-art result for AMR parsing , boosting end-to-end performance by 3 F 1 on both the LDC2013E117 and LDC2014T12 datasets.
relation identification	SRL++	The second step is relation identification, which adds arcs to link these nodes into a fully connected AMR graph, which we'll call SRL++ (Section 3.2).
sentiment classification	Stanford Sentiment Treebank	Existing benchmark datasets for sentiment classification such as Stanford Sentiment Treebank () typically only have text information, but do not contain users who express the sentiment or products which are evaluated.
tokenization	Stanford CoreNLP	We split each corpus into training, development and testing sets with a 80/10/10 split, and conduct tokenization and sentence splitting with Stanford CoreNLP ( ).
sentiment classifi- cation	IMDB dataset	 Table 1: Statistical information of IMDB, Yelp 2014 and Yelp 2013 datasets used for sentiment classifi- cation. The rating scale of IMDB dataset is 1-10. The rating scale of Yelp 2014 and Yelp 2013 datasets is  1-5. |V | is the vocabulary size of words in each dataset. #users is the number of users, #docs/user means  the average number of documents per user posts in the corpus.
sentiment classifi- cation	Yelp 2013 datasets	 Table 1: Statistical information of IMDB, Yelp 2014 and Yelp 2013 datasets used for sentiment classifi- cation. The rating scale of IMDB dataset is 1-10. The rating scale of Yelp 2014 and Yelp 2013 datasets is  1-5. |V | is the vocabulary size of words in each dataset. #users is the number of users, #docs/user means  the average number of documents per user posts in the corpus.
tokenization	Stanford  CoreNLP	 Table 1.  We split each corpus into training, development  and testing sets with a 80/10/10 split, and conduct  tokenization and sentence splitting with Stanford  CoreNLP (
Sentiment classification	IMDB	 Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
Sentiment classification	Yelp 2014 and Yelp 2013 datasets	 Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
parsing of discontinuous constituents	CFG	Direct parsing of discontinuous constituents can be done with Linear Context-Free Rewriting System (LCFRS), an extension of CFG which allows its non-terminals to cover more than one continuous block.
detecting genericity	ACE corpora	In computational linguistics, most research on detecting genericity has been done in relation to the ACE corpora (), focusing on assigning genericity labels to noun phrases (, see Section 2.
parsing	WSJ Sections 02-21	For our experiments we will follow the standard practice in supervised parsing of using WSJ Sections 02-21 for training, Section 22 for development and error analysis, and a final evaluation of the best models on Section 23.
classification	20 Newsgroups dataset	shows the classification accuracy of those three models on the larger datasets, i.e., the 20 Newsgroups dataset, and the Reuters-21578 dataset.
classification	Reuters-21578 dataset	shows the classification accuracy of those three models on the larger datasets, i.e., the 20 Newsgroups dataset, and the Reuters-21578 dataset.
Spearman	WordSim-353  Task	 Table 1: Spearman results on the WordSim-353  Task.
predicting the semantic relatedness of two sentences	Stanford Sentiment Treebank)	Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).
sentiment classification	Stanford Sentiment Treebank)	Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).
summarization task	TAC 2010	The data set of traditional summarization task in TAC 2010 is employed as the development/tuning data set.
summarization evaluation	TAC	Therefore, in recent summarization evaluation workshops such as TAC, the pyramid is used as the major metric.
summarization task	TAC 2010 data	 Table 3. Our performance is  slightly better than System 22, and it is not as good  as System 43 and 17. The reason is that System 43  and 17 used category-specific features and trained  the feature weights with the category information in TAC 2010 data. These features help them se- lect better category-specific content for the sum- mary. However, the usability of such features de- pends on the availability of predefined categories  in the summarization task, as well as the avail- ability of training data with the same predefined  categories for estimating feature weights. There- fore, the adaptability of these methods is limited to  some extent. In contrast, our framework does not  define any category-specific feature and only uses  TAC 2010 data to tune the parameters for general  summarization purpose.
POS tagging	CTB	Experiments show our approach can significantly improve POS tagging accuracy from 94.10% to 95.00% on CTB.
Sample word pairs	Farsi RG-65 datasets	 Table 1: Sample word pairs from the English and the newly created Spanish and Farsi RG-65 datasets.
Coreference resolution	CoNLL shared task 2012	Coreference resolution has been extensively addressed in NLP research, e.g. in the CoNLL shared task 2012 ( or in the SemEval shared task.
SemEval Shared Task	TüBa 8)	 Table 1: SemEval Shared Task 2010 post-task  evaluation for track regular (on TüBa 8), includ- ing and excluding singletons
RSMs	IMDB	We evaluated the new estimator to train RSMs on two text datasets: 20 Newsgroups and IMDB.
sentiment analysis	IMDB	The IMDB dataset 3 is a benchmark for sentiment analysis, which consists of 100,000 movie reviews taken from IMDB.
retrieval task	20 Newsgroups dataset	Figure 2: Precision-Recall curves for the retrieval task on the 20 Newsgroups dataset using RSMs.
sentiment classification	BoW-based baselines	The error rate of sentiment classification on the testing set is reported, compared with several BoW-based baselines.
RSM	BoW	It is clear that α-NCE learns RSM better than CD, and outperforms BoW and other BoW-based models 4 such as LDA.
RSM	BoW	Note that RSM is also based on BoW, indicating α-NCE has arguably reached the limits of learning BoW-based models.
classification	20 Newsgroups dataset	 Table 1: Comparison of classification accuracy on  the 20 Newsgroups dataset using RSMs.
sentiment and question classification tasks	TREC	Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC.
parsing	CCGBank	In this paper, we show how directly capturing sequence information using a recurrent neural network leads to further accuracy improvements for both su-pertagging (up to 1.9%) and parsing (up to 1% F1), on CCGBank, Wikipedia and biomedical text.
tagging	CCGBank Section 00	 Table 1: 1-best tagging accuracy and speed com- parison on CCGBank Section 00 with a single  CPU core (1,913 sentences), tagging time in secs.
Text Comprehension Task	MC500 dataset	Results on Text Comprehension Task We report results (in percentage of correct answers) on the whole of MC500 dataset (ignoring train-devtest split) since all our methods are unsupervised.
Mathematical information retrieval (MIR)	NTCIR Math IR test collection	Recent interest in Mathematical information retrieval (MIR) has prompted the construction of the NTCIR Math IR test collection (.
Neural reordering	NPLM	Neural reordering models were trained by the toolkit NPLM (.
PAM	Sina Microblog 1	To learn PAM, we manually select 40 users in each domain from the official expert lists released by Sina Microblog 1 , and crawl all of their posts.
Answer Selection	cQA	We trained and evaluated our models on data from SemEval-2015 Task 3 on Answer Selection in cQA.
AMR parser	JAMR	The first published AMR parser, JAMR), performs AMR parsing in two stages: concept identification and relation identification.
AMR parsing	LDC2014T12 test set	 Table 5: AMR parsing performance on newswire  section of LDC2014T12 test set
tagging	BMMM	In terms of tagging performance, we can see that the two hard clustering systems significantly outperform the HMM, but the relative performance of Brown and BMMM is mixed.
dependency parsing	Persian treebank	The main contributions of this paper are: 1) showing the usefulness of Ezafe construction on dependency parsing and chunking, 2) developing a statistical chunker for the Persian language, 3) enriching the Persian treebank with manual Ezafe tags.
parsing	Persian dependency treebank	In this section we describe our experiments on Ezafe tagging, parsing and also adding manual Ezafe tags to the Persian dependency treebank.
Chunking	Persian depen- dency treebank test data	 Table 3: Chunking results on the Persian depen- dency treebank test data with automatic POS tags.
SMT	XY /Y coreference	However, the character by itself could also be translated as 'shoe' or 'footwear', as observed with a baseline SMT system that is not aware of the XY /Y coreference.
AMR	PropBank (PB))	AMR relies heavily on predicate-argument structures defined in the PropBank (PB)).
coreference resolution	MUC score	Based on the observation that representations in entity linking (mentions linked to the same KB entry) are very similar to those encountered in coreference resolution (mentions linked by coreference relations to the same entity), these metrics include ones originally proposed for evaluation of coreference resolutions systems, such as the MUC score (, B 3 (Bagga and, and CEAF () and variants thereof.
semantic role labeling (SRL)	Chinese Treebank	A dual-layer semantic role labeling (SRL) system is built using Chinese Treebank and Propbank data.
semantic role labeling (SRL)	Propbank data	A dual-layer semantic role labeling (SRL) system is built using Chinese Treebank and Propbank data.
POS tagging	CTB5.1	Specifically, we trained and tested word segmentation, POS tagging, chunking, and constituent parsing on CTB5.1: articles 001-270 and 440-1151 were used for training and articles 271-300 were used for testing.
parsing	LUNAR [WOO?3	Some techniques similar to ours have been used for parsing, see for example the conjunction mechanism in LUNAR [WOO?3).
Machine Translation (MT)	ALPAC report	Although funding for Machine Translation (MT) research virtua11y ended in the U.S. with the release of the ALPAC report in 1966, there has been a continuing interest in this field.
parsing	ovendl	Depending on the relative computational efficiency of parsing versus interpretation operations, dynamic intcrlc:ning might increase or decrease ovendl system efli:'ctivcness.
semantic analysis	PHLIQAI program	In order to deal with this problem in a systematic way, different levels of semantic analysis are distinguished in the PHLIQAI program.
Language Planning Douglas E.	SRI International	Providing A UniSed Account of De/Inite Noun Phra~s in Discourse Generation Telegram: A Grammar Formalism for Language Planning Douglas E. Appelt, SRI International
semantic analysis	TENDUM dialogue system	This paper shows a form for such a representation and how "ambiguous" representations are used in an elegant and efficient procedure for semantic analysis, incorporated in the TENDUM dialogue system.
MT	ALPAC	Paradoxically, MT systems were still being used by various government agencies here and abroad, because there was simply no alternative means of gathering information from foreign sources so quickly; in addition, private companies were developing and selling MT sysEoms based on the mid-60"s technology so roundly castigated by ALPAC.
knowledge transfer	Champs/gn-Urbana	IIqKA is not intended to standalone as the sole mechanism for knowledge transfer, but to be supt A summer intern at Tektronix, currently at the University of llfinois, Champs/gn-Urbana.
Segmenting	TNT Introduction	 Table 1. Segmenting the TNT Introduction
1 Direction Assistance	Thinking Maf_~ines Corporation of Cambridge	1 Direction Assistance was originally developed by Jim Davis and Tom Trobaugh in 1985 at the Thinking Maf_~ines Corporation of Cambridge.
parse	CFGs	We show how with the use of substitution in TAGs the system is able to parse directly CFGs and TAGs.
graph unification	TASLINK natural language system	This performance drain is illustrated in Figure 1, where average parsing statistics are given for the original implementation of graph unification in the TASLINK natural language system.
Parsing and Generating with Unification Grammars	ELU	Asymmetry in Parsing and Generating with Unification Grammars: Case Studies From ELU
sort inheritance and unification	HPSG	We apply our logical analysis to the sort inheritance and unification system of HPSG and also to classification in systemic choice systems.
plan recognition problem	US WEST Advanced	In contrast, presented a theoretical formalization of the plan recognition problem, *This research has been supported by US WEST Advanced Technologies and by a Bellcore Graduate Fellowship. and a corresponding algorithm, in which the only conclusions that are drawn are those that are "absolutely justified."
plan recognition problem	Bellcore Graduate Fellowship.	In contrast, presented a theoretical formalization of the plan recognition problem, *This research has been supported by US WEST Advanced Technologies and by a Bellcore Graduate Fellowship. and a corresponding algorithm, in which the only conclusions that are drawn are those that are "absolutely justified."
parsing	Reape's version of HPSG	Although I will not impose any restrictions on the head, it will turnout that the parsing strategy to be proposed will be very sensitive to the choice of heads, with the effect that F-LCFRS's in which the notion 'head' is defined in a systematic way (Pollard's Head Grammars, Reape's version of HPSG, Dowty's version of Categorial Grammar), maybe much more efficiently parsed than other grammars.
predicting the location of intonational phrase boundaries in natural speech	DARPA Air Travel Information Service database	This work investigates the use of text analysis in predicting the location of intonational phrase boundaries in natural speech, through analyzing 298 utterances from the DARPA Air Travel Information Service database.
parsing	Associated Press news wire	In an effort to gauge the state of the art in parsing, the authors conducted an experiment in Summer 1990 in which 35 sentences, all of length 13 words or less, were selected randomly from a several-millionword corpus of Associated Press news wire.
Parsing Free Word Order Languages	Paninian Framework	Parsing Free Word Order Languages in the Paninian Framework
AUTOMATIC ACQUISITION	CORPORA	AUTOMATIC ACQUISITION OF A LARGE SUBCATEGORIZATION DICTIONARY FROM CORPORA
morphological analysis	British Telecom Scholarship	The difficulties in morphological analysis and error detection in Semitic arise from the following facts: * Supported by a British Telecom Scholarship, administered by the Cambridge Commonwealth Trust in conjunction with the Foreign sad Commonwealth Office.
morphological analysis	Cambridge Commonwealth Trust	The difficulties in morphological analysis and error detection in Semitic arise from the following facts: * Supported by a British Telecom Scholarship, administered by the Cambridge Commonwealth Trust in conjunction with the Foreign sad Commonwealth Office.
TAG analyses	HPSG	Our answers, while consistent with basic tenets of traditional TAG analyses, are general enough to allow an alternate linguistic theory, such as HPSG, to be used as a basis for deriving a TAG.
segmentation	Bellcore	In this paper, we discuss two methods for developing segmentation algorithms using multiple know-*Bellcore did not support the second author's work.
Utilizing Statistical Dialogue Act Processing	Verbmobil	Utilizing Statistical Dialogue Act Processing in Verbmobil
dialogue act processing	VERBMOBIL	In this paper, we present a statistical approach for dialogue act processing in the dialogue component of the speech-to-speech translation system VERBMOBIL.
parsing	CFG	Bernard Lang defines parsing as ~ calculation of the intersection of a FSA (the input) and a CFG.
FST representation of the parses of sentences	ATIS	The first set of experiments, (Experiments l(a) through 1(c)), are intended to measure the coverage of the FST representation of the parses of sentences from a range of corpora (ATIS, IBM-Manual and Alvey).
FST representation of the parses of sentences	IBM-Manual	The first set of experiments, (Experiments l(a) through 1(c)), are intended to measure the coverage of the FST representation of the parses of sentences from a range of corpora (ATIS, IBM-Manual and Alvey).
WSD	WOttDNET	To our knowledge, this is the first time that a WSD program has been tested on such a large scale, and yielding results better than the most frequent heuristic on highly ambiguous words with the refined sense distinctions of WOttDNET.
segmentation	Chinese LM	Therefore, segmentation is a key issue in building the Chinese LM.
segmentation	Chinese LM	Therefore, segmentation is a key issue in building the Chinese LM.
Segmentation	Accuracy  ORG  P1  P2  ORG  P1  85.9  P2  79.1  90.9  P3  87.4  85.7 82.2	 Table 2: Segmentation Accuracy  ORG  P1  P2  ORG  P1  85.9  P2  79.1  90.9  P3  87.4  85.7 82.2
chunking	Old" scheme	The experiment was carried out using both the chunking criteria from  (the "Old" scheme), and the chunking criteria described in Section 3 above (the "New" scheme).
Structural Disambiguation	TRAINS	Using Parsed Corpora for Structural Disambiguation in the TRAINS Domain
Distribution of Anaphors	Text Corpus	 Table 2: Distribution of Anaphors in the Text Corpus
CFG parsing	CFG	The standard methods for CFG parsing are the CKY algorithm and Earley's algorithm, both of which have a worst-case running time of O(gN 3) fora CFG (in Chomsky normal form) of size g and a string of length N. give a variant of Earley's algorithm which runs in time O(gN3/log N).
syntactic analysis	Penn Tree-bank	This approach has been successfully used for syntactic analysis , using corpora with syntactic annotations such as the Penn Tree-bank.
referential description	Donellan	Over the last decade,, and others 2 have contributed to this issue The term 'referential description' is due to Donellan.
SIMR	U.N. annual report  I.L.O. report	 Table 2: SIMR accuracy on different text genres in three language pairs.  language  number of  number of  RMS Error  pair  training TPCs  genre  test TPCs  in characters  French / English  598  parliamentary debates  CITI technical reports  other technical reports  court transcripts  U.N. annual report  I.L.O. report
segmentation	WSJ	In this paper we will report all scores as a balanced F-measure (precision and recall weighted equally) with/~ = 1, such that F = 2PR/(P + R)  For an initial experiment, segmentation was performed using the maximum matching algorithm, with a large lexicon of 34272 English words compiled from the WSJ.
parsing and translating natural language	Wall Street Journal	We present a knowledge and context-based system for parsing and translating natural language and evaluate it on sentences from the Wall Street Journal.
translational equivalence	Hansard corpus	For instance, reports that his word-to-word model for translational equivalence produced lexicon entries with 99% precision and 46% recall when trained on 13 million words of the Hansard corpus, where recall was measured as the fraction of words from the bitext that were assigned some translation.
Binding Theory	HPS.G grammars	Binding Theory is still waiting to be accommpdate~ into HPS.G grammars ......
NP identification	R&M and Empire corpora	Recall, on the other hand, is an attempt to measure coverage: # of correct proposed NPs P = # of proposed NPs # of correct proposed NPs R = # of NPs in the annotated text summarizes the performance of the treebank approach to base NP identification on the R&M and Empire corpora using the initial and pruned rule sets.
name searching	WWW	The name searching facility is implemented on an MT sever for information retrieval on the WWW.
WSD	Brown corpus	Definitions and example sentences from LDOCE are employed as training materials for WSD, while passages from the Brown corpus and Wall Street Journal are used for testing.
WSD	Wall Street Journal	Definitions and example sentences from LDOCE are employed as training materials for WSD, while passages from the Brown corpus and Wall Street Journal are used for testing.
VIT	Verbmobil	The aim of VIT is to describe a consistent interface structure between the different language analysis modules within Verbmobil.
VIT	USR semantic structure of Sectiom 2	Thus, in contrast to our USR, VIT is a representation that encodes all the linguistic information of an utterance; in addition to the USR semantic structure of Sectiom 2, the Verbmobil Interface Term contains prosodic, syntactic, and discourse related information.
translation	Test-NYT	The translation experiment from English to Japanese was carried out on Test-NYT.
parsing	Sparc Station 10 workstation	All of the parsing times include accessing time for an example database (i.e. corresponding to the whole transfer time) and were measured on a Sparc Station 10 workstation with 256 MB of memory.
hypertext authoring of newspaper articles	WordNet	J. does hypertext authoring of newspaper articles by word's lexical chains which are calculated using WordNet.
summarization	SHARP Laboratories of Europe	An implementation of this approach to lexical cohesion has been used as the driving engine of a summarization system developed at SHARP Laboratories of Europe.
summarization	41 Reuter's news stories	To evaluate the utility of the approach to lexical cohesion developed for summarization, a testsuite was created using 41 Reuter's news stories and related summaries (available at http ://www.
topic identification	switchboard corpus	McDonough proposed a topic identification method on switchboard corpus.
tagging English text	Wall Street Journal and the Brown Corpora	We have applied this approach to tagging English text from the Wall Street Journal and the Brown Corpora.
character recognition	Sakai 93)	It is because a lots of works has been done on character recognition and layout analysis (Sakai 93) (Mino 96) (Sato 98).
layout analysis	Sakai 93)	It is because a lots of works has been done on character recognition and layout analysis (Sakai 93) (Mino 96) (Sato 98).
segmentation	LDC	We measured performance by comparing our segmentation to the gold standard annotation produced by the LDC.
table recognition	Wall Street Journal (WSJ) news documents from the ACL/DCI CD-ROM	To determine how well our learning approach performs on the task of table recognition, we selected 100 Wall Street Journal (WSJ) news documents from the ACL/DCI CD-ROM.
parsing	Collins 97)	We describe our experience in building on the parsing model of (Collins 97).
parsing	Collins 97)	This paper first describes a baseline approach, based on the parsing model of (Collins 97), which recovers dependencies with 72% accuracy.
FOMs	PCFG data	Later work introduced other FOMs formed from PCFG data;.
parsing development	WSJ22	141 axiom(c 0 ) : The dynamic programming arc-standard transition-based deductive system without spurious ambiguity: the symbol represents that the root node of the topmost element on the stack has not been scanned yet.: Unlabeled accuracy scores (UAS) and parsing times (+forest dumping times, second per sentence) for parsing development (WSJ22) and test (WSJ23) data with spurious shift-reduce and proposed shift-reduce parser (non-sp.) using several beam sizes.
parsing development	WSJ23	141 axiom(c 0 ) : The dynamic programming arc-standard transition-based deductive system without spurious ambiguity: the symbol represents that the root node of the topmost element on the stack has not been scanned yet.: Unlabeled accuracy scores (UAS) and parsing times (+forest dumping times, second per sentence) for parsing development (WSJ22) and test (WSJ23) data with spurious shift-reduce and proposed shift-reduce parser (non-sp.) using several beam sizes.
parsing  development	WSJ22	 Table 2: Unlabeled accuracy scores (UAS) and parsing times (+forest dumping times, second per sentence) for parsing  development (WSJ22) and test (WSJ23) data with spurious shift-reduce and proposed shift-reduce parser (non-sp.)  using several beam sizes.
parsing  development	WSJ23	 Table 2: Unlabeled accuracy scores (UAS) and parsing times (+forest dumping times, second per sentence) for parsing  development (WSJ22) and test (WSJ23) data with spurious shift-reduce and proposed shift-reduce parser (non-sp.)  using several beam sizes.
MT	IBM Models	There are excellent introductory texts-depending on the level of detail required, instructors can choose from a comprehensive MT textbook, a chapter of a popular NLP textbook), a tutorial survey (, or an intuitive tutorial on the IBM Models), among many others.
generative word alignment	IBM Models	During the past two decades, generative word alignment models such as the IBM Models () and the HMM model () have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit.
word alignment quality evaluation	SSMT2007 2	For word alignment quality evaluation, we take the handaligned data sets from SSMT2007 2 , which contains 505 sentence pairs in the testing set and 502 sentence pairs in the development set.
DA tagging	SANCL 2012 shared task	We show that FLORS achieves excellent DA tagging results on the five domains of the SANCL 2012 shared task  and outperforms three state-of-the-art taggers on biomedical data.
NER	CoNLL-03 NER corpus	For German NER experiments, we evaluate using the standard CoNLL-03 NER corpus.
POS tagging	English Penn Treebank	For POS tagging, this method () yields the best performance to date; 91.6% tagging accuracy on a standard test dataset from the English Penn Treebank.
speech repairs and restarts	Switchboard corpus	We consider our main contributions to be: • a novel non-monotonic transition system, for speech repairs and restarts, A flight to um and the Switchboard corpus.
disfluency detection	MRG files	Unlike most other disfluency detection research, we train only on the MRG files, giving us 619,236 words of training data instead of the 1,482,845 used by the pipeline systems.
coreference resolution	MUC measure	To accommodate the 2-level scheme, we therefore average F ↑ 1 , in which all weak links have been converted to strong links, and F ↓ 1 , in which they have been removed: If neither annotation contains any weak links, this equals the MUC As a criterion for coreference resolution, the MUC measure has perceived shortcomings which have prompted several other measures (see Recasens and Hovy, 2011 fora review).
Extrinsic evaluation	STS 2013 data	 Table 4: Extrinsic evaluation on STS 2013 data
synonym  choice	Reader's Digest Word Choice dataset.  MGHZ07: Mohammad et al	 Table 5: Exp. 1: Accuracy and Coverage for synonym  choice on the Reader's Digest Word Choice dataset.  MGHZ07: Mohammad et al. (2007). Best results for  each model class in bold.
synonym choice	CroSyn dataset	 Table 8: Experiment 3: Accuracy and Coverage for  synonym choice on the CroSyn dataset. SPA13: Šna- jder et al. (2013). In boldface: best results.
synonym choice	SPA13: Šna- jder et al. (2013)	 Table 8: Experiment 3: Accuracy and Coverage for  synonym choice on the CroSyn dataset. SPA13: Šna- jder et al. (2013). In boldface: best results.
dependency parsing	Wall Street Journal	Our approach leads to substantial improvements in dependency parsing results over the standard supervised CCG parser when evaluated on Wall Street Journal (0.8%), Wikipedia (1.8%) and biomedical (3.4%) text.
PP attachment	WordNet	 Table 7: PP attachment accuracy when enriching  word vectors with part-of-speech tags of the candi- date head (POS) and the following word (NextPOS),  and with WordNet and VerbNet features.
event discovery	NS13	For our event discovery algorithm, we set the number of event relations to be 30 and ran the algorithm on NS13.
Sentences	OpenNLP	Sentences are tokenized using OpenNLP.
predicting sentence similarity	FrameNet	The PPDB has recently been used for monolingual alignment (, for predicting sentence similarity (), and to improve the coverage of FrameNet ().
predicting sentence similarity	FrameNet	The PPDB has recently been used for monolingual alignment (, for predicting sentence similarity (), and to improve the coverage of FrameNet ().
SMT	Europarl	This corpus has been used extensively in SMT (, and was even adapted specifically for research in translation studies: compiled a customized version of Europarl, where the direction of translation is indicated.
resolution of gold named entity mentions	NIL	However, following most existing work, we only evaluate the resolution of gold named entity mentions to either KB or NIL.
SemEval task	CCGbank	We evaluate the performance of our parser on four linguistic data sets: those used in the recent SemEval task on semantic dependency parsing (, and the dependency graphs extracted from CCGbank.
SENSEVAL	WordNet-or-not-WordNet	Since SENSEVAL was first mooted, in 1997, WordNet-or-not-WordNet has been a recurring theme.
SENSEVAL-2 English all-words task	WordNet 1.7 lexical database	In the SENSEVAL-2 English all-words task, all ambiguous content words (nouns, verbs, adjectives, and adverbs) are to be classified with a sense tag from the WordNet 1.7 lexical database (Miller, 1990).
Semantic Tagging	W ordN	Semantic Tagging Using W ordN et Examples
sense disambiguation	Swedish data	The methodology followed for sense disambiguation of the Swedish data by the Sprakdata-ML system is supervised, based on Machine Learning (ML) techniques, particularly Memory Based Learning (MBL).
SM	WordNet	SM does not need a previously tagged corpus, it uses the semantic information stored in WordNet.
word sense disambiguation task	CIM	As a classification problem, word sense disambiguation task can be solved by the CIM.
preposition disambiguation task	TPP	The development of the datasets for the preposition disambiguation task grew directly out of TPP.
SemEval-2007	WePS Evaluation	The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task
SRL	WSD	There are three points that make this task harder and more interesting than earlier SRL tasks: (1) while previous tasks focused on role assignment, the current task also comprises the identification of the appropriate FrameNet frame, similar to WSD, (2) the task comprises not only the labeling of individual predicates and their arguments, but also the integration of all labels into an overall semantic dependency graph, a partial semantic representation of the overall sentence meaning based on frames and roles, and (3) the test data includes occurrences of frames that are not seen in the training data.
betaken	CL Research KMS	Finally, we identify next steps that can betaken within the CL Research KMS and DIMAP environments to extend the FrameNet data.
betaken	FrameNet data	Finally, we identify next steps that can betaken within the CL Research KMS and DIMAP environments to extend the FrameNet data.
metonymy resolution task at SemEval-2007	BNC	Annotating a subset of the BNC (British National Corpus), they extracted a set of metonymic proper nouns from two categories: country names () and organization names . In the metonymy resolution task at SemEval-2007, the goal was to identify metonymic names in a subset of the BNC.
SemEval-2007	English All-Words test data	 Table 3. POS-wise Evaluation on SemEval-2007  English All-Words test data
word sense disambiguation (WSD) of all content words (noun, adjective, verb, and adverb) occurring in five documents	WordNet sense inventory	In this paper, we describe the systems we developed for the coarse-grained English all-words task A system developed by one of the task organizers of the coarse-grained English all-words task gave the highest overall score for the coarse-grained English all-words task, but this score is not considered part of the official scores. and fine-grained English all-words task of In the coarse-grained English all-words task, systems have to perform word sense disambiguation (WSD) of all content words (noun, adjective, verb, and adverb) occurring in five documents, using a coarse-grained version of the WordNet sense inventory.
SMO	WEKA machine learning toolkit	For our system we use the SMO implementation of a support vector machine provided with the WEKA machine learning toolkit.
WSD	MCyT TIN2006-15265-C06-04	The unsupervised WSD overcomes this drawback by using clustering algorithms which do * This work has been partially supported by the MCyT TIN2006-15265-C06-04 project, as well as by the BUAP-701 PROMEP/103.5/05/1536 grant not need training data in order to determine the possible sense fora given ambiguous word.
WSD	BUAP-701 PROMEP/103.5/05/1536 grant	The unsupervised WSD overcomes this drawback by using clustering algorithms which do * This work has been partially supported by the MCyT TIN2006-15265-C06-04 project, as well as by the BUAP-701 PROMEP/103.5/05/1536 grant not need training data in order to determine the possible sense fora given ambiguous word.
WSD	Fuzzy Borda Voting	UPV-WSD : Combining different WSD Methods by means of Fuzzy Borda Voting
WSD	Web1T Corpus	USYD: WSD and Lexical Substitution using the Web1T Corpus
extracting frame semantic structures	FrameNet (FN) database	The SemEval-2007 task for extracting frame semantic structures relies on the human annotated data available in the FrameNet (FN) database.
SVM learning	TinySVM	We compared the following methods using a ten-fold crossvalidation test: For SVM learning, we used TinySVM with a linear kernel 4 . presents the results.
SemEval task	MUC	Three runs have been submitted for the SemEval task 1 on Coreference Resolution, optimizing Corry's performance for BLANC, MUC and CEAF.
SemEval task	CEAF	Three runs have been submitted for the SemEval task 1 on Coreference Resolution, optimizing Corry's performance for BLANC, MUC and CEAF.
Coreference Resolution	MUC	Three runs have been submitted for the SemEval task 1 on Coreference Resolution, optimizing Corry's performance for BLANC, MUC and CEAF.
Coreference Resolution	CEAF	Three runs have been submitted for the SemEval task 1 on Coreference Resolution, optimizing Corry's performance for BLANC, MUC and CEAF.
Keyphrase Extraction Task	ACM digital library	In the Keyphrase Extraction Task (, each system receives two set of scientific papers from the ACM digital library; a training set and a testing set.
recognition and classification of events	TimeML EVENT tag	Secondly, task B addresses the recognition and classification of events as defined by TimeML EVENT tag.
crossframework parser evaluation	Penn Treebank-trees	However, crossframework parser evaluation is a difficult problem: previous attempts to evaluate the C&C parser on grammatical relations and Penn Treebank-trees have also produced upper bounds between 80 and 90% F-score.
WSD	JAIST-2	We compared WSD precision of three systems for 50 individual target words, and found that JAIST-2 is almost always the best.
RACAI	RACAI test set	 Table 3: RACAI systems results (accuracy) on the  RACAI test set
RACAI	SEMEVAL test set	 Table 4: RACAI systems results (accuracy) on the  SEMEVAL test set
DNI linking	BayesNet	For DNI linking, we use BayesNet ( as classifier, implemented in Weka).
DNI linking	Weka	For DNI linking, we use BayesNet ( as classifier, implemented in Weka).
Recognition of NIs	FN database	Each frame fin the test corpus is processed, involving the following steps: (i) Recognition of NIs is performed by consulting the FN database and determining the FN core roles that are unfilled.
Identifying Interlingual Links	Wikipedia	Towards Building a Multilingual Semantic Network: Identifying Interlingual Links in Wikipedia
reference resolution	ExpA	ExpB showed that reference resolution is almost always successful despite the variation in colour terms observed in ExpA.
WSD	NSERC	Unsupervised WSD has been shown to work very well when the target word is embedded in a large We thank NSERC and U. Toronto for financial support.
WSD	U. Toronto	Unsupervised WSD has been shown to work very well when the target word is embedded in a large We thank NSERC and U. Toronto for financial support.
negation scope	CoNLL	Detection of negation cues and negation scope at CoNLL (,) and the Negation and Speculation in NLP Workshop) laid the foundation for the *Sem 2012 Shared Task.
factuality detection	CDTD	 Table 4: Results for factuality detection (using gold  negation cues and scopes). Due to the limited train- ing data for factuality, the classifier is only opti- mized by 10-fold cross-validation on CDTD.
NR	BioScope Corpus	The data set most prominently used for the development of systems for automatic NR is the BioScope Corpus (), a collection of clinical reports and papers in the biomedical domain annotated with negation and speculation cues and their scopes.
negation scope	BioScope corpus	In these sets, the concept of negation scope extends on the one adopted in the BioScope corpus in several aspects: Negation cues are not part of the scope, morphological (affixal) cues are annotated and scopes can be discontinuous.
STS task	WordNet	In our approach to the STS task, semantic vector is used and the semantic relatedness between words is derived from two sources: WordNet and Wikipedia.
paraphrase identification	MSR Paraphrase Corpus	From previous experience with paraphrase identification over the MSR Paraphrase Corpus, we retained stop words in all of our experiments.
tagging semantic roles	CoNLL-2005 2 benchmark	SENNA has been reported to achieve an Fmeasure of 75.79% for tagging semantic roles on the CoNLL-2005 2 benchmark.
SMT	FNWN	We have to question whether this improvement is an artifact of the rating distributions of these two sets (SMT contains virtually only high ratings, FNWN contains virtually only low ratings): such wild mismatches in priors among training and test sets can be mitigated using more elaborate machine learning algorithms (rather than employing better semantic similarity features or algorithms).
SMT	FNWN	 Table 2. Our best run was the simplest one, using  a purely linear model and effectively no adaptation.  Adding a more aggressive adaptation strategy im- proved results in the FNWN and SMT sets, so there  is definitely some potential, however the improve- ment observed is nowhere near that observed in the  training data or the same task of SemEval 2012. We  have to question whether this improvement is an ar- tifact of the rating distributions of these two sets  (SMT contains virtually only high ratings, FNWN  contains virtually only low ratings): such wild mis- matches in priors among training and test sets can  be mitigated using more elaborate machine learning  algorithms (rather than employing better semantic  similarity features or algorithms). Overall the sys- tem performs well in the two sets containing large  similarity rating ranges.
SMT	FNWN	 Table 2. Our best run was the simplest one, using  a purely linear model and effectively no adaptation.  Adding a more aggressive adaptation strategy im- proved results in the FNWN and SMT sets, so there  is definitely some potential, however the improve- ment observed is nowhere near that observed in the  training data or the same task of SemEval 2012. We  have to question whether this improvement is an ar- tifact of the rating distributions of these two sets  (SMT contains virtually only high ratings, FNWN  contains virtually only low ratings): such wild mis- matches in priors among training and test sets can  be mitigated using more elaborate machine learning  algorithms (rather than employing better semantic  similarity features or algorithms). Overall the sys- tem performs well in the two sets containing large  similarity rating ranges.
validation	2013 test set	For the validation the whole 2013 test set was used as it wasnot used for training.
SMT	SMTnews	For both headlines and SMT, we selected SMTnews and SMTeuroparl from STS 2012.
SMT	SMTeuroparl from STS 2012	For both headlines and SMT, we selected SMTnews and SMTeuroparl from STS 2012.
Message Understanding Conferences (MUCs)	Sheffield Temporal Annotation scheme (STAG))	In the series of Message Understanding Conferences (MUCs) that started from 1987 and the Sheffield Temporal Annotation scheme (STAG)) the aim was to identify events in news text and determine their relationship with points on a temporal line.
WSD	WordNet	Firstly, using multilingual unlabeled parallel corpora contributes to clearing the data acquisition bottleneck for WSD, because using translations as sense labels excludes the need for manually created sense-tagged corpora and sense inventories such as WordNet.
SMTbasic)	Europarl corpus	The baseline version of the system (SMTbasic) represents a standard phrase-based SMT baseline, that is trained only on the intersected Europarl corpus.
SMT	Europarl corpus	The baseline version of the system (SMTbasic) represents a standard phrase-based SMT baseline, that is trained only on the intersected Europarl corpus.
SMTadapt2	Europarl corpus	The optimized version of the system (SMTadapt2) is trained on the Europarl corpus and additional news data, and uses mixture models that are developed for domain adaptation in SMT.
SemEval disambiguation	OntoNotes	In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself) and from OntoNotes (.
MWSD	EuroWordNet	For it to be useful, MWSD needs a multilingual resource that contains different languages, such as BabelNet ( and EuroWordNet.
SRA	Beetle	The data for SRA consist of two data sets Beetle (5,199 instances) and SciEntsBank (10,804 instances) divided into training and test sets (76%-24% for Beetle and 46%-54% SciEntsBank).
WSI tasks	ukWaC	Furthermore, unlike in previous WSI tasks, organizers allow participants to use additional contexts not found in the ukWaC under the condition that they submit systems for both using only the ukWaC and with their augmented corpora.
WSI tasks	ukWaC	Furthermore, unlike in previous WSI tasks, organizers allow participants to use additional contexts not found in the ukWaC under the condition that they submit systems for both using only the ukWaC and with their augmented corpora.
Extraction of Drug-Drug Interactions from Biomedical Texts	DDIExtraction 2013)	SemEval-2013 Task 9 : Extraction of Drug-Drug Interactions from Biomedical Texts (DDIExtraction 2013)
Submitted	Twitter Data	 Table 1: Submitted System on Twitter Data
Submitted	Twitter test data	 Table 2: Submitted results: Twitter test data
Submitted	SMS test data	 Table 3: Submitted results: SMS test data
sentiment classification	ESA	In this work, we present our approach for sentiment classification which uses a combination of ESA and Naive Bayes classifier.
SemEval 2013 Task 7 challenge	A4 project 1 of the SFB 833	Our contribution to the SemEval 2013 Task 7 challenge () presented here is based on our research in the A4 project 1 of the SFB 833, which is dedicated to the question how meaning can be computationally compared in realistic situations.
Relation extraction	Moara+SL+TEES	 Table 5: Relation extraction results on the training and test set. Run 1 builds a majority voting on Moara+SL+TEES,  Run 2 on APG+Moara+SL+SLW+TEES, and Run 3 on SL+SLW+TEES. Partial characterizes only DDI detection  without classification of subtypes, whereas strict requires correct identification of subtypes as well.
SemEval 2013 challenge of recognition and classification of drug names	ChEBI ontology	For participating in the SemEval 2013 challenge of recognition and classification of drug names, we adapted our chemical entity recognition approach consisting in Conditional Random Fields for recognizing chemical terms and lexical similarity for entity resolution to the ChEBI ontology.
Detection	DDI	 Table 3: Detection and classification of DDI
SEM	Dublin	The 2014 edition of SEM takes place in Dublin on August 23 and 24 and is collocated with SemEval and COLING.
SEM	COLING	The 2014 edition of SEM takes place in Dublin on August 23 and 24 and is collocated with SemEval and COLING.
Emotion classification	NRC-lex	 Table 3: Emotion classification results  Method  Average F-Score  Anger  Fear  Joy  Sadness  Surprise Love  Baselines  WNA-lex  25.82%  6.61%  12.94%  8.76%  0.76%  2.67%  NRC-lex  21.37%  3.97%  16.04%  8.87%  1.54%  7.22%  Bow  56.5%  13.56%  63.34%  50.57%  21.65%  20.52%  PMI-lex  56.42%  2.39%  63.4%  50.57%  0.69%  22.31%  Our Learnt Lexicons  TF-lex  55.85%  19.03%  62.01%  50.54%  11.29%  37.69%  EMallclass-lex  56.64%  14.53%  61.89%  50.48%  12.33%  38.13%  EMclass-corpus-lex  57.35%  16.1%  62.74%  51.14%  12.05%  39.19%
binary sentiment classification of paragraph-length movie reviews	IMBD website	This study explores the potential of using deep semantic features to improve binary sentiment classification of paragraph-length movie reviews from the IMBD website.
SRL	CoNLL 2009 shared task scoring script 1	To evaluate SRL performance, we use the CoNLL 2009 shared task scoring script 1 , which assumes a semantic dependency between the argument and predicate and the predicate and a dummy root node and then calculates the precision (P), recall (R) and F 1 of identification of these dependencies and classification (labelling) of them.
TT	macro dataset	We evaluated TT on the macro dataset without providing the number of boundaries.
parsing	NL dataset	In contrast, this paper focuses on parsing using additional situational context for disambiguation and by using a larger NL dataset, in comparison to previous robotics research.
Aspect term polarity detection	Laptop  Restaurant  Best  0.7049  0.8095  Blinov  0.5229  0.6358  Baseline  0.5107  0.6428	 Table 5: Aspect term polarity detection results  (Accuracy).  Laptop  Restaurant  Best  0.7049  0.8095  Blinov  0.5229  0.6358  Baseline  0.5107  0.6428
Sentiment Analysis of Twitter shared task	SemEval 2014	In this paper, we describe our system for the Sentiment Analysis of Twitter shared task in SemEval 2014.
sentiment classification	restaurant data set	Note that the sentiment classification algorithm is used for subtasks 2 and 4, so two scores are reported, and that subtasks 3 and 4 can only be performed with the restaurant data set.
sentiment analysis	twitter (task 9) organized in SemEval 2014	In this paper, we describe our approaches used for sentiment analysis in twitter (task 9) organized in SemEval 2014.
message polarity disambiguation	SMS datatype	For message polarity disambiguation we obtain the highest F-score of 77.04% for the SMS datatype in Task-A. The system shows the F-scores of 76.03%, 70.91%, 72.25% and 66.35% for LiveJournal2014, Twitter2013, Twitter2014 and Twitter2014sarcasm, respectively.
message polarity disambiguation	Twitter2013	For message polarity disambiguation we obtain the highest F-score of 77.04% for the SMS datatype in Task-A. The system shows the F-scores of 76.03%, 70.91%, 72.25% and 66.35% for LiveJournal2014, Twitter2013, Twitter2014 and Twitter2014sarcasm, respectively.
message polarity disambiguation	Twitter2014	For message polarity disambiguation we obtain the highest F-score of 77.04% for the SMS datatype in Task-A. The system shows the F-scores of 76.03%, 70.91%, 72.25% and 66.35% for LiveJournal2014, Twitter2013, Twitter2014 and Twitter2014sarcasm, respectively.
sentiment analysis	Live Journal data	The main contributions of this paper include a) developing a sentiment analysis classifer for phrases; b) training on Twitter data and testing on other domains such as SMS and Live Journal data to see how well the classifier generalizes to different types of text, and c) testing on sarcastic tweets.
message-level sentiment classification task	Live-Journal blog posts test set	In the message-level sentiment classification task, our submissions obtained highest scores on the Live-Journal blog posts test set, sarcastic tweets test set, and the 2013 SMS test set.
message-level sentiment classification task	2013 SMS test set	In the message-level sentiment classification task, our submissions obtained highest scores on the Live-Journal blog posts test set, sarcastic tweets test set, and the 2013 SMS test set.
SDP task	WSJ text	For the SDP task, three such annotation layers are provided on top of the WSJ text of the Penn Treebank (PTB) -DM: the reduction of DeepBank HPSG annotation ( ) into bi-lexical dependencies following), -PAS: the predicate-argument structures derived from the training set of the Enju HPSG parser () and -PCEDT: a subset of the tectogrammatical annotation layer from the English side of the Prague Czech-English Dependency).
SDP task	Penn Treebank (PTB	For the SDP task, three such annotation layers are provided on top of the WSJ text of the Penn Treebank (PTB) -DM: the reduction of DeepBank HPSG annotation ( ) into bi-lexical dependencies following), -PAS: the predicate-argument structures derived from the training set of the Enju HPSG parser () and -PCEDT: a subset of the tectogrammatical annotation layer from the English side of the Prague Czech-English Dependency).
SDP task	English side of the Prague Czech-English Dependency	For the SDP task, three such annotation layers are provided on top of the WSJ text of the Penn Treebank (PTB) -DM: the reduction of DeepBank HPSG annotation ( ) into bi-lexical dependencies following), -PAS: the predicate-argument structures derived from the training set of the Enju HPSG parser () and -PCEDT: a subset of the tectogrammatical annotation layer from the English side of the Prague Czech-English Dependency).
SemEval 2014	ShARe/CLEF eHealth 2013 task	In this context, Task 7 of SemEval 2014, which is a continuation of the ShARe/CLEF eHealth 2013 task (, provides a testbed to evaluate systems that automatically tag and normalize mentions of diseases, signs and symptoms in clinical records, which include discharge summaries and echo, radiology and ECG reports.
detecting and normalizing mentions of chemicals, proteins/genes, diseases and action terms	eHealth 2013 task	We have tested the system in a variety of tasks, such as detecting and normalizing mentions of chemicals, proteins/genes, diseases and action terms in the BioCreative 13 Chemdner and CTD tasks, as well as in detecting cellular and pathological events in the BioNLP cancer genetics task (Ramanan and Senthil Nathan, 2013c); we also participated in the eHealth 2013 task ().
MSTS	OnWN	 Table 8: MSTS training results on the English, En- glish OnWN, and Spanish tasks.
SemEval 2013 task	SAP	The goal of the SemEval 2013 task 2: Sentiment Analysis in Twitter () * The work was done during an internship at SAP.
Sentiment Analysis	SAP	The goal of the SemEval 2013 task 2: Sentiment Analysis in Twitter () * The work was done during an internship at SAP.
Sentiment Analysis	Twit-ter	We describe the submission of the team of the Sofia University to SemEval-2014 Task 9 on Sentiment Analysis in Twit-ter.
Sentiment analysis	TASS 2013	Sentiment analysis is one such technology, and several workshops such as,, and TASS 2013) have recently targeted tweets or cellphone messages as analysis text.
sentiment classification	Twit-ter	We present a sentiment classification system that participated in the SemEval 2014 shared task on sentiment analysis in Twit-ter.
CUI mapping problem	SNOMED CT concept names	We describe our general approach to address the aforementioned CUI mapping problem, based on similarity search and on the information content of SNOMED CT concept names.
recognition of medical concepts	UMLS semantic group disorders	In the first task, recognition of medical concepts, systems have to detect continuous and discontinuous medical concepts that belong to the UMLS semantic group disorders.
CUI mapping of medical concepts	SNOMED CT	For supporting the recognition and CUI mapping of medical concepts, we retrieved the disorders subset of SNOMED CT directly from UMLS . The evaluation can be done in a strict or a in a relaxed way.
CUI mapping of medical concepts	UMLS	For supporting the recognition and CUI mapping of medical concepts, we retrieved the disorders subset of SNOMED CT directly from UMLS . The evaluation can be done in a strict or a in a relaxed way.
Sentiment Analysis in Twitter	UMCC DLSI Sem	This paper describes a system submitted to SemEval-2014 Task 4B: Sentiment Analysis in Twitter, by the team UMCC DLSI Sem integrated by researchers of the University of Matanzas, Cuba and the University of Alicante, Spain.
MTL	STS datasets	We apply MTL to cope with the challenge of unbalanced performances across domains and unknown domains present in the STS datasets.
normalize disorder mentions	UMLS	To normalize disorder mentions, our system first looked for exact matches with disorder mentions in the training data and in the UMLS.
predicting 5-category sentiment	Stanford Sentiment Treebank	 Table 1: Model performances (accuracies) on predicting 5-category sentiment at the sentence (root) level and phrase- level on Stanford Sentiment Treebank. The numbers in the bold font are the best performances achieved on the two  tasks. Both results are statistically significantly better (p < 0.05) than the corresponding RNTN results.
SEMEVAL training	GERBERCHAI	Experiment 1 extends the SEMEVAL training data with out-of-domain data from GERBERCHAI and evaluates on SEMEVAL.
SEMEVAL training	SEMEVAL	Experiment 1 extends the SEMEVAL training data with out-of-domain data from GERBERCHAI and evaluates on SEMEVAL.
IMS	MASC	In our experiments we train IMS with the train examples of the crowdsourced MASC.
Scope detection	BioScope corpus	Scope detection is crucial for interpreting negations, and to that end, the BioScope corpus () was released, with annotations of both negation cues and their associated scopes.
interpreting negations	BioScope corpus	Scope detection is crucial for interpreting negations, and to that end, the BioScope corpus () was released, with annotations of both negation cues and their associated scopes.
Spearman	gold standard datasets	 Table 3: Spearman rank correlation of relatedness measures with gold standard datasets
keyphrase extraction	Inspec dataset of ACM abstracts	We evaluate our keyphrase extraction algorithm by comparing it to two state-of-the-art algorithms, KP-Miner and TextRank, on three datasets: The Semeval 2010 keyphrase extraction shared task dataset, the Inspec dataset of ACM abstracts and the Krapivin dataset of full length papers.
keyphrase extraction	Krapivin dataset of full length papers	We evaluate our keyphrase extraction algorithm by comparing it to two state-of-the-art algorithms, KP-Miner and TextRank, on three datasets: The Semeval 2010 keyphrase extraction shared task dataset, the Inspec dataset of ACM abstracts and the Krapivin dataset of full length papers.
Semantic Language Classification	Europarl	Towards Semantic Language Classification: Inducing and Clustering Semantic Association Networks from Europarl
summarization	DUC 2007 1	The main summarization task in DUC 2007 1 is the generation of 250-word summaries of 45 clusters of 25 newswire documents (from the AQUAINT corpus) and 4 human reference summaries.
summarization	AQUAINT corpus	The main summarization task in DUC 2007 1 is the generation of 250-word summaries of 45 clusters of 25 newswire documents (from the AQUAINT corpus) and 4 human reference summaries.
SRL	Propbank-Br	In this paper, we report the strategies used to build a lexical resource to support SRL in Portuguese (hereafter referred as Verbo-Brasil), profiting from the English resource developed within the Propbank project and of annotated instances of the corpus Propbank-Br (.
SRL	PropBank	In the standard supervised approach to building SRL systems, collections of multiway classifiers are trained using annotated corpora such as PropBank (Palmer et al.).
paraphrase recognition	Microsoft Research paraphrase corpus (MSRP)	Other than similarity features, we also use evaluation metrics for machine translation as suggested in () for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) ().
translation acts	Biçici and Way, 2014b)	MTPP features for translation acts are provided in (Biçici and Way, 2014b).
answer quality evaluation	CQA	As a consequently, the task of answer quality evaluation and answer selection in CQA have attracted more and more attention in recent years.
answer selection	CQA	As a consequently, the task of answer quality evaluation and answer selection in CQA have attracted more and more attention in recent years.
answer selection	CQA	It created avenue and provided annotated datasets for researchers to compare their methods for answer selection in CQA.
paraphrase recognition	Microsoft Research paraphrase corpus (MSRP)	We also use evaluation metrics for machine translation as suggested in () for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) ().
STS interpretable	Headlines data set	 Table 5: STS interpretable results for the gold chunks scenario. Best results have been marked in bold. 'H' stands  for Headlines data set and 'I' stands for Images data set. + symbol denotes resubmissions and  *  symbol denotes task  organizers.
STS interpretable	Headlines data set	 Table 6: STS interpretable results for the system chunks scenario. Best results have been marked in bold. 'H' stands  for Headlines data set and 'I' stands for Images data set. + symbol denotes resubmissions and  *  symbol denotes task  organizers.
entity recognition	UMLS CUI	Evaluation on the test data set showed that our system achieved the F-measure of 0.898 for entity recognition and the F-measure of 0.794 for UMLS CUI.
identification of disorder named entities	UMLS Metathesaurus	In this paper, identification of disorder named entities and the mapping of identified disorder entities to SNOMED-CT terminology using UMLS Metathesaurus is presented.
entity recognition	Stanford NER tool	The entity recognition module is based on the Stanford NER tool (), and it uses CRF models trained on annotated biomedical notes.
entity recognition	NGram	Both systems used the same approach for entity recognition but, in terms of the normalization component, the system from 2014 was entirely based on a lexical similarity approach using NGram, Levenstein and JaroWinkler distances.
detection of disorder mentions in clinical	UMLS/SNOMED-CT CUI	In the following section we describe our approach to the detection of disorder mentions in clinical texts and their categorization with the relevant UMLS/SNOMED-CT CUI.
sarcasm detection	2014 dataset	Regarding sarcasm detection in 2014 dataset, our system had good results in tweets with hashtags (25 right answers out of 35) whereas it was more prone to fail when users expressed positive opinions over negative events.
Subtask A	NRC Canada	The top system for Subtask A in both 2013 and 2014 from NRC Canada () used a simple linear SVM while putting great effort into creating and incorporating sentiment lexicons as well as carefully handling negation contexts.
Sentiment Analysis of Figurative Tweets	NotReally	LT3: Sentiment Analysis of Figurative Tweets: piece of cake #NotReally
Sentiment Analysis in Twitter	Spanish fellowship RYC-2009-04291	Task 10 concerned "Sentiment Analysis in Twitter" * The research described in this paper is partially funded by the Spanish fellowship RYC-2009-04291, the SKATER-TALN UPF project (TIN2012-38584-C06-03), and the EU project Dr. Inventor (n. 611383). and included different subtasks.
SA	International Workshop on Semantic Evaluation 2014	Consequently, to achieve a higher level of detail, part of the scientific community related to this area is working on SA at aspect level,), (Lu et al., 2011),) and even, there is a competition on this topic that began to conduct last year () in the International Workshop on Semantic Evaluation 2014.
Sentiment Polarity	restaurant data set	Our submission for Sentiment Polarity is ranked third over ten submissions on the restaurant data set, third over thirteen on the laptops data set, but the first over eleven on the hotel data set that is out-of-domain set.
Sentiment Polarity	laptops data set	Our submission for Sentiment Polarity is ranked third over ten submissions on the restaurant data set, third over thirteen on the laptops data set, but the first over eleven on the hotel data set that is out-of-domain set.
Sentiment Polarity	hotel data set	Our submission for Sentiment Polarity is ranked third over ten submissions on the restaurant data set, third over thirteen on the laptops data set, but the first over eleven on the hotel data set that is out-of-domain set.
timeline extraction	CLTL Lab of the VU Amsterdam	This paper reports on a system (SPINOZA VU) for timeline extraction developed at the CLTL Lab of the VU Amsterdam in the context of the SemEval 2015 Task 4: Cross Document TimeLines.
SVMs-based classification	NewsReader NLP pipeline	The HLT-FBK system is a suite of SVMs-based classification models for extracting time expressions, events and temporal relations, each with a set of features obtained with the NewsReader NLP pipeline.
Temporal information annotation	TimeML	Temporal information annotation should follow TimeML scheme ().
Automatic Recognition of Spatial Information	ISO-Space Annotation	IXAGroupEHUSpaceEval: (X-Space) A WordNet-based approach towards the Automatic Recognition of Spatial Information following the ISO-Space Annotation Scheme
Classification	training data set	 Table 3: Classification accuracy of various feature sets,  using 10-fold cross-validiation on the training data set.
Structural Measures	USAAR-WLV	 Table 1: Structural Measures and Comparison against Gold Standards for USAAR-WLV. The labels of the columns  refer to no. of distinct vertices and edges in induced taxonomy (|V| and |E|), no. of connected components (#c.c),  whether the taxonomy is a Directed Acyclic Graph (cycles), vertex and edge coverage, i.e. proportion of gold standard  vertices and edges covered by system (%VC and %EC), no. of vertices and edges in common with gold standard  (#VC and #EC) and ratio of novel edges (:NE).
taxonomy extraction task	WordNet	We propose an approach to the taxonomy extraction task in) with the following contributions:  To derive the statistical information about individual concepts in a given domain, the study uses WordNet and Wikipedia to find the definition for the concept.
interpretation	Tipster corpus	In this paper we will define this phenomenon and illustrate its impact on interpretation by examining short texts excerpted from the Tipster corpus and other online sources.
Dialogue Management	Mercury Flight Reservation	Dialogue Management in the Mercury Flight Reservation System
SETI	International	Outside our own immediate NLP sphere, generic communication techniques are of particular interest in the astronautical community, where two sessions are dedicated to SETI at their annual International conference with topics ranging from detecting ET technology to the ethics and logistics of message construction (E1-liott and Atwell, 1999; Ollongren, 2000; Vakoch, 2000).
Information Extraction (IE)	MUC conferences	In fact, a number of Information Extraction (IE) systems has emerged in the past few years in relation to the MUC conferences 1.
IE task	TIPSTER sponsored MUC-6 conference as named entity (NE)	This paper begins to address this issue, in particular the lowest level of IE task, defined in the TIPSTER sponsored MUC-6 conference as named entity (NE).
acquisition of names of people, organizations and monetary units	MUC-6 data set	We then test the predictions • Newswire: acquisition of names of people, organizations and monetary units etc., from the MUC-6 data set.
summarization-based categorization	Reuters news corpus	Experimental results indicate that summarization-based categorization can achieve acceptable performance on Reuters news corpus.
summarization-based categorization	Reuters news corpus	Experimental results indicate that summarization-based categorization can achieve acceptable performance on Reuters news corpus.
syntactic parsing	English test set of the Penn Treebank	The success of statistical methods in particular has been quite evident in the area of syntactic parsing, most recently with the outstanding results of) and on the now-standard English test set of the Penn Treebank (.
Annotating information structures in Chinese texts	HowNet	Annotating information structures in Chinese texts using HowNet
question answering (QA)	TREC-8	The need for question answering (QA) systems has prompted the initiation of the question answering track in TREC-8) to address this problem.
question answering	TREC-8	The need for question answering (QA) systems has prompted the initiation of the question answering track in TREC-8) to address this problem.
SGML	DTD	One important leature of SGML is the DTD.
tim construction	TRIPS system (Allen et al., 2000)	2 Previous work We are engaged in tim construction and inlplemen-ration of a theory of content-planning for complex, mixed-initiative task-oriented dialogs based on corpus analysis, for use in dialog systems such as the TRIPS system (Allen et al., 2000) 1.
evaluation	NLG	However, evaluation has thus far not been as well-established in NLG as it has become in NLU.
evaluation	NLU	However, evaluation has thus far not been as well-established in NLG as it has become in NLU.
hand tagging	WordNet senses	Besides, the usefulness of hand tagging using WordNet senses will be tested, training on one corpus and testing in the other.
information extraction task	MUC	In information extraction task, annotated corpora have been made mainly for the judgment set of information extraction competitions such as MUC.
statistical parsing	Wall Street Journal portion of the Penn Treebank	Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.
parsing	WSJ corpus	Our investigation of these questions leads us to a surprising result about parsing the WSJ corpus: over a third of the model's parameters can be eliminated with little impact on performance.
translating corpora between syntactic formalisms	Penn Treebank annotation	A supervised machine learning technique is presented for translating corpora between syntactic formalisms and is applied to the task of translating the Penn Treebank annotation into a Categorial Grammar annotation.
NLP system development	AUF	There are also several by-products of this campaign: first, two corpora which can be used for NLP system development and evaluation as the AUF recommended; and then terminology products: for each corpus a list of terms characterizing the field is available.
Misclassification Matrix	CMU dataset	 Table 4: Misclassification Matrix using SVM with presentation features from CMU dataset
Misclassification Matrix	CMU dataset	 Table 5: Misclassification Matrix using SVM with presentation + word features from CMU dataset
Misclassification	CMU dataset	 Table 3: Misclassification Matrix for SVM using 323 word frequencies from CMU dataset
SVM	CMU dataset	 Table 3: Misclassification Matrix for SVM using 323 word frequencies from CMU dataset
question parsing	Penn treebank training corpus	We empirically show how question parsing dramatically improves when augmenting a semantically enriched Penn treebank training corpus with an additional question treebank.
question answering	TREC8	We developed a system for question answering, QALC, evaluated in the framework of the QA tracks at TREC8 and TREC9.
question answering	TREC9	We developed a system for question answering, QALC, evaluated in the framework of the QA tracks at TREC8 and TREC9.
question answering	TREC collection	Motivated partially by the TREC-8 QA collection), question answering has of late become one of the major topics within the natural language processing and information retrieval communities, and a number of QA systems targeting the TREC collection have been proposed (.
Directional PPs	DRT	Directional PPs and Reference Frames in DRT
translation of spoken dialogues	VERBMOBIL project	We discuss the properties of the system components and report results on the translation of spoken dialogues in the VERBMOBIL project.
parsing	HPSG	This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.
parsing	LTAG	shows the average parsing time with the LTAG and HPSG parsers.
parsing	HPSG	shows the average parsing time with the LTAG and HPSG parsers.
parsing	HPSG	This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.
recognition output	BoosTexterr	These results for recognition output were obtained with the Phi classiier module rather than BoosTexterr the Phi classiier performance is generally the same as, or slightly better than, BoosTexter when applied to recognition output.
Type Agreement	WSJ Data	 Table 3: 񮽙 Values for Type Agreement Using All Types in the WSJ Data
Computational linguistics	UT	Computational linguistics as an independent subject has been taught at UT from the study year 1998/99, various separate courses from the study year 1996/97.
DATE labelling	June-2000 corpus	In previous work (), the DATE labelling of the June-2000 corpus was done using a semi-automatic method that involved collection of a large number of utterance patterns from the different sites participating in the collection and subsequent hand labelling of these patterns.
SVM	GENIA corpus-tractable	To make the SVM training with the available largest corpus-the GENIA corpus-tractable, we propose to split the nonentity class into sub-classes, using part-of-speech information.
SFST	MGTI	In the case of sufficient training data, the source language model associated to a SFST learnt by the MGTI or OMEGA is better than trigrams (Section 2.1).
SFST	OMEGA	In the case of sufficient training data, the source language model associated to a SFST learnt by the MGTI or OMEGA is better than trigrams (Section 2.1).
ATR-MATRIX speech translation	ATR Interpreting Telecommunications Research Laboratories	Experiments are conducted on the ATR-MATRIX speech translation system, developed at ATR Interpreting Telecommunications Research Laboratories.
translation	TOEIC	Consequently, the translation capability of the language translation system equals that of the examinees at around a score of 700 points on the TOEIC.
SENSEVAL-2	WordNet	Official SENSEVAL-2 results were generated using WordNet, and separately using the New Oxford Dictionary of English (NODE).
SENSEVAL-2	New Oxford Dictionary of English (NODE)	Official SENSEVAL-2 results were generated using WordNet, and separately using the New Oxford Dictionary of English (NODE).
classifier combination	MMVC classifiers	This paper offers a detailed comparative evaluation and description of the problem of classifier combination over a structurally and procedurally diverse set of six both well established and original classifiers: extended Naïve Bayes, BayesRatio, Cosine, non-hierarchical Decision Lists, Transformation Based Learning (TBL), and the MMVC classifiers, briefly described in Section 4.
SDR	CREST	State-of-the-art SDR methods, where speech recognition error rate is 20-30%, are ¡ The first and second authors are also members of CREST, Japan Science and Technology Corporation.
SDR	Japan Science and Technology Corporation	State-of-the-art SDR methods, where speech recognition error rate is 20-30%, are ¡ The first and second authors are also members of CREST, Japan Science and Technology Corporation.
classification task	Reuters-21578	It is generally agreed that these methods using statistical and machine learning techniques are effective for classification task, since most of them showed significant improvement (the performance over 0.85 F1 score) for Reuters-21578,,).
SMT	Hansard	In section 3, we quantify and analyse the performance deterioration of an SMT engine trained on a broad-based corpus (the Hansard) when used to translate a domain specific text (in this study, a manual for military snipers).
IE	MEDLINE	Our more linguistic approach maybe of assistence in IE: see for discussion of methods for IE from MEDLINE.
parsing of Classical Chinese	PCFG	Now in this paper, we move onto the parsing of Classical Chinese by PCFG model.
SVC identification	TOTAL-CK	In this paper, we proposes a different categorization of SVCs defined by the contrastive analysis of the two languages, and also an SVC identification method that is adopted in a Chinese-to-Korean MT system, TOTAL-CK.
MT	TOTAL-CK	In this paper, we proposes a different categorization of SVCs defined by the contrastive analysis of the two languages, and also an SVC identification method that is adopted in a Chinese-to-Korean MT system, TOTAL-CK.
FAQ retrieval	FAQFinder	It is also a database for the applications of FAQ retrieval, e.g. AskJeeves (www.ask.com), .faq finder (members.tripod.com/~FAQ_Home/), and FAQFinder (www1.ics.uci.edu/~burke/faqfinder/).
IIS	GIS	First, while IIS converges in fewer steps the GIS, it takes substantially more time.
phoneme recognitionthe	TIMIT corpus	One reason is the relatively poor performance of phoneme recognitionthe best phoneme recognition accuracy is about 75% for the TIMIT corpus.
SELSA	LSA	We observe that the best performance for SELSA is achieved at 300 dimensions with 126 correct and 30 false evaluations, while for LSA it is at 400 dimensions with 123 correct and 30 false evaluations.
parse reranking problem	WSJ section 23 of Penn Treebank	We have applied this algorithm to the parse reranking problem, and achieved labeled recall and precision of 89.4%/89.8% on WSJ section 23 of Penn Treebank.
classification tasks	Penn Treebank	Selective sampling has typically been applied to classification tasks, but has also been shown to reduce the number of examples needed for inducing Lexicalized Tree Insertion Grammars from the Penn Treebank).
clause identification	Penn Treebank	 Table 3: Scores for our clause identification system on  the Penn Treebank compared to the best and average  CoNLL-2001 scores.
clause identification	CoNLL-2001	 Table 3: Scores for our clause identification system on  the Penn Treebank compared to the best and average  CoNLL-2001 scores.
Text Summarization Challenge	NTCIR Workshop 3	We describe the outline of Text Summarization Challenge 2 (TSC2 hereafter), a sequel text summarization evaluation conducted as one of the tasks at the NTCIR Workshop 3.
GATE	Clearforest	Among these, the most widely used are the GATE system from the University of Sheffield [, the IE components from Clearforest (www.clearforest.com), SIFT from BBN [, REES from SRA] and various tools provided by Inxight (www.inxight.com).
GATE	BBN	Among these, the most widely used are the GATE system from the University of Sheffield [, the IE components from Clearforest (www.clearforest.com), SIFT from BBN [, REES from SRA] and various tools provided by Inxight (www.inxight.com).
Lexical semantics	WordNet	Lexical semantics has made significant progress theoretically (e.g., in the area of generative lexicon), descriptively (e.g., WordNet and the many satellite projects that it engendered) and processing-wise (as witnessed, e.g., by the SEMEVAL experiment).
generative lexicon	WordNet	Lexical semantics has made significant progress theoretically (e.g., in the area of generative lexicon), descriptively (e.g., WordNet and the many satellite projects that it engendered) and processing-wise (as witnessed, e.g., by the SEMEVAL experiment).
Extracting and Evaluating General World Knowledge	Brown Corpus	Extracting and Evaluating General World Knowledge from the Brown Corpus
extracting general world knowledge from miscellaneous texts	Brown corpus	We have been developing techniques for extracting general world knowledge from miscellaneous texts by a process of approximate interpretation and abstraction, focusing initially on the Brown corpus.
translation	French test sets	Results Tables A and B below list evaluation results for translation on the Arabic and French test sets respectively.
parse reranking	WSJ section 23 of Penn Treebank	We use LTAG based features for the parse reranking task and obtain labeled recall and precision of 89.7%/90.0% on WSJ section 23 of Penn Treebank for sentences of length ≤ 100 words.
Spam filtering	IIS-0208798	Spam filtering systems * This work was supported by the National Science Foundation under grants IIS-0208798, IIS-0208985, and IRI-9704240.
Spam filtering	IIS-0208985	Spam filtering systems * This work was supported by the National Science Foundation under grants IIS-0208798, IIS-0208985, and IRI-9704240.
Spam filtering	IRI-9704240	Spam filtering systems * This work was supported by the National Science Foundation under grants IIS-0208798, IIS-0208985, and IRI-9704240.
IR	WWW	The accuracy of IR result continues to grow on importance as exponential growth of WWW, and it is therefore increasingly important that appropriate retrieval technologies be developed for the web.
document clustering	NewsML standard	Under the hypothesis that document clustering algorithm can get better result with more information about data used, we suggest that using additional meta-data contained in NewsML standard could enhance the performance of document clustering algorithms.
NE recognizer	FALCON	In particular, the NE recognizer is important module in the well-known question answering systems such as FALCON, IBM.
NE recognizer	IBM	In particular, the NE recognizer is important module in the well-known question answering systems such as FALCON, IBM.
question answering	FALCON	In particular, the NE recognizer is important module in the well-known question answering systems such as FALCON, IBM.
question answering	IBM	In particular, the NE recognizer is important module in the well-known question answering systems such as FALCON, IBM.
SDS task	DUC-2001	shows the results of a subjective evaluation in the SDS task at DUC-2001.
Passage Retrieval	TREC-8	At first, ODQA followed the Passage Retrieval method as used at TREC-8.
Gene Name Extraction	FlyBase	Gene Name Extraction Using FlyBase Resources
disambiguation	MUCHMORE project	The goal of our work on disambiguation in the MUCHMORE project is to enable the correct semantic annotation of entire document collections with all terms which are potentially relevant for organisation, retrieval and summarisation of information.
information extraction of protein-protein interaction	GenBank 1 ID	For the purpose of information extraction of protein-protein interaction, the ID information of recognized proteins, such as GenBank 1 ID or SwissProt 2 ID, is indispensable to integrate the extracted information with the data in other information sources.
information extraction of protein-protein interaction	SwissProt 2 ID	For the purpose of information extraction of protein-protein interaction, the ID information of recognized proteins, such as GenBank 1 ID or SwissProt 2 ID, is indispensable to integrate the extracted information with the data in other information sources.
NER	V3.0	We evaluate our NER system on both V3.0 and V1.1, each of which has been split into a training set and a testing set.
NER	V1.1	We evaluate our NER system on both V3.0 and V1.1, each of which has been split into a training set and a testing set.
name classification	MUC conferences	For this reason, name classification has been studied in solving the named entity extraction task in the NLP and information extraction communities (see, for example,) and various approaches reported in the MUC conferences).
coreference resolution	UMLS system 1	In fact, the coreference resolution method described in ) seeks to use such information by using the UMLS system 1 and by applying type coercion.
encoding of information on metaphors	EuroWordNet	In this paper we address the issue of the encoding of information on metaphors in a WordNet-like database, i.e. the Italian wordnet in EuroWordNet (ItalWordNet).
synonym extraction.	Shimohata	Other resources are also used for synonym extraction., and Shimohata and used bilingual corpora to extract synonyms.
sense tagging	Sinorama	 Table 6. Experimental results of sense tagging the Sinorama
segmentation bakeoff-PK-open	CTB-open	The system participated in four tracks of the segmentation bakeoff-PK-open, PK-close, CTB-open and CTB-closed-and ranked #1, #2, #2 and #3 respectively in those tracks.
segmentation	Beijing University corpus	However, we have withdrawn our segmentation results on the Beijing University corpus.
segmentation module	Bakeoff	This paper gives a general description of the segmentation module, as well as the results and analysis of its performance in the Bakeoff.
coreference	Penn Treebank	To work on coreference we used information from syntactic annotated corpus, the Penn Treebank.
word sense disambiguation (WSD)	WordNet	The annotation of word senses such as used by machine-learning based word sense disambiguation (WSD) tools corresponds to the task of selecting the correct semantic class or concept fora word from an underlying ontology such as WordNet.
collection of cue phrases	CFG	For the analysis, the collection of cue phrases, and the creation of the CFG, we used patents in 1998.
DA classification	TiMBL	For DA classification, there were no significant differences between the TiMBL, C4.5, and SNNS classifiers for English or German.
Multimodal Dialogue Management	COMIC	Multimodal Dialogue Management in the COMIC Project
answer extraction	Conexor FDG	Using answer extraction as an example of an NLP application, we compared the performance of the Link Grammar system and Conexor FDG.
Incremental Parsing	ACL-2004	Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together Held in cooperation with ACL-2004
parsing	Penn Treebank	The labeled precision and recall of the parsing are 80.8% and 78.5%, respectively for the section 23 in Penn Treebank.
question answering (QA)	NTCIR	Although most current research in question answering (QA) is oriented towards open domains, as witnessed by evaluation exercises such as TREC, CLEF, and NTCIR, various significant applications concern restricted domains, e.g., software manuals.
name recognition and classification	MUC-6 Evaluation	The problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the MUC-6 Evaluation (.
DN detection	Wall Street Journal portion of the Penn Treebank	We reexamine the literature on the topic in detail, and propose a revised algorithm, taking advantage of the improved DN detection techniques developed by. carried out corpus studies indicating that in corpora like the Wall Street Journal portion of the Penn Treebank (, around 52% of DDs are discourse-new, and another 15% or so are bridging references, fora total of about 66-67% firstmention.
DN detection	GUITAR	The baseline algorithm without DN detection incorporated in GUITAR described above (i.e., only the direct anaphora resolution part of (Vieira and Poesio, 2000)); 2.
word sense classification tasks	Wordnet	One of the main difficulties in word sense classification tasks stems from the fact that word senses, such as Wordnet's synsets, define very specific classes . As a consequence training instances are often too few in number to capture extremely fine-grained semantic distinctions.
SRL task	Framenet () corpus	The SRL task in Senseval-3 used the Framenet () corpus: given a sentence instance from the corpus, a system's job would be to identify the phrase constituents and their corresponding role.
integration	TALP group	The integration was carried out by the TALP group.
Relative clause interpretation	MUC conference series	Relative clause interpretation is a core component of text understanding, as demonstrated in the context of the MUC conference series.
text understanding	MUC conference series	Relative clause interpretation is a core component of text understanding, as demonstrated in the context of the MUC conference series.
summarisation	T&M	We have chosen to work with law reports for three main reasons: (a) the existence of manual summaries means that we have evaluation material for the final summarisation system; (b) the existence of differing target audiences allows us to explore the issue of tailored summaries; and (c) the texts have much in common with the academic papers that T&M worked with, while remaining challengingly different in many respects.
summarisation process	T&M	This is a key part of the summarisation process and our work can bethought of as a test of portability of the T&M approach to anew domain.
summarization	DUC	A total of 10 different automatic summarization systems submitted their summaries to DUC.
ontology construction	UMLS	To avoid the reiteration in ontology construction, the algorithm of ontology merge (UMLS http://umlsks.nlm.nih.gov/) ( and ontology alignment (Vossen and Peters 1997))) were invested.
Segmentation	Mandarin Broadcast News	Combining Prosodic and Text Features for Segmentation of Mandarin Broadcast News
POS	GENIA V3.02p	For POS, all the POS taggers are trained on the training data with POS imported from the corresponding GENIA V3.02p with POS annotated.
Information extraction (IE)	MEDLINE	Information extraction (IE) in the biomedical domain is now regarded as an essential technique for utilizing information contained in archived journal articles and abstract collections such as MEDLINE.
Bio-Entity Recognition Task	JNLPBA	Introduction to the Bio-Entity Recognition Task at JNLPBA
MT Evaluation	French Technolangue action	CESTA, the first European Campaign dedicated to MT Evaluation, is a project labelled by the French Technolangue action.
Dependency Grammar	Paninian Grammar Framework (PGF)	Among the recent activities in Dependency Grammar, () have established a computational approach to Indian languages which they call the Paninian Grammar Framework (PGF).
generative synchronous grammars	CFG	Mathematically, generative synchronous grammars share many good properties similar to their monolingual counterparts such as CFG or TAG (.
POS tagging of 542,543 Arabic words	Buckwalter Arabic Morphological Analyzer	This paper discusses several issues in Arabic orthography that were encountered in the process of performing morphology analysis and POS tagging of 542,543 Arabic words in three newswire corpora at the LDC during 2002-2004, by means of the Buckwalter Arabic Morphological Analyzer.
speech recognition	Section 5	The speech recognition experiments and results are reported in Section 5.
Term Extraction	Korean Corpora	Term Extraction from Korean Corpora via Japanese
Grammatical Relations	GETARUNS	 Table 2: Grammatical Relations produced by GETARUNS with Precision and Recall
Alignment of adjectives and nouns	DepGov	 Table 3: Alignment of adjectives and nouns by  means of the DepGov propagation
Classifying Spoken Chess Move Instructions Semi-Automatic Generation of Dialogue	GEMINI Project	5th SIGdial Workshop on Discourse and Dialogue Proceedings of the Workshop 5th SIGdial Workshop on Discourse and Dialogue Proceedings of the Workshop Edited by Published by the Association for Computational Linguistics SIGdial '04 5th SIGdial Workshop on Discourse and Dialogue Stochastic Language Generation in a Dialogue System: Toward a Domain Independent Generator Conversational Dialogue Management in the FASiL project The NICE Fairy-tale Game System Combining Acoustic Confidences and Pragmatic Plausibility for Classifying Spoken Chess Move Instructions Semi-Automatic Generation of Dialogue Applications in the GEMINI Project A View on Dialogue Move Taxonomies for Tutorial Dialogues
SLU	Boostexter tool	We have performed the SLU tests using the Boostexter tool).
summarization	FX Palo Alto Laboratory	In Section 5, we sketch the architecture of the PALSUMM system, a summarization system being developed at FX Palo Alto Laboratory that uses algorithms operating on discourse representations generated by a U-LDM parser to summarize written English prose texts.
DA tagging	ICSI-MR data	 Table 2: Maximal accuracy of DA tagging of the  ICSI-MR data that could be reached using a limited  number of tags per label.
predicate argument annotation	PropBank project	A standard for predicate argument annotation is provided in the PropBank project.
SRL	FrameNet	Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems:) and FrameNet ().
Semantic Role Labeling (SRL)	CoNLL 2004 shared task	This paper presents a system for Semantic Role Labeling (SRL) for the CoNLL 2004 shared task).
Question Answering (Q/A)	HLT-NAACL 2004 Conference in Boston MA	We welcome you to the Workshop of the Pragmatics of Question Answering (Q/A), held in conjunction with the HLT-NAACL 2004 Conference in Boston MA,.
Textual Q/A	ACL	This workshop continues the discussion on results of research in Textual Q/A that took place in Workshops at ACL,, and the AAAI Spring Symposium series.
Textual Q/A	AAAI Spring Symposium series	This workshop continues the discussion on results of research in Textual Q/A that took place in Workshops at ACL,, and the AAAI Spring Symposium series.
question answering	TREC QA track	Since 1999 the performance of question answering systems are measured in the TREC QA track.
Distribution of Agreement	ARG1	 Table 1: Distribution of Agreement by Connective, with  ARG1 and ARG2 Annotations Counted Independently
Distribution of Agreement	ARG1	 Table 2: Distribution of Agreement by Connective, with  ARG1 and ARG2 Annotations Counted Together
semantic representation	Penn Treebank	The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicateargument information, or semantic roles, to the syntactic structures of the Penn Treebank.
Spoken Language Understanding	HLT/NAACL 2004)	Preface This volume contains the papers accepted for presentation at the workshop on Spoken Language Understanding in Conversational Systems and Higher Level Linguistic Knowledge for Speech Processing organized at the Human Language Technology conference/North American chapter of the Association for Computational Linguistics annual meeting (HLT/NAACL 2004).
question answering	WordNet	Many NLP tasks, such as question answering, summarization, and machine translation could benefit from broad-coverage semantic resources such as WordNet) and EVCA (English Verb Classes and Alternations)).
summarization	WordNet	Many NLP tasks, such as question answering, summarization, and machine translation could benefit from broad-coverage semantic resources such as WordNet) and EVCA (English Verb Classes and Alternations)).
parse disambiguation	Redwoods corpus	We apply these ideas in the context of parse disambiguation for sentence analyses produced by a Head-driven Phrase Structure Grammar (HPSG), the grammar formalism underlying the Redwoods corpus ( ).
NEs	Microsoft Research Asia	NEs may not be well recognized, or only * The work was done while the first author was visiting Microsoft Research Asia.
Question Answering (QA)	TREC QA track	Ever since Question Answering (QA) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation metrics -as reflected by the TREC QA track.
Statistical Machine Translation	WFST	Efficient Decoding for Statistical Machine Translation with a Fully Expanded WFST Model
parsing	Brown corpus	These annotated corpora have led to high-level improvements for parsing and word sense disambiguation (WSD), on the same scale as previously occurred for Part of Speech tagging by the annotation of the Brown corpus and, more recently, the British National Corpus (BNC)).
parsing	British National Corpus (BNC))	These annotated corpora have led to high-level improvements for parsing and word sense disambiguation (WSD), on the same scale as previously occurred for Part of Speech tagging by the annotation of the Brown corpus and, more recently, the British National Corpus (BNC)).
Part of Speech tagging	Brown corpus	These annotated corpora have led to high-level improvements for parsing and word sense disambiguation (WSD), on the same scale as previously occurred for Part of Speech tagging by the annotation of the Brown corpus and, more recently, the British National Corpus (BNC)).
Part of Speech tagging	British National Corpus (BNC))	These annotated corpora have led to high-level improvements for parsing and word sense disambiguation (WSD), on the same scale as previously occurred for Part of Speech tagging by the annotation of the Brown corpus and, more recently, the British National Corpus (BNC)).
semantic representation	Penn Treebank	It takes a practical approach to semantic representation, adding a layer of predicate argument information, or semantic roles, to the syntactic structures of the Penn Treebank ( ).
NERs	PNCBL	 Table 4: Identification Performance  for 14 NERs by PNCBL
Boundary recognition	CoNLL004	Boundary recognition as mentioned in CoNLL004 does play a very important role in this system as well.
CoNLL 2005 Semantic Role Labeling shared task	English PropBank data	This paper presents a system for the CoNLL 2005 Semantic Role Labeling shared task), which is based on the current release of the English PropBank data ().
part-of-speech tagging of written Arabic	Arabic Treebank	We explore the application of memory-based learning to morphological analysis and part-of-speech tagging of written Arabic, based on data from the Arabic Treebank.
coreference resolution tasks	MUC-6	The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of several recent investigations (), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks.
coreference resolution tasks	MUC-7	The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of several recent investigations (), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks.
coreference resolution tasks	CoNLL'02 and CoNLL'03 shared tasks	The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of several recent investigations (), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks.
mention detection sys- tem	ACE'04 subtasks	 Table 3: Performance of the mention detection sys- tem including all ACE'04 subtasks
SOMs	Matlab Neural Network Toolbox	The SOMs were implemented using the Matlab Neural Network Toolbox.
word alignment evaluation	HLT/NAACL 2003 workshop	This year's shared task follows on the success of the previous word alignment evaluation that was organized during the HLT/NAACL 2003 workshop on "Building and Using Parallel Texts: Data Driven Machine Translation and Beyond" (Mihalcea and Ped-ersen, 2003).
alignment	IBM Model 4	The most widely used alignment model is IBM Model 4 (.
translation equivalence extraction	Princeton Wordnet	In) we described a translation equivalence extraction program called TREQ the development of which was twofold motivated: to help enriching the synsets of the Romanian wordnet () with new literals based on bilingual corpora evidence and to check the interlingual alignment of our wordnet against the Princeton Wordnet.
Translation	BTEC	 Table 2: Translation results with optimal reorder- ing constraints and window sizes for the BTEC  Japanese-to-English and Chinese-to-English devel- opment corpora. * Optimized for the NIST score.
Translation	NIST score	 Table 2: Translation results with optimal reorder- ing constraints and window sizes for the BTEC  Japanese-to-English and Chinese-to-English devel- opment corpora. * Optimized for the NIST score.
TM	Linguistic Data Consortium (LDC)	The parallel texts for the TM come from several Chinese-English parallel corpora, all available from the Linguistic Data Consortium (LDC).
SMT	EnglishFrench	We obtained the same training and test data used in (, and evaluated a number of SMT systems which use the Pharaoh decoder 1 against the Marker-Based EBMT system of, for French-English and EnglishFrench.
Seeding Pharaoh	EBMT	 Table 5: Seeding Pharaoh with Giza++ word and EBMT
Seeding Pharaoh	EBMT	 Table 6: Seeding Pharaoh with Giza++ word and EBMT
MT	ACL05 MT workshop data set	For BLEU and HWCM, in order to avoid assigning zero scores to individual  sentences, when precision for n-grams of a particular length is zero we replace it with an epsilon value of 10 −3 . We choose E14 and E15 as two representative MT systems in the ACL05 MT workshop data set, which have relatively high human scores and low human scores respectively.
MT	ACL05 MT workshop test set	To answer this question, we used our syntactic metrics and BLEU to evaluate all the human-scored MT systems (E9 E11 E12 E14 E15 E17 E22) in the ACL05 MT workshop test set, and computed the correlation with human overall judgments.
summaries of business meetings	ICSI Meeting Corpus	The research below explores schemes for evaluating automatic summaries of business meetings, using the ICSI Meeting Corpus (Janin et al., 2003).
SMT	NIST metric	Ina discriminative rerank-ing experiment for phrase-based SMT we show that the NIST metric is more sensitive than BLEU or F-score despite their incorporation of aspects of fluency or meaning adequacy into MT evaluation.
MT	DARPA TIDES research program	BLEU and the closely related NIST metric) have been extensively used for comparative evaluation of the various MT systems developed under the DARPA TIDES research program, as well as by other MT researchers.
MT evaluation	Chinese portion of the Tides 2003 dataset	To provide a point of comparison however, table 1 shows the system level correlation between human judgments and various MT evaluation algorithms and sub components of METEOR over the Chinese portion of the Tides 2003 dataset.
MT evaluation	NIST	We then computed the Pearson correlation between these system level human judgments and the system level scores for each algorithm; these numbers are presented in Observe that simply using Recall as the MT evaluation metric results in a significant improvement in correlation with human judgment over both the BLEU and the NIST algorithms.
extraction of lexical semantic relations	WordNet	On the other hand, the only work on the extraction of lexical semantic relations we are aware of has concentrated on the type of relations found in WordNet: hyponymy and meronymy).
SORT	RANDOM	The four-fold reduction in efficiency for SORT (see) is a result of the mean number of relations being over 65 times that of RANDOM.
MT	NIST	Consequently, here we employ multiple references to evaluate MT systems like BLEU () and NIST).
Asymmetric textual entailment recognition (RTE)	Pascal Recognising Textual Entailment Challenge Corpus	Asymmetric textual entailment recognition (RTE) datasets, in particular the Pascal Recognising Textual Entailment Challenge Corpus (), provide testbeds that abstract over many tasks, including information retrieval, comparable documents, reading comprehension, question answering, information extraction, machine translation, and paraphrase acquisition.
NER	GENIA genes	 Table 2. Results of creating dictionary from a single database for  NER of GENIA genes and proteins.
NER	GENIA genes	 Table 3. Results of creating dictionary from a combination of two  databases for NER of GENIA genes and proteins.
sys-tem evaluation	Wisconsin	From the perspective of sys-tem evaluation, a number of these corpora (Wisconsin, GENETAG) are very well designed, with large numbers of both positive and negative examples for system training and testing.
sys-tem evaluation	GENETAG	From the perspective of sys-tem evaluation, a number of these corpora (Wisconsin, GENETAG) are very well designed, with large numbers of both positive and negative examples for system training and testing.
Smad signaling pathway	Medline	While some moderate-scale interaction maps have been created, such as for the purified TNFa/NFKB protein complex () and the proteins involved in the human Smad signaling pathway), the bulk of known human protein interaction data derives from individual, smallscale experiments reported in Medline.
SSN function parsers	PTB	All SSN function parsers were trained on sections 2-21 from the PTB and validated on section 24.
tagger	PTB	Trained on section 2-21, the tagger reaches a performance of 95.8% on the test set (section 23) of the PTB using our new tag set.
parsing	PTB	Both parsing results taking function labels into account in the evaluation (FLABEL) and results not taking them into account in the evaluation (FLABEL-less) are reported in, which shows results on the test set, section 23 of the PTB.
parsing function labels	FLABEL column	Parsing results outputting function labels (FLA-BEL columns) reported in indicate that parsing function labels is more difficult than parsing bare phrase-structure labels (compare the FLABEL column to the FLABEL-less column).
parsing function labels	FLABEL-less	Parsing results outputting function labels (FLA-BEL columns) reported in indicate that parsing function labels is more difficult than parsing bare phrase-structure labels (compare the FLABEL column to the FLABEL-less column).
parsing	Penn treebank (Section 23)	The precision, recall and average parsing time for the Penn treebank (Section 23) were 87.85%, 86.85%, and 360 ms, respectively.
parsing	Penn treebank	The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank.
beam thresholding	Penn treebank	In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing fora large-scale corpora, the Penn treebank.
parsing	Penn Treebank corpus	We ran parsing experiments using the Penn Treebank corpus, which is widely used for evaluating parsing algorithms.
parsing disfluent sentences	Switchboard corpus	This paper describes our effort on the task of edited region identification for parsing disfluent sentences in the Switchboard corpus.
parsing disfluent sentences	Switchboard corpus	In this paper we describe our effort towards the task of edited region identification with the intention of parsing disfluent sentences in the Switchboard corpus.
bracketing	Monroe corpus	We tested bracketing accuracy on the Monroe corpus, which contains collaborative emergency-management dialogues.
SURGE surface realizer	Penn TreeBank	We analyze data from an earlier wide coverage experiment on the FUF/SURGE surface realizer with the Penn TreeBank in order to empirically classify the sources of errors and describe their frequency and distribution.
coreference resolution	MUC-7 scoring program	Our results were evaluated using the MUC scoring program which reports recall, precision and F-measure, where the F-measure is defined as the harmonic mean of precision and recall: presents the results of our coreference resolution system on the outputs of both the parsing-based and heuristic-based entity detection systems, as measured by the MUC-7 scoring program.
word segmentation task	Microsoft Research (MSR) closed testing track	We participate in the word segmentation task at the Microsoft Research (MSR) closed testing track.
NER	MUC/DUC	Much of the NER research was pioneered in the MUC/DUC and Multilingual Entity Task (MET) evaluations, as a result of which significant progress has been made and many NER systems of fairly high accuracy have been constructed.
NER task	MSRA corpus	 Table 1: Official results in the closed test of the  NER task on MSRA corpus.
NER task	CITYU corpus	 Table 4: Official results in the closed test of the  NER task on CITYU corpus.
NER	MSRA Open test  MSRA NER	 Table 6 The NER performance in MSRA Open test  MSRA NER  Precision Recall  F Score  PER  93.68%  86.37%  89.87  LOC  85.50%  59.67%  70.29  ORG  75.87%  47.48%  58.41  Overall  86.97%  65.56%  74.76
NER	Chinese Peoples' Daily  MSRA NER	 Table 7 The NER test in Chinese Peoples' Daily  MSRA NER  Precision Recall  F Score  CPN  93.56  90.96  92.24  FPN  90.42  86.47  88.40  LOC  91.94  90.52  91.22  ORG  88.38  84.52  86.40  Overall  91.35  88.85  90.08
POS tagging task	Penn Chinese Treebank (CTB)	Furthermore, to evaluate our reranking method's impact on the POS tagging task, we also performed 10-fold cross-validation tests on the 250k Penn Chinese Treebank (CTB) ().
segmentation	SIGHAN Bakeoff 2006 dataset	We evaluated our system's segmentation results on the SIGHAN Bakeoff 2006 dataset.
POS tagging	Penn Chinese Treebank (CTB 250k)	To evaluate our reranking method's impact on the POS tagging part, we also performed 10-fold cross-validation tests on the 250k Penn Chinese Treebank (CTB 250k).
POS tagging	CTB 250k	The CRF model for POS tagging is trained on CTB 250k in all the experiments.
word segmentation task	UPUC Chinese Treebank data	The preliminary experimental result show that in the word segmentation task, our method can achieve 91.00 in F rate for the UPUC Chinese Treebank data, while it at-tends 78.76 F rate for the Microsoft Chinese named entity recognition task.
NER	Microsoft Research (MSRA)	Our systems participated in the NER tests on open and closed tracks of Microsoft Research (MSRA).
Lexical Chains	WordNet	One common point of all these works is that Lexical Chains are built using WordNet as the standard linguistic resource.
IE	MUC-4 training data	Section 3 describes how we create a baseline IE system from the MUC-4 training data.
IE	MUC-4 test set	 Table 3: Performance of new IE patterns on MUC-4 test set
Machine Translation	Czech Republic GA405/06/0589	Capturing of valency is profitable in Machine Translation, Information Extraction and Question Answering since it enables the machines to correctly recognize types of * The research reported in this paper has been partially supported by the grant of Grant Agency of the Czech Republic GA405/06/0589, the project of the Information Society No. 1ET101470416, and the grant of the Grant Agency of the Charles University No. 372/2005/A-INF/MFF.
Machine Translation	Charles University No. 372/2005/A-INF/MFF	Capturing of valency is profitable in Machine Translation, Information Extraction and Question Answering since it enables the machines to correctly recognize types of * The research reported in this paper has been partially supported by the grant of Grant Agency of the Czech Republic GA405/06/0589, the project of the Information Society No. 1ET101470416, and the grant of the Grant Agency of the Charles University No. 372/2005/A-INF/MFF.
Question Answering	Czech Republic GA405/06/0589	Capturing of valency is profitable in Machine Translation, Information Extraction and Question Answering since it enables the machines to correctly recognize types of * The research reported in this paper has been partially supported by the grant of Grant Agency of the Czech Republic GA405/06/0589, the project of the Information Society No. 1ET101470416, and the grant of the Grant Agency of the Charles University No. 372/2005/A-INF/MFF.
Question Answering	Charles University No. 372/2005/A-INF/MFF	Capturing of valency is profitable in Machine Translation, Information Extraction and Question Answering since it enables the machines to correctly recognize types of * The research reported in this paper has been partially supported by the grant of Grant Agency of the Czech Republic GA405/06/0589, the project of the Information Society No. 1ET101470416, and the grant of the Grant Agency of the Charles University No. 372/2005/A-INF/MFF.
summarisation	Embra system	Next, Section 3 contains a discussion of related work using SVD for summarisation and a description of the sentence selection component in the Embra system.
parse	British National Corpus (henceforward BNC	This procedure becomes less reliable as the grammar gets larger, and is especially difficult when the grammar is developed in a distributed manner., among many others, for instance, have investigated the main causes of parse failure, parsing a random sample of 20,000 strings from the written component of the British National Corpus (henceforward BNC) using the English Resource Grammar), a broad-coverage precision HPSG grammar for English.
parse	English Resource Grammar	This procedure becomes less reliable as the grammar gets larger, and is especially difficult when the grammar is developed in a distributed manner., among many others, for instance, have investigated the main causes of parse failure, parsing a random sample of 20,000 strings from the written component of the British National Corpus (henceforward BNC) using the English Resource Grammar), a broad-coverage precision HPSG grammar for English.
answer selection	SGT Blackwell data	 Table 1: Comparison of three different algorithms for answer selection on SGT Blackwell data. Each  performance number is given in percentages.
dialogue management	JASPIS	The multi-agent approach to dialogue management itself is not new: JASPIS) is a multiagent framework for dialogue systems which allows for implementations of several agents for the same tasks, varying from input interpretation and output presentation to dialogue management.
Binding Theory)	LTAG literature	Binding Theory) is an issue at the interface of syntax and semantics which has previously been avoided in the LTAG literature.
identification of semantic role labels	LTAG derivation tree	In this paper we compare two models for the identification of semantic role labels in a parse tree: A model that uses a path in the parse tree (or the derived tree in TAG terminology) and various associated features related to this, and we compare this model with a model that converts the syntactic parse tree into a Lexicalized Tree-Adjoining Grammar (LTAG) derivation tree and uses features extracted from the elementary trees and the LTAG derivation tree.
Extracting Syntactic Features	Korean Treebank	Extracting Syntactic Features from a Korean Treebank
paraphrase acquisition	PASCAL Recognizing Textual Entailment (RTE) Challenge corpus	The second corpus is the paraphrase acquisition subset of the PASCAL Recognizing Textual Entailment (RTE) Challenge corpus ().
MT	NIST	We use the same training corpus, 138.7M words of parallel Chinese-English data released by LDC, in order to train several statistical-based MT systems: • PBMT, a strong state of the art phrase-based system that implements the alignment template model (; this is the system ISI has used in the 2004 and 2005 NIST evaluations.
SPMT	NIST development corpus	As development data for the SPMT systems, we used the sentences in the 2002 NIST development corpus that are shorter than 20 words; we made this choice in order to finish all experiments in time for this submission.
phrasetable smoothing	WMT06 shared task	In order to measure the benefit of phrasetable smoothing for relatively small corpora, we used the data made available for the WMT06 shared task).
parser evaluation	Penn Treebank	For parser evaluation and for the machine translation experiments reported here, we used an automatic POS tagger () trained on sections 02-21 of the Penn Treebank.
machine translation	Penn Treebank	For parser evaluation and for the machine translation experiments reported here, we used an automatic POS tagger () trained on sections 02-21 of the Penn Treebank.
parsing	Negra	The experiments also show that there is a big difference in parsing performance, when trained on the Negra and on the TüBa-D/Z treebanks.
parsing	TüBa-D/Z treebanks	The experiments also show that there is a big difference in parsing performance, when trained on the Negra and on the TüBa-D/Z treebanks.
Semantic Role Labeling (SRL)	PropBank	Automatic Semantic Role Labeling (SRL) systems, made possible by the availability of PropBank (, and encouraged by evaluation efforts in, have been shown to accurately determine the argument structure of verb predicates.
SRL	NomBank (NomBank.0.8)	Our implemented SRL system and experiments are based on the September 2005 release of NomBank (NomBank.0.8).
parsing	HPSG grammar	We evaluated the speed and accuracy of parsing with extremely lexicalized models by using Enju 2.1, the HPSG grammar for English ( ).
boundary recognition	Brown corpus	For consistency, the performance of PARA for boundary recognition is tested using the official evaluation script from  Experimental results were obtained for part of the Brown corpus (the part provided by) and for Wall Street Journal (WSJ) Sections 21, 23, and 24 using different training data sets (WSJ 21, WSJ 15 to 18, and WSJ 02 to 21) shown in.
boundary recognition	Wall Street Journal (WSJ) Sections 21	For consistency, the performance of PARA for boundary recognition is tested using the official evaluation script from  Experimental results were obtained for part of the Brown corpus (the part provided by) and for Wall Street Journal (WSJ) Sections 21, 23, and 24 using different training data sets (WSJ 21, WSJ 15 to 18, and WSJ 02 to 21) shown in.
boundary recognition	WSJ	For consistency, the performance of PARA for boundary recognition is tested using the official evaluation script from  Experimental results were obtained for part of the Brown corpus (the part provided by) and for Wall Street Journal (WSJ) Sections 21, 23, and 24 using different training data sets (WSJ 21, WSJ 15 to 18, and WSJ 02 to 21) shown in.
boundary recognition	WSJ	For consistency, the performance of PARA for boundary recognition is tested using the official evaluation script from  Experimental results were obtained for part of the Brown corpus (the part provided by) and for Wall Street Journal (WSJ) Sections 21, 23, and 24 using different training data sets (WSJ 21, WSJ 15 to 18, and WSJ 02 to 21) shown in.
boundary recognition	WSJ	For consistency, the performance of PARA for boundary recognition is tested using the official evaluation script from  Experimental results were obtained for part of the Brown corpus (the part provided by) and for Wall Street Journal (WSJ) Sections 21, 23, and 24 using different training data sets (WSJ 21, WSJ 15 to 18, and WSJ 02 to 21) shown in.
semantic classification	TS1	Figure 3: Learning curves of bootstrapping methods for semantic classification on TS1.
alignment	Chinese-English newswire corpus	We tested the performance of our heuristics for alignment on a Chinese-English newswire corpus.
translation from German to English	Europarl data set	We performed experiments on translation from German to English on the Europarl data set.
translation from German to English	Europarl corpus	We applied the approach to translation from German to English, using the Europarl corpus () for our training data.
WSD	WordNet 2	For example, one performs WSD in information technology related texts using WordNet 2 as sense inventory.
sense disambiguation	SENSEVAL-3 data	Section 3 will provide experimental results of this algorithm for sense disambiguation on SENSEVAL-3 data.
duplicate detection	MP3 Players reviews	This filtering was performed before duplicate detection and discarded 45.7% of the MP3 Players reviews and 32.7% of the Digital Cameras reviews.
extraction of synonyms	Macquarie, Roget's and Moby thesauri	To reduce the problem of limited coverage, our evaluation of the extraction of synonyms combines three electronic thesauri: the Macquarie, Roget's and Moby thesauri.
NER	MUC6	There have been a number of conferences aimed at evaluating NER systems, for example, MUC6, MUC7, CoNLL2002 and CoNLL2003, and ACE (automatic content extraction) evaluations.
NER	MUC7	There have been a number of conferences aimed at evaluating NER systems, for example, MUC6, MUC7, CoNLL2002 and CoNLL2003, and ACE (automatic content extraction) evaluations.
parsing	Penn Treebank Wall Street Journal corpus	To perform empirical evaluations of the proposed methods, we considered the task of parsing the Penn Treebank Wall Street Journal corpus).
clustering	CTILC	The data for the clustering were first extracted from a fragment of CTILC (14 million word).
Size	Catalan corpus	 Table 1: Size of the Catalan corpus
answer assessment	German questions from QA@CLEF 2004	This is in part due to the too optimistic assumption during answer assessment that all German questions from QA@CLEF 2004 have a correct answer on the web (therefore the column labelled right empty answers contains 0.0 for InSicht-W3).
answer assessment	InSicht-W3	This is in part due to the too optimistic assumption during answer assessment that all German questions from QA@CLEF 2004 have a correct answer on the web (therefore the column labelled right empty answers contains 0.0 for InSicht-W3).
Text Retrieval Conference (TREC)	NTCIR	QA has become a popular task in the NL Processing (NLP) research community in the framework of different international ODQA evaluation contests such as: Text Retrieval Conference (TREC) for English, Cross-Lingual Evaluation Forum (CLEF) for European languages, and NTCIR for Asian languages.
Name discrimination	con-1 Search conducted January texts	Name discrimination takes some number of contexts that include an ambiguous name, and divides them into groups or clusters, where the con-1 Search conducted January texts in each cluster should ideally refer to the same underlying entity (and each cluster should refer to a different entity).
WSD evaluation	TWA dataset	For coarse-grained WSD evaluation, we used TWA dataset, which is a binarily sense-tagged corpus drawn from the British National Corpus (BNC), for 6 nouns.
WSD evaluation	British National Corpus (BNC)	For coarse-grained WSD evaluation, we used TWA dataset, which is a binarily sense-tagged corpus drawn from the British National Corpus (BNC), for 6 nouns.
translation of prepositions	HPSG	The accepted papers include proposals for the extraction of prepositional phrases from corpora, creation of a preposition ontology, disambiguation of verb particle sequences, translation of prepositions, interpretation of prepositions for iii practical applications, such as question answering systems, quantitative analyses of prepositions and prepositional phrases, as well as analyses within formal grammatical frameworks such as HPSG, LFG and OT.
question answering	HPSG	The accepted papers include proposals for the extraction of prepositional phrases from corpora, creation of a preposition ontology, disambiguation of verb particle sequences, translation of prepositions, interpretation of prepositions for iii practical applications, such as question answering systems, quantitative analyses of prepositions and prepositional phrases, as well as analyses within formal grammatical frameworks such as HPSG, LFG and OT.
adaptive extraction and mining from texts	NIST	The increasing interest of adaptive extraction and mining from texts is also demonstrated by several recent initiatives: • The Automatic Content Extraction program was started a few years ago by NIST (www.itl.nist.gov/iad/894.01/tests/ace).
Bio-entity Recognition Shared Task	JNLPBA 2004	For example, in the Bio-entity Recognition Shared Task at JNLPBA 2004 () the best performing system obtained a considerable performance improvement adopting domain specific hacks.
Anaphora Resolution	Susanne corpus	In this paper we will present an evaluation of current state-of-the-art algorithms for Anaphora Resolution based on a segment of Susanne corpus (itself a portion of Brown
parsing task	PropBank	We achieve promising results both on the simple parsing task, where the accuracy of the parser is measured on the standard Parseval measures, and also on the parsing task where the more complex labels of PropBank are taken into account.
parsing task	PropBank	We achieve promising results both on the simple parsing task, where the accuracy of the parser is measured on the standard Parseval measures, and also on the parsing task where the more complex labels of PropBank are taken into account.
SSN parser	PropBank	Our extended semantic role SSN parser was trained on sections 2-21 and validated on section 24 from the PropBank.
parsing task	PropBank labels	To evaluate the first parsing task, we compute the standard Parseval measures of labelled recall and precision of constituents, taking into account not only the 33 original labels but also the 580 newly introduced PropBank labels.
SSN parser	PTB data sets	The third line of gives the performance on the simpler PTB parsing task of the original SSN parser, that was trained on the PTB data sets contrary to our SSN model trained on the PropBank data sets.
SSN parser	PropBank data sets	The third line of gives the performance on the simpler PTB parsing task of the original SSN parser, that was trained on the PTB data sets contrary to our SSN model trained on the PropBank data sets.
parsing process	Johnson98	Among them, we can underline three proposals: -Implementing recovering mechanisms, triggering specific treatments in case of error (cf.) -Controlling the parsing process by means of probabilistic information (cf. [Johnson98]) -Controlling deep parsers by means of shallow parsing techniques (cf. The last kind of control mechanism consists in adapting the system to the material to be parsed.
labeling	EFE 2002 newspaper's articles	It was the TALP 3 research group the one that was in charge of labeling EFE 2002 newspaper's articles for the Spanish version, in which 106,473 different named entities were dealt with.
indexing biomedical scientific publications	Medline	MeSH, an ontology within UMLS, is heavily used for indexing biomedical scientific publications, e.g. Medline . Hospitals, medical practices and biomedical research increasingly rely on the UMLS, or a subset ontology within it, to index and retrieve relevant information.
clustering word senses	SEGR goldstandard	Previous research on clustering word senses has focused on comparison to the SEGR goldstandard.
WSD	WordNet	Approaches to WSD as an applicationindependent task usually apply standardised sense repositories, such as WordNet.
Entity Recognition English NER	CoNLL-2003 shared task 2	Named Entity Recognition English NER experiments were carried out on the CoNLL-2003 shared task 2 . This benchmark is based on texts from the Reuters Corpus which were manually annotated with parts-of-speech, chunk tags, and named entity categories.
Entity Recognition English NER	Reuters Corpus	Named Entity Recognition English NER experiments were carried out on the CoNLL-2003 shared task 2 . This benchmark is based on texts from the Reuters Corpus which were manually annotated with parts-of-speech, chunk tags, and named entity categories.
SRL	CoNLL 2005 Shared Task data	Since the aim of this study was to design areal SRL system we adopted the Charniak parse trees from the CoNLL 2005 Shared Task data (available at www.lsi.upc.edu/∼srlconll/).
question answering	CLEF 2006	Furthermore, finding similar content acrose different languages can form the basis for multilingual summarization and question answering support for Wikipedia; at present the latter task is being developed into a pilot for CLEF 2006 ).
SSN parsing	WSJ corpus	To evaluate the pure porting scenario (transferring), described in section 3.1, we trained the SSN parsing model on the WSJ corpus.
SSN parser	Brown corpus sections	For the sake of comparison, we also trained the SSN parser on only training data from one of the Brown corpus sections (section P), producing a "SSN-Brown" model.
relevancy recognition	TREC data	In this paper we propose a data driven approach for the task of relevancy recognition and evaluate it on two data sets: the TREC data and the HandQA data.
relevancy recognition	HandQA data	In this paper we propose a data driven approach for the task of relevancy recognition and evaluate it on two data sets: the TREC data and the HandQA data.
question answering task QAC	NTCIR	In question answering task QAC of NTCIR ()(, interactive use of question answering is proposed as one of evaluation task called Information Access Dialogue (IAD) task, which was called subtask3 in QAC1,2.
question answering task QAC	QAC1,2	In question answering task QAC of NTCIR ()(, interactive use of question answering is proposed as one of evaluation task called Information Access Dialogue (IAD) task, which was called subtask3 in QAC1,2.
question answering	QAC2	As for core QA system which is our main question answering system, we have integrated previous systems modules which are developed for QAC2.
Translation	EUTRANS cor- pus	 Table 3: Translation experiments for EUTRANS cor- pus using a generalized stack algorithm with differ- ent values of G and a fixed value of S = 2 12
translation	NIST scores	The outcome was that the confidence measures did not result in improvements of the translation quality measured with the BLEU and NIST scores.
Translation	NIST	 Table 3: Translation Results using Refined Segmentation Methods on NIST task
Translation	FBIS Training	 Table 4: Translation Results on Sentence Alignment Task with FBIS Training Corpus
Sentence Alignment	FBIS Training	 Table 4: Translation Results on Sentence Alignment Task with FBIS Training Corpus
SMT	NIST MT Evaluation Workshop	Experiments were performed with our phrasebased SMT system ) on two large-vocabulary tasks: the translation of European Parliament Plenary Sessions from Spanish to English, and the translation of news agencies from Chinese to English, according to the setup defined by the 2005 NIST MT Evaluation Workshop.
translation of European Parliament Plenary Sessions from Spanish to English	NIST MT Evaluation Workshop	Experiments were performed with our phrasebased SMT system ) on two large-vocabulary tasks: the translation of European Parliament Plenary Sessions from Spanish to English, and the translation of news agencies from Chinese to English, according to the setup defined by the 2005 NIST MT Evaluation Workshop.
translation of news agencies from Chinese to English	NIST MT Evaluation Workshop	Experiments were performed with our phrasebased SMT system ) on two large-vocabulary tasks: the translation of European Parliament Plenary Sessions from Spanish to English, and the translation of news agencies from Chinese to English, according to the setup defined by the 2005 NIST MT Evaluation Workshop.
translation of European languages	Europarl corpus	As then, we concentrated on the translation of European languages and the use of the Europarl corpus for training.
SMT	SRILM	For their part, language models currently in use in SMT systems can be trained using packages such as SRILM) and the CMU-SLM toolkit).
Translation	Fr-En '06 Shared Task 'Development Set'	 Table 1: Translation results (IBM BLEU) for each system on the Fr-En '06 Shared Task 'Development Set' (used for MER
PMI	HPRD	The comparative results for PMI and HG are shown in, together with the scores for three human curated databases: HPRD, BIND and Reactome.
SRL	BioProp	To compare the effects of using biomedical training data vs. using newswire data, we train our SRL system on 30 randomly selected training sets from BioProp (g 1 ,.., g 30 ) and 30 from PropBank (w 1 ,.., w 30 ), each having 1200 training PAS's.
SRL	PropBank	To compare the effects of using biomedical training data vs. using newswire data, we train our SRL system on 30 randomly selected training sets from BioProp (g 1 ,.., g 30 ) and 30 from PropBank (w 1 ,.., w 30 ), each having 1200 training PAS's.
NER	YAPEX test cor- pus	 Table 1: Performance of the NER systems on the  mouse evaluation corpus and the YAPEX test cor- pus.
Recognizing Nested Named Entities	GENIA	Recognizing Nested Named Entities in GENIA corpus
identification of biomedical terms in research publications	Perceptron HMM algorithm	We propose a novel approach to the identification of biomedical terms in research publications using the Perceptron HMM algorithm.
POS tagging	Penn Treebank	Many machine-learning and statistical techniques employed for POS tagging train a model on an annotated corpus, such as the Penn Treebank ().
novelty detection	TREC 2004 novelty track	We discuss several feature sets for novelty detection at the sentence level, using the data and procedure established in task 2 of the TREC 2004 novelty track.
semantics construction	USR	This simplifies semantics construction, and current algorithms support the efficient enumeration of readings from an USR.
redundancy elimination	USRs	To our knowledge, this is the first algorithm in the literature for redundancy elimination on the level of USRs.
Type Theory	King's College London	In 2005, the 2 nd Workshop on Lambda-Calculus, Type Theory, and Natural Language took place at King's College London ().
Size	European Constitution corpus	 Table 1: Size of the European Constitution corpus.
SR	WCG	gives an overview of our experimental results of evaluating SR measures based on the WCG on three German datasets.
SR	German datasets	gives an overview of our experimental results of evaluating SR measures based on the WCG on three German datasets.
Parsing	WSJ10	 Table 1: Parsing results on WSJ10
identification of non-local depenencies	Penn Treebank data	We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using Penn Treebank data) and semantic role labeling (using Proposition Bank data).
identification of non-local depenencies	Proposition Bank data	We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using Penn Treebank data) and semantic role labeling (using Proposition Bank data).
Translation	NIST	 Table 5: Translation performance for the Chinese-English IWSLT task  WER[%] PER[%] NIST BLEU[%]  IWSLT04  baseline  47.3  38.2  7.78  39.1  source reordering  46.3  37.2  7.70  40.9  IWSLT05  baseline  45.0  37.3  7.40  41.8  source reordering  44.6  36.8  7.51  42.3  IWSLT06  baseline  67.4  50.0  6.65  22.4  source reordering  65.6  50.4  6.46  23.3  source reordering+non-monotone decoder  66.5  50.3  6.52  22.4
Translation	IWSLT 2004 test set	 Table 6: Translation performance of reordering  methods on IWSLT 2004 test set  WER PER NIST BLEU  [%]  [%]  [%]  Baseline 47.3 38.2 7.78  39.1
SSST	NAACL-HLT 2007 / AMTA Workshop	Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Binarization, Synchronous Binarization, and Target-side Binarization *
TR	PCEDT 1.0	 Table 2: Reordering accuracy for TR trees on development data from PCEDT 1.0. We include performance on the interior nodes
MT	Europarl	Moreover, when designing a statistical MT system, the need for large amounts of training data limits the researcher to collections of parallel corpora such as Europarl (, which provides only one reference, namely the target text; and the cost of creating additional reference translations of the test set, usually a few thousand sentences long, is often prohibitive.
MT	NIST	Scores for sentences with reordered adjuncts in an ideal situation  In the first experiment, we attempted to determine whether the dependency-based measure is biased towards statistical MT output, a problem that has been observed for n-gram-based metrics like BLEU and NIST.
machine translation (MT)	Europarl corpus	We also conducted machine translation (MT) experiments on the Europarl corpus.
translation of news from Chinese to English	NIST MT Evaluation Workshop of 2006	The  The task chosen for our experiments is the translation of news from Chinese to English, as proposed by the NIST MT Evaluation Workshop of 2006.
translation	NIST	As in the previous experiment, the translation was scored using BLEU, NIST, GTM, TER, METEOR, and our labelled dependencybased method.
MT research	Linguistic Data Consortium (LDC)	A common source of translated data for MT research is the Linguistic Data Consortium (LDC), where an elaborate process is undertaken that involves translation agencies, detailed translation guidelines, and quality control processes ().
Mixture modelling	G. J.)	Mixture modelling is a popular approach for density estimation in many scientific areas (G. J.).
translation	2007 test set	The translation quality achieved on the 2007 test set is similar to that on the 2006 test set.
translation between Spanish and English	TC-STAR evaluation	A system with a similar architecture was successfully applied to the translation between Spanish and English in the framework of the 2007 TC-STAR evaluation.
tokenization	Moses SMT toolkit	We applied the tokenization proposed by the Moses SMT toolkit and the case was preserved.
SMT	RBMT	We incorporate phrases extracted from these alignments into the phrase table of the SMT system and use the open-source decoder Moses to find good combinations of phrases from SMT training data with the phrases derived from RBMT.
SMT training	RBMT	We incorporate phrases extracted from these alignments into the phrase table of the SMT system and use the open-source decoder Moses to find good combinations of phrases from SMT training data with the phrases derived from RBMT.
SMT	Europarl data	To better understand this behavior, we trained series of APE and SMT systems on the Europarl data, using increasing amounts of training data.
translation	SAMT toolkit	We begin by describing the probabilistic model of translation applied by the SAMT toolkit.
Implementation	GF	Implementation of the Arabic Numerals and their Syntax in GF
MT	Systran	Not translated sentences (4%) are mostly caused by specialized medical terms describing pain In order to compare our results with commercial MT systems output, we submitted the first 124 well analyzed sentences to Systran.
ASG	Buckwalter Arabic transliteration	All the internal processing of ASG is done with the Buckwalter Arabic transliteration -though of course ASG can take real Arabic script (in UTF-8 form) as input.
SG	ATB	But SG is a rule-based system, and there is no automatic training from the ATB.
anchoring	GTAA	The anchoring experiment is described in section 3: first the GTAA case (section 3.1) and then the GTT one (section 3.2), as a reusability test.
anchoring	GTT	The anchoring experiment is described in section 3: first the GTAA case (section 3.1) and then the GTT one (section 3.2), as a reusability test.
conversion	SF BioInfer annotation	In the evaluation of the conversion rules against the gold standard SF BioInfer annotation, we find a precision of 98.0% and a recall of 96.2%.
classification	ABTA system	It is seen that based on all three features the system achieves a classification accuracy of 87.5%, which is comparable to the results of the original ABTA system.
identification	BC2	For identification, it was sufficient in BC2 to report the ID, regardless of number of occurrences or name variations.
Relationship Extraction	GENIA	The majority of corpora consist of abstracts annotated for bio-entity recognition and Relationship Extraction, such as the GENIA () and the BioCreAtIvE corpora.
Relationship Extraction	BioCreAtIvE corpora	The majority of corpora consist of abstracts annotated for bio-entity recognition and Relationship Extraction, such as the GENIA () and the BioCreAtIvE corpora.
parsing	HPSG	For parsing with constraint-based grammars, such as HPSG, which do not possess an explicit context-free backbone,) have proposed an efficient packing algorithm based on feature structure subsumption only.
RTE	RTE-2	It is our first attempt to RTE and we have taken profit of an analysis of the approaches followed in previous challenges (see), and () for overviews of RTE-1 and RTE-2).
Answer Validation Exercise 4 (AVE) 2006 English data set	Microsoft Research Paraphrase Corpus 5 (MSRPC)	We performed the same experiment joining the Answer Validation Exercise 4 (AVE) 2006 English data set) and the Microsoft Research Paraphrase Corpus 5 (MSRPC) () to the previous corpora (RTE-1 and RTE-2) resulting a total of 8585 entailment pairs filtering pairs with a text or a hypothesis with more than 1 sentence.
RTE	WordNet	Most RTE systems incorporate some form of lexical paraphrasing, usually relying on WordNet to identify synonym, hypernym and hyponym relations among word pairs from text and hypothesis,).
RTE	Stanford system	In applying NatLog to RTE problems, we use alignments from the Stanford system as input to our entailment model.
data extraction	Alpino corpora	In section 3, we discuss the existing approaches to data extraction from Alpino corpora.
translation studies	LinES	Similar projects include Croco) which is aimed at building a GermanEnglish parallel treebank for translation studies, LinES an English-Swedish parallel treebank, and the Czech-English parallel dependency treebank builtin Prague (.
Coreference task on Message Understanding Conference (MUC)	MUC	For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of specification of annotating coreference relation have been discussed for several years.
Entity Detection and Tracking (EDT) task	MUC	For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of specification of annotating coreference relation have been discussed for several years.
NEs	MUC-7 specification	Although there are other classification systems for NEs, the results of our system are annotated NEs which are following MUC-7 specification.
parsing	Prolog	For parsing we have used a phrase-spotting grammar written in Prolog that pattern matches phrases to dialogue moves.
parse selection	Brown Corpus	For instance, reports that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus.
generative statistical parsers	Lalic, Université Paris 4 La Sorbonne. from the LA Times newspaper	Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results.) proceed as follows: sentences * Now affiliated to Lalic, Université Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker.
generative statistical parsers	WSJ Sections 2-21	Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results.) proceed as follows: sentences * Now affiliated to Lalic, Université Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker.
generative statistical parser	Brown seed data	They report a performance boost of 4.2% on WSJ Section 23 fora generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees.
generative statistical parser	WSJ parse trees	They report a performance boost of 4.2% on WSJ Section 23 fora generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees.
generative statistical parser	WSJ treebank trees	We retrain the first-stage generative statistical parser of Charniak and Johnson using combinations of BNC trees (parsed using the reranking parser) and WSJ treebank trees.
parsing	HPSG grammar	We evaluated the speed and accuracy of parsing by using Enju 2.1, the HPSG grammar for English ( ).
dependency parsing	CoNLL 2006 and 2007 shared tasks	Dependency treebanks are becoming available in many languages, and several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks.
human-like language generation	KTH	This paper describes a data collection with the goal of modelling more human-like language generation in DEAL, a spoken dialogue system developed at KTH.
role classification	AMIEVALSET	 Table 1: Confusion matrix for role classification on  AMIEVALSET;
Sentence retriever	Doctor Perez's NLG problem	Sentence retriever is based on the crosslanguage information retrieval techniques described in (), and is currently in use for Doctor Perez's NLG problem.
SMT	GIZA++-Toolkit	Therefore, fora given source sentence f J 1 and a given target sentence e I 1 a set of links (j, i) has to be found, which describes which source word f j is translated into which target word e i . Most SMT systems use the freely available GIZA++-Toolkit to generate the word alignment.
Translation	EN-ES	 Table 4: Translation results for EN-ES
Translation	CH-EN	 Table 5: Translation results for CH-EN
MERT learning	MT03	MT02 was used as a dev set for MERT learning, while MT03 and MT05 were used as our test sets.
MERT learning	MT05	MT02 was used as a dev set for MERT learning, while MT03 and MT05 were used as our test sets.
machine translation evaluation shared task	ACL WMT-08	This paper describes our submissions to the machine translation evaluation shared task in ACL WMT-08.
translation	Europarl	 Table 2. The translation model was trained on  the Europarl and the news-commentary data, aug- mented by parts of the dictionary. The LM was  trained on all the data, but the additional out-of- domain data has probably little impact given the  small improvements in perplexity (see
translation	news-commentary data	 Table 2. The translation model was trained on  the Europarl and the news-commentary data, aug- mented by parts of the dictionary. The LM was  trained on all the data, but the additional out-of- domain data has probably little impact given the  small improvements in perplexity (see
translation	Europarl	 Table 3. The translation model  was trained on the news-commentary, Europarl and  Hansard bitexts as well as parts of the dictionary.  The LM was again trained on all data.
translation	Hansard bitexts	 Table 3. The translation model  was trained on the news-commentary, Europarl and  Hansard bitexts as well as parts of the dictionary.  The LM was again trained on all data.
MT	WMT07 DevTest	 Table 1: English-to-Czech BLEU scores for syntax-based  MT on WMT07 DevTest.
Translation	NIST-BLEU	The experiments do not give evidence that genre-dependent confidence can improve over using the general confidence: Translation results (NIST-BLEU) using pc with different genre-specific language models for Es↔En systems shows experiments results in NIST-BLEU using pc score as an additional feature on phrase tables in Es↔En systems.
Translation	NIST-BLEU	The experiments do not give evidence that genre-dependent confidence can improve over using the general confidence: Translation results (NIST-BLEU) using pc with different genre-specific language models for Es↔En systems shows experiments results in NIST-BLEU using pc score as an additional feature on phrase tables in Es↔En systems.
Translation	NIST-BLEU	 Table 4: Translation results (NIST-BLEU) using gdsc with dif- ferent genre-specific language models for Es↔En systems
SMT	XRCE	The phrase-based SMT system MATRAX, developed at XRCE, is used as the baseline in the experiments.
parsing	Xerox Research Centre Europe	For parsing, we use the Xerox Incremental Parser XIP (A¨ıtA¨ıt-), which is a robust dependency parser developed at the Xerox Research Centre Europe.
SMT training	RBMT	Using this approach it is possible to employ a vanilla installation of the open-source decoder Moses 1 ( ) to find good combinations of phrases from SMT training data with the phrases derived from RBMT.
machine translation (MT)	DARPA GALE	Confusion network decoding has been the most successful approach in combining outputs from multiple machine translation (MT) systems in the recent DARPA GALE and NIST Open MT evaluations.
MT sentences	WMT 2006 and WMT 2007	For the experiments reported in this paper, we used human-evaluated MT sentences from past sharedtasks of the WMT 2006 and WMT 2007.
MT training	MT05	Having lexicon-based features reduced the MT training lexicon by 29.5%, reduced the MT test data OOV rate by 34.1%, and led to a 0.38 BLEU point gain on the test data (MT05).
MT	CTB standard	For MT, we found that it is preferred to have words slightly shorter than the CTB standard.
Translation	NIST MT benchmark tests	 Table 1: Decoder Comparison: Translation speed and  quality on the 2003 and 2005 NIST MT benchmark tests.
Translation	MT06 data	 Table 2: Translation results on the MT06 data. w is the distortion limit.
information extraction	MEDLINE	In this paper we explore some of these issues empirically in an information extraction task in the biomedical domain, the identification of proteinprotein interactions (PPI) mentioned in papers abstracts from MEDLINE, a large database of biomedical papers.
identification of proteinprotein interactions (PPI)	MEDLINE	In this paper we explore some of these issues empirically in an information extraction task in the biomedical domain, the identification of proteinprotein interactions (PPI) mentioned in papers abstracts from MEDLINE, a large database of biomedical papers.
PPI extraction	GENIA Treebank	In this section we present our PPI extraction experiments applying the dependency parsers trained with the different amounts of the GENIA Treebank in our PPI system.
parsing	WSJ Penn Treebank	We see that using domain-specific training data for the parsing component for the PPI extraction system produces superior results, compared to using training data from the WSJ Penn Treebank.
PPI extraction	WSJ Penn Treebank	We see that using domain-specific training data for the parsing component for the PPI extraction system produces superior results, compared to using training data from the WSJ Penn Treebank.
Dialogue Management	RavenClaw	It has the three usual main blocks: Language Understanding, through Phoenix parser and Helios confidence-based annotation module, Dialogue Management, through RavenClaw (, and Language Generation, through Rosetta.
entity extraction	CLEF IE system	We have previously reported entity extraction in the CLEF IE system ().
handling negation and uncertainty in biomedical texts	BioScope corpus	This article reports on a corpus annotation project that has produced a freely available resource for research on handling negation and uncertainty in biomedical texts (we call this corpus the BioScope corpus).
NER	JNLPBA-2004 data set	We evaluated performance of our NER on the standard JNLPBA-2004 data set.
TI	TE dataset	 Table 5: Results of TI on the TE dataset. All figures, except 'Avg. Rank', are percentages. There are four entity types  in the TE data, i.e., protein, gene, mRNAcDNA and GOMOP. The evaluation was carried out on all entity types.
TI	TE data	 Table 5: Results of TI on the TE dataset. All figures, except 'Avg. Rank', are percentages. There are four entity types  in the TE data, i.e., protein, gene, mRNAcDNA and GOMOP. The evaluation was carried out on all entity types.
TI	GOMOP	 Table 5: Results of TI on the TE dataset. All figures, except 'Avg. Rank', are percentages. There are four entity types  in the TE data, i.e., protein, gene, mRNAcDNA and GOMOP. The evaluation was carried out on all entity types.
TI	BioCreAtIvE GN test  dataset	 Table 7: Performance of TI with or without the automati- cally predicted species on the joint BioCreAtIvE GN test  dataset.
Subheading attachment	Anatomy" tree	Subheading attachment can be accomplished using indexing rules such as: If a main heading from the "Anatomy" tree and a "Carboxylic Acids" term are recommended for indexing, then the pair "[Carboxylic Acids]/pharmacology" should also be recommended.
Protein-Protein Interaction Extraction	AImed corpus	Protein-Protein Interaction Extraction: We used the AImed corpus, which was previously used for training protein interaction extraction systems in ().
Parse selection	German HPSG	Parse selection with a German HPSG grammar
dependency parsing	CoNLL shared tasks 2006, 2007	The focus on labeled dependencies also provides a direct link to recent work on dependency-based evaluation (e.g., and dependency parsing (e.g., CoNLL shared tasks 2006, 2007).
parsing	Negra	The goal of the following experiments is a comparison of parsing performance across different types of evaluation metrics for parsers trained on Negra (Ver. 2) and TüBa-D/Z (Ver. 2).
parsing	Stanford Parser	present the results shown in for the parsing performance of the unlexicalized model of the Stanford Parser ().
coreference resolution	MUC-6	There does not appear to be a single standard eval-uation metric in the coreference resolution community, so we opted to use three: MUC-6 (, CEAF, and B-CUBED (, which seem to be the most widely accepted metrics.
Question Answering (QA) domain	CLEF corpus)	In the first set of experiments we focus on the Question Answering (QA) domain (CLEF corpus).
summarization	Dagstuhl Seminar-that	Nevertheless, although the pioneering efforts on summarization go back to the work of and, it is only after the renaissance of summarization as a research area of great activity-following upon the Dagstuhl Seminar-that the first multi-document news summarization system, SUMMONS, makes its breakthrough (.
summarization	Dagstuhl Seminar-that	Nevertheless, although the pioneering efforts on summarization go back to the work of and, it is only after the renaissance of summarization as a research area of great activity-following upon the Dagstuhl Seminar-that the first multi-document news summarization system, SUMMONS, makes its breakthrough (.
Questions within a Context Question Answering (QA)	TREC competitions	1 Questions within a Context Question Answering (QA) systems have reached a high level of performance within the scenario originally described in the TREC competitions, and are ready to tackle new challenges as shown by the new tracks proposed in recent instantiations (Voorhees, 2004).
parsing	English grammar	A typical breakdown of parsing time of XLE components with the English grammar is Morphology (1.6%), Chart (5.8%) and Unifier (92.6%).
Information retrieval (IR)	Lucene	Information retrieval (IR) performance, provided by engines such as Lucene, places abound on overall system performance.
SRL	2006 question set	As observed in, the SRL approach gives the best results for all question sets on all evaluation metrics, with the exception of c@50 on the 2006 question set.
Affinity Measures	Graph Laplacian	Affinity Measures based on the Graph Laplacian
summarize biomedical literature	UMLS	We have applied our approach to summarize biomedical literature , taking advantages of free resources as UMLS.
parsing	written part of Talbanken05	All parsing experiments are performed using 10-fold cross-validation for training and testing on the entire written part of Talbanken05.
MRG transformation	GEOQUERY domain	We next tested our MRG transformation algorithm on SQL as the MRL for the GEOQUERY domain.
MRG transformation	CLANG domain	Finally, we tested our MRG transformation algorithm on the CLANG domain using its original MRG in which all the chief regions of the soccer field were in the form of numeric MR expressions which do not correspond to their meanings in the natural language.
MT evaluation	NIST script version 11b	For MT evaluation, we used BLEU measure () calculated by the NIST script version 11b.
SRL	MaltParser	For evaluation, we will mainly report our official SRL performance using MaltParser.
WSD task	EVALITA 2007 initiative	In order to evaluate the effectiveness of the proposed approach, experimental sessions were carried out on the dataset used for the WSD task in the EVALITA 2007 initiative, devoted to the evaluation of Natural Language Processing tools for Italian.
disambiguation	Alpino treebank	The disambiguation model is trained on the manually verified Alpino treebank (about 7100 sentences from newspaper texts).
question translation	Chi-41	If we wanted to make a model of question translation from Chi-41
paraphrase assessment	SEMCOR-PARA	The results for paraphrase assessment on SEMCOR-PARA are shown in.
MT hypotheses	NIST	Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf. BLEU (), NIST).
TER alignment	WMT09 system combination task	Experimental evaluation comparing the pair-wise and incremental TER alignment algorithms with the refined alignment algorithm on WMT09 system combination task is presented in Section 3.
Translation	News2008	 Table 1: Translation results [%] for the German- English language pair, News2008 dev-b.
Translation	News2008	 Table 2: Translation results [%] for the French- English language pair, News2008 dev-b.
Translation	News2008	 Table 3: Translation results [%] for the Spanish- English language pair, News2008 dev-b.
SMT	Europarl (EP) corpus	For the training of the SMT engines, we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (, which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus, which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde.
translation	NC/EP data sets	In order to get an idea how difficult the translation task maybe, we also calculated the language perplexity of the respective evaluation data sets according to 5-gram target language models trained on the NC/EP data sets.
SMT	EP corpus	Concerning the development sets, the newsdev2009 data taken from the same news sources as the evaluation set of the shared task was used for the tuning of the SMT engines, and the devtest2006 data taken from the EP corpus was used for system parameter optimization.
SMT	GALE DARPA Contract No. HR0011-06-C-0023	In non-European languages, such as Arabic, heavy effort has been put in identifying appropriate input representations to improve SMT quality (e.g.,) As a first step toward using morphology information in our French-English SMT system, this submission focused on studying the impact of * The author was partially funded by GALE DARPA Contract No. HR0011-06-C-0023.
SMT	GALE DARPA Contract No. HR0011-06-C-0023	In non-European languages, such as Arabic, heavy effort has been put in identifying appropriate input representations to improve SMT quality (e.g.,) As a first step toward using morphology information in our French-English SMT system, this submission focused on studying the impact of * The author was partially funded by GALE DARPA Contract No. HR0011-06-C-0023.
MT	VietnameseEnglish	Until now, in Vietnam, there are only four research groups working on MT for VietnameseEnglish (.
SMTs	Tst set	 Table 9. Evaluation of SMTs on the Tst set.
Tuning	Och's Minimum Error Training	Tuning is done for each experimental condition using Och's Minimum Error Training.
SMT	United Nation corpus	This work focused on the second issue, namely the adaptation of a Spanish-to-English phrasebased SMT system across two apparently close domains: the United Nation corpus and the Euro-pean Parliament corpus.
SMT	Euro-pean Parliament corpus	This work focused on the second issue, namely the adaptation of a Spanish-to-English phrasebased SMT system across two apparently close domains: the United Nation corpus and the Euro-pean Parliament corpus.
reordering	Hiero	This is possibly the most direct evidence of reordering performance so far, and again shows how Hiero has a slight advantage over the phrase-based system with regard to reordering performance.
translation task	WMT'08 evaluation	We performed the experiments on the translation task of the WMT'08 evaluation.
MERT	NIST MT06 data set	The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences.
MT	NIST MT evaluation datasets	 Table 5: MT experiments of different settings on various NIST MT evaluation datasets. We used both the BLEU and TER  metrics for evaluation. All differences between DE-Annotated and BASELINE are significant at the level of 0.05 with the  approximate randomization test in (
describing motion	NLG literature	There has been little work to our knowledge on describing motion in the NLG literature.
realisation task	OPENCCG	Over the past several years, a significant consensus has emerged over the definition of the realisation task, through the development of realisers such as REALPRO), ALETH-GEN,, FUF/SURGE (, HALO-GEN), YAG (), and OPENCCG).
realizational analysis	KATR	We represent our realizational analysis of SY in the KATR formalism.
Lexical Functional Grammar	Penn Arabic Treebank (ATB)	The paper first provides a brief overview of Lexical Functional Grammar, and the Penn Arabic Treebank (ATB).
translation task	EuroParl	Experiments fora state-of-the-art, voluminous translation task, such as the EuroParl, are reported.
SRL task	CoNLL'05 evaluator	Since the main topic of this paper is the assessment of the efficiency and accuracy of our linearization technique, we did not carryout an evaluation on the whole SRL task using the official CoNLL'05 evaluator.
20Newsgroups 3 newsgroup article classification	WebKB web page classification datasets	Other TC datasets we use are the 20Newsgroups 3 newsgroup article classification and the WebKB web page classification datasets.
NER	GENIA corpus	For NER, we use the publicly available GENIA corpus . Our features, based on those from (), are surface features such as the words in the named entity and two words on each side, suffix information, and positional information.
SEPA	WSJ20	Based on the analysis of SEPA performance with different assignments of its parameters given by Reichart and Rappoport (2007) (see Section 3), we ran the SEPA algorithm with sample size (SEPA parameter S) of 30% and 80%, and with 2 -10 committee members (N ) . The optimal parameters were N = 10,S = 80 for WSJ20, and Random selection is presented for reference as a dotted line.
parsing	MST	Our system uses the first approach since we saw better chance to improve the parsing speed and additionally, the MST had so far slightly better parsing results.
Mutation extraction	MMR database	We evaluated our systems in two ways: • Header classification: performance of different systems on predicting the classes of each column/row of the tables; • Mutation extraction: recall of our system over the subset of the hand-curated MMR database.
ML Algorithms	Mj.Cl.	 Table 4: Results for ML Algorithms -Micro-Averaged  FScores. Mj.Cl.: Majority Class. The best results per  column are given in bold.
text mining (TM)	MUC, TREC and ACE	The history of text mining (TM) shows that shared tasks based on carefully curated resources, such as those organized in the MUC, TREC and ACE () events, have significantly contributed to the progress of their respective fields.
event extraction	JULIELab Team from FSU Jena	We describe the approach to event extraction which the JULIELab Team from FSU Jena (Germany) pursued to solve Task 1 in the "BioNLP'09 Shared Task on Event Ex-traction".
Offical Event Extraction	JULIELab Team	 Table 3: Offical Event Extraction results on the shared task test data of the JULIELab Team. Approximate  Span Matching/Approximate Recursive Matching (columns 3-5). Event decomposition, Approximate Span Match- ing/Approximate Recursive Matching (columns 7-9).
event extraction shared task	BioNLP 2009	We submitted the output of our approach to the event extraction shared task at BioNLP 2009, where our methods suffered from low recall, although we were one of the few teams to provide answers for task 3.
event extraction shared task	BioNLP 2009	We present in this paper our techniques for the tasks 1 and 3 of the event extraction shared task at BioNLP 2009.
Tunable Domain-Independent Event Extraction	MIRA Framework	Tunable Domain-Independent Event Extraction in the MIRA Framework
question answering	CLEF	In it is suggested that information harvested from infoboxes can be used for question answering in CLEF.
opinion detection	English, Chinese and Japanese corpora	In this paper we present anew statistical approach to opinion detection and its' evaluation on the English, Chinese and Japanese corpora.
summarization evaluation	TAC	Further, we discuss the baselines traditionally used for summarization evaluation and suggest the revival of an old baseline to suit the current summarization task at TAC: the Update Summarization task.
summarization task	TAC	Further, we discuss the baselines traditionally used for summarization evaluation and suggest the revival of an old baseline to suit the current summarization task at TAC: the Update Summarization task.
summarization evaluation	Ngram statistics	ROUGE, an automated summarization evaluation package based on Ngram statistics, is found to be highly correlated with human evaluations (.
identification of the main words	Haiku	The only human judgement in compiling this set was in the identification of the main words of the human Haiku.
gen­ erating lyrics	Tra­la­ Lyrics	One of the recent attempts for automatically gen­ erating lyrics fora given melody is the Tra­la­ Lyrics system ( . This system uses the ABC notation for repre­ senting melody and the corresponding suite of tools for analyzing the melodies.
ASR	OGI and CMU Kids' corpora	The ASR system's acoustic model (AM) was trained using portions of the OGI and CMU Kids' corpora as well as a randomly selected sub-set of our own passage and word list data sets described in the previous section.
AM training	OGI, CMU Kids	Since the size of our own data set was too small for AM training, we had to augment it with the two mentioned corpora (OGI, CMU Kids), although they were not a perfect match in age range and accent.
replicate	Cambridge Learner Corpus errors	We describe initial attempts to replicate Cambridge Learner Corpus errors using GenERRate.
replicate	GenERRate	We describe initial attempts to replicate Cambridge Learner Corpus errors using GenERRate.
CRF	Mallet	The toolbox we used for CRF is Mallet).
MT	Europarl parallel corpus	We extracted the lexical resources for our MT system from version 3 of the French-English Europarl parallel corpus ( pairs.
Translation	NIST	Translation accuracy is reported in terms of BLEU, NIST, and METEOR metrics.
MT evaluation	ACL05	The multiple translations and human assessments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43).
MT evaluation	NistMATR08 (LDC2008E43)	The multiple translations and human assessments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43).
MT	MT02	We then apply the log probability of the phrase orientation classifier as an extra feature in a phrase-based MT system, and get significant BLEU point gains on three test sets: MT02 (+0.59), MT03 (+1.00) and MT05 (+0.77).
MT	MT03	We then apply the log probability of the phrase orientation classifier as an extra feature in a phrase-based MT system, and get significant BLEU point gains on three test sets: MT02 (+0.59), MT03 (+1.00) and MT05 (+0.77).
MT	MT05	We then apply the log probability of the phrase orientation classifier as an extra feature in a phrase-based MT system, and get significant BLEU point gains on three test sets: MT02 (+0.59), MT03 (+1.00) and MT05 (+0.77).
MERT	NIST MT06 data set	The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences.
MT	NIST MT evaluation datasets	 Table 4: MT experiments of different settings on various NIST MT evaluation datasets. All differences marked in bold  are significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005).
phrase-based translation model training	GIZA++ toolkit	For phrase-based translation model training, we used the GIZA++ toolkit, and 1.0M bilingual sentences.
language model training	SRI language model toolkit	For language model training, we used the SRI language model toolkit, and 1.0M sentences for the translation model training.
WSD	DARPA GALE project	Advances in research on WSD in the current millennium can be attributed to several key factors: the availability of large scale computational lexical resources such as * The second author has been partially funded by DARPA GALE project.
NE detection	HAREM	Our first concern in this pilot track was to make a clear separation between the evaluation of relations and the evaluation of NE detection, which was the goal of HAREM.
alignment evaluation	RTE4 dataset	We consider two datasets: RTE2 test, the alignment evaluation dataset, and the most recent RTE4 dataset, where current numbers for the Stanford system are available from last year's Text Analysis Conference (TAC).
Question Answering (QA)	RTE-2 dataset	This subtype is of particular relevance to Question Answering (QA): in the RTE-2 dataset, 1 for example, all is-a Hypotheses were drawn from QA data.
SRL annotation	PropBank corpus	[ is an example of SRL annotation from the PropBank corpus), where the subscripted information maps the semantic roles A0, A1 and A2 to arguments for the predicate sell as defined in the PropBank Frame Scheme.
SRL annotation	PropBank Frame Scheme	[ is an example of SRL annotation from the PropBank corpus), where the subscripted information maps the semantic roles A0, A1 and A2 to arguments for the predicate sell as defined in the PropBank Frame Scheme.
coreference resolution	MUC-6	There does not appear to be a single standard evaluation metric in the coreference resolution community, so we opted to use three: MUC-6 (, CEAF, and B-CUBED (, which seem to be the most widely accepted metrics.
parsing	British National Corpus (BNC)	For parsing, for instance,, found that fora random sample of 20,000 strings from the British National Corpus (BNC) even with a broad-coverage grammar for English) missing MWEs accounted for 8% of total parsing errors.
MWE	PMI	 Table 1: Top 5 and Bottom 5 MWE candidates  ranked by PMI and alignment-based approach
MWE	PMI	 Table 2: Evaluation of MWE candidates -PMI and  MI
detection tasks	Penn Treebank	We empirically validate the effectiveness of these modified AMs in detection tasks in English, performed on the Penn Treebank, which shows significant improvement over the original AMs.
CP mining	EMILLE (McEnery, Baker,) English-Hindi parallel corpus	The CP mining methodology outlined earlier has been implemented and tested over multiple files of EMILLE (McEnery, Baker,) English-Hindi parallel corpus.
POS labeling task	Bangla and Hindi	Section 4 presents the experiments conducted for POS labeling task of Bangla and Hindi while the results of these experiments are discussed in Section 5.
GS intuition	Doc Title	 Table 2: Results for Automatic Image Annotation for GS intuition . In both Tables 2 and 3, statistically  significant results are marked with  *  (measured against Doc Title, p<0.05, paired t-test), × (measured  against tf*idf, p<0.1, paired t-test),  † (measured against tf*idf, p<0.05, paired t-test).
Syntactic annotation of spoken utterances	Czech Academic Corpus	Syntactic annotation of spoken utterances: A case study on the Czech Academic Corpus
SMT	NIST	According to the performance trajectories it seems that a reasonable increase in the contribution of the corpus of literal translations effectively improves the decoding performance of the SMT system since the BLEU scores with λ = 0.67 and 0.8 are higher than that of the baseline which are 0.0121 and 0.0105, and of the NIST scores which are 0.
SMT	XIN extracted corpus	For our experiments on effect on SMT quality we use only the XIN extracted corpus.
event coreference resolution	ACE 2005 English corpus	We developed and tested the spectral clustering algorithm for event coreference resolution using the ACE 2005 English corpus which contains 560 documents.
polarity classification	ICA-LinkNeigh	Accordingly, for polarity classification, we implemented three scenarios: ICA-LinkNeigh, ICALinkOnly and ICA-noInfo.
polarity classification	ICALinkOnly	Accordingly, for polarity classification, we implemented three scenarios: ICA-LinkNeigh, ICALinkOnly and ICA-noInfo.
polarity classification	ICA-noInfo	Accordingly, for polarity classification, we implemented three scenarios: ICA-LinkNeigh, ICALinkOnly and ICA-noInfo.
NER evaluation	Wikipedia gold standard (WG) corpus	We present the first NER evaluation on a Wikipedia gold standard (WG) corpus.
Sentence selection	WP2	Sentence selection leaves 571 sentences (33.7%) after the WP2 process and 698 (41.2%) after the WP4 process (see).
Sentence selection	WP4 process	Sentence selection leaves 571 sentences (33.7%) after the WP2 process and 698 (41.2%) after the WP4 process (see).
synonym detection	WordNet	Previous metrics used for synonym detection had to be built using co-occurrence statistics of collected corpora) or expensive, expert-created resources such as WordNet or Roget's Thesaurus (.
parser evaluation	1984 Wall Street Journal (WSJ)	The vast majority of parser evaluation is conducted on the 1984 Wall Street Journal (WSJ).
Synset assignment	Thai-English dictionary	 Table 1. Synset assignment to entries in  Thai-English dictionary
transliteration shared task	NEWS workshop	This introduction outlines the purpose of the transliteration shared task conducted as apart of the NEWS workshop.
MT	FSM toolkit	MT has also been modeled with transducers, and descendants of the FSM toolkit are now used to implement phrase-based machine translation.
SST	Semcor adopted	We evaluated the performances of the SST generated so far by adopting a n-fold cross validation strategy on the Semcor adopted for training.
transliteration of foreign names	Beckham	In fact, transliteration of foreign names into Chinese is often based on the surface orthographic forms, as exemplified in the transliteration of Beckham, where the supposedly silent h in "ham" is taken as pronounced, resulting in 汉姆 han4-mu3 in Mandarin Chinese and 咸 haam4 in Cantonese . However, as we have observed, there is considerable graphemic ambiguity in E2C, where an English segment might correspond to different Chinese segments.
NER	IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL) 1	Various works of NER involving Indian languages are reported in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL) 1 using various techniques.
coreference resolution	MUC-7 formal corpus	 Table 2: Evaluation results for coreference resolution against the MUC-7 formal corpus.
Phrase Detectives game	Uni Essex)	In this talk, I will discuss an ongoing effort to use the 'Games with a Purpose' methodology to create a large-scale anaphorically annotated corpus in which multiple judgments are maintained about the interpretation of each anaphoric expression-and in particular, the Phrase Detectives game: http://www.phrasedetectives.org Joint work with Jon Chamberlain and Udo Kruschwitz (Uni Essex) 3
segmentation	NGT	We can compute the accuracy of the segmentation given by the NGT model and the HMM-based model from the reference segmentation.
DA assignation	NGT	The results show that the clearest source of errors in DA assignation that the NGT model can produce is due to the split of the turn into a wrong number of utterances.
normalisation	Propbank corpus	We evaluated our normalisation method both on a testsuite of constructed examples and on real world data namely, the Propbank corpus.
translation task	WMT'08 shared task 3	We evaluate the parser on a translation task (WMT'08 shared task 3 ).
dependency parsing	Basque Dependency Treebank (BDT	This work presents several experiments performed on dependency parsing of the Basque Dependency Treebank (BDT,).
parsing	FTB	In order to evaluate the relations between tagsets and parsing accuracy on a given treebank, we extract the optimal tagsets 13 from the FTB, the CC tagset and we convert the MFT POS tags to this tagset.
SIGDIAL meeting	ACL	This year the SIGDIAL meeting has been elevated from Workshop to Conference by ACL, its main sponsoring organization.
SMT-system	JRC-ACQUIS	In this paper the performance of a simplistic Mosesbased SMT-system, when trained and tested on JRC-ACQUIS (version 2.2), is investigated.
SMT	Moses 10	The SMT system used follows the description of the baseline system given for the EACL 2009 4th Workshop on SMT 9 and it is based on Moses 10 -see.
SMT	JRC-ACQUIS Test Data	 Table 3: Evaluation Results for the SMT System for  the JRC-ACQUIS Test Data
event coreference resolution	ACE 1	Starting with some motivating examples, we formally state the problem of event coreference resolution in the ACE 1
PR pseudonymisation	Acad.	The PR pseudonymisation is done by the information system of the University Specialised Hospital for Active Treatment of Endocrinology "Acad.
question answering	ICU database	The named entities will serve as a prerequisite for clinical relation extraction, clinical notes indexing and question answering from the ICU database.
LSA processing	Wall Street Journal text collection	In all three cases, the large corpus used for LSA processing was the Wall Street Journal text collection (, comprising about 86,000 articles.
citation classification	Pendlebury, 2009)	The results also confirm obervations made in other works: (1) citation classification is a hard task, (2) there are only a few strongly negative citations which coincides with observations made by), (Pendlebury, 2009) and others, (3) the majority of citations is neutral or of unknown type.
Detecting Semantic Category	EEG/MEG Recordings	Detecting Semantic Category in Simultaneous EEG/MEG Recordings
machine translation evaluation task	Amazon Mechanical Turk	This paper discusses a machine translation evaluation task conducted using Amazon Mechanical Turk.
Collecting Image Annotations	Amazon's Mechanical Turk	Collecting Image Annotations Using Amazon's Mechanical Turk
MT	MTurk-produced test sets	More importantly , in experiments with multiple MT systems, we find that the MTurk-produced test sets yield essentially the same conclusions regarding system performance as the professionally-produced test sets yield.
MT	NIST2009 test set	We evaluate multiple MT systems on both the professionallyproduced NIST2009 test set and our MTurkproduced test set and find that the MTurk-produced test set yields essentially the same conclusions about system performance as the NIST2009 set yields.
MT	MTurkproduced test set	We evaluate multiple MT systems on both the professionallyproduced NIST2009 test set and our MTurkproduced test set and find that the MTurk-produced test set yields essentially the same conclusions about system performance as the NIST2009 set yields.
MT	MTurk-produced test set	We evaluate multiple MT systems on both the professionallyproduced NIST2009 test set and our MTurkproduced test set and find that the MTurk-produced test set yields essentially the same conclusions about system performance as the NIST2009 set yields.
MT	NIST2009 set	We evaluate multiple MT systems on both the professionallyproduced NIST2009 test set and our MTurkproduced test set and find that the MTurk-produced test set yields essentially the same conclusions about system performance as the NIST2009 set yields.
MT	NIST2009 test set	We evaluated three different MT systems on the NIST2009 test set and on our two MTurk-produced test sets (MTurk-NoEditing and MTurk-Edited).
MT	MTurk-produced test sets	We evaluated three different MT systems on the NIST2009 test set and on our two MTurk-produced test sets (MTurk-NoEditing and MTurk-Edited).
MT	MTurk-NoEditing	We evaluated three different MT systems on the NIST2009 test set and on our two MTurk-produced test sets (MTurk-NoEditing and MTurk-Edited).
MT	MTurk-Edited	We evaluated three different MT systems on the NIST2009 test set and on our two MTurk-produced test sets (MTurk-NoEditing and MTurk-Edited).
MT	NIST2009	Two of the MT systems (ISI Syntax (  2004;) and JHU Syntax () augmented with ()) were chosen because they represent stateof-the-art performance, having achieved the highest scores on NIST2009 to our knowledge.
MT	NIST2009	The third MT system (Joshua-Hierarchical) (, an open source implementation of, was chosen because though it is a competitive system, it had clear, markedly lower performance on NIST2009 than the other two systems and we want to see if that difference in performance is also maintained if we were to shift evaluation to our MTurk-produced test sets.
MT	MTurk-produced test sets	The third MT system (Joshua-Hierarchical) (, an open source implementation of, was chosen because though it is a competitive system, it had clear, markedly lower performance on NIST2009 than the other two systems and we want to see if that difference in performance is also maintained if we were to shift evaluation to our MTurk-produced test sets.
MT	MTurkproduced test sets	The main observation of the results in is that both the relative performance of the various MT systems and the amount of the differences in performance (in terms of percentage performance of the baseline) are maintained when we use the MTurkproduced test sets as when we use the NIST2009 test set.
MT	NIST2009 test set	The main observation of the results in is that both the relative performance of the various MT systems and the amount of the differences in performance (in terms of percentage performance of the baseline) are maintained when we use the MTurkproduced test sets as when we use the NIST2009 test set.
SRL	Propbank	Second, SRL is trained on news corpora using a resource like Propbank, and so may face recall loss due to out of vocabulary verbs and precision loss due to different writing styles found on the Web.
slot fillings" tasks	Knowledge Base Population track at the 2009 Text Analysis Conference	This approach might be particularly useful in "slot fillings" tasks like the one in the Knowledge Base Population track at the 2009 Text Analysis Conference.
Rethinking Grammatical Error Annotation	Amazon Mechanical Turk	Rethinking Grammatical Error Annotation and Evaluation with the Amazon Mechanical Turk
Negation Detection	Swedish Clinical Text	Negation Detection in Swedish Clinical Text
data mining	EHR	This field is focusing on combining related research areas such as visualization, data mining and statistics to handle large and heterogeneous volumes of data, such as EHR.
MRR	testing dataset (TREC 2004)	 Table 1: The MRR results of the models presented in  §5 on testing dataset (TREC 2004) using different window sizes  of candidate passages. The statistically significant model results in each corresponding MRR category are bolded.  Baseline MRR=%67.6, Top1=%58, Top5=%82.2.
sentiment analysis	Pang	Other work in sentiment analysis (often referred as opinion mining) has explored several facets of the problem, ranging from predicting binary ratings (e.g., thumbs up/down)), to more detailed opinion analysis methods predicting multi-scale ratings (e.g., number of stars) (Pang and).
WSD	SoundNet label dataset	The evaluation of WSD with evocation and the measure-combined voting algorithm was carried out primarily on the SoundNet label dataset because of the availability of ground truth data.
parsing	Section 23 of the PTB	Many of the challenges reported here are mostly irrelevant when parsing Section 23 of the PTB but they are of primordial importance in other tasks, including out-of-domain parsing, statistical machine translation, and parsing resource-poor languages.
parsing	Modern Standard Arabic (MSA)	In this paper, we explore the role of morphological features in parsing Modern Standard Arabic (MSA).
dependency parsing	Basque Dependency Tree-bank (BDT)	We present a set of experiments on dependency parsing of the Basque Dependency Tree-bank (BDT).
dependency parsing	Basque Dependency Treebank (BDT	This paper presents several experiments performed on dependency parsing of the Basque Dependency Treebank (BDT,.
parsing	Korean Treebank 2.0	We investigate parsing accuracy on the Korean Treebank 2.0 with a number of different grammars.
Tagging	L'est Républicain corpus	 Table 5: Tagging accuracy and UAS scores for modified terminal symbols in the dev set, grouped by ranges of fre- quencies in the modified training sets. The "replaced by UNKC*" line corresponds to the case where the desinflected  form or the POS+lemma pair does not appear more than 200 times in the L'est Républicain corpus.
parsing English	Wall Street Journal Penn Treebank (PTB	Large parse-annotated corpora have led to an explosion of interest in statistical parsing methods, including the development of highly successful models for parsing English using the Wall Street Journal Penn Treebank (PTB,).
parsing	PTB	Over the last 10 years, parsing performance on the PTB has hit a performance plateau of 90-92% f-score using the PARSEVAL evaluation metric.
parsing	PARSEVAL evaluation metric	Over the last 10 years, parsing performance on the PTB has hit a performance plateau of 90-92% f-score using the PARSEVAL evaluation metric.
PMI	CM	 Table 2: Evaluating the PMI rankings obtained using CM
Sentence-level	WMT10 German-English News Task	 Table 15: Sentence-level ranking for the WMT10 German-English News Task
Sentence-level	WMT10 Czech-English News Task	 Table 19: Sentence-level ranking for the WMT10 Czech-English News Task
translation task	ocial test set	Section 4 presents the experimental framework, corpus used and a description of the dierent systems built for the translation task; the section ends showing the results we obtained over the ocial test set.
translation task	WMT 2010	We describe our system for the translation task of WMT 2010.
Translation	Europarl	 Table 2: LMs perplexities and BLEU scores mea- sured on news-test2009. Translation models  used here were trained on nc and Europarl.
MT	NIST09 evaluation	While Portage has typically performed well in Chinese to English MT evaluations, most recently in the NIST09 evaluation, our participation in WMT 2010 revealed some interesting differences between Chinese-English and E-F/F-E translation , and alerted us to certain weak spots in our system.
SMT	NewsCommentary data	Although we were able to build a vanilla SMT system on all parallel data available (News-Commentary + CzEng), we also attempted to build additional systems using NewsCommentary data (which we considered indomain) and various in-domain subsets of CzEng hoping to achieve better results on domainspecific data.
SMT	EBMT	Specifying the model in this manner ties together the two different mod-eling approaches pursued by SMT and EBMT; the SMT model of Equation 1 is merely a special case of our model when the features for all instances of a translation are constant such that φ k (s, s , t , t) = φ k (s, t) ∀s , t.
Translation	FR-EN) language pair	 Table 6: Translation Results for the French-English (FR-EN) language pair, shown in single-reference  lowercase IBM BLEU. Bold results correspond to submitted systems.
Translation	IBM	 Table 6: Translation Results for the French-English (FR-EN) language pair, shown in single-reference  lowercase IBM BLEU. Bold results correspond to submitted systems.
detokenisation	Europarl	For the final run, the detokenisation and recasing was performed according to the technique described in the Workshop baseline description.: Main figures of the Spanish resources provided: Europarl v5, News-Commentary (NC), United Nations (UN) and News-shuffled (News).
detokenisation	News-Commentary (NC)	For the final run, the detokenisation and recasing was performed according to the technique described in the Workshop baseline description.: Main figures of the Spanish resources provided: Europarl v5, News-Commentary (NC), United Nations (UN) and News-shuffled (News).
detokenisation	United Nations (UN)	For the final run, the detokenisation and recasing was performed according to the technique described in the Workshop baseline description.: Main figures of the Spanish resources provided: Europarl v5, News-Commentary (NC), United Nations (UN) and News-shuffled (News).
detokenisation	News-shuffled (News)	For the final run, the detokenisation and recasing was performed according to the technique described in the Workshop baseline description.: Main figures of the Spanish resources provided: Europarl v5, News-Commentary (NC), United Nations (UN) and News-shuffled (News).
MT	WMT 2010 test  set	 Table 1: BLEU and NIST evaluation of four con- figurations of our MT system; the WMT 2010 test  set was used.
MT System Combination	WMT'10	MANY : Open Source MT System Combination at WMT'10
translation task	WMT10	Section 4 presents our experiments, instance selection techniques, and results on the translation task for WMT10.
translation task	WMT10	We perform experiments on the translation task of the English-German, German-English, EnglishFrench, English-Spanish, and English-Czech language pairs using the training corpus provided in WMT10.
Translation	WMT 2010 system combination task	 Table 3: Translation scores (case-sensitive) on the  test corpora of our primary and secondary submis- sions to the WMT 2010 system combination task.
Tuning	news-2008-test" data	Tuning was performed on a 200-sentence subset of the "news-2008-test" data, and all 2525 sentences of the "news-2009-test" data were used as unseen test data.
Tuning	news-2009-test" data	Tuning was performed on a 200-sentence subset of the "news-2008-test" data, and all 2525 sentences of the "news-2009-test" data were used as unseen test data.
SMT	Moses SMT toolkit	The baseline system is a standard phrase-based SMT system based on the Moses SMT toolkit ).
SMT	Basic Travel Expressions Corpus (BTEC)	The effects of using different word segmentations and integrating them into an SMT engine are investigated using the multilingual Basic Travel Expressions Corpus (BTEC), which is a collection of sentences that bilingual travel experts consider useful for people going to or coming from other countries ( summarizes the characteristics of the BTEC corpus used for the training (train) of the SMT models, the tuning of model weights and stop conditions of the iterative bootstrap method (dev), and the evaluation of translation quality (test).
SMT	BTEC corpus	The effects of using different word segmentations and integrating them into an SMT engine are investigated using the multilingual Basic Travel Expressions Corpus (BTEC), which is a collection of sentences that bilingual travel experts consider useful for people going to or coming from other countries ( summarizes the characteristics of the BTEC corpus used for the training (train) of the SMT models, the tuning of model weights and stop conditions of the iterative bootstrap method (dev), and the evaluation of translation quality (test).
emotion classification	EmotiBlog	In the second experiment, we tested the performance of emotion classification using the two models built using EmotiBlog on the three corpora -JRC quotes, SemEval 2007 Task No.14 test set and the ISEAR corpus.
emotion classification	JRC quotes	In the second experiment, we tested the performance of emotion classification using the two models built using EmotiBlog on the three corpora -JRC quotes, SemEval 2007 Task No.14 test set and the ISEAR corpus.
emotion classification	SemEval 2007 Task No.14 test set	In the second experiment, we tested the performance of emotion classification using the two models built using EmotiBlog on the three corpora -JRC quotes, SemEval 2007 Task No.14 test set and the ISEAR corpus.
emotion classification	ISEAR corpus	In the second experiment, we tested the performance of emotion classification using the two models built using EmotiBlog on the three corpora -JRC quotes, SemEval 2007 Task No.14 test set and the ISEAR corpus.
emotion classification	EmotiBlog annotations	Results for emotion classification using the models built from the EmotiBlog annotations.
emotion classification	EmotiBlog annotations	 Table 4. Results for emotion classification using the  models built from the EmotiBlog annotations.
validation of coding	Carletta96	The measure is based on Cohen's corrected kappa coefficient for the validation of coding schemes (Carletta96).
Biomedical Event Extraction	Entire PubMed	Scaling up Biomedical Event Extraction to the Entire PubMed
summarization of 150 biomedical scientific articles	BioMed Central corpus	The system is applied to the summarization of 150 biomedical scientific articles from the BioMed Central corpus and it is found that WSD improves the quality of the summaries.
WSD	UMLS	Section 5 presents a graph-based WSD algorithm which has been adapted to assign concepts from the UMLS.
event extraction	GENIA corpus	However, event extraction efforts have so far been limited to publication abstracts, with most studies further considering only the specific transcription factor-related subdomain of molecular biology of the GENIA corpus.
parsing	Wall Street Journal section of the Penn Treebank	The parsing model described in Section 3 has previously been evaluated on the standard task of parsing the Wall Street Journal section of the Penn Treebank.
parsing the	Wall Street Journal section of the Penn Treebank	The parsing model described in Section 3 has previously been evaluated on the standard task of parsing the Wall Street Journal section of the Penn Treebank.
parsing	HHMM parser	Thus, the first experiments in this paper evaluate the degradation of parsing accuracy depending on beam width of the HHMM parser.
parsing	HHMM	shows a plot of parsing time versus sentence length for the HHMM parser fora beam width of 20.
summarization	NTCIR-3 SUMM	One reason is that the difference of summarization technique, i.e., our work is extractive summarization, while the gold standard data provided by NTCIR-3 SUMM is the abstracts written by human professionals.
Trans-literation Mining Shared Task	NEWS 2010)	This report documents the details of the Trans-literation Mining Shared Task that was run as apart of the Named Entities Workshop (NEWS 2010), an ACL 2010 workshop.
transliteration mining	NEWS 2010 Shared Task	We explore a number of different approaches to transliteration mining in the context of the NEWS 2010 Shared Task.
transliteration mining	ACL 2010 NEWS workshop shared translitera-tion mining task data	This paper presents transliteration mining on the ACL 2010 NEWS workshop shared translitera-tion mining task data.
TM	ACL 2010 NEWS workshop	TM has been gaining some attention of late with a shared task in the ACL 2010 NEWS workshop . In this paper, TM was performed using a transliterator that was used to generate possible transliterations of a word while constraining the output to tokens that exist in a target language word sequence.
TM	ACL 2010 NEWS workshop data	1 http://translit.i2r.a-star.edu.sg/news2010/ The proposed improvements in TM were tested using the ACL 2010 NEWS workshop data for Arabic, English-Chinese, English-Hindi, EnglishRussian, and English-Tamil.
transliteration mining	NEWS 2010 White paper	The transliteration mining task as defined in the NEWS 2010 White paper () required identifying single word transliteration pairs from a set of candidate transliteration pairs.
NERC task	MUC	The classical NERC task is confined to coarsegrained named entity (NE) classes established in the MUC or) competitions, typically PERS, LOC, ORG, MISC.
NERC task	MISC	The classical NERC task is confined to coarsegrained named entity (NE) classes established in the MUC or) competitions, typically PERS, LOC, ORG, MISC.
Name Recognition	John & Sons	Name Recognition becomes important in situations when the person or the organization is more important than the action it performed, for example, bankruptcy of the corner shop John & Sons is not as interesting as the bankruptcy of General Motors, an American car manufacturer.
translation of trees	IIS-0908532	In an independent tradition, the automatatheoretic investigation of the translation of trees * financially supported by NSF STAGES project, grant #IIS-0908532.
parsing and translation of strings	WXTTs	In contrast with the cascaded approach above, which maybe rather inefficient, we investigate a more direct technique for both parsing and translation of strings based on WXTTs.
parsing and translation of input strings	WXTTs	We then derive computational complexity results for parsing and translation of input strings on the basis of WXTTs.
ASR	Dragon Naturally Speaking	We initially considered a range of possibilities for the ASR and settled on Dragon Naturally Speaking, version 10 (DNS).
Parsing	Full Monolingual baseline	 Table 4: Parsing results. Rows are grouped by data  condition. We bold entries that are best in their  group and beat the the Full Monolingual baseline.
hedge detection	Wikipedia dataset	By allowing an efficient handling of combinations of large-scale input features, the discriminative approach we adopted showed highly competitive empirical results for hedge detection on the Wikipedia dataset: our system is ranked as the first with an F-score of 60.17%.
SVM training	BioScope dataset	Since the training data are unbalanced (e.g. 18% of the total number of sentences in the BioScope training data are labeled as "uncertain"), for SVM training we used the following class weights: • 0.1801 for the "certain" class and 0.8198 for the "uncertain" class on the BioScope dataset; • 0.2235 for the "certain" class and 0.7764 for the "uncertain" class on the Wikipedia dataset.
SVM training	Wikipedia dataset	Since the training data are unbalanced (e.g. 18% of the total number of sentences in the BioScope training data are labeled as "uncertain"), for SVM training we used the following class weights: • 0.1801 for the "certain" class and 0.8198 for the "uncertain" class on the BioScope dataset; • 0.2235 for the "certain" class and 0.7764 for the "uncertain" class on the Wikipedia dataset.
hedge detection	Wikipedia training data	shows the variability of hedge detection results on Wikipedia training data when changing the RBF-specific kernel parameter and the regularization parameter C.
detecting uncertain information	Wikipedia data	Task 1 on detecting uncertain information was performed by an SVM-based system to process the Wikipedia data and by a memory-based system to process the biological data.
scope detection task	BIO-encoding	We model the in-sentence uncertainty cue and scope detection task as an L 2-regularised approximate maximum margin sequence labelling problem, using the BIO-encoding.
hedge detection	CoNLL-2010	This paper describes the approach to hedge detection we developed, in order to participate in the shared task at CoNLL-2010.
negation	Stanford NER CRF	 Table 3: The results for negation and speculation on consensus when executing Stanford NER CRF using  ten-fold cross validation.
negations	BioScope Corpus	 Table 5: The results for negations, speculation cues and scopes on the BioScope Corpus when executing  Stanford NER CRF using ten-fold cross validation.
negations	Stanford NER CRF	 Table 5: The results for negations, speculation cues and scopes on the BioScope Corpus when executing  Stanford NER CRF using ten-fold cross validation.
negations	Stanford NER CRF	 Table 6: The results for negations and speculation cues and scopes for annotator A, F and H respectively  when executing Stanford NER CRF using ten-fold cross validation.
multi-label classification of medical reports	CMC	Prompted by these contradicting results in the literature, we explore the role assertion classification plays in the multi-label classification of medical reports from both the CMC and i2b2 challenges.
negation annotation	BioScope	Speculation and negation annotation in natural language texts: what the case of BioScope might (not) reveal
negation extraction	BioScope negation corpus	A new negation corpus is presented that was constructed for the domain of English product reviews obtained from the open web, and the proposed negation extraction system is evaluated against the reviews corpus as well as the standard BioScope negation corpus, achieving 80.0% and 75.5% F1 scores, respectively.
question answering	QALL-ME project system 1	This evaluation provides useful cues for researchers and developers aiming at the integration of TE components in larger applications (see, for instance, the use of a TE engine for question answering in the QALL-ME project system 1 , the use in relation extraction (), and in reading comprehension systems).
TE	RTE datasets	Although the RTE evaluations showed progresses in TE technologies, we think that there is still large room for improving qualitative analysis of both the RTE datasets and the system results.
TE	RTE datasets	More specifically, we address two distinguishing aspects of TE: (i) the variety of linguistic phenomena that are relevant for contradiction and how their distribution is represented in RTE datasets; (ii) the fact that in TE it is not enough to detect the polarity of a sentence, as in traditional semantic analysis, but rather it is necessary to analyze the dependencies between two sentences (i.e. the pair) in order to establish whether a contradiction holds between the pair.
Automatic Mapping Large-Scale Heterogene- ous Language	KorLex	Considerations on Automatic Mapping Large-Scale Heterogene- ous Language Resources: Sejong Semantic Classes and KorLex
semantic role labeling(SRL)	Chinese FrameNet	In this paper, semantic role labeling(SRL) on Chinese FrameNet is divided into the subtasks of boundary identification(BI) and semantic role classification(SRC).
SRL	FrameNet	The first SRL model on FrameNet was proposed by.
SRL	Chinese Tree Bank	Sun and Gildea only studied the SRL of 10 Chinese verbs and extracted 1,138 sentences in the Chinese Tree Bank.
SRL	CFN	Therefore, we attempted to use another processing technique for SRL on CFN.
SRL task	CFN	In this paper, the SRL task of CFN comprises two subtasks: BI and SRC.
word translation disambiguation	Japa-nese-English newspaper texts	In our word translation disambiguation experiment, the results show that our method achieved 42% recall and 49% precision for Japa-nese-English newspaper texts, and 45% recall and 76% precision for Chi-nese-Japanese technical documents.
QE	Khmer IR system	The experiment to evaluate the proposed QE techniques was initially conducted only on the prototype of the Khmer IR system that we have implemented.
indexing	Google Webmaster Tools 6 service	Then we followed up the indexing progress by consulting the Google Webmaster Tools 6 service.
word segmentation	Orchid corpus	For example, introduced CRF for word segmentation and POS tagging trained over Orchid corpus (.
POS tagging	Orchid corpus	For example, introduced CRF for word segmentation and POS tagging trained over Orchid corpus (.
SMT-MS	filtered gold standard corpus	To evaluate the effectiveness of SMT-MS and SMT 2 , we divide the filtered gold standard corpus into two sets for training (90%) and testing (10%) respectively.
SMT	filtered gold standard corpus	To evaluate the effectiveness of SMT-MS and SMT 2 , we divide the filtered gold standard corpus into two sets for training (90%) and testing (10%) respectively.
synonym acquisition	EuroWordnet	In previous work on synonym acquisition for the general domain, Van der Plas and used the synsets in Dutch EuroWordnet for the evaluation of the proposed synonyms.
information extraction	Firth	Whether the task is wordsense dis-ambiguation, certain forms of textual entail-ment, information extraction, paraphrase learning, and soon, it turns out to be very useful to consider a word(sense) as being defined by the distribution of word(senses) that regularly accompany it (in the classic words of Firth, "you shall know a word by the company it keeps").
Terminologists	Agbago	Terminologists must often look through many texts before finding appropriate ones (Agbago and).
coreference resolution	English Automatic Content Extraction (ACE) corpus	In the recent years, coreference resolution systems have been evaluated on various versions of the English Automatic Content Extraction (ACE) corpus).
coreference resolution	ACE 2005 corpus	The extension can be exploited by coreference resolution systems, which already use ACE 2005 corpus for development and testing purposes, e.g. (. Moreover, English ACE 2005 corpus is multi-purpose and can be used in other information extraction (IE) tasks as well, e.g. relation extraction.
coreference resolution	English ACE 2005 corpus	The extension can be exploited by coreference resolution systems, which already use ACE 2005 corpus for development and testing purposes, e.g. (. Moreover, English ACE 2005 corpus is multi-purpose and can be used in other information extraction (IE) tasks as well, e.g. relation extraction.
WLM	ESA	Furthermore, we use the same background data as for WLM, which is less than 10% of the data required for ESA.
Identifying	Blogosphere	Identifying and Ranking Topic Clusters in the Blogosphere
SB	Thai WB	As our SB model relies on Thai WB, we review our approach to this problem, plus related preprocessing, in the next section.
Clause Identification and Classification	Bengali	Clause Identification and Classification in Bengali
MT	NIST	Instead we carryout extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (), METEOR (Banerjee and), NIST), WER, PER and TER ().
MT evaluation	NIST	Instead we carryout extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (), METEOR (Banerjee and), NIST), WER, PER and TER ().
IR	TREC 5 and TREC 6 competitions	Most Chinese IR systems are evaluated on the data from the TREC 5 and TREC 6 competitions (.
information retrieval	Lucene package	We conducted our information retrieval experiments using the Lucene package).
SMT	MOSES decoder	For the training of our SMT model, we use a modified training toolkit adapted from the MOSES decoder.
SMT	MOSES toolkit	The SMT system used in the experiments was implemented within the open-source MOSES toolkit ( . Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page 1 . The MSD model was used together with a distance-based reordering model.
SMT	MOSES web page 1	The SMT system used in the experiments was implemented within the open-source MOSES toolkit ( . Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page 1 . The MSD model was used together with a distance-based reordering model.
information retrieval (IR)	NTCIR	These keyword-based searches have been focused on evaluation conferences for information retrieval (IR) such as TREC and NTCIR.
Adverse-Effect Relations Extraction	Massive Clinical Records	Adverse-Effect Relations Extraction from Massive Clinical Records
sentiment identification	FBS	Lastly, we compared our approach to sentiment identification with FBS (see).
sentiment identification	WordNet	There are several explanations for the poor results of FBS: (1) The inferior results of feature detection affect the subsequent task of sentiment identification; and (2) the polarity inference depends heavily on a semantic dictionary WordNet.
domain adaption Word Segmenter	Sighan Bakeoff 2010	A domain adaption Word Segmenter For Sighan Bakeoff 2010
parsing	PCFG	As for the parsing model, currently there are four commonly used parsing models, PCFG model, the model based on historical, Hierarchical model of progressive, headdriven model.
parse selection	source Treebank	To provide good parse selection strategies which are needed in self-training, we score the automatically generated parse trees with parse trees in source Treebank as a reference.
parse	Tsinghua Chinese Treebank	In our experiments, SSPTC strategy is utilized to parse Tsinghua Chinese Treebank with the help of Penn Chinese Treebank.
parse	Penn Chinese Treebank	In our experiments, SSPTC strategy is utilized to parse Tsinghua Chinese Treebank with the help of Penn Chinese Treebank.
parse selection	PCTB	In this subsection we evaluate our parse selection strategies with the help of PCTB.
NER	Chinese PND	For address these challenges, we designed a rulebased combination technique to improve NER performance and propose a multi-stage clustering framework for Chinese PND.
pollen prediction	Scotland	Turner collected 68 examples of pollen prediction data for Scotland (each consisting of 6 small integers and a characterisation of the previous trend) with humanwritten forecasts, which we took as both our training and test data.
data sharing and exchange	NLG-the RAGS framework	This paper discusses one particular approach to data sharing and exchange that was developed for NLG-the RAGS framework.
GRE	OWL2	In this paper, we propose a DL-based approach to GRE that exploits the full power of OWL2.
explanation of medical histories	Wall Street Journal	This strand of work was developed further in the NECA project and has since been extended to other domains, including explanation of medical histories, patient information leaflets (  and Wall Street Journal articles.
NEG task	GREC'10	We present the UMUS (Université du Maine/Universität Stuttgart) submission for the NEG task at GREC'10.
Question Generation from Paragraphs (Task A)	QG STEC	A preference poll was conducted and the most preferred tasks, Question Generation from Paragraphs (Task A) and Question Generation from Sentences (Task B), were chosen to be offered in the first QG STEC.
Question Generation from Sentences	QG STEC	A preference poll was conducted and the most preferred tasks, Question Generation from Paragraphs (Task A) and Question Generation from Sentences (Task B), were chosen to be offered in the first QG STEC.
speech disfluency detection	Switchboard corpus	We build a model for speech disfluency detection based on conditional random fields (CRFs) using the Switchboard corpus.
ASR	JSGF	In addition, the system prompts and the grammars for ASR are implemented in VoiceXMLcompliant formats, for example, JSGF or SRGS.
ASR	SRGS	In addition, the system prompts and the grammars for ASR are implemented in VoiceXMLcompliant formats, for example, JSGF or SRGS.
Recognizing Textual Entailment task	RTE-2 challenge	We test the proposed procedure and the obtained knowledge base on the Recognizing Textual Entailment task using the data sets from the RTE-2 challenge for evaluation.
PPI extraction	BoW	PPI extraction methods use various sentence representation, e.g., are based only on BoW (), use only DTs (, or combine representations (.
PAGE	National Health and Nutrition Examination Surveys (NHANES)	The studies that comprise PAGE include: the study, which utilizes genotypic and phenotypic data from the National Health and Nutrition Examination Surveys (NHANES) from the Centers for Disease Control and Prevention (CDC).
coreference resolution	MEDCo 1 corpus of abstracts	Of these, only two corpora have been used in coreference resolution systems developed outside the research group that annotated them: MEDSTRACT (), and the MEDCo 1 corpus of abstracts which was used by the different teams who participated in the Coreference Supporting Task of the BioNLP 2011 Shared Task 2 . These two corpora are widely used, despite the fact that they are composed only of abstracts.
segmentation	Penn Arabic Treebank (ATB)	Data set We evaluate segmentation performance on the Penn Arabic Treebank (ATB).
Parsing such a word	CoNLL-X	Parsing such a word requires tokenization, and performing dependency parsing in the tradition of the CoNLL-X () and) also requires part of speech tagging, lemmatization, linguistic features, and vocalization, all of which were in the human annotated gold standard form in the shared task.
Judgement	IMDb62	We considered three datasets that cover different writing styles and settings: Judgement, IMDb62 and Blog.
Judgement	Blog	We considered three datasets that cover different writing styles and settings: Judgement, IMDb62 and Blog.
word sense disambiguation (WSD) task	WordNet	We evaluate our network's performance on a word sense disambiguation (WSD) task and show: a) the network is competitive with WordNet when used as a stand-alone knowledge source for two WSD algorithms; b) combining our network with WordNet achieves disambiguation results that exceed the performance of either resource individually; and c) our network outperforms a similar resource that has been automatically derived from semantic annotations in the Wikipedia corpus.
SRL	CoNLL 2005 shared task	Our baseline SRL model is an implementation of () which was the top performing system in CoNLL 2005 shared task.
RST	RST review	For more information about RST we recommend the original article of, the website of RST 1 and the RST review by.
RST	Spanish Treebank	 Table 1: RST Spanish Treebank statistics
MTE	TEI P5	The encoding of the MTE specifications follows the Text Encoding Initiative Guidelines, TEI P5 (, and this paper concentrates on developing a semi-automatic procedure for converting them from TEI XML to OWL.
MTE	OWL	The encoding of the MTE specifications follows the Text Encoding Initiative Guidelines, TEI P5 (, and this paper concentrates on developing a semi-automatic procedure for converting them from TEI XML to OWL.
Distribution of arities	PropBank rolesets	 Table 3: Distribution of arities by percentage and by  count in the 4,654 PropBank rolesets.
SSC	GSC	In this paper, we investigate the practical usability of an SSC when a machine learning system is trained on it and tested on an unseen benchmark GSC.
SSC annotation	GSC	The primary objective of SSC annotation is to compensate the cost, time and manual effort required fora GSC.
SSC	GSC	3. Can an SSC combined with a GSC produce a better trained system?
Sentiment Annotation	Modern Standard Arabic Newswire	Subjectivity and Sentiment Annotation of Modern Standard Arabic Newswire
summarizing chat	US Navy watchstanders	We describe the beginning stages of our work on summarizing chat, which is motivated by our observations concerning the information overload of US Navy watchstanders.
Summarization	DUC	Summarization conferences such as DUC have competitions where different summarization systems compete on a standard task of generating summaries fora publicly available dataset.
Automating Analysis of Social Media Communication	CMDA	Keynote Automating Analysis of Social Media Communication: Insights from CMDA
Annotating Social Acts	Wikipedia Talk Pages	Annotating Social Acts: Authority Claims and Alignment Moves in Wikipedia Talk Pages
MWE identification	BCN	The loss in retrieval quality as a result from MWE identification for BCN was not expected.
MWE	mwetoolkit	Fast and Flexible MWE Candidate Generation with the mwetoolkit
event identification	TimeML framework	A hybrid approach that consists of Conditional Random Field (CRF) based machine learning framework in conjunction with several rule based strategies has been adopted for event identification within the TimeML framework.
Labeling	Brown.	 Table 5: Labeling accuracies evaluated on the Brown.
MT	NIST	For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER ().
SRL	Xinhua corpus	 Table 2: SRL results on triple-gold Xinhua corpus. "arg  match" is the standard CoNLL 2005 evaluation metric,  "oracle" is the oracle SRL based on automatic parser out- put, and "word match" is scoring based on length of ar- gument overlap with the reference
SRL	CoNLL 2005 evaluation metric	 Table 2: SRL results on triple-gold Xinhua corpus. "arg  match" is the standard CoNLL 2005 evaluation metric,  "oracle" is the oracle SRL based on automatic parser out- put, and "word match" is scoring based on length of ar- gument overlap with the reference
WSD	WordNet	Traditionally, WSD relies on a predefined monolingual senseinventory such as WordNet and WSD modules are trained on corpora, which are manually tagged with senses from these inventories.
MT	MT08 Newswire set	 Table 3: MT results on MT08 Newswire set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;  Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).
MT	MT08 Weblog set	 Table 4: MT results on MT08 Weblog set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;  Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).
Parser domain adaptation	ACL WMT 2005 test set	Parser domain adaptation is obtained by adding to this corpus a set of 1200 sentences from the ACL WMT 2005 test set, parsed by DeSR and then corrected by hand.
Parser domain adaptation	DeSR	Parser domain adaptation is obtained by adding to this corpus a set of 1200 sentences from the ACL WMT 2005 test set, parsed by DeSR and then corrected by hand.
ontology translation	NIST	As such, for the taxonomy and ontology translation task we do not recommend using BLEU or NIST as an evaluation metric.
estimate semantic similarity between documents	WordNet	A commonly used approach to estimate semantic similarity between documents is to use an external knowledge source like WordNet ().
Summarization	ACL anthology	We chose a subset of papers in 3 topics (Ma- chine Translation, Dependency Parsing, Summarization) from the ACL anthology.
slot filling task	NIST TAC Knowledge Base Population (KBP) track	The slot filling task at NIST TAC Knowledge Base Population (KBP) track () is a relatively new and popular task with the goal of automatically building profiles of entities from large amounts of unstructured data, and using these profiles to populate an existing knowledge base.
Elicited Imitation	OPI Test Scores	Elicited Imitation for Prediction of OPI Test Scores
PRLM	OGI data	First, the accuracy of the PRLM method was evaluated based on multiple forced-choice experiments with two alternatives using OGI data; in addition to non-English responses in EN-detection partition, English responses from the OGI data were used in this experiment.
PRLM	MaxLanguage	The model based on all PRLM features did not achieve a higher accuracy than the model based on only MaxLanguage.
stance classification	ConvinceMe.net	We examine stance classification on a corpus of 4873 posts across 14 topics on ConvinceMe.net, ranging from the playful to the ideological.
SW	A1A2	 Table 5: Kappa agreement for SW and A1A2
SUC	PWN 3.0	 Table 2: SUC Statistics According to the POS of  the Synsets in PWN 3.0
emotion analysis of Japanese sentences	Japanese WordNet Affect	A baseline system for emotion analysis of Japanese sentences has been developed based on the Japanese WordNet Affect.
FS	OpenNLP 2 Maximum Entropy classifier	We compare the resultant feature sets after each FS scheme using the OpenNLP 2 Maximum Entropy classifier.
sentiment classifier	WordNet	2. Can a sentiment classifier be made robust with respect to features unseen in the training corpus using similarity metrics defined for concepts in WordNet?
sentiment classification	WordNet	Using senses as features allows us to achieve robustness for sentiment classification by exploiting the definition of concepts (sense) and hierarchical structure of WordNet.
question answering	TAC 2008	Specifically, we will propose methods to improve the task of question answering and summarization over opinionated data, as defined in the TAC 2008 "Opinion Summarization pilot" . Given the performance improvements obtained, we conclude that the approaches we proposed for these three components are adequate.
summarization	TAC 2008	Specifically, we will propose methods to improve the task of question answering and summarization over opinionated data, as defined in the TAC 2008 "Opinion Summarization pilot" . Given the performance improvements obtained, we conclude that the approaches we proposed for these three components are adequate.
ID task	GE training set	We note that for our results on the ID task we augment the training data with events from the GE training set.
Bacteria Biotope task	BioNLP Shared Tasks 2011	This paper presents the Bacteria Biotope task as part of the BioNLP Shared Tasks 2011.
Protein Coreference task	BioNLP Shared Task 2011	Overview of the Protein Coreference task in BioNLP Shared Task 2011
coreference resolution	BioNLP-ST 2011	To address the problem of coreference resolution in molecular biology literature, the Protein Coreference (COREF) task is arranged in BioNLP-ST 2011 as a supporting task.
coreference resolution	Genia corpus	The benchmark data sets for developing and testing coreference resolution system were developed based on various manual annotations made to the Genia corpus ().
coreference resolution	SVM light library 5	For both classification of anaphoric pairs in coreference resolution and determining relationship of two entites, we used the SVM light library 5 , a stateof-the-art classifier, with the linear kernel.
recognition of renaming acts	Bacillus subtilis gene databases	The present work focuses on the recognition of renaming acts in the literature between gene synonyms that are recorded in the Bacillus subtilis gene databases.
Coreference Resolution Task	BioNLP 2011	An Incremental Model for the Coreference Resolution Task of BioNLP 2011
extracting complex events and their arguments	BioNLP-2011 shared task	We describe a system for extracting complex events and their arguments as applied to the BioNLP-2011 shared task.
GE-NIA event extraction (GE) task	BioNLP-ST 2011	We participated in the), and applied a graph matching-based approach () to tackling the Task 1 of the GE-NIA event extraction (GE) task), and the core task of the Epigenetics and Post-translational Modifications (EPI) task ), two main tasks of the BioNLP-ST 2011.
coreference resolution	WordNet	Since world knowledge is an important factor in coreference resolution, even in the closed task participants were allowed to use some limited, outside sources, including WordNet and a pre-computed table predicting number and gender information for noun phrases.
predicting coreference	OntoNotes data	The CoNLL-2011 shared task involved predicting coreference using OntoNotes data.
coreference resolution	CoNLL-2011 shared task	This paper details the coreference resolution system submitted by Stanford at the CoNLL-2011 shared task.
coreference resolution	CoNLL-2011 shared task	This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task).
coreference resolution	COPA	We present our end-to-end coreference resolution system, COPA, which implements a global decision via hypergraph partitioning.
coreference resolution	ACE data	While COPA has been developed originally to perform coreference resolution on MUC and ACE data, the move to the OntoNotes data ) required mainly to update the mention detector and the feature set.
coreference resolution	OntoNotes data	While COPA has been developed originally to perform coreference resolution on MUC and ACE data, the move to the OntoNotes data ) required mainly to update the mention detector and the feature set.
coreference	UniTN / IITP / Essex submission to the 2011 CONLL Shared Task	Multi-metric optimization for coreference: The UniTN / IITP / Essex submission to the 2011 CONLL Shared Task
coreference resolution	MUC	In particular, coreference resolution is a critical component of information extraction systems and a series of coreference resolution tasks have been introduced and evaluated from MUC.
information extraction	MUC	In particular, coreference resolution is a critical component of information extraction systems and a series of coreference resolution tasks have been introduced and evaluated from MUC.
coreference resolution	MUC	In particular, coreference resolution is a critical component of information extraction systems and a series of coreference resolution tasks have been introduced and evaluated from MUC.
coreference resolution	CONLL-2011 Share Task	In this paper, we exploit multi-features to a coreference resolution system for the CONLL-2011 Share Task, including flat features and a tree structure feature.
coreference resolution	MUC	And the performance of coreference resolution in the second step is measured using the average F1-measures of MUC, B-CUBED and CEAFE metrics).
coreference resolution	CEAFE	And the performance of coreference resolution in the second step is measured using the average F1-measures of MUC, B-CUBED and CEAFE metrics).
coreference resolution	CoNLL-2011	This paper presents our error tolerable system for coreference resolution in CoNLL-2011(Pradhan et al., 2011) shared task (closed track).
coref-erence resolution	CoNLL 2011 shared task	We introduce an incremental model for coref-erence resolution that competed in the CoNLL 2011 shared task (open regular).
coreference resolution	CoNLL-2011 Shared Task	This paper presents the coreference resolution system Poly-co submitted to the closed track of the CoNLL-2011 Shared Task.
coreference resolution	CoNLL 2011 shared task	This paper describes our coreference resolution system participating in the close track of CoNLL 2011 shared task).
coreference identification	OntoNotes data set	We detail a number of linguistic features that are used during the experiments, and highlight their contribution in improving coreference identification performance over the OntoNotes data set.
coreference resolution	ConLL corpus	In this paper, we present our supervised learning approach to coreference resolution in ConLL corpus.
ASR	newspaper article corpus	A word Ngram language model for ASR dedicated to the domain was trained using the relevant newspaper article corpus.
ASR	DATE	We found that re-ranking degraded ASR accuracy for all slots, except DATE where it had a trivial positive impact.
Temporal Expressions	12th January"	Temporal Expressions contain two types of references: absolute references such as "Tuesday" and "12th January", and relative references such as "tomorrow" and "this Tuesday".
AM adaptation	Phoenix book title subgrammar	For each experiment, indicates the acoustic model (AM) used, the number of hours of domain-specific spontaneous speech used for AM adaptation, the number of titles used to construct the language model (LM), the type of LM, the type of grammar rules in the Phoenix book title subgrammar, and average WER as measured by Levenstein word edit distance.
MT	WMT data	To compare Meteor 1.3 against previous versions of the metric on the task of evaluating MT system outputs, we tune aversion for each language on 2009 WMT data and evaluate on 2010 data.
Tunable Metrics Task	WMT11	Additionally, we describe our submission to the Tunable Metrics Task in WMT11.
MT	WMT08 Hungarian-English	The number of MT systems was different for each language pair and year, from as few as 2 systems (WMT08 Hungarian-English) to as many as 25 systems (WMT10 German-English).
MT	WMT10 German-English	The number of MT systems was different for each language pair and year, from as few as 2 systems (WMT08 Hungarian-English) to as many as 25 systems (WMT10 German-English).
translation	WMT 2011 shared task	In this paper, we apply the same approach to a different translation scenario, namely the WMT 2011 shared task.
SMT training	Danish dataset	In all three cases the SMT training data has been used for the list used by the heuristic, so this is unexpected, especially considering the fact that the Danish dataset is in the same domain as one of the Swedish datasets.
SMT training	Swedish datasets	In all three cases the SMT training data has been used for the list used by the heuristic, so this is unexpected, especially considering the fact that the Danish dataset is in the same domain as one of the Swedish datasets.
translation evaluation	WMT'11	 Table 3: Available training data for the translation be- tween French and English for the translation evaluation  at WMT'11 (number of words after tokenisation).
Translation	newstest2010	 Table 2: Translation results in terms of BLEU score  and translation edit rate (TER) estimated on newstest2010  with the NIST scoring script.
Translation	NIST scoring script	 Table 2: Translation results in terms of BLEU score  and translation edit rate (TER) estimated on newstest2010  with the NIST scoring script.
Translation	newstest2010	 Table 3: Translation results from English to French and  English to German measured on newstest2010 using a  100-best rescoring with SOUL LMs of different orders.
translation	RegMT	We present translation results using our training instance selection methods, translation results using graph decoding, system combination results with RegMT, and performance evaluation with the F 1 measure over target features as a metric for evaluating translation quality.
translation	RegMT	We present translation results using our training instance selection methods, translation results using graph decoding, system combination results with RegMT, and performance evaluation with the F 1 measure over target features as a metric for evaluating translation quality.
WMT Submission	QUAERO	Joint WMT Submission of the QUAERO Project
SMT	Haitian Creole- English test set	 Table 4: Syntax-based SMT on the Haitian Creole- English test set with (=malt) or without (=hiero) English  parse trees and various parse relaxation strategies. The  final system submitted to WMT11 is malt(target)-samt2.
SMT	WMT11	 Table 4: Syntax-based SMT on the Haitian Creole- English test set with (=malt) or without (=hiero) English  parse trees and various parse relaxation strategies. The  final system submitted to WMT11 is malt(target)-samt2.
Translation	new-dev2009 data set	Translation experiments were evaluated using reference translations from the new-dev2009 data set, provided by the organizors of shared translation task with the Workshop on Statistical Machine Translation.
Translation	OOVs	 Table 5: Translation results, using the approximate string matching algorithm for OOVs. The submitted system is  marked with bold.
translation	Europarl and news commentary data	To train the translation models, we used the provided Europarl and news commentary data.
translation	test10	Comparing to last year's translation output, the improvement is over one percent absolutely (from 14.42% to 15.55%) in the BLEU score on the test10.
SMT	opensource SMT toolkit Moses	The baseline SMT system is built with the opensource SMT toolkit Moses ( , in its standard setup.
PoS tagging	Penn treebank	For example, in unsupervised PoS tagging authors sometimes simplify the gold standard by collapsing the original 45 PoS tags of the Penn treebank to 17, e.g. by removing the distinctions between different noun tags.
Slot Value Translation	Ran- dom Sample	 Table 4: Slot Value Translation Assessment from Ran- dom Sample of 1000
TTS	MOS	To evaluate the TTS system, different tests over and above the conventional MOS [ [ITU-T Rec, P. ], [ITU-T Rec, P. ] were performed.
TTS	ITU-T Rec, P. ]	To evaluate the TTS system, different tests over and above the conventional MOS [ [ITU-T Rec, P. ], [ITU-T Rec, P. ] were performed.
TTS	ITU-T Rec	To evaluate the TTS system, different tests over and above the conventional MOS [ [ITU-T Rec, P. ], [ITU-T Rec, P. ] were performed.
question answering	FREyA	The former is, for example, pursued by the question answering system FREyA ().
translation	Okinawa dialect	The lowest BLEU scores were obtained for the translation of the Okinawa dialect.
article and preposition correction	WEB 1T 5-GRAM CORPUS	The language model filter for article and preposition correction uses a 5-gram language model built from the complete WEB 1T 5-GRAM CORPUS using RandLM).
spelling correction	ACL-ANTHOLOGY data set	For spelling correction, the language model filter is built from the ACL-ANTHOLOGY data set.
SMT tasks	WFSTs	Ultimately, SMT tasks maybe described in this framework, as witnessed by toolboxes that exist for WFSTs and XTTs ( In this paper, we propose a weighted formulation of STAGs which is closed under input and output product with WRTGs, and we present a corresponding tree-parsing algorithm.
WSTAGs	WRTGs	3, we show that WSTAGs are closed under input and output product with tree languages generated by WRTGs (cf. Theorem 1).
parsing	EMEA	shows the effect of self-training on parsing the EMEA and FTB dev sets.
parsing	FTB dev sets	shows the effect of self-training on parsing the EMEA and FTB dev sets.
parsing	EMEA	Consistent with previous results on English biomedical texts (, self-training helps in parsing the EMEA, with more predicted parses generally leading to better performance on the EMEA (and worse performance on the FTB).
parsing	EMEA	Consistent with previous results on English biomedical texts (, self-training helps in parsing the EMEA, with more predicted parses generally leading to better performance on the EMEA (and worse performance on the FTB).
parsing	FTB	Consistent with previous results on English biomedical texts (, self-training helps in parsing the EMEA, with more predicted parses generally leading to better performance on the EMEA (and worse performance on the FTB).
dependency parsing	CoNLL 2007 Shared Task	In dependency parsing, domain adaptation received attention in the CoNLL 2007 Shared Task.
generative parsing	Penn Treebank	Using semi-supervised EM, we learn fine-grained but sparse lexical parameters of a generative parsing model (a PCFG) initially estimated over the Penn Treebank.
parsing	goldstandard treebank	With respect to parsing, we evaluate the results against the goldstandard treebank before and after reranking.
recognition of sentences	Penn Treebank	 Table 3: CPU time for recognition of sentences of  length ≤ 10 from section 23 of the Penn Treebank, for  a varying probability p.
translation	BBC	Again the result showed that using IRNA resulted in the best translation, followed by BBC, then Hamshahri.
translation	Hamshahri	Again the result showed that using IRNA resulted in the best translation, followed by BBC, then Hamshahri.
Information Retrieval (IR)	TREC	Information Retrieval (IR) on unstructured data, e.g. Web documents and large textual collections, has been widely researched and is now a mature technology, partly thanks to evaluation campaigns such as TREC . The same goes for QA systems, which are currently able to solve complex questions by exploiting search engines as their information retrieval modules (see e.g. ().
Machine Transliteration Shared Task	NEWS 2011)	This report documents the Machine Transliteration Shared Task conducted as apart of the Named Entities Workshop (NEWS 2011), an IJCNLP 2011 workshop.
machine transliteration shared task	IJC-NLP 2011 workshop	To compare the performance of our system against state-of-the-art approaches, we participated in the machine transliteration shared task conducted as apart of the Named Entities Workshop), an IJC-NLP 2011 workshop.
alignment	GIZA++	In addition, we compare our proposed alignment method to GIZA++.
transliterating from English to Chinese	NEWS2011	For transliterating from English to Chinese, our combined system achieved Accuracy in Top-1 0.312, compared with the best performance in NEWS2011, which was 0.348.
accent prediction	WMT2009	 Table 1: Results (in CER-R, in %) of accent prediction on WMT2009 and MTLog
accent prediction	MTLog	 Table 1: Results (in CER-R, in %) of accent prediction on WMT2009 and MTLog
NE classification	HeiNER's entries	Those contexts provide useful training material for NE classification, thus the goal of this work is to add NE types to HeiNER's entries.
Emotion Modeling from Writer/Reader Perspectives	Microblog Dataset	Emotion Modeling from Writer/Reader Perspectives Using a Microblog Dataset
Sentic Corner	LiveJournal (LJ) (LiveJournal, 2011)	In order to test Sentic Corner's affect recognition capabilities, we evaluated the system with a corpus of mood-tagged blogs from LiveJournal (LJ) (LiveJournal, 2011), a virtual community of more than 23 millions users who keep a blog, journal or diary.
identification of emotional words	WordNet Affect lists	The sentences are then passed through the preprocessing steps followed by the identification of emotional words based on the WordNet Affect lists ().
parsing	Basque Dependency Treebank (BDT	There have been lots of attempts at parsing the Basque Dependency Treebank (BDT,, starting from the CoNLL 2007 Shared Task on Dependency Parsing (), where multiple systems competed on getting the best parsing results, and continued by the work done by.
digitize cultural heritage materials	RDF	During the past few years several projects have been undertaken to digitize cultural heritage materials ( through the use of Semantic Technologies such as RDF () and OWL ().
digitize cultural heritage materials	OWL	During the past few years several projects have been undertaken to digitize cultural heritage materials ( through the use of Semantic Technologies such as RDF () and OWL ().
translation shifts	LinEs parallel treebank	As far as translation studies are concerned, the FuSe project), for example, aims at studying translation shifts in an English-German corpus annotated with regard to the predicateargument structure, while the LinEs parallel treebank for focuses on this aspect by means of complete alignments of segment pairs.
Machine Translation	ATLAS-WCMS	Machine Translation is a key component of the ATLAS-WCMS and it will be embedded in all three services of the system.
question answering in legal domains	Katayama 07	a preliminary step to support tasks in legal text processing, such as translating legal articles into logical and formal representations and verifying legal documents, legal article retrieval, legal text summarization, question answering in legal domains, etc (Katayama 07;.
RRE task	Japanese National Pension Law corpus	This sub-section presents our corpus for the RRE task and evaluation method.The Japanese National Pension Law corpus includes 764 annotated Japanese law sentences 6 . Some statistics on this corpus are shown in.
downward tracing	FTrace	We discuss not only the current tool for downward tracing, but also the challenges that we face in the further development of FTrace, especially in upward tracing.
downward tracing	FTrace	It constitutes work in progress, and we therefore will discuss not only the current tool for downward tracing, but also the challenges that we face in the further development of FTrace, especially in upward tracing.
Semantic role labeling (SRL)	Framenet corpus	Semantic role labeling (SRL), as an NLP task based on annotated corpus, was first addressed by, employing Framenet corpus ().
Splitting	Fapesp-v2	 Table 1. Splitting of the Fapesp-v2 for training and test purposes
MT	WMT	State-of-the-art phrase-based SMT) is the most successful MT approach in many large scale evaluations, such as WMT, and.
MT	PLUTO	The performance of the patent MT systems in PLUTO is evaluated using a range of methods aimed not only at gauging general quality, but also identifying areas for improvement and relative performance against similar systems.
MT	Europarl	The approach shows significant improvement over the baseline for MT systems with limited training data and structural improvement for MT systems trained on Europarl.
MT	Europarl	The approach shows significant improvement over the baseline for MT systems with limited training data and structural improvement for MT systems trained on Europarl.
WSD	NIST	Results are evaluated in three ways: a manual evaluation of WSD performance from MT perspective, an analysis of agreement between the WSD-proposed equivalent and those suggested by the three systems, and finally by computing BLEU, NIST and METEOR scores for all translation versions.
WSD	UKB	For WSD we use UKB (Agirre and Soroa 2009), a graph-based algorithm that uses wordnet) and computes the probability of each sense of a polysemous word by taking into account the senses of context words.
WSD	UKB/wordnet-derived translation	Results are evaluated in several ways: • By manually evaluating WSD performance from the MT perspective, • By analysing the agreement between each of the MT systems and the UKB/wordnet-derived translation, • By comparing BLEU, NIST and ME-TEOR scores achieved with each translation version.
WSD	NIST	Results are evaluated in several ways: • By manually evaluating WSD performance from the MT perspective, • By analysing the agreement between each of the MT systems and the UKB/wordnet-derived translation, • By comparing BLEU, NIST and ME-TEOR scores achieved with each translation version.
MT	NIST	Finally, we wanted to see how the WSD/wordnet-based translation compares with the three MT systems using the BLEU, NIST and METEOR scores.
MT	NIST score	We can see that our generated version using disambiguated equivalents does not outperform any of the MT systems on any metric, except once when the WSD-align version outperforms Presis on the NIST score and comes fairly close to the Bing score.
chunk alignment	EnglishBengali parallel corpus	The objective of the present research work is to analyze effects of chunk alignment in EnglishBengali parallel corpus in a Phrase Based Statistical Machine Translation system.
MT	NIST	Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU () and NIST).
MT evaluation	NIST	Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU () and NIST).
NER	Ester2 evaluation campaign	This paper focuses on NER in the context of the Ester2 evaluation campaign (.
translation	Europarl corpus	For the English-German experiments, the translation system was trained and tested using apart of the Europarl corpus ().
VPC	CHILDES	 Table 3: VPC usage in CHILDES
Lexicon construction	CoBaLT editor	Lexicon construction and corpus annotation of historical language with the CoBaLT editor
Extraction de lexiques bilingues	Wikipédia	Extraction de lexiques bilingues à partir de Wikipédia
SMT training	Arabic Gigaword corpus (LDC2007T40)	For SMT training and language modeling (LM), we use 200M words from the Arabic Gigaword corpus (LDC2007T40).
language modeling (LM)	Arabic Gigaword corpus (LDC2007T40)	For SMT training and language modeling (LM), we use 200M words from the Arabic Gigaword corpus (LDC2007T40).
decoding weight optimization	NIST MT evaluation test set (MT04)	The decoding weight optimization is done using a set of 300 Arabic sentences from the 2004 NIST MT evaluation test set (MT04).
RL	USC/ICT.	Traditional RL algorithms require on the order * This work was done when the first author was a visiting researcher at USC/ICT. of thousands of dialogues to achieve good performance.
ASR	NPCEditor	We also have Baseline 2, which forwards all 3 ASR results to the NPCEditor (using child, male, and female AMs).
parsing coherence relations	Penn Discourse Tree Bank (PDTB)	In this paper we address the problem of parsing coherence relations as defined in the Penn Discourse Tree Bank (PDTB).
location identification task	DPOT framework	In this paper, we describe a belief tracking system fora location identification task that combines a semantic belief tracker for categorical concepts based on the DPOT framework (Raux and Ma, 2011) with a kernel density estimator that incorporates landmark evidence from multiple turns and landmark hypotheses, into a posterior probability over candidate locations.
belief tracking challenge task	Spoken dialog challenge corpus	This paper amplifies past informal discussions (Raux, 2011) to call fora belief tracking challenge task, based on the Spoken dialog challenge corpus (Black et al., 2011).
word alignment	WordNet	Regardless of the performance of the model in word alignment, if the model learns probabilities for senses that are reasonable, it can be used as a word sense disambiguation system for parallel corpora, with the candidate senses being made up from the senses out of WordNet.
sentiment analysis	Urdu blog data	The main aim of this work is to perform sentiment analysis on Urdu blog data.
Distribution	Fountas and Pinnell  reading	 Table 1: Distribution of books over Fountas and Pinnell  reading levels
word alignment	NEPSY Narrative Memory subtest	This method for word alignment and score extraction is general enough to be easily adapted to other tests used in neuropsychological evaluation, including not only those related to narrative recall, such as the NEPSY Narrative Memory subtest () but also picture description tasks, such as the Cookie Theft picture description task of the Boston Diagnostic Aphasia Examination () or the Renfrew Bus Story (.
word alignment	Boston Diagnostic Aphasia Examination	This method for word alignment and score extraction is general enough to be easily adapted to other tests used in neuropsychological evaluation, including not only those related to narrative recall, such as the NEPSY Narrative Memory subtest () but also picture description tasks, such as the Cookie Theft picture description task of the Boston Diagnostic Aphasia Examination () or the Renfrew Bus Story (.
word alignment	Renfrew Bus Story	This method for word alignment and score extraction is general enough to be easily adapted to other tests used in neuropsychological evaluation, including not only those related to narrative recall, such as the NEPSY Narrative Memory subtest () but also picture description tasks, such as the Cookie Theft picture description task of the Boston Diagnostic Aphasia Examination () or the Renfrew Bus Story (.
ML	ConText	The performance of the ML algorithms are compared to the rule-based system -ConText.
ID	UTurku	For ID, we find a different effect also on F-score, with all but one system showing reduced performance under the new criteria, with some very clear drops in performance; the only system to benefit is UTurku.
NER	GENIA	Previous NER approaches are mostly developed on a small number of gold-standard sets including GENIA ( and BioCreative () corpora.
WSD	UMLS Metathesaurus	This paper describes a large scale WSD system based on automatically labeled examples generated using information from the UMLS Metathesaurus.
WSD	UMLS Metathesaurus	This paper describes the development of a large scale WSD system that is able to disambiguate all terms that are ambiguous in the UMLS Metathesaurus.
WSD	NLM-WSD	The second supervised approach is to train the WSD system using manually labeled examples from the NLM-WSD and MSH-WSD corpora.
WSD	MSH-WSD corpora	The second supervised approach is to train the WSD system using manually labeled examples from the NLM-WSD and MSH-WSD corpora.
Segmentation	The Waste Land	 Table 2: Segmentation accuracy in The Waste Land
Trend Analysis	Harper's Bazaar	Trend Analysis in Harper's Bazaar
relation extraction	TAC	Recent work in relation extraction has reported challenges using Freebase to distantly supervise training data derived from news documents () and TAC's standard slot-filling task (.
interpreting noun compounds	Young touchdown pass.	For example, showed that a collection of distributional knowledge about football entities helped in interpreting noun compounds like "Young touchdown pass." used distributional information about entity types to achieve state-of-the-art coreference resolution results.
KB evaluation	TAC 2012 evaluation	In the next section we discuss the general problem of KB evaluation and present a concrete proposal for 68 evaluating a KB constructed from text, which will be implemented at the TAC 2012 evaluation.
Statistical MT (WMT)	NIST MetricsMatr	This is evidenced by the fact that there are now specific forums to present and test new metrics, such as the Workshop for Statistical MT (WMT) or the NIST MetricsMatr.
MT evaluation	Microsoft Research Paraphrase Corpus	To apply Sagan to MT evaluation, we first, preprocess the pairs from Microsoft Research Paraphrase Corpus) with dates and time normalization, and then optional modules are applied depending on the metric we want to calculate.
MT	NIST	We consider four widely used MT metrics (BLEU, NIST, METEOR (Banerjee and) (v0.7), and TER) as our baselines.
translations	Moses SMT system	The translations were produced by the Moses SMT system () trained on Europarl data.
translations	Europarl data	The translations were produced by the Moses SMT system () trained on Europarl data.
translation	Hiero	When it comes to translation time, the three smaller models (Hiero, phrase structure syntax, and CCG 1-best derivations) are significantly faster than the two larger ones.
Sentence level correlation	NIST	Sentence level correlation analysis, following standard NIST MetricsMATR protocol, shows that this fully automated version of HMEANT achieves significantly higher Kendall correlation with human adequacy judgments than BLEU, NIST, METEOR , PER, CDER, WER, or TER.
MT evaluation	NIST	For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU), NIST), METEOR (Banerjee and), PER (), CDER (), WER (), and TER).
MT evaluation	NIST	Other lexical similarity based automatic MT evaluation metrics, like NIST), ME-TEOR (), PER (), CDER (), WER (), and TER (), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumptionthat a good translation is one that shares the same lexical choices as the reference translationis not justified semantically.
translation	Europarl corpus	In order to train the translation model, we used the union of the Europarl corpus, the United Nations Organization (UNO) corpus and the News Commentary corpus.
translation	United Nations Organization (UNO) corpus	In order to train the translation model, we used the union of the Europarl corpus, the United Nations Organization (UNO) corpus and the News Commentary corpus.
translation	News Commentary corpus	In order to train the translation model, we used the union of the Europarl corpus, the United Nations Organization (UNO) corpus and the News Commentary corpus.
translations	PROMT DeepHybrid engine	The translations were made using the PROMT DeepHybrid engine, which is the first hybrid version of the PROMT system.
Translation Task	WMT-12	Kriya -The SFU System for Translation Task at WMT-12
SMT sys	European Union Seventh Framework Programme (FP7)	The DEPFIX system) attempts to correct some of the frequent SMT sys- * This research has been supported by the European Union Seventh Framework Programme (FP7) under grant agreement n • 247762 (Faust), and by the grants GAUK116310, GA201/09/H057 (Res-Informatica), and LH12093.
SMT sys	GAUK116310	The DEPFIX system) attempts to correct some of the frequent SMT sys- * This research has been supported by the European Union Seventh Framework Programme (FP7) under grant agreement n • 247762 (Faust), and by the grants GAUK116310, GA201/09/H057 (Res-Informatica), and LH12093.
SMT sys	GA201/09/H057	The DEPFIX system) attempts to correct some of the frequent SMT sys- * This research has been supported by the European Union Seventh Framework Programme (FP7) under grant agreement n • 247762 (Faust), and by the grants GAUK116310, GA201/09/H057 (Res-Informatica), and LH12093.
QC	Bridgeman art library 1	This study will focus on QC in art, culture and history domain, using the Bridgeman art library 1 , although our framework is general enough to be used in different domains.
parsing	French Treebank	We obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the French Treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach.
parsing	Penn Korean Treebank	To show the effect of the transformation methods more clearly, the Penn Korean Treebank () is used as another treebank for experimentation: (Chung et al., 2010) describes about major difficulties of parsing Penn Korean Treebank.
parsing	Penn Korean treebank	Actually, ( also investigated parsing accuracy on the Penn Korean treebank; the direct comparison could be very difficult because parsing criteria is different.
Formal annotation of language	Brown Corpus	Formal annotation of language data is an activity that dates back at least to the classic work of Kucera and Francis on the Brown Corpus.
Sentiment Polarity Detection	WordNet	This paper presents a novel approach in Sentiment Polarity Detection on Twitter posts, by extracting a vector of weighted nodes from the graph of WordNet.
classification	Stanford dataset	Using supervised machine learning approach, we achieve classification accuracy of 88% on Stanford dataset.
sentiment classification	Gold Standard	We subsequently tested the performance of the sentiment classification using the Gold Standard for the corresponding language, represented using the features of this model.
SMT	Gold Standard	 Table 3: For each language, each classifier has been  trained merging the translated data coming form differ- ent SMT systems, and tested using the Gold Standard.   *  Classifier is not able to discriminate between positive  and negative classes, and assigns most of the test points  to one class, and zero to the other.
Naive Bayes classification	NPS	 Table 3: Results of Naive Bayes classification applied to perverted-justice data and NPS chats.
factuality detection	CD DEV	 Table 2: Results for factuality detection (using gold nega- tion cues and scopes), reporting 10-fold cross-validation  on CD DEV and held-out testing on CD EVAL .
factuality detection	CD EVAL	 Table 2: Results for factuality detection (using gold nega- tion cues and scopes), reporting 10-fold cross-validation  on CD DEV and held-out testing on CD EVAL .
Hedge Detection	GMO Debates	Hedge Detection as a Lens on Framing in the GMO Debates: A Position Paper
Extracting Context-Rich Entailment	Wikipedia Revision	Extracting Context-Rich Entailment Rules from Wikipedia Revision History
relation mention identification task	AllTrue baseline	-Leave-one-out performance results on the relation mention identification task on the kdd09cma1 corpus (excluding the three tuning abstracts) by SDOI, SDOI with its feature space restricted to those originally proposed for TeGRR, and the AllTrue baseline.
MT outputs	DEPFIX	We test these two techniques on English-Czech MT outputs using our own reimplementation of the MST parser () named RUR 1 parser. and evaluate their contribution to the SMT post-editing quality of the DEPFIX system), which we outline in Section 5.
SMT post-editing	DEPFIX	We test these two techniques on English-Czech MT outputs using our own reimplementation of the MST parser () named RUR 1 parser. and evaluate their contribution to the SMT post-editing quality of the DEPFIX system), which we outline in Section 5.
SMT output parsing	WMT10	This leads us to believe that the two proposed methods are able to produce slightly better SMT output parsing results.: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and processed by DEPFIX.
SMT output parsing	WMT11	This leads us to believe that the two proposed methods are able to produce slightly better SMT output parsing results.: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and processed by DEPFIX.
SMT output parsing	WMT12 test sets	This leads us to believe that the two proposed methods are able to produce slightly better SMT output parsing results.: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and processed by DEPFIX.
SMT output parsing	DEPFIX	This leads us to believe that the two proposed methods are able to produce slightly better SMT output parsing results.: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and processed by DEPFIX.
SMT	WMT10	This leads us to believe that the two proposed methods are able to produce slightly better SMT output parsing results.: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and processed by DEPFIX.
SMT	WMT11	This leads us to believe that the two proposed methods are able to produce slightly better SMT output parsing results.: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and processed by DEPFIX.
SMT	WMT12 test sets	This leads us to believe that the two proposed methods are able to produce slightly better SMT output parsing results.: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and processed by DEPFIX.
SMT	DEPFIX	This leads us to believe that the two proposed methods are able to produce slightly better SMT output parsing results.: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and processed by DEPFIX.
SMT output parsed	DEPFIX	This leads us to believe that the two proposed methods are able to produce slightly better SMT output parsing results.: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and processed by DEPFIX.
SMT	WMT10	 Table 2: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and  UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and  processed by DEPFIX. The score of RUR+WORS+PARA is significantly higher at 95% confidence level than the scores  marked with '*' on the same data.
SMT	WMT11	 Table 2: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and  UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and  processed by DEPFIX. The score of RUR+WORS+PARA is significantly higher at 95% confidence level than the scores  marked with '*' on the same data.
SMT	WMT12 test sets	 Table 2: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and  UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and  processed by DEPFIX. The score of RUR+WORS+PARA is significantly higher at 95% confidence level than the scores  marked with '*' on the same data.
SMT	DEPFIX	 Table 2: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and  UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and  processed by DEPFIX. The score of RUR+WORS+PARA is significantly higher at 95% confidence level than the scores  marked with '*' on the same data.
SMT output parsed	DEPFIX	 Table 2: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and  UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and  processed by DEPFIX. The score of RUR+WORS+PARA is significantly higher at 95% confidence level than the scores  marked with '*' on the same data.
MT	NIST	For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER () because of their support on fast and inexpensive evaluation.
MT evaluation	NIST	For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER () because of their support on fast and inexpensive evaluation.
parse Chinese sentences	Chinese HPSG treebank	To parse Chinese sentences, we used Chinese Enju (, an HPSG-based parser trained with the Chinese HPSG treebank converted from Penn Chinese Treebank.
parse Chinese sentences	Penn Chinese Treebank	To parse Chinese sentences, we used Chinese Enju (, an HPSG-based parser trained with the Chinese HPSG treebank converted from Penn Chinese Treebank.
SMT evaluation	MT toolkit Moses	The baseline system was trained following the instructions of recent SMT evaluation campaigns) by using the MT toolkit Moses ( ) in its default configuration.
translation	CWMT extended corpus	 Table 7: Evaluation of translation quality of a test set when CWMT and CWMT extended corpus were used for  training. Results are given in terms of BLEU, RIBES, TER, and WER for baseline, head finalization, and proposed  refinement of head finalization reordering rules.
translation	XBRL European Business Registers 3 (xEBR) Working Group	For the translation dataset a financial ontology developed by the XBRL European Business Registers 3 (xEBR) Working Group was used.
translating xEBR ontology terms	NIST	Tables 4 to 5 illustrate the final results for our experiments on translating xEBR ontology terms, using the NIST), BLEU (), and Meteor (Lavie and) algorithms.
translation	MOSES phrasebased SMT decoder	For translation, MOSES phrasebased SMT decoder has been used.
Machine Transliteration Shared Task	NEWS 2012)	This report documents the Machine Transliteration Shared Task conducted as apart of the Named Entities Workshop (NEWS 2012), an ACL 2012 workshop.
NE tagger	CoNLL test set	The situation here is similar (see for results): the NE tagger trained on WP does not achieve as high performance tested against CoNLL test set (enwikiCoNLL) as one trained on its own train set (enwikienwiki).
coreference resolution	OntoNotes 5.0 corpus	This was an extension of the CoNLL-2011 shared task and involved automatic anaphoric mention detection and coreference resolution across three languages -English, Chinese and Arabic -using the OntoNotes 5.0 corpus, given predicted information on the syntax, proposition, word sense and named entity layers as input.
coreference resolution	WordNet	Since world knowledge is an important factor in coreference resolution, even in the closed task participants were allowed to use some limited, outside sources, including WordNet and a pre-computed table predicting number and gender information for noun phrases for the English task.
coreference resolution	MUC corpora	The first systematic learning-based study in coreference resolution was conducted on the MUC corpora, using a decision tree learner, by.
ICT	CoNLL-2012	ICT: System Description for CoNLL-2012
coreference resolution	CoNLL-2012 shared task	This paper presents a mixed deterministic model for coreference resolution in the CoNLL-2012 shared task.
coreference resolution	CoNLL-2012 shared task	We present a mixed deterministic model for coreference resolution in the CoNLL-2012 shared task).
coreference resolution	CONLL 2012 shared task	This paper describes a coreference resolution system for CONLL 2012 shared task developed by HLT_HITSZ group, which incorporates rule-based and statistic-based techniques.
coreference resolution	HLT_HITSZ	This paper describes a coreference resolution system for CONLL 2012 shared task developed by HLT_HITSZ group, which incorporates rule-based and statistic-based techniques.
coreference resolution	TEST set	The mention detection performance after coreference resolution is also significantly improved.: The results of our submitted system on the TEST set.
Translation Process Research (TPR)	Tobii	This technique is applied on the Translation Process Research (TPR) database  recorded by Tobii eye-tracker using Translog-II (Carl 2012) software.
Translation Process Research (TPR)	Translog-II (Carl 2012)	This technique is applied on the Translation Process Research (TPR) database  recorded by Tobii eye-tracker using Translog-II (Carl 2012) software.
question answering contests	NII Test Collection for IR Systems (NTCIR)	In recent years, many international question answering contests have been held at conferences and workshops, such as Text REtrieval Conference (TREC), Cross Language Evaluation Forum (CLEF) and NII Test Collection for IR Systems (NTCIR).
ASR	IBM Via voice	The most prevalent ASR systems among them are IBM Via voice and Microsoft SAPI.
ASR	Microsoft SAPI	The most prevalent ASR systems among them are IBM Via voice and Microsoft SAPI.
tagging corpora	BIS tagset	This paper aims at shedding light on the peculiarities of the Konkani language that have posed challenges in tagging corpora using the standard BIS tagset.
parsing	DMLRL	We outline here the parsing results on the six thesauri: DLR, completely parsed (175,000 entries; 15,000 pages; 37 volumes) at 98.01% accuracy, DAR completely parsed (25,000 entries, 3000 pages, 5 volumes), but no gold standard was available for automatic evaluation, while for TLF, DWB, GWB, and DMLRL, around 50 significant (including very large) entries have been parsed with very sound outcomes but not gold standard available for the parsing evaluation.
sentiment analysis	Dow Jones Industrial Average (DJIA)	Our approach departs from another major work into sentiment analysis for tracking the Dow Jones Industrial Average (DJIA) by) in that we do not pre-assign a sentiment lexicon or assume mood dimensions.
NER	MSFT	 Table 6: Overall NER experimental results  Stock market Accuracy with NER Accuracy without NER  AAPL  82.93%  73.17%  GOOG  80.49%  75.61%  MSFT  75.61%  68.29%  AMZN  75.00%  71.88%
NER	AMZN	 Table 6: Overall NER experimental results  Stock market Accuracy with NER Accuracy without NER  AAPL  82.93%  73.17%  GOOG  80.49%  75.61%  MSFT  75.61%  68.29%  AMZN  75.00%  71.88%
MT	Akkhor Bangla Software 2	The first available free MT system from Bangladesh was Akkhor Bangla Software 2 . The second available online MT system was apertium based Anubadok . These systems used Rule-Based approach and did not handle OOV Words considering low-resource scenario.
clustering CSTs	WordNet-Online	For clustering CSTs, we used <lexical filename> information for each words, provided by WordNet-Online . Translaing OOV words using WordNet did not quantify the translation quality improvement).
clustering CSTs	WordNet	For clustering CSTs, we used <lexical filename> information for each words, provided by WordNet-Online . Translaing OOV words using WordNet did not quantify the translation quality improvement).
translation	EBMT	To improve the translation quality, we implemented the proposed method in EBMT.
MT	NIST	The effectiveness of the approach was tested on 5 English to Hindi MT systems and it was observed that the system-level DELiC4MT scores correlate well with the scores produced by the most commonly used automatic evaluation metrics (BLEU, NIST, METEOR and TER) while providing finer-grained information.
MT evaluation	NIST	The state-of-the-art methods for automatic MT evaluation are represented by BLEU () and closely related NIST), METEOR (Banerjee and and TER ().
parsing Hindi	ICON	This work on parsing Hindi loosely follows from our contribution to ICON).
Hindi Dependency Parsing	MaltParser	Two-stage Approach for Hindi Dependency Parsing Using MaltParser
dependency parsing	Hindi Shared Task on Parsing, COLING 2012	In this paper, we present our approach towards dependency parsing of Hindi language as apart of Hindi Shared Task on Parsing, COLING 2012.
MT and parsing in Indian Languages	Coling 2012	We report our results on both development and test data provided in the Hindi Shared Task on Parsing at workshop on MT and parsing in Indian Languages, Coling 2012.
MT and parsing in Indian Languages	Coling 2012	We report our results on both development and test data provided in the Hindi Shared Task on Parsing at workshop on MT and parsing in Indian Languages, Coling 2012.
MT and parsing in Indian Languages	Coling 2012	We report our results on both development and test data provided in the Hindi Shared Task on Parsing at workshop on MT and parsing in Indian Languages, Coling 2012.
SMT	Stanford Phrasal library	To build a baseline SMT system, we used the Stanford Phrasal library trained on EuroParl corpus (.
SMT	EuroParl corpus	To build a baseline SMT system, we used the Stanford Phrasal library trained on EuroParl corpus (.
Translation	Meteor	 Table 1: Translation quality of ML4HMT-12 submissions measured using Meteor, NIST, and BLEU  scores for language pair Spanish→English. Best system per metric printed in bold face.
Translation	NIST	 Table 1: Translation quality of ML4HMT-12 submissions measured using Meteor, NIST, and BLEU  scores for language pair Spanish→English. Best system per metric printed in bold face.
syllabus retrieval	Online Course Catalogue (UTOCC)	We performed a practical application of the system for syllabus retrieval for the University of Tokyo's Online Course Catalogue (UTOCC), 3 for the Open Course Ware (UT-OCW) site, and for the syllabus-structuring (SS) site 5 for the School/Department of Engineering.
syllabus retrieval	Open Course Ware (UT-OCW) site	We performed a practical application of the system for syllabus retrieval for the University of Tokyo's Online Course Catalogue (UTOCC), 3 for the Open Course Ware (UT-OCW) site, and for the syllabus-structuring (SS) site 5 for the School/Department of Engineering.
question answering evaluation	NTCIR	Many international question answering evaluation tracks have taken place at conferences and workshops, such as TREC 1 , CLEF 2 , and NTCIR 3 to improve question-answering systems.
disambiguation rules discovery	POS-Tagger	The particle swarm optimization algorithm for disambiguation rules discovery is outlined in section 4 and the POS-Tagger in section 5.
MRs	World Health Organization's 9th Revision of the International Classification of Diseases 1 (ICD-9)	In our context, the MRs produced in a hospital have to be classified with respect to the World Health Organization's 9th Revision of the International Classification of Diseases 1 (ICD-9).
Extending	Chinese Treebank Annotation	Extending and Scaling up the Chinese Treebank Annotation
word segmentation bake-off	Chinese micro-blog data in the 2nd CIPS-SIGHAN joint conference	This paper describes the model we designed for the word segmentation bake-off on Chinese micro-blog data in the 2nd CIPS-SIGHAN joint conference on Chi-nese language processing.
segmentation	MicroBlog corpora	However, the performance of segmentation is not so satisfying for MicroBlog corpora.
CRF-based learning	People's Daily corpus (PDC)	On one hand, it is difficult to obtain large-scale labelled corpora of micro-blog domain for CRF-based learning, and the only labelled corpus we have is People's Daily corpus (PDC) which comes from the News domain; on the other, compared with the News text, the micro-blog text contains a large number of new words, name entities, URLs, emoticons (such as ":)"), punctuation patterns (such as "...."), as well as structured symbols representing conversation ("@"), repost("//@"), and topic ("#...#") etc.
segmentation	MicroBlog corpora	However, the performance of segmentation is not so satisfying for MicroBlog corpora.
Phrase Structure Grammar (PSG) parsing	CIPS-Bakeoff2012 Task3	We describe our method of traditional Phrase Structure Grammar (PSG) parsing in CIPS-Bakeoff2012 Task3.
parsing	CLP2012	There are two parsing tasks in CLP2012: the simplified Chinese parsing task (task 3) and the traditional Chinese parsing task (task 4).
parsing	Tsinghua Chinese Treebank	We present experiments which show that parsing with hierarchical state-splitting is fast and accurate on Tsinghua Chinese Treebank.
parsing	Sinica Treebank	This paper describes the methods used for the parsing the Sinica Treebank for the bakeoff task of SigHan 2012.
parsing	SigHan 2012	This paper describes the methods used for the parsing the Sinica Treebank for the bakeoff task of SigHan 2012.
parsing	Sinica Treebank	Based on the statistics of the training data and the experimental results, we show that the major difficulties in parsing the Sinica Treebank comes from both the data sparse problem caused by the fine-grained annotation and the tagging ambiguity.
parsing	Sinica Treebank	In this paper, we analyze the difficulties in parsing the Sinica Treebank.
parsing	Sinica Treebank	One should note that we do not argue the parsing performance of the Sinica Treebank and CTB are directly comparable.
parsing	CTB	One should note that we do not argue the parsing performance of the Sinica Treebank and CTB are directly comparable.
parsing	Sinica Treebank	However, we do believe that the difference between the statistics of the two Treebanks helps to identify some difficulties in parsing the Sinica Treebank.
tagging	Sinica Treebank	These results illustrate that improving tagging accuracy can significantly boosting parsing performance on the Sinica Treebank.
parsing	Sinica Treebank	These results illustrate that improving tagging accuracy can significantly boosting parsing performance on the Sinica Treebank.
Adapting Multilingual Parsing Models	Sinica Treebank	Adapting Multilingual Parsing Models to Sinica Treebank
NI resolution	FrameNet-based data set	While obtain reasonable results for NI resolution within a restricted PropBankbased scenario, the accuracies obtained on the FrameNet-based data set provided for the) are much lower.
parsing	WordNet	In order to obtain this information, we must rely on parsing and on other two resources, WordNet(Miller) and SUMO.
presupposition projection	DRT framework	In van der empirically-driven theory of presupposition projection, formalized in the DRT framework, this discrepancy between compositionality and empirical soundness becomes very clear: presuppositions are only resolved in a second stage of processing by moving them from an embedded context to their context of interpretation.
TE	RTE-5	In this spirit, the main TE forum, the yearly Recognising Textual Entailment (RTE) Challenge, has created a number of datasets that incorporate the properties of particular tasks, such as Semantic Search in RTE-5 ( ) or Novelty Detection in RTE-7).
parsing	ACE	The first step in the pipeline is the parsing process with ACE.
Responding to a query with a query	British National Corpus	Responding to a query with a query is a common occurrence, representing on a rough estimate more than 20% of all responses to queries found in the British National Corpus.
Ontology authoring	CNL	Ontology authoring aided by CNL has been addressed from various points.
NER	VA NLP shared-task corpus (i2b2 2010)	As a gold standard for clinical NER, the fourth i2b2/VA NLP shared-task corpus (i2b2 2010) for extracting concepts of the classes-problems, treatments, and tests-is used.
Switchboard Dialogue Act Corpus	LDC	The Switchboard Dialogue Act Corpus is distributed by LDC.
Translation	MT08 test sets	Translation quality is measured in truecase with BLEU () on the MT08 test sets.
Phrase tables	Jane toolkit	Phrase tables are in the Jane toolkit's binarized format.
automatic detection of metaphors	WordNet	Several approaches to automatic detection of metaphors have been proposed;), all of which rely on the availability of extensive manually crafted lexical resources such as WordNet, VerbNet, FrameNet, TreeBank, etc.
metaphor identification	WordNet	Most existing work on metaphor identification;) 1 has relied on some or all of handwritten rules, syntactic parsing, and semantic databases like WordNet) and FrameNet (.
SemEval'10 CrossLingual Word Sense Disambiguation task	Europarl	The data sets used for the SemEval'10 CrossLingual Word Sense Disambiguation task were constructed by making a 'sense inventory' of all possible target language translations of a given source language word based on word-alignments in Europarl (, with alignments involving the relevant source words being manually checked.
PMI	DISCO workshop	The poor results achieved by employing PMI are similar to the results of random baselines and in accordance with those of participants of the DISCO workshop).
POS tagging	MWEannotated PTB	We conduct POS tagging experiments on the MWEannotated PTB, using sections 0-18 for training and sections 22-24 for test as usual.
sentiment analysis	Irish General Election of February 2011	In this paper we describe sentiment analysis experiments in a more complicated setup: the task is three-class positive/negative/neutral classification, the sentiment being classified is not at the general document level but rather directed towards a topic, the documents are tweets, and the topic is politics, specifically the Irish General Election of February 2011.
clustering	The Waste Land	We show that clustering The Waste Land is a fairly difficult task, though we can do much better than random baselines, particularly if we begin with a good initial segmentation.
multi-emotion classification	Geneva Emotion Wheel (GEW)	In multi-emotion classification, we show that it is highly accurate in classifying tweets into 20 emotion categories of the Geneva Emotion Wheel (GEW) (.
sentiment classification of tweets	SemEval 2013 training data	The results of these evaluations are shown in.: Results in terms of accuracy for 10-fold crossvalidation using different combinations of features for the sentiment classification of tweets on the entire set of SemEval 2013 training data.
Diagnostic classification AUC	NERRUTTMOD	 Table 7: Diagnostic classification AUC using automatically extracted NERRUTTMOD
native language identification (NLI) task	BEA-8	This paper describes MITRE's participation in the native language identification (NLI) task at BEA-8.
Classification	PTB	 Table 2: Classification accuracy results for POS n-grams  of size N using both the PTB and RASP tagset. The larger  RASP tagset performed significantly better for all N.
Classification	RASP tagset	 Table 2: Classification accuracy results for POS n-grams  of size N using both the PTB and RASP tagset. The larger  RASP tagset performed significantly better for all N.
parsing	Penn Treebank	Many such representations exist, and are routinely employed to improve performance on the widely studied task of parsing the Penn Treebank.
Native Language Identification (NLI)	NLI shared task 2013	This paper describes our approaches to Native Language Identification (NLI) for the NLI shared task 2013.
identification of pathologies	RadLex taxonomy	The results show that the identification of pathologies is not feasible by only using (1) suffixes to determine diseases and (2) available pathology descriptions from the RadLex taxonomy.
parser adaptation	Genia treebank	Three different methods of parser adaptation for the biomedical domain have been proposed by) who, starting from the results of unknown word rate experiments carried out on the Genia treebank, adapted a PTB-trained parser by improving the Part-Of-Speech tagging accuracy and by relying on an external domain-specific lexicon.
question understanding	Watson system	These collectively developed approaches to question understanding were successfully applied and expanded upon in IBM's Watson system).
identification of gene and protein mentions	BioNLP	The identification of gene and protein mentions ('named entity recognition') is a central task and a prerequisite for any follow-up work in BioNLP.
Genia Event Extraction task	BioNLP Shared Task 2013	The Genia Event Extraction task is organized for the third time, in BioNLP Shared Task 2013.
ASM	TEES	 Table 2: Impact of adding extra training data to the  ASM method. top5k,20k,30k: using the top 5,000,  20,000, and 30,000 events. pt1k: using the top  1,000 events per event-type. trx4: following the  training bias of events, with a multiplying factor  of four. For TEES we always use the top 10,000  events. Evaluated over GE13dev.
ASM	GE13dev	 Table 2: Impact of adding extra training data to the  ASM method. top5k,20k,30k: using the top 5,000,  20,000, and 30,000 events. pt1k: using the top  1,000 events per event-type. trx4: following the  training bias of events, with a multiplying factor  of four. For TEES we always use the top 10,000  events. Evaluated over GE13dev.
coreference resolution	BioNLP'11 task 1	To the best of our knowledge, pipeline system incorporating domain adaptation and coreference resolution, is the best biomedical event extraction system on BioNLP'11 task 1 so far.
coreference resolution task	BioNLP'11	Furthermore, the coreference resolution task separated from event extraction task in BioNLP'11 is integrated to BioNLP'13 task 1, and there are more event types in the BioNLP'13 task 1 than those in BioNLP'11 task 1.
Genia Event Extraction task	BioNLP 2013	We describe a biological event detection method implemented for the Genia Event Extraction task of BioNLP 2013.
Relation Identification from Assertions	BIOlogical Taxonomy	Building A Contrasting Taxa Extractor for Relation Identification from Assertions: BIOlogical Taxonomy & Ontology Phrase Extraction System
Bacteria Biotope task	BioNLP Shared Task 2013	This paper presents the Bacteria Biotope task of the BioNLP Shared Task 2013, which follows BioNLP-ST-11.
Bacteria Biotope task	BioNLP-ST-11	This paper presents the Bacteria Biotope task of the BioNLP Shared Task 2013, which follows BioNLP-ST-11.
Bacteria Biotope Task	BioNLP 2011 Shared Task	The Bacteria Biotope Task () is one of the new challenges in this domain, which was firstly presented in the BioNLP 2011 Shared Task.
Entity Detection and Categorization)	BioNLP 2013 Shared Task	In this paper, we present two systems, one for Sub-task 1 (Entity Detection and Categorization) and one for Sub-task 2 (Localization Relation Extraction) of the Bacteria Biotope Task in the BioNLP 2013 Shared Task.
Localization Relation Extraction)	BioNLP 2013 Shared Task	In this paper, we present two systems, one for Sub-task 1 (Entity Detection and Categorization) and one for Sub-task 2 (Localization Relation Extraction) of the Bacteria Biotope Task in the BioNLP 2013 Shared Task.
information extraction	IRISA-TexMex	This paper describes the information extraction techniques developed in the framework of the participation of IRISA-TexMex to BioNLP-ST13.
information extraction	BioNLP-ST13	This paper describes the information extraction techniques developed in the framework of the participation of IRISA-TexMex to BioNLP-ST13.
Adapting	SimpleNLG	Adapting SimpleNLG for bilingual English-French realisation
MIME	Babytalk BT-Nurse system	MIME was inspired by the Babytalk BT-Nurse system, which generates shift handover reports for nurses in a neonatal intensive care unit.
translation task	WMT09-WMT12	This was the outcome of the manual evaluation of the translation task from WMT09-WMT12.
SMT	WMT13	We perform SMT experiments in all language pairs of the WMT13) and obtain SMT performance close to the baseline Moses () system using less resources for training.
translation task	WMT12	Thus the size of the LM corpora is 10M plus the number of sentences in the training set as given in: Best BLEUc results obtained on the translation task together with the LM order used when obtaining the result compared with the best constrained Moses results in WMT12 and WMT13.
translation task	WMT13	Thus the size of the LM corpora is 10M plus the number of sentences in the training set as given in: Best BLEUc results obtained on the translation task together with the LM order used when obtaining the result compared with the best constrained Moses results in WMT12 and WMT13.
translation task	WMT12	 Table 3: Best BLEUc results obtained on the translation task together with the LM order used when  obtaining the result compared with the best constrained Moses results in WMT12 and WMT13. The last  row compares the BLEUc result with respect to using a different LM order.
translation task	WMT13	 Table 3: Best BLEUc results obtained on the translation task together with the LM order used when  obtaining the result compared with the best constrained Moses results in WMT12 and WMT13. The last  row compares the BLEUc result with respect to using a different LM order.
SMT	CzEng corpus	Our baseline SMT system (Moses) trained on CzEng corpus only was then also used for WMT 2013 test set translation, and we obtained smt (newstest2013).
SMT	newstest2013	Our baseline SMT system (Moses) trained on CzEng corpus only was then also used for WMT 2013 test set translation, and we obtained smt (newstest2013).
SPE	PHRASEFIX	We trained abase SPE system as described in Section 2.1 and dubbed it PHRASEFIX.
translation task	WMT 13	This paper describes shallow semanticallyinformed Hierarchical Phrase-based SMT (HPBSMT) and Phrase-Based SMT (PBSMT) systems developed at Dublin City University for participation in the translation task between EN-ES and ES-EN at WMT 13.
translation task	WMT13	For the corpus, we used all the resources provided for the translation task at WMT13 for lan- Experimental results are shown in.
translations	RWTH	The translations were joined using the RWTH's system combination approach.
translation	KIT	However, the translation quality of the single systems provided by KIT is stable.
rescoring	newstest2011 set	For rescoring, we used the newstest2011 set as tuning set and re-optimized the parameters with MERT on 1000-best lists.
Translation	NIST	 Table 3: Translation results, shown in lowercase NIST BLEU. Bold results correspond to submitted  systems.
Machine Translation	DCU	This paper presents the experiments conducted by the Machine Translation group at DCU and Prompsit Language Engineering for the WMT13 translation task.
Machine Translation	Prompsit Language Engineering	This paper presents the experiments conducted by the Machine Translation group at DCU and Prompsit Language Engineering for the WMT13 translation task.
Machine Translation	DCU	This paper presents the experiments conducted by the Machine Translation group at DCU 1 and Prompsit Language Engineering 2 for the WMT13 translation task on three language pairs: SpanishEnglish, French-English and German-English.
statistical machine translation (SMT) of patents	International Patent Classification (IPC)	present multi-task learning for statistical machine translation (SMT) of patents from different classes (so-called sections) according to the International Patent Classification (IPC) . While the vocabulary may differ between the different IPC sections, specific legal jargon and atypical textual structure will be shared across IPC sections.
SMT	QET 2013 challenge	In particular, we do not use the baseline software or the SMT resources provided with the QET 2013 challenge.
MT evaluation	NIST	The traditional MT evaluation metrics require reference translations in order to measure a score reflecting some aspects of its quality, e.g. the BLEU and NIST.
Sentence Length	English-Spanish Parallel Corpus	 Table 1: Sentence Length Statistics for the  English-Spanish Parallel Corpus
Tunable Machine Translation Evaluation	WMT13 Metrics	A Description of Tunable Machine Translation Evaluation Systems in WMT13 Metrics Task
MT evaluation task	ACL WMT	In the MT evaluation task, the Spearman rank correlation coefficient method is usually used by the authoritative ACL WMT to evaluate the correlation of different MT evaluation metrics.
MT evaluation	NIST	show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), CDER (), WER (), and TER).
PSMT	GermanEnglish	We hypothesize that PSMT can be as successful for GermanEnglish as the more computationally costly HSMT approach, provided that the reordering-related parameters are carefully chosen and the best available reordering models are used.
phrase orientation scoring	Chinese→English 2008 NIST task 2	We evaluate the effect of phrase orientation scoring in hierarchical translation on the Chinese→English 2008 NIST task 2 and on the French→German language pair using the standard WMT 3 newstest sets for development and testing.
phrase orientation scoring	WMT 3 newstest sets	We evaluate the effect of phrase orientation scoring in hierarchical translation on the Chinese→English 2008 NIST task 2 and on the French→German language pair using the standard WMT 3 newstest sets for development and testing.
Tuning	MERT	Tuning was performed via MERT using newstest2010 as development set; test sentences were extracted from newstest2011.
HMT alignment	EnglishJapanese dataset	We evaluate the performance of our HMT alignment model in terms of the standard alignment error rate 2 (AER) on a publicly available EnglishJapanese dataset, and compare it with the IBM Model 4 ( and HMM alignment with distance-based (HMM) and syntax-based (S-HMM) distortion models (.
MT	DWL	In contrast, we try to feedback information about possible errors of the MT system into the DWL.
Spelling normalization	Anselm texts	Spelling normalization with the Norma tool shows rather positive results even for small training samples: with only 100 tokens used for training, it achieves a normalization accuracy of 69% for the Anselm texts, and raises the score for the GerManC-GS texts by 5-10 percentage points.
Spelling normalization	GerManC-GS texts	Spelling normalization with the Norma tool shows rather positive results even for small training samples: with only 100 tokens used for training, it achieves a normalization accuracy of 69% for the Anselm texts, and raises the score for the GerManC-GS texts by 5-10 percentage points.
parsing	Vietnamese Treebank	Analysis of the treebank and parsing errors revealed how problems with the Vietnamese Treebank influenced the parsing results and real difficulties of Viet-namese parsing that required further improvements to existing parsing technologies .
Query Segmentation	Microsoft Research Lab India	The closely related task of Query Segmentation is of special interest to us here, as it is * The work was done during author's internship at Microsoft Research Lab India.
Converting	Italian Treebanks	Converting Italian Treebanks: Towards an Italian Stanford Dependency Treebank
Converting	Italian Stanford Dependency Treebank	Converting Italian Treebanks: Towards an Italian Stanford Dependency Treebank
SD	Turku treebank	SD has already been applied to different languages, e.g. Finnish in the Turku treebank, Swedish in the Talbanken treebank 2 , Chinese in the Classical Chinese Literature treebank or Persian in the Uppsala Persian Dependency Treebank (.
SD	Talbanken treebank 2	SD has already been applied to different languages, e.g. Finnish in the Turku treebank, Swedish in the Talbanken treebank 2 , Chinese in the Classical Chinese Literature treebank or Persian in the Uppsala Persian Dependency Treebank (.
SD	Classical Chinese Literature treebank	SD has already been applied to different languages, e.g. Finnish in the Turku treebank, Swedish in the Talbanken treebank 2 , Chinese in the Classical Chinese Literature treebank or Persian in the Uppsala Persian Dependency Treebank (.
SD	Uppsala Persian Dependency Treebank	SD has already been applied to different languages, e.g. Finnish in the Turku treebank, Swedish in the Talbanken treebank 2 , Chinese in the Classical Chinese Literature treebank or Persian in the Uppsala Persian Dependency Treebank (.
Parsing	ISDT	 Table 1: Parsing results with ISDT resources
Importing MASC	ANNIS linguistic database	Importing MASC into the ANNIS linguistic database: A case study of mapping GrAF
Importing MASC	GrAF	Importing MASC into the ANNIS linguistic database: A case study of mapping GrAF
coreference	Prague Dependency Treebank	Generic noun phrases and annotation of coreference and bridging re- lations in the Prague Dependency Treebank
semantic similarity evaluation	SEMEVAL-2012 STS gold	Since we could not find any reference corpus for semantic similarity evaluation apart from the SEMEVAL-2012 STS gold that was also acquired via crowdsourcing, we resorted to training a machine learning classifier and comparing relative performance on the collected training data.
sentiment analysis	NTCIR	Therefore during the last years, two workshops on the evaluation of sentiment analysis systems were organized within the framework of Russian Information Retrieval Seminar ROMIP . In many respects ROMIP seminars are similar to other international information retrieval events such as TREC and NTCIR, which have already conducted 1 http://romip.ru/en/index.html different sentiment analysis tracks.
sentiment analysis	ROMIP-2011	In this paper we partly overview the sentiment analysis tasks proposed at ROMIP-2011 and, the data prepared for evaluation (and therefore available for other interested researchers), and the results obtained by participants.
news quotation classification task	ROMIP 2012	 Table 3: Best results for the news quotation classification task in ROMIP 2012
Sentence splitting	Europarl v3 Preprocessing Tools	Sentence splitting was done with a tool from Europarl v3 Preprocessing Tools (http://www.statmt.org/europarl) written by Philipp Koehn and Josh Schroeder.
NER	CoNLL	The concept of NER originated in the 1990s in the course of the Message Understanding Conferences (, and since then there has been a steady increase in research boosted by evaluation programs such as CoNLL) and ACE).
NER	ACE	The concept of NER originated in the 1990s in the course of the Message Understanding Conferences (, and since then there has been a steady increase in research boosted by evaluation programs such as CoNLL) and ACE).
Message Understanding Conferences	CoNLL	The concept of NER originated in the 1990s in the course of the Message Understanding Conferences (, and since then there has been a steady increase in research boosted by evaluation programs such as CoNLL) and ACE).
Message Understanding Conferences	ACE	The concept of NER originated in the 1990s in the course of the Message Understanding Conferences (, and since then there has been a steady increase in research boosted by evaluation programs such as CoNLL) and ACE).
cross-language identification of semantic relations	WordNet	We propose a method for cross-language identification of semantic relations based on word similarity measurement and mor-phosemantic relations in WordNet.
MT	NIST	We carried out evaluation of the MT quality using two automatic MT evaluation metrics: BLEU () and NIST.
MT	NIST	We carried out evaluation of the MT quality using two automatic MT evaluation metrics: BLEU () and NIST.
SMT	WMT 2011 guidelines	The SMT systems are trained with the Moses toolkit (, according to the WMT 2011 guidelines . The translation performance was measured using the BLEU evaluation metric on a single reference translation.
SMT	Wikipedia data set	When replacing the out-of-domain: SMT results for German-French translation model with a combined one (including the Wikipedia data set) and keeping only the adapted language models, we can observe two tendencies.
first story detection	WordNet	In the case of Twitter, Petrovi´c showed improvements on first story detection by using paraphrases extracted from WordNet.
Machine Translation	NIST	For Machine Translation we report two automatic evaluation scores, BLEU and NIST.
translation	CWMT	 Table 3: Evaluation of translation quality of two  test sets when CWMT, News and the combination  of both corpora were used for training.
SMT	SFLM	Section 8 gives a short overview of the data and tools used to buildup our SMT system and shows the experimental details of our system using SFLM and the morphological analyzer MORPH2.
SMT	MORPH2	Section 8 gives a short overview of the data and tools used to buildup our SMT system and shows the experimental details of our system using SFLM and the morphological analyzer MORPH2.
MT	NIST	Eextrinsic evaluation was carried out on the MT quality using BLEU () and NIST).
translation	NIST (NIST 2002)	To objectively evaluate the translation accuracy, four automatic evaluation metrics have been chosen, namely BLEU (), NIST (NIST 2002), Meteor (Denkowski and Lavie, 2011) and TER ().
MT evaluation	NIST	The first group of approaches include human judges ratings of simplification, content preservation, and grammaticality, standard MT evaluation scores (BLEU and NIST), a variety of other automatic metrics (perplexity, precision/recall/F-measure, and edit distance).
TS	NIST	The recent approaches considering TS as an MT task, such as Specia (2010),, and, apply standard MT evaluation techniques, such as BLEU (), NIST, and TERp ().
MT task	NIST	The recent approaches considering TS as an MT task, such as Specia (2010),, and, apply standard MT evaluation techniques, such as BLEU (), NIST, and TERp ().
MT evaluation	NIST	The recent approaches considering TS as an MT task, such as Specia (2010),, and, apply standard MT evaluation techniques, such as BLEU (), NIST, and TERp ().
CW discovery	Simple Wikipedia 1 edit histories	Instead, our CW discovery process (presented in Section 2) mines simplifications from Simple Wikipedia 1 edit histories.
summarisation	LSA	Then we show our summarisation approach based on LSA (Section 3).
AN task	Fulllex	In the AN task, Lexfunc significantly outperforms Fulllex and Dil and, visually, its distribution is slightly more skewed towards lower (better) ranks than any other model.
SMT	EN/CZ text	In the following, we describe a series of SMT experiments that made direct use of the EN/CZ text as provided with the PCEDT.
SMT	PCEDT	In the following, we describe a series of SMT experiments that made direct use of the EN/CZ text as provided with the PCEDT.
tuning	Docent	First of all, we need to extract documents for tuning and testing with Docent.
Arabic	S&B data  set	 Table 3: Results for Arabic on the S&B data  set
Hebrew	S&B data  set	 Table 4: Results for Hebrew on the S&B data  set
sentence compression	Edinburgh compression corpus	The task of sentence compression in particular has benefited from the availability of a number of useful resources such as the the Ziff-Davis compression corpus) and the Edinburgh compression corpus) which make compression problems highly relevant for datadriven approaches involving language generation.
replicating	MC dataset	Such procedure has been demonstrated by in replicating ratings for the MC dataset, achieving close inter-annotator agreement with expert raters.
terminology extraction	ukWaC corpus	From each of the two crawls, we randomly sample 20,000 reviews, which we use as foreground corpora for the terminology extraction task . As a background corpus, we utilize a 100,000 document subset (randomly sampled) of the "ukWaC corpus" ().
Nara Institute of Science and Technology (NAIST) error correction	CoNLL 2013 Shared Task	This paper describes the Nara Institute of Science and Technology (NAIST) error correction system in the CoNLL 2013 Shared Task.
SMT-based	m2scorer	 Table 4: Results of the submitted system for each type of error and results of additional experiments  with the SMT-based system. The score is evaluated on the m2scorer
SMT	NUCLE v2.3 corpus	We first built a baseline SMT system using only the NUCLE v2.3 corpus and compared it to other systems trained on incremental additions of the remaining corpora.
GEC	NUCLE corpus	Generally, for GEC on annotated data such as the NUCLE corpus () in this year's shared task which contains both original errors and human annotations, there are two main types of approaches.
resolution	Ay-mara	Since overt marking of information structure is partial, this paper also devotes considerable attention to the resolution of underspecification in Ay-mara.
standardization	W3C Community Group	In the last years, we have been developing the lemon model for this purpose that has formed the initial input for standardization activities carried on in the context of the W3C Community Group on the ontology-lexicon interface.
Lexical Match alignment	Italian MWN glosses	 Table 2: Results for Lexical Match alignment  adding the Italian MWN glosses.
Norms and Exploitations	PDEV	This work draws on )#(*%+%& ,-eory of Norms and Exploitations) and applies Corpus Pattern Analy-%4%& ,'& #& %:=%.,& '0& <."=%& 0"'2& >.<4(+%& 56'4%'(+& class (class 42.2) [asphyxiate, crucify, drown, hang, knife, poison, smother, stab, strangle, suffocate: those verbs available in PDEV at the time of writing],).
Simile identification	VUAMC	 Table 2: Simile identification performance, with  respect to the 53 instances of mFlag=like annota- tion in VUAMC. LEXBL is the baseline that re- trieves all sentences that contain the preposition  like. The last column measures the number of re- trieved instances.
recognition	PHOENIX	More context than ±2 frames degrades recognition accuracy on PHOENIX, capturing too much information of the following glosses.
RST	RST treebank Carlson, Marcu, and Okurowski 2002 and the Potsdam commentary corpus Stede 2004	We focus on two theories: RST, which offers the model for the annotations of the RST treebank Carlson, Marcu, and Okurowski 2002 and the Potsdam commentary corpus Stede 2004, and on SDRT, which counts several small corpora annotated with semantic scopes, Discor.
Moves	Find Corpus	 Table 3: Moves Statistics in Find Corpus
generative trackers	DSTC data	The deterministic and generative trackers are detailed in Section 2 and the presented models are evaluated on the DSTC data in Section 3.
generative dialogue (GT) trackers	DSTC data	The discriminative (DT) and generative dialogue (GT) trackers described in Sections 2.1 and 2.2 were evaluated on the DSTC data.
belief tracking	DSTC	The successful use of discriminative models for belief tracking has recently been alluded to by Williams (2012a) and, and was a prominent theme in the results of the DSTC.
Valence alternations	HPSG grammar	Valence alternations and marking structures in a HPSG grammar for Mandarin Chinese
identification of events	English side	The identification of events on English side, we have followed the guidelines of TimeML view ().
identification of events	TimeML view	The identification of events on English side, we have followed the guidelines of TimeML view ().
Response Generation	POMDP	Response Generation Based on Hierarchical Semantic Structure with POMDP Re-ranking for Conversational Dialogue Systems
ASR	Bakeoff 2013 evaluation	Although, we have already applied our parser and LM to ASR and achieved many successes, we would like to take the chance of Bakeoff 2013 evaluation to examine again how generalization and sophistication our parser and LM are.
spelling check	Bakeoff 2013 sub-task 2	These results show the benefits of combining CRF-based parser and LM in the second stage of spelling check system.: Evaluation results of the proposed system on Bakeoff 2013 sub-task 2.
spelling correction selection	Academia Sinica Segmentation Corpus	In our system, we employ the SRILM Toolkit() to build n-gram models for spelling correction selection from the Academia Sinica Segmentation Corpus(3.0)).
Extracting and Aggregating False Information	Microblogs	Extracting and Aggregating False Information from Microblogs
Alignment	Kyoto Free Translation Task	Alignment is performed using the unsupervised aligner GIZA++ for Japanese-Chinese, and the supervised aligner Nile for Japanese-English (, with the alignment models being trained on the alignments distributed with the Kyoto Free Translation Task.
parsing morphologically-rich languages	SPMRL 2013	SPMRL has also been host to discussions on realistic and appropriate evaluation methods that can be applied in the face of morphological and/or segmentation ambiguities; these discussions have culminated in the first shared task for parsing morphologically-rich languages, co-located with SPMRL 2013.
Parsing Croatian and Serbian	Croatian Dependency Treebankš	Parsing Croatian and Serbian by Using Croatian Dependency Treebankš
dependency parsing	Penn Arabic Treebank (ATB)	In this paper, we describe CTF-TMs, which can be used fora wide variety of NLP tasks, and present our Arabic CTF-TM for Arabic tokenization, affix detection, affix labeling, part-of-speech tagging, and dependency parsing as well as the results obtained in applying it to our dependency conversion of the Penn Arabic Treebank (ATB) ( ).
tokenization	ATB part 3	Our system achieves tokenization accuracy similar to state-of-the-art system fora standard split of the ATB part 3, and, in our experiments using ATB parts 1-3, our system achieves the highest labeled attachment, unlabeled attachment, and clitic separation figures (including pronomial clitics) for Arabic yet reported (although no other work can be compared directly).
constituency parsing	SPMRL 2013 shared task datasets	While the realistic scenario of predicting both MWEs and syntax has already been investigated for constituency parsing, the SPMRL 2013 shared task datasets offer the possibility to investigate it in the dependency framework.
MWE recognition	French dataset	In that case, errors in MWE recognition alleviate their positive effect on parsing performance . While the realistic scenario of syntactic parsing with automatic MWE recognition (either done jointly or in a pipeline) has already been investigated in constituency parsing, the French dataset of the) offers one of the first opportunities to evaluate this scenario within the framework of dependency syntax.
parsing	French dataset	In that case, errors in MWE recognition alleviate their positive effect on parsing performance . While the realistic scenario of syntactic parsing with automatic MWE recognition (either done jointly or in a pipeline) has already been investigated in constituency parsing, the French dataset of the) offers one of the first opportunities to evaluate this scenario within the framework of dependency syntax.
MWE recognition	French dataset	In that case, errors in MWE recognition alleviate their positive effect on parsing performance . While the realistic scenario of syntactic parsing with automatic MWE recognition (either done jointly or in a pipeline) has already been investigated in constituency parsing, the French dataset of the) offers one of the first opportunities to evaluate this scenario within the framework of dependency syntax.
dependency parsing	FEATS	Following this line of research, our first step will be to determine which is the concrete value of each feature on dependency parsing, adding one of the morphological features at a time starting with an empty FEATS column.
SMA	Morfette	This paper revisits the work of (Malladi and Mannem, 2013) which focused on building a Statistical Morphological Analyzer (SMA) for Hindi and compares the performance of SMA with other existing statistical analyzer, Morfette.
SMA	Morfette	SMA is compared with a baseline system, Morfette and two versions of the PBA wherever relevant.
SMA	PBA	SMA is compared with a baseline system, Morfette and two versions of the PBA wherever relevant.
SMA	HTB	SMA builds models for lemma, gender, number, person and case prediction trained on the training data of the HTB.
parsing	IMS	In this section, our focus is on comparing parsing results across constituency and dependency parsers based on the protocol of We have only one submission from IMS:SZEGED:CIS in the constituency track, and. from the same group, a submission on the dependency track.
distance metric learning	Mahalanobis distance metric	One of the most common approaches to distance metric learning is to learn a Mahalanobis distance metric.
tokenization	GATE NLP platform	Preprocessing steps including tokenization, sentence splitting and partof-speech tagging were accomplished using ANNIE components in GATE NLP platform).
newborn screening for rare diseases	National Health Service	Thus, newborn screening for rare diseases has become a routine procedure in USA, Canada and the member states of the European Union. or the Newborn Screening website by the National Health Service Emergence of user-friendly online technologies prompted the general public to turn to the Internet to gain more knowledge on health-related issues, a phenomenon often referred to as Dr. Google.
porting linguistic and semantic annotation	Austrian Baroque Corpus (ABaC:us)	We describe work on porting linguistic and semantic annotation applied to the Austrian Baroque Corpus (ABaC:us) to a format supporting its publication in the Linked Open Data Framework.
Token tagging	Catalan- Spanish datasets	 Table 6: Token tagging accuracy on the Catalan- Spanish datasets.
identification	Ijekavian variant	As it will be reported, the identification of Ijekavian variant is a much more difficult task since the observed tools are not adopted to it at all.
Size	WordNets	 Table 1: Size of the WordNets
definition assignment	Onto.PT v0.4.1	The accuracy of the definition assignment step was estimated to be between 79-80% for Onto.PT v0.4.1, with 0.62 κ agreement.
SWN	META-SHARE 1 repositories	Recently, anew impetus to the enhancement and upgrade of the SWN was given by the CESAR project, in the scope of which many Polish, Slovak, Hungarian, Croatian, Serbian and Bulgarian resources were thoroughly described by meta-data and made public through the META-SHARE 1 repositories).
Aligning Word Senses	DWDS Dictionary of the German Language	Aligning Word Senses in GermaNet and the DWDS Dictionary of the German Language
sense marking	WordNet	The sense marking activity mainly helped in validation of WordNet and improving the WordNet coverage.
sense marking	WordNet coverage	The sense marking activity mainly helped in validation of WordNet and improving the WordNet coverage.
validation	WordNet	The sense marking activity mainly helped in validation of WordNet and improving the WordNet coverage.
validation	WordNet coverage	The sense marking activity mainly helped in validation of WordNet and improving the WordNet coverage.
sense marking	WordNet	Section 5 gives the details about how sense marking activity helped in improving the quality of the WordNet, followed by the conclusion and future work.
MT	Assamese WordNet synsets	This paper further continues with a description of Previous Notable Work done while implementing a MT system for other Indian Languages in Section2, Section3 portrays our methodology to implement a English-Assamese MT system . This section starts with a description of tools used in implementing our system, an explanation of our English-Assamese parallel corpus, a system architecture where it gives us a overview of our baseline translation system, an elaboration of our transliteration system and the process of enhancing the translation output through mapping with the Assamese WordNet synsets.
MT	Assamese WordNet synsets	This paper further continues with a description of Previous Notable Work done while implementing a MT system for other Indian Languages in Section2, Section3 portrays our methodology to implement a English-Assamese MT system . This section starts with a description of tools used in implementing our system, an explanation of our English-Assamese parallel corpus, a system architecture where it gives us a overview of our baseline translation system, an elaboration of our transliteration system and the process of enhancing the translation output through mapping with the Assamese WordNet synsets.
Lexical Match alignment	BabelNet data	 Table 2: Results for Lexical Match alignment with  extensions with BabelNet data and MWN Italian  glosses.
Lexical Match alignment	MWN Italian  glosses	 Table 2: Results for Lexical Match alignment with  extensions with BabelNet data and MWN Italian  glosses.
dialog tracking task	DSTC	In the next section we formally introduce the dialog tracking task together with datasets used in the DSTC.
MT evaluation	ACL WMT workshop series	The advances in statistical machine translation over the past years have been driven to a large extent by frequent (friendly) competitive MT evaluation campaigns, such as the shared tasks at the ACL WMT workshop series ( and IWSLT (, and the NIST Open MT Evaluation.
MT evaluation	NIST Open MT Evaluation	The advances in statistical machine translation over the past years have been driven to a large extent by frequent (friendly) competitive MT evaluation campaigns, such as the shared tasks at the ACL WMT workshop series ( and IWSLT (, and the NIST Open MT Evaluation.
MT evaluation	IBM	In order to study the behaviour of automatic MT evaluation scores, we conducted three experiments by applying IBM BLEU () and Meteor) to a sample of KOPTE translations that were produced by translation students preparing for their final master's exams.
SMT	English Wikipedia)	The fact that our proposed method outperforms SMT implies that using such readily-available monolingual data (English Wikipedia) is a better way to exploit crosslingual information.
quotation mining	Google Books corpus	The only approach known to us that can be paralleled to ours is the one described in Kolak and Schilit (2008) for quotation mining within the Google Books corpus with algorithm searching for verbatim quotations only.
estimation	BoosTexter	For the estimation of age using quotebased features, a boosting approach was followed using BoosTexter).
domain adaptation	MEDLINE abstracts	Experimental results show that domain adaptation with active learning and target domain instance weighting achieves performance on MEDLINE abstracts similar to a system trained on coref-erence annotation of only target domain training instances, but with a greatly reduced number of target domain training instances that we need to annotate.
translation	SNOMED CT's Terminology content	We have designed the translation algorithm and the first two phases of the algorithm that feed the SNOMED CT's Terminology content, have been implemented (it is composed of four phases).
classification	WEKA	As classification algorithm, we used the Sequential Minimal Optimization (SMO) implementation in WEKA, which marginally outperformed (1-1.5%) some other classification algorithms (J48 Decision tree, Logistic Regression and Random Forest) we tried in initial experiments.
classification	J48 Decision tree	As classification algorithm, we used the Sequential Minimal Optimization (SMO) implementation in WEKA, which marginally outperformed (1-1.5%) some other classification algorithms (J48 Decision tree, Logistic Regression and Random Forest) we tried in initial experiments.
abbreviation expansion	SEPR	Results from abbreviation expansion using semantically related words with filtering and normalization to refine the selection of expansions on SEPR and SEPR-X development sets are shown in.
abbreviation expansion	SEPR-X development sets	Results from abbreviation expansion using semantically related words with filtering and normalization to refine the selection of expansions on SEPR and SEPR-X development sets are shown in.
normalization	Turkish Tweeter Data Set	These two problems compose almost the quarter (26.5%) of the normalization errors within a 25K Turkish Tweeter Data Set.
tagging and parsing	WSJ	For example, one such well-known bias from the tagging and parsing literature is what we may refer to as the WSJ FALLACY.
SCR	WebCrow system	Additionally, we measured the impact of our best model for SCR in the WebCrow system by comparing with it.
Temporal Scoping of Relational Facts	Wikipedia	Temporal Scoping of Relational Facts based on Wikipedia Data
translation	MOSES	The experiments were carried outwith MOSES 1.0 4 (Philipp . The translation and the re-ordering model utilizes the "grow-diag-final" symmetrized word-to-word alignments created with GIZA++ 5 and the training scripts of MOSES.
grammatical error correction	CoNLL 2014	There have been four shared tasks on grammatical error correction, with the most recent edition hosted at CoNLL 2014.
Repeats	AZELLA	The final generated holistic scores for Repeats were scaled to a 0 − 4 range and non-Repeat holistic scores were scaled to a 0 − 10 range to satisfy an AZELLA design requirement that Repeat items count for 4 points and non-Repeats count for 10 points.
Surprisal	Wall Street Journal corpus	 Table 1: Surprisal values of two example relative-clause sentences. The values were computed using a  top-down parser by Roark et al. (2009) trained on the Wall Street Journal corpus.
scoring of spontaneous speech	WordNet	also used a vector space model for the scoring of spontaneous speech, but extended it by using the ontological information contained in WordNet.
feature extraction	LightSide 7 toolkit	However, in our work both feature extraction and model learning are performed using the LightSide 7 toolkit.
argumentation	Hutchby	Our coarse-grained scheme for argumentation is based on Pragmatic Argumentation Theory (PAT); Hutchby,.
splitting of meaning representations	Groningen Meaning Bank	We propose a set of heuristics that render the splitting of meaning representations feasible on a large-scale corpus, and present a method for grammar induction capable of extracting a semantic CCG from the Groningen Meaning Bank.
question answering	Groningen Meaning Bank (GMB)	While approaches are available that work on small corpora focused on specific domains (such as Geoquery and Freebase QA for question answering), we are not aware of any approach that allows the extraction of a semantic CCG from a wide-coverage corpus such as the Groningen Meaning Bank (GMB) (.
grammar induction	Earth Negotiations Bulletin	In the following we motivate our use of the ADIOS algorithm for grammar induction (2.1), and introduce the Earth Negotiations Bulletin (2.2).
SVM	Reuters evaluation set	shows the performance of SVM with BM25 weighting on our Reuters evaluation set versus several baselines.
Sentiment classification	Exp	 Table 3: Sentiment classification accuracy (average 10-fold cross-validation), Scott's π, Krippendorff's  α, Cohen's κ and trade returns of different feature sets and term frequency weighting schemes in Exp. 3.  The same folds were used for the different representations. The non-annualized returns are presented in  columns 3-6.
sentiment polarity prediction	Wikipedia AfD forum	To evaluate the performance of our sentiment polarity prediction algorithm, we randomly selected 236 sentences from the Wikipedia AfD forum and manually annotated their sentiment polarity.
classification	MaxEnt	For classification, we experimented with Naive Bayes and MaxEnt (via MALLET 2 ) and SVMs (via LIBSVM 3 ).
SD level classification	FirstP	 Table 4: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the average value of G F 1 , M F 1 and H  F 1 . SDTM outperforms all other methods com- pared. The difference between SDTM and FirstP  is statistically significant (p-value < 0.05 for ac- curacy, < 0.0001 for Avg F 1 ).
Tracking Political Sentiment	Microblog Data	Towards Tracking Political Sentiment through Microblog Data
Augmenting FrameNet	PPDB Center	Augmenting FrameNet Via PPDB Center for Language and Speech Processing
Dementia	Montreal Cognitive Assessment (MoCA)	Currently, AD can only be diagnosed by examining the patient's brain after death and Dementia is diagnosed typically through consensus using specific diagnostic criteria and extensive neu-ropsychological examinations with tools such as the Mini-Mental State Examination (MMSE) or the Montreal Cognitive Assessment (MoCA).
ASD	American Psychiatric)	The American Psychiatric Association defines the two characteristics of ASD as: 1) persistent deficits in social communication and social interaction across multiple contexts, and 2) restricted, repetitive patterns of behavior, interests, or activities (American Psychiatric).
DDep	MyPersonality dataset	To operationalize DDep, we use the depression facet scores of the "Big 5" item pool) from the MyPersonality dataset.
MT	USHEFF	For Task 1.1, data from multiple MT systems was explicitly used by USHEFF though the idea of consensus translations.
translation of news text	WMT	As distinguished from our EU-BRIDGE joint submission to the IWSLT 2013 evaluation campaign, we particularly focused on translation of news text (instead of talks) for WMT.
Exact Decoding	IMS-TTT submission to WMT14	Large-scale Exact Decoding: The IMS-TTT submission to WMT14 *
TMs	News Commentary parallel corpus	To train the TMs, first a baseline is built using the News Commentary parallel corpus.
TMs	MOSES	For training TMs, we use MOSES () version 2.1 with MGIZA++.
TMs	MGIZA++	For training TMs, we use MOSES () version 2.1 with MGIZA++.
MT	Apertium dictionar- ies	 Table 5. A slight im- provement over the baseline is observed, which  motivates the use of synthetic rules in our final MT  system. This small improvement may be related  to the small coverage of the Apertium dictionar- ies: the English-French bilingual dictionary has a  low number of entries compared to more mature  language pairs in Apertium which have around 20  times more bilingual entries.
MT	Apertium	 Table 5. A slight im- provement over the baseline is observed, which  motivates the use of synthetic rules in our final MT  system. This small improvement may be related  to the small coverage of the Apertium dictionar- ies: the English-French bilingual dictionary has a  low number of entries compared to more mature  language pairs in Apertium which have around 20  times more bilingual entries.
SMT	WMT14 test set	 Table 7: Case sensitive results obtained with  our final English-French SMT system on new- stest2013 when experimenting with different de- coding parameters. The best parameters are kept  to translate the WMT14 test set (newstest2014)  and official results are reported in the last two  rows.
Machine Translation 2014	Manawi MT system	In this paper, we present Saarland University (USAAR) submission to Workshop for Machine Translation 2014) using the Manawi MT system.
SMT model training	MOSES toolkit	The output from these NLP tools was appended to the training corpus prior to the SMT model training with the MOSES toolkit ().
MT	WMT test set	 Table 1: BLEU scores of the English-to-Hindi MT  Systems on the WMT test set.
translation	MGIZA++ 11	The experiments were carried outwith the Moses 1.0 10 ( . The translation and the re-ordering model utilizes the "growdiag-final" symmetrized word-to-word alignments created with MGIZA++ 11 and the training scripts from Moses.
DCU Terminology Translation	WMT14	The DCU Terminology Translation System for the Medical Query Subtask at WMT14
Medical Translation Shared Task	WMT 2014	Experiments in Medical Translation Shared Task at WMT 2014
MT	WMT 2012 participant systems	 Table 1: Accuracy of randomized significance tests for Spanish-to-English MT with four automatic  metrics, based on the WMT 2012 participant systems.
MT	WMT 2012 participant systems	 Table 2: Accuracy of randomized significance tests for English-to-Spanish MT with four automatic  metrics, based on the WMT 2012 participant systems.
SMT	WMT13	Our SMT system is a standard PBSMT system trained on WMT13: Pearson correlations between gold standard word alignment evaluation on the link level and on translation unit level.
HTER prediction	WMT13 data	Best HTER prediction was achieved by adding dedupli-cated WMT13 data and additional features such as (a) rule-based language corrections (language tool) (b) PCFG parsing statistics and count of tree labels (c) position statistics of parsing labels (d) position statistics of tri-grams with low probability.
SMT system extrinsic machine translation performance predictor	QET12	develop the Machine Translation Performance Predictor (MTPP), a state-ofthe-art, language independent, and SMT system extrinsic machine translation performance predictor, which can predict translation quality by looking at the test source sentences and becomes the 2nd overall after also looking at the translation outputs as well in QET12.
Smoothing	NIST evaluation toolkit	Smoothing 3 is implemented in the standard official NIST evaluation toolkit (mteval-v13a.pl).
named entity recognition (NER)	GenBank geographic metadata	We then evaluate four different named entity recognition (NER) systems which may help in the automatic extraction of information from related articles that can be used to improve the GenBank geographic metadata.
normalization task	BioCreative challenges	The normalization task has been highlighted in the BioCreative challenges, where a variety of methods have been explored for normalizing gene names, including string matching, pattern matching, and heuristic rules.
Predicting	ICD-10-PCS Codes from Electronic Health Records	A System for Predicting ICD-10-PCS Codes from Electronic Health Records
relation extraction task	SpanishDrugEffectBD database	Regarding to the results for the relation extraction task, shows the overall results obtained using a baseline system, which considers all pairs (drug, effect) occurring in messages as positive relation instances, and a second approach using the SpanishDrugEffectBD database (a relation instance is positive only if it is found into the database).
recognition of such ambiguous mentions	MEDLINE	(ii) We show that the recognition of such ambiguous mentions is important as their string representation is frequent in collections such as MEDLINE.
word alignment learned	CREG corpus	The basis for our specific paraphrasing method is word alignment learned from parallel corpora which we create from the available data in the CREG corpus (Corpus for Reading Comprehension Exercises for German).
automatic correction of Arabic	QALB corpus	The Qatar Arabic Language Bank (QALB) project 1 is one of the first large scale data and system development efforts for automatic correction of Arabic which has resulted in annotation of the QALB corpus.
NER	Egyptian Dialect (EGY)	In this paper, we present an NER system for DA specifically focusing on the Egyptian Dialect (EGY).
DA	Egyptian Dialect (EGY)	In this paper, we present an NER system for DA specifically focusing on the Egyptian Dialect (EGY).
Evaluating Distant Supervision	Arabic Twitter Feeds	Evaluating Distant Supervision for Subjectivity and Sentiment Analysis on Arabic Twitter Feeds
Sentiment Analysis	Arabic Twitter Feeds	Evaluating Distant Supervision for Subjectivity and Sentiment Analysis on Arabic Twitter Feeds
SMT	Stanford ATB segmenter	We tried different values of its parameter, and we trained it using corpora of different sizes to find balanced settings that improve SMT quality as compared with no segmentation and with segmentation using the Stanford ATB segmenter.
segmentation	Stanford ATB segmenter	We tried different values of its parameter, and we trained it using corpora of different sizes to find balanced settings that improve SMT quality as compared with no segmentation and with segmentation using the Stanford ATB segmenter.
SMT	QCA parallel corpus	shows the results for our SMT system when trained on the QCA parallel corpus, which was segmented using different training models of Morfessor with B = 40.
SMT	Qatari segmented data	 Table 4: The effect of varying the perplexity  threshold parameter B of Morfessor on SMT qual- ity. "After merging" are the results using the post- processed Qatari segmented data.
Name tagging	Train-S	 Table 3: Name tagging results on Dev with Train-S
Tagging	Train-L	 Table 6: Tagging results on Dev using Train-L
ASR	SEAME corpus	This section presents all the experiments and results regarding language models and ASR on the development and the evaluation set of the SEAME corpus.
translation	Jane toolkit	All translation experiments are performed with the Jane toolkit ().
MT	WMT13	 Table 2. The top four MT systems for the en-ru  translation task at WMT13. The scores were  calculated for the subset of translations which we  used in experiments.
MT	RNNLM	In Section 4, we present our experiments and results on reranking the MT output using RNNLM.
adjunct identification	Europarl corpus	We evaluate adjunct identification accuracy using a set of 100 English and French sentences, drawn randomly from the Europarl corpus.
Translation-equivalence	Hansards	 Table 6: Translation-equivalence of adjuncts  in the manual Hansards
Predicting Attrition	UIUC	Predicting Attrition Along the Way: The UIUC Model
proper name translation	JSL	This paper describes proper name translation from Japanese into JSL.
dialog state tracking	DSTC	Thus the effort to separately improve the performance of dialog state tracking as carried out in the DSTC maybe justified.
dialog state tracking	DSTC 2 dataset	To demonstrate the effectiveness of our proposed sequential labeling approach for dialog state tracking, we performed experiments on the DSTC 2 dataset which consists of 3,235 dialog sessions on restaurant information domain which were collected using Amazon Mechanical Turk.
dialog state tracking	Amazon Mechanical Turk	To demonstrate the effectiveness of our proposed sequential labeling approach for dialog state tracking, we performed experiments on the DSTC 2 dataset which consists of 3,235 dialog sessions on restaurant information domain which were collected using Amazon Mechanical Turk.
Adapting	SimpleNLG	Adapting SimpleNLG for Brazilian Portuguese realisation
PoS tagger	Scottish Gaelic	This paper describes an ongoing project that seeks to develop the first automatic PoS tagger for Scottish Gaelic.
Cross-lingual Transfer Parsing	Irish Case	Cross-lingual Transfer Parsing for Low-Resourced Languages: An Irish Case Study
cross-lingual parsing of Irish	Irish Dependency Treebank	We then experiment with the universally annotated treebanks often languages from four language family groups to assess which languages are the most useful for cross-lingual parsing of Irish by using these treebanks to train delexicalised parsing models which are then applied to sentences from the Irish Dependency Treebank.
Predicting sense convergence	CogALex-IV 2014 shared task	Predicting sense convergence with distributional semantics: an application to the CogALex-IV 2014 shared task
SemEval-2012 task	ACL wiki	 Table 6: The top five results with SemEval-2012 task 2, from the ACL wiki. MaxDiff is a measure which  ranges from 0 to 100%, the higher the better.
predicting non/literal items	VUAMC	 Table 1: Results (t scores) of logistic regression model for predicting non/literal items  from the VUAMC, n=1855 (nb. p-values are shown by asterisks, ***=p<.0001, **=p<.001, *=p<.01)
question answering	DBpedia knowledge base	Finally, we show how these semantics can be helpful in practical applications of question answering over the DBpedia knowledge base.
translation	ETB	In our case, we favour first the translation provided by ETB.
terminology extraction	English-to-Hindi data sets	We manually evaluate our novel terminology extraction model on English-to-Spanish and English-to-Hindi data sets, and observe excellent performance for all domains.
Identifying Evidence	Microblogs	A Corpus Study for Identifying Evidence on Microblogs
Answer classification	CoMiC	 Table 4: Answer classification accuracy with the CoMiC system
User acceptance testing (UAT)	Alveo	User acceptance testing (UAT) constituted an integral part of the development and evolution of Alveo and we present the distributed testing organisation, the test development process and the evolution of the tests.
Multimodal Analysis and Recommendation	UIMA	EUMSSI: a Platform for Multimodal Analysis and Recommendation using UIMA
CP formation	HUTB	Thus, we construct a seed list of nouns known to partake in CP formation in the HUTB.
bracketing	Wikipedia	Section 4 presents the bracketing algorithm, and Section 5 the implementation of a word association model using Wikipedia.
sentiment analysis	HealthAffect	The paper is organized as follows: Section 2 presents related work in sentiment analysis, Section 3 introduces the data set and the annotation results, Section 4 presents HealthAffect, Section 5 describes the automated sentiment recognition experiments, and Section 6 discusses the results.
SUP	VVA	However, Annotator 1's scores for SUP and VVA are good approximations of how well a human being should be expected to perform and the system's scores should be compared to Annotator 1 (i.e., accounting for the adjudicator's bias).
POS tagging	STTS	Our findings for POS tagging show that Morfette reaches the highest accuracy on UTS and overall on unknown words while TnT reaches the best performance for STTS and the RF-Tagger for STTSmorph.
POS tagging	STTSmorph	Our findings for POS tagging show that Morfette reaches the highest accuracy on UTS and overall on unknown words while TnT reaches the best performance for STTS and the RF-Tagger for STTSmorph.
parsing	SPMRL 2013 dependency corpus	In the present study we turn to parsing (section 5.1), and attempt to apply knowledge-rich targeted features for coordination to the SPMRL 2013 dependency corpus for French (.
sentence analysis/synthesis	Electronic dictionary of words	We describe a sentence analysis/synthesis application in music domain, MusiTAL, we have conceived from data described in Dubois' Electronic dictionary of words and developed by means of the ILLICO software.
SVM	MaxEnt	(2) The accuracy rate of SVM has slightly less than our model, but the results of MaxEnt and CR-F is unbalanced.
Microblog text segmentation	Microblog	Another notable problem is the Microblog text segmentation because Microblog has become anew Internet literary which is different from the genres of common text.
Slot filling task	TAC KBP workshop	Slot filling task has been proposed as one of shared tasks in the TAC KBP workshop since 2009.
slot filling	TAC KBP workshop	The task of person attributes extraction in Chinese text in CLP 2014 bakeoff is designed on the basis of the slot filling task in the TAC KBP workshop.
Slot filling task	TAC KBP workshop science 2009	Slot filling task has been one of shared tasks in the TAC KBP workshop science 2009.
TCT parse	TCT data	Firstly, we automatically transformed all the TCT parse trees into CCG derivation trees by using the TCT2CCG tool (Zhou, 2011), and built a CCG bank version 1.0 for the TCT data.
Personal Attributes Extraction	CLP2014	Extraction system for Personal Attributes Extraction of CLP2014
Personal Attributes Extraction	CLP2014	Extraction system for Personal Attributes Extraction of CLP2014
speech recognizer (ASR)	Bake-off 2013 evaluation	Although, these two components are originally designed for automatic speech recognizer (ASR), the system did get some success on Bake-off 2013 evaluation . These results have confirmed the generalization and sophistication of our parser and LM.
Sentiment Analysis	Skipgrams	A Supervised Approach for Sentiment Analysis using Skipgrams
segmentation	SAS®  Text Miner	 Table 1: Effect of the segmentation tool of SAS®  Text Miner.
SMT	Kytea	 Table 9: Evaluation results for Chinese-Japanese translation across two SMT systems (baseline and  baseline + additional quasi-parallel data), Moses version:1.0, segmentation tools: Kytea.
SMT	Kytea	 Table 10: Evaluation results for Chinese-Japanese translation across two SMT systems (baseline and  baseline + additional quasi-parallel data), Moses version: 2.1.1, segmentation tools: Kytea.
resolving prepositional phrase attachment	VERBNET	We propose and investigate an approach to resolving prepositional phrase attachment that centers around the ways of incorporating semantic knowledge derived from the lexico-semantic ontologies such as VERBNET and WORDNET.
Lexical semantics	VERBNET	Developments in the field of Lexical semantics have made such systematic large scale datasets, including) and VERBNET (), a reality.
Annotation of Measure Expressions	ISO Standards	The Annotation of Measure Expressions in ISO Standards
ontology construction	Protégé	In order to support ontology construction, ontology editors such as Protégé have to be adapted as well.
sentiment detection	Internet Argument Corpus (IAC)	Successful models of these tasks have many possible applications in sentiment detection, automatic summarization, argumentative agents (, and in systems that support human argumentative behavior . Our research examines FACTUAL versus FEELING argument styles, drawing on annotations provided in the Internet Argument Corpus (IAC) (.
grammatical error correction	CoNLL 2014	There have been three shared tasks on grammatical error correction with the most recent edition hosted at CoNLL 2014.
classification	TOEFL11 training data	We report classification accuracy under 10-fold cross-validation using the TOEFL11 training data and also on the test set from the 2013 shared task, shown in.
WDS Subscales	Pretest	 Table 3: Descriptives for WDS Subscales at Pretest
MT shared task	WMT	A common procedure (e.g. conducted in the MT shared task at WMT) consists of ranking MT translations.
event coreference resolution	ECB+ corpus	 Table 5: Sentence template approach to event coreference resolution evaluated on the ECB+ corpus in MUC, B3,  mention-based CEAF, BLANC and CoNLL F in comparison to the singleton baseline BL.
event coreference resolution	MUC	 Table 5: Sentence template approach to event coreference resolution evaluated on the ECB+ corpus in MUC, B3,  mention-based CEAF, BLANC and CoNLL F in comparison to the singleton baseline BL.
Encoding event structure	VerbNet	Encoding event structure in Urdu/Hindi VerbNet
Sampling of	Reuters Corpus	 Table 3: Sampling of Topics from Reuters Corpus for Annotation
parsing	French and Dutch treebanks	 Table 3: Performance of the parsing models on the  French and Dutch treebanks, with respect to parsing  results (F1 score and exact match) and the MWE-F1  score, for sentences ≤ 40 words.
MWE	NEMWEL	shows some examples of MWE candidates extracted by NEMWEL.
MWE extraction	Turkish Treebanks	Inspired by these recent studies, to shed light and provide a direction for future studies on adequate MWE extraction techniques for Turkish, in this paper we present our annotation for MWEs on recently introduced Turkish Treebanks.
Predicate-argument mapping	Xinhua  News	 Table 2: Predicate-argument mapping counts on Xinhua  News, where only verb predicate annotations were avail- able or verb and nominal/adjective predicate annotations.  V c represents Chinese verb predicates, N e represents En- glish nominal/adjective predicates
negation detection	NegEx Corpus	Since we focus on negation detection, we selected only assertions corresponding to the positive and negative classes from the five assertion classes in the corpus, which simulates the type of data found in the NegEx Corpus.
metaphor identification	VU Amsterdam corpus	The use of supervised machine learning techniques for metaphor identification has increased manyfold in the recent years (see section 10, Related Work, fora review and references), partially due to the availability of largescale annotated resources for training and evaluating the algorithms, such as the VU Amsterdam corpus (, datasets built as part of a U.S. government-funded initiative to advance the state-of-art in metaphor identification and interpretation (, and recent annotation efforts with other kinds of data).
normalization	FST	The normalization table was subsequently inverted and compiled into a FST.
Sentence level word normalization	English Twitter data	 Table 3: Sentence level word normalization on English Twitter data
Sentence level phrase normalization	English Twitter data	 Table 5: Sentence level phrase normalization on English Twitter data
sense induction	ukWaC corpus	For sense induction, we sample 1800 instances of very target word at random, from the ukWaC corpus.
relation extraction	ACE 2005 dataset	These experiments are carried out for relation extraction on the ACE 2005 dataset via 5-fold cross validation.
relation classification task	SemEval 2010 dataset	In order to further verify the effectiveness of the system, we test the system on the relation classification task with the SemEval 2010 dataset and compare the results with the state-of-the-art systems in this area.
NER task	CoNLL 2003 benchmark	 Table 1. Performance on the NER task, using the  CoNLL 2003 benchmark.
paraphrase detection	Switchboard corpus	To evaluate the five different CBOW-based models proposed in Section 3, we use the following datasets: similarity of transitive verbs with multiple senses from Grefenstette and Sadrzadeh (2011a), three-word sentence similarity from , paraphrase detection from, and dialog act tagging for the Switchboard corpus) from.
synonyms and definitions	UKWaC	The best result we get for synonyms and definitions is 0.706, while for lists from UKWaC, it is 0.693.
WSD	UKB	To evaluate our new WSD system, we applied it to two test sets and first compared it to a number of baselines, and finally to UKB, a well-known graph-based WSD system.
sentence compression training	Danish DSim corpus	For the sentence compression training and evaluation data we extracted a subset of ordinary and simplified newswire texts from the Danish DSim corpus ().
parsing	English Web Treebank (EWT)	The re-trained parser obtains improved parsing accuracy on a range of different data sets, including the five web domains of the English Web Treebank (EWT) () and the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB).
parsing	Wall Street Journal (WSJ)	The re-trained parser obtains improved parsing accuracy on a range of different data sets, including the five web domains of the English Web Treebank (EWT) () and the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB).
parsing	Penn Treebank (PTB)	The re-trained parser obtains improved parsing accuracy on a range of different data sets, including the five web domains of the English Web Treebank (EWT) () and the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB).
parsing	UD treebank	We also provide a first set of experiments comparing the parsing scores of language-specific treebank annotation to that of a UD treebank, providing an evaluation of both the conversion quality and the feasibility of UD annotation as a parsing target.
PoS Tagging	Icelandic Gold Standards	Analysing Inconsistencies and Errors in PoS Tagging in two Icelandic Gold Standards
PoS taggers	Icelandic Frequency Dictionary (IFD)	All PoS taggers tested so far for Icelandic have been developed or trained and tested on the Icelandic Frequency Dictionary (IFD) (.
Tagging	MIM-GOLD	 Table 1: Tagging accuracy when tagging IFD and  MIM-GOLD using 10-fold cross-validation.
splitting	Kilgarriff, 1999)	As work progressed, this inclination towards splitting, in the perpetual lumpers and splitters discussion (Kilgarriff, 1999), proved to be progressively untenable.
Classifying Syntactic Categories	Chinese Dependency Network	Classifying Syntactic Categories in the Chinese Dependency Network
Sizes	FDGs	 Table 1: Sizes of the FDGs
dependency parsing	universal dependencies data set	This paper presents cross-lingual models for dependency parsing using the first release of the universal dependencies data set.
significance testing	CoNLL 2007 shared task	For significance testing, we take Dan Bikel's randomized parsing evaluation comparator that was used by the CoNLL 2007 shared task with the default settings of 10,000 iterations ().
significance testing	CoNLL 2007 shared task	For significance testing, we use the script provided by the CoNLL 2007 shared task which is Dan Bikel's randomized parsing evaluation comparator with the default settings of 10,000 iterations.
parsing evaluation	FDGs	In order to test the usability of ParTes for parsing evaluation, it has been applied as a gold standard in an evaluation task of the FDGs.
constituent parsing	Penn WSJ corpus	For constituent parsing, using a recursive neural network (RNN) got an F1 score close to the state-of-the-art on the Penn WSJ corpus.
dative alternation	Child Language Data Exchange System (CHILDES))	The dative alternation has been the topic of several studies in which computational models are trained on naturalistic data (), such as offered by the Child Language Data Exchange System (CHILDES)), a publicly available database of children's speech produced in a natural environment.
MT evaluation	NIST	The standard automatic MT evaluation scores (BLEU, NIST, TER, METEOR;) do not offer specific insights about pronoun translation, but it is still useful to consider them first for an easy overview over the submitted systems.
Sentence-level pronoun type	TED Talk 767	 Table 3: Sentence-level pronoun type + align- ment mismatches for TED Talk 767
translation	Stanford CoreNLP system	At translation time, coreference information is obtained from the Stanford CoreNLP system.
MT	Second DiscoMT Workshop	In this paper we describe the rule-based MT system Its-2 developed at the University of Geneva and submitted for the shared task on pronoun translation organized within the Second DiscoMT Workshop.
SMT	Idiap Research Institute	The SMT system we submitted to the pronounfocused translation sub-task (Section 3) combines * Work performed while at the Idiap Research Institute.
translation	UU-TIEDEMANN)	Thereafter, we discuss the translation model that we used in our submission (UU-TIEDEMANN).
Translation	DET+PRON	 Table 5: Translation with and without pronoun  language model on development data. PRON uses  words linked to English pronouns and DET+PRON  includes words linked to determiners as well.
SMT	Gen&Topic benchmark set	Recently, we studied the impact of topic and genre differences on SMT quality using the Gen&Topic benchmark set, an Arabic-English evaluation set with controlled topic distributions over two genres; newswire and UG comments (van der).
MT	MT03-MT09	 Table 2: MT performance on MT03-MT09 in terms of TER and BLEU.
MT	MT09 newswire	 Table 3: MT performance on MT09 newswire and  weblog in terms of TER and BLEU.
parsing	WSJ-trained CCG	Although the Penn Treebank has been a vital tool in the development and evaluation of parsing technology, providing a standard dataset for comparison of parsers, practical application of these techniques usually requires adaptation to new domains., for example, examine the adaptation of a WSJ-trained CCG parser to the biomedical domain.
relation extraction	ML data sets	 Table 2: Results for relation extraction system evaluated against DL and ML data sets
Identifying Key Concepts	EHR Notes	Identifying Key Concepts from EHR Notes Using Domain Adaptation
synonym ranking	Wikipedia data	To evaluate our synonym ranking methods, we created a gold standard evaluation dataset from the Wikipedia data we extracted.
translation	Microsoft Asia group system	Only a few systems perform the two-sided translation, which is the case of the platform implemented by the Microsoft Asia group system, and the Virtual Sign Translator ().
Deceptive opinions detection	Opinion Spam corpus	 Table 2: Deceptive opinions detection with SVM  for negative reviews of Opinion Spam corpus (800  opinions).
sentiment analysis	Stanford library	For sentiment analysis the Stanford library is used.
Phrase-based Machine Translation	WMT'15	Order print-on-demand copies from: Co-Organizers iii The AFRL-MITLL WMT15 System: There's More than One Way to Decode It! The KIT-LIMSI Translation System for WMT 2015 The Edinburgh/JHU Phrase-based Machine Translation Systems for WMT 2015 Montreal Neural Machine Translation Systems for WMT'15
Phrase pair count distribution	APE 2015 training	 Table 23: Phrase pair count distribution in two phrase tables  built using the APE 2015 training and the Autodesk dataset.
Phrase pair count distribution	Autodesk dataset	 Table 23: Phrase pair count distribution in two phrase tables  built using the APE 2015 training and the Autodesk dataset.
translationese classifier	Europarl	We also conduct cross-corpus experiments in which we train translationese classifier on one corpus (Europarl) and test its contribution to SMT on another (Hansard, News).
translationese classifier	Hansard, News)	We also conduct cross-corpus experiments in which we train translationese classifier on one corpus (Europarl) and test its contribution to SMT on another (Hansard, News).
SMT	Hansard, News)	We also conduct cross-corpus experiments in which we train translationese classifier on one corpus (Europarl) and test its contribution to SMT on another (Hansard, News).
translationese classifier	Europarl training data	We train an (English) translationese classifier on the Europarl training data, but use the Hansard corpus for the translation model.
translationese classifier	Hansard corpus	We train an (English) translationese classifier on the Europarl training data, but use the Hansard corpus for the translation model.
SMT	Hansard training corpus	We then train a French-to-English SMT system with the (predicted) translation model, comparing it to systems that use the entire Hansard training corpus, the (actual) S → T texts and the actual T → S texts.
SMT	dev2010 data	All SMT systems were tuned using MIRA () on the dev2010 data from, and then evaluated on the test2010 IWSLT test set using both BLEU () and TER ().
SMT	test2010 IWSLT test set	All SMT systems were tuned using MIRA () on the dev2010 data from, and then evaluated on the test2010 IWSLT test set using both BLEU () and TER ().
MT	Popel andŽabokrtsk´yandˇandŽabokrtsk´andŽabokrtsk´y, 2010)	CHIMERA () is our English-to-Czech MT system designed as a combination of three very different components: • TectoMT (Popel andŽabokrtsk´yandˇandŽabokrtsk´andŽabokrtsk´y, 2010), a deep-syntactic transfer-based system, • Moses (, where we use a factored phrase-based setup with large language models, • Depfix (, an automatic postediting system, aimed at correcting mainly errors in morphological agreement but successful also in semantic corrections, esp.
SMT	CIMS-primary	 Table 2: Names and components of our SMT systems; the submitted system are named CIMS-primary  and CIMS.
SMT	CIMS	 Table 2: Names and components of our SMT systems; the submitted system are named CIMS-primary  and CIMS.
MT Submission	newstest2015	 Table 3: MT Submission Systems decoding  newstest2015
APE	APE baseline	Our primary APE system submitted at this shared task performs significantly better than the standard APE baseline.
translating from German to English	SLCFRS	Our experiments for translating from German to English demonstrate the feasibility of training and decoding with more expressive translation models such as SLCFRS and show a modest improvement over a context-free baseline.
translating English TED lectures	IWSLT 2015 evaluation campaign	The second is the task of translating English TED lectures into German using the data from the IWSLT 2015 evaluation campaign ().
Referential Translation Machines	WMT15	We present top results with Referential Translation Machines (Biçici, 2015; Biçici and Way, 2014) at quality estimation task (QET15) in WMT15 (Bojar et al., 2015).
Alignment-based sense selection	RATATOUILLE	Alignment-based sense selection in METEOR and the RATATOUILLE recipe
sense selection	WMT14 evaluation dataset	The mechanism used for sense selection is described in detail in the next section where we also present the results obtained by the Meteor-WSD metric on the WMT14 evaluation dataset.
MT	WMT14 evaluation	The reported MT evaluation results showed the beneficial impact of disambiguation which improved the correlation of the metric to human judgments from almost all languages involved in the WMT14 evaluation into English (except for Czech-English).
Tuning	WMT14	 Table 1: Tuning results with BEER without bias  on WMT14 as tuning and WMT13 as test set
Tuning	WMT13	 Table 1: Tuning results with BEER without bias  on WMT14 as tuning and WMT13 as test set
MT evaluation	IBM	The most popular MT evaluation metrics are IBM BLEU () and NIST, used not only for tuning MT systems, but also as evaluation metrics for translation shared tasks, such as the Workshop on Statistical Machine Translation (WMT).
MT evaluation	NIST	The most popular MT evaluation metrics are IBM BLEU () and NIST, used not only for tuning MT systems, but also as evaluation metrics for translation shared tasks, such as the Workshop on Statistical Machine Translation (WMT).
machine translation	WMT15 Metrics Shared Task	This paper describes the LeBLEU evaluation score for machine translation, submitted to WMT15 Metrics Shared Task.
WMT metrics shared tasks	WMT 2014 data	 Table 2: Performance of LeBLEU in recent WMT metrics shared tasks. Pearson's correlation coefficients  (system-level data) and average Kendall's tau correlation coefficients (segment-level data) for LeBLEU  with default parameters (def.), LeBLEU with optimized parameters (opt.), and topline method for the  shared task (top). For WMT 2014 data, also two reference methods are included: BLEU (ref-B) and  AMBER (ref-A).
Translation	WMT15 tuning task  dev set	 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
Translation	MEANT	 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
MT	WMT15 tuning task  dev set	 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
MT	MEANT	 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
Translation	WMT15 tuning task  test set	 Table 4: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  test set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
Translation	MEANT	 Table 4: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  test set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
MT	WMT15 tuning task  test set	 Table 4: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  test set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
MT	MEANT	 Table 4: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task  test set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and  backoff algorithm.
GF	Ranta, 2011)	The standard textbook on GF, (Ranta, 2011) has recently been translated to Chinese.
SRL processing	PropBank	Automatic SRL processing has two major drawbacks: firstly, the scale of the training data is quite limited and although manually annotated data such as PropBank is available as training data for learning semantic role prediction models, it is still hard to learn lexical preferences due its limited size.
SRL	Chinese section of CoNLL-2009 shared task data	For SRL, we used the Chinese section of CoNLL-2009 shared task data for experiments.
sentiment analysis	CUCsas	Taking 20 given topics and a total of 19,469 weibo messages released by Bake-off as the test data, the experimental results of the sentiment analysis system CUCsas are as follows:, we can seethe introduction of the phrase rule base improved the system overall performance, but only to a small extent.
text correction of Arabic	Qatar Arabic Language Bank (QALB) project,	The QALB shared task on automatic text correction of Arabic, organized within the framework of the Qatar Arabic Language Bank (QALB) project, 1 is the first effort aimed at constructing a benchmark data set, which will allow for development and evaluation of automatic correction systems for Arabic.
Annotating Targets of Opinions	Crowdsourcing	Annotating Targets of Opinions in Arabic using Crowdsourcing
segmentation	ATB2	 Table 8. The model  trained on the whole training set is tested on the  test set of each part. Then a single model is built  from each training set of each part and tested on  the test set of the given part. The highest scores  are in bold showing the best tagging was  achieved on ATB1 and best segmentation on  ATB2.
spelling correction	QALB 2015 Shared Task	We describe the CMU-Q and QCRI's joint efforts in building a spelling correction system for Arabic in the QALB 2015 Shared Task.
parsing	Chinese CCG parser	In order to have a better understanding of how the parsing performance changes with the size of the training data, we trained the Chinese CCG parser on both the full training set (ccg.full) and the same training set used for training the Chinese GCG parser (ccg.same).
parsing	GCG-reannotated corpus	Results in show that the parsing performance of the parser trained on the GCG-reannotated corpus is more accurate with strong significance (p < 0.001) than the same parser trained on the CCG-reannotated corpus of the same size.
parsing	CCG-reannotated corpus	Results in show that the parsing performance of the parser trained on the GCG-reannotated corpus is more accurate with strong significance (p < 0.001) than the same parser trained on the CCG-reannotated corpus of the same size.
ATB extraction	HPSG	As suggested by the mutual compatibility of gaps and resumptives in ATB extraction, however, we shall conclude that both strategies must be regarded as unbounded dependencies (UDCs) to be modelled via the feature in HPSG.
predicting poetry sentiment	PMI Lexicon	Using our lexicon achieves an accuracy of 71% on predicting poetry sentiment, which is 14% better than using PMI Lexicon.
Quantitative analysis	Exp	 Table 2: Quantitative analysis results of Exp. 1
MT	EHR notes	We also asked a bilingual domain expert to manually evaluate the five MT system outputs of the three EHR notes.
MT	EHR notes	A bilingual human expert performed a blind review of the outputs of all five MT systems on the three EHR notes (a total of 15 Spanish outputs).
MT	ESPAC EHR	 Table 2. MT systems on ESPAC EHR
Automatic Detection of Answers to Research Questions	Medline Abstracts	Automatic Detection of Answers to Research Questions from Medline Abstracts
Classification	SICK test data	 Table 7: Classification accuracy, including a cat- egory breakdown for SICK test data. Categories  are shown with their frequencies.
Hybrid Approaches to Translation (HyTra-4)	Beijing	Welcome to the Fourth Workshop on Hybrid Approaches to Translation (HyTra-4) held in conjunction with ACL-2015, Beijing ! The workshop series on Hybrid Approaches to Translation aims at providing a communication platform, building a research community and informing research agenda around theoretical and practical issues of Hybrid MT, and specifically -the problems, methodologies, resources and theoretical ideas which originate outside the mainstream MT paradigm, but have potential to enhance the quality of state-ofthe-art MT systems.
translation pipeline	Europarl	Like the transfer module in the translation pipeline, the SMT model is trained over 1.9 million sentences from Europarl.
SMT	Europarl	Like the transfer module in the translation pipeline, the SMT model is trained over 1.9 million sentences from Europarl.
MT	NIST	MT setups are evaluated regarding the translation quality, based on a selection of widely-used MT metrics: BLEU (), NIST),) and TER ().
SMT	Japanese-English dataset	We experimented the passive and pervasive uses of dictionary in SMT using the Japanese-English dataset provided in the Workshop for Asian Translation ().
WAT shared task	ASPEC corpus	In our experiments we follow the setup of the WAT shared task with 1800 development and test sentences each from the ASPEC corpus.
SMT	WAT test set	For the pervasive use of the dictionary, we used the xml-input function in the Moses toolkit to force lexical knowledge in the decoding process . presents the BLEU scores of the Japanese to English (JA-EN) translation outputs from the phrase-based SMT system on the WAT test set.
SMT	WAT test set	presents the BLEU scores of the English to Japanese (EN-JA) translation outputs from the phrase-based SMT system on the WAT test set.
SMT	UG benchmarks	• SMT model coverage dramatically deteriorates for phrases of length 3 or longer inmost of the UG benchmarks.
language modelling	Twitter garden hose	For language modelling, we used a set of 250 million tweets drawn from the Twitter garden hose, which is a fair 10% sample of all tweets ().
normalization	IHS R&D team	 We have also tried to improve the normalization results by using a did-you-mean (DYM) module that is currently being developed at IHS R&D team.
MWE identification	MWE data set tagged	The MWE identification tool is re-trained using the MWE data set tagged by) on the Penn Treebank sections of OntoNotes Release 4.0.
MWE identification	Penn Treebank sections of OntoNotes Release 4.0	The MWE identification tool is re-trained using the MWE data set tagged by) on the Penn Treebank sections of OntoNotes Release 4.0.
translation	Lang-8 Learner Corpora v2.0	The translation model was trained on the Lang-8 Learner Corpora v2.0.
SMT-based grammatical error  correction	NLP-TEA-1 dataset	 Table 3: F1 score of SMT-based grammatical error  correction system on NLP-TEA-1 dataset, with and  without tuning.
ASR	NLU	Modern systems maintain a probability distribution over possible states, reflecting all the uncertainty and ambiguity in ASR and NLU.
opensource recognizers	German Verbmobil and English Wall Street Journal corpora	Moreover, in a recent study, () it was found that Kaldi significantly outperformed other opensource recognizers on recognition tasks on German Verbmobil and English Wall Street Journal corpora.
SMO classifier	Weka3.	We train an SMO classifier in Weka3.) by using 10 fold cross-validation.
card sorting game	Furhat	We will demonstrate the capabilities of IrisTK by showing an application where two users are playing a collaborative card sorting game together with the robot head Furhat, where the cards are shown on a touch table between the players.
NI detection	SemEval test set	2 has shown that combining supervised and mildly supervised strategies to NI detection achieves the best results on the SemEval test set.
DIP dialogue	GP-SARSA	Firstly, we trained DIP dialogue policies in the restaurant search domain using GP-SARSA based on a state-of-the-art agenda-based user simulator 3 (, in comparison with the GP-SARSA learning process for the well-known BUDS system (where full beliefs are used), as shown in.
DIP dialogue	BUDS system	Firstly, we trained DIP dialogue policies in the restaurant search domain using GP-SARSA based on a state-of-the-art agenda-based user simulator 3 (, in comparison with the GP-SARSA learning process for the well-known BUDS system (where full beliefs are used), as shown in.
SMT hybridization	Europarl	 Table 9: An example of SMT hybridization in Europarl
Fuzzy-match repair	TAUS	Fuzzy-match repair is one of the technologies that TAUS, the Translation Automation User Society, calls advanced leveraging; 4 commercial examples of these are DeepMiner in Atril's Déjà Vu, and ALTM in MultiCorpora's MultiTrans.
SMT	TEP++	Benchmarking SMT Performance for Farsi Using the TEP++ Corpus
SMT	IWSLT-2013 dataset	Finally in the last part we show how SF2FF boosts the SMT quality for Farsi and report our results on the IWSLT-2013 dataset providing a comparison with FBK's system.
SMT	FBK	Finally in the last part we show how SF2FF boosts the SMT quality for Farsi and report our results on the IWSLT-2013 dataset providing a comparison with FBK's system.
SMT	DGT-TM parallel corpus (Steinberger et al., 2012)	The automatic evaluation was performed for three language pairs (English-German, Latvian, and Lithuanian) using general domain SMT systems that were trained in the LetsMT platform using the DGT-TM parallel corpus (Steinberger et al., 2012) (the releases of).
SMT	DGT-TM corpus	The results of the baseline systems are given in: Baseline system performance ("(a)" -automotive domain evaluation sets; "(g)" -SMT system in-domain evaluation sets from the DGT-TM corpus) dent that the results for English-Latvian are significantly higher (although still relatively low) than for the other language pairs.
terminology integration	Microsoft Terminology Collection 2	For terminology integration, the freely available Microsoft Terminology Collection 2 was used.
SMT sys	DGT-TM  corpus	 Table 1: Baseline system performance ("(a)" -au- tomotive domain evaluation sets; "(g)" -SMT sys- tem in-domain evaluation sets from the DGT-TM  corpus)
MT	RBMT-1	Three MT systems were considered in this analysis: UEDIN (an SMT system), PROMT (a hybrid system) and RBMT-1 (a rule-based system).
SMT	TED corpus	In our low-resource condition, we trained an SMT system using only training data from the TED corpus ().
MT evaluation	IBM c � 2015	To date, the most popular MT evaluation metrics essentially measure lexical overlap between reference and hypothesis translation such as IBM c � 2015 The authors.
translation studies	WMT2013 1	Being aware of the two communities, machine translation and translation studies, we took the available online data from the WMT2013 1 and tried to reproduce the ranking task with translation studies students for the English to German translations.
predicting inflectional features and prepositions	Wapiti toolkit	The models for predicting inflectional features and prepositions were built with the Wapiti toolkit (.
MT	EU Bookshop corpus	For the MT evaluation we trained an SMT system with the original EU Bookshop corpus and noted the BLEU score.
SMT	EU Bookshop corpus	For the MT evaluation we trained an SMT system with the original EU Bookshop corpus and noted the BLEU score.
MT	Apertium	Both language pairs have MT provided by Apertium.
SMT	International Maritime Organization	SMT at the International Maritime Organization: Experiences with Combining In-house Corpora with Out-of-domain Corpora
MT	ACCURAT	Evaluation of the domain adaptation of MT systems in ACCURAT
MT	ACCURAT project	1 The contribution reports on an evaluation of efforts to improve MT quality by domain adaptation, for both rule-based and statistical MT, as done in the ACCURAT project (Skadiņa et al. 2012).
MT	ACCURAT project	1 The contribution reports on an evaluation of efforts to improve MT quality by domain adaptation, for both rule-based and statistical MT, as done in the ACCURAT project (Skadiņa et al. 2012).
SMT	Europarl	On the SMT side, a baseline Moses system was trained with standard parallel data (Europarl, JRC etc.), plus some initial comparable corpus data as collected in the first phase of ACCURAT.
SMT	ACCURAT	On the SMT side, a baseline Moses system was trained with standard parallel data (Europarl, JRC etc.), plus some initial comparable corpus data as collected in the first phase of ACCURAT.
SMT	RMT	It is a bit more for the SMT than for the RMT, due to a strong RMT baseline system.
SMT	RMT	The improvement is more significant for the SMT system than for the RMT; this maybe due to the fact that the RMT baseline system was stronger than the SMT baseline.
MT-enhanced fuzzy matching	Transit NXT	MT-enhanced fuzzy matching with Transit NXT and STAR Moses
MT-enhanced fuzzy matching	STAR	MT-enhanced fuzzy matching with Transit NXT and STAR Moses
Neural Reranking Improves Subjective Quality of Machine Translation	NAIST	Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015
JE translation	PB SMT baseline	 Table 3: Evaluation of our submission on JE translation compared with the organizer's PB SMT baseline.
SPE translation	RBMT	In all the tasks, we submitted SPE translation results based on our RBMT and SMT.
SMT	JPOzh-ja	On the other hand, more than half of translations selected from SMT in JPOzh-ja and JPOko-ja. shows the translation examples that COMB achieves better results than SPE with reranking in sentence-level BLEU.
SMT	JPOko-ja.	On the other hand, more than half of translations selected from SMT in JPOzh-ja and JPOko-ja. shows the translation examples that COMB achieves better results than SPE with reranking in sentence-level BLEU.
detecting repetition stuttering	MFCC	shows the average error rates of detecting repetition stuttering using 5-fold cross validation with MFCC and LPCC features.
early detection of AD	WAIS-III	Some of the most relevant are the kiosk system designed to use at home as a prevention instrument for early detection of AD described in, the end-to-end system for automatically scoring the Logic Memory test of the WAIS-III presented in, and the system that implements a modified version of the MMSE based on the IBM ViaVoice recognition engine of.
TM expansion	MemoQ CAT system	The asset of the TM expansion methods were evaluated by the pre-translation analysis of widely used MemoQ CAT system and the METEOR metric was used for measuring the quality of fully expanded new translation segments.
author profiling	Author Profiling task at PAN 1 at CLEF 2013	It is noteworthy the interest in author profiling since 2013, as can be seen in the number of shared tasks: i) Age and gender identification at the Author Profiling task at PAN 1 at CLEF 2013 () and 2014 ().
word sense disambiguation (WSD)	WordNet	For graph-based word sense disambiguation (WSD), one approach has been to first translate terms into English in order to disambiguate using richer, fuller lexical knowledge bases (LKBs) such as WordNet.
WSD	WordNet	WSD research has tended to derive knowledge bases from stand-alone dictionaries and ontologies such as WordNet, where nouns, verbs, adjectives and adverbs are stored as 'synsets' and linked by their semantic relations.
Susa	PTB	2, Susa generates 66 unique concepts while 50 concepts are generated by PTB, the union of both is 77 unique concepts.
FSTs	JCLexT	 Table 1. Size of the FSTs generated by JCLexT and Foma
FSTs	Foma	 Table 1. Size of the FSTs generated by JCLexT and Foma
NER task	HAREM corpus	Therefore, this evaluation aims to find the most appropriate annotation to the NER task in the HAREM corpus.
SMO classification	Weka tool	In our case study, we used a subset of the PLN­BR CATEG corpus and SMO classification algorithm from the Weka tool.
Procedure Impairment-Invalidate	WISHFUL	Procedure Impairment-Invalidate has been implemented in a system called WISHFUL, which was run on several instances of the network in.
RST	GSDT	Given this, it would be hard to argue that RST's text spans correspond to GSDT's discourse segments.
text planning	McK82	1. Rhetorical Relations as Text Planning Operators Rhetorical relations have been used for text planning in many text generation systems ([McK82] [Hov88] [Moo89], among others), hut how they are used vary rather significantly from one text planner to another.
tagging	MTD	The examples that support the tagging operation come from the system MTD.
PST	Brown corpus	The PST was trained on the Brown corpus with maximal depth of five.
information extraction	Brown Corpus	Furthermore, training corpora for information extraction are typically annotated with domain-specific tags, in contrast to general-purpose annotations such as part-of-speech tags or noun-phrase bracketing (e.g., the Brown Corpus and the Penn Treebank).
information extraction	Penn Treebank	Furthermore, training corpora for information extraction are typically annotated with domain-specific tags, in contrast to general-purpose annotations such as part-of-speech tags or noun-phrase bracketing (e.g., the Brown Corpus and the Penn Treebank).
topic identification	19Mb of Japanese newspaper articles	The significance of exploiting information on the structure for topic identification is demonstrated by a set of experiments conducted on the 19Mb of Japanese newspaper articles.
divergence	BPP	From the graphs, we found out that the maximum value of F-measure is 0.75 in the case of divergence while it is only 0.65 in the case of BPP.
POS tagging	Penn Treebank Wall St. Journal corpus	This paper briefly describes the maximum entropy and maximum likelihood properties of the model, features used for POS tagging, and the experiments on the Penn Treebank Wall St. Journal corpus.
tagging	Wall St. Journal data	In order to conduct tagging experiments, the Wall St. Journal data has been split into three contiguous sections, as shown in.
Data-Oriented Parsing (DOP) of natural language texts	Bod, 1993c)	Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993c).
labeling definition sentences	Longman Dictionary of Contemporary English (LDOCE)	We also describe an implementation of the algorithm for labeling definition sentences in Longman Dictionary of Contemporary English (LDOCE).
SPL input	Nigel grammar	The system processes the SPL input by querying different knowledge resources, including the Nigel grammar, the upper model, and a domain model, eventually producing a realization of the SPL plan in the form of an English sentence.
NL analysis	TAG Tree Bank	To simplify this investigation, I will not tackle the many problems of NL analysis, but will use already parsed texts from the TAG Tree Bank.
summarizes articles given	Netsumm	It has an impressive user interface, anti is practically domain-independent, but suffers from two major problems: it only summarizes a single article at a time, and it only summarizes articles given by the user, which means that the user has to go through hundreds of articles to select the ones he will send to Netsumm.
Parsing	IBM Manua!s task	 Table 4: Parsing results reported by Jelinek et. at. for IBM Manua!s task; see
Parsing	WSJ	 Table 1: Parsing accuracy using the WSJ corpus
parsing	PDG	We are planning to evaluate the parsing model based on the reestimated PDG and the PDG-based language model.
hand tagging the senses	Wall Street Journal Treebank corpus	This paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12,925 sentences of the Wall Street Journal Treebank corpus (Marcus et al. 1993).
hand tagging the senses	Wall Street Journal Treebank corpus	This paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12,925 sentences of the Wall Street Journal Treebank corpus ().
parsing	Wall St. Journal domain	The parsing accuracy--roughly 87% precision and 86% recall--surpasses the best previously published results on the Wall St. Journal domain.
thresholding optimization	Section 5	We ran a series of experiments using the thresholding optimization algorithm of Section 5.
translation	VERBMOBIL project [Wahlster 1993	In the next section, we demonstrate that these particles can be quite problematic in translation , drawing on extensive corpus analyses we performed in the VERBMOBIL project [Wahlster 1993]; thus our examples are all from the domain of appointment scheduling.
SLT	VERBMOBIL	Finally, we discuss how our inventory of discourse functions can be used to improve translation quality in an SLT system such as VERBMOBIL.
SLT	ATIS domain (Hemphill et al., 1990)	The current SLT system carries out multilingual speech translation in near real time in the ATIS domain (Hemphill et al., 1990) for several language pairs.
Spoken Language Translation	ITSVox	Spoken Language Translation with the ITSVox System
noun semantic categorization	WordNet	While some information on noun semantic categorization can be gleaned from online lexical resources such as WordNet, it is well beyond the state of the art to glean the kind of verb semantics necessary from online resources.
AAC	Darragh	Prediction techniques have been extensively used in AAC, see, for example, the review in Darragh and.
Grammatical analysis	OVIS	Grammatical analysis in the OVIS spoken-dialogue system
dialogue management	Openbaar Vervoer Reisinformatie (OVR)	The current focus is on dialogue management fora research project of Openbaar Vervoer Reisinformatie (OVR) (.
selectional preference acquisition	Penn Treebank parses	Most current research on selectional preference acquisition has used the Penn Treebank parses) These are obtained semi-automatically with a deterministic parser and manual correction.
generalization	WordNet	First, it introduces the idea of generalization; then it describes our Generalization Tree (GT) model based on the WordNet and illustrates how GT controls the degree of generalization according to the user's needs.
SENSE	EB	Note that this does not stop SENSE from recognising them in unseen contexts; e.g. in EB there is only one pattern exemplifying the verb sense abbassare_3 'reduce' (namely, abbassare_3-prezzo/O), but this does not prevent SENSE from recognising it in target contexts such as abbassare_?-sttpendzo/O.
RAP	Hobbs 76 paper	A significant aspect of that research is that both RAP and the Naive Algorithm were machine executed--the Naive Algorithm was not machine executed in either the Hobbs 76 paper or in the evaluation in this work.
Lexical Resource Reconciliation	Xerox Linguistic Environment	Lexical Resource Reconciliation in the Xerox Linguistic Environment
TREC-6 Spoken Document Retrieval (SDR) task	NIST/DARPA HUB4 Broadcast News corpus	In developing the SCAN (Spoken Content-based Audio Navigation) system to retrieve information from an audio domain, we have attempted to study the problems of how users navigate audio databases, hand in hand with the development of the speech and information retrieval technologies which enable this navigation3 SCAN was developed initially for the TREC-6 Spoken Document Retrieval (SDR) task, which employs the NIST/DARPA HUB4 Broadcast News corpus.
Aligning	WordNet	I I ! ! Aligning WordNet with Additional Lexical Resources
name recognition	TAGARAB	of Name Patterns The scores for the name recognition in TAGARAB over the training set of texts are given in.
semantic tagging of text	Sixth Message Understanding Conference (MUC-6)	This paper considers semantic tagging of text within the context of information extraction, as in the Sixth Message Understanding Conference (MUC-6).
summarization	TREC corpus	 Table 7: A cross-analysis of summarization results in the TREC corpus.
Separation	CFG	(3) Separation of local and global constraints keeps down the number of CFG rules.
Spelling correction	WordNet	We'll speak about the main modules of our program : Spelling correction, Different uses of WordNet, and Generation of comments.
argument generation	Australian Research Council grant A49531227	During argument generation, it focuses the argument construction process on concepts which are related to the "This research was supported in part by Australian Research Council grant A49531227.
parsing	LEX model	The results of full parsing accuracy show that model-1 under the LEX model outperforms other models.
Rhetorical Structure Theory	GROSS	However both Rhetorical Structure Theory (Mann and Thompson, 1988) and the intention-based approach to diacourse strut-ture associated with C~ntering Theory (GROSS and Sidner, 1986) involve high-level reamning.
Tagging of Speech Acts	Spanish Call Home	Tagging of Speech Acts and Dialogue Games in Spanish Call Home
coding discourse structure in dialogue	DR/effort	A two-level scheme for coding discourse structure in dialogue has been proposed and undergone initial testing within the DR/effort.
IR	NAC-SIS collection	Our research is partly motivated by the "NACSIS" test collection for IR systems ( ) 1 , which consists of Japanese queries and Japanese/English abstracts extracted from technical papers (we will elaborate on the NAC-SIS collection in Section 4).
pronoun resolution	Penn Treebank Wall Street Journal corpus	Some exceptions exist, however.) present a probabilistic model for pronoun resolution trained on a small subset of the Penn Treebank Wall Street Journal corpus (.
coreference resolution	MUC-6 data set	More importantly, the clustering approach outperforms the only MUC-6 system to view coreference resolution as a learning problem: The RESOLVE system employs decision tree induction and achieves an Fmeasure of 47% on the MUC-6 data set.
coreference resolution	MUC-6 coreference cotpora	We developed and evaluated the clustering approach to coreference resolution using the "dry run" and "formal evaluation" MUC-6 coreference cotpora.
routing of Hypertext'91 abstracts	Deerwester et al., 1990)	In groundbreaking work, Dumais and Nielsen (1992) developed a system for the routing of Hypertext'91 abstracts using latent semantic indexing (Deerwester et al., 1990), trained from available text sources including a small set of reviewer-submitted abstracts, on-line books and ACM articles as a source for the term-by-document matrix used in their singular value de-compositions.
information extraction (IE)	DAI:tPA Message Understanding Conferences	In particular, information extraction (IE) systems like those builtin the DAI:tPA Message Understanding Conferences have revealed that coreference resolution is such a critical component of IE systems that a separate coreference subtask has been defined and evaluated since MUC-6.
information extraction (IE)	MUC-6	In particular, information extraction (IE) systems like those builtin the DAI:tPA Message Understanding Conferences have revealed that coreference resolution is such a critical component of IE systems that a separate coreference subtask has been defined and evaluated since MUC-6.
coreference resolution	MUC-6	In particular, information extraction (IE) systems like those builtin the DAI:tPA Message Understanding Conferences have revealed that coreference resolution is such a critical component of IE systems that a separate coreference subtask has been defined and evaluated since MUC-6.
coreference resolution	MUC-6	In order to evaluate the performance of our learning approach to coreference resolution on a common data set, we utilized the annotated corpus and scoring program from MUC-6, which assembled a set of newswire documents annotated with coreference chains.