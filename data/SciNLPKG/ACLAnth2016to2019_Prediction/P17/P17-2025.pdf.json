{"title": [{"text": "Improving Neural Parsing by Disentangling Model Combination and Reranking Effects", "labels": [], "entities": [{"text": "Improving Neural Parsing", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8792059222857157}]}], "abstractContent": [{"text": "Recent work has proposed several genera-tive neural models for constituency parsing that achieve state-of-the-art results.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.9178714156150818}]}, {"text": "Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward.", "labels": [], "entities": []}, {"text": "We first present an algorithm for direct search in these gen-erative models.", "labels": [], "entities": []}, {"text": "We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects.", "labels": [], "entities": []}, {"text": "Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.", "labels": [], "entities": [{"text": "PTB", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.8939638137817383}, {"text": "F1", "start_pos": 149, "end_pos": 151, "type": "METRIC", "confidence": 0.9611058831214905}, {"text": "F1", "start_pos": 194, "end_pos": 196, "type": "METRIC", "confidence": 0.9806550741195679}]}], "introductionContent": [{"text": "Recent work on neural constituency parsing) has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler.", "labels": [], "entities": [{"text": "neural constituency parsing", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.6640611787637075}, {"text": "generative scoring", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.9242268800735474}]}, {"text": "Let Abe a parser that we want to parse with (here one of the generative models), and let B be abase parser that we use to propose candidate parses which are then scored by the less-tractable parser A.", "labels": [], "entities": []}, {"text": "We denote this cross-scoring setup by B \u2192 A.", "labels": [], "entities": []}, {"text": "The papers above repeatedly saw that the cross-scoring setup B \u2192 A under which their generative models were applied outperformed the standard singleparser setup B \u2192 B.", "labels": [], "entities": []}, {"text": "We term this a cross-scoring gain.", "labels": [], "entities": []}, {"text": "This paper asks two questions.", "labels": [], "entities": []}, {"text": "First, why do recent discriminative-to-generative cross-scoring se- * Equal contribution.", "labels": [], "entities": []}, {"text": "tups B \u2192 A outperform their base parsers B?", "labels": [], "entities": []}, {"text": "Perhaps generative models A are simply superior to the base models B and direct generative parsing (A \u2192 A) would be better still if it were feasible.", "labels": [], "entities": [{"text": "generative parsing", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.9361595213413239}]}, {"text": "If so, we would characterize the cross-scoring gain from B \u2192 B to B \u2192 A as a reranking gain.", "labels": [], "entities": []}, {"text": "However, it's also possible that the hybrid system B \u2192 A shows gains merely from subtle model combination effects.", "labels": [], "entities": []}, {"text": "If so, scoring candidates using some combined score A + B would be even better, which we would characterize as a model combination gain.", "labels": [], "entities": []}, {"text": "It might even be the case that B is a better parser overall (i.e. B \u2192 B outperforms A \u2192 A).", "labels": [], "entities": []}, {"text": "Of course, many real hybrids will exhibit both reranking and model combination gains.", "labels": [], "entities": []}, {"text": "In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of, and the LSTM language modeling generative parser (LM) of.", "labels": [], "entities": [{"text": "generative neural parsing", "start_pos": 119, "end_pos": 144, "type": "TASK", "confidence": 0.867418130238851}, {"text": "Recurrent Neural Network Grammar generative parser (RG", "start_pos": 157, "end_pos": 211, "type": "TASK", "confidence": 0.6844551153481007}, {"text": "LSTM language modeling generative parser", "start_pos": 225, "end_pos": 265, "type": "TASK", "confidence": 0.6651326000690461}]}, {"text": "In particular, we present and use a beam-based search procedure with an augmented state space that can search directly in the generative models, allowing us to explore A \u2192 A for these generative parsers A independent of any base parsers.", "labels": [], "entities": []}, {"text": "Our findings suggest the presence of model combination effects in both generative parsers: when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser (the RNNG discriminative parser, RD), performance decreases when compared to using just candidates from the base parser, i.e., B \u222a A \u2192 A has lower evaluation performance than B \u2192 A (Section 3.1).", "labels": [], "entities": [{"text": "generative parsers", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.8975634574890137}]}, {"text": "This result suggests that both generative models benefit from fortuitous search errors in the rescoring setting -there are trees with higher probability under the generative model than any tree proposed by the base parser, but which would decrease evaluation performance if selected.", "labels": [], "entities": []}, {"text": "Because of this, we hypothesize that model combination effects between the base and generative models are partially responsible for the high performance of the generative reranking systems, rather than the generative model being generally superior.", "labels": [], "entities": []}, {"text": "Here we consider our second question: if crossscoring gains are at least partly due to implicit model combination, can we gain even more by combining the models explicitly?", "labels": [], "entities": []}, {"text": "We find that this is indeed the case: simply taking a weighted average of the scores of both models when selecting a parse from the base parser's candidate list improves over using only the score of the generative model, in many cases substantially.", "labels": [], "entities": []}, {"text": "Using this technique, in combination with ensembling, we obtain new state-of-the-art results on the Penn Treebank: 94.25 F1 when training only on gold parse trees and 94.66 F1 when using external silver data.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.9934859871864319}, {"text": "F1", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.9928436875343323}, {"text": "F1", "start_pos": 173, "end_pos": 175, "type": "METRIC", "confidence": 0.9848299026489258}]}], "datasetContent": [{"text": "Using the above decoding procedures, we attempt to separate reranking effects from model combination effects through a set of reranking experiments.", "labels": [], "entities": []}, {"text": "Our base experiments are performed on the Penn Treebank (, using sections 2-21 for training, section 22 for development, and section 23 for testing.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.9948367178440094}]}, {"text": "For the LSTM generative model (LM), we use the pre-trained model released by.", "labels": [], "entities": [{"text": "LSTM generative", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.7875508069992065}]}, {"text": "We train RNNG discriminative (RD) and generative (RG) models, following by using the same hyperparameter settings, and using pretrained word embeddings from for the discriminative model.", "labels": [], "entities": [{"text": "RNNG discriminative (RD) and generative (RG)", "start_pos": 9, "end_pos": 53, "type": "TASK", "confidence": 0.6619106769561768}]}, {"text": "The automaticallypredicted part-of-speech tags we use as input for RD are the same as those used by.", "labels": [], "entities": [{"text": "RD", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9646119475364685}]}, {"text": "In each experiment, we obtain a set of candidate parses for each sentence by performing beam search in one or more parsers.", "labels": [], "entities": []}, {"text": "We use actionsynchronous beam search (Section 2.1) with beam size K = 100 for RD and word-synchronous beam (Section 2.2) with K w = 100 and K a = 1000 for the generative models RG and LM.", "labels": [], "entities": []}, {"text": "In the case that we are using only the scores from a single generative model to rescore candidates taken from the discriminative parser, this setup is close to the reranking procedures originally proposed for these generative models.", "labels": [], "entities": []}, {"text": "For RG, the original work also used RD to produce candidates, but drew samples from it, whereas we use abeam search to approximate its k-best list.", "labels": [], "entities": [{"text": "RG", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.927961528301239}]}, {"text": "The LM generative model was originally used to rerank a 50-best list taken from the Charniak parser).", "labels": [], "entities": [{"text": "LM generative", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.5387868881225586}]}, {"text": "In comparison, we found higher performance for the LM model when using a candidate list from the RD parser: 93.66 F1 versus 92.79 F1 on the development data.", "labels": [], "entities": [{"text": "F1", "start_pos": 114, "end_pos": 116, "type": "METRIC", "confidence": 0.9958482980728149}, {"text": "F1", "start_pos": 130, "end_pos": 132, "type": "METRIC", "confidence": 0.9869880676269531}]}, {"text": "This maybe attributable to having a stronger set of candidates: with beam size 100, RD has an oracle F1 of 98.2, compared to 95.9 for the 50-best list from the Charniak parser.", "labels": [], "entities": [{"text": "RD", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.6489099860191345}, {"text": "F1", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.6987167596817017}]}], "tableCaptions": [{"text": " Table 1: F1 on the development set for word-synchronous  beam search when searching in the RNNG generative (RG)  and LSTM generative (LM) models. Ka is set to 10 \u00d7 Kw.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9984820485115051}, {"text": "word-synchronous  beam search", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.5928882360458374}, {"text": "RNNG generative (RG)  and LSTM generative (LM)", "start_pos": 92, "end_pos": 138, "type": "TASK", "confidence": 0.6884450045498934}]}, {"text": " Table 2: Development F1 scores on section 22 of the PTB  when using various models to produce candidates and to  score them. \u222a denotes taking the union of candidates from  each of two models; + denotes using a weighted average of  the models' log-probabilities.", "labels": [], "entities": [{"text": "PTB", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.9521711468696594}]}, {"text": " Table 3: Test F1 scores on section 23 of the PTB, by tree- bank training data conditions: either using only the training  sections of the PTB, or using additional silver data (+S).", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9810848236083984}, {"text": "PTB", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.9445247054100037}, {"text": "PTB", "start_pos": 139, "end_pos": 142, "type": "DATASET", "confidence": 0.981964111328125}]}]}