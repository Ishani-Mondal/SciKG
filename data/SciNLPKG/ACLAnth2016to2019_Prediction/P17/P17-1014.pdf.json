{"title": [{"text": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation", "labels": [], "entities": [{"text": "Neural AMR", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.5966801047325134}, {"text": "Parsing and Generation", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.813410222530365}]}], "abstractContent": [{"text": "Sequence-to-sequence models have shown strong performance across abroad range of applications.", "labels": [], "entities": []}, {"text": "However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs.", "labels": [], "entities": [{"text": "parsing and generating text", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.7704803198575974}, {"text": "Abstract Meaning Representation (AMR)", "start_pos": 64, "end_pos": 101, "type": "TASK", "confidence": 0.7123444378376007}]}, {"text": "We present a novel training procedure that can lift this limitation using millions of unla-beled sentences and careful preprocessing of the AMR graphs.", "labels": [], "entities": [{"text": "AMR graphs", "start_pos": 140, "end_pos": 150, "type": "DATASET", "confidence": 0.8590437471866608}]}, {"text": "For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9671306312084198}, {"text": "SMATCH", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9665705561637878}]}, {"text": "For AMR generation, our model establishes anew state-of-the-art performance of BLEU 33.8.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9397610425949097}, {"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9681926369667053}]}, {"text": "We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.", "labels": [], "entities": [{"text": "AMR", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.8862352967262268}]}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) is a semantic formalism to encode the meaning of natural language text.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8534351587295532}]}, {"text": "As shown in, AMR represents the meaning using a directed graph while abstracting away the surface forms in text.", "labels": [], "entities": []}, {"text": "AMR has been used as an intermediate meaning representation for several applications including machine translation (MT) (), summarization (, sentence compression (, and event extraction (.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.8361653447151184}, {"text": "summarization", "start_pos": 124, "end_pos": 137, "type": "TASK", "confidence": 0.9893462061882019}, {"text": "sentence compression", "start_pos": 141, "end_pos": 161, "type": "TASK", "confidence": 0.7968575656414032}, {"text": "event extraction", "start_pos": 169, "end_pos": 185, "type": "TASK", "confidence": 0.7538369297981262}]}, {"text": "While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use . AMR encodes semantic dependencies between entities mentioned in the sentence, such as \"Obama\" being the \"arg0\" of the verb \"elected\".", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct all experiments on the AMR corpus used in SemEval-2016 Task 8 (LDC2015E86), which contains 16,833/1,368/1,371 train/dev/test examples.", "labels": [], "entities": [{"text": "AMR corpus", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.8223123550415039}, {"text": "SemEval-2016 Task 8 (LDC2015E86)", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.568000853061676}]}, {"text": "For the paired training procedure of Algorithm 1, we use Gigaword as our external corpus and sample sentences that only contain words from the AMR corpus vocabulary W . We subsampled the original sentence to ensure there is no overlap with the AMR training or test sets., and AMR generation using BLEU) . We validated word embedding sizes and RNN hidden representation sizes by maximizing AMR development set performance (Algorithm 1 -line 1).", "labels": [], "entities": [{"text": "AMR corpus vocabulary W", "start_pos": 143, "end_pos": 166, "type": "DATASET", "confidence": 0.9156051278114319}, {"text": "AMR training or test sets.", "start_pos": 244, "end_pos": 270, "type": "DATASET", "confidence": 0.7693371534347534}, {"text": "AMR generation", "start_pos": 276, "end_pos": 290, "type": "TASK", "confidence": 0.7660918831825256}, {"text": "BLEU", "start_pos": 297, "end_pos": 301, "type": "METRIC", "confidence": 0.996364951133728}]}, {"text": "We searched over the set {128, 256, 500, 1024} for the best combinations of sizes and set both to 500.", "labels": [], "entities": []}, {"text": "Models were trained by optimizing cross-entropy loss with stochastic gradient descent, using a batch size of 100 and dropout rate of 0.5.", "labels": [], "entities": []}, {"text": "Across all models when performance does not improve on the AMR dev set, we decay the learning rate by 0.8.", "labels": [], "entities": [{"text": "AMR dev set", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.7657536665598551}, {"text": "learning rate", "start_pos": 85, "end_pos": 98, "type": "METRIC", "confidence": 0.957046627998352}]}, {"text": "For the initial parser trained on the AMR corpus, (Algorithm 1 -line 1), we use a single stack version of our model, set initial learning rate to 0.5 and train for 60 epochs, taking the best performing model on the development set.", "labels": [], "entities": [{"text": "AMR corpus", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.9208774268627167}]}, {"text": "All subsequent models benefited from increased depth and we used 2-layer stacked versions, maintaining the same embedding sizes.", "labels": [], "entities": []}, {"text": "We set the initial Gigaword sample size to k = 200, 000 and executed a maximum of 3 iterations of self-training.", "labels": [], "entities": []}, {"text": "For pretraining the parser and generator, (Algorithm 1 -lines 4 and 9), we used an initial learning rate of 1.0, and ran for 20 epochs.", "labels": [], "entities": []}, {"text": "We attempt to fine-tune the parser and generator, respectively, after every epoch of pre-training, setting the initial learning rate to 0.1.", "labels": [], "entities": []}, {"text": "We select the best performing model on the development set among all of these fine-tuning We use the multi-BLEU script from the MOSES decoder suite (   attempts.", "labels": [], "entities": [{"text": "MOSES decoder suite", "start_pos": 128, "end_pos": 147, "type": "DATASET", "confidence": 0.9296416838963827}]}, {"text": "During prediction we perform decoding using beam search and set the beam size to 5 both for parsing and generation.", "labels": [], "entities": [{"text": "prediction", "start_pos": 7, "end_pos": 17, "type": "TASK", "confidence": 0.9645271301269531}, {"text": "parsing", "start_pos": 92, "end_pos": 99, "type": "TASK", "confidence": 0.9786828756332397}]}, {"text": "Our final parser outperforms comparable seq2seq and character LSTM models by over 10 points.", "labels": [], "entities": []}, {"text": "While much of this improvement comes from self-training, our model without Gigaword data outperforms these approaches by 3.5 points on F1.", "labels": [], "entities": [{"text": "Gigaword data", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.8002122044563293}, {"text": "F1", "start_pos": 135, "end_pos": 137, "type": "METRIC", "confidence": 0.983183741569519}]}, {"text": "We attribute this increase in performance to different handling of preprocessing and more careful hyper-parameter tuning.", "labels": [], "entities": []}, {"text": "All other models that we compare against use semantic resources, such as WordNet, dependency parsers or CCG parsers (models marked with * were trained with less data, but only evaluate on newswire text; the rest evaluate on the full test set, containing text from blogs).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9554587602615356}, {"text": "dependency parsers", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.6733222752809525}]}, {"text": "Our full models outperform JAMR, a graph-based model but still lags behind other parser-dependent systems (CAMR 6 ), and resource heavy approaches (SBMT).", "labels": [], "entities": []}, {"text": "summarizes our AMR generation results on the development and test set.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.8981594145298004}]}, {"text": "We outperform all previous state-of-theart systems by the first round of self-training and further improve with the next rounds.", "labels": [], "entities": []}, {"text": "Our final model trained on GIGA-20M outperforms TSP and TREETOSTR trained on LDC2015E86, by over 9 BLEU points.", "labels": [], "entities": [{"text": "GIGA-20M", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.8167449235916138}, {"text": "TSP", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.5792827606201172}, {"text": "TREETOSTR", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9854485392570496}, {"text": "LDC2015E86", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.9014280438423157}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9990637898445129}]}, {"text": "7 Overall, our model incorporates less data than previous approaches as all reported methods train language models on the whole Gigaword corpus.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 128, "end_pos": 143, "type": "DATASET", "confidence": 0.934294581413269}]}, {"text": "We leave scaling our models to all of Gigaword for future work.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9549344182014465}]}, {"text": "In this section we evaluate three strategies for converting AMR graphs into sequences in the context of AMR generation and show that our models are largely agnostic to linearization orders.", "labels": [], "entities": [{"text": "AMR generation", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.9478732645511627}]}, {"text": "Our results argue, unlike SMT-based AMR generation methods (, that seq2seq models can learn to ignore artifacts of the conversion of graphs to linear sequences.", "labels": [], "entities": [{"text": "SMT-based AMR generation", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8578333059946696}]}], "tableCaptions": [{"text": " Table 1: SMATCH scores for AMR Parsing. *Reported numbers are on the newswire portion of a  previous release of the corpus (LDC2014T12).", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8055922389030457}, {"text": "AMR Parsing", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9033314883708954}, {"text": "newswire portion of a  previous release of the corpus (LDC2014T12)", "start_pos": 70, "end_pos": 136, "type": "DATASET", "confidence": 0.9111141810814539}]}, {"text": " Table 2: LDC2015E86 AMR training set,  GIGA-200k, GIGA-2M and GIGA-20M statistics;  OOV@1 and OOV@5 are the out-of-vocabulary  rates on the NL side with thresholds of 1 and 5, re- spectively. Vocabulary sizes are 13027 tokens for  the AMR side, and 17319 tokens for the NL side.", "labels": [], "entities": [{"text": "LDC2015E86 AMR training set", "start_pos": 10, "end_pos": 37, "type": "DATASET", "confidence": 0.8048358410596848}, {"text": "GIGA-200k", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.7657744288444519}, {"text": "OOV", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.994355320930481}, {"text": "OOV@5", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9230701526006063}]}, {"text": " Table 3: BLEU results for AMR Generation.  *Model has been trained on a previous release of  the corpus (LDC2014T12).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989245533943176}, {"text": "AMR Generation", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.9191596508026123}]}, {"text": " Table 4: BLEU scores for AMR generation abla- tions on preprocessing (DEV set).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989138841629028}, {"text": "AMR generation", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.812964141368866}]}, {"text": " Table 5: SMATCH scores for AMR parsing abla- tions on preprocessing (DEV set).", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8945916891098022}, {"text": "AMR parsing abla- tions", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.8921581983566285}]}, {"text": " Table 6: BLEU scores for AMR generation for dif- ferent linearization orders (DEV set).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992297887802124}, {"text": "AMR generation", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.9618855714797974}]}, {"text": " Table 7: Error analysis for AMR generation on a  sample of 50 examples from the development set.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9610276818275452}, {"text": "AMR generation", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.9864165782928467}]}]}