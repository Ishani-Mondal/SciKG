{"title": [{"text": "Answering Complex Questions Using Open Information Extraction", "labels": [], "entities": [{"text": "Answering Complex Questions", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9228312571843466}, {"text": "Open Information Extraction", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6601759394009908}]}], "abstractContent": [{"text": "While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging , typically requiring both a large body of knowledge and inference techniques.", "labels": [], "entities": [{"text": "factoid question-answering (QA)", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.8230852365493775}]}, {"text": "Open Information Extraction (Open IE) provides away to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrieval-based methods.", "labels": [], "entities": [{"text": "Open Information Extraction (Open IE)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7664201557636261}]}, {"text": "We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled.", "labels": [], "entities": []}, {"text": "Using a recently proposed support graph optimization framework for QA, we develop anew inference model for Open IE, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples.", "labels": [], "entities": []}, {"text": "Our model significantly outperforms a state-of-the-art struc-tured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge.", "labels": [], "entities": []}], "introductionContent": [{"text": "Effective question answering (QA) systems have been a long-standing quest of AI research.", "labels": [], "entities": [{"text": "Effective question answering (QA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7815838704506556}]}, {"text": "Structured curated KBs have been used successfully for this task).", "labels": [], "entities": []}, {"text": "However, these KBs are expensive to build and typically domain-specific.", "labels": [], "entities": []}, {"text": "Automatically constructed open vocabulary (subject; predicate; object) style tuples have broader coverage, but have only been used for simple questions where a single tuple suffices.", "labels": [], "entities": []}, {"text": "Our goal in this work is to develop a QA system that can perform reasoning with Open IE () tuples for complex multiple-choice questions that require tuples from multiple sentences.", "labels": [], "entities": []}, {"text": "Such a system can answer complex questions in resource-poor domains where curated knowledge is unavailable.", "labels": [], "entities": []}, {"text": "Elementary-level science exams is one such domain, requiring complex reasoning.", "labels": [], "entities": []}, {"text": "Due to the lack of a large-scale structured KB, state-of-the-art systems for this task either rely on shallow reasoning with large text corpora or deeper, structured reasoning with a small amount of automatically acquired) or manually curated (  knowledge.", "labels": [], "entities": []}, {"text": "Consider the following question from an Alaska state 4th grade science test: Which object in our solar system reflects light and is a satellite that orbits around one planet?", "labels": [], "entities": []}, {"text": "(A) Earth (B) Mercury (C) the Sun (D) the Moon This question is challenging for QA systems because of its complex structure and the need for multi-fact reasoning.", "labels": [], "entities": []}, {"text": "A natural way to answer it is by combining facts such as (Moon; is; in the solar system), (Moon; reflects; light), (Moon; is; satellite), and (Moon; orbits; around one planet).", "labels": [], "entities": []}, {"text": "A candidate system for such reasoning, and which we draw inspiration from, is the TABLEILP system of . TABLEILP treats QA as a search for an optimal subgraph that connects terms in the question and answer via rows in a set of curated tables, and solves the optimization problem using Integer Linear Programming (ILP).", "labels": [], "entities": []}, {"text": "We similarly want to search for an optimal subgraph.", "labels": [], "entities": []}, {"text": "However, a large, automatically extracted tuple KB makes the reasoning context different on three fronts: (a) unlike reasoning with tables, chaining tuples is less important and reliable as join rules aren't available; inference model isn't the best fit for noisy tuples.", "labels": [], "entities": []}, {"text": "To address this challenge, we present anew ILP-based model of inference with tuples, implemented in a reasoner called TUPLEINF.", "labels": [], "entities": []}, {"text": "We demonstrate that TUPLEINF significantly outperforms TABLEILP by 11.8% on abroad set of over 1,300 science questions, without requiring manually curated tables, using a substantially simpler ILP formulation, and generalizing well to higher grade levels.", "labels": [], "entities": [{"text": "TUPLEINF", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.8978080153465271}, {"text": "TABLEILP", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9697666764259338}]}, {"text": "The gains persist even when both solvers are provided identical knowledge.", "labels": [], "entities": [{"text": "solvers", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9573845267295837}]}, {"text": "This demonstrates for the first time how Open IE based QA can be extended from simple lookup questions to an effective system for complex questions.", "labels": [], "entities": [{"text": "Open IE based QA", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.5023802369832993}]}], "datasetContent": [{"text": "Comparing our method with two state-of-the-art systems for 4th and 8th grade science exams, we demonstrate that (a) TUPLEINF with only automatically extracted tuples significantly outperforms TABLEILP with its original curated knowl-   , and includes professionally written licensed questions.", "labels": [], "entities": []}, {"text": "(2) 8th Grade set (293 train, 282 test) contains 8th grade questions from various states.", "labels": [], "entities": [{"text": "8th Grade set", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.6816360354423523}]}, {"text": "We consider two knowledge sources: (1) The Sentence corpus (S) consists of domain-targeted 80K sentences and 280 GB of plain text extracted from web pages used by . This corpus is used as a collection of sentences by the IR solver.", "labels": [], "entities": [{"text": "IR solver", "start_pos": 221, "end_pos": 230, "type": "TASK", "confidence": 0.8768394291400909}]}, {"text": "It is also used to create the tuple KB T (Sec. 3.1) and on-the-fly questionspecific tuples T 0 qa (Sec. 3.2) for TUPLEINF.", "labels": [], "entities": []}, {"text": "(2) TABLEILP uses \u21e070 Curated tables (C) containing about 7,600 rows, designed for 4th grade NY Regents exams.", "labels": [], "entities": [{"text": "TABLEILP", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.7605263590812683}, {"text": "NY Regents exams", "start_pos": 93, "end_pos": 109, "type": "DATASET", "confidence": 0.831028401851654}]}, {"text": "We compare TUPLEINF with two state-of-theart baselines.", "labels": [], "entities": [{"text": "TUPLEINF", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.6831925511360168}]}, {"text": "IR is a simple yet powerful information-retrieval baseline  that selects the answer option with the best matching sentence in a corpus.", "labels": [], "entities": [{"text": "IR", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.6268163323402405}]}, {"text": "TABLEILP is the stateof-the-art structured inference baseline ( ) developed for science questions.", "labels": [], "entities": [{"text": "TABLEILP", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8478810787200928}]}, {"text": "shows that while TUPLEINF achieves similar scores as the IR solver, the approaches are complementary (structured lossy knowledge reasoning vs. lossless sentence retrieval).", "labels": [], "entities": [{"text": "IR solver", "start_pos": 57, "end_pos": 66, "type": "TASK", "confidence": 0.9218280911445618}]}, {"text": "The two solvers, in fact, differ on 47.3% of the training questions.", "labels": [], "entities": [{"text": "solvers", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.9710219502449036}]}, {"text": "To exploit this complementarity, we train an ensemble system  which, as shown in the table, provides a substantial boost over the individual solvers.", "labels": [], "entities": []}, {"text": "Further, IR + TUPLEINF is consistently better than IR + TABLEILP.", "labels": [], "entities": [{"text": "IR +", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9383663237094879}, {"text": "TUPLEINF", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.5383240580558777}, {"text": "IR + TABLEILP", "start_pos": 51, "end_pos": 64, "type": "METRIC", "confidence": 0.7856306831041971}]}], "tableCaptions": [{"text": " Table 2: TUPLEINF is significantly better at struc- tured reasoning than TABLEILP. 9", "labels": [], "entities": [{"text": "TUPLEINF", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.7167138457298279}, {"text": "TABLEILP", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.969859778881073}]}, {"text": " Table 3: TUPLEINF is complementarity to IR, re- sulting in a strong ensemble", "labels": [], "entities": [{"text": "TUPLEINF", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9751606583595276}, {"text": "IR", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.7531766891479492}]}]}