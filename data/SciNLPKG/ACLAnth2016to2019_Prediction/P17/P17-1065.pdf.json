{"title": [], "abstractContent": [{"text": "Nowadays atypical Neural Machine Translation (NMT) model generates translations from left to right as a linear sequence, during which latent syntactic structures of the target sentences are not explicitly concerned.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.8283120890458425}]}, {"text": "Inspired by the success of using syntactic knowledge of target language for improving statistical machine translation, in this paper we propose a novel Sequence-to-Dependency Neural Machine Translation (SD-NMT) method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.6960063179334005}, {"text": "Sequence-to-Dependency Neural Machine Translation (SD-NMT)", "start_pos": 152, "end_pos": 210, "type": "TASK", "confidence": 0.7288498452731541}, {"text": "word generations", "start_pos": 385, "end_pos": 401, "type": "TASK", "confidence": 0.700921356678009}]}, {"text": "Experimental results show that the proposed method significantly outperforms state-of-the-art base-lines on Chinese-English and Japanese-English translation tasks.", "labels": [], "entities": [{"text": "Japanese-English translation tasks", "start_pos": 128, "end_pos": 162, "type": "TASK", "confidence": 0.7666151920954386}]}], "introductionContent": [{"text": "Recently, Neural Machine Translation (NMT) with the attention-based encoder-decoder framework () has achieved significant improvements in translation quality of many language pairs (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.7620532512664795}]}, {"text": "Ina conventional NMT model, an encoder reads in source sentences of various lengths, and transforms them into sequences of intermediate hidden vector representations.", "labels": [], "entities": []}, {"text": "After weighted by attention operations, combined hidden vectors are used by the decoder to generate translations.", "labels": [], "entities": []}, {"text": "In most of cases, both encoder and decoder are implemented as recurrent neural networks (RNNs).", "labels": [], "entities": []}, {"text": "* Contribution during internship at Microsoft Research.", "labels": [], "entities": []}, {"text": "Many methods have been proposed to further improve the sequence-to-sequence NMT model since it was first proposed by and.", "labels": [], "entities": []}, {"text": "Previous work ranges from addressing the problem of out-ofvocabulary words (), designing attention mechanism (, to more efficient parameter learning, using source-side syntactic trees for better encoding () and soon.", "labels": [], "entities": []}, {"text": "All these NMT models employ a sequential recurrent neural network for target generations.", "labels": [], "entities": []}, {"text": "Although in theory RNN is able to remember sufficiently long history, we still observe substantial incorrect translations which violate long-distance syntactic constraints.", "labels": [], "entities": []}, {"text": "This suggests that it is still very challenging fora linear RNN to learn models that effectively capture many subtle long-range word dependencies.", "labels": [], "entities": []}, {"text": "For example, shows an incorrect translation related to the long-distance dependency.", "labels": [], "entities": []}, {"text": "The translation fragment in italic is locally fluent around the word is, but from a global view the translation is ungrammatical.", "labels": [], "entities": []}, {"text": "Actually, this part of translation should be mostly affected by the distant plural noun foreigners rather than words Venezuelan government nearby.", "labels": [], "entities": [{"text": "translation", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.9699563384056091}]}, {"text": "Fortunately, such long-distance word correspondence can be well addressed and modeled by syntactic dependency trees.", "labels": [], "entities": []}, {"text": "In, the headword foreigners in the partial dependency tree (top dashed box) can provide correct structural context for the next target word, with this information it is more likely to generate the correct word will rather than is.", "labels": [], "entities": []}, {"text": "This structure has been successfully applied to significantly improve the performance of statistical machine translation).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 89, "end_pos": 120, "type": "TASK", "confidence": 0.7216803828875223}]}, {"text": "On the NMT side, introducing target syntactic structures could help solve the problem of ungrammatical output because it can bring two advantages over state-of-the-art NMT models: a) syntactic trees can be used to model the grammatical validity of translation candidates; b) partial syntactic structures can be used as additional context to facilitate future target word prediction.", "labels": [], "entities": []}, {"text": "Ref : He added that foreign visitors to Venezuela who criticize the Venezuelan government will face serious consequences and will be deported . NMT : He also said that foreigners to Venezuela who attack the Venezuelan government is facing serious consequences, will be deported . However, it is not trivial to build and leverage syntactic structures on the target side in current NMT framework.", "labels": [], "entities": [{"text": "NMT", "start_pos": 144, "end_pos": 147, "type": "DATASET", "confidence": 0.9254524111747742}]}, {"text": "Several practical challenges arise: (1) How to model syntactic structures such as dependency parse trees with recurrent neural network; (2) How to efficiently perform both target word generation and syntactic structure construction tasks simultaneously in a single neural network; (3) How to effectively leverage target syntactic context to help target word generation.", "labels": [], "entities": [{"text": "dependency parse trees", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7894209126631418}, {"text": "target word generation and syntactic structure construction", "start_pos": 172, "end_pos": 231, "type": "TASK", "confidence": 0.6431676021644047}, {"text": "target word generation", "start_pos": 346, "end_pos": 368, "type": "TASK", "confidence": 0.7956853906313578}]}, {"text": "To address these issues, we propose and empirically evaluate a novel Sequence-to-Dependency Neural Machine Translation (SD-NMT) model in our paper.", "labels": [], "entities": [{"text": "Sequence-to-Dependency Neural Machine Translation (SD-NMT)", "start_pos": 69, "end_pos": 127, "type": "TASK", "confidence": 0.7026150396892002}]}, {"text": "An SD-NMT model encodes source inputs with bi-directional RNNs and associates them with target word prediction via attention mechanism as inmost NMT models, but it comes with anew decoder which is able to jointly generate target translations and construct their syntactic dependency trees.", "labels": [], "entities": []}, {"text": "The key difference from conventional NMT decoders is that we use two RNNs, one for translation generation and the other for dependency parse tree construction, in which incremental parsing is performed with the arc-standard shift-reduce algorithm proposed by.", "labels": [], "entities": [{"text": "translation generation", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.959353893995285}, {"text": "dependency parse tree construction", "start_pos": 124, "end_pos": 158, "type": "TASK", "confidence": 0.825037807226181}]}, {"text": "We will describe in detail how these two RNNs work interactively in Section 3.", "labels": [], "entities": []}, {"text": "We evaluate our method on publicly available data sets with Chinese-English and JapaneseEnglish translation tasks.", "labels": [], "entities": [{"text": "JapaneseEnglish translation", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.6647028028964996}]}, {"text": "Experimental results show that our model significantly improves translation accuracy over the conventional NMT and SMT baseline systems.", "labels": [], "entities": [{"text": "translation", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.9362314939498901}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.8834404349327087}, {"text": "SMT", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9031650424003601}]}], "datasetContent": [{"text": "The experiments are conducted on the ChineseEnglish task as well as the Japanese-English translation tasks where the same data set from WAT 2016 ASPEC corpus (Nakazawa et al., 2016) 3 is used fora fair comparison with other work.", "labels": [], "entities": [{"text": "Japanese-English translation tasks", "start_pos": 72, "end_pos": 106, "type": "TASK", "confidence": 0.7185418506463369}, {"text": "WAT 2016 ASPEC corpus", "start_pos": 136, "end_pos": 157, "type": "DATASET", "confidence": 0.8843337893486023}]}, {"text": "In addition to evaluate translation performance, we also investigate the quality of dependency parsing as a by-product and the effect of parsing quality against translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9693756103515625}, {"text": "dependency parsing", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.6879656314849854}]}, {"text": "We evaluate our method on the Chinese-English translation task.", "labels": [], "entities": [{"text": "Chinese-English translation task", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.7548761367797852}]}, {"text": "The evaluation results overall NIST test sets against baselines are listed in.", "labels": [], "entities": [{"text": "NIST test sets", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.9448002974192301}]}, {"text": "Generally, RNNsearch outperforms HPSMT by 3.78 BLEU points on average while SD-NMT surpasses RNNsearch 2.03 BLUE point gains on average, which shows that NMT models usually achieve better results than SMT models, and our proposed sequence-to-dependency NMT model performs much better than traditional sequence-tosequence NMT model.", "labels": [], "entities": [{"text": "HPSMT", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.8743875026702881}, {"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9976155757904053}, {"text": "BLUE", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9698992371559143}, {"text": "SMT", "start_pos": 201, "end_pos": 204, "type": "TASK", "confidence": 0.9808261394500732}]}, {"text": "We also investigate the effect of syntactic knowledge context by excluding its computation in Equation 12 and 13.", "labels": [], "entities": []}, {"text": "The alternative model is denoted by SD-NMT\\K.", "labels": [], "entities": []}, {"text": "According to, SD-NMT\\K outperforms RNNsearch by 0.54 BLEU points but degrades SD-NMT by 1.49 BLEU points on average, which demonstrates that the long distance dependencies captured by the target syntactic knowledge context, such as leftmost/rightmost children together with their dependency relationships, really bring strong positive effects on the prediction of target words.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9973918199539185}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9899376630783081}]}, {"text": "In addition to translation quality, we compare the perplexity (PPL) changes on the development set in terms of numbers of training mini-batches for RNNsearch and SD-NMT in.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9630348086357117}]}, {"text": "We can see that the PPL of SD-NMT is initially higher than that of RNNsearch, but decreased to be lower overtime.", "labels": [], "entities": [{"text": "PPL", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9681618213653564}, {"text": "overtime", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9749027490615845}]}, {"text": "This is mainly because the quality of parse tree is too poor at the beginning which degrades translation quality and leads to higher PPL.", "labels": [], "entities": []}, {"text": "After some training iterations, the SD-NMT: Evaluation results on Japanese-English translation task.", "labels": [], "entities": [{"text": "Japanese-English translation", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.6454252004623413}]}, {"text": "model learns reasonable inferences of parse trees which begins to help target word generation and leads to lower PPL.", "labels": [], "entities": [{"text": "word generation", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.7189239263534546}]}, {"text": "In our experiments, the time cost of SD-NMT is two times of that for RNNsearch due to a more complicated model structure.", "labels": [], "entities": []}, {"text": "But we think it is a worthy trade to pursue high quality translations.", "labels": [], "entities": []}, {"text": "In this section, we report results on the JapaneseEnglish translation task.", "labels": [], "entities": [{"text": "JapaneseEnglish translation task", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.8202847639719645}]}, {"text": "To ensure fair comparisons, we use the same training data and follow the pre-processing steps recommended in WAT 2016 . shows the comparison results from 8 systems with the evaluation metrics of BLEU and RIBES.", "labels": [], "entities": [{"text": "WAT 2016", "start_pos": 109, "end_pos": 117, "type": "DATASET", "confidence": 0.7837314307689667}, {"text": "BLEU", "start_pos": 195, "end_pos": 199, "type": "METRIC", "confidence": 0.9977558255195618}, {"text": "RIBES", "start_pos": 204, "end_pos": 209, "type": "METRIC", "confidence": 0.746143102645874}]}, {"text": "The results in the first 3 rows are produced by SMT systems taken from the official WAT 2016.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.948591947555542}, {"text": "WAT 2016", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.6873209327459335}]}, {"text": "The remaining results are produced by NMT systems, among which the bottom two row results are taken from our in-house NMT systems and others refer to the work in) that are the competitive NMT results on WAT 2016.", "labels": [], "entities": [{"text": "WAT 2016", "start_pos": 203, "end_pos": 211, "type": "DATASET", "confidence": 0.8179590702056885}]}, {"text": "According to, NMT results still outperform SMT results similar to our Chinese-English evaluation results.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9836894869804382}]}, {"text": "The SD-NMT model significantly outperforms most other NMT models, which shows that our proposed approach to modeling target dependency tree benefit NMT systems since our RNNsearch baseline achieves comparable performance with the single layer attention-based NMT system in.", "labels": [], "entities": []}, {"text": "Note that our SD-NMT gets comparable results with the 4 single-layer ensemble model in).", "labels": [], "entities": []}, {"text": "We believe SD-NMT can get more improvements with an ensemble of multiple models in future experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results on Chinese-English translation task with BLEU% metric. The \"Average\"  column is the averaged result of all test sets. The numbers in bold indicate statistically significant differ- ence (p < 0.05) from baselines.", "labels": [], "entities": [{"text": "Chinese-English translation task", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.6578235924243927}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9992386102676392}, {"text": "statistically significant differ- ence", "start_pos": 176, "end_pos": 214, "type": "METRIC", "confidence": 0.7493042469024658}]}, {"text": " Table 2: Evaluation results on Japanese-English translation task.", "labels": [], "entities": [{"text": "Japanese-English translation", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.673193946480751}]}]}