{"title": [{"text": "STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset", "labels": [], "entities": [{"text": "STAIR Captions", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8212649822235107}, {"text": "Constructing a Large-Scale Japanese Image Caption", "start_pos": 16, "end_pos": 65, "type": "TASK", "confidence": 0.6094717631737391}]}], "abstractContent": [{"text": "In recent years, automatic generation of image descriptions (captions), that is, image captioning, has attracted a great deal of attention.", "labels": [], "entities": [{"text": "automatic generation of image descriptions (captions)", "start_pos": 17, "end_pos": 70, "type": "TASK", "confidence": 0.8631784841418266}, {"text": "image captioning", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.7037603408098221}]}, {"text": "In this paper, we particularly consider generating Japanese captions for images.", "labels": [], "entities": []}, {"text": "Since most available caption datasets have been constructed for English language , there are few datasets for Japanese.", "labels": [], "entities": []}, {"text": "To tackle this problem, we construct a large-scale Japanese image caption dataset based on images from MS-COCO, which is called STAIR Captions.", "labels": [], "entities": [{"text": "Japanese image caption", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.6081091066201528}, {"text": "MS-COCO", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9355008602142334}, {"text": "STAIR Captions", "start_pos": 128, "end_pos": 142, "type": "TASK", "confidence": 0.7494021356105804}]}, {"text": "STAIR Captions consists of 820,310 Japanese captions for 164,062 images.", "labels": [], "entities": [{"text": "STAIR Captions", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7371731102466583}]}, {"text": "In the experiment, we show that a neural network trained using STAIR Captions can generate more natural and better Japanese captions, compared to those generated using English-Japanese machine translation after generating En-glish captions.", "labels": [], "entities": [{"text": "STAIR Captions", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.666364312171936}]}], "introductionContent": [{"text": "Integrated processing of natural language and images has attracted attention in recent years.", "labels": [], "entities": []}, {"text": "The Workshop on Vision and Language held in 2011 has since become an annual event1.", "labels": [], "entities": []}, {"text": "In this research area, methods to automatically generate image descriptions (captions), that is, image captioning, have attracted a great deal of attention ( . Image captioning is to automatically generate a caption fora given image.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 97, "end_pos": 113, "type": "TASK", "confidence": 0.696267381310463}, {"text": "Image captioning", "start_pos": 160, "end_pos": 176, "type": "TASK", "confidence": 0.6713268458843231}]}, {"text": "By improving the quality of image captioning, image search using natural sentences and image recognition support for 1In recent years it has been held as a joint workshop such as EMNLP and ACL; https://vision.cs.hacettepe.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7171060293912888}, {"text": "image search", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.8434283137321472}, {"text": "image recognition", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.6741162836551666}, {"text": "EMNLP", "start_pos": 179, "end_pos": 184, "type": "DATASET", "confidence": 0.9360480308532715}, {"text": "ACL", "start_pos": 189, "end_pos": 192, "type": "DATASET", "confidence": 0.8329408168792725}]}, {"text": "edu.tr/vl2017/ visually impaired people by outputting captions as sounds can be made available.", "labels": [], "entities": []}, {"text": "Recognizing various images and generating appropriate captions for the images necessitates the compilation of a large number of image and caption pairs.", "labels": [], "entities": []}, {"text": "In this study, we consider generating image captions in Japanese.", "labels": [], "entities": [{"text": "generating image captions", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.6260922451814016}]}, {"text": "Since most available caption datasets have been constructed for English language, there are few datasets for Japanese.", "labels": [], "entities": []}, {"text": "A straightforward solution is to translate English captions into Japanese ones by using machine translation such as Google Translate.", "labels": [], "entities": [{"text": "translate English captions", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.8100422024726868}]}, {"text": "However, the translated captions maybe literal and unnatural because image information cannot be reflected in the translation.", "labels": [], "entities": []}, {"text": "Therefore, in this study, we construct a Japanese image caption dataset, and forgiven images, we aim to generate more natural Japanese captions than translating the generated English captions into the Japanese ones.", "labels": [], "entities": [{"text": "Japanese image caption dataset", "start_pos": 41, "end_pos": 71, "type": "DATASET", "confidence": 0.6211981773376465}]}, {"text": "The contributions of this paper are as follows: \u2022 We constructed a large-scale Japanese image caption dataset, STAIR Captions, which consists of Japanese captions for all the images in MS-COCO () (Section 3).", "labels": [], "entities": [{"text": "Japanese image caption", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.5798837343851725}, {"text": "STAIR Captions", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.794958770275116}]}, {"text": "\u2022 We confirmed that quantitatively and qualitatively better Japanese captions than the ones translated from English captions can be generated by applying a neural network-based image caption generation model learned on STAIR Captions (Section 5).", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 177, "end_pos": 201, "type": "TASK", "confidence": 0.8077486753463745}, {"text": "STAIR Captions", "start_pos": 219, "end_pos": 233, "type": "TASK", "confidence": 0.7678234577178955}]}, {"text": "STAIR Captions is available for download from http://captions.stair.center.", "labels": [], "entities": [{"text": "STAIR Captions", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7791753709316254}]}], "datasetContent": [{"text": "In this section, we perform an experiment which generates Japanese captions using STAIR Captions.", "labels": [], "entities": [{"text": "STAIR Captions", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.6154729723930359}]}, {"text": "The aim of this experiment is to show the necessity of a Japanese caption dataset.", "labels": [], "entities": [{"text": "Japanese caption dataset", "start_pos": 57, "end_pos": 81, "type": "DATASET", "confidence": 0.8531855543454488}]}, {"text": "In particular, we evaluate quantitatively and qualitatively how fluent Japanese captions can be generated by using a neural network-based caption generation model trained on STAIR Captions.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.6887987852096558}, {"text": "STAIR Captions", "start_pos": 174, "end_pos": 188, "type": "TASK", "confidence": 0.7329199314117432}]}, {"text": "Following the literature (, we use BLEU (), ROUGE, and CIDEr (  as evaluation measures.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9988025426864624}, {"text": "ROUGE", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9975267052650452}, {"text": "CIDEr", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.7814013361930847}]}, {"text": "Although BLEU and ROUGE were developed originally for evaluating machine translation and text summarization, we use them here because they are often used for measuring the quality of caption generation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9974129796028137}, {"text": "ROUGE", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.9924744963645935}, {"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7622223198413849}, {"text": "text summarization", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7379485368728638}, {"text": "caption generation", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.8907532095909119}]}, {"text": "Following the experimental setting in the previous studies, we used 123,287 images included in the MS-COCO training and validation sets and their corresponding Japanese captions.", "labels": [], "entities": [{"text": "MS-COCO training and validation sets", "start_pos": 99, "end_pos": 135, "type": "DATASET", "confidence": 0.8024916291236878}]}, {"text": "We divided the dataset into three parts, i.e., 113,287 images for the training set, 5,000 images for the validation set, and 5,000 images for the test set.", "labels": [], "entities": []}, {"text": "The hyper-parameters of the neural network were tuned based on CIDEr scores by using the validation set.", "labels": [], "entities": []}, {"text": "As preprocessing, we applied morphological analysis to the Japanese captions using MeCab6.", "labels": [], "entities": [{"text": "Japanese captions", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.8368232548236847}, {"text": "MeCab6", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.919142484664917}]}, {"text": "The results show that Ja-generator, that is, the approach in which Japanese captions were used as training data, outperformed En-generator \u2192 MT, which was trained without Japanese captions.", "labels": [], "entities": []}, {"text": "shows two examples where Ja-generator generated appropriate captions, whereas Engenerator \u2192 MT generated unnatural ones.", "labels": [], "entities": []}, {"text": "In the example at the top in, En-generator first generated the term, \"A double decker bus.\"", "labels": [], "entities": []}, {"text": "MT translated the term into as \" \", but the translation is word-by-word and inappropriate as a Japanese term.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.6959818601608276}]}, {"text": "By contrast, Jagenerator generated \" (two-story bus),\" which is appropriate as the Japanese translation of A double decker bus.", "labels": [], "entities": []}, {"text": "In the example at the bottom of the table, En-generator \u2192 MT yielded the incorrect caption by translating \"A bunch of food\" as \" (A bundle of food).\"", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.6899951696395874}]}, {"text": "By contrast, Ja-generator correctly recognized that the food pictured in the image is a donut, and expressed it as \" (A bunch of donuts).\"", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experimental results of Japanese caption generation. The numbers in boldface indicate the best  score for each evaluation measure.  BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE_L CIDEr", "labels": [], "entities": [{"text": "Japanese caption generation", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7760757009188334}, {"text": "BLEU-1", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.998106837272644}, {"text": "BLEU-2", "start_pos": 149, "end_pos": 155, "type": "METRIC", "confidence": 0.8020585179328918}, {"text": "BLEU-3", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.803704023361206}, {"text": "BLEU-4 ROUGE_L CIDEr", "start_pos": 163, "end_pos": 183, "type": "METRIC", "confidence": 0.8290303111076355}]}, {"text": " Table 3:  Examples of generated image cap- tions. En-generator denotes the caption genera- tor trained with MS-COCO. En-generator \u2192 MT  is the pipeline method: it first generates English  caption and performs machine translation subse- quently. Ja-generator was trained with Japanese  captions.", "labels": [], "entities": [{"text": "MS-COCO", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.8979174494743347}, {"text": "machine translation subse- quently", "start_pos": 210, "end_pos": 244, "type": "TASK", "confidence": 0.8015102446079254}]}]}