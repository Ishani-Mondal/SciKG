{"title": [{"text": "Neural Joint Model for Transition-based Chinese Syntactic Analysis", "labels": [], "entities": [{"text": "Syntactic Analysis", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7713774144649506}]}], "abstractContent": [{"text": "We present neural network-based joint models for Chinese word segmentation, POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.5897395114103953}, {"text": "POS tagging", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.8460634648799896}, {"text": "dependency parsing", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7920465767383575}]}, {"text": "Our models are the first neural approaches for fully joint Chinese analysis that is known to prevent the error propagation problem of pipeline models.", "labels": [], "entities": []}, {"text": "Although word em-beddings play a key role in dependency parsing, they cannot be applied directly to the joint task in the previous work.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8442059755325317}]}, {"text": "To address this problem, we propose embed-dings of character strings, in addition to words.", "labels": [], "entities": []}, {"text": "Experiments show that our models outperform existing systems in Chinese word segmentation and POS tagging, and perform preferable accuracies in dependency parsing.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.5795119206110636}, {"text": "POS tagging", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.7675249576568604}, {"text": "dependency parsing", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.7958717048168182}]}, {"text": "We also explore bi-LSTM models with fewer features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency parsers have been enhanced by the use of neural networks and embedding vectors.", "labels": [], "entities": [{"text": "Dependency parsers", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8195261657238007}]}, {"text": "When these dependency parsers process sentences in English and other languages that use symbols for word separations, they can be very accurate.", "labels": [], "entities": []}, {"text": "However, for languages that do not contain word separation symbols, dependency parsers are used in pipeline processes with word segmentation and POS tagging models, and encounter serious problems because of error propagations.", "labels": [], "entities": [{"text": "word separation symbols", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.7783935169378916}, {"text": "word segmentation", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.720960944890976}, {"text": "POS tagging", "start_pos": 145, "end_pos": 156, "type": "TASK", "confidence": 0.6883739978075027}]}, {"text": "In particular, Chinese word segmentation is notoriously difficult because sentences are written without word dividers and Chinese words are not clearly defined.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.6226232051849365}]}, {"text": "Hence, the pipeline of word segmentation, POS tagging and dependency parsing always suffers from word segmentation errors.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.756918340921402}, {"text": "POS tagging", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.8492381870746613}, {"text": "dependency parsing", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.6902606338262558}, {"text": "word segmentation", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.6954481154680252}]}, {"text": "Once words have been wronglysegmented, word embeddings and traditional onehot word features, used in dependency parsers, will mistake the precise meanings of the original sentences.", "labels": [], "entities": []}, {"text": "As a result, pipeline models achieve dependency scores of around 80% for Chinese.", "labels": [], "entities": []}, {"text": "A traditional solution to this error propagation problem is to use joint models.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7526844441890717}]}, {"text": "Many Chinese words play multiple grammatical roles with only one grammatical form.", "labels": [], "entities": []}, {"text": "Therefore, determining the word boundaries and the subsequent tagging and dependency parsing are closely correlated.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.6901056170463562}]}, {"text": "Transition-based joint models for Chinese word segmentation, POS tagging and dependency parsing are proposed by and.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.5858547190825144}, {"text": "POS tagging", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.8624716997146606}, {"text": "dependency parsing", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.7703053653240204}]}, {"text": "state that dependency information improves the performances of word segmentation and POS tagging, and develop the first transition-based joint word segmentation, POS tagging and dependency parsing model.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.746839165687561}, {"text": "POS tagging", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.7412964999675751}, {"text": "POS tagging", "start_pos": 162, "end_pos": 173, "type": "TASK", "confidence": 0.7011994123458862}, {"text": "dependency parsing", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.6593502312898636}]}, {"text": "expand this and find that both the inter-word dependencies and intraword dependencies are helpful in word segmentation and POS tagging.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7544042468070984}, {"text": "POS tagging", "start_pos": 123, "end_pos": 134, "type": "TASK", "confidence": 0.838901549577713}]}, {"text": "Although the models of and perform better than pipeline models, they rely on the one-hot representation of characters and words, and do not assume the similarities among characters and words.", "labels": [], "entities": []}, {"text": "In addition, not only words and characters but also many incomplete tokens appear in the transitionbased joint parsing process.", "labels": [], "entities": [{"text": "transitionbased joint parsing", "start_pos": 89, "end_pos": 118, "type": "TASK", "confidence": 0.517667144536972}]}, {"text": "Such incomplete or unknown words (UNK) could become important cues for parsing, but they are not listed in dictionaries or pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "Some recent studies show that character-based embeddings are effective in neural parsing (, but their models could not be directly applied to joint models because they use given word segmentations.", "labels": [], "entities": [{"text": "neural parsing", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.7336404323577881}]}, {"text": "To solve these problems, we propose neural network-based joint models for word segmentation, POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.786593109369278}, {"text": "POS tagging", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.8825919926166534}, {"text": "dependency parsing", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7918553650379181}]}, {"text": "We use both character and word embeddings for known tokens and apply character string embeddings for unknown tokens.", "labels": [], "entities": []}, {"text": "Another problem in the models of and is that they rely on detailed feature engineering.", "labels": [], "entities": []}, {"text": "Recently, bidirectional LSTM (bi-LSTM) based neural network models with very few feature extraction are proposed.", "labels": [], "entities": []}, {"text": "In their models, the bi-LSTM is used to represent the tokens including their context.", "labels": [], "entities": []}, {"text": "Indeed, such neural networks can observe whole sentence through the bi-LSTM.", "labels": [], "entities": []}, {"text": "This bi-LSTM is similar to that of neural machine translation models of.", "labels": [], "entities": []}, {"text": "As a result, achieve competitive scores with the previous state-of-theart models.", "labels": [], "entities": []}, {"text": "We also develop joint models with ngram character string bi-LSTM.", "labels": [], "entities": []}, {"text": "In the experiments, we obtain state-of-the-art Chinese word segmentation and POS tagging scores, and the pipeline of the dependency model achieves the better dependency scores than the previous joint models.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.5754006008307139}, {"text": "POS tagging", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.6627780646085739}]}, {"text": "To the best of our knowledge, this is the first model to use embeddings and neural networks for Chinese full joint parsing.", "labels": [], "entities": [{"text": "Chinese full joint parsing", "start_pos": 96, "end_pos": 122, "type": "TASK", "confidence": 0.6284159198403358}]}, {"text": "Our contributions are summarized as follows: (1) we propose the first embedding-based fully joint parsing model, (2) we use character string embeddings for UNK and incomplete tokens.", "labels": [], "entities": []}, {"text": "(3) we also explore bi-LSTM models to avoid the detailed feature engineering in previous approaches.", "labels": [], "entities": []}, {"text": "(4) in experiments using Chinese corpus, we achieve state-of-the-art scores in word segmentation, POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7496022880077362}, {"text": "POS tagging", "start_pos": 98, "end_pos": 109, "type": "TASK", "confidence": 0.8902147710323334}, {"text": "dependency parsing", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.7992745637893677}]}], "datasetContent": [{"text": "We use the Penn Chinese Treebank 5.1 (CTB-5) and 7 (CTB-7) datasets to evaluate our models, following the splitting of Jiang et al. for CTB-5 and Wang et al.", "labels": [], "entities": [{"text": "Penn Chinese Treebank 5.1 (CTB-5) and 7 (CTB-7) datasets", "start_pos": 11, "end_pos": 67, "type": "DATASET", "confidence": 0.8195808484004095}]}, {"text": "(2011) for CTB-7.", "labels": [], "entities": [{"text": "CTB-7", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.9662737250328064}]}, {"text": "The statistics of datasets are presented in.", "labels": [], "entities": []}, {"text": "We use the Chinese Gigaword Corpus for embedding pre-training.", "labels": [], "entities": [{"text": "Chinese Gigaword Corpus", "start_pos": 11, "end_pos": 34, "type": "DATASET", "confidence": 0.9092246691385905}]}, {"text": "Our model is developed for unlabeled dependencies.", "labels": [], "entities": []}, {"text": "The development set is used for parameter tuning.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7703472375869751}]}, {"text": "Following and, we use the standard word-level evaluation with F1-measure.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9976252913475037}]}, {"text": "The POS tags and dependencies cannot be correct unless the corresponding words are correctly segmented.", "labels": [], "entities": []}, {"text": "We trained three models: SegTag, SegTagDep and Dep.", "labels": [], "entities": []}, {"text": "SegTag is the joint word segmentation and POS tagging model.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7146138101816177}, {"text": "POS tagging", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.7727807760238647}]}, {"text": "SegTagDep is the full joint segmentation, tagging and dependency parsing model.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.6507602781057358}]}, {"text": "Dep is the dependency parsing model which is similar to  and, but uses the embeddings of character strings.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7644281387329102}]}, {"text": "Dep compensates for UNKs and segmentation errors caused by previous word segmentation using embeddings of character strings.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.751487523317337}]}, {"text": "We will examine this effect later.", "labels": [], "entities": []}, {"text": "Most experiments are conducted on GPUs, but some of beam decoding processes are performed on CPUs because of the large mini-batch size.", "labels": [], "entities": []}, {"text": "The neural network is implemented with Theano.", "labels": [], "entities": [{"text": "Theano", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.9592602252960205}]}, {"text": "We also test the SegTagDep and SegTag+Dep models on CTB-7.", "labels": [], "entities": [{"text": "CTB-7", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.9876694083213806}]}, {"text": "In these experiments, we no-: Results from SegTag+Dep and SegTagDep applied to the CTB-7 corpus.", "labels": [], "entities": [{"text": "CTB-7 corpus", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.9704790115356445}]}, {"text": "(g) denotes greedy trained models.", "labels": [], "entities": []}, {"text": "\u2021 denotes that the improvement is statistically siginificant at p < 0.01 compared with SegTagDep(g) using paired t-test.", "labels": [], "entities": []}, {"text": "tice that the MLP with four hidden layers performs better than the MLP with three hidden layers, but we could not find definite differences in the experiments in CTB-5.", "labels": [], "entities": [{"text": "CTB-5", "start_pos": 162, "end_pos": 167, "type": "DATASET", "confidence": 0.9429923295974731}]}, {"text": "We speculate that this is caused by the difference in the training set size.", "labels": [], "entities": []}, {"text": "We present the final results with four hidden layers in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parameters for neural network structure  and training.", "labels": [], "entities": []}, {"text": " Table 3: Features for the bi-LSTM models. All  features are words and characters. We experiment  both four and eight features models.", "labels": [], "entities": []}, {"text": " Table 4: Summary of datasets.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.7657264471054077}]}, {"text": " Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).", "labels": [], "entities": [{"text": "Joint segmentation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7535759210586548}, {"text": "POS tagging", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.7556692659854889}, {"text": "F-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9675851464271545}]}, {"text": " Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.", "labels": [], "entities": [{"text": "Joint Segmentation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6230637431144714}, {"text": "POS Tagging", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.816882461309433}, {"text": "Dependency Parsing", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.6807893812656403}, {"text": "EAG", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.9869419932365417}]}, {"text": " Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.", "labels": [], "entities": []}, {"text": " Table 9: SegTagDep model with and without  (-q0bX) features across the buffer and stack. We  compare these models with greedy training (g).", "labels": [], "entities": []}, {"text": " Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  \u2021 denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.", "labels": [], "entities": [{"text": "CTB-7 corpus", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.9857155680656433}]}, {"text": " Table 11: Bi-LSTM feature extraction model.  \"4feat.\" and \"8feat.\" denote the use of four and  eight features.", "labels": [], "entities": [{"text": "Bi-LSTM feature extraction", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.5514088670412699}]}]}