{"title": [], "abstractContent": [{"text": "Cross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same tax-onomy of categories.", "labels": [], "entities": [{"text": "Cross-lingual text classification(CLTC)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8060798744360606}]}, {"text": "This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression.", "labels": [], "entities": []}, {"text": "Using soft proba-bilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents , we train classifiers successfully for new languages in which labeled training data are not available.", "labels": [], "entities": []}, {"text": "An adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch.", "labels": [], "entities": [{"text": "adversarial feature adaptation", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.7508997122446696}]}, {"text": "We conducted experiments on two benchmark CLTC datasets, treating English as the source language and German, French, Japan and Chinese as the unlabeled target languages.", "labels": [], "entities": [{"text": "CLTC datasets", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.8281242847442627}]}, {"text": "The proposed approach had the advantageous or comparable performance of the other state-of-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "The availability of massive multilingual data on the Internet makes cross-lingual text classification (CLTC) increasingly important.", "labels": [], "entities": [{"text": "cross-lingual text classification (CLTC)", "start_pos": 68, "end_pos": 108, "type": "TASK", "confidence": 0.7886765549580256}]}, {"text": "The task is defined as to classify documents in different languages using the same taxonomy of predefined categories.", "labels": [], "entities": []}, {"text": "CLTC systems build on supervised machine learning require a sufficiently amount of labeled training data for every domain of interest in each language.", "labels": [], "entities": []}, {"text": "But in reality, labeled data are not evenly distributed among languages and across domains.", "labels": [], "entities": []}, {"text": "English, for example, is a label-rich language in the domains of news stories, Wikipedia pages and reviews of hotels, products, etc.", "labels": [], "entities": []}, {"text": "But many other languages do not necessarily have such rich amounts of labeled data.", "labels": [], "entities": []}, {"text": "This leads to an open challenge in CLTC, i.e., how can we effectively leverage the trained classifiers in a label-rich source language to help the classification of documents in other label-poor target languages?", "labels": [], "entities": []}, {"text": "Existing methods in CLTC use either a bilingual dictionary or a parallel corpus to bridge language barriers and to translate classification models () or text data().", "labels": [], "entities": [{"text": "translate classification", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.7492422759532928}]}, {"text": "There are limitations and challenges in using either type of resources.", "labels": [], "entities": []}, {"text": "Dictionary-based methods often ignore the dependency of word meaning and its context, and cannot leverage domainspecific disambiguation when the dictionary on hand is a general-purpose one.", "labels": [], "entities": []}, {"text": "Parallel-corpus based methods, although more effective in deploying context (when combined with word embedding in particular), often have an issue of domain mismatch or distribution mismatch if the available source-language training data, the parallel corpus (human-aligned or machine-translation induced one) and the target documents of interest are not in exactly the same domain and genre).", "labels": [], "entities": []}, {"text": "How to solve such domain/distribution mismatch problems is an open question for research.", "labels": [], "entities": []}, {"text": "This paper proposes anew parallel-corpus based approach, focusing on the reduction of domain/distribution matches in CLTC.", "labels": [], "entities": []}, {"text": "We call this approach Cross-lingual Distillation with Feature Adaptation or CLDFA in short.", "labels": [], "entities": [{"text": "Cross-lingual Distillation", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.7917977273464203}]}, {"text": "It is inspired by the recent work in model compression where a large ensemble model is transformed to a compact (small) model.", "labels": [], "entities": [{"text": "model compression", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7421048730611801}]}, {"text": "The assumption of knowledge distillation for model compression is that the knowledge learned by the large model can be viewed as a mapping from in-put space to output (label) space.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7361465096473694}, {"text": "model compression", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.752212643623352}]}, {"text": "Then, by training with the soft labels predicted by the large model, the small model can capture most of the knowledge from the large model.", "labels": [], "entities": []}, {"text": "Extending this key idea to CLTC, if we see parallel documents as different instantiations of the same semantic concepts in different languages, a target-language classifier should gain the knowledge from a welltrained source classifier by training with the targetlanguage part of the parallel corpus and the soft labels made by the source classifier on the source language side.", "labels": [], "entities": []}, {"text": "More specifically, we propose to distillate knowledge from the source language to the target language in the following 2-step process: \u2022 Firstly, we train a source-language classifier with both labeled training documents and adapt it to the unlabeled documents from the source-language side of the parallel corpus.", "labels": [], "entities": []}, {"text": "The adaptation enforces our classifier to extract features that are: 1) discriminative for the classification task and 2) invariant with regard to the distribution shift between training and parallel data.", "labels": [], "entities": []}, {"text": "\u2022 Secondly, we use the trained source-language classifier to obtain the soft labels fora parallel corpus, and the target-language part of the parallel corpus to train a target classifier, which yields a similar category distribution over target-language documents as that over source-language documents.", "labels": [], "entities": []}, {"text": "We also use unlabeled testing documents in the target language to adapt the feature extractor in this training step.", "labels": [], "entities": []}, {"text": "Intuitively, the first step addresses the potential domain/distribution mismatch between the labeled data and the unlabeled data in the source language.", "labels": [], "entities": []}, {"text": "The second step addresses the potential mismatch between the target-domain training data (in the parallel corpus) and the test data (not in the parallel corpus).", "labels": [], "entities": []}, {"text": "The soft-label based training of target classifiers makes our approach unique among parallel-corpus based CLTC methods (Section 2.1.", "labels": [], "entities": []}, {"text": "The feature adaptation step makes our framework particularly robust in addressing the distributional difference between in-domain documents and parallel corpus, which is important for the success of CLTC with low-resource languages.", "labels": [], "entities": []}, {"text": "The main contributions in this paper are the following: \u2022 We propose a novel framework (CLDFA) for knowledge distillation in CLTC through a parallel corpus.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.7128468751907349}]}, {"text": "It has the flexibility to be built on a large family of existing monolingual text classification methods and enables the use of a large amount of unlabeled data from both source and target language.", "labels": [], "entities": [{"text": "text classification", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.769821047782898}]}, {"text": "\u2022 CLDFA has the same computational complexity as the plug-in text classification method and hence is very efficient and scalable with the proper choice of plug-in text classifier.", "labels": [], "entities": [{"text": "plug-in text classification", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.6334906617800394}]}, {"text": "\u2022 Our evaluation on benchmark datasets shows that our method had a better or at least comparable performance than that of other stateof-art CLTC methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments used two benchmark datasets, as described below.", "labels": [], "entities": []}, {"text": "We used the multilingual multi-domain Amazon review dataset created by.", "labels": [], "entities": [{"text": "Amazon review dataset", "start_pos": 38, "end_pos": 59, "type": "DATASET", "confidence": 0.7331906060377756}]}, {"text": "The dataset contains Amazon reviews in three domains: book, DVD and music.", "labels": [], "entities": []}, {"text": "Each domain has the reviews in four different languages: English, German, French and Japanese.", "labels": [], "entities": []}, {"text": "We treated English as the source language and the rest three as the target languages, respectively.", "labels": [], "entities": []}, {"text": "This gives us 9 tasks (the product of the 3 domains and the 3 target languages) in total.", "labels": [], "entities": []}, {"text": "For each task, there are 1000 positive and 1000 negative reviews in English and the target language, respectively.", "labels": [], "entities": []}, {"text": "() also provides 2000 parallel reviews per task, that were generated using Google Translate 1 , and used by us for cross-language distillation.", "labels": [], "entities": []}, {"text": "There are also several thousands of unlabeled reviews in each language.", "labels": [], "entities": []}, {"text": "The statistics of unlabeled data is summarized in.", "labels": [], "entities": []}, {"text": "All the reviews are tokenized using standard regular expressions except for Japanese, for which we used a publicly available segmenter 2 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset Statistics for the Amazon reviews  dataset", "labels": [], "entities": [{"text": "Amazon reviews  dataset", "start_pos": 37, "end_pos": 60, "type": "DATASET", "confidence": 0.9170417189598083}]}, {"text": " Table 2: Accuracy scores of methods on the Amazon Reviews dataset: the best score in each row (a  task) is highlighted in bold face. If the score of CLDFA-KCNN is statistically significantly better (in  one-sample proportion tests) than the best among the baseline methods, it is marked using a star.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.986689567565918}, {"text": "Amazon Reviews dataset", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.9464985132217407}]}, {"text": " Table 3: Accuracy scores of methods on the  English-Chinese Yelp Hotel Reviews dataset", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9984344840049744}, {"text": "English-Chinese Yelp Hotel Reviews dataset", "start_pos": 45, "end_pos": 87, "type": "DATASET", "confidence": 0.8710150718688965}]}]}