{"title": [{"text": "Modeling Source Syntax for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.7507612506548563}]}], "abstractContent": [{"text": "Even though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 61, "end_pos": 93, "type": "TASK", "confidence": 0.8530879418055216}]}, {"text": "Specifically, we linearize parse trees of source sentences to obtain structural label sequences.", "labels": [], "entities": []}, {"text": "On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly ; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed.", "labels": [], "entities": []}, {"text": "Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.7021403014659882}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.8197179436683655}]}, {"text": "It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9986935257911682}]}, {"text": "Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) * Work done at Huawei Noah's Ark Lab, HongKong.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.8451703190803528}, {"text": "statistical machine translation (SMT)", "start_pos": 143, "end_pos": 180, "type": "TASK", "confidence": 0.7755700250466665}, {"text": "Huawei Noah's Ark Lab", "start_pos": 196, "end_pos": 217, "type": "DATASET", "confidence": 0.9031849265098572}, {"text": "HongKong", "start_pos": 219, "end_pos": 227, "type": "DATASET", "confidence": 0.6064920425415039}]}, {"text": "on various language pairs ().", "labels": [], "entities": []}, {"text": "However, show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus.", "labels": [], "entities": []}, {"text": "Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT.", "labels": [], "entities": []}, {"text": "As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax.", "labels": [], "entities": []}, {"text": "In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9320968389511108}]}, {"text": "In principle, syntax is a promising avenue for translation modeling.", "labels": [], "entities": [{"text": "translation modeling", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.9854443669319153}]}, {"text": "This has been verified by tremendous encouraging studies on syntaxbased SMT that substantially improves translation by integrating various kinds of syntactic knowledge (.", "labels": [], "entities": [{"text": "syntaxbased SMT", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.5441939234733582}, {"text": "translation", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.9708712100982666}]}, {"text": "While it is yet to be seen how syntax can benefit NMT effectively, we find that translations of NMT sometimes fail to well respect source syntax.", "labels": [], "entities": []}, {"text": "(a) shows a Chinese-to-English translation example of NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.7385115623474121}]}, {"text": "In this example, the NMT seq2seq model incorrectly translates the Chinese noun phrase (i.e., /xinsheng /yinhang) into a discontinuous phrase in English (i.e., new ...", "labels": [], "entities": []}, {"text": "bank) due to the failure of capturing the internal syntactic structure in the input Chinese sentence.", "labels": [], "entities": []}, {"text": "Statistics on our development set show that one forth of Chinese noun phrases are translated into discontinuous phrases in English, indicating the substantial disrespect of syntax in NMT translation.", "labels": [], "entities": [{"text": "NMT translation", "start_pos": 183, "end_pos": 198, "type": "TASK", "confidence": 0.8142835795879364}]}, {"text": "1 (b) shows another example with over translation, where the noun phrase /liang /ge /nvhai is translated twice in English.", "labels": [], "entities": [{"text": "over translation", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.6942310780286789}]}, {"text": "Similar to discontinuous translation, over translation usually happens along with the disrespect of syntax which results in the repeated translation of the same source words in multiple positions of the target sentence.", "labels": [], "entities": [{"text": "discontinuous translation", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.7475489377975464}, {"text": "over translation", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7359551638364792}]}, {"text": "In this paper we are not aiming at solving any particular issue, either the discontinuous translation or the over translation.", "labels": [], "entities": [{"text": "over translation", "start_pos": 109, "end_pos": 125, "type": "TASK", "confidence": 0.761931836605072}]}, {"text": "Alternatively, we address how to incorporate explicitly the source syntax to improve the NMT translation accuracy with the expectation of alleviating the issues above in general.", "labels": [], "entities": [{"text": "NMT translation", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.676985114812851}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9049486517906189}]}, {"text": "Specifically, rather than directly assigning each source word with manually designed syntactic labels, as Sennrich and Haddow (2016) do, we linearize a phrase parse tree into a structural label sequence and let the model automatically learn useful syntactic information.", "labels": [], "entities": []}, {"text": "On the basis, we systematically propose and compare several different approaches to incorporating the label sequence into the seq2seq NMT model.", "labels": [], "entities": []}, {"text": "Experimentation on Chinese-to-English translation demonstrates that all proposed approaches are able to improve the translation accuracy.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.6749668121337891}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.940177321434021}]}], "datasetContent": [{"text": "We have presented our approaches to incorporating the source syntax into NMT encoders.", "labels": [], "entities": []}, {"text": "In this section, we evaluate their effectiveness on Chinese-to-English translation.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.6518417149782181}]}, {"text": "Our training data for the translation task consists of for the translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.9324651956558228}, {"text": "translation task", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.9327952861785889}]}, {"text": "For efficient training of neural networks, we limit the maximum sentence length on both source and target sides to 50.", "labels": [], "entities": []}, {"text": "We also limit both the source and target vocabularies to the most frequent 16K words in Chinese and English, covering approximately 95.8% and 98.2% of the two corpora respectively.", "labels": [], "entities": []}, {"text": "All the out-of-vocabulary words are mapped to a special token UNK.", "labels": [], "entities": [{"text": "UNK", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.8160106539726257}]}, {"text": "Besides, the word embedding dimension is 620 and the size of a hidden layer is 1000.", "labels": [], "entities": []}, {"text": "All the other settings are the same as in The inventory of structural labels includes 16 phrase labels and 32 POS tags.", "labels": [], "entities": []}, {"text": "In both our Parallel RNN encoder and Hierarchical RNN encoder, we set the embedding dimension of these labels as 100 and the size of a hidden layer as 100.", "labels": [], "entities": []}, {"text": "Besides, the maximum structural label sequence length is set to 100.", "labels": [], "entities": []}, {"text": "In our Mixed RNN encoder, since we only have one input sequence, we equally treat the structural labels and words (i.e., a structural label is also initialized with 620 dimension embedding).", "labels": [], "entities": []}, {"text": "Compared to the baseline NMT model, the only different setting is that we increase the maximum sentence length on source-side from 50 to 150.", "labels": [], "entities": []}, {"text": "We compare our method with two state-of-theart models of SMT and NMT: \u2022 cdec (: an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9880203604698181}]}, {"text": "7 \u2022 RNNSearch: a re-implementation of the attentional NMT system (Bahdanau et al., 2015) with slight changes taken from dl4mt tutorial.", "labels": [], "entities": []}, {"text": "8 For the activation function f of an RNN, RNNSearch uses the gated recurrent unit (GRU) recently proposed by).", "labels": [], "entities": []}, {"text": "It incorporates dropout () on the output layer and improves the attention model by feeding the lastly generated word.", "labels": [], "entities": []}, {"text": "We use AdaDelta to optimize model parameters in training with the mini-batch size of 80.", "labels": [], "entities": []}, {"text": "For translation, abeam search with size 10 is employed.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9918091893196106}]}, {"text": "Comparison with the SMT model (cdec) Table 1 also shows that all NMT systems outperform the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9681683778762817}, {"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9523299336433411}]}, {"text": "This is very consistent with other studies on Chinese-to-English translation (.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.6339161396026611}]}], "tableCaptions": [{"text": " Table 3: Percentages (%) of syntactic phrases in  our test sets being translated continuously, discon- tinuously, or not being translated. Here PP is for  prepositional phrase, NP for noun phrase, CP for  clause headed by a complementizer, QP for quain- ter phrase.", "labels": [], "entities": []}, {"text": " Table 4: Ratio of over translation (ROT) on test  sets. Here NR is for proper noun, CD for cardi- nal number, DT for determiner, and NN for nouns  except proper nouns and temporal nouns.", "labels": [], "entities": [{"text": "Ratio of over translation (ROT)", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.8104810501847949}]}, {"text": " Table 5: Percentages (%) of rare words in our test  sets being translated into a non-UNK word (non- UNK), UNK (UNK), or if it is not translated at all  (Un.).", "labels": [], "entities": []}]}