{"title": [{"text": "Joint Optimization of User-desired Content in Multi-document Summaries by Learning from User Feedback", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose an extractive multi-document summarization (MDS) system using joint optimization and active learning for content selection grounded in user feedback.", "labels": [], "entities": [{"text": "extractive multi-document summarization (MDS)", "start_pos": 29, "end_pos": 74, "type": "TASK", "confidence": 0.763962984085083}]}, {"text": "Our method interactively obtains user feedback to gradually improve the results of a state-of-the-art integer linear programming (ILP) framework for MDS.", "labels": [], "entities": []}, {"text": "Our methods complement fully automatic methods in producing high-quality summaries with a minimum number of iterations and feedbacks.", "labels": [], "entities": []}, {"text": "We conduct multiple simulation-based experiments and analyze the effect of feedback-based concept selection in the ILP setup in order to maximize the user-desired content in the summary.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of producing summaries from a cluster of multiple topic-related documents has gained much attention during the Document Understanding Conference (DUC) and the Text Analysis Conference 2 (TAC) series.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC)", "start_pos": 120, "end_pos": 159, "type": "TASK", "confidence": 0.8276026248931885}, {"text": "Text Analysis Conference 2 (TAC)", "start_pos": 168, "end_pos": 200, "type": "TASK", "confidence": 0.7999017792088645}]}, {"text": "Despite a lot of research in this area, it is still a major challenge to automatically produce summaries that are on par with human-written ones.", "labels": [], "entities": []}, {"text": "To a large extent, this is due to the complexity of the task: a good summary must include the most relevant information, omit redundancy and irrelevant information, satisfy a length constraint, and be cohesive and grammatical.", "labels": [], "entities": []}, {"text": "But an even bigger challenge is the high degree of subjectivity in content selection, as it can be seen in the small overlap of what is considered important by different users.", "labels": [], "entities": []}, {"text": "Optimizing a system towards one single best summary that fits all users, as it is assumed by current state-of-the-art systems, is highly impractical and diminishes the usefulness of a system for real-world use cases.", "labels": [], "entities": []}, {"text": "In this paper, we propose an interactive conceptbased model to assist users in creating a personalized summary based on their feedback.", "labels": [], "entities": []}, {"text": "Our model employs integer linear programming (ILP) to maximize user-desired content selection while using a minimum amount of user feedback and iterations.", "labels": [], "entities": []}, {"text": "In addition to the joint optimization framework using ILP, we explore pool-based active learning to further reduce the required feedback.", "labels": [], "entities": []}, {"text": "Although there have been previous attempts to assist users in single-document summarization, no existing work tackles the problem of multi-document summaries using optimization techniques for user feedback.", "labels": [], "entities": [{"text": "single-document summarization", "start_pos": 62, "end_pos": 91, "type": "TASK", "confidence": 0.4930764436721802}, {"text": "multi-document summaries", "start_pos": 133, "end_pos": 157, "type": "TASK", "confidence": 0.5529317855834961}]}, {"text": "Additionally, most existing systems produce only a single, globally optimal solution.", "labels": [], "entities": []}, {"text": "Instead, we put the human in the loop and create a personalized summary that better captures the users' needs and their different notions of importance.", "labels": [], "entities": []}, {"text": "shows the ROUGE scores) of multiple existing summarization systems, namely TF*IDF,), TextRank (), LSA (Gong and), KL-Greedy (Haghighi and Vanderwende, 2009), provided by the sumy package and ICSI (, a strong state-of-the-art approach) in comparison to the extractive upper bound on DUC'04 and DBS.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9887604713439941}, {"text": "summarization", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9668735265731812}, {"text": "TextRank", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.8802919387817383}, {"text": "ICSI", "start_pos": 191, "end_pos": 195, "type": "METRIC", "confidence": 0.5749887824058533}, {"text": "DUC'04", "start_pos": 282, "end_pos": 288, "type": "DATASET", "confidence": 0.9100708365440369}, {"text": "DBS", "start_pos": 293, "end_pos": 296, "type": "DATASET", "confidence": 0.8167494535446167}]}, {"text": "DUC'04 is an English dataset of abstractive summaries from ho-: Lexical overlap of a reference summary) with the summary produced by ICSI's state-of-the-art system (: ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-SU4 (SU4) scores of multiple systems compared to the extractive upper bound (UB) mogenous news texts, whereas DBS () is a German dataset of cohesive extracts from heterogeneous sources from the educational domain (see details in section 4.1).", "labels": [], "entities": [{"text": "DUC'04", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9462299942970276}, {"text": "ICSI", "start_pos": 133, "end_pos": 137, "type": "DATASET", "confidence": 0.9263184070587158}]}, {"text": "For each dataset, we compute an extractive upper bound (UB) by optimizing the sentence selection which maximizes ROUGE-2, i.e., the occurrence of bigrams as in the reference summary.", "labels": [], "entities": [{"text": "extractive upper bound (UB)", "start_pos": 32, "end_pos": 59, "type": "METRIC", "confidence": 0.691321074962616}, {"text": "ROUGE-2", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.9975540041923523}]}, {"text": "Although some systems achieve state-ofthe-art performance, their scores are still far from the extractive upper bound of individual reference summaries as shown in.", "labels": [], "entities": []}, {"text": "This is due to low inter-annotator agreement for concept selection: Zechner (2002) reports, for example, \u03ba = .13 and Benikova et al.", "labels": [], "entities": []}, {"text": "(2016) \u03ba = .23.", "labels": [], "entities": []}, {"text": "Most systems try to optimize for all reference summaries instead of personalizing, which we consider essential to capture user-desired content.", "labels": [], "entities": []}, {"text": "The goal of concept selection is finding the important information within a given set of source documents.", "labels": [], "entities": [{"text": "concept selection", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7525892853736877}]}, {"text": "Although existing summarization algorithms come up with a generic notion of importance, it is still far from the user-specific importance as shown in.", "labels": [], "entities": [{"text": "summarization", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.9710636138916016}]}, {"text": "In contrast, humans can easily assess importance given a topic or a query.", "labels": [], "entities": []}, {"text": "One way to achieve personalized summarization is thus by combining the advantages of both human feedback and the generic notion of importance builtin a system.", "labels": [], "entities": [{"text": "personalized summarization", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.6397706270217896}]}, {"text": "This allows users to interactively steer the summarization process and integrate their user-specific notion of importance.", "labels": [], "entities": [{"text": "summarization process", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.90290766954422}]}, {"text": "In this work, (1) we propose a novel ILP-based model using an interactive loop to create multi-document user-desired summaries, and (2) we develop models using pool-based active learning and joint optimization techniques to collect user feedback on identifying important concepts of a topic.", "labels": [], "entities": []}, {"text": "In order to encourage the community to advance research and replicate our results, we provide our interactive summarizer implementation as open-source software.", "labels": [], "entities": []}, {"text": "5 . Our proposed method and our new interactive summarization framework can be used in multiple application scenarios: as an interactive annotation tool, which highlights important sentences for the annotators, as a journalistic writing aid that suggests important, user-adapted content from multiple source feeds (e.g., live blogs), and as a medical data analysis tool that suggests key information assisting a patient's personalized medical diagnosis.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: In section 2, we discuss related work.", "labels": [], "entities": []}, {"text": "Section 3 introduces our computer-assisted summarization framework using the concept-based optimization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.7793114185333252}]}, {"text": "Section 4 describes our experiment data and setup.", "labels": [], "entities": []}, {"text": "In section 5, we then discuss our results and analyze the performance of our models across different datasets.", "labels": [], "entities": []}, {"text": "Finally, we conclude the paper in section 6 and discuss future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "For DBS, it becomes clear that the JOINT model converges faster with an optimum amount of feedback as compared to other models.", "labels": [], "entities": [{"text": "DBS", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9160265922546387}, {"text": "JOINT", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.6110184788703918}]}, {"text": "AC-CEPT takes relatively more feedbacks than JOINT, but performs low in terms of ROUGE scores.", "labels": [], "entities": [{"text": "JOINT", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.6009079217910767}, {"text": "ROUGE", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9978199005126953}]}, {"text": "The best performing models are AL and AL+, which reach closest to the upper bound.", "labels": [], "entities": []}, {"text": "This is clearly due to the exploratory nature of the models which use semantic representations of the concepts to predict uncertainty and importance of possible concepts for user feedback.", "labels": [], "entities": []}, {"text": "For DUC'04, the JOINT model reaches the closest to the upper bound, closely followed by AL.", "labels": [], "entities": [{"text": "DUC'04", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.8492426872253418}, {"text": "JOINT", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.74168461561203}, {"text": "AL", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9956181049346924}]}, {"text": "The JOINT model consistently stays above all other models and it gathers more important concepts due to optimizing feedbacks for concepts which lack feedback.", "labels": [], "entities": []}, {"text": "Interestingly, AL+ performs rather worse in terms of both ROUGE scores and gathering important concepts.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 58, "end_pos": 70, "type": "METRIC", "confidence": 0.9751878380775452}]}, {"text": "The primary reason for this is the fewer feedback collected from the simulation due to the abstractive property of reference summaries, which makes the AL+ model's prediction inconsistent.", "labels": [], "entities": []}, {"text": "shows the performance of different models in comparison to two different oracles for the same document cluster.", "labels": [], "entities": []}, {"text": "For DBS, the JOINT, AL, and AL+ models consistently converge to the upper bound in 4 iterations for different oracles, whereas ACCEPT takes longer for one oracle and does not reach the upper bound for the other.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE-1 (R1), ROUGE-2 (R2), and  ROUGE-SU4 (SU4) scores of multiple systems  compared to the extractive upper bound (UB)", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.900318443775177}]}, {"text": " Table 3: ROUGE-1 (R1), ROUGE-2 (R2) and ROUGE SU-4 (SU4) achieved by our models after the  tenth iteration of the interactive loop in comparison to the upper bound and the basic ILP setup", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8690196871757507}]}, {"text": " Table 4: Average amount of user feedback (#F)  considered by our models at the end of the tenth  iteration of the interactive summarization loop", "labels": [], "entities": [{"text": "Average amount of user feedback (#F)", "start_pos": 10, "end_pos": 46, "type": "METRIC", "confidence": 0.7542841732501984}]}]}