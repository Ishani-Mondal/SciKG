{"title": [{"text": "Temporal Word Analogies: Identifying Lexical Replacement with Diachronic Word Embeddings", "labels": [], "entities": [{"text": "Temporal Word Analogies", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8202810684839884}, {"text": "Identifying Lexical Replacement", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.9066898226737976}]}], "abstractContent": [{"text": "This paper introduces the concept of temporal word analogies: pairs of words which occupy the same semantic space at different points in time.", "labels": [], "entities": []}, {"text": "One well-known property of word embeddings is that they are able to effectively model traditional word analogies (\"word w 1 is to word w 2 as word w 3 is to word w 4 \") through vector addition.", "labels": [], "entities": []}, {"text": "Here, I show that temporal word analogies (\"word w 1 at time t \u03b1 is like word w 2 at time t \u03b2 \") can effectively be mod-eled with diachronic word embeddings, provided that the independent embedding spaces from each time period are appropriately transformed into a common vector space.", "labels": [], "entities": []}, {"text": "When applied to a diachronic corpus of news articles, this method is able to identify temporal word analogies such as \"Ronald Reagan in 1987 is like Bill Clin-ton in 1997\", or \"Walkman in 1987 is like iPod in 2007\".", "labels": [], "entities": []}, {"text": "1 Background The meanings of utterances changeover time, due both to changes within the linguistic system and to changes in the state of the world.", "labels": [], "entities": []}, {"text": "For example, the meaning of the word awful has changed over the past few centuries from something like \"awe-inspiring\" to something more like \"very bad\", due to a process of semantic drift.", "labels": [], "entities": []}, {"text": "On the other hand, the phrase president of the United States has meant different things at different points in time due to the fact that different people have occupied that same position at different times.", "labels": [], "entities": []}, {"text": "These are very different types of changes, and the latter may not even be considered a linguistic phenomenon, but both types of change are relevant to the concept of temporal word analogies.", "labels": [], "entities": []}, {"text": "I define a temporal word analogy (TWA) as a pair of words which occupy a similar semantic space at different points in time.", "labels": [], "entities": [{"text": "temporal word analogy (TWA)", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.8165480196475983}]}, {"text": "For example, assuming that there is a semantic space associated with \"President of the USA\", this space was occupied by Ronald Reagan in the 1980s, and by Bill Clinton in the 1990s.", "labels": [], "entities": []}, {"text": "So a temporal analogy holds: \"Ronald Reagan in 1987 is like Bill Clinton in 1997\".", "labels": [], "entities": []}, {"text": "Distributional semantics methods, particularly vector-space models of word meanings, have been employed to study both semantic change and word analogies, and as such are well-suited for the task of identifying TWAs.", "labels": [], "entities": [{"text": "identifying TWAs", "start_pos": 198, "end_pos": 214, "type": "TASK", "confidence": 0.6880561113357544}]}, {"text": "The principle behind these models, that the meaning of words can be captured by looking at the contexts in which they appear (i.e. other words), is not a recent idea, and is generally attributed to Harris (1954) or Firth (1957).", "labels": [], "entities": []}, {"text": "The modern era of applying this principle algo-rithmically began with latent semantic analysis (LSA) (Landauer and Dumais, 1997), and the recent explosion in popularity of word embeddings is largely due to the very effective word2vec neural network approach to computing word embeddings (Mikolov et al., 2013a).", "labels": [], "entities": [{"text": "latent semantic analysis (LSA)", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.6931942701339722}]}, {"text": "In these types of vector space models (VSMs), the meaning of a word is represented as a multi-dimensional vector, and semantically-related words tend to have vectors that relate to one another in regular ways (e.g. by occupying nearby points in the vector space).", "labels": [], "entities": []}, {"text": "One factor in word embeddings' recent popularity is their eye-catching ability to model word analogies using vector addition, as in the well-known example king + man \u2212 woman = queen (Mikolov et al., 2013b).", "labels": [], "entities": []}, {"text": "(2011) were the first to advocate the use of distributional semantics methods (specifi-cally LSA) to automate and quantify large-scale studies of semantic change, in contrast to a more traditional approach in which a researcher inspects 448", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Standard evaluation sets of word analogies exist and are commonly used as a measure of the quality of word embeddings (but see Linzen (2016) for why this can be problematic and misleading).", "labels": [], "entities": []}, {"text": "No data set of manually-verified TWAs currently exists, so a small evaluation set was assembled by hand: ten TWA categories were selected which could be expected to be both newsworthy and unambiguous, and values for each year in the corpus were identified using encyclopedic sources.", "labels": [], "entities": []}, {"text": "When all pairs of years are considered, this yields a total of 4,200 individual TWAs.", "labels": [], "entities": []}, {"text": "This data set, including the prediction outputs, is available online.", "labels": [], "entities": []}, {"text": "For comparison, a baseline system which always predicts the output w 2 to be the same as the input w 1 was implemented.", "labels": [], "entities": []}, {"text": "(A system based on word co-occurrence counts was also implemented, but produced no usable output.", "labels": [], "entities": []}, {"text": "3 ) shows the accuracy of the embedding-based system and the baseline for each category.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996246099472046}]}, {"text": "Accuracy is determined by exact match, so mayor giuliani is incorrect for giuliani.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9920915365219116}, {"text": "exact match", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.8957371413707733}]}, {"text": "Some categories are clearly much more difficult than others: prediction accuracy is 96% for \"President of the USA\", but less than 1% for \"Best Actress\".", "labels": [], "entities": [{"text": "prediction", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9693463444709778}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.8904186487197876}, {"text": "USA", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.7979167103767395}]}, {"text": "The names of Oscar Best Actress winners change every year with very little repetition, and it maybe that an actress' role as an award winner only constitutes a small part of her overall news coverage.", "labels": [], "entities": [{"text": "Oscar Best Actress winners", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.6201860159635544}, {"text": "repetition", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9656548500061035}]}, {"text": "Accuracy is a useful metric, but it is not necessarily the best way to evaluate TWAs.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.994848906993866}]}, {"text": "Due to the nature of the data (the U.S. President, for example, only changes every four or eight years), the baseline system works quite well when the time depth of the analogy (\u03b4 t = |t \u03b1 \u2212 t \u03b2 |) is small.", "labels": [], "entities": []}, {"text": "However,  as time depth increases, its accuracy drops sharply, while the embedding-based method remains effective, as illustrated in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9994321465492249}]}, {"text": "And even when the embedding-based system makes the \"wrong\" prediction, the output may still be insightful for data exploration, which is a more likely application for this method rather than prediction.", "labels": [], "entities": [{"text": "data exploration", "start_pos": 110, "end_pos": 126, "type": "TASK", "confidence": 0.7377201318740845}]}, {"text": "The analogies evaluated here have the benefits of being easy to compile and evaluate, but they represent only one specific subset of TWAs.", "labels": [], "entities": []}, {"text": "Other, less-clearly-defined, types of analogies (like the yuppie and walkman examples) would require a less rigid (and more expensive), form of evaluation, such as obtaining human acceptability judgments of the automatically-produced analogies.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Analogy prediction accuracy.", "labels": [], "entities": [{"text": "Analogy prediction", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9108745157718658}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9327475428581238}]}]}