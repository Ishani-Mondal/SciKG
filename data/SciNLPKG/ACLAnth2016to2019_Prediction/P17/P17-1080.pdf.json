{"title": [{"text": "What do Neural Machine Translation Models Learn about Morphology?", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.724423885345459}]}], "abstractContent": [{"text": "Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture.", "labels": [], "entities": [{"text": "Neural machine translation (MT)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8019263744354248}]}, {"text": "However, little is known about what these models learn about source and target languages during the training process.", "labels": [], "entities": []}, {"text": "In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks.", "labels": [], "entities": []}, {"text": "We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations.", "labels": [], "entities": []}, {"text": "Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural network models are quickly becoming the predominant approach to machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.8404626727104187}]}, {"text": "Training neural MT (NMT) models can be done in an end-to-end fashion, which is simpler and more elegant than traditional MT systems.", "labels": [], "entities": [{"text": "Training neural MT (NMT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7492510825395584}]}, {"text": "Moreover, NMT systems have become competitive with, or better than, the previous state-of-the-art, especially since the introduction of sequence-to-sequence models and the attention mechanism ().", "labels": [], "entities": []}, {"text": "The improved translation quality is often attributed to better handling of non-local dependencies and morphology generation).", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9475507736206055}, {"text": "morphology generation", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.7068382352590561}]}, {"text": "However, little is known about what and how much these models learn about each language and its features.", "labels": [], "entities": []}, {"text": "Recent work has started exploring the role of the NMT encoder in learning source syntax (, but research studies are yet to answer important questions such as: (i) what do NMT models learn about word morphology?", "labels": [], "entities": []}, {"text": "(ii) what is the effect on learning when translating into/from morphologically-rich languages?", "labels": [], "entities": []}, {"text": "(iii) what impact do different representations (character vs. word) have on learning? and (iv) what do different modules learn about the syntactic and semantic structure of a language?", "labels": [], "entities": []}, {"text": "Answering such questions is imperative for fully understanding the NMT architecture.", "labels": [], "entities": []}, {"text": "In this paper, we strive towards exploring (i), (ii), and (iii) by providing quantitative, data-driven answers to the following specific questions: \u2022 Which parts of the NMT architecture capture word structure?", "labels": [], "entities": [{"text": "NMT architecture capture word structure", "start_pos": 169, "end_pos": 208, "type": "TASK", "confidence": 0.6802838444709778}]}, {"text": "\u2022 What is the division of labor between different components (e.g. different layers or encoder vs. decoder)?", "labels": [], "entities": []}, {"text": "\u2022 How do different word representations help learn better morphology and modeling of infrequent words?", "labels": [], "entities": []}, {"text": "\u2022 How does the target language affect the learning of word structure?", "labels": [], "entities": [{"text": "learning of word structure", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.6319514885544777}]}, {"text": "To achieve this, we follow a simple but effective procedure with three steps: (i) train a neural MT system on a parallel corpus; (ii) use the trained model to extract feature representations for words in a language of interest; and (iii) train a classifier using extracted features to make predictions for another task.", "labels": [], "entities": []}, {"text": "We then evaluate the quality of the trained classifier on the given task as a proxy to the quality of the extracted representations.", "labels": [], "entities": []}, {"text": "In this way, we obtain a quantitative measure of how well the original MT system learns features that are relevant to the given task.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.98069828748703}]}, {"text": "We focus on the tasks of part-of-speech (POS) and full morphological tagging.", "labels": [], "entities": [{"text": "full morphological tagging", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.7051740090052286}]}, {"text": "We investigate how different neural MT systems capture POS and morphology through a series of experiments along several parameters.", "labels": [], "entities": []}, {"text": "For instance, we contrast word-based and character-based representations, use different encoding layers, vary source and target languages, and compare extracting features from the encoder vs. the decoder.", "labels": [], "entities": []}, {"text": "We experiment with several languages with varying degrees of morphological richness: French, German, Czech, Arabic, and Hebrew.", "labels": [], "entities": []}, {"text": "Our analysis reveals interesting insights such as: \u2022 Character-based representations are much better for learning morphology, especially for low-frequency words.", "labels": [], "entities": []}, {"text": "This improvement is correlated with better BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9994138479232788}]}, {"text": "On the other hand, word-based models are sufficient for learning the structure of common words.", "labels": [], "entities": []}, {"text": "\u2022 Lower layers of the encoder are better at capturing word structure, while deeper networks improve translation quality, suggesting that higher layers focus more on word meaning.", "labels": [], "entities": []}, {"text": "\u2022 The target language impacts the kind of information learned by the MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9811850190162659}]}, {"text": "Translating into morphologically-poorer languages leads to better source-side word representations.", "labels": [], "entities": []}, {"text": "This is partly, but not completely, correlated with BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9993126392364502}]}, {"text": "\u2022 The neural decoder learns very little about word structure.", "labels": [], "entities": [{"text": "word structure", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.7210836708545685}]}, {"text": "The attention mechanism removes much of the burden of learning word representations from the decoder.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics for annotated corpora in Arabic  (Ar), German (De), French (Fr), and Czech (Cz).", "labels": [], "entities": []}, {"text": " Table 2: POS accuracy on gold and predicted tags  using word-based and character-based representa- tions, as well as corresponding BLEU scores.", "labels": [], "entities": [{"text": "POS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8712337613105774}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.8803344368934631}, {"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.998957633972168}]}, {"text": " Table 4: POS tagging accuracy using word-based  and char-based encoder/decoder representations.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8058418929576874}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9481779336929321}]}, {"text": " Table 5: POS and morphology accuracy on pre- dicted tags using word-and char-based represen- tations from different layers of *-to-En systems.", "labels": [], "entities": [{"text": "POS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8130744099617004}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9913448095321655}]}, {"text": " Table 6: Impact of changing the target language  on POS tagging accuracy. Self = German/Czech  in rows 1/2 respectively.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.8025358617305756}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9344026446342468}]}, {"text": " Table 7: POS accuracy and BLEU using decoder  representations from different language pairs.", "labels": [], "entities": [{"text": "POS accuracy", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.6127452850341797}, {"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9986538887023926}]}]}