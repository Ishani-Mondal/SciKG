{"title": [{"text": "Implicitly-Defined Neural Networks for Sequence Labeling *", "labels": [], "entities": [{"text": "Sequence Labeling", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.9221487045288086}]}], "abstractContent": [{"text": "In this work, we propose a novel, implicitly-defined neural network architecture and describe a method to compute its components.", "labels": [], "entities": []}, {"text": "The proposed architecture forgoes the causality assumption used to formulate recurrent neural networks and instead couples the hidden states of the network, allowing improvement on problems with complex, long-distance dependencies.", "labels": [], "entities": []}, {"text": "Initial experiments demonstrate the new architecture outperforms both the Stanford Parser and baseline bidirectional networks on the Penn Treebank Part-of-Speech tagging task and a baseline bidi-rectional network on an additional artificial random biased walk task.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 133, "end_pos": 146, "type": "DATASET", "confidence": 0.9850740730762482}, {"text": "Part-of-Speech tagging task", "start_pos": 147, "end_pos": 174, "type": "TASK", "confidence": 0.6298442482948303}]}], "introductionContent": [{"text": "Feedforward neural networks were designed to approximate and interpolate functions.", "labels": [], "entities": []}, {"text": "Recurrent Neural Networks (RNNs) were developed to predict sequences.", "labels": [], "entities": []}, {"text": "RNNs can be 'unwrapped' and thought of as very deep feedforward networks, with weights shared between each layer.", "labels": [], "entities": []}, {"text": "Computation proceeds one step at a time, like the trajectory of an ordinary differential equation when solving an initial value problem.", "labels": [], "entities": []}, {"text": "The path of an initial value problem depends only on the current state and the current value of the forcing function.", "labels": [], "entities": []}, {"text": "Ina RNN, the analogy is the current hidden state and the current input sequence.", "labels": [], "entities": []}, {"text": "However, in certain applications in natural language processing, especially those with long-distance dependencies or where grammar matters, sequence predic-tion maybe better thought of as a boundary value problem.", "labels": [], "entities": []}, {"text": "Changing the value of the forcing function (analogously, of an input sequence element) at any point in the sequence will affect the values everywhere else.", "labels": [], "entities": []}, {"text": "The bidirectional recurrent network) attempts to addresses this problem by creating a network with two recurrent hidden states -one that progresses in the forward direction and one that progresses in the reverse.", "labels": [], "entities": []}, {"text": "This allows information to flow in both directions, but each state can only consider information from one direction.", "labels": [], "entities": []}, {"text": "In practice many algorithms require more than two passes through the data to determine an answer.", "labels": [], "entities": []}, {"text": "We provide a novel mechanism that is able to process information in both directions, with the motivation being a program which iterates over itself until convergence.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Biased walk classification performance.", "labels": [], "entities": [{"text": "Biased walk classification", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6444909969965616}]}, {"text": " Table 2: Tagging performance relative to recur- rent architectures and Stanford POS Tagger.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9854782819747925}]}]}