{"title": [{"text": "Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.7503870129585266}]}], "abstractContent": [{"text": "Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of syntactic information.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 5, "end_pos": 37, "type": "TASK", "confidence": 0.8517832060654958}]}, {"text": "In this paper , we improve this model by explicitly incorporating source-side syntactic trees.", "labels": [], "entities": []}, {"text": "More specifically, we propose (1) a bidi-rectional tree encoder which learns both sequential and tree structured representations ; (2) a tree-coverage model that lets the attention depend on the source-side syntax.", "labels": [], "entities": []}, {"text": "Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential atten-tional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.6247249990701675}]}], "introductionContent": [{"text": "Recently, neural machine translation (NMT) models ( have obtained state-of-the-art performance on many language pairs.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8162402709325155}]}, {"text": "Their success depends on the representation they use to bridge the source and target language sentences.", "labels": [], "entities": []}, {"text": "However, this representation, a sequence of fixed-dimensional vectors, differs considerably from most theories about mental representations of sentences, and from traditional natural language processing pipelines, in which semantics is built up compositionally using a recursive syntactic structure.", "labels": [], "entities": []}, {"text": "Perhaps as evidence of this, current NMT models still suffer from syntactic errors such as attachment (.", "labels": [], "entities": [{"text": "attachment", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.8901422619819641}]}, {"text": "We argue that instead of letting the NMT model rely solely on the implicit structure it learns during training (  (b) binarized source side tree: An example sentence pair (a), with its binarized source side tree (b).", "labels": [], "entities": []}, {"text": "We use xi to represent the i-th word in the source sentence.", "labels": [], "entities": []}, {"text": "We will use this sentence pairs as the running example throughout this paper.", "labels": [], "entities": []}, {"text": "2014a), we can improve its performance by augmenting it with explicit structural information and using this information throughout the model.", "labels": [], "entities": []}, {"text": "First, the explicit syntactic information will help the encoder generate better source side representations.", "labels": [], "entities": []}, {"text": "show that for tasks in which long-distance semantic dependencies matter, representations learned from recursive models using syntactic structures maybe more powerful than those from sequential recurrent models.", "labels": [], "entities": []}, {"text": "In the NMT case, given syntactic information, it will be easier for the encoder to incorporate long distance dependencies into better representations, which is especially important for the translation of long sentences.", "labels": [], "entities": [{"text": "translation of long sentences", "start_pos": 189, "end_pos": 218, "type": "TASK", "confidence": 0.8533431887626648}]}, {"text": "Second, it becomes possible for the decoder to use syntactic information to guide its reordering decisions better (especially for language pairs with significant reordering, like Chinese-English).", "labels": [], "entities": []}, {"text": "Although the attention model () and the coverage model ( provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages.", "labels": [], "entities": []}, {"text": "With explicit syntactic structure, the decoder can generate the translation more inline with the source syntactic structure.", "labels": [], "entities": []}, {"text": "For example, when translating the phrase zhu manila dashiguan in, the tree structure indicates that zhu 'in' and manila form a syntactic unit, so that the model can avoid breaking this unit up to make an incorrect translation like \"in embassy of manila\" 2 . In this paper, we propose a novel encoderdecoder model that makes use of a precomputed source-side syntactic tree in both the encoder and decoder.", "labels": [], "entities": []}, {"text": "In the encoder ( \u00a73.3), we improve the tree encoder of by introducing a bidirectional tree encoder.", "labels": [], "entities": []}, {"text": "For each source tree node (including the source words), we generate a representation containing information both from below (as with the original bottom-up encoder) and from above (using a top-down encoder).", "labels": [], "entities": []}, {"text": "Thus, the annotation of each node summarizes the surrounding sequential context, as well as the entire syntactic context.", "labels": [], "entities": []}, {"text": "In the decoder ( \u00a73.4), we incorporate source syntactic tree structure into the attention model via an extension of the coverage model of.", "labels": [], "entities": []}, {"text": "With this tree-coverage model, we can better guide the generation phase of translation, for example, to learn a preference for phrasal cohesion.", "labels": [], "entities": []}, {"text": "Moreover, with a tree encoder, the decoder may try to translate both a parent and a child node, even though they overlap; the treecoverage model enables the decoder to learn to avoid this problem.", "labels": [], "entities": []}, {"text": "To demonstrate the effectiveness of the proposed model, we carryout experiments on Chinese-English translation.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 83, "end_pos": 110, "type": "TASK", "confidence": 0.6513658314943314}]}, {"text": "Our experiments show that: (1) our bidirectional tree encoder based NMT system achieves significant improvements over the standard attention-based NMT system, and (2) incorporating source tree structure into the attention model yields a further improvement.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: BLEU scores of different systems. \"Sequential\", \"Tree-LSTM\", \"Tree-GRU\" and \"Bidirec- tional\" denote the encoder part for the standard sequential encoder, Tree-LSTM encoder, Tree-GRU  encoder and the bidirectional tree encoder, respectively. \"no\", \"word\" and \"tree\" in column \"Coverage\"  represents the decoder part for using no coverage (standard attention), word coverage (Tu et al., 2016)  and our proposed tree-coverage model, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9956117868423462}]}, {"text": " Table 3: BLEU scores of different systems based on LSTM. \"Seq-LSTM\" denotes both the encoder and  decoder parts for the sequential model are based on LSTM; \"SeqTree-LSTM\" means using Tree-LSTM  encoder on top of \"Seq-LSTM\".", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9981694221496582}]}, {"text": " Table 4: Experiments with 512 hidden units in each direction of the sequential encoder. The bidirectional  tree encoder using head-lexicalization (Bidirectional-head), proposed by", "labels": [], "entities": []}]}