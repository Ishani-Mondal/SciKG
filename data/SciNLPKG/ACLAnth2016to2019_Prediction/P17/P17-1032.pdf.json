{"title": [{"text": "Deep Learning in Semantic Kernel Spaces", "labels": [], "entities": []}], "abstractContent": [{"text": "Kernel methods enable the direct usage of structured representations of textual data during language learning and inference tasks.", "labels": [], "entities": []}, {"text": "Expressive kernels, such as Tree Kernels, achieve excellent performance in NLP.", "labels": [], "entities": []}, {"text": "On the other side, deep neural networks have been demonstrated effective in automatically learning feature representations during training.", "labels": [], "entities": []}, {"text": "However, their input is tensor data, i.e., they cannot manage rich structured information.", "labels": [], "entities": []}, {"text": "In this paper , we show that expressive kernels and deep neural networks can be combined in a common framework in order to (i) explicitly model structured information and (ii) learn non-linear decision functions.", "labels": [], "entities": []}, {"text": "We show that the input layer of a deep architecture can be pre-trained through the application of the Nystr\u00f6m low-rank approximation of kernel spaces.", "labels": [], "entities": []}, {"text": "The resulting \"kernelized\" neural network achieves state-of-the-art accuracy in three different tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9980130195617676}]}], "introductionContent": [{"text": "Learning for Natural Language Processing (NLP) requires to more or less explicitly account for trees or graphs to express syntactic and semantic information.", "labels": [], "entities": [{"text": "Learning for Natural Language Processing (NLP)", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7089739814400673}]}, {"text": "A straightforward modeling of such information has been obtained in statistical language learning with Tree Kernels (TKs)), or by means of structured neural models).", "labels": [], "entities": []}, {"text": "In particular, kernel-based methods have been largely applied in language processing for alleviating the need of complex activities of manual feature engineering (e.g.,).", "labels": [], "entities": []}, {"text": "Although ad-hoc features are adopted by many successful approaches to language learning (e.g.,), kernels provide a natural way to capture textual generalizations directly operating over (possibly complex) linguistic structures.", "labels": [], "entities": []}, {"text": "Sequence ( or tree kernels) are of particular interest as the feature space they implicitly generate reflects linguistic patterns.", "labels": [], "entities": []}, {"text": "On the other hand, Recursive Neural Networks () have been shown to learn dense feature representations of the nodes in a structure, thus exploiting similarities between nodes and sub-trees.", "labels": [], "entities": []}, {"text": "Also, Long-Short Term Memory) networks build intermediate representations of sequences, resulting in similarity estimates over sequences and their inner sub-sequences.", "labels": [], "entities": []}, {"text": "While such methods are highly effective and reach state-of-the-art results in many tasks, their adoption can be problematic.", "labels": [], "entities": []}, {"text": "In kernel-based Support Vector Machine (SVM) the classification model corresponds to the set of support vectors (SVs) and weights justifying the maximal margin hyperplane: the classification cost crucially depends on their number, as classifying anew instance requires a kernel computation against all SVs, making their adoption in large data settings prohibitive.", "labels": [], "entities": []}, {"text": "This scalability issue is evident in many NLP and Information Retrieval applications, such as in answer re-ranking in question answering (, where the number of SVs is typically very large.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.792186439037323}, {"text": "answer re-ranking in question answering", "start_pos": 97, "end_pos": 136, "type": "TASK", "confidence": 0.6986273229122162}]}, {"text": "Improving the efficiency of kernel-based methods is a largely studied topic.", "labels": [], "entities": []}, {"text": "The reduction of computational costs has been early designed by imposing a budget;, that is limiting the maximum number of SVs in a model.", "labels": [], "entities": []}, {"text": "However, in complex tasks, such methods still require large budgets to reach adequate accuracies.", "labels": [], "entities": []}, {"text": "On the other hand, training complex neural networks is also difficult as no common design practice is established against complex data structures.", "labels": [], "entities": []}, {"text": "In, a careful analysis of neural word embedding models is carried out and the role of the hyper-parameter estimation is outlined.", "labels": [], "entities": []}, {"text": "Different neural architectures result in the same performances, whenever optimal hyper-parameter tuning is applied.", "labels": [], "entities": []}, {"text": "In this latter case, no significant difference is observed across different architectures, making the choice between different neural architectures a complex and empirical task.", "labels": [], "entities": []}, {"text": "A general approach to the large scale modeling of complex structures is a critical and open problem.", "labels": [], "entities": []}, {"text": "A viable and general solution to this scalability issue is provided by the Nystr\u00f6m method (; it allows to approximate the Gram matrix of a kernel function and support the embedding of future input examples into a low-dimensional space.", "labels": [], "entities": []}, {"text": "For example, if used over TKs, the Nystr\u00f6m projection corresponds to the embedding of any tree into a lowdimensional vector.", "labels": [], "entities": []}, {"text": "In this paper, we show that the Nystr\u00f6m based low-rank embedding of input examples can be used as the early layer of a deep feed-forward neural network.", "labels": [], "entities": []}, {"text": "A standard NN back-propagation training can thus be applied to induce non-linear functions in the kernel space.", "labels": [], "entities": []}, {"text": "The resulting deep architecture, called Kernel-based Deep Architecture (KDA), is a mathematically justified integration of expressive kernel functions and deep neural architectures, with several advantages: it (i) directly operates over complex non-tensor structures, e.g., trees, without any manual feature or architectural engineering, (ii) achieves a drastic reduction of the computational cost w.r.t. pure kernel methods, and (iii) exploits the non-linearity of NNs to produce accurate models.", "labels": [], "entities": []}, {"text": "The experimental evaluation shows that the proposed approach achieves state-of-the-art results in three semantic inference tasks: Semantic Parsing, Question Classification and Community Question Answering.", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 130, "end_pos": 146, "type": "TASK", "confidence": 0.7683037519454956}, {"text": "Question Classification", "start_pos": 148, "end_pos": 171, "type": "TASK", "confidence": 0.8158779442310333}, {"text": "Community Question Answering", "start_pos": 176, "end_pos": 204, "type": "TASK", "confidence": 0.5914330879847208}]}, {"text": "In the rest of the paper, Section 2 surveys some of the investigated kernels.", "labels": [], "entities": []}, {"text": "In Section 3 the Nystr\u00f6m methodology and KDA are presented.", "labels": [], "entities": [{"text": "Nystr\u00f6m methodology", "start_pos": 17, "end_pos": 36, "type": "DATASET", "confidence": 0.7143924832344055}, {"text": "KDA", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.5732887983322144}]}, {"text": "Experimental evaluations are described in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 derives the conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results in terms of Accuracy and saving  in the Question Classification task", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9995385408401489}, {"text": "saving", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.993943989276886}, {"text": "Question Classification", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.7919997572898865}]}, {"text": " Table 2: Results in terms of F 1 and savings in the  Community Question Answering task", "labels": [], "entities": [{"text": "F 1", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9936816692352295}, {"text": "Community Question Answering task", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.6710861027240753}]}, {"text": " Table 3: Results in terms of F 1 and saving in the  Argument Boundary Detection task.", "labels": [], "entities": [{"text": "F 1", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9937878251075745}, {"text": "saving", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9900081157684326}, {"text": "Argument Boundary Detection task", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.7662226855754852}]}]}