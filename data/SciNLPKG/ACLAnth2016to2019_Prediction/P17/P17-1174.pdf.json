{"title": [{"text": "Chunk-based Decoder for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.7757938901583353}]}], "abstractContent": [{"text": "Chunks (or phrases) once played a piv-otal role in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7376238107681274}]}, {"text": "By using a chunk rather than a word as the basic translation unit, local (intra-chunk) and global (inter-chunk) word orders and dependencies can be easily mod-eled.", "labels": [], "entities": []}, {"text": "The chunk structure, despite its importance , has not been considered in the decoders used for neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 95, "end_pos": 127, "type": "TASK", "confidence": 0.8269622226556143}]}, {"text": "In this paper, we propose chunk-based decoders for NMT, each of which consists of a chunk-level decoder and a word-level decoder.", "labels": [], "entities": []}, {"text": "The chunk-level decoder models global dependencies while the word-level decoder decides the local word order in a chunk.", "labels": [], "entities": []}, {"text": "To output a target sentence, the chunk-level decoder generates a chunk representation containing global information, which the word-level decoder then uses as a basis to predict the words inside the chunk.", "labels": [], "entities": []}, {"text": "Experimental results show that our proposed de-coders can significantly improve translation performance in a WAT '16 English-to-Japanese translation task.", "labels": [], "entities": [{"text": "translation", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.9624145030975342}, {"text": "WAT '16 English-to-Japanese translation task", "start_pos": 109, "end_pos": 153, "type": "TASK", "confidence": 0.870683878660202}]}], "introductionContent": [{"text": "Neural machine translation (NMT) performs endto-end translation based on a simple encoderdecoder model) and has now overtaken the classical, complex statistical machine translation (SMT) in terms of performance and simplicity.", "labels": [], "entities": [{"text": "Neural machine translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7455443382263184}, {"text": "endto-end translation", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7250203788280487}, {"text": "statistical machine translation (SMT)", "start_pos": 149, "end_pos": 186, "type": "TASK", "confidence": 0.8148165245850881}]}, {"text": "In NMT, an encoder first maps a source sequence into vector representations and * Contribution during internship at Microsoft Research.", "labels": [], "entities": []}, {"text": "a decoder then maps the vectors into a target sequence ( \u00a7 2).", "labels": [], "entities": []}, {"text": "This simple framework allows researchers to incorporate the structure of the source sentence as in SMT by leveraging various architectures as the encoder).", "labels": [], "entities": [{"text": "SMT", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9658303260803223}]}, {"text": "Most of the NMT models, however, still rely on a sequential decoder based on a recurrent neural network (RNN) due to the difficulty in capturing the structure of a target sentence that is unseen during translation.", "labels": [], "entities": []}, {"text": "With the sequential decoder, however, there are two problems to be solved.", "labels": [], "entities": []}, {"text": "First, it is difficult to model long-distance dependencies (.", "labels": [], "entities": []}, {"text": "A hidden state ht in an RNN is only conditioned by its previous output y t\u22121 , previous hidden state h t\u22121 , and current input x t . This makes it difficult to capture the dependencies between an older output y t\u2212N if they are too far from the current output.", "labels": [], "entities": []}, {"text": "This problem can become more serious when the target sequence becomes longer.", "labels": [], "entities": []}, {"text": "For example, in, when we translate the English sentence into the Japanese one, after the decoder predicts the content word \" (go back)\", it has to predict four function words \" (suffix)\", \" (perfect tense)\", \" (desire)\", and \" (to)\" before predicting the next content word \" (feel)\".", "labels": [], "entities": []}, {"text": "In such a case, the decoder is required to capture the longer dependencies in a target sentence.", "labels": [], "entities": []}, {"text": "Another problem with the sequential decoder is that it is expected to cover multiple possible word orders simply by memorizing the local word se-quences in the limited training data.", "labels": [], "entities": []}, {"text": "This problem can be more serious in free word-order languages such as Czech, German, Japanese, and Turkish.", "labels": [], "entities": []}, {"text": "In the case of the example in, the order of the phrase \" (early)\" and the phrase \" (to home)\" is flexible.", "labels": [], "entities": []}, {"text": "This means that simply memorizing the word order in training data is not enough to train a model that can assign a high probability to a correct sentence regardless of its word order.", "labels": [], "entities": []}, {"text": "In the past, chunks (or phrases) were utilized to handle the above problems in statistical machine translation (SMT) ( and in example-based machine translation (EBMT) (.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 79, "end_pos": 116, "type": "TASK", "confidence": 0.775789866844813}, {"text": "example-based machine translation (EBMT)", "start_pos": 126, "end_pos": 166, "type": "TASK", "confidence": 0.7996735076109568}]}, {"text": "By using a chunk rather than a word as the basic translation unit, one can treat a sentence as a shorter sequence.", "labels": [], "entities": []}, {"text": "This makes it easy to capture the longer dependencies in a target sentence.", "labels": [], "entities": []}, {"text": "The order of words in a chunk is relatively fixed while that in a sentence is much more flexible.", "labels": [], "entities": []}, {"text": "Thus, modeling intra-chunk (local) word orders and inter-chunk (global) dependencies independently can help capture the difference of the flexibility between the word order and the chunk order in free word-order languages.", "labels": [], "entities": []}, {"text": "In this paper, we refine the original RNN decoder to consider chunk information in NMT.", "labels": [], "entities": []}, {"text": "We propose three novel NMT models that capture and utilize the chunk structure in the target language ( \u00a7 3).", "labels": [], "entities": []}, {"text": "Our focus is the hierarchical structure of a sentence: each sentence consists of chunks, and each chunk consists of words.", "labels": [], "entities": []}, {"text": "To encourage an NMT model to capture the hierarchical structure, we start from a hierarchical RNN that consists of a chunk-level decoder and a word-level decoder (Model 1).", "labels": [], "entities": []}, {"text": "Then, we improve the word-level decoder by introducing inter-chunk connections to capture the interaction between chunks (Model 2).", "labels": [], "entities": []}, {"text": "Finally, we introduce a feedback mechanism to the chunk-level decoder to enhance the memory capacity of previous outputs (Model 3).", "labels": [], "entities": []}, {"text": "We evaluate the three models on the WAT '16 English-to-Japanese translation task ( \u00a7 4).", "labels": [], "entities": [{"text": "WAT '16 English-to-Japanese translation task", "start_pos": 36, "end_pos": 80, "type": "TASK", "confidence": 0.8727041681607565}]}, {"text": "The experimental results show that our best model outperforms the best single NMT model reported in WAT '16 ().", "labels": [], "entities": [{"text": "WAT '16", "start_pos": 100, "end_pos": 107, "type": "DATASET", "confidence": 0.6239458123842875}]}, {"text": "Our contributions are twofold: (1) chunk information is introduced into NMT to improve translation performance, and (2) a novel hierarchical decoder is devised to model the properties of chunk structure in the encoder-decoder framework.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of the target language  (Japanese) in extracted corpus after preprocessing.", "labels": [], "entities": []}, {"text": " Table 2: Hyperparameters for training.", "labels": [], "entities": []}, {"text": " Table 3: The settings and results of the baseline systems and our systems. |V src | and |V trg | denote the  vocabulary size of the source language and the target language, respectively. d emb and d hid are the  dimension size of the word embeddings and hidden states, respectively. Only single NMT models (w/o  ensembling) reported in WAT '16 are listed here. Full results are available on the WAT '16 Website. 10", "labels": [], "entities": [{"text": "WAT '16", "start_pos": 337, "end_pos": 344, "type": "TASK", "confidence": 0.5436902244885763}, {"text": "WAT '16 Website", "start_pos": 396, "end_pos": 411, "type": "DATASET", "confidence": 0.7738819271326065}]}, {"text": " Table 4: Chunk-based BLEU and RIBES with the  systems using the word-based encoder.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9769932627677917}, {"text": "RIBES", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9631636142730713}]}]}