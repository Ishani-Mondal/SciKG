{"title": [{"text": "Probabilistic Typology: Deep Generative Models of Vowel Inventories", "labels": [], "entities": []}], "abstractContent": [{"text": "Linguistic typology studies the range of structures present inhuman language.", "labels": [], "entities": []}, {"text": "The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent.", "labels": [], "entities": []}, {"text": "For example , all languages have vowels, while most-but not all-languages have an [u] sound.", "labels": [], "entities": []}, {"text": "In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory?", "labels": [], "entities": []}, {"text": "We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches.", "labels": [], "entities": []}, {"text": "We provide a comprehensive suite of experiments on over 200 distinct languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human languages exhibit a wide range of phenomena, within some limits.", "labels": [], "entities": []}, {"text": "However, some structures seem to occur or co-occur more frequently than others.", "labels": [], "entities": []}, {"text": "Linguistic typology attempts to describe the range of natural variation and seeks to organize and quantify linguistic universals, such as patterns of co-occurrence.", "labels": [], "entities": []}, {"text": "Perhaps one of the simplest typological questions comes from phonology: which vowels tend to occur and co-occur within the phoneme inventories of different languages?", "labels": [], "entities": []}, {"text": "Drawing inspiration from the linguistic literature, we propose models of the probability distribution from which the attested vowel inventories have been drawn.", "labels": [], "entities": []}, {"text": "It is a typological universal that every language contains both vowels and consonants.", "labels": [], "entities": []}, {"text": "But which vowels a language contains is guided by softer constraints, in that certain configurations are more widely attested than others.", "labels": [], "entities": []}, {"text": "For instance, in atypical phoneme inventory, there tend to be far fewer vowels than consonants.", "labels": [], "entities": []}, {"text": "Likewise, all languages contrast vowels based on height, although which contrast is made is language-dependent (.", "labels": [], "entities": []}, {"text": "Moreover, while over 600 unique vowel: The transformed vowel space that is constructed within one of our deep generative models (see \u00a77.1).", "labels": [], "entities": []}, {"text": "A deep network nonlinearly maps the blue grid (\"formant space\") to the red grid (\"metric space\"), with individual vowels mapped from blue to red position as shown.", "labels": [], "entities": []}, {"text": "Vowel pairs such as- that are brought close together are anti-correlated in the point process.", "labels": [], "entities": []}, {"text": "Other pairs such as [y]- are driven apart.", "labels": [], "entities": []}, {"text": "For purposes of the visualization, we have transformed the red coordinate system to place red vowels near their blue positions-while preserving distances up to a constant factor (a \"Procrustes transformation\").", "labels": [], "entities": []}, {"text": "phonemes have been attested cross-linguistically), certain regions of acoustic space are used much more often than others, e.g., the regions conventionally transcribed as,, and.", "labels": [], "entities": []}, {"text": "Human language also seems to prefer inventories where phonologically distinct vowels are spread out in acoustic space (\"dispersion\") so that they can be easily distinguished by a listener.", "labels": [], "entities": []}, {"text": "We depict the acoustic space for English in.", "labels": [], "entities": []}, {"text": "In this work, we regard the proper goal of linguistic typology as the construction of a universal prior distribution from which linguistic systems are drawn.", "labels": [], "entities": []}, {"text": "For vowel system typology, we propose three formal probability models based on stochastic point processes.", "labels": [], "entities": []}, {"text": "We estimate the parameters of the model on one set of languages and evaluate performance on a held-out set.", "labels": [], "entities": []}, {"text": "We explore three questions: (i) How well do the properties of our proposed probability models lineup experimentally with linguistic theory?", "labels": [], "entities": []}, {"text": "(ii) How well can our models predict held-out vowel systems?", "labels": [], "entities": []}, {"text": "(iii) Do our models benefit from a \"deep\" transformation from formant space to metric space?", "labels": [], "entities": []}], "datasetContent": [{"text": "At this point it is helpful to introduce the empirical dataset we will model.", "labels": [], "entities": []}, {"text": "For each of 223 languages, Becker-Kristal (2010) provides the vowel inventory as a set of IPA symbols, listing the first 5 formants for each vowel (or fewer when not available in the original source).", "labels": [], "entities": []}, {"text": "Some corpus statistics are shown in Figs. 4 and 5.", "labels": [], "entities": [{"text": "Figs.", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.9405964612960815}]}, {"text": "For the present paper, we take V to be the set of all 53 IPA symbols that appear in the corpus.", "labels": [], "entities": []}, {"text": "We treat these IPA labels as meaningful, in that we consider two vowels in different languages to be the same vowel in V if (for example) they are both annotated as.", "labels": [], "entities": []}, {"text": "We characterize that vowel by its average formant vector across all languages in the corpus that contain the vowel: e.g., (F 1 , F 2 , . . .)", "labels": [], "entities": []}, {"text": "= (500, 700, . . .) for.", "labels": [], "entities": []}, {"text": "In future work, we plan to relax this idealization (see footnote 3), allowing us to investigate natural questions such as whether [u] is pronounced higher (smaller F 1 ) in languages that also contain (to achieve better dispersion).", "labels": [], "entities": []}, {"text": "Fundamentally, we are interested in whether our model has abstracted the core principles of what makes a good vowel system.", "labels": [], "entities": []}, {"text": "Our choice of a probabilistic model provides a natural test: how surprised is our model by held-out languages?", "labels": [], "entities": []}, {"text": "In other words, how likely does our model think unobserved, but attested vowel systems are?", "labels": [], "entities": []}, {"text": "While this is a natural evaluation paradigm in NLP, it has not-to the best of our knowledge-been applied to a quantitative investigation of linguistic typology.", "labels": [], "entities": []}, {"text": "As a second evaluation, we introduce a vowel system cloze task that could also be used to evaluate non-probabilistic models.", "labels": [], "entities": []}, {"text": "This task is defined by analogy to the traditional semantic cloze task, where the reader is asked to fill in a missing word in the sentence from the context.", "labels": [], "entities": []}, {"text": "In our vowel system cloze task, we present a learner with a subset of the vowels in a held-out vowel system and ask them to predict the remaining vowels.", "labels": [], "entities": []}, {"text": "Consider, as a concrete example, the general American English vowel system (excluding long vowels) {[i],,,,,,,,}.", "labels": [], "entities": []}, {"text": "One potential cloze task would be to predict,,,,} and the fact that two vowels are missing from the inventory.", "labels": [], "entities": []}, {"text": "Within the cloze task, we report accuracy, i.e., did we guess the missing vowel right?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9930641055107117}]}, {"text": "We consider three versions of the cloze tasks.", "labels": [], "entities": []}, {"text": "First, we predict one missing vowel in a setting where exactly one vowel was deleted.", "labels": [], "entities": []}, {"text": "Second, we predict up to one missing vowel where a vowel may have been deleted.", "labels": [], "entities": []}, {"text": "Third, we predict up to two missing vowels, where one or two vowels maybe deleted.", "labels": [], "entities": []}, {"text": "We evaluate our models using 10-fold crossvalidation over the 223 languages.", "labels": [], "entities": []}, {"text": "We report the mean performance over the 10 folds.", "labels": [], "entities": []}, {"text": "The performance on each fold (\"test\") was obtained by training many models on 8 of the other 9 folds (\"train\"), selecting the model that obtained the best task-specific performance on the remaining fold (\"development\"), and assessing it on the test fold.", "labels": [], "entities": []}, {"text": "Minimization of the parameters is performed with the L-BFGS algorithm (.", "labels": [], "entities": []}, {"text": "As a preprocessing step, the first two formants values F 1 and F 2 are centered around zero and scaled down by a factor of 1000 since the formant values themselves maybe quite large.", "labels": [], "entities": []}, {"text": "Specifically, we use the development fold to select among the following combinations of hyperparameters.", "labels": [], "entities": []}, {"text": "For neural embeddings, we tried r \u2208 {2, 10, 50, 100, 150, 200}.", "labels": [], "entities": []}, {"text": "For prototype embeddings, we took the number of components r \u2208 {20, 30, 40, 50}.", "labels": [], "entities": []}, {"text": "We tried network depths d \u2208 {0, 1, 2, 3}.", "labels": [], "entities": []}, {"text": "We sweep the coefficient for an L 2 regularizer on the neural network parameters.", "labels": [], "entities": []}, {"text": "visualizes the diffeomorphism from formant space to metric space for one of our DPP models (depth d = 3 with r = 20 prototypes).", "labels": [], "entities": []}, {"text": "Similar figures can be generated for all of the interpretable models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cross-entropy in nats (lower is better) and cloze prediction accuracy (higher is better). \"BPP\" is a simple BPP with one  parameter for each of the 53 vowels in V. This model does artificially well by modeling an \"accidental\" feature of our data: it is  able to learn not only which vowels are popular among languages, but also which IPA symbols are popular or conventional among  the descriptive phoneticists who created our dataset (see footnote 6), something that would become irrelevant if we upgraded our  task to predict actual formant vectors rather than IPA symbols (see footnote 3). Our point processes, by contrast, are appropriately  allowed to consider a vowel only through its formant vector. The \"u-\" versions of the models use the uninterpretable neural  embedding of the formant vector into R r : by taking r to be large, they are still able to learn special treatment for each vowel in  V (which is why uBPP performs identically to BPP, before being beaten by uMPP and uDPP). The \"i-\" versions limit themselves  to an interpretable neural embedding into R k , giving a more realistic description that does not perform as well. The \"p-\"versions  lift that R k embedding into R r by measuring similarities to r prototypes; they thereby improve on the corresponding i-versions.  For each result shown, the depth d of our neural network was tuned on a development set (typically d = 2). r was also tuned  when applicable (typically r > 100 dimensions for the u-models and r \u2248 30 prototypes for the p-models).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.8230504989624023}]}]}