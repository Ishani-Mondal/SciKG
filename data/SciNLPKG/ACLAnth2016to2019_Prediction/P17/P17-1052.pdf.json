{"title": [{"text": "Deep Pyramid Convolutional Neural Networks for Text Categorization", "labels": [], "entities": [{"text": "Text Categorization", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7499488294124603}]}], "abstractContent": [{"text": "This paper proposes a low-complexity word-level deep convolutional neural network (CNN) architecture for text catego-rization that can efficiently represent long-range associations in text.", "labels": [], "entities": []}, {"text": "In the literature, several deep and complex neural networks have been proposed for this task, assuming availability of relatively large amounts of training data.", "labels": [], "entities": []}, {"text": "However, the associated computational complexity increases as the networks go deeper, which poses serious challenges in practical applications.", "labels": [], "entities": []}, {"text": "Moreover , it was shown recently that shallow word-level CNNs are more accurate and much faster than the state-of-the-art very deep nets such as character-level CNNs even in the setting of large training data.", "labels": [], "entities": []}, {"text": "Motivated by these findings, we carefully studied deepening of word-level CNNs to capture global representations of text, and found a simple network architecture with which the best accuracy can be obtained by increasing the network depth without increasing computational cost by much.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9976276755332947}]}, {"text": "We call it deep pyramid CNN.", "labels": [], "entities": []}, {"text": "The proposed model with 15 weight layers out-performs the previous best models on six benchmark datasets for sentiment classification and topic categorization.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 109, "end_pos": 133, "type": "TASK", "confidence": 0.969737708568573}, {"text": "topic categorization", "start_pos": 138, "end_pos": 158, "type": "TASK", "confidence": 0.737856924533844}]}], "introductionContent": [{"text": "Text categorization is an important task whose applications include spam detection, sentiment classification, and topic classification.", "labels": [], "entities": [{"text": "Text categorization", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7865194082260132}, {"text": "spam detection", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.9377548098564148}, {"text": "sentiment classification", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.9656617939472198}, {"text": "topic classification", "start_pos": 114, "end_pos": 134, "type": "TASK", "confidence": 0.8395254611968994}]}, {"text": "In recent years, neural networks that can make use of word order have been shown to be effective for text categorization.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.8315471112728119}]}, {"text": "While simple and shallow convolutional neural networks (CNNs)) were proposed for this task earlier, more recently, deep and more complex neural networks have also been studied, assuming availability of relatively large amounts of training data (e.g., one million documents).", "labels": [], "entities": []}, {"text": "Examples are deep character-level CNNs (), a complex combination of CNNs and recurrent neural networks (RNNs) (, and RNNs in a wordsentence hierarchy.", "labels": [], "entities": []}, {"text": "A CNN is a feedforward network with convolution layers interleaved with pooling layers.", "labels": [], "entities": []}, {"text": "Essentially, a convolution layer converts to a vector every small patch of data (either the original data such as text or image or the output of the previous layer) at every location (e.g., 3-word windows around every word), which can be processed in parallel.", "labels": [], "entities": []}, {"text": "By contrast, an RNN has connections that form a cycle.", "labels": [], "entities": []}, {"text": "In its typical application to text, a recurrent unit takes words one by one as well as its own output on the previous word, which is parallel-processing unfriendly.", "labels": [], "entities": []}, {"text": "While both CNNs and RNNs can take advantage of word order, the simple nature and parallel-processing friendliness of CNNs make them attractive particularly when large training data causes computational challenges.", "labels": [], "entities": []}, {"text": "There have been several recent studies of CNN for text categorization in the large training data setting.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8012196719646454}]}, {"text": "For example, in (, very deep 32-layer character-level CNNs were shown to outperform deep 9-layer character-level CNNs of ( ).", "labels": [], "entities": []}, {"text": "However, in, very shallow 1-layer word-level CNNs were shown to be more accurate and much faster than the very deep characterlevel CNNs of (.", "labels": [], "entities": []}, {"text": "Although character-level approaches have merit in not having to deal with millions of distinct words, shallow word-level CNNs turned out to be superior even when used with only a manageable number (30K) of the most frequent words.", "labels": [], "entities": []}, {"text": "This demonstrates the basic fact -knowledge of word leads to a powerful representation.", "labels": [], "entities": []}, {"text": "These results motivate us to pursue an effective and efficient design of deep wordlevel CNNs for text categorization.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7639690339565277}]}, {"text": "Note, however, that it is not as simple as merely replacing characters with words in character-level CNNs; doing so rather degraded accuracy in ( . We carefully studied deepening of word-level CNNs in the large-data setting and found a deep but low-complexity network architecture with which the best accuracy can be obtained by increasing the depth but not the order of computation time -the total computation time is bounded by a constant.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9994148015975952}, {"text": "accuracy", "start_pos": 301, "end_pos": 309, "type": "METRIC", "confidence": 0.997913658618927}]}, {"text": "We call it deep pyramid CNN (DPCNN), as the computation time per layer decreases exponentially in a 'pyramid shape'.", "labels": [], "entities": []}, {"text": "After converting discrete text to continuous representation, the DPCNN architecture simply alternates a convolution block and a downsampling layer over and over 1 , leading to a deep network in which internal data size (as well as per-layer computation) shrinks in a pyramid shape.", "labels": [], "entities": []}, {"text": "The network depth can be treated as a meta-parameter.", "labels": [], "entities": []}, {"text": "The computational complexity of this network is bounded to be no more than twice that of one convolution block.", "labels": [], "entities": []}, {"text": "At the same time, as described later, the 'pyramid' enables efficient discovery of long-range associations in the text (and so more global information), as the network is deepened.", "labels": [], "entities": []}, {"text": "This is why DPCNN can achieve better accuracy than the shallow CNN mentioned above (hereafter ShallowCNN), which can use only short-range associations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.997977077960968}]}, {"text": "Moreover, DPCNN can be regarded as a deep extension of ShallowCNN, which we proposed in) and later tested with large datasets in.", "labels": [], "entities": []}, {"text": "We show that DPCNN with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.9637354612350464}, {"text": "topic classification", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.8178047239780426}]}, {"text": "2 Word-level deep pyramid CNN (DPCNN) for text categorization Overview of DPCNN: DPCNN is illustrated in.", "labels": [], "entities": []}, {"text": "The first layer performs text region embedding, which generalizes commonly used word embedding to the embedding of text regions covering one or more words.", "labels": [], "entities": []}, {"text": "It is followed by stacking of convolution blocks (two convolution layers and a shortcut) interleaved with pooling layers with stride 2 for downsampling.", "labels": [], "entities": []}, {"text": "The final pooling layer aggregates internal data for each document into one vector.", "labels": [], "entities": []}, {"text": "We use max pooling for all pooling layers.", "labels": [], "entities": []}, {"text": "The key features of DPCNN are as follows.", "labels": [], "entities": [{"text": "DPCNN", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.7609873414039612}]}, {"text": "\u2022 Downsampling without increasing the number of feature maps (dimensionality of layer output, 250 in).", "labels": [], "entities": []}, {"text": "Downsampling enables efficient representation of long-range associations (and so more global information) in the text.", "labels": [], "entities": []}, {"text": "By keeping the same number of feature maps, every 2-stride downsampling reduces the per-block computation by half and thus the total computation time is bounded by a constant.", "labels": [], "entities": []}, {"text": "\u2022 Shortcut connections with pre-activation and identity mapping (He et al., 2016) for enabling training of deep networks.", "labels": [], "entities": [{"text": "identity mapping", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.717315062880516}]}, {"text": "\u2022 Text region embedding enhanced with unsupervised embeddings (embeddings trained in an unsupervised manner) (Johnson and Zhang, 2015b) for improving accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9934598207473755}]}], "datasetContent": [{"text": "We report the experiments with DPCNNs in comparison with previous models and alternatives.", "labels": [], "entities": []}, {"text": "The code is publicly available on the internet.", "labels": [], "entities": []}, {"text": "Data and data preprocessing To facilitate comparisons with previous results, we used the eight datasets compiled by , summarized in.", "labels": [], "entities": []}, {"text": "AG and Sogou are news.", "labels": [], "entities": [{"text": "AG", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8769088983535767}, {"text": "Sogou", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.8728650212287903}]}, {"text": "Yahoo consists of questions and answers from the 'Yahoo!", "labels": [], "entities": []}, {"text": "Yelp and Amazon ('Ama') are reviews where '.p' (polarity) in the names indicates that labels are binary (positive/negative), and '.f' (full) indicates that labels are the number of stars.", "labels": [], "entities": [{"text": "Yelp", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9646859169006348}, {"text": "Amazon", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.7388755679130554}]}, {"text": "Sogou is in Romanized Chinese, and the others are in English.", "labels": [], "entities": []}, {"text": "Classes are balanced on all the datasets.", "labels": [], "entities": []}, {"text": "Data preprocessing was done as in.", "labels": [], "entities": [{"text": "Data preprocessing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6693846881389618}]}, {"text": "That is, upper-case letters were converted to lower-case letters.", "labels": [], "entities": []}, {"text": "Unlike, variable-sized documents were handled as variable-sized without any shortening or padding; however, the vocabulary size was limited to 30K words.", "labels": [], "entities": []}, {"text": "For example, as also mentioned in, the complete vocabulary of the Ama.p training set contains 1.3M words.", "labels": [], "entities": [{"text": "Ama.p training set", "start_pos": 66, "end_pos": 84, "type": "DATASET", "confidence": 0.8881205519040426}]}, {"text": "A vocabulary of 30K words is only a small portion of it, but it covers about 98% of the text and produced good accuracy as reported below.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9992772936820984}]}, {"text": "Training protocol We held out 10K documents from the training data for use as a validation set on each dataset, and meta-parameter tuning was done based on the performance on the validation set.", "labels": [], "entities": []}, {"text": "To minimize a log loss with softmax, minibatch SGD with momentum 0.9 was conducted for n epochs (n was fixed to 50 for AG, 30 for Yelp.f/p and Dbpedia, and 15 for the rest) while the learning rate was set to \u03b7 for the first 5 n epochs and then 0.1\u03b7 for the rest . The initial learning rate \u03b7 was considered to be a meta-parameter.", "labels": [], "entities": []}, {"text": "The minibatch size was fixed to 100.", "labels": [], "entities": []}, {"text": "Regularization was done by weight decay with the parameter 0.0001 and by optional dropout (Hinton et al., 2012) with 0.5 applied to the input to the top layer.", "labels": [], "entities": [{"text": "Regularization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8432085514068604}]}, {"text": "In some cases overfitting was observed, and so we performed early stopping, based on the validation performance, after reducing the learning rate to 0.1\u03b7.", "labels": [], "entities": [{"text": "validation", "start_pos": 89, "end_pos": 99, "type": "TASK", "confidence": 0.913564920425415}]}, {"text": "Weights were initialized by the Gaussian distribution with zero mean and standard deviation 0.01.", "labels": [], "entities": []}, {"text": "The discrete input to the region embedding layer was fixed to the bow input, and the region size was chosen from {1,3,5}, while fixing output dimensionality to 250 (same as convolution layers).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data. Note that Yelp.f is a balanced subset of Yelp 2015. The results on these two datasets are  not comparable.", "labels": [], "entities": [{"text": "Yelp.f", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.948138415813446}, {"text": "Yelp 2015", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.9352993667125702}]}, {"text": " Table 2: Error rates (%) on larger datasets in comparison with previous models. The previous results are  roughly sorted in the order of error rates (best to worst). The best results and the second best are shown  in bold and italic, respectively. 'tv' stands for tv-embeddings. 'w2v' stands for word2vec. '(w2v)' in  row 7 indicates that the best results among those with and without word2vec pretraining are shown. Note  that 'best' in rows 4&6-8 indicates that we are giving an 'unfair' advantage to these models by choosing  the best test error rate among a number of variations presented in the respective papers.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9916718602180481}]}, {"text": " Table 3: Error rates (%) on smaller datasets in comparison with previous models. The previous results  are roughly sorted in the order of error rates (best to worst). Notation follows that of Table 2.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9969614148139954}]}, {"text": " Table 4: Error rates (%) of DPCNN variations that differ in use of unsupervised embeddings. The rows  are roughly sorted from best to worst.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9923189282417297}]}]}