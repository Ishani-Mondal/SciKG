{"title": [{"text": "Neural Architectures for Multilingual Semantic Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we address semantic parsing in a multilingual context.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7674910128116608}]}, {"text": "We train one multilingual model that is capable of parsing natural language sentences from multiple different languages into their corresponding formal semantic representations.", "labels": [], "entities": [{"text": "parsing natural language sentences", "start_pos": 51, "end_pos": 85, "type": "TASK", "confidence": 0.8667216449975967}]}, {"text": "We extend an existing sequence-to-tree model to a multi-task learning framework which shares the decoder for generating semantic representations.", "labels": [], "entities": []}, {"text": "We report evaluation results on the multilingual GeoQuery corpus and introduce anew multilingual version of the ATIS corpus.", "labels": [], "entities": [{"text": "GeoQuery corpus", "start_pos": 49, "end_pos": 64, "type": "DATASET", "confidence": 0.8173092305660248}, {"text": "ATIS corpus", "start_pos": 112, "end_pos": 123, "type": "DATASET", "confidence": 0.9650144279003143}]}], "introductionContent": [{"text": "In this work, we address multilingual semantic parsing -the task of mapping natural language sentences coming from multiple different languages into their corresponding formal semantic representations.", "labels": [], "entities": [{"text": "multilingual semantic parsing", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.6433405379454294}]}, {"text": "We consider two multilingual scenarios: 1) the single-source setting, where the input consists of a single sentence in a single language, and 2) the multi-source setting, where the input consists of parallel sentences in multiple languages.", "labels": [], "entities": []}, {"text": "Previous work handled the former by means of monolingual models;, while the latter has only been explored by who ensembled many monolingual models together.", "labels": [], "entities": []}, {"text": "Unfortunately, training a model for each language separately ignores the shared information among the source languages, which maybe potentially beneficial for typologically related languages.", "labels": [], "entities": []}, {"text": "Practically, it is also inconvenient to train, tune, and configure anew model for each language, which can be a laborious process.", "labels": [], "entities": []}, {"text": "In this work, we propose a parsing architecture that accepts as input sentences in several languages.", "labels": [], "entities": []}, {"text": "We extend an existing sequence-totree model) to a multitask learning framework, motivated by its success in other fields, e.g., neural machine translation (MT) (.", "labels": [], "entities": [{"text": "neural machine translation (MT)", "start_pos": 128, "end_pos": 159, "type": "TASK", "confidence": 0.8188575704892477}]}, {"text": "Our model consists of multiple encoders, one for each language, and one decoder that is shared across source languages for generating semantic representations.", "labels": [], "entities": []}, {"text": "In this way, the proposed model potentially benefits from having a generic decoder that works well across languages.", "labels": [], "entities": []}, {"text": "Intuitively, the model encourages each source language encoder to find a common structured representation for the decoder.", "labels": [], "entities": []}, {"text": "We further modify the attention mechanism ( to integrate multisource information, such that it can learn whereto focus during parsing; i.e., which input positions in which languages.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 We investigate semantic parsing in two multilingual scenarios that are relatively unexplored in past research, \u2022 We present novel extensions to the sequenceto-tree architecture that integrates multilingual information for semantic parsing, and \u2022 We release anew ATIS semantic dataset annotated in two new languages.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7369479835033417}, {"text": "semantic parsing", "start_pos": 258, "end_pos": 274, "type": "TASK", "confidence": 0.7312033772468567}, {"text": "ATIS semantic dataset", "start_pos": 298, "end_pos": 319, "type": "DATASET", "confidence": 0.8190558751424154}]}], "datasetContent": [{"text": "We conduct our experiments on two multilingual benchmark datasets, which we describe below.", "labels": [], "entities": []}, {"text": "Both datasets use a meaning representation based on lambda calculus.", "labels": [], "entities": []}, {"text": "The GeoQuery (GEO) dataset is a standard benchmark evaluation for semantic parsing.", "labels": [], "entities": [{"text": "GeoQuery (GEO) dataset", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.6348513126373291}, {"text": "semantic parsing", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.7898532450199127}]}, {"text": "The multilingual version consists of 880 instances of natural language queries related to US geography facts in four languages (English, German, Greek, and Thai) ().", "labels": [], "entities": []}, {"text": "We use the standard split which consists of 600 training examples and 280 test examples.", "labels": [], "entities": []}, {"text": "The ATIS dataset contains natural language queries to a flight database.", "labels": [], "entities": [{"text": "ATIS dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.916071355342865}]}, {"text": "The data is split into 4,434 instances for training, 491 for development, and 448 for evaluation, same as.", "labels": [], "entities": []}, {"text": "The original version only includes English.", "labels": [], "entities": []}, {"text": "In this work, we annotate the corpus in Indonesian and Chinese.", "labels": [], "entities": []}, {"text": "The Chinese corpus was annotated (with segmentations) by hiring professional translation service.", "labels": [], "entities": []}, {"text": "The Indonesian corpus was annotated by a native Indonesian speaker.", "labels": [], "entities": [{"text": "Indonesian corpus", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.7274564653635025}]}, {"text": "We use the same pre-processing as, where entities and numbers are replaced with their type names and unique IDs.", "labels": [], "entities": []}, {"text": "English words are stemmed using NLTK (.", "labels": [], "entities": [{"text": "NLTK", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.5742810964584351}]}, {"text": "Each query is paired with its corresponding semantic representation in lambda calculus (.", "labels": [], "entities": []}, {"text": "In all experiments, following Dong and Lapata (2016), we use a one-layer LSTM with 200-dimensional cells and embeddings.", "labels": [], "entities": []}, {"text": "We use a minibatch size of 20 with RMSProp updates (Tieleman and Hinton, 2012) fora fixed number of epochs, with gradient clipping at 5.", "labels": [], "entities": []}, {"text": "Parameters are uniformly initialized at [-0.08,0.08] and regularized using dropout ().", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9342291355133057}]}, {"text": "See Appendix A for detailed experimental settings.", "labels": [], "entities": []}, {"text": "For each model configuration, all experiments are repeated 3 times with different random seed values, in order to make sure that our findings are reliable.", "labels": [], "entities": []}, {"text": "We found empirically that the random seed may affect SEQ2TREE performance.", "labels": [], "entities": [{"text": "SEQ2TREE", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.6370512247085571}]}, {"text": "This is especially important due to the relatively small dataset.", "labels": [], "entities": []}, {"text": "As previously done in multitask sequence-to-sequence learning (, we report the average performance for the baseline and our model.", "labels": [], "entities": []}, {"text": "The evaluation metric is defined in terms of exact match accuracy with the ground-truth logical forms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.6281740069389343}]}, {"text": "See Appendix B for the accuracy of individual runs.", "labels": [], "entities": [{"text": "Appendix B", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9736756980419159}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9994045495986938}]}, {"text": "compares the performance of the monolingual sequence-to-tree model, SINGLE, and our multilingual model, MULTI, with separate and shared output parameters under the single-source setting as described in Section 3.1.", "labels": [], "entities": []}, {"text": "On average, both variants of the multilingual model outperform the monolingual model by up to 1.34% average accuracy on GEO.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9994741082191467}, {"text": "GEO", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.970014214515686}]}, {"text": "Parameter sharing is shown to be helpful, in particular for GEO.", "labels": [], "entities": [{"text": "Parameter sharing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7662739753723145}, {"text": "GEO", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.807224690914154}]}, {"text": "We observe that the average performance increase on ATIS mainly comes from Chinese and Indonesian.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.5856779217720032}]}, {"text": "We also learn that although including English is often helpful for the other languages, it may affect its individual performance.", "labels": [], "entities": []}, {"text": "multi-source parsing by combining 3 to 4 languages for GEO and 2 to 3 languages for ATIS.", "labels": [], "entities": [{"text": "GEO", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.9746931791305542}, {"text": "ATIS", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.9293504357337952}]}, {"text": "For RANKING, we combine the predictions from each language by selecting the one with the highest probability.", "labels": [], "entities": [{"text": "RANKING", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.7454665303230286}]}, {"text": "Indeed, we observe that system combination at the model level is able to give better performance on average (up to 4.29% on GEO) than doing so at the output level.", "labels": [], "entities": [{"text": "GEO", "start_pos": 124, "end_pos": 127, "type": "DATASET", "confidence": 0.862910807132721}]}, {"text": "Combining at the word level and sentence level shows comparable performance on both datasets.", "labels": [], "entities": []}, {"text": "It can be seen that the benefit is more apparent when we include English in the system combination.", "labels": [], "entities": []}, {"text": "Regarding comparison to previous monolingual works, we want to highlight that there exist two different versions of the GeoQuery dataset annotated with completely different semantic representations: semantic tree and lambda calculus.", "labels": [], "entities": [{"text": "GeoQuery dataset", "start_pos": 120, "end_pos": 136, "type": "DATASET", "confidence": 0.8397759199142456}]}, {"text": "As noted in Section 5 of, results obtained from these two versions are not comparable.", "labels": [], "entities": []}, {"text": "We use lambda calculus same as.", "labels": [], "entities": []}, {"text": "Under the multilingual setting, the closest work is Jie and Lu.", "labels": [], "entities": []}, {"text": "Nonetheless, they used the semantic tree version of GeoQuery.", "labels": [], "entities": [{"text": "GeoQuery", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9254343509674072}]}, {"text": "They eval-  In and 7, we report the accuracy of the 3 runs for each model and dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9995158910751343}]}, {"text": "In both settings, we observe that the best accuracy on both datasets is often achieved by MULTI.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9992037415504456}, {"text": "MULTI", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.5359969735145569}]}, {"text": "This is the same conclusion that we reached when averaging the results overall runs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Single-source parsing results in terms of  average accuracy % over 3 runs. Best results are  in bold.", "labels": [], "entities": [{"text": "Single-source parsing", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.6478574872016907}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.931341826915741}]}, {"text": " Table 2: Multi-source parsing results in terms of  average accuracy % over 3 runs. Best results are  in bold.", "labels": [], "entities": [{"text": "Multi-source parsing", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7020005583763123}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9539012908935547}]}, {"text": " Table 3: Example output from monolingual and multilingual models trained on ATIS.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8506978154182434}]}, {"text": " Table 6: Single-source parsing results showing the accuracy of the 3 runs. Best results are in bold.", "labels": [], "entities": [{"text": "Single-source parsing", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.5666316747665405}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.999471127986908}]}, {"text": " Table 7: Multi-source parsing results showing the accuracy of the 3 runs. Best results are in bold.", "labels": [], "entities": [{"text": "Multi-source parsing", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6108243465423584}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9993797540664673}]}]}