{"title": [{"text": "Attention-over-Attention Neural Networks for Reading Comprehension", "labels": [], "entities": []}], "abstractContent": [{"text": "Cloze-style reading comprehension is a representative problem in mining relationship between document and query.", "labels": [], "entities": []}, {"text": "In this paper, we present a simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task.", "labels": [], "entities": []}, {"text": "The proposed model aims to place another attention mechanism over the document-level attention and induces \"attended attention\" for final answer predictions.", "labels": [], "entities": []}, {"text": "One advantage of our model is that it is simpler than related works while giving excellent performance.", "labels": [], "entities": []}, {"text": "In addition to the primary model, we also propose an N-best re-ranking strategy to double check the validity of the candidates and further improve the performance.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed methods significantly outperform various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test.", "labels": [], "entities": [{"text": "CNN", "start_pos": 156, "end_pos": 159, "type": "DATASET", "confidence": 0.9340996146202087}, {"text": "Children's Book Test", "start_pos": 164, "end_pos": 184, "type": "DATASET", "confidence": 0.8318215757608414}]}], "introductionContent": [{"text": "To read and comprehend the human languages are challenging tasks for the machines, which requires that the understanding of natural languages and the ability to do reasoning over various clues.", "labels": [], "entities": []}, {"text": "Reading comprehension is a general problem in the real world, which aims to read and comprehend a given article or context, and answer the questions based on it.", "labels": [], "entities": []}, {"text": "Recently, the cloze-style reading comprehension problem has become a popular task in the community.", "labels": [], "entities": []}, {"text": "The cloze-style query is a problem that to fill in an appropriate word in the given sentences while taking the context information into account.", "labels": [], "entities": []}, {"text": "To teach the machine to do cloze-style reading comprehensions, large-scale training data is necessary for learning relationships between the given document and query.", "labels": [], "entities": []}, {"text": "To create large-scale training data for neural networks, released the CNN/Daily Mail news dataset, where the document is formed by the news articles and the queries are extracted from the summary of the news.", "labels": [], "entities": [{"text": "CNN/Daily Mail news dataset", "start_pos": 70, "end_pos": 97, "type": "DATASET", "confidence": 0.9366493423779806}]}, {"text": "released the Children's Book Test dataset afterwards, where the training samples are generated from consecutive 20 sentences from books, and the query is formed by 21st sentence.", "labels": [], "entities": [{"text": "Children's Book Test dataset", "start_pos": 13, "end_pos": 41, "type": "DATASET", "confidence": 0.9313529253005981}]}, {"text": "Following these datasets, avast variety of neural network approaches have been proposed (, and most of them stem from the attention-based neural network ( , which has become a stereotype inmost of the NLP tasks and is well-known by its capability of learning the \"importance\" distribution over the inputs.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel neural network architecture, called attention-over-attention model.", "labels": [], "entities": []}, {"text": "As we can understand the meaning literally, our model aims to place another attention mechanism over the existing document-level attention.", "labels": [], "entities": []}, {"text": "Unlike the previous works, that are using heuristic merging functions ( , or setting various pre-defined non-trainable terms, our model could automatically generate an \"attended attention\" over various document-level attentions, and make a mutual look not only from query-to-document but also document-to-query, which will benefit from the interactive information.", "labels": [], "entities": []}, {"text": "To sum up, the main contributions of our work are listed as follows.", "labels": [], "entities": []}, {"text": "the mechanism of nesting another attention over the existing attentions is proposed, i.e. attention-over-attention mechanism.", "labels": [], "entities": []}, {"text": "\u2022 Unlike the previous works on introducing complex architectures or many non-trainable hyper-parameters to the model, our model is much more simple but outperforms various state-of-the-art systems by a large margin.", "labels": [], "entities": []}, {"text": "\u2022 We also propose an N-best re-ranking strategy to re-score the candidates in various aspects and further improve the performance.", "labels": [], "entities": []}, {"text": "The following of the paper will be organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we will give a brief introduction to the cloze-style reading comprehension task as well as related public datasets.", "labels": [], "entities": []}, {"text": "Then the proposed attention-over-attention reader will be presented in detail in Section 3 and N-best reranking strategy in Section 4.", "labels": [], "entities": []}, {"text": "The experimental results and analysis will be given in Section 5 and Section 6.", "labels": [], "entities": []}, {"text": "Related work will be discussed in Section 7.", "labels": [], "entities": []}, {"text": "Finally, we will give a conclusion of this paper and envisions on future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Large-scale training data is essential for training neural networks.", "labels": [], "entities": []}, {"text": "Several public datasets for the cloze-style reading comprehension has been released.", "labels": [], "entities": []}, {"text": "Here, we introduce two representative and widely-used datasets.", "labels": [], "entities": []}, {"text": "\u2022 CNN / Daily Mail have firstly published two datasets: CNN and Daily Mail news data 1 . They construct these datasets with web-crawled CNN and Daily Mail news data.", "labels": [], "entities": [{"text": "CNN / Daily Mail", "start_pos": 2, "end_pos": 18, "type": "DATASET", "confidence": 0.8009508550167084}, {"text": "CNN", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.9576194882392883}, {"text": "Daily Mail news data", "start_pos": 64, "end_pos": 84, "type": "DATASET", "confidence": 0.8617023080587387}, {"text": "Daily Mail news data", "start_pos": 144, "end_pos": 164, "type": "DATASET", "confidence": 0.88238824903965}]}, {"text": "One of the characteristics of these datasets is that the news article is often associated with a summary.", "labels": [], "entities": []}, {"text": "So they first regard the main body of the news article as the Document, and the Query is formed by the summary of the article, where one entity word is replaced by a special placeholder to indicate the missing word.", "labels": [], "entities": []}, {"text": "The replaced entity word will be the Answer of the Query.", "labels": [], "entities": []}, {"text": "Apart from releasing the dataset, they also proposed a methodology that anonymizes the named entity tokens in the data, and these tokens are also re-shuffle in each sample.", "labels": [], "entities": []}, {"text": "The motivation is that the news articles are containing limited named entities, which are usually celebrities, and the world knowledge can be learned from the dataset.", "labels": [], "entities": []}, {"text": "So this methodology aims to exploit general relationships between anonymized named entities within a single document rather than the common knowledge.", "labels": [], "entities": []}, {"text": "The following research on these datasets showed that the entity word anonymization is not as effective as expected ).", "labels": [], "entities": []}, {"text": "\u2022 Children's Book Test There was also a dataset called the Children's Book Test (CBTest) released by, which is built on the children's book story through Project Gutenberg 2 . Different from the CNN/Daily Mail datasets, there is no summary available in the children's book.", "labels": [], "entities": [{"text": "Children's Book Test", "start_pos": 2, "end_pos": 22, "type": "DATASET", "confidence": 0.8566473126411438}, {"text": "Children's Book Test (CBTest)", "start_pos": 59, "end_pos": 88, "type": "DATASET", "confidence": 0.8032954973833901}, {"text": "CNN/Daily Mail datasets", "start_pos": 195, "end_pos": 218, "type": "DATASET", "confidence": 0.7337263643741607}]}, {"text": "So they proposed another way to extract query from the original data.", "labels": [], "entities": []}, {"text": "The document is composed of 20 consecutive sentences in the story, and the 21st sentence is regarded as the query, where one word is blanked with a special placeholder.", "labels": [], "entities": []}, {"text": "In the CBTest datasets, there are four types of sub-datasets available which are classified by the part-of-speech and named entity tag of the answer word, containing Named Entities (NE), Common Nouns (CN), Verbs and Prepositions.", "labels": [], "entities": [{"text": "CBTest datasets", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9063220918178558}]}, {"text": "In their studies, they have found that the answering of verbs and prepositions are relatively less dependent on the content of document, and the humans can even do preposi-   The general settings of our neural network model are listed below in detail.", "labels": [], "entities": []}, {"text": "\u2022  For regularization purpose, we adopted l 2 -regularization to 0.0001 and dropout rate of 0.1 ().", "labels": [], "entities": [{"text": "regularization", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.9794753789901733}, {"text": "dropout rate", "start_pos": 76, "end_pos": 88, "type": "METRIC", "confidence": 0.9655180275440216}]}, {"text": "Also, it should be noted that we do not exploit any pretrained embedding models.", "labels": [], "entities": []}, {"text": "\u2022 Hidden Layer: Internal weights of GRUs are initialized with random orthogonal matrices ().", "labels": [], "entities": []}, {"text": "\u2022 Optimization: We adopted ADAM optimizer for weight updating, with an initial learning rate of 0.001.", "labels": [], "entities": [{"text": "weight updating", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7247188687324524}]}, {"text": "As the GRU units still suffer from the gradient exploding issues, we set the gradient clipping threshold to 5 (.", "labels": [], "entities": [{"text": "GRU", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.9170383214950562}]}, {"text": "We used batched training strategy of 32 samples.", "labels": [], "entities": []}, {"text": "Dimensions of embedding and hidden layer for each task are listed in.", "labels": [], "entities": []}, {"text": "In re-ranking step, we generate 5-best list from the baseline neural network model, as we did not observe a significant variance when changing the N-best list size.", "labels": [], "entities": []}, {"text": "All language model features are trained on the training proportion of each dataset, with 8-gram wordbased setting and Kneser-Ney smoothing trained by SRILM toolkit).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 150, "end_pos": 163, "type": "DATASET", "confidence": 0.845343679189682}]}, {"text": "The results are reported with the best model, which is selected by the performance of validation set.", "labels": [], "entities": []}, {"text": "The ensemble model is made up of four best models, which are trained using different random seed.", "labels": [], "entities": []}, {"text": "Implementation is done with and, and all models are trained on Tesla K40 GPU.: Embedding and hidden layer dimensions for each task.", "labels": [], "entities": [{"text": "Tesla K40 GPU.", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.8224316835403442}]}], "tableCaptions": [{"text": " Table 1: Statistics of cloze-style reading comprehension datasets: CNN news and CBTest NE / CN.", "labels": [], "entities": [{"text": "CNN news", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.9610204100608826}, {"text": "CBTest NE / CN", "start_pos": 81, "end_pos": 95, "type": "DATASET", "confidence": 0.9049244225025177}]}, {"text": " Table 2: Results on the CNN news, CBTest NE and CN datasets. The best baseline results are depicted  in italics, and the overall best results are in bold face.", "labels": [], "entities": [{"text": "CNN news", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.9762825667858124}, {"text": "CBTest NE and CN datasets", "start_pos": 35, "end_pos": 60, "type": "DATASET", "confidence": 0.8040374398231507}]}, {"text": " Table 3: Embedding and hidden layer dimensions  for each task.", "labels": [], "entities": []}, {"text": " Table 4: Detailed results of 5-best re-ranking on  CBTest NE/CN datasets. Each row includes all  of the features from previous rows. LM global de- notes the global LM, LM local denotes the local  LM, LM wc denotes the word-class LM.", "labels": [], "entities": [{"text": "CBTest NE/CN datasets", "start_pos": 52, "end_pos": 73, "type": "DATASET", "confidence": 0.9061215877532959}]}, {"text": " Table 5: Weight of each feature in N-best re- ranking step. NN denotes the feature (probability)  produced by baseline neural network model.", "labels": [], "entities": []}]}