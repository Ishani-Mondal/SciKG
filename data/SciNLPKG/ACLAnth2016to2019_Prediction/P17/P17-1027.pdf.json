{"title": [{"text": "A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing", "labels": [], "entities": [{"text": "Unrestricted Non-Projective Parsing", "start_pos": 43, "end_pos": 78, "type": "TASK", "confidence": 0.6057396531105042}]}], "abstractContent": [{"text": "Restricted non-monotonicity has been shown beneficial for the projective arc-eager dependency parser in previous research , as posterior decisions can repair mistakes made in previous states due to the lack of information.", "labels": [], "entities": [{"text": "projective arc-eager dependency parser", "start_pos": 62, "end_pos": 100, "type": "TASK", "confidence": 0.6238252371549606}]}, {"text": "In this paper, we propose a novel, fully non-monotonic transition system based on the non-projective Covington algorithm.", "labels": [], "entities": []}, {"text": "As a non-monotonic system requires exploration of erroneous actions during the training process, we develop several non-monotonic variants of the recently defined dynamic oracle for the Covington parser, based on tight approximations of the loss.", "labels": [], "entities": []}, {"text": "Experiments on data-sets from the CoNLL-X and CoNLL-XI shared tasks show that a non-monotonic dynamic oracle outperforms the mono-tonic version in the majority of languages.", "labels": [], "entities": [{"text": "CoNLL-X", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.8916658759117126}]}], "introductionContent": [{"text": "Greedy transition-based dependency parsers are widely used in different NLP tasks due to their speed and efficiency.", "labels": [], "entities": []}, {"text": "They parse a sentence from left to right by greedily choosing the highestscoring transition to go from the current parser configuration or state to the next.", "labels": [], "entities": [{"text": "parse a sentence from left to right", "start_pos": 5, "end_pos": 40, "type": "TASK", "confidence": 0.8326486945152283}]}, {"text": "The resulting sequence of transitions incrementally builds a parse for the input sentence.", "labels": [], "entities": []}, {"text": "The scoring of the transitions is provided by a statistical model, previously trained to approximate an oracle, a function that selects the needed transitions to parse a gold tree.", "labels": [], "entities": []}, {"text": "Unfortunately, the greedy nature that grants these parsers their efficiency also represents their main limitation.", "labels": [], "entities": []}, {"text": "show that greedy transition-based parsers lose accuracy to error propagation: a transition erroneously chosen by the greedy parser can place it in an incorrect and unknown configuration, causing more mistakes in the rest of the transition sequence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9976370334625244}]}, {"text": "Training with a dynamic oracle) improves robustness in these situations, but in a monotonic transition system, erroneous decisions made in the past are permanent, even when the availability of further information in later states might be useful to correct them.", "labels": [], "entities": []}, {"text": "show that allowing some degree of non-monotonicity, by using a limited set of non-monotonic actions that can repair past mistakes and replace previously-built arcs, can increase the accuracy of a transition-based parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9990469813346863}]}, {"text": "In particular, they present a modified arc-eager transition system where the Left-Arc and Reduce transitions are non-monotonic: the former is used to repair invalid attachments made in previous states by replacing them with a leftward arc, and the latter allows the parser to link two words with a rightward arc that were previously left unattached due to an erroneous decision.", "labels": [], "entities": []}, {"text": "Since the Right-Arc transition is still monotonic and leftward arcs can never be repaired because their dependent is removed from the stack by the arc-eager parser and rendered inaccessible, this approach can only repair certain kinds of mistakes: namely, it can fix erroneous rightward arcs by replacing them with a leftward arc, and connect a limited set of unattached words with rightward arcs.", "labels": [], "entities": []}, {"text": "In addition, they argue that non-monotonicity in the training oracle can be harmful for the final accuracy and, therefore, they suggest to apply it only as a fallback component fora monotonic oracle, which is given priority over the non-monotonic one.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9964058995246887}]}, {"text": "Thus, this strategy will follow the path dictated by the monotonic oracle the majority of the time.", "labels": [], "entities": []}, {"text": "present an extension of this transition system with an Unshift transition allowing it some extra flexibility to correct past errors.", "labels": [], "entities": []}, {"text": "However, the restriction that only rightward arcs can be deleted, and only by replacing them with leftward arcs, is still in place.", "labels": [], "entities": []}, {"text": "Furthermore, both versions of the algorithm are limited to projective trees.", "labels": [], "entities": []}, {"text": "In this paper, we propose a non-monotonic transition system based on the non-projective Covington parser, together with a dynamic oracle to train it with erroneous examples that will need to be repaired.", "labels": [], "entities": []}, {"text": "Unlike the system developed in, we work with full non-monotonicity.", "labels": [], "entities": []}, {"text": "This has a twofold meaning: (1) our approach can repair previous erroneous attachments regardless of their original direction, and it can replace them either with a rightward or leftward arc as both arc transitions are non-monotonic; 1 and (2) we use exclusively a non-monotonic oracle, without the interferences of monotonic decisions.", "labels": [], "entities": []}, {"text": "These modifications are feasible because the non-projective Covington transition system is less rigid than the arc-eager algorithm, as words are never deleted from the parser's data structures and can always be revisited, making it a better option to exploit the full potencial of non-monotonicity.", "labels": [], "entities": []}, {"text": "To our knowledge, the presented system is the first nonmonotonic parser that can produce non-projective dependency analyses.", "labels": [], "entities": []}, {"text": "Another novel aspect is that our dynamic oracle is approximate, i.e., based on efficiently-computable approximations of the loss due to the complexity of calculating its actual value in a non-monotonic and non-projective scenario.", "labels": [], "entities": []}, {"text": "However, this is not a problem in practice: experimental results show how our parser and oracle can use non-monotonic actions to repair erroneous attachments, outperforming the monotonic version developed by in a large majority of the datasets tested.", "labels": [], "entities": []}], "datasetContent": [{"text": "To determine how close the lower bound |U(c, t G )| and the upper bounds |U(c, t G )| + n pc (A\u222aI(c, t G )) and |U(c, t G )|+n c (A\u222aI(c, t G )) are to the actual loss in practical scenarios, we use exhaustive search to calculate the real loss of a given configuration, to then compare it with the bounds.", "labels": [], "entities": []}, {"text": "This is feasible because the lower and upper bounds allow us to prune the search space: if an upper and a lower bound coincide fora configuration we already know the loss and need not keep searching, and if we can branch to two configurations such that the lower bound of one is greater or equal than an upper bound of the other, we can discard the former as it will never lead to smaller loss than the latter.", "labels": [], "entities": []}, {"text": "Therefore, this ex- L 0l p; L 0l l; L0rw; L0rp; L0rl; L0wd; L0pd; L0wvr; L0pvr; L0wv l ; L0pv l ; L0ws l ; L0ps l ; L0wsr; L0psr; L1w; L1p; L1wp; R0w; R0p; R0wp; R 0h w; R 0h p;R 0h l; R 0h2 w; R 0h2 p; R 0l w; R 0l p; R 0l l; R 0l w; R 0l p; R 0l l; R0wd; R0pd; R0wv l ; R0pv l ; R0ws l ; R0ps l ; R1w; R1p; R1wp; R2w; R2p; R2wp; CLw; CLp; CLwp; CRw; CRp; CRwp; Pairs L0wp+R0wp; L0wp+R0w; L0w+R0wp; L0wp+R0p; L0p+R0wp; L0w+R0w; L0p+R0p; R0p+R1p; L0w+R0wd; L0p+R0pd; Triples R0p+R1p+R2p; L0p+R0p+R1p; L 0h p+L0p+R0p; L0p+L 0l p+R0p; L0p+L 0r p+R0p; L0p+R0p+R 0l p; L0p+L 0l p+L 0l p; L0p+L 0r p+L0rp; L0p+L 0h p+L 0h2 p; R0p+R 0l p+R 0l p;: Feature templates.", "labels": [], "entities": []}, {"text": "L 0 and R 0 denote the left and right focus words; L 1 , L 2 , . .", "labels": [], "entities": []}, {"text": ". are the words to the left of L 0 and R 1 , R 2 , . .", "labels": [], "entities": []}, {"text": "those to the right of R 0 . X ih means the head of X i , X ih2 the grandparent, X il and X il the farthest and closest left dependents, and X ir and X ir the farthest and closest right dependents, respectively.", "labels": [], "entities": []}, {"text": "CL and CR are the first and last words between L 0 and R 0 whose head is not in the interval [L 0 , R 0 ].", "labels": [], "entities": [{"text": "CR", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.769769012928009}]}, {"text": "Finally, w stands for word form; p for PoS tag; l for dependency label; dis the distance between L 0 and R 0 ; v l , v rare the left/right valencies (number of left/right dependents); and s l , s r the left/right label sets (dependency labels of left/right dependents).", "labels": [], "entities": []}, {"text": "haustive search with pruning guarantees to find the exact loss.", "labels": [], "entities": []}, {"text": "Due to the time complexity of this process, we undertake the analysis of only the first 100,000 transitions on each dataset of the nineteen languages available from CoNLL-X and CoNLL-XI shared tasks (.", "labels": [], "entities": []}, {"text": "In, we present the average values for the lower bound, both upper bounds and the loss, as well as the relative differences from each bound to the real loss.", "labels": [], "entities": []}, {"text": "After those experiments, we conclude that the lower and the closer upper bounds area tight approximation of the loss, with both bounds incurring relative errors below 0.8% in all datasets.", "labels": [], "entities": []}, {"text": "If we compare them, the real loss is closer to the upper bound |U(c, t G )| + n pc) in the majority of datasets (12 out of 18 languages, excluding Japanese where both bounds were exactly equal to the real loss in the whole sample of configurations).", "labels": [], "entities": []}, {"text": "This means that the term n pc) provides a close approximation of the gold arcs missed by the presence of cycles in A.", "labels": [], "entities": []}, {"text": "Regarding the upper bound |U(c, t G )| + n c), it presents a more variable relative error, ranging from 0.1% to 4.0%.", "labels": [], "entities": []}, {"text": "Thus, although we do not know an algorithm to obtain the exact loss which is fast enough to be practical, any of the three studied loss bounds can be used to obtain a feasible approximate dynamic oracle with full non-monotonicity.", "labels": [], "entities": []}, {"text": "To prove the usefulness of our approach, we implement the static, dynamic monotonic and nonmonotonic oracles for the non-projective Covington algorithm and compare their accuracies on nine datasets 4 from the CoNLL-X shared task () and all datasets from the CoNLL-XI shared task ( . For the non-monotonic algorithm, we test the three different loss expressions defined in the previous section.", "labels": [], "entities": [{"text": "CoNLL-XI shared task", "start_pos": 258, "end_pos": 278, "type": "DATASET", "confidence": 0.843001127243042}]}, {"text": "We train an averaged perceptron model for 15 iterations and use the same feature templates for all languages 5 which are listed in detail in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average value of the different bounds and the loss, and of the relative differences from each  bound to the loss, on CoNLL-XI (first block) and CoNLL-X (second block) datasets during 100,000  transitions. For each language, we show in boldface the average value and relative difference of the  bound that is closer to the loss.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9643857479095459}, {"text": "CoNLL-XI", "start_pos": 127, "end_pos": 135, "type": "DATASET", "confidence": 0.9162416458129883}, {"text": "CoNLL-X (second block) datasets", "start_pos": 154, "end_pos": 185, "type": "DATASET", "confidence": 0.7336099942525228}]}, {"text": " Table 4: Comparison of the average Unlabeled  and Labeled Attachment Scores (including punc- tuation) achieved by some widely-used transition- based algorithms with dynamic oracles on nine  CoNLL-X datasets and all CoNLL-XI datatsets,  as well as their average parsing speed (sen- tences per second across all datasets) measured  on a 2.30GHz Intel Xeon processor. The first  block corresponds to monotonic parsers, while the  second gathers non-monotonic parsers.", "labels": [], "entities": [{"text": "CoNLL-X datasets", "start_pos": 191, "end_pos": 207, "type": "DATASET", "confidence": 0.9509632289409637}, {"text": "CoNLL-XI datatsets", "start_pos": 216, "end_pos": 234, "type": "DATASET", "confidence": 0.9584223031997681}]}]}