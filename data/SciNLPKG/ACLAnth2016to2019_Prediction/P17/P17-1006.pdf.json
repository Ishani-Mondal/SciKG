{"title": [{"text": "Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules", "labels": [], "entities": []}], "abstractContent": [{"text": "Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for low-frequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures.", "labels": [], "entities": []}, {"text": "These effects are detrimental for language understanding systems, which may infer that inexpensive is a rephrasing for expensive or may not associate acquire with acquires.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.7488548457622528}]}, {"text": "In this work, we propose a novel morph-fitting procedure which moves past the use of curated semantic lexicons for improving distributional vector spaces.", "labels": [], "entities": []}, {"text": "Instead, our method injects morphological constraints generated using simple language-specific rules, pulling in-flectional forms of the same word close together and pushing derivational antonyms far apart.", "labels": [], "entities": []}, {"text": "In intrinsic evaluation over four languages, we show that our approach: 1) improves low-frequency word estimates; and 2) boosts the semantic quality of the entire word vector collection.", "labels": [], "entities": []}, {"text": "Finally, we show that morph-fitted vectors yield large gains in the downstream task of dialogue state tracking, highlighting the importance of morphology for tackling long-tail phenomena in language understanding tasks.", "labels": [], "entities": [{"text": "dialogue state tracking", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.7605839172999064}, {"text": "language understanding tasks", "start_pos": 190, "end_pos": 218, "type": "TASK", "confidence": 0.8206592003504435}]}], "introductionContent": [{"text": "Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing, machine translation (, and many others ().", "labels": [], "entities": [{"text": "Word representation learning", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8086546659469604}, {"text": "natural language processing (NLP)", "start_pos": 81, "end_pos": 114, "type": "TASK", "confidence": 0.8058718144893646}, {"text": "machine translation", "start_pos": 196, "end_pos": 215, "type": "TASK", "confidence": 0.7922335267066956}]}, {"text": "Most prominent word representation techniques are grounded in the distributional hypothesis, relying on word co-occurrence information in large textual corpora.", "labels": [], "entities": [{"text": "word representation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.740663930773735}]}, {"text": "Morphologically rich languages, in which \"substantial grammatical information.", "labels": [], "entities": []}, {"text": ". is expressed at word level\" (, pose specific challenges for NLP.", "labels": [], "entities": []}, {"text": "This is not always considered when techniques are evaluated on languages such as English or Chinese, which do not have rich morphology.", "labels": [], "entities": []}, {"text": "In the case of distributional vector space models, morphological complexity brings two challenges to the fore: 1.", "labels": [], "entities": []}, {"text": "Estimating Rare Words: A single lemma can have many different surface realisations.", "labels": [], "entities": []}, {"text": "Naively treating each realisation as a separate word leads to sparsity problems and a failure to exploit their shared semantics.", "labels": [], "entities": []}, {"text": "On the other hand, lemmatising the entire corpus can obfuscate the differences that exist between different word forms even though they share some aspects of meaning.", "labels": [], "entities": []}, {"text": "2. Embedded Semantics: Morphology can encode semantic relations such as antonymy (e.g. literate and illiterate, expensive and inexpensive) or (near-)synonymy (north, northern, northerly).", "labels": [], "entities": []}, {"text": "In this work, we tackle the two challenges jointly by introducing a resource-light vector space finetuning procedure termed morph-fitting.", "labels": [], "entities": []}, {"text": "The proposed method does not require curated knowledge bases or gold lexicons.", "labels": [], "entities": []}, {"text": "Instead, it makes use of the observation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the: The nearest neighbours of three example words (expensive, slow and book) in English, German and Italian before (top) and after (bottom) morph-fitting.", "labels": [], "entities": []}, {"text": "proliferation of word forms in morphologically rich languages.", "labels": [], "entities": []}, {"text": "Formalised as an instance of the post-processing semantic specialisation paradigm (, morphfitting is steered by a set of linguistic constraints derived from simple language-specific rules which describe (a subset of) morphological processes in a language.", "labels": [], "entities": []}, {"text": "The constraints emphasise similarity on one side (e.g., by extracting morphological synonyms), and antonymy on the other (by extracting morphological antonyms), see and Tab.", "labels": [], "entities": [{"text": "Tab", "start_pos": 169, "end_pos": 172, "type": "DATASET", "confidence": 0.9249447584152222}]}, {"text": "2. The key idea of the fine-tuning process is to pull synonymous examples described by the constraints closer together in the transformed vector space, while at the same time pushing antonymous examples away from each other.", "labels": [], "entities": []}, {"text": "The explicit post-hoc injection of morphological constraints enables: a) the estimation of more accurate vectors for lowfrequency words which are linked to their highfrequency forms by the constructed constraints; 1 this tackles the data sparsity problem; and b) specialising the distributional space to distinguish between similarity and relatedness, thus supporting language understanding applications such as dialogue state tracking (DST).", "labels": [], "entities": [{"text": "dialogue state tracking (DST)", "start_pos": 412, "end_pos": 441, "type": "TASK", "confidence": 0.7881278196970621}]}, {"text": "As a post-processor, morph-fitting allows the integration of morphological rules with any distributional vector space in any language: it treats an input distributional word vector space as a black box and fine-tunes it so that the transformed space reflects the knowledge coded in the input morphological constraints (e.g., Italian words rispettoso and irrispetosa should be far apart in the trans-1 For instance, the vector for the word katalanischem which occurs only 9 times in the German Wikipedia will be pulled closer to the more reliable vectors for katalanisch and katalanischer, with frequencies of 2097 and 1383 respectively.", "labels": [], "entities": []}, {"text": "2 Representation models that do not distinguish between synonyms and antonyms may have grave implications in downstream language understanding applications such as spoken dialogue systems: a user looking for 'an affordable Chinese restaurant in west Cambridge' does not want a recommendation for 'an expensive Thai place in east Oxford'.", "labels": [], "entities": [{"text": "downstream language understanding", "start_pos": 109, "end_pos": 142, "type": "TASK", "confidence": 0.6252284944057465}]}, {"text": "formed vector space, see).", "labels": [], "entities": []}, {"text": "1 illustrates the effects of morph-fitting by qualitative examples in three languages: the vast majority of nearest neighbours are \"morphological\" synonyms.", "labels": [], "entities": []}, {"text": "We demonstrate the efficacy of morph-fitting in four languages (English, German, Italian, Russian), yielding large and consistent improvements on benchmarking word similarity evaluation sets such as, its multilingual extension (, and SimVerb-3500 ().", "labels": [], "entities": []}, {"text": "The improvements are reported for all four languages, and with a variety of input distributional spaces, verifying the robustness of the approach.", "labels": [], "entities": []}, {"text": "We then show that incorporating morph-fitted vectors into a state-of-the-art neural-network DST model results in improved tracking performance, especially for morphologically rich languages.", "labels": [], "entities": []}, {"text": "We report an improvement of 4% on Italian, and 6% on German when using morph-fitted vectors instead of the distributional ones, setting anew state-of-theart DST performance for the two datasets.", "labels": [], "entities": [{"text": "DST", "start_pos": 157, "end_pos": 160, "type": "TASK", "confidence": 0.8776979446411133}]}], "datasetContent": [{"text": "Evaluation Setup and Datasets The first set of experiments intrinsically evaluates morph-fitted vector spaces on word similarity benchmarks, using Spearman's rank correlation as the evaluation metric.", "labels": [], "entities": []}, {"text": "First, we use the SimLex-999 dataset, as well as SimVerb-3500, a recent EN verb pair similarity dataset providing similarity ratings for 3,500 verb pairs.", "labels": [], "entities": [{"text": "SimLex-999 dataset", "start_pos": 18, "end_pos": 36, "type": "DATASET", "confidence": 0.8958294689655304}, {"text": "EN verb pair similarity dataset", "start_pos": 72, "end_pos": 103, "type": "DATASET", "confidence": 0.5674280226230621}]}, {"text": "7 SimLex-999 was translated to DE, IT, and RU by, and they crowdsourced similarity scores from native speakers.", "labels": [], "entities": [{"text": "DE", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.5053220391273499}, {"text": "RU", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.7770408987998962}, {"text": "similarity", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9256040453910828}]}, {"text": "We use this dataset for our multilingual evaluation.", "labels": [], "entities": []}, {"text": "8 Morph-fitting EN Word Vectors As the first experiment, we morph-fit a wide spectrum of EN distributional vectors induced by various architectures (see Sect. 3).", "labels": [], "entities": []}, {"text": "The results on SimLex and SimVerb are summarised in Tab.", "labels": [], "entities": [{"text": "SimVerb", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9117898344993591}, {"text": "Tab.", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.9742743968963623}]}, {"text": "4. The results with EN SGNS-LARGE vectors are shown in.", "labels": [], "entities": []}, {"text": "Morphfitted vectors bring consistent improvement across all experiments, regardless of the quality of the initial distributional space.", "labels": [], "entities": []}, {"text": "This finding confirms that the method is robust: its effectiveness does not depend on the architecture used to construct the initial space.", "labels": [], "entities": []}, {"text": "To illustrate the improvements, note that the best score on SimVerb fora model trained on running text is achieved by Context2vec (\u03c1 = 0.388); injecting morphological constraints into this vector space results in again of 7.1 \u03c1 points.", "labels": [], "entities": []}, {"text": "Experiments on Other Languages We next extend our experiments to other languages, testing both morph-fitting variants.", "labels": [], "entities": []}, {"text": "The results are summarised in Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9645473062992096}]}, {"text": "5, while show results for the morph-fitted SGNS-LARGE vectors.", "labels": [], "entities": []}, {"text": "These scores confirm the effectiveness and robustness of morph-fitting across languages, suggesting that the idea of fitting to morphological constraints is indeed language-agnostic, given the set of languagespecific rule-based constraints.", "labels": [], "entities": []}, {"text": "also demon-   The principal metric we use to measure DST performance is the joint goal accuracy, which represents the proportion of test set dialogue turns where all user goals expressed up to that point of the dialogue were decoded correctly ().", "labels": [], "entities": [{"text": "DST", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9862142205238342}, {"text": "joint goal accuracy", "start_pos": 76, "end_pos": 95, "type": "METRIC", "confidence": 0.6815448800722758}]}, {"text": "The NBT models for EN, DE and IT are trained using four variants of the SGNS-LARGE vectors: 1) the initial distributional vectors; 2) morph-fixed vectors; 3) and 4) the two variants of morph-fitted vectors (see Sect. 3).", "labels": [], "entities": []}, {"text": "As shown by Mrk\u0161i\u00b4c, semantic specialisation of the employed word vectors ben-  efits DST performance across all three languages.", "labels": [], "entities": [{"text": "DST", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9749313592910767}]}, {"text": "However, large gains on SimLex-999 do not always induce correspondingly large gains in downstream performance.", "labels": [], "entities": [{"text": "SimLex-999", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.8956702947616577}]}, {"text": "In our experiments, we investigate the extent to which morph-fitting improves DST performance, and whether these gains exhibit stronger correlation with intrinsic performance.", "labels": [], "entities": [{"text": "DST", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9827879071235657}]}], "tableCaptions": [{"text": " Table 3: Vocabulary sizes and counts of ATTRACT  (A) and REPEL (R) constraints.", "labels": [], "entities": [{"text": "ATTRACT  (A)", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.8399267345666885}, {"text": "REPEL (R)", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.8697601407766342}]}, {"text": " Table 4: The impact of morph-fitting (MFIT-AR  used) on a representative set of EN vector space  models. All results show the Spearman's \u03c1 corre- lation before and after morph-fitting. The numbers  in parentheses refer to the vector dimensionality.", "labels": [], "entities": [{"text": "MFIT-AR", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.6228742003440857}, {"text": "Spearman's \u03c1 corre- lation", "start_pos": 127, "end_pos": 153, "type": "METRIC", "confidence": 0.7275046904881796}]}]}