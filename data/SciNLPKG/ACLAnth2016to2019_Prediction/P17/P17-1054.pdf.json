{"title": [], "abstractContent": [{"text": "Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content.", "labels": [], "entities": [{"text": "understanding, organizing and retrieving text content", "start_pos": 81, "end_pos": 134, "type": "TASK", "confidence": 0.7307466609137399}]}, {"text": "Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7207389771938324}]}, {"text": "These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text.", "labels": [], "entities": []}, {"text": "We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks.", "labels": [], "entities": [{"text": "keyphrase prediction", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.8636499047279358}]}, {"text": "We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method.", "labels": [], "entities": [{"text": "deep keyphrase generation", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.6798030932744344}]}, {"text": "Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text.", "labels": [], "entities": []}, {"text": "Code and dataset are available at https://github.com/memray/seq2seq-keyphrase.", "labels": [], "entities": []}], "introductionContent": [{"text": "A keyphrase or keyword is apiece of short, summative content that expresses the main semantic meaning of a longer text.", "labels": [], "entities": [{"text": "A keyphrase or keyword is apiece of short, summative content that expresses the main semantic meaning of a longer text", "start_pos": 0, "end_pos": 118, "type": "Description", "confidence": 0.7584223264739627}]}, {"text": "The typical use of a keyphrase or keyword is in scientific publications to provide the core information of a paper.", "labels": [], "entities": []}, {"text": "We use * Corresponding author the term \"keyphrase\" interchangeably with \"keyword\" in the rest of this paper, as both terms have an implication that they may contain multiple words.", "labels": [], "entities": []}, {"text": "High-quality keyphrases can facilitate the understanding, organizing, and accessing of document content.", "labels": [], "entities": [{"text": "understanding, organizing, and accessing of document content", "start_pos": 43, "end_pos": 103, "type": "TASK", "confidence": 0.6881881124443479}]}, {"text": "As a result, many studies have focused on ways of automatically extracting keyphrases from textual content ().", "labels": [], "entities": []}, {"text": "Due to public accessibility, many scientific publication datasets are often used as test beds for keyphrase extraction algorithms.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.83542600274086}]}, {"text": "Therefore, this study also focuses on extracting keyphrases from scientific publications.", "labels": [], "entities": [{"text": "extracting keyphrases from scientific publications", "start_pos": 38, "end_pos": 88, "type": "TASK", "confidence": 0.8224022507667541}]}, {"text": "Automatically extracting keyphrases from a document is called keypharase extraction, and it has been widely used in many applications, such as information retrieval), text summarization (), text categorization (), and opinion mining).", "labels": [], "entities": [{"text": "keypharase extraction", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.7225240617990494}, {"text": "information retrieval", "start_pos": 143, "end_pos": 164, "type": "TASK", "confidence": 0.7664145827293396}, {"text": "text summarization", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.7544193863868713}, {"text": "text categorization", "start_pos": 190, "end_pos": 209, "type": "TASK", "confidence": 0.7465185523033142}, {"text": "opinion mining", "start_pos": 218, "end_pos": 232, "type": "TASK", "confidence": 0.8173772990703583}]}, {"text": "Most of the existing keyphrase extraction algorithms have addressed this problem through two steps (.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.8165646493434906}]}, {"text": "The first step is to acquire a list of keyphrase candidates.", "labels": [], "entities": []}, {"text": "Researchers have tried to use n-grams or noun phrases with certain part-of-speech patterns for identifying potential candidates.", "labels": [], "entities": []}, {"text": "The second step is to rank candidates on their importance to the document, either through supervised or unsupervised machine learning methods with a set of manually-defined features).", "labels": [], "entities": []}, {"text": "There are two major drawbacks in the above keyphrase extraction approaches.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7393621653318405}]}, {"text": "First, these methods can only extract the keyphrases that ap-pear in the source text; they fail at predicting meaningful keyphrases with a slightly different sequential order or those that use synonyms.", "labels": [], "entities": []}, {"text": "However, authors of scientific publications commonly assign keyphrases based on their semantic meaning, instead of following the written content in the publication.", "labels": [], "entities": []}, {"text": "In this paper, we denote phrases that do not match any contiguous subsequence of source text as absent keyphrases, and the ones that fully match apart of the text as present keyphrases.", "labels": [], "entities": []}, {"text": "shows the proportion of present and absent keyphrases from the document abstract in four commonly-used datasets, from which we can observe large portions of absent keyphrases in all the datasets.", "labels": [], "entities": []}, {"text": "The absent keyphrases cannot be extracted through previous approaches, which further prompts the development of a more powerful keyphrase prediction model.", "labels": [], "entities": [{"text": "keyphrase prediction", "start_pos": 128, "end_pos": 148, "type": "TASK", "confidence": 0.7906386852264404}]}, {"text": "Second, when ranking phrase candidates, previous approaches often adopted machine learning features such as TF-IDF and PageRank.", "labels": [], "entities": [{"text": "TF-IDF", "start_pos": 108, "end_pos": 114, "type": "DATASET", "confidence": 0.7723296880722046}]}, {"text": "However, these features only target to detect the importance of each word in the document based on the statistics of word occurrence and co-occurrence, and are unable to reveal the full semantics that underlie the document content.", "labels": [], "entities": []}, {"text": "To overcome the limitations of previous studies, we re-examine the process of keyphrase prediction with a focus on how real human annotators would assign keyphrases.", "labels": [], "entities": [{"text": "keyphrase prediction", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.8290900886058807}]}, {"text": "Given a document, human annotators will first read the text to get a basic understanding of the content, then they try to digest its essential content and summarize it into keyphrases.", "labels": [], "entities": []}, {"text": "Their generation of keyphrases relies on an understanding of the content, which may not necessarily use the exact words that occur in the source text.", "labels": [], "entities": []}, {"text": "For example, when human annotators see \"Latent Dirichlet Allocation\" in the text, they might write down \"topic modeling\" and/or \"text mining\" as possible keyphrases.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 105, "end_pos": 119, "type": "TASK", "confidence": 0.7552674114704132}, {"text": "text mining", "start_pos": 129, "end_pos": 140, "type": "TASK", "confidence": 0.6990924328565598}]}, {"text": "In addition to the semantic understanding, human annotators might also go back and pickup the most important parts, based on syntactic features.", "labels": [], "entities": []}, {"text": "For example, the phrases following \"we propose/apply/use\" could be important in the text.", "labels": [], "entities": []}, {"text": "As a result, a better keyphrase prediction model should understand the semantic meaning of the content, as well as capture the contextual features.", "labels": [], "entities": [{"text": "keyphrase prediction", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.8296369016170502}]}, {"text": "To effectively capture both the semantic and syntactic features, we use recurrent neural networks (RNN) () to compress the semantic information in the given text into a dense vector (i.e., semantic understanding).", "labels": [], "entities": []}, {"text": "Furthermore, we incorporate a copying mechanism () to allow our model to find important parts based on positional information.", "labels": [], "entities": []}, {"text": "Thus, our model can generate keyphrases based on an understanding of the text, regardless of the presence or absence of keyphrases in the text; at the same time, it does not lose important in-text information.", "labels": [], "entities": []}, {"text": "The contribution of this paper is three-fold.", "labels": [], "entities": []}, {"text": "First, we propose to apply an RNN-based generative model to keyphrase prediction, as well as incorporate a copying mechanism in RNN, which enables the model to successfully predict phrases that rarely occur.", "labels": [], "entities": [{"text": "keyphrase prediction", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.845271646976471}]}, {"text": "Second, this is the first work that concerns the problem of absent keyphrase prediction for scientific publications, and our model recalls up to 20% of absent keyphrases.", "labels": [], "entities": [{"text": "absent keyphrase prediction", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.619390994310379}]}, {"text": "Third, we conducted a comprehensive comparison against six important baselines on abroad range of datasets, and the results show that our proposed model significantly outperforms existing supervised and unsupervised extraction methods.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we first review the related work in Section 2.", "labels": [], "entities": []}, {"text": "Then, we elaborate upon the proposed model in Section 3.", "labels": [], "entities": []}, {"text": "After that, we present the experiment setting in Section 4 and results in Section 5, followed by our discussion in Section 6.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section begins by discussing how we designed our evaluation experiments, followed by the description of training and testing datasets.", "labels": [], "entities": []}, {"text": "Then, we introduce our evaluation metrics and baselines.", "labels": [], "entities": []}, {"text": "There are several publicly-available datasets for evaluating keyphrase generation.", "labels": [], "entities": [{"text": "keyphrase generation", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.8849450647830963}]}, {"text": "The largest one came from, which contains 2,304 scientific publications.", "labels": [], "entities": []}, {"text": "However, this amount of data is unable to train a robust recurrent neural network model.", "labels": [], "entities": []}, {"text": "In fact, there are millions of scientific papers available online, each of which contains the keyphrases that were assigned by their authors.", "labels": [], "entities": []}, {"text": "Therefore, we collected a large amount of high-quality scientific metadata in the computer science domain from various online digital libraries, including ACM Digital Library, ScienceDirect, Wiley, and Web of Science etc.", "labels": [], "entities": [{"text": "ACM Digital Library", "start_pos": 155, "end_pos": 174, "type": "DATASET", "confidence": 0.9462770024935404}, {"text": "Wiley, and Web of Science", "start_pos": 191, "end_pos": 216, "type": "DATASET", "confidence": 0.8733818928400675}]}, {"text": "(. In total, we obtained a dataset of 567,830 articles, after removing duplicates and overlaps with testing datasets, which is 200 times larger than the one of.", "labels": [], "entities": []}, {"text": "Note that our model is only trained on 527,830 articles, since 40,000 publications are randomly held out, among which 20,000 articles were used for building anew test dataset KP20k.", "labels": [], "entities": [{"text": "KP20k", "start_pos": 175, "end_pos": 180, "type": "DATASET", "confidence": 0.7400199770927429}]}, {"text": "Another 20,000 articles served as the validation dataset to check the convergence of our model, as well as the training dataset for supervised baselines.", "labels": [], "entities": []}, {"text": "For evaluating the proposed model more comprehensively, four widely-adopted scientific publication datasets were used.", "labels": [], "entities": []}, {"text": "In addition, since these datasets only contain a few hundred or a few thousand publications, we contribute anew testing dataset KP20k with a much larger number of scientific articles.", "labels": [], "entities": [{"text": "KP20k", "start_pos": 128, "end_pos": 133, "type": "DATASET", "confidence": 0.652869701385498}]}, {"text": "We take the title and abstract as the source text.", "labels": [], "entities": []}, {"text": "Each dataset is described in detail below.", "labels": [], "entities": []}, {"text": "-Inspec (Hulth, 2003): This dataset provides 2,000 paper abstracts.", "labels": [], "entities": [{"text": "Inspec (Hulth, 2003)", "start_pos": 1, "end_pos": 21, "type": "DATASET", "confidence": 0.8693928519884745}]}, {"text": "We adopt the 500 testing papers and their corresponding uncontrolled keyphrases for evaluation, and the remaining 1,500 papers are used for training the supervised baseline models.", "labels": [], "entities": []}, {"text": "-Krapivin (): This dataset provides 2,304 papers with full-text and author-assigned keyphrases.", "labels": [], "entities": []}, {"text": "However, the author did not mention how to split testing data, so we selected the first 400 papers in alphabetical order as the testing data, and the remaining papers are used to train the supervised baselines.", "labels": [], "entities": []}, {"text": "-NUS (Nguyen and Kan, 2007): We use the author-assigned keyphrases and treat all 211 papers as the testing data.", "labels": [], "entities": [{"text": "NUS", "start_pos": 1, "end_pos": 4, "type": "DATASET", "confidence": 0.9497988224029541}]}, {"text": "Since the NUS dataset did not specifically mention the ways of splitting training and testing data, the results of the supervised baseline models are obtained through a five-fold cross-validation.", "labels": [], "entities": [{"text": "NUS dataset", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.9821447134017944}]}, {"text": "-: 288 articles were collected from the ACM Digital Library.", "labels": [], "entities": [{"text": "ACM Digital Library", "start_pos": 40, "end_pos": 59, "type": "DATASET", "confidence": 0.9605992237726847}]}, {"text": "100 articles were used for testing and the rest were used for training supervised baselines.", "labels": [], "entities": []}, {"text": "-KP20k: We built anew testing dataset that contains the titles, abstracts, and keyphrases of 20,000 scientific articles in computer science.", "labels": [], "entities": [{"text": "KP20k", "start_pos": 1, "end_pos": 6, "type": "DATASET", "confidence": 0.8669427037239075}]}, {"text": "They were randomly selected from our obtained 567,830 articles.", "labels": [], "entities": []}, {"text": "Due to the memory limits of implementation, we were notable to train the supervised baselines on the whole training set.", "labels": [], "entities": []}, {"text": "Thus we take the 20,000 articles in the validation set to train the supervised baselines.", "labels": [], "entities": []}, {"text": "It is worth noting that we also examined their performance by enlarging the training dataset to 50,000 articles, but no significant improvement was observed.", "labels": [], "entities": []}, {"text": "Three evaluation metrics, the macro-averaged precision, recall and F-measure (F 1 ) are employed for measuring the algorithm's performance.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.8938739895820618}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9987457990646362}, {"text": "F-measure (F 1 )", "start_pos": 67, "end_pos": 83, "type": "METRIC", "confidence": 0.9387953877449036}]}, {"text": "Following the standard definition, precision is defined as the number of correctly-predicted keyphrases over the number of all predicted keyphrases, and recall is computed by the number of correctlypredicted keyphrases over the total number of data records.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9993657469749451}, {"text": "recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9993744492530823}]}, {"text": "Note that, when determining the match of two keyphrases, we use Porter Stemmer for preprocessing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Proportion of the present keyphrases and  absent keyphrases in four public datasets", "labels": [], "entities": []}, {"text": " Table 2: The performance of predicting present keyphrases of various models on five benchmark datasets", "labels": [], "entities": [{"text": "predicting present keyphrases", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.8980070352554321}]}, {"text": " Table 3: Absent keyphrases prediction perfor- mance of RNN and CopyRNN on five datasets", "labels": [], "entities": [{"text": "Absent", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9903610944747925}]}, {"text": " Table 4: Keyphrase prediction performance of  CopyRNN on DUC-2001. The model is trained  on scientific publication and evaluated on news.", "labels": [], "entities": [{"text": "Keyphrase prediction", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6276447027921677}, {"text": "DUC-2001", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.8159618377685547}]}]}