{"title": [{"text": "CANE: Context-Aware Network Embedding for Relation Modeling", "labels": [], "entities": [{"text": "Relation Modeling", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.9653123319149017}]}], "abstractContent": [{"text": "Network embedding (NE) is playing a critical role in network analysis, due to its ability to represent vertices with efficient low-dimensional embedding vectors.", "labels": [], "entities": [{"text": "Network embedding (NE)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6100420653820038}, {"text": "network analysis", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.731586366891861}]}, {"text": "However, existing NE models aim to learn a fixed context-free embedding for each vertex and neglect the diverse roles when interacting with other vertices.", "labels": [], "entities": []}, {"text": "In this paper, we assume that one vertex usually shows different aspects when interacting with different neighbor vertices, and should own different embeddings respectively.", "labels": [], "entities": []}, {"text": "Therefore, we present Context-Aware Network Embedding (CANE), a novel NE model to address this issue.", "labels": [], "entities": []}, {"text": "CANE learns context-aware embeddings for vertices with mutual attention mechanism and is expected to model the semantic relationships between vertices more precisely.", "labels": [], "entities": []}, {"text": "In experiments, we compare our model with existing NE models on three real-world datasets.", "labels": [], "entities": []}, {"text": "Experimental results show that CANE achieves significant improvement than state-of-the-art methods on link prediction and comparable performance on vertex classification.", "labels": [], "entities": [{"text": "CANE", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.8740952014923096}, {"text": "link prediction", "start_pos": 102, "end_pos": 117, "type": "TASK", "confidence": 0.8274244964122772}, {"text": "vertex classification", "start_pos": 148, "end_pos": 169, "type": "TASK", "confidence": 0.8297633230686188}]}, {"text": "The source code and datasets can be obtained from https://github.com/ thunlp/CANE.", "labels": [], "entities": []}], "introductionContent": [{"text": "Network embedding (NE), i.e., network representation learning (NRL), aims to map vertices of a network into a low-dimensional space according to their structural roles in the network.", "labels": [], "entities": [{"text": "Network embedding (NE)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6373536050319671}, {"text": "network representation learning (NRL)", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.8242313961187998}]}, {"text": "NE provides an efficient and effective way to represent * Indicates equal contribution \u2020 Corresponding Author: Z. Liu (liuzy@tsinghua.edu.cn) and manage large-scale networks, alleviating the computation and sparsity issues of conventional symbol-based representations.", "labels": [], "entities": []}, {"text": "Hence, NE is attracting many research interests in recent years (, and achieves promising performance on many network analysis tasks including link prediction, vertex classification, and community detection.", "labels": [], "entities": [{"text": "NE", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9414196014404297}, {"text": "link prediction", "start_pos": 143, "end_pos": 158, "type": "TASK", "confidence": 0.806330144405365}, {"text": "vertex classification", "start_pos": 160, "end_pos": 181, "type": "TASK", "confidence": 0.8753463327884674}, {"text": "community detection", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.7431060373783112}]}, {"text": "I am studying NLP problems, including syntactic parsing, machine translation and soon.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7265507876873016}, {"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.8390562832355499}]}, {"text": "My research focuses on typical NLP tasks, including word segmentation, tagging and syntactic parsing.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7707465887069702}, {"text": "syntactic parsing", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.698672890663147}]}, {"text": "I am a NLP researcher in machine translation, especially using deep learning models to improve machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.8064236044883728}, {"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7479152083396912}]}, {"text": "In real-world social networks, it is intuitive that one vertex may demonstrate various aspects when interacting with different neighbor vertices.", "labels": [], "entities": []}, {"text": "For example, a researcher usually collaborates with various partners on diverse research topics (as illustrated in), a social-media user contacts with various friends sharing distinct interests, and a web page links to multiple pages for different purposes.", "labels": [], "entities": []}, {"text": "However, most existing NE methods only arrange one single embedding vector to each vertex, and give rise to the following two invertible issues: (1) These methods cannot flexibly cope with the aspect transition of a vertex when interacting with different neighbors.", "labels": [], "entities": []}, {"text": "(2) In these models, a vertex tends to force the embeddings of its neighbors close to each other, which maybe not the case all the time.", "labels": [], "entities": []}, {"text": "For example, the left user and right user in, share less common interests, but are learned to be close to each other since they both link to the middle person.", "labels": [], "entities": []}, {"text": "This will accordingly make vertex embeddings indiscriminative.", "labels": [], "entities": []}, {"text": "To address these issues, we aim to propose a Context-Aware Network Embedding (CANE) framework for modeling relationships between vertices precisely.", "labels": [], "entities": []}, {"text": "More specifically, we present CANE on information networks, where each vertex also contains rich external information such as text, labels or other meta-data, and the significance of context is more critical for NE in this scenario.", "labels": [], "entities": [{"text": "NE", "start_pos": 212, "end_pos": 214, "type": "TASK", "confidence": 0.969794511795044}]}, {"text": "Without loss of generality, we implement CANE on text-based information networks in this paper, which can easily extend to other types of information networks.", "labels": [], "entities": []}, {"text": "In conventional NE models, each vertex is represented as a static embedding vector, denoted as context-free embedding.", "labels": [], "entities": []}, {"text": "On the contrary, CANE assigns dynamic embeddings to a vertex according to different neighbors it interacts with, named as context-aware embedding.", "labels": [], "entities": []}, {"text": "Take a vertex u and its neighbor vertex v for example.", "labels": [], "entities": []}, {"text": "The contextfree embedding of u remains unchanged when interacting with different neighbors.", "labels": [], "entities": []}, {"text": "On the contrary, the context-aware embedding of u is dynamic when confronting different neighbors.", "labels": [], "entities": []}, {"text": "When u interacting with v, their context embeddings concerning each other are derived from their text information, S u and S v respectively.", "labels": [], "entities": []}, {"text": "For each vertex, we can easily use neural models, such as convolutional neural networks and recurrent neural networks (, to build context-free text-based embedding.", "labels": [], "entities": []}, {"text": "In order to realize context-aware textbased embeddings, we introduce the selective attention scheme and build mutual attention between u and v into these neural models.", "labels": [], "entities": []}, {"text": "The mutual attention is expected to guide neural models to emphasize those words that are focused by its neighbor vertices and eventually obtain contextaware embeddings.", "labels": [], "entities": []}, {"text": "Both context-free embeddings and contextaware embeddings of each vertex can be efficiently learned together via concatenation using existing NE methods such as DeepWalk), LINE () and node2vec.", "labels": [], "entities": []}, {"text": "We conduct experiments on three real-world datasets of different areas.", "labels": [], "entities": []}, {"text": "Experimental results on link prediction reveal the effectiveness of our framework as compared to other state-of-the-art methods.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.8793781995773315}]}, {"text": "The results suggest that context-aware embeddings are critical for network analysis, in particular for those tasks concerning about complicated interactions between vertices such as link prediction.", "labels": [], "entities": [{"text": "network analysis", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.7660126984119415}, {"text": "link prediction", "start_pos": 182, "end_pos": 197, "type": "TASK", "confidence": 0.703167662024498}]}, {"text": "We also explore the performance of our framework via vertex classification and case studies, which again confirms the flexibility and superiority of our models.", "labels": [], "entities": [{"text": "vertex classification", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.8397373557090759}]}], "datasetContent": [{"text": "To investigate the effectiveness of CANE on modeling relationships between vertices, we conduct experiments of link prediction on several realworld datasets.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 111, "end_pos": 126, "type": "TASK", "confidence": 0.7407649457454681}]}, {"text": "Besides, we also employ vertex classification to verify whether context-aware embeddings of a vertex can compose a high-quality context-free embedding in return.", "labels": [], "entities": [{"text": "vertex classification", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7527826428413391}]}, {"text": "We select three real-world network datasets as follows:  Cora 1 is atypical paper citation network constructed by).", "labels": [], "entities": []}, {"text": "After filtering out papers without text information, there are 2, 277 machine learning papers in this network, which are divided into 7 categories.", "labels": [], "entities": []}, {"text": "HepTh 2 (High Energy Physics Theory) is another citation network from arXiv 3 released by ().", "labels": [], "entities": [{"text": "HepTh 2 (High Energy Physics Theory)", "start_pos": 0, "end_pos": 36, "type": "DATASET", "confidence": 0.695221833884716}]}, {"text": "We filter out papers without abstract information and retain 1, 038 papers at last.", "labels": [], "entities": []}, {"text": "Zhihu 4 is the largest online Q&A website in China.", "labels": [], "entities": [{"text": "Zhihu 4", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9034122824668884}, {"text": "Q&A website", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.7805628776550293}]}, {"text": "Users follow each other and answer questions on this site.", "labels": [], "entities": []}, {"text": "We randomly crawl 10, 000 active users from Zhihu, and take the descriptions of their concerned topics as text information.", "labels": [], "entities": []}, {"text": "The detailed statistics are listed in.", "labels": [], "entities": []}, {"text": "For link prediction, we adopt a standard evaluation metric AUC, which represents the probability that vertices in a random unobserved link are more similar than those in a random nonexistent link.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8140082657337189}, {"text": "AUC", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.7339106202125549}]}, {"text": "For vertex classification, we employ L2-regularized logistic regression (L2R-LR)) to train classifiers, and evaluate the classification accuracies of various methods.", "labels": [], "entities": [{"text": "vertex classification", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8274936974048615}]}, {"text": "To be fair, we set the embedding dimension to 200 for all methods.", "labels": [], "entities": []}, {"text": "In LINE, we set the number of negative samples to 5; we learn the 100 dimensional first-order and second-order embeddings respectively, and concatenate them to form the 200 dimensional embeddings.", "labels": [], "entities": []}, {"text": "In node2vec, we employ grid search and select the best-performed hyper-parameters for training.", "labels": [], "entities": []}, {"text": "We also apply grid search to set the hyper-parameters \u03b1, \u03b2 and \u03b3 in CANE.", "labels": [], "entities": [{"text": "CANE", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.8445336222648621}]}, {"text": "Besides, we set the number of negative samples k to 1 in CANE to speedup the training process.", "labels": [], "entities": [{"text": "CANE", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.6044343113899231}]}, {"text": "To demonstrate the effectiveness of considering attention mechanism and two types of objectives in Eqs. and, we design three versions of CANE for evaluation, i.e., CANE with text only, CANE without attention and CANE.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of Datasets.", "labels": [], "entities": [{"text": "Statistics of Datasets", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.6161840657393137}]}, {"text": " Table 3: AUC values on HepTh. (\u03b1 = 0.7, \u03b2 = 0.2, \u03b3 = 0.2)", "labels": [], "entities": [{"text": "AUC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9178867936134338}, {"text": "HepTh", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.9255414605140686}]}]}