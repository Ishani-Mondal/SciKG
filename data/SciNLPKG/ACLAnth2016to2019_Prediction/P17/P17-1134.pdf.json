{"title": [{"text": "Unsupervised Text Segmentation Based on Native Language Characteristics", "labels": [], "entities": [{"text": "Unsupervised Text Segmentation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6242929399013519}]}], "abstractContent": [{"text": "Most work on segmenting text does soon the basis of topic changes, but it can be of interest to segment by other, stylistically expressed characteristics such as change of authorship or native language.", "labels": [], "entities": [{"text": "segmenting text", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.8960380256175995}]}, {"text": "We propose a Bayesian unsupervised text seg-mentation approach to the latter.", "labels": [], "entities": []}, {"text": "While baseline models achieve essentially random segmentation on our task, indicating its difficulty, a Bayesian model that incorporates appropriately compact language models and alternating asymmetric priors can achieve scores on the standard metrics around halfway to perfect segmentation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most work on automatically segmenting text has been on the basis of topic: segment boundaries correspond to topic changes.", "labels": [], "entities": [{"text": "automatically segmenting text", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.7536028524239858}]}, {"text": "There are various contexts, however, in which it is of interest to identify changes in other characteristics; for example, there has been work on identifying changes in authorship () and poetic voice (.", "labels": [], "entities": []}, {"text": "In this paper we investigate text segmentation on the basis of change in the native language of the writer.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7357986271381378}]}, {"text": "Two illustrative contexts where this task might be of interest are patchwriting detection and literary analysis.", "labels": [], "entities": [{"text": "patchwriting detection", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.9495863914489746}, {"text": "literary analysis", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.8025826811790466}]}, {"text": "Patchwriting is the heavy use of text from a different source with some modification and insertion of additional words and sentences to form anew text.", "labels": [], "entities": []}, {"text": "notes that this is a kind of textual plagiarism, but is a strategy for learning to write in an appropriate language and style, rather than for deception., and found that non-native speakers, not surprisingly in situations of imperfect mastery of a language, are strongly over-represented in this kind of textual plagiarism.", "labels": [], "entities": []}, {"text": "In these cases the boundaries between the writer's original text and (near-)copied native text are often quite apparent to the reader, as in this short example from (copied text italicised): \"Nevertheless, doubtfulness can be cleared reasonably by the experiments conducted upon the 'split-brain patients', in whom intra-hemispheric communication is no longer possible.", "labels": [], "entities": []}, {"text": "To illustrate, one experiment has the patient sit at a table with a non-transparent screen blocking the objects behind, who is then asked to reach the objects with different hand respectively.\"", "labels": [], "entities": []}, {"text": "Because patchwriting can indicate imperfect comprehension of the source, identifying it and supporting novice writers to improve it has become a focus of programmes like the Citation Project.", "labels": [], "entities": []}, {"text": "For the second, perhaps more speculative context of literary analysis, consider Joseph Conrad, known for having written a number of famous English-language novels, such as Heart of Darkness; he was born in Poland and moved to England at the age of 21.", "labels": [], "entities": []}, {"text": "His writings have been the subject of much manual analysis, with one particular direction of such research being the identification of likely influences on his English writing, including his native Polish language and the French he learnt before English., for instance, notes aspects of his writing that exhibit Polish-like syntax, verb inflection, or other linguistic characteristics (e.g. \"Several had still their staves in their hands\" where the awkwardly placed adverb still is typical of Polish).", "labels": [], "entities": []}, {"text": "These appear both in isolated sentences and in larger chunks of text, and part of an analysis can involve identifying these chunks.", "labels": [], "entities": []}, {"text": "This raises the question: Can NLP and computational models identify the points in a text where native language changes?", "labels": [], "entities": []}, {"text": "Treating this as an unsupervised text segmentation problem, we present the first Bayesian model of text segmentation based on authorial characteristics, applied to native language.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7698146402835846}, {"text": "text segmentation", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.7316928058862686}]}], "datasetContent": [{"text": "We investigate the task of L1-based segmentation in three stages: 1.", "labels": [], "entities": []}, {"text": "Can we define any models that do better than random, in a best case scenario?", "labels": [], "entities": []}, {"text": "For this best case scenario, we determine results over a devset with the best prior found by a grid search, fora single language pair likely to be relatively easily distinguishable.", "labels": [], "entities": []}, {"text": "Note that as this is unsupervised segmentation, it is a devset in the sense that it is used to find the best prior, and also in a sense that some models as described in \u00a74 use information from a related NLI task on the underlying data.", "labels": [], "entities": []}, {"text": "2. If the above is true, do the results also hold for test data, using priors derived from the devset?", "labels": [], "entities": []}, {"text": "3. Further, do the results also hold for all language pairs available in our dataset, not just a single easily distinguishable pair?", "labels": [], "entities": []}, {"text": "We first describe the evaluation data -artificial texts generated from learner essays, similar to the artificially constructed texts of previously described work on Bible authorship and poetry segmentation -and evaluation metrics, followed in \u00a74 by the definitions of our Bayesian models.", "labels": [], "entities": [{"text": "Bible authorship and poetry segmentation", "start_pos": 165, "end_pos": 205, "type": "TASK", "confidence": 0.6796981215476989}]}, {"text": "We use the standard P k () and WindowDiff (WD)) metrics, which (broadly speaking) select sentences using a moving window of size k and determines whether these sentences correctly or incorrectly fall into the same or different reference segmentations.", "labels": [], "entities": []}, {"text": "P k and WD scores range between 0 and 1, with a lower score indicating better performance, and 0 a perfect segmentation.", "labels": [], "entities": [{"text": "WD", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.829235315322876}]}, {"text": "It has been noted that some \"degenerate\" algorithmssuch as placing boundaries randomly or at every possible position -can score 0.).", "labels": [], "entities": []}, {"text": "WD scores are typically similar to P k , correcting for differential penalties between false positive boundaries and false negatives implicit in P k . P k and WD scores reported in \u00a75 are averages across all documents in a dataset.", "labels": [], "entities": [{"text": "WD", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7877202033996582}]}, {"text": "Formal definitions are given in Appendix A.3.", "labels": [], "entities": [{"text": "Appendix A.3", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.7629126012325287}]}, {"text": "Prior: Results on devsets for single L1 pair (German-Italian).: Results on testset L1SEG-CLAWS2-GI-TEST for single L1 pair (German-Italian).", "labels": [], "entities": []}, {"text": "Priors are the ones from the corresponding devsets in Table 1.: Results on dev and test datasets (upper: L1SEG-CLAWS2-ALL-DEV, lower: L1SEG-CLAWS2-ALL-TEST): means and standard deviations (in parentheses) across datasets for all 55 L1 pairs.", "labels": [], "entities": []}, {"text": "Given two segmentations r (reference) and h (hypothesis) fora corpus of N sentences, 9 This is the average number of segments per chapter in the written text used by.", "labels": [], "entities": []}, {"text": "However, we have also successfully replicated our results using s = 7, 9.", "labels": [], "entities": []}, {"text": "Figure 2: A visualization of sentences from a single segment.", "labels": [], "entities": []}, {"text": "Each row represents a sentence and each token is represented by a square.", "labels": [], "entities": []}, {"text": "Token trigrams considered discriminative for either of our two L1 classes are shown in blue or red, with the rest being considered non-discriminative.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on devsets for single L1 pair (German-Italian).", "labels": [], "entities": []}]}