{"title": [{"text": "Automatically Labeled Data Generation for Large Scale Event Extraction", "labels": [], "entities": [{"text": "Automatically Labeled Data Generation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.5289873704314232}, {"text": "Large Scale Event Extraction", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.6640234664082527}]}], "abstractContent": [{"text": "Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7181589901447296}]}, {"text": "However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population.", "labels": [], "entities": []}, {"text": "To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge , which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically.", "labels": [], "entities": [{"text": "data labeling", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.707800418138504}, {"text": "event extraction", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.7693009376525879}]}, {"text": "The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data.", "labels": [], "entities": []}, {"text": "And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Event Extraction (EE), a challenging task in Information Extraction, aims at detecting and typing events (Event Detection), and extracting arguments with different roles (Argument Identification) from natural-language texts.", "labels": [], "entities": [{"text": "Event Extraction (EE)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8444389045238495}, {"text": "Information Extraction", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.7534822523593903}, {"text": "Argument Identification) from natural-language texts", "start_pos": 171, "end_pos": 223, "type": "TASK", "confidence": 0.7851954648892084}]}, {"text": "For example, in the sentence shown in, an EE system is expected to identify an Attack event triggered by threw and extract the corresponding five augments with different roles: Yesterday (Role=Time), demonstrators (Role=Attacker), stones (Role=Instrument), soldiers (Role=Target), and Israeli (Role=Place).", "labels": [], "entities": []}, {"text": "To this end, so far most methods (Nguyen et al.,  2016;) usually adopted supervised learning paradigm which relies on elaborate human-annotated data, such as ACE 2005 1 , to train extractors.", "labels": [], "entities": [{"text": "ACE 2005 1", "start_pos": 158, "end_pos": 168, "type": "DATASET", "confidence": 0.9264894326527914}]}, {"text": "Although this paradigm was widely studied, existing approaches still suffer from high costs for manually labeling training data and low coverage of predefined event types.", "labels": [], "entities": []}, {"text": "In ACE 2005, all 33 event types are manually predefined and the corresponding event information (including triggers, event types, arguments and their roles) are manually annotated only in 599 English documents since the annotation process is extremely expensive.", "labels": [], "entities": [{"text": "ACE 2005", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.928138792514801}]}, {"text": "As shown, nearly 60% of event types in ACE 2005 have less than 100 labeled samples and there are even three event types which have less than ten labeled samples.", "labels": [], "entities": [{"text": "ACE 2005", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.8703430593013763}]}, {"text": "Moreover, those predefined 33 event types are in low coverage for Natural Language Processing (NLP) applications on large-scale data.", "labels": [], "entities": []}, {"text": "Therefore, for extracting large scale events, especially in open domain scenarios, how to automatically and efficiently generate sufficient training data is an important problem.", "labels": [], "entities": [{"text": "extracting large scale events", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.8834387511014938}]}, {"text": "This paper aims to automatically generate training data for EE, which involves labeling triggers, event types, arguments and their roles.", "labels": [], "entities": [{"text": "EE", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9034194946289062}]}, {"text": "shows an example of labeled sentence.", "labels": [], "entities": []}, {"text": "Recent improvements of Distant Supervision (DS) have been proven to be effective to label training data for Relation, which aims to predict semantic re- lations between pairs of entities, formulated as (entity 1 , relation, entity 2 ).", "labels": [], "entities": [{"text": "Distant Supervision (DS)", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.7593926072120667}, {"text": "Relation", "start_pos": 108, "end_pos": 116, "type": "TASK", "confidence": 0.9651079177856445}, {"text": "predict semantic re- lations between pairs of entities", "start_pos": 132, "end_pos": 186, "type": "TASK", "confidence": 0.7635637587971158}]}, {"text": "And DS for RE assumes that if two entities have a relationship in a known knowledge base, then all sentences that mention these two entities will express that relationship in someway ().", "labels": [], "entities": [{"text": "RE", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.8908361792564392}]}, {"text": "However, when we use DS for RE to EE, we meet following challenges: Triggers are not given out in existing knowledge bases.", "labels": [], "entities": [{"text": "EE", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.8659129738807678}]}, {"text": "EE aims to detect an event instance of a specific type and extract their arguments and roles, formulated as (event instance, event type; role 1 , argument 1 ; role 2 , argument 2 ; ...; role n , argument n ), which can be regarded as a kind of multiple or complicated relational data.", "labels": [], "entities": []}, {"text": "In, the right part shows an example of spouse of relation between Barack Obama and Michelle Obama, where two rectangles represent two entities and the edge connecting them represents their relation.", "labels": [], "entities": []}, {"text": "DS for RE uses two entities to automatically label training data; In comparison, the left part in shows a marriage event of Barack Obama and Michelle Obama, where the dash circle represents the marriage event instance of Barack Obama and Michelle Obama, rectangles represent arguments of the event instance, and each edge connecting an argument and the event instance expresses the role of the argument.", "labels": [], "entities": [{"text": "RE", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.8973796367645264}]}, {"text": "For example, Barack Obama plays a Spouse role in this marriage event instance.", "labels": [], "entities": []}, {"text": "It seems that we could use an event instance and an argument to automatically generate training data for argument identification just like DS for RE.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.742625892162323}]}, {"text": "However, an event instance is a virtual node in existing knowledge bases and mentioned implicitly in texts.", "labels": [], "entities": []}, {"text": "For example, in Freebase, the aforementioned marriage event instance is represented as m.02nqglv (see details in Section 2).", "labels": [], "entities": [{"text": "Freebase", "start_pos": 16, "end_pos": 24, "type": "DATASET", "confidence": 0.9316570162773132}]}, {"text": "Thus we cannot directly use an event instance and an argument, like m.02nqglv and Barack Obama, to label back in sentences.", "labels": [], "entities": []}, {"text": "In ACE event extraction program, an event instance is represented as a trigger word, which is the main word that most clearly represents an event occurrence in sentences, like threw in.", "labels": [], "entities": [{"text": "ACE event extraction", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.6996556917826334}]}, {"text": "Following ACE, we can use trigger words to represent event instance, like married for people.marriage event instance.", "labels": [], "entities": [{"text": "ACE", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.8079522848129272}]}, {"text": "Unfortunately, triggers are not given out in existing knowledge bases.", "labels": [], "entities": []}, {"text": "To resolve the trigger missing problem mentioned above, we need to discover trigger words before employing distant supervision to automatically label event arguments.", "labels": [], "entities": []}, {"text": "Following DS in RE, we could naturally assume that a sentence contains all arguments of an event in the knowledge base tend to express that event, and the verbs occur in these sentences tend to evoke this type of events.", "labels": [], "entities": []}, {"text": "However, arguments fora specific event instance are usually mentioned in multiple sentences.", "labels": [], "entities": []}, {"text": "Simply employing all arguments in the knowledge base to label back in sentences will generate few sentences as training samples.", "labels": [], "entities": []}, {"text": "As shown in  To solve above problems, we propose an approach to automatically generate labeled data for large scale EE by jointly using world knowledge (Freebase) and linguistic knowledge (FrameNet).", "labels": [], "entities": []}, {"text": "At first, we put forward an approach to prioritize arguments and select key or representative arguments (see details in Section 3.1) for each event type by using Freebase; Secondly, we merely use key arguments to label events and figure out trigger words; Thirdly, an external linguistic knowledge resource, FrameNet, is employed to filter noisy trigger words and expand more triggers; After that, we propose a Soft Distant Supervision (SDS) for EE to automatically label training data, which assumes that any sentence containing all key arguments in Freebase and a corresponding trigger word is likely to express that event in someway, and arguments occurring in that sentence are likely to play the corresponding roles in that event.", "labels": [], "entities": []}, {"text": "Finally, we evaluate the quality of the automatically labeled training data by both manual and automatic evaluations.", "labels": [], "entities": []}, {"text": "In addition, we employ a CNNbased EE approach with multi-instance learning for the automatically labeled data as a baseline for further research on this data.", "labels": [], "entities": []}, {"text": "In summary, the contributions of this paper are as follows: \u2022 To our knowledge, it is the first work to automatically label data for large scale EE via world knowledge and linguistic knowledge.", "labels": [], "entities": []}, {"text": "All the labeled data in this paper have been released and can be downloaded freely 2 . \u2022 We propose an approach to figure out key arguments of an event by using Freebase, and use them to automatically detect events and corresponding trigger words.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 161, "end_pos": 169, "type": "DATASET", "confidence": 0.9571263194084167}]}, {"text": "Moreover, we employ FrameNet to filter noisy triggers and expand more triggers.", "labels": [], "entities": []}, {"text": "\u2022 The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately humanannotated data.", "labels": [], "entities": []}, {"text": "Also, our automatically labeled data can augment traditional humanannotated data, which could significantly improve the extraction performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first manually evaluate our automatically labeled data.", "labels": [], "entities": []}, {"text": "Then, we conduct automatic evaluations for our labeled data based on ACE corpus and analyze effects of different approaches to automatically label training data.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.9678224623203278}]}, {"text": "Finally, we shows the performance of DMCNNs-MIL on our automatically labeled data.", "labels": [], "entities": []}, {"text": "To prove the effectiveness of the proposed approach automatically, we add automatically generated labeled data into ACE dataset to expand the training sets and see whether the performance of the event extractor trained on such expanded training sets is improved.", "labels": [], "entities": [{"text": "ACE dataset", "start_pos": 116, "end_pos": 127, "type": "DATASET", "confidence": 0.9431571364402771}]}, {"text": "In our automatically labeled data, there are some event types that can correspond to those in ACE dataset.", "labels": [], "entities": [{"text": "ACE dataset", "start_pos": 94, "end_pos": 105, "type": "DATASET", "confidence": 0.9404135942459106}]}, {"text": "For example, our people.marriage events can be mapped to life.marry events in ACE2005 dataset.", "labels": [], "entities": [{"text": "ACE2005 dataset", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.987672358751297}]}, {"text": "We mapped these types of events manually and we add them into ACE training corpus in two ways.", "labels": [], "entities": [{"text": "ACE training corpus", "start_pos": 62, "end_pos": 81, "type": "DATASET", "confidence": 0.9159630735715231}]}, {"text": "(1) we delete the human annotated ACE data for these mapped event types in ACE dataset and add our automatically labeled data to remainder ACE training data.", "labels": [], "entities": [{"text": "ACE dataset", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.9553554058074951}, {"text": "ACE training data", "start_pos": 139, "end_pos": 156, "type": "DATASET", "confidence": 0.7442816694577535}]}, {"text": "We call this Expanded Data (ED) as ED Only.", "labels": [], "entities": []}, {"text": "(2) We directly add our automatically labeled data of mapped event types to ACE training data and we call this training data as ACE+ED.", "labels": [], "entities": [{"text": "ACE training data", "start_pos": 76, "end_pos": 93, "type": "DATASET", "confidence": 0.8691766262054443}, {"text": "ACE+ED", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.6886225839455923}]}, {"text": "Then we use such data to train the same event extraction model (DMCNN) and evaluate them on the ACE testing data set.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.7345182299613953}, {"text": "ACE testing data set", "start_pos": 96, "end_pos": 116, "type": "DATASET", "confidence": 0.9451407492160797}]}, {"text": "Following, we used the same test set with 40 newswire articles and the same development set with 30 documents and the rest 529 documents are used for ACE training set.", "labels": [], "entities": [{"text": "ACE training set", "start_pos": 150, "end_pos": 166, "type": "DATASET", "confidence": 0.817633867263794}]}, {"text": "And we use the same evaluation metric P, R, F as ACE task defined.", "labels": [], "entities": []}, {"text": "We select three baselines trained with ACE data.", "labels": [], "entities": [{"text": "ACE data", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.850718766450882}]}, {"text": "(1) Li's structure, which is the best reported structured-based system (.", "labels": [], "entities": []}, {"text": "Chen's DMCNN, which is the best reported CNN-based system . (3) Nguyen's JRNN, which is the state-ofthe-arts system.", "labels": [], "entities": [{"text": "DMCNN", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.8850986957550049}, {"text": "JRNN", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.8474463820457458}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Compared with all models, DMCNN trained with ACE+ED achieves the highest performance.", "labels": [], "entities": [{"text": "ACE+ED", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.5237290362517039}]}, {"text": "This demonstrates that our automatically generated labeled data could expand human annotated training data effectively.", "labels": [], "entities": []}, {"text": "Moreover, compared with Chen's DM-CNN trained with ACE, DMCNN trained with ED Only achieves a competitive performance.", "labels": [], "entities": [{"text": "ACE", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.9201714992523193}, {"text": "ED", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.7443905472755432}]}, {"text": "This demonstrates that our large scale automatically labeled data is competitive with elaborately humanannotated data.", "labels": [], "entities": []}, {"text": "In the held-out evaluation, we holdout part of the Freebase event data during training, and compare newly discovered event instances against this heldout data.", "labels": [], "entities": [{"text": "Freebase event data", "start_pos": 51, "end_pos": 70, "type": "DATASET", "confidence": 0.9349224170049032}]}, {"text": "We use the following criteria to judge the correctness of each predicted event automatically: (1) An event is correct if its key arguments and event type match those of an event instance in Freebase; (2) An argument is correctly classified if its event type and argument role match those of any of the argument instance in the corresponding Freebase event. and show the precision-recall (P-R) curves for each method in the two stages of event extraction respectively.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 190, "end_pos": 198, "type": "DATASET", "confidence": 0.955024778842926}, {"text": "precision-recall (P-R)", "start_pos": 370, "end_pos": 392, "type": "METRIC", "confidence": 0.9209324866533279}, {"text": "event extraction", "start_pos": 437, "end_pos": 453, "type": "TASK", "confidence": 0.7508823275566101}]}, {"text": "We can see that multi-instance learning is effective to alleviate the noise problem in our distant supervised event extraction.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 110, "end_pos": 126, "type": "TASK", "confidence": 0.7832302153110504}]}, {"text": "Because the incomplete nature of Freebase, heldout evaluation suffers from false negatives problem.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9688414335250854}]}, {"text": "We also perform a manual evaluation to eliminate these problems.", "labels": [], "entities": []}, {"text": "In the manual evaluation, we manually check the newly discovered event instances that are not in Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.9816119074821472}]}, {"text": "Because the number of these event instances in the test data is unknown, we cannot calculate the recall in this case.", "labels": [], "entities": [{"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9994255304336548}]}, {"text": "Instead, we calculate the precision of the top n extracted event instances.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9991137385368347}]}, {"text": "The human evaluation results are presented in: Precision for top 100, 300, and 500 events", "labels": [], "entities": [{"text": "Precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9726930260658264}]}], "tableCaptions": [{"text": " Table 1: Statistics of events in Freebase. EI# de- notes number of event instances in Freebase. A#  denotes number of arguments for each event type- s, and S# indicates number of sentences contain all  arguments of each event type in Wikipedia.", "labels": [], "entities": [{"text": "A", "start_pos": 97, "end_pos": 98, "type": "METRIC", "confidence": 0.9796661734580994}]}, {"text": " Table 2: The statistics of five largest automatically labeled events in selected 21 Freebase events, with  their size of instances in Freebase, sentences labeled with key argument (KA) and KA + Triggers(T),  examples of arguments roles sorted by KR and examples of triggers.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 135, "end_pos": 143, "type": "DATASET", "confidence": 0.9809457659721375}]}, {"text": " Table 4: Overall performance on ACE blind test data", "labels": [], "entities": [{"text": "ACE blind test data", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.6976222917437553}]}, {"text": " Table 5: Effects of ER, RS and KR", "labels": [], "entities": [{"text": "ER", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9911177754402161}, {"text": "RS", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9402493834495544}]}, {"text": " Table 6: Effects of TCF, TETF,TR and FrameNet", "labels": [], "entities": [{"text": "TETF,TR", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.8090771436691284}, {"text": "FrameNet", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.3825503885746002}]}, {"text": " Table 7. We can see that  DMCNNs-MIL achieves the best performance.", "labels": [], "entities": []}, {"text": " Table 7: Precision for top 100, 300, and 500 events", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9863815307617188}]}]}