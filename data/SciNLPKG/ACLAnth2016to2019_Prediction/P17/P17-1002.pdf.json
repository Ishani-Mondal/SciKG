{"title": [{"text": "Neural End-to-End Learning for Computational Argumentation Mining", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate neural techniques for end-to-end computational argumentation mining (AM).", "labels": [], "entities": [{"text": "end-to-end computational argumentation mining (AM)", "start_pos": 37, "end_pos": 87, "type": "TASK", "confidence": 0.759836597102029}]}, {"text": "We frame AM both as a token-based dependency parsing and as a token-based sequence tagging problem, including a multi-task learning setup.", "labels": [], "entities": [{"text": "AM", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9569428563117981}, {"text": "token-based dependency parsing", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.6543168922265371}, {"text": "token-based sequence tagging", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.6517444252967834}]}, {"text": "Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.716425895690918}]}, {"text": "In contrast, less complex (local) tagging models based on BiL-STMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the AM problem.", "labels": [], "entities": [{"text": "AM problem", "start_pos": 177, "end_pos": 187, "type": "TASK", "confidence": 0.8931851983070374}]}, {"text": "Moreover, we find that jointly learning 'natural' subtasks, in a multi-task learning setup, improves performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computational argumentation mining (AM) deals with finding argumentation structures in text.", "labels": [], "entities": [{"text": "Computational argumentation mining (AM)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8648841977119446}]}, {"text": "This involves several subtasks, such as: (a) separating argumentative units from non-argumentative units, also called 'component segmentation'; (b) classifying argument components into classes such as \"Premise\" or \"Claim\"; (c) finding relations between argument components; (d) classifying relations into classes such as \"Support\" or \"Attack\".", "labels": [], "entities": []}, {"text": "Thus, AM would have to detect claims and premises (reasons) in texts such as the following, where premise P supports claim C: Since it killed many marine lives P , While different research has addressed different subsets of the AM problem (see below), the ultimate goal is to solve all of them, starting from unannotated plain text.", "labels": [], "entities": [{"text": "AM", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.8944869041442871}]}, {"text": "Two recent approaches to this end-to-end learning scenario are and.", "labels": [], "entities": []}, {"text": "Both solve the end-to-end task by first training independent models for each subtask and then defining an integer linear programming (ILP) model that encodes global constraints such as that each premise has a parent, etc.", "labels": [], "entities": []}, {"text": "Besides their pipeline architecture the approaches also have in common that they heavily rely on hand-crafted features.", "labels": [], "entities": []}, {"text": "Hand-crafted features pose a problem because AM is to some degree an \"arbitrary\" problem in that the notion of \"argument\" critically relies on the underlying argumentation theory (.", "labels": [], "entities": []}, {"text": "Accordingly, datasets typically differ with respect to their annotation of (often rather complex) argument structure.", "labels": [], "entities": []}, {"text": "Thus, feature sets would have to be manually adapted to and designed for each new sample of data, a challenging task.", "labels": [], "entities": []}, {"text": "The same critique applies to the designing of ILP constraints.", "labels": [], "entities": []}, {"text": "Moreover, from a machine learning perspective, pipeline approaches are problematic because they solve subtasks independently and thus lead to error propagation rather than exploiting interrelationships between variables.", "labels": [], "entities": []}, {"text": "In contrast to this, we investigate neural techniques for end-to-end learning in computational AM, which do not require the hand-crafting of features or constraints.", "labels": [], "entities": []}, {"text": "The models we survey also all capture some notion of \"joint\"-rather than \"pipeline\"-learning.", "labels": [], "entities": []}, {"text": "First, we frame the end-to-end AM problem as a dependency parsing problem.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7530668377876282}]}, {"text": "Dependency parsing maybe considered a natural choice for AM, because argument structures often form trees, or closely resemble them (see \u00a73).", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8358071446418762}, {"text": "AM", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9768799543380737}]}, {"text": "Hence, it is not surprising that 'discourse parsing) has been suggested for AM.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7178544849157333}, {"text": "AM", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.9762372970581055}]}, {"text": "What distinguishes our approach from these previous ones is that we operate on the token level, rather than on the level of components, because we address the end-toend framework and, thus, do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given.", "labels": [], "entities": []}, {"text": "Second, we frame the problem as a sequence tagging problem.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.6955109387636185}]}, {"text": "This is a natural choice especially for component identification (segmentation and classification), which is atypical entity recognition problem for which BIO tagging is a standard approach, pursued in AM, e.g., by.", "labels": [], "entities": [{"text": "component identification (segmentation and classification)", "start_pos": 40, "end_pos": 98, "type": "TASK", "confidence": 0.7659253520624978}, {"text": "entity recognition", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.7296546399593353}, {"text": "BIO tagging", "start_pos": 155, "end_pos": 166, "type": "TASK", "confidence": 0.6868497580289841}]}, {"text": "The challenge in the end-to-end setting is to also include relations into the tagging scheme, which we realize by coding the distances between linked components into the tag label.", "labels": [], "entities": []}, {"text": "Since related entities in AM are oftentimes several dozens of tokens apart from each other, neural sequence tagging models are in principle ideal candidates for such a framing because they can take into account long-range dependencies-something that is inherently difficult to capture with traditional feature-based tagging models such as conditional random fields (CRFs).", "labels": [], "entities": [{"text": "neural sequence tagging", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.7147765556971232}]}, {"text": "Third, we frame AM as a multi-task (tagging) problem.", "labels": [], "entities": [{"text": "AM", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9775585532188416}]}, {"text": "We experiment with subtasks of AM-e.g., component identification-as auxiliary tasks and investigate whether this improves performance on the AM problem.", "labels": [], "entities": [{"text": "component identification-as", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.779999166727066}]}, {"text": "Adding such subtasks can be seen as analogous to de-coupling, e.g., component identification from the full AM problem.", "labels": [], "entities": [{"text": "component identification", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.7800118029117584}]}, {"text": "Fourth, we evaluate the model of that combines sequential (entity) and tree structure (relation) information and is in principle applicable to any problem where the aim is to extract entities and their relations.", "labels": [], "entities": []}, {"text": "As such, this model makes fewer assumptions than our dependency parsing and tagging approaches.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7641921043395996}]}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "(1) We present the first neural end-to-end solutions to computational AM.", "labels": [], "entities": [{"text": "computational AM", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.6327267587184906}]}, {"text": "(2) We show that several of them perform better than the state-of-theart joint ILP model.", "labels": [], "entities": []}, {"text": "(3) We show that a framing of AM as a token-based dependency parsing problem is ineffective-in contrast to what has been proposed for systems that operate on the coarser component level and that (4) a standard neural sequence tagging model that encodes distance information between components performs robustly in different environments.", "labels": [], "entities": [{"text": "token-based dependency parsing", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.5397263964017233}]}, {"text": "Finally, (5) we show that a multi-task learning setup where natural subtasks of the full AM problem are added as auxiliary tasks improves performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents and discusses the empirical results for the AM framings outlined in \u00a74.", "labels": [], "entities": [{"text": "AM framings", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.9227412641048431}]}, {"text": "We relegate issues of pre-trained word embeddings, hyperparameter optimization and further practical issues to the supplementary material.", "labels": [], "entities": [{"text": "hyperparameter optimization", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.7481094896793365}]}, {"text": "Links to software used as well as some additional error analysis can also be found there.", "labels": [], "entities": []}, {"text": "Evaluation Metric We adopt the evaluation metric suggested in.", "labels": [], "entities": []}, {"text": "This computes true positives TP, false positives FP, and false negatives FN, and from these calculates component and relation F 1 scores as For space reasons, we refer to for specifics, but to illustrate, for components, true positives are defined as the set of components in the gold standard for which there exists a predicted component with the same type that 'matches'.", "labels": [], "entities": [{"text": "FP", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.7806646823883057}]}, {"text": "define a notion of what we may term 'level \u03b1 matching': for example, at the 100% level (exact match) predicted and gold components must have exactly the same spans, whereas at the 50% level they must only share at least 50% of their tokens (approximate match).", "labels": [], "entities": []}, {"text": "We refer to these scores as C-F1 (100%) and C-F1 (50%), respectively.", "labels": [], "entities": []}, {"text": "For relations, an analogous F1 score is determined, which we denote by R-F1 (100%) and R-F1 (50%).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9898886382579803}, {"text": "R-F1", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9273794293403625}]}, {"text": "We note that R-F1 scores depend on C-F1 scores because correct relations must have correct arguments.", "labels": [], "entities": []}, {"text": "We also define a 'global' F1 score, which is the F1-score of C-F1 and R-F1.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9828493893146515}, {"text": "F1-score", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9982771873474121}]}, {"text": "Most of our results are shown in.", "labels": [], "entities": []}, {"text": "We train and test all parsers on the paragraph level, because training them on essay level was typically too memory-exhaustive.", "labels": [], "entities": []}, {"text": "MST mostly labels only non-argumentative units correctly, except for recognizing individual major claims, but never finds their exact spans (e.g., \"tourism can create negative impacts on\" while the gold major claim is \"international tourism can create negative impacts on the destination countries\").", "labels": [], "entities": [{"text": "MST", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5712161660194397}]}, {"text": "Mate is slightly better and in particular recognizes several major claims correctly.", "labels": [], "entities": []}, {"text": "Kiperwasser performs decently on the approximate match level, but not on exact level.", "labels": [], "entities": [{"text": "exact", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.9744748473167419}]}, {"text": "Upon inspection, we find that the parser often predicts 'too large' component spans, e.g., by including following punctuation.", "labels": [], "entities": []}, {"text": "The best parser by far is the LSTM-Parser.", "labels": [], "entities": [{"text": "LSTM-Parser", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.8671955466270447}]}, {"text": "It is over 100% better than Kiperwasser on exact spans and still several percentage points on approximate spans.", "labels": [], "entities": []}, {"text": "How does performance change when we switch to the essay level?", "labels": [], "entities": []}, {"text": "For the LSTM-Parser, the best performance on essay level is 32.84%/47.44% C-F1 (100%/50% level), and 9.11%/14.45% on R-F1, but performance result varied drastically between different parametrizations.", "labels": [], "entities": [{"text": "LSTM-Parser", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.8581184148788452}]}, {"text": "Thus, the performance drop between paragraph and essay level is in any case immense.", "labels": [], "entities": []}, {"text": "Since the employed features of modern featurebased parsers are rather general-such as distance between words or word identities-we had expected them to perform much better.", "labels": [], "entities": []}, {"text": "The mini-: Performance of dependency parsers, STag BLCC , LSTM-ER and ILP (from top to bottom).", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7574417889118195}, {"text": "STag", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.4018365144729614}, {"text": "BLCC", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.5522754788398743}]}, {"text": "The ILP model operates on both levels.", "labels": [], "entities": []}, {"text": "Best scores in each column in bold (signific. at p < 0.01; Two-sided Wilcoxon signed rank test, pairing F1 scores for documents).", "labels": [], "entities": [{"text": "F1", "start_pos": 104, "end_pos": 106, "type": "METRIC", "confidence": 0.9431098103523254}]}, {"text": "We also report token level accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.7570081353187561}]}, {"text": "mal feature set employed by Kiperwasser is apparently not sufficient for accurate AM but still a lot more powerful than the hand-crafted feature approaches.", "labels": [], "entities": [{"text": "accurate AM", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.6803231537342072}]}, {"text": "We hypothesize that the LSTM-Parser's good performance, relative to the other parsers, is due to its encoding of the whole stack historyrather than just the top elements on the stack as in Kiperwasser-which makes it aware of much larger 'contexts'.", "labels": [], "entities": []}, {"text": "While the drop in performance from paragraph to essay level is expected, the LSTM-Parser's deterioration is much more severe than the other models' surveyed below.", "labels": [], "entities": []}, {"text": "We believe that this is due to a mixture of the following: (1) 'capacity', i.e., model complexity, of the parsersthat is, risk of overfitting; and (2) few, but very long sequences on essay level-that is, little training data (trees), paired with a huge search space on each train/test instance, namely, the number of possible trees on n tokens.", "labels": [], "entities": []}, {"text": "See also our discussions below, particularly, our stability analysis.", "labels": [], "entities": [{"text": "stability analysis", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8663029968738556}]}, {"text": "(b) Sequence Tagging For these experiments, we use the BLCC tagger from and refer to the resulting system as STag BLCC . Again, we observe that paragraph level is considerably easier than essay level; e.g., for relations, there is \u223c5% points increase from essay to paragraph level.", "labels": [], "entities": [{"text": "Sequence Tagging", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8892936706542969}, {"text": "BLCC", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.8014823794364929}, {"text": "STag", "start_pos": 109, "end_pos": 113, "type": "DATASET", "confidence": 0.6097815036773682}, {"text": "BLCC", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.41042765974998474}]}, {"text": "Overall, STag BLCC is \u223c13% better than the best parser for C-F1 and \u223c11% better for R-F1 on the paragraph level.", "labels": [], "entities": [{"text": "STag", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.4565449357032776}, {"text": "BLCC", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.6019932627677917}]}, {"text": "Our explanation is that taggers are simpler local models, and thus needless training data and are less prone to overfitting.", "labels": [], "entities": [{"text": "taggers", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9630991220474243}]}, {"text": "Moreover, they can much better deal with the long sequences because they are largely invariant to length: e.g., it does in principle not matter, from a parameter estimation perspective, whether we train our taggers on two sequences of lengths n and m, respectively, or on one long sequence of length n + m.", "labels": [], "entities": []}, {"text": "(c) MTL As indicated, we use the MTL tagging framework from S\u00f8gaard and Goldberg (2016) for multi-task experiments.", "labels": [], "entities": [{"text": "MTL tagging", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.7616142630577087}]}, {"text": "The underlying tagging framework is weaker than that of BLCC: there is no CNN which can take subword information into account and there are no dependencies between output labels: each tagging prediction is made independently of the other predictions.", "labels": [], "entities": []}, {"text": "We refer to this system as STag BL . Accordingly, as shows for the essay level (paragraph level omitted for space reasons), results are generally weaker: For exact match, C-F1 values are about \u223c10% points below those of STag BLCC , while approximate match performances are much closer.", "labels": [], "entities": [{"text": "STag BL", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.45591384172439575}]}, {"text": "Hence, the independence assumptions of the BL tagger apparently lead to more 'local' errors such as exact argument span identification (cf. error analysis).", "labels": [], "entities": [{"text": "BL tagger", "start_pos": 43, "end_pos": 52, "type": "TASK", "confidence": 0.5814913213253021}, {"text": "exact argument span identification", "start_pos": 100, "end_pos": 134, "type": "TASK", "confidence": 0.5425918251276016}]}, {"text": "An analogous trend holds for argument relations.", "labels": [], "entities": []}, {"text": "Additional Tasks: We find that when we train STag BL with only its main task-with label set Y as in Eq.", "labels": [], "entities": [{"text": "STag BL", "start_pos": 45, "end_pos": 52, "type": "TASK", "confidence": 0.6591823399066925}]}, {"text": "(1)-the overall result is worst.", "labels": [], "entities": []}, {"text": "In contrast, when we include the 'natural subtasks' \"C\" (label set Y C consists of the projection on the coordinates (b, t) in Y) and/or \"R\" (label set Y R consists of the projection on the coordinates (d, s)), performance increases typically by a few percentage points.", "labels": [], "entities": []}, {"text": "This indicates that complex sequence tagging may benefit when we train a \"sublabeler\" in the same neural architecture, a finding that maybe particularly relevant for morphological POS tagging.", "labels": [], "entities": [{"text": "complex sequence tagging", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6463856895764669}, {"text": "POS tagging", "start_pos": 180, "end_pos": 191, "type": "TASK", "confidence": 0.8769368231296539}]}, {"text": "Unlike, we do not find that the optimal architecture is the one in which \"lower\" tasks (such as C or R) feed from lower layers.", "labels": [], "entities": []}, {"text": "In fact, in one of the best parametrizations the C task and the full task feed from the same layer in the deep BiLSTM.", "labels": [], "entities": []}, {"text": "Moreover, we find that the C task is consistently more helpful as an auxiliary task than the R task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of dependency parsers, STag BLCC , LSTM-ER and ILP (from top to bottom). The  ILP model operates on both levels. Best scores in each column in bold (signific. at p < 0.01; Two-sided  Wilcoxon signed rank test, pairing F1 scores for documents). We also report token level accuracy.", "labels": [], "entities": [{"text": "F1", "start_pos": 240, "end_pos": 242, "type": "METRIC", "confidence": 0.9336382746696472}, {"text": "accuracy", "start_pos": 293, "end_pos": 301, "type": "METRIC", "confidence": 0.9369689226150513}]}, {"text": " Table 3: Performance of MTL sequence tagging  approaches, essay level. Tasks separated by \":\".  Layers from which tasks feed are indicated by re- spective numbers.", "labels": [], "entities": [{"text": "MTL sequence tagging", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.9185044566790262}]}, {"text": " Table 4: C-F1 (100%) in % for the two indicated  systems; essay vs. paragraph level. Note that the  mean performances are lower than the majority  performances over the runs given in", "labels": [], "entities": []}, {"text": " Table 5: F1 scores in % on BIO tagging task.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9996552467346191}, {"text": "BIO tagging task", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7309843997160593}]}]}