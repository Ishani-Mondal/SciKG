{"title": [{"text": "TextFlow: A Text Similarity Measure based on Continuous Sequences", "labels": [], "entities": []}], "abstractContent": [{"text": "Text similarity measures are used in multiple tasks such as plagiarism detection, information ranking and recognition of paraphrases and textual entailment.", "labels": [], "entities": [{"text": "plagiarism detection", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.7599842846393585}, {"text": "information ranking", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.8604072034358978}, {"text": "textual entailment", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.6765627264976501}]}, {"text": "While recent advances in deep learning highlighted further the relevance of sequential models in natural language generation, existing similarity measures do not fully exploit the sequential nature of language.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 97, "end_pos": 124, "type": "TASK", "confidence": 0.7379186550776163}]}, {"text": "Examples of such similarity measures include n-grams and skip-grams overlap which rely on distinct slices of the input texts.", "labels": [], "entities": []}, {"text": "In this paper we present a novel text similarity measure inspired from a common representation in DNA sequence alignment algorithms.", "labels": [], "entities": [{"text": "DNA sequence alignment algorithms", "start_pos": 98, "end_pos": 131, "type": "TASK", "confidence": 0.7330726683139801}]}, {"text": "The new measure, called TextFlow, represents input text pairs as continuous curves and uses both the actual position of the words and sequence matching to compute the similarity value.", "labels": [], "entities": []}, {"text": "Our experiments on eight different datasets show very encouraging results in paraphrase detection, textual entailment recognition and ranking relevance.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.9379990100860596}, {"text": "textual entailment recognition", "start_pos": 99, "end_pos": 129, "type": "TASK", "confidence": 0.8385316928227743}]}], "introductionContent": [], "datasetContent": [{"text": "This evaluation was performed on 8 datasets from 3 different classification tasks: Textual Entailment Recognition, Paraphrase Detection, and ranking relevance.", "labels": [], "entities": [{"text": "Textual Entailment Recognition", "start_pos": 83, "end_pos": 113, "type": "TASK", "confidence": 0.7227154771486918}, {"text": "Paraphrase Detection", "start_pos": 115, "end_pos": 135, "type": "TASK", "confidence": 0.7194764465093613}]}, {"text": "The datasets are as follows: \u2022 RTE 1, 2, and 3: the first three datasets from the Recognizing Textual Entailment (RTE) challenge ().", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE) challenge", "start_pos": 82, "end_pos": 128, "type": "TASK", "confidence": 0.7244891481740134}]}, {"text": "Each dataset consists of sentence pairs which are annotated with 2 labels: entailment, and nonentailment.", "labels": [], "entities": [{"text": "nonentailment", "start_pos": 91, "end_pos": 104, "type": "METRIC", "confidence": 0.9816607236862183}]}, {"text": "They contain respectively (200, 800), (800, 800), and (800, 800) (train, test) pairs.", "labels": [], "entities": []}, {"text": "\u2022 Guardian: an RTE dataset collected from 78,696 Guardian articles 5 published from January 2004 onwards and consisting of 32K pairs which we split randomly in 90%/10% training/test sets.", "labels": [], "entities": [{"text": "Guardian", "start_pos": 2, "end_pos": 10, "type": "DATASET", "confidence": 0.951605498790741}, {"text": "RTE dataset collected from 78,696 Guardian articles", "start_pos": 15, "end_pos": 66, "type": "DATASET", "confidence": 0.899809113570622}]}, {"text": "Positive examples were collected from the titles and first sentences.", "labels": [], "entities": []}, {"text": "Negative examples were collected from the same source by selecting consecutive sentences and random sentences.", "labels": [], "entities": []}, {"text": "\u2022 SNLI: a recent RTE dataset consisting of 560K training sentence pairs annotated with \u2022 MSRP: the Microsoft Research Paraphrase corpus, consisting of 5,800 sentence pairs annotated with a binary label indicating whether the two sentences are paraphrases or not.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.7788410186767578}, {"text": "Microsoft Research Paraphrase corpus", "start_pos": 99, "end_pos": 135, "type": "DATASET", "confidence": 0.82704858481884}]}, {"text": "\u2022 Semeval-16-3B: a dataset of questionquestion similarity collected from StackOverflow ().", "labels": [], "entities": []}, {"text": "The dataset contains 3,169 training pairs and 700 test pairs.", "labels": [], "entities": []}, {"text": "Three labels are considered: \"Perfect Match\", \"Relevant\" or \"Irrelevant\".", "labels": [], "entities": [{"text": "Perfect Match", "start_pos": 30, "end_pos": 43, "type": "METRIC", "confidence": 0.8537990748882294}, {"text": "Relevant", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9953216910362244}, {"text": "Irrelevant", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9939193725585938}]}, {"text": "We combined the first two into the same positive category for our evaluation.", "labels": [], "entities": []}, {"text": "\u2022 Semeval-14-1: a corpus of Sentences Involving Compositional Knowledge () consisting of 10,000 English sentence pairs annotated with both similarity scores and relevance labels.", "labels": [], "entities": []}, {"text": "After a preprocessing step where we removed stopwords, we computed the similarity values using 7 different types of sequences constructed, respectively, with the following value from each token: \u2022 Word (plain text value) \u2022 Lemma \u2022 Part-Of-Speech (POS) tag  In additional experiments, we compared T F c and T F t with the other similarity measures when applied to bi-grams and tri-grams instead of individual tokens.", "labels": [], "entities": []}, {"text": "The results were significantly lower on all datasets (between 3 and 10 points loss in accuracy) for both the soa measures and TextFlow variants.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9991211295127869}]}, {"text": "This result could be explained by the fact that n-grams are too rigid when a sub-sequence varies even slightly, e.g., the insertion of anew word inside a 3-words sequence leads to a tri-gram mismatch and reduces bi-gram overlap from 100% to 50% for the considered sub-sequence.", "labels": [], "entities": []}, {"text": "This issue is not encountered with TextFlow as it relies on the token level, and such an insertion will not cancel or reduce significantly the gains from the correct ordering of the words.", "labels": [], "entities": []}, {"text": "It must be noted here that not all languages grant the same level of importance to sequences and that additional multilingual tests have to be carried out.", "labels": [], "entities": []}, {"text": "In addition to binary classification output such as textual entailment and paraphrase recognition, text similarity measures can be evaluated more precisely when we consider the correlation of their values for ranking purposes.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7348610460758209}, {"text": "paraphrase recognition", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.7933486998081207}]}, {"text": "We conducted ranking correlation experiments on three test datasets provided at the semantic text similarity task at Semeval 2012, with gold score values for their text pairs.", "labels": [], "entities": [{"text": "semantic text similarity task at Semeval 2012", "start_pos": 84, "end_pos": 129, "type": "TASK", "confidence": 0.7365237304142543}]}, {"text": "The datasets have 750 sentence pairs each, and are extracted from the Microsoft Research video descriptions corpus, MSRP and the SMTeuroparl . When compared to the traditional similarity measures, TextFlow had the best correlation on the first two datasets with, for instance, 0.54 and 0.46 pearson correlation on the lemmas sequences and the second best on the MSRP extract where the Cosine similarity had the best performance with 0.71 vs 0.68, noting that the Cosine similarity uses word frequencies when the evaluated version of TextFlow did not use word-level weights.", "labels": [], "entities": [{"text": "Microsoft Research video descriptions corpus", "start_pos": 70, "end_pos": 114, "type": "DATASET", "confidence": 0.8454669713973999}, {"text": "MSRP", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.901154637336731}, {"text": "MSRP extract", "start_pos": 362, "end_pos": 374, "type": "DATASET", "confidence": 0.8096209764480591}]}, {"text": "Including word weights is one of the promising perspectives inline with this work as it could be done simply by making the deltas vary according to the weight/importance of the (un)matched word.", "labels": [], "entities": []}, {"text": "Also, in such a setting, the impact of a sequence of N words will naturally increase or decrease according to the word weights (cf..", "labels": [], "entities": []}, {"text": "We conducted a preliminary test using the inverse document frequency of the words as extracted from Wikipedia with Gensim , which led to an improvement of up to 2% for most datasets, with performance decreasing slightly on two of them.", "labels": [], "entities": []}, {"text": "Other kinds of weights could also be included just as easily, such as contextual word relatedness using embeddings or other semantic relatedness factors such as WordNet distances).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy evaluation with different aggregations of XF using an SVM classifier.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy values using. The best result is highlighted, the second best is underlined.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9985313415527344}]}, {"text": " Table 3: F1 scores. The best result is highlighted, the second best is underlined.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9990780353546143}]}, {"text": " Table 4: Precision values. The best result is highlighted, the second best is underlined.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9949442744255066}]}, {"text": " Table 5: Recall values. The best result is highlighted, the second best is underlined.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9856584668159485}]}]}