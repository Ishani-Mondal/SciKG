{"title": [{"text": "Differentiable Scheduled Sampling for Credit Assignment", "labels": [], "entities": [{"text": "Credit Assignment", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7018382251262665}]}], "abstractContent": [{"text": "We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding for sequence-to-sequence (seq2seq) models.", "labels": [], "entities": [{"text": "argmax", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9439499974250793}]}, {"text": "By incorporating this approximation into the scheduled sampling training procedure (Bengio et al., 2015)-a well-known technique for correcting exposure bias-we introduce anew training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value.", "labels": [], "entities": [{"text": "correcting exposure bias-we", "start_pos": 132, "end_pos": 159, "type": "TASK", "confidence": 0.7789511680603027}]}, {"text": "In addition, by using a related approximation, we demonstrate a similar approach to sampled-based training.", "labels": [], "entities": []}, {"text": "Finally , we show that our approach outper-forms cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 140, "end_pos": 164, "type": "TASK", "confidence": 0.6180513401826223}, {"text": "machine translation", "start_pos": 169, "end_pos": 188, "type": "TASK", "confidence": 0.7665358483791351}]}], "introductionContent": [{"text": "Sequence-to-Sequence (seq2seq) models have demonstrated excellent performance in several tasks including machine translation), summarization, dialogue generation (, and image captioning (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7681538760662079}, {"text": "summarization", "start_pos": 127, "end_pos": 140, "type": "TASK", "confidence": 0.9920501112937927}, {"text": "dialogue generation", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.8844363689422607}, {"text": "image captioning", "start_pos": 169, "end_pos": 185, "type": "TASK", "confidence": 0.7634658515453339}]}, {"text": "However, the standard cross-entropy training procedure for these models suffers from the well-known problem of exposure bias: because cross-entropy training always uses gold contexts, the states and contexts encountered during training do not match those encountered attest time.", "labels": [], "entities": []}, {"text": "This issue has been addressed using several approaches that try to incorporate awareness of decoding choices into the training optimization.", "labels": [], "entities": []}, {"text": "These include reinforcement learning (, imitation learning (, and beam-based approaches.", "labels": [], "entities": []}, {"text": "In this paper, we focus on one the simplest to implement and least computationally expensive approaches, scheduled sampling ( , which stochastically incorporates contexts from previous decoding decisions into training.", "labels": [], "entities": []}, {"text": "While scheduled sampling has been empirically successful, its training objective has a drawback: because the procedure directly incorporates greedy decisions at each time step, the objective is discontinuous at parameter settings where previous decisions change their value.", "labels": [], "entities": []}, {"text": "As a result, gradients near these points are non-informative and scheduled sampling has difficulty assigning credit for errors.", "labels": [], "entities": []}, {"text": "In particular, the gradient does not provide information useful in distinguishing between local errors without future consequences and cascading errors which are more serious.", "labels": [], "entities": []}, {"text": "Here, we propose a novel approach based on scheduled sampling that uses a differentiable approximation of previous greedy decoding decisions inside the training objective by incorporating a continuous relaxation of argmax.", "labels": [], "entities": [{"text": "argmax", "start_pos": 215, "end_pos": 221, "type": "METRIC", "confidence": 0.9888768792152405}]}, {"text": "As a result, our end-to-end relaxed greedy training objective is differentiable everywhere and fully continuous.", "labels": [], "entities": []}, {"text": "By making the objective continuous at points where previous decisions change value, our approach provides gradients that can respond to cascading errors.", "labels": [], "entities": []}, {"text": "In addition, we demonstrate a related approximation and reparametrization for sample-based training (another training scenario considered by scheduled sampling ( ) that can yield stochastic gradients with lower variance than in standard scheduled sampling.", "labels": [], "entities": []}, {"text": "In our experiments on two different tasks, machine translation (MT) and named entity recognition (NER), we show that our approach outperforms both cross-entropy training and standard scheduled sampling procedures with greedy and sampled-based training.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.8614835500717163}, {"text": "named entity recognition (NER)", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.7614024380842844}]}], "datasetContent": [{"text": "We perform experiments with machine translation (MT) and named entity recognition (NER).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8210186243057251}, {"text": "named entity recognition (NER)", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.757021407286326}]}, {"text": "Data: For MT, we use the same dataset (the German-English portion of the IWSLT 2014 machine translation evaluation campaign (), preprocessing and data splits as.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9858642816543579}, {"text": "IWSLT 2014 machine translation evaluation", "start_pos": 73, "end_pos": 114, "type": "TASK", "confidence": 0.7758790612220764}]}, {"text": "For named entity recognition, we use the CONLL 2003 shared task data for German language and use the provided data splits.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6548129618167877}, {"text": "CONLL 2003 shared task data", "start_pos": 41, "end_pos": 68, "type": "DATASET", "confidence": 0.9436636567115784}]}, {"text": "We perform no preprocessing on the data.The output vocabulary length for MT is 32000 and 10 for NER.", "labels": [], "entities": [{"text": "MT", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.6223272085189819}, {"text": "NER", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.8557274341583252}]}, {"text": "Implementation details: For MT, we use a seq2seq model with a simple attention mechanism (), a bidirectional LSTM encoder (1 layer, 256 units), and an LSTM decoder (1 layer, 256 units).", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9914632439613342}]}, {"text": "For NER, we use a seq2seq model with an LSTM encoder (1 layer, 64 units) and an LSTM decoder (1 layer, 64 units) with a fixed attention mechanism that deterministically attends to the ith input token when decoding the ith output, and hence does not involve learning of attention parameters.", "labels": [], "entities": [{"text": "NER", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8707598447799683}]}, {"text": "4 Hyperparameter tuning: We start by training with actual ground truth sequences for the first epoch and decay the probability of selecting the ground truth token as an inverse sigmoid (  of epochs with a decay strength parameter k.", "labels": [], "entities": []}, {"text": "We also tuned for different values of \u03b1 and explore the effect of varying \u03b1 exponentially (annealing) with the epochs.", "labels": [], "entities": []}, {"text": "In table 1, we report results for the best performing configuration of decay parameter and the \u03b1 parameter on the validation set.", "labels": [], "entities": []}, {"text": "To account for variance across randomly started runs, we ran multiple random restarts (RR) for all the systems evaluated and always used the RR with the best validation set score to calculate test performance.", "labels": [], "entities": []}, {"text": "Comparison We report validation and test metrics for NER and MT tasks in and BLEU respectively.", "labels": [], "entities": [{"text": "NER", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.7583311796188354}, {"text": "MT tasks", "start_pos": 61, "end_pos": 69, "type": "TASK", "confidence": 0.7063143849372864}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9981265664100647}]}, {"text": "'Greedy' in the table refers to scheduled sampling with soft argmax decisions (either soft or hard) and 'Sample' refers the corresponding reparametrized sample-based decoding scenario.", "labels": [], "entities": []}, {"text": "We compare our approach with two baselines: standard cross-entropy loss minimization for seq2seq models ('Baseline CE') and the standard scheduled sampling procedure ( ).", "labels": [], "entities": []}, {"text": "We report results for two variants of our approach: one with a fixed \u03b1 parameter throughout the training procedure (\u03b1-soft fixed), and the other in which we vary \u03b1 exponentially with the number of epochs (\u03b1-soft annealed).: Result on NER and MT.", "labels": [], "entities": [{"text": "NER", "start_pos": 234, "end_pos": 237, "type": "DATASET", "confidence": 0.8693166971206665}, {"text": "MT", "start_pos": 242, "end_pos": 244, "type": "DATASET", "confidence": 0.5837723016738892}]}, {"text": "We compare our approach (\u03b1-soft argmax with fixed and annealed temperature) with standard cross entropy training (Baseline CE) and discontinuous scheduled sampling (", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Result on NER and MT. We compare our approach (\u03b1-soft argmax with fixed and annealed temperature) with standard  cross entropy training (Baseline CE) and discontinuous scheduled sampling (", "labels": [], "entities": [{"text": "NER", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.48366236686706543}, {"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.6719112396240234}]}, {"text": " Table 2: Effect of different schedules for scheduled sampling  on NER. k is the decay strength parameter. Higher k cor- responds to gentler decay schedules. Always refers to the  case when predictions at the previous predictions are always  passed on as inputs to the next step.", "labels": [], "entities": [{"text": "NER", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.7877324819564819}]}]}