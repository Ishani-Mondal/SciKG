{"title": [{"text": "Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning", "labels": [], "entities": [{"text": "Generating Natural Answers", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8432915409406027}, {"text": "Sequence-to-Sequence Learning", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.8776797354221344}]}], "abstractContent": [{"text": "Generating answer with natural language sentence is very important in real-world question answering systems, which needs to obtain aright answer as well as a coherent natural response.", "labels": [], "entities": [{"text": "question answering", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7367387861013412}]}, {"text": "In this paper, we propose an end-to-end question answering system called COREQA in sequence-to-sequence learning, which incorporates copying and retrieving mechanisms to generate natural answers within an encoder-decoder framework.", "labels": [], "entities": [{"text": "question answering", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7162542939186096}]}, {"text": "Specifically , in COREQA, the semantic units (words, phrases and entities) in a natural answer are dynamically predicted from the vocabulary, copied from the given question and/or retrieved from the corresponding knowledge base jointly.", "labels": [], "entities": []}, {"text": "Our empirical study on both synthetic and real-world datasets demonstrates the efficiency of COREQA, which is able to generate correct, coherent and natural answers for knowledge inquired questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering (QA) systems devote to providing exact answers, often in the form of phrases and entities for natural language questions, which mainly focus on analyzing questions, retrieving related facts from text snippets or knowledge bases (KBs), and finally predicting the answering semantic units-SU (words, phrases and entities) through ranking () and reasoning ().", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8875070333480835}]}, {"text": "However, in real-world environments, most people prefer the correct answer replied with a more natural way.", "labels": [], "entities": []}, {"text": "For example, most existing commercial products such as Siri 1 will reply a natural answer \"Jet Li is 1.64m in height.\" for the question \"How tall is Jet Li?\", rather than only answering one entity \"1.64m\".", "labels": [], "entities": []}, {"text": "Basic on this observation, we define the \"natural answer\" as the natural response in our daily communication for replying factual questions, which is usually expressed in a complete/partial natural language sentence rather than a single entity/phrase.", "labels": [], "entities": []}, {"text": "In this case, the system needs to not only parse question, retrieve relevant facts from KB but also generate a proper reply.", "labels": [], "entities": [{"text": "parse question", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.8596561551094055}]}, {"text": "To this end, most previous approaches employed message-response patterns.", "labels": [], "entities": []}, {"text": "schematically illustrates the major steps and features in this process.", "labels": [], "entities": []}, {"text": "The system first needs to recognize the topic entity \"Jet Li\" in the question and then extract multiple related facts <Jet Li, gender, Male>, <Jet Li, birthplace, Beijing> and <Jet Li, nationality, Singapore> from KB.", "labels": [], "entities": [{"text": "KB", "start_pos": 214, "end_pos": 216, "type": "DATASET", "confidence": 0.6964842677116394}]}, {"text": "Based on the chosen facts and the commonly used messageresponse patterns \"where was %entity from?\"", "labels": [], "entities": []}, {"text": "-\"%entity was born in %birthplace, %pronoun is %nationality citizen.\"", "labels": [], "entities": []}, {"text": "2 , the system could finally generate the natural answer (.", "labels": [], "entities": []}, {"text": "In order to generate natural answers, typical products need lots of Natural Language Processing (NLP) tools and pattern engineering, which not only suffers from high costs of manual annotations for training data and patterns, but also have low coverage that cannot flexibly deal with variable linguistic phenomena in different domains.", "labels": [], "entities": []}, {"text": "Therefore, this paper devotes to develop an end-to-end paradigm that generates natural answers without any NLP tools (e.g. POS tagging, parsing, etc.) and pattern engineering.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 123, "end_pos": 134, "type": "TASK", "confidence": 0.7462472021579742}]}, {"text": "This paradigm tries to consider question answering in an end-to-end framework.", "labels": [], "entities": [{"text": "question answering", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7920692265033722}]}, {"text": "In this way, the complicated QA process, including analyzing question, retrieving relevant facts from KB, and generating correct, coherent, natural answers, could be resolved jointly.", "labels": [], "entities": [{"text": "QA process", "start_pos": 29, "end_pos": 39, "type": "TASK", "confidence": 0.876750648021698}]}, {"text": "Nevertheless, generating natural answers in an end-to-end manner is not an easy task.", "labels": [], "entities": []}, {"text": "The key challenge is that the words in a natural answer maybe generated by different ways, including: 1) the common words usually are predicted using a (conditional) language model (e.g. \"born\" in); 2) the major entities/phrases are selected from the source question (e.g. \"Jet Li\"); 3) the answering entities/phrases are retrieved from the corresponding KB (e.g. \"Beijing\").", "labels": [], "entities": [{"text": "Beijing\")", "start_pos": 365, "end_pos": 374, "type": "DATASET", "confidence": 0.9132920503616333}]}, {"text": "In addition, some words or phrases even need to be inferred from related knowledge (e.g. \"He\" should be inferred from the value of \"gender\").", "labels": [], "entities": []}, {"text": "And we even need to deal with some morphological variants (e.g. \"Singapore\" in KB but \"Singaporean\" in answer).", "labels": [], "entities": []}, {"text": "Although existing end-to-end models for KB-based question answering, such as GenQA (, were able to retrieve facts from KBs with neural models.", "labels": [], "entities": [{"text": "KB-based question answering", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.685801108678182}, {"text": "GenQA", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.9322767853736877}]}, {"text": "Unfortunately, they cannot copy SUs from the question in generating answers.", "labels": [], "entities": []}, {"text": "Moreover, they could not deal with complex questions which need to utilize multiple facts.", "labels": [], "entities": []}, {"text": "In addition, existing approaches for conversational (Dialogue) systems are able to generate natural utterances) in sequence-tosequence learning (Seq2Seq).", "labels": [], "entities": []}, {"text": "But they cannot interact with KB and answer information-inquired questions.", "labels": [], "entities": []}, {"text": "For example, CopyNet () is able to copy words from the original source in generating the target through incorporating copying mechanism in conventional Seq2Seq learning, but they cannot retrieve SUs from external memory (e.g. KBs, Texts, etc.).", "labels": [], "entities": []}, {"text": "Therefore, facing the above challenges, this paper proposes a neural generative model called COREQA with Seq2Seq learning, which is able to reply an answer in a natural way fora given question.", "labels": [], "entities": []}, {"text": "Specifically, we incorporate COpying and REtrieving mechanisms within Seq2Seq learning.", "labels": [], "entities": [{"text": "REtrieving", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9671974778175354}]}, {"text": "COREQA is able to analyze the question, retrieve relevant facts and generate a sequence of SUs using a hybrid method with a completely end-to-end learning framework.", "labels": [], "entities": [{"text": "COREQA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.919388473033905}]}, {"text": "We conduct experiments on both synthetic data sets and real-world datasets, and the experimental results demonstrate the efficiency of COREQA compared with existing endto-end QA/Dialogue methods.", "labels": [], "entities": []}, {"text": "In brief, our main contributions are as follows: \u2022 We propose anew and practical question answering task which devotes to generating natural answers for information inquired questions.", "labels": [], "entities": [{"text": "question answering", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7608648836612701}]}, {"text": "It can be regarded as a fusion task of QA and Dialogue.", "labels": [], "entities": []}, {"text": "\u2022 We propose a neural network based model, named as COREQA, by incorporating copying and retrieving mechanism in Seq2Seq learning.", "labels": [], "entities": []}, {"text": "In our knowledge, it is the first end-to-end model that could answer complex questions in a natural way.", "labels": [], "entities": []}, {"text": "\u2022 We implement experiments on both synthetic and real-world datasets.", "labels": [], "entities": []}, {"text": "The experimental results demonstrate that the proposed model could be more effective for generating correct, coherent and natural answers for knowledge inquired questions compared with existing approaches.", "labels": [], "entities": []}, {"text": "2 Background: Neural Models for Sequence-to-Sequence Learning", "labels": [], "entities": [{"text": "Sequence-to-Sequence Learning", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.9457100629806519}]}], "datasetContent": [{"text": "In this section, we present our main experimental results in two datasets.", "labels": [], "entities": []}, {"text": "The first one is a small synthetic dataset in a restricted domain (only involving four properties of persons) (Section 4.1) learning rule to update gradients in all experimental configures.", "labels": [], "entities": []}, {"text": "The sources codes and data will be released at the personal homepage of the first author 4 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. It is very clear from Ta- ble 2 that COREQA significantly outperforms all  other compared methods. The reason of the Gen- QA's poor performance is that all synthetic ques- tions need multiple facts, and GenQA will \"safe- ly\" choose the most frequent property (\"gender\")  for all questions. We also found the performances  on \"year\" and \"day\" have a little worse than other  properties such as \"gender\", it may because there  have more ways to answer questions about \"year\"  and \"day\".", "labels": [], "entities": []}, {"text": " Table 2: The AE results (%) on synthetic test data.", "labels": [], "entities": [{"text": "AE", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9992793202400208}]}, {"text": " Table 3: The AE (%) for seen and unseen entities.", "labels": [], "entities": [{"text": "AE", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9934752583503723}]}, {"text": " Table 4: The AE accuracies (%) on real world test  data.", "labels": [], "entities": [{"text": "AE accuracies", "start_pos": 14, "end_pos": 27, "type": "METRIC", "confidence": 0.825067400932312}]}, {"text": " Table 5: The ME results (%) on sampled mixed  test data.", "labels": [], "entities": [{"text": "ME", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9992914199829102}]}]}