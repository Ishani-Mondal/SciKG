{"title": [{"text": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision", "labels": [], "entities": [{"text": "Neural Symbolic Machines", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7945927778879801}]}], "abstractContent": [{"text": "Harnessing the statistical power of neu-ral networks to perform language understanding and symbolic reasoning is difficult , when it requires executing efficient discrete operations against a large knowledge-base.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.7184267789125443}, {"text": "symbolic reasoning", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.6805795729160309}]}, {"text": "In this work, we introduce a Neural Symbolic Machine (NSM), which contains (a) a neural \"program-mer\", i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic \"com-puter\", i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space.", "labels": [], "entities": []}, {"text": "We apply REINFORCE to directly optimize the task reward of this structured prediction problem.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.995881199836731}]}, {"text": "To train with weak supervision and improve the stability of REINFORCE we augment it with an iterative maximum-likelihood training process.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.5230312347412109}]}, {"text": "NSM outperforms the state-of-the-art on the WEBQUESTIONSSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.", "labels": [], "entities": [{"text": "NSM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7011594772338867}, {"text": "WEBQUESTIONSSP dataset", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.9640991687774658}]}], "introductionContent": [{"text": "Deep neural networks have achieved impressive performance in supervised classification and structured prediction tasks such as speech recognition, machine translation (Bahdanau et al.,;) and more.", "labels": [], "entities": [{"text": "supervised classification and structured prediction tasks", "start_pos": 61, "end_pos": 118, "type": "TASK", "confidence": 0.6671315183242162}, {"text": "speech recognition", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7852848172187805}, {"text": "machine translation", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.8052338063716888}]}, {"text": "However, training neural networks for semantic parsing) or program induction, where language is mapped to a sym-\u21e4 Work done while the author was interning at Google \u2020 Work done while the author was visiting Google bolic representation that is executed by an executor, through weak supervision remains challenging.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7640444040298462}, {"text": "program induction", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.8000981211662292}]}, {"text": "This is because the model must interact with a symbolic executor through non-differentiable operations to search over a large program space.", "labels": [], "entities": []}, {"text": "In semantic parsing, recent work handled this ( by training from manually annotated programs and avoiding program execution at training time.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.8261114358901978}]}, {"text": "However, annotating programs is known to be expensive and scales poorly.", "labels": [], "entities": []}, {"text": "In program induction, attempts to address this problem ( either utilized low-level memory (, or required memory to be differentiable () so that the model can be trained with backpropagation.", "labels": [], "entities": [{"text": "program induction", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7897854745388031}]}, {"text": "This makes it difficult to use the efficient discrete operations and memory of a traditional computer, and limited the application to synthetic or small knowledge bases.", "labels": [], "entities": []}, {"text": "In this paper, we propose to utilize the memory and discrete operations of a traditional com-puter in a novel Manager-Programmer-Computer (MPC) framework for neural program induction, which integrates three components: 1.", "labels": [], "entities": [{"text": "neural program induction", "start_pos": 158, "end_pos": 182, "type": "TASK", "confidence": 0.654013971487681}]}, {"text": "A \"manager\" that provides weak supervision (e.g., 'NYC' in) through a reward indicating how well a task is accomplished.", "labels": [], "entities": []}, {"text": "Unlike full supervision, weak supervision is easy to obtain at scale (Section 3.1).", "labels": [], "entities": []}, {"text": "2. A \"programmer\" that takes natural language as input and generates a program that is a sequence of tokens ().", "labels": [], "entities": []}, {"text": "The programmer learns from the reward and must overcome the hard search problem of finding correct programs (Section 2.2).", "labels": [], "entities": []}, {"text": "3. A \"computer\" that executes programs in a high level programming language.", "labels": [], "entities": []}, {"text": "Its nondifferentiable memory enables abstract, scalable and precise operations, but makes training more challenging (Section 2.3).", "labels": [], "entities": []}, {"text": "To help the \"programmer\" prune the search space, it provides a friendly neural computer interface, which detects and eliminates invalid choices (Section 2.1).", "labels": [], "entities": []}, {"text": "Within this framework, we introduce the Neural Symbolic Machine (NSM) and apply it to semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.7145453840494156}]}, {"text": "NSM contains a neural sequenceto-sequence (seq2seq) \"programmer\") and a symbolic non-differentiable Lisp interpreter (\"computer\") that executes programs against a large knowledge-base (KB).", "labels": [], "entities": [{"text": "NSM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8664349913597107}]}, {"text": "Our technical contribution in this work is threefold.", "labels": [], "entities": []}, {"text": "First, to support language compositionality, we augment the standard seq2seq model with a key-variable memory to save and reuse intermediate execution results).", "labels": [], "entities": [{"text": "language compositionality", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.7009725570678711}]}, {"text": "This is a novel application of pointer networks to compositional semantics.", "labels": [], "entities": []}, {"text": "Second, to alleviate the search problem of finding correct programs when training from questionanswer pairs,we use the computer to execute partial programs and prune the programmer's search space by checking the syntax and semantics of generated programs.", "labels": [], "entities": []}, {"text": "This generalizes the weakly supervised semantic parsing framework) by leveraging semantic denotations during structural search.", "labels": [], "entities": []}, {"text": "Third, to train from weak supervision and directly maximize the expected reward we turn to the REINFORCE (Williams, 1992) algorithm.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.995665967464447}]}, {"text": "Since learning from scratch is difficult for RE-INFORCE, we combine it with an iterative maximum likelihood (ML) training process, where beam search is used to find pseudo-gold programs, which are then used to augment the objective of REINFORCE.", "labels": [], "entities": [{"text": "RE-INFORCE", "start_pos": 45, "end_pos": 55, "type": "TASK", "confidence": 0.6156094670295715}]}, {"text": "On the WEBQUESTIONSSP dataset (, NSM achieves new state-of-the-art results with weak supervision, significantly closing the gap between weak and full supervision for this task.", "labels": [], "entities": [{"text": "WEBQUESTIONSSP dataset", "start_pos": 7, "end_pos": 29, "type": "DATASET", "confidence": 0.9143269062042236}]}, {"text": "Unlike prior works, it is trained end-toend, and does not require feature engineering or domain-specific knowledge.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now empirically show that NSM can learn a semantic parser from weak supervision over a large KB.", "labels": [], "entities": []}, {"text": "We evaluate on WEBQUESTIONSSP, a challenging semantic parsing dataset with strong baselines.", "labels": [], "entities": [{"text": "WEBQUESTIONSSP", "start_pos": 15, "end_pos": 29, "type": "DATASET", "confidence": 0.5109630227088928}, {"text": "semantic parsing", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.7341421842575073}]}, {"text": "Experiments show that NSM achieves new state-of-the-art performance on WEBQUES-TIONSSP with weak supervision, and significantly closes the gap between weak and full supervisions for this task.", "labels": [], "entities": [{"text": "WEBQUES-TIONSSP", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.8514716029167175}]}, {"text": "The WEBQUESTIONSSP dataset contains full semantic parses fora subset of the questions from WEBQUESTIONS (, because 18.5% of the original dataset were found to be \"not answerable\".", "labels": [], "entities": [{"text": "WEBQUESTIONSSP dataset", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.9750172197818756}, {"text": "WEBQUESTIONS", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9613802433013916}]}, {"text": "It consists of 3,098 question-answer pairs for training and 1,639 for testing, which were collected using Google Suggest API, and the answers were originally obtained using Amazon Mechanical Turk workers.", "labels": [], "entities": []}, {"text": "They were updated in () by annotators who were familiar with the design of Freebase and added semantic parses.", "labels": [], "entities": []}, {"text": "We further separated out 620 questions from the training set as a validation set.", "labels": [], "entities": []}, {"text": "For query pre-processing we used an in-house named entity linking system to find the entities in a question.", "labels": [], "entities": [{"text": "query pre-processing", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8315216600894928}]}, {"text": "The quality of the entity linker is similar to that of () at 94% of the gold root entities being included.", "labels": [], "entities": []}, {"text": "Similar to, we replaced named entity tokens with a special token \"ENT\".", "labels": [], "entities": [{"text": "ENT", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9768394827842712}]}, {"text": "For example, the question \"who plays meg in family guy\" is changed to \"who plays ENT in ENT ENT\".", "labels": [], "entities": []}, {"text": "This helps reduce overfitting, because instead of memorizing the correct program fora specific entity, the model has to focus on other context words in the sentence, which improves generalization.", "labels": [], "entities": []}, {"text": "Following we used the last publicly available snapshot of.", "labels": [], "entities": []}, {"text": "Since NSM training requires random access to Freebase during decoding, we preprocessed Freebase by removing predicates that are not related to world knowledge (starting with \"/common/ \", \"/type/ \", \"/freebase/ \"), 2 and removing all text valued predicates, which are rarely the answer.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.9502421021461487}]}, {"text": "Out of all 27K relations, 434 relations are removed during preprocessing.", "labels": [], "entities": []}, {"text": "This results in a graph that fits in memory with 23K relations, 82M nodes, and 417M edges.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Average F1 on the validation set for augmented RE- INFORCE, REINFORCE, and iterative ML.", "labels": [], "entities": [{"text": "F1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.905850350856781}, {"text": "RE- INFORCE", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.8681707779566447}, {"text": "REINFORCE", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9713596105575562}]}, {"text": " Table 5: Contributions of different overfitting techniques on  the validation set.", "labels": [], "entities": []}, {"text": " Table 6: Percentage and performance of model generated  programs with different complexity (number of expressions).", "labels": [], "entities": []}]}