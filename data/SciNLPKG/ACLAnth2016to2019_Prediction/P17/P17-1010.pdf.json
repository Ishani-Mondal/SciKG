{"title": [{"text": "Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution", "labels": [], "entities": [{"text": "Zero Pronoun Resolution", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.6537524859110514}]}], "abstractContent": [{"text": "Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers.", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7796934445699056}]}, {"text": "Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task.", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.6255125204722086}]}, {"text": "Also, it is expensive to spend manpower on labeling the data for better performance.", "labels": [], "entities": []}, {"text": "To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution.", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 149, "end_pos": 172, "type": "TASK", "confidence": 0.6744790176550547}]}, {"text": "Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one.", "labels": [], "entities": [{"text": "zero pronoun resolution task", "start_pos": 102, "end_pos": 130, "type": "TASK", "confidence": 0.7725223302841187}]}, {"text": "Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1% F-score on OntoNotes 5.0 data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 146, "end_pos": 153, "type": "METRIC", "confidence": 0.9992088675498962}, {"text": "OntoNotes 5.0 data", "start_pos": 157, "end_pos": 175, "type": "DATASET", "confidence": 0.937475581963857}]}], "introductionContent": [{"text": "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches.", "labels": [], "entities": [{"text": "zero pronoun (ZP) resolution", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.7343215147654215}]}, {"text": "However, a major obstacle for training the supervised learning models for ZP resolution is the lack of annotated data.", "labels": [], "entities": [{"text": "ZP resolution", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.9200586080551147}]}, {"text": "An important step is to organize the shared task on anaphora and coreference resolution, such as the ACE evaluations, SemEval-2010 shared task on Coreference Resolution in Multiple Languages and CoNLL-2012 shared task on Modeling Multilingual Unrestricted Coreference in OntoNotes.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.9326080083847046}, {"text": "Coreference Resolution in Multiple Languages", "start_pos": 146, "end_pos": 190, "type": "TASK", "confidence": 0.865711760520935}]}, {"text": "Following these shared tasks, the annotated evaluation data can be released for the following researches.", "labels": [], "entities": []}, {"text": "Despite the success and contributions of these shared tasks, it still faces the challenge of spending manpower on labeling the extended data for better training performance and domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 177, "end_pos": 194, "type": "TASK", "confidence": 0.7090166658163071}]}, {"text": "To address the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution.", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 147, "end_pos": 170, "type": "TASK", "confidence": 0.6751142938931783}]}, {"text": "Inspired by data generation on cloze-style reading comprehension, we can treat the zero pronoun resolution task as a special case of reading comprehension problem.", "labels": [], "entities": [{"text": "zero pronoun resolution task", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.7710427343845367}]}, {"text": "So we can adopt similar data generation methods of reading comprehension to the zero pronoun resolution task.", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.7275767723719279}]}, {"text": "For the noun or pronoun in the document, which has the frequency equal to or greater than 2, we randomly choose one position where the noun or pronoun is located on, and replace it with a specific symbol blank.", "labels": [], "entities": []}, {"text": "Let query Q and answer A denote the sentence that contains a blank, and the noun or pronoun which is replaced by the blank, respectively.", "labels": [], "entities": []}, {"text": "Thus, a pseudo training sample can be represented as a triple: For the zero pronoun resolution task, a blank represents a zero pronoun (ZP) in query Q, and A indicates the corresponding antecedent of the ZP.", "labels": [], "entities": [{"text": "zero pronoun resolution task", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.7851942926645279}]}, {"text": "In this way, tremendous pseudo training samples can be generated from the various documents, such as news corpus.", "labels": [], "entities": [{"text": "news corpus", "start_pos": 101, "end_pos": 112, "type": "DATASET", "confidence": 0.8291115462779999}]}, {"text": "Towards the shortcomings of the previous approaches that are based on feature engineering, we propose a neural network architecture, which is an attention-based neural network model, for zero pronoun resolution.", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 187, "end_pos": 210, "type": "TASK", "confidence": 0.7001092632611593}]}, {"text": "Also we propose a two-step training method, which benefit from both largescale pseudo training data and task-specific data, showing promising performance.", "labels": [], "entities": []}, {"text": "To sum up, the contributions of this paper are listed as follows.", "labels": [], "entities": []}, {"text": "\u2022 To our knowledge, this is the first time that utilizing reading comprehension neural network model into zero pronoun resolution task.", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.7182415326436361}]}, {"text": "\u2022 We propose a two-step training approach, namely pre-training-then-adaptation, which benefits from both the large-scale automatically generated pseudo training data and taskspecific data.", "labels": [], "entities": []}, {"text": "\u2022 Towards the shortcomings of the feature engineering approaches, we first propose an attention-based neural network model for zero pronoun resolution.", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 127, "end_pos": 150, "type": "TASK", "confidence": 0.669336179892222}]}], "datasetContent": [{"text": "Same to the previous researches that are related to zero pronoun resolution, we evaluate our system performance in terms of F-score (F).", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.6296727259953817}, {"text": "F-score (F)", "start_pos": 124, "end_pos": 135, "type": "METRIC", "confidence": 0.923201397061348}]}, {"text": "We focus on AZP resolution process, where we assume that gold AZPs and gold parse trees are given 3 . The same experimental setting is utilized in).", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.8110135793685913}]}, {"text": "The overall results are shown in, where the performances of each domain are listed in detail and overall performance is also shown in the last column.", "labels": [], "entities": []}, {"text": "\u2022 Overall Performance We employ four Chinese ZP resolution baseline systems on OntoNotes 5.0 dataset.", "labels": [], "entities": [{"text": "OntoNotes 5.0 dataset", "start_pos": 79, "end_pos": 100, "type": "DATASET", "confidence": 0.9517330924669901}]}, {"text": "As we can NW MZ   see that our model significantly outperforms the previous state-of-the-art system (Chen and Ng, 2016) by 3.1% in overall F-score, and substantially outperform the other systems by a large margin.", "labels": [], "entities": [{"text": "F-score", "start_pos": 139, "end_pos": 146, "type": "METRIC", "confidence": 0.9983755350112915}]}, {"text": "When observing the performances of different domains, our approach also gives relatively consistent improvements among various domains, except for BN and TC with a slight drop.", "labels": [], "entities": [{"text": "BN", "start_pos": 147, "end_pos": 149, "type": "METRIC", "confidence": 0.7732425332069397}]}, {"text": "All these results approve that our proposed approach is effective and achieves significant improvements in AZP resolution.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 107, "end_pos": 121, "type": "METRIC", "confidence": 0.8158524036407471}]}, {"text": "In our quantitative analysis, we investigated the reasons of the declines in the BN and TC domain.", "labels": [], "entities": []}, {"text": "A primary observation is that the word distributions in these domains are fairly different from others.", "labels": [], "entities": []}, {"text": "The average document length of BN and TC are quite longer than other domains, which suggest that there is a bigger chance to have unknown words than other domains, and add difficulties to the model training.", "labels": [], "entities": []}, {"text": "Also, we have found that in the BN and TC domains, the texts are often in oral form, which means that there are many irregular expressions in the context.", "labels": [], "entities": []}, {"text": "Such expressions add noise to the model, and it is difficult for the model to extract useful information in these contexts.", "labels": [], "entities": []}, {"text": "These phenomena indicate that further improvements can be obtained by filtering stop words in contexts, or increasing the size of task-specific data, while we leave this in the future work.", "labels": [], "entities": []}, {"text": "\u2022 Effect of UNK processing As we have mentioned in the previous section, traditional unknown word replacing methods are vulnerable to the real word test.", "labels": [], "entities": [{"text": "UNK processing", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.738360732793808}, {"text": "word replacing", "start_pos": 93, "end_pos": 107, "type": "TASK", "confidence": 0.7624719738960266}]}, {"text": "To alleviate this issue, we proposed the UNK processing mechanism to recover the UNK tokens to the real words.", "labels": [], "entities": []}, {"text": "In, we compared the performance that with and without the proposed UNK processing, to show whether the proposed UNK processing method is effective.", "labels": [], "entities": [{"text": "UNK processing", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.6329550743103027}, {"text": "UNK processing", "start_pos": 112, "end_pos": 126, "type": "TASK", "confidence": 0.5537347048521042}]}, {"text": "As we can see that, by applying our UNK processing mechanism, the model do learned the positional features in these lowfrequency words, and brings over 3% improvements in F-score, which indicated the effectiveness of our UNK processing approach.", "labels": [], "entities": [{"text": "F-score", "start_pos": 171, "end_pos": 178, "type": "METRIC", "confidence": 0.9990863800048828}]}], "tableCaptions": [{"text": " Table 1: Statistics of training data, including  pseudo training data and OntoNotes 5.0 training  data.", "labels": [], "entities": [{"text": "OntoNotes 5.0 training  data", "start_pos": 75, "end_pos": 103, "type": "DATASET", "confidence": 0.8578031361103058}]}, {"text": " Table 2: Statistics of test set (OntoNotes 5.0 de- velopment data).", "labels": [], "entities": [{"text": "OntoNotes 5.0 de- velopment data", "start_pos": 34, "end_pos": 66, "type": "METRIC", "confidence": 0.7156759848197302}]}, {"text": " Table 3: Experimental result (F-score) on the OntoNotes 5.0 test data. The best results are marked  with bold face.  \u2020 indicates that our approach is statistical significant over the baselines (using t-test, with  p < 0.05). The number in the brackets indicate the number of AZPs.", "labels": [], "entities": [{"text": "F-score", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9845113754272461}, {"text": "OntoNotes 5.0 test data", "start_pos": 47, "end_pos": 70, "type": "DATASET", "confidence": 0.958748996257782}, {"text": "AZPs", "start_pos": 276, "end_pos": 280, "type": "METRIC", "confidence": 0.9006272554397583}]}, {"text": " Table 5: Performance comparison of using differ- ent training data.", "labels": [], "entities": []}]}