{"title": [{"text": "Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access", "labels": [], "entities": [{"text": "Information Access", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7059418261051178}]}], "abstractContent": [{"text": "This paper proposes KB-InfoBot 1-a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries.", "labels": [], "entities": []}, {"text": "Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge.", "labels": [], "entities": []}, {"text": "Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes.", "labels": [], "entities": []}, {"text": "However, such symbolic operations break the differ-entiability of the system and prevent end-to-end training of neural dialogue agents.", "labels": [], "entities": []}, {"text": "In this paper, we address this limitation by replacing symbolic queries with an induced \"soft\" posterior distribution over the KB that indicates which entities the user is interested in.", "labels": [], "entities": []}, {"text": "Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users.", "labels": [], "entities": []}, {"text": "We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards person-alized dialogue agents.", "labels": [], "entities": []}], "introductionContent": [{"text": "The design of intelligent assistants which interact with users in natural language ranks high on the agenda of current NLP research.", "labels": [], "entities": []}, {"text": "With an increasing focus on the use of statistical and machine learning based approaches ( , the last few years have seen some truly remarkable conversational agents appear on the market (e.g. Apple Siri, Microsoft Cortana, Google Allo).", "labels": [], "entities": []}, {"text": "These agents can perform simple tasks, answer factual questions, and sometimes also aimlessly chit-chat with the user, but they still lag far behind a human assistant in terms of both the variety and complexity of tasks they can perform.", "labels": [], "entities": []}, {"text": "In particular, they lack the ability to learn from interactions with a user in order to improve and adapt with time.", "labels": [], "entities": []}, {"text": "Recently, Reinforcement Learning (RL) has been explored to leverage user interactions to adapt various dialogue agents designed, respectively, for task completion , information access (), and chitchat ( ).", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.7614753365516662}, {"text": "task completion", "start_pos": 147, "end_pos": 162, "type": "TASK", "confidence": 0.7060856819152832}]}, {"text": "We focus on KB-InfoBots, a particular type of dialogue agent that helps users navigate a Knowledge Base (KB) in search of an entity, as illustrated by the example in.", "labels": [], "entities": []}, {"text": "Such agents must necessarily query databases in order to retrieve the requested information.", "labels": [], "entities": []}, {"text": "This is usually done by performing semantic parsing on the input to construct a symbolic query representing the beliefs of the agent about the user goal, such as,, and's work.", "labels": [], "entities": []}, {"text": "We call such an operation a Hard-KB lookup.", "labels": [], "entities": []}, {"text": "While natural, this approach has two drawbacks: (1) the retrieved results do not carry any information about uncertainty in semantic parsing, and (2) the retrieval operation is non differentiable, and hence the parser and dialog policy are trained separately.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.7403514981269836}]}, {"text": "This makes online endto-end learning from user feedback difficult once the system is deployed.", "labels": [], "entities": []}, {"text": "In this work, we propose a probabilistic framework for computing the posterior distribution of the user target over a knowledge base, which we term a Soft-KB lookup.", "labels": [], "entities": []}, {"text": "This distribution is constructed from the agent's belief about the attributes of the entity being searched for.", "labels": [], "entities": []}, {"text": "The dialogue policy network, which decides the next system action, receives as input this full distribution instead of a handful of retrieved results.", "labels": [], "entities": []}, {"text": "We show in our ex-: An interaction between a user looking fora movie and the KB-InfoBot.", "labels": [], "entities": [{"text": "KB-InfoBot", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.9406399130821228}]}, {"text": "An entity-centric knowledge base is shown above the KB-InfoBot (missing values denoted by X).", "labels": [], "entities": []}, {"text": "periments that this framework allows the agent to achieve a higher task success rate in fewer dialogue turns.", "labels": [], "entities": []}, {"text": "Further, the retrieval process is differentiable, allowing us to construct an end-to-end trainable KB-InfoBot, all of whose components are updated online using RL.", "labels": [], "entities": []}, {"text": "Reinforcement learners typically require an environment to interact with, and hence static dialogue corpora cannot be used for their training.", "labels": [], "entities": []}, {"text": "Running experiments on human subjects, on the other hand, is unfortunately too expensive.", "labels": [], "entities": []}, {"text": "A common workaround in the dialogue community) is to instead use user simulators which mimic the behavior of real users in a consistent manner.", "labels": [], "entities": []}, {"text": "For training KB-InfoBot, we adapt the publicly available 2 simulator described in . Evaluation of dialogue agents has been the subject of much research ().", "labels": [], "entities": []}, {"text": "While the metrics for evaluating an InfoBot are relatively clear -the agent should return the correct entity in a minimum number of turns -the environment for testing it not so much.", "labels": [], "entities": []}, {"text": "Unlike previous KB-based QA systems, our focus is on multi-turn interactions, and as such there are no publicly available benchmarks for this problem.", "labels": [], "entities": []}, {"text": "We evaluate several versions of KB-InfoBot with the simulator and on real users, and show that the proposed Soft-KB lookup helps the reinforcement learner discover better dialogue policies.", "labels": [], "entities": []}, {"text": "Initial experiments on the end-to-end agent also demonstrate its strong learning capability.", "labels": [], "entities": []}], "datasetContent": [{"text": "Previous work in KB-based QA has focused on single-turn interactions and is not directly comparable to the present study.", "labels": [], "entities": [{"text": "KB-based QA", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.5573182702064514}]}, {"text": "Instead we compare different versions of the KB-InfoBot described above to test our claims.", "labels": [], "entities": []}, {"text": "We compare each of the discussed versions along three metrics: the average rewards obtained (R), success rate (S) (where success is defined as providing the user target among top R results), and the average number of turns per dialogue (T).", "labels": [], "entities": [{"text": "success rate (S)", "start_pos": 97, "end_pos": 113, "type": "METRIC", "confidence": 0.9609117388725281}]}, {"text": "For the RL and E2E agents, during training we fix the model every 100 updates and run 2000 simulations with greedy action selection to evaluate its performance.", "labels": [], "entities": []}, {"text": "Then after training we select the model with the highest average reward and run a further 5000 simulations and report the performance in.", "labels": [], "entities": []}, {"text": "For reference we also show the performance of an agent which receives perfect information about the user target without any errors, and selects actions based on the entropy of the slots (Max).", "labels": [], "entities": []}, {"text": "This can be considered as an upper bound on the performance of any agent (.", "labels": [], "entities": []}, {"text": "In each case the Soft-KB versions achieve the highest average reward, which is the metric all agents optimize.", "labels": [], "entities": []}, {"text": "In general, the trade-off between minimizing average turns and maximizing success rate can be controlled by changing the reward signal.", "labels": [], "entities": []}, {"text": "Note that, except the E2E version, all versions share the same belief trackers, but by re-asking values of some slots they can have different posteriors pt T to inform the results.", "labels": [], "entities": [{"text": "E2E", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.9196071028709412}]}, {"text": "This shows that having full information about the current state of beliefs over the KB helps the Soft-KB agent discover better policies.", "labels": [], "entities": []}, {"text": "Further, reinforcement learning helps discover better policies than the handcrafted rule-based agents, and we see a higher reward for RL agents compared to Rule ones.", "labels": [], "entities": []}, {"text": "This is due to the noisy natural language inputs; with perfect information the rule-based strategy is optimal.", "labels": [], "entities": []}, {"text": "Interestingly, the RL-Hard agent has the minimum number of turns in 2 out of the 4 settings, at the cost of a lower success rate and average reward.", "labels": [], "entities": []}, {"text": "This agent does not receive any information about the uncertainty in semantic parsing, and it tends to inform as soon as the number of retrieved results becomes small, even if they are incorrect.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.7155351489782333}]}, {"text": "Among the Soft-KB agents, we see that E2E>RL>Rule, except for the X-Large KB.", "labels": [], "entities": []}, {"text": "For E2E, the action space grows exponentially with the size of the KB, and hence credit assignment gets more difficult.", "labels": [], "entities": [{"text": "credit assignment", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.8153505921363831}]}, {"text": "Future work should focus on improving the E2E agent in this setting.", "labels": [], "entities": []}, {"text": "The difficulty of a KB-split depends on number of entities it has, as well as the number of unique values for each slot (more unique values make the problem easier).", "labels": [], "entities": []}, {"text": "Hence we see that both the \"Small\" and \"X-Large\" settings lead to lower reward for the agents, since is small for them.", "labels": [], "entities": []}, {"text": "We further evaluate the KB-InfoBot versions trained using the simulator against real subjects, recruited from the author's affiliations.", "labels": [], "entities": []}, {"text": "In each session, in a typed interaction, the subject was first presented with a target movie from the \"Medium\" KB-split along with a subset of its associated slot-: Sample dialogues between users and the KB-InfoBot (RL-Soft version).", "labels": [], "entities": []}, {"text": "Each turn begins with a user utterance followed by the agent response.", "labels": [], "entities": []}, {"text": "Rank denotes the rank of the target movie in the KB-posterior after each turn.", "labels": [], "entities": []}, {"text": "To simulate the scenario where end-users may not know slot values correctly, the subjects in our evaluation were presented multiple values for the slots from which they could choose anyone while interacting with the agent.", "labels": [], "entities": []}, {"text": "Subjects were asked to initiate the conversation by specifying some of these values, and respond to the agent's subsequent requests, all in natural language.", "labels": [], "entities": []}, {"text": "We test RL-Hard and the three Soft-KB agents in this study, and in each session one of the agents was picked at random for testing.", "labels": [], "entities": []}, {"text": "In total, we collected 433 dialogues, around 20 per subject.", "labels": [], "entities": []}, {"text": "shows a comparison of these agents in terms of success rate and number of turns, and shows some sample dialogues from the user interactions with RL-Soft.", "labels": [], "entities": [{"text": "RL-Soft", "start_pos": 145, "end_pos": 152, "type": "DATASET", "confidence": 0.8497398495674133}]}, {"text": "In comparing Hard-KB versus Soft-KB lookup methods we see that both Rule-Soft and RL-Soft agents achieve a higher success rate than RL-Hard, while E2E-Soft does comparably.", "labels": [], "entities": []}, {"text": "They do so in an increased number of average turns, but achieve a higher average reward as well.", "labels": [], "entities": []}, {"text": "Between RL-Soft and Rule-Soft agents, the success rate is similar, however the RL agent achieves that rate in a lower number of turns on average.", "labels": [], "entities": []}, {"text": "RL-Soft achieves a success rate of 74% on the human evaluation and 80% against the simulated user, indicating minimal overfitting.", "labels": [], "entities": []}, {"text": "However, all agents take a higher number of turns against real users as compared to the simulator, due to the noisier inputs.", "labels": [], "entities": []}, {"text": "The E2E gets the highest success rate against the simulator, however, when tested against real users it performs poorly with a lower success rate and a higher number of turns.", "labels": [], "entities": []}, {"text": "Since it has more trainable components, this agent is also most prone to overfitting.", "labels": [], "entities": []}, {"text": "In particular, the vocabulary of the simulator it is trained against is quite limited (V n = 3078), and hence when real users While its generalization performance is poor, the E2E system also exhibits the strongest learning capability.", "labels": [], "entities": []}, {"text": "In, we compare how different agents perform against the simulator as the temperature of the output softmax in its NLG is increased.", "labels": [], "entities": []}, {"text": "A higher temperature means a more uniform output distribution, which leads to generic simulator responses irrelevant to the agent questions.", "labels": [], "entities": []}, {"text": "This is a simple way of introducing noise in the utterances.", "labels": [], "entities": []}, {"text": "The performance of all agents drops as the temperature is increased, but less so for the E2E agent, which can adapt its belief tracker to the inputs it receives.", "labels": [], "entities": []}, {"text": "Such adaptation is key to the personalization of dialogue agents, which motivates us to introduce the E2E agent.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Movies-KB statistics for four splits. Re- fer to Section 3.2 for description of columns.", "labels": [], "entities": []}, {"text": " Table 2: Performance comparison. Average (\u00b1std error) for 5000 runs after choosing the best model  during training. T: Average number of turns. S: Success rate. R: Average reward.", "labels": [], "entities": [{"text": "Average (\u00b1std error)", "start_pos": 34, "end_pos": 54, "type": "METRIC", "confidence": 0.8015905380249023}, {"text": "Success rate", "start_pos": 148, "end_pos": 160, "type": "METRIC", "confidence": 0.9745316505432129}, {"text": "R", "start_pos": 162, "end_pos": 163, "type": "METRIC", "confidence": 0.9547472596168518}, {"text": "Average reward", "start_pos": 165, "end_pos": 179, "type": "METRIC", "confidence": 0.6969082653522491}]}]}