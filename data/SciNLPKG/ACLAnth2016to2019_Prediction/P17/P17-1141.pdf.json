{"title": [{"text": "Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search", "labels": [], "entities": [{"text": "Sequence Generation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.9627675116062164}]}], "abstractContent": [{"text": "We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints.", "labels": [], "entities": [{"text": "beam search", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.7879619896411896}]}, {"text": "The algorithm can be used with any model that generates a se-quenc\u00ea y = {y 0.", "labels": [], "entities": []}, {"text": ".. y T }, by maximizing p(y|x) = t p(y t |x; {y 0. .. y t\u22121 }).", "labels": [], "entities": []}, {"text": "Lexical constraints take the form of phrases or words that must be present in the output sequence.", "labels": [], "entities": []}, {"text": "This is a very general way to incorporate additional knowledge into a model's output without requiring any modification of the model parameters or training data.", "labels": [], "entities": []}, {"text": "We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation.", "labels": [], "entities": [{"text": "Neural Interactive-Predictive Translation", "start_pos": 110, "end_pos": 151, "type": "TASK", "confidence": 0.7259100675582886}, {"text": "Neural Machine Translation", "start_pos": 186, "end_pos": 212, "type": "TASK", "confidence": 0.6958350539207458}]}, {"text": "Experiments show that GBS can provide large improvements in translation quality in interactive scenarios , and that, even without any user input , GBS can be used to achieve significant gains in performance in domain adaptation scenarios.", "labels": [], "entities": []}], "introductionContent": [{"text": "The output of many natural language processing models is a sequence of text.", "labels": [], "entities": []}, {"text": "Examples include automatic summarization (, machine translation), caption generation (, and dialog generation), among others.", "labels": [], "entities": [{"text": "summarization", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.8534582853317261}, {"text": "machine translation)", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.8427746097246805}, {"text": "caption generation", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.9478358626365662}, {"text": "dialog generation", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.8178672790527344}]}, {"text": "In some real-world scenarios, additional information that could inform the search for the optimal output sequence maybe available at inference time.", "labels": [], "entities": []}, {"text": "Humans can provide corrections after viewing a system's initial output, or separate classification models maybe able to predict parts of the output with high confidence.", "labels": [], "entities": []}, {"text": "When the domain of the input is known, a domain terminology maybe employed to ensure specific phrases are present in a system's predictions.", "labels": [], "entities": []}, {"text": "Our goal in this work is to find away to force the output of a model to contain such lexical constraints, while still taking advantage of the distribution learned from training data.", "labels": [], "entities": []}, {"text": "For Machine Translation (MT) usecases in particular, final translations are often produced by combining automatically translated output with user inputs.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8514137744903565}]}, {"text": "Examples include Post-Editing (PE) and InteractivePredictive MT).", "labels": [], "entities": [{"text": "InteractivePredictive MT", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.5828769952058792}]}, {"text": "These interactive scenarios can be unified by considering user inputs to be lexical constraints which guide the search for the optimal output sequence.", "labels": [], "entities": []}, {"text": "In this paper, we formalize the notion of lexical constraints, and propose a decoding algorithm which allows the specification of subsequences that are required to be present in a model's output.", "labels": [], "entities": []}, {"text": "Individual constraints maybe single tokens or multi-word phrases, and any number of constraints maybe specified simultaneously.", "labels": [], "entities": []}, {"text": "Although we focus upon interactive applications for MT in our experiments, lexically constrained decoding is relevant to any scenario where a model is asked to generate a sequenc\u00easequenc\u00ea y = {y 0 . .", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9920583367347717}]}, {"text": "y T } given both an input x, and a set {c 0 ...c n }, where each c i is a sub-sequence {c i0 . .", "labels": [], "entities": []}, {"text": "c ij }, that must appear somewhere in\u02c6yin\u02c6 in\u02c6y.", "labels": [], "entities": []}, {"text": "This makes our work applicable to a wide range of text generation scenarios, including image description, dialog generation, abstractive summarization, and question answering.", "labels": [], "entities": [{"text": "text generation", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7206764966249466}, {"text": "image description", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7464030086994171}, {"text": "dialog generation", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.8254617750644684}, {"text": "abstractive summarization", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.5580379962921143}, {"text": "question answering", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.8871996402740479}]}, {"text": "The rest of this paper is organized as follows: Section 2 gives the necessary background for our discussion of GBS, Section 3 discusses the lexically constrained decoding algorithm in detail, Section 4 presents our experiments, and Section 5 gives an overview of closely related work.", "labels": [], "entities": [{"text": "GBS", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.5606826543807983}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for four simulated editing cycles using WMT test data. EN-DE uses newstest2013, EN-FR uses newstest2014,  and EN-PT uses the Autodesk corpus discussed in Section 4.2. Improvement in BLEU score over the previous cycle is shown  in parentheses. * indicates use of our test corpus created from Autodesk post-editing data.", "labels": [], "entities": [{"text": "WMT test data", "start_pos": 58, "end_pos": 71, "type": "DATASET", "confidence": 0.9172943035761515}, {"text": "Autodesk corpus", "start_pos": 143, "end_pos": 158, "type": "DATASET", "confidence": 0.857464462518692}, {"text": "BLEU score", "start_pos": 200, "end_pos": 210, "type": "METRIC", "confidence": 0.9781723618507385}]}]}