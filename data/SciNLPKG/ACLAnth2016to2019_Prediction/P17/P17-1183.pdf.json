{"title": [{"text": "Morphological Inflection Generation with Hard Monotonic Attention", "labels": [], "entities": [{"text": "Morphological Inflection Generation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7356540163358053}]}], "abstractContent": [{"text": "We present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection.", "labels": [], "entities": [{"text": "morphological inflection generation", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.7163229584693909}]}, {"text": "We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and non-neural approaches.", "labels": [], "entities": []}, {"text": "Finally we present an analysis of the continuous representations learned by both the hard and soft attention (Bahdanau et al., 2015) models for the task, shedding some light on the features such models extract.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphological inflection generation involves generating a target word (e.g. \"h\u00e4rtestem\", the German word for \"hardest\"), given a source word (e.g. \"hart\", the German word for \"hard\") and the morpho-syntactic attributes of the target (POS=adjective, gender=masculine, type=superlative, etc.).", "labels": [], "entities": [{"text": "Morphological inflection generation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8295297821362814}]}, {"text": "The task is important for many down-stream NLP tasks such as machine translation, especially for dealing with data sparsity in morphologically rich languages where a lemma can be inflected into many different word forms.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8275077044963837}]}, {"text": "Several studies have shown that translating into lemmas in the target language and then applying inflection generation as a post-processing step is beneficial for phrase-based machine translation ( and more recently for neural machine translation.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 163, "end_pos": 195, "type": "TASK", "confidence": 0.672185480594635}, {"text": "neural machine translation", "start_pos": 220, "end_pos": 246, "type": "TASK", "confidence": 0.6599920491377512}]}, {"text": "The task was traditionally tackled with hand engineered finite state transducers (FST) which rely on expert knowledge, or using trainable weighted finite state transducers () which combine expert knowledge with datadriven parameter tuning.", "labels": [], "entities": []}, {"text": "Many other machinelearning based methods) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence.", "labels": [], "entities": []}, {"text": "More recently, the task was modeled as neural sequence-to-sequence learning over character sequences with impressive results.", "labels": [], "entities": []}, {"text": "The vanilla encoder-decoder models as used by the input sequence to a single, fixed-sized continuous representation.", "labels": [], "entities": []}, {"text": "Instead, the soft-attention based sequence to sequence learning paradigm (  allows directly conditioning on the entire input sequence representation, and was utilized for morphological inflection generation with great success (.", "labels": [], "entities": [{"text": "morphological inflection generation", "start_pos": 171, "end_pos": 206, "type": "TASK", "confidence": 0.6812742948532104}]}, {"text": "However, the neural sequence-to-sequence models require large training sets in order to perform well: their performance on the relatively small CELEX dataset is inferior to the latent variable WFST model of.", "labels": [], "entities": [{"text": "CELEX dataset", "start_pos": 144, "end_pos": 157, "type": "DATASET", "confidence": 0.9701553285121918}]}, {"text": "Interestingly, the neural WFST model by also suffered from the same issue on the CELEX dataset, and surpassed the latent variable model only when given twice as much data to train on.", "labels": [], "entities": [{"text": "CELEX dataset", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9902357459068298}]}, {"text": "We propose a model which handles the above issues by directly modeling an almost monotonic alignment between the input and output character sequences, which is commonly found in the morphological inflection generation task (e.g. in languages with concatenative morphology).", "labels": [], "entities": [{"text": "morphological inflection generation task", "start_pos": 182, "end_pos": 222, "type": "TASK", "confidence": 0.7831016629934311}]}, {"text": "The model consists of an encoder-decoder neural network with a dedicated control mechanism: in each step, the model attends to a single input state and either writes a symbol to the output sequence or advances the attention pointer to the next state from the bi-directionally encoded sequence, as described visually in.", "labels": [], "entities": []}, {"text": "This modeling suits the natural monotonic alignment between the input and output, as the network learns to attend to the relevant inputs before writing the output which they are aligned to.", "labels": [], "entities": []}, {"text": "The encoder is a bi-directional RNN, where each character in the input word is represented using a concatenation of a forward RNN and a backward RNN states over the word's characters.", "labels": [], "entities": []}, {"text": "The combination of the bi-directional encoder and the controllable hard attention mechanism enables to condition the output on the entire input sequence.", "labels": [], "entities": []}, {"text": "Moreover, since each character representation is aware of the neighboring characters, nonmonotone relations are also captured, which is important in cases where segments in the output word area result of long range dependencies in the input word.", "labels": [], "entities": []}, {"text": "The recurrent nature of the decoder, together with a dedicated feedback connection that passes the last prediction to the next decoder step explicitly, enables the model to also condition the current output on all the previous outputs at each prediction step.", "labels": [], "entities": []}, {"text": "The hard attention mechanism allows the network to jointly align and transduce while using a focused representation at each step, rather then the weighted sum of representations used in the soft attention model.", "labels": [], "entities": []}, {"text": "This makes our model Resolution Preserving ( while also keeping decoding time linear in the output sequence length rather than multiplicative in the input and output lengths as in the softattention model.", "labels": [], "entities": [{"text": "Resolution Preserving", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.8954643309116364}]}, {"text": "In contrast to previous sequenceto-sequence work, we do not require the training procedure to also learn the alignment.", "labels": [], "entities": []}, {"text": "Instead, we use a simple training procedure which relies on independently learned character-level alignments, from which we derive gold transduction+control sequences.", "labels": [], "entities": []}, {"text": "The network can then be trained using straightforward cross-entropy loss.", "labels": [], "entities": []}, {"text": "To evaluate our model, we perform extensive experiments on three previously studied morphological inflection generation datasets: the CELEX dataset (, the Wiktionary dataset and the SIG-MORPHON2016 dataset ( . We show that while our model is on par with or better than the previous neural and non-neural state-of-the-art approaches, it also performs significantly better with very small training sets, being the first neural model to surpass the performance of the weighted FST model with latent variables which was specifically tailored for the task by.", "labels": [], "entities": [{"text": "CELEX dataset", "start_pos": 134, "end_pos": 147, "type": "DATASET", "confidence": 0.9776766896247864}, {"text": "Wiktionary dataset", "start_pos": 155, "end_pos": 173, "type": "DATASET", "confidence": 0.8587981760501862}, {"text": "SIG-MORPHON2016 dataset", "start_pos": 182, "end_pos": 205, "type": "DATASET", "confidence": 0.7892851233482361}]}, {"text": "Finally, we analyze and compare our model and the soft attention model, showing how they function very similarly with respect to the alignments and representations they learn, in spite of our model being much simpler.", "labels": [], "entities": []}, {"text": "This analysis also sheds light on the representations such models learn for the morphological inflection generation task, showing how they encode specific features like a symbol's type and the symbol's location in a sequence.", "labels": [], "entities": [{"text": "morphological inflection generation task", "start_pos": 80, "end_pos": 120, "type": "TASK", "confidence": 0.7594305276870728}]}, {"text": "To summarize, our contributions in this paper are three-fold: 1.", "labels": [], "entities": []}, {"text": "We present a hard attention model for nearlymonotonic sequence to sequence learning, as common in the morphological inflection setting.", "labels": [], "entities": []}, {"text": "2. We evaluate the model on the task of morphological inflection generation, establishing anew state of the art on three previouslystudied datasets for the task.", "labels": [], "entities": [{"text": "morphological inflection generation", "start_pos": 40, "end_pos": 75, "type": "TASK", "confidence": 0.7616867621739706}]}, {"text": "3. We perform an analysis and comparison of our model and the soft-attention model, shedding light on the features such models extract for the inflection generation task.", "labels": [], "entities": [{"text": "inflection generation task", "start_pos": 143, "end_pos": 169, "type": "TASK", "confidence": 0.8105223973592123}]}], "datasetContent": [{"text": "We perform extensive experiments with three previously studied morphological inflection generation datasets to evaluate our hard attention model in various settings.", "labels": [], "entities": []}, {"text": "In all experiments we compare our hard attention model to the best performing neural and non-neural models which were previously published on those datasets, and to our implementation of the global (soft) attention model presented by which we train with identical hyper-parameters as our hardattention model.", "labels": [], "entities": []}, {"text": "The implementation details for our models are described in the supplementary material section of this paper.", "labels": [], "entities": []}, {"text": "The source code and data for our models is available on github.", "labels": [], "entities": []}, {"text": "CELEX Our first evaluation is on a very small dataset, to see if our model indeed avoids the tendency to overfit with small training sets.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9592832922935486}]}, {"text": "We report exact match accuracy on the German inflection generation dataset compiled by from the CELEX database ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.750751256942749}, {"text": "German inflection generation dataset compiled", "start_pos": 38, "end_pos": 83, "type": "DATASET", "confidence": 0.8047215342521667}, {"text": "CELEX database", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.980660080909729}]}, {"text": "The dataset includes only 500 training examples for each of the four inflection types: 13SIA\u219213SKE, 2PIE\u219213PKE, 2PKE\u2192z, and rP\u2192pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.", "labels": [], "entities": []}, {"text": "We first compare our model to three competitive models from the literature that reported results on this dataset: the Morphological Encoder-Decoder (MED) of  Wiktionary To neutralize the negative effect of very small training sets on the performance of the different learning approaches, we also evaluate our model on the dataset created by, which contains up to 360k training examples per language.", "labels": [], "entities": []}, {"text": "It was built by extracting Finnish, German and Spanish inflection tables from Wiktionary, used in order to evaluate their system based on string alignments and a semi-CRF sequence classifier with linguistically inspired features, which we use a baseline.", "labels": [], "entities": []}, {"text": "We also used the dataset expansion made by to include French and Dutch inflections as well.", "labels": [], "entities": []}, {"text": "Their system also performs an alignand-transduce approach, extracting rules from the aligned training set and applying them in inference time with a proprietary character sequence classifier.", "labels": [], "entities": []}, {"text": "In addition to those systems we also compare to the results of the recent neural approach of, which did not use an attention mechanism, and, which coupled the alignment and transduction tasks.", "labels": [], "entities": []}, {"text": "SIGMORPHON As different languages show different morphological phenomena, we also experiment with how our model copes with these various phenomena using the morphological inflection dataset from the SIGMORPHON2016 shared task ( . Here the training data consists often languages, with five morphological system types (detailed in): Russian (RU), German (DE), Spanish (ES), Georgian (GE), Finnish (FI), Turkish (TU), Arabic (AR), Navajo (NA), Hungarian (HU) and Maltese (MA) with roughly 12,800 training and 1600 development examples per language.", "labels": [], "entities": [{"text": "MA", "start_pos": 469, "end_pos": 471, "type": "METRIC", "confidence": 0.908524751663208}]}, {"text": "We compare our model to two soft attention baselines on this dataset: MED (Kann and Sch\u00fctze, 2016b), which was the best participating system in the shared task, and our implementation of the global (soft) attention model presented by.", "labels": [], "entities": [{"text": "MED", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.8294395208358765}]}], "tableCaptions": [{"text": " Table 1: Results on the CELEX dataset", "labels": [], "entities": [{"text": "CELEX dataset", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.9401147663593292}]}, {"text": " Table 3: Results on the SIGMORPHON 2016 morphological inflection dataset. The text above each lan- guage lists the morphological phenomena it includes: circ.=circumfixing, agg.=agglutinative, v.h.=vowel  harmony, c.h.=consonant harmony", "labels": [], "entities": [{"text": "SIGMORPHON 2016 morphological inflection dataset", "start_pos": 25, "end_pos": 73, "type": "DATASET", "confidence": 0.6269359767436982}]}]}