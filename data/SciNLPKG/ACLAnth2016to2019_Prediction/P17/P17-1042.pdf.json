{"title": [{"text": "Learning bilingual word embeddings with (almost) no bilingual data", "labels": [], "entities": []}], "abstractContent": [{"text": "Most methods to learn bilingual word em-beddings rely on large parallel corpora, which is difficult to obtain for most language pairs.", "labels": [], "entities": []}, {"text": "This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead.", "labels": [], "entities": []}, {"text": "In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique.", "labels": [], "entities": []}, {"text": "Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multilingual word embeddings have attracted a lot of attention in recent times.", "labels": [], "entities": [{"text": "Multilingual word embeddings", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6717984775702158}]}, {"text": "In addition to having a direct application in inherently crosslingual tasks like machine translation () and crosslingual entity linking, they provide an excellent mechanism for transfer learning, where a model trained in a resource-rich language is transferred to a less-resourced one, as shown with part-of-speech tagging (, parsing) and document classification (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7733790874481201}, {"text": "crosslingual entity linking", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.6491334637006124}, {"text": "transfer learning", "start_pos": 177, "end_pos": 194, "type": "TASK", "confidence": 0.943400114774704}, {"text": "part-of-speech tagging", "start_pos": 300, "end_pos": 322, "type": "TASK", "confidence": 0.7122282087802887}, {"text": "document classification", "start_pos": 339, "end_pos": 362, "type": "TASK", "confidence": 0.7556252479553223}]}, {"text": "Most methods to learn these multilingual word embeddings make use of large parallel corpora (, but there have been several proposals to relax this requirement, given its scarcity inmost language pairs.", "labels": [], "entities": []}, {"text": "A possible relaxation is to use document-aligned or label-aligned comparable corpora (, but large amounts of such corpora are not always available for some language pairs.", "labels": [], "entities": []}, {"text": "An alternative approach that we follow here is to independently train the embeddings for each language on monolingual corpora, and then learn a linear transformation to map the embeddings from one space into the other by minimizing the distances in a bilingual dictionary, usually in the range of a few thousand entries (.", "labels": [], "entities": []}, {"text": "However, dictionaries of that size are not readily available for many language pairs, specially those involving less-resourced languages.", "labels": [], "entities": []}, {"text": "In this work, we reduce the need of large bilingual dictionaries to much smaller seed dictionaries.", "labels": [], "entities": []}, {"text": "Our method can work with as little as 25 word pairs, which are straightforward to obtain assuming some basic knowledge of the languages involved.", "labels": [], "entities": []}, {"text": "The method can also work with trivially generated seed dictionaries of numerals (i.e. 1-1, 2-2, 3-3, 4-4...) making it possible to learn bilingual word embeddings without any real bilingual data.", "labels": [], "entities": []}, {"text": "In either case, we obtain very competitive results, comparable to other state-of-the-art methods that make use of much richer bilingual resources.", "labels": [], "entities": []}, {"text": "The proposed method is an extension of existing mapping techniques, where the dictionary is used to learn the embedding mapping and the embedding mapping is used to induce anew dictionary iteratively in a self-learning fashion (see.", "labels": [], "entities": []}, {"text": "In spite of its simplicity, our analysis of the implicit optimization objective reveals that the method is exploiting the structural similarity of independently trained embeddings.", "labels": [], "entities": []}, {"text": "We analyze previous work in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 describes the self-learning framework, while Section 4 presents the experiments.", "labels": [], "entities": []}, {"text": "Section 5 analyzes the underlying optimization objective, and Section 6 presents an error analysis.", "labels": [], "entities": [{"text": "error", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.9520471096038818}]}, {"text": "XW Z X: A general schema of the proposed self-learning framework.", "labels": [], "entities": []}, {"text": "Previous works learn a mapping W based on the seed dictionary D, which is then used to learn the full dictionary.", "labels": [], "entities": []}, {"text": "In our proposal we use the new dictionary to learn anew mapping, iterating until convergence.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we experimentally test the proposed method in bilingual lexicon induction and crosslingual word similarity.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.7093252539634705}, {"text": "crosslingual word similarity", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.6722782353560129}]}, {"text": "Subsection 4.1 describes the experimental settings, while Subsections 4.2 and 4.3 present the results obtained in each of the tasks.", "labels": [], "entities": []}, {"text": "The code and resources necessary to reproduce our experiments are available at https://github.com/artetxem/ vecmap.", "labels": [], "entities": []}, {"text": "For easier comparison with related work, we evaluated our mappings on bilingual lexicon induction using the public English-Italian dataset by, which includes monolingual word embeddings in both languages together with a bilingual dictionary split in a training set and a test set 2 . The embeddings were trained with the word2vec toolkit with CBOW and negative sampling (Mikolov et al., 2013b) 3 , using a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC) and a 1.6 billion word corpus for Italian (itWaC).", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.6791050632794698}, {"text": "ukWaC + Wikipedia + BNC", "start_pos": 443, "end_pos": 466, "type": "DATASET", "confidence": 0.8284887671470642}]}, {"text": "The training and test sets were derived from a dictionary built form Europarl word alignments and available at OPUS (Tiedemann, 2012), taking 1,500 random entries uniformly distributed in 5 frequency bins as the test set and the 5,000 most frequent of the remaining word pairs as the training set.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.9781215190887451}, {"text": "OPUS (Tiedemann, 2012)", "start_pos": 111, "end_pos": 133, "type": "DATASET", "confidence": 0.9028107424577078}]}, {"text": "In addition to English-Italian, we selected two other languages from different language families with publicly available resources.", "labels": [], "entities": []}, {"text": "We thus created analogous datasets for English-German and English-Finnish.", "labels": [], "entities": []}, {"text": "In the case of German, the embeddings were trained on the 0.9 billion word corpus SdeWaC, which is part of the WaCky collection () that was also used for English and Italian.", "labels": [], "entities": [{"text": "WaCky collection", "start_pos": 111, "end_pos": 127, "type": "DATASET", "confidence": 0.8292393386363983}]}, {"text": "Given that Finnish is not included in this collection, we used the 2.8 billion word Common Crawl corpus provided at WMT 2016 4 instead, which we tokenized using the Stanford Tokenizer ().", "labels": [], "entities": [{"text": "Common Crawl corpus provided at WMT 2016 4", "start_pos": 84, "end_pos": 126, "type": "DATASET", "confidence": 0.8468552231788635}, {"text": "Stanford Tokenizer", "start_pos": 165, "end_pos": 183, "type": "DATASET", "confidence": 0.9069189131259918}]}, {"text": "In addition to that, we created training and test sets for both pairs from their respective Europarl dictionaries from OPUS following the exact same procedure used for English-Italian, and the word embeddings were also trained using the same configuration as.", "labels": [], "entities": [{"text": "Europarl dictionaries from OPUS", "start_pos": 92, "end_pos": 123, "type": "DATASET", "confidence": 0.9077381789684296}]}, {"text": "Given that the main focus of our work is on small seed dictionaries, we created random subsets of 2,500, 1,000, 500, 250, 100, 75, 50 and 25 entries from the original training dictionaries of 5,000 entries.", "labels": [], "entities": []}, {"text": "This was done by shuffling once the training dictionaries and taking their first k entries, so it is guaranteed that each dictionary is a strict subset of the bigger dictionaries.", "labels": [], "entities": []}, {"text": "In addition to that, we explored using automatically generated dictionaries as a shortcut to practical unsupervised learning.", "labels": [], "entities": []}, {"text": "For that purpose, we created numeral dictionaries, consisting of words matching the [0-9]+ regular expression in both vocabularies (e.g. 1-1, 2-2, 3-3, 1992-1992 etc.).", "labels": [], "entities": []}, {"text": "The resulting dictionary had 2772 entries for English-Italian, 2148 for English-German, and 2345 for English-Finnish.", "labels": [], "entities": []}, {"text": "While more sophisticated approaches are possible (e.g. involving the edit distance of all words), we believe that this method is general enough that should work with practically any language pair, as Arabic numerals are often used even in languages with a different writing system (e.g. Chinese and Russian).", "labels": [], "entities": []}, {"text": "While bilingual lexicon induction is a standard evaluation task for seed dictionary based methods like ours, it is unsuitable for bilingual corpus based methods, as statistical word alignment already provides a reliable way to derive dictionaries from bilingual corpora and, in fact, this is how the test dictionary itself is builtin our case.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 6, "end_pos": 33, "type": "TASK", "confidence": 0.7099752028783163}, {"text": "statistical word alignment", "start_pos": 165, "end_pos": 191, "type": "TASK", "confidence": 0.6476283172766367}]}, {"text": "For that reason, we carried out some experiments in crosslingual word similarity as away to test our method in a different task and allowing to compare it to systems that use richer bilingual data.", "labels": [], "entities": [{"text": "crosslingual word similarity", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.7141259213288625}]}, {"text": "There are no many crosslingual word similarity datasets, and we used the RG-65 and WordSim-353 crosslingual datasets for English-German and the WordSim-353 crosslingual dataset for EnglishItalian as published by . As for the convergence criterion, we decide to stop training when the improvement on the average dot product for the induced dictionary falls below a given threshold from one iteration to the next.", "labels": [], "entities": [{"text": "RG-65", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.909436047077179}, {"text": "WordSim-353 crosslingual datasets", "start_pos": 83, "end_pos": 116, "type": "DATASET", "confidence": 0.8815093636512756}, {"text": "WordSim-353 crosslingual dataset", "start_pos": 144, "end_pos": 176, "type": "DATASET", "confidence": 0.866778572400411}]}, {"text": "After length normalization, the dot product ranges from -1 to 1, so we decide to set this threshold at 1e-6, which we find to be a very conservative value yet enough that training takes a reasonable amount of time.", "labels": [], "entities": []}, {"text": "The curves in the next section confirm that this was a reasonable choice.", "labels": [], "entities": []}, {"text": "This convergence criterion is usually met in less than 100 iterations, each of them taking 5 minutes on a modest desktop computer (Intel Core i5-4670 CPU with 8GiB of RAM), including the induction of a dictionary of 200,000 words at each iteration.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy (%) on bilingual lexicon induction for different seed dictionaries", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991724491119385}]}, {"text": " Table 2: Spearman correlations on English-Italian  and English-German crosslingual word similarity", "labels": [], "entities": [{"text": "crosslingual word similarity", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.6408378581206003}]}]}