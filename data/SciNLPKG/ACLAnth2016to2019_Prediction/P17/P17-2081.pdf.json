{"title": [{"text": "Question Answering through Transfer Learning from Large Fine-grained Supervision Data", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8508966267108917}]}], "abstractContent": [{"text": "We show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.8737058401107788}]}, {"text": "We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.7148587554693222}, {"text": "WikiQA", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.9171034693717957}]}, {"text": "For WikiQA, our model outperforms the previous best model by more than 8%.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.8893734812736511}]}, {"text": "We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision , through quantitative results and visual analysis.", "labels": [], "entities": []}, {"text": "We also show that a similar transfer learning procedure achieves the state of the art on an entailment task.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.8824526071548462}]}], "introductionContent": [{"text": "Question answering (QA) is a long-standing challenge in NLP, and the community has introduced several paradigms and datasets for the task over the past few years.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9397137999534607}]}, {"text": "These paradigms differ from each other in the type of questions and answers and the size of the training data, from a few hundreds to millions of examples.", "labels": [], "entities": []}, {"text": "We are particularly interested in the contextaware QA paradigm, where the answer to each question can be obtained by referring to its accompanying context (paragraph or a list of sentences).", "labels": [], "entities": []}, {"text": "Under this setting, the two most notable types of supervisions are coarse sentence-level and finegrained span-level.", "labels": [], "entities": []}, {"text": "In sentence-level QA, the task is to pick sentences that are most relevant to the question among a list of candidates (.", "labels": [], "entities": []}, {"text": "In span-level QA, the task is to locate the * All work was done while the author was an exchange student at University of smallest span in the given paragraph that answers the question (.", "labels": [], "entities": []}, {"text": "In this paper, we address coarser, sentencelevel QA through a standard transfer learning 1 technique of a model trained on a large, spansupervised QA dataset.", "labels": [], "entities": []}, {"text": "We demonstrate that the target task not only benefits from the scale of the source dataset but also the capability of the finegrained span supervision to better learn syntactic and lexical information.", "labels": [], "entities": []}, {"text": "For the source dataset, we pretrain on SQuAD (), a recentlyreleased, span-supervised QA dataset.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 85, "end_pos": 95, "type": "DATASET", "confidence": 0.6646771430969238}]}, {"text": "For the source and target models, we adopt BiDAF (, one of the top-performing models in the dataset's leaderboard.", "labels": [], "entities": [{"text": "BiDAF", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9470527768135071}]}, {"text": "For the target datasets, we evaluate on two recent QA datasets, WikiQA ( and SemEval 2016 (Task 3A) (, which possess sufficiently different characteristics from that of SQuAD.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.8471093475818634}, {"text": "WikiQA", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.94132000207901}]}, {"text": "Our results show 8% improvement in WikiQA and 1% improevement in SemEval.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.7911628484725952}, {"text": "improevement", "start_pos": 49, "end_pos": 61, "type": "METRIC", "confidence": 0.9951310157775879}]}, {"text": "In addition, we report state-of-the-art results on recognizing textual entailment (RTE) in SICK () with a similar transfer learning procedure.", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE)", "start_pos": 51, "end_pos": 87, "type": "TASK", "confidence": 0.7198463280995687}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results on WikiQA and SemEval-2016 (Task 3A).", "labels": [], "entities": []}, {"text": " Table 3: Results with varying sizes of SQuAD dataset used", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.7601868212223053}]}, {"text": " Table 4: Results on SICK after finetuning. The first row is", "labels": [], "entities": [{"text": "SICK", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.889499306678772}]}, {"text": " Table 7: Comparison of ranked answers by SQuAD-T-", "labels": [], "entities": []}, {"text": " Table 9: Comparison of performance of model with-", "labels": [], "entities": []}]}