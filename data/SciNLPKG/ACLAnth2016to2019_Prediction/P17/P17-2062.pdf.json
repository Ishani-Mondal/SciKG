{"title": [{"text": "Efficient Extraction of Pseudo-Parallel Sentences from Raw Monolingual Data Using Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose anew method for extracting pseudo-parallel sentences from a pair of large monolingual corpora, without relying on any document-level information.", "labels": [], "entities": []}, {"text": "Our method first exploits word embed-dings in order to efficiently evaluate trillions of candidate sentence pairs and then a classifier to find the most reliable ones.", "labels": [], "entities": []}, {"text": "We report significant improvements in domain adaptation for statistical machine translation when using a translation model trained on the sentence pairs extracted from in-domain monolingual corpora.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.71678626537323}, {"text": "statistical machine translation", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.6812441448370615}]}], "introductionContent": [{"text": "Parallel corpus is an indispensable resource for statistical and neural machine translation.", "labels": [], "entities": [{"text": "statistical and neural machine translation", "start_pos": 49, "end_pos": 91, "type": "TASK", "confidence": 0.7231531381607056}]}, {"text": "Generally, using more sentence pairs to train a translation system makes it able to produce better translations.", "labels": [], "entities": []}, {"text": "However, for most language pairs and domains, parallel corpora remain scarce due mainly to the cost of their creation).", "labels": [], "entities": []}, {"text": "In the last two decades, numerous methods have been proposed to extract parallel sentences from comparable corpora.", "labels": [], "entities": []}, {"text": "In addition to comparable corpora in large quantity, to the best of our knowledge, all previous methods heavily rely on document-level information and/or lexical translation models, such as those for statistical machine translation (SMT) systems) and manually-created bilingual lexicon.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 200, "end_pos": 237, "type": "TASK", "confidence": 0.7846915622552236}]}, {"text": "The most successful approaches use cross-lingual information retrieval techniques) to extract sentence pairs from comparable documents.", "labels": [], "entities": []}, {"text": "Using such document pairs has the strong advantage that it drastically reduces the search space; we need to consider only sentence pairs in each document pair instead of scoring all sentence pairs in the two monolingual corpora.", "labels": [], "entities": []}, {"text": "However, in many cases, we do not have access to document-level information.", "labels": [], "entities": []}, {"text": "Only Tillmann and have explored this scenario using efficient caching strategies to extract useful sentence pairs from nearly one trillion candidates in comparable data.", "labels": [], "entities": []}, {"text": "Yet, their approach is tightly related to the exploitation of accurate lexical translation models and does not allow us to introduce other features.", "labels": [], "entities": []}, {"text": "The reliance on lexical translation models implies that we must have already access to parallel data sufficiently large for obtaining accurate estimates.", "labels": [], "entities": []}, {"text": "Nevertheless, the most useful sentence pairs for SMT are actually the ones that contain infrequent or even unseen tokens in these parallel data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9956104159355164}]}, {"text": "Relying only on lexical translation models thus seems rather inadequate to extract sentence pairs containing numerous infrequent or unseen tokens, and may actually be more prone to extract sentence pairs that contain words and phrases for which we already have accurate translation probability estimates.", "labels": [], "entities": []}, {"text": "This paper proposes anew method that exploits word embeddings to efficiently extract pseudoparallel sentences 1 from raw monolingual data without using any document-level information.", "labels": [], "entities": []}, {"text": "We report significant improvements of translation quality in a domain adaptation scenario for SMT.", "labels": [], "entities": [{"text": "translation", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.959613025188446}, {"text": "SMT", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.9923908114433289}]}], "datasetContent": [{"text": "We evaluated our method in a scenario of domain adaptation for phrase-based SMT (PBSMT).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7286677658557892}, {"text": "phrase-based SMT (PBSMT)", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.6702008247375488}]}, {"text": "In this scenario, we assumed a lot of general-domain parallel data to train a general-domain phrase table and a lot of in-domain monolingual data as our source of in-domain pseudo-parallel sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores (", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9538095593452454}]}, {"text": " Table 2: Results (BLEU) obtained without us- ing some of the features during the classification  (see Section 2.2). The features removed, indepen- dently, are the following: averaged word embed- dings (avg. emb.), maximum alignment between  embeddings (max. al. emb.), lexical translation  probabilities (lex. prob.) and the length ratio of  the source and target sentences (length). The \"th\"  column indicates the threshold value for the clas- sifier's score above which we retain the sentence  pairs. This value was selected among the val- ues {0.5,0.6,0.7,0.8,0.9} with respect to the BLEU  score on the development data, through the tuning  of the PBSMT system, for each configuration.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9955993890762329}, {"text": "val- ues", "start_pos": 538, "end_pos": 546, "type": "METRIC", "confidence": 0.9368073145548502}, {"text": "BLEU", "start_pos": 589, "end_pos": 593, "type": "METRIC", "confidence": 0.9976792931556702}]}]}