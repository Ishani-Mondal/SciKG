{"title": [{"text": "A Generative Parser with a Discriminative Recognition Algorithm", "labels": [], "entities": []}], "abstractContent": [{"text": "Generative models defining joint distributions over parse trees and sentences are useful for parsing and language modeling, but impose restrictions on the scope of features and are often outperformed by dis-criminative models.", "labels": [], "entities": [{"text": "parsing", "start_pos": 93, "end_pos": 100, "type": "TASK", "confidence": 0.9842056035995483}, {"text": "language modeling", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.6529050171375275}]}, {"text": "We propose a framework for parsing and language modeling which marries a generative model with a discriminative recognition model in an encoder-decoder setting.", "labels": [], "entities": [{"text": "parsing and language modeling", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.6396291702985764}]}, {"text": "We provide interpretations of the framework based on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 130, "end_pos": 137, "type": "TASK", "confidence": 0.963722288608551}]}, {"text": "On the English Penn Treen-bank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art single-model language modeling score.", "labels": [], "entities": [{"text": "English Penn Treen-bank", "start_pos": 7, "end_pos": 30, "type": "DATASET", "confidence": 0.9690582553545634}, {"text": "constituency parsing", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.7676289975643158}]}], "introductionContent": [{"text": "Generative models defining joint distributions over parse trees and sentences are good theoretical models for interpreting natural language data, and appealing tools for tasks such as parsing, grammar induction and language modeling.", "labels": [], "entities": [{"text": "interpreting natural language", "start_pos": 110, "end_pos": 139, "type": "TASK", "confidence": 0.8521743218104044}, {"text": "parsing", "start_pos": 184, "end_pos": 191, "type": "TASK", "confidence": 0.9734062552452087}, {"text": "grammar induction", "start_pos": 193, "end_pos": 210, "type": "TASK", "confidence": 0.7053605914115906}, {"text": "language modeling", "start_pos": 215, "end_pos": 232, "type": "TASK", "confidence": 0.7574435770511627}]}, {"text": "However, they often impose strong independence assumptions which restrict the use of arbitrary features for effective disambiguation.", "labels": [], "entities": []}, {"text": "Moreover, generative parsers are typically trained by maximizing the joint probability of the parse tree and the sentence-an objective that only indirectly relates to the goal of parsing.", "labels": [], "entities": [{"text": "generative parsers", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9590571522712708}]}, {"text": "At test time, these models require a relatively expensive recognition algo-rithm to recover the parse tree, but the parsing performance consistently lags behind their discriminative competitors, which are directly trained to maximize the conditional probability of the parse tree given the sentence, where linear-time decoding algorithms exist (e.g., for transition-based parsers).", "labels": [], "entities": []}, {"text": "In this work, we propose a parsing and language modeling framework that marries a generative model with a discriminative recognition algorithm in order to have the best of both worlds.", "labels": [], "entities": [{"text": "parsing and language modeling", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.6449368894100189}]}, {"text": "The idea of combining these two types of models is not new.", "labels": [], "entities": []}, {"text": "For example, propose to use a generative model to generate candidate constituency trees and a discriminative model to rank them.", "labels": [], "entities": []}, {"text": "follow the opposite direction and employ a generative model to re-rank the dependency trees produced by a discriminative parser.", "labels": [], "entities": []}, {"text": "However, previous work combines the two types of models in a goal-oriented, pipeline fashion, which lacks model interpretations and focuses solely on parsing.", "labels": [], "entities": []}, {"text": "In comparison, our framework unifies generative and discriminative parsers with a single objective, which connects to expectation maximization and variational inference in grammar induction settings.", "labels": [], "entities": [{"text": "generative and discriminative parsers", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.8007068037986755}]}, {"text": "Ina nutshell, we treat parse trees as latent factors generating natural language sentences and parsing as a posterior inference task.", "labels": [], "entities": []}, {"text": "We showcase the framework using Recurrent Neural Network Grammars (RNNGs;), a recently proposed probabilistic model of phrase-structure trees based on neural transition systems.", "labels": [], "entities": []}, {"text": "Different from this work which introduces separately trained discriminative and generative models, we integrate the two in an auto-encoder which fits our training objective.", "labels": [], "entities": []}, {"text": "We show how the framework enables grammar induction, parsing and language modeling within a single implementation.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7258964478969574}, {"text": "language modeling", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.6946192234754562}]}, {"text": "On the English Penn Treebank, we achieve competitive performance on constituency parsing and state-ofthe-art single-model language modeling score.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 7, "end_pos": 28, "type": "DATASET", "confidence": 0.9670459429423014}, {"text": "constituency parsing", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.8110223710536957}, {"text": "single-model language modeling", "start_pos": 109, "end_pos": 139, "type": "TASK", "confidence": 0.6368311246236166}]}], "datasetContent": [{"text": "We performed experiments on the English Penn Treebank dataset; we used sections 2-21 for training, 24 for validation, and 23 for testing.", "labels": [], "entities": [{"text": "English Penn Treebank dataset", "start_pos": 32, "end_pos": 61, "type": "DATASET", "confidence": 0.9361390918493271}]}, {"text": "Following, we represent each word in three ways: as a learned vector, a pretrained vector, and a POS tag vector.", "labels": [], "entities": []}, {"text": "The encoder word embedding is the concatenation of all three vectors while the decoder uses only the first two since we do not consider POS tags in generation.", "labels": [], "entities": []}, {"text": "Table 1 presents details on the hyper-parameters we used.", "labels": [], "entities": []}, {"text": "To find the MAP parse tree argmax a p(a, x) (where p(a, x) is used rank the output of q(a|x)) and to compute the language modeling perplexity with the evidence lower bound (where a \u223c q(a|x)), we collect 100 samples from q(a|x), same as.", "labels": [], "entities": [{"text": "MAP parse", "start_pos": 12, "end_pos": 21, "type": "TASK", "confidence": 0.6617315113544464}]}, {"text": "Experimental results for constituency parsing and language modeling are shown in use an LSTM as the syntactic composition function of each subtree, we adopt a rather simple composition function based on embedding averaging for memory concern.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.9156382977962494}, {"text": "language modeling", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7550378441810608}]}, {"text": "On language modeling, our framework achieves lower perplexity compared to and baseline models.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.6928785741329193}]}, {"text": "This gain possibly comes from the joint optimization of both the generative and discriminative components towards a language modeling objective.", "labels": [], "entities": []}, {"text": "However, we acknowledge a subtle difference between and our approach compared to baseline language models: while the latter incrementally estimate the next word probability, our approach) directly assigns probability to the entire sentence.", "labels": [], "entities": []}, {"text": "Overall, the advantage of our framework compared to is that it opens an avenue to unsupervised training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Language modeling results (perplexity).", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7193299233913422}]}]}