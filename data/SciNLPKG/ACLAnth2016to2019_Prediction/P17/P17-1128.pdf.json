{"title": [{"text": "Transductive Non-linear Learning for Chinese Hypernym Prediction", "labels": [], "entities": [{"text": "Chinese Hypernym Prediction", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6096783975760142}]}], "abstractContent": [{"text": "Finding the correct hypernyms for entities is essential for taxonomy learning, fine-grained entity categorization, knowledge base construction, etc.", "labels": [], "entities": [{"text": "taxonomy learning", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.8567517995834351}, {"text": "knowledge base construction", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.629469245672226}]}, {"text": "Due to the flexibility of the Chinese language, it is challenging to identify hypernyms in Chinese accurately.", "labels": [], "entities": []}, {"text": "Rather than extracting hyper-nyms from texts, in this paper, we present a transductive learning approach to establish mappings from entities to hyper-nyms in the embedding space directly.", "labels": [], "entities": []}, {"text": "It combines linear and non-linear embedding projection models, with the capacity of encoding arbitrary language-specific rules.", "labels": [], "entities": []}, {"text": "Experiments on real-world datasets illustrate that our approach outperforms previous methods for Chinese hypernym prediction .", "labels": [], "entities": [{"text": "Chinese hypernym prediction", "start_pos": 97, "end_pos": 124, "type": "TASK", "confidence": 0.7096760968367258}]}], "introductionContent": [{"text": "A hypernym of an entity characterizes the type or the class of the entity.", "labels": [], "entities": []}, {"text": "For example, the word country is the hypernym of the entity Canada.", "labels": [], "entities": []}, {"text": "The accurate prediction of hypernyms benefits a variety of NLP tasks, such as taxonomy learning (), fine-grained entity categorization, knowledge base construction (, etc.", "labels": [], "entities": [{"text": "knowledge base construction", "start_pos": 136, "end_pos": 163, "type": "TASK", "confidence": 0.6040534675121307}]}, {"text": "In previous work, the detection of hypernyms requires lexical, syntactic and/or semantic analysis of relations between entities and their respective hypernyms from a language-specific knowledge source.", "labels": [], "entities": []}, {"text": "For example, is the pioneer work to extract is-a relations from a text corpus based on handcraft patterns.", "labels": [], "entities": []}, {"text": "The followingup work mostly focuses on is-a relation extraction using automatically generated patterns (Snow * Corresponding author.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7709070444107056}]}, {"text": "et al.,;) and relation inference based on distributional similarity measures (.", "labels": [], "entities": [{"text": "relation inference", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.9208158552646637}]}, {"text": "While these approaches have relatively high precision over English corpora, extracting hypernyms for entities is still challenging for Chinese.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9939809441566467}]}, {"text": "From the linguistic perspective, Chinese is a lower-resourced language with very flexible expressions and grammatical rules ( ).", "labels": [], "entities": []}, {"text": "For instance, there are no word spaces, explicit tenses and voices, and distinctions between singular and plural forms in Chinese.", "labels": [], "entities": []}, {"text": "The order of words can be changed flexibly in sentences.", "labels": [], "entities": []}, {"text": "Hence, as previous research indicates, hypernym extraction methods for English are not necessarily suitable for the Chinese language (.", "labels": [], "entities": [{"text": "hypernym extraction", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7955895364284515}]}, {"text": "Based on such conditions, several classification methods are proposed to distinguish is-a and notis-a relations based on Chinese encyclopedias (.", "labels": [], "entities": []}, {"text": "Similar to Princeton WordNet, a few Chinese wordnets have also been developed (.", "labels": [], "entities": [{"text": "Princeton WordNet", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.9168595671653748}]}, {"text": "The most recent approaches for Chinese is-a relation extraction () use word embedding based linear projection models to map embeddings of hyponyms to those of their hypernyms, which outperform previous algorithms.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8360663056373596}]}, {"text": "However, we argue that these projection-based methods may have three potential limitations: (i) Only positive is-a relations are used for projection learning.", "labels": [], "entities": [{"text": "projection learning", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.944657027721405}]}, {"text": "The distinctions between is-a and not-is-a relations in the embedding space are not modeled.", "labels": [], "entities": []}, {"text": "(ii) These methods lack the capacity to encode linguistic rules, which are designed by linguists and usually have high precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9928922653198242}]}, {"text": "(iii) It assumes that the linguistic regularities of is-a rela-tions can be solely captured by single or multiple linear projection models.", "labels": [], "entities": []}, {"text": "In this paper, we address these limitations by a two-stage transductive learning approach.", "labels": [], "entities": []}, {"text": "It distinguishes is-a and not-is-a relations given a Chinese word/phrase pair as input.", "labels": [], "entities": []}, {"text": "In the initial stage, we train linear projection models on positive and negative training data separately and predict isa relations jointly.", "labels": [], "entities": []}, {"text": "In the transductive learning stage, the initial prediction results, linguistic rules and the non-linear mappings from entities to hypernyms are optimized simultaneously in a unified framework.", "labels": [], "entities": []}, {"text": "This optimization problem can be efficiently solved by blockwise gradient descent.", "labels": [], "entities": []}, {"text": "We evaluate our method over two public datasets and show that it outperforms state-of-the-art approaches for Chinese hypernym prediction.", "labels": [], "entities": [{"text": "Chinese hypernym prediction", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.6321678558985392}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We summarize the related work in Section 2.", "labels": [], "entities": []}, {"text": "Our approach is introduced in Section 3.", "labels": [], "entities": []}, {"text": "Experimental results are presented in Section 4.", "labels": [], "entities": []}, {"text": "We conclude our paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we conduct experiments to evaluate our method.", "labels": [], "entities": []}, {"text": "Section 4.1 to Section 4.5 report the experimental stepson Chinese datasets.", "labels": [], "entities": []}, {"text": "We present the performance on English datasets in Section 4.6 and a discussion in Section 4.7.", "labels": [], "entities": [{"text": "English datasets", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.8373439013957977}]}, {"text": "We have two collections of Chinese word/phase pairs as ground truth datasets.", "labels": [], "entities": []}, {"text": "Each pair is labeled with an is-a or not-is-a tag.", "labels": [], "entities": []}, {"text": "The first one (denoted as FD) is from, containing 1,391 is-a pairs and 4,294 not-is-a pairs, which is the first publicly available dataset to evaluate this task.", "labels": [], "entities": [{"text": "FD", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9776538014411926}]}, {"text": "The second one (denoted as BK) is larger in size and crawled from Baidu Baike by ourselves, consisting of <entity, category> pairs.", "labels": [], "entities": [{"text": "BK", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9017961621284485}, {"text": "Baidu Baike", "start_pos": 66, "end_pos": 77, "type": "DATASET", "confidence": 0.9594087898731232}]}, {"text": "For each pair in BK, we ask multiple human annotators to label the tag and discard the pair with inconsistent labels by different annotators.", "labels": [], "entities": [{"text": "BK", "start_pos": 17, "end_pos": 19, "type": "DATASET", "confidence": 0.7715641260147095}]}, {"text": "In total, it contains 3,870 is-a pairs and 3,582 not-is-a pairs . The Chinese text corpus is extracted from the contents of 1.2M entity pages from Baidu Baike 5 , a Chinese online encyclopedia.", "labels": [], "entities": [{"text": "Chinese text corpus", "start_pos": 70, "end_pos": 89, "type": "DATASET", "confidence": 0.6662387251853943}, {"text": "Baidu Baike 5 , a Chinese online encyclopedia", "start_pos": 147, "end_pos": 192, "type": "DATASET", "confidence": 0.9078251719474792}]}, {"text": "It contains approximately 1.1B words.", "labels": [], "entities": []}, {"text": "We use the open source toolkit Ansj 6 for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.5868063867092133}]}, {"text": "Chinese words/phrases in our test sets may consist of multiple Chinese characters.", "labels": [], "entities": []}, {"text": "We treat such word/phrase as a whole to learn embeddings, instead of using character-level embeddings.", "labels": [], "entities": []}, {"text": "In the following experiments, we use 60% of the data for training, 20% for development and 20% for testing, partitioned randomly.", "labels": [], "entities": []}, {"text": "By rotating the 5-fold subsets of the datasets, we report the performance of each method on average.", "labels": [], "entities": []}, {"text": "To examine how our method can benefit hypernym prediction for the English language, we use two standard datasets in this paper.", "labels": [], "entities": [{"text": "hypernym prediction", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8187224566936493}]}, {"text": "The first one is a benchmark dataset for distributional semantic evaluation, i.e., BLESS (: Performance comparison on test sets for English hypernym prediction (%).", "labels": [], "entities": [{"text": "distributional semantic evaluation", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.6242591540018717}, {"text": "BLESS", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.9984885454177856}, {"text": "English hypernym prediction", "start_pos": 132, "end_pos": 159, "type": "TASK", "confidence": 0.5619301497936249}]}, {"text": "statistics, and the pre-trained embedding vectors of English words 9 . For comparison, we test all the baselines over English datasets except.", "labels": [], "entities": []}, {"text": "This is because most features in can only be used in the Chinese environment.", "labels": [], "entities": []}, {"text": "To implement for English, we use the original Hearst patterns to perform relation selection and do not consider not-is-a patterns.", "labels": [], "entities": [{"text": "relation selection", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.8388047814369202}]}, {"text": "We also take two recent DSM based approaches () as baselines.", "labels": [], "entities": []}, {"text": "As for our own method, we do not use linguistic rules in for English.", "labels": [], "entities": []}, {"text": "The results are illustrated in.", "labels": [], "entities": []}, {"text": "As seen, our method is superior to all the baselines over BLESS, with an F-measure of 81.9%.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9884591698646545}, {"text": "F-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9995837807655334}]}, {"text": "In Shwartz, while the approach ( has the highest F-measure of 80.1%, our method is generally comparable to theirs and outperforms others.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9991443157196045}]}, {"text": "The results suggest that although our method is not necessarily the state-of-the-art for English hypernym prediction, it has several potential applications.", "labels": [], "entities": [{"text": "English hypernym prediction", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.6588434974352518}]}, {"text": "Refer to Section 4.7 for discussion.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance comparison on test sets for Chinese hypernym prediction (%).", "labels": [], "entities": [{"text": "Chinese hypernym prediction", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6423161625862122}]}, {"text": " Table 3: Examples of model prediction. (P: prediction result, T: ground truth,  \u221a : positive, \u00d7: negative)", "labels": [], "entities": [{"text": "model prediction", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.743224710226059}]}, {"text": " Table 4: TP/TN rates of three linguistic rules (%).", "labels": [], "entities": [{"text": "TP/TN rates", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.5909054353833199}]}, {"text": " Table 5: Performance comparison on test sets for English hypernym prediction (%).", "labels": [], "entities": [{"text": "English hypernym prediction", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.672416607538859}]}]}