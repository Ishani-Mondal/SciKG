{"title": [{"text": "End-to-End Non-Factoid Question Answering with an Interactive Visualization of Neural Attention Weights", "labels": [], "entities": [{"text": "End-to-End Non-Factoid Question Answering", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.5038800165057182}, {"text": "Neural Attention Weights", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.6454148491223654}]}], "abstractContent": [{"text": "Advanced attention mechanisms are an important part of successful neural network approaches for non-factoid answer selection because they allow the models to focus on few important segments within rather long answer texts.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.7767583727836609}]}, {"text": "Analyzing attention mechanisms is thus crucial for understanding strengths and weaknesses of particular models.", "labels": [], "entities": []}, {"text": "We present an extensible, highly modular service architecture that enables the transformation of neural network models for non-factoid answer selection into fully featured end-to-end question answering systems.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 135, "end_pos": 151, "type": "TASK", "confidence": 0.7029116153717041}, {"text": "question answering", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.7121403068304062}]}, {"text": "The primary objective of our system is to enable researchers away to interactively explore and compare attention-based neural networks for answer selection.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 139, "end_pos": 155, "type": "TASK", "confidence": 0.910423994064331}]}, {"text": "Our interactive user interface helps researchers to better understand the capabilities of the different approaches and can aid qualitative analyses.", "labels": [], "entities": []}, {"text": "The source-code of our system is publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Attention-based neural networks are increasingly popular because of their ability to focus on the most important segments of a given input.", "labels": [], "entities": []}, {"text": "These models have proven to be extremely effective in many different tasks, for example neural machine translation (, neural image caption generation (, and multiple sub-tasks in question answering.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 88, "end_pos": 114, "type": "TASK", "confidence": 0.6237577597300211}, {"text": "neural image caption generation", "start_pos": 118, "end_pos": 149, "type": "TASK", "confidence": 0.6992756277322769}, {"text": "question answering", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.8190215826034546}]}, {"text": "Attention-based neural networks are especially successful in answer selection for non-factoid ques-tions, where approaches have to deal with complex multi-sentence texts.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.9178074300289154}]}, {"text": "The objective of this task is to re-rank a list of candidate answers according to a non-factoid question, where the best-ranked candidate is selected as an answer.", "labels": [], "entities": []}, {"text": "Models usually learn to generate dense vector representations for questions and candidates, where representations of a question and an associated correct answer should lie closely together within the vector space.", "labels": [], "entities": []}, {"text": "Accordingly, the ranking score can be determined with a simple similarity metric.", "labels": [], "entities": []}, {"text": "Attention in this scenario works by calculating weights for each individual segment in the input (attention vector), where segments with a higher weight should have a stronger impact on the resulting representation.", "labels": [], "entities": []}, {"text": "Several approaches have been recently proposed, achieving state-of-the-art results on different datasets.", "labels": [], "entities": []}, {"text": "The success of these approaches clearly shows the importance of sophisticated attention mechanisms for effective answer selection models.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.8456357419490814}]}, {"text": "However, it has also been shown that attention mechanisms can introduce certain biases that negatively influence the results (.", "labels": [], "entities": []}, {"text": "As a consequence, the creation of better attention mechanisms can improve the overall answer selection performance.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.8697509169578552}]}, {"text": "To achieve this goal, researchers are required to perform in-depth analyses and comparisons of different approaches to understand what the individual models learn and how they can be improved.", "labels": [], "entities": []}, {"text": "Due to the lack of existing tool-support to aid this process, such analyses are complex and require substantial development effort.", "labels": [], "entities": []}, {"text": "This important issue led us to creating an integrated solution that helps researchers to better understand the capabilities of different attention-based models and can aid qualitative analyses.", "labels": [], "entities": []}, {"text": "In this work, we present an extensible service architecture that can transform models for non-: A high-level view on our service architecture.", "labels": [], "entities": []}, {"text": "factoid answer selection into fully featured end-toend question answering systems.", "labels": [], "entities": [{"text": "factoid answer selection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6926524639129639}, {"text": "question answering", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.6984124481678009}]}, {"text": "Our sophisticated user interface allows researchers to ask arbitrary questions while visualizing the associated attention vectors with support for both, one-way and twoway attention mechanisms.", "labels": [], "entities": []}, {"text": "Users can explore different attention-based models at the same time and compare two attention mechanisms side-by-side within the same view.", "labels": [], "entities": []}, {"text": "Due to the loose coupling and the strictly separated responsibilities of the components in our service architecture, our system is highly modular and can be easily extended with new datasets and new models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}