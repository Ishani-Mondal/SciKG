{"title": [{"text": "An Empirical Study on End-to-End Sentence Modelling", "labels": [], "entities": [{"text": "Sentence Modelling", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7299674451351166}]}], "abstractContent": [{"text": "Accurately representing the meaning of apiece of text, otherwise known as sentence modelling, is an important component in many natural language inference tasks.", "labels": [], "entities": [{"text": "Accurately representing the meaning of apiece of text", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.6725393570959568}, {"text": "sentence modelling", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.6870266199111938}]}, {"text": "We survey the spectrum of these methods, which lie along two dimensions: input representation granularity and composition model complexity.", "labels": [], "entities": []}, {"text": "Using this framework, we reveal in our quantitative and qualitative experiments the limitations of the current state-of-the-art model in the context of sentence similarity tasks.", "labels": [], "entities": [{"text": "sentence similarity tasks", "start_pos": 152, "end_pos": 177, "type": "TASK", "confidence": 0.7858827213446299}]}], "introductionContent": [{"text": "Accurately representing the meaning of apiece of text remains an open problem.", "labels": [], "entities": []}, {"text": "To illustrate why it is difficult, consider the pair of sentences A and B below in the context of a sentence similarity task.", "labels": [], "entities": [{"text": "sentence similarity task", "start_pos": 100, "end_pos": 124, "type": "TASK", "confidence": 0.7565326790014902}]}, {"text": "If we use a very na\u00a8\u0131vena\u00a8\u0131ve model such as bagof-words to represent a sentence and use discrete counting of common words between the two sentences to determine their similarity, the score would be very low although they are highly similar.", "labels": [], "entities": []}, {"text": "How then do we represent the meaning of sentences?", "labels": [], "entities": []}, {"text": "Firstly, we must be able to represent them in ways that computers can understand.", "labels": [], "entities": []}, {"text": "Based on the Principle of Compositionality (Frege, 1892), we define the meaning of a sentence as a function of the meaning of its constituents (i.e., words, phrases, morphemes).", "labels": [], "entities": [{"text": "Principle of Compositionality (Frege, 1892)", "start_pos": 13, "end_pos": 56, "type": "TASK", "confidence": 0.6430078074336052}]}, {"text": "Generally, there are two main approaches to representing constituents: localist and distributed representations.", "labels": [], "entities": []}, {"text": "With the localist representation 1 , we represent each constituent with a unique representation usually taken from its position in a vocabulary V.", "labels": [], "entities": []}, {"text": "However, this kind of representation suffers from the curse of dimensionality and does not consider the syntactic relationship of a constituent with other constituents.", "labels": [], "entities": []}, {"text": "These two shortcomings are addressed by the distributed representation which encodes a constituent based on its co-occurrence with other constituents appearing within its context, into a dense n-dimensional vector where n \u2327 |V |.", "labels": [], "entities": []}, {"text": "Estimating the distributed representation has been an active research topic in itself.", "labels": [], "entities": []}, {"text": "conducted a systematic comparative evaluation of context-counting and context-predicting models for generating distributed representations and concluded that the latter outperforms the former, but later have shown that simple pointwise mutual information (PMI) methods also perform similarly if they are properly tuned.", "labels": [], "entities": []}, {"text": "To date, the most popular architectures to efficiently estimate these distributed representations are word2vec () and).", "labels": [], "entities": []}, {"text": "Subsequent developments estimate distributed representations at other levels of granularity (see Section 2.1).", "labels": [], "entities": []}, {"text": "While much research has been directed into constructing representations for constituents, there has been far less consensus regarding the representation of larger semantic structures such as phrases and sentences.", "labels": [], "entities": []}, {"text": "A simple approach is based onlooking up the vector representation of the constituents (i.e., embeddings) and taking their sum or average which yields a single vector of the same dimension.", "labels": [], "entities": []}, {"text": "This strategy is effective in simple tasks but loses word order information and syntactic relations in the process ().", "labels": [], "entities": []}, {"text": "Most modern neural network models have a sentence encoder that learns the representation of sentences more efficiently while preserving word or-der and compositionality (see Section 2.1).", "labels": [], "entities": []}, {"text": "In this work, we present a generalised framework for sentence modelling based on a survey of state-of-the-art methods.", "labels": [], "entities": [{"text": "sentence modelling", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.8748985826969147}]}, {"text": "Using the framework as a guide, we conducted preliminary experiments by implementing an end-to-end version of the stateof-the-art model in which we reveal its limitations after evaluation on sentence similarity tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the preliminary experiments we conducted in order to gain deeper understanding on the limitations of the state-of-the-art model.", "labels": [], "entities": []}, {"text": "Firstly, we define sentence similarity as a supervised learning task where each training example consists of a pair of sentences ) of fixed-sized vectors (where x a i , x b j 2 Rd input denoting constituent vectors from each sentence, respectively, which maybe of different lengths Ta 6 = Tb ) along with a single real-valued label y for the pair.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7242304533720016}]}, {"text": "We evaluated the performance of the state-of-the-art model on this task.", "labels": [], "entities": []}, {"text": "We measure the model's performance on three benchmark datasets, i.e.,   Furthermore, using the framework described in Section 2.1, we chose to compare the model performance at two levels of input representation (i.e., character-level vs word-level) and composition models (i.e., LSTM vs vector sum) in order to eliminate the need for external tools such as parsers.", "labels": [], "entities": []}, {"text": "shows the performance across input representations and composition models.", "labels": [], "entities": []}, {"text": "As expected, our simplified model performs relatively worse (Pearson correlation = 0.7355) when compared to what was reported in the original MaLSTM paper (Pearson correlation = 0.8822) on the SICK dataset (using word2vec).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 61, "end_pos": 80, "type": "METRIC", "confidence": 0.9611834287643433}, {"text": "MaLSTM paper", "start_pos": 142, "end_pos": 154, "type": "DATASET", "confidence": 0.8800491690635681}, {"text": "Pearson correlation = 0.8822)", "start_pos": 156, "end_pos": 185, "type": "METRIC", "confidence": 0.9412759065628051}, {"text": "SICK dataset", "start_pos": 193, "end_pos": 205, "type": "DATASET", "confidence": 0.8838003575801849}]}, {"text": "This performance difference (around 15%) could be attributed to the additional features (see) that the state-ofthe-art model added to their system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Pearson Correlation. Performance comparison across input representations and composition  models. Baseline method uses cosine similarity measure to predict similarity between sentences.", "labels": [], "entities": []}, {"text": " Table 3: Percentage of Out-of-Embedding- Vocabulary (OOEV) words", "labels": [], "entities": [{"text": "Percentage of Out-of-Embedding- Vocabulary (OOEV", "start_pos": 10, "end_pos": 58, "type": "METRIC", "confidence": 0.7001304541315351}]}, {"text": " Table 4: Examples of difficult sentence pairs. Compositional models use GloVe embeddings.", "labels": [], "entities": []}]}