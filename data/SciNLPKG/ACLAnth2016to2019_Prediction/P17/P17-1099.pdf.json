{"title": [{"text": "Get To The Point: Summarization with Pointer-Generator Networks", "labels": [], "entities": [{"text": "Summarization", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.9526550769805908}]}], "abstractContent": [{"text": "Neural sequence-to-sequence models have provided a viable new approach for ab-stractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text).", "labels": [], "entities": [{"text": "ab-stractive text summarization", "start_pos": 75, "end_pos": 106, "type": "TASK", "confidence": 0.5503390232721964}]}, {"text": "However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves.", "labels": [], "entities": []}, {"text": "In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways.", "labels": [], "entities": []}, {"text": "First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator.", "labels": [], "entities": []}, {"text": "Second, we use coverage to keep track of what has been summarized, which discourages repetition.", "labels": [], "entities": []}, {"text": "We apply our model to the CNN / Daily Mail sum-marization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.", "labels": [], "entities": [{"text": "CNN / Daily Mail sum-marization", "start_pos": 26, "end_pos": 57, "type": "DATASET", "confidence": 0.8632787942886353}, {"text": "ROUGE", "start_pos": 133, "end_pos": 138, "type": "METRIC", "confidence": 0.9726594686508179}]}], "introductionContent": [{"text": "Summarization is the task of condensing apiece of text to a shorter version that contains the main information from the original.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9833679795265198}]}, {"text": "There are two broad approaches to summarization: extractive and abstractive.", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.989018976688385}]}, {"text": "Extractive methods assemble summaries exclusively from passages (usually whole sentences) taken directly from the source text, while abstractive methods may generate novel words and phrases not featured in the source text -as a human-written abstract usually does.", "labels": [], "entities": [{"text": "summaries exclusively from passages (usually whole sentences) taken directly from the source text", "start_pos": 28, "end_pos": 125, "type": "TASK", "confidence": 0.6004230399926503}]}, {"text": "The extractive approach is easier, because copying large Original Text (truncated): lagos, nigeria (cnn) a day after winning nigeria's presidency, muhammadu buhari told cnn's christiane amanpour that he plans to aggressively fight corruption that has long plagued nigeria and go after the root of the nation's unrest.", "labels": [], "entities": []}, {"text": "buhari said he'll \"rapidly give attention\" to curbing violence in the northeast part of nigeria, where the terrorist group boko haram operates. by cooperating with neighboring nations chad, cameroon and niger, he said his administration is confident it will be able to thwart criminals and others contributing to nigeria's instability.", "labels": [], "entities": []}, {"text": "for the first time in nigeria's history, the opposition defeated the ruling party in democratic elections.", "labels": [], "entities": [{"text": "nigeria's", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.9302027523517609}]}, {"text": "buhari defeated incumbent goodluck jonathan by about 2 million votes, according to nigeria's independent national electoral commission.", "labels": [], "entities": [{"text": "nigeria's independent national electoral commission", "start_pos": 83, "end_pos": 134, "type": "DATASET", "confidence": 0.7974908649921417}]}, {"text": "the win comes after along history of military rule, coups and botched attempts at democracy in africa's most populous nation.", "labels": [], "entities": []}, {"text": "Baseline Seq2Seq + Attention: UNK UNK says his administration is confident it will be able to destabilize nigeria's economy.", "labels": [], "entities": [{"text": "Attention", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9059715270996094}, {"text": "UNK UNK", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.8014918267726898}]}, {"text": "UNK says his administration is confident it will be able to thwart criminals and other nigerians.", "labels": [], "entities": [{"text": "UNK", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9309799671173096}]}, {"text": "he says the country has long nigeria and nigeria's economy.", "labels": [], "entities": []}, {"text": "Pointer-Gen: muhammadu buhari says he plans to aggressively fight corruption in the northeast part of nigeria.", "labels": [], "entities": [{"text": "Pointer-Gen", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9574980735778809}]}, {"text": "he says he'll \"rapidly give attention\" to curbing violence in the northeast part of nigeria.", "labels": [], "entities": [{"text": "curbing violence", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.9172682464122772}]}, {"text": "he says his administration is confident it will be able to thwart criminals.", "labels": [], "entities": []}, {"text": "Pointer-Gen + Coverage: muhammadu buhari says he plans to aggressively fight corruption that has long plagued nigeria.", "labels": [], "entities": [{"text": "Pointer-Gen", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9104275107383728}, {"text": "nigeria", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.9395958185195923}]}, {"text": "he says his administration is confident it will be able to thwart criminals.", "labels": [], "entities": []}, {"text": "the win comes after along history of military rule, coups and botched attempts at democracy in africa's most populous nation.: Comparison of output of 3 abstractive summarization models on a news article.", "labels": [], "entities": []}, {"text": "The baseline model makes factual errors, a nonsensical sentence and struggles with OOV words muhammadu buhari.", "labels": [], "entities": []}, {"text": "The pointer-generator model is accurate but repeats itself.", "labels": [], "entities": []}, {"text": "The final summary is composed from several fragments.", "labels": [], "entities": []}, {"text": "chunks of text from the source document ensures baseline levels of grammaticality and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9961569905281067}]}, {"text": "On the other hand, sophisticated abilities that are crucial to high-quality summarization, such as paraphrasing, generalization, or the incorporation of real-world knowledge, are possible only in an abstractive framework (see).", "labels": [], "entities": []}, {"text": "Due to the difficulty of abstractive summarization, the great majority of past work has been extractive (.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.6258160769939423}]}, {"text": "However, the recent success of sequence-to-sequence models (Sutskever: Baseline sequence-to-sequence model with attention.", "labels": [], "entities": []}, {"text": "The model may attend to relevant words in the source text to generate novel words, e.g., to produce the novel word beat in the abstractive summary Germany beat Argentina 2-0 the model may attend to the words victorious and win in the source text.", "labels": [], "entities": []}, {"text": "et al., 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 102, "end_pos": 127, "type": "TASK", "confidence": 0.5908811390399933}]}, {"text": "Though these systems are promising, they exhibit undesirable behavior such as inaccurately reproducing factual details, an inability to deal with out-of-vocabulary (OOV) words, and repeating themselves (see).", "labels": [], "entities": []}, {"text": "In this paper we present an architecture that addresses these three issues in the context of multi-sentence summaries.", "labels": [], "entities": []}, {"text": "While most recent abstractive work has focused on headline generation tasks (reducing one or two sentences to a single headline), we believe that longer-text summarization is both more challenging (requiring higher levels of abstraction while avoiding repetition) and ultimately more useful.", "labels": [], "entities": [{"text": "headline generation tasks", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.8413250843683878}]}, {"text": "Therefore we apply our model to the recently-introduced CNN/ Daily Mail dataset (, which contains news articles (39 sentences on average) paired with multi-sentence summaries, and show that we outperform the stateof-the-art abstractive system by at least 2 ROUGE points.", "labels": [], "entities": [{"text": "CNN/ Daily Mail dataset", "start_pos": 56, "end_pos": 79, "type": "DATASET", "confidence": 0.8957050442695618}, {"text": "ROUGE", "start_pos": 257, "end_pos": 262, "type": "METRIC", "confidence": 0.984928548336029}]}, {"text": "Our hybrid pointer-generator network facilitates copying words from the source text via pointing (, which improves accuracy and handling of OOV words, while retaining the ability to generate new words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9972917437553406}]}, {"text": "The network, which can be viewed as a balance between extractive and abstractive approaches, is similar to Forced-Attention Sentence Compression, that were applied to short-text summarization.", "labels": [], "entities": []}, {"text": "We propose a novel variant of the coverage vector () from Neural Machine Translation, which we use to track and control coverage of the source document.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.596726248661677}]}, {"text": "We show that coverage is remarkably effective for eliminating repetition.", "labels": [], "entities": [{"text": "eliminating repetition", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.6177253723144531}]}], "datasetContent": [{"text": "We use the CNN/Daily Mail dataset (), which contains online news articles (781 tokens on average) paired with multi-sentence summaries (3.75 sentences or 56 tokens on average).", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 11, "end_pos": 33, "type": "DATASET", "confidence": 0.9196125030517578}]}, {"text": "We used scripts supplied by  to obtain the same version of the the data, which has 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs.", "labels": [], "entities": []}, {"text": "Both the dataset's published results () use the anonymized version of the data, which has been pre-processed to replace each named entity, e.g., The United Nations, with its own unique identifier for the example pair, e.g., @entity5.", "labels": [], "entities": []}, {"text": "By contrast, we operate directly on the original text (or non-anonymized version of the data), 2 which we believe is the favorable problem to solve because it requires no pre-processing.", "labels": [], "entities": []}, {"text": "For all experiments, our model has 256-dimensional hidden states and 128-dimensional word embeddings.", "labels": [], "entities": []}, {"text": "For the pointer-generator models, we use a vocabulary of 50k words for both source and target -note that due to the pointer network's ability to handle OOV words, we can use , we do not pretrain the word embeddings -they are learned from scratch during training.", "labels": [], "entities": []}, {"text": "We train using Adagrad) with learning rate 0.15 and an initial accumulator value of 0.1.", "labels": [], "entities": [{"text": "learning rate 0.15", "start_pos": 29, "end_pos": 47, "type": "METRIC", "confidence": 0.9495120247205099}]}, {"text": "(This was found to work best of Stochastic Gradient Descent, Adadelta, Momentum, Adam and RMSProp).", "labels": [], "entities": []}, {"text": "We use gradient clipping with a maximum gradient norm of 2, but do not use any form of regularization.", "labels": [], "entities": []}, {"text": "We use loss on the validation set to implement early stopping.", "labels": [], "entities": []}, {"text": "During training and attest time we truncate the article to 400 tokens and limit the length of the summary to 100 tokens for training and 120 tokens attest time.", "labels": [], "entities": []}, {"text": "3 This is done to expedite training and testing, but we also found that truncating the article can raise the performance of the model (see section 7.1 for more details).", "labels": [], "entities": []}, {"text": "For training, we found it efficient to start with highly-truncated sequences, then raise the maximum length once converged.", "labels": [], "entities": []}, {"text": "We train on a single Tesla K40m GPU with a batch size of 16.", "labels": [], "entities": []}, {"text": "At test time our summaries are produced using beam search with beam size 4.", "labels": [], "entities": []}, {"text": "We trained both our baseline models for about 600,000 iterations (33 epochs) -this is similar to the 35 epochs required by best model.", "labels": [], "entities": []}, {"text": "Training took 4 days and 14 hours for the 50k vocabulary model, and 8 days 21 hours for the 150k vocabulary model.", "labels": [], "entities": []}, {"text": "We found the pointer-generator model quicker to train, requiring less than 230,000 training iterations (12.8 epochs); a total of 3 days and 4 hours.", "labels": [], "entities": []}, {"text": "In particular, the pointer-generator model makes much quicker progress in the early phases of training.", "labels": [], "entities": []}, {"text": "To obtain our final coverage model, we added the coverage mechanism with coverage loss weighted to \u03bb = 1 (as described in equation 13), and trained fora further 3000 iterations (about 2 hours).", "labels": [], "entities": [{"text": "coverage loss weighted", "start_pos": 73, "end_pos": 95, "type": "METRIC", "confidence": 0.9584830602010092}]}, {"text": "In this time the coverage loss converged to about 0.2, down from an initial value of about 0.5.", "labels": [], "entities": [{"text": "coverage", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9970099925994873}]}, {"text": "We also tried a more aggressive value of \u03bb = 2; this reduced coverage loss but increased the primary loss function, thus we did not use it.", "labels": [], "entities": [{"text": "coverage loss", "start_pos": 61, "end_pos": 74, "type": "METRIC", "confidence": 0.9797953963279724}]}, {"text": "We tried training the coverage model without the loss function, hoping that the attention mechanism may learn by itself not to attend repeatedly to the same locations, but we found this to be ineffective, with no discernible reduction in repetition.", "labels": [], "entities": [{"text": "repetition", "start_pos": 238, "end_pos": 248, "type": "METRIC", "confidence": 0.9898661971092224}]}, {"text": "We also tried training with coverage from the first iteration rather than as a separate training phase, but found that in the early phase of training, the coverage objective interfered with the main objective, reducing overall performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most \u00b10.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.", "labels": [], "entities": [{"text": "ROUGE F 1", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.7769115169843038}, {"text": "METEOR", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9728553891181946}, {"text": "METEOR", "start_pos": 421, "end_pos": 427, "type": "METRIC", "confidence": 0.865951418876648}]}]}