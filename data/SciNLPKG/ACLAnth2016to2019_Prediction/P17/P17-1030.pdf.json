{"title": [{"text": "Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7325012385845184}]}], "abstractContent": [{"text": "Recurrent neural networks (RNNs) have shown promising performance for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.8167336881160736}]}, {"text": "However, traditional training of RNNs using back-propagation through time often suffers from overfit-ting.", "labels": [], "entities": []}, {"text": "One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty.", "labels": [], "entities": [{"text": "stochastic optimization", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.7653033435344696}]}, {"text": "This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs.", "labels": [], "entities": []}, {"text": "It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing.", "labels": [], "entities": [{"text": "model averaging", "start_pos": 147, "end_pos": 162, "type": "TASK", "confidence": 0.7362900674343109}]}, {"text": "Extensive experiments on various RNN models and across abroad range of applications demonstrate the superiority of the proposed approach relative to stochastic optimization.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language modeling is a fundamental task, used for example to predict the next word or character in a text sequence given the context.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7340343147516251}, {"text": "predict the next word or character in a text sequence", "start_pos": 61, "end_pos": 114, "type": "TASK", "confidence": 0.827594268321991}]}, {"text": "Recently, recurrent neural networks (RNNs) have shown promising performance on this task (.", "labels": [], "entities": []}, {"text": "RNNs with Long Short-Term Memory (LSTM) units) have emerged as a popular architecture, due to their representational power and effectiveness at capturing long-term dependencies.", "labels": [], "entities": []}, {"text": "RNNs are usually trained via back-propagation through time, using stochastic op- * Equal contribution.", "labels": [], "entities": []}, {"text": "timization methods such as stochastic gradient descent (SGD); stochastic methods of this type are particularly important for training with large data sets.", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD)", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.789010485013326}]}, {"text": "However, this approach often provides a maximum a posteriori (MAP) estimate of model parameters.", "labels": [], "entities": [{"text": "maximum a posteriori (MAP) estimate", "start_pos": 40, "end_pos": 75, "type": "METRIC", "confidence": 0.8023634765829358}]}, {"text": "The MAP solution is a single point estimate, ignoring weight uncertainty (.", "labels": [], "entities": [{"text": "weight uncertainty", "start_pos": 54, "end_pos": 72, "type": "METRIC", "confidence": 0.9164314270019531}]}, {"text": "Natural language often exhibits significant variability, and hence such a point estimate may make over-confident predictions on test data.", "labels": [], "entities": []}, {"text": "To alleviate overfitting RNNs, good regularization is known as a key factor to successful applications.", "labels": [], "entities": []}, {"text": "In the neural network literature, Bayesian learning has been proposed as a principled method to impose regularization and incorporate model uncertainty, by imposing prior distributions on model parameters.", "labels": [], "entities": []}, {"text": "Due to the intractability of posterior distributions in neural networks, Hamiltonian Monte Carlo (HMC) has been used to provide sample-based approximations to the true posterior.", "labels": [], "entities": []}, {"text": "Despite the elegant theoretical property of asymptotic convergence to the true posterior, HMC and other conventional Markov Chain Monte Carlo methods are not scalable to large training sets.", "labels": [], "entities": []}, {"text": "This paper seeks to scale up Bayesian learning of RNNs to meet the challenge of the increasing amount of \"big\" sequential data in natural language processing, leveraging recent advances in stochastic gradient Markov Chain Monte Carlo (SG-MCMC) algorithms (.", "labels": [], "entities": []}, {"text": "Specifically, instead of training a single network, SG-MCMC is employed to train an ensemble of networks, where each network has its parameters drawn from a shared posterior distribution.", "labels": [], "entities": []}, {"text": "This is implemented by adding additional This simple procedure has the following salutary properties for training neural networks: (i) When training, the injected noise encourages model-parameter trajectories to better explore the parameter space.", "labels": [], "entities": []}, {"text": "This procedure was also empirically found effective in.", "labels": [], "entities": []}, {"text": "(ii) Model averaging when testing alleviates overfitting and hence improves generalization, transferring uncertainty in the learned model parameters to subsequent prediction.", "labels": [], "entities": [{"text": "Model averaging", "start_pos": 5, "end_pos": 20, "type": "TASK", "confidence": 0.8559128940105438}]}, {"text": "(iii) In theory, both asymptotic and non-asymptotic consistency properties of SG-MCMC methods in posterior estimation have been recently established to guarantee convergence.", "labels": [], "entities": []}, {"text": "(iv) SG-MCMC is scalable; it shares the same level of computational cost as SGD in training, by only requiring the evaluation of gradients on a small mini-batch.", "labels": [], "entities": []}, {"text": "To the authors' knowledge, RNN training using SG-MCMC has not been investigated previously, and is a contribution of this paper.", "labels": [], "entities": [{"text": "RNN training", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.8966679871082306}]}, {"text": "We also perform extensive experiments on several natural language processing tasks, demonstrating the effectiveness of SG-MCMC for RNNs, including character/word-level language modeling, image captioning and sentence classification.", "labels": [], "entities": [{"text": "character/word-level language modeling", "start_pos": 147, "end_pos": 185, "type": "TASK", "confidence": 0.5850840270519256}, {"text": "image captioning", "start_pos": 187, "end_pos": 203, "type": "TASK", "confidence": 0.7432685494422913}, {"text": "sentence classification", "start_pos": 208, "end_pos": 231, "type": "TASK", "confidence": 0.7641049325466156}]}], "datasetContent": [{"text": "We present results on several tasks, including character/word-level language modeling, image captioning, and sentence classification.", "labels": [], "entities": [{"text": "character/word-level language modeling", "start_pos": 47, "end_pos": 85, "type": "TASK", "confidence": 0.6017719447612763}, {"text": "image captioning", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.7443741261959076}, {"text": "sentence classification", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.7850286364555359}]}, {"text": "We do not perform any dataset-specific tuning other than early stopping on validation sets.", "labels": [], "entities": []}, {"text": "When dropout is utilized, the dropout rate is set to 0.5.", "labels": [], "entities": []}, {"text": "All experiments are implemented in Theano (Theano Development Team, 2016), using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory.", "labels": [], "entities": [{"text": "Theano (Theano Development Team, 2016)", "start_pos": 35, "end_pos": 73, "type": "DATASET", "confidence": 0.9257214665412903}]}, {"text": "The hyper-parameters for the proposed algorithm include step size, minibatch size, thinning interval, number of burn-in epochs and variance of the Gaussian priors.", "labels": [], "entities": [{"text": "minibatch size", "start_pos": 67, "end_pos": 81, "type": "METRIC", "confidence": 0.9439988136291504}]}, {"text": "We list the specific values used in our experiments in.", "labels": [], "entities": []}, {"text": "The explanation of these hyperparameters, the initialization of model parameters and model specifications on each dataset are provided in the Supplementary Material.", "labels": [], "entities": [{"text": "Supplementary Material", "start_pos": 142, "end_pos": 164, "type": "DATASET", "confidence": 0.8850031495094299}]}], "tableCaptions": [{"text": " Table 2: Hyper-parameter settings of pSGLD for different datasets. For PTB, SGLD is used.", "labels": [], "entities": [{"text": "PTB", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.8034557104110718}]}, {"text": " Table 3: Test cross-entropy loss on WP dataset.  Methods LSTM GRU  RNN  RMSprop 1.3607 1.2759 1.4239  pSGLD  1.3375 1.2561 1.4093", "labels": [], "entities": [{"text": "WP dataset", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.7181542217731476}, {"text": "LSTM GRU  RNN  RMSprop 1.3607 1.2759 1.4239  pSGLD  1.3375 1.2561 1.4093", "start_pos": 58, "end_pos": 130, "type": "DATASET", "confidence": 0.8790336955677379}]}, {"text": " Table 4: Test perplexity on Penn Treebank.  Methods  Small Medium Large", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.9946923553943634}]}, {"text": " Table 5: Performance on Flickr8k & Flickr30k: BLEU's, METEOR, CIDEr, ROUGE-L and perplexity.", "labels": [], "entities": [{"text": "Flickr8k", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.8465293645858765}, {"text": "Flickr30k", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.7711853981018066}, {"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9992035031318665}, {"text": "METEOR", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9963454604148865}, {"text": "CIDEr", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9151374101638794}, {"text": "ROUGE-L", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9965798258781433}, {"text": "perplexity", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9546457529067993}]}, {"text": " Table 6: Sentence classification errors on five benchmark datasets.", "labels": [], "entities": [{"text": "Sentence classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.9596983194351196}]}, {"text": " Table 7: Ablation study on TREC and PTB.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9835752248764038}, {"text": "TREC", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.6123678088188171}, {"text": "PTB", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.8254222869873047}]}, {"text": " Table 8. For pSGLD, the extra cost in  training comes from adding gradient noise, and the  extra cost in testing comes from model averaging.  However, the cost in model averaging can be alle- viated via the distillation methods: learning a sin- gle neural network that approximates the results  of either a large model or an ensemble of mod- els (", "labels": [], "entities": []}]}