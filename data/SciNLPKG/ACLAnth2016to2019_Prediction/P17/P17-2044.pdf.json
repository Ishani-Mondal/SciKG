{"title": [{"text": "Japanese Sentence Compression with a Large Training Dataset", "labels": [], "entities": [{"text": "Japanese Sentence Compression", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6129047473271688}]}], "abstractContent": [{"text": "In English, high-quality sentence compression models by deleting words have been trained on automatically created large training datasets.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7586226761341095}]}, {"text": "We work on Japanese sentence compression by a similar approach.", "labels": [], "entities": [{"text": "Japanese sentence compression", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.5750292440255483}]}, {"text": "To create a large Japanese training dataset, a method of creating En-glish training dataset is modified based on the characteristics of the Japanese language.", "labels": [], "entities": [{"text": "En-glish training dataset", "start_pos": 66, "end_pos": 91, "type": "DATASET", "confidence": 0.6592982808748881}]}, {"text": "The created dataset is used to train Japanese sentence compression models based on the recurrent neural network.", "labels": [], "entities": [{"text": "Japanese sentence compression", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.6010335385799408}]}], "introductionContent": [{"text": "Sentence compression is the task of shortening a sentence while preserving its important information and grammaticality.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.924444168806076}]}, {"text": "Robust sentence compression systems are useful by themselves and also as a module in an extractive summarization system.", "labels": [], "entities": [{"text": "Robust sentence compression", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6556533177693685}]}, {"text": "In this paper, we work on Japanese sentence compression by deleting words.", "labels": [], "entities": [{"text": "Japanese sentence compression", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.6519699494043986}]}, {"text": "One advantage of compression by deleting words as opposed to abstractive compression lies in the small search space.", "labels": [], "entities": []}, {"text": "Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence.", "labels": [], "entities": []}, {"text": "There are many sentence compression models for Japanese () and for English).", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7343223541975021}]}, {"text": "In recent years, a high-quality English sentence compression model by deleting words was trained on a large training dataset (.", "labels": [], "entities": [{"text": "English sentence compression", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.5895252823829651}]}, {"text": "While it is impractical to create a large training dataset by hand, one can be created automatically from news articles ().", "labels": [], "entities": []}, {"text": "The procedure is as follows (where S, H, and C respectively denote the first sentence of an article, the headline, and the created compressed sentence of S).", "labels": [], "entities": []}, {"text": "Firstly, to restrict the training data to grammatical and informative sentences, only news articles satisfying certain conditions are used.", "labels": [], "entities": []}, {"text": "Then, nouns, verbs, adjectives, and adverbs (i.e., content words) shared by Sand H are identified by matching word lemmas, and a rooted dependency subtree that contains all the shared content words is regarded as C.", "labels": [], "entities": []}, {"text": "However, their method is designed for English, and cannot be applied to Japanese as it is.", "labels": [], "entities": []}, {"text": "Thus, in this study, their method is modified based on the following three characteristics of the Japanese language: (a) Abbreviation of nouns and nominalization of verbs frequently occur in Japanese.", "labels": [], "entities": [{"text": "Abbreviation", "start_pos": 121, "end_pos": 133, "type": "METRIC", "confidence": 0.9703084826469421}]}, {"text": "(b) Words that are not verbs can also be the root node especially in headlines.", "labels": [], "entities": []}, {"text": "(c) Subjects and objects that can be easily estimated from the context are often omitted.", "labels": [], "entities": []}, {"text": "The created training dataset is used to train three models.", "labels": [], "entities": []}, {"text": "The first model is the original Filippova et al.'s model, an encoder-decoder model with along short-term memory (LSTM), which we extend in this paper to make the other two models that can control the output length (, because controlling the output length makes a compressed sentence more informative under the desired length.", "labels": [], "entities": [{"text": "along short-term memory (LSTM)", "start_pos": 88, "end_pos": 118, "type": "METRIC", "confidence": 0.7415240655342737}]}, {"text": "We modified their method based on the characteristics of the Japanese language as follows.", "labels": [], "entities": []}, {"text": "To explain our method, a dependency tree of Sand a sequence of bunsetsu chunks of H in Japanese are shown in Figures 1 and 2.", "labels": [], "entities": []}, {"text": "Note that nodes of dependency trees in Japanese are bunsetsu chunks each consisting of content words followed by function words.", "labels": [], "entities": []}], "datasetContent": [{"text": "The created training datasets were used to train three models for sentence compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.8058021664619446}]}, {"text": "To seethe effect of the modified subtree extraction method (Section 2.3), two training datasets were tested: rooted and multi-root+.", "labels": [], "entities": [{"text": "subtree extraction", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7264296859502792}]}, {"text": "rooted includes only deletion of the leafs in a dependency tree.", "labels": [], "entities": []}, {"text": "In contrast, multi-root+ includes deleting the global root and reflecting abbreviated forms besides it.", "labels": [], "entities": []}, {"text": "Setting: Training datasets were created from seven million, 35-years' worth of news articles from the Mainichi, Nikkei, and Yomiuri newspapers, from which duplicate sentences and sentences in test data were filtered out.", "labels": [], "entities": [{"text": "Mainichi, Nikkei, and Yomiuri newspapers", "start_pos": 102, "end_pos": 142, "type": "DATASET", "confidence": 0.7280294213976178}]}, {"text": "Gold-standard data were composed of the first sentences of the 1,000 news articles from Mainichi, each of which has 5 compressed sentences separately created by five human annotators.", "labels": [], "entities": [{"text": "Mainichi", "start_pos": 88, "end_pos": 96, "type": "DATASET", "confidence": 0.8653732538223267}]}, {"text": "100 sentences of the gold-standard data were used as development data, while the other sentences were used as test data.", "labels": [], "entities": []}, {"text": "The three models were trained on datasets created with each value of threshold \u03b8 6 , which is the parameter used in the condition (introduced in Sec-: Automatic evaluation.", "labels": [], "entities": []}, {"text": "R-1, R-2, and R-L are ROUGE-1, ROUGE-2, and ROUGE-L score.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9843429327011108}, {"text": "ROUGE-2", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9767690896987915}, {"text": "ROUGE-L", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9780867695808411}]}, {"text": "R, P and F are recall, precision and F-measure.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9994460940361023}, {"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9994438290596008}, {"text": "F-measure", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9856460690498352}]}, {"text": "The properties of the dataset in relation to \u03b8 are shown in. \u03b8 is tuned for ROUGE-2 score on the development data.", "labels": [], "entities": [{"text": "ROUGE-2 score", "start_pos": 76, "end_pos": 89, "type": "METRIC", "confidence": 0.9745208024978638}]}, {"text": "In, we show the tendency of the ROUGE-2 scores when lstm+leninit was trained on created rooted with each \u03b8.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9919862747192383}]}, {"text": "From, we chose 0.5 as \u03b8.", "labels": [], "entities": []}, {"text": "All models are three stacked LSTM layers . Words with frequency lower than five are regarded as unknown words.", "labels": [], "entities": []}, {"text": "ADAM 8 was used as the optimization method.", "labels": [], "entities": [{"text": "ADAM 8", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9135444760322571}]}, {"text": "The desired length was set to the bytes of a compressed sentence randomly chosen from the five human-generated sentences.", "labels": [], "entities": []}, {"text": "In the test step, beam-search was used (beam size: 20) and candidates exceeding the desired length were truncated.", "labels": [], "entities": []}, {"text": "tree-base and prop-w-dpnd: Existing methods for Japanese sentence compression are not based on the training on a large dataset.", "labels": [], "entities": [{"text": "Japanese sentence compression", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.6340577701727549}]}, {"text": "Therefore, the proposed method is compared with two methods, tree-base, (Filippova and Strube, 2008) and propw-dpnd (Harashima and Kurohashi, 2012), which are not based on supervised learning.", "labels": [], "entities": []}, {"text": "tree-base is implemented as an integer linear programming problem that finds a subtree of a dependency tree.", "labels": [], "entities": []}, {"text": "prop-w-dpnd is also implemented as an integer linear programming problem, but modified based on characteristics of the Japanese language.", "labels": [], "entities": []}, {"text": "prop-wdpnd allows the deletion inside the chunks.", "labels": [], "entities": []}, {"text": "The models trained on a large training dataset achieved higher F-measure than the unsupervised models.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9996291399002075}]}, {"text": "Moreover, F-measure of lstm+leninit and lstm+lenemb, which were trained on either multiroot+ or rooted, is significantly better than propw-dpnd (p < 0.001).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9990654587745667}]}, {"text": "lstm achieved lower R-1 and R-2 than the other models trained on a large training dataset, probably because it tends to generate too short sentences, as indicated by low CR.", "labels": [], "entities": [{"text": "R-1", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9963524341583252}, {"text": "R-2", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9829277396202087}, {"text": "CR", "start_pos": 170, "end_pos": 172, "type": "METRIC", "confidence": 0.9868005514144897}]}, {"text": "It is also noteworthy that CRs of lstm+leninit and lstm+lenemb are mostly closer to the average CR of the test data than lstm.", "labels": [], "entities": [{"text": "CRs", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9720453023910522}]}, {"text": "Furthermore, lstm+leninit and lstm+lenemb trained on multiroot+ instead of rooted worked better in terms of F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9171302318572998}]}, {"text": "We consider it is because various types of deletion make the compression model more flexible, as indicated by closer CR of the model trained on multi-root+ instead of rooted to the average CR of the test data.", "labels": [], "entities": [{"text": "CR", "start_pos": 117, "end_pos": 119, "type": "METRIC", "confidence": 0.9655094742774963}]}, {"text": "The difference between lstm+leninit and lstm+lenemb trained on multi-root+ was investigated first.", "labels": [], "entities": []}, {"text": "With lstm+leninit, 2 out of 100 sentences, chosen randomly, ended with a word that cannot be located at the end of a sentence.", "labels": [], "entities": []}, {"text": "In contrast, with lstm+lenemb, 24 sentences ended with such words and therefore are ungrammatical, although lenemb has shown to be effective in abstractive sentence summarization ().", "labels": [], "entities": [{"text": "abstractive sentence summarization", "start_pos": 144, "end_pos": 178, "type": "TASK", "confidence": 0.6084197461605072}]}, {"text": "This result suggests that lstm+lenemb is excessively affected by the desired length because lenemb receives the potential desired length at each time of decoding.", "labels": [], "entities": []}, {"text": "In fact, 21 out of the 24 sentences are as long as the desired length.", "labels": [], "entities": []}, {"text": "Then, lstm+leninit trained on multi-root+ was evaluated by crowdsourcing in comparison with the gold-standard and tree-base.", "labels": [], "entities": []}, {"text": "Each crowd-: Human relative evaluation sourcing worker reads each source sentence and a set of the compressed sentences, reordered randomly, and gives a score from 1 to 5 (where 5 is best) to each compressed sentence in terms of informativeness (info) and readability (read).", "labels": [], "entities": []}, {"text": "As shown in, in terms of both read and info, lstm+leninit archived higher scores than prop-wdpnd, and the difference is significant (p < 0.001).", "labels": [], "entities": []}, {"text": "Next, lstm+leninit was compared with lstm (both are trained on multi-root+), and multi-root+ was compared with rooted (lstm+leninit was used as the model), by human relative evaluation.", "labels": [], "entities": []}, {"text": "In this evaluation, each worker votes for one of lstm+leninit, lstm, or tie, and for one of multiroot+, rooted, or tie.", "labels": [], "entities": []}, {"text": "250 votes in total were received (50 sentences \u00d7 5 votes).", "labels": [], "entities": []}, {"text": "The results are shown in. lstm+leninit is better than lstm in terms of read, which is not directly related to the output length, as well as of info.", "labels": [], "entities": []}, {"text": "It is also clear that multi-root+ achieves much higher info with a negligible reduction in read than rooted.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Created training data with each \u03b8.  ROUGE-2 is the score of compressed sentence  generated by the model trained on each training  data to target.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9982767105102539}]}, {"text": " Table 2: Automatic evaluation. R-1, R-2, and R-L are ROUGE-1, ROUGE-2, and ROUGE-L score. R,  P and F are recall, precision and F-measure.", "labels": [], "entities": [{"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9994805455207825}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9985145926475525}, {"text": "F-measure", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9925854802131653}]}, {"text": " Table 1. \u03b8 is tuned for ROUGE- 2 score on the development data. In", "labels": [], "entities": [{"text": "\u03b8", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9768734574317932}, {"text": "ROUGE- 2 score", "start_pos": 25, "end_pos": 39, "type": "METRIC", "confidence": 0.983708530664444}]}, {"text": " Table 3: Human absolute evaluation", "labels": [], "entities": [{"text": "Human absolute evaluation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.836048404375712}]}, {"text": " Table 4: Human relative evaluation", "labels": [], "entities": [{"text": "Human relative evaluation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8576935529708862}]}]}