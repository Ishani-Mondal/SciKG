{"title": [{"text": "Interactive Learning of Grounded Verb Semantics towards Human-Robot Communication", "labels": [], "entities": []}], "abstractContent": [{"text": "To enable human-robot communication and collaboration, previous works represent grounded verb semantics as the potential change of state to the physical world caused by these verbs.", "labels": [], "entities": []}, {"text": "Grounded verb semantics are acquired mainly based on the parallel data of the use of a verb phrase and its corresponding sequences of primitive actions demonstrated by humans.", "labels": [], "entities": []}, {"text": "The rich interaction between teachers and students that is considered important in learning new skills has not yet been explored.", "labels": [], "entities": []}, {"text": "To address this limitation, this paper presents anew interactive learning approach that allows robots to proactively engage in interaction with human partners by asking good questions to learn models for grounded verb semantics.", "labels": [], "entities": []}, {"text": "The proposed approach uses reinforcement learning to allow the robot to acquire an optimal policy for its question-asking behaviors by maximizing the long-term reward.", "labels": [], "entities": []}, {"text": "Our empirical results have shown that the interactive learning approach leads to more reliable models for grounded verb semantics, especially in the noisy environment which is full of uncertainties.", "labels": [], "entities": []}, {"text": "Compared to previous work, the models acquired from interactive learning result in a 48% to 145% performance gain when applied in new situations.", "labels": [], "entities": []}], "introductionContent": [{"text": "In communication with cognitive robots, one of the challenges is that robots do not have sufficient linguistic or world knowledge as humans do.", "labels": [], "entities": []}, {"text": "For example, if a human asks a robot to boil the water but the robot has no knowledge what this verb phrase means and how this verb phrase relates to its own actuator, the robot will not be able to execute this command.", "labels": [], "entities": []}, {"text": "Thus it is important for robots to continuously learn the meanings of new verbs and how the verbs are grounded to its underlying action representations from its human partners.", "labels": [], "entities": []}, {"text": "To support learning of grounded verb semantics, previous works) rely on multiple instances of human demonstrations of corresponding actions.", "labels": [], "entities": [{"text": "learning of grounded verb semantics", "start_pos": 11, "end_pos": 46, "type": "TASK", "confidence": 0.6764987349510193}]}, {"text": "From these demonstrations, robots capture the state change of the environment caused by the actions and represent verb semantics as the desired goal state.", "labels": [], "entities": []}, {"text": "One advantage of such state-based representation is that, when robots encounter the same verbs/commands in anew situation, the desired goal state will trigger the action planner to automatically plan a sequence of primitive actions to execute the command.", "labels": [], "entities": []}, {"text": "While the state-based verb semantics provides an important link to connect verbs to the robot's actuator, previous works also present several limitations.", "labels": [], "entities": []}, {"text": "First of all, previous approaches were developed under the assumption of perfect perception of the environment.", "labels": [], "entities": []}, {"text": "However, this assumption does not hold in real-world situated interaction.", "labels": [], "entities": []}, {"text": "The robot's representation of the environment is often incomplete and error-prone due to its limited sensing capabilities.", "labels": [], "entities": []}, {"text": "Thus it is not clear whether previous approaches can scale up to handle noisy and incomplete environment.", "labels": [], "entities": []}, {"text": "Second, most previous works rely on multiple demonstration examples to acquire grounded verb models.", "labels": [], "entities": []}, {"text": "Each demonstration is simply a sequence of primitive actions associated with a verb.", "labels": [], "entities": []}, {"text": "No other type of interaction between humans and robots is explored.", "labels": [], "entities": []}, {"text": "Previous cognitive studies () on how people learn have shown that social interaction (e.g., conver-sation with teachers) can enhance student learning experience and improve learning outcomes.", "labels": [], "entities": []}, {"text": "For robotic learning, previous work) has also demonstrated the necessity of question answering in the learning process.", "labels": [], "entities": [{"text": "question answering", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7537761330604553}]}, {"text": "Thus, in our view, interactive learning beyond demonstration of primitive actions should play a vital role in the robot's acquisition of more reliable models of grounded verb semantics.", "labels": [], "entities": []}, {"text": "This is especially important because the robot's perception of the world is noisy and incomplete, human language can be ambiguous, and the robot may lack the relevant linguistic or world knowledge during the learning process.", "labels": [], "entities": []}, {"text": "To address these limitations, we have developed anew interactive learning approach where robots actively engage with humans to acquire models of grounded verb semantics.", "labels": [], "entities": []}, {"text": "Our approach explores the space of interactive question answering between humans and robots during the learning process.", "labels": [], "entities": [{"text": "question answering between humans and robots", "start_pos": 47, "end_pos": 91, "type": "TASK", "confidence": 0.8190862685441971}]}, {"text": "In particular, motivated by previous work on robot learning, we designed a set of questions that are pertinent to verb semantic representations.", "labels": [], "entities": [{"text": "verb semantic representations", "start_pos": 114, "end_pos": 143, "type": "TASK", "confidence": 0.7243224879105886}]}, {"text": "We further applied reinforcement learning to learn an optimal policy that guides the robot in deciding when to ask what questions.", "labels": [], "entities": []}, {"text": "Our empirical results have shown that this interactive learning process leads to more reliable representations of grounded verb semantics, which contribute to significantly better action performance in new situations.", "labels": [], "entities": []}, {"text": "When the environment is noisy and uncertain (as in a realistic situation), the models acquired from interactive learning result in a performance gain between 48% and 145% when applied in new situations.", "labels": [], "entities": []}, {"text": "Our results further demonstrate that the interaction policy acquired from reinforcement learning leads to the most efficient interaction and the most reliable verb models.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our approach, we utilized the benchmark made available by.", "labels": [], "entities": []}, {"text": "Individual language commands and corresponding action sequences are extracted similarly as.", "labels": [], "entities": []}, {"text": "This dataset includes common tasks in the kitchen and living room domains, where each data instance comes with a language command (e.g., \"boil the water\", \"throw the beer into the trashcan\") and the corresponding sequence of primitive actions.", "labels": [], "entities": []}, {"text": "In total, there are 979 instances, including 75 different verbs and 215 different noun phrases.", "labels": [], "entities": []}, {"text": "The length of primitive action sequences range from 1 to 51 with an average of 4.82 (+/-4.8).", "labels": [], "entities": []}, {"text": "We divided the dataset into three groups: (1) 200 data instances were used by reinforcement learning to acquire optimal interaction policies; (2) 600 data instances were used by different approaches (i.e., previous approaches and our interactive learning approach) to acquire grounded verb semantics models; and (3) 179 data instances were used as testing data to evaluate the learned verb models.", "labels": [], "entities": []}, {"text": "The performance on applying the learned models to execute actions for the testing data is reported.", "labels": [], "entities": []}, {"text": "To learn interaction policies, a simulated human model is created from the dataset () to continuously interact with the robot learner . This simulated user can answer the robot's different types of questions and make decisions on whether the robot's execution is correct.", "labels": [], "entities": []}, {"text": "During policy learning, one data instance can be used multiple times.", "labels": [], "entities": [{"text": "policy learning", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.9227420687675476}]}, {"text": "At each time, the interaction sequence is different due to exploitation and exploration in RL in selecting the next action.", "labels": [], "entities": []}, {"text": "The RL discount factor \u03b3 is set to 0.99, the ingreedy is 0.1, and the learning rate is 0.01.", "labels": [], "entities": [{"text": "RL discount factor \u03b3", "start_pos": 4, "end_pos": 24, "type": "METRIC", "confidence": 0.9502034187316895}, {"text": "learning rate", "start_pos": 70, "end_pos": 83, "type": "METRIC", "confidence": 0.9729328155517578}]}, {"text": "The original data provided by) is based on the assumption that environment sensing is perfect and deterministic.", "labels": [], "entities": []}, {"text": "To enable incomplete and noisy environment representation, for each fluent (e.g., grasping(Cup 3 ), near(robot 1 , Cup 3 )) in the original data, we independently sampled a confidence value to simulate the likelihood that a particular fluent can be detected correctly from the environment.", "labels": [], "entities": []}, {"text": "We applied the following four different variations in sampling the confidence values, which correspond to different levels of sensor reliability.", "labels": [], "entities": []}, {"text": "(1) PerfectEnv represents the most reliable sensor.", "labels": [], "entities": [{"text": "PerfectEnv", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.5538531541824341}]}, {"text": "If a fluent is true in the original data, its sampled confidence is 1, and 0 otherwise.", "labels": [], "entities": []}, {"text": "(2) NormStd3 represents a relatively reliable sensor.", "labels": [], "entities": [{"text": "NormStd3", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7823905348777771}]}, {"text": "For each fluent in the original environment, a confidence is sampled according to a normal distribution N (1, 0.3 2 ) with an interval.", "labels": [], "entities": []}, {"text": "This distribution has a large probability of sampling a number larger than 0.5, meaning the corresponding fluent is still more likely to be true.", "labels": [], "entities": []}, {"text": "(3) NormStd5 represents a less reliable sensor.", "labels": [], "entities": [{"text": "NormStd5", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.8126189708709717}]}, {"text": "The sampling distribution is N (1, 0.5 2 ), which has a larger probability of generating a number smaller than 0.5 compared to NormStd3.", "labels": [], "entities": [{"text": "NormStd3", "start_pos": 127, "end_pos": 135, "type": "DATASET", "confidence": 0.9665997624397278}]}, {"text": "(4) UniEnv represents an unreliable sensor.", "labels": [], "entities": []}, {"text": "Each number is sampled with a uniform distribution between 0 and 1.", "labels": [], "entities": []}, {"text": "This means the sensor works randomly.", "labels": [], "entities": []}, {"text": "A fluent has a equal change to be true or false no matter what the true environment is.", "labels": [], "entities": []}, {"text": "We used the same evaluation metrics as in the previous works to evaluate the performance of applying the learned models to testing instances on action planning.", "labels": [], "entities": []}, {"text": "\u2022 IED: Instruction Editing Distance.", "labels": [], "entities": [{"text": "IED", "start_pos": 2, "end_pos": 5, "type": "DATASET", "confidence": 0.567317545413971}, {"text": "Instruction Editing Distance", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.7802740434805552}]}, {"text": "This is a number between 0 and 1 measuring the similarity between the predicted action sequence and the ground-truth action sequence.", "labels": [], "entities": []}, {"text": "IED equals 1 if the two sequences are exactly the same.", "labels": [], "entities": [{"text": "IED", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.881392776966095}]}, {"text": "\u2022 SJI: State Jaccard Index.", "labels": [], "entities": [{"text": "State Jaccard Index", "start_pos": 7, "end_pos": 26, "type": "DATASET", "confidence": 0.9366130431493124}]}, {"text": "This is a number between 0 and 1 measuring the similarity between the predicted and the ground-truth state changes.", "labels": [], "entities": []}, {"text": "SJI equals 1 if action planning leads to exactly the same state change as in the ground-truth.", "labels": [], "entities": [{"text": "SJI", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8573756814002991}]}, {"text": "To understand the role of interactive learning in model acquisition and action planning, we first compared the interactive learning approach with the previous leading approach (presented as She16).", "labels": [], "entities": [{"text": "model acquisition", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7343093752861023}, {"text": "action planning", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.6878648549318314}]}, {"text": "To further evaluate the interaction policies acquired by reinforcement learning, we also compared the learned policy (i.e., RLPolicy) with the following two baseline policies: \u2022 RandomPolicy which randomly selects questions to ask during interaction.", "labels": [], "entities": [{"text": "RLPolicy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9832331538200378}]}, {"text": "\u2022 ManualPolicy which continuously asks for yes/no confirmations (i.e., object grounding questions (GroundQ), environment questions (EnvQ), goal prediction questions (GoalQ)) until there's no more questions before making a decision on model acquisition or action execution.", "labels": [], "entities": [{"text": "model acquisition", "start_pos": 234, "end_pos": 251, "type": "TASK", "confidence": 0.745845764875412}]}, {"text": "When the environment becomes noisy (i.e., NormStd3, NormStd5, and UniEnv), the performance of She16 that only relies on demonstrations decreases significantly.", "labels": [], "entities": [{"text": "NormStd3", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9315575957298279}, {"text": "NormStd5", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9398489594459534}]}, {"text": "While the interactive learning improves the performance under the perfect environment condition, its effect in noisy environment is more remarkable.", "labels": [], "entities": []}, {"text": "It leads to a significant performance gain between 48% and 145%.", "labels": [], "entities": []}, {"text": "These results validate our hypothesis that interactive question answering can help to alleviate the problem of uncertainties in environment representation and goal prediction.", "labels": [], "entities": [{"text": "interactive question answering", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.677824467420578}, {"text": "goal prediction", "start_pos": 159, "end_pos": 174, "type": "TASK", "confidence": 0.7793803811073303}]}, {"text": "shows the performance of the various learned models on the testing data, based on a varying number of training instances and different interaction policies.", "labels": [], "entities": []}, {"text": "The interactive learning guided by the policy acquired from RL outperforms the previous approach She16.", "labels": [], "entities": [{"text": "RL", "start_pos": 60, "end_pos": 62, "type": "DATASET", "confidence": 0.8136334419250488}, {"text": "She16", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.9274700880050659}]}, {"text": "The RL policy slightly outperforms interactive learning using manually defined policy (i.e., ManualPolicy).", "labels": [], "entities": [{"text": "RL", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8825474381446838}]}, {"text": "However, as shown in the next section, the Man-  ualPolicy results in much longer interaction (i.e., more questions) than the RL acquired policy.", "labels": [], "entities": []}, {"text": "These results further demonstrate that the policy learned from RL enables efficient interactions and the acquisition of more reliable verb models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Performance comparison between She16  and our interactive learning based on environment  representations with different levels of noise. All  the improvements (marked *) are statistically sig- nificant (p < 0.01).", "labels": [], "entities": []}, {"text": " Table 5: Comparison between different policies including the average number (and standard deviation)  of different types of questions asked during the execution phase and the learning phase respectively, and  the performance on action planning for the testing data. The results are based on the noisy environment  sampled by NormStd3. * indicates statistically significant difference (p < 0.05) comparing RLPolicy  with ManualPolicy.", "labels": [], "entities": [{"text": "NormStd3", "start_pos": 326, "end_pos": 334, "type": "DATASET", "confidence": 0.9306962490081787}]}]}