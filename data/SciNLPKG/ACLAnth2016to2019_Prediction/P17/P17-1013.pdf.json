{"title": [{"text": "Deep Neural Machine Translation with Linear Associative Unit", "labels": [], "entities": [{"text": "Deep Neural Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.5376948565244675}]}], "abstractContent": [{"text": "Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art Neural Machine Translation (NMT) with their capability in modeling complex functions and capturing complex linguistic structures.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 72, "end_pos": 104, "type": "TASK", "confidence": 0.8587908645470937}]}, {"text": "However NMT systems with deep architecture in their encoder or decoder RNNs often suffer from severe gradient diffusion due to the non-linear recurrent activations, which often make the optimization much more difficult.", "labels": [], "entities": []}, {"text": "To address this problem we propose novel linear associative units (LAU) to reduce the gradient propagation length inside the recurrent unit.", "labels": [], "entities": []}, {"text": "Different from conventional approaches (LSTM unit and GRU), LAUs utilizes linear associative connections between input and output of the recurrent unit, which allows unimpeded information flow through both space and time direction.", "labels": [], "entities": []}, {"text": "The model is quite simple, but it is surprisingly effective.", "labels": [], "entities": []}, {"text": "Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.45282913744449615}, {"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.998508632183075}]}, {"text": "On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art.", "labels": [], "entities": [{"text": "WMT14 English-German task", "start_pos": 3, "end_pos": 28, "type": "DATASET", "confidence": 0.8482651909192404}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7782198240359625}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7473925352096558}]}, {"text": "Unlike conventional Statistical Machine Translation (SMT) systems (;) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.8160842855771383}]}, {"text": "Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation ( ).", "labels": [], "entities": []}, {"text": "Driven by the breakthrough achieved in computer vision (, research in NMT has recently turned towards studying Deep Neural Networks (DNNs). and found that deep architectures in both the encoder and decoder are essential for capturing subtle irregularities in the source and target languages.", "labels": [], "entities": []}, {"text": "However, training a deep neural network is not as simple as stacking layers.", "labels": [], "entities": []}, {"text": "Optimization often becomes increasingly difficult with more layers.", "labels": [], "entities": []}, {"text": "One reasonable explanation is the notorious problem of vanishing/exploding gradients which was first studied in the context of vanilla RNNs ().", "labels": [], "entities": []}, {"text": "Most prevalent approaches to solve this problem rely on short-cut connections between adjacent layers such as residual or fastforward connections.", "labels": [], "entities": []}, {"text": "Differ-ent from previous work, we choose to reduce the gradient path inside the recurrent units and propose a novel Linear Associative Unit (LAU) which creates a fusion of both linear and nonlinear transformations of the input.", "labels": [], "entities": []}, {"text": "Through this design, information can flow across several steps both in time and in space with little attenuation.", "labels": [], "entities": []}, {"text": "The mechanism makes it easy to train deep stack RNNs which can efficiently capture the complex inherent structures of sentences for NMT.", "labels": [], "entities": []}, {"text": "Based on LAUs, we also propose a NMT model , called DEEPLAU, with deep architecture in both the encoder and decoder.", "labels": [], "entities": [{"text": "DEEPLAU", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.7193270325660706}]}, {"text": "Although DEEPLAU is fairly simple, it gives remarkable empirical results.", "labels": [], "entities": [{"text": "DEEPLAU", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.3743743598461151}]}, {"text": "On the NIST Chinese-English task, DEEPLAU with proper settings yields the best reported result and also a 4.9 BLEU improvement over a strong NMT baseline with most known techniques (e.g, dropout) incorporated.", "labels": [], "entities": [{"text": "NIST Chinese-English task", "start_pos": 7, "end_pos": 32, "type": "DATASET", "confidence": 0.8340288003285726}, {"text": "DEEPLAU", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9976524710655212}, {"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9990636706352234}]}, {"text": "On WMT English-German and English-French tasks, it also achieves performance superior or comparable to the state-of-the-art.", "labels": [], "entities": [{"text": "WMT English-German", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.4807244837284088}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Case-insensitive BLEU scores on Chinese-English translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.935401976108551}]}, {"text": " Table 4:  BLEU scores of DEEPLAU and  DEEPGRU with different model sizes.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9993805885314941}, {"text": "DEEPLAU", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.6442888975143433}, {"text": "DEEPGRU", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.793536901473999}]}]}