{"title": [], "abstractContent": [{"text": "Sentences are important semantic units of natural language.", "labels": [], "entities": []}, {"text": "A generic, distributional representation of sentences that can capture the latent semantics is beneficial to multiple downstream applications.", "labels": [], "entities": []}, {"text": "We observe a simple geometry of sentences-the word representations of a given sentence (on average 10.23 words in all Se-mEval datasets with a standard deviation 4.84) roughly lie in a low-rank subspace (roughly, rank 4).", "labels": [], "entities": []}, {"text": "Motivated by this observation , we represent a sentence by the low-rank subspace spanned by its word vectors.", "labels": [], "entities": []}, {"text": "Such an unsupervised representation is empirically validated via semantic textual similarity tasks on 19 different datasets, where it outperforms the sophisticated neural network models, including skip-thought vectors, by 15% on average.", "labels": [], "entities": []}], "introductionContent": [{"text": "Real-valued word representations have brought afresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (.", "labels": [], "entities": []}, {"text": "Going beyond words, sentences capture much of the semantic information.", "labels": [], "entities": []}, {"text": "Given the success of lexical representations, a natural question of great topical interest is how to extend the power of distributional representations to sentences.", "labels": [], "entities": []}, {"text": "There are currently two approaches to represent sentences.", "labels": [], "entities": []}, {"text": "A sentence contains rich syntactic information and can be modeled through sophisticated neural networks (e.g., convolutional neural networks), recurrent neural networks () and recursive neural networks).", "labels": [], "entities": []}, {"text": "Another simple and common approach ignores the latent structure of sentences: a prototypical approach is to represent a sentence by summing or averaging over the vectors of the words in this sentence (.", "labels": [], "entities": []}, {"text": "Recently,; reveal that even though the latter approach ignores all syntactic information, it is simple, straightforward, and remarkably robust at capturing the sentential semantics.", "labels": [], "entities": []}, {"text": "Such an approach successfully outperforms the neural network based approaches on textual similarity tasks in both supervised and unsupervised settings.", "labels": [], "entities": []}, {"text": "We follow the latter approach but depart from representing sentences in a vector space as in these prior works; we present a novel Grassmannian property of sentences.", "labels": [], "entities": []}, {"text": "The geometry is motivated by where an interesting phenomenon is observed -the local context of a given word/phrase can be well represented by a low rank subspace.", "labels": [], "entities": []}, {"text": "We propose to generalize this observation to sentences: not only do the word vectors in a snippet of a sentence (i.e., a context fora given word defined as several words surrounding it) lie in a low-rank subspace, but the entire sentence (on average 10.23 words in all SemEval datasets with standard deviation 4.84) follows this geometric property as well: Geometry of Sentences: The word representations lie in a low-rank subspaces (rank 3-5) for all words in a target sen-tence.", "labels": [], "entities": [{"text": "SemEval datasets", "start_pos": 269, "end_pos": 285, "type": "DATASET", "confidence": 0.6973559111356735}]}, {"text": "The observation indicates that the subspace contains most of the information about this sentence, and therefore motivates a sentence representation method: the sentences should be represented in the space of subspaces (i.e., the Grassmannian manifold) instead of a vector space; formally: Sentence Representation: A sentence can be represented by a low-rank subspace spanned by its word representations.", "labels": [], "entities": [{"text": "Sentence Representation", "start_pos": 289, "end_pos": 312, "type": "TASK", "confidence": 0.9677282571792603}]}, {"text": "Analogous to word representations of similar words being similar vectors, the principle of sentence representations is: similar sentences should have similar subspaces.", "labels": [], "entities": []}, {"text": "Two questions arise: (a) how to define the similarity between sentences and (b) how to define the similarity between subspaces.", "labels": [], "entities": []}, {"text": "The first question has been already addressed by the popular semantic textual similarity (STS) tasks.", "labels": [], "entities": [{"text": "semantic textual similarity (STS) tasks", "start_pos": 61, "end_pos": 100, "type": "TASK", "confidence": 0.7523719710963113}]}, {"text": "Unlike textual entailment (which aims at inferring directional relation between two sentences) and paraphrasing (which is a binary classification problem), STS provides a unified framework of measuring the degree of semantic equivalence) in a continuous fashion.", "labels": [], "entities": []}, {"text": "Motivated by the cosine similarity between vectors being a good index for word similarity, we generalize this metric to subspaces: the similarity between subspaces defined in this paper is the 2 -norm of the singular values between two subspaces; note that the singular values are in fact the cosine of the principal angles.", "labels": [], "entities": []}, {"text": "The key justification for our approach comes from empirical results that outperform state-ofthe-art in some cases, and being comparable in others.", "labels": [], "entities": []}, {"text": "In summary, representing sentences by subspaces outperforms representing sentences by averaged word vectors (by 14% on average) and sophisticated neural networks (by 15%) on 19 different STS datasets, ranging over different domains (News, WordNet definition, and Twitter).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate our sentence representations empirically on the STS tasks.", "labels": [], "entities": []}, {"text": "The objective of this task is to test the degree to which the algorithm can capture the semantic equivalence between two sentences.", "labels": [], "entities": []}, {"text": "For example, the similarity between \"a kid jumping a ledge with a bike\" and \"a black and white cat playing with a blanket\" is 0 and the similarity between \"a cat standing on tree branches\" and \"a black and white cat is high upon tree branches\" is 3.6.", "labels": [], "entities": []}, {"text": "The algorithm is then evaluated in terms of Pearson's correlation between the predicted score and the human judgment.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 44, "end_pos": 65, "type": "METRIC", "confidence": 0.9556480447451273}]}, {"text": "Datasets We test the performances of our algorithm on 19 different datasets, which include SemEval STS share tasks (, sourced from multiple domains (for example, News, WordNet definitions and Twitter).", "labels": [], "entities": [{"text": "SemEval STS share tasks", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.6837093234062195}]}, {"text": "Baselines and Preliminaries Our main comparisons are with algorithms that perform unsupervised sentence representation: average of word representations (i.e., avg.", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 95, "end_pos": 118, "type": "TASK", "confidence": 0.730623871088028}]}, {"text": "(of GloVe and skipgram) where we use the average of word vectors), doc2vec (D2V) () and sophisticated neural networks (i.e., skip-thought vectors (ST) (, Siamese CBOW (SC) ().", "labels": [], "entities": []}, {"text": "In order to enable a fair comparison, we use the Toronto Book Corpus () to train word embeddings.", "labels": [], "entities": [{"text": "Toronto Book Corpus", "start_pos": 49, "end_pos": 68, "type": "DATASET", "confidence": 0.9744433164596558}]}, {"text": "In our experiment, we adapt the same setting as in () where we use skip-gram () to train a 300-dimensional word vectors for the words that occur 5 times or more in the training corpus.", "labels": [], "entities": []}, {"text": "The rank of subspaces is set to be 4 for both word2vec and GloVe.", "labels": [], "entities": []}, {"text": "Results The detailed results are reported in Table 1, from where we can observe two phenomena: (a) representing sentences by its averaged word vectors provides a strong baseline and the performances are remarkably stable across different datasets; (b) our subspace-based method outperforms the average-based method by 14% on average and the neural network based approaches by 15%.", "labels": [], "entities": []}, {"text": "This suggests that representing sentences by subspaces maintains more information than simply taking the average, and is more robust than highly-tuned sophisticated models.", "labels": [], "entities": []}, {"text": "When we average over the words, the average vector is biased because of many irrelevant words (for example, function words) in a given sentence.", "labels": [], "entities": []}, {"text": "Therefore, given a longer sentence, the effect of useful word vectors become smaller and thus the average vector is less reliable at capturing the semantics.", "labels": [], "entities": []}, {"text": "On the other hand, the subspace representation is immune to this phenomenon: the word vectors capturing the semantics of the sentence tend to concentrate on a few directions which dominate the subspace representation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson's correlation (x100) on SemEval textual similarity task using 19 different datasets,  where l is the average sentence length of each dataset. Results that are better than the baselines are  marked with underlines and the best results are in bold.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.5282665292421976}, {"text": "SemEval textual similarity task", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.8453804850578308}]}]}