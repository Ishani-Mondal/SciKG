{"title": [{"text": "A Convolutional Encoder Model for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.7389920155207316}]}], "abstractContent": [{"text": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.7049916585286459}]}, {"text": "We present a faster and simpler architecture based on a succession of convolutional layers.", "labels": [], "entities": []}, {"text": "This allows to encode the source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.", "labels": [], "entities": []}, {"text": "On WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and on WMT'15 English-German we outper-form several recently published results.", "labels": [], "entities": [{"text": "WMT'16 English-Romanian translation", "start_pos": 3, "end_pos": 38, "type": "DATASET", "confidence": 0.8397279580434164}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9991174340248108}, {"text": "WMT'15", "start_pos": 102, "end_pos": 108, "type": "DATASET", "confidence": 0.9756598472595215}]}, {"text": "Our models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9995023012161255}, {"text": "WMT'14 English-French translation", "start_pos": 72, "end_pos": 105, "type": "DATASET", "confidence": 0.9492101271947225}]}, {"text": "We speedup CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9977284073829651}]}], "introductionContent": [{"text": "Neural machine translation (NMT) is an end-to-end approach to machine translation ( ).", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8030307193597158}, {"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7524738609790802}]}, {"text": "The most successful approach to date encodes the source sentence with a bi-directional recurrent neural network (RNN) into a variable length representation and then generates the translation left-to-right with another RNN where both components interface via a soft-attention mechanism ().", "labels": [], "entities": []}, {"text": "Recurrent networks are typically parameterized as long short term memory networks (LSTM; Hochreiter et al.", "labels": [], "entities": []}, {"text": "1997) or gated recurrent units (GRU;), often with residual or skip connections () to enable stacking of several layers ( \u00a72).", "labels": [], "entities": []}, {"text": "There have been several attempts to use convolutional encoder models for neural machine trans-lation in the past but they were either only applied to rescoring n-best lists of classical systems or were not competitive to recurrent alternatives ().", "labels": [], "entities": []}, {"text": "This is despite several attractive properties of convolutional networks.", "labels": [], "entities": []}, {"text": "For example, convolutional networks operate over a fixed-size window of the input sequence which enables the simultaneous computation of all features fora source sentence.", "labels": [], "entities": []}, {"text": "This contrasts to RNNs which maintain a hidden state of the entire past that prevents parallel computation within a sequence.", "labels": [], "entities": []}, {"text": "A succession of convolutional layers provides a shorter path to capture relationships between elements of a sequence compared to RNNs.", "labels": [], "entities": []}, {"text": "This also eases learning because the resulting tree-structure applies a fixed number of non-linearities compared to a recurrent neural network for which the number of non-linearities vary depending on the time-step.", "labels": [], "entities": []}, {"text": "Because processing is bottom-up, all words undergo the same number of transformations, whereas for RNNs the first word is over-processed and the last word is transformed only once.", "labels": [], "entities": []}, {"text": "In this paper we show that an architecture based on convolutional layers is very competitive to recurrent encoders.", "labels": [], "entities": []}, {"text": "We investigate simple average pooling as well as parameterized convolutions as an alternative to recurrent encoders and enable very deep convolutional encoders by using residual connections (.", "labels": [], "entities": []}, {"text": "We experiment on several standard datasets and compare our approach to variants of recurrent encoders such as uni-directional and bi-directional LSTMs.", "labels": [], "entities": []}, {"text": "On WMT'16 English-Romanian translation we achieve accuracy that is very competitive to the current state-of-the-art result.", "labels": [], "entities": [{"text": "WMT'16 English-Romanian translation", "start_pos": 3, "end_pos": 38, "type": "DATASET", "confidence": 0.8647768497467041}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9995359182357788}]}, {"text": "We perform competitively on WMT'15 English-German, and nearly match the performance of the best WMT'14 English-French system based on a deep LSTM setup when comparing on a commonly used subset of the training data (; \u00a74, \u00a75).", "labels": [], "entities": [{"text": "WMT'15 English-German", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.9337802529335022}, {"text": "WMT'14", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.8902072310447693}]}], "datasetContent": [{"text": "We evaluate different encoders and ablate architectural choices on a small dataset from the GermanEnglish machine translation track of IWSLT 2014 () with a similar setting to (.", "labels": [], "entities": [{"text": "GermanEnglish machine translation track of IWSLT 2014", "start_pos": 92, "end_pos": 145, "type": "DATASET", "confidence": 0.8970487373215812}]}, {"text": "Unless otherwise stated, we restrict training sentences to have no more than 175 words; test sentences are not filtered.", "labels": [], "entities": []}, {"text": "This is a higher threshold compared to other publications but ensures proper training of the position embeddings for non-recurrent encoders; the length threshold did not significantly effect recurrent encoders.", "labels": [], "entities": []}, {"text": "Length filtering results in 167K sentence pairs and we test on the concatenation of tst2010, tst2011, tst2012, tst2013 and dev2010 comprising 6948 sentence pairs.", "labels": [], "entities": [{"text": "Length filtering", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7500631213188171}]}, {"text": "3 Our final results are on three major WMT tasks: WMT'16 English-Romanian.", "labels": [], "entities": [{"text": "WMT tasks", "start_pos": 39, "end_pos": 48, "type": "DATASET", "confidence": 0.6862148642539978}, {"text": "WMT'16 English-Romanian", "start_pos": 50, "end_pos": 73, "type": "DATASET", "confidence": 0.8577855825424194}]}, {"text": "We use the same data and pre-processing as (Sennrich et al., 2016a) and train on 2.8M sentence pairs.", "labels": [], "entities": []}, {"text": "Our model is word-based instead of relying on byte-pair encoding ().", "labels": [], "entities": []}, {"text": "We use all available parallel training data, namely Europarl v7, Com- mon Crawl and News Commentary v10 and apply the standard Moses tokenization to obtain 3.9M sentence pairs (.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9835578799247742}]}, {"text": "We report results on newstest2015.", "labels": [], "entities": [{"text": "newstest2015", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.9785398244857788}]}, {"text": "We use a commonly used subset of 12M sentence pairs, and remove sentences longer than 150 words.", "labels": [], "entities": []}, {"text": "This results in 10.7M sentence-pairs for training.", "labels": [], "entities": []}, {"text": "Results are reported on ntst14.", "labels": [], "entities": []}, {"text": "A small subset of the training data serves as validation set (5% for IWSLT'14 and 1% for WMT) for early stopping and learning rate annealing ( \u00a74.3).", "labels": [], "entities": [{"text": "IWSLT'14", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.8492810726165771}, {"text": "WMT", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.4799758493900299}]}, {"text": "For IWSLT'14, we replace words that occur fewer than 3 times with a <unk> symbol, which results in a vocabulary of 24158 English and 35882 German word types.", "labels": [], "entities": [{"text": "IWSLT'14", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.8168454170227051}]}, {"text": "For WMT datasets, we retain 200K source and 80K target words.", "labels": [], "entities": [{"text": "WMT datasets", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8365224301815033}]}, {"text": "For English-French only, we set the target vocabulary to 30K types to be comparable with previous work.", "labels": [], "entities": []}, {"text": "We report accuracy of single systems by training several identical models with different ran-dom seeds (5 for IWSLT'14, 3 for WMT) and pick the one with the best validation perplexity for final BLEU evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990610480308533}, {"text": "IWSLT'14", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.8370615243911743}, {"text": "BLEU", "start_pos": 194, "end_pos": 198, "type": "METRIC", "confidence": 0.98846834897995}]}, {"text": "Translations are generated by abeam search and we normalize log-likelihood scores by sentence length.", "labels": [], "entities": []}, {"text": "On IWSLT'14 we use abeam width of 10 and for WMT models we tune beam width and word penalty on a separate test set, that is newsdev2016 for WMT'16 English-Romanian, newstest2014 for WMT'15 English-German and ntst1213 for WMT'14 English-French.", "labels": [], "entities": [{"text": "IWSLT'14", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.8924393057823181}, {"text": "WMT'14", "start_pos": 221, "end_pos": 227, "type": "DATASET", "confidence": 0.9165741801261902}]}, {"text": "The word penalty adds a constant factor to log-likelihoods, except for the end-of-sentence token.", "labels": [], "entities": []}, {"text": "Prior to scoring the generated translations against the respective references, we perform unknown word replacement based on attention scores (.", "labels": [], "entities": [{"text": "unknown word replacement", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.6276484628518423}]}, {"text": "Unknown words are replaced by looking up the source word with the maximum attention score in a pre-computed dictionary.", "labels": [], "entities": []}, {"text": "If the dictionary contains no translation, then we simply copy the source word.", "labels": [], "entities": []}, {"text": "Dictionaries were extracted from the aligned training data that was aligned with fast align.", "labels": [], "entities": []}, {"text": "Each source word is mapped to the target word it is most frequently aligned to.", "labels": [], "entities": []}, {"text": "For convolutional encoders with stacked CNN-c layers we noticed for some models that the attention maxima were consistently shifted by one word.", "labels": [], "entities": []}, {"text": "We determine this per-model offset on the abovementioned development sets and correct for it.", "labels": [], "entities": []}, {"text": "Finally, we compute case-sensitive tokenized BLEU, except for WMT'16 English-Romanian where we use detokenized BLEU to be comparable with Sennrich et al.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9711102843284607}, {"text": "WMT'16 English-Romanian", "start_pos": 62, "end_pos": 85, "type": "DATASET", "confidence": 0.9042068123817444}, {"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.943282425403595}]}, {"text": "Next, we evaluate the BiLSTM encoder and the convolutional encoder architecture on three larger tasks and compare against previously published results.", "labels": [], "entities": [{"text": "BiLSTM encoder", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.8205830156803131}]}, {"text": "On WMT'16 English-Romanian translation we compare to (), the winning single system entry for this language pair.", "labels": [], "entities": [{"text": "WMT'16 English-Romanian translation", "start_pos": 3, "end_pos": 38, "type": "DATASET", "confidence": 0.912659207979838}]}, {"text": "Their model consists of a bi-directional GRU encoder, a GRU decoder and MLP-based attention.", "labels": [], "entities": []}, {"text": "They use byte pair encoding (BPE) to achieve openvocabulary translation and dropout in all components of the neural network to achieve 28.1 BLEU; we use the same pre-processing but no BPE ( \u00a74).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9521728754043579}, {"text": "BPE", "start_pos": 184, "end_pos": 187, "type": "METRIC", "confidence": 0.9761263132095337}]}, {"text": "The results show that a deep convolutional encoder can perform competitively to the state of the art on this dataset ().", "labels": [], "entities": []}, {"text": "Our bi-directional LSTM encoder baseline is 0.6 BLEU lower than the state of the art but uses only 512 hidden units compared to 1024.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.998456597328186}]}, {"text": "A singlelayer convolutional encoder with embedding size 256 performs at 27.1 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9937589168548584}]}, {"text": "Increasing the number of convolutional layers to 8 in CNN-a and 4 in CNN-c achieves 27.8 BLEU which outperforms our baseline and is competitive to the state of the art.", "labels": [], "entities": [{"text": "CNN-a", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.9027109146118164}, {"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9959670305252075}]}, {"text": "On WMT'15 English to German, we compare to a BiLSTM baseline and prior work: (Jean et al., 2015) introduce a large output vocabulary; the decoder of () operates on the character-level;) uses LSTMs instead of GRUs and feeds the conditional input to the output layer as well as to the decoder.", "labels": [], "entities": [{"text": "WMT'15 English to German", "start_pos": 3, "end_pos": 27, "type": "DATASET", "confidence": 0.8525594025850296}, {"text": "BiLSTM baseline", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.7816134095191956}]}, {"text": "Our single-layer BiLSTM baseline is competitive to prior work and a two-layer BiLSTM encoder performs 0.6 BLEU better at 24.1 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.998867392539978}, {"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9891191720962524}]}, {"text": "Previous work also used multi-layer setups, e.g.,) has two layers both in the encoder and the decoder with 1024 hidden units, and) use 1000 hidden units per LSTM.", "labels": [], "entities": []}, {"text": "We use 512 hidden units for both LSTM and convolutional encoders.", "labels": [], "entities": []}, {"text": "Our convolutional model with either 8 or 15 layers in CNN-a outperform the BiL-STM encoder with both a single decoder layer or two decoder layers.", "labels": [], "entities": []}, {"text": "Finally, we evaluate on the larger WMT'14 English-French corpus.", "labels": [], "entities": [{"text": "WMT'14 English-French corpus", "start_pos": 35, "end_pos": 63, "type": "DATASET", "confidence": 0.9497511585553488}]}, {"text": "On this dataset the recurrent architectures benefit from an additional layer both in the encoder and the decoder.", "labels": [], "entities": []}, {"text": "For a singlelayer decoder, a deep convolutional encoder outperforms the BiLSTM accuracy by 0.3 BLEU and fora two-layer decoder, our very deep convolutional encoder with up to 20 layers outperforms the BiLSTM by 0.4 BLEU.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.71761554479599}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.5134680867195129}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.8506097793579102}, {"text": "BLEU", "start_pos": 215, "end_pos": 219, "type": "METRIC", "confidence": 0.9894324541091919}]}, {"text": "It has 40% fewer parameters than the BiLSTM due to the smaller embedding sizes.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.8599893450737}]}, {"text": "We also outperform several previous systems, including the very deep encoder-decoder model proposed by.", "labels": [], "entities": []}, {"text": "Our best result is just 0.2 BLEU below () who use a very deep LSTM setup with a 9-layer encoder, a 7-layer decoder, shortcut connections and extensive regularization with dropout and L2 regularization.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9991852641105652}]}], "tableCaptions": [{"text": " Table 1: Accuracy of encoders with position fea- tures (wrd+pos) and without (wrd) in terms of  BLEU and perplexity (PPL) on IWSLT'14 Ger- man to English translation; results include unknown  word replacement. Deep Convolutional 6/3 is the  only multi-layer configuration, more layers for the  LSTMs did not improve accuracy on this dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9990528225898743}, {"text": "IWSLT'14 Ger- man", "start_pos": 126, "end_pos": 143, "type": "DATASET", "confidence": 0.8400589525699615}, {"text": "unknown  word replacement", "start_pos": 184, "end_pos": 209, "type": "TASK", "confidence": 0.6671489377816519}, {"text": "accuracy", "start_pos": 317, "end_pos": 325, "type": "METRIC", "confidence": 0.999014139175415}]}, {"text": " Table 2: Accuracy on three WMT tasks, including results published in previous work. For deep convolu- tional encoders, we include the number of layers in CNN-a and CNN-c, respectively.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9842385649681091}, {"text": "WMT tasks", "start_pos": 28, "end_pos": 37, "type": "TASK", "confidence": 0.9203196465969086}]}, {"text": " Table 3: Generation speed in source words per second on a single CPU core using vocabulary selection.", "labels": [], "entities": []}]}