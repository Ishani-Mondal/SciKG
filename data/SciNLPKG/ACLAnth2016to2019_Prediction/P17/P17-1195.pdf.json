{"title": [{"text": "Semantic Parsing of Pre-university Math Problems", "labels": [], "entities": []}], "abstractContent": [{"text": "We have been developing an end-to-end math problem solving system that accepts natural language input.", "labels": [], "entities": [{"text": "math problem solving", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.5855289697647095}]}, {"text": "The current paper focuses on how we analyze the problem sentences to produce logical forms.", "labels": [], "entities": []}, {"text": "We chose a hybrid approach combining a shallow syntactic analyzer and a manually-developed lexicalized grammar.", "labels": [], "entities": []}, {"text": "A feature of the grammar is that it is extensively typed on the basis of a formal ontology for pre-university math.", "labels": [], "entities": []}, {"text": "These types are helpful in semantic disambiguation inside and across sentences.", "labels": [], "entities": [{"text": "semantic disambiguation", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.7711290717124939}]}, {"text": "Experimental results show that the hybrid system produces a well-formed logical form with 88% precision and 56% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9994826316833496}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9991195797920227}]}], "introductionContent": [{"text": "Frege and Russell, the initiators of the mathematical logic, delved also into the exploration of a theory of natural language semantics.", "labels": [], "entities": [{"text": "mathematical logic", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7506611943244934}]}, {"text": "Since then, symbolic logic has been a fundamental tool and a source of inspiration in the study of language meaning.", "labels": [], "entities": [{"text": "symbolic logic", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.7203948348760605}]}, {"text": "It suggests that the formalization of the two realms, mathematical reasoning and language meaning, is actually the two sides of the same coin -probably, we could not even conceive the idea of formalizing language meaning without grounding it onto mathematical reasoning.", "labels": [], "entities": []}, {"text": "This point was first clarified by) mainly on formal languages and then extended to natural languages by. further embodied it by putting forward a terrifyingly arrogant and attractive idea of seeing a natural language as a formal language.", "labels": [], "entities": []}, {"text": "The automation of end-to-end math problem solving thus has an outstanding status in the reDefine the two straight lines L1 and L2 on the xy-plane as L1: y = 0 (x-axis) and L2: y = \u221a 3x.", "labels": [], "entities": [{"text": "problem solving", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.728904515504837}]}, {"text": "Let P be a point on the xy-plane.", "labels": [], "entities": []}, {"text": "Let Q be the point symmetric to P about the straight line L1, and let R be the point symmetric to P about the straight line L2.", "labels": [], "entities": []}, {"text": "Answer the following questions: (1) Let (a, b) be the coordinates of P , then represent the coordinates of R using a and b.", "labels": [], "entities": []}, {"text": "(2) Assuming that the distance between the two points Q and R is 2, find the locus C of P . (3) When the point P moves on C, find the maximum area of the triangle P QR and the coordinates of P that gives the maximum area.", "labels": [], "entities": []}, {"text": "(Hokkaido Univ., 1999-Sci-3): Example problem search themes in natural language processing.", "labels": [], "entities": [{"text": "Hokkaido Univ., 1999-Sci-3", "start_pos": 1, "end_pos": 27, "type": "DATASET", "confidence": 0.9347331821918488}]}, {"text": "The conceptual basis has been laid down, which connects text to the truth (= answer) through reasoning.", "labels": [], "entities": []}, {"text": "However, we have not seen a fully automated system that instantiates it end-to-end.", "labels": [], "entities": []}, {"text": "We wish to add apiece to the big picture by materializing it.", "labels": [], "entities": []}, {"text": "Past studies have mainly targeted at primary school level arithmetic word problems).", "labels": [], "entities": []}, {"text": "In their nature, arithmetic questions are quantifier-free.", "labels": [], "entities": []}, {"text": "Moreover they tend to include only \u2227 (and) as the logical connective.", "labels": [], "entities": []}, {"text": "The main challenge in these works was to extract simple numerical relations (most typically equations) from a real-world scenario described in a text.", "labels": [], "entities": []}, {"text": "took SAT geometry questions as their benchmark.", "labels": [], "entities": [{"text": "SAT geometry questions", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.7940528094768524}]}, {"text": "However, the nature of SAT geometry questions restricts the resulting formula's complexity.", "labels": [], "entities": [{"text": "SAT geometry", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.8132455348968506}]}, {"text": "In \u00a73, we will show that none of them includes \u2200 (for all), \u2228 (or) or \u2192 (implies  We take pre-university math problems falling in the theory of real-closed fields (RCF) as our benchmark because of their variety and complexity.", "labels": [], "entities": []}, {"text": "The subject areas include real and linear algebra, complex numbers, calculus, and geometry.", "labels": [], "entities": []}, {"text": "Furthermore, many problems involve more than one subject: e.g., algebraic curves and calculus as in.", "labels": [], "entities": []}, {"text": "Their logical forms include all the logical connectives, quantifiers, and \u03bb-abstraction.", "labels": [], "entities": []}, {"text": "Our goal is to recognize the complex logical structures precisely, including the scopes of the quantifiers and other logical operators.", "labels": [], "entities": []}, {"text": "In the rest of the paper, we first present an overview of an end-to-end problem solving system ( \u00a72) and analyze the complexity of the preuniversity math benchmark in comparison with others ( \u00a73).", "labels": [], "entities": [{"text": "problem solving", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.7300218790769577}, {"text": "preuniversity math benchmark", "start_pos": 135, "end_pos": 163, "type": "DATASET", "confidence": 0.6748602191607157}]}, {"text": "Among the modules in the end-to-end system, we focus on the sentence-level semantic parsing component and describe an extensivelytyped grammar ( \u00a74 and \u00a75), an analyzer for the math expressions in the text ( \u00a76), and two semantic parsing techniques to fight against the scarcity of the training data ( \u00a77) and the complexity of the domain ( \u00a78).", "labels": [], "entities": [{"text": "sentence-level semantic parsing", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.6283631920814514}, {"text": "semantic parsing", "start_pos": 221, "end_pos": 237, "type": "TASK", "confidence": 0.75339475274086}]}, {"text": "Experimental results show the effectiveness of the presented techniques as well as the complexity of the task through an in-depth analysis of the end-to-end problem solving results ( \u00a79).", "labels": [], "entities": []}, {"text": "2 End-to-end Math Problem Solving presents an overview of our end-to-end math problem solving system.", "labels": [], "entities": [{"text": "End-to-end Math Problem Solving", "start_pos": 2, "end_pos": 33, "type": "TASK", "confidence": 0.5865244194865227}, {"text": "end-to-end math problem solving", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.7023089528083801}]}, {"text": "A math problem text is firstly analyzed with a dependency parser.", "labels": [], "entities": []}, {"text": "Anaphoric and coreferential expressions in the text are then identified and their antecedents are determined.", "labels": [], "entities": []}, {"text": "We assume the math formulas in the problems are encoded in MathML presentation mark-up.", "labels": [], "entities": [{"text": "MathML presentation mark-up", "start_pos": 59, "end_pos": 86, "type": "DATASET", "confidence": 0.8574011524518331}]}, {"text": "A specialized parser processes each one of them to determine its syntactic category and semantic content.", "labels": [], "entities": []}, {"text": "The semantic representation of each sentence is determined by a semantic parser based on Combinatory Categorial Grammar (CCG): Performance of the reasoning module on manually formalized pre-university problems After the sentence-level processing steps, we determine the logical relations among the sentence-level logical forms (discourse parsing) by a simple rule-based system.", "labels": [], "entities": []}, {"text": "It produces a tree structure whose leaves are labeled with sentences and internal nodes with logical connectives.", "labels": [], "entities": []}, {"text": "Free variables in the logical form are then bound by some quantifiers (or kept free) and their scopes are determined according to the logical structure of the problem.", "labels": [], "entities": []}, {"text": "A semantic representation of a problem is obtained as a formula in a higher-order logic through these language analysis steps.", "labels": [], "entities": []}, {"text": "The logical representation is then rewritten using a set of axioms that define the meanings of the predicate and function symbols in the formula, such as maximum defined as follows: as well as several logical rules such as \u03b2-reduction.", "labels": [], "entities": []}, {"text": "We hope to obtain a representation of the initial problem expressed in a decidable math theory such as RCF through these equivalencepreserving rewriting.", "labels": [], "entities": []}, {"text": "Once we find such a formula, we invoke a computer algebra system (CAS) or an automatic theorem prover (ATP) to derive the answer.", "labels": [], "entities": []}, {"text": "The reasoning module (i.e., the formula rewriting and the deduction with CAS and ATP) of the system has been extensively tested on a large collection of manually formalized pre-university math problems that includes more than 1,500 problems.", "labels": [], "entities": [{"text": "CAS", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9872560501098633}]}, {"text": "It solves 70% of the them in the time limit of 10 minutes per problem.", "labels": [], "entities": []}, {"text": "shows the rate of successfully solved problems in the manually formalized version of the benchmark problems used in the current paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents the overall performance of the current end-to-end system and demonstrates the effectiveness of the proposed parsing techniques.", "labels": [], "entities": [{"text": "parsing", "start_pos": 130, "end_pos": 137, "type": "TASK", "confidence": 0.9683325886726379}]}, {"text": "We also present an analysis of the failures.", "labels": [], "entities": []}, {"text": "presents the result of end-to-end problem solving on the UNIV data.", "labels": [], "entities": [{"text": "problem solving", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.734490156173706}, {"text": "UNIV data", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.9862010478973389}]}, {"text": "It shows the failure in the semantic parsing is a major bottleneck in the current system.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7726112902164459}]}, {"text": "Since a problem in UNIV includes more than three sentences on average, parsing a whole problem is quite a high bar fora semantic parser.", "labels": [], "entities": []}, {"text": "It is however necessary to solve it by the nature of the task.", "labels": [], "entities": []}, {"text": "Once a problem-level logical form was produced, the system yielded a correct solution for 44% of such problems in DEV and 36% in TEST.", "labels": [], "entities": [{"text": "DEV", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.928158164024353}, {"text": "TEST", "start_pos": 129, "end_pos": 133, "type": "DATASET", "confidence": 0.7150849103927612}]}, {"text": "lists the fraction of the sentences on which the two-step parser produced a CCG tree within top-N dependency trees.", "labels": [], "entities": []}, {"text": "We compared the results obtained with the dependency parser trained only on a news corpus (News) (, which is annotated with bunsetsu level dependencies, and that trained additionally with a math problem corpus consisting of 6,000 sentences 6 (News+Math).", "labels": [], "entities": []}, {"text": "The math problem corpus was developed according to the same annotation guideline for the news corpus.", "labels": [], "entities": []}, {"text": "The attachment accuracy of the dependency parser was 84% on math problem text when trained only on the news corpus but improved to 94% by the addition of the math problem corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.8523370027542114}]}, {"text": "The performance gain by increasing N is more evident in the results with the News parser than that with the News+Math parser.", "labels": [], "entities": [{"text": "News", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.9509350657463074}, {"text": "News+Math parser", "start_pos": 108, "end_pos": 124, "type": "DATASET", "confidence": 0.9130439609289169}]}, {"text": "It suggests the grammar properly rejected wrong dependency trees, which were ranked higher by the News parser.", "labels": [], "entities": [{"text": "News parser", "start_pos": 98, "end_pos": 109, "type": "DATASET", "confidence": 0.9279093444347382}]}, {"text": "The effect of the additional training is very large at small N sand still significant at N = 20.", "labels": [], "entities": []}, {"text": "It means that we successfully boosted both the speed and the success rate of CCG parsing only with the shallow dependency annotation on in-domain data.: Reasons for the parse failures shows the effect of CCG parsing with type environments.", "labels": [], "entities": [{"text": "speed", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.998333752155304}, {"text": "CCG parsing", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.6937202215194702}, {"text": "CCG parsing", "start_pos": 204, "end_pos": 215, "type": "TASK", "confidence": 0.6672931015491486}]}, {"text": "The column headed 'Typing failure' is the fraction of the problems on which no logical form was obtained due to typing failure.", "labels": [], "entities": [{"text": "Typing", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.8232854604721069}]}, {"text": "Parsing with type environment eliminated almost all such failures and significantly improved the number of correct answers.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9351767897605896}]}, {"text": "The remaining type failure was due to beam thresholding where a necessary derivation fell out of the beam.", "labels": [], "entities": [{"text": "beam thresholding", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.8650476634502411}]}, {"text": "lists the reasons for the parse failures on 1/4 of the TEST section (the problems taken from exams on 2007).", "labels": [], "entities": [{"text": "parse", "start_pos": 26, "end_pos": 31, "type": "TASK", "confidence": 0.8984323740005493}, {"text": "TEST section", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.7981949150562286}]}, {"text": "In the table, \"unknown usage\" means a missing lexical item fora word already in the lexicon.", "labels": [], "entities": []}, {"text": "\"Unknown word\" means no lexical item was defined for the word.", "labels": [], "entities": []}, {"text": "Collecting unknown usages (especially that of a function word) is much harder than just compiling a list of words.", "labels": [], "entities": [{"text": "Collecting unknown usages", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8789660334587097}]}, {"text": "Our experience in the lexicon development tells us that once we find a usage example, in the large majority of the cases, it is not difficult to write down its syntactic category and semantic function.", "labels": [], "entities": []}, {"text": "Table 9 suggests that we can efficiently detect and collect unknown word usages through parsing failures on a large raw corpus of math problems.", "labels": [], "entities": []}, {"text": "presents the accuracy of the sentenceand problem-level logical forms produced on the year 1999 subset of DEV and the year 2007 subset of TEST.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9993869066238403}, {"text": "DEV", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.8595407605171204}, {"text": "TEST", "start_pos": 137, "end_pos": 141, "type": "DATASET", "confidence": 0.780603289604187}]}, {"text": "Although the recall on the unseen test data is not as high as we hope, the high precision of the sentence-level logical forms is encouraging.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9996947050094604}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9990583062171936}]}, {"text": "provides the counts of the error types found in the wrong sentence-level logical forms produced on.", "labels": [], "entities": []}, {"text": "It reveals the majority of the errors are related to the choice of quantifier (\u2203, \u2200, or free) and logical op-: Types of errors in the logical forms erators (e.g., \u2192 vs. \u2194) as well as the determination of their scopes.", "labels": [], "entities": []}, {"text": "Meanwhile, we did not find an error related to the predicate-argument structure of a logical form.", "labels": [], "entities": []}, {"text": "This fact and the results in suggest that the selectional restrictions, encoded in the lexicon, properly rejected nonsensical predicate-argument relations.", "labels": [], "entities": []}, {"text": "Our next step is to introduce a more sophisticated disambiguation model on top of the grammar, enjoying the properly confined search space.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Profile of pre-university math benchmark data and other semantic parsing benchmark data sets", "labels": [], "entities": []}, {"text": " Table 6: Result of end-to-end problem solving", "labels": [], "entities": [{"text": "Result of end-to-end problem solving", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.6105934977531433}]}, {"text": " Table 7: Fraction of sentences on which a CCG  tree was obtained in top N dependency trees", "labels": [], "entities": []}, {"text": " Table 8: Effect of parsing with type environment", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9690973162651062}]}, {"text": " Table 10: Accuracy of logical forms", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9883912801742554}]}, {"text": " Table 11: Types of errors in the logical forms", "labels": [], "entities": []}]}