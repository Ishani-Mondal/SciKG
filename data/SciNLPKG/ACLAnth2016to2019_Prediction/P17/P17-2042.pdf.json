{"title": [{"text": "Improving Implicit Discourse Relation Recognition with Discourse-specific Word Embeddings", "labels": [], "entities": [{"text": "Improving Implicit Discourse Relation Recognition", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.9120582461357116}]}], "abstractContent": [{"text": "We introduce a simple and effective method to learn discourse-specific word embeddings (DSWE) for implicit discourse relation recognition.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 98, "end_pos": 137, "type": "TASK", "confidence": 0.6131614148616791}]}, {"text": "Specifically, DSWE is learned by performing connective classification on massive explicit discourse data, and capable of capturing discourse relationships between words.", "labels": [], "entities": [{"text": "connective classification", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.7925263941287994}]}, {"text": "On the PDTB data set, using DSWE as features achieves significant improvements over baselines.", "labels": [], "entities": [{"text": "PDTB data set", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9769526124000549}]}], "introductionContent": [{"text": "Recognizing discourse relations (e.g., Contrast, Conjunction) between two sentences is a crucial subtask of discourse structure analysis.", "labels": [], "entities": [{"text": "discourse structure analysis", "start_pos": 108, "end_pos": 136, "type": "TASK", "confidence": 0.7361851334571838}]}, {"text": "These relations can benefit many downstream NLP tasks, including question answering, machine translation and soon.", "labels": [], "entities": [{"text": "question answering", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.9191671013832092}, {"text": "machine translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.8338207304477692}]}, {"text": "A discourse relation instance is usually defined as a discourse connective (e.g., but, and) taking two arguments (e.g., clause, sentence).", "labels": [], "entities": []}, {"text": "For explicit discourse relation recognition, using only connectives as features achieves more than 93% inaccuracy . Without obvious clues like connectives, implicit discourse relation recognition is still challenging.", "labels": [], "entities": [{"text": "explicit discourse relation recognition", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.6041193678975105}, {"text": "implicit discourse relation recognition", "start_pos": 156, "end_pos": 195, "type": "TASK", "confidence": 0.5886875987052917}]}, {"text": "The earlier researches usually develop linguistically informed features and use supervised learning method to perform the task (.", "labels": [], "entities": []}, {"text": "Among these features, word pairs occurring in argument pairs are considered as important features, since they can partially catch discourse relationships between two arguments.", "labels": [], "entities": []}, {"text": "For example, synonym word pairs like (good, great) may indicate a Conjunction relation, while antonym word pairs like (good, bad) * Corresponding author.", "labels": [], "entities": []}, {"text": "may mean a Contrast relation.", "labels": [], "entities": []}, {"text": "However, classifiers based on word pairs in previous work do notwork well because of the data sparsity problem.", "labels": [], "entities": []}, {"text": "To address this problem, recent researches use word embeddings (aka distributed representations) instead of words as input features, and design various neural networks to capture discourse relationships between arguments (.", "labels": [], "entities": []}, {"text": "While these researches achieve promising results, they are all based on pre-trained word embeddings ignoring discourse information (e.g., good, great, and bad are often mapped into close vectors).", "labels": [], "entities": []}, {"text": "Intuitively, using word embeddings sensitive to discourse relations would further boost the performance.", "labels": [], "entities": []}, {"text": "In this paper, we propose to learn discoursespecific word embeddings (DSWE) from explicit data for implicit discourse relation recognition.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 99, "end_pos": 138, "type": "TASK", "confidence": 0.610345758497715}]}, {"text": "Our method is inspired by the observation that synonym (antonym) word pairs tend to appear around the discourse connective and (but).", "labels": [], "entities": []}, {"text": "Other connectives can also provide some discourse clues.", "labels": [], "entities": []}, {"text": "We expect to encode these discourse clues into the distributed representations of words, to capture discourse relationships between them.", "labels": [], "entities": []}, {"text": "To this end, we use a simple neural network to perform connective classification on massive explicit data.", "labels": [], "entities": [{"text": "connective classification", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.817892462015152}]}, {"text": "Explicit data can be considered to be automatically labeled by connectives.", "labels": [], "entities": []}, {"text": "While they cannot be directly used as training data for implicit discourse relation recognition and contain some noise, they are effective enough to provide weakly supervised signals for training the discourse-specific word embeddings.", "labels": [], "entities": [{"text": "discourse relation recognition", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.6227464278539022}]}, {"text": "We apply DSWE as features in a supervised neural network for implicit discourse relations recognition.", "labels": [], "entities": [{"text": "implicit discourse relations recognition", "start_pos": 61, "end_pos": 101, "type": "TASK", "confidence": 0.6109240129590034}]}, {"text": "On the PDTB (), using DSWE yields significantly better performance than using off-the-shelf word embeddings, or recent systems incorporating explicit data.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 7, "end_pos": 11, "type": "DATASET", "confidence": 0.9267934560775757}]}, {"text": "We detail our method in Section 2 and evaluate it in Section 3.", "labels": [], "entities": []}, {"text": "Conclusions are given in Section 4.", "labels": [], "entities": []}, {"text": "Our learned DSWE is publicly available at here.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Hyper-parameters for training DSWE and  CDRR. wdim means the dimension of word em- beddings, hsizes the sizes of hidden layers, lr  the learning rate, \u03bb the regularization coefficient,  update the parameter update strategy and f the  nonlinear function. Note that", "labels": [], "entities": []}, {"text": " Table 2: Statistics of data sets on the PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.5920478105545044}]}, {"text": " Table 3: Results of using different word embed- dings. We also list the Precision, Recall and F 1  score for each relation.", "labels": [], "entities": [{"text": "Precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9988445043563843}, {"text": "Recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9605039954185486}, {"text": "F 1  score", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9797677397727966}]}, {"text": " Table 4: Comparison with recent systems.", "labels": [], "entities": []}]}