{"title": [{"text": "Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization", "labels": [], "entities": [{"text": "Improving Semantic Relevance", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8779970606168112}, {"text": "Sequence-to-Sequence Learning of Chinese Social Media Text Summarization", "start_pos": 33, "end_pos": 105, "type": "TASK", "confidence": 0.8850421160459518}]}], "abstractContent": [{"text": "Current Chinese social media text summa-rization models are based on an encoder-decoder framework.", "labels": [], "entities": [{"text": "Chinese social media text summa-rization", "start_pos": 8, "end_pos": 48, "type": "TASK", "confidence": 0.5628358781337738}]}, {"text": "Although its generated summaries are similar to source texts literally, they have low semantic relevance.", "labels": [], "entities": []}, {"text": "In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media sum-marization.", "labels": [], "entities": []}, {"text": "We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries.", "labels": [], "entities": []}, {"text": "In our model, the source text is represented by a gated attention en-coder, while the summary representation is produced by a decoder.", "labels": [], "entities": []}, {"text": "Besides, the similarity score between the representations is maximized during training.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 13, "end_pos": 29, "type": "METRIC", "confidence": 0.9823737740516663}]}, {"text": "Our experiments show that the proposed model outperforms baseline systems on asocial media corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text summarization is to produce a brief summary of the main ideas of the text.", "labels": [], "entities": [{"text": "Text summarization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7371154427528381}]}, {"text": "For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts (.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.7394545376300812}]}, {"text": "However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise.", "labels": [], "entities": [{"text": "Chinese social media text summarization", "start_pos": 30, "end_pos": 69, "type": "TASK", "confidence": 0.5898402810096741}]}, {"text": "Therefore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice (.", "labels": [], "entities": [{"text": "abstractive text summarization", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.6125224729379019}]}, {"text": "For extractive summarization, the selected sentences often have high semantic relevance to the text.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8424406349658966}]}, {"text": "However, for abstractive text summarization, current models tend to produce grammatical Text: \u6628\u665a\uff0c\u4e2d\u8054\u822a\u7a7a\u6210\u90fd\u98de\u5317\u4eac\u4e00\u67b6\u822a\u73ed \u88ab\u53d1\u73b0\u6709\u591a\u4eba\u5438\u70df\u3002\u540e\u56e0\u5929\u6c14\u539f\u56e0\uff0c\u98de\u673a \u5907\u964d\u592a\u539f\u673a\u573a\u3002\u6709\u4e58\u5ba2\u8981\u6c42\u91cd\u65b0\u5b89\u68c0\uff0c\u673a \u957f\u51b3\u5b9a\u7ee7\u7eed\u98de\u884c\uff0c\u5f15\u8d77\u673a\u7ec4\u4eba\u5458\u4e0e\u672a\u5438\u70df \u4e58\u5ba2\u51b2\u7a81\u3002", "labels": [], "entities": [{"text": "abstractive text summarization", "start_pos": 13, "end_pos": 43, "type": "TASK", "confidence": 0.6934474110603333}]}, {"text": "Last night, several people were caught to smoke on a flight of China United Airlines from Chendu to Beijing.", "labels": [], "entities": [{"text": "China United Airlines from Chendu", "start_pos": 63, "end_pos": 96, "type": "DATASET", "confidence": 0.8850912690162659}]}, {"text": "Later the flight temporarily landed on Taiyuan Airport.", "labels": [], "entities": [{"text": "Taiyuan Airport", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.966324120759964}]}, {"text": "Some passengers asked fora security check but were denied by the captain, which led to a collision between crew and passengers.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the evaluation of our model and show its performance on a popular social media corpus.", "labels": [], "entities": []}, {"text": "Besides, we use a case to explain the semantic relevance between generated summary and source text.", "labels": [], "entities": []}, {"text": "Our dataset is Large Scale Chinese Short Text Summarization Dataset (LCSTS), which is constructed by.", "labels": [], "entities": [{"text": "Large Scale Chinese Short Text Summarization", "start_pos": 15, "end_pos": 59, "type": "TASK", "confidence": 0.5575743764638901}]}, {"text": "The dataset consists of more than 2.4 million text-summary pairs, constructed from a famous Chinese social media website called Sina Weibo . It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III.", "labels": [], "entities": []}, {"text": "All the textsummary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5, and we only reserve pairs with scores no less than 3.", "labels": [], "entities": [{"text": "PART", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.7655445337295532}, {"text": "PART", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.7317039370536804}]}, {"text": "Following the previous work, we use PART I as training set, PART II as development set, and PART III as test set.", "labels": [], "entities": []}, {"text": "To alleviate the risk of word segmentation mistakes (Xu and Sun, 2016), we use Chinese character sequences as both source inputs and target outputs.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7403658032417297}]}, {"text": "We limit the model vocabulary size to 4000, which covers most of the common characters.", "labels": [], "entities": []}, {"text": "Each character is represented by a random initialized word embedding.", "labels": [], "entities": []}, {"text": "We tune our parameter on the development set.", "labels": [], "entities": []}, {"text": "In our model, the embedding size is 400, the hidden state size of encoderdecoder is 500, and the size of gated attention network is 1000.", "labels": [], "entities": []}, {"text": "We use Adam optimizer to learn the model parameters, and the batch size is set as 32.", "labels": [], "entities": []}, {"text": "The parameter \u03bb is 0.0001.", "labels": [], "entities": []}, {"text": "Both the encoder and decoder are based on LSTM unit.", "labels": [], "entities": []}, {"text": "Following the previous work (, our evaluation metric is F-score of ROUGE: ROUGE-1, ROUGE-2 and ROUGE-L (Lin and Hovy, 2003).", "labels": [], "entities": [{"text": "F-score", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9986259937286377}, {"text": "ROUGE", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9889783263206482}, {"text": "ROUGE-1", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.972736120223999}, {"text": "ROUGE-2", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9547950625419617}, {"text": "ROUGE-L", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9258501529693604}]}], "tableCaptions": [{"text": " Table 1: Results of our model and baseline systems. Our models achieve substantial improvement of all  ROUGE scores over baseline systems.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.9775165319442749}]}]}