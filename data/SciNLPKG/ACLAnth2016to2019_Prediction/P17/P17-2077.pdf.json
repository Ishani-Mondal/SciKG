{"title": [{"text": "A Network Framework for Noisy Label Aggregation in Social Media", "labels": [], "entities": [{"text": "Noisy Label Aggregation", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.6814549664656321}]}], "abstractContent": [{"text": "This paper focuses on the task of noisy label aggregation in social media, where users with different social or culture backgrounds may annotate invalid or malicious tags for documents.", "labels": [], "entities": [{"text": "noisy label aggregation", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.6588975588480631}]}, {"text": "To aggregate noisy labels at a small cost, a network framework is proposed by calculating the matching degree of a document's topics and the annotators' meta-data.", "labels": [], "entities": []}, {"text": "Unlike using the back-propagation algorithm, a probabilis-tic inference approach is adopted to estimate network parameters.", "labels": [], "entities": []}, {"text": "Finally, anew simulation method is designed for validating the effectiveness of the proposed framework in aggregating noisy labels.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social media allows users to share their views, opinions, emotion tendencies, and other personal information online.", "labels": [], "entities": []}, {"text": "It is quite valuable to analyze and predict user opinions from these materials (, in which supervised learning is one of the effective paradigms (.", "labels": [], "entities": []}, {"text": "However, the performance of a supervised learning algorithm relies heavily on the quality of training labels.", "labels": [], "entities": []}, {"text": "In social media, many training data are collected via simple heuristic rules or online crowdsourcing systems, such as Amazon's Mechanical Turk (www.mturk.com) which allows multiple labelers to annotate the same object (.", "labels": [], "entities": []}, {"text": "Due to the lack * The corresponding author. of quality control, it can be hard fora model to reconcile such noise in training labels.", "labels": [], "entities": []}, {"text": "This study aims to aggregate noisy labels by matching annotators and documents.", "labels": [], "entities": []}, {"text": "Unlike other noisy label aggregation and integration tasks (or algorithms), such as Learning to Rank (LtR) and integrating crowdsourced labels which rely on accurate instance sources ( or confidence scores (, we only need features that can be obtained with a small cost (i.e., topics).", "labels": [], "entities": []}, {"text": "Compared with acquiring accurate instance sources or confidence scores, which is very hard, extracting topics can be done conveniently by many existing topic models.", "labels": [], "entities": []}, {"text": "Note that label noise is not always random, as adversarial noise may occur in real-world environments when a malicious agent is permitted to select labels for certain instances.", "labels": [], "entities": []}, {"text": "For example, a fake annotator is purchased to promote defective goods by giving high ratings.", "labels": [], "entities": []}, {"text": "Noisy labels in such a manner are extremely difficult to be handled (.", "labels": [], "entities": []}, {"text": "To validate the effectiveness of aggregating the aforementioned noisy labels, we propose to design anew simulation method in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "As sentiment and emotion detection are widely studied in social media analysis (), we test model performance based on the Stanford Twitter Sentiment (STS) and the International Survey on Emotion Antecedents and Reactions (ISEAR) corpus.", "labels": [], "entities": [{"text": "sentiment and emotion detection", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.8187153041362762}, {"text": "Stanford Twitter Sentiment (STS)", "start_pos": 122, "end_pos": 154, "type": "DATASET", "confidence": 0.7178428471088409}, {"text": "International Survey on Emotion Antecedents and Reactions (ISEAR)", "start_pos": 163, "end_pos": 228, "type": "TASK", "confidence": 0.7273372292518616}]}, {"text": "The original STS dataset () contains 1.6 million tweets that were automatically labeled as positive or negative using emoticons as labels, in which 80K (5%) randomly selected tweets were used to speedup the training process, 16K (1%) randomly selected tweets were used as the validation set, and 359 tweets were manually annotated as the testing set (dos.", "labels": [], "entities": [{"text": "STS dataset", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.8525077700614929}]}, {"text": "ISEAR is composed of 7, 666 sentences annotated by 1, 096 participants with different culture backgrounds (.", "labels": [], "entities": [{"text": "ISEAR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6189345717430115}]}, {"text": "These participants completed questionnaires about their 34 kinds of personal information (e.g., age, gender, city, country, and religion), as well as their experiences and reactions over seven emotions.", "labels": [], "entities": []}, {"text": "For the ISEAR corpus, we randomly selected 60% of sentences as the training set, 20% as the validation set, and the remaining 20% as the testing set.", "labels": [], "entities": [{"text": "ISEAR corpus", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.8840481042861938}]}, {"text": "We use the following models for comparison: Majority Voting (MV) (, Maximum Likelihood Estimator (MLE) (), and Generative model of Labels, Abilities and Difficulties (GLAD) ().", "labels": [], "entities": [{"text": "Maximum Likelihood Estimator (MLE)", "start_pos": 68, "end_pos": 102, "type": "METRIC", "confidence": 0.8021886050701141}]}, {"text": "The baselines of MV and MLE are implemented by following, and GLAD is run by the software that is available in public at ().", "labels": [], "entities": [{"text": "GLAD", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.8932130932807922}]}, {"text": "We also implement the multivariate version of GLAD, called MGLAD as the baseline for the ISEAR corpus with seven emotions.", "labels": [], "entities": [{"text": "MGLAD", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.7193776369094849}, {"text": "ISEAR corpus", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.8235590755939484}]}, {"text": "Although there are some more recent models on label aggregation () or refinement (, they either require additional features like users' reported confidence scores, or are only suitable to a corpus with one label for each document.", "labels": [], "entities": []}, {"text": "To compare sentiment and emotion classification performance using the aggregated labels for training, we further apply the above noisy label aggregation models to a linear Support Vector Machine (SVM) with squared hinge loss ().", "labels": [], "entities": [{"text": "sentiment and emotion classification", "start_pos": 11, "end_pos": 47, "type": "TASK", "confidence": 0.8134900629520416}]}, {"text": "As shown in the existing studies with refined labels, the linear SVM performed well on sentiment classification of reviews () and tweets (Vo and Zhang, 2015).", "labels": [], "entities": [{"text": "sentiment classification of reviews", "start_pos": 87, "end_pos": 122, "type": "TASK", "confidence": 0.8742755353450775}]}, {"text": "To evaluate the performance of noisy label aggregation models, each instance should be annotated by multiple users.", "labels": [], "entities": []}, {"text": "Unlike previous studies which introduced a parameter to disturb ground truth labels () or employed online crowdsourcing systems () to generate noisy annotations, we design anew simulation approach by following the process of Profile Injection Attack in Collaborative Recommender Systems ().", "labels": [], "entities": []}, {"text": "This is because the existing methods cannot assign multiple labels to each instance, or are difficult to generate virtual users and access their information (e.g., age and gender).", "labels": [], "entities": []}, {"text": "In particular, the following steps have been performed.", "labels": [], "entities": []}, {"text": "First, we generate virtual users with different features, making them the neighbors of existing (actual) annotators.", "labels": [], "entities": []}, {"text": "For each dimension of the actual annotators' features, we take the mean value if the attribute is continuous.", "labels": [], "entities": []}, {"text": "For discrete attributes, we randomly select one type from the existing attribute values.", "labels": [], "entities": []}, {"text": "If the dataset has no user features, we set it as a unit vector.", "labels": [], "entities": []}, {"text": "Second, we generate document annotating vectors for virtual users.", "labels": [], "entities": []}, {"text": "Each annotating vector is composed of three parts: annotating for filler instances (I F ), which is a set of randomly chosen filler instances drawn from the whole dataset, untagged instances (I \u2205 ), and the target instance (i t ).", "labels": [], "entities": []}, {"text": "The purpose of setting I F and I \u2205 is to make the virtual user looks like an ordinary annotator.", "labels": [], "entities": [{"text": "F", "start_pos": 25, "end_pos": 26, "type": "METRIC", "confidence": 0.7511374950408936}]}, {"text": "We select three simulation types from Profile Injection Attack (), i.e., random, average, and love/hate.", "labels": [], "entities": []}, {"text": "In the random method, the label for each instance i \u2208 I F is drawn from a normal distribution around the annotations across the whole dataset, and the probability of labeling correctly to i is 1/C.", "labels": [], "entities": []}, {"text": "The corresponding probabilities are 0.5 and 1 for the average and love/hate methods, respectively.", "labels": [], "entities": []}, {"text": "In all these methods, the annotation for it is randomly selected from wrong labels.", "labels": [], "entities": []}, {"text": "We tune the number of topics D and annotator features U by performing a grid search overall D and U values, with D \u2208 {2, 3, 4, ..., 10} on both datasets, U = 34 on ISEAR, and U \u2208 {1, 10, 100, 500, 1000} on STS that contains user ID only.", "labels": [], "entities": [{"text": "ISEAR", "start_pos": 164, "end_pos": 169, "type": "DATASET", "confidence": 0.8662850260734558}, {"text": "STS", "start_pos": 206, "end_pos": 209, "type": "DATASET", "confidence": 0.9121572971343994}]}, {"text": "The value of K is set to the maximum of D and U . Based on the performance on the validation set, we set D = 6, U = 1000, K = 1000 for STS, and D = 2, U = 34, K = 34 for ISEAR.", "labels": [], "entities": [{"text": "ISEAR", "start_pos": 170, "end_pos": 175, "type": "DATASET", "confidence": 0.7198933959007263}]}, {"text": "For the sum of |I F | and |i t | (i.e., attack size) for each virtual user, we set it as the mean number of annotations in actual users.", "labels": [], "entities": []}, {"text": "The sum of selecting it in each simulation is called the profile size, and the percentage of the profile size is denoted as o.", "labels": [], "entities": []}, {"text": "Following the previous criterion of choosing the noise rate), we set o \u2208 {0.05, 0.1, 0.2, 0.5}.", "labels": [], "entities": []}, {"text": "According to (, each target instance except for those in I F is annotated by three users.", "labels": [], "entities": []}, {"text": "Thus, the number of virtual users is set to 2oN . We set the parameter values of MV, MLE, and M/GLAD according to, and apply the grid search method to obtain the optimal parameters for SVM.", "labels": [], "entities": []}], "tableCaptions": []}