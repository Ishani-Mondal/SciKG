{"title": [{"text": "Detecting annotation noise in automatically labelled data", "labels": [], "entities": [{"text": "Detecting annotation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8477019965648651}]}], "abstractContent": [{"text": "We introduce a method for error detection in automatically annotated text, aimed at supporting the creation of high-quality language resources at affordable cost.", "labels": [], "entities": [{"text": "error detection in automatically annotated text", "start_pos": 26, "end_pos": 73, "type": "TASK", "confidence": 0.8071253399054209}]}, {"text": "Our method combines an unsupervised gener-ative model with human supervision from active learning.", "labels": [], "entities": []}, {"text": "We test our approach on in-domain and out-of-domain data in two languages, in AL simulations and in areal world setting.", "labels": [], "entities": []}, {"text": "For all settings, the results show that our method is able to detect annotation errors with high precision and high recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9986482262611389}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9983850717544556}]}], "introductionContent": [{"text": "Until recently, most of the work in Computational Linguistics has been focussed on standard written text, often from newswire.", "labels": [], "entities": [{"text": "Computational Linguistics", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.8234322667121887}]}, {"text": "The emergence of two new research areas, Digital Humanities and Computational Sociolinguistics, have however shifted the interest towards large, noisy text collections from various sources.", "labels": [], "entities": []}, {"text": "More and more researchers are working with social media text, historical data, or spoken language transcripts, to name but a few.", "labels": [], "entities": []}, {"text": "Thus the need for NLP tools that are able to process this data has become more and more apparent, and has triggered a lot of work on domain adaptation and on developing more robust preprocessing tools.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.7509554624557495}]}, {"text": "Studies are usually carried out on large amounts of data, and thus fully manual annotation or even error correction of automatically prelabelled text is not feasible.", "labels": [], "entities": []}, {"text": "Given the importance of identifying noisy annotations in automatically annotated data, it is all the more surprising that up to now this area of research has been severely understudied.", "labels": [], "entities": []}, {"text": "This paper addresses this gap and presents a method for error detection in automatically labelled text.", "labels": [], "entities": [{"text": "error detection in automatically labelled text", "start_pos": 56, "end_pos": 102, "type": "TASK", "confidence": 0.7610832750797272}]}, {"text": "As test cases, we use POS tagging and Named Entity Recognition, both standard preprocessing steps for many NLP applications.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.6713805347681046}, {"text": "Named Entity Recognition", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6134033302466074}]}, {"text": "However, our approach is general and can also be applied to other classification tasks.", "labels": [], "entities": []}, {"text": "Our approach is based on the work of who develop a generative model for estimating the reliability of multiple annotators in a crowdsourcing setting.", "labels": [], "entities": []}, {"text": "We adapt the generative model to the task of finding errors in automatically labelled data by integrating it in an active learning (AL) framework.", "labels": [], "entities": [{"text": "generative", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.9651439785957336}]}, {"text": "We first show that the approach of on its own is notable to beat a strong baseline.", "labels": [], "entities": []}, {"text": "We then present our integrated model, in which we impose human supervision on the generative model through AL, and show that we are able to achieve substantial improvements in two different tasks and for two languages.", "labels": [], "entities": []}, {"text": "Our contributions are the following.", "labels": [], "entities": []}, {"text": "We provide a novel approach to error detection that is able to identify errors in automatically labelled text with high precision and high recall.", "labels": [], "entities": [{"text": "error detection", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.7056708037853241}, {"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9977964162826538}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9975918531417847}]}, {"text": "To the best of our knowledge, our method is the first that addresses this task in an AL framework.", "labels": [], "entities": []}, {"text": "We show how AL can be used to guide an unsupervised generative model, and we will make our code available to the research community.", "labels": [], "entities": []}, {"text": "Our approach works particularly well in out-of-domain settings where no annotated training data is yet available.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report results for different evaluation measures to asses the usefulness of our method.", "labels": [], "entities": []}, {"text": "First, we report tagger accuracy on the data, obtained during preprocessing ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9739958047866821}]}, {"text": "This corresponds to the accuracy of the labels in the corpus before error correction (baseline accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9991866946220398}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.7755677700042725}]}, {"text": "Label accuracy measures the accuracy of the labels in the corpus after N iterations of error correction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.8830224275588989}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9987117052078247}]}, {"text": "Please note that we do not retrain the tools used for preprocessing, but assess the quality of the data after N iterations of manual inspection and correction.", "labels": [], "entities": []}, {"text": "We also report precision and recall for the error detection itself.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9997683167457581}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9993681311607361}, {"text": "error detection", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.7059507668018341}]}, {"text": "True positives (tp) refers to the number of instances selected for correction during AL that were actual annotation errors.", "labels": [], "entities": [{"text": "True positives (tp)", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.8838994979858399}]}, {"text": "We compute Error detection (ED) precision as the number of true positives divided by the number of all instances selected for error correction during N iterations of AL, and recall as the ratio of correctly identified errors to all errors in the data.", "labels": [], "entities": [{"text": "Error detection (ED) precision", "start_pos": 11, "end_pos": 41, "type": "METRIC", "confidence": 0.8937970896561941}, {"text": "recall", "start_pos": 174, "end_pos": 180, "type": "METRIC", "confidence": 0.9996019005775452}]}, {"text": "shows the accuracies for the individual POS taggers used in experiments 1, 2 and 4.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9881836771965027}, {"text": "POS taggers", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.6494945287704468}]}, {"text": "Please note that this is not a fair comparison as each tagger was trained on a different randomly sampled subset of the data and, crucially, we did not optimise any of the taggers but used default settings in all experiments.", "labels": [], "entities": []}, {"text": "The accuracies of the To increase the number of annotators we use an older version of the StanfordNER (2009-01-16) and a newer version (2015-12-09), with both the DeWaC and HGC models, resulting in a total of 5 annotators for the NER task.", "labels": [], "entities": [{"text": "StanfordNER", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.985226571559906}, {"text": "DeWaC", "start_pos": 163, "end_pos": 168, "type": "DATASET", "confidence": 0.9594205021858215}, {"text": "NER task", "start_pos": 230, "end_pos": 238, "type": "TASK", "confidence": 0.8353583216667175}]}, {"text": "In our first experiment, we explore the benefits of our AL approach to error detection in a setting where we have a reasonably large amount of training data, and where training and test data come from the same domain (in-domain setting).", "labels": [], "entities": [{"text": "error detection", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.68419149518013}]}, {"text": "We implement two selection strategies.", "labels": [], "entities": []}, {"text": "The first one is a Query-by-Committee approach (QBC) where we use the disagreements in the predictions of our tagger ensemble to identify potential errors.", "labels": [], "entities": []}, {"text": "For each instance i, we compute the entropy over the predicted labels M by the 7 taggers and select  the N instances with the highest entropy (Equation 1).", "labels": [], "entities": []}, {"text": "For each selected instance, we then replace the label predicted by majority vote with the gold label.", "labels": [], "entities": []}, {"text": "Please note that the selected instances might already have the correct label, and thus the replacement does not necessarily increase accuracy but only does so when the algorithm selects a true error.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9987949132919312}]}, {"text": "We then evaluate the accuracy of the majority predictions after updating the N instances ranked highest for entropy 9 (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9993847608566284}]}, {"text": "We compare the QBC setting to our integrated approach where we guide the generative model with human supervision.", "labels": [], "entities": []}, {"text": "Here the instances are selected according to their posterior entropy as assigned by the variational model, and after being disambiguated by the oracle, the predictions of a randomly selected classifier are updated with the oracle tags.", "labels": [], "entities": []}, {"text": "We run the AL simulation for 500 iterations and select one new instance in each iteration.", "labels": [], "entities": []}, {"text": "After replacing the predicted label for this instance by the gold label, we retrain the variational model and select the next instance, based on the new posterior probabilities learned on the modified dataset.", "labels": [], "entities": []}, {"text": "We refer to this setting as VI-AL.  detection.", "labels": [], "entities": [{"text": "VI-AL.  detection", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.5851057171821594}]}, {"text": "In the succeeding iterations, the precision slowly decreases as it gets harder to identify new errors.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9996938705444336}]}, {"text": "We even observe a slight decrease in label accuracy after 400 iterations that is due to the fact that ties are broken randomly and thus the vote for the same instance can vary between iterations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.8289889693260193}]}, {"text": "Looking at the AL setting with variational inference, we also seethe highest precision for identifying errors during the first 100 iterations.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9983781576156616}]}, {"text": "However, the precision for error dection is more than 3 times as high as for QBC (41% vs. 13%), and we are still able to detect new errors during the last 100 iterations.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.999591052532196}]}, {"text": "This results in an increase in POS label accuracy in the corpus from 97.56% to 99.34%, a near perfect result.", "labels": [], "entities": [{"text": "POS label", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.7259141206741333}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.5105701684951782}]}, {"text": "To find out what error types we were notable to identify, we manually checked the remaining 33 errors that we failed to detect in the first 500 iterations.", "labels": [], "entities": []}, {"text": "Most of those are cases where an adjective (JJ) was mistaken fora past participle (VBN).", "labels": [], "entities": []}, {"text": "In the second experiment, we test how our approach performs in an out-of-domain setting.", "labels": [], "entities": []}, {"text": "For this, we use the English Web treebank (Bies et al.,: Increase in POS label accuracy on the web genres (5,000 tokens) after N iterations of error correction with VI-AL.", "labels": [], "entities": [{"text": "English Web treebank", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.8630117177963257}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.5473971366882324}, {"text": "error correction", "start_pos": 143, "end_pos": 159, "type": "TASK", "confidence": 0.6404623985290527}]}, {"text": "2012), a corpus of over 250,000 words of English weblogs, newsgroups, email, reviews and question-answers manually annotated for parts-ofspeech and syntax.", "labels": [], "entities": []}, {"text": "Our objective is to develop and test a method for error detection that can also be applied to out-of-domain scenarios for creating and improving language resources when no indomain training data is available.", "labels": [], "entities": [{"text": "error detection", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.715586319565773}]}, {"text": "We thus abstain from retraining the taggers on the web data and use the tools and models from experiment 1 ( \u00a75.1) as is, trained on the WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 137, "end_pos": 140, "type": "DATASET", "confidence": 0.9835365414619446}]}, {"text": "As the English Web treebank uses an extended tagset with additional tags for URLs and email addresses etc., we allow the oracle to assign new tags unknown to the preprocessing classifiers.", "labels": [], "entities": [{"text": "English Web treebank", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.8075665831565857}]}, {"text": "Ina traditional AL setting, this would not be possible, as all class labels have to be known from the start.", "labels": [], "entities": []}, {"text": "In our setting, however, this can be easily implemented.", "labels": [], "entities": []}, {"text": "For each web genre, we extract samples of 5,000 tokens and run an active learning simulation with 500 iterations, wherein each iteration one new instance is selected and disambiguated.", "labels": [], "entities": []}, {"text": "After each iteration, we update the variational model and the predictions of a randomly selected classifier, as described in Section 5.1.", "labels": [], "entities": []}, {"text": "shows the performance of the WSJtrained taggers on the web data.", "labels": [], "entities": [{"text": "WSJtrained taggers", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.6759245097637177}]}, {"text": "As expected, the results are much lower than the ones from the indomain setting.", "labels": [], "entities": []}, {"text": "This allows us to explore the behaviour of our error detection approach under different conditions, in particular to test our approach on tag predictions of a lower quality.", "labels": [], "entities": [{"text": "error detection", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7030502259731293}]}, {"text": "The last three rows in give the average tagger accuracy, the accuracy for the majority vote for the ensemble (not to be confused with QBC), and the accuracy we get when using the predictions from the variational model without AL (MACE: No. of true positives (# tp), precision (ED prec) and recall for error detection on 5,000 tokens from the answers set after N iterations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9912448525428772}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9995085000991821}, {"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9987529516220093}, {"text": "precision (ED prec)", "start_pos": 266, "end_pos": 285, "type": "METRIC", "confidence": 0.8209095478057862}, {"text": "recall", "start_pos": 290, "end_pos": 296, "type": "METRIC", "confidence": 0.9988127946853638}]}, {"text": "We can see that the majority baseline often, but not always succeeds in beating the best individual tagger.", "labels": [], "entities": []}, {"text": "Results for MACE are more or less in the same range as the majority vote, same as in experiment 1, but do not improve over the baseline.", "labels": [], "entities": [{"text": "MACE", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.8494148850440979}]}, {"text": "Next, we employ AL in the out-of-domain setting (Tables 4, 5 and 6).", "labels": [], "entities": [{"text": "AL", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.9900896549224854}]}, {"text": "shows the increase in POS label accuracy for the five web genres after running N iterations of AL with variational inference (VI-AL).", "labels": [], "entities": [{"text": "POS label accuracy", "start_pos": 22, "end_pos": 40, "type": "METRIC", "confidence": 0.7479912837346395}]}, {"text": "compares the results of the two selection strategies, QBC and VI-AL, on the answers subcorpus after an increasing number of AL iterations.", "labels": [], "entities": []}, {"text": "11 completes the picture by showing results for error detection for all web genres, for QBC and VI-AL, after inspecting 10% of the data (500 iterations).", "labels": [], "entities": [{"text": "error detection", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.6943059116601944}]}, {"text": "shows that using VI-AL for error detection results in a substantial increase in POS label accuracy for all genres.", "labels": [], "entities": [{"text": "error detection", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.652777686715126}, {"text": "POS label accuracy", "start_pos": 80, "end_pos": 98, "type": "METRIC", "confidence": 0.8425349593162537}]}, {"text": "VI-AL still detects new errors after a high number of iterations, without retraining the ensemble taggers.", "labels": [], "entities": [{"text": "VI-AL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8737392425537109}]}, {"text": "This is especially useful in a setting where no labelled target domain data is yet available.", "labels": [], "entities": []}, {"text": "shows the number of true positives amongst the selected error candidates as well as precision and recall for error detection for different stages of AL on the answers genre.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.999721109867096}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9994920492172241}, {"text": "error detection", "start_pos": 109, "end_pos": 124, "type": "TASK", "confidence": 0.6584490388631821}]}, {"text": "We can see that during the early learning stages, both selection strategies have a high precision and QBC beats VI-AL.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9981498718261719}, {"text": "QBC", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9732668995857239}, {"text": "VI-AL", "start_pos": 112, "end_pos": 117, "type": "METRIC", "confidence": 0.8692329525947571}]}, {"text": "After 200 iterations it becomes more difficult to detect new errors, and the precision for both methods decreases.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9996187686920166}]}, {"text": "The decrease, however, is much slower for VI-AL, leading to higher precision after the initial rounds of training, and the gap in results becomes more and more pronounced.: No. of true positives (# tp), precision (ED prec) and recall for error detection on 5,000 tokens after 500 iterations on all web genres.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9992941617965698}, {"text": "No.", "start_pos": 173, "end_pos": 176, "type": "METRIC", "confidence": 0.9709553718566895}, {"text": "true positives (# tp)", "start_pos": 180, "end_pos": 201, "type": "METRIC", "confidence": 0.811953604221344}, {"text": "precision (ED prec)", "start_pos": 203, "end_pos": 222, "type": "METRIC", "confidence": 0.8090360760688782}, {"text": "recall", "start_pos": 227, "end_pos": 233, "type": "METRIC", "confidence": 0.9989596605300903}, {"text": "error detection", "start_pos": 238, "end_pos": 253, "type": "TASK", "confidence": 0.6766989380121231}]}, {"text": "After 600 iterations, VI-AL beats QBC by more than 10%, thus resulting in a lower number of instances that have to be checked to obtain the same POS accuracy in the final dataset.", "labels": [], "entities": [{"text": "POS", "start_pos": 145, "end_pos": 148, "type": "METRIC", "confidence": 0.9499711394309998}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.5155360698699951}]}, {"text": "Looking at recall, we see that by manually inspecting 10% of the data VI-AL manages to detect more than 50% of all errors, and after validating 20% of the data, we are able to eliminate 75% of all errors in the corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9983240962028503}]}, {"text": "In contrast, QBC detects less than 60% of the annotation errors in the dataset.", "labels": [], "entities": []}, {"text": "In the out-of-domain setting where we start with low-quality POS predictions, we are able to detect errors in the data with a much higher precision than in the in-domain setting, where the number of errors in the dataset is much lower.", "labels": [], "entities": [{"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9927433729171753}]}, {"text": "Even after 1,000 iterations, the precision for error detection is close to 50% in the answers data.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9996693134307861}, {"text": "error detection", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.8388291299343109}]}, {"text": "shows that the same trend appears for the other web genres, where we observe a substantially higher precision and recall when guiding AL with variational inference (VI-AL).", "labels": [], "entities": [{"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9991112351417542}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9993786811828613}]}, {"text": "Only on the email data are the results below the ones for QBC, but the gap is small.", "labels": [], "entities": [{"text": "QBC", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.8880391716957092}]}, {"text": "We now want to test if the approach generalises well to other classification tasks, and also to new languages.", "labels": [], "entities": []}, {"text": "To that end, we apply our approach to the task of Named Entity Recognition (NER) on German data ( \u00a74).", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.7785247961680094}, {"text": "German data", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.8632517457008362}]}, {"text": "shows results for error detection for NER.", "labels": [], "entities": [{"text": "error detection", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.7178591191768646}, {"text": "NER", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.900431215763092}]}, {"text": "In comparison to the POS experiments, we observe a much lower recall, for both QBC and VI-AL.", "labels": [], "entities": [{"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9996718168258667}, {"text": "QBC", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.90349280834198}, {"text": "VI-AL", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.8568559885025024}]}, {"text": "This is due to the larger size of the NER testset which results in a higher absolute number of errors in the data.", "labels": [], "entities": [{"text": "NER testset", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.7877286076545715}, {"text": "absolute number of errors", "start_pos": 76, "end_pos": 101, "type": "METRIC", "confidence": 0.8395112007856369}]}, {"text": "Please bear in mind that recall is computed as the ratio of correctly identified errors to all errors in the testset (here we have a total of 110,405 tokens in the test set which means that we identified >35% of all errors by querying less than 1% of the data).", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9991790652275085}]}, {"text": "Also note that the overall number of errors is higher in the QBC setting  errors) than in the VI-AL setting (1,628 errors), as in the first setting we used a majority vote for generating the data pool while in the second setting we relied on the predictions of MACE.", "labels": [], "entities": [{"text": "VI-AL setting", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.8256194889545441}, {"text": "MACE", "start_pos": 261, "end_pos": 265, "type": "DATASET", "confidence": 0.6961448192596436}]}, {"text": "For POS tagging, we did not observe a difference between the initial data pools).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.8623945116996765}]}, {"text": "For NER, however, the initial predictions of MACE are better than the majority vote.", "labels": [], "entities": [{"text": "NER", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.6063755750656128}, {"text": "MACE", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.5930415987968445}]}, {"text": "During the first 800 iterations, precision for VI-AL is much higher than for QBC, but then slowly decreases.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9997274279594421}]}, {"text": "For QBC, however, we seethe opposite trend.", "labels": [], "entities": [{"text": "QBC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8764384388923645}]}, {"text": "Here precision stays in the range of 52-56% for the first 600 iterations.", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9996229410171509}]}, {"text": "After that, it slowly increases, and during the last iterations QBC precision outperforms VI-AL.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.7881680130958557}, {"text": "VI-AL", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.6019593477249146}]}, {"text": "Recall, however, is higher for the VI-AL model, for all iterations.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9949047565460205}]}, {"text": "This means that even if precision is slightly lower than in the QBC setting after 800 iterations, it is still better to use the VI-AL model.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9995365142822266}]}, {"text": "For comparison, in the QBC setting we still have 1,139 errors left in the corpus after 1,000 iterations, while for VI-AL the number of errors remaining in the data is much lower (1,043).", "labels": [], "entities": [{"text": "VI-AL", "start_pos": 115, "end_pos": 120, "type": "DATASET", "confidence": 0.8195247054100037}]}, {"text": "In our final experiment, we test our approach in a real-world scenario with a human annotator in the loop.", "labels": [], "entities": []}, {"text": "To that end, we let two linguistically trained human annotators correct POS errors identified by AL.", "labels": [], "entities": []}, {"text": "We use the out-of-domain data from experiment 2 ( \u00a75.2), specifically the answers and weblog subcorpora.", "labels": [], "entities": []}, {"text": "We run two VI-AL experiments where the oracle is presented with new error candidates for 500 iterations.", "labels": [], "entities": []}, {"text": "The time needed for correction was 135 minutes (annotator 1, answers) and 157 minutes (annotator 2, weblog) for correcting 500 instances: POS results for VI-AL with a human annotator on 2 web genres (true positives, precision and recall for error detection on 5,000 tokens) each.", "labels": [], "entities": [{"text": "precision", "start_pos": 216, "end_pos": 225, "type": "METRIC", "confidence": 0.9992848038673401}, {"text": "recall", "start_pos": 230, "end_pos": 236, "type": "METRIC", "confidence": 0.996954083442688}]}, {"text": "This includes the time needed to consult the annotation guidelines, as both annotators had no prior experience with the extended English Web treebank guidelines.", "labels": [], "entities": [{"text": "English Web treebank guidelines", "start_pos": 129, "end_pos": 160, "type": "DATASET", "confidence": 0.8176702708005905}]}, {"text": "We expect that the amount of time needed for correction will decrease when the annotators become more familiar with the annotation scheme.", "labels": [], "entities": []}, {"text": "As expected, precision as well as recall are lower for the human annotators as compared to the simulation study).", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9996916055679321}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9995487332344055}]}, {"text": "However, even with some annotation noise we were able to detect more than 40% of all errors in the answers data and close to 60% of all errors in the weblog corpus, by manually inspecting only 10% of the data.", "labels": [], "entities": [{"text": "weblog corpus", "start_pos": 150, "end_pos": 163, "type": "DATASET", "confidence": 0.8375889360904694}]}, {"text": "This results in an increase in POS label accuracy from 88.8 to 92.5% for the answers corpus and from 93.9 to 97.5% for the weblogs, which is very close to the 97.8% we obtained in the simulation study).", "labels": [], "entities": [{"text": "POS label accuracy", "start_pos": 31, "end_pos": 49, "type": "METRIC", "confidence": 0.7409138282140096}]}], "tableCaptions": [{"text": " Table 1: Tagger accuracies for POS taggers  trained on subsamples of the WSJ with 20,000 to- kens (for the majority vote, ties were broken ran- domly).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.7616066336631775}, {"text": "POS taggers", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.7496773302555084}, {"text": "WSJ", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.9412179589271545}]}, {"text": " Table 2: Label accuracies on 5,000 tokens of  WSJ text after N iterations, and precision for error  detection (ED prec).", "labels": [], "entities": [{"text": "Label accuracies", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.7833890616893768}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9994888305664062}, {"text": "error  detection", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.7651579082012177}, {"text": "ED prec)", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9467358986536661}]}, {"text": " Table 3: Tagger accuracies on different web gen- res (trained on the WSJ); avg. accuracy, accu- racy for majority vote (major.), and accuracy for  MACE.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9232890009880066}, {"text": "WSJ", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.976423978805542}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.990082323551178}, {"text": "accu- racy", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.9430556694666544}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9995131492614746}, {"text": "MACE", "start_pos": 148, "end_pos": 152, "type": "TASK", "confidence": 0.5450012683868408}]}, {"text": " Table 4: Increase in POS label accuracy on the  web genres (5,000 tokens) after N iterations of er- ror correction with VI-AL.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8142247200012207}]}, {"text": " Table 5: No. of true positives (# tp), precision (ED  prec) and recall for error detection on 5,000 tokens  from the answers set after N iterations.", "labels": [], "entities": [{"text": "No.", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9618160128593445}, {"text": "precision (ED  prec)", "start_pos": 40, "end_pos": 60, "type": "METRIC", "confidence": 0.8253494381904602}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.999264657497406}, {"text": "error detection", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.6827187538146973}]}, {"text": " Table 6: No. of true positives (# tp), precision (ED  prec) and recall for error detection on 5,000 tokens  after 500 iterations on all web genres.", "labels": [], "entities": [{"text": "No.", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9751875400543213}, {"text": "precision (ED  prec)", "start_pos": 40, "end_pos": 60, "type": "METRIC", "confidence": 0.8105756878852844}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.999049961566925}, {"text": "error detection", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.671155720949173}]}, {"text": " Table 7: Error detection results on the GermEval  2014 NER testset after N iterations (true positives,  ED precision and recall).", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9628449082374573}, {"text": "GermEval  2014 NER testset", "start_pos": 41, "end_pos": 67, "type": "DATASET", "confidence": 0.8505611270666122}, {"text": "ED", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.978306233882904}, {"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.6209363341331482}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9986758828163147}]}, {"text": " Table 8: POS results for VI-AL with a human an- notator on 2 web genres (true positives, precision  and recall for error detection on 5,000 tokens)", "labels": [], "entities": [{"text": "POS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8662049174308777}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9995039701461792}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9985087513923645}]}]}