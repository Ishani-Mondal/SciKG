{"title": [{"text": "Deep Semantic Role Labeling: What Works and What's Next", "labels": [], "entities": [{"text": "Deep Semantic Role Labeling", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5454475954174995}]}], "abstractContent": [{"text": "We introduce anew deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.7760718911886215}]}, {"text": "We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initializa-tion and regularization.", "labels": [], "entities": []}, {"text": "Our 8-layer ensemble model achieves 83.2 F1 on the CoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10% relative error reduction over the previous state of the art.", "labels": [], "entities": [{"text": "F1", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9817807674407959}, {"text": "CoNLL 2005 test set", "start_pos": 51, "end_pos": 70, "type": "DATASET", "confidence": 0.9787002205848694}, {"text": "F1", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.9725356101989746}, {"text": "CoNLL 2012", "start_pos": 86, "end_pos": 96, "type": "DATASET", "confidence": 0.9389178156852722}]}, {"text": "Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic role labeling (SRL) systems aim to recover the predicate-argument structure of a sentence, to determine essentially \"who did what to whom\", \"when\", and \"where.\"", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8325342933336893}]}, {"text": "Recently breakthroughs involving end-to-end deep models for SRL without syntactic input () seem to overturn the long-held belief that syntactic parsing is a prerequisite for this task.", "labels": [], "entities": [{"text": "SRL", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9889879822731018}, {"text": "syntactic parsing", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.6933296918869019}]}, {"text": "In this paper, we show that this result can be pushed further using deep highway bidirectional LSTMs with constrained decoding, again significantly moving the state of the art (another 2 points on CoNLL 2005).", "labels": [], "entities": [{"text": "CoNLL 2005", "start_pos": 197, "end_pos": 207, "type": "DATASET", "confidence": 0.9635333120822906}]}, {"text": "We also present a careful empirical analysis to determine what works well and what might be done to progress even further.", "labels": [], "entities": []}, {"text": "Our model combines a number of best practices in the recent deep learning literature.", "labels": [], "entities": []}, {"text": "Following, we treat SRL as a BIO tagging problem and use deep bidirectional LSTMs.", "labels": [], "entities": [{"text": "SRL", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9276914000511169}, {"text": "BIO tagging", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.7160409688949585}]}, {"text": "However, we differ by (1) simplifying the input and output layers, (2) introducing highway connections (, (3) using recurrent dropout (), (4) decoding with BIOconstraints, and (5) ensembling with a product of experts.", "labels": [], "entities": []}, {"text": "Our model gives a 10% relative error reduction over previous state of the art on the test sets of CoNLL 2005 and 2012.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 22, "end_pos": 46, "type": "METRIC", "confidence": 0.7696715792020162}, {"text": "CoNLL 2005", "start_pos": 98, "end_pos": 108, "type": "DATASET", "confidence": 0.8452186584472656}]}, {"text": "We also report performance with predicted predicates to encourage future exploration of end-to-end SRL systems.", "labels": [], "entities": [{"text": "SRL", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9649661183357239}]}, {"text": "We present detailed error analyses to better understand the performance gains, including (1) design choices on architecture, initialization, and regularization that have a surprisingly large impact on model performance; (2) different types of prediction errors showing, e.g., that deep models excel at predicting long-distance dependencies but still struggle with known challenges such as PPattachment errors and adjunct-argument distinctions; (3) the role of syntax, showing that there is significant room for improvement given oracle syntax but errors from existing automatic parsers prevent effective use in SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 611, "end_pos": 614, "type": "TASK", "confidence": 0.9676070213317871}]}, {"text": "In summary, our main contributions incluede: \u2022 A new state-of-the-art deep network for endto-end SRL, supported by publicly available code and models.", "labels": [], "entities": [{"text": "SRL", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.852128803730011}]}, {"text": "1 \u2022 An in-depth error analysis indicating where the model works well and where it still struggles, including discussion of structural consistency and long-distance dependencies.", "labels": [], "entities": []}, {"text": "\u2022 Experiments that point toward directions for future improvements, including a detailed discussion of how and when syntactic parsers could be used to improve these results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We measure the performance of our SRL system on two PropBank-style, span-based SRL datasets:) and CoNLL-2012 (Pradhan et al., 2013) 3 . Both datasets provide gold predicates (their index in the sentence) as part of the input.", "labels": [], "entities": [{"text": "SRL", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9796165227890015}, {"text": "PropBank-style, span-based SRL datasets", "start_pos": 52, "end_pos": 91, "type": "DATASET", "confidence": 0.6466521203517914}, {"text": "CoNLL-2012", "start_pos": 98, "end_pos": 108, "type": "DATASET", "confidence": 0.8803852200508118}]}, {"text": "Therefore, each provided predicate corresponds to one training/test tag sequence.", "labels": [], "entities": []}, {"text": "We follow the traindevelopment-test split for both datasets and use the official evaluation script from the CoNLL 2005 shared task for evaluation on both datasets.", "labels": [], "entities": [{"text": "CoNLL 2005 shared task", "start_pos": 108, "end_pos": 130, "type": "DATASET", "confidence": 0.9401067495346069}]}], "tableCaptions": [{"text": " Table 1: Experimental results on CoNLL 2005, in terms of precision (P), recall (R), F1 and percentage of  completely correct predicates (Comp.). We report results of our best single and ensemble (PoE) model.  The comparison models are Zhou and Xu (2015), FitzGerald et al. (2015), T\u00e4ckstr\u00f6m et al. (2015),  Toutanova et al. (2008) and Punyakanok et al. (2008).", "labels": [], "entities": [{"text": "CoNLL 2005", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.8607032597064972}, {"text": "precision (P)", "start_pos": 58, "end_pos": 71, "type": "METRIC", "confidence": 0.9302889704704285}, {"text": "recall (R)", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9558751881122589}, {"text": "F1", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.9985958933830261}]}, {"text": " Table 2: Experimental results on CoNLL 2012 in the same metrics as above. We compare our best  single and ensemble (PoE) models against Zhou and Xu (2015), FitzGerald et al. (2015), T\u00e4ckstr\u00f6m  et al. (2015) and Pradhan et al. (2013).", "labels": [], "entities": [{"text": "CoNLL 2012", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.8876651227474213}]}, {"text": " Table 3: Predicate detection performance and end-to-end SRL results using predicted predicates. \u2206 F1  shows the absolute performance drop compared to our best ensemble model with gold predicates.", "labels": [], "entities": [{"text": "Predicate detection", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7405061423778534}, {"text": "SRL", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9341826438903809}, {"text": "F1", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.9905503988265991}]}, {"text": " Table 4: Oracle transformations paired with the  relative error reduction after each operation. All  the operations are permitted only if they do not  cause any overlapping arguments.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 50, "end_pos": 74, "type": "METRIC", "confidence": 0.6724026103814443}]}, {"text": " Table 5: Confusion matrix for labeling errors,  showing the percentage of predicted labels for  each gold label. We only count predicted argu- ments that match gold span boundaries.", "labels": [], "entities": []}, {"text": " Table 6: Comparison of BiLSTM models without  BIO decoding. We compare F1 and token-level  accuracy (Token), averaged BIO violations per to- ken (BIO), overall model entropy (All) model en- tropy at tokens involved in BIO violations (BIO).", "labels": [], "entities": [{"text": "F1", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9973493814468384}, {"text": "accuracy (Token)", "start_pos": 92, "end_pos": 108, "type": "METRIC", "confidence": 0.7375749051570892}, {"text": "BIO violations per to- ken (BIO)", "start_pos": 119, "end_pos": 151, "type": "METRIC", "confidence": 0.9291491243574355}, {"text": "All) model en- tropy", "start_pos": 176, "end_pos": 196, "type": "METRIC", "confidence": 0.6910352955261866}]}, {"text": " Table 7: Comparison of models with different  depths and decoding constraints (in addition to  BIO) as well as two previous systems. We com- pare F1, unlabeled agreement with gold con- stituency (Syn%) and each type of SRL-constraint  violations (Unique core roles, Continuation roles  and Reference roles). Our best model produces a  similar number of constraint violations to the gold  annotation, explaining why deterministically en- forcing these constraints is not helpful.", "labels": [], "entities": [{"text": "BIO", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9417440891265869}, {"text": "F1", "start_pos": 147, "end_pos": 149, "type": "METRIC", "confidence": 0.9871942400932312}]}, {"text": " Table 8: F1 on CoNLL 2005, and the devel- opment set of CoNLL 2012, broken down by  genres. Syntax-constrained decoding (+AutoSyn)  shows bigger improvement on in-domain data  (CoNLL 05 and CoNLL 2012 NW).", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9987151622772217}, {"text": "CoNLL 2005", "start_pos": 16, "end_pos": 26, "type": "DATASET", "confidence": 0.9561640024185181}, {"text": "CoNLL 2012", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.8776515424251556}, {"text": "CoNLL 2012 NW", "start_pos": 191, "end_pos": 204, "type": "DATASET", "confidence": 0.8657820423444113}]}]}