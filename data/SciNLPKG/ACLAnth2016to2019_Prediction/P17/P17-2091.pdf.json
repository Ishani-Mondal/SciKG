{"title": [{"text": "Speeding Up Neural Machine Translation Decoding by Shrinking Run-time Vocabulary", "labels": [], "entities": [{"text": "Neural Machine Translation Decoding", "start_pos": 12, "end_pos": 47, "type": "TASK", "confidence": 0.8532417267560959}]}], "abstractContent": [{"text": "We speedup Neural Machine Translation (NMT) decoding by shrinking run-time target vocabulary.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT) decoding", "start_pos": 11, "end_pos": 52, "type": "TASK", "confidence": 0.7964908821242196}]}, {"text": "We experiment with two shrinking approaches: Locality Sensitive Hashing (LSH) and word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.7675650417804718}]}, {"text": "Using the latter method, we get a 2x overall speed-up over a highly-optimized GPU implementation, without hurting BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.989733874797821}]}, {"text": "On certain low-resource language pairs, the same methods improve BLEU by 0.5 points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9990302324295044}]}, {"text": "We also report a negative result for LSH on GPUs, due to relatively large overhead, though it was successful on CPUs.", "labels": [], "entities": [{"text": "LSH", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.850426435470581}]}, {"text": "Compared with Locality Sensitive Hashing (LSH), decoding with word alignments is GPU-friendly, orthogonal to existing speedup methods and more robust across language pairs.", "labels": [], "entities": [{"text": "Locality Sensitive Hashing (LSH)", "start_pos": 14, "end_pos": 46, "type": "TASK", "confidence": 0.7816206216812134}, {"text": "word alignments", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.6789389997720718}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) has been demonstrated as an effective model and been put into large-scale production (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8045534988244375}]}, {"text": "For online translation services, decoding speed is a crucial factor to achieve a better user experience.", "labels": [], "entities": [{"text": "online translation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.5807539224624634}]}, {"text": "Several recently proposed training methods aim to solve the exposure bias problem, but require decoding the whole training set multiple times, which is extremely time-consuming for millions of sentences.", "labels": [], "entities": []}, {"text": "Slow decoding speed is partly due to the large target vocabulary size V, which is usually in the tens of thousands.", "labels": [], "entities": []}, {"text": "The first two columns of Table 1 show the breakdown of the runtimes required by sub-modules to decode 1812 Japanese sentences to English using a sequence-to-sequence model with local attention (: Time breakdown and BLEU score of full vocabulary decoding and our \"WA50\" decoding, both with beam size 12.", "labels": [], "entities": [{"text": "Time breakdown", "start_pos": 196, "end_pos": 210, "type": "METRIC", "confidence": 0.9842869341373444}, {"text": "BLEU score", "start_pos": 215, "end_pos": 225, "type": "METRIC", "confidence": 0.9785782098770142}]}, {"text": "WA50 means decoding informed byword alignments, where each source word can select at most 50 relevant target words.", "labels": [], "entities": [{"text": "decoding informed byword alignments", "start_pos": 11, "end_pos": 46, "type": "TASK", "confidence": 0.6058182716369629}]}, {"text": "The model is a 2-layer, 1000-hidden dimension, 50,000-target vocabulary LSTM seq2seq model with local attention trained on the AS-PEC Japanese-to-English corpus (.", "labels": [], "entities": [{"text": "AS-PEC Japanese-to-English corpus", "start_pos": 127, "end_pos": 160, "type": "DATASET", "confidence": 0.7853772441546122}]}, {"text": "The time is measured on a single Nvidia Tesla K20 GPU.", "labels": [], "entities": [{"text": "Nvidia Tesla K20 GPU", "start_pos": 33, "end_pos": 53, "type": "DATASET", "confidence": 0.8608323037624359}]}, {"text": "Softmax is the most computationally intensive part, where each hidden vector ht \u2208 Rd needs to dot-product with V target embeddings e i \u2208 Rd . It occupies 40% of the total decoding time.", "labels": [], "entities": []}, {"text": "Another sub-module whose computation time is proportional to V is Beam Expansion, where we need to find the top B words among all V vocabulary according to their probability.", "labels": [], "entities": []}, {"text": "It takes around 17% of the decoding time.", "labels": [], "entities": []}, {"text": "Several approaches have proposed to improve decoding speed: 1.", "labels": [], "entities": []}, {"text": "Using special hardware, such as GPU and Tensor Processing Unit (TPU), and lowprecision calculation ().", "labels": [], "entities": []}, {"text": "2. Compressing deep neural models through knowledge distillation and weight pruning ().", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.6949775516986847}]}, {"text": "3. Several variants of Softmax have been proposed to solve its poor scaling properties on large vocabularies.", "labels": [], "entities": []}, {"text": "propose hierarchical softmax, whereat each step log 2 V binary classifications are performed instead of a single classification on a large number of classes.", "labels": [], "entities": []}, {"text": "propose noise-contrastive estimation which discriminate between positive labels and k (k << V ) negative labels sampled from a distribution, and is applied successfully on natural language processing tasks.", "labels": [], "entities": []}, {"text": "Although these two approaches provide good speedups for training, they still suffer attest time.", "labels": [], "entities": []}, {"text": "introduces differentiated softmax, where frequent words have more parameters in the embedding and rare words have less, offering speedups on both training and testing.", "labels": [], "entities": []}, {"text": "In this work, we aim to speedup decoding by shrinking the run-time target vocabulary size, and this approach is orthogonal to the methods above.", "labels": [], "entities": []}, {"text": "It is important to note that approaches 1 and 2 will maintain or even increase the ratio of target word embedding parameters to the total parameters, thus the Beam Expansion and Softmax will occupy the same or greater portion of the decoding time.", "labels": [], "entities": []}, {"text": "A small run-time vocabulary will dramatically reduce the time spent on these two portions and gain a further speedup even after applying other speedup methods.", "labels": [], "entities": []}, {"text": "To shrink the run-time target vocabulary, our first method uses Locality Sensitive Hashing.", "labels": [], "entities": [{"text": "Locality Sensitive Hashing", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.7097966074943542}]}, {"text": "successfully applies it on CPUs and gains speedup on single step prediction tasks such as image classification and video identification.", "labels": [], "entities": [{"text": "image classification", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.7424173057079315}, {"text": "video identification", "start_pos": 115, "end_pos": 135, "type": "TASK", "confidence": 0.7134311646223068}]}, {"text": "Our second method is to use word alignments to select a very small number of candidate target words given the source sentence.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.6865682452917099}]}, {"text": "Recent works () apply a similar strategy and report speedups for decoding on CPUs on richsource language pairs.", "labels": [], "entities": []}, {"text": "Our major contributions are: 1.", "labels": [], "entities": []}, {"text": "To our best of our knowledge, this work is the first attempt to apply LSH technique on sequence generation tasks on GPU other than single-step classification on CPU.", "labels": [], "entities": [{"text": "sequence generation tasks", "start_pos": 87, "end_pos": 112, "type": "TASK", "confidence": 0.7748211125532786}]}, {"text": "We find current LSH algorithms have a poor performance/speed trade-off on GPU, due to the large overhead introduced by many hash table lookups and list-merging involved in LSH.", "labels": [], "entities": []}, {"text": "2. For our word alignment method, we find that only the candidate list derived from lexical translation table of IBM model 4 is adequate to achieve good BLEU/speedup trade-off for decoding on GPU.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.8058990240097046}, {"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9977946281433105}]}, {"text": "There is no need to combine the top frequent words or words from phrase table, as proposed in.", "labels": [], "entities": []}, {"text": "3. We conduct our experiments on GPU and provide a detailed analysis of BLEU/speedup trade-off on both resource-rich/poor language pairs and both attention/non-attention NMT models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.997835099697113}]}, {"text": "We achieve more than 2x speedup on 4 language pairs with only a tiny BLEU drop, demonstrating the robustness and efficiency of our methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9991514682769775}]}], "datasetContent": [{"text": "To examine the robustness of these decoding methods, we vary experiment settings in different ways: 1) We train both attention (: Word type coverage (TC), BLEU score, and speedups (X) for full-vocabulary decoding (Full), top frequency vocabulary decoding (TF*), LSH decoding (LSH*), and decoding with word alignments(WA*).", "labels": [], "entities": [{"text": "Word type coverage (TC)", "start_pos": 130, "end_pos": 153, "type": "METRIC", "confidence": 0.7687460780143738}, {"text": "BLEU score, and speedups (X)", "start_pos": 155, "end_pos": 183, "type": "METRIC", "confidence": 0.7827053479850292}]}, {"text": "TF10K represents decoding with top 10,000 frequent target vocabulary (C = 10, 000).", "labels": [], "entities": [{"text": "TF10K", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.47395917773246765}]}, {"text": "WA10 means decoding with word alignments, where each source word can select at most 10 candidate target words (M = 10).", "labels": [], "entities": [{"text": "word alignments", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.6623607575893402}]}, {"text": "For LSH decoding, we choose (32, 5000, 1000) for (K,P ,W ), and vary C. both resource-rich language pairs, French to English (F2E) and Japanese to English (J2E), and a resource-poor language pair, Uzbek to English (U2E); 3) We translate both to English (F2E, J2E, and U2E) and from English (E2J).", "labels": [], "entities": []}, {"text": "We use 2-layer LSTM seq2seq models with different attention settings, hidden dimension sizes, dropout rates, and initial learning rates, as shown in.", "labels": [], "entities": []}, {"text": "We use the ASPEC Japanese-English Corpus (, French-English Corpus from WMT2014 (), and Uzbek-English Corpus (Linguistic Data Consortium, 2016).", "labels": [], "entities": [{"text": "ASPEC Japanese-English Corpus", "start_pos": 11, "end_pos": 40, "type": "DATASET", "confidence": 0.8218452135721842}, {"text": "French-English Corpus from WMT2014", "start_pos": 44, "end_pos": 78, "type": "DATASET", "confidence": 0.749814584851265}, {"text": "Uzbek-English Corpus (Linguistic Data Consortium, 2016)", "start_pos": 87, "end_pos": 142, "type": "DATASET", "confidence": 0.854035324520535}]}, {"text": "follows: T C = |{run-time vocab} \u2229 {word types in test}| |{word types in test}| The top 1000 words only cover 14% word types of J2E test data, whereas WA10 covers 75%, whose run-time vocabulary is no more than 200 fora 20 words source sentence.", "labels": [], "entities": [{"text": "J2E test data", "start_pos": 128, "end_pos": 141, "type": "DATASET", "confidence": 0.8373542428016663}]}, {"text": "The speedup of English-to-Uzbek translation is relatively low (around 1.7x).", "labels": [], "entities": [{"text": "speedup", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9798778891563416}, {"text": "English-to-Uzbek translation", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.5807509124279022}]}, {"text": "This is because the original full vocabulary size is small (25k), leaving less room for shrinkage.", "labels": [], "entities": []}, {"text": "LSH achieves better BLEU than decoding with top frequent words of the same run-time vocabulary size C on attention models.", "labels": [], "entities": [{"text": "LSH", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.5864294767379761}, {"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9994426369667053}]}, {"text": "However, it in-troduces too large an overhead (50 times slower), especially when softmax is highly optimized on GPU.", "labels": [], "entities": []}, {"text": "When doing sequential beam search, search error accumulates rapidly.", "labels": [], "entities": [{"text": "sequential beam search", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.6842180689175924}, {"text": "search error", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.7240341901779175}]}, {"text": "To reach reasonable performance, we have to apply an adequately large number of permutations (P = 5000).", "labels": [], "entities": []}, {"text": "We also find that decoding with word alignments can even improve BLEU on resource-poor languages.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9983106851577759}]}, {"text": "Our conjecture is that rare words are not trained enough, so neural models confuse them, and word alignments can provide a hard constraint to rule out the unreasonable word choices.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.7143172025680542}]}], "tableCaptions": [{"text": " Table 1: Time breakdown and BLEU score of  full vocabulary decoding and our \"WA50\" decod- ing, both with beam size 12. WA50 means de- coding informed by word alignments, where each  source word can select at most 50 relevant target  words. The model is a 2-layer, 1000-hidden di- mension, 50,000-target vocabulary LSTM seq2seq  model with local attention trained on the AS- PEC Japanese-to-English corpus (", "labels": [], "entities": [{"text": "Time breakdown", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9591041803359985}, {"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9996803998947144}, {"text": "de- coding informed by word alignments", "start_pos": 131, "end_pos": 169, "type": "TASK", "confidence": 0.6536484062671661}, {"text": "AS- PEC Japanese-to-English corpus", "start_pos": 371, "end_pos": 405, "type": "DATASET", "confidence": 0.8044477224349975}]}, {"text": " Table 2: Word type coverage (TC), BLEU score, and speedups (X) for full-vocabulary decoding (Full),  top frequency vocabulary decoding (TF*), LSH decoding (LSH*), and decoding with word align- ments(WA*). TF10K represents decoding with top 10,000 frequent target vocabulary (C = 10, 000).  WA10 means decoding with word alignments, where each source word can select at most 10 candidate  target words (M = 10). For LSH decoding, we choose (32, 5000, 1000) for (K,P ,W ), and vary C.", "labels": [], "entities": [{"text": "Word type coverage (TC)", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.6698825558026632}, {"text": "BLEU score, and speedups (X)", "start_pos": 35, "end_pos": 63, "type": "METRIC", "confidence": 0.7886080294847488}]}, {"text": " Table 3: Training configurations on different lan- guage pairs.", "labels": [], "entities": []}]}