{"title": [{"text": "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation", "labels": [], "entities": [{"text": "Multi-modal Neural Machine Translation", "start_pos": 29, "end_pos": 67, "type": "TASK", "confidence": 0.6129540130496025}]}], "abstractContent": [{"text": "We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks , bridging the gap between image description and translation.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.5919020672639211}, {"text": "image description and translation", "start_pos": 227, "end_pos": 260, "type": "TASK", "confidence": 0.6941523402929306}]}, {"text": "Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language.", "labels": [], "entities": []}, {"text": "We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora.", "labels": [], "entities": [{"text": "MT corpora", "start_pos": 139, "end_pos": 149, "type": "TASK", "confidence": 0.8193927109241486}]}, {"text": "We also report state-of-the-art results on the Multi30k data set.", "labels": [], "entities": [{"text": "Multi30k data set", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.9869944850603739}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) has been successfully tackled as a sequence to sequence learning problem) where each training example consists of one source and one target variable-length sequences, with no prior information on the alignment between the two.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8120686014493307}]}, {"text": "In the context of NMT, first proposed to use an attention mechanism in the decoder, which is trained to attend to the relevant source-language words as it generates each word of the target sentence.", "labels": [], "entities": []}, {"text": "Similarly, proposed an attention-based model for the task of image description generation (IDG) where a model learns to attend to specific parts of an image representation (the source) as it generates its description (the target) in natural language.", "labels": [], "entities": [{"text": "image description generation (IDG)", "start_pos": 61, "end_pos": 95, "type": "TASK", "confidence": 0.8481733798980713}]}, {"text": "We are inspired by recent successes in applying attention-based models to NMT and IDG.", "labels": [], "entities": []}, {"text": "In this work, we propose an end-to-end attention-based multi-modal neural machine translation (MNMT) model which effectively incorporates two independent attention mechanisms, one over sourcelanguage words and the other over different areas of an image.", "labels": [], "entities": [{"text": "multi-modal neural machine translation (MNMT)", "start_pos": 55, "end_pos": 100, "type": "TASK", "confidence": 0.7778563158852714}]}, {"text": "Our main contributions are: \u2022 We propose a novel attention-based MNMT model which incorporates spatial visual features in a separate visual attention mechanism; \u2022 We use a medium-sized, back-translated multi-modal in-domain data set and large general-domain text-only MT corpora to pretrain our models and show that our MNMT model can efficiently exploit both; \u2022 We show that images bring useful information into an NMT model, e.g. in situations in which sentences describe objects illustrated in the image.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, previous MNMT models in the literature that utilised spatial visual features did not significantly improve over a comparable model that used global visual features or even only textual features.", "labels": [], "entities": []}, {"text": "In this work, we wish to address this issue and propose an MNMT model that uses, in addition to an attention mechanism over the source-language words, an additional visual attention mechanism to incorporate spatial visual features, and still improves on simpler text-only and multi-modal attention-based NMT models.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first briefly revisit the attentionbased NMT framework ( \u00a72) and expand it into an MNMT framework ( \u00a73).", "labels": [], "entities": []}, {"text": "In \u00a74, we introduce the datasets we use to train and evaluate our models, in \u00a75 we discuss our experimental setup and analyse and discuss our results.", "labels": [], "entities": []}, {"text": "Finally, in \u00a76 we discuss relevant related work and in \u00a77 we draw conclusions and provide avenues for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our encoder is a bidirectional RNN with GRU, one 1024D single-layer forward and one 1024D single-layer backward RNN.", "labels": [], "entities": [{"text": "GRU", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9685072302818298}]}, {"text": "Source and target word embeddings are 620D each and trained jointly with the model.", "labels": [], "entities": []}, {"text": "Word embeddings and other non-recurrent matrices are initialised by sampling from a Gaussian N (0, 0.01 2 ), recurrent matrices are random orthogonal and bias vectors are all initialised to zero.", "labels": [], "entities": []}, {"text": "Visual features are obtained by feeding images to the pre-trained ResNet-50 and using the activations of the res4f layer ( ).", "labels": [], "entities": []}, {"text": "We apply dropout with a probability of 0.5 in the encoder bidirectional RNN, the image features, the decoder RNN and before emitting a target word.", "labels": [], "entities": []}, {"text": "We follow and apply dropout to the encoder bidirectional and the decoder RNN using one same mask in all time steps.", "labels": [], "entities": []}, {"text": "All models are trained using stochastic gradient descent with ADADELTA (Zeiler, 2012) with minibatches of size 80 (text-only NMT) or 40 (MNMT), where each training instance consists of one English sentence, one German sentence and one image (MNMT).", "labels": [], "entities": [{"text": "ADADELTA", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9842451214790344}]}, {"text": "We apply early stopping for model selection based on BLEU4, so that if a model does not improve on BLEU4 in the validation set for more than 20 epochs, training is halted.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 9, "end_pos": 23, "type": "METRIC", "confidence": 0.910164088010788}, {"text": "BLEU4", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9891618490219116}, {"text": "BLEU4", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.9744924306869507}]}, {"text": "The translation quality of our models is evaluated quantitatively in terms of BLEU4), METEOR), TER (), and chrF3.", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9988553524017334}, {"text": "METEOR", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9954934120178223}, {"text": "TER", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9965619444847107}]}, {"text": "We report statistical significance with approximate randomisation for the first three metrics with).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU4, METEOR, chrF3, character-level precision and recall (higher is better) and TER scores  (lower is better) on the translated Multi30k (M30k T ) test set. Best text-only baselines results are under- lined and best overall results appear in bold. We show", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9990580677986145}, {"text": "METEOR", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9886112809181213}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9513285756111145}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.998723566532135}, {"text": "TER scores", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9813672304153442}, {"text": "M30k T ) test set", "start_pos": 150, "end_pos": 167, "type": "DATASET", "confidence": 0.7454996168613434}]}, {"text": " Table 2: BLEU4, METEOR, chrF3 (higher is bet- ter), and TER scores (lower is better) on the trans- lated Multi30k (M30k T ) test set. Best text-only  baselines results are underlined and best overall  results appear in bold. Results are significantly  better than the NMT baseline (  \u2020 ) and the SMT  baseline (  \u2021 ) with p < 0.01.", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9987001419067383}, {"text": "METEOR", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9841258525848389}, {"text": "TER", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9992121458053589}, {"text": "M30k T ) test set", "start_pos": 116, "end_pos": 133, "type": "DATASET", "confidence": 0.7639808893203736}, {"text": "NMT baseline", "start_pos": 269, "end_pos": 281, "type": "DATASET", "confidence": 0.931511640548706}, {"text": "SMT  baseline", "start_pos": 297, "end_pos": 310, "type": "DATASET", "confidence": 0.7452979981899261}]}]}