{"title": [{"text": "Affect-LM: A Neural Language Model for Customizable Affective Text Generation", "labels": [], "entities": [{"text": "Customizable Affective Text Generation", "start_pos": 39, "end_pos": 77, "type": "TASK", "confidence": 0.6548492312431335}]}], "abstractContent": [{"text": "Human verbal communication includes affective messages which are conveyed through use of emotionally colored words.", "labels": [], "entities": []}, {"text": "There has been a lot of research in this direction but the problem of integrating state-of-the-art neural language models with affective information remains an area ripe for exploration.", "labels": [], "entities": []}, {"text": "In this paper, we propose an extension to an LSTM (Long Short-Term Memory) language model for generating conversational text, conditioned on affect categories.", "labels": [], "entities": []}, {"text": "Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter.", "labels": [], "entities": []}, {"text": "Perception studies conducted using Amazon Mechanical Turk show that Affect-LM generates naturally looking emotional sentences without sacrificing grammatical correctness.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.9180161356925964}]}, {"text": "Affect-LM also learns affect-discriminative word representations, and perplexity experiments show that additional affective information in conversational text can improve language model prediction.", "labels": [], "entities": [{"text": "language model prediction", "start_pos": 171, "end_pos": 196, "type": "TASK", "confidence": 0.7331414322058359}]}], "introductionContent": [{"text": "Affect is a term that subsumes emotion and longer term constructs such as mood and personality and refers to the experience of feeling or emotion (.", "labels": [], "entities": [{"text": "Affect", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9532793164253235}]}, {"text": "provides a detailed discussion of the importance of affect analysis inhuman communication and interaction.", "labels": [], "entities": []}, {"text": "Within this context the analysis of human affect from text is an important topic in natural language understanding, examples of which include sentiment analysis from Twitter (, affect analysis from poetry (Kao and Jurafsky, 2012) and studies of correlation between function words and social/psychological processes).", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 84, "end_pos": 114, "type": "TASK", "confidence": 0.6811627745628357}, {"text": "sentiment analysis", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.7886145412921906}]}, {"text": "People exchange verbal messages which not only contain syntactic information, but also information conveying their mental and emotional states.", "labels": [], "entities": []}, {"text": "Examples include the use of emotionally colored words (such as furious and joy) and swear words.", "labels": [], "entities": []}, {"text": "The automated processing of affect inhuman verbal communication is of great importance to understanding spoken language systems, particularly for emerging applications such as dialogue systems and conversational agents.", "labels": [], "entities": []}, {"text": "Statistical language modeling is an integral component of speech recognition systems, with other applications such as machine translation and information retrieval.", "labels": [], "entities": [{"text": "Statistical language modeling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8400683800379435}, {"text": "speech recognition", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.8170678019523621}, {"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.8166020214557648}, {"text": "information retrieval", "start_pos": 142, "end_pos": 163, "type": "TASK", "confidence": 0.7975515127182007}]}, {"text": "There has been a resurgence of research effort in recurrent neural networks for language modeling, which have yielded performances far superior to baseline language models based on n-gram approaches.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7710029184818268}]}, {"text": "However, there has not been much effort in building neural language models of text that leverage affective information.", "labels": [], "entities": []}, {"text": "Current literature on deep learning for language understanding focuses mainly on representations based on word semantics (, encoderdecoder models for sentence representations, language modeling integrated with symbolic knowledge) and neural caption generation (), but to the best of our knowledge there has been no work on augmenting neural language modeling with affective information, or on data-driven approaches to generate emotional text.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7428662478923798}, {"text": "neural caption generation", "start_pos": 234, "end_pos": 259, "type": "TASK", "confidence": 0.8141537507375082}]}, {"text": "Motivated by these advances in neural language modeling and affective analysis of text, in this paper we propose a model for representation and generation of emotional text, which we call the Affect-LM.", "labels": [], "entities": [{"text": "neural language modeling", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.7411232391993204}, {"text": "representation and generation of emotional text", "start_pos": 125, "end_pos": 172, "type": "TASK", "confidence": 0.7766962349414825}]}, {"text": "Our model is trained on conversational speech corpora, common in language modeling for speech recognition applications (.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7124230414628983}]}, {"text": "provides an overview of our Affect-LM and its ability to generate emotionally colored conversational text in a number of affect categories with varying affect strengths.", "labels": [], "entities": []}, {"text": "While these parameters can be manually tuned to generate conversational text, the affect category can also be automatically inferred from preceding context words.", "labels": [], "entities": []}, {"text": "Specifically for model training, the affect category is derived from features generated using keyword spotting from a dictionary of emotional words, such as the LIWC (Linguistic Inquiry and Word Count) tool).", "labels": [], "entities": []}, {"text": "Our primary research questions in this paper are: Q1:Can Affect-LM be used to generate affective sentences fora target emotion with varying degrees of affect strength through a customizable model parameter?", "labels": [], "entities": []}, {"text": "Q2:Are these generated sentences rated as emotionally expressive as well as grammatically correct in an extensive crowd-sourced perception experiment?", "labels": [], "entities": []}, {"text": "Q3:Does the automatic inference of affect category from the context words improve language modeling performance of the proposed Affect-LM over the baseline as measured by perplexity?", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss prior work in the fields of neural language modeling, and generation of affective conversational text.", "labels": [], "entities": [{"text": "neural language modeling", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.6785714824994405}, {"text": "generation of affective conversational text", "start_pos": 83, "end_pos": 126, "type": "TASK", "confidence": 0.8091758608818054}]}, {"text": "In Section 3 we describe the baseline LSTM model and our proposed Affect-LM model.", "labels": [], "entities": []}, {"text": "Section 4 details the experimental setup, and in Section 5, we discuss results for customizable emotional text generation, perception studies for each affect category, and perplexity improvements over the baseline model before concluding the paper in Section 6.", "labels": [], "entities": [{"text": "customizable emotional text generation", "start_pos": 83, "end_pos": 121, "type": "TASK", "confidence": 0.6898179054260254}]}], "datasetContent": [{"text": "In Section 1, we have introduced three primary research questions related to the ability of the proposed Affect-LM model to generate emotionally colored conversational text without sacrificing grammatical correctness, and to obtain lower perplexity than a baseline LSTM language model when evaluated on emotionally colored corpora.", "labels": [], "entities": []}, {"text": "In this section, we discuss our experimental setup to address these questions, with a description of Affect-LM's architecture and the corpora used for training and evaluating the language models.", "labels": [], "entities": []}, {"text": "Affect-LM can also be used as a language model where the next predicted word is estimated from the words in the context, along with an affect category extracted from the context words themselves (instead of being encoded externally as in generation).", "labels": [], "entities": []}, {"text": "To evaluate whether additional emotional information could improve the prediction performance, we train the corpora detailed in Section 4.1 in two stages as described below: (1) Training and validation of the language models on Fisher dataset-The Fisher corpus is split in a 75:15:10 ratio corresponding to the training, validation and evaluation subsets respectively, and following the implementation in, we train the language models (both the baseline and Affect-LM) on the training split for 13 epochs, with a learning rate of 1.0 for the first four epochs, and the rate decreasing by a factor of 2 after every subsequent epoch.", "labels": [], "entities": [{"text": "Fisher dataset-The Fisher corpus", "start_pos": 228, "end_pos": 260, "type": "DATASET", "confidence": 0.9021929949522018}]}, {"text": "The learning rate and neural architecture are the same for all models.", "labels": [], "entities": []}, {"text": "We validate the model over the affect strength \u03b2 \u2208 [1.0, 1.5, 1.75, 2.0, 2.25, 2.5, 3.0].", "labels": [], "entities": []}, {"text": "The best performing model on the Fisher validation set is chosen and used as a seed for subsequent adaptation on the emotionally colored corpora.", "labels": [], "entities": []}, {"text": "(2) Fine-tuning the seed model on other corpora-Each of the three corpora -CMU-MOSI, DAIC and SEMAINE are split in a 75:15:10 ratio to create individual training, validation and evaluation subsets.", "labels": [], "entities": [{"text": "DAIC", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.8264692425727844}, {"text": "SEMAINE", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.8908263444900513}]}, {"text": "For both the baseline and Affect-LM, the best performing model from Stage 1 (the seed model) is fine-tuned on each of the training corpora, with a learning rate of 0.25 which is constant throughout, and a validation grid of \u03b2 \u2208 [1.0, 1.5, 1.75, 2.0].", "labels": [], "entities": [{"text": "Affect-LM", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9442362189292908}]}, {"text": "For each model adapted on a corpus, we compare the perplexities obtained by Affect-LM and the baseline model when evaluated on that corpus.", "labels": [], "entities": [{"text": "Affect-LM", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.8078212141990662}]}, {"text": "In the following we address research question Q2 by reporting the main statistical findings of our MTurk study, which are visualized in.", "labels": [], "entities": [{"text": "MTurk study", "start_pos": 99, "end_pos": 110, "type": "DATASET", "confidence": 0.8349049091339111}]}, {"text": "The multivariate result was significant for positive emotion generated sentences (Pillai's Trace=.327, F(4,437)=6.44, p<.0001).", "labels": [], "entities": [{"text": "F", "start_pos": 103, "end_pos": 104, "type": "METRIC", "confidence": 0.9886864423751831}]}, {"text": "Follow up ANOVAs revealed significant results for all DVs except angry with p<.0001, indicating that both affective valence and happy DVs were successfully manipulated with \u03b2, as seen in.", "labels": [], "entities": [{"text": "ANOVAs", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.7364041805267334}]}, {"text": "Grammatical correctness was also significantly influenced by the affect strength parameter \u03b2 and results show that the correctness deteriorates with increasing \u03b2 (see.", "labels": [], "entities": [{"text": "Grammatical correctness", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7614719569683075}]}, {"text": "However, a post-hoc Tukey test revealed that only the highest \u03b2 value shows a significant drop in grammatical correctness at p<.05.", "labels": [], "entities": []}, {"text": "The multivariate result was significant for negative emotion generated sentences (Pillai's Trace=.130, F(4,413)=2.30, p<.0005).", "labels": [], "entities": [{"text": "Pillai's Trace=.130", "start_pos": 82, "end_pos": 101, "type": "METRIC", "confidence": 0.6367513954639434}, {"text": "F", "start_pos": 103, "end_pos": 104, "type": "METRIC", "confidence": 0.9661290645599365}]}, {"text": "Follow up ANOVAs revealed significant results for affective valence and happy DVs with p<.0005, indicating that the affective valence DV was successfully manipulated with \u03b2, as seen in.", "labels": [], "entities": [{"text": "ANOVAs", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.6073275804519653}]}, {"text": "Further, as intended there were no significant differences for DVs angry, sad and anxious, indicating that the negative emotion DV refers to a more general affect related concept rather than a specific negative emotion.", "labels": [], "entities": []}, {"text": "This finding is in concordance with the intended LIWC category of negative affect that forms a parent category above the more specific emotions, such as angry, sad, and anxious ().", "labels": [], "entities": []}, {"text": "Grammatical correctness was also significantly influenced by the affect strength \u03b2 and results show that the correctness deteriorates with increasing \u03b2 (see).", "labels": [], "entities": [{"text": "Grammatical correctness", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6847291141748428}]}, {"text": "As for positive emotion, a post-hoc Tukey test revealed that only the highest \u03b2 value shows a significant drop in grammatical correctness at p<.05.", "labels": [], "entities": []}, {"text": "The multivariate result was significant for angry generated sentences, F(4,433)=3.76, p<.0001).", "labels": [], "entities": [{"text": "F", "start_pos": 71, "end_pos": 72, "type": "METRIC", "confidence": 0.998342752456665}]}, {"text": "Follow up ANOVAs revealed significant results for affective valence, happy, and angry DVs with p<.0001, indicating that both affective valence and angry DVs were successfully manipulated with \u03b2, as seen in(c).", "labels": [], "entities": [{"text": "ANOVAs", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.6511295437812805}]}, {"text": "Grammatical correctness was not significantly influenced by the affect strength parameter \u03b2, which indicates that angry sentences are highly stable across a wide range of \u03b2 (see 3).", "labels": [], "entities": [{"text": "Grammatical correctness", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.644108772277832}, {"text": "affect strength parameter \u03b2", "start_pos": 64, "end_pos": 91, "type": "METRIC", "confidence": 0.7207624092698097}]}, {"text": "However, it seems that human raters could not successfully distinguish between angry, sad, and anxious affect categories, indicating that the generated sentences likely follow a general negative affect dimension.", "labels": [], "entities": []}, {"text": "The multivariate result was significant for sad generated sentences (Pillai's Trace=.377, F(4,425)=7.33, p<.0001).", "labels": [], "entities": [{"text": "Pillai's Trace", "start_pos": 69, "end_pos": 83, "type": "METRIC", "confidence": 0.5060335099697113}, {"text": "F", "start_pos": 90, "end_pos": 91, "type": "METRIC", "confidence": 0.9841214418411255}]}, {"text": "Follow up ANOVAs revealed significant results only for the sad DV with p<.0001, indicating that while the sad DV can be successfully manipulated with \u03b2, as seen in.", "labels": [], "entities": [{"text": "ANOVAs", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.5846128463745117}]}, {"text": "The grammatical correctness deteriorates significantly with \u03b2.", "labels": [], "entities": []}, {"text": "Specifically, a post-hoc Tukey test revealed that only the two highest \u03b2 values show a significant drop in grammatical correctness at p<.05 (see).", "labels": [], "entities": []}, {"text": "A post-hoc Tukey test for sad reveals that \u03b2 = 3 is optimal for this DV, since it leads to a significant jump in the perceived sadness scores at p<.005 for \u03b2 \u2208 {0, 1, 2}.", "labels": [], "entities": []}, {"text": "The multivariate result was significant for anxious generated sentences (Pillai's Trace=.289, F(4,421)=6.44, p<.0001).", "labels": [], "entities": [{"text": "F", "start_pos": 94, "end_pos": 95, "type": "METRIC", "confidence": 0.9881440997123718}]}, {"text": "Follow up ANOVAs revealed significant results for affective valence, happy and anxious DVs with p<.0001, indicating that both affective valence and anxiety DVs were successfully manipulated with \u03b2, as seen in(e).", "labels": [], "entities": [{"text": "ANOVAs", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.7334912419319153}]}, {"text": "Grammatical correctness was also significantly influenced by the affect strength parameter \u03b2 and results show that the correctness deteriorates with increasing \u03b2.", "labels": [], "entities": [{"text": "Grammatical correctness", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7058638632297516}]}, {"text": "Similarly for sad, a post-hoc Tukey test revealed that only the two highest \u03b2 values show a significant drop in grammatical correctness at p<.05 (see.", "labels": [], "entities": []}, {"text": "Again, a post-hoc Tukey test for anxious reveals that \u03b2 = 3 is optimal for this DV, since it leads to a significant jump in the perceived: Evaluation perplexity scores obtained by the baseline and Affect-LM models when trained on Fisher and subsequently adapted on DAIC, SEMAINE and CMU-MOSI corpora anxiety scores at p<.005 for \u03b2 \u2208 {0, 1, 2}.", "labels": [], "entities": [{"text": "Affect-LM", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9412146806716919}]}], "tableCaptions": [{"text": " Table 1: Summary of corpora used in this paper. CMU-MOSI and SEMAINE are observed to have  higher emotional content than Fisher and DAIC corpora.", "labels": [], "entities": [{"text": "CMU-MOSI", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.8831430673599243}, {"text": "SEMAINE", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.8085140585899353}, {"text": "DAIC corpora", "start_pos": 133, "end_pos": 145, "type": "DATASET", "confidence": 0.8746246099472046}]}, {"text": " Table 3: Evaluation perplexity scores obtained by  the baseline and Affect-LM models when trained  on Fisher and subsequently adapted on DAIC,  SEMAINE and CMU-MOSI corpora", "labels": [], "entities": [{"text": "Affect-LM", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9503835439682007}, {"text": "DAIC", "start_pos": 138, "end_pos": 142, "type": "DATASET", "confidence": 0.8692106604576111}]}]}