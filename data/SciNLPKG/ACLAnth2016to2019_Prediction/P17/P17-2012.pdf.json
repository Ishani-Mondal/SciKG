{"title": [{"text": "Learning to Parse and Translate Improves Neural Machine Translation", "labels": [], "entities": [{"text": "Parse and Translate Improves Neural Machine Translation", "start_pos": 12, "end_pos": 67, "type": "TASK", "confidence": 0.7829777640955788}]}], "abstractContent": [{"text": "There has been relatively little attention to incorporating linguistic prior to neu-ral machine translation.", "labels": [], "entities": [{"text": "neu-ral machine translation", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.5813722014427185}]}, {"text": "Much of the previous work was further constrained to considering linguistic prior on the source side.", "labels": [], "entities": []}, {"text": "In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation.", "labels": [], "entities": [{"text": "attention-based neural machine translation", "start_pos": 153, "end_pos": 195, "type": "TASK", "confidence": 0.6183333992958069}]}, {"text": "Our approach encourages the neu-ral machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward.", "labels": [], "entities": [{"text": "neu-ral machine translation", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.5659592747688293}]}, {"text": "Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) has enjoyed impressive success without relying on much, if any, prior linguistic knowledge.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.861891895532608}]}, {"text": "Some of the most recent studies have for instance demonstrated that NMT systems work comparably to other systems even when the source and target sentences are given simply as flat sequences of characters () or statistically, not linguistically, motivated subword units (.", "labels": [], "entities": []}, {"text": "recently made an observation that the encoder of NMT captures syntactic properties of a source sentence automatically, indirectly suggesting that explicit linguistic prior may not be necessary.", "labels": [], "entities": []}, {"text": "On the other hand, there have only been a couple of recent studies showing the potential benefit of explicitly encoding the linguistic prior into NMT.", "labels": [], "entities": []}, {"text": "for instance proposed to augment each source word with its corresponding part-of-speech tag, lemmatized form and dependency label.", "labels": [], "entities": []}, {"text": "instead replaced the sequential encoder with a tree-based encoder which computes the representation of the source sentence following its parse tree.", "labels": [], "entities": []}, {"text": "let the lattice from a hierarchical phrase-based system guide the decoding process of neural machine translation, which results in two separate models rather than a single end-to-end one.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 86, "end_pos": 112, "type": "TASK", "confidence": 0.7336167494455973}]}, {"text": "Despite the promising improvements, these explicit approaches are limited in that the trained translation model strictly requires the availability of external tools during inference time.", "labels": [], "entities": []}, {"text": "More recently, researchers have proposed methods to incorporate target-side syntax into NMT models.", "labels": [], "entities": []}, {"text": "have proposed a doubly-recurrent neural network that can generate a tree-structured sentence, but its effectiveness in a full scale NMT task is yet to be shown.", "labels": [], "entities": []}, {"text": "introduced a method to serialize a parsed tree and to train the serialized parsed sentences.", "labels": [], "entities": []}, {"text": "We propose to implicitly incorporate linguistic prior based on the idea of multi-task learning).", "labels": [], "entities": []}, {"text": "More specifically, we design a hybrid decoder for NMT, called NMT+RNNG 1 , that combines a usual conditional language model and a recently proposed recurrent neural network grammars (RNNGs,).", "labels": [], "entities": []}, {"text": "This is done by plugging in the conventional language model decoder in the place of the buffer in RNNG, while sharing a subset of parameters, such as word vectors, between the language model and RNNG.", "labels": [], "entities": []}, {"text": "We train this hybrid model to maximize both the log-probability of a target sentence and the log-probability of a parse action sequence.", "labels": [], "entities": []}, {"text": "We use an external parser) to generate target parse actions, but unlike the previous explicit approaches, we do not need it during test time.", "labels": [], "entities": []}, {"text": "We evaluate the proposed NMT+RNNG on four language pairs ({JP, Cs, De, Ru}-En).", "labels": [], "entities": [{"text": "NMT+RNNG", "start_pos": 25, "end_pos": 33, "type": "TASK", "confidence": 0.4687858819961548}]}, {"text": "We observe significant improvements in terms of BLEU scores on three out of four language pairs and RIBES scores on all the language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9994924068450928}, {"text": "RIBES", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.9987757802009583}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of parallel corpora.", "labels": [], "entities": []}, {"text": " Table 2: BLEU and RIBES scores by the baseline  and proposed models on the test set. We use the  bootstrap resampling method from Koehn (2004)  to compute the statistical significance. We use  \u2020 to  mark those significant cases with p < 0.005.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991433620452881}, {"text": "RIBES", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9912892580032349}]}, {"text": " Table 3: Effect of each component in RNNG.", "labels": [], "entities": [{"text": "RNNG", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.6121649146080017}]}]}