{"title": [{"text": "Neural Belief Tracker: Data-Driven Dialogue State Tracking", "labels": [], "entities": [{"text": "Neural Belief Tracker", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7798750400543213}, {"text": "Data-Driven Dialogue State Tracking", "start_pos": 23, "end_pos": 58, "type": "TASK", "confidence": 0.6570784449577332}]}], "abstractContent": [{"text": "One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue.", "labels": [], "entities": []}, {"text": "However, most current approaches have difficulty scaling to larger, more complex dialogue domains.", "labels": [], "entities": []}, {"text": "This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language.", "labels": [], "entities": [{"text": "Spoken Language Understanding", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.7221360802650452}]}, {"text": "We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning.", "labels": [], "entities": [{"text": "Neural Belief Tracking (NBT)", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.811070958773295}, {"text": "representation learning", "start_pos": 123, "end_pos": 146, "type": "TASK", "confidence": 0.9040277004241943}]}, {"text": "NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context.", "labels": [], "entities": [{"text": "NBT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8297674059867859}]}, {"text": "Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spoken dialogue systems (SDS) allow users to interact with computer applications through conversation.", "labels": [], "entities": []}, {"text": "Task-based systems help users achieve goals such as finding restaurants or booking flights.", "labels": [], "entities": []}, {"text": "The dialogue state tracking (DST) component of an SDS serves to interpret user input and update the belief state, which is the system's internal representation of the state of the conversation ( . This is a probability distribution over dialogue states used by the downstream dialogue manager to decide which action the system should perform next (,b); the system action is then verbalised by the natural language generator.", "labels": [], "entities": [{"text": "dialogue state tracking (DST)", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.7496795157591502}]}, {"text": "The Dialogue State Tracking Challenge (DSTC) series of shared tasks has provided a common evaluation framework accompanied by labelled datasets.", "labels": [], "entities": [{"text": "Dialogue State Tracking Challenge (DSTC) series of shared tasks", "start_pos": 4, "end_pos": 67, "type": "TASK", "confidence": 0.8076548793099143}]}, {"text": "In this framework, the dialogue system is supported by a domain ontology which describes the range of user intents the system can process.", "labels": [], "entities": []}, {"text": "The ontology defines a collection of slots and the values that each slot can take.", "labels": [], "entities": []}, {"text": "The system must track the search constraints expressed by users (goals or informable slots) and questions the users ask about search results (requests), taking into account each user utterance (input via a speech recogniser) and the dialogue context (e.g., what the system just said).", "labels": [], "entities": []}, {"text": "The example in shows the true state after each user utterance in a three-turn conversation.", "labels": [], "entities": []}, {"text": "As can be seen in this example, DST models depend on identifying mentions of ontology items in user utterances.", "labels": [], "entities": [{"text": "DST", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.983346700668335}]}, {"text": "This becomes a non-trivial task when confronted with lexical variation, the dynamics of context and noisy automated speech recognition (ASR) output.", "labels": [], "entities": [{"text": "automated speech recognition (ASR) output", "start_pos": 106, "end_pos": 147, "type": "TASK", "confidence": 0.8192312461989266}]}, {"text": "Traditional statistical approaches use separate Spoken Language Understanding (SLU) modules to address lexical variability within a single dialogue turn.", "labels": [], "entities": []}, {"text": "However, training such models requires substantial amounts of domain-specific annotation.", "labels": [], "entities": []}, {"text": "Alternatively, turn-level SLU and cross-turn DST can be coalesced into a single model to achieve superior belief tracking performance, as shown by.", "labels": [], "entities": [{"text": "belief tracking", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.7629678845405579}]}, {"text": "Such coupled models typically rely on manually constructed semantic dictionaries to identify alternative mentions of ontology items that vary lexically or morphologically.", "labels": [], "entities": []}, {"text": "gives an example of such a dictionary for three slot-value pairs.", "labels": [], "entities": []}, {"text": "This approach, which we term delexicalisation, is clearly not scalable to larger, more complex dialogue domains.", "labels": [], "entities": []}, {"text": "Importantly, the focus on English in DST research understates the considerable challenges that morphology poses to systems based on exact matching in morphologically richer languages such as Italian or German (see Vuli\u00b4c).", "labels": [], "entities": []}, {"text": "In this paper, we present two new models, collectively called the Neural Belief Tracker (NBT) family.", "labels": [], "entities": [{"text": "Neural Belief Tracker (NBT)", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.7619936267534891}]}, {"text": "The proposed models couple SLU and DST, efficiently learning to handle variation without requiring any hand-crafted resources.", "labels": [], "entities": []}, {"text": "To do that, NBT models move away from exact matching and instead reason entirely over pre-trained word vectors.", "labels": [], "entities": []}, {"text": "The vectors making up the user utterance and preceding system output are first composed into intermediate representations.", "labels": [], "entities": []}, {"text": "These representations are then used to decide which of the ontologydefined intents have been expressed by the user up to that point in the conversation.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, NBT models are the first to successfully use pre-trained word vector spaces to improve the language understanding capability of belief tracking models.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.6888835579156876}, {"text": "belief tracking", "start_pos": 158, "end_pos": 173, "type": "TASK", "confidence": 0.747633695602417}]}, {"text": "In evaluation on two datasets, we show that: a) NBT models match the performance of delexicalisation-based models which make use of hand-crafted semantic lexicons; and b) the NBT models significantly outperform those models when such resources are not available.", "labels": [], "entities": []}, {"text": "Consequently, we believe this work proposes a framework better-suited to scaling belief tracking models for deployment in real-world dialogue systems operating over sophisticated application domains where the creation of such domain-specific lexicons would be infeasible.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two datasets were used for training and evaluation.", "labels": [], "entities": []}, {"text": "Both consist of user conversations with taskoriented dialogue systems designed to help users find suitable restaurants around Cambridge, UK.", "labels": [], "entities": []}, {"text": "The two corpora share the same domain ontology, which contains three informable (i.e. goal-tracking) slots: FOOD, AREA and PRICE.", "labels": [], "entities": [{"text": "FOOD", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9542284607887268}, {"text": "AREA", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9531498551368713}]}, {"text": "The users can specify values for these slots in order to find restaurants 1.", "labels": [], "entities": []}, {"text": "DSTC2: We use the transcriptions, ASR hypotheses and turn-level semantic labels provided for the Dialogue State Tracking Challenge 2.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9511559009552002}, {"text": "Dialogue State Tracking Challenge", "start_pos": 97, "end_pos": 130, "type": "TASK", "confidence": 0.7908115237951279}]}, {"text": "The official transcriptions contain various spelling errors which we corrected manually; the cleaned version of the dataset is available at mi.eng.cam.ac.uk/ \u02dc nm480/ dstc2-clean.zip.", "labels": [], "entities": []}, {"text": "The training data contains 2207 dialogues and the test set consists of 1117 dialogues.", "labels": [], "entities": []}, {"text": "We train NBT models on transcriptions but report belief tracking performance on test set ASR hypotheses provided in the original challenge.", "labels": [], "entities": []}, {"text": "2. WOZ 2.0: Wen et al. performed a Wizard of Oz style experiment in which Amazon Mechanical Turk users assumed the role of the system or the user of a task-oriented dialogue system based on the DSTC2 ontology.", "labels": [], "entities": []}, {"text": "Users typed instead of using speech, which means performance in the WOZ experiments is more indicative of the model's capacity for semantic understanding than its robustness to ASR errors.", "labels": [], "entities": [{"text": "semantic understanding", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.7149804830551147}, {"text": "ASR", "start_pos": 177, "end_pos": 180, "type": "TASK", "confidence": 0.9764605164527893}]}, {"text": "Whereas in the DSTC2 dialogues users would quickly adapt to the system's (lack of) language understanding capability, the WOZ experimental design gave them freedom to use more sophisticated language.", "labels": [], "entities": []}, {"text": "We expanded the original WOZ dataset from using the same data collection procedure, yielding a total of 1200 dialogues.", "labels": [], "entities": [{"text": "WOZ dataset", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.8822689354419708}]}, {"text": "We divided these into 600 training, 200 validation and 400 test set dialogues.", "labels": [], "entities": []}, {"text": "The WOZ 2.0 dataset is available at mi.eng.cam.ac. uk/ \u02dc nm480/woz_2.0.zip.", "labels": [], "entities": [{"text": "WOZ 2.0 dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8938390612602234}]}], "tableCaptions": [{"text": " Table 1: DSTC2 and WOZ 2.0 test set accuracies for: a) joint goals; and b) turn-level requests. The  asterisk indicates statistically significant improvement over the baseline trackers (paired t-test; p < 0.05).", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.9169477224349976}, {"text": "WOZ 2.0 test", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.8050874869028727}]}]}