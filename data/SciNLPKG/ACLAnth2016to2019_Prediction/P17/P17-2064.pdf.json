{"title": [{"text": "Detection of Chinese Word Usage Errors for Non-Native Chinese Learners with Bidirectional LSTM", "labels": [], "entities": [{"text": "Detection of Chinese Word Usage", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8929502606391907}]}], "abstractContent": [{"text": "Selecting appropriate words to compose a sentence is one common problem faced by non-native Chinese learners.", "labels": [], "entities": []}, {"text": "In this paper , we propose (bidirectional) LSTM sequence labeling models and explore various features to detect word usage errors in Chinese sentences.", "labels": [], "entities": [{"text": "LSTM sequence labeling", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.738741914431254}]}, {"text": "By combining CWIN-DOW word embedding features and POS information, the best bidirectional LSTM model achieves accuracy 0.5138 and MRR 0.6789 on the HSK dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9994237422943115}, {"text": "MRR", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.9859364032745361}, {"text": "HSK dataset", "start_pos": 148, "end_pos": 159, "type": "DATASET", "confidence": 0.9757710993289948}]}, {"text": "For 80.79% of the test data, the model ranks the ground-truth within the top two at position level.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, more and more people around the world choose Chinese as their second language.", "labels": [], "entities": []}, {"text": "That results in an increasing need for automatic grammatical error detection and correction (GEC) tools.", "labels": [], "entities": [{"text": "grammatical error detection and correction (GEC)", "start_pos": 49, "end_pos": 97, "type": "TASK", "confidence": 0.7941118590533733}]}, {"text": "To measure the performance of GEC systems in a standardized manner, several shared tasks have been conducted for English () and Chinese (.", "labels": [], "entities": []}, {"text": "In Chinese sentences, a word usage error (WUE) is a grammatically or semantically incorrect token which is written in a wrong form itself, or is an existent word but is improper for its context (refer to example (E1)).", "labels": [], "entities": [{"text": "word usage error (WUE)", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.6033874253431956}]}, {"text": "In fact, many Chinese WUEs result from subtle semantic unsuitability instead of violation of syntactic constraints.", "labels": [], "entities": []}, {"text": "In example (E1), both \u6b0a\u529b (power) and \u6b0a\u5229 (right) are nouns in Chinese, and both versions are grammatically correct.", "labels": [], "entities": []}, {"text": "It is difficult to formulate an explicit rule for recognizing this kind of errors.", "labels": [], "entities": []}, {"text": "(E1) \u4eba\u5011 \u6709 (*\u6b0a\u529b,\u6b0a\u5229) \u5403 \u5b89\u5168 \u7684 \u98df\u54c1 \u3002 ( People have the (*power, right) to enjoy safe food.", "labels": [], "entities": []}, {"text": ") Shiue and Chen (2016) adopted the HSK corpus, a dynamic composition corpus built by Beijing Language and Culture University, to study the detection of WUEs.", "labels": [], "entities": [{"text": "HSK corpus", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.9274448752403259}, {"text": "Beijing Language and Culture University", "start_pos": 86, "end_pos": 125, "type": "DATASET", "confidence": 0.8525529503822327}]}, {"text": "Instead of specific position information, their model only determines whether a sentence segment contains WUEs.", "labels": [], "entities": []}, {"text": "used the HSK corpus to study the preposition selection problem.", "labels": [], "entities": [{"text": "HSK corpus", "start_pos": 9, "end_pos": 19, "type": "DATASET", "confidence": 0.9510282874107361}, {"text": "preposition selection", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.8116340041160583}]}, {"text": "They proposed gated recurrent unit (GRU)-based models to select the most suitable one from a closed set of Chinese prepositions given the sentential context.", "labels": [], "entities": []}, {"text": "Although their approach can be utilized to detect and correct preposition errors, it is still worth investigating how to recognize WUEs involving other types of words such as verbs and nouns.", "labels": [], "entities": []}, {"text": "In the past few years, distributed word representations derived from neural network models () have become popular among various studies in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 139, "end_pos": 166, "type": "TASK", "confidence": 0.6471416652202606}]}, {"text": "Beyond surface forms, these low-dimensional vector representations can encode syntactic and semantic information implicitly).", "labels": [], "entities": []}, {"text": "Because WUEs involve syntactic or semantic problems, vector representations could be promising for finding the erroneous tokens.", "labels": [], "entities": []}, {"text": "One challenging aspect of dealing with grammatical errors is that the errors usually do not stand on their own, but are dependent on the context ().", "labels": [], "entities": []}, {"text": "Therefore, we need a model that considers the sequence of words in a sentence as a whole to determine which position needs correction.", "labels": [], "entities": []}, {"text": "One possible model for this task is the Long Short-Term Memory (LSTM) model, which processes sequential data and generates the output based not only on the information of the current time step, but also on the past information stored in the memory layer.", "labels": [], "entities": []}, {"text": "adopted neural network models, including LSTM, to detect errors in English learner writing.", "labels": [], "entities": []}, {"text": "However, they mainly focused on comparing different composition architectures under the same word representation, so it remained unclear to what extent pre-trained word embeddings can help.", "labels": [], "entities": []}, {"text": "used LSTM for Chinese grammatical error diagnosis, but their models are trained only on learner data, without external well-formed text.", "labels": [], "entities": [{"text": "Chinese grammatical error diagnosis", "start_pos": 14, "end_pos": 49, "type": "TASK", "confidence": 0.769148588180542}]}, {"text": "That means the performance might be limited by the relatively small amount of annotated sentences written by foreign learners.", "labels": [], "entities": []}, {"text": "This paper utilizes LSTM and its extension (Bidirectional LSTM) along with the information derived from external resources to deal with Chinese WUE detection.", "labels": [], "entities": [{"text": "WUE detection", "start_pos": 144, "end_pos": 157, "type": "TASK", "confidence": 0.6817242950201035}]}, {"text": "Several types of pre-trained word embeddings and additional token-level features are considered.", "labels": [], "entities": []}, {"text": "Each token in a sentence will be labeled corrector incorrect.", "labels": [], "entities": []}, {"text": "Experimental results show that our models can rank the groundtruth error position toward the top of the candidate list.", "labels": [], "entities": []}], "datasetContent": [{"text": "We obtain the \"wrong\" part of the HSK dataset used in.", "labels": [], "entities": [{"text": "HSK dataset", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.9428881108760834}]}, {"text": "Each sentence segment has exactly one token-level position that is erroneous.", "labels": [], "entities": []}, {"text": "Word segmentation and POS tagging are performed with the Stanford CoreNLP toolkit ( ).", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6792266219854355}, {"text": "POS tagging", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.8390212953090668}, {"text": "Stanford CoreNLP toolkit", "start_pos": 57, "end_pos": 81, "type": "DATASET", "confidence": 0.9434002637863159}]}, {"text": "We filter out any sentence segment whose corrected version differs from it by more than one token due to segmentation issue.", "labels": [], "entities": []}, {"text": "That is, we only focus on the cases in which the error can be corrected by replacing one single token.", "labels": [], "entities": []}, {"text": "After filtering, we end up with 10,510 sentence segments.", "labels": [], "entities": []}, {"text": "We use 10% data for validation and testing respectively, and the remaining 80% data as the training set.", "labels": [], "entities": [{"text": "validation", "start_pos": 20, "end_pos": 30, "type": "TASK", "confidence": 0.9135192632675171}]}], "tableCaptions": [{"text": " Table 1: Performance of the LSTM/Bi-LSTM sequence labeling models with different sets of features.", "labels": [], "entities": []}, {"text": " Table 2: Hit@20% rates of LSTM and Bi-LSTM  on segments with different lengths.", "labels": [], "entities": [{"text": "Bi-LSTM", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9589649438858032}]}, {"text": " Table 3: Summary of the analysis of the depen- dency between the top two candidates proposed by  the CWIN+POS+n-gram Bi-LSTM model. a de- notes the ground-truth error position. c 1 and c 2  denote the first and the second candidate positions  proposed by the model. dis(c 1 , c 2 ) is the distance  between c 1 and c 2 on the dependency graph.", "labels": [], "entities": []}, {"text": " Table 4: Hit@20% rates of Bi-LSTM models  with or without POS features on three most fre- quent POS tags of the erroneous token.", "labels": [], "entities": []}, {"text": " Table 4. As can be seen, the POS information of  the erroneous segment, which potentially contains  errors, can still be helpful for detecting anomaly  of the segment. In example (E6) we show the  scores of incorrectness predicted by models with  or without POS features. The \"DEC + AD\" con- struction is invalid in Chinese, so in this case the  error can be detected more easily if POS informa- tion is available.", "labels": [], "entities": []}]}