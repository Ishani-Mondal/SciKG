{"title": [{"text": "An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.708812952041626}]}], "abstractContent": [{"text": "In this paper, we propose a novel domain adaptation method named \"mixed fine tuning\" for neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 89, "end_pos": 121, "type": "TASK", "confidence": 0.8172152737776438}]}, {"text": "We combine two existing approaches namely fine tuning and multi domain NMT.", "labels": [], "entities": [{"text": "fine tuning", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.820362776517868}]}, {"text": "We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the in-domain and out-of-domain corpora.", "labels": [], "entities": []}, {"text": "All corpora are augmented with artificial tags to indicate specific domains.", "labels": [], "entities": []}, {"text": "We empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the most attractive features of neural machine translation (NMT) () is that it is possible to train an end to end system without the need to deal with word alignments, translation rules and complicated decoding algorithms, which area characteristic of statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.8498582641283671}, {"text": "word alignments", "start_pos": 158, "end_pos": 173, "type": "TASK", "confidence": 0.7075742036104202}, {"text": "statistical machine translation (SMT)", "start_pos": 259, "end_pos": 296, "type": "TASK", "confidence": 0.7824792563915253}]}, {"text": "However, it is reported that NMT works better than SMT only when there is an abundance of parallel corpora.", "labels": [], "entities": [{"text": "SMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.991050124168396}]}, {"text": "In the case of low resource domains, vanilla NMT is either worse than or comparable to SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.9542985558509827}]}, {"text": "Domain adaptation has been shown to be effective for low resource NMT.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7980616986751556}]}, {"text": "The conventional domain adaptation method is fine tuning, in which an out-of-domain model is further trained on indomain data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7918731570243835}, {"text": "fine tuning", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.8758296966552734}]}, {"text": "However, fine tuning * This work was done when the first author was a researcher of Japan Science and Technology Agency.", "labels": [], "entities": [{"text": "Japan Science and Technology Agency", "start_pos": 84, "end_pos": 119, "type": "DATASET", "confidence": 0.961420738697052}]}, {"text": "tends to overfit quickly due to the small size of the in-domain data.", "labels": [], "entities": []}, {"text": "On the other hand, multi domain NMT () involves training a single NMT model for multiple domains.", "labels": [], "entities": []}, {"text": "This method adds tags \"<2domain>\" to the source sentences in the parallel corpora to indicate domains without any modifications to the NMT system architecture.", "labels": [], "entities": []}, {"text": "However, this method has not been studied for domain adaptation in particular.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.788181483745575}]}, {"text": "Motivated by these two lines of studies, we propose anew domain adaptation method called \"mixed fine tuning,\" where we first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus that is a mix of the in-domain and out-of-domain corpora.", "labels": [], "entities": []}, {"text": "Fine tuning on the mixed corpus instead of the indomain corpus can address the overfitting problem.", "labels": [], "entities": []}, {"text": "All corpora are augmented with artificial tags to indicate specific domains.", "labels": [], "entities": []}, {"text": "We tried two different corpora settings on two different language pairs: \u2022 Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) () to improve the translation quality for the IWSLT data (TED talks; resource poor) ().", "labels": [], "entities": [{"text": "NTCIR data", "start_pos": 157, "end_pos": 167, "type": "DATASET", "confidence": 0.9753744006156921}, {"text": "IWSLT data", "start_pos": 245, "end_pos": 255, "type": "DATASET", "confidence": 0.9279195964336395}]}, {"text": "\u2022 Automatically extracted resource poor corpus (Chinese-to-Japanese translation): Using the ASPEC data (scientific domain; resource rich) ( ) to improve the translation quality for the Wiki data (resource poor).", "labels": [], "entities": [{"text": "Chinese-to-Japanese translation)", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.7962956428527832}, {"text": "ASPEC data", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.8862994313240051}, {"text": "Wiki data", "start_pos": 185, "end_pos": 194, "type": "DATASET", "confidence": 0.9104937016963959}]}, {"text": "The parallel corpus of the latter domain was automatically extracted ().", "labels": [], "entities": []}, {"text": "We observed that \"mixed fine tuning\" works significantly better than methods that use fine tuning In it ia li z e \u2022 We propose a novel method that combines the best of existing approaches and show that it is effective.", "labels": [], "entities": []}, {"text": "\u2022 To the best of our knowledge this is the first work on an empirical comparison of various domain adaptation methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted NMT domain adaptation experiments in two different settings as follows:", "labels": [], "entities": [{"text": "NMT domain adaptation", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.8464548587799072}]}], "tableCaptions": [{"text": " Table 1: Domain adaptation results (BLEU-4 scores) for IWSLT-CE using NTCIR-CE.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7413913905620575}, {"text": "BLEU-4", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9988736510276794}, {"text": "IWSLT-CE", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.8299559354782104}, {"text": "NTCIR-CE", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9124060869216919}]}, {"text": " Table 2: Domain adaptation results (BLEU-4  scores) for WIKI-CJ using ASPEC-CJ.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7756966650485992}, {"text": "BLEU-4", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9988260865211487}, {"text": "WIKI-CJ", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.8671934604644775}, {"text": "ASPEC-CJ", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.7668541073799133}]}]}