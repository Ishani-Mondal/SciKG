{"title": [{"text": "Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective Function", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we define a measure of dependency between two random variables, based on the Jensen-Shannon (JS) divergence between their joint distribution and the product of their marginal distributions.", "labels": [], "entities": []}, {"text": "Then, we show that word2vec's skip-gram with negative sampling embedding algorithm finds the optimal low-dimensional approximation of this JS dependency measure between the words and their contexts.", "labels": [], "entities": []}, {"text": "The gap between the optimal score and the low-dimensional approximation is demonstrated on a standard text corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Continuous word representations, derived from unlabeled text, have proven useful in many NLP tasks.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 89, "end_pos": 98, "type": "TASK", "confidence": 0.8896260857582092}]}, {"text": "Such word representations (or embeddings) associate a low-dimensional, real-valued vector with each word, typically induced via neural language models or matrix factorization.", "labels": [], "entities": []}, {"text": "Substantial benefit arises when embeddings can be efficiently trained on large volumes of data.", "labels": [], "entities": []}, {"text": "Hence the recent considerable interest in the continuous bag-of-words (CBOW) and skip-gram with negative sampling (SGNS) models, described in (, as implemented in the opensource toolkit word2vec.", "labels": [], "entities": []}, {"text": "These models are based on a relatively simple log-linear method and avoid hidden layers typical to neural networks.", "labels": [], "entities": []}, {"text": "Consequently, they can be trained to produce high-quality word embeddings on large corpora like the entirety of English Wikipedia in several hours, compared to days or even weeks in the case of other continuous models.", "labels": [], "entities": []}, {"text": "Recent studies obtained state-of-the-art results by using skip-gram embeddings on a variety of natural language processing tasks, such as named entity extraction ( and dependency parsing ().", "labels": [], "entities": [{"text": "named entity extraction", "start_pos": 138, "end_pos": 161, "type": "TASK", "confidence": 0.6305912335713705}, {"text": "dependency parsing", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.8023251295089722}]}, {"text": "In recent years, there were several attempts to mathematically interpret word embedding models (.", "labels": [], "entities": []}, {"text": "Our study pursues this established line of work, attempting to explain the objective function of the SGNS word embedding algorithm.", "labels": [], "entities": [{"text": "SGNS word embedding algorithm", "start_pos": 101, "end_pos": 130, "type": "TASK", "confidence": 0.8560666590929031}]}, {"text": "In the SGNS model, the energy function takes the form of a dot product between the vectors of an observed word and an observed context.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 7, "end_pos": 11, "type": "TASK", "confidence": 0.9610406756401062}]}, {"text": "The objective function is a binary logistic regression classifier that treats a word and its observed context as a positive example, and a word and a randomly sampled context as a negative example.", "labels": [], "entities": []}, {"text": "offered a motivation for this function by showing that it obtains its global maximum value at the word-context pointwise mutual information (PMI) matrix.", "labels": [], "entities": []}, {"text": "In this study, we take their analysis one step further and provide an informationtheoretical interpretation of the SGNS objective function.", "labels": [], "entities": [{"text": "SGNS objective", "start_pos": 115, "end_pos": 129, "type": "TASK", "confidence": 0.8625097274780273}]}, {"text": "In Section 2, we define anew measure of mutual information between random variables based the Jensen-Shennon divergence instead of the KL divergence.", "labels": [], "entities": []}, {"text": "In Section 3, we show that the value of the SGNS objective computed at the PMI matrix is this information measure.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 44, "end_pos": 48, "type": "TASK", "confidence": 0.8615192770957947}]}, {"text": "We then derive an explicit expression for the information loss caused by the low-dimensional embedding learned by the SGNS algorithm.", "labels": [], "entities": []}, {"text": "Finally, in Section 4, we illustrate this by computing the information loss caused by actual SGNS embeddings learned on a standard text corpus.", "labels": [], "entities": []}, {"text": "There are several standard methods of measuring the distance between two discrete probability distributions, defined on a given finite set A.", "labels": [], "entities": []}, {"text": "The Kullback-Leibler (KL) divergence of a distribution p from a distribution q is defined as follows: . The mutual information between two jointly distributed random variables X and Y is defined as the KL divergence of the joint distribution p(x, y) from the product p(x)p(y) of the marginal distributions of X and Y, i.e. I(X; Y ) = KL(p(x, y)||p(x)p(y)).", "labels": [], "entities": []}, {"text": "The Jensen-Shannon (JS) divergence (Lin, 1991) between distributions p and q is: such that 0 < \u03b1 < 1, r = \u03b1p + (1 \u2212 \u03b1)q and H is the entropy function (i.e. H(p) = \u2212 i pi log pi ).", "labels": [], "entities": []}, {"text": "Unlike KL divergence, JS divergence is bounded from above and 0 \u2264 JS \u03b1 (p, q) \u2264 1.", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 7, "end_pos": 20, "type": "TASK", "confidence": 0.7222601473331451}, {"text": "JS divergence", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.8115941286087036}]}, {"text": "We next propose anew measure for mutualinformation using the JS-divergence between p(x, y) and p(x)p(y) instead of the KL-divergence.", "labels": [], "entities": [{"text": "mutualinformation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.9683552980422974}]}, {"text": "We define the Jensen-Shannon Mutual information (JSMI) as follows: It can be easily verified that X and Y are independent if and only if JSMI \u03b1 (X, Y ) = 0.", "labels": [], "entities": [{"text": "Mutual information (JSMI)", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.4715748906135559}]}, {"text": "We next derive an alternative definition of the JSMI dependency measure.", "labels": [], "entities": [{"text": "JSMI dependency measure", "start_pos": 48, "end_pos": 71, "type": "DATASET", "confidence": 0.624444325764974}]}, {"text": "Assume we choose between the two distributions, p(x, y) and the product of marginal distributions p(x)p(y), according to a binary random variable Z, such that p(Z = 1) = \u03b1.", "labels": [], "entities": []}, {"text": "We first sample a binary value for Z and next, we sample a r.v.", "labels": [], "entities": []}, {"text": "W as follows: The divergence measure JSMI \u03b1 (X, Y ) can be alternatively defined in terms of mutual information between W and Z.", "labels": [], "entities": [{"text": "divergence measure JSMI \u03b1", "start_pos": 18, "end_pos": 43, "type": "METRIC", "confidence": 0.7901826500892639}]}, {"text": "The mutual-information between W and Z is: Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.9753225445747375}]}, {"text": "(1) thus implies that: Applying Bayes rule we obtain: is the sigmoid function and is a shifted version of the PMI function.", "labels": [], "entities": []}, {"text": "Equations and imply that: = h(\u03b1)+\u03b1 is the binary entropy function.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we use word2vec to train real skipgram with negative sampling (SGNS) embedding models.", "labels": [], "entities": []}, {"text": "By measuring the value of their objective function and comparing it against the optimal one using exact PMI values, we demonstrate how a well-trained model minimizes the difference in Eq.", "labels": [], "entities": []}, {"text": "We note that this is an intrinsic measure that does not necessarily reflect the usefulness of the learned embeddings for other tasks.", "labels": [], "entities": []}, {"text": "We used the Penn Tree Bank (PTB), a popular small-scale corpus, for our experiments.", "labels": [], "entities": [{"text": "Penn Tree Bank (PTB)", "start_pos": 12, "end_pos": 32, "type": "DATASET", "confidence": 0.9801447292168936}]}, {"text": "A version of this dataset is available from Tomas Mikolov.", "labels": [], "entities": []}, {"text": "1 It consists of 929K training words with a 10K word vocabulary, which we used to train our models.", "labels": [], "entities": []}, {"text": "To learn the SGNS word embeddings, we used word2vec's default parameter values: windowsize = 5, min-count = 5, and number of negative samples k = 5.", "labels": [], "entities": []}, {"text": "We varied the dimensionality of the embeddings and the number of training iterations performed.", "labels": [], "entities": []}, {"text": "Once the models were trained, we measured their score (9) on the training corpus.", "labels": [], "entities": []}, {"text": "Based on the same learning corpus, we computed S(pmi) = JSMI \u03b1 (X, Y ) for \u03b1 = 1 k+1 = 1/6.", "labels": [], "entities": []}, {"text": "Note that p(x, y) = 0 implies that pmi x,y = \u2212\u221e and therefore log \u03c3(\u2212pmi x,y ) = 0.", "labels": [], "entities": []}, {"text": "Hence, as in the second term, to compute the third term of S(m) (8) for the case of m = pmi, we can sum only over the positive pairs (x, y) that actually appear in the corpus.", "labels": [], "entities": []}, {"text": "In other words, for the special case m = pmi, it is feasible to compute the exact score and not just its approximation (9) that is based on negative sampling.", "labels": [], "entities": [{"text": "exact score", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.9689893424510956}]}, {"text": "illustrates the optimal PMI-based score, compared with the scores obtained by different models with varied embedding dimensionality and number of training iterations.", "labels": [], "entities": [{"text": "PMI-based", "start_pos": 24, "end_pos": 33, "type": "TASK", "confidence": 0.6009610295295715}]}, {"text": "As can be seen, the embeddings score gets close to the optimal value using higher dimensionality and more training iterations, but doesn't surpass it.", "labels": [], "entities": []}], "tableCaptions": []}