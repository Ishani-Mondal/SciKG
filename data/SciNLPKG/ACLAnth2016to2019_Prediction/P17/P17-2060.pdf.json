{"title": [{"text": "Neural System Combination for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7822434902191162}]}], "abstractContent": [{"text": "Neural machine translation (NMT) becomes anew approach to machine translation and generates much more fluent results compared to statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7754852523406347}, {"text": "machine translation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7677208483219147}, {"text": "statistical machine translation (SMT)", "start_pos": 129, "end_pos": 166, "type": "TASK", "confidence": 0.7977497577667236}]}, {"text": "However, SMT is usually better than NMT in translation adequacy.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9856690168380737}]}, {"text": "It is therefore a promising direction to combine the advantages of both NMT and SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9638211727142334}]}, {"text": "In this paper, we propose a neu-ral system combination framework lever-aging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation.", "labels": [], "entities": []}, {"text": "Extensive experiments on the Chinese-to-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods.", "labels": [], "entities": [{"text": "Chinese-to-English translation task", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.7548639674981436}, {"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9986887574195862}, {"text": "BLEU", "start_pos": 180, "end_pos": 184, "type": "METRIC", "confidence": 0.9987301230430603}]}], "introductionContent": [{"text": "Neural machine translation has significantly improved the quality of machine translation in recent several years ().", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6508728464444479}, {"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7330713868141174}]}, {"text": "Although most sentences are more fluent than translations by statistical machine translation (SMT) (), NMT has a problem to address translation adequacy especially for the rare and unknown words.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 61, "end_pos": 98, "type": "TASK", "confidence": 0.7957275112469991}]}, {"text": "Additionally, it suffers from over-translation and under-translation to some extent ().", "labels": [], "entities": []}, {"text": "Compared to N-MT, SMT, such as phrase-based machine translation (PBMT, () and hierarchical phrase-based machine translation (HPMT, * Corresponding author.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9868052005767822}, {"text": "phrase-based machine translation", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.6012691458066305}, {"text": "phrase-based machine translation", "start_pos": 91, "end_pos": 123, "type": "TASK", "confidence": 0.6526158849398295}]}, {"text": "(), does not need to limit the vocabulary and can guarantee translation coverage of source sentences.", "labels": [], "entities": [{"text": "translation coverage of source sentences", "start_pos": 60, "end_pos": 100, "type": "TASK", "confidence": 0.8715862989425659}]}, {"text": "It is obvious that NMT and SMT have different strength and weakness.", "labels": [], "entities": [{"text": "NMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.48446816205978394}, {"text": "SMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9591895937919617}]}, {"text": "In order to take full advantages of both NMT and SMT, system combination can be a good choice.", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9708608984947205}]}, {"text": "Traditionally, system combination has been explored respectively in sentence-level, phrase-level, and word-level ().", "labels": [], "entities": [{"text": "system combination", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7122776061296463}]}, {"text": "Among them, word-level combination approaches that adopt confusion network for decoding have been quite successful ().", "labels": [], "entities": []}, {"text": "However, these approaches are mainly designed for SMT without considering the features of NMT results.", "labels": [], "entities": [{"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9958945512771606}]}, {"text": "NMT opts to produce diverse words and free word order, which are quite different from SMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7037808895111084}, {"text": "SMT", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9632084965705872}]}, {"text": "And this will make it hard to construct a consistent confusion network.", "labels": [], "entities": []}, {"text": "Furthermore, traditional system combination approaches cannot guarantee the fluency of the final translation results.", "labels": [], "entities": []}, {"text": "In this paper, we propose a neural system combination framework, which is adapted from the multi-source NMT model.", "labels": [], "entities": []}, {"text": "Different encoders are employed to model the semantics of the source language input and each best translation produced by different NMT and SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.8322567343711853}]}, {"text": "The encoders produce multiple context vector representations, from which the decoder generates the final output word byword.", "labels": [], "entities": []}, {"text": "Since the same training data is used for NMT, SMT and neural system combination, we further design a smart strategy to simulate the real training data for neural system combination.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9740895628929138}]}, {"text": "Specifically, we make the following contributions in this paper: \u2022 We propose a neural system combination method, which is adapted from multi-source NMT model and can accommodate both source inputs and different system translations.", "labels": [], "entities": []}, {"text": "It combines the fluency of NMT and adequacy (especially the ability to address rare words) of SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.9733315110206604}]}, {"text": "\u2022 We design a good strategy to construct appropriate training data for neural system combination.", "labels": [], "entities": []}, {"text": "\u2022 The extensive experiments on ChineseEnglish translation show that our model archives significant improvement by 3.4 BLEU points over the state-of-the-art system combination methods and 5.3 BLEU points over the best individual system output.", "labels": [], "entities": [{"text": "ChineseEnglish translation", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.7744903862476349}, {"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9991399049758911}, {"text": "BLEU", "start_pos": 191, "end_pos": 195, "type": "METRIC", "confidence": 0.9988343119621277}]}], "datasetContent": [{"text": "We perform our experiments on the ChineseEnglish translation task.", "labels": [], "entities": [{"text": "ChineseEnglish translation task", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.8335618376731873}]}, {"text": "The MT systems participating in system combination are PBMT, H-PMT and NMT.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9401335716247559}]}, {"text": "The evaluation metric is caseinsensitive BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9894337058067322}]}], "tableCaptions": [{"text": " Table 1: Translation results (BLEU score) for different machine translation and system combination  methods. Jane is a open source machine translation system combination toolkit that uses confusion  network decoding. Best and important results per category are highlighted.", "labels": [], "entities": [{"text": "BLEU score)", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.975516140460968}, {"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7235687971115112}, {"text": "Jane", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.9571802616119385}]}, {"text": " Table 2: The number of unknown words in the re- sults of NMT and our model.", "labels": [], "entities": []}, {"text": " Table 3: Translation results (BLEU score) when  we replace original NMT with strong E-NMT,  which uses ensemble strategy with four NMT  models. All results of system combination are  based on strong outputs of E-NMT.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9396112561225891}, {"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9756551086902618}]}]}