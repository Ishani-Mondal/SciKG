{"title": [{"text": "Discourse Annotation of Non-native Spontaneous Spoken Responses Using the Rhetorical Structure Theory Framework", "labels": [], "entities": [{"text": "Discourse Annotation of Non-native Spontaneous Spoken Responses", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.7003508891378131}]}], "abstractContent": [{"text": "The availability of the Rhetorical Structure Theory (RST) Discourse Treebank has spurred substantial research into discourse analysis of written texts; however , limited research has been conducted to date on RST annotation and parsing of spoken language, in particular, non-native spontaneous speech.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST) Discourse", "start_pos": 24, "end_pos": 67, "type": "TASK", "confidence": 0.6987593940326146}, {"text": "discourse analysis of written texts", "start_pos": 115, "end_pos": 150, "type": "TASK", "confidence": 0.8246008396148682}, {"text": "RST annotation and parsing of spoken language", "start_pos": 209, "end_pos": 254, "type": "TASK", "confidence": 0.7540143941129956}]}, {"text": "Considering that the measurement of discourse coherence is typically a key metric inhuman scoring rubrics for assessments of spoken language, we initiated a research effort to obtain RST annotations of a large number of non-native spoken responses from a standardized assessment of academic English proficiency.", "labels": [], "entities": [{"text": "RST annotations", "start_pos": 183, "end_pos": 198, "type": "TASK", "confidence": 0.8610211610794067}]}, {"text": "The resulting inter-annotator \u03ba agreements on the three different levels of Span, Nuclear-ity, and Relation are 0.848, 0.766, and 0.653, respectively.", "labels": [], "entities": [{"text": "Relation", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9623774290084839}]}, {"text": "Furthermore, a set of features was explored to evaluate the discourse structure of non-native spontaneous speech based on these annotations; the highest performing feature showed a correlation of 0.612 with scores of discourse coherence provided by expert human raters.", "labels": [], "entities": []}], "introductionContent": [{"text": "The spread of English as the global language of education and commerce is continuing, and there is a strong interest in developing assessment systems that can automatically score spontaneous speech from non-native speakers with the goals of reducing the burden on human raters, improving reliability, and generating feedback that can be used by language learners.", "labels": [], "entities": [{"text": "reliability", "start_pos": 288, "end_pos": 299, "type": "METRIC", "confidence": 0.9741355180740356}]}, {"text": "Discourse coherence, which refers to the conceptual relations between different units within a response, is an important aspect of communicative competence, as is reflected inhuman scoring rubrics for assessments of non-native English.", "labels": [], "entities": []}, {"text": "However, discourse-level features have rarely been investigated in the context of automated speech scoring.", "labels": [], "entities": [{"text": "automated speech scoring", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.6250726083914439}]}, {"text": "This study aims to construct a discourselevel annotation of non-native spontaneous speech in the framework of Rhetorical Structure Theory (RST) (, which can then be used in automated discourse analysis and coherence measurement for non-native spoken responses, thereby improving the validity of the automated scoring systems.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 110, "end_pos": 143, "type": "TASK", "confidence": 0.6943045655886332}, {"text": "validity", "start_pos": 283, "end_pos": 291, "type": "METRIC", "confidence": 0.9507268667221069}]}, {"text": "RST is a descriptive framework that has been widely used in the analysis of discourse organization of written texts), and has been applied to various natural language processing tasks, including language generation, text summarization, and machine translation ().", "labels": [], "entities": [{"text": "RST", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8946121335029602}, {"text": "analysis of discourse organization of written texts", "start_pos": 64, "end_pos": 115, "type": "TASK", "confidence": 0.7554395241396767}, {"text": "language generation", "start_pos": 195, "end_pos": 214, "type": "TASK", "confidence": 0.7229771167039871}, {"text": "text summarization", "start_pos": 216, "end_pos": 234, "type": "TASK", "confidence": 0.7393359839916229}, {"text": "machine translation", "start_pos": 240, "end_pos": 259, "type": "TASK", "confidence": 0.8103041350841522}]}, {"text": "In particular, the availability of RST annotations on a selection of 385 Wall Street Journal articles from the Penn Treebank 1 ) has facilitated RST-based discourse analysis of written texts, since it provides a standard benchmark for comparing the performance of different techniques for document-level discourse parsing (.", "labels": [], "entities": [{"text": "Wall Street Journal articles from the Penn Treebank 1", "start_pos": 73, "end_pos": 126, "type": "DATASET", "confidence": 0.9266427622901069}, {"text": "RST-based discourse analysis of written texts", "start_pos": 145, "end_pos": 190, "type": "TASK", "confidence": 0.922815610965093}, {"text": "document-level discourse parsing", "start_pos": 289, "end_pos": 321, "type": "TASK", "confidence": 0.6624220212300619}]}, {"text": "Another important application of RST closely related to our research is the automated evaluation of discourse in student essays.", "labels": [], "entities": [{"text": "RST", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9743084907531738}]}, {"text": "For example, one study used features for each sentence in an essay to reflect the status of its parent node as well as its rhetorical relation based on automatically parsed RST trees with the goal of providing feedback to students about the discourse structure in the essay (.", "labels": [], "entities": []}, {"text": "Another study compared features derived from deep hierarchical discourse relations based on RST parsing and features derived from shallow discourse relations based on Penn Discourse Treebank (PDTB) ( parsing in the task of essay scoring and demonstrated the effectiveness of deep discourse structure in better differentiation of text coherence . Related work has also been conducted to annotate discourse relations in spoken language, which is produced and processed differently from written texts, and often lacks explicit discourse connectives that are more frequent in written language.", "labels": [], "entities": [{"text": "RST parsing", "start_pos": 92, "end_pos": 103, "type": "TASK", "confidence": 0.9593773186206818}, {"text": "Penn Discourse Treebank (PDTB)", "start_pos": 167, "end_pos": 197, "type": "DATASET", "confidence": 0.9407320419947306}, {"text": "essay scoring", "start_pos": 223, "end_pos": 236, "type": "TASK", "confidence": 0.692945584654808}]}, {"text": "Instead of the rootedtree structure that is employed in RST, the annotation scheme with shallow discourse structure and relations from the PDTB () has been generally used for spoken language, for the annotation of discourse relations in spoken language (.", "labels": [], "entities": [{"text": "RST", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9402172565460205}]}, {"text": "In contrast to these previous studies, this study focuses on monologic spoken responses produced by non-native speakers within the context of a language proficiency assessment.", "labels": [], "entities": []}, {"text": "A discourse annotation scheme based on the RST framework was selected due to the fact that it can effectively demonstrate the deep hierarchical discourse structure across an entire response, rather than focusing on the local coherence of adjacent units.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Human agreement on RST annotations in  terms of \u03ba and F1-Measure.  Span Nuclearity Relation  \u03ba  0.848  0.766  0.653  F1-Measure 0.872  0.724  0.522", "labels": [], "entities": [{"text": "RST annotations", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.9113719165325165}, {"text": "F1-Measure", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9987649917602539}, {"text": "Span Nuclearity Relation  \u03ba  0.848  0.766  0.653  F1-Measure 0.872  0.724  0.522", "start_pos": 77, "end_pos": 157, "type": "METRIC", "confidence": 0.6254645111885938}]}, {"text": " Table 2: The average number of awkward relations  appearing in responses from each of the four pro- ficiency score levels.  1  2  3  4  Annotator 1 3.2 1.1 1.1 0.3  Annotator 2 2.1 1.2 0.7 0.3", "labels": [], "entities": []}, {"text": " Table 3: Pearson correlation coefficients (r) of dis- course features with both the holistic proficiency  scores as well as the discourse coherence scores.  For the 120 double-annotated responses, the aver- aged feature values were used.  Features  Proficiency Coherence  n edu  0.58  0.397  n rel  0.584  0.396  n awk rel  -0.396  -0.509  n rhe rel  0.691  0.541  n rhe relTypes  0.64  0.557  perc rhe rel  0.589  0.612  tree depth  0.365  0.25  ratio nedu depth  0.529  0.367", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.921113520860672}]}]}