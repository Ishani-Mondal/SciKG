{"title": [{"text": "From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood", "labels": [], "entities": []}], "abstractContent": [{"text": "Our goal is to learn a semantic parser that maps natural language utterances into ex-ecutable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself.", "labels": [], "entities": []}, {"text": "Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coinci-dentally output the correct result.", "labels": [], "entities": []}, {"text": "We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present anew learning algorithm that combines the strengths of both.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6440252900123596}, {"text": "maximum marginal likelihood (MML)", "start_pos": 74, "end_pos": 107, "type": "METRIC", "confidence": 0.8015079696973165}]}, {"text": "The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs.", "labels": [], "entities": []}, {"text": "We apply our learning algorithm to anew neural semantic parser and show significant gains over existing state-of-the-art results on a recent context-dependent semantic parsing task.", "labels": [], "entities": [{"text": "neural semantic parser", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7595974008242289}, {"text": "context-dependent semantic parsing task", "start_pos": 141, "end_pos": 180, "type": "TASK", "confidence": 0.7074301913380623}]}], "introductionContent": [{"text": "We are interested in learning a semantic parser that maps natural language utterances into executable programs (e.g., logical forms).", "labels": [], "entities": []}, {"text": "For example, in, a program corresponding to the utterance transforms an initial world state into anew world state.", "labels": [], "entities": []}, {"text": "We would like to learn from indirect supervision, where each training example is only labeled with the correct output (e.g. a target world state), but not the program that produced that out- The task is to map natural language utterances to a program that manipulates the world state.", "labels": [], "entities": []}, {"text": "The correct program captures the true meaning of the utterances, while spurious programs arrive at the correct output for the wrong reasons.", "labels": [], "entities": []}, {"text": "We develop methods to prevent the model from being drawn to spurious programs.", "labels": [], "entities": []}, {"text": "The process of constructing a program can be formulated as a sequential decision-making process, where feedback is only received at the end of the sequence when the completed program is executed.", "labels": [], "entities": []}, {"text": "In the natural language processing literature, there are two common approaches for handling this situation: 1) reinforcement learning (RL), particularly the REINFORCE algorithm), which maximizes the expected reward of a sequence of actions; and 2) maximum marginal likelihood (MML), which treats the sequence of actions as a latent variable, and then maximizes the marginal likelihood of observing the correct program output.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9657405614852905}, {"text": "maximum marginal likelihood (MML)", "start_pos": 248, "end_pos": 281, "type": "METRIC", "confidence": 0.8288446962833405}]}, {"text": "While the two approaches have enjoyed success on many tasks, we found them to work poorly out of the box for our task.", "labels": [], "entities": []}, {"text": "This is because in addition to the sparsity of correct programs, our task also requires weeding out spurious programs: incorrect interpretations of the utterances that accidentally produce the correct output, as illustrated in.", "labels": [], "entities": []}, {"text": "We show that MML and RL optimize closely related objectives.", "labels": [], "entities": []}, {"text": "Furthermore, both MML and RL methods have a mechanism for exploring program space in search of programs that generate the correct output.", "labels": [], "entities": []}, {"text": "We explain why this exploration tends to quickly concentrate around short spurious programs, causing the model to sometimes overlook the correct program.", "labels": [], "entities": []}, {"text": "To address this problem, we propose RANDOMER, anew learning algorithm with two parts: First, we propose randomized beam search, an exploration strategy which combines the systematic beam search traditionally employed in MML with the randomized off-policy exploration of RL.", "labels": [], "entities": [{"text": "RANDOMER", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.7740192413330078}]}, {"text": "This increases the chance of finding correct programs even when the beam size is small or the parameters are not pre-trained.", "labels": [], "entities": []}, {"text": "Second, we observe that even with good exploration, the gradients of both the RL and MML objectives may still upweight entrenched spurious programs more strongly than correct programs with low probability under the current model.", "labels": [], "entities": []}, {"text": "We propose a meritocratic parameter update rule, a modification to the MML gradient update, which more equally upweights all programs that produce the correct output.", "labels": [], "entities": []}, {"text": "This makes the model less likely to overfit spurious programs.", "labels": [], "entities": []}, {"text": "We apply RANDOMER to train anew neural semantic parser, which outputs programs in a stackbased programming language.", "labels": [], "entities": [{"text": "RANDOMER", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.6175459027290344}]}, {"text": "We evaluate our resulting system on SCONE, the context-dependent semantic parsing dataset of.", "labels": [], "entities": [{"text": "context-dependent semantic parsing", "start_pos": 47, "end_pos": 81, "type": "TASK", "confidence": 0.649882177511851}]}, {"text": "Our approach outperforms standard RL and MML methods in a direct comparison, and achieves new state-of-the-art results, improving over in all three domains of SCONE, and by over 30% accuracy on the most challenging one.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9991590976715088}]}], "datasetContent": [{"text": "We evaluate our proposed methods on all three domains of the SCONE dataset.", "labels": [], "entities": [{"text": "SCONE dataset", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.860670655965805}]}, {"text": "Accuracy is defined as the percentage of test examples where the model produces the correct final world state w M . All test examples have M = 5 (5utts), but we also report accuracy after processing the first 3 utterances (3utts).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.994959831237793}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9967401623725891}]}, {"text": "To control for the effects of randomness, we train 5 instances of each model with different random seeds.", "labels": [], "entities": []}, {"text": "We report the median accuracy of the instances unless otherwise noted..", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9224404692649841}]}, {"text": "Model parameters are randomly initialized, with no pre-training.", "labels": [], "entities": []}, {"text": "We use the Adam optimizer ( (which is applied to the gradient in), a learning rate of 0.001, a minibatch size of 8 examples (different from the beam size), and train until accuracy on the validation set converges (on average about 13,000 steps).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.998746395111084}]}, {"text": "We: RANDOMER combines qualities of both REINFORCE (RL) and BS-MML.", "labels": [], "entities": [{"text": "RANDOMER", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.8119449615478516}, {"text": "REINFORCE (RL)", "start_pos": 40, "end_pos": 54, "type": "METRIC", "confidence": 0.9144890755414963}, {"text": "BS-MML", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.8187608122825623}]}, {"text": "For approximating the expectation over q in the gradient, we use numerical integration as in BS-MML.", "labels": [], "entities": [{"text": "BS-MML", "start_pos": 93, "end_pos": 99, "type": "DATASET", "confidence": 0.9297579526901245}]}, {"text": "Our exploration strategy is a hybrid of search (MML) and off-policy sampling (RL).", "labels": [], "entities": []}, {"text": "Our gradient weighting is equivalent to MML when \u03b2 = 1 and more \"meritocratic\" than both MML and REINFORCE for lower values of \u03b2. use fixed GloVe vectors ( to embed the words in each utterance.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9865082502365112}]}, {"text": "For all models, we performed a grid search over hyperparameters to maximize accuracy on the validation set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.999160885810852}]}, {"text": "Hyperparameters include the learning rate, the baseline in REINFORCE, -greediness and \u03b2-meritocraticness.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9679164886474609}]}, {"text": "For REINFORCE, we also experimented with a regression-estimated baseline (), but found it to perform worse than a constant baseline.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.8282005190849304}]}], "tableCaptions": [{"text": " Table 1: RANDOMER combines qualities of both REINFORCE (RL) and BS-MML. For approximating  the expectation over q in the gradient, we use numerical integration as in BS-MML. Our exploration  strategy is a hybrid of search (MML) and off-policy sampling (RL). Our gradient weighting is equivalent  to MML when \u03b2 = 1 and more \"meritocratic\" than both MML and REINFORCE for lower values of \u03b2.", "labels": [], "entities": []}, {"text": " Table 2: Comparison to prior work. LONG+16  results are directly from Long et al. (2016). Hy- perparameters are chosen by best performance on  validation set (see Appendix A).", "labels": [], "entities": [{"text": "LONG+16", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9118387500445048}]}, {"text": " Table 3: Randomized beam search. All listed  models use gradient weight q MML and TOKENS to  represent execution history.", "labels": [], "entities": [{"text": "Randomized beam search", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6003749271233877}, {"text": "TOKENS", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9249883890151978}]}, {"text": " Table 4: \u03b2-meritocratic updates. All listed  models use randomized beam search, = 0.15  and TOKENS to represent execution history.", "labels": [], "entities": [{"text": "TOKENS", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9893817901611328}]}, {"text": " Table 5: TOKENS vs STACK embedding. Both  models use = 0.15 and gradient weight q MML .", "labels": [], "entities": [{"text": "TOKENS", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8154554963111877}]}, {"text": " Table 6: Top-scoring predictions for a training ex- ample from SCENE (* = correct, o = spurious, x  = incorrect). RANDOMER distributes probabil- ity mass over numerous reward-earning programs  (including the correct ones), while classic beam  search MML overfits to one spurious program,  giving it very high probability.", "labels": [], "entities": [{"text": "RANDOMER", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.4615989029407501}]}]}