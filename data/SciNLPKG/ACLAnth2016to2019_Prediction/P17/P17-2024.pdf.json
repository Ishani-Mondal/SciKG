{"title": [{"text": "Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization", "labels": [], "entities": [{"text": "Grouped Weight Sharing", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.6311864058176676}]}], "abstractContent": [{"text": "A fundamental advantage of neural models for NLP is their ability to learn representations from scratch.", "labels": [], "entities": []}, {"text": "However, in practice this often means ignoring existing external linguistic resources, e.g., Word-Net or domain specific ontologies such as the Unified Medical Language System (UMLS).", "labels": [], "entities": [{"text": "Word-Net", "start_pos": 93, "end_pos": 101, "type": "DATASET", "confidence": 0.9371549487113953}]}, {"text": "We propose a general, novel method for exploiting such resources via weight sharing.", "labels": [], "entities": []}, {"text": "Prior work on weight sharing in neural networks has considered it largely as a means of model compression.", "labels": [], "entities": [{"text": "weight sharing in neural networks", "start_pos": 14, "end_pos": 47, "type": "TASK", "confidence": 0.8094803810119628}, {"text": "model compression", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7638112902641296}]}, {"text": "In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models.", "labels": [], "entities": [{"text": "weight sharing", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7089358568191528}]}, {"text": "We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural models are powerful in part due to their ability to learn good representations of raw textual inputs, mitigating the need for extensive task-specific feature engineering).", "labels": [], "entities": []}, {"text": "However, a downside of learning from scratch is failing to capitalize on prior linguistic or semantic knowledge, often encoded in existing resources such as ontologies.", "labels": [], "entities": []}, {"text": "Such prior knowledge can be particularly valuable when estimating highly flexible models.", "labels": [], "entities": []}, {"text": "In this work, we address how to exploit known relationships between words when training neural models for NLP tasks.", "labels": [], "entities": []}, {"text": "We propose exploiting the feature-hashing trick, originally proposed as a means of neural network compression: An example of grouped partial weight sharing.", "labels": [], "entities": [{"text": "neural network compression", "start_pos": 83, "end_pos": 109, "type": "TASK", "confidence": 0.7902828256289164}, {"text": "grouped partial weight sharing", "start_pos": 125, "end_pos": 155, "type": "TASK", "confidence": 0.5718153193593025}]}, {"text": "Here there are two groups.", "labels": [], "entities": []}, {"text": "We stochastically select embedding weights to be shared between words belonging to the same group(s).", "labels": [], "entities": []}, {"text": "to be similar a priori.", "labels": [], "entities": []}, {"text": "In effect, this acts as a regularizer that constrains the model to learn weights that agree with the domain knowledge codified in external resources like ontologies.", "labels": [], "entities": []}, {"text": "More specifically, as external resources we use Brown clusters, WordNet and the Unified Medical Language System (UMLS).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9473425149917603}]}, {"text": "From these we derive groups of words with similar meaning.", "labels": [], "entities": []}, {"text": "We then use feature hashing to share a subset of weights between the embeddings of words that belong to the same semantic group(s).", "labels": [], "entities": []}, {"text": "This forces the model to respect prior domain knowledge, insofar as words similar under a given ontology are compelled to have similar embeddings.", "labels": [], "entities": []}, {"text": "Our contribution is a novel, simple and flexible method for injecting domain knowledge into neural models via stochastic weight sharing.", "labels": [], "entities": []}, {"text": "Results on seven diverse classification tasks (three sentiment and four biomedical) show that our method consistently improves performance over (1) baselines that fail to capitalize on domain knowledge, and (2) an approach that uses retrofitting) as a preprocessing step to encode domain knowledge prior to training.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use three sentiment datasets: a movie review (MR) dataset (Pang and ; a customer review (CR) dataset (Hu and Liu, 2004) 4 ; and an opinion dataset (MPQA) () . We also use four biomedical datasets, which concern systematic reviews.", "labels": [], "entities": []}, {"text": "The task here is to classify published articles describing clinical trials as relevant or not to a well-specified clinical question.", "labels": [], "entities": []}, {"text": "Articles deemed relevant are included in the corresponding review, which is a synthesis of all pertinent evidence ().", "labels": [], "entities": []}, {"text": "We use data from reviews that concerned: clopidogrel (CL) for cardiovascular conditions; biomarkers for assessing iron deficiency in anemia (AN) experienced by patients with kidney disease (Chung et al., 2012); statins (ST) (); and proton beam (PB) therapy (Terasawa et al., 2009).", "labels": [], "entities": [{"text": "iron deficiency in anemia (AN)", "start_pos": 114, "end_pos": 144, "type": "METRIC", "confidence": 0.7768753213541848}]}], "tableCaptions": []}