{"title": [{"text": "Learning word-like units from joint audio-visual analysis", "labels": [], "entities": []}], "abstractContent": [{"text": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions.", "labels": [], "entities": []}, {"text": "For example, our model is able to detect spoken instances of the words \"lighthouse\" within an utterance and associate them with image regions containing lighthouses.", "labels": [], "entities": []}, {"text": "We do not use any form of conventional automatic speech recognition , nor do we use any text transcriptions or conventional linguistic annotations.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.7672093709309896}]}, {"text": "Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "labels": [], "entities": [{"text": "spoken language acquisition", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.7457553942998251}]}], "introductionContent": [], "datasetContent": [{"text": "We employ a corpus of over 200,000 spoken captions for images taken from the Places205 dataset (), corresponding to over 522 hours of speech data.", "labels": [], "entities": [{"text": "Places205 dataset", "start_pos": 77, "end_pos": 94, "type": "DATASET", "confidence": 0.9943395853042603}]}, {"text": "The captions were collected using Amazon's Mechanical Turk service, in which workers were shown images and asked to describe them verbally in a free-form manner.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk service", "start_pos": 34, "end_pos": 66, "type": "DATASET", "confidence": 0.85048006772995}]}, {"text": "The data collection scheme is described in detail in, but the experiments in this paper leverage nearly twice the amount of data.", "labels": [], "entities": [{"text": "data collection", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7092635929584503}]}, {"text": "For training our multimodal neural network as well as the pattern discovery experiments, we use a subset of 214,585 image/caption pairs, and we holdout a set of 1,000 pairs for evaluating the multimodal network's retrieval ability.", "labels": [], "entities": [{"text": "pattern discovery", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7960373759269714}]}, {"text": "Because we lack ground truth text transcripts for the data, we used Google's Speech Recognition public API to generate proxy transcripts which we use when analyzing our system.", "labels": [], "entities": []}, {"text": "Note that the ASR was only used for analysis of the results, and was not involved in any of the learning.", "labels": [], "entities": [{"text": "ASR", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.6314548850059509}]}, {"text": "1. Word embedding layer of dimension 200 2.", "labels": [], "entities": []}, {"text": "Temporal Convolution: C=512, W=3, ReLU 3.", "labels": [], "entities": [{"text": "ReLU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9900572896003723}]}, {"text": "Temporal Convolution: C=1024, W=3 4.", "labels": [], "entities": [{"text": "Temporal", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9182969927787781}]}, {"text": "Meanpool over entire caption 5.", "labels": [], "entities": [{"text": "Meanpool", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.967093288898468}]}, {"text": "L2 normalization One would expect that access to ASR hypotheses should improve the recall scores, but the performance gap is not enormous.", "labels": [], "entities": [{"text": "L2 normalization", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6478673368692398}, {"text": "ASR hypotheses", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.8956137299537659}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9992188215255737}]}, {"text": "Access to the ASR hypotheses provides a relative improvement of approximately 21.8% for image search R@10 and 12.5% for annotation R@10 compared to using no transcriptions or ASR whatsoever.", "labels": [], "entities": [{"text": "ASR", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9844824075698853}]}, {"text": "We performed the grounding and pattern clustering stepson the entire training dataset, which resulted in a total of 1,161,305 unique grounding pairs.", "labels": [], "entities": []}, {"text": "For evaluation, we wish to assign a label to each cluster and cluster member, but this is not completely straightforward since each acoustic segment may capture part of a word, a whole word, multiple words, etc.", "labels": [], "entities": []}, {"text": "Our strategy is to forcealign the Google recognition hypothesis text to the audio, and then assign a label string to each acoustic segment based upon which words it overlaps in time.", "labels": [], "entities": [{"text": "forcealign the Google recognition hypothesis text", "start_pos": 19, "end_pos": 68, "type": "TASK", "confidence": 0.6750979522864023}]}, {"text": "The alignments are created with the help of a Kaldi (Povey et al., 2011) speech recognizer  based on the standard WSJ recipe and trained using the Google ASR hypothesis as a proxy for the transcriptions.", "labels": [], "entities": [{"text": "WSJ recipe", "start_pos": 114, "end_pos": 124, "type": "DATASET", "confidence": 0.9118559956550598}]}, {"text": "Any word whose duration is overlapped 30% or more by the acoustic segment is included in the label string for the segment.", "labels": [], "entities": []}, {"text": "We then employ a majority vote scheme to derive the overall cluster labels.", "labels": [], "entities": []}, {"text": "When computing the purity of a cluster, we count a cluster member as matching the cluster label as long as the overall cluster label appears in the member's label string.", "labels": [], "entities": []}, {"text": "In other words, an acoustic segment overlapping the words \"the lighthouse\" would receive credit for matching the overall cluster label \"lighthouse\".", "labels": [], "entities": []}, {"text": "A breakdown of the segments captured by two clusters is shown in.", "labels": [], "entities": []}, {"text": "We investigated some simple schemes for predicting highly pure clusters, and found that the empirical variance of the cluster members (average squared distance to the cluster centroid) was a good indicator.", "labels": [], "entities": []}, {"text": "displays a scatter plot of cluster purity weighted by the natural log of the cluster size against the empirical variance.", "labels": [], "entities": []}, {"text": "Large, pure clusters are easily predicted by their low empirical variance, while a high variance is indicative of a garbage cluster.", "labels": [], "entities": []}, {"text": "Ranking a set of k = 500 acoustic clusters by their variance, displays some statistics for the 50 lowest-variance clusters.", "labels": [], "entities": []}, {"text": "We see that most of the clusters are very large and highly pure, and their labels reflect interesting object categories being identified by the neural network.", "labels": [], "entities": []}, {"text": "We additionally compute the coverage of each cluster by counting the total number of instances of the clus-  ter label anywhere in the training data, and then compute what fraction of those instances were captured by the cluster.", "labels": [], "entities": []}, {"text": "There are many examples of high coverage clusters, e.g. the \"skyscraper\" cluster captures 84% of all occurrences of the word \"skyscraper\", while the \"baseball\" cluster captures 86% of all occurrences of the word \"baseball\".", "labels": [], "entities": []}, {"text": "This is quite impressive given the fact that no conventional speech recognition was employed, and neither the multimodal neural network nor the grounding algorithm had access to the text transcripts of the captions.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7203470915555954}]}, {"text": "To get an idea of the impact of the k parameter as well as a variance-based cluster pruning threshold based on, we swept k from 250 to 2000 and computed a set of statistics shown in.", "labels": [], "entities": []}, {"text": "We compute the standard overall cluster purity evaluation metric in addition to the average coverage across clusters.", "labels": [], "entities": []}, {"text": "The table shows the natural tradeoff between cluster purity and redundancy (indicated by the average cluster coverage) ask is increased.", "labels": [], "entities": []}, {"text": "In all cases, the variance-based cluster pruning greatly increases both the overall purity and average cluster coverage metrics.", "labels": [], "entities": [{"text": "purity", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9616730809211731}]}, {"text": "We also notice that more unique cluster labels are discovered with a larger k.", "labels": [], "entities": []}, {"text": "Next, we examine the image clusters.", "labels": [], "entities": []}, {"text": "displays the 9 most central image crops fora set of 10 different image clusters, along with the majority-vote label of each image cluster's associated audio cluster.", "labels": [], "entities": []}, {"text": "In all cases, we see that the image crops are highly relevant to their audio cluster label.", "labels": [], "entities": []}, {"text": "We include many more example image clusters in Appendix A. In order to examine the semantic embedding space in more depth, we took the top 150 clusters from the same k = 500 clustering run described in and performed t-SNE) analysis on the cluster centroid vectors.", "labels": [], "entities": []}, {"text": "We projected each centroid down to 2 di- mensions and plotted their majority-vote labels in.", "labels": [], "entities": []}, {"text": "Immediately we see that different clusters which capture the same label closely neighbor one another, indicating that distances in the embedding space do indeed carry information discriminative across word types (and suggesting that a more sophisticated clustering algorithm than kmeans would perform better).", "labels": [], "entities": []}, {"text": "More interestingly, we see that semantic information is also reflected in these distances.", "labels": [], "entities": []}, {"text": "The cluster centroids for \"lake,\" \"river,\" \"body,\" \"water,\" \"waterfall,\" \"pond,\" and \"pool\" all form a tight meta-cluster, as do \"restaurant,\" \"store,\" \"shop,\" and \"shelves,\" as well as \"children,\" \"girl,\" \"woman,\" and \"man.\"", "labels": [], "entities": []}, {"text": "Many other semantic meta-clusters can be seen in, suggesting that the embedding space is capturing information that is highly discriminative both acoustically and semantically.", "labels": [], "entities": []}, {"text": "Because our experiments revolve around the discovery of word and object categories, a key question to address is the extent to which the supervision used to train the VGG network constrains or influences the kinds of objects learned.", "labels": [], "entities": []}, {"text": "Because the 1,000 object classes from the ILSVRC2012 task ( used to train the VGG network were derived from WordNet synsets, we can measure the semantic similarity between the words learned by our network and the ILSVRC2012 class labels by using synset similarity measures within WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 280, "end_pos": 287, "type": "DATASET", "confidence": 0.9637706875801086}]}, {"text": "We do this by first building a list of the 1,000 WordNet synsets associated with the ILSVRC2012 classes.", "labels": [], "entities": [{"text": "ILSVRC2012 classes", "start_pos": 85, "end_pos": 103, "type": "DATASET", "confidence": 0.8885606527328491}]}, {"text": "We then take the set of unique majority-vote labels associated with the discovered word clusters fork = 500, filtered by setting a threshold on their variance (\u03c3 2 \u2264 0.65) so as to get rid of garbage clusters, leaving us with 197 unique acoustic cluster labels.", "labels": [], "entities": []}, {"text": "We then lookup each cluster label in WordNet, and compare all noun senses of the label to every ILSVRC2012 class synset according to the path similarity measure.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.9657992720603943}]}, {"text": "This measure describes the distance between two synsets in a hyponym/hypernym hierarchy, where a score of 1 represents identity and lower scores indicate less similarity.", "labels": [], "entities": []}, {"text": "We retain the highest score between any sense of the cluster label and any ILSVRC2012 synset.", "labels": [], "entities": []}, {"text": "Of the 197 unique cluster labels, only 16 had a distance of 1 from any ILSVRC12 class, which would indicate an exact match.", "labels": [], "entities": [{"text": "ILSVRC12 class", "start_pos": 71, "end_pos": 85, "type": "DATASET", "confidence": 0.8695631921291351}]}, {"text": "A path similarity of 0.5 indicates one degree of separation in the hyponym/hypernym hierarchy -for example, the similarity between \"desk\" and \"table\" is 0.5. 47 cluster labels were found to have a similarity of 0.5 to some ILSVRC12 class, leaving 134 cluster labels whose highest similarity to any ILSVRC12 class was less than 0.5.", "labels": [], "entities": []}, {"text": "In other words, more than two thirds of the highly pure pattern clusters learned by our network were dissimilar to all of the 1,000 ILSVRC12 classes used to pretrain the VGG network, indicating that our model is able to generalize far beyond the set of classes found in the ILSVRC12 data.", "labels": [], "entities": [{"text": "ILSVRC12 data", "start_pos": 274, "end_pos": 287, "type": "DATASET", "confidence": 0.7730322480201721}]}, {"text": "We display the labels of the 40 lowest variance acoustic clusters labels along with the name and similarity score of their closest ILSVRC12 synset in", "labels": [], "entities": [{"text": "similarity", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9150760173797607}]}], "tableCaptions": [{"text": " Table 1: Results for image search and annotation  on the Places audio caption data (214k training  pairs, 1k testing pairs). Recall is shown for the  top 1, 5, and 10 hits. The model we use in this  paper is compared against the meanpool variant of  the model architecture presented in Harwath et al.  (2016). For both training and testing, the captions  were truncated/zero-padded to 10 seconds.", "labels": [], "entities": [{"text": "image search", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.8156474530696869}, {"text": "Places audio caption data", "start_pos": 58, "end_pos": 83, "type": "DATASET", "confidence": 0.6258670017123222}, {"text": "Recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9980701804161072}]}, {"text": " Table 3: Top 50 clusters with k = 500 sorted by increasing variance. Legend: |C c | is acoustic cluster  size, |C i | is associated image cluster size, Pur. is acoustic cluster purity, \u03c3 2 is acoustic cluster variance,  and Cov. is acoustic cluster coverage. A dash (-) indicates a cluster whose majority label is silence.", "labels": [], "entities": []}, {"text": " Table 4: Clustering statistics of the acoustic clusters for various values of k and different settings of the  variance-based cluster pruning threshold. Legend: |C| = number of clusters remaining after pruning, |X |  = number of datapoints after pruning, Pur = purity, |L| = number of unique cluster labels, AC = average  cluster coverage", "labels": [], "entities": []}, {"text": " Table 4. We compute the standard overall clus- ter purity evaluation metric in addition to the aver- age coverage across clusters. The table shows the  natural tradeoff between cluster purity and redun-", "labels": [], "entities": [{"text": "clus- ter purity evaluation metric", "start_pos": 42, "end_pos": 76, "type": "METRIC", "confidence": 0.7113644381364187}, {"text": "aver- age coverage", "start_pos": 96, "end_pos": 114, "type": "METRIC", "confidence": 0.8169663399457932}]}, {"text": " Table 5: The 40 lowest variance, uniquely-labeled  acoustic clusters paired with their most similar  ILSVRC2012 synset.", "labels": [], "entities": []}]}