{"title": [{"text": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "labels": [], "entities": [{"text": "Reading Comprehension", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.6942800581455231}]}], "abstractContent": [{"text": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples.", "labels": [], "entities": []}, {"text": "TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions.", "labels": [], "entities": [{"text": "TriviaQA", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8468167185783386}]}, {"text": "We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences , and (3) requires more cross sentence reasoning to find answers.", "labels": [], "entities": []}, {"text": "We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neu-ral network, that performs well on SQuAD reading comprehension.", "labels": [], "entities": [{"text": "SQuAD reading comprehension", "start_pos": 130, "end_pos": 157, "type": "TASK", "confidence": 0.7739999890327454}]}, {"text": "Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that Trivi-aQA is a challenging testbed that is worth significant future study.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reading comprehension (RC) systems aim to answer any question that could be posed against the facts in some reference text.", "labels": [], "entities": [{"text": "Reading comprehension (RC)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6315784990787506}]}, {"text": "This goal is challenging fora number of reasons: (1) the questions can be complex (e.g. have highly compositional semantics), (2) finding the correct answer can require complex reasoning (e.g. combining facts from multiple sentences or background knowledge) and (3) individual facts can be difficult to Question: The Dodecanese Campaign of WWII that was an attempt by the Allied forces to capture islands in the Aegean Sea was the inspiration for which acclaimed 1961 commando film?", "labels": [], "entities": []}, {"text": "Answer: The Guns of Navarone Excerpt: The Dodecanese Campaign of World War II was an attempt by Allied forces to capture the Italianheld Dodecanese islands in the Aegean Sea following the surrender of Italy in September 1943, and use them as bases against the German-controlled Balkans.", "labels": [], "entities": []}, {"text": "The failed campaign, and in particular the Battle of Leros, inspired the 1957 novel The Guns of Navarone and the successful 1961 movie of the same name.", "labels": [], "entities": [{"text": "Navarone", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.7963476777076721}]}, {"text": "Question: American Callan Pinckney's eponymously named system became a best-selling book/video franchise in what genre?", "labels": [], "entities": [{"text": "American Callan Pinckney", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.8604579965273539}]}, {"text": "Answer: Fitness Excerpt: Callan Pinckney was an American fitness professional.", "labels": [], "entities": []}, {"text": "She achieved unprecedented success with her Callanetics exercises.", "labels": [], "entities": []}, {"text": "Her 9 books all became international best-sellers and the video series that followed went onto sell over 6 million copies.", "labels": [], "entities": []}, {"text": "Pinckney's first video release \"Callanetics: 10 Years Younger In 10 Hours\" outsold every other fitness video in the US.: Question-answer pairs with sample excerpts from evidence documents from TriviaQA exhibiting lexical and syntactic variability, and requiring reasoning from multiple sentences.", "labels": [], "entities": []}, {"text": "recover from text (e.g. due to lexical and syntactic variation).", "labels": [], "entities": []}, {"text": "shows examples of all these phenomena.", "labels": [], "entities": []}, {"text": "This paper presents TriviaQA, anew reading comprehension dataset designed to simultaneously test all of these challenges.", "labels": [], "entities": []}, {"text": "Recently, significant progress has been made by introducing large new reading comprehension datasets that primarily focus on one of the challenges listed above, for example by crowdsourcing the gathering of question answer pairs) or using cloze-style sentences instead of questions () (see for more examples).", "labels": [], "entities": []}, {"text": "In general, system performance has improved rapidly as each resource is released.", "labels": [], "entities": []}, {"text": "The best models of-: Comparison of TriviaQA with existing QA datasets.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.8287569284439087}]}, {"text": "Our dataset is unique in that it is naturally occurring, well-formed questions collected independent of the evidences.", "labels": [], "entities": []}, {"text": "*NewsQA uses evidence articles indirectly by using only article summaries.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 1, "end_pos": 7, "type": "DATASET", "confidence": 0.9699892997741699}]}, {"text": "ten achieve near-human performance levels within months or a year, fueling a continual need to build evermore difficult datasets.", "labels": [], "entities": []}, {"text": "We argue that TriviaQA is such a dataset, by demonstrating that a high percentage of its questions require solving these challenges and showing that there is a large gap between state-of-the-art methods and human performance levels.", "labels": [], "entities": []}, {"text": "TriviaQA contains over 650K question-answerevidence triples, that are derived by combining 95K Trivia enthusiast authored question-answer pairs with on average six supporting evidence documents per question.", "labels": [], "entities": [{"text": "TriviaQA", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.898236870765686}]}, {"text": "To our knowledge, TriviaQA is the first dataset where full-sentence questions are authored organically (i.e. independently of an NLP task) and evidence documents are collected retrospectively from Wikipedia and the Web.", "labels": [], "entities": []}, {"text": "This decoupling of question generation from evidence collection allows us to control for potential bias in question style or content, while offering organically generated questions from various topics.", "labels": [], "entities": []}, {"text": "Designed to engage humans, TriviaQA presents anew challenge for RC models.", "labels": [], "entities": []}, {"text": "They should be able to deal with large amount of text from various sources such as news articles, encyclopedic entries and blog articles, and should handle inference over multiple sentences.", "labels": [], "entities": []}, {"text": "For example, our dataset contains three times as many questions that require inference over multiple sentences than the recently released SQuAD ( dataset.", "labels": [], "entities": []}, {"text": "Section 4 present a more detailed discussion of these challenges.", "labels": [], "entities": []}, {"text": "Finally, we present baseline experiments on the TriviaQA dataset, including a linear classifier inspired by work on) and a state-of-the-art neural network baseline ().", "labels": [], "entities": [{"text": "TriviaQA dataset", "start_pos": 48, "end_pos": 64, "type": "DATASET", "confidence": 0.9535705745220184}]}, {"text": "The neural model performs best, but only achieves 40% for TriviaQA in comparison to 68% on SQuAD, perhaps due to the challenges listed above.", "labels": [], "entities": []}, {"text": "The baseline results also fall far short of human performance levels, 79.7%, suggesting significant room for the future work.", "labels": [], "entities": []}, {"text": "In summary, we make the following contributions.", "labels": [], "entities": []}, {"text": "\u2022 We collect over 650K question-answerevidence triples, with questions originating from trivia enthusiasts independent of the evidence documents.", "labels": [], "entities": []}, {"text": "A high percentage of the questions are challenging, with substantial syntactic and lexical variability and often requiring multi-sentence reasoning.", "labels": [], "entities": []}, {"text": "The dataset and code are available at http://nlp.cs.washington.", "labels": [], "entities": []}, {"text": "edu/triviaqa/, offering resources for training new reading-comprehension models.", "labels": [], "entities": []}, {"text": "\u2022 We present a manual analysis quantifying the quality of the dataset and the challenges involved in solving the task.", "labels": [], "entities": []}, {"text": "\u2022 We present experiments with two baseline methods, demonstrating that the TriviaQA tasks are not easily solved and are worthy of future study.", "labels": [], "entities": []}, {"text": "\u2022 In addition to the automatically gathered large-scale (but noisy) dataset, we present a clean, human-annotated subset of 1975 question-document-answer triples whose documents are certified to contain all facts required to answer the questions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collected a large dataset to support the reading comprehension task described above.", "labels": [], "entities": []}, {"text": "First we gathered question-answer pairs from 14 trivia and quiz-league websites.", "labels": [], "entities": []}, {"text": "We removed questions with less than four tokens, since these were generally either too simple or too vague.", "labels": [], "entities": []}, {"text": "We then collected textual evidence to answer questions using two sources: documents from Web search results and Wikipedia articles for entities in the question.", "labels": [], "entities": []}, {"text": "To collect the former, we posed each question 4 as a search query to the Bing Web search API, and collected the top 50 search result URLs.", "labels": [], "entities": []}, {"text": "To exclude the trivia websites, we removed from the results all pages from the trivia websites we scraped and any page whose url included the keywords trivia, question, or answer.", "labels": [], "entities": []}, {"text": "We then crawled the top 10 search result Web pages and pruned PDF and other ill formatted documents.", "labels": [], "entities": []}, {"text": "The search output includes a diverse set of documents such as blog articles, news articles, and encyclopedic entries.", "labels": [], "entities": []}, {"text": "Wikipedia pages for entities mentioned in the question often provide useful information.", "labels": [], "entities": []}, {"text": "We therefore collected an additional set of evidence documents by applying TAGME, an off-the-shelf entity linker, to find Wikipedia entities mentioned in the question, and added the corresponding pages as evidence documents.", "labels": [], "entities": [{"text": "TAGME", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.9507219791412354}]}, {"text": "Finally, to support learning from distant supervision, we further filtered the evidence documents to exclude those missing the correct answer string and formed evidence document sets as described in Section 2.", "labels": [], "entities": []}, {"text": "This left us with 95K questionanswer pairs organized into.", "labels": [], "entities": []}, {"text": "On comparing evidence sentences with their corresponding questions, we found that 69% of the questions had a different syntactic structure while 41% were lexically different.", "labels": [], "entities": []}, {"text": "For 40% of the questions, we found that the information re-quired to answer them was scattered over multiple sentences.", "labels": [], "entities": []}, {"text": "Compared to SQuAD, over three times as many questions in TriviaQA require reasoning over multiple sentences.", "labels": [], "entities": []}, {"text": "Moreover, 17% of the examples required some form of world knowledge.", "labels": [], "entities": []}, {"text": "Question-evidence pairs in TriviaQA display more lexical and syntactic variance than SQuAD.", "labels": [], "entities": []}, {"text": "This supports our earlier assertion that decoupling question generation from evidence collection results in a more challenging problem.", "labels": [], "entities": [{"text": "decoupling question generation", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.626832902431488}, {"text": "evidence collection", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.6744614392518997}]}, {"text": "An evaluation of our baselines shows that both of our tasks are challenging, and that the TriviaQA dataset supports significant future work.: Data statistics for each task setup.", "labels": [], "entities": [{"text": "TriviaQA dataset", "start_pos": 90, "end_pos": 106, "type": "DATASET", "confidence": 0.9642026722431183}]}, {"text": "The Wikipedia domain is evaluated over questions while the web domain is evaluated over documents.", "labels": [], "entities": []}, {"text": "We use the same evaluation metrics as SQuADexact match (EM) and F1 over words in the answer(s).", "labels": [], "entities": [{"text": "SQuADexact match (EM)", "start_pos": 38, "end_pos": 59, "type": "METRIC", "confidence": 0.7693312585353851}, {"text": "F1", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9958289265632629}]}, {"text": "For questions that have Numerical and FreeForm answers, we use a single given answer as ground truth.", "labels": [], "entities": []}, {"text": "For questions that have Wikipedia entities as answers, we use Wikipedia aliases as valid answer along with the given answer.", "labels": [], "entities": []}, {"text": "Since Wikipedia and the web are vastly different in terms of style and content, we report performance on each source separately.", "labels": [], "entities": []}, {"text": "While using Wikipedia, we evaluate at the question level since facts needed to answer a question are generally stated only once.", "labels": [], "entities": []}, {"text": "On the other hand, due to high information redundancy in web documents (around 6 documents per question), we report document level accuracy and F1 when evaluating on web documents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9261323809623718}, {"text": "F1", "start_pos": 144, "end_pos": 146, "type": "METRIC", "confidence": 0.9997013211250305}]}, {"text": "Lastly, in addition to distant supervision, we also report evaluation on the clean dev and test questions collection using a human annotator (section 4)  We randomly partition QA pairs in the dataset into train (80%), development (10%), and test set (10%).", "labels": [], "entities": []}, {"text": "In addition to distant supervision evaluation, we also evaluate baselines on verified subsets (see section 4) of the dev and test partitions.", "labels": [], "entities": []}, {"text": "contains the number of questions and documents for each task.", "labels": [], "entities": []}, {"text": "We trained the entity classifier on a random sample of 50,000 questions from the training set.", "labels": [], "entities": []}, {"text": "For training BiDAF on the web domain, we first randomly sampled 80,000 documents.", "labels": [], "entities": []}, {"text": "For both domains, we used only those (training) documents where the answer appears in the first 400 tokens to keep training time manageable.", "labels": [], "entities": []}, {"text": "Designing scalable techniques that can use the entirety of the data is an interesting direction for future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Properties of questions on 200 annotated examples show that a majority of TriviaQA questions  contain multiple entities. The boldfaced words hint at the presence of corresponding property.", "labels": [], "entities": []}, {"text": " Table 4: Distribution of answer types on 200 an- notated examples.", "labels": [], "entities": []}, {"text": " Table 6: Data statistics for each task setup. The  Wikipedia domain is evaluated over questions  while the web domain is evaluated over docu- ments.", "labels": [], "entities": [{"text": "Wikipedia domain", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.9343768954277039}]}, {"text": " Table 7: Performance of all systems on TriviaQA using distantly supervised evaluation. The best per- forming system is indicated in bold.", "labels": [], "entities": [{"text": "TriviaQA", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.8865314722061157}]}, {"text": " Table 8: Qualitative error analysis of BiDAF on  Wikipedia evidence documents.", "labels": [], "entities": [{"text": "Qualitative error analysis of BiDAF", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.5843436479568481}]}]}