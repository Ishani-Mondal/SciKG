{"title": [{"text": "Adversarial Multi-Criteria Learning for Chinese Word Segmentation", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.529502789179484}]}], "abstractContent": [{"text": "Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS).", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.7683781882127126}]}, {"text": "Most existing methods focus on improve the performance for each single criterion.", "labels": [], "entities": []}, {"text": "However , it is interesting to exploit these different criteria and mining their common underlying knowledge.", "labels": [], "entities": []}, {"text": "In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria.", "labels": [], "entities": []}, {"text": "Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning.", "labels": [], "entities": []}, {"text": "Source codes of this paper are available on Github 1 .", "labels": [], "entities": [{"text": "Github 1", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.9520242512226105}]}], "introductionContent": [{"text": "Chinese word segmentation (CWS) is a preliminary and important task for Chinese natural language processing (NLP).", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7641580800215403}, {"text": "Chinese natural language processing (NLP)", "start_pos": 72, "end_pos": 113, "type": "TASK", "confidence": 0.7500281163624355}]}, {"text": "Currently, the state-ofthe-art methods are based on statistical supervised learning algorithms, and rely on a large-scale annotated corpus whose cost is extremely expensive.", "labels": [], "entities": []}, {"text": "Although there have been great achievements in building CWS corpora, they are somewhat incompatible due to different segmentation criteria.", "labels": [], "entities": []}, {"text": "As shown in, given a sentence \"\u59da\u660e\u8fdb \u5165\u603b\u51b3\u8d5b (YaoMing reaches the final)\", the two commonly-used corpora, PKU's People's Daily (PKU) () and Penn Chinese Treebank (CTB), use different segmentation criteria.", "labels": [], "entities": [{"text": "PKU's People's Daily (PKU)", "start_pos": 101, "end_pos": 127, "type": "DATASET", "confidence": 0.9440755695104599}, {"text": "Penn Chinese Treebank (CTB)", "start_pos": 135, "end_pos": 162, "type": "DATASET", "confidence": 0.9686106145381927}]}, {"text": "Ina sense, it is a waste of resources if we fail to fully exploit these corpora.", "labels": [], "entities": []}, {"text": "Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.6206571658452352}, {"text": "part-of-speech tagging", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.700722947716713}]}, {"text": "These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other.", "labels": [], "entities": []}, {"text": "However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model.", "labels": [], "entities": []}, {"text": "Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks.", "labels": [], "entities": []}, {"text": "In this paper, we propose an adversarial multicriteria learning for CWS by integrating shared knowledge from multiple segmentation criteria.", "labels": [], "entities": []}, {"text": "Specifically, we regard each segmentation criterion as a single task and propose three different shared-private models under the framework of multi-task learning, where a shared layer is used to extract the criteria-invariant features, and a private layer is used to extract the criteria-specific features.", "labels": [], "entities": []}, {"text": "Inspired by the success of adversarial strategy on domain adaption (, we further utilize adversarial strategy to make sure the shared layer can extract the common underlying and criteria-invariant features, which are suitable for all the criteria.", "labels": [], "entities": []}, {"text": "Finally, we exploit the eight segmentation criteria on the five simplified Chi-nese and three traditional Chinese corpora.", "labels": [], "entities": []}, {"text": "Experiments show that our models are effective to improve the performance for CWS.", "labels": [], "entities": [{"text": "CWS", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9432812929153442}]}, {"text": "We also observe that traditional Chinese could benefit from incorporating knowledge from simplified Chinese.", "labels": [], "entities": []}, {"text": "The contributions of this paper could be summarized as follows.", "labels": [], "entities": []}, {"text": "\u2022 Multi-criteria learning is first introduced for CWS, in which we propose three sharedprivate models to integrate multiple segmentation criteria.", "labels": [], "entities": []}, {"text": "\u2022 An adversarial strategy is used to force the shared layer to learn criteria-invariant features, in which an new objective function is also proposed instead of the original cross-entropy loss.", "labels": [], "entities": []}, {"text": "\u2022 We conduct extensive experiments on eight CWS corpora with different segmentation criteria, which is by far the largest number of datasets used simultaneously.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIG-HAN2005 () and SIGHAN2008.", "labels": [], "entities": [{"text": "CWS datasets", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.7568065226078033}, {"text": "SIG-HAN2005", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.6310731768608093}, {"text": "SIGHAN2008", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.927925169467926}]}, {"text": "gives the details of the eight datasets.", "labels": [], "entities": []}, {"text": "Among these datasets, AS, CI-TYU and CKIP are traditional Chinese, while the Algorithm 1 Adversarial multi-criteria learning for CWS task.", "labels": [], "entities": [{"text": "AS", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.9040206074714661}]}, {"text": "1: for i = 1; i <= n_epoch; i + + do end for 14: end for remains, MSRA, PKU, CTB, NCC and SXU, are simplified Chinese.", "labels": [], "entities": [{"text": "MSRA", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.8944818377494812}]}, {"text": "We use 10% data of shuffled train set as development set for all datasets.", "labels": [], "entities": []}, {"text": "For hyper-parameter configurations, we set both the character embedding size d e and the dimensionality of LSTM hidden states d h to 100.", "labels": [], "entities": []}, {"text": "The initial learning rate \u03b1 is set to 0.01.", "labels": [], "entities": [{"text": "initial learning rate \u03b1", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.7399359047412872}]}, {"text": "The loss weight coefficient \u03bb is set to 0.05.", "labels": [], "entities": [{"text": "loss weight coefficient \u03bb", "start_pos": 4, "end_pos": 29, "type": "METRIC", "confidence": 0.9293325990438461}]}, {"text": "Since the scale of each dataset varies, we use different training batch sizes for datasets.", "labels": [], "entities": []}, {"text": "Specifically, we set batch sizes of AS and MSR datasets as 512 and 256 respectively, and 128 for remains.", "labels": [], "entities": [{"text": "MSR datasets", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.8000410199165344}]}, {"text": "We employ dropout strategy on embedding layer, keeping 80% inputs (20% dropout rate).", "labels": [], "entities": []}, {"text": "For initialization, we randomize all parameters following uniform distribution at (\u22120.05, 0.05).", "labels": [], "entities": [{"text": "initialization", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9727320671081543}]}, {"text": "We simply map traditional Chinese characters to simplified Chinese, and optimize on the same character embedding matrix across datasets, which is pre-trained on Chinese Wikipedia corpus, using word2vec toolkit (.", "labels": [], "entities": [{"text": "Chinese Wikipedia corpus", "start_pos": 161, "end_pos": 185, "type": "DATASET", "confidence": 0.748492568731308}]}, {"text": "Following previous work, all experiments including baseline results are using pre-trained character embedding with bigram feature.", "labels": [], "entities": []}, {"text": "(2) In the second block, our proposed three models based on multi-criteria learning boost performance.", "labels": [], "entities": []}, {"text": "Model-I gains 0.75% improvement on averaging F-measure score compared with Bi-LSTM result (94.14%).", "labels": [], "entities": [{"text": "F-measure score", "start_pos": 45, "end_pos": 60, "type": "METRIC", "confidence": 0.868733286857605}, {"text": "Bi-LSTM", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9548325538635254}]}, {"text": "Only the performance on MSRA drops slightly.", "labels": [], "entities": [{"text": "MSRA", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.9286691546440125}]}, {"text": "Compared to the baseline results (Bi-LSTM and stacked Bi-LSTM), the proposed models boost the performance with the help of exploiting information across these heterogeneous segmentation criteria.", "labels": [], "entities": []}, {"text": "Although various criteria have different segmentation granularities, there are still some underlying information shared.", "labels": [], "entities": []}, {"text": "For instance, MSRA and CTB treat family name and last name as one token \"\u5b81 \u6cfd \u6d9b (NingZeTao)\", whereas some other datasets, like PKU, regard them as two tokens, \"\u5b81 (Ning)\" and \"\u6cfd\u6d9b (ZeTao)\".", "labels": [], "entities": [{"text": "MSRA", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.8863038420677185}, {"text": "CTB", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.8648606538772583}, {"text": "PKU", "start_pos": 127, "end_pos": 130, "type": "DATASET", "confidence": 0.9326208233833313}]}, {"text": "The partial boundaries (before \"\u5b81 (Ning)\" or after \"\u6d9b (Tao)\") can be shared.", "labels": [], "entities": []}, {"text": "We use the NLPCC 2016 dataset 2 () to evaluate our model on micro-blog texts.", "labels": [], "entities": [{"text": "NLPCC 2016 dataset 2", "start_pos": 11, "end_pos": 31, "type": "DATASET", "confidence": 0.9741303324699402}]}, {"text": "The NLPCC 2016 data are provided by the shared task in the 5th CCF Conference on Natural Language Processing & Chinese Computing (NLPCC 2016): Chinese Word Segmentation and POS Tagging for micro-blog Text.", "labels": [], "entities": [{"text": "NLPCC 2016 data", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9224395950635275}, {"text": "Chinese Word Segmentation", "start_pos": 143, "end_pos": 168, "type": "TASK", "confidence": 0.6058151026566824}, {"text": "POS Tagging", "start_pos": 173, "end_pos": 184, "type": "TASK", "confidence": 0.7609793841838837}]}, {"text": "Unlike the popular used newswire dataset, the NLPCC 2016 dataset is collected from Sina Weibo 3 , which consists of the informal texts from micro-blog with the various topics, such as finance, sports, entertainment, and soon.", "labels": [], "entities": [{"text": "NLPCC 2016 dataset", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.9619873563448588}, {"text": "Sina Weibo 3", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.8840019106864929}]}, {"text": "The information of the dataset is shown in) and are fixed for NLPCC dataset.", "labels": [], "entities": [{"text": "NLPCC dataset", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.9805073440074921}]}, {"text": "Here, we conduct Model-I without incorporating adversarial training strategy.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.", "labels": [], "entities": [{"text": "CWS datasets", "start_pos": 59, "end_pos": 71, "type": "DATASET", "confidence": 0.6960193514823914}, {"text": "OOV", "start_pos": 359, "end_pos": 362, "type": "METRIC", "confidence": 0.9818902015686035}, {"text": "precision", "start_pos": 376, "end_pos": 385, "type": "METRIC", "confidence": 0.9991193413734436}, {"text": "recall", "start_pos": 387, "end_pos": 393, "type": "METRIC", "confidence": 0.9861646890640259}, {"text": "F value", "start_pos": 395, "end_pos": 402, "type": "METRIC", "confidence": 0.9542669057846069}, {"text": "OOV", "start_pos": 407, "end_pos": 410, "type": "METRIC", "confidence": 0.9630923867225647}, {"text": "recall rate", "start_pos": 412, "end_pos": 423, "type": "METRIC", "confidence": 0.9224582314491272}]}, {"text": " Table 4: Segmentation cases of personal names.", "labels": [], "entities": [{"text": "Segmentation cases of personal names", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.8770761013031005}]}, {"text": " Table 5: Performance on 3 traditional Chinese da- tasets. Model-I  *  means that the shared parameters  are trained on 5 simplified Chinese datasets and  are fixed for traditional Chinese datasets. Here, we  conduct Model-I without incorporating adversarial  training strategy.", "labels": [], "entities": []}, {"text": " Table 6: Statistical information of NLPCC 2016 dataset.", "labels": [], "entities": [{"text": "NLPCC 2016 dataset", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.9407443801561991}]}, {"text": " Table 7: Performances on the test set of NLPCC  2016 dataset. Model-I  *  means that the shared pa- rameters are trained on 8 Chinese datasets", "labels": [], "entities": [{"text": "NLPCC  2016 dataset", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.9596854249636332}]}]}