{"title": [{"text": "Chunk-Based Bi-Scale Decoder for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.7783979972203573}]}], "abstractContent": [{"text": "In typical neural machine translation (NMT), the decoder generates a sentence word byword, packing all linguistic granularities in the same timescale of RNN.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.8671523133913676}]}, {"text": "In this paper, we propose anew type of decoder for NMT, which splits the decode state into two parts and updates them in two different timescales.", "labels": [], "entities": []}, {"text": "Specifically, we first predict a chunk timescale state for phrasal modeling, on top of which multiple word timescale states are generated.", "labels": [], "entities": []}, {"text": "In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged.", "labels": [], "entities": []}, {"text": "Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model.", "labels": [], "entities": [{"text": "translation", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.9591663479804993}]}], "introductionContent": [{"text": "Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 15, "end_pos": 47, "type": "TASK", "confidence": 0.8706142405668894}, {"text": "machine translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.7816885411739349}, {"text": "statistical machine translation", "start_pos": 314, "end_pos": 345, "type": "TASK", "confidence": 0.6702866752942404}]}, {"text": "This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps ().", "labels": [], "entities": []}, {"text": "The decoder state stores translation information at different granularities, determining which segment should be expressed (phrasal), and which word should be generated (lexical), respectively.", "labels": [], "entities": []}, {"text": "However, due to the extensive existence of multiword phrases and expressions, the varying speed of the lexical component is much faster than the phrasal one.", "labels": [], "entities": []}, {"text": "As in the generation of \"the French Republic\", the lexical component in the decoder will change thrice, each of which fora separate word.", "labels": [], "entities": [{"text": "French Republic\"", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.9301160375277201}]}, {"text": "But the phrasal component may only change once.", "labels": [], "entities": []}, {"text": "The inconsistent varying speed of the two components may cause translation errors.", "labels": [], "entities": [{"text": "translation", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.9472556710243225}]}, {"text": "Typical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation.", "labels": [], "entities": []}, {"text": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside.", "labels": [], "entities": []}, {"text": "However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7632402777671814}]}, {"text": "We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.", "labels": [], "entities": []}, {"text": "The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling).", "labels": [], "entities": []}, {"text": "At each step of decoding, our model first predict a chunk state with a chunk attention, based on which multiple word states are generated with-out attention.", "labels": [], "entities": []}, {"text": "The word state is updated at every step, while the chunk state is only updated when the chunk boundary is detected by a boundary gate automatically.", "labels": [], "entities": []}, {"text": "In this way, we incorporate soft phrases into NMT, which makes the model flexible at capturing both global reordering of phrases and local translation inside phrases.", "labels": [], "entities": []}, {"text": "Our model has following benefits: 1.", "labels": [], "entities": []}, {"text": "The chunk-based NMT model explicitly splits the lexical and phrasal components of the decode state for different time-scales, which addresses the issue of inconsistent updating speeds of different components, making the model more flexible.", "labels": [], "entities": []}, {"text": "2. Our model recognizes phrase structures explicitly.", "labels": [], "entities": []}, {"text": "Phrase information are then used for word predictions, the representations of which are then used to help predict corresponding words.", "labels": [], "entities": [{"text": "word predictions", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.706055223941803}]}, {"text": "3. Instead of incorporating source side linguistic information (, our model incorporates linguistic knowledges in the target side (for deciding chunks), which will guide the translation more inline with linguistic grammars.", "labels": [], "entities": []}, {"text": "4. Given the predicted phrase representation, our NMT model could extract attentive source context by chunk attention, which is more specific and thus more useful compared to the word-level counterpart.", "labels": [], "entities": []}, {"text": "Experiments show that our proposed model obtains considerable BLEU score improvements upon an attention-based NMT baseline on the Chinese to English and the German to English datasets simultaneously.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9776073694229126}]}], "datasetContent": [{"text": "We carryout experiments on a Chinese-English translation task.", "labels": [], "entities": [{"text": "Chinese-English translation task", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.7414807577927908}]}, {"text": "Our training data consists of We also evaluate our model on the WMT translation task of German-English, newstest2014 (DE14) is adopted as development set and newstest2012, newstest2013 (DE1213) are adopted as testing set.", "labels": [], "entities": [{"text": "WMT translation task", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.9195198019345602}]}, {"text": "The English sentences are labeled by a neural chunker, which is implemented according to.", "labels": [], "entities": []}, {"text": "We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric).", "labels": [], "entities": [{"text": "NIST", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.4005509614944458}, {"text": "BLEU score", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9190696477890015}]}, {"text": "In training, we limit the source and target vocabularies to the most frequent 30K words.", "labels": [], "entities": []}, {"text": "We train each model with the sentences of length up to 50 words.", "labels": [], "entities": []}, {"text": "Sizes of the chunk representation and chunk hidden state are set to 1000.", "labels": [], "entities": []}, {"text": "All the other settings are the same as in .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores for different systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984312653541565}]}, {"text": " Table 2: Results with different attention models.", "labels": [], "entities": []}, {"text": " Table 3: Accuracies of predicted chunk boundary  and chunk label.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9977694749832153}]}, {"text": " Table 4: Subjective evaluation results.", "labels": [], "entities": [{"text": "Subjective evaluation", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8923060297966003}]}, {"text": " Table 5: Results on German-English", "labels": [], "entities": [{"text": "German-English", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.74953293800354}]}]}