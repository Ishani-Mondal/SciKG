{"title": [], "abstractContent": [{"text": "We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexical-ized constituency trees.", "labels": [], "entities": []}, {"text": "Experiments on the WMT16 German-English news translation task shown improved BLEU scores when compared to a syntax-agnostic NMT baseline trained on the same dataset.", "labels": [], "entities": [{"text": "WMT16 German-English news translation task", "start_pos": 19, "end_pos": 61, "type": "TASK", "confidence": 0.8164212226867675}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9996188879013062}]}, {"text": "An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline.", "labels": [], "entities": []}, {"text": "A small-scale human evaluation also showed an advantage to the syntax-aware system.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Experimental Setup We first experiment in a resource-rich setting by using the German-English portion of the WMT16 news translation task, with 4.5 million sentence pairs.", "labels": [], "entities": [{"text": "WMT16 news translation task", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.85138139128685}]}, {"text": "We then experiment in a low-resource scenario using the German, Russian and Czech to English training data from the News Commentary v8 corpus, following.", "labels": [], "entities": [{"text": "News Commentary v8 corpus", "start_pos": 116, "end_pos": 141, "type": "DATASET", "confidence": 0.8835443258285522}]}, {"text": "In all cases we parse the English sentences into constituency trees using the BLLIP parser).", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.8515334129333496}]}, {"text": "To enable an open vocabulary translation we used sub-word units obtained via BPE (Sennrich et al., 2016b) on both source and target.", "labels": [], "entities": [{"text": "open vocabulary translation", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.6327110727628072}, {"text": "BPE", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.6216514110565186}]}, {"text": "In each experiment we train two models.", "labels": [], "entities": []}, {"text": "A baseline model (bpe2bpe), trained to translate from the source language sentences to English sentences without any syntactic annotation, and a string-to-linearized-tree model (bpe2tree), trained to translate into English linearized constituency trees as shown in.", "labels": [], "entities": []}, {"text": "Words are segmented into sub-word units using the BPE model we learn on the raw parallel data.", "labels": [], "entities": [{"text": "BPE", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.7625735998153687}]}, {"text": "We use the NEMATUS (Sennrich et al., 2017) 3 implementation of an attention-based NMT model.", "labels": [], "entities": [{"text": "NEMATUS", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.8521789312362671}]}, {"text": "We trained the models until there was no improvement on the development set in 10 consecutive checkpoints.", "labels": [], "entities": []}, {"text": "Note that the only difference between the baseline and the bpe2tree model is the syntactic information, as they have a nearly-identical amount of model parameters (the only additional parameters to the syntax-aware system are the embeddings for the brackets of the trees).", "labels": [], "entities": []}, {"text": "For all models we report results of the best performing single model on the dev-set (newstest2013+newstest2014 in the resource rich setting, newstest2015 in the rest, as measured by BLEU) when translating newstest2015 and newstest2016, similarly to;.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 182, "end_pos": 186, "type": "METRIC", "confidence": 0.9952627420425415}]}, {"text": "To evaluate the string-totree translations we derive the surface form by removing the symbols that stand for non-terminals in the tree, followed by merging the sub-words.", "labels": [], "entities": []}, {"text": "We also report the results of an ensemble of the last 5 checkpoints saved during each model training.", "labels": [], "entities": []}, {"text": "We compute BLEU scores using the mteval-v13a.pl script from the Moses toolkit ( Results As shown in, for the resource-rich setting, the single models (bpe2bpe, bpe2tree) perform similarly in terms of BLEU on newstest2015.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9971110820770264}, {"text": "BLEU", "start_pos": 200, "end_pos": 204, "type": "METRIC", "confidence": 0.99710613489151}]}, {"text": "On newstest2016 we witness an advantage to the bpe2tree model.", "labels": [], "entities": [{"text": "newstest2016", "start_pos": 3, "end_pos": 15, "type": "DATASET", "confidence": 0.9541971683502197}]}, {"text": "A similar trend is found when evaluating the model ensembles: while they improve results for both models, we again see an advantage to the bpe2tree model on newstest2016.", "labels": [], "entities": []}, {"text": "shows the results in the low-resource setting, where the bpe2tree model is consistently better than the bpe2bpe baseline.", "labels": [], "entities": []}, {"text": "We find this interesting as the syntax-aware system performs a much harder task (predicting trees on top of the translations, thus handling much longer output sequences) while having a nearly-identical amount of model parameters.", "labels": [], "entities": []}, {"text": "In order to better understand where or how the syntactic information improves translation quality, we perform a closer analysis of the WMT16 experiment.", "labels": [], "entities": [{"text": "WMT16 experiment", "start_pos": 135, "end_pos": 151, "type": "DATASET", "confidence": 0.842565655708313}]}, {"text": "The bpe2tree translations read better than their bpe2bpe counterparts, both syntactically and semantically, and we highlight some examples which demonstrate this.", "labels": [], "entities": []}, {"text": "lists some representative examples, highlighting improvements that correspond to syntactic phenomena involving reordering or global structure.", "labels": [], "entities": []}, {"text": "We also performed a small-scale human-evaluation using mechanical turk on the first 500 sentences in the dev-set.", "labels": [], "entities": []}, {"text": "Further details are available in the supplementary material.", "labels": [], "entities": []}, {"text": "The results are summarized in the following As can be seen, in 186 cases (37.2%) the human evaluators preferred the bpe2tree translations, vs. 154 cases (30.8%) for bpe2bpe, with the rest of the cases (30%) being neutral.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU results for the WMT16 experiment", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991773962974548}, {"text": "WMT16", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.6838110089302063}]}, {"text": " Table 2: BLEU results for the low-resource exper- iments (News Commentary v8)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998412013053894}, {"text": "News Commentary v8", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.9280580282211304}]}, {"text": " Table 3: Top dev-set GHKM Rules with reordering. Numbers: rule counts. Bolded: reordering rules.", "labels": [], "entities": [{"text": "GHKM", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.8293244242668152}]}]}