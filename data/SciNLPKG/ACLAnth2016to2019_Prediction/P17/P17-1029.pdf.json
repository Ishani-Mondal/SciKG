{"title": [{"text": "Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction", "labels": [], "entities": [{"text": "Labeled Sequence Transduction", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.5967301825682322}]}], "abstractContent": [{"text": "Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels.", "labels": [], "entities": [{"text": "Labeled sequence transduction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5980043808619181}]}, {"text": "In this paper we propose multi-space variational encoder-decoders, anew model for labeled sequence transduction with semi-supervised learning.", "labels": [], "entities": [{"text": "labeled sequence transduction", "start_pos": 82, "end_pos": 111, "type": "TASK", "confidence": 0.6217776238918304}]}, {"text": "The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data.", "labels": [], "entities": []}, {"text": "Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the un-labeled data.", "labels": [], "entities": []}, {"text": "On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-of-art results by a large margin for the majority of languages.", "labels": [], "entities": [{"text": "SIGMORPHON morphological inflection benchmark", "start_pos": 7, "end_pos": 52, "type": "DATASET", "confidence": 0.7005420029163361}]}], "introductionContent": [{"text": "This paper proposes a model for labeled sequence transduction tasks, tasks where we are given an input sequence and a set of labels, from which we are expected to generate an output sequence that reflects the content of the input sequence and desiderata specified by the labels.", "labels": [], "entities": [{"text": "labeled sequence transduction tasks", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.7675492763519287}]}, {"text": "Several examples of these tasks exist in prior work: using labels to moderate politeness in machine translation results (, modifying the output language of a machine translation system, or controlling the length of a summary in summarization ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.6893640011548996}]}, {"text": "In particular, however, we are motivated by the task of morphological reinflection (Cotterell et al., An implementation of our model are available at https://github.com/violet-zct/ MSVED-morph-reinflection.", "labels": [], "entities": []}, {"text": "2016), which we will use as an example in our description and test bed for our models.", "labels": [], "entities": []}, {"text": "In morphologically rich languages, different affixes (i.e. prefixes, infixes, suffixes) can be combined with the lemma to reflect various syntactic and semantic features of a word.", "labels": [], "entities": []}, {"text": "The ability to accurately analyze and generate morphological forms is crucial to creating applications such as machine translation ( or information retrieval) in these languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7977937161922455}, {"text": "information retrieval)", "start_pos": 136, "end_pos": 158, "type": "TASK", "confidence": 0.7601496676603953}]}, {"text": "As shown in 1, re-inflection of an inflected form given the target linguistic labels is a challenging subtask of handling morphology as a whole, in which we take as input an inflected form (in the example, \"playing\") and labels representing the desired form (\"pos=Verb, tense=Past\") and must generate the desired form (\"played\").", "labels": [], "entities": []}, {"text": "Approaches to this task include those utilizing hand-crafted linguistic rules and heuristics, as well as learning-based approaches using alignment and extracted transduction rules.", "labels": [], "entities": []}, {"text": "There have also been methods proposed using neural sequenceto-sequence models, and currently ensembles of attentional encoder-decoder models have achieved state-of-art results on this task.", "labels": [], "entities": []}, {"text": "One feature of these neural models however, is that they are trained in a largely supervised fashion (top of), using data explicitly labeled with the input sequence and labels, along with the output representation.", "labels": [], "entities": []}, {"text": "Needless to say, the ability to obtain this annotated data for many languages is limited.", "labels": [], "entities": []}, {"text": "However, we can expect that for most languages we can obtain large amounts of unlabeled surface forms that may allow for semi-supervised learning over this unlabeled data (entirety of).", "labels": [], "entities": []}, {"text": "In this work, we propose anew framework for labeled sequence transduction problems: multi-space variational encoder-decoders (MSVED, \u00a73.3).", "labels": [], "entities": [{"text": "labeled sequence transduction", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.6187963088353475}]}, {"text": "MSVEDs employ continuous or discrete latent variables belonging to multiple separate probability distributions 3 to explain the observed data.", "labels": [], "entities": []}, {"text": "In the example of morphological reinflection, we introduce a vector of continuous random variables that represent the lemma of the source and target words, and also one discrete random variable for each of the labels, which are on the source or the target side.", "labels": [], "entities": [{"text": "morphological reinflection", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.8630941808223724}]}, {"text": "This model has the advantage of both providing a powerful modeling framework for supervised learning, and allowing for learning in an unsupervised setting.", "labels": [], "entities": []}, {"text": "For labeled data, we maximize the variational lower bound on the marginal log likelihood of the data and annotated labels.", "labels": [], "entities": [{"text": "variational", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9595876932144165}]}, {"text": "For unlabeled data, we train an auto-encoder to reconstruct a word conditioned on its lemma and morphological labels.", "labels": [], "entities": []}, {"text": "While these labels are unavailable, a set of discrete latent variables are associated with each unlabeled word.", "labels": [], "entities": []}, {"text": "Afterwards we can perform posterior inference on these latent variables and maximize the variational lower bound on the marginal log likelihood of data.", "labels": [], "entities": [{"text": "variational", "start_pos": 89, "end_pos": 100, "type": "METRIC", "confidence": 0.9622607231140137}]}, {"text": "Experiments on the SIGMORPHON morphological reinflection task ( ) find that our model beats the state-of-the-art fora single model in the majority of languages, and is particularly effective in languages with more complicated inflectional phenomena.", "labels": [], "entities": [{"text": "SIGMORPHON morphological reinflection task", "start_pos": 19, "end_pos": 61, "type": "TASK", "confidence": 0.8224477469921112}]}, {"text": "Further, we find that semi-supervised learning allows for significant further gains.", "labels": [], "entities": []}, {"text": "Finally, qualitative evaluation of lemma representations finds that our model is able to learn lemma embeddings that match with human intuition.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for Task 3 of SIGMORPHON 2016 on Morphology Reinflection.  \u2020 represents the best single supervised  model score,  \u2021 represents the best model including semi-supervised models, and bold represents the best score overall. #LD  and #ULD are the number of supervised data and unlabeled words respectively.", "labels": [], "entities": [{"text": "Morphology Reinflection", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.81645867228508}, {"text": "ULD", "start_pos": 248, "end_pos": 251, "type": "METRIC", "confidence": 0.9242396354675293}]}, {"text": " Table 2: Percentage of inflected word forms that have mod- ified each part of the lemma (Cotterell et al., 2016) (some  words can be inflected zero or multiple times, thus sums may  not add to 100%).", "labels": [], "entities": []}, {"text": " Table 3: Randomly picked output examples on the test data. Within each block, the first, second and third lines are outputs that  ours is correct and MED's is wrong, ours is wrong and MED's is correct, both are wrong respectively.", "labels": [], "entities": []}]}