{"title": [{"text": "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction", "labels": [], "entities": []}], "abstractContent": [{"text": "Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated.", "labels": [], "entities": [{"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9976189732551575}]}, {"text": "To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to anew, dataset-agnostic, rule-based framework.", "labels": [], "entities": [{"text": "ERRANT", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9586793184280396}]}, {"text": "This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets.", "labels": [], "entities": [{"text": "error type evaluation", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.5614010691642761}, {"text": "GEC datasets", "start_pos": 159, "end_pos": 171, "type": "DATASET", "confidence": 0.9507237672805786}]}, {"text": "Human experts rated the automatic edits as \"Good\" or \"Accept-able\" in at least 95% of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carryout a detailed error type analysis for the first time.", "labels": [], "entities": [{"text": "Accept-able", "start_pos": 54, "end_pos": 65, "type": "METRIC", "confidence": 0.9990963935852051}, {"text": "ERRANT", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9950823783874512}, {"text": "CoNLL-2014 shared task", "start_pos": 142, "end_pos": 164, "type": "DATASET", "confidence": 0.7978724837303162}]}], "introductionContent": [{"text": "Grammatical Error Correction (GEC) systems are often only evaluated in terms of overall performance because system hypotheses are not annotated.", "labels": [], "entities": [{"text": "Grammatical Error Correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6977769335110983}]}, {"text": "This can be misleading however, and a system that performs poorly overall may in fact outperform others at specific error types.", "labels": [], "entities": []}, {"text": "This is significant because a robust specialised system is actually more desirable than a mediocre general system.", "labels": [], "entities": []}, {"text": "Without an error type analysis however, this information is completely unknown.", "labels": [], "entities": []}, {"text": "The main aim of this paper is hence to rectify this situation and provide a method by which parallel error correction data can be automatically annotated with error type information.", "labels": [], "entities": []}, {"text": "This not only facilitates error type evaluation, but can also be used to provide detailed error type feedback to non-native learners.", "labels": [], "entities": [{"text": "error type evaluation", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.7064897616704305}]}, {"text": "Given that different corpora are also annotated according to different standards, we also attempted to standardise existing datasets under a common error type framework.", "labels": [], "entities": []}, {"text": "Our approach consists of two main steps.", "labels": [], "entities": []}, {"text": "First, we automatically extract the edits between parallel original and corrected sentences by means of a linguistically-enhanced alignment algorithm) and second, we classify them according to anew, rule-based framework that relies solely on dataset-agnostic information such as lemma and part-of-speech.", "labels": [], "entities": []}, {"text": "We demonstrate the value of our approach, which we call the ERRor ANnotation Toolkit (ERRANT) , by carrying out a detailed error type analysis of each system in the CoNLL-2014 shared task on grammatical error correction (.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 191, "end_pos": 219, "type": "TASK", "confidence": 0.5230188369750977}]}, {"text": "It is worth mentioning that despite an increased interest in GEC evaluation in recent years, ERRANT is the only toolkit currently capable of producing error types scores.", "labels": [], "entities": [{"text": "GEC evaluation", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.9136605858802795}, {"text": "ERRANT", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.73395836353302}]}], "datasetContent": [{"text": "One of the key strengths of a rule-based approach is that by being dependent only on automatic mark-up information, our classifier is entirely dataset independent and does not require labelled training data.", "labels": [], "entities": []}, {"text": "This is in contrast with machine learning approaches which not only learn dataset specific biases, but also presuppose the existence of sufficient quantities of training data.", "labels": [], "entities": []}, {"text": "A second significant advantage of our approach is that it is also always possible to determine precisely why an edit was assigned a particular error category.", "labels": [], "entities": []}, {"text": "In contrast, human and machine learning classification decisions are often much less transparent.", "labels": [], "entities": [{"text": "machine learning classification decisions", "start_pos": 23, "end_pos": 64, "type": "TASK", "confidence": 0.7047536373138428}]}, {"text": "Finally, by being fully deterministic, our approach bypasses bias effects altogether and should hence be more consistent.", "labels": [], "entities": []}, {"text": "As our new error scheme is based solely on automatically obtained properties of the data, there are no gold standard labels against which to evaluate classifier performance.", "labels": [], "entities": []}, {"text": "For this reason, we instead carried out a small-scale manual evaluation, where we simply asked 5 GEC researchers to rate the appropriateness of the predicted error types for 200 randomly chosen edits in context (100 from FCE-test and 100 from CoNLL-2014) as \"Good\", \"Acceptable\" or \"Bad\".", "labels": [], "entities": [{"text": "GEC researchers", "start_pos": 97, "end_pos": 112, "type": "DATASET", "confidence": 0.9017578959465027}, {"text": "FCE-test", "start_pos": 221, "end_pos": 229, "type": "DATASET", "confidence": 0.9231886267662048}, {"text": "CoNLL-2014", "start_pos": 243, "end_pos": 253, "type": "DATASET", "confidence": 0.8867929577827454}, {"text": "Acceptable", "start_pos": 267, "end_pos": 277, "type": "METRIC", "confidence": 0.9955562949180603}]}, {"text": "\"Good' meant the chosen type was the most appropriate for the given edit, \"Acceptable\" meant the chosen type was appropriate, but probably not optimum, while \"Bad\" meant the chosen type was not appropriate for the edit.", "labels": [], "entities": [{"text": "Acceptable", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9954168796539307}]}, {"text": "Raters were warned that the edit boundaries had been determined automatically and hence might be unusual, but that they should focus on the appropriateness of the error type regardless of whether they agreed with the boundary or not.", "labels": [], "entities": []}, {"text": "It is worth stating that the main purpose of this evaluation was not to evaluate the specific strengths and weaknesses of the classifier, but rather ascertain how well humans believed the predicted error types characterised each edit.", "labels": [], "entities": []}, {"text": "GEC is known to be a highly subjective task (Bryant and", "labels": [], "entities": [{"text": "GEC", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.799063503742218}]}], "tableCaptions": [{"text": " Table 3: The percent distribution for how each ex- pert rated the appropriateness of the predicted er- ror types. E.g. Rater 3 considered 83% of all pre- dicted types to be \"Good\".", "labels": [], "entities": []}, {"text": " Table 4: Overall scores for each team in CoNLL- 2014 using gold and auto references with both the  M 2 scorer and our simpler edit comparison ap- proach. All scores are in terms of F 0.5 .", "labels": [], "entities": [{"text": "CoNLL- 2014", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.8839254379272461}, {"text": "M 2 scorer", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9061697920163473}, {"text": "F 0.5", "start_pos": 182, "end_pos": 187, "type": "METRIC", "confidence": 0.9694351851940155}]}, {"text": " Table 5: Precision, recall and F 0.5 for Missing, Unnecessary, and Replacement errors for each team.  A dash indicates the team's system did not attempt to correct the given error type (TP+FP = 0).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9995256662368774}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9993682503700256}, {"text": "F 0.5", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9921916723251343}, {"text": "Replacement errors", "start_pos": 68, "end_pos": 86, "type": "METRIC", "confidence": 0.9201504290103912}, {"text": "TP+FP", "start_pos": 187, "end_pos": 192, "type": "METRIC", "confidence": 0.8423132101694742}]}, {"text": " Table 6: Precision, recall and F 0.5 for each team and error type. A dash indicates the team's system did  not attempt to correct the given error type (TP+FP = 0). The highest F-score for each type is highlighted.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9995720982551575}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9993422627449036}, {"text": "F 0.5", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9919831156730652}, {"text": "TP+FP", "start_pos": 153, "end_pos": 158, "type": "METRIC", "confidence": 0.8759015997250875}, {"text": "F-score", "start_pos": 177, "end_pos": 184, "type": "METRIC", "confidence": 0.9973946809768677}]}, {"text": " Table 7: Detailed breakdown of Determiner errors  for two teams.", "labels": [], "entities": [{"text": "Detailed breakdown of Determiner errors", "start_pos": 10, "end_pos": 49, "type": "METRIC", "confidence": 0.8169197082519531}]}, {"text": " Table 8: Each team's performance at correcting  multi-token edits; i.e. there are at least two tokens  on one side of the edit.", "labels": [], "entities": [{"text": "correcting  multi-token edits", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.8664618531862894}]}, {"text": " Table 9: There are 55 total possible error types. This table shows all of them except UNK, which  indicates an uncorrected error. A dash indicates an impossible combination.", "labels": [], "entities": [{"text": "UNK", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.6258234977722168}]}, {"text": " Table 10: True Positive, False Positive and False Negative counts for each team in terms of Missing,  Replacement and Unnecessary edits. The total number of edits may vary for each system, as this depends  on the individual references that are chosen during evaluation. These results were used to make Table 5.", "labels": [], "entities": [{"text": "False", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9895707368850708}, {"text": "False", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9835524559020996}, {"text": "Missing", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9563964605331421}]}]}