{"title": [{"text": "Segmentation guided attention networks for Visual Question Answering", "labels": [], "entities": [{"text": "Segmentation guided attention", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8634566068649292}, {"text": "Visual Question Answering", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.6472502052783966}]}], "abstractContent": [{"text": "In this paper we propose to solve the problem of Visual Question Answering by using a novel segmentation guided attention based network which we call SegAttend-Net.", "labels": [], "entities": [{"text": "Visual Question Answering", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.597057024637858}]}, {"text": "We use image segmentation maps, generated by a Fully Convolutional Deep Neural Network to refine our attention maps and use these refined attention maps to make the model focus on the relevant parts of the image to answer a question.", "labels": [], "entities": [{"text": "image segmentation", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7132898718118668}]}, {"text": "The refined attention maps are used by the LSTM network to learn to produce the answer.", "labels": [], "entities": []}, {"text": "We presently train our model on the visual7W dataset and do a category wise evaluation of the 7 question categories.", "labels": [], "entities": [{"text": "visual7W dataset", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.8947620391845703}]}, {"text": "We achieve state of the art results on this dataset and beat the previous benchmark on this dataset by a 1.5% margin improving the question answering accuracy from 54.1% to 55.6% and demonstrate improvements in each of the question categories.", "labels": [], "entities": [{"text": "question answering", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.7695900201797485}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.940601110458374}]}, {"text": "We also visualize our generated attention maps and note their improvement over the attention maps generated by the previous best approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Visual Question Answering (VQA) is a recent problem in the intersection of the fields of Computer Vision and Natural Language Processing, where a system is required to answer arbitrary questions about the images, which may require reasoning about the relationships of objects with each other and the overall scene.", "labels": [], "entities": [{"text": "Visual Question Answering (VQA)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7550016740957896}]}, {"text": "There are many potential applications for VQA.", "labels": [], "entities": [{"text": "VQA", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.7226840853691101}]}, {"text": "The most immediate is as an aid to blind and visually impaired individuals, enabling them to get information about images both on the web and in the real world.", "labels": [], "entities": []}, {"text": "The task of Image Question answering has received a lot of traction from the research community of late ( , ,,) due to the inherent challenging nature of the problem which involves combining question understanding in context of the image, scene understanding and commonsense reasoning to be able to answer the question effectively.", "labels": [], "entities": [{"text": "Image Question answering", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.8021555344263712}]}, {"text": "The problem is much more complicated than the purely text based Question answering problem which has been extensively studied in the past,, , ) and needs the model to be able to combine information from multiple sources and reason about them together.", "labels": [], "entities": [{"text": "Question answering problem", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.9106239477793375}]}, {"text": "Most recent approaches are based on Neural Networks, where a Convolutional Neural is first used to extract out image features and then these image features are used along with some RNN model to understand the question and generate an answer.", "labels": [], "entities": []}, {"text": "However the problem with such approaches is that they do not know whereto look.", "labels": [], "entities": []}, {"text": "Recent approaches solve this problem by calculating an attention over the image by using the question embeddings to try and guide the model whereto look, however such attention maps are still not very precise and not grounded at the image level.", "labels": [], "entities": []}, {"text": "Moreover, there is noway to explicitly train these attention maps and the hope is that the model will implicitly learn them during training.", "labels": [], "entities": []}, {"text": "In this paper we propose an approach which tries to guide these attention maps to learn to focus on the right regions in this image by giving them pixel level grounded annotations in the form of segmentation maps which we generate using a Fully Convolutional Deep Neural Network.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "The existing literature on this problem is presented in Section 2 followed by a description of the datasets we used in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 introduces our approach and gives a detailed explanation of how we generate the segment maps and use them to guide our model to learn better attention maps which are subsequently used to perform the task of visual question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 224, "end_pos": 242, "type": "TASK", "confidence": 0.6918652802705765}]}, {"text": "Finally we present the results in Section 5 and outline the papers conclusions and directions for future research in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We did our experimentation on the Visual7W Dataset which was introduced by.", "labels": [], "entities": [{"text": "Visual7W Dataset", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.9730480015277863}]}, {"text": "Visual7W is named after the seven categories of questions it contains: What, Where, How, When, Who, Why, and Which.", "labels": [], "entities": []}, {"text": "The dataset also provides object level groundings in the form of bounding boxes for the objects occuring in the question.", "labels": [], "entities": []}, {"text": "The Visual7W dataset is collected on 47,300 COCO images.", "labels": [], "entities": [{"text": "Visual7W dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9727376401424408}]}, {"text": "In total, it has 327,939 QA pairs, together with 1,311,756 human-generated multiple-choices and 561,459 object groundings from 36,579 categories.", "labels": [], "entities": []}, {"text": "In addition, it also provides complete grounding annotations that link the object mentioned in the QA sentences to their bounding boxes in the images and therefore introduce anew QA type with image regions as the visually grounded answers.", "labels": [], "entities": []}, {"text": "We use this dataset for our task as we wanted to study how having pixel level groundings inform of segmentation maps affect each particular question type among how, when, where, why etc.", "labels": [], "entities": []}, {"text": "We expect the improvement to be substantial for questions like 'how many' and 'where' which intuitively should benefit most from such pixel level groundings.", "labels": [], "entities": []}, {"text": "This study allows us to validate this.", "labels": [], "entities": []}, {"text": "We can also compare how these segmentation maps correspond with the provided object level groundings.", "labels": [], "entities": []}, {"text": "Hence this dataset is our dataset of choice for this study.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of results of our model against some existing approaches on the VQA task", "labels": [], "entities": [{"text": "VQA", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.8885135054588318}]}]}