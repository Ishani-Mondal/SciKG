{"title": [{"text": "Jointly Extracting Relations with Class Ties via Effective Deep Ranking", "labels": [], "entities": []}], "abstractContent": [{"text": "Connections between relations in relation extraction, which we call class ties, are common.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7750681638717651}]}, {"text": "In distantly supervised scenario, one entity tuple may have multiple relation facts.", "labels": [], "entities": []}, {"text": "Exploiting class ties between relations of one entity tuple will be promising for distantly supervised relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7083060145378113}]}, {"text": "However, previous models are not effective or ignore to model this property.", "labels": [], "entities": []}, {"text": "In this work, to effectively leverage class ties, we propose to make joint relation extraction with a unified model that integrates convolutional neural network (CNN) with a general pairwise ranking framework, in which three novel ranking loss functions are introduced.", "labels": [], "entities": [{"text": "joint relation extraction", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.6473146577676138}]}, {"text": "Additionally, an effective method is presented to relieve the severe class imbalance problem from NR (not relation) for model training.", "labels": [], "entities": [{"text": "NR", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.839738667011261}]}, {"text": "Experiments on a widely used dataset show that leverag-ing class ties will enhance extraction and demonstrate the effectiveness of our model to learn class ties.", "labels": [], "entities": []}, {"text": "Our model outperforms the baselines significantly, achieving state-of-the-art performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation extraction (RE) aims to classify the relations between two given named entities from natural-language text.", "labels": [], "entities": [{"text": "Relation extraction (RE)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9298743367195129}]}, {"text": "Supervised machine learning methods require numerous labeled data to work well.", "labels": [], "entities": []}, {"text": "With the rapid growth of volume of relation types, traditional methods cannot keep up with the step for the limitation of labeled data.", "labels": [], "entities": []}, {"text": "In order to narrow down the gap of data sparsity, propose distant supervision (DS) for relation extraction, which automati- cally generates training data by aligning a knowledge facts database (ie.) with texts.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8439993262290955}]}, {"text": "Class ties mean the connections between relations in relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7496915459632874}]}, {"text": "In general, we conclude that class ties can have two types: weak class ties and strong class ties.", "labels": [], "entities": []}, {"text": "Weak class ties mainly involve the co-occurrence of relations such as place of birth and place lived, CEO of and founder of.", "labels": [], "entities": []}, {"text": "On the contrary, strong class ties mean that relations have latent logical entailments.", "labels": [], "entities": []}, {"text": "Take the two relations of capital of and city of for example, if one entity tuple has the relation of capital of, it must express the relation fact of city of, because the two relations have the entailment of capital of \u21d2 city of.", "labels": [], "entities": []}, {"text": "Obviously the opposite induction is not correct.", "labels": [], "entities": []}, {"text": "Further take the sentence of \"Jonbenet told me that her mother [Patsy Ramsey] e 1 never left e 2 since she was born.\" in DS scenario for example.", "labels": [], "entities": [{"text": "DS", "start_pos": 121, "end_pos": 123, "type": "DATASET", "confidence": 0.7657495141029358}]}, {"text": "This sentence expresses two relation facts which are place of birth and place lived.", "labels": [], "entities": []}, {"text": "However, the word \"born\" is a strong bios to extract place of birth, so it may not be easy to predict the relation of place lived, but if we can incorporate the weak ties between the two relations, extracting place of birth will provide evidence for prediction of place lived.", "labels": [], "entities": []}, {"text": "Exploiting class ties is necessary for DS based relation extraction.", "labels": [], "entities": [{"text": "DS based relation extraction", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.7977240830659866}]}, {"text": "In DS scenario, there is a challenge that one entity tuple can have multiple rela-tion facts as shown in, which is called relation overlapping.", "labels": [], "entities": []}, {"text": "However, the relations of one entity tuple can have class ties mentioned above which can be leveraged to enhance relation extraction for it narrowing down potential searching spaces and reducing uncertainties between relations when predicting unknown relations.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7758257389068604}]}, {"text": "If one pair entities has CEO of, it will contain founder of with high possibility.", "labels": [], "entities": [{"text": "founder", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.8163367509841919}]}, {"text": "To exploit class ties between relations, we propose to make joint extraction for all positive labels of one entity tuple with considering pairwise connections between positive and negative labels inspired by).", "labels": [], "entities": []}, {"text": "As the two relations with class ties shown in, by joint extraction of two relations, we can maintain the class ties (co-occurrence) of them from training samples to be learned by potential model, and then leverage this learned information to extract instances with unknown relations, which cannot be achieved by separated extraction for it dividing labels apart losing information of cooccurrence.", "labels": [], "entities": []}, {"text": "To classify positive labels from negative ones, we adopt pairwise ranking to rank positive ones higher than negative ones, exploiting pairwise connections between them.", "labels": [], "entities": []}, {"text": "Ina word, joint extraction exploits class ties between relations and pairwise ranking classify positive labels from negative ones.", "labels": [], "entities": [{"text": "joint extraction", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7415482699871063}]}, {"text": "Furthermore, combining information across sentences will be more appropriate for joint extraction which provides more information from other sentences to extract each relation (.", "labels": [], "entities": [{"text": "joint extraction", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.7472144365310669}]}, {"text": "In, sentence #1 is the evidence for place of birth, but it also expresses the meaning of \"living in someplace\", so it can be aggregated with sentence #2 to extract place lived.", "labels": [], "entities": []}, {"text": "Meanwhile, the word of \"hometown\" in sentence #2 can provide evidence for place of birth which should be combined with sentence #1 to extract place of birth.", "labels": [], "entities": []}, {"text": "In this work, we propose a unified model that integrates pairwise ranking with CNN to exploit class ties.", "labels": [], "entities": []}, {"text": "Inspired by the effectiveness of deep learning for modeling sentences (, we use CNN to encode sentences.", "labels": [], "entities": []}, {"text": "Similar to: The main architecture of our model.", "labels": [], "entities": []}, {"text": "embedded sentences into one bag representation vector aiming to aggregate information across sentences, after that we measure the similarity between bag representation and relation class in realvalued space.", "labels": [], "entities": []}, {"text": "With two variants for combining sentences, three novel pairwise ranking loss functions are proposed to make joint extraction.", "labels": [], "entities": [{"text": "joint extraction", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.7150360196828842}]}, {"text": "Besides, to relieve the bad impact of class imbalance from NR (not relation)) for training our model, we cut down loss propagation from NR class during training.", "labels": [], "entities": []}, {"text": "Our experimental results on dataset of are evident that: (1) Our model is much more effective than the baselines; (2) Leveraging class ties will enhance relation extraction and our model is efficient to learn class ties by joint extraction; (3) A much better model can be trained after relieving class imbalance from NR.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.8245384097099304}]}, {"text": "Our contributions in this paper can be encapsulated as follows: \u2022 We propose to leverage class ties to enhance relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.826930969953537}]}, {"text": "An effective deep ranking model which integrates CNN and pairwise ranking framework is introduced to exploit class ties.", "labels": [], "entities": []}, {"text": "\u2022 We propose an effective method to relieve the impact of data imbalance from NR for model training.", "labels": [], "entities": []}, {"text": "\u2022 Our method achieves state-of-the-art performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct our experiments on a widely used dataset, developed by  Following, we adopt heldout evaluation framework in all experiments.", "labels": [], "entities": []}, {"text": "Aggregated precision/recall curves are drawn and precision@N (P@N) is reported to illustrate the model performance.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9985257983207703}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9004743099212646}, {"text": "precision@N (P@N)", "start_pos": 49, "end_pos": 66, "type": "METRIC", "confidence": 0.9045344963669777}]}, {"text": "We use a word2vec tool that is gensim 3 to train word embeddings on NYT corpus.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.9723231494426727}]}, {"text": "Similar to, we keep the words that appear more than 100 times to construct word dictionary and use \"UNK\" to represent the other ones.", "labels": [], "entities": []}, {"text": "Three-fold validation on the training dataset is adopted to tune the parameters following.", "labels": [], "entities": []}, {"text": "We use grid search to determine the optimal hyperparameters.", "labels": [], "entities": []}, {"text": "We select word embedding size from {50, 100, 150, 200, 250, 300}.", "labels": [], "entities": []}, {"text": "Batch size is tuned from {80, 160, 320, 640}.", "labels": [], "entities": []}, {"text": "We determine learning rate among {0.01, 0.02, 0.03, 0.04}.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 13, "end_pos": 26, "type": "METRIC", "confidence": 0.949912816286087}]}, {"text": "The window size of convolution is tuned from {1, 3, 5}.", "labels": [], "entities": []}, {"text": "We keep other hyper-parameters same as: the number of kernels is 230, position embedding size is 5 and dropout rate is 0.5.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 103, "end_pos": 115, "type": "METRIC", "confidence": 0.9357369840145111}]}, {"text": "shows the detailed parameter settings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The proportions of NR samples from  SemEval-2010 Task 8 dataset and Riedel dataset.", "labels": [], "entities": [{"text": "SemEval-2010 Task 8 dataset", "start_pos": 46, "end_pos": 73, "type": "DATASET", "confidence": 0.6464031264185905}, {"text": "Riedel dataset", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.8496550917625427}]}, {"text": " Table 4: Precisions for top 100, 200, 300, 400, 500  and average of them for impact of joint extraction  and class ties.", "labels": [], "entities": [{"text": "Precisions", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9923086166381836}, {"text": "joint extraction", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.756599485874176}]}, {"text": " Table 4. From the results we can see that: (1) For  Rank + ATT and Rank + ExATT, joint extraction  exhibits better performance than separated extrac- tion, which demonstrates class ties will improve  relation extraction and the two methods are effec- tive to learn class ties; (2) For Rank + AVE, sur- prisingly joint extraction does not keep up with  separated extraction. For the second phenomenon,  the explanation may lie in the AVE method to ag- gregate sentences will incorporate noise data con- sistent with the finding in", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 201, "end_pos": 220, "type": "TASK", "confidence": 0.8072989284992218}]}, {"text": " Table 5: Precisions for top 100, 200, 300, 400, 500  and average of them for Rank + AVE, Rank + ATT  and Rank + ExATT.", "labels": [], "entities": [{"text": "Precisions", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9878415465354919}, {"text": "AVE", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.6612244844436646}, {"text": "ATT", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.5748370289802551}, {"text": "ExATT", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.7180032730102539}]}]}