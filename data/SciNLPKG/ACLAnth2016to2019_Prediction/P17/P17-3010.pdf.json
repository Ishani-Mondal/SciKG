{"title": [{"text": "Variation Autoencoder Based Network Representation Learning for Classification", "labels": [], "entities": [{"text": "Classification", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.7958875894546509}]}], "abstractContent": [{"text": "Network representation is the basis of many applications and of extensive interest in various fields, such as information retrieval, social network analysis , and recommendation systems.", "labels": [], "entities": [{"text": "Network representation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7776214778423309}, {"text": "information retrieval", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.796117901802063}, {"text": "social network analysis", "start_pos": 133, "end_pos": 156, "type": "TASK", "confidence": 0.6267390151818594}]}, {"text": "Most previous methods for network representation only consider the incomplete aspects of a problem, including link structure , node information, and partial integration.", "labels": [], "entities": []}, {"text": "The present study introduces a deep network representation model that seamlessly integrates the text information and structure of a network.", "labels": [], "entities": []}, {"text": "The model captures highly non-linear relationships between nodes and complex features of a network by exploiting the variational au-toencoder (VAE), which is a deep unsu-pervised generation algorithm.", "labels": [], "entities": [{"text": "variational au-toencoder (VAE)", "start_pos": 117, "end_pos": 147, "type": "METRIC", "confidence": 0.8750268816947937}]}, {"text": "The representation learned with a paragraph vector model is merged with that learned with the VAE to obtain the network representation, which preserves both structure and text information.", "labels": [], "entities": [{"text": "VAE", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.49280473589897156}]}, {"text": "Comprehensive experiments is conducted on benchmark datasets and find that the introduced model performs better than state-of-the-art techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information network representation is an important research issue because it is the basis of many applications, such as document classification in citation networks, functional label prediction in protein-protein interaction networks, and potential friend recommendations in social networks.", "labels": [], "entities": [{"text": "Information network representation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7230336864789327}, {"text": "document classification in citation networks", "start_pos": 120, "end_pos": 164, "type": "TASK", "confidence": 0.7356226027011872}, {"text": "functional label prediction", "start_pos": 166, "end_pos": 193, "type": "TASK", "confidence": 0.6691017349561056}]}, {"text": "Although there are not a few recent work proposed to study the issue, it is still far from satisfactory because of the intrinsic difficulty.", "labels": [], "entities": []}, {"text": "In essence, the rich and complex information (i.e., link structure and node contents) embedded in information networks poses a significant challenge in the effective representation of networks.", "labels": [], "entities": []}, {"text": "Network-distributed representation learning can be viewed as a problem using low-dimensional vectors to represent nodes in a network.", "labels": [], "entities": [{"text": "Network-distributed representation learning", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7689303159713745}]}, {"text": "Most network representation methods are based on a network structure.", "labels": [], "entities": []}, {"text": "The traditional representation is based on matrix decomposition and uses eigenvectors as representation).", "labels": [], "entities": []}, {"text": "Furthermore, they extend to high-order information (.", "labels": [], "entities": []}, {"text": "However, these methods are not applicable to large-scale networks, and although many approximate approaches have been developed to solve this problem, they are not effective enough.", "labels": [], "entities": []}, {"text": "Some methods are based on optimization objective functions (.", "labels": [], "entities": []}, {"text": "Although they are suitable for large-scale network data, they adopt shallow models that are limited in terms of performance and are difficult to use to obtain highly non-linear relationships that are vital to the preservation of network structure.", "labels": [], "entities": []}, {"text": "Inspired by deep learning techniques in natural language processing, () adopted several stunted random walks in networks to generate node sequences serving as sentence corpus and then applied the skip-gram model to these sequences to learn node representation.", "labels": [], "entities": []}, {"text": "However, they cannot easily handle additional information during random walks in a network.", "labels": [], "entities": []}, {"text": "To capture highly non-linear structures for large-scale networks, ( introduced an autoencoder to model training instead of using a sampling based method to generate linear sequences.", "labels": [], "entities": []}, {"text": "Motivated by this model, we develop the variational autoencoder (VAE), which is a deep generation model, instead of a basic autoencoder.", "labels": [], "entities": [{"text": "variational autoencoder (VAE)", "start_pos": 40, "end_pos": 69, "type": "METRIC", "confidence": 0.5653305649757385}]}, {"text": "Most previous studies utilized only one type of information in networks.", "labels": [], "entities": []}, {"text": "The work in) focused on node content, and others () explored link structure.", "labels": [], "entities": []}, {"text": "Although a few previous models) combined both text information and network structure, they did not preserve the complete network structure and only partially utilized node content.", "labels": [], "entities": []}, {"text": "A straightforward method is to learn representations from text features and network structure independently, and then concatenate the two separate representations.", "labels": [], "entities": []}, {"text": "To address the above issues, we introduce a deep generative model to learn network representation by modeling both node content information and network structure comprehensively.", "labels": [], "entities": []}, {"text": "First, the representation based on node content through the paragraph vector model is obtained.", "labels": [], "entities": []}, {"text": "Then, we feed the network adjacency matrix and representation obtained into a deep generative model, the building block of which is the VAE.", "labels": [], "entities": [{"text": "VAE", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.7509282827377319}]}, {"text": "After stacking several layers of the VAE, the result of the first layer is chosen before decoding as the final representation.", "labels": [], "entities": [{"text": "VAE", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.8332543969154358}]}, {"text": "Intuitively, we can obtain the representation containing both content information and structure in a d-dimensional feature space.", "labels": [], "entities": []}, {"text": "The experimental evaluation demonstrates the superior performance of the model on the benchmark datasets.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Macro-F1 score on Citeseer-M10 Network", "labels": [], "entities": []}, {"text": " Table 2: Macro-F1 score on DBLP Network", "labels": [], "entities": [{"text": "DBLP Network", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.977217972278595}]}]}