{"title": [{"text": "Aggregating and Predicting Sequence Labels from Crowd Annotations", "labels": [], "entities": [{"text": "Aggregating and Predicting Sequence Labels from Crowd Annotations", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.6871984228491783}]}], "abstractContent": [{"text": "Despite sequences being core to NLP, scant work has considered how to handle noisy sequence labels from multiple anno-tators for the same text.", "labels": [], "entities": []}, {"text": "Given such annotations , we consider two complementary tasks: (1) aggregating sequential crowd labels to infer a best single set of consensus annotations; and (2) using crowd annotations as training data fora model that can predict sequences in unannotated text.", "labels": [], "entities": []}, {"text": "For aggregation, we propose a novel Hidden Markov Model variant.", "labels": [], "entities": []}, {"text": "To predict sequences in unannotated text, we propose a neural approach using Long Short Term Memory.", "labels": [], "entities": []}, {"text": "We evaluate a suite of methods across two different applications and text genres: Named-Entity Recognition in news articles and Information Extraction from biomedical abstracts.", "labels": [], "entities": [{"text": "Named-Entity Recognition in news articles", "start_pos": 82, "end_pos": 123, "type": "TASK", "confidence": 0.7998839974403381}, {"text": "Information Extraction from biomedical abstracts", "start_pos": 128, "end_pos": 176, "type": "TASK", "confidence": 0.7879608929157257}]}, {"text": "Results show improvement over strong baselines.", "labels": [], "entities": []}, {"text": "Our source code and data are available online 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Many important problems in Natural Language Processing (NLP) maybe viewed as sequence labeling tasks, such as part-of-speech (PoS) tagging, named-entity recognition (NER), and Information Extraction (IE).", "labels": [], "entities": [{"text": "part-of-speech (PoS) tagging", "start_pos": 110, "end_pos": 138, "type": "TASK", "confidence": 0.7234410285949707}, {"text": "named-entity recognition (NER)", "start_pos": 140, "end_pos": 170, "type": "TASK", "confidence": 0.8489550530910492}, {"text": "Information Extraction (IE)", "start_pos": 176, "end_pos": 203, "type": "TASK", "confidence": 0.8422114372253418}]}, {"text": "As with other machine learning tasks, automatic sequence labeling typically requires annotated corpora on which to train predictive models.", "labels": [], "entities": [{"text": "automatic sequence labeling", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.5662956734498342}]}, {"text": "While such annotation was traditionally performed by domain experts, crowdsourcing has become a popular means to acquire large labeled datasets at lower cost, though annotations from laypeople maybe lower quality than those from domain experts ( is therefore essential to model crowdsourced label quality, both to estimate individual annotator reliability and to aggregate individual annotations to induce a single set of \"reference standard\" consensus labels.", "labels": [], "entities": []}, {"text": "While many models have been proposed for aggregating crowd labels for binary or multiclass classification problems, far less work has explored crowdbased annotation of sequences (.", "labels": [], "entities": []}, {"text": "In this paper, we investigate two complementary challenges in using sequential crowd labels: how to best aggregate them (Task 1); and how to accurately predict sequences in unannotated text given training data from the crowd (Task 2).", "labels": [], "entities": []}, {"text": "For aggregation, one might want to induce a single set of high-quality consensus annotations for various purposes: (i) for direct use at run-time (when a given application requires human-level accuracy in identifying sequences); (ii) for sharing with others; or (iii) for training a predictive model.", "labels": [], "entities": []}, {"text": "When human-level accuracy in tagging of sequences is not crucial, automatic labeling of unannotated text is typically preferable, as it is more efficient, scalable, and cost-effective.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9835559725761414}, {"text": "tagging of sequences", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.8687800168991089}]}, {"text": "Given a training set of crowd labels, how can we best predict sequences in unannotated text?", "labels": [], "entities": []}, {"text": "Should we: (i) consider Task 1 as a pre-processing step and train the model using consensus labels; or (ii) instead directly train the model on all of the individual annotations, as done by?", "labels": [], "entities": []}, {"text": "We investigate both directions in this work.", "labels": [], "entities": []}, {"text": "Our approach is to augment existing sequence labeling models such as HMMs and) by introducing an explicit \"crowd component\".", "labels": [], "entities": []}, {"text": "For HMMs, we model this crowd component by including additional parameters for worker label quality and crowd label variables.", "labels": [], "entities": []}, {"text": "For the LSTM, we introduce a vector representation for each annotator.", "labels": [], "entities": []}, {"text": "In both cases, the crowd component models both the noise from labels and the label quality from each annotator.", "labels": [], "entities": []}, {"text": "We find that principled combination of the \"crowd component\" with the \"sequence component\" yields strong improvement.", "labels": [], "entities": []}, {"text": "For evaluation, we consider two practical applications in two text genres: NER in news and IE from medical abstracts.", "labels": [], "entities": [{"text": "NER in news", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.8607566356658936}, {"text": "IE from medical abstracts", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.7589918524026871}]}, {"text": "Recognizing namedentities such as people, organizations or locations can be viewed as a sequence labeling task in which each label specifies whether each word is Inside, Outside or Beginning (IOB) a namedentity.", "labels": [], "entities": [{"text": "Recognizing namedentities such as people, organizations or locations", "start_pos": 0, "end_pos": 68, "type": "TASK", "confidence": 0.8082674145698547}]}, {"text": "For this task, we consider the English portion of the CoNLL-2003 dataset, using crowd labels collected by.", "labels": [], "entities": [{"text": "CoNLL-2003 dataset", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.9730764627456665}]}, {"text": "For the IE application, we use a set of biomedical abstracts that describe Randomized Controlled Trials (RCTs).", "labels": [], "entities": [{"text": "IE", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9827142357826233}]}, {"text": "The crowdsourced annotations comprise labeled text spans that describe the patient populations enrolled in the corresponding RCTs.", "labels": [], "entities": []}, {"text": "For example, an abstract may contain the text: we recruited and enrolled diabetic patients.", "labels": [], "entities": []}, {"text": "Identifying these sequences is useful for downstream systems that process biomedical literature, e.g., clinical search engines (.", "labels": [], "entities": []}, {"text": "We present a systematic investigation and evaluation of alternative methods for handling and utilizing crowd labels for sequential annotation tasks.", "labels": [], "entities": []}, {"text": "We consider both how to best aggregate sequential crowd labels (Task 1) and how to best predict sequences in unannotated text given a training set of crowd annotations (Task 2).", "labels": [], "entities": []}, {"text": "As part of this work, we propose novel models for working with noisy sequence labels from the crowd.", "labels": [], "entities": []}, {"text": "Reported experiments both benchmark existing state-of-the-art approaches (sequential and non-sequential) and show that our proposed models achieve best-in-class performance.", "labels": [], "entities": []}, {"text": "As noted in the Abstract, we have also shared our sourcecode and data online for use by the community.", "labels": [], "entities": [{"text": "Abstract", "start_pos": 16, "end_pos": 24, "type": "DATASET", "confidence": 0.8526135683059692}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Datasets used for each application. We  list the total number of articles/abstracts and the  number which have Gold/Crowd labels.", "labels": [], "entities": []}, {"text": " Table 2: NER results for Task 1 (crowd label ag- gregation). Rows 1-3 show non-sequential meth- ods while Rows 4-6 show sequential methods.", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8980981707572937}]}, {"text": " Table 3: NER results on Task 2: predicting sequences on unannotated text when trained on crowd labels.  Rows 1-4 train the predictive model using individual crowd labels, while Rows 5-8 first aggregate crowd  labels then train the model on the induced consensus labels. The last row indicates an upper-bound from  training on gold labels. LSTM-Crowd and LSTM-Crowd-cat are described in Section 3.", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9233005046844482}, {"text": "predicting sequences", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.8947637379169464}]}, {"text": " Table 4: Biomedical IE results for Task 1: aggregating sequential crowd labels to induce consensus  labels. Rows 1-3 indicate non-sequential baselines. Results are averaged over 100 bootstrap re-samples.  We report the standard deviation of F1, std, due to this dataset having fewer gold labels for evaluation.", "labels": [], "entities": [{"text": "Biomedical IE", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.5962026417255402}, {"text": "F1", "start_pos": 242, "end_pos": 244, "type": "METRIC", "confidence": 0.998690664768219}]}, {"text": " Table 5: Biomedical IE results for Task 2. Rows 1-3 correspond to training on all labels, while Rows  4-7 first aggregate crowd labels then train the sequence labeling model on consensus annotations.", "labels": [], "entities": [{"text": "Biomedical IE", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.6622973680496216}]}]}