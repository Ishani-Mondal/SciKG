{"title": [{"text": "Context-Dependent Sentiment Analysis in User-Generated Videos", "labels": [], "entities": [{"text": "Context-Dependent Sentiment Analysis", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.838021993637085}]}], "abstractContent": [{"text": "Multimodal sentiment analysis is a developing area of research, which involves the identification of sentiments in videos.", "labels": [], "entities": [{"text": "Multimodal sentiment analysis", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8554817835489908}, {"text": "identification of sentiments in videos", "start_pos": 83, "end_pos": 121, "type": "TASK", "confidence": 0.859762442111969}]}, {"text": "Current research considers utterances as independent entities, i.e., ignores the inter-dependencies and relations among the utterances of a video.", "labels": [], "entities": []}, {"text": "In this paper, we propose a LSTM-based model that enables utterances to capture contextual information from their surroundings in the same video, thus aiding the classification process.", "labels": [], "entities": []}, {"text": "Our method shows 5-10% performance improvement over the state of the art and high robustness to generalizability.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis is a 'suitcase' research problem that requires tackling many NLP sub-tasks, e.g., aspect extraction (), named entity recognition (, concept extraction (, sarcasm detection (), personality recognition (, and more.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9279187321662903}, {"text": "aspect extraction", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7163156270980835}, {"text": "named entity recognition", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.6318325102329254}, {"text": "concept extraction", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.71818408370018}, {"text": "sarcasm detection", "start_pos": 173, "end_pos": 190, "type": "TASK", "confidence": 0.8105107843875885}, {"text": "personality recognition", "start_pos": 195, "end_pos": 218, "type": "TASK", "confidence": 0.8141165673732758}]}, {"text": "Sentiment analysis can be performed at different granularity levels, e.g., subjectivity detection simply classifies data as either subjective (opinionated) or objective (neutral), while polarity detection focuses on determining whether subjective data indicate positive or negative sentiment.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9363225400447845}, {"text": "subjectivity detection", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.7135798633098602}]}, {"text": "Emotion recognition further breaks down the inferred polarity into a set of emotions conveyed by the subjective data, e.g., positive sentiment can be caused by joy or anticipation, while negative sentiment can be caused by fear or disgust.", "labels": [], "entities": [{"text": "Emotion recognition", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.920698881149292}]}, {"text": "Even though the primary focus of this paper is to classify sentiment in videos, we also show the performance of the proposed method for the finergrained task of emotion recognition.", "labels": [], "entities": [{"text": "classify sentiment in videos", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.8812235593795776}, {"text": "emotion recognition", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.7172234058380127}]}, {"text": "Emotion recognition and sentiment analysis have become anew trend in social media, helping users and companies to automatically extract the opinions expressed in user-generated content, especially videos.", "labels": [], "entities": [{"text": "Emotion recognition", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.849010705947876}, {"text": "sentiment analysis", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9233576953411102}]}, {"text": "Thanks to the high availability of computers and smartphones, and the rapid rise of social media, consumers tend to record their reviews and opinions about products or films and upload them on social media platforms, such as YouTube and Facebook.", "labels": [], "entities": []}, {"text": "Such videos often contain comparisons, which can aid prospective buyers make an informed decision.", "labels": [], "entities": []}, {"text": "The primary advantage of analyzing videos over text is the surplus of behavioral cues present in vocal and visual modalities.", "labels": [], "entities": []}, {"text": "The vocal modulations and facial expressions in the visual data, along with textual data, provide important cues to better identify affective states of the opinion holder.", "labels": [], "entities": []}, {"text": "Thus, a combination of text and video data helps to create a more robust emotion and sentiment analysis model ().", "labels": [], "entities": [{"text": "emotion and sentiment analysis", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.6576492339372635}]}, {"text": "An utterance is a unit of speech bound by breathes or pauses.", "labels": [], "entities": []}, {"text": "Utterance-level sentiment analysis focuses on tagging every utterance of a video with a sentiment label (instead of assigning a unique label to the whole video).", "labels": [], "entities": [{"text": "Utterance-level sentiment analysis", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8957927028338114}]}, {"text": "In particular, utterance-level sentiment analysis is useful to understand the sentiment dynamics of different aspects of the topics covered by the speaker throughout his/her speech.", "labels": [], "entities": [{"text": "utterance-level sentiment analysis", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.8427504301071167}]}, {"text": "Recently, a number of approaches to multimodal sentiment analysis, producing interesting results, have been proposed.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.7630032300949097}]}, {"text": "However, there are major issues that remain unaddressed.", "labels": [], "entities": []}, {"text": "Not considering the relation and dependencies among the utterances is one of such issues.", "labels": [], "entities": []}, {"text": "State-of-the-art approaches in this area treat utterances independently and ignore the order of utterances in a video ( ).", "labels": [], "entities": []}, {"text": "Every utterance in a video is spoken at a distinct time and in a particular order.", "labels": [], "entities": []}, {"text": "Thus, a video can be treated as a sequence of utterances.", "labels": [], "entities": []}, {"text": "Like any other sequence classification problem, sequential utterances of a video may largely be contextually correlated and, hence, influence each other's sentiment distribution.", "labels": [], "entities": [{"text": "sequence classification", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.7532243430614471}]}, {"text": "In our paper, we give importance to the order in which utterances appear in a video.", "labels": [], "entities": []}, {"text": "We treat surrounding utterances as the context of the utterance that is aimed to be classified.", "labels": [], "entities": []}, {"text": "For example, the MOSI dataset () contains a video, in which a girl reviews the movie 'Green Hornet'.", "labels": [], "entities": [{"text": "MOSI dataset", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.9106960296630859}, {"text": "Green Hornet'", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.8879274328549703}]}, {"text": "At one point, she says \"The Green Hornet did something similar\".", "labels": [], "entities": [{"text": "The Green Hornet", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.7433656454086304}]}, {"text": "Normally, doing something similar, i.e., monotonous or repetitive might be perceived as negative.", "labels": [], "entities": []}, {"text": "However, the nearby utterances \"It engages the audience more\", \"they took anew spin on it\", \"and I just loved it\" indicate a positive context.", "labels": [], "entities": []}, {"text": "The hypothesis of the independence of tokens is quite popular in information retrieval and data mining, e.g., bag-of-words model, but it has a lot limitations.", "labels": [], "entities": [{"text": "data mining", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.7276185750961304}]}, {"text": "In this paper, we discard such an oversimplifying hypothesis and develop a framework based on long shortterm memory (LSTM) that takes a sequence of utterances as input and extracts contextual utterancelevel features.", "labels": [], "entities": []}, {"text": "The other uncovered major issues in the literature are the role of speaker-dependent versus speaker-independent models, the impact of each modality across the dataset, and generalization ability of a multimodal sentiment classifier.", "labels": [], "entities": []}, {"text": "Leaving these issues unaddressed has presented difficulties in effective comparison of different multimodal sentiment analysis methods.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 97, "end_pos": 126, "type": "TASK", "confidence": 0.6827671130498251}]}, {"text": "In this work, we address all of these issues.", "labels": [], "entities": []}, {"text": "Our model preserves the sequential order of utterances and enables consecutive utterances to share information, thus providing contextual information to the utterance-level sentiment classification process.", "labels": [], "entities": [{"text": "utterance-level sentiment classification process", "start_pos": 157, "end_pos": 205, "type": "TASK", "confidence": 0.8073777705430984}]}, {"text": "Experimental results show that the proposed framework has outperformed the state of the art on three benchmark datasets by 5-10%.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 provides a brief literature review on multimodal sentiment analysis; Section 3 describes the proposed method in detail; experimental results and discussion are shown in Section 4; finally, Section 5 concludes the paper.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.7908854881922404}]}], "datasetContent": [{"text": "MOSI The MOSI dataset () is a dataset rich in sentimental expressions where 93 people review topics in English.", "labels": [], "entities": [{"text": "MOSI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8597511053085327}, {"text": "MOSI dataset", "start_pos": 9, "end_pos": 21, "type": "DATASET", "confidence": 0.8714330196380615}]}, {"text": "The videos are segmented with each segments sentiment label scored between +3 (strong positive) to -3 (strong negative) by 5 annotators.", "labels": [], "entities": []}, {"text": "We took the average of these five annotations as the sentiment polarity and, hence, considered only two classes (positive and negative).", "labels": [], "entities": []}, {"text": "The train/validation set consists of the first 62 individuals in the dataset.", "labels": [], "entities": []}, {"text": "The test set contains opinionated videos by rest 31 speakers.", "labels": [], "entities": []}, {"text": "In particular, 1447 and 752 utterances are used in training and test, respectively.", "labels": [], "entities": []}, {"text": "MOUD This dataset contains product review videos provided by 55 persons.", "labels": [], "entities": [{"text": "MOUD This dataset", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.8012602925300598}]}, {"text": "The reviews are in Spanish (we used Google Translate API 2 to get the English transcripts).", "labels": [], "entities": []}, {"text": "The utterances are labeled to be either positive, negative or neutral.", "labels": [], "entities": []}, {"text": "However, we drop the neutral label to maintain consistency with previous work.", "labels": [], "entities": []}, {"text": "Out of 79 videos in the dataset, 59 videos are considered in the train/val set.", "labels": [], "entities": []}, {"text": "IEMOCAP The IEMOCAP ( contains the acts of 10 speakers in a twoway conversation segmented into utterances.", "labels": [], "entities": [{"text": "IEMOCAP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7495493292808533}, {"text": "IEMOCAP", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.624988317489624}]}, {"text": "The medium of the conversations in all the videos is English.", "labels": [], "entities": []}, {"text": "The database contains the following categorical labels: anger, happiness, sadness, neutral, excitement, frustration, fear, surprise, and other, but we take only the first four so as to compare with the state of the art (.", "labels": [], "entities": []}, {"text": "Videos by the first 8 speakers are considered in the training set.", "labels": [], "entities": []}, {"text": "The train/test split details are provided in, which provides information regarding train/test split of all the datasets.", "labels": [], "entities": [{"text": "train/test split", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.6167006865143776}]}, {"text": "also provides cross-dataset split details where the datasets MOSI and MOUD are used for training and testing, respectively.", "labels": [], "entities": [{"text": "MOSI", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.5099860429763794}, {"text": "MOUD", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.6354568600654602}]}, {"text": "The proposed model being used on reviews from different languages allows us to analyze its robustness and generalizability.", "labels": [], "entities": []}, {"text": "In order to evaluate the robustness of our proposed method, we employ it on multiple datasets of different kinds.", "labels": [], "entities": []}, {"text": "Both MOSI and MOUD are used for the sentiment classification task but they consist of review videos spoken in different languages, i.e., English and Spanish, respectively.", "labels": [], "entities": [{"text": "MOSI", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9752966165542603}, {"text": "MOUD", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.8721259236335754}, {"text": "sentiment classification task", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.9561355908711752}]}, {"text": "IEMOCAP dataset is different from MOSI and MOUD since it is annotated with emotion labels.", "labels": [], "entities": [{"text": "IEMOCAP dataset", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.7347253412008286}]}, {"text": "Apart from this, IEMOCAP dataset was created using a different method than MOSI and MOUD.", "labels": [], "entities": [{"text": "IEMOCAP dataset", "start_pos": 17, "end_pos": 32, "type": "DATASET", "confidence": 0.7013341933488846}, {"text": "MOSI", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.6955109238624573}, {"text": "MOUD", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.8004904985427856}]}, {"text": "These two datasets were developed by crawling consumers' spontaneous online product review videos from popular social websites and later labeled with sentiment labels.", "labels": [], "entities": []}, {"text": "To curate the IEMOCAP dataset, instead, subjects were provided affect-related scripts and asked to act.", "labels": [], "entities": [{"text": "IEMOCAP dataset", "start_pos": 14, "end_pos": 29, "type": "DATASET", "confidence": 0.7468938678503036}]}, {"text": "As pointed out by), acted dataset like IEMOCAP can suffer from biased labeling and incorrect acting which can further cause the poor generalizability of the models trained on the acted datasets.", "labels": [], "entities": [{"text": "IEMOCAP", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.7675191164016724}]}, {"text": "It should be noted that the datasets' individual configuration and splits are same throughout all the experiments (i.e., context-independent unimodal feature extraction, LSTM-based contextdependent unimodal and multimodal feature extraction and classification).", "labels": [], "entities": [{"text": "context-independent unimodal feature extraction", "start_pos": 121, "end_pos": 168, "type": "TASK", "confidence": 0.5997333899140358}, {"text": "multimodal feature extraction", "start_pos": 211, "end_pos": 240, "type": "TASK", "confidence": 0.6805435319741567}]}], "tableCaptions": [{"text": " Table 2: uttrnce: Utterance; Person-Independent Train/Test  split details of each dataset (\u2248 70/30 % split). Legenda: X\u2192Y  represents train: X and test: Y; Validation sets are extracted  from the shuffled training sets using 80/20 % train/val ratio.", "labels": [], "entities": [{"text": "Utterance", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.8934658765792847}]}, {"text": " Table 3: Comparison of models mentioned in Section 3.2.3. The table reports the accuracy of classification. Legenda: non-hier  \u2190 Non-hierarchical bc-lstm. For remaining fusion, hierarchical fusion framework is used (Section 3.3.2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9993497729301453}]}, {"text": " Table 4: Accuracy % on textual (T), visual (V), audio (A)  modality and comparison with the state of the art. For the  fusion, the hierarchical fusion framework was used.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9783491492271423}]}, {"text": " Table 5: Cross-dataset comparison in terms of classification  accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.9403419494628906}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9644995927810669}]}]}