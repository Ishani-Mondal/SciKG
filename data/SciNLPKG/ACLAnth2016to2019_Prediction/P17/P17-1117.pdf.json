{"title": [{"text": "Multi-Task Video Captioning with Video and Entailment Generation", "labels": [], "entities": [{"text": "Multi-Task Video Captioning", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6054928600788116}]}], "abstractContent": [{"text": "Video captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data.", "labels": [], "entities": [{"text": "Video captioning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.651454746723175}]}, {"text": "We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsuper-vised video prediction task to learn richer context-aware video encoder representations , and a logically-directed language entailment generation task to learn better video-entailing caption decoder representations.", "labels": [], "entities": [{"text": "video captioning", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.748290628194809}, {"text": "language entailment generation", "start_pos": 238, "end_pos": 268, "type": "TASK", "confidence": 0.6919686396916708}]}, {"text": "For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks.", "labels": [], "entities": []}, {"text": "We achieve significant improvements and the new state-of-the-art on several standard video captioning datasets using diverse automatic and human evaluations.", "labels": [], "entities": []}, {"text": "We also show mutual multi-task improvements on the entailment generation task.", "labels": [], "entities": [{"text": "entailment generation task", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.7874129414558411}]}], "introductionContent": [{"text": "Video captioning is the task of automatically generating a natural language description of the content of a video, as shown in.", "labels": [], "entities": [{"text": "Video captioning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7090406566858292}]}, {"text": "It has various applications such as assistance to a visually impaired person and improving the quality of online video search or retrieval.", "labels": [], "entities": [{"text": "online video search or retrieval", "start_pos": 106, "end_pos": 138, "type": "TASK", "confidence": 0.6749680399894714}]}, {"text": "This task has gained recent momentum in the natural language processing and computer vision communities, esp. with the advent of powerful image processing features as well as sequence-to-sequence LSTM models.", "labels": [], "entities": []}, {"text": "It A video captioning example from the YouTube2Text dataset, with the ground truth captions and our many-to-many multi-task model's predicted caption. is also a step forward from static image captioning, because in addition to modeling the spatial visual features, the model also needs to learn the temporal across-frame action dynamics and the logical storyline language dynamics.", "labels": [], "entities": [{"text": "YouTube2Text dataset", "start_pos": 39, "end_pos": 59, "type": "DATASET", "confidence": 0.9790551960468292}, {"text": "static image captioning", "start_pos": 179, "end_pos": 202, "type": "TASK", "confidence": 0.6789579093456268}]}, {"text": "Previous work in video captioning () has shown that recurrent neural networks (RNNs) area good choice for modeling the temporal information in the video.", "labels": [], "entities": [{"text": "video captioning", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7050275802612305}]}, {"text": "A sequence-to-sequence model is then used to 'translate' the video to a caption.", "labels": [], "entities": []}, {"text": "showed linguistic improvements over this by fusing the decoder with external language models.", "labels": [], "entities": []}, {"text": "Furthermore, an attention mechanism between the video frames and the caption words captures some of the temporal matching relations better ().", "labels": [], "entities": []}, {"text": "More recently, hierarchical two-level RNNs were proposed to allow for longer inputs and to model the full paragraph caption dynamics of long video clips (.", "labels": [], "entities": []}, {"text": "Despite these recent improvements, video captioning models still suffer from the lack of sufficient temporal and logical supervision to be able to correctly capture the action sequence and storydynamic language in videos, esp.", "labels": [], "entities": [{"text": "video captioning", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7416104078292847}]}, {"text": "in the case of short clips.", "labels": [], "entities": []}, {"text": "Hence, they would benefit from incorporating such complementary directed knowledge, both visual and textual.", "labels": [], "entities": []}, {"text": "We address this by jointly training the task of video captioning with two related directed-generation tasks: a temporally-directed unsupervised video prediction task and a logically-directed language entailment generation task.", "labels": [], "entities": [{"text": "video captioning", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7251603156328201}, {"text": "temporally-directed unsupervised video prediction task", "start_pos": 111, "end_pos": 165, "type": "TASK", "confidence": 0.7344897866249085}, {"text": "language entailment generation task", "start_pos": 191, "end_pos": 226, "type": "TASK", "confidence": 0.703189305961132}]}, {"text": "We model this via many-to-many multi-task learning based sequence-to-sequence models () that allow the sharing of parameters among the encoders and decoders across the three different tasks, with additional shareable attention mechanisms.", "labels": [], "entities": []}, {"text": "The unsupervised video prediction task, i.e., video-to-video generation (adapted from), shares its encoder with the video captioning task's encoder, and helps it learn richer video representations that can predict their temporal context and action sequence.", "labels": [], "entities": [{"text": "video prediction task", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.8037955164909363}, {"text": "video-to-video generation", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.7401478290557861}]}, {"text": "The entailment generation task, i.e., premise-to-entailment generation (based on the image caption domain SNLI corpus), shares its decoder with the video captioning decoder, and helps it learn better video-entailing caption representations, since the caption is essentially an entailment of the video, i.e., it describes subsets of objects and events that are logically implied by or follow from the full video content).", "labels": [], "entities": [{"text": "entailment generation", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7649014890193939}, {"text": "premise-to-entailment generation", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.7753585577011108}, {"text": "SNLI corpus", "start_pos": 106, "end_pos": 117, "type": "DATASET", "confidence": 0.7265841066837311}]}, {"text": "The overall many-tomany multi-task model combines all three tasks.", "labels": [], "entities": []}, {"text": "Our three novel multi-task models show statistically significant improvements over the state-ofthe-art, and achieve the best-reported results (and rank) on multiple datasets, based on several automatic and human evaluations.", "labels": [], "entities": []}, {"text": "We also demonstrate that video captioning, in turn, gives mutual improvements on the new multi-reference entailment generation task.", "labels": [], "entities": [{"text": "video captioning", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.6826124042272568}, {"text": "multi-reference entailment generation task", "start_pos": 89, "end_pos": 131, "type": "TASK", "confidence": 0.7429240942001343}]}], "datasetContent": [{"text": "Video Captioning Datasets We report results on three popular video captioning datasets.", "labels": [], "entities": [{"text": "Video Captioning Datasets", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.7950500249862671}]}, {"text": "First, we use the YouTube2Text or MSVD (Chen and Dolan, 2011) for our primary results, which con-2 Empirically, logical entailment helped captioning more than simple fusion with language modeling (i.e., partial sentence completion with no logical implication), because a caption also entails a video in a logically-directed sense and hence the entailment generation task matches the video captioning task better than language modeling.", "labels": [], "entities": []}, {"text": "Moreover, a multi-task setup is more suitable to add directed information such as entailment (as opposed to pretraining or fusion with only the decoder).", "labels": [], "entities": []}, {"text": "tains 1970 YouTube videos in the wild with several different reference captions per video (40 on average).", "labels": [], "entities": []}, {"text": "We also use MSR-VTT ( ) with 10, 000 diverse video clips (from a video search engine) -it has 200, 000 video clipsentence pairs and around 20 captions per video; and M-VAD ( ) with 49, 000 movie-based video clips but only 1 or 2 captions per video, making most evaluation metrics (except paraphrase-based METEOR) infeasible.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 305, "end_pos": 311, "type": "METRIC", "confidence": 0.9406329393386841}]}, {"text": "We use the standard splits for all three datasets.", "labels": [], "entities": []}, {"text": "Further details about all these datasets are provided in the supplementary.", "labels": [], "entities": []}, {"text": "Video Prediction Dataset For our unsupervised video representation learning task, we use the UCF-101 action videos dataset (, which contains 13, 320 video clips of 101 action categories, and suits our video captioning task well because it also contains short video clips of a single action or few actions.", "labels": [], "entities": [{"text": "UCF-101 action videos dataset", "start_pos": 93, "end_pos": 122, "type": "DATASET", "confidence": 0.931566447019577}, {"text": "video captioning task", "start_pos": 201, "end_pos": 222, "type": "TASK", "confidence": 0.8311730225880941}]}, {"text": "We use the standard splits -further details in supplementary.", "labels": [], "entities": []}, {"text": "Entailment Generation Dataset For the entailment generation encoder-decoder model, we use the Stanford Natural Language Inference (SNLI) corpus, which contains human-annotated English sentence pairs with classification labels of entailment, contradiction and neutral.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) corpus", "start_pos": 94, "end_pos": 143, "type": "DATASET", "confidence": 0.716047503054142}]}, {"text": "It has a total of 570, 152 sentence pairs out of which 190, 113 correspond to true entailment pairs, and we use this subset in our multi-task video captioning model.", "labels": [], "entities": [{"text": "multi-task video captioning", "start_pos": 131, "end_pos": 158, "type": "TASK", "confidence": 0.628129114707311}]}, {"text": "For improving video captioning, we use the same training/validation/test splits as provided by, which is 183, 416 training, 3, 329 validation, and 3, 368 testing pairs (for the entailment subset).", "labels": [], "entities": [{"text": "video captioning", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.7461572885513306}]}, {"text": "However, for the entailment generation multitask results (see results in Sec.", "labels": [], "entities": []}, {"text": "5.3), we modify the splits so as to create a multi-reference setup which can afford evaluation with automatic metrics.", "labels": [], "entities": []}, {"text": "A given premise usually has multiple entailed hypotheses but the original SNLI corpus is setup as single-reference (for classification).", "labels": [], "entities": [{"text": "SNLI corpus", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.8036341965198517}]}, {"text": "Due to this, the different entailed hypotheses of the same premise land up in different splits of the dataset (e.g., one in train and one in test/validation) in many cases.", "labels": [], "entities": []}, {"text": "Therefore, we regroup the premiseentailment pairs and modify the split as follows: among the 190, 113 premise-entailment pairs subset of the SNLI corpus, there are 155, 898 unique premises; out of which 145, 822 have only one hy-pothesis and we make this the training set, and the rest of them (10, 076) have more than one hypothesis, which we randomly shuffle and divide equally into test and validation sets, so that each of these two sets has approximately the same distribution of the number of reference hypotheses per premise.", "labels": [], "entities": [{"text": "SNLI corpus", "start_pos": 141, "end_pos": 152, "type": "DATASET", "confidence": 0.8458987176418304}]}, {"text": "These new validation and test sets hence contain premises with multiple entailed hypotheses as ground truth references, thus allowing for automatic metric evaluation, where differing generations still get positive scores by matching one of the multiple references.", "labels": [], "entities": [{"text": "metric evaluation", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.6785014420747757}]}, {"text": "Also, this creates a more challenging dataset for entailment generation because of zero premise overlap between the training and val/test sets.", "labels": [], "entities": [{"text": "entailment generation", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.8955788910388947}]}, {"text": "We will make these split details publicly available.", "labels": [], "entities": []}, {"text": "Pre-trained Visual Frame Features For the three video captioning and UCF-101 datasets, we fix our sampling rate to 3f ps to bring uniformity in the temporal representation of actions across all videos.", "labels": [], "entities": [{"text": "UCF-101 datasets", "start_pos": 69, "end_pos": 85, "type": "DATASET", "confidence": 0.9844744503498077}]}, {"text": "These sampled frames are then converted into features using several stateof-the-art pre-trained models on ImageNet () -VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (, and Inception-v4 ().", "labels": [], "entities": [{"text": "ImageNet", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.9268863797187805}, {"text": "VGGNet", "start_pos": 119, "end_pos": 125, "type": "DATASET", "confidence": 0.9370650053024292}]}, {"text": "Details of these feature dimensions and layer positions are in the supplementary.", "labels": [], "entities": []}, {"text": "For our video captioning as well as entailment generation results, we use four diverse automatic evaluation metrics that are popular for image/video captioning and language generation in general: METEOR, BLEU-4 (), CIDEr-D, and ROUGE-L).", "labels": [], "entities": [{"text": "video captioning", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.7343927323818207}, {"text": "entailment generation", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7133289575576782}, {"text": "image/video captioning and language generation", "start_pos": 137, "end_pos": 183, "type": "TASK", "confidence": 0.6206253809588296}, {"text": "METEOR", "start_pos": 196, "end_pos": 202, "type": "METRIC", "confidence": 0.9289732575416565}, {"text": "BLEU-4", "start_pos": 204, "end_pos": 210, "type": "METRIC", "confidence": 0.9955604076385498}, {"text": "ROUGE-L", "start_pos": 228, "end_pos": 235, "type": "METRIC", "confidence": 0.9920210242271423}]}, {"text": "Particularly, METEOR and CIDEr-D have been justified to be better for generation tasks, because CIDEr-D uses consensus among the (large) number of references and METEOR uses soft matching based on stemming, paraphrasing, and WordNet synonyms.", "labels": [], "entities": []}, {"text": "We use the standard evaluation code from the Microsoft COCO server to obtain these results and also to compare the results with previous papers.", "labels": [], "entities": [{"text": "Microsoft COCO server", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.7919754187266032}]}, {"text": "We also present human evaluation results based on relevance (i.e., how related is the generated caption w.r.t. the video contents such as actions, objects, and events; or is the generated hypothesis entailed or implied by the premise) and coherence (i.e., a score on the logic, readability, and fluency of the generated sentence).", "labels": [], "entities": []}, {"text": "In addition to the automated evaluation metrics, we present pilot-scale human evaluations on the YouTube2Text and entailment generation results.", "labels": [], "entities": [{"text": "YouTube2Text", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.9714420437812805}]}, {"text": "In each case, we compare our strongest baseline with our final multi-task model by taking a random sample of 200 generated captions (or entailed hypotheses) from the test set and removing the model identity to anonymize the two models, and ask the human evaluator to choose the better model based on relevance and coherence (described in Sec. 4.2).", "labels": [], "entities": []}, {"text": "As shown in, the multi-task models are always better than the strongest baseline for both video captioning and entailment generation, on both relevance   Given Premise Generated Entailment a man on stilts is playing a tuba for money on the boardwalk a man is playing an instrument a girl looking through a large telescope on a school trip a girl is looking at something several young people sit at a table playing poker people are playing a game the stop sign is folded up against the side of the bus the sign is not moving a blue and silver monster truck making a huge jump over crushed cars a truck is being driven: Examples of our multi-task model's generated entailment hypotheses given a premise. and coherence, and with similar improvements (2-7%) as the automatic metrics (shown in).", "labels": [], "entities": [{"text": "video captioning", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.7581261992454529}, {"text": "entailment generation", "start_pos": 111, "end_pos": 132, "type": "TASK", "confidence": 0.8236456513404846}, {"text": "Premise Generated Entailment a man on stilts is playing a tuba for money on the boardwalk a man is playing an instrument a girl looking through a large telescope on a school trip a girl is looking at something several young people sit at a table playing poker people are playing a game the stop sign", "start_pos": 160, "end_pos": 459, "type": "Description", "confidence": 0.8511103381003652}]}, {"text": "shows video captioning generation results on the YouTube2Text dataset where our final M-to-M multi-task model is compared with our strongest attention-based baseline model for three categories of videos: (a) complex examples where the multi-task model performs better than the baseline; (b) ambiguous examples (i.e., ground truth itself confusing) where multi-task model still correctly predicts one of the possible categories (c) complex examples where both models perform poorly.", "labels": [], "entities": [{"text": "video captioning generation", "start_pos": 6, "end_pos": 33, "type": "TASK", "confidence": 0.8346220056215922}, {"text": "YouTube2Text dataset", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.9776206016540527}]}, {"text": "Overall, we find that the multi-task model generates captions that are better at both temporal action prediction and logical entailment (i.e., correct subset of full video premise) w.r.t. the ground truth captions.", "labels": [], "entities": [{"text": "temporal action prediction", "start_pos": 86, "end_pos": 112, "type": "TASK", "confidence": 0.6088098784287771}]}, {"text": "The supplementary also provides ablation examples of improvements by the 1-to-M video prediction based multi-task model alone, as well as by the M-to-1 entailment based multi-task model alone (over the baseline).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Primary video captioning results on Youtube2Text (MSVD)", "labels": [], "entities": [{"text": "Primary video captioning", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6460732817649841}, {"text": "Youtube2Text (MSVD)", "start_pos": 46, "end_pos": 65, "type": "DATASET", "confidence": 0.8826483935117722}]}, {"text": " Table 2: Results on MSR-VTT dataset on the 4 metrics.  Results are reimplementations as per Xu et al. (2016).  We also report the top 3 leaderboard systems -our model  achieves the new rank 1 based on their ranking method.", "labels": [], "entities": [{"text": "MSR-VTT dataset", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.8880595564842224}]}, {"text": " Table 3: Results on M-VAD dataset.", "labels": [], "entities": [{"text": "M-VAD dataset", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.8989264965057373}]}, {"text": " Table 5: Human evaluation on captioning and entailment.", "labels": [], "entities": [{"text": "captioning", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.9673168063163757}]}]}