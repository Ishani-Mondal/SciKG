{"title": [], "abstractContent": [{"text": "Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering.", "labels": [], "entities": [{"text": "document classification", "start_pos": 114, "end_pos": 137, "type": "TASK", "confidence": 0.7449221611022949}, {"text": "machine translation", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.722980871796608}, {"text": "question answering", "start_pos": 174, "end_pos": 192, "type": "TASK", "confidence": 0.7137243002653122}]}, {"text": "Despite their promise, many recurrent models have to read the whole text word byword, making it slow to handle long documents.", "labels": [], "entities": []}, {"text": "For example, it is difficult to use a recurrent network to read a book and answer questions about it.", "labels": [], "entities": []}, {"text": "In this paper, we present an approach of reading text while skipping irrelevant information if needed.", "labels": [], "entities": []}, {"text": "The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text.", "labels": [], "entities": []}, {"text": "We employ a standard policy gradient method to train the model to make discrete jumping decisions.", "labels": [], "entities": []}, {"text": "In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.", "labels": [], "entities": [{"text": "number prediction", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.9155744910240173}, {"text": "sentiment analysis", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.9719860255718231}, {"text": "news article classification", "start_pos": 92, "end_pos": 119, "type": "TASK", "confidence": 0.598264475663503}, {"text": "Q&A", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.7573803663253784}, {"text": "accuracy", "start_pos": 290, "end_pos": 298, "type": "METRIC", "confidence": 0.9985394477844238}]}], "introductionContent": [{"text": "The last few years have seen much success of applying neural networks to many important applications in natural language processing, e.g., partof-speech tagging, chunking, named entity recognition), sentiment analysis, document classification, machine translation; Sutskever * Most of work was done when AWY was with Google.", "labels": [], "entities": [{"text": "partof-speech tagging", "start_pos": 139, "end_pos": 160, "type": "TASK", "confidence": 0.8101363778114319}, {"text": "named entity recognition", "start_pos": 172, "end_pos": 196, "type": "TASK", "confidence": 0.6368725895881653}, {"text": "sentiment analysis", "start_pos": 199, "end_pos": 217, "type": "TASK", "confidence": 0.955309122800827}, {"text": "document classification", "start_pos": 219, "end_pos": 242, "type": "TASK", "confidence": 0.7907552421092987}, {"text": "machine translation", "start_pos": 244, "end_pos": 263, "type": "TASK", "confidence": 0.794657975435257}, {"text": "Sutskever", "start_pos": 265, "end_pos": 274, "type": "TASK", "confidence": 0.8243192434310913}, {"text": "AWY", "start_pos": 304, "end_pos": 307, "type": "DATASET", "confidence": 0.8979824185371399}]}, {"text": "et al.,;, conversational/dialogue modeling (, document summarization (), parsing and automatic question answering (Q&A) ().", "labels": [], "entities": [{"text": "conversational/dialogue modeling", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.67202328145504}, {"text": "document summarization", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.649374470114708}, {"text": "question answering (Q&A)", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.7499408679349082}]}, {"text": "An important characteristic of all these models is that they read all the text available to them.", "labels": [], "entities": []}, {"text": "While it is essential for certain applications, such as machine translation, this characteristic also makes it slow to apply these models to scenarios that have long input text, such as document classification or automatic Q&A.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.8035646975040436}, {"text": "document classification", "start_pos": 186, "end_pos": 209, "type": "TASK", "confidence": 0.7848930656909943}, {"text": "Q&A", "start_pos": 223, "end_pos": 226, "type": "TASK", "confidence": 0.8253289858500162}]}, {"text": "However, the fact that texts are usually written with redundancy inspires us to think about the possibility of reading selectively.", "labels": [], "entities": []}, {"text": "In this paper, we consider the problem of understanding documents with partial reading, and propose a modification to the basic neural architectures that allows them to read input text with skipping.", "labels": [], "entities": []}, {"text": "The main benefit of this approach is faster inference because it skips irrelevant information.", "labels": [], "entities": []}, {"text": "An unexpected benefit of this approach is that it also helps the models generalize better.", "labels": [], "entities": []}, {"text": "In our approach, the model is a recurrent network, which learns to predict the number of jumping steps after it reads one or several input tokens.", "labels": [], "entities": []}, {"text": "Such a discrete model is therefore not fully differentiable, but it can be trained by a standard policy gradient algorithm, where the reward can be the accuracy or its proxy during training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9986262321472168}]}, {"text": "In our experiments, we use the basic LSTM recurrent networks) as the base model and benchmark the proposed algorithm on a range of document classification or reading comprehension tasks, using various datasets such as Rotten Tomatoes (Pang: A synthetic example of the proposed model to process a text document.", "labels": [], "entities": [{"text": "document classification", "start_pos": 131, "end_pos": 154, "type": "TASK", "confidence": 0.7481866776943207}, {"text": "Rotten Tomatoes", "start_pos": 218, "end_pos": 233, "type": "DATASET", "confidence": 0.9218564331531525}, {"text": "Pang", "start_pos": 235, "end_pos": 239, "type": "DATASET", "confidence": 0.8387619853019714}]}, {"text": "In this example, the maximum size of jump K is 5, the number of tokens read before a jump R is 2 and the number of jumps allowed N is 10.", "labels": [], "entities": []}, {"text": "The green softmax are for jumping predictions.", "labels": [], "entities": [{"text": "jumping predictions", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.9671690165996552}]}, {"text": "The processing stops if a) the jumping softmax predicts a 0 or b) the jump times exceeds N or c) the network processed the last token.", "labels": [], "entities": []}, {"text": "We only show the case a) in this and), IMDB (Maas et al., 2011), AG News () and Children's Book Test.", "labels": [], "entities": [{"text": "IMDB (Maas et al., 2011)", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.8203848972916603}, {"text": "AG News", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9349032938480377}, {"text": "Children's Book Test", "start_pos": 80, "end_pos": 100, "type": "DATASET", "confidence": 0.9564324468374252}]}, {"text": "We find that the proposed approach of selective reading speeds up the base model by two to six times.", "labels": [], "entities": []}, {"text": "Surprisingly, we also observe our model beats the standard LSTM in terms of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9987861514091492}]}, {"text": "In summary, the main contribution of our work is to design an architecture that learns to skim text and show that it is both faster and more accurate in practical applications of text processing.", "labels": [], "entities": []}, {"text": "Our model is simple and flexible enough that we anticipate it would be able to incorporate to recurrent nets with more sophisticated structures to achieve even better performance in the future.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present our empirical studies to understand the efficiency of the proposed model in reading text.", "labels": [], "entities": []}, {"text": "The tasks under experimentation are: synthetic number prediction, sentiment analysis, news topic classification and automatic question answering.", "labels": [], "entities": [{"text": "synthetic number prediction", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6138145426909128}, {"text": "sentiment analysis", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.9747484028339386}, {"text": "news topic classification", "start_pos": 86, "end_pos": 111, "type": "TASK", "confidence": 0.6758945882320404}, {"text": "question answering", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7156046777963638}]}, {"text": "Those, except the first one, are representative tasks in text reading involving different sizes of datasets and various levels of text processing, from character to word and to sentence.", "labels": [], "entities": [{"text": "text reading", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.7241012006998062}]}, {"text": "Table 1 summarizes the statistics of the dataset in our experiments.", "labels": [], "entities": []}, {"text": "To exclude the potential impact of advanced models, we restrict our comparison between the vanilla LSTM (Hochreiter and Schmidhuber, 1997) and our model, which is referred to as LSTM-Jump.", "labels": [], "entities": []}, {"text": "Ina nutshell, we show that, while achieving the same or even better testing accuracy, our model is up to 6 times and 66 times faster than the baseline LSTM model in real and synthetic datasets, respectively, as we are able to selectively skip a large fraction of text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9939163327217102}]}, {"text": "In fact, the proposed model can be readily extended to other recurrent neural networks with sophisticated mechanisms such as attention and/or hierarchical structure to achieve higher accuracy than those presented below.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9954276084899902}]}, {"text": "However, this is orthogonal to the main focus of this work and would be left as an interesting future work.", "labels": [], "entities": []}, {"text": "General Experiment Settings We use the Adam optimizer) with a learning rate of 0.001 in all experiments.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 62, "end_pos": 75, "type": "METRIC", "confidence": 0.9668944776058197}]}, {"text": "We also apply gradient clipping to all the trainable variables with the threshold of 1.0.", "labels": [], "entities": []}, {"text": "The dropout rate between the LSTM layers is 0.2 and the embedding dropout rate is 0.1.", "labels": [], "entities": []}, {"text": "We repeat the notations N, K, R defined previously in, so readers can easily refer to when looking at Tables 4,5,6 and 7.", "labels": [], "entities": []}, {"text": "While K is fixed during both training and testing, we would fix Rand Nat training but vary their values during test to seethe impact of parameter changes.", "labels": [], "entities": [{"text": "Rand Nat training", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.8102932572364807}]}, {"text": "Note that N is essentially a constraint which can be relaxed.", "labels": [], "entities": []}, {"text": "Yet we prefer to enforce this constraint hereto let the model learn to read fewer tokens.", "labels": [], "entities": []}, {"text": "Finally, the reported test time is measured by running one pass of the whole test set instance by instance, and the speedup is over the base LSTM model.", "labels": [], "entities": []}, {"text": "The code is written with TensorFlow.", "labels": [], "entities": []}, {"text": "We first test whether LSTM-Jump is indeed able to learn how to jump if a very clear jumping sig-  The results of LSTM and our method, LSTMJump, are shown in.", "labels": [], "entities": []}, {"text": "The first observation is that LSTM-Jump is faster than LSTM; the longer the sequence is, the more significant speedup LSTM-Jump can gain.", "labels": [], "entities": []}, {"text": "This is because the well-trained LSTM-Jump is aware of the jumping signal at the first token and hence can directly jump to the output position to make prediction, while LSTM is agnostic to the signal and has to read the whole sequence.", "labels": [], "entities": []}, {"text": "As a result, the reading speed of LSTM-Jump is hardly affected by the length of sequence, but that of LSTM is linear with respect to length.", "labels": [], "entities": []}, {"text": "Besides, LSTM-Jump also outperforms LSTM in terms of test accuracy under all cases.", "labels": [], "entities": [{"text": "LSTM-Jump", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.7145374417304993}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9855471849441528}]}, {"text": "This is not surprising either, as LSTM has to read a large amount of tokens that are potentially not helpful and could interfere with the prediction.", "labels": [], "entities": []}, {"text": "In summary, the results indicate LSTM-Jump is able to learn to jump if the signal is clear.", "labels": [], "entities": [{"text": "LSTM-Jump", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.7077464461326599}]}, {"text": "As LSTM-Jump has shown great speedups in the synthetic dataset, we would like to understand whether it could carry this benefit to real-world data, where \"jumping\" signal is not explicit.", "labels": [], "entities": []}, {"text": "So in this section, we conduct sentiment analysis on two movie review datasets, both containing equal numbers of positive and negative reviews.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9270792603492737}, {"text": "movie review datasets", "start_pos": 57, "end_pos": 78, "type": "DATASET", "confidence": 0.6120935479799906}]}, {"text": "The first dataset is Rotten Tomatoes, which contains 10,662 documents.", "labels": [], "entities": [{"text": "Rotten Tomatoes", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.95957350730896}]}, {"text": "Since there is not a standard split, we randomly select around 80% for training, 10% for validation, and 10% for testing.", "labels": [], "entities": []}, {"text": "The average and maximum lengths of the reviews are 22 and 56 words respectively, and we pad each of them to 60.", "labels": [], "entities": []}, {"text": "We choose the pre-trained word2vec embeddings) as our fixed word embedding that we do not update this matrix during training.", "labels": [], "entities": []}, {"text": "Both LSTM-Jump and LSTM contain 2 layers, 256 hidden units and the batch size is 100.", "labels": [], "entities": []}, {"text": "As the amount of training data is small, we slightly augment the data by sampling a continuous 50-word sequence in each padded reviews as one training sample.", "labels": [], "entities": []}, {"text": "During training, we enforce LSTM-Jump to read 8 tokens before a jump (R = 8), and the maximum skipping tokens per jump is 10 (K = 10), while the number of jumps allowed is 3.", "labels": [], "entities": [{"text": "LSTM-Jump", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.7265065312385559}]}, {"text": "The testing result is reported in.", "labels": [], "entities": []}, {"text": "Ina nutshell, LSTM-Jump is always faster than LSTM under different combinations of Rand N . At the same time, the accuracy is on par with that of LSTM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9997140765190125}]}, {"text": "In particular, the combination of (R, N ) = (7, 4) even achieves slightly better accuracy than LSTM while having a 1.5x speedup.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9993554949760437}]}, {"text": "which contains 25,000 training and 25,000 testing movie reviews, where the average length of text is 240 words, much longer than that of Rotten Tomatoes.", "labels": [], "entities": []}, {"text": "We randomly set aside about 15% of training data as validation set.", "labels": [], "entities": []}, {"text": "Both LSTM-Jump and LSTM has one layer and 128 hidden units, and the batch size is 50.", "labels": [], "entities": []}, {"text": "Again, we use pretrained word2vec embeddings as initialization but they are updated during training.", "labels": [], "entities": []}, {"text": "We either pad a short sequence to 400 words or randomly select a 400-word segment from along sequence as a training example.", "labels": [], "entities": []}, {"text": "During training, we set R = 20, K = 40 and N = 5.", "labels": [], "entities": [{"text": "R", "start_pos": 24, "end_pos": 25, "type": "METRIC", "confidence": 0.9841957092285156}]}, {"text": "As shows, the result exhibits a similar trend as found in Rotten Tomatoes that LSTMJump is uniformly faster than LSTM under many settings.", "labels": [], "entities": []}, {"text": "The various (R, N ) combinations again demonstrate the trade-off between efficiency and accuracy.", "labels": [], "entities": [{"text": "efficiency", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9843965172767639}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9964428544044495}]}, {"text": "If one cares more about accuracy, then allowing LSTM-Jump to read and jump more   The result is summarized in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9992542862892151}]}, {"text": "It is interesting to see that even with skipping, LSTM-Jump is not always faster than LSTM.", "labels": [], "entities": [{"text": "skipping", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.7862526178359985}]}, {"text": "This is mainly due to the fact that the embedding size and hidden layer are both much smaller than those used previously, and accordingly the processing of a token is much faster.", "labels": [], "entities": []}, {"text": "In that case, other computation overhead such as calculating and sampling from the jump softmax might become a dominating factor of efficiency.", "labels": [], "entities": [{"text": "calculating", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9574394822120667}]}, {"text": "By this cross-task comparison, we can see that the larger the hidden unit size of recurrent neural network and the embedding are, the more speedup LSTM-Jump can gain, which is also confirmed by the task below.", "labels": [], "entities": []}, {"text": "The last task is automatic question answering, in which we aim to test the sentence level skimming of LSTM-Jump.", "labels": [], "entities": [{"text": "question answering", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7618114054203033}]}, {"text": "We benchmark on the data set Children's Book Test (CBT) (.", "labels": [], "entities": [{"text": "Children's Book Test (CBT)", "start_pos": 29, "end_pos": 55, "type": "DATASET", "confidence": 0.8281609543732235}]}, {"text": "In each document, there are 20 contiguous sentences (context) extracted from a children's book followed by a query sentence.", "labels": [], "entities": []}, {"text": "A word of the query is deleted and the task is to select the best fit for this position from 10 candidates.", "labels": [], "entities": []}, {"text": "Originally, there are four types of tasks according to the part of speech of the missing word, from which, we choose the most difficult two, i.e., the name entity (NE) and common noun (CN) as our focus, since simple language models can already achieve human-level performance for the other two types . The models, LSTM or LSTM-Jump, firstly read the whole query, then the context sentences and finally output the predicted word.", "labels": [], "entities": []}, {"text": "While LSTM reads everything, our jumping model would decide how many context sentences should skip after reading one sentence.", "labels": [], "entities": []}, {"text": "Whenever a model finishes reading, the context and query are encoded in its hidden state ho , and the best answer from the candidate words has the same index that maximizes the following: where C \u2208 R 10\u00d7d is the word embedding matrix of the 10 candidates and W \u2208 R d\u00d7hidden size is a trainable weight variable.", "labels": [], "entities": []}, {"text": "Using such bilinear form to select answer basically follows the idea of , as it is shown to have good performance.", "labels": [], "entities": []}, {"text": "The task is now distilled to a classification problem of 10 classes.", "labels": [], "entities": []}, {"text": "We either truncate or pad each context sentence, such that they all have length 20.", "labels": [], "entities": []}, {"text": "The same preprocessing is applied to the query sentences except that the length is set as 30.", "labels": [], "entities": []}, {"text": "For both models, the number of layers is 2, the number of hidden units is 256 and the batch size is 32.", "labels": [], "entities": []}, {"text": "Pretrained word2vec embeddings are again used and they are not adjusted during training.", "labels": [], "entities": []}, {"text": "The maximum number of context sentences LSTM-Jump can skipper time is K = 5 while the number of total jumping is limited to N = 5.", "labels": [], "entities": []}, {"text": "We let the model jump after reading every sentence, so R = 1 (20 words).", "labels": [], "entities": [{"text": "R", "start_pos": 55, "end_pos": 56, "type": "METRIC", "confidence": 0.975207507610321}]}, {"text": "The result is reported in.", "labels": [], "entities": []}, {"text": "The performance of LSTM-Jump is superior to LSTM in terms of both accuracy and efficiency under all settings in our experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9994574189186096}]}, {"text": "In particular, the fastest LSTM-Jump configuration achieves a remarkable 6x speedup over LSTM, while also having respectively 1.4% and 4.4% higher accuracy in Children's Book Test -Named Entity and Children's Book Test -Common Noun.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9992094039916992}, {"text": "Children's Book Test -Named Entity", "start_pos": 159, "end_pos": 193, "type": "DATASET", "confidence": 0.8538803202765328}, {"text": "Children's Book Test -Common Noun", "start_pos": 198, "end_pos": 231, "type": "DATASET", "confidence": 0.8848834889275687}]}, {"text": "The dominant performance of LSTM-Jump over LSTM might be interpreted as follows.", "labels": [], "entities": []}, {"text": "After reading the query, both LSTM and LSTM-Jump know what the question is.", "labels": [], "entities": []}, {"text": "However, LSTM still has to process the remaining 20 sentences and thus at the very end of the last sentence, the long dependency between the question and output might become weak that the prediction is hampered.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.4574556350708008}]}, {"text": "On the contrary, the question can guide LSTM-Jump on how to read selectively and stop early when the answer is clear.", "labels": [], "entities": []}, {"text": "Therefore, when it comes to the output stage, the \"memory\" is both fresh and uncluttered that a more accurate answer is likely to be picked.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Task and dataset statistics.", "labels": [], "entities": []}, {"text": " Table 3: Testing accuracy and time of synthetic  number prediction problem. The jumping level is  number.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9976983666419983}, {"text": "time", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9599106311798096}, {"text": "synthetic  number prediction", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.6324158410231272}]}, {"text": " Table 4. In a  nutshell, LSTM-Jump is always faster than LSTM  under different combinations of R and N . At the  same time, the accuracy is on par with that of  LSTM. In particular, the combination of (R, N ) =  (7, 4) even achieves slightly better accuracy than  LSTM while having a 1.5x speedup.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9996678829193115}, {"text": "accuracy", "start_pos": 250, "end_pos": 258, "type": "METRIC", "confidence": 0.998573899269104}]}, {"text": " Table 4: Testing time and accuracy on the Rotten  Tomatoes review classification dataset. The max- imum size of jumping K is set to 10 for all the  settings. The jumping level is word.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9997087121009827}, {"text": "Rotten  Tomatoes review classification dataset", "start_pos": 43, "end_pos": 89, "type": "DATASET", "confidence": 0.9482679963111877}, {"text": "max- imum size", "start_pos": 95, "end_pos": 109, "type": "METRIC", "confidence": 0.9425040185451508}]}, {"text": " Table 5: Testing time and accuracy on the IMDB  sentiment analysis dataset. The maximum size of  jumping K is set to 40 for all the settings. The  jumping level is word.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9996131062507629}, {"text": "IMDB  sentiment analysis dataset", "start_pos": 43, "end_pos": 75, "type": "DATASET", "confidence": 0.8279113471508026}]}, {"text": " Table 6. It is inter- esting to see that even with skipping, LSTM-Jump", "labels": [], "entities": [{"text": "skipping", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9201080799102783}, {"text": "LSTM-Jump", "start_pos": 62, "end_pos": 71, "type": "TASK", "confidence": 0.5082719922065735}]}, {"text": " Table 6: Testing time and accuracy on the AG  news classification dataset. The maximum size of  jumping K is set to 40 for all the settings. The  jumping level is character.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9996447563171387}, {"text": "AG  news classification dataset", "start_pos": 43, "end_pos": 74, "type": "DATASET", "confidence": 0.8867953270673752}]}, {"text": " Table 7. The perfor- mance of LSTM-Jump is superior to LSTM in  terms of both accuracy and efficiency under all set- tings in our experiments. In particular, the fastest  LSTM-Jump configuration achieves a remarkable  6x speedup over LSTM, while also having respec- tively 1.4% and 4.4% higher accuracy in Chil- dren's Book Test -Named Entity and Children's  Book Test -Common Noun.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.999359667301178}, {"text": "accuracy", "start_pos": 295, "end_pos": 303, "type": "METRIC", "confidence": 0.9990392923355103}, {"text": "Chil- dren's Book Test -Named Entity", "start_pos": 307, "end_pos": 343, "type": "DATASET", "confidence": 0.8390884333186679}, {"text": "Children's  Book Test -Common Noun", "start_pos": 348, "end_pos": 382, "type": "DATASET", "confidence": 0.8765033653804234}]}, {"text": " Table 7: Testing time and accuracy on the Chil- dren's Book Test dataset. The maximum size of  jumping K is set to 5 for all the settings. The  jumping level is sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9996259212493896}, {"text": "Chil- dren's Book Test dataset", "start_pos": 43, "end_pos": 73, "type": "DATASET", "confidence": 0.8888876778738839}]}]}