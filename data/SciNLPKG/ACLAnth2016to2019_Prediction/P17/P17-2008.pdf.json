{"title": [{"text": "Incorporating Uncertainty into Deep Learning for Spoken Language Assessment", "labels": [], "entities": [{"text": "Spoken Language Assessment", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.8944234649340311}]}], "abstractContent": [{"text": "There is a growing demand for automatic assessment of spoken English proficiency.", "labels": [], "entities": [{"text": "assessment of spoken English proficiency", "start_pos": 40, "end_pos": 80, "type": "TASK", "confidence": 0.6898029863834381}]}], "introductionContent": [{"text": "Systems for automatic assessment of spontaneous spoken language proficiency ( are becoming increasingly important to meet the demand for English second language learning.", "labels": [], "entities": [{"text": "assessment of spontaneous spoken language proficiency", "start_pos": 22, "end_pos": 75, "type": "TASK", "confidence": 0.668947751323382}]}, {"text": "Such systems are able to provide throughput and consistency which are unachievable with human examiners.", "labels": [], "entities": [{"text": "consistency", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9885134100914001}]}, {"text": "This is a challenging task.", "labels": [], "entities": []}, {"text": "There is a large vari- ation in the quality of spoken English across all proficiency levels.", "labels": [], "entities": []}, {"text": "In addition, candidates of the same skill level will have different accents, voices, mispronunciations, and sentence construction errors.", "labels": [], "entities": [{"text": "sentence construction", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.7053326070308685}]}, {"text": "All of which are heavily influenced by the candidate's L1 language and compounded by ASR errors.", "labels": [], "entities": [{"text": "ASR", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.6813456416130066}]}, {"text": "It is therefore impossible in practice to observe all these variants in training.", "labels": [], "entities": []}, {"text": "At test time, the predicted grade's validity will decrease the more the candidate is mismatched to the data used to train the system.", "labels": [], "entities": [{"text": "validity", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.968568742275238}]}, {"text": "For deployment of these systems to high-stakes tests the performance on all candidates needs to be consistent and highly correlated with human graders.", "labels": [], "entities": []}, {"text": "To achieve this it is important that these systems can detect outlier speakers who need to be examined by, for example, human graders.", "labels": [], "entities": []}, {"text": "Previously, separate models were used to filter out \"non-scorable\" candidates ().", "labels": [], "entities": []}, {"text": "However, such models reject candidates based on whether they can be scored at all, rather than an automatic grader's uncertainty 1 in its predictions.", "labels": [], "entities": [{"text": "automatic grader's uncertainty 1", "start_pos": 98, "end_pos": 130, "type": "METRIC", "confidence": 0.732721495628357}]}, {"text": "It was shown by van that Gaussian Process (GP) graders give state-of-the-art performance for automatic assessment and yield meaningful uncertainty estimates for rejection of candidates.", "labels": [], "entities": []}, {"text": "There are, however, computational constraints on training set sizes for GPs.", "labels": [], "entities": []}, {"text": "In contrast, Deep Neural Networks (DNNs) are able to scale to large data sets, but lack a native measure of uncertainty.", "labels": [], "entities": []}, {"text": "However, have shown that Monte-Carlo Dropout (MCD) can be used to derive an uncertainty estimate fora DNN.", "labels": [], "entities": []}, {"text": "Alternatively, a Deep Density Network (DDN), which is a Mixture Density Network with only one mixture component, maybe used to yield a mean and variance corresponding to the predicted grade and the uncertainty in the prediction.", "labels": [], "entities": []}, {"text": "Similar to GP and DNNs with MCD, a standard DDN provides an implicit modelling of uncertainty in its prediction.", "labels": [], "entities": []}, {"text": "This implicit model may not be optimal for the task at hand.", "labels": [], "entities": []}, {"text": "Hence, a novel approach to explicitly model uncertainty is proposed in which the DDN is trained in a multitask fashion to model a low variance real data distribution and a high variance artificial data distribution which represents candidates with unseen characteristics.", "labels": [], "entities": []}], "datasetContent": [{"text": "As previously stated, the operating scenario is to use a model's estimate of the uncertainty in its prediction to reject candidates to be assessed by human graders for high-stakes tests, maximizing the increase in performance while rejecting the least number of candidates.", "labels": [], "entities": []}, {"text": "The rejection process is illustrated using a rejection plot.", "labels": [], "entities": [{"text": "rejection", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9478302001953125}]}, {"text": "As the rejection fraction is increased, model predictions are replaced with human scores in some particular order, increasing overall correlation with human graders. has 3 curves representing different orderings: expected random rejection, optimal rejection and model rejection.", "labels": [], "entities": []}, {"text": "The expected random performance curve is a straight line from the base predictive performance to 1.0, representing rejection in a random order.", "labels": [], "entities": []}, {"text": "The optimal rejection curve is constructed by rejecting predictions in order of decreasing mean square error relative to human graders.", "labels": [], "entities": []}, {"text": "A rejection curve derived from a model should sit between the random and optimal curves.", "labels": [], "entities": []}, {"text": "In this work, model rejection is in order of decreasing predicted variance.", "labels": [], "entities": [{"text": "model rejection", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.7165884077548981}]}, {"text": "The following metrics are used to assess and compare models: Pearson Correlation Coefficient (PCC) with human graders, the standard performance metric in assessment (); 10% rejection PCC, which illustrates the predictive performance at a particular operating point, i.e. rejecting 10% of candidates; and Area under a model's rejection curve (AUC).", "labels": [], "entities": [{"text": "Pearson Correlation Coefficient (PCC)", "start_pos": 61, "end_pos": 98, "type": "METRIC", "confidence": 0.9464978774388632}, {"text": "Area", "start_pos": 304, "end_pos": 308, "type": "METRIC", "confidence": 0.9935833811759949}, {"text": "rejection curve (AUC)", "start_pos": 325, "end_pos": 346, "type": "METRIC", "confidence": 0.8194220900535584}]}, {"text": "However, AUC is influenced by the base PCC of a model, making it difficult to compare the rejection performance.", "labels": [], "entities": [{"text": "AUC", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.6600497364997864}]}, {"text": "Thus, a metric independent of predictive performance is needed.", "labels": [], "entities": []}, {"text": "The proposed metric, AUC RR (Eq. 12), is the ratio of the areas under the actual (AUC var ) and optimal (AUC max ) rejection curves relative to the random rejection curve.", "labels": [], "entities": [{"text": "AUC RR", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.8935496211051941}, {"text": "AUC var ) and optimal (AUC max ) rejection", "start_pos": 82, "end_pos": 124, "type": "METRIC", "confidence": 0.7048950314521789}]}, {"text": "Ratios of 1.0 and 0.0 correspond to perfect and random rejection, respectively.", "labels": [], "entities": []}, {"text": "All experiments were done using 33-dimensional pronunciation, fluency and acoustic features derived from audio and ASR transcriptions of responses to questions from the BULATS exam ().", "labels": [], "entities": [{"text": "BULATS exam", "start_pos": 169, "end_pos": 180, "type": "DATASET", "confidence": 0.6643180400133133}]}, {"text": "The ASR system has a WER of 32% on a development set.", "labels": [], "entities": [{"text": "ASR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.7720498442649841}, {"text": "WER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9996376037597656}]}, {"text": "The training and test sets have 4300 and 224 candidates, respectively.", "labels": [], "entities": []}, {"text": "Each candidate provided a response to 21 questions, and the features used are aggregated overall 21 questions into a single feature vector.", "labels": [], "entities": []}, {"text": "The test data was graded by expert graders at Cambridge English.", "labels": [], "entities": [{"text": "Cambridge English", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.9561969339847565}]}, {"text": "These experts have inter-grader PCCs in the range 0.95-0.97.", "labels": [], "entities": []}, {"text": "Candidates are equally distributed across CEFR grade levels).", "labels": [], "entities": [{"text": "CEFR grade levels", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.8795713980992635}]}, {"text": "The input features where whitened by subtracting the mean and dividing by the standard deviation for each dimension computed on all training speakers.", "labels": [], "entities": []}, {"text": "The Adam optimizer (, dropout () regularization with a dropout keep probability of 0.6 and an exponentially decaying learning rate are used with decay factor of 0.86 per epoch, batch size 50.", "labels": [], "entities": [{"text": "dropout keep probability", "start_pos": 55, "end_pos": 79, "type": "METRIC", "confidence": 0.6938876509666443}]}, {"text": "All networks have 2 hidden layers with 180 rectified linear units (ReLU) in each layer.", "labels": [], "entities": [{"text": "rectified linear units (ReLU)", "start_pos": 43, "end_pos": 72, "type": "METRIC", "confidence": 0.876040905714035}]}, {"text": "DNN and DDN models were implemented in.", "labels": [], "entities": [{"text": "DNN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.890566349029541}]}, {"text": "Models were initialized using the Xavier Initializer (.", "labels": [], "entities": []}, {"text": "A validation set of 100 candidates was selected from the training data to tune the model and hyperparameters.", "labels": [], "entities": []}, {"text": "GPs were run using Scikit-Learn (Pedregosa et al., 2011) using a squared exponential covariance function.", "labels": [], "entities": []}, {"text": "The Gaussian Process grader, GP, is a competitive baseline (Tab. 1).", "labels": [], "entities": [{"text": "GP", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.4939449727535248}]}, {"text": "GP variance clearly yields uncertainty which is useful for rejection.", "labels": [], "entities": [{"text": "GP variance", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.6338016539812088}, {"text": "uncertainty", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9768660068511963}, {"text": "rejection", "start_pos": 59, "end_pos": 68, "type": "TASK", "confidence": 0.9871401190757751}]}, {"text": "A DNN with ReLU activation, MCD, achieves grading performance similar to the GP.", "labels": [], "entities": [{"text": "ReLU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9595789909362793}]}, {"text": "However, MCD fails to yield an informative uncertainty for rejection, with performance barely above random.", "labels": [], "entities": [{"text": "rejection", "start_pos": 59, "end_pos": 68, "type": "TASK", "confidence": 0.9690882563591003}]}, {"text": "If the tanh activation function, MCD tanh , is used instead, then a DNN is able to provide a meaningful measure of uncertainty using MCD, at the cost of grading performance.", "labels": [], "entities": []}, {"text": "It is likely that ReLU activations correspond to a GP covariance function which is not suited for rejection on this data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Grading and rejection performance", "labels": [], "entities": []}]}