{"title": [{"text": "Are you asking the right questions? Teaching Machines to Ask Clarification Questions", "labels": [], "entities": []}], "abstractContent": [{"text": "Inquiry is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions.", "labels": [], "entities": []}, {"text": "In this thesis work, we explore how can we teach machines to ask clarification questions when faced with uncertainty , a goal of increasing importance in today's automated society.", "labels": [], "entities": []}, {"text": "We do a preliminary study using data from StackEx-change, a plentiful online resource where people routinely ask clarifying questions to posts so that they can better offer assistance to the original poster.", "labels": [], "entities": []}, {"text": "We build neu-ral network models inspired by the idea of the expected value of perfect information: a good question is one whose expected answer is going to be most useful.", "labels": [], "entities": []}, {"text": "To build generalizable systems, we propose two future research directions: a template-based model and a sequence-to-sequence based neural generative model.", "labels": [], "entities": []}], "introductionContent": [{"text": "A main goal of asking questions is to fill information gaps, typically through clarification questions, which naturally occur in conversations.", "labels": [], "entities": []}, {"text": "A good question is one whose likely answer is going to be the most useful.", "labels": [], "entities": []}, {"text": "Consider the exchange in, in which an initial poster (who we'll call \"Terry\") asks for help configuring environment variables.", "labels": [], "entities": []}, {"text": "This question is underspecified and a responder (\"Parker\") asks a clarifying question \"(a) What version of Ubuntu do you have?\"", "labels": [], "entities": []}, {"text": "Parker could alternatively have asked one of:  Parker should not ask (b) because it's not useful; they should not ask (c) because it's too specific and an answer of \"No\" gives little help.", "labels": [], "entities": []}, {"text": "Parker's question (a) is optimal: it is both likely to be useful, and is plausibly answerable by Terry.", "labels": [], "entities": []}, {"text": "Our goal in this work is to automate Parker.", "labels": [], "entities": [{"text": "automate Parker", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.6585493981838226}]}, {"text": "Specifically, after Terry writes their initial post, we aim to generate a clarification question so that Terry can immediately amend their post in hopes of getting faster and better replies.", "labels": [], "entities": []}, {"text": "Our work has two main contributions: 1.", "labels": [], "entities": []}, {"text": "A novel neural-network model for addressing this task that integrates the notion of expected value of perfect information ( \u00a72).", "labels": [], "entities": []}, {"text": "2. A novel dataset, derived from StackExchange, that enables us to learn a model to ask clarifying questions by looking at the types of questions people ask ( \u00a74.1).", "labels": [], "entities": []}, {"text": "1 To develop our model we take inspiration from the decision theoretic framework of the Expected Value of Perfect Information (EVPI), a measure of the value of gathering additional information.", "labels": [], "entities": []}, {"text": "In our setting, we use EVPI to calculate which question is most likely to elicit an answer that would make the post more informative.", "labels": [], "entities": []}, {"text": "Formally, for an input post p, we want to choose a question q that maximizes E a\u223cp,q [U(p+a)], where a is a hypothetical answer and U is a utility function measuring the completeness of post p if a were to be added to it.", "labels": [], "entities": []}, {"text": "To achieve this, we construct two models: an answer model, which estimates P[a | p, q], the likelihood of receiving answer a if one were to ask question q on post p; (2) a completeness model, U(p), which measures how complete a post is.", "labels": [], "entities": []}, {"text": "Given these two models, at prediction time we search over a shortlist of possible questions for that which maximizes the EVPI.", "labels": [], "entities": [{"text": "EVPI", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.7217857837677002}]}, {"text": "We are able to train these models jointly based on (p, q, a) triples that we extract automatically from StackExchange.", "labels": [], "entities": []}, {"text": "depicts how we do this using StackExchange's edit history.", "labels": [], "entities": []}, {"text": "In the figure, the initial post fails to state what version of Ubuntu is being run.", "labels": [], "entities": []}, {"text": "In response to Parker's question in the comments section, Terry, the author of the post, edits the post to answer Parker's clarification question.", "labels": [], "entities": []}, {"text": "We extract the initial post asp, question posted in the comments section as q, and edit to the original post as answer a to form our (p, q, a) triples.", "labels": [], "entities": []}, {"text": "Our results show significant improvements from using the EVPI formalism over both standard feedforward network architectures and bag-ofngrams baselines, even when our system builds on strong information retrieval scaffolding.", "labels": [], "entities": []}, {"text": "In comparison, without this scaffolding, the bag-ofngrams model outperforms the feedforward network.", "labels": [], "entities": []}, {"text": "We additionally analyze the difficulty of this task for non-expert humans.", "labels": [], "entities": []}], "datasetContent": [{"text": "StackExchange is a network of online question answering websites containing timestamped information about the posts, comments on the post and the history of the revisions made to the post.", "labels": [], "entities": []}, {"text": "Using this, we create our dataset of {post, question, answer} triples: where post is the initial unedited post, question is the comment containing a question and answer is the edit made to the post that matches the question comment . We extract a total of 37K triples from the following three domains of StackExchange: askubuntu, unix and superuser.", "labels": [], "entities": []}, {"text": "We define our task as given a post and 10 question candidates, select the correct question candidate.", "labels": [], "entities": []}, {"text": "For every post pin our dataset of (p, q, a) triples, the question q paired with p is our positive question candidate.", "labels": [], "entities": []}, {"text": "We define two approaches to generate negative question candidates: Lucene Negative Candidates: We retrieve nine question candidates using Lucene ( \u00a73.1) and Random Negative Candidates: We randomly sample nine other questions from our dataset.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 138, "end_pos": 144, "type": "DATASET", "confidence": 0.9724609851837158}]}], "tableCaptions": [{"text": " Table 1: Results of two setups 'Lucene negative candidates' and 'Random negative candidates' on askubuntu when trained", "labels": [], "entities": []}]}