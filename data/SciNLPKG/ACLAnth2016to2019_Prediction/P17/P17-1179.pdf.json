{"title": [{"text": "Adversarial Training for Unsupervised Bilingual Lexicon Induction", "labels": [], "entities": [{"text": "Bilingual Lexicon Induction", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.6630791127681732}]}], "abstractContent": [{"text": "Word embeddings are well known to capture linguistic regularities of the language on which they are trained.", "labels": [], "entities": []}, {"text": "Researchers also observe that these regularities can transfer across languages.", "labels": [], "entities": []}, {"text": "However, previous endeavors to connect separate mono-lingual word embeddings typically require cross-lingual signals as supervision, either in the form of parallel corpus or seed lexicon.", "labels": [], "entities": []}, {"text": "In this work, we show that such cross-lingual connection can actually be established without any form of supervision.", "labels": [], "entities": []}, {"text": "We achieve this end by formulating the problem as a natural adversarial game, and investigating techniques that are crucial to successful training.", "labels": [], "entities": []}, {"text": "We carryout evaluation on the unsupervised bilingual lexicon induction task.", "labels": [], "entities": [{"text": "bilingual lexicon induction task", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.7295240014791489}]}, {"text": "Even though this task appears intrinsically cross-lingual, we are able to demonstrate encouraging performance without any cross-lingual clues.", "labels": [], "entities": []}], "introductionContent": [{"text": "As word is the basic unit of a language, the betterment of its representation has significant impact on various natural language processing tasks.", "labels": [], "entities": []}, {"text": "Continuous word representations, commonly known as word embeddings, have formed the basis for numerous neural network models since their advent.", "labels": [], "entities": []}, {"text": "Their popularity results from the performance boost they bring, which should in turn be attributed to the linguistic regularities they capture ().", "labels": [], "entities": []}, {"text": "Soon following the success on monolingual tasks, the potential of word embeddings for crosslingual natural language processing has attracted much attention.", "labels": [], "entities": [{"text": "crosslingual natural language processing", "start_pos": 86, "end_pos": 126, "type": "TASK", "confidence": 0.6375017166137695}]}, {"text": "In their pioneering work, Mikolov * Corresponding author.", "labels": [], "entities": []}, {"text": "Although trained independently, the two sets of embeddings exhibit approximate isomorphism.", "labels": [], "entities": []}, {"text": "(2013a) observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages, as illustrated in.", "labels": [], "entities": []}, {"text": "This interesting finding is inline with research on human cognition (.", "labels": [], "entities": []}, {"text": "It also means a linear transformation maybe established to connect word embedding spaces, allowing word feature transfer.", "labels": [], "entities": [{"text": "word feature transfer", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.6787362496058146}]}, {"text": "This has far-reaching implication on low-resource scenarios), because word embeddings only require plain text to train, which is the most abundant form of linguistic resource.", "labels": [], "entities": []}, {"text": "However, connecting separate word embedding spaces typically requires supervision from crosslingual signals.", "labels": [], "entities": []}, {"text": "For example, use five thousand seed word translation pairs to train the linear transformation.", "labels": [], "entities": []}, {"text": "Ina recent study, Vuli\u00b4c show that at least hundreds of seed word translation pairs are needed for the model to generalize.", "labels": [], "entities": []}, {"text": "This is unfortunate for low-resource languages and domains, because data encoding cross-lingual equivalence is often expensive to obtain.", "labels": [], "entities": []}, {"text": "In this work, we aim to entirely eliminate the need for cross-lingual supervision.", "labels": [], "entities": []}, {"text": "Our approach draws inspiration from recent advances in generative adversarial networks ().", "labels": [], "entities": [{"text": "generative adversarial networks", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.9210302631060282}]}, {"text": "We first formulate our task in a fashion that naturally admits an adversarial game.", "labels": [], "entities": []}, {"text": "Then we propose three models that implement the game, and explore techniques to ensure the success of training.", "labels": [], "entities": []}, {"text": "Finally, our evaluation on the bilingual lexicon induction task reveals encouraging performance, even though this task appears formidable without any cross-lingual supervision.", "labels": [], "entities": [{"text": "bilingual lexicon induction task", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.6792831569910049}]}], "datasetContent": [{"text": "We evaluate the quality of the cross-lingual embedding transformation on the bilingual lexicon induction task.", "labels": [], "entities": [{"text": "bilingual lexicon induction task", "start_pos": 77, "end_pos": 109, "type": "TASK", "confidence": 0.7049111351370811}]}, {"text": "After a source word embedding is transformed into the target space, its M nearest target embeddings (in terms of cosine similarity) are retrieved, and compared against the entry in aground truth bilingual lexicon.", "labels": [], "entities": []}, {"text": "Performance is measured by top-M accuracy: If any of the M translations is found in the ground truth bilingual lexicon, the source word is considered to be handled correctly, and the accuracy is calculated as the percentage of correctly translated source words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.8819708228111267}, {"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9990406632423401}]}, {"text": "We generally report the harshest top-1 accuracy, unless when comparing with published figures in Section 4.4.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.6826080679893494}]}], "tableCaptions": [{"text": " Table 2: Chinese-English top-1 accuracies of the  MonoGiza baseline and our models, along with  the translation matrix (TM) and isometric align- ment (IA) methods that utilize 50 and 100 seeds.", "labels": [], "entities": [{"text": "translation matrix (TM)", "start_pos": 101, "end_pos": 124, "type": "METRIC", "confidence": 0.7750143527984619}]}, {"text": " Table 4: Top-1 accuracies (%) of the MonoGiza baseline and our approach on Spanish-English, Italian- English, Japanese-Chinese, and Turkish-English. The results for translation matrix (TM) and isometric  alignment (IA) using 50 and 100 seeds are also listed.", "labels": [], "entities": [{"text": "isometric  alignment (IA)", "start_pos": 194, "end_pos": 219, "type": "TASK", "confidence": 0.6101516485214233}]}, {"text": " Table 5: Top-1 accuracies (%) of our approach  to inducing bilingual lexica for Chinese-English  from Wikipedia and Gigaword. Also listed are  results for translation matrix (TM) and isometric  alignment (IA) using 50 and 100 seeds.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 117, "end_pos": 125, "type": "DATASET", "confidence": 0.9411377310752869}]}, {"text": " Table 6: Top-5 accuracies (%) of 5k and 10k most  frequent words in the French-English setting. The  figures for the baselines are taken from", "labels": [], "entities": [{"text": "accuracies", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.6613645553588867}]}]}