{"title": [{"text": "Word Embedding for Response-To-Text Assessment of Evidence", "labels": [], "entities": [{"text": "Response-To-Text Assessment of Evidence", "start_pos": 19, "end_pos": 58, "type": "TASK", "confidence": 0.8299193680286407}]}], "abstractContent": [{"text": "Manually grading the Response to Text Assessment (RTA) is labor intensive.", "labels": [], "entities": [{"text": "Response to Text Assessment (RTA)", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.6448041115488324}]}, {"text": "Therefore, an automatic method is being developed for scoring analytical writing when the RTA is administered in large numbers of classrooms.", "labels": [], "entities": [{"text": "scoring analytical writing", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.8420884609222412}, {"text": "RTA", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.849052906036377}]}, {"text": "Our long-term goal is to also use this scoring method to provide formative feedback to students and teachers about students' writing quality.", "labels": [], "entities": []}, {"text": "As a first step towards this goal, in-terpretable features for automatically scoring the evidence rubric of the RTA have been developed.", "labels": [], "entities": [{"text": "RTA", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.6370102167129517}]}, {"text": "In this paper, we present a simple but promising method for improving evidence scoring by employing the word embedding model.", "labels": [], "entities": [{"text": "evidence scoring", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.8808560371398926}]}, {"text": "We evaluate our method on corpora of responses written by upper elementary students.", "labels": [], "entities": []}], "introductionContent": [{"text": "In, it was noted that the 2010 Common Core State Standards emphasize the ability of young students from grades 4-8 to interpret and evaluate texts, construct logical arguments based on substantive claims, and marshal relevant evidence in support of these claims.", "labels": [], "entities": [{"text": "Common Core State Standards", "start_pos": 31, "end_pos": 58, "type": "DATASET", "confidence": 0.90965636074543}]}, {"text": "relatedly developed the Response to Text Assessment (RTA) for assessing students' analytic response-to-text writing skills.", "labels": [], "entities": [{"text": "Response to Text Assessment (RTA)", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.46818773661340984}]}, {"text": "The RTA was designed to evaluate writing skills in Analysis, Evidence, Organization, Style, and MUGS (Mechanics, Usage, Grammar, and Spelling) dimensions.", "labels": [], "entities": [{"text": "RTA", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.80545574426651}, {"text": "MUGS", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.855670690536499}]}, {"text": "To both score the RTA and provide formative feedback to students and teachers at scale, an automated RTA scoring tool is now being developed (.", "labels": [], "entities": [{"text": "RTA", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.5556633472442627}, {"text": "RTA scoring", "start_pos": 101, "end_pos": 112, "type": "TASK", "confidence": 0.8469415605068207}]}, {"text": "This paper focuses on the Evidence dimension of the RTA, which evaluates students' ability to find and use evidence from an article to support their position.", "labels": [], "entities": [{"text": "Evidence", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9074984192848206}, {"text": "RTA", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.8486228585243225}]}, {"text": "previously developed a set of interpretable features for scoring the Evidence rubric of RTA.", "labels": [], "entities": [{"text": "Evidence rubric of RTA", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.8273079842329025}]}, {"text": "Although these features significantly improve over competitive baselines, the feature extraction approach is largely based on lexical matching and can be enhanced.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7360807806253433}]}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "First, we employ anew way of using the word embedding model to enhance the system of.", "labels": [], "entities": []}, {"text": "Second, we use word embeddings to deal with noisy data given the disparate writing skills of students at the upper elementary level.", "labels": [], "entities": []}, {"text": "In the following sections, we first present research on related topics, describe our corpora, and review the interpretable features developed by.", "labels": [], "entities": []}, {"text": "Next, we explain how we use the word embedding model for feature extraction to improve performance by addressing the limitations of prior work.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7382169961929321}]}, {"text": "Finally, we discuss the results of our experiments and present future plans.", "labels": [], "entities": []}], "datasetContent": [{"text": "We configure experiments to test several hypotheses: H1) the model with the word embedding trained on our own corpus will outperform or at least perform equally well as the baseline (denoted by Rubric) presented by.", "labels": [], "entities": []}, {"text": "H2) the model with the word embedding trained on our corpus will outperform or at least perform equally well as the model with off-the-shelf word embedding models.", "labels": [], "entities": []}, {"text": "H3) the model with word embedding trained on our own corpus will generalize better across students of different ages.", "labels": [], "entities": []}, {"text": "Note that while all models with word embeddings use the same features as the Rubric baseline, the feature extraction process was changed to allow non-exact matching via the word embeddings.", "labels": [], "entities": [{"text": "Rubric baseline", "start_pos": 77, "end_pos": 92, "type": "DATASET", "confidence": 0.8165104985237122}]}, {"text": "We stratify each corpus into 3 parts: 40% of the data are used for training the word embedding models; 20% of the data are used to select the best word embedding model and best threshold (this is the development set of our model); and another 40% of data are used for final testing.", "labels": [], "entities": []}, {"text": "For word embedding model training, we also add essays not graded by the first rater (Space has 229, M VP L has 222, M VP H has 296, and M VP ALL has 518) to 40% of the data from the corpus in order to enlarge the training corpus to get better word embedding models.", "labels": [], "entities": [{"text": "word embedding model training", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.7518959492444992}]}, {"text": "We train multiple word embedding models with different parameters, and select the best word embedding model by using the development set.", "labels": [], "entities": []}, {"text": "Two off-the-shelf word embeddings are used for comparison.", "labels": [], "entities": []}, {"text": "presented vectors that have 300 dimensions and were trained on a newspaper corpus of about 100 billion words.", "labels": [], "entities": []}, {"text": "The other is presented by and includes 400 dimensions, with the context window size of 5, 10 negative samples and subsampling.", "labels": [], "entities": []}, {"text": "We use 10 runs of 10-fold cross validation in the final testing, with Random Forest (max-depth = 5) implemented in Weka () as the classifier.", "labels": [], "entities": [{"text": "Weka", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.9630975127220154}]}, {"text": "This is the setting used by.", "labels": [], "entities": []}, {"text": "Since our corpora are imbalanced with respect to the four evidence scores being predicted, we use SMOTE oversampling method ().", "labels": [], "entities": [{"text": "SMOTE", "start_pos": 98, "end_pos": 103, "type": "METRIC", "confidence": 0.6687913537025452}]}, {"text": "This involves creating \"synthetic\" examples for minority classes.", "labels": [], "entities": []}, {"text": "We only oversample the training data.", "labels": [], "entities": []}, {"text": "All experiment performances are measured by Quadratic Weighted Kappa (QWKappa).", "labels": [], "entities": [{"text": "Quadratic Weighted Kappa (QWKappa)", "start_pos": 44, "end_pos": 78, "type": "METRIC", "confidence": 0.9418156643708547}]}], "tableCaptions": [{"text": " Table 1: The distribution of Evidence scores, and  grading agreement of two raters.", "labels": [], "entities": [{"text": "grading agreement", "start_pos": 52, "end_pos": 69, "type": "METRIC", "confidence": 0.9301492273807526}]}, {"text": " Table 2: Rubric for the Evidence dimension of RTA. The abbreviations in the parentheses identify the  corresponding feature group discussed in the Rubric Features section of this paper that is aligned with  that specific criteria (Rahimi et al., 2017).", "labels": [], "entities": [{"text": "RTA", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.8131000995635986}]}, {"text": " Table 3: The performance (QWKappa) of the off-the-shelf embeddings and embeddings trained on  our corpus compared to the rubric baseline on all corpora. The numbers in parenthesis show the model  numbers over which the current model performs significantly better. The best results in each row are in  bold.", "labels": [], "entities": [{"text": "QWKappa)", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9152147173881531}]}]}