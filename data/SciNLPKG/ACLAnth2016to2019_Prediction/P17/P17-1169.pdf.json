{"title": [{"text": "Determining Gains Acquired from Word Embedding Quantitatively Using Discrete Distribution Clustering", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings have become widely-used in document analysis.", "labels": [], "entities": [{"text": "document analysis", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.75264573097229}]}, {"text": "While a large number of models for mapping words to vector spaces have been developed, it remains undetermined how much net gain can be achieved over traditional approaches based on bag-of-words.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew document clustering approach by combining any word embedding with a state-of-the-art algorithm for clustering empirical distributions.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.6924863159656525}]}, {"text": "By using the Wasserstein distance between distributions, the word-to-word semantic relationship is taken into account in a principled way.", "labels": [], "entities": []}, {"text": "The new clustering method is easy to use and consistently outperforms other methods on a variety of data sets.", "labels": [], "entities": []}, {"text": "More importantly, the method provides an effective framework for determining when and how much word embeddings contribute to document analysis.", "labels": [], "entities": [{"text": "document analysis", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.7351958006620407}]}, {"text": "Experimental results with multiple embedding models are reported.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings (a.k.a. word vectors) have been broadly adopted for document analysis.", "labels": [], "entities": [{"text": "document analysis", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7731159627437592}]}, {"text": "The embeddings can be trained from external large-scale corpus and then easily utilized for different data.", "labels": [], "entities": []}, {"text": "To a certain degree, the knowledge mined from the corpus, possibly in very intricate ways, is coded in the vector space, Correspondence should be sent to J.", "labels": [], "entities": []}, {"text": "Ye (jxy198@psu.edu) and J. Li (jiali@psu.edu).", "labels": [], "entities": []}, {"text": "The work was done when Z.", "labels": [], "entities": []}, {"text": "Wu was with Penn State.", "labels": [], "entities": [{"text": "Penn State", "start_pos": 12, "end_pos": 22, "type": "DATASET", "confidence": 0.940227210521698}]}, {"text": "the samples of which are easy to describe and ready for mathematical modeling.", "labels": [], "entities": []}, {"text": "Despite the appeal, researchers will be interested in knowing how much gain an embedding can bring forth over the performance achievable by existing bag-ofwords based approaches.", "labels": [], "entities": []}, {"text": "Moreover, how can the gain be quantified?", "labels": [], "entities": []}, {"text": "Such a preliminary evaluation will be carried out before building a sophisticated pipeline of analysis.", "labels": [], "entities": []}, {"text": "Almost every document analysis model used in practice is constructed assuming a certain basic representation-bag-of-words or word embeddings-for the sake of computational tractability.", "labels": [], "entities": [{"text": "document analysis", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7075606137514114}]}, {"text": "For example, afterword embedding is done, high-level models in the embedded space, such as entity representations, similarity measures, data manifolds, hierarchical structures, language models, and neural architectures, are designed for various tasks.", "labels": [], "entities": []}, {"text": "In order to inventor enhance analysis tools, we want to understand precisely the pros and cons of the highlevel models and the underlying representations.", "labels": [], "entities": []}, {"text": "Because the model and the representation are tightly coupled in an analytical system, it is not easy to pinpoint where the gain or loss found in practice comes from.", "labels": [], "entities": []}, {"text": "Should the gain be credited to the mechanism of the model or to the use of word embeddings?", "labels": [], "entities": []}, {"text": "As our experiments demonstrate, introducing certain assumptions will make individual methods effective only if certain constraints are met.", "labels": [], "entities": []}, {"text": "We will address this issue under an unsupervised learning framework.", "labels": [], "entities": []}, {"text": "Our proposed clustering paradigm has several advantages.", "labels": [], "entities": []}, {"text": "Instead of packing the information of a document into a fixed-length vector for subsequent analysis, we treat a document more thoroughly as a distributional entity.", "labels": [], "entities": []}, {"text": "In our approach, the distance between two empirical nonparametric measures (or discrete distributions) over the word embedding space is defined as the Wasserstein metric (a.k.a. the Earth Mover's Distance or EMD).", "labels": [], "entities": []}, {"text": "Comparing with a vector representation, an empirical distribution can represent with higher fidelity a cloud of points such as words in a document mapped to a certain space.", "labels": [], "entities": []}, {"text": "In the extreme case, the empirical distribution can beset directly as the cloud of points.", "labels": [], "entities": []}, {"text": "In contrast, a vector representation reduces data significantly, and its effectiveness relies on the assumption that the discarded information is irrelevant or nonessential to later analysis.", "labels": [], "entities": []}, {"text": "This simplification itself can cause degradation in performance, obscuring the inherent power of the word embedding space.", "labels": [], "entities": []}, {"text": "Our approach is intuitive and robust.", "labels": [], "entities": []}, {"text": "In addition to a high fidelity representation of the data, the Wasserstein distance takes into account the crossterm relationship between different words in a principled fashion.", "labels": [], "entities": []}, {"text": "According to the definition, the distance between two documents A and B are the minimum cumulative cost that words from document A need to \"travel\" to match exactly the set of words for document B.", "labels": [], "entities": []}, {"text": "Here, the travel cost of a path between two words is their (squared) Euclidean distance in the word embedding space.", "labels": [], "entities": []}, {"text": "Therefore, how much benefit the Wasserstein distance brings also depends on how well the word embedding space captures the semantic difference between words.", "labels": [], "entities": []}, {"text": "While Wasserstein distance is well suited for document analysis, a major obstacle of approaches based on this distance is the computational intensity, especially for the original D2-clustering method (.", "labels": [], "entities": [{"text": "document analysis", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.777716189622879}]}, {"text": "The main technical hurdle is to compute efficiently the Wasserstein barycenter, which is itself a discrete distribution, fora given set of discrete distributions.", "labels": [], "entities": []}, {"text": "Thanks to the recent advances in the algorithms for solving Wasserstein barycenters, one can now perform document clustering by directly treating them as empirical measures over a word embedding space.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7525806427001953}]}, {"text": "Although the computational cost is still higher than the usual vector-based clustering methods, we believe that the new clustering approach has reached a level of efficiency to justify its usage given how important it is to obtain high-quality clustering of unstructured text data.", "labels": [], "entities": []}, {"text": "For instance, clustering is a crucial step performed ahead of cross-document co-reference resolution), document summarization, retrospective events detection, and opinion mining).", "labels": [], "entities": [{"text": "cross-document co-reference resolution", "start_pos": 62, "end_pos": 100, "type": "TASK", "confidence": 0.6811100244522095}, {"text": "document summarization", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.6578452885150909}, {"text": "retrospective events detection", "start_pos": 127, "end_pos": 157, "type": "TASK", "confidence": 0.8131114641825358}, {"text": "opinion mining", "start_pos": 163, "end_pos": 177, "type": "TASK", "confidence": 0.8109199702739716}]}], "datasetContent": [{"text": "We prepare six datasets to conduct a set of experiments.", "labels": [], "entities": []}, {"text": "Two short-text datasets are created as follows.", "labels": [], "entities": []}, {"text": "(D1) BBCNews abstract: We concatenate the title and the first sentence of news posts from BBCNews dataset 1 to create an abstract version.", "labels": [], "entities": [{"text": "BBCNews", "start_pos": 5, "end_pos": 12, "type": "DATASET", "confidence": 0.9369867444038391}, {"text": "BBCNews dataset 1", "start_pos": 90, "end_pos": 107, "type": "DATASET", "confidence": 0.9826095898946127}]}, {"text": "(D2) Wiki events: Each cluster/class contains a set of news abstracts on the same story such as \"2014 Crimean Crisis\" crawled from Wikipedia current events following (; this dataset offers more challenges because it has more finegrained classes and fewer documents (with shorter length) per class than the others.", "labels": [], "entities": []}, {"text": "It also shows more realistic nature of applications such as news event clustering.", "labels": [], "entities": [{"text": "news event clustering", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.6499924858411154}]}, {"text": "We also experiment with two long-text datasets and two domain-specific text datasets.", "labels": [], "entities": []}, {"text": "(D3) Reuters-21578: We obtain the original Reuters-21578 text dataset and process as follows: remove documents with multiple categories, remove documents with empty body, remove duplicates, and select documents from the largest ten categories.", "labels": [], "entities": [{"text": "Reuters-21578", "start_pos": 5, "end_pos": 18, "type": "DATASET", "confidence": 0.8247405886650085}, {"text": "Reuters-21578 text dataset", "start_pos": 43, "end_pos": 69, "type": "DATASET", "confidence": 0.9012119174003601}]}, {"text": "Reuters dataset is a highly unbalanced dataset (the top category has more than 3,000 documents while the 10-th category has fewer than 100).", "labels": [], "entities": [{"text": "Reuters dataset", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9526031315326691}]}, {"text": "This imbalance induces some extra randomness in comparing the results.", "labels": [], "entities": []}, {"text": "(D4) 20Newsgroups \"bydate\" version: We obtain the raw \"bydate\" version and process them as follows: remove headers and footers, remove URLs and Email addresses, delete documents with less than ten words..", "labels": [], "entities": []}, {"text": "For sensitivity analysis, we use the homogeneity score as a projection dimension of other metrics, creating a 2D plot to visualize the metrics of a method along different homogeneity levels.", "labels": [], "entities": [{"text": "sensitivity analysis", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.9372752606868744}]}, {"text": "Generally speaking, more clusters leads to higher homogeneity by chance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Based on the results,  our approach is much more practical as a basic  document clustering tool.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.6896685510873795}]}, {"text": " Table 1: Percentage of total 18612 2 Wasserstein  distance pairs needed to compute on the full  Newsgroup dataset.", "labels": [], "entities": [{"text": "Newsgroup dataset", "start_pos": 97, "end_pos": 114, "type": "DATASET", "confidence": 0.9894851744174957}]}, {"text": " Table 2: Description of corpus data that have  been used in our experiments. \u21e4 Ohsumed-full  dataset is used for pre-training word embeddings  only. Ohsumed is a downsampled evaluation set  resulting from removing posts from Ohsumed-full  that belong to multiple categories.", "labels": [], "entities": [{"text": "Ohsumed-full  dataset", "start_pos": 80, "end_pos": 101, "type": "DATASET", "confidence": 0.7181526720523834}]}, {"text": " Table 3.  Our  preliminary result indicates state-of-the-art word  embeddings do not provide enough gain here to  exceed the performance of existing methodolo- gies. On the unchallenging one, the \"BBCSport\"  dataset, basic bag-of-words approaches (Tfidf and  Tfidf-N) already suffice to discriminate different  sport categories; and on the difficult one, the  \"Ohsumed\" dataset, D2-clustering only slightly  improves over Tfidf and others, ranking behind", "labels": [], "entities": [{"text": "BBCSport\"  dataset", "start_pos": 198, "end_pos": 216, "type": "DATASET", "confidence": 0.9170625408490499}, {"text": "Ohsumed\" dataset", "start_pos": 362, "end_pos": 378, "type": "DATASET", "confidence": 0.7516159216562907}]}, {"text": " Table 3: Best AMIs (Vinh et al., 2010) of compared methods on different datasets and their averaging.  The best results are marked in bold font for each dataset, the 2nd and 3rd are marked by blue and magenta  colors respectively.", "labels": [], "entities": []}, {"text": " Table 4: Comparison between random word em- beddings (upper row) and meaningful pre-trained  word embeddings (lower row), based on their best  ARI, AMI, and V-measures. The improvements  by percentiles are also shown in the subscripts.", "labels": [], "entities": [{"text": "ARI", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.9908241033554077}, {"text": "AMI", "start_pos": 149, "end_pos": 152, "type": "METRIC", "confidence": 0.8255000114440918}]}]}