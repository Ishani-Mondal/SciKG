{"title": [{"text": "Hybrid Neural Network Alignment and Lexicon Model in Direct HMM for Statistical Machine Translation", "labels": [], "entities": [{"text": "Neural Network Alignment", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.7125457922617594}, {"text": "Statistical Machine Translation", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.7815980315208435}]}], "abstractContent": [{"text": "Recently, the neural machine translation systems showed their promising performance and surpassed the phrase-based systems for most translation tasks.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.7593138019243876}, {"text": "translation tasks", "start_pos": 132, "end_pos": 149, "type": "TASK", "confidence": 0.9132807850837708}]}, {"text": "Retreating into conventional concepts machine translation while utilizing effective neural models is vital for comprehending the leap accomplished by neural machine translation over phrase-based methods.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7614153027534485}]}, {"text": "This work proposes a direct hidden Markov model (HMM) with neu-ral network-based lexicon and alignment models, which are trained jointly using the Baum-Welch algorithm.", "labels": [], "entities": []}, {"text": "The direct HMM is applied to rerank the n-best list created by a state-of-the-art phrase-based translation system and it provides improvements by up to 1.0% BLEU scores on two different translation tasks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 157, "end_pos": 161, "type": "METRIC", "confidence": 0.9989660978317261}]}], "introductionContent": [{"text": "The hidden Markov model (HMM) was first introduced to statistical machine translation for addressing the word alignment problem (.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6475111345450083}, {"text": "word alignment", "start_pos": 105, "end_pos": 119, "type": "TASK", "confidence": 0.7517651915550232}]}, {"text": "Then the HMM-based approach was widely used along with the IBM models () for aligning the source and target words.", "labels": [], "entities": []}, {"text": "In the conventional approach, the Bayes' theorem is used and the HMM is applied to the inverse translation model In this case, as apart of a noisy channel model, the marginalisation becomes intractable for every e.", "labels": [], "entities": []}, {"text": "This work proposes a novel concept focusing on direct HMM for Pr(e I 1 |f J 1 ), in which the alignment direction is from target to source positions.", "labels": [], "entities": []}, {"text": "This specific property allows us to introduce dependencies into the translation model that take the full source sentence into account.", "labels": [], "entities": []}, {"text": "This aspect will be important for the future decoder to be developed.", "labels": [], "entities": []}, {"text": "The lexicon and alignment probabilities in the HMM are modeled using feedforward neural networks (FFNN) and they are trained jointly.", "labels": [], "entities": []}, {"text": "The trained HMM is then applied for reranking the n-best lists created by a state-of-the-art open source phrase-based translation system.", "labels": [], "entities": []}, {"text": "The experiments are conducted on the IWSLT 2016 German\u2192English and BOLT Chinese\u2192English translation tasks.", "labels": [], "entities": [{"text": "IWSLT 2016 German\u2192English", "start_pos": 37, "end_pos": 62, "type": "DATASET", "confidence": 0.909176516532898}, {"text": "BOLT Chinese\u2192English translation", "start_pos": 67, "end_pos": 99, "type": "TASK", "confidence": 0.7197980642318725}]}, {"text": "The FFNNbased hybrid HMM provides improvements by up to 1.0% BLEU scores.", "labels": [], "entities": [{"text": "FFNNbased hybrid HMM", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.5944824616114298}, {"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9991739392280579}]}], "datasetContent": [{"text": "The experiments are conducted on the IWSLT 2016 German\u2192English and BOLT Chinese\u2192English translation tasks, which consist of 20M and 4M parallel sentence pairs respectively.", "labels": [], "entities": [{"text": "IWSLT 2016 German\u2192English", "start_pos": 37, "end_pos": 62, "type": "DATASET", "confidence": 0.9059459567070007}, {"text": "BOLT Chinese\u2192English translation tasks", "start_pos": 67, "end_pos": 105, "type": "TASK", "confidence": 0.7204715758562088}]}, {"text": "The feed-forward neural network alignment and lexicon models are jointly trained on the subset of about 200K sentence pairs.", "labels": [], "entities": [{"text": "feed-forward neural network alignment", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.5822649821639061}]}, {"text": "As an initial research of this topic, our new model is only applied for reranking n-best lists created by a phrase-based decoder.", "labels": [], "entities": []}, {"text": "The maximum size of the n-best lists is 500.", "labels": [], "entities": []}, {"text": "The translation quality is evaluated by case-insensitive BLEU () and TER () metrics using MultEval ).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9852650761604309}, {"text": "TER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9964821338653564}, {"text": "MultEval", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.4959678649902344}]}, {"text": "The scaling factors are tuned with MERT with BLEU as optimization criterion on the development sets.", "labels": [], "entities": [{"text": "MERT", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9951152801513672}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9989810585975647}]}, {"text": "For the translation experiments, the averaged scores are presented on the development set from three optimization runs.", "labels": [], "entities": [{"text": "translation", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9725046157836914}]}, {"text": "Our direct HMM consists of a feed-forward neural network lexicon model with following configuration: \u2022 Five one-hot input vectors for source words and three for target words \u2022 Projection layer size 100 for each word \u2022 Two non-linear hidden layers with 1000 and 500 nodes respectively \u2022 A class-factored output layer with 1000 singleton classes dedicated to the most frequent words, and 1000 classes shared among the rest of the words. and a feed-forward neural network alignment model with the same configuration as the lexicon model, except a small output layer with 201 nodes, which reflects that the aligned position can jump within the scope from \u2212100 to 100).", "labels": [], "entities": []}, {"text": "We conducted experiments on the source and target window size of both network models.", "labels": [], "entities": []}, {"text": "Larger source and target windows could not provide significant improvements on BLEU scores, at least for rescoring experiments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9918450713157654}]}, {"text": "The model is applied for reranking the n-best lists created by the Jane toolkit () with a log-linear framework containing phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features), a hierarchical reordering model (, a word class language mode and an n-gram language model.", "labels": [], "entities": [{"text": "Jane toolkit", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.9666840434074402}]}, {"text": "The word alignments used for the training of phrase-tables are generated by GIZA++, which performs the alignment training sequentially for IBM-1, HMM and IBM-4.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7037968188524246}]}, {"text": "More details about our phrase-based baseline system can be found in.", "labels": [], "entities": []}, {"text": "The experimental results are demonstrated in.", "labels": [], "entities": []}, {"text": "The rescoring experiments are conducted by adding HMM probability as feature and tuned with MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.8357585668563843}]}, {"text": "The applied attention-based neural network is a neural machine translation system similar to (.", "labels": [], "entities": []}, {"text": "The decoder and encoder word embeddings are of size 620, the encoder uses a bidirectional layer with 1000 LSTMs) to encode the source side.", "labels": [], "entities": []}, {"text": "A layer with 1000 LSTMs: Experimental results of rescoring using neural network-based direct HMM.", "labels": [], "entities": []}, {"text": "The model with sum denotes the system proposed in this work, while the model with Viterbi denotes the model with the same neural network structure, which was trained based on the word-aligned data (alignments generated by GIZA++) (.", "labels": [], "entities": []}, {"text": "Improvements by systems marked by * have a 95% statistical significance from the NN-based direct HMM (Viterbi) system, whereas \u2020 denotes the 95% statistical significant improvements with respect to the attention-based system in rescoring.", "labels": [], "entities": [{"text": "NN-based direct HMM (Viterbi) system", "start_pos": 81, "end_pos": 117, "type": "DATASET", "confidence": 0.7429243241037641}, {"text": "rescoring", "start_pos": 228, "end_pos": 237, "type": "TASK", "confidence": 0.9637888669967651}]}, {"text": "1 was used in reranking the n-best lists, while 2 denotes the stand-alone attention-based decoder..", "labels": [], "entities": []}, {"text": "During training a batch size of 50 is used.", "labels": [], "entities": []}, {"text": "More details about our neural machine translation system can be found in ( ).", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.7516236503918966}]}, {"text": "With n-best rescoring, all neural network-based systems achieve significant improvements over the phrase-based system.", "labels": [], "entities": []}, {"text": "The neural network-based HMMs provide promising performance, even with simple feed-forward neural networks.", "labels": [], "entities": []}, {"text": "The direct HMM trained by the EM procedure with marginalizing the hidden alignments outperformed the same model trained on the word-aligned data.", "labels": [], "entities": []}, {"text": "For the rescoring tasks, it provides comparable performance with the attention-based network.", "labels": [], "entities": [{"text": "rescoring tasks", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.9060238003730774}]}, {"text": "The neural network-based HMM also helps the phrase-based system achieve comparable results with the stand-alone attention-based system on the German\u2192English task.", "labels": [], "entities": []}], "tableCaptions": []}