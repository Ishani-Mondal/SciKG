{"title": [], "abstractContent": [{"text": "Though feature extraction is a necessary first step in statistical NLP, it is often seen as a mere preprocessing step.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7855250537395477}]}, {"text": "Yet, it can dominate computation time, both during training, and especially at deployment.", "labels": [], "entities": []}, {"text": "In this paper, we formalize feature extraction from an algebraic perspective.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.6962633430957794}]}, {"text": "Our for-malization allows us to define a message passing algorithm that can restructure feature templates to be more computationally efficient.", "labels": [], "entities": [{"text": "message passing", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.8013683557510376}]}, {"text": "We show via experiments on text chunking and relation extraction that this restructuring does indeed speedup feature extraction in practice by reducing redundant computation.", "labels": [], "entities": [{"text": "text chunking", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.7908383011817932}, {"text": "relation extraction", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8724946081638336}, {"text": "feature extraction", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7564571797847748}]}], "introductionContent": [{"text": "Often, the first step in building statistical NLP models involves feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7079343795776367}]}, {"text": "It is well understood that the right choice of features can substantially improve classifier performance.", "labels": [], "entities": []}, {"text": "However, from the computational point of view, the process of feature extraction is typically treated, at best as the preprocessing step of caching featurized inputs over entire datasets, and at worst, as 'somebody else's problem'.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7340775579214096}]}, {"text": "While such approaches work for training, when trained models are deployed, the computational cost of feature extraction cannot be ignored.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7644654512405396}]}, {"text": "In this paper, we present the first (to our knowledge) algebraic characterization of the process of feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.756183385848999}]}, {"text": "We formalize feature extractors as arbitrary functions that map objects (words, sentences, etc) to a vector space and show that this set forms a commutative semiring with respect to feature addition and feature conjunction.", "labels": [], "entities": []}, {"text": "An immediate consequence of the semiring characterization is a computational one.", "labels": [], "entities": []}, {"text": "Every semiring admits the Generalized Distributive Law (GDL) Algorithm) that exploits the distributive property to provide computational speedups.", "labels": [], "entities": []}, {"text": "Perhaps the most common manifestation of this algorithm in NLP is in the form of inference algorithms for factor graphs and Bayesian networks like the max-product, maxsum and sum-product algorithms (e.g.).", "labels": [], "entities": []}, {"text": "When applied to feature extractors, the GDL algorithm can refactor a feature extractor into a faster one by reducing redundant computation.", "labels": [], "entities": [{"text": "feature extractors", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7137078046798706}]}, {"text": "In this paper, we propose a junction tree construction to allow such refactoring.", "labels": [], "entities": []}, {"text": "Since the refactoring is done at the feature template level, the actual computational savings grow as classifiers encounter more examples.", "labels": [], "entities": []}, {"text": "We demonstrate the practical utility of our approach by factorizing existing feature sets for text chunking and relation extraction.", "labels": [], "entities": [{"text": "text chunking", "start_pos": 94, "end_pos": 107, "type": "TASK", "confidence": 0.7561132907867432}, {"text": "relation extraction", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8634741604328156}]}, {"text": "We show that, by reducing the number of operations performed, we can obtain significant savings in the time taken to extract features.", "labels": [], "entities": []}, {"text": "To summarize, the main contribution of this paper is the recognition that feature extractors form a commutative semiring over addition and conjunction.", "labels": [], "entities": []}, {"text": "We demonstrate a practical consequence of this characterization in the form of a mechanism for automatically refactoring any feature extractor into a faster one.", "labels": [], "entities": []}, {"text": "Finally, we show the empirical usefulness of our approach on relation extraction and text chunking tasks.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.9229331612586975}, {"text": "text chunking tasks", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.8409187197685242}]}], "datasetContent": [{"text": "We show the practical usefulness of feature function refactoring using text chunking and relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.8103662133216858}]}, {"text": "In both cases, the question we seek to evaluate empirically is: Does the feature function refactoring algorithm improve feature extraction time?", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.7334595918655396}]}, {"text": "We should point out that our goal is not to measure accuracy of prediction, but the efficiency of feature extraction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9990121126174927}, {"text": "feature extraction", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7609139382839203}]}, {"text": "Indeed, we are guaranteed that refactoring will not change accuracy; factorized feature extractors produce the same feature vectors as the original ones.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9987975358963013}]}, {"text": "In all experiments, we compare a feature extractor and its refactored variant.", "labels": [], "entities": []}, {"text": "For the factorization, we incentivized the junction tree to factor out base feature extractors that occurred most frequently in the feature extractor.", "labels": [], "entities": []}, {"text": "For both tasks, we use existing feature representations that we briefly describe.", "labels": [], "entities": []}, {"text": "We refer the reader to the original work that developed the feature representations for further details.", "labels": [], "entities": []}, {"text": "For both the original and the factorized feature extractors, we report (a) the number of additions and conjunctions at the template level, and, (b) the time for feature extraction on the entire dataset.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.7230437397956848}]}, {"text": "For the time measurements, we report average times for the original and factorized feature extractors over five paired runs to average out variations in system load.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of the original and factorized feature", "labels": [], "entities": []}, {"text": " Table 2: Comparison of the original and factorized feature", "labels": [], "entities": []}]}