{"title": [{"text": "Cross-lingual and cross-domain discourse segmentation of entire documents", "labels": [], "entities": [{"text": "cross-domain discourse segmentation of entire documents", "start_pos": 18, "end_pos": 73, "type": "TASK", "confidence": 0.7690141548713049}]}], "abstractContent": [{"text": "Discourse segmentation is a crucial step in building end-to-end discourse parsers.", "labels": [], "entities": [{"text": "Discourse segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8234285712242126}]}, {"text": "However, discourse segmenters only exist fora few languages and domains.", "labels": [], "entities": []}, {"text": "Typically they only detect intra-sentential segment boundaries, assuming gold standard sentence and token segmentation, and relying on high-quality syntactic parses and rich heuristics that are not generally available across languages and domains.", "labels": [], "entities": [{"text": "sentence and token segmentation", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.7441636919975281}]}, {"text": "In this paper, we propose statistical discourse segmenters for five languages and three domains that do not rely on gold pre-annotations.", "labels": [], "entities": [{"text": "statistical discourse segmenters", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.6254006028175354}]}, {"text": "We also consider the problem of learning discourse segmenters when no labeled data is available fora language.", "labels": [], "entities": []}, {"text": "Our fully supervised system obtains 89.5% F 1 for English newswire, with slight drops in performance on other domains , and we report supervised and un-supervised (cross-lingual) results for five languages in total.", "labels": [], "entities": [{"text": "F 1", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9961133301258087}]}], "introductionContent": [{"text": "Discourse segmentation is the first step in building a discourse parser.", "labels": [], "entities": [{"text": "Discourse segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7868309915065765}]}, {"text": "The goal is to identify the minimal units -called Elementary Discourse Units (EDU) -in the documents that will then be linked by discourse relations.", "labels": [], "entities": []}, {"text": "For example, the sentences (1a) and (1b) 1 are each segmented into two EDUs, then respectively linked by a CONTRAST and an ATTRIBUTION relation.", "labels": [], "entities": [{"text": "CONTRAST", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9728647470474243}, {"text": "ATTRIBUTION", "start_pos": 123, "end_pos": 134, "type": "METRIC", "confidence": 0.9958368539810181}]}, {"text": "The EDUs are mostly clauses and may cover a full sentence.", "labels": [], "entities": []}, {"text": "This step is crucial: making a segmentation error leads to an error in the final analysis.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.9522415399551392}]}, {"text": "Discourse segmentation can also inform other tasks, such as argumentation The examples come from the RST Discourse Treebank.", "labels": [], "entities": [{"text": "Discourse segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7917523086071014}, {"text": "RST Discourse Treebank", "start_pos": 101, "end_pos": 123, "type": "DATASET", "confidence": 0.904578685760498}]}, {"text": "mining, anaphora resolution, or speech act assignment (.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7251115590333939}, {"text": "speech act assignment", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.6797482967376709}]}, {"text": "[Such trappings suggest a glorious past] [but give no hint of a troubled present.] b. [He said] [the thrift will to get regulators to reverse the decision.]", "labels": [], "entities": []}, {"text": "We focus on the Rhetorical Structure Theory (RST) ( -and resources such as the RST Discourse Treebank (RST-DT)) -in which discourse structures are trees covering the documents.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.6945602595806122}, {"text": "RST Discourse Treebank (RST-DT))", "start_pos": 79, "end_pos": 111, "type": "DATASET", "confidence": 0.7730221400658289}]}, {"text": "Most recent works on RST discourse parsing focuses on the task of tree building, relying on a gold discourse segmentation).", "labels": [], "entities": [{"text": "RST discourse parsing", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.927328904469808}, {"text": "tree building", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.7356633245944977}]}, {"text": "However, discourse parsers' performance drops by 12-14% when relying on predicted segmentation (, underscoring the importance of discourse segmentation.", "labels": [], "entities": [{"text": "discourse parsers'", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7059978693723679}]}, {"text": "State-of-the-art performance for discourse segmentation on the RST-DT is about 91% in F 1 with predicted parses, but these systems rely on a gold segmentation of sentences and words, therefore probably overestimating performance in the wild.", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.7368110120296478}, {"text": "RST-DT", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.783765435218811}, {"text": "F 1", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9222968816757202}]}, {"text": "We propose to build discourse segmenters without making any data assumptions.", "labels": [], "entities": []}, {"text": "Specifically, rather than segmenting sentences, our systems segment documents directly.", "labels": [], "entities": []}, {"text": "Furthermore, only a few systems have been developed for languages other than English and domains other than the Wall Street Journal texts from the RST-DT.", "labels": [], "entities": [{"text": "Wall Street Journal texts from the RST-DT", "start_pos": 112, "end_pos": 153, "type": "DATASET", "confidence": 0.9560385942459106}]}, {"text": "We are the first to perform experiments across 5 languages, and 3 non-newswire English domains.", "labels": [], "entities": []}, {"text": "Since our goal is to provide a system usable for low-resource languages, we only use language-independent resources: here, the Universal Dependencies (UD) () Part-of-Speech (POS) tags, for which annotations exist for about 50 languages.", "labels": [], "entities": []}, {"text": "For the cross-lingual experiments, we also rely on crosslingual word embeddings induced from parallel data.", "labels": [], "entities": []}, {"text": "With a shared representation, we can transfer model parameters across languages, or learn models jointly through multi-task learning.", "labels": [], "entities": []}, {"text": "Contributions: We (i) propose a general statistical discourse segmenter (ii) that does not assume gold sentences and tokens, and (iii) evaluate it across 5 languages and 3 domains.", "labels": [], "entities": [{"text": "statistical discourse segmenter", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.6503478984038035}]}, {"text": "We make our code available at https://bitbucket.", "labels": [], "entities": []}, {"text": "org/chloebt/discourse.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data We use the official test sets for the En-DT (38 documents) and the Es-DT (84).", "labels": [], "entities": [{"text": "En-DT", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.6739343404769897}, {"text": "Es-DT", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9250324964523315}]}, {"text": "For the others, we randomly choose 38 documents as test set, and either keep the rest as development set (Nl-DT) or split it into a train and a development set.", "labels": [], "entities": []}, {"text": "Baselines As baselines at the document level, we report the scores obtained (a) when only considering the sentence boundaries predicted using UDPipe () (UDP-S), 8 and (b) when EDU boundaries are added after each token PoS-tagged with \"PUNCT\" (UDP-P), marking either an inter-or an intra-sentential boundary.", "labels": [], "entities": []}, {"text": "Systems As described in Section 3, our systems are either mono-lingual or mono-domain (mono), or based on a joint training across languages or domains (cross).", "labels": [], "entities": []}, {"text": "The \"mono\" systems are built for  the languages and domains represented by enough data (upper part of).", "labels": [], "entities": []}, {"text": "The \"cross\" models are trained using multi-task learning.", "labels": [], "entities": []}, {"text": "Parameters The hyper-parameters are tuned on the development set: number of iterations i \u2208 {10, 20, 30}, Gaussian noise \u03c3 \u2208 {0.1, 0.2}, and number of dimensions d \u2208 {50, 500}.", "labels": [], "entities": [{"text": "Gaussian noise \u03c3", "start_pos": 105, "end_pos": 121, "type": "METRIC", "confidence": 0.9153814117113749}]}, {"text": "We fix the number n of stacked hidden layers to 2 and the size of the hidden layers h to 100 after experimenting on the En-DT.", "labels": [], "entities": [{"text": "En-DT", "start_pos": 120, "end_pos": 125, "type": "DATASET", "confidence": 0.9367468357086182}]}, {"text": "Our final models use \u03c3 = 0.2 and d = 500.", "labels": [], "entities": []}, {"text": "Representation We use tokens and POS tags as input data.", "labels": [], "entities": []}, {"text": "The aim is to build a representation considering the current word and its context, i.e. its POS and the surrounding words/POS.", "labels": [], "entities": []}, {"text": "We use the pre-trained UDPipe models to postag the documents for all languages.", "labels": [], "entities": []}, {"text": "We experiment with randomly initialized and pre-trained cross-lingual word embeddings built on Europarl (, keeping either the full 500 dimensions, or the first 50 ones.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 95, "end_pos": 103, "type": "DATASET", "confidence": 0.9927988052368164}]}, {"text": "Results Our systems are evaluated using F 1 over the boundaries (B labels), disregarding the first word of each document.", "labels": [], "entities": [{"text": "F 1", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9692470133304596}]}, {"text": "Our scores are summarized in.", "labels": [], "entities": []}, {"text": "Our supervised, monolingual systems unsurprisingly give the best performance, with F 1 above 80%.", "labels": [], "entities": [{"text": "F 1", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.994498074054718}]}, {"text": "The results are generally linked to the size of the corpora, the larger the better.", "labels": [], "entities": []}, {"text": "Only exception is the En-SFU-DT, which, however, include more varied annotation (the authors stated that the annotations \"have not been checked for reliability\").", "labels": [], "entities": [{"text": "En-SFU-DT", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.8706473708152771}, {"text": "reliability", "start_pos": 148, "end_pos": 159, "type": "METRIC", "confidence": 0.9663696885108948}]}, {"text": "The (semi-supervised) cross-domain setting allows us to present the scores one can expect when only 25 documents are annotated fora new domain (i.e. the development set for the target domain), and to give the first results on the En-Gum-DT, but here, our model is actually outperformed by the sentence-based baseline (UDP-S).", "labels": [], "entities": []}, {"text": "The (unsupervised) cross-lingual models are generally largely better than UDPipe.", "labels": [], "entities": []}, {"text": "These are scores that one can expect when doing crosslingual transfer to build a discourse segmenter fora new language for which no annotated data are available.", "labels": [], "entities": [{"text": "crosslingual transfer", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7388058602809906}]}, {"text": "The performance is still quite high, demonstrating the coherence between the annotation schemes, and the potential of cross-lingual transfer.", "labels": [], "entities": []}, {"text": "We acknowledge that this is a small set of relatively similar Indo-European languages, however.", "labels": [], "entities": []}, {"text": "Note that the sentence-based baseline has a high precision (e.g. 96.6 on Es-DT against 59.8 for the cross-lingual system), but a much lower recall, since it mainly predicts the sentence boundaries.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.998759388923645}, {"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9994168281555176}]}, {"text": "On corpora that mostly contain sentential EDUs (e.g. Nl-DT, see), this is a good strategy.", "labels": [], "entities": []}, {"text": "Using the punctuation (UDP-P) could be a better approximation for corpora with more varied EDUs, seethe large gain for the Pt-DT and the En-Instr-DT.", "labels": [], "entities": []}, {"text": "Our scores are not directly comparable with sentence-level state-of-the-art systems (see Section 2).", "labels": [], "entities": []}, {"text": "However, for En-DT, our best system correctly identifies 950 sentence boundaries out of 991, but gets only 84.5% in F 1 for intra-sentential boundaries, 11 thus lower than the state-of-the-art (91.0%).", "labels": [], "entities": [{"text": "F 1", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9824318587779999}]}, {"text": "This is because we consider much less information, and because the system was not optimized for this task.", "labels": [], "entities": []}, {"text": "Interestingly, our simple system beats HILDA () (74.1% in F 1 ), is as good as the other neural network based system, and is close to SPADE (Soricut and Marcu, 2003) (85.2% in F 1 ) (, while all of these systems use parse tree information.", "labels": [], "entities": [{"text": "HILDA", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9084036350250244}, {"text": "SPADE", "start_pos": 134, "end_pos": 139, "type": "METRIC", "confidence": 0.7205620408058167}]}, {"text": "Finally, looking at the errors of our system on the En-DT, we found that most of them are on the tokens \"to\" (30 out of 94 not predicted as 'B') and \"and\" (24 out of 103), as expected given the annotation guidelines (see Section 3).", "labels": [], "entities": [{"text": "En-DT", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.9236662983894348}]}, {"text": "These words are highly ambiguous regarding discourse segmentation (e.g. in the test set, 42.3% of \"and\" indicates a boundary).", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.7199032455682755}]}, {"text": "We also found errors with coordinated This score ignores the sentences containing only one EDU).", "labels": [], "entities": [{"text": "EDU", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9257234930992126}]}, {"text": "verb phrases -e.g. \"[when rates are rising] [and shift out at times]\" -that should be split), a distinction hard to make without syntactic trees.", "labels": [], "entities": []}, {"text": "Finally, since we use predicted POS tags, our system learns from noisy data and makes errors due to postagging and tokenisation errors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of documents, EDUs, sentences  and words (according to UDPipe, see Section 7).", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.926116943359375}]}, {"text": " Table 2: Results (F 1 ), comparing cross-lingual  and cross-domain results with UDPipe.", "labels": [], "entities": []}]}