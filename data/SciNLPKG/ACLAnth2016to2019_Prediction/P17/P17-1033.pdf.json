{"title": [], "abstractContent": [{"text": "Language models are typically applied at the sentence level, without access to the broader document context.", "labels": [], "entities": []}, {"text": "We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence.", "labels": [], "entities": []}, {"text": "Experiments over a range of datasets demonstrate that our model out-performs a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model.", "labels": [], "entities": []}, {"text": "Our model also has the ability to generate related sentences fora topic, providing another way to interpret topics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (;.", "labels": [], "entities": []}, {"text": "A myriad of variants of the classical LDA method () have been proposed, including recent work on neural topic models (.", "labels": [], "entities": []}, {"text": "Separately, language models have long been a foundational component of any NLP task involving generation or textual normalisation of a noisy input (including speech, OCR and the processing of social media text).", "labels": [], "entities": [{"text": "generation or textual normalisation of a noisy input", "start_pos": 94, "end_pos": 146, "type": "TASK", "confidence": 0.7482513748109341}]}, {"text": "The primary purpose of a language model is to predict the probability of a span of text, traditionally at the sentence level, under the assumption that sentences are independent of one another, although recent work has started using broader local context such as the preceding sentences (.", "labels": [], "entities": []}, {"text": "In this paper, we combine the benefits of a topic model and language model in proposing a topically-driven language model, whereby we jointly learn topics and word sequence information.", "labels": [], "entities": []}, {"text": "This allows us to both sensitise the predictions of the language model to the larger document narrative using topics, and to generate topics which are better sensitised to local context and are hence more coherent and interpretable.", "labels": [], "entities": []}, {"text": "Our model has two components: a language model and a topic model.", "labels": [], "entities": []}, {"text": "We implement both components using neural networks, and train them jointly by treating each component as a sub-task in a multi-task learning setting.", "labels": [], "entities": []}, {"text": "We show that our model is superior to other language models that leverage additional context, and that the generated topics are potentially more coherent than LDA topics.", "labels": [], "entities": []}, {"text": "The architecture of the model provides an extra dimensionality of topic interpretability, in supporting the generation of sentences from a topic (or mix of topics).", "labels": [], "entities": [{"text": "topic interpretability", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.6867635101079941}]}, {"text": "It is also highly flexible, in its ability to be supervised and incorporate side information, which we show to further improve language model performance.", "labels": [], "entities": []}, {"text": "An open source implementation of our model is available at: https://github.com/jhlau/ topically-driven-language-model.", "labels": [], "entities": []}, {"text": "propose a model that learns topics and word dependencies using a Bayesian framework.", "labels": [], "entities": []}, {"text": "Word generation is driven by either LDA or an HMM.", "labels": [], "entities": [{"text": "Word generation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6544976681470871}]}, {"text": "For LDA, a word is generated based on a sampled topic in the document.", "labels": [], "entities": []}, {"text": "For the A key difference over our model is that their language model is driven by an HMM, which uses a fixed window and is therefore unable to track longrange dependencies.", "labels": [], "entities": []}, {"text": "relate the topic model view of documents and words -documents having a multinomial distribution over topics and topics having a multinomial distributional over words -from a neural network perspective by embedding these relationships in differentiable functions.", "labels": [], "entities": []}, {"text": "With that, the model lost the stochasticity and Bayesian inference of LDA but gained non-linear complex representations.", "labels": [], "entities": []}, {"text": "The authors further propose extensions to the model to do supervised learning where document labels are given. and relax the sentence independence assumption in language modelling, and use preceeding sentences as additional context.", "labels": [], "entities": [{"text": "sentence independence", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.6861417144536972}]}, {"text": "By treating words in preceeding sentences as a bag of words, use an attentional mechanism to focus on these words when predicting the next word.", "labels": [], "entities": []}, {"text": "The authors show that the incorporation of additional context helps language models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use standard language model perplexity as the evaluation metric.", "labels": [], "entities": []}, {"text": "In terms of dataset, we use doc-ument collections from 3 sources: APNEWS, IMDB and BNC.", "labels": [], "entities": [{"text": "APNEWS", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.9471428990364075}, {"text": "IMDB", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.9125160574913025}, {"text": "BNC", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.9408805966377258}]}, {"text": "APNEWS is a collection of Associated Press 5 news articles from 2009 to 2016.", "labels": [], "entities": [{"text": "APNEWS is a collection of Associated Press 5 news articles from 2009", "start_pos": 0, "end_pos": 68, "type": "DATASET", "confidence": 0.9362837125857671}]}, {"text": "IMDB is a set of movie reviews collected by.", "labels": [], "entities": [{"text": "IMDB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9402526617050171}]}, {"text": "BNC is the written portion of the British National Corpus, which contains excerpts from journals, books, letters, essays, memoranda, news and other types of text.", "labels": [], "entities": [{"text": "BNC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.4870058298110962}, {"text": "British National Corpus, which contains excerpts from journals, books, letters, essays, memoranda, news and other types of text", "start_pos": 34, "end_pos": 161, "type": "Description", "confidence": 0.8013614602386951}]}, {"text": "For APNEWS and BNC, we randomly sub-sample a set of documents for our experiments.", "labels": [], "entities": [{"text": "APNEWS", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.8165008425712585}, {"text": "BNC", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.8225807547569275}]}, {"text": "For preprocessing, we tokenise words and sentences using Stanford CoreNLP ().", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.9524795711040497}]}, {"text": "We lowercase all word tokens, filter word types that occur less than 10 times, and exclude the top 0.1% most frequent word types.", "labels": [], "entities": []}, {"text": "We additionally remove stopwords for the topic model document context.", "labels": [], "entities": []}, {"text": "All datasets are partitioned into training, development and test sets; preprocessed dataset statistics are presented in.", "labels": [], "entities": []}, {"text": "We tune hyper-parameters of tdlm based on development set language model perplexity.", "labels": [], "entities": []}, {"text": "In general, we find that optimal settings are fairly robust across collections, with the exception of m 3 , as document length is collection dependent; optimal hyper-parameter values are given in.", "labels": [], "entities": []}, {"text": "In terms of LSTM size, we explore 2 settings: a small model with 1 LSTM layer and 600 hidden units, and a large model with 2 layers and 900 hidden units.", "labels": [], "entities": []}, {"text": "For the topic number, we experiment with 50, 100 and 150 topics.", "labels": [], "entities": []}, {"text": "Word embeddings are pre-trained 300-dimension word2vec Google News vectors.", "labels": [], "entities": []}, {"text": "For comparison, we compare tdlm with: 10 vanilla-lstm: A standard LSTM language model, using the same tdlm hyper-parameters where applicable.", "labels": [], "entities": []}, {"text": "This is the baseline model.", "labels": [], "entities": []}, {"text": "attentional mechanism when predicting the next word.", "labels": [], "entities": [{"text": "predicting the next word", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.8840451538562775}]}, {"text": "An additional hyper-parameter in lclm is the number of preceeding sentences to incorporate, which we tune based on a development set (to 4 sentences in each case).", "labels": [], "entities": []}, {"text": "All other hyperparameters (such as n batch , e, n epoch , k 2 ) are the same as tdlm.", "labels": [], "entities": []}, {"text": "lstm+lda: A standard LSTM language model that incorporates LDA topic information.", "labels": [], "entities": []}, {"text": "We first train an LDA model () to learn 50/100/150 topics for APNEWS, IMDB and BNC.", "labels": [], "entities": [{"text": "APNEWS", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.9179033041000366}, {"text": "IMDB", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.8317314386367798}, {"text": "BNC", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.8188126087188721}]}, {"text": "11 For a document, the LSTM incorporates the LDA topic distribution (q) by concatenating it with the output hidden state (h t ) to predict the next word (i.e. ht = ht \u2295 q).", "labels": [], "entities": []}, {"text": "That is, it incorporates topical information into the language model, but unlike tdlm the language model and topic model are trained separately.", "labels": [], "entities": []}, {"text": "We present language model perplexity performance in.", "labels": [], "entities": []}, {"text": "All models outperform the baseline vanilla-lstm, with tdlm performing the Based on Gibbs sampling; \u03b1 = 0.1, \u03b2 = 0.01.", "labels": [], "entities": []}, {"text": "lclm is competitive over the BNC, although the superiority of tdlm for the other collections is substantial.", "labels": [], "entities": [{"text": "lclm", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.864387571811676}, {"text": "BNC", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.9288288354873657}]}, {"text": "lstm+lda performs relatively well over APNEWS and IMDB, but very poorly over BNC.", "labels": [], "entities": [{"text": "APNEWS", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.8850032687187195}, {"text": "IMDB", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.8970081210136414}, {"text": "BNC", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.9114908576011658}]}, {"text": "The strong performance of tdlm over lclm suggests that compressing document context into topics benefits language modelling more than using extra context words directly.", "labels": [], "entities": []}, {"text": "Overall, our results show that topical information can help language modelling and that joint inference of topic and language model produces the best results.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7539000809192657}]}, {"text": "We saw that tdlm performs well as a language model, but it is also a topic model, and like LDA it produces: (1) a probability distribution over topics for each document (Equation (1)); and (2) a probability distribution over word types for each topic.", "labels": [], "entities": []}, {"text": "Recall that sis a weighted mean of topic vectors fora document (Equation).", "labels": [], "entities": []}, {"text": "Generating the vocabulary distribution fora particular topic is therefore trivial: we can do so by treating s as having maximum weight (1.0) for the topic of interest, and no weight (0.0) for all other topics.", "labels": [], "entities": []}, {"text": "Let B t denote the topic output vector for the t-th topic.", "labels": [], "entities": []}, {"text": "To generate the multinomial distribution over word types for the t-th topic, we replace s with B t before computing the softmax over the vocabulary.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Language model perplexity performance of all models over APNEWS, IMDB and BNC. Boldface  indicates best performance in each row.", "labels": [], "entities": [{"text": "APNEWS", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.9184741377830505}, {"text": "BNC", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8789761066436768}]}, {"text": " Table 4: Mean topic coherence of all models over  APNEWS, IMDB and BNC. Boldface indicates the  best performance for each dataset and topic set- ting.", "labels": [], "entities": [{"text": "APNEWS", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.9389423727989197}, {"text": "IMDB", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.8906119465827942}, {"text": "BNC", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.908499538898468}]}, {"text": " Table 5: 20NEWS preprocessed statistics.", "labels": [], "entities": [{"text": "20NEWS preprocessed statistics", "start_pos": 10, "end_pos": 40, "type": "DATASET", "confidence": 0.7724147439002991}]}, {"text": " Table 6: 20NEWS classification accuracy. All  models are supervised extensions of the original  models. Boldface indicates the best performance  for each topic setting.", "labels": [], "entities": [{"text": "20NEWS classification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.5289412289857864}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9856288433074951}]}, {"text": " Table 7: Topic coherence and language model per- plexity by incorporating classification tags on AP- NEWS. Boldface indicates optimal coherence and  perplexity performance for each topic setting.", "labels": [], "entities": [{"text": "AP- NEWS", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.9180252154668173}]}]}