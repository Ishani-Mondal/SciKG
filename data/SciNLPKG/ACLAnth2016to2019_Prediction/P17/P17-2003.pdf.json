{"title": [{"text": "Lexical Features in Coreference Resolution: To be Used With Caution", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.9766364395618439}]}], "abstractContent": [{"text": "Lexical features area major source of information in state-of-the-art coreference resolvers.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.8798106014728546}]}, {"text": "Lexical features implicitly model some of the linguistic phenomena at a fine granularity level.", "labels": [], "entities": []}, {"text": "They are especially useful for representing the context of mentions.", "labels": [], "entities": []}, {"text": "In this paper we investigate a drawback of using many lexical features in state-of-the-art coreference resolvers.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.8794703483581543}]}, {"text": "We show that if coreference resolvers mainly rely on lexical features, they can hardly generalize to unseen domains.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.9689970314502716}]}, {"text": "Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.9645890891551971}]}], "introductionContent": [{"text": "Similar to many other tasks, lexical features area major source of information in current coreference resolvers.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.9212396740913391}]}, {"text": "Coreference resolution is a set partitioning problem in which each resulting partition refers to an entity.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9368530511856079}]}, {"text": "As shown by, lexical features implicitly model some linguistic phenomena, which were previously modeled by heuristic features, but at a finer level of granularity.", "labels": [], "entities": []}, {"text": "However, we question whether the knowledge that is mainly captured by lexical features can be generalized to other domains.", "labels": [], "entities": []}, {"text": "The introduction of the CoNLL dataset enabled a significant boost in the performance of coreference resolvers, i.e. about 10 percent difference between the CoNLL score of the currently best coreference resolver, deep-coref by, and the winner of the CoNLL 2011 shared task, the Stanford rule-based system by.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9506340622901917}, {"text": "coreference resolvers", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.8678171336650848}]}, {"text": "However, this substantial improvement does not seem to be visible in downstream tasks.", "labels": [], "entities": []}, {"text": "Worse, the difference between stateof-the-art coreference resolvers and the rule-based system drops significantly when they are applied on anew dataset, even with consistent definitions of mentions and coreference relations.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.8207463026046753}]}, {"text": "In this paper, we show that if we mainly rely on lexical features, as it is the casein state-of-theart coreference resolvers, overfitting become more sever.", "labels": [], "entities": [{"text": "casein state-of-theart coreference resolvers", "start_pos": 80, "end_pos": 124, "type": "TASK", "confidence": 0.6365711987018585}]}, {"text": "Overfitting to the training dataset is a problem that cannot be completely avoided.", "labels": [], "entities": [{"text": "training dataset", "start_pos": 19, "end_pos": 35, "type": "DATASET", "confidence": 0.6877145767211914}]}, {"text": "However, there is a notable overlap between the CoNLL training, development and test sets that encourages overfitting.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.7796065807342529}]}, {"text": "Therefore, the current coreference evaluation scheme is flawed by only evaluating on this overlapped validation set.", "labels": [], "entities": []}, {"text": "To ensure meaningful improvements in coreference resolution, we believe an out-of-domain evaluation is a must in the coreference literature.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.9758621752262115}]}], "datasetContent": [{"text": "Aside from the evident success of lexical features, it is debatable how well the knowledge that is mainly captured by the lexical information of the training data can be generalized to other domains.", "labels": [], "entities": []}, {"text": "As reported by, state-of-the-art coreference resolvers trained on the CoNLL dataset perform poorly, i.e. worse than the rule-based system (), on the new dataset, WikiCoref (Ghaddar and Langlais, 2016b), even though WikiCoref is annotated with the same annotation guidelines as the CoNLL dataset.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.8545084595680237}, {"text": "CoNLL dataset", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.9767528474330902}, {"text": "CoNLL dataset", "start_pos": 281, "end_pos": 294, "type": "DATASET", "confidence": 0.9557623565196991}]}, {"text": "The results of some of recent coreference resolvers on this dataset are listed in.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.9146324098110199}]}, {"text": "The results are reported using MUC.", "labels": [], "entities": [{"text": "MUC", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.7994827032089233}]}, {"text": "berkeley is the mention-ranking model of Durrett and Klein (2013) with the FINAL feature set including the head, first, last, preceding and following words of a mention, the ancestry, length, gender and number of a mention, distance of two mentions, whether the anaphor and antecedent are nested, same speaker and a small set of string match features.", "labels": [], "entities": [{"text": "FINAL", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.9366975426673889}]}, {"text": "cort is the mention-ranking model of. cort uses the following set of features: the head, first, last, preceding and following words of a mention, the ancestry, length, gender, number, type, semantic class, dependency relation and dependency governor of a mention, the named entity type of the headword, distance of two mentions, same speaker, whether the anaphor and antecedent are nested, and a set of string match features.", "labels": [], "entities": []}, {"text": "berkeley and cort scores in are taken from.", "labels": [], "entities": [{"text": "berkeley", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9799804091453552}, {"text": "cort", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9589825868606567}]}, {"text": "deep-coref is the mention-ranking model of. deep-coref incorporates a large set of embeddings, i.e. embeddings of the head, first, last, two previous/following words, and the dependency governor of a mention in addition to the averaged embeddings of the five previous/following words, all words of the mention, sentence words, and document words.", "labels": [], "entities": []}, {"text": "deep-coref also incorporates type, length, and position of a mention, whether the mention is nested in any other mention, distance of two mentions, speaker features and a small set of string match features.", "labels": [], "entities": [{"text": "length", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.927253782749176}]}, {"text": "For deep-coref [conll] the averaged CoNLL score is used to select the best trained model on the development set.", "labels": [], "entities": [{"text": "CoNLL score", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.8309778571128845}]}, {"text": "deep-coref: Ratio of non-pronominal coreferent mentions in the test set that are seen as coreferent in the training data.", "labels": [], "entities": []}, {"text": "metric for choosing the best model.", "labels": [], "entities": []}, {"text": "It is worth noting that the results of deep-coref 's ranking model maybe slightly different at various experiments.", "labels": [], "entities": []}, {"text": "However, the performance of deep-coref is always higher than that of deep-coref.", "labels": [], "entities": []}, {"text": "We add WikiCoref's words to deep-coref 's dictionary for both deep-coref and deep-coref.", "labels": [], "entities": []}, {"text": "deep-coref \u2212 reports the performance of deep-coref in which WikiCoref's words are not incorporated into the dictionary.", "labels": [], "entities": []}, {"text": "Therefore, for deep-coref \u2212 , WikiCoref's words that do not exist in CoNLL will be initialized randomly instead of using pre-trained word2vec word embeddings.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.8910260796546936}]}, {"text": "The performance gain of deep-coref in comparison to deep-coref \u2212 indicates the benefit of using pre-trained word embeddings and word embeddings in general.", "labels": [], "entities": []}, {"text": "Henceforth, we refer to deep-coref as deep-coref.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of the results on the CoNLL test set and WikiCoref.", "labels": [], "entities": [{"text": "CoNLL test set", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.9697616298993429}, {"text": "WikiCoref", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.910637617111206}]}, {"text": " Table 2: Ratio of non-pronominal coreferent men- tions in the test set that are seen as coreferent in  the training data.", "labels": [], "entities": []}, {"text": " Table 3: In-domain and out-of-domain evaluations for a high and a low overlapped genres.", "labels": [], "entities": []}, {"text": " Table 4: Ratio of links created by deep-coref for  which the head-pair is seen in the training data.", "labels": [], "entities": []}, {"text": " Table 6: Ratio of deep-coref's recall errors for  which the head-pair exists in the training data.", "labels": [], "entities": [{"text": "Ratio", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9501519203186035}, {"text": "recall errors", "start_pos": 32, "end_pos": 45, "type": "METRIC", "confidence": 0.9747268557548523}]}]}