{"title": [{"text": "How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Generative conversational systems are attracting increasing attention in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 73, "end_pos": 106, "type": "TASK", "confidence": 0.7793925901254019}]}, {"text": "Recently, researchers have noticed the importance of context information in dialog processing, and built various models to utilize context.", "labels": [], "entities": [{"text": "dialog processing", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.8901461958885193}]}, {"text": "However, there is no systematic comparison to analyze how to use context effectively.", "labels": [], "entities": []}, {"text": "In this paper, we conduct an empirical study to compare various models and investigate the effect of context information in dialog systems.", "labels": [], "entities": []}, {"text": "We also propose a variant that explicitly weights context vectors by context-query relevance, outper-forming the other baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, human-computer conversation is attracting increasing attention due to its promising potentials and alluring commercial values.", "labels": [], "entities": []}, {"text": "Researchers have proposed both retrieval methods ( and generative methods () for automatic conversational systems.", "labels": [], "entities": []}, {"text": "With the success of deep learning techniques, neural networks have demonstrated powerful capability of learning human dialog patterns; given a user-issued utterance as an input query q, the network can generate a reply r, which is usually accomplished in a sequence-to-sequence (Seq2Seq) manner.", "labels": [], "entities": []}, {"text": "In the literature, there are two typical research setups for dialog systems: single-turn and multiturn.", "labels": [], "entities": []}, {"text": "Single-turn conversation is, perhaps, the simplest setting where the model only takes q into consideration when generating r (Shang et al., 2015;.", "labels": [], "entities": []}, {"text": "However, most realworld dialogs comprise multiple turns.", "labels": [], "entities": []}, {"text": "Previous utterances (referred to as context in this paper) could also provide useful information about the dialog status and are the key to coherent multi-turn conversation.", "labels": [], "entities": []}, {"text": "Existing studies have realized the importance of context, and proposed several context-aware conversational systems.", "labels": [], "entities": []}, {"text": "For example,  directly concatenate context utterances and the current query; others use hierarchical models, first capturing the meaning of individual utterances and then integrating them as discourses ( ).", "labels": [], "entities": []}, {"text": "There could be several ways of combining context and the current query, e.g., pooling or concatenation ( . Unfortunately, previous literature lacks a systematic comparison of the above methods.", "labels": [], "entities": []}, {"text": "In this paper, we conduct an empirical study on context modeling in Seq2Seq-like conversational systems.", "labels": [], "entities": [{"text": "context modeling", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7176752686500549}]}, {"text": "We emphasize the following research questions: \u2022 RQ1.", "labels": [], "entities": [{"text": "RQ1", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.6056060194969177}]}, {"text": "How can we make better use of context information?", "labels": [], "entities": []}, {"text": "Our study shows that hierarchical models are generally better than nonhierarchical ones.", "labels": [], "entities": []}, {"text": "We also propose a variant of context integration that explicitly weights a context vector by its relevance measure, outperforming simple vector pooling or concatenation.", "labels": [], "entities": [{"text": "context integration", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7463515102863312}]}, {"text": "What is the effect of context on neural dialog systems?", "labels": [], "entities": []}, {"text": "We find context information is useful to neural conversational models.", "labels": [], "entities": []}, {"text": "It yields longer, more informative and diversified replies.", "labels": [], "entities": []}, {"text": "To sum up, the contributions of this paper are two-fold: (1) We conduct a systematic study on context modeling in neural conversational models.", "labels": [], "entities": [{"text": "context modeling", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.7197420001029968}]}, {"text": "(2) We further propose an explicitly con-text weighting approach, outperforming the other baselines.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance of different models.", "labels": [], "entities": []}, {"text": " Table 2: The length, entropy, and diversity of  the replies on the context-insensitive and context- aware (WSeq,concat) methods.", "labels": [], "entities": [{"text": "length", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9623537659645081}]}]}