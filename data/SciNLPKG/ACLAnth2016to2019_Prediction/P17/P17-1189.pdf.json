{"title": [{"text": "A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data", "labels": [], "entities": [{"text": "Chinese SRL", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.5969868302345276}]}], "abstractContent": [{"text": "Previous studies on Chinese semantic role labeling (SRL) have concentrated on a single semantically annotated corpus.", "labels": [], "entities": [{"text": "Chinese semantic role labeling (SRL)", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.7386845648288727}]}, {"text": "But the training data of single corpus is often limited.", "labels": [], "entities": []}, {"text": "Whereas the other existing semantically annotated corpora for Chi-nese SRL are scattered across different annotation frameworks.", "labels": [], "entities": []}, {"text": "But still, Data sparsity remains a bottleneck.", "labels": [], "entities": []}, {"text": "This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data.", "labels": [], "entities": []}, {"text": "In this paper, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together.", "labels": [], "entities": [{"text": "SRL", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.7124881744384766}]}, {"text": "We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters.", "labels": [], "entities": []}, {"text": "The model can accommodate heterogeneous inputs and effectively transfer knowledge between them.", "labels": [], "entities": []}, {"text": "We also release anew corpus, Chinese Sem-Bank, for Chinese SRL 1.", "labels": [], "entities": [{"text": "Chinese Sem-Bank", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.9443864226341248}, {"text": "Chinese SRL 1", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9163995186487833}]}, {"text": "Experiments on CPB 1.0 show that our model outperforms state-of-the-art methods.", "labels": [], "entities": [{"text": "CPB 1.0", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.8790609836578369}]}], "introductionContent": [{"text": "Semantic role labeling (SRL) is one of the fundamental tasks in natural language processing because of its important role in information extraction (, statistical machine translation (, and soon.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8412707050641378}, {"text": "natural language processing", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.6632153590520223}, {"text": "information extraction", "start_pos": 125, "end_pos": 147, "type": "TASK", "confidence": 0.8163652420043945}, {"text": "statistical machine translation", "start_pos": 151, "end_pos": 182, "type": "TASK", "confidence": 0.7326343655586243}]}, {"text": "However, state-of-the-art performance of Chinese SRL is still far from satisfactory.", "labels": [], "entities": [{"text": "Chinese SRL", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.8039651215076447}]}, {"text": "And data sparsity has been a bottleneck which cannot be  ] \u3002: Sentences from (a) CPB and (b) our heterogeneous dataset.", "labels": [], "entities": [{"text": "CPB", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.9038633108139038}]}, {"text": "In CPB, each predicate (e.g., \u4fee\u6539) has a specific set of core roles given with numbers (e.g., Arg0).", "labels": [], "entities": [{"text": "Arg0", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9951115250587463}]}, {"text": "While our dataset uses a different semantic role set, and all roles are nonpredicate-specific. ignored.", "labels": [], "entities": []}, {"text": "For English, the most commonly used benchmark dataset PropBank ( has about 54,900 sentences.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.5769695043563843}]}, {"text": "But for Chinese, there are only 10,364 sentences in Chinese PropBank 1.0 (CPB) (with about 35,700 propositions).", "labels": [], "entities": [{"text": "Chinese PropBank 1.0 (CPB)", "start_pos": 52, "end_pos": 78, "type": "DATASET", "confidence": 0.7655964642763138}]}, {"text": "To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (.", "labels": [], "entities": [{"text": "SRL", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9056851267814636}]}, {"text": "The heterogeneous resources introduced by these models include other semantically annotated corpora with annotation schema different to that used in PropBank, and even of a different language.", "labels": [], "entities": []}, {"text": "The challenge here lies in the fact that those newly introduced resources are heterogeneous in nature, without sharing the same tagging schema, semantic role set, syntactic tag set and domain.", "labels": [], "entities": []}, {"text": "For example, introduced a heterogeneous dataset, Chinese NetBank, by pretraining word embeddings.", "labels": [], "entities": [{"text": "Chinese NetBank", "start_pos": 49, "end_pos": 64, "type": "DATASET", "confidence": 0.9351620674133301}]}, {"text": "Specifically, they learn an LSTM RNN model based on NetBank first, then initialize anew model with the pretrained embeddings obtained from NetBank, and then train it on CPB.", "labels": [], "entities": [{"text": "NetBank", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.9667186141014099}, {"text": "CPB", "start_pos": 169, "end_pos": 172, "type": "DATASET", "confidence": 0.9888667464256287}]}, {"text": "Chinese NetBank is also a corpus annotated with semantic roles, but using a very different role set and annotation schema.", "labels": [], "entities": [{"text": "Chinese NetBank", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9219848215579987}]}, {"text": "Wang's method can inherit knowledge acquired from other resources conveniently, but only at word representation level, missing more generalized semantic meanings in higher hidden layers.", "labels": [], "entities": []}, {"text": "proposed a twopass training approach to use corpora of two languages, but a few non-common roles are ignored in the first pass.", "labels": [], "entities": []}, {"text": "proposed a unified neural network model for SRL and relation classification (RC).", "labels": [], "entities": [{"text": "SRL", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.994789719581604}, {"text": "relation classification (RC)", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.8301081538200379}]}, {"text": "It can learn two tasks at the same time, but cannot filter out harmful features learned in incompatible tasks.", "labels": [], "entities": []}, {"text": "Recently, Progressive Neural Networks (PNN) model was proposed by to transfer learned reinforcement learning policies from one game to another, or from simulation to the real robot.", "labels": [], "entities": []}, {"text": "PNN \"freezes\" learned parameters once starting to learn anew task, and it uses lateral connections, namely adapter, to access previously learned features.", "labels": [], "entities": []}, {"text": "Inspired by the PNN model, we propose a progressive learning model to Chinese semantic role labeling in this paper.", "labels": [], "entities": [{"text": "Chinese semantic role labeling", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.5817850679159164}]}, {"text": "Especially, we extend the model with Gated Recurrent Adapters (GRA).", "labels": [], "entities": []}, {"text": "Since the standard PNN takes pixels as input, policies as output, it is not suitable for SRL task we focus in this context.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 89, "end_pos": 97, "type": "TASK", "confidence": 0.8961949050426483}]}, {"text": "Moreover, to handle long sentences in the corpus, we enhance adapters with internal memories, and gates to keep the gradient stable.", "labels": [], "entities": []}, {"text": "The contributions of this paper are threefold: 1.", "labels": [], "entities": []}, {"text": "We reconstruct PNN columns with bidirectional LSTMs to introduce heterogeneous corpora to improve Chinese SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.48140233755111694}]}, {"text": "The architecture can also be applied to a wider range of NLP tasks, like event extraction and relation classification, etc.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.7471742928028107}, {"text": "relation classification", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.806425541639328}]}, {"text": "2. We further extend the model with GRA to remember and take advantage of what has been transferred, thus improve the performance on long sentences.", "labels": [], "entities": [{"text": "GRA", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9536038637161255}, {"text": "remember", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.942751944065094}]}, {"text": "3. We also release anew corpus, Chinese SemBank, which was annotated with the schema different to that used in CPB.", "labels": [], "entities": [{"text": "Chinese SemBank", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.9155114889144897}, {"text": "CPB", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.9264079332351685}]}, {"text": "We hope that it will be helpful for future work on SRL tasks.", "labels": [], "entities": [{"text": "SRL tasks", "start_pos": 51, "end_pos": 60, "type": "TASK", "confidence": 0.9441473484039307}]}, {"text": "Subjective roles: agent(\u65bd\u4e8b), co-agent(\u540c\u4e8b), experiencer(\u5f53\u4e8b) , indirect experiencer(\u63a5\u4e8b) Objective roles: patient(\u53d7\u4e8b), relative(\u7cfb\u4e8b), dative(\u4e0e\u4e8b) , result(\u7ed3\u679c), content(\u5185\u5bb9), target(\u5bf9\u8c61) Space roles: a point of departure(\u8d77\u70b9) , a point of arrival(\u7ec8\u70b9) , path(\u8def\u5f84), direction(\u65b9\u5411), location(\u5904\u6240) Time roles: start time(\u8d77\u59cb), end time(\u7ed3\u675f), time point(\u65f6\u70b9) , duration(\u65f6\u6bb5) Comparison roles: comparison subject(\u6bd4\u8f83 \u4e3b\u4f53), comparison object(\u6bd4\u8f83\u5bf9\u8c61) , comparison range(\u6bd4\u8f83\u8303\u56f4), comparison thing(\u6bd4\u8f83\u9879\u76ee) , comparison result(\u6bd4\u8f83\u7ed3 \u679c) Others: instrument(\u5de5\u5177) , material(\u6750\u6599) , manner(\u65b9\u5f0f) , quantity(\u7269\u91cf) , range(\u8303\u56f4) , reason(\u539f\u56e0) , purpose(\u76ee\u7684) We use our new corpus as a heterogeneous resource, and evaluate the proposed model on the benchmark dataset CPB 1.0.", "labels": [], "entities": [{"text": "benchmark dataset CPB 1.0", "start_pos": 694, "end_pos": 719, "type": "DATASET", "confidence": 0.8106950968503952}]}, {"text": "The experiment shows that our approach achieves 79.67% F1 score, significantly outperforms existing state-ofthe-art systems by a large margin (Section 5).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9654767513275146}]}], "datasetContent": [{"text": "To compare our approach with others, we designed four experimental setups: (1) A simple LSTM setup on CSB and CPB with automatic PoS tagging.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 129, "end_pos": 140, "type": "TASK", "confidence": 0.6786454617977142}]}, {"text": "Since CPB is about two times as large as the new corpus, we need to know whether CSB can be used for training good semantic parsers and how much information can be learned from CSB by machine.", "labels": [], "entities": []}, {"text": "So we conduct this experiment to provide two baselines for CSB and CPB respectively.", "labels": [], "entities": [{"text": "CPB", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.7383880019187927}]}, {"text": "In this setup we train and evaluate a one-column LSTM model on CSB.", "labels": [], "entities": []}, {"text": "(2) A simple LSTM setup on CPB with pretrained word embedding on CSB (marked as bi-LSTM+CSB embedding).", "labels": [], "entities": []}, {"text": "Previous work found that using pretrained word embeddings can improve performance () on Chinese SRL.", "labels": [], "entities": [{"text": "Chinese SRL", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.701943576335907}]}, {"text": "So we conduct this experiment to compare with the method using embeddings trained on large-scale unlabeled data like Gigaword 2 , and NetBank.", "labels": [], "entities": [{"text": "NetBank", "start_pos": 134, "end_pos": 141, "type": "DATASET", "confidence": 0.9499800801277161}]}, {"text": "(3) A two-column finetuning setup where we pretrain the first column on CSB and finetune both two columns on CPB.", "labels": [], "entities": [{"text": "CPB", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.9685746431350708}]}, {"text": "Clearly, finetuning is a traditional method for continual learning scenarios.", "labels": [], "entities": []}, {"text": "But the disadvantage of it is that learned features will be gradually forgotten when the model is adapting new tasks.", "labels": [], "entities": []}, {"text": "To assess this empirically, we design this experiment.", "labels": [], "entities": []}, {"text": "The model uses the same network structure as PNN does, but it does not \"freeze\" parameters in the first column when tuning two columns.", "labels": [], "entities": []}, {"text": "(4) A progressive network setup where we train column 1 on CSB, then train column 2 and adapters on CPB.", "labels": [], "entities": [{"text": "CSB", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.9668118953704834}, {"text": "CPB", "start_pos": 100, "end_pos": 103, "type": "DATASET", "confidence": 0.9876582026481628}]}, {"text": "We conduct this experiment to evaluate the proposed model and compare it to all previous methods.", "labels": [], "entities": []}, {"text": "To further analyze effectiveness of the new adapter structure, we also conduct an experiment for progressive nets with GRA.", "labels": [], "entities": [{"text": "GRA", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9792674779891968}]}, {"text": "We apply grid-search technique to explore hyper-parameters including learning rates and width of layers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of Chinese SRL tested on CPB  and CSB with automatic PoS tagging, using stan- dard LSTM RNN model (Experiment 1).", "labels": [], "entities": [{"text": "SRL", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.6028986573219299}, {"text": "PoS tagging", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.6478995829820633}]}, {"text": " Table 3: Result comparison on CPB dataset. Compared to learning with single corpus using bi-LSTM  model (77.09%), learning with CSB can improve the performance by at list 0.59%. Also the best score  (79.67%) was achieved by the PNN GRA model.", "labels": [], "entities": [{"text": "CPB dataset", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.8777887523174286}, {"text": "PNN GRA", "start_pos": 229, "end_pos": 236, "type": "DATASET", "confidence": 0.5847935229539871}]}]}