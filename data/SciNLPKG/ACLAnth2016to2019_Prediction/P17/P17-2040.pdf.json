{"title": [{"text": "A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations", "labels": [], "entities": [{"text": "Recognition of Chinese Implicit Discourse", "start_pos": 48, "end_pos": 89, "type": "TASK", "confidence": 0.8658498406410218}]}], "abstractContent": [{"text": "We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches.", "labels": [], "entities": []}, {"text": "Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank.", "labels": [], "entities": [{"text": "Chinese Discourse Treebank", "start_pos": 127, "end_pos": 153, "type": "DATASET", "confidence": 0.9710425933202108}]}, {"text": "We also visualize its attention activity to illustrate the model's ability to selectively focus on the relevant parts of an input sequence.", "labels": [], "entities": []}], "introductionContent": [{"text": "True text understanding is one of the key goals in Natural Language Processing and requires capabilities beyond the lexical semantics of individual words or phrases.", "labels": [], "entities": [{"text": "True text understanding", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.607138325770696}]}, {"text": "Natural language descriptions are typically driven by an inter-sentential coherent structure, exhibiting specific discourse properties, which in turn contribute significantly to the global meaning of a text.", "labels": [], "entities": []}, {"text": "Automatically detecting how meaning units are organized benefits practical downstream applications, such as question answering (, recognizing textual entailment, sentiment analysis (, or text summarization (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.8946012556552887}, {"text": "recognizing textual entailment", "start_pos": 130, "end_pos": 160, "type": "TASK", "confidence": 0.7327876289685568}, {"text": "sentiment analysis", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.9514304995536804}, {"text": "text summarization", "start_pos": 187, "end_pos": 205, "type": "TASK", "confidence": 0.7394917011260986}]}, {"text": "Various formalisms in terms of semantic coherence frameworks have been proposed to account for these contextual assumptions ().", "labels": [], "entities": []}, {"text": "The annotation schemata of the Penn Discourse Treebank (, PDTB) and the Chinese Discourse Treebank (, for instance, define * Both first authors contributed equally to this work.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (, PDTB", "start_pos": 31, "end_pos": 62, "type": "DATASET", "confidence": 0.9314003686110178}, {"text": "Chinese Discourse Treebank", "start_pos": 72, "end_pos": 98, "type": "DATASET", "confidence": 0.9700704415639242}]}, {"text": "discourse units as syntactically motivated character spans in the text, augmented with relations pointing from the second argument (Arg2, prototypically, a discourse unit associated with an explicit discourse marker) to its antecedent, i.e., the discourse unit Arg1.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 261, "end_pos": 265, "type": "METRIC", "confidence": 0.737713098526001}]}, {"text": "Relations are labeled with a relation type (its sense) and the associated discourse marker.", "labels": [], "entities": []}, {"text": "Both, PDTB and CDTB, distinguish explicit from implicit relations depending on the presence of such a marker (e.g., because/ \u56e0).", "labels": [], "entities": []}, {"text": "1 Sense classification for implicit relations is by far more challenging because the argument pairs lack the marker as an important feature.", "labels": [], "entities": []}, {"text": "Consider, for instance, the following example from the CDTB as implicit CONJUNCTION: Arg1: \u4f1a\u8c08\u5c31\u4e00\u4e9b\u539f\u5219\u548c\u5177\u4f53\u95ee\u9898\u8fdb\u884c\u4e86 \u6df1\u5165\u8ba8\u8bba\uff0c\u8fbe\u6210\u4e86\u4e00\u4e9b\u8c05\u89e3 In the talks, they discussed some principles and specific questions in depth, and reached some understandings Arg2: \u53cc \u65b9 \u4e00 \u81f4 \u8ba4 \u4e3a \u4f1a \u8c08 \u5177 \u6709 \u79ef \u6781 \u6210 \u679c Both sides agree that the talks have positive results Motivation: Previous work on implicit sense labeling is heavily feature-rich and requires domainspecific, semantic lexicons (.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9635283350944519}]}, {"text": "Only recently, resource-lean architectures have been proposed.", "labels": [], "entities": []}, {"text": "These promising neural methods attempt to infer latent representations appropriate for implicit relation classification (.", "labels": [], "entities": [{"text": "implicit relation classification", "start_pos": 87, "end_pos": 119, "type": "TASK", "confidence": 0.6006442904472351}]}, {"text": "So far, unfortunately, these models have been evaluated only on four top-level senses-sometimes even with inconsistent evaluation setups.", "labels": [], "entities": []}, {"text": "Furthermore, most systems have initially been designed for the English PDTB and involve complex, task-specific architectures (, while discourse modeling techniques for Chinese have received very little attention in the literature and are still seriously underrepresented in terms of publicly available systems.", "labels": [], "entities": []}, {"text": "What is more, over 80% of all words in Chinese discourse relations are implicit-compared to only 52% in English (.", "labels": [], "entities": []}, {"text": "Recently, in the context of the CoNLL 2016 shared task ( ), a first independent evaluation platform beyond class level has been established.", "labels": [], "entities": [{"text": "CoNLL 2016 shared task", "start_pos": 32, "end_pos": 54, "type": "DATASET", "confidence": 0.8186621814966202}]}, {"text": "Surprisingly, the best performing neural architectures to date are standard feedforward networks, cf.;;.", "labels": [], "entities": []}, {"text": "Even though these specific models completely ignore word order within arguments, such feedforward architectures have been claimed by  to generally outperform any thoroughly-tuned recurrent architecture.", "labels": [], "entities": []}, {"text": "Our Contribution: In this work, we release the first attention-based recurrent neural sense classifier, specifically developed for Chinese implicit discourse relations.", "labels": [], "entities": [{"text": "attention-based recurrent neural sense classifier", "start_pos": 53, "end_pos": 102, "type": "TASK", "confidence": 0.6099331855773926}, {"text": "Chinese implicit discourse relations", "start_pos": 131, "end_pos": 167, "type": "TASK", "confidence": 0.6407914385199547}]}, {"text": "Inspired by, our system is a practical adaptation of the recent advances in relation modeling extended by a novel sampling scheme.", "labels": [], "entities": [{"text": "relation modeling", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.9129143953323364}]}, {"text": "Contrary to previous assertions by , our model demonstrates superior performance over traditional bag-of-words approaches with feedfoward networks by treating discourse arguments as a joint sequence.", "labels": [], "entities": []}, {"text": "We evaluate our method within an independent framework and show that it performs very well beyond standard class-level predictions, achieving stateof-the-art accuracy on the CDTB test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.998997151851654}, {"text": "CDTB test set", "start_pos": 174, "end_pos": 187, "type": "DATASET", "confidence": 0.9775955677032471}]}, {"text": "We illustrate how our model's attention mechanism provides means to highlight those parts of an input sequence that are relevant for the classification decision, and thus, it may enable a better understanding of the implicit discourse parsing problem.", "labels": [], "entities": [{"text": "implicit discourse parsing", "start_pos": 216, "end_pos": 242, "type": "TASK", "confidence": 0.6310888429482778}]}, {"text": "Our proposed network architecture is flexible and largely language-independent as it operates only on word embeddings.", "labels": [], "entities": []}, {"text": "It stands out due to its structural simplicity and builds a solid ground for further development towards other textual domains.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our recurrent model on the CoNLL 2016 shared task data 5 which include the official training, development and test sets of the CDTB; cf. for an overview of the implicit sense distribution.", "labels": [], "entities": [{"text": "CoNLL 2016 shared task data 5", "start_pos": 39, "end_pos": 68, "type": "DATASET", "confidence": 0.9554780622323354}]}, {"text": "In accordance with previous setups (Rutherford et al., 2016), we treat entity relations (ENTREL) as implicit and exclude ALTLEX relations.", "labels": [], "entities": [{"text": "ENTREL", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9178064465522766}]}, {"text": "In the evaluation, we focus on the sense-only track, the subtask for which gold arguments are provided and a system is supposed to label a given argument pair with the correct sense.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "With our proposed architecture it is possible to correctly label 257/352 (73.01%) of implicit rela-tions on the test set, outperforming the best feedforward system of and all other word order-agnostic approaches.", "labels": [], "entities": []}, {"text": "Development and test set performances suggest the robustness of our approach and its ability to generalize to unseen data.", "labels": [], "entities": []}, {"text": "Ablation Study: We perform an ablation study to quantitatively assess the contribution of two of the characteristic aspects of our model.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.901760995388031}]}, {"text": "First, we compare the use of the attention mechanism against the simpler alternative of feeding the final LSTM hidden vectors (h k and h 1 ) directly to the output layer.", "labels": [], "entities": []}, {"text": "When attention is turned off, this yields an absolute decrease in performance of 2.70% on the test set, which is substantial and significant according to a Welch two-sample t-test (p < .001).", "labels": [], "entities": []}, {"text": "Second, we independently compare the use of the partial sampling scheme against training on the standard argument pairs in the CDTB.", "labels": [], "entities": [{"text": "CDTB", "start_pos": 127, "end_pos": 131, "type": "DATASET", "confidence": 0.9584649801254272}]}, {"text": "Here, the absence of the partial sampling scheme yields an absolute decrease inaccuracy of 5.74% (p < .001), which demonstrates its importance for achieving competitive performance on the task.", "labels": [], "entities": [{"text": "absolute decrease inaccuracy", "start_pos": 59, "end_pos": 87, "type": "METRIC", "confidence": 0.8241860270500183}]}, {"text": "Performance on the PDTB: As aside experiment, we investigate the model's language independence by applying it to the implicit argument pairs of the English PDTB.", "labels": [], "entities": []}, {"text": "Due to computational time constraints we do not optimize hyperparameters, but instead train the model using identical settings as for Chinese, which is expected to lead to suboptimal performance on the evaluation data.", "labels": [], "entities": []}, {"text": "Nevertheless, we measure 27.09% accuracy on the PDTB test set (surpassing the majority class baseline of 22.01%), which shows that the model has potential to generalize across implicit discourse relations in a different language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9996176958084106}, {"text": "PDTB test set", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9616911212603251}]}, {"text": "In the talks, they discussed some principles and specific questions in depth, and reached some understandings Both sides agree that the talks have positive results He said: We hope that the Macao government will continue to pay attention to these three issues, Visualizing Attention Weights: Finally, in, we illustrate the learned attention weights which pinpoint important subcomponents within a given implicit discourse relation.", "labels": [], "entities": []}, {"text": "For the implicit CONJUNCTION relation the weights indicate a peak on the transition between the argument boundary, establishing a connection between the semantically related terms understandings-agree.", "labels": [], "entities": []}, {"text": "Most ENTRELs show an opposite trend: here second arguments exhibit larger intensities than Arg1, as most entity relations follow the characteristic writing style of newspapers by adding additional information by reference to the same entity.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9870172142982483}]}], "tableCaptions": [{"text": " Table 1: Non-explicit parser scores on the official CoNLL 2016 CDTB development and test sets.  (  *  Scores on development set are obtained through partial sampling and are not directly comparable.)", "labels": [], "entities": [{"text": "CoNLL 2016 CDTB development and test sets", "start_pos": 53, "end_pos": 94, "type": "DATASET", "confidence": 0.9609571610178266}]}, {"text": " Table 2: Implicit sense labels in the CDTB.", "labels": [], "entities": [{"text": "CDTB", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.9671985507011414}]}, {"text": " Table 1.  With our proposed architecture it is possible to  correctly label 257/352 (73.01%) of implicit rela-tions on the test set, outperforming the best feed- forward system of", "labels": [], "entities": []}]}