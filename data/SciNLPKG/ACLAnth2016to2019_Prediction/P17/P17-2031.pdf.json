{"title": [{"text": "Attention Strategies for Multi-Source Sequence-to-Sequence Learning", "labels": [], "entities": [{"text": "Sequence-to-Sequence Learning", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.7383311092853546}]}], "abstractContent": [{"text": "Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities.", "labels": [], "entities": [{"text": "Modeling attention", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.932017982006073}]}, {"text": "We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence, flat and hierarchical.", "labels": [], "entities": []}, {"text": "We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and Automatic Post-editing tasks.", "labels": [], "entities": [{"text": "WMT16 Multimodal Translation", "start_pos": 126, "end_pos": 154, "type": "TASK", "confidence": 0.7637197573979696}]}, {"text": "We show that the proposed methods achieve competitive results on both tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequence-to-sequence (S2S) learning with attention mechanism recently became the most successful paradigm with state-of-the-art results in machine translation (MT) (), image captioning (), text summarization () and other NLP tasks.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 139, "end_pos": 163, "type": "TASK", "confidence": 0.8440927624702453}, {"text": "image captioning", "start_pos": 168, "end_pos": 184, "type": "TASK", "confidence": 0.7048121690750122}, {"text": "text summarization", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.7696737945079803}]}, {"text": "All of the above applications of S2S learning make use of a single encoder.", "labels": [], "entities": [{"text": "S2S learning", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.9153326153755188}]}, {"text": "Depending on the modality, it can be either a recurrent neural network (RNN) for textual input data, or a convolutional network for images.", "labels": [], "entities": []}, {"text": "In this work, we focus on a special case of S2S learning with multiple input sequences of possibly different modalities and a single output-generating recurrent decoder.", "labels": [], "entities": [{"text": "S2S learning", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.9286454319953918}]}, {"text": "We explore various strategies the decoder can employ to attend to the hidden states of the individual encoders.", "labels": [], "entities": []}, {"text": "The existing approaches to this problem do not explicitly model different importance of the inputs to the decoder (.", "labels": [], "entities": []}, {"text": "In multimodal MT (MMT), where an image and its caption are on the input, we might expect the caption to be the primary source of information, whereas the image itself would only play a role in output disambiguation.", "labels": [], "entities": [{"text": "MT (MMT)", "start_pos": 14, "end_pos": 22, "type": "TASK", "confidence": 0.7778379768133163}]}, {"text": "In automatic post-editing (APE), where a sentence in a source language and its automatically generated translation are on the input, we might want to attend to the source text only in case the model decides that there is an error in the translation.", "labels": [], "entities": [{"text": "automatic post-editing (APE)", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.6428716778755188}]}, {"text": "We propose two interpretable attention strategies that take into account the roles of the individual source sequences explicitly-flat and hierarchical attention combination.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: In Section 2, we review the attention mechanism in single-source S2S learning.", "labels": [], "entities": []}, {"text": "Section 3 introduces new attention combination strategies.", "labels": [], "entities": [{"text": "attention combination", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.6981669217348099}]}, {"text": "In Section 4, we evaluate the proposed models on the MMT and APE tasks.", "labels": [], "entities": [{"text": "MMT and APE tasks", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.5629797056317329}]}, {"text": "We summarize the related work in Section 5, and conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the attention combination strategies presented in Section 3 on the tasks of multimodal translation (Section 4.1) and automatic post-editing (Section 4.2).", "labels": [], "entities": [{"text": "multimodal translation", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.6309027671813965}]}, {"text": "The models were implemented using the Neural Monkey sequence-to-sequence learning toolkit.", "labels": [], "entities": []}, {"text": "In both setups, we process the textual input with bidirectional GRU network ( ) with 300 units in the hidden state in each direction and 300 units in embeddings.", "labels": [], "entities": []}, {"text": "For the attention projection space, we use 500 hidden units.", "labels": [], "entities": [{"text": "attention projection", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.8391604423522949}]}, {"text": "We optimize the network to minimize the output cross-entropy using the Adam algorithm () with learning rate 10 \u22124 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of our experiments on the test sets  of Multi30k dataset and the APE dataset. The col- umn 'share' denotes whether the projection matrix  is shared for energies and context vector computa- tion, 'sent.' indicates whether the sentinel vector  has been used or not.", "labels": [], "entities": [{"text": "Multi30k dataset", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.9856364130973816}, {"text": "APE dataset", "start_pos": 83, "end_pos": 94, "type": "DATASET", "confidence": 0.9802789092063904}]}]}