{"title": [{"text": "Learning to Ask: Neural Question Generation for Reading Comprehension", "labels": [], "entities": [{"text": "Neural Question Generation", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.608004073301951}]}], "abstractContent": [{"text": "We study automatic question generation for sentences from text passages in reading comprehension.", "labels": [], "entities": [{"text": "automatic question generation", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.6327925026416779}]}, {"text": "We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence-vs. paragraph-level information.", "labels": [], "entities": []}, {"text": "In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning.", "labels": [], "entities": []}, {"text": "Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system.", "labels": [], "entities": []}, {"text": "In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammat-icality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).", "labels": [], "entities": []}], "introductionContent": [{"text": "Question generation (QG) aims to create natural questions from a given a sentence or paragraph.", "labels": [], "entities": [{"text": "Question generation (QG)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8672831892967224}]}, {"text": "One key application of question generation is in the area of education -to generate questions for reading comprehension materials., for example, shows three manually generated questions that test a user's understanding of the associated text passage.", "labels": [], "entities": [{"text": "question generation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7441168427467346}]}, {"text": "Question generation systems can also be deployed as chatbot components (e.g., asking questions to start a conversation or to request feedback () or, arguably, as a clinical tool for evaluating or improving mental health.", "labels": [], "entities": [{"text": "Question generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7972290813922882}]}, {"text": "In addition to the above applications, question generation systems can aid in the development of Oxygen is used in cellular respiration and released by photosynthesis, which uses the energy of sunlight to produce oxygen from water.", "labels": [], "entities": [{"text": "question generation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7903159260749817}]}, {"text": "annotated data sets for natural language processing (NLP) research in reading comprehension and question answering.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.7877207299073538}, {"text": "question answering", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.8736096918582916}]}, {"text": "Indeed the creation of such datasets, e.g., and MS MARCO (, has spurred research in these areas.", "labels": [], "entities": [{"text": "MS", "start_pos": 48, "end_pos": 50, "type": "DATASET", "confidence": 0.8508912324905396}, {"text": "MARCO", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.4359915256500244}]}, {"text": "For the most part, question generation has been tackled in the past via rule-based approaches (e.g.,;.", "labels": [], "entities": [{"text": "question generation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.9029363393783569}]}, {"text": "The success of these approaches hinges critically on the existence of well-designed rules for declarative-to-interrogative sentence transformation, typically based on deep linguistic knowledge.", "labels": [], "entities": [{"text": "declarative-to-interrogative sentence transformation", "start_pos": 94, "end_pos": 146, "type": "TASK", "confidence": 0.7411921223004659}]}, {"text": "To improve over a purely rule-based system, introduced an overgenerate-and-rank approach that generates multiple questions from an input sentence using a rule-based approach and then ranks them using a supervised learning-based ranker.", "labels": [], "entities": []}, {"text": "Although the ranking algorithm helps to produce more ac-ceptable questions, it relies heavily on a manually crafted feature set, and the questions generated often overlap word for word with the tokens in the input sentence, making them very easy to answer.", "labels": [], "entities": []}, {"text": "point out that learning to ask good questions is an important task in NLP research in its own right, and should consist of more than the syntactic transformation of a declarative sentence.", "labels": [], "entities": []}, {"text": "In particular, a natural sounding question often compresses the sentence on which it is based (e.g., question 3 in), uses synonyms for terms in the passage (e.g., \"form\" for \"produce\" in question 2 and \"get\" for \"produce\" in question 3), or refers to entities from preceding sentences or clauses (e.g., the use of \"photosynthesis\" in question 2).", "labels": [], "entities": []}, {"text": "Othertimes, world knowledge is employed to produce a good question (e.g., identifying \"photosynthesis\" as a \"life process\" in question 1).", "labels": [], "entities": []}, {"text": "In short, constructing natural questions of reasonable difficulty would seem to require an abstractive approach that can produce fluent phrasings that do not exactly match the text from which they were drawn.", "labels": [], "entities": []}, {"text": "As a result, and in contrast to all previous work, we propose hereto frame the task of question generation as a sequence-to-sequence learning problem that directly maps a sentence from a text passage to a question.", "labels": [], "entities": [{"text": "question generation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.738708034157753}]}, {"text": "Importantly, our approach is fully data-driven in that it requires no manually generated rules.", "labels": [], "entities": []}, {"text": "More specifically, inspired by the recent success in neural machine translation, summarization (, and image caption generation (, we tackle question generation using a conditional neural language model with a global attention mechanism ().", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.6869758566220602}, {"text": "summarization", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.9880508184432983}, {"text": "image caption generation", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.8375718593597412}, {"text": "question generation", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.7486865520477295}]}, {"text": "We investigate several variations of this model, including one that takes into account paragraph-rather than sentence-level information from the reading passage as well as other variations that determine the importance of pre-trained vs. learned word embeddings.", "labels": [], "entities": []}, {"text": "In evaluations on the SQuAD dataset) using three automatic evaluation metrics, we find that our system significantly outperforms a collection of strong baselines, including an information retrieval-based system, a statistical machine translation approach (, and the overgenerate-and-rank approach of Heilman and.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.8214908838272095}, {"text": "statistical machine translation", "start_pos": 214, "end_pos": 245, "type": "TASK", "confidence": 0.6223446627457937}]}, {"text": "Human evaluations also rated our generated questions as more grammatical, fluent, and challenging (in terms of syntactic divergence from the original reading passage and reasoning needed to answer) than the state-of-theart system.", "labels": [], "entities": []}, {"text": "In the sections below we discuss related work (Section 2), specify the task definition (Section 3) and describe our neural sequence learning based models (Section 4).", "labels": [], "entities": []}, {"text": "We explain the experimental setup in Section 5.", "labels": [], "entities": []}, {"text": "Lastly, we present the evaluation results as well as a detailed analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with our neural question generation model on the processed SQuAD dataset.", "labels": [], "entities": [{"text": "question generation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7217722833156586}, {"text": "SQuAD dataset", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.8579084575176239}]}, {"text": "In this section, we firstly describe the corpus of the task.", "labels": [], "entities": []}, {"text": "We then give implementation details of our neural generation model, the baselines to compare, and their experimental settings.", "labels": [], "entities": [{"text": "neural generation", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7589652836322784}]}, {"text": "Lastly, we introduce the evaluation methods by automatic metrics and human raters.", "labels": [], "entities": []}, {"text": "With the SQuAD dataset (, we extract sentences and pair them with the ques- Since there is a hidden part of the original SQuAD that we do not have access to, we treat the accessible parts (\u223c90%) as the entire dataset henceforth.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 9, "end_pos": 22, "type": "DATASET", "confidence": 0.8908906877040863}]}, {"text": "We first run Stanford CoreNLP ( ) for pre-processing: tokenization and sentence splitting.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 13, "end_pos": 29, "type": "DATASET", "confidence": 0.9136219918727875}, {"text": "tokenization", "start_pos": 54, "end_pos": 66, "type": "TASK", "confidence": 0.971615195274353}, {"text": "sentence splitting", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7400505095720291}]}, {"text": "We then lower-case the entire dataset.", "labels": [], "entities": []}, {"text": "With the offset of the answer to each question, we locate the sentence containing the answer and use it as the input sentence.", "labels": [], "entities": []}, {"text": "In some cases (< 0.17% in training set), the answer spans two or more sentences, and we then use the concatenation of the sentences as the input \"sentence\".", "labels": [], "entities": []}, {"text": "shows the distribution of the token overlap percentage of the sentence-question pairs.", "labels": [], "entities": []}, {"text": "Although most of the pairs have over 50% overlap rate, about 6.67% of the pairs have no nonstop-words in common, and this is mostly because of the answer offset error introduced during annotation.", "labels": [], "entities": [{"text": "overlap rate", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9657948017120361}]}, {"text": "Therefore, we prune the training set based on the constraint: the sentence-question pair must have at least one non-stop-word in common.", "labels": [], "entities": []}, {"text": "Lastly we add <SOS> to the beginning of the sen- tences, and <EOS> to the end of them.", "labels": [], "entities": [{"text": "EOS", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.973471999168396}]}, {"text": "We randomly divide the dataset at the articlelevel into a training set (80%), a development set (10%), and a test set (10%).", "labels": [], "entities": []}, {"text": "We report results on the 10% test set.", "labels": [], "entities": []}, {"text": "provides some statistics on the processed dataset: there are around 70k training samples, the sentences are around 30 tokens, and the questions are around 10 tokens on average.", "labels": [], "entities": []}, {"text": "For each sentence, there might be multiple corresponding questions, and, on average, there are 1.4 questions for each sentence.", "labels": [], "entities": []}, {"text": "We use the evaluation package released by, which was originally used to score image captions.", "labels": [], "entities": []}, {"text": "The package includes BLEU 1, BLEU 2, BLEU 3, BLEU 4 (), METEOR) and ROUGE L) evaluation scripts.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9950615763664246}, {"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9920805096626282}, {"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9931269884109497}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9943268895149231}, {"text": "METEOR", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9809703230857849}, {"text": "ROUGE", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9383649230003357}]}, {"text": "BLEU measures the average n-gram precision on a set of reference sentences, with a penalty for overly short sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9796246290206909}, {"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9335699677467346}]}, {"text": "BLEU-n is BLEU score that uses up to n-grams for counting co-occurrences.", "labels": [], "entities": [{"text": "BLEU-n", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9846326112747192}, {"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9815807342529297}]}, {"text": "ME-TEOR is a recall-oriented metric, which calculates the similarity between generations and references by considering synonyms, stemming and paraphrases.", "labels": [], "entities": [{"text": "recall-oriented", "start_pos": 13, "end_pos": 28, "type": "METRIC", "confidence": 0.9697402119636536}]}, {"text": "ROUGE is commonly employed to evaluate n-grams recall of the summaries with goldstandard sentences as references.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9780808091163635}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9803364872932434}]}, {"text": "ROUGE L (measured based on longest common subsequence) results are reported.", "labels": [], "entities": [{"text": "ROUGE L", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9147871732711792}]}, {"text": "We also perform human evaluation studies to measure the quality of questions generated by our system and the H&S system.", "labels": [], "entities": [{"text": "H&S system", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.6157784536480904}]}, {"text": "We consider two modalities: naturalness, which indicates the grammaticality and fluency; and difficulty, which measures the sentence-question syntactic divergence and the reasoning needed to answer the question.", "labels": [], "entities": [{"text": "difficulty", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.9840314984321594}]}, {"text": "We randomly sampled 100 sentence-question pairs.", "labels": [], "entities": []}, {"text": "We ask four professional English speakers to rate the pairs in terms of the modalities above on a 1-5 scale (5 for the best).", "labels": [], "entities": []}, {"text": "We then ask the human raters to give a ranking of the questions according to the overall quality, with ties allowed.", "labels": [], "entities": []}, {"text": "shows automatic metric evaluation results for our models and baselines.", "labels": [], "entities": []}, {"text": "Our model which only encodes sentence-level information achieves Sentence 1: the largest of these is the eldon square shopping centre , one of the largest city centre shopping complexes in the uk .", "labels": [], "entities": [{"text": "eldon square shopping centre", "start_pos": 105, "end_pos": 133, "type": "DATASET", "confidence": 0.9718241393566132}, {"text": "uk", "start_pos": 193, "end_pos": 195, "type": "DATASET", "confidence": 0.9878222346305847}]}], "tableCaptions": [{"text": " Table 1: Dataset (processed) statistics. Sentence  average # tokens, question average # tokens, and  average # questions per sentence statistics are  from training set. These averages are close to the  statistics on development set and test set.", "labels": [], "entities": []}, {"text": " Table 2: Automatic evaluation results of different systems by BLEU 1-4, METEOR and ROUGE L .", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9946073889732361}, {"text": "METEOR", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9113494157791138}, {"text": "ROUGE L", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9553743302822113}]}, {"text": " Table 3: Human evaluation results for question  generation. Naturalness and difficulty are rated  on a 1-5 scale (5 for the best). Two-tailed t- test results are shown for our method compared to  H&S (statistical significance is indicated with  *  (p  < 0.005),  *  *  (p < 0.001)).", "labels": [], "entities": [{"text": "question  generation", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.8170645534992218}, {"text": "difficulty", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.99090975522995}]}, {"text": " Table 4: An estimate of categories of questions of the processed dataset and per-category performance  comparison of the systems. The estimate is based on our analysis of the 346 pairs from the dev set.  Categories are decided by the information needed to generate the question. Bold numbers represent the  best performing method for a given metric.  *  Here, we leave out performance results for \"w/ article\"  category (2 samples, 0.58%) and \"not askable\" category (33 samples, 9.54%).", "labels": [], "entities": []}]}