{"title": [{"text": "A Constituent-Centric Neural Architecture for Reading Comprehension", "labels": [], "entities": []}], "abstractContent": [{"text": "Reading comprehension (RC), aiming to understand natural texts and answer questions therein, is a challenging task.", "labels": [], "entities": [{"text": "Reading comprehension (RC)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8398919701576233}]}, {"text": "In this paper, we study the RC problem on the Stanford Question Answering Dataset (SQuAD).", "labels": [], "entities": [{"text": "RC problem", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.8955762088298798}, {"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 46, "end_pos": 89, "type": "DATASET", "confidence": 0.8795651027134487}]}, {"text": "Observing from the training set that most correct answers are centered around constituents in the parse tree, we design a constituent-centric neural architecture where the generation of candidate answers and their representation learning are both based on constituents and guided by the parse tree.", "labels": [], "entities": []}, {"text": "Under this architecture , the search space of candidate answers can be greatly reduced without sacrificing the coverage of correct answers and the syntactic, hierarchical and compositional structure among constituents can be well captured, which contributes to better representation learning of the candidate answers.", "labels": [], "entities": []}, {"text": "On SQuAD, our method achieves the state of the art performance and the ab-lation study corroborates the effectiveness of individual modules.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reading comprehension (RC) aims to answer questions by understanding texts, which is a challenge task in natural language processing.", "labels": [], "entities": [{"text": "Reading comprehension (RC)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7676250219345093}]}, {"text": "Various RC tasks and datasets have been developed, including Machine Comprehension Test () for multiple-choice question answering (QA) (), Algebra () and Science  for passing standardized tests , CNN/Daily Mail ( and Children's Book Test) for cloze-style The most authoritative account at the time came from the medical faculty in Paris in a report to the king of France that blamed the heavens.", "labels": [], "entities": [{"text": "multiple-choice question answering (QA)", "start_pos": 95, "end_pos": 134, "type": "TASK", "confidence": 0.8207876284917196}, {"text": "CNN/Daily Mail", "start_pos": 196, "end_pos": 210, "type": "DATASET", "confidence": 0.8068187236785889}]}, {"text": "This report became the first and most widely circulated of a series of plague tracts that sought to give advice to sufferers.", "labels": [], "entities": []}, {"text": "That the plague was caused by bad air became the most widely accepted theory.", "labels": [], "entities": []}, {"text": "Today, this is known as the Miasma theory.", "labels": [], "entities": [{"text": "Miasma theory", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.93207186460495}]}, {"text": "1. Who was the medical report written for?", "labels": [], "entities": [{"text": "Who was the medical report written", "start_pos": 3, "end_pos": 37, "type": "DATASET", "confidence": 0.6049275994300842}]}, {"text": "the king of France 2.", "labels": [], "entities": []}, {"text": "What is the newer, more widely accepted theory behind the spread of the plague?", "labels": [], "entities": [{"text": "spread of the plague", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.8158930689096451}]}, {"text": "What is the bad air theory officially known as?", "labels": [], "entities": []}, {"text": "Miasma theory: An example of the SQuAD QA task QA (, WikiQA (, Stanford Question Answering Dataset (SQuAD) ( and Microsoft Machine Reading Comprehension () for open domain QA.", "labels": [], "entities": [{"text": "SQuAD QA task QA", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.6275204718112946}, {"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 63, "end_pos": 106, "type": "DATASET", "confidence": 0.7171840752874102}]}, {"text": "In this paper, we are specifically interested in solving the SQuAD QA task shows an example), in light of its following features: (1) large scale: 107,785 questions, 23,215 paragraphs; (2) nonsynthetic: questions are generated by crowdworkers; (3) large search space of candidate answers.", "labels": [], "entities": [{"text": "SQuAD QA task", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.7831282019615173}]}, {"text": "We study two major problems: (1) how to generate candidate answers?", "labels": [], "entities": []}, {"text": "Unlike in multiplechoice QA and cloze-style QA where a small amount of answer choices are given, an answer in SQuAD could be any span in the text, resulting in a large search space with size O(n 2 ) (, where n is the number of words in the sentence.", "labels": [], "entities": []}, {"text": "This would incur a lot of noise, ambigu-ity and uncertainty, making it highly difficult to pickup the correct answer.", "labels": [], "entities": []}, {"text": "(2) how to effectively represent the candidate answers?", "labels": [], "entities": []}, {"text": "First, long-range semantics spanning multiple sentences need to be captured.", "labels": [], "entities": []}, {"text": "As noted in (, the answering of many questions requires multiplesentence reasoning.", "labels": [], "entities": []}, {"text": "For instance, in, the last two sentences in the passages are needed to answer the third question.", "labels": [], "entities": []}, {"text": "Second, local syntactic structure needs to be incorporated into representation learning.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.9200071394443512}]}, {"text": "The study by shows that syntax plays an important role in SQuAD QA: there area wide range of syntactic divergence between a question and the sentence containing the answer; the answering of 64.1% questions needs to deal with syntactic variation; experiments show that syntactic features are the major contributing factors to good performance.", "labels": [], "entities": [{"text": "SQuAD QA", "start_pos": 58, "end_pos": 66, "type": "TASK", "confidence": 0.9457796216011047}]}, {"text": "To tackle the first problem, motivated by the observation in () that the correct answers picked up by human are not arbitrary spans, but rather centered around constituents in the parse tree, we generate candidate answers based upon constituents, which significantly reduces the search space.", "labels": [], "entities": []}, {"text": "Different from) who only consider exact constituents, we adopt a constituent expansion mechanism which greatly improves the coverage of correct answers.", "labels": [], "entities": []}, {"text": "For the representation learning of candidate answers which are sequences of constituents, we first encode individual constituents using a chainof-trees LSTM (CT-LSTM) and tree-guided attention mechanism, then feed these encodings into a chain LSTM (Hochreiter and Schmidhuber, 1997) to generate representations for the constituent sequences.", "labels": [], "entities": [{"text": "representation learning of candidate answers", "start_pos": 8, "end_pos": 52, "type": "TASK", "confidence": 0.8427610516548156}]}, {"text": "The CT-LSTM seamlessly integrates intra-sentence tree) which capture the local syntactic properties of constituents and an inter-sentence chain LSTM which glues together the sequence of tree LSTMs such that the semantics of each sentence can be propagated to others.", "labels": [], "entities": []}, {"text": "The tree-guided attention leverages the hierarchical relations among constituents to learn question-aware representations.", "labels": [], "entities": []}, {"text": "Putting these pieces together, we design a constituent-centric neural network (CCNN), which contains four layers: a chain-of-trees LSTM encoding layer, a tree-guided attention layer and a candidate-answer generation layer, a prediction layer.", "labels": [], "entities": []}, {"text": "Evaluation on SQuAD demonstrates the ef-  2 Constituent-Centric Neural Network for Reading Comprehension", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments are conducted on the Stanford Question Answering Dataset (SQuAD) v1.1, which contains 107,785 questions and 23,215 passages coming from 536 Wikipedia articles.", "labels": [], "entities": [{"text": "Stanford Question Answering Dataset (SQuAD) v1.1", "start_pos": 37, "end_pos": 85, "type": "DATASET", "confidence": 0.8919292613863945}]}, {"text": "The data was randomly partitioned into a training set (80%), a development set (10%) and an unreleased test set (10%).", "labels": [], "entities": []}, {"text": "build a leaderboard to evaluate and publish results on the test set.", "labels": [], "entities": []}, {"text": "Due to software copyright issues, we did not participate this online evaluation.", "labels": [], "entities": []}, {"text": "Instead, we use the development set (which is untouched during model training) as test set.", "labels": [], "entities": []}, {"text": "In training, if the correct answer is not in the candidate-answer set, we use the shortest candidate containing the correct answer as the target.", "labels": [], "entities": []}, {"text": "The Stanford parser is utilized to obtain the constituent parse trees for questions and passages.", "labels": [], "entities": []}, {"text": "In the parse tree, any internal node which has one child is merged together with its child.", "labels": [], "entities": []}, {"text": "For instance, in \"(NP (NNS sufferers))\", the parent \"NP\" has only one child \"(NNS sufferers)\", we merge them into \"(NP sufferers)\".", "labels": [], "entities": []}, {"text": "We use 300-dimensional word embeddings from GloVe) to initialize the model.", "labels": [], "entities": []}, {"text": "Words not found in GloVe are initialized as zero vectors.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.8938415050506592}]}, {"text": "We use a feed-forward network with 2 hidden layers (both having the same amount of units) for answer prediction.", "labels": [], "entities": [{"text": "answer prediction", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.9402879774570465}]}, {"text": "The activation function is set to rectified linear.", "labels": [], "entities": []}, {"text": "Hyperparameters in CCNN are tuned via 5-fold cross validation (CV) on the training set, summarized in.", "labels": [], "entities": [{"text": "CCNN", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.8896693587303162}, {"text": "5-fold cross validation (CV)", "start_pos": 38, "end_pos": 66, "type": "METRIC", "confidence": 0.6709022472302119}]}, {"text": "We use the ADAM ( optimizer to train the model with an initial learning rate 0.001 and a mini-batch size 100.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.7714387774467468}]}, {"text": "An ensemble model is also trained, consisting of 10 training runs using the same hyperparameters.", "labels": [], "entities": []}, {"text": "The performance is evaluated by two metrics: (1) exact match (EM) which measures the percentage of predictions that match anyone of the ground truth answers exactly; (2) F1 score which measures the average overlap between the prediction and ground truth answer.", "labels": [], "entities": [{"text": "exact match (EM)", "start_pos": 49, "end_pos": 65, "type": "METRIC", "confidence": 0.9790910601615905}, {"text": "F1 score", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9888058304786682}]}, {"text": "In the development set each question has about three ground truth answers.", "labels": [], "entities": []}, {"text": "F1 scores with the best matching answers are used to compute the average F1 score.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9845376014709473}, {"text": "F1 score", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9765737056732178}]}, {"text": "shows the performance of our model and previous approaches on the development set.", "labels": [], "entities": []}, {"text": "CCNN (single model) achieves an EM score of 69.3% and an F1 score of 78.5%, significantly outperforming all previous approaches (single model).", "labels": [], "entities": [{"text": "CCNN", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9204738736152649}, {"text": "EM score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.986856609582901}, {"text": "F1 score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9933730661869049}]}, {"text": "Through ensembling, the performance of CCNN is further improved and outperforms the baseline ensemble methods.", "labels": [], "entities": []}, {"text": "The key difference between our method and previous approaches is that CCNN is constituent-centric where the generation and encoding of candidate answers are both based on constituents while the baseline approaches are mostly word-based where the candidate answer is an arbitrary span of words and the encoding is performed over individual words rather than at the constituent level.", "labels": [], "entities": []}, {"text": "The constituent-centric model-design enjoys two major benefits.", "labels": [], "entities": []}, {"text": "First, restricting the candidate answers from arbitrary spans to neighborhoods around the constituents greatly reduces the search space, which mitigates the ambiguity and uncertainty in picking up the correct answer.", "labels": [], "entities": []}, {"text": "Second, the tree LSTMs and tree-guided attention mechanism encapsulate the syntactic, hierarchical and compositional structure among constituents, which leads to better representation learning of the candidate answers.", "labels": [], "entities": []}, {"text": "We conjecture these are the primary reasons that CCNN outperforms the baselines and provide a validation in the next section.", "labels": [], "entities": []}], "tableCaptions": []}