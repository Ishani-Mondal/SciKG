{"title": [{"text": "Alternative Objective Functions for Training MT Evaluation Metrics", "labels": [], "entities": [{"text": "MT Evaluation Metrics", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.8940312266349792}]}], "abstractContent": [{"text": "MT evaluation metrics are tested for correlation with human judgments either at the sentence-or the corpus-level.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9012111127376556}]}, {"text": "Trained metrics ignore corpus-level judgments and are trained for high sentence-level correlation only.", "labels": [], "entities": []}, {"text": "We show that training only for one objective (sentence or corpus level), cannot only harm the performance on the other objective, but it can also be subopti-mal for the objective being optimized.", "labels": [], "entities": []}, {"text": "To this end we present a metric trained for corpus-level and show empirical comparison against a metric trained for sentence-level exemplifying how their performance may vary per language pair, type and level of judgment.", "labels": [], "entities": []}, {"text": "Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than-and on average outperforms-both models on both objectives.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ever since BLEU () many proposals for an improved automatic evaluation metric for Machine Translation (MT) have been made.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9626848697662354}, {"text": "Machine Translation (MT)", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.8435080051422119}]}, {"text": "Some proposals use additional information for extracting quality indicators, like paraphrasing (), syntactic trees (Liu and or shallow semantics () etc.", "labels": [], "entities": []}, {"text": "Whereas others use different matching strategies, like n-grams (), treelets (Liu and) and skip-bigrams ().", "labels": [], "entities": []}, {"text": "Most metrics use several indicators of translation quality which are often combined in a linear model whose weights are estimated on a training set of human judgments.", "labels": [], "entities": [{"text": "translation", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.9634832143783569}]}, {"text": "Because the most widely available type of human judgments are relative ranking (RR) judgments, the main machine learning method used for training the metrics were based on the learningto-rank framework.", "labels": [], "entities": []}, {"text": "While the effectiveness of this framework for training evaluation metrics has been confirmed many times, e.g.,, so far there is no prior work exploring alternative objective functions for training learning-to-rank models.", "labels": [], "entities": []}, {"text": "Without exception, all existing learning-to-rank models are trained to rank sentences while completely ignoring the corpora judgments, likely because human judgments come in the form of sentence rankings.", "labels": [], "entities": []}, {"text": "It might seem that sentence and corpus level tasks are very similar but that is not the case.", "labels": [], "entities": []}, {"text": "Empirically it has been shown that many metrics that perform well on the sentence level do not perform well on the corpus level and vice versa.", "labels": [], "entities": []}, {"text": "By training to rank sentences the model does not necessarily learn to give scores that are well scaled, but only to give higher scores to better translations.", "labels": [], "entities": []}, {"text": "Training for the corpus level score would force the metric to give well scaled scores on the sentence level.", "labels": [], "entities": []}, {"text": "Human judgments of sentences can be aggregated in different ways to hypothesize human judgments of full corpora.", "labels": [], "entities": []}, {"text": "However, this fact has not been used so far to train learning-to-rank models that are good for ranking different corpora.", "labels": [], "entities": []}, {"text": "This work fills-in this gap by exploring the merits of different objective functions that take corpus level judgments into consideration.", "labels": [], "entities": []}, {"text": "We first create a learning-to-rank model for ranking corpora and compare it to the standard learning-to-rank model that is trained for ranking sentences.", "labels": [], "entities": []}, {"text": "This comparison shows that performance of these two objectives can vary radically depending on the chosen meta-evaluation method.", "labels": [], "entities": []}, {"text": "To tackle this prob-: Computation Graph lem we contribute anew objective function, inspired by multi-task learning, in which we train for both objectives simultaneously.", "labels": [], "entities": []}, {"text": "This multiobjective model behaves a lot more stable overall methods of meta-evaluation and achieves a higher correlation than both single objective models.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments are conducted on WMT13), WMT14) and WMT16 () datasets which were used as training, validation and testing datasets respectively.", "labels": [], "entities": [{"text": "WMT13", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.9624761343002319}, {"text": "WMT14", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.9094125628471375}, {"text": "WMT16 () datasets", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.7991845011711121}]}, {"text": "All of the models are implemented using TensorFlow and trained with L2 regularization \u03bb = 0.001 and ADAM optimizer with learning rate 0.001.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.8707826733589172}]}, {"text": "The mini-batch size for sentence level judgments is 2000 and for the corpus level is one comparison.", "labels": [], "entities": []}, {"text": "Each model is trained for 200 epochs out of which the one performing best on the validation set for the objective function being optimized is used during the test time.", "labels": [], "entities": []}, {"text": "We show the results for the relative ranking (RR) judgments correlation in.", "labels": [], "entities": [{"text": "relative ranking (RR) judgments correlation", "start_pos": 28, "end_pos": 71, "type": "METRIC", "confidence": 0.8284466522080558}]}, {"text": "For all language pairs that are of the form en-X we show it under the column X and for all the language pairs that have English on the target side we present their average under the column en.", "labels": [], "entities": []}, {"text": "RR corpus vs. sentence objective The corpusobjective is better than the sentence-objective for both corpus and sentence level RR judgments on 5 out of 7 languages and also on average correlation.", "labels": [], "entities": []}, {"text": "RR joint vs. single-objectives Training for the joint objective improves even more on both levels of RR correlation and outperforms both singleobjective models on average and on 4 out of 7 languages.", "labels": [], "entities": []}, {"text": "Making confident conclusions from these results is difficult because, to the best of our knowledge, there is no principled way of measuring statistical significance on the RR judgments.", "labels": [], "entities": [{"text": "RR judgments", "start_pos": 172, "end_pos": 184, "type": "TASK", "confidence": 0.8644315004348755}]}, {"text": "That is why we also tested on direct assessment (DA) judgments available from WMT16.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.949150025844574}]}, {"text": "On DA we can measure statistical significance on the sentence level using Williams test and on the corpus level using combination of hybrid-supersampling and Williams test.", "labels": [], "entities": []}, {"text": "The results of correlation with human judgment are for sentence and corpus level are shown in: Direct Assessment (DA) Pearson r Correlation.", "labels": [], "entities": [{"text": "Direct Assessment (DA) Pearson r Correlation", "start_pos": 95, "end_pos": 139, "type": "METRIC", "confidence": 0.7810459099709988}]}, {"text": "Super-and sub-scripts S, C and J signify that the model outperforms with statistical significance (p < 0.05) the model trained for sentence, corpus or joint objective respectively.", "labels": [], "entities": []}, {"text": "Bold marks that the system has outperformed both other models significantly.", "labels": [], "entities": []}, {"text": "DA corpus vs. other objectives On DA judgments the results for corpus level objective are completely different than on the RR judgments.", "labels": [], "entities": []}, {"text": "On DA judgments the corpus-objective model is significantly outperformed on both levels and on all languages by both of the other objectives.", "labels": [], "entities": []}, {"text": "This shows that gambling on one objective function (being that sentence or corpus level objective) could give unpredictable results.", "labels": [], "entities": []}, {"text": "This is precisely the motivation for creating the joint model with multi-objective training.", "labels": [], "entities": []}, {"text": "DA joint vs. single objectives By choosing to jointly optimize both objectives we get a much more stable model that performs well both on DA and RR judgments and on both levels of judgment.", "labels": [], "entities": []}, {"text": "On the DA sentence level, the joint model was not outperformed by any other model and on 3 out of 7 language pairs it significantly outperforms both alternative objectives.", "labels": [], "entities": []}, {"text": "On the corpus level results area bit mixed, but still joint objective outperforms both other models on 4 out of 7 language pairs and also it gives higher correlation on average.", "labels": [], "entities": [{"text": "correlation", "start_pos": 154, "end_pos": 165, "type": "METRIC", "confidence": 0.949944257736206}]}], "tableCaptions": [{"text": " Table 1: Relative Ranking (RR) Correlation. The corpus level correlation is measured with Pearson r  and sentence level with Kendall \u03c4", "labels": [], "entities": [{"text": "Relative Ranking (RR) Correlation", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.8808594246705373}, {"text": "Pearson r", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9679054915904999}]}, {"text": " Table 2: Direct Assessment (DA) Pearson r Correlation. Super-and sub-scripts S, C and J signify that  the model outperforms with statistical significance (p < 0.05) the model trained for sentence, corpus or  joint objective respectively. Bold marks that the system has outperformed both other models significantly.", "labels": [], "entities": [{"text": "Pearson r Correlation", "start_pos": 33, "end_pos": 54, "type": "METRIC", "confidence": 0.9266788760821024}]}]}