{"title": [{"text": "Context Sensitive Lemmatization Using Two Successive Bidirectional Gated Recurrent Networks", "labels": [], "entities": [{"text": "Context Sensitive Lemmatization", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9042824308077494}]}], "abstractContent": [{"text": "We introduce a composite deep neural network architecture for supervised and language independent context sensitive lemmatization.", "labels": [], "entities": []}, {"text": "The proposed method considers the task as to identify the correct edit tree representing the transformation between a word-lemma pair.", "labels": [], "entities": []}, {"text": "To find the lemma of a surface word, we exploit two successive bidirectional gated recurrent structures-the first one is used to extract the character level dependencies and the next one captures the contextual information of the given word.", "labels": [], "entities": []}, {"text": "The key advantages of our model compared to the state-of-the-art lemmatizers such as Lem-ming and Morfette are-(i) it is independent of human decided features (ii) except the gold lemma, no other expensive morphological attribute is required for joint learning.", "labels": [], "entities": []}, {"text": "We evaluate the lemmatizer on nine languages-Bengali, Catalan, Dutch, Hindi, Hungarian, Italian, Latin, Roma-nian and Spanish.", "labels": [], "entities": []}, {"text": "It is found that except Bengali, the proposed method outper-forms Lemming and Morfette on the other languages.", "labels": [], "entities": []}, {"text": "To train the model on Ben-gali, we develop a gold lemma annotated dataset 1 (having 1, 702 sentences with a total of 20, 257 word tokens), which is an additional contribution of this work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lemmatization is the process to determine the root/dictionary form of a surface word.", "labels": [], "entities": []}, {"text": "Morphologically rich languages suffer due to the existence of various inflectional and derivational variations of a root depending on several linguistic properties such as honorificity, parts of speech (POS), person, tense etc.", "labels": [], "entities": []}, {"text": "Lemmas map the related word forms to lexical resources thus identifying them as the members of the same group and providing their semantic and syntactic information.", "labels": [], "entities": []}, {"text": "Stemming is away similar to lemmatization producing the common portion of variants but it has several limitations -(i) there is no guarantee of a stem to be a legitimate word form (ii) words are considered in isolation.", "labels": [], "entities": []}, {"text": "Hence, for context sensitive languages i.e. where same inflected word form may come from different sources and can only be disambiguated by considering its neighbouring information, there lemmatization defines the foremost task to handle diverse text processing problems (e.g. sense disambiguation, parsing, translation).", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 277, "end_pos": 297, "type": "TASK", "confidence": 0.7245497703552246}, {"text": "parsing", "start_pos": 299, "end_pos": 306, "type": "TASK", "confidence": 0.9062771201133728}, {"text": "translation", "start_pos": 308, "end_pos": 319, "type": "TASK", "confidence": 0.8864666223526001}]}, {"text": "The key contributions of this work are as follows.", "labels": [], "entities": []}, {"text": "We address context sensitive lemmatization introducing a two-stage bidirectional gated recurrent neural network (BGRNN) architecture.", "labels": [], "entities": []}, {"text": "Our model is a supervised one that needs lemma tagged continuous text to learn.", "labels": [], "entities": []}, {"text": "Its two most important advantages compared to the state-ofthe-art supervised models ( are -(i) we do not need to define hand-crafted features such as the word form, presence of special characters, character alignments, surrounding words etc.", "labels": [], "entities": [{"text": "character alignments", "start_pos": 197, "end_pos": 217, "type": "TASK", "confidence": 0.687183678150177}]}, {"text": "(ii) parts of speech and other morphological attributes of the surface words are not required for joint learning.", "labels": [], "entities": []}, {"text": "Additionally, unknown word forms are also taken care of as the transformation between word-lemma pair is learnt, not the lemma itself.", "labels": [], "entities": []}, {"text": "We exploit two steps learning in our method.", "labels": [], "entities": []}, {"text": "At first, characters in the words are passed sequentially through a BGRNN to get a syntactic embedding of each word and then the outputs are combined with the corresponding semantic embeddings.", "labels": [], "entities": [{"text": "BGRNN", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.840650737285614}]}, {"text": "Finally, mapping between the combined embeddings to word-lemma transformations are learnt using another BGRNN.", "labels": [], "entities": [{"text": "BGRNN", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.539444625377655}]}, {"text": "For the present work, we assess our model on nine languages having diverse morphological variations.", "labels": [], "entities": []}, {"text": "Out of them, two (Bengali and Hindi) belong to the Indic languages family and the rests (Catalan, Dutch, Hungarian, Italian, Latin, Romanian and Spanish) are taken from the European languages.", "labels": [], "entities": []}, {"text": "To evaluate the proposed model on Bengali, a lemma annotated continuous text has been developed.", "labels": [], "entities": []}, {"text": "As so far there is no such standard large dataset for supervised lemmatization in Bengali, the prepared one would surely contribute to the respective NLP research community.", "labels": [], "entities": []}, {"text": "For the remaining languages, standard datasets are used for experimentation.", "labels": [], "entities": []}, {"text": "Experimental results reveal that our method outperforms) and Morfette ( on all the languages except Bengali.", "labels": [], "entities": [{"text": "Morfette", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.7454600930213928}]}], "datasetContent": [{"text": "Out of the nine reference languages, initially we choose four of them (Bengali, Hindi, Latin and Spanish) for in-depth analysis.", "labels": [], "entities": []}, {"text": "We conduct an exhaustive set of experiments -such as determining the direct lemmatization accuracy, accuracy obtained without using applicable edit trees in training, measuring the model's performance on the unseen words etc.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.8920757174491882}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.999117910861969}]}, {"text": "Later we consider five more languages (Catalan, Dutch, Hungarian, Italian and Romanian) mostly for testing the generalization ability of the proposed method.", "labels": [], "entities": []}, {"text": "For these additional languages, we present only the lemmatization accuracy in section 3.2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9358497262001038}]}, {"text": "Datasets: As Bengali is a low-resourced language, a relatively large lemma annotated dataset is prepared for the present work using Tagore's short stories collection.", "labels": [], "entities": []}, {"text": "We assess the lemmatization performance by measuring the direct accuracy which is the ratio of the number of correctly lemmatized words to the total number of input words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.936722457408905}]}, {"text": "The experiments are performed using 4 fold cross validation technique i.e. the datasets are equi-partitioned into 4 parts at sentence level and then each part is tested exactly once using the model trained on the remaining 3 parts.", "labels": [], "entities": []}, {"text": "Finally, we report the average accuracy over 4 fold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9987143278121948}]}, {"text": "Induction of Edit Tree Set: Initially, distinct edit trees are induced from the word-lemma pairs present in the training set.", "labels": [], "entities": []}, {"text": "Next, the words in the training data are annotated with their corresponding edit trees.", "labels": [], "entities": []}, {"text": "Training is accomplished on this edit tree tagged text.", "labels": [], "entities": []}, {"text": "plots the growth of the edit tree set against the number of word-lemma samples in the four languages.", "labels": [], "entities": []}, {"text": "With the increase of samples, the size of edit tree set gradually converges revealing the fact that most of the frequent transformation patterns (both regular and irregular) are covered by the induction process.", "labels": [], "entities": []}, {"text": "From  , morphological richness can be compared across the languages.", "labels": [], "entities": []}, {"text": "When convergence happens quickly i.e. at relatively less number of samples, it evidences that the language is less complex.", "labels": [], "entities": []}, {"text": "Among the four reference languages, Latin stands out as the most intricate, followed by Bengali, Spanish and Hindi.", "labels": [], "entities": []}, {"text": "Semantic Embeddings: We obtain the distributional word vectors for Bengali and Hindi by training the word2vec model on FIRE Bengali and Hindi news corpora . Following the work by, continuous-bag-ofwords architecture with negative sampling is used to get 200 dimensional word vectors.", "labels": [], "entities": [{"text": "FIRE Bengali and Hindi news corpora", "start_pos": 119, "end_pos": 154, "type": "DATASET", "confidence": 0.827616959810257}]}, {"text": "For Latin and Spanish, we use the embeddings released by and Cardellino (2016) 8 respectively.", "labels": [], "entities": []}, {"text": "Syntactic Representation: We acquire the statistics of word length versus frequency from the datasets and find out that irrespective of the languages, longer words (have more than 20-25 characters) are few in numbers.", "labels": [], "entities": [{"text": "Syntactic Representation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8872420191764832}]}, {"text": "Based on this finding, each word is limited to a sequence of 25 characters.", "labels": [], "entities": []}, {"text": "Smaller words are padded null characters at the end and for the longer words, excess characters are truncated out.", "labels": [], "entities": []}, {"text": "So, each word is represented as a 25 length array of one hot encoded vectors which is given input to the embedding layer that works as a lookup table producing an equal length array of embedded vectors.", "labels": [], "entities": []}, {"text": "Initialization of the embedding layer is done randomly and the embedded vector dimension is set to 10.", "labels": [], "entities": []}, {"text": "Eventually, the output of the embedding layer is passed to the first level BGRNN for learning the syntactic representation.", "labels": [], "entities": [{"text": "BGRNN", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.9151609539985657}]}, {"text": "Hyper Parameters: There are several hyper parameters in our model such as the number of neurons in the hidden layer (h t ) of both first and second level BGRNN, learning mode, number of epochs to train the models, optimization algorithm, dropout rate etc.", "labels": [], "entities": [{"text": "BGRNN", "start_pos": 154, "end_pos": 159, "type": "DATASET", "confidence": 0.6704825162887573}]}, {"text": "We experiment with different settings of these parameters and report where optimum results are achieved.", "labels": [], "entities": []}, {"text": "For both the bidirectional networks, number of hidden layer neurons is set to 64.", "labels": [], "entities": []}, {"text": "Online learning is applied for updation of the weights.", "labels": [], "entities": []}, {"text": "Number of epochs varies across languages to converge the training.", "labels": [], "entities": []}, {"text": "It is maximum for Bengali (around 80 epochs), followed by Latin, Spanish and Hindi taking around 50, 35 and 15 respectively.", "labels": [], "entities": []}, {"text": "Throughout the experiments, we set the dropout rate as 0.2 to prevent over-fitting.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.9304982423782349}]}, {"text": "Different optimization algorithms like AdaDelta,), are explored.", "labels": [], "entities": []}, {"text": "Out of them, Adam yields the best result.", "labels": [], "entities": []}, {"text": "We use the categorical cross-entropy as the loss function in our model.", "labels": [], "entities": []}, {"text": "Baselines: We compare our method with Lemming and Morfette . Both the model jointly learns lemma and other morphological tags in context.", "labels": [], "entities": []}, {"text": "Lemming uses a 2nd-order linear-chain CRF to predict the lemmas whereas, the current version of Morfette is based on structured perceptron learning.", "labels": [], "entities": []}, {"text": "As POS information is a compulsory requirement of these two models, the Bengali data is manually POS annotated.", "labels": [], "entities": []}, {"text": "For the other languages, the tags were already available.", "labels": [], "entities": []}, {"text": "Although this comparison is partially biased as the proposed method does not need POS information, but the experimental results show the effectiveness of our model.", "labels": [], "entities": []}, {"text": "There is an option in Lemming and Morfette to provide an exhaustive set of root words which is used to exploit the dictionary features i.e. to verify if a candidate lemma is a valid form or not.", "labels": [], "entities": []}, {"text": "To make the comparisons consistent, we do not exploit any external dictionary in our experiments.", "labels": [], "entities": []}, {"text": "As mentioned earlier, five additional languages (Catalan, Dutch, Hungarian, Italian and Romanian) are considered to test the generalization ability of the method.", "labels": [], "entities": []}, {"text": "The datasets are taken from the UD Treebanks 11.", "labels": [], "entities": [{"text": "UD Treebanks 11", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.9854347705841064}]}, {"text": "For each language, we merge the training and development data together and perform 4 fold cross validation on it to measure the average accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9983645081520081}]}, {"text": "The dataset statistics are shown in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset statistics of the 4 languages.", "labels": [], "entities": []}, {"text": " Table 2: Lemmatization accuracy (in %) without/with restricting output classes.", "labels": [], "entities": [{"text": "Lemmatization", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9516737461090088}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9114004373550415}]}, {"text": " Table 3: Lemmatization accuracy (in %) without using applicable edit trees in training.", "labels": [], "entities": [{"text": "Lemmatization", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9425826072692871}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9410197138786316}]}, {"text": " Table 5: Lemmatization accuracy (in %) on unseen words.", "labels": [], "entities": [{"text": "Lemmatization", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9489313364028931}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9215080142021179}]}, {"text": " Table 6: Lemmatization accuracy (in %) on unseen words without using applicable edit trees in training.", "labels": [], "entities": [{"text": "Lemmatization", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9177281260490417}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9092535972595215}]}, {"text": " Table 6. As expected, for every model,  the accuracy drops compared to the results shown  in", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9996155500411987}]}, {"text": " Table 5. The only exception that we find out  is in the entry for Hindi with BLSTM-BLSTM.  Though without restricting the output, the accu- racy in", "labels": [], "entities": [{"text": "BLSTM-BLSTM", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.8047329783439636}]}, {"text": " Table 7: Results (in %) obtained using semantic  and syntactic embeddings separately.", "labels": [], "entities": []}, {"text": " Table 8: Dataset statistics of the 5 additional lan- guages.", "labels": [], "entities": []}, {"text": " Table 9: Lemmatization accuracy (in %) for the 5 languages.", "labels": [], "entities": [{"text": "Lemmatization", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9794142842292786}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9122112393379211}]}]}