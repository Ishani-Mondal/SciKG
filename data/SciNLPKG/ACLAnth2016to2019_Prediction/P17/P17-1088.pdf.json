{"title": [{"text": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion", "labels": [], "entities": [{"text": "Interpretable Knowledge Transfer", "start_pos": 3, "end_pos": 35, "type": "TASK", "confidence": 0.5974448621273041}, {"text": "Knowledge Base Completion", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.6035901010036469}]}], "abstractContent": [{"text": "Knowledge bases are important resources fora variety of natural language processing tasks but suffer from incompleteness.", "labels": [], "entities": []}, {"text": "We propose a novel embedding model, ITransF, to perform knowledge base completion.", "labels": [], "entities": [{"text": "ITransF", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.8869263529777527}, {"text": "knowledge base completion", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.6900610526402792}]}, {"text": "Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts.", "labels": [], "entities": []}, {"text": "Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors , can be interpreted easily.", "labels": [], "entities": []}, {"text": "We evaluate ITransF on two benchmark datasets-WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, overall baselines that do not use additional information.", "labels": [], "entities": [{"text": "ITransF", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.8349394202232361}, {"text": "FB15k", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.6740893721580505}, {"text": "knowledge base completion", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.5634443561236063}]}], "introductionContent": [{"text": "Knowledge bases (KB), such as WordNet),, and DBpedia (, are useful resources for many applications such as question answering () and information extraction ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9602594971656799}, {"text": "DBpedia", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.8097021579742432}, {"text": "question answering", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.8946194052696228}, {"text": "information extraction", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.8110979497432709}]}, {"text": "However, knowledge bases suffer from incompleteness despite their formidable sizes), leading to a number of studies on automatic knowledge base completion (KBC) ( or link prediction.", "labels": [], "entities": [{"text": "knowledge base completion (KBC)", "start_pos": 129, "end_pos": 160, "type": "TASK", "confidence": 0.7013905743757883}, {"text": "link prediction", "start_pos": 166, "end_pos": 181, "type": "TASK", "confidence": 0.7384649217128754}]}, {"text": "The fundamental motivation behind these studies is that there exist some statistical regularities under the intertwined facts stored in the multirelational knowledge base.", "labels": [], "entities": []}, {"text": "By discovering generalizable regularities in known facts, missing ones maybe recovered in a faithful way.", "labels": [], "entities": []}, {"text": "Due to its excellent generalization capability, distributed representations, a.k.a. embeddings, have been popularized to address the KBC task.", "labels": [], "entities": []}, {"text": "As a seminal work, proposes the TransE, which models the statistical regularities with linear translations between entity embeddings operated by a relation embedding.", "labels": [], "entities": []}, {"text": "Implicitly, TransE assumes both entity embeddings and relation embeddings dwell in the same vector space, posing an unnecessarily strong prior.", "labels": [], "entities": []}, {"text": "To relax this requirement, a variety of models first project the entity embeddings to a relationdependent space), and then model the translation property in the projected space.", "labels": [], "entities": []}, {"text": "Typically, these relation-dependent spaces are characterized by the projection matrices unique to each relation.", "labels": [], "entities": []}, {"text": "As a benefit, different aspects of the same entity can be temporarily emphasized or depressed as an effect of the projection.", "labels": [], "entities": []}, {"text": "For instance,) utilizes two projection matrices per relation, one for the head entity and the other for the tail entity.", "labels": [], "entities": []}, {"text": "Despite the superior performance of STransE compared to TransE, it is more prone to the data sparsity problem.", "labels": [], "entities": []}, {"text": "Concretely, since the projection spaces are unique to each relation, projection matrices associated with rare relations can only be exposed to very few facts during training, resulting in poor generalization.", "labels": [], "entities": []}, {"text": "For common relations, a similar issue exists.", "labels": [], "entities": [{"text": "common relations", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.9222974479198456}]}, {"text": "Without any restrictions on the number of projection matrices, logically related or conceptually similar relations may have distinct projection spaces, hindering the discovery, sharing, and generalization of statistical regularities.", "labels": [], "entities": []}, {"text": "Previously, a line of research makes use of external information such as textual relations from web-scale corpus or node features (, alleviating the sparsity problem.", "labels": [], "entities": []}, {"text": "In parallel, recent work has proposed to model regularities beyond local facts by considering multirelation paths.", "labels": [], "entities": []}, {"text": "Since the number of paths grows exponentially with its length, as aside effect, path-based models enjoy much more training cases, suffering less from the problem.", "labels": [], "entities": []}, {"text": "In this paper, we propose an interpretable knowledge transfer model (ITransF), which encourages the sharing of statistic regularities between the projection matrices of relations and alleviates the data sparsity problem.", "labels": [], "entities": [{"text": "interpretable knowledge transfer", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.5564287801583608}]}, {"text": "At the core of ITransF is a sparse attention mechanism, which learns to compose shared concept matrices into relation-specific projection matrices, leading to a better generalization property.", "labels": [], "entities": [{"text": "ITransF", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.8840084075927734}]}, {"text": "Without any external resources, ITransF improves mean rank and Hits@10 on two benchmark datasets, overall previous approaches of the same kind.", "labels": [], "entities": [{"text": "mean rank", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9056061208248138}, {"text": "Hits@10", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9281320174535116}]}, {"text": "In addition, the parameter sharing is clearly indicated by the learned sparse attention vectors, enabling us to interpret how knowledge transfer is carried out.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7266408503055573}]}, {"text": "To induce the desired sparsity during optimization, we further introduce a block iterative optimization algorithm.", "labels": [], "entities": []}, {"text": "In summary, the contributions of this work are: (i) proposing a novel knowledge embedding model which enables knowledge transfer by learning to discover shared regularities; (ii) introducing a learning algorithm to directly optimize a sparse representation from which the knowledge transferring procedure is interpretable; (iii) showing the effectiveness of our model by outperforming baselines on two benchmark datasets for knowledge base completion task.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.7343149185180664}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of FB15k and WN18 used in  experiments. #E, #R denote the number of enti- ties and relation types respectively. #Train, #Valid  and #Test are the numbers of triples in the training,  validation and test sets respectively.", "labels": [], "entities": [{"text": "FB15k", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.8856126666069031}, {"text": "WN18", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.866159975528717}, {"text": "Train", "start_pos": 134, "end_pos": 139, "type": "METRIC", "confidence": 0.9519646763801575}, {"text": "Test", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.8631913065910339}]}, {"text": " Table 2: Link prediction results on two datasets. Higher Hits@10 or lower Mean Rank indicates better  performance. Following Nguyen et al. (2016b) and Shen et al. (2016), we divide the models into two  groups. The first group contains intrinsic models without using extra information. The second group  make use of additional information. Results in the brackets are another set of results STransE reported.", "labels": [], "entities": [{"text": "Link prediction", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8057056665420532}, {"text": "Mean Rank", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9745706915855408}, {"text": "STransE", "start_pos": 391, "end_pos": 398, "type": "DATASET", "confidence": 0.8709598779678345}]}, {"text": " Table 3: Performance of model with dense atten- tion vectors or sparse attention vectors. MR, H10  and Time denotes mean rank, Hits@10 and train- ing time per epoch respectively", "labels": [], "entities": [{"text": "MR", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.9887856245040894}, {"text": "Time", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9689249396324158}, {"text": "train- ing time", "start_pos": 140, "end_pos": 155, "type": "METRIC", "confidence": 0.8670121133327484}]}, {"text": " Table 4: Different methods to obtain sparse repre- sentations", "labels": [], "entities": []}]}