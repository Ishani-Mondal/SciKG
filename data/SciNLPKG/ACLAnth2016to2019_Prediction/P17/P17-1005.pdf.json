{"title": [{"text": "Learning Structured Natural Language Representations for Semantic Parsing", "labels": [], "entities": [{"text": "Learning Structured Natural Language Representations", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.5780207812786102}, {"text": "Semantic Parsing", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.6699092537164688}]}], "abstractContent": [{"text": "We introduce a neural semantic parser which is interpretable and scalable.", "labels": [], "entities": []}, {"text": "Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains.", "labels": [], "entities": []}, {"text": "The semantic parser is trained end-to-end using annotated logical forms or their de-notations.", "labels": [], "entities": []}, {"text": "We achieve the state of the art on SPADES and GRAPHQUESTIONS and obtain competitive results on GEO-QUERY and WEBQUESTIONS.", "labels": [], "entities": [{"text": "GRAPHQUESTIONS", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.9879242181777954}, {"text": "GEO-QUERY", "start_pos": 95, "end_pos": 104, "type": "DATASET", "confidence": 0.8229919672012329}, {"text": "WEBQUESTIONS", "start_pos": 109, "end_pos": 121, "type": "METRIC", "confidence": 0.6299203634262085}]}, {"text": "The induced predicate-argument structures shed light on the types of representations useful for semantic parsing and how these are different from linguistically motivated ones.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.7196613997220993}]}], "introductionContent": [{"text": "Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8256235718727112}]}, {"text": "Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 115, "end_pos": 131, "type": "TASK", "confidence": 0.7205178141593933}]}, {"text": "Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar.", "labels": [], "entities": []}, {"text": "Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded representation (.", "labels": [], "entities": []}, {"text": "A merit of the two-stage approach is that it creates reusable intermediate interpretations, which potentially enables the handling of unseen words and knowledge transfer across domains.", "labels": [], "entities": [{"text": "handling of unseen words", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.8326167613267899}]}, {"text": "The successful application of encoder-decoder models () to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 119, "end_pos": 135, "type": "TASK", "confidence": 0.7439236044883728}]}, {"text": "Such models still fall under the first approach, however, in contrast to previous work () they reduce the need for domain-specific assumptions, grammar learning, and more generally extensive feature engineering.", "labels": [], "entities": []}, {"text": "But this modeling flexibility comes at a cost since it is no longer possible to interpret how meaning composition is performed.", "labels": [], "entities": []}, {"text": "Such knowledge plays a critical role in understand modeling limitations so as to build better semantic parsers.", "labels": [], "entities": []}, {"text": "Moreover, without any taskspecific prior knowledge, the learning problem is fairly unconstrained, both in terms of the possible derivations to consider and in terms of the target output which can be ill-formed (e.g., with extra or missing brackets).", "labels": [], "entities": []}, {"text": "In this work, we propose a neural semantic parser that alleviates the aforementioned problems.", "labels": [], "entities": [{"text": "neural semantic parser", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.6903389692306519}]}, {"text": "Our model falls under the second class of approaches where utterances are first mapped to an intermediate representation containing natural language predicates.", "labels": [], "entities": []}, {"text": "However, rather than using an external parser ( or manually specified CCG grammars (, we induce intermediate representations in the form of predicate-argument structures from data.", "labels": [], "entities": []}, {"text": "This is achieved with a transition-based approach which by design yields recursive semantic structures, avoiding the problem of generating ill-formed meaning representations.", "labels": [], "entities": []}, {"text": "Compared to most existing semantic parsers which employ a CKY style bottom-up parsing strategy), the transition-based approach we proposed does not require feature decomposition over structures and thereby enables the exploration of rich, non-local features.", "labels": [], "entities": []}, {"text": "The output of the transition system is then grounded (e.g., to a knowledge base) with a neural mapping model under the assumption that grounded and ungrounded structures are isomorphic.", "labels": [], "entities": []}, {"text": "As a result, we obtain a neural model that jointly learns to parse natural language semantics and induce a lexicon that helps grounding.", "labels": [], "entities": []}, {"text": "The whole network is trained end-to-end on natural language utterances paired with annotated logical forms or their denotations.", "labels": [], "entities": []}, {"text": "We conduct experiments on four datasets, including GEOQUERY (which has logical forms; Zelle and Mooney 1996), SPADES, WEB-QUESTIONS (, and GRAPH-QUESTIONS () (which have denotations).", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8834322690963745}, {"text": "SPADES", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.6264455318450928}, {"text": "WEB-QUESTIONS", "start_pos": 118, "end_pos": 131, "type": "METRIC", "confidence": 0.8593184351921082}, {"text": "GRAPH-QUESTIONS", "start_pos": 139, "end_pos": 154, "type": "METRIC", "confidence": 0.994719386100769}]}, {"text": "Our semantic parser achieves the state of the art on SPADES and GRAPHQUESTIONS, while obtaining competitive results on GEOQUERY and WEBQUESTIONS.", "labels": [], "entities": [{"text": "GRAPHQUESTIONS", "start_pos": 64, "end_pos": 78, "type": "METRIC", "confidence": 0.9926162362098694}, {"text": "GEOQUERY", "start_pos": 119, "end_pos": 127, "type": "DATASET", "confidence": 0.8402083516120911}, {"text": "WEBQUESTIONS", "start_pos": 132, "end_pos": 144, "type": "DATASET", "confidence": 0.45565804839134216}]}, {"text": "A side-product of our modeling framework is that the induced intermediate representations can contribute to rationalizing neural predictions (.", "labels": [], "entities": []}, {"text": "Specifically, they can shed light on the kinds of representations (especially predicates) useful for semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.7599575221538544}]}, {"text": "Evaluation of the induced predicate-argument relations against syntax-based ones reveals that they are interpretable and meaningful compared to heuristic baselines, but they sometimes deviate from linguistic conventions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we verify empirically that our semantic parser derives useful meaning representations.", "labels": [], "entities": []}, {"text": "We give details on the evaluation datasets and baselines used for comparison.", "labels": [], "entities": []}, {"text": "We also describe implementation details and the features used in the discriminative ranker.", "labels": [], "entities": []}, {"text": "We evaluated our model on the following datasets which cover different domains, and use different types of training data, i.e., pairs of natural language utterances and grounded meanings or question-answer pairs.", "labels": [], "entities": []}, {"text": "GEOQUERY (Zelle and Mooney, 1996) contains 880 questions and database queries about US geography.", "labels": [], "entities": [{"text": "GEOQUERY (Zelle and Mooney, 1996)", "start_pos": 0, "end_pos": 33, "type": "DATASET", "confidence": 0.8638633638620377}, {"text": "US geography", "start_pos": 84, "end_pos": 96, "type": "TASK", "confidence": 0.3814735412597656}]}, {"text": "The utterances are compositional, but the language is simple and vocabulary size small.", "labels": [], "entities": []}, {"text": "The majority of questions include at most one entity.", "labels": [], "entities": []}, {"text": "SPADES () contains 93,319 questions derived from CLUEWEB09 () sentences.", "labels": [], "entities": []}, {"text": "Specifically, the questions were created by randomly removing an entity, thus producing sentence-denotation pairs ().", "labels": [], "entities": []}, {"text": "The sentences include two or more entities and although they are not very compositional, they constitute a large-scale dataset for neural network training.", "labels": [], "entities": []}, {"text": "WEBQUESTIONS ( contains 5,810 question-answer pairs.", "labels": [], "entities": [{"text": "WEBQUESTIONS", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9221145510673523}]}, {"text": "Similar to SPADES, it is based on Freebase and the questions are not very compositional.", "labels": [], "entities": [{"text": "SPADES", "start_pos": 11, "end_pos": 17, "type": "TASK", "confidence": 0.7038750052452087}]}, {"text": "However, they are real questions asked by people on the Web.", "labels": [], "entities": []}, {"text": "Finally, GRAPHQUESTIONS (Su et al., 2016) contains 5,166 question-answer pairs which were created by showing 500 Freebase graph queries to Amazon Mechanical Turk workers and asking them to paraphrase them into natural language.", "labels": [], "entities": [{"text": "GRAPHQUESTIONS", "start_pos": 9, "end_pos": 23, "type": "METRIC", "confidence": 0.9643692970275879}]}], "tableCaptions": [{"text": " Table 4: GRAPHQUESTIONS results. Numbers for  comparison systems are from Su et al. (2016).", "labels": [], "entities": [{"text": "GRAPHQUESTIONS", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9915462136268616}]}, {"text": " Table 7: GEOQUERY evaluation of ungrounded  meaning representations. We report accuracy  against a manually created gold standard.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.5848613977432251}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.999431312084198}]}, {"text": " Table 8: Evaluation of predicates induced by  SCANNER against EASYCCG. We report F1(%)  across datasets. For SPADES, we also provide a  breakdown for various utterance types.", "labels": [], "entities": [{"text": "EASYCCG", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.8648266792297363}, {"text": "F1", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9993533492088318}, {"text": "SPADES", "start_pos": 110, "end_pos": 116, "type": "TASK", "confidence": 0.865081787109375}]}]}