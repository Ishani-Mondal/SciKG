{"title": [{"text": "Detecting Good Arguments in a Non-Topic-Specific Way: An Oxymoron?", "labels": [], "entities": [{"text": "Detecting Good Arguments", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9040362040201823}]}], "abstractContent": [{"text": "Automatic identification of good arguments on a controversial topic has applications in civics and education, to name a few.", "labels": [], "entities": [{"text": "Automatic identification of good arguments on a controversial topic", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.8431420293119218}]}, {"text": "While in the civics context it might be acceptable to create separate models for each topic, in the context of scoring of stu-dents' writing there is a preference fora single model that applies to all responses.", "labels": [], "entities": []}, {"text": "Given that good arguments for one topic are likely to be irrelevant for another, is a single model for detecting good arguments a contradiction in terms?", "labels": [], "entities": []}, {"text": "We investigate the extent to which it is possible to close the performance gap between topic-specific and across-topics models for identification of good arguments.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Classification accuracies for generic vs  non-generic sentences. Our best results for same- topic and across-topics scenarios are boldfaced.", "labels": [], "entities": []}, {"text": " Table 2: Weights of the transition features.", "labels": [], "entities": [{"text": "Weights", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.949325442314148}]}, {"text": " Table 3: Performance of feature selection, for the  dr pn+1-3gr ppos model, across topics.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.655743345618248}]}]}