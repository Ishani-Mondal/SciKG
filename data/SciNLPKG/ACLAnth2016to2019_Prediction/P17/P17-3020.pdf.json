{"title": [{"text": "Modeling Situations in Neural Chat Bots", "labels": [], "entities": [{"text": "Modeling Situations in Neural Chat Bots", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7127929826577505}]}], "abstractContent": [{"text": "Social media accumulates vast amounts of online conversations that enable data-driven modeling of chat dialogues.", "labels": [], "entities": []}, {"text": "It is, however, still hard to utilize the neural network-based SEQ2SEQ model for dialogue modeling in spite of its acknowledged success in machine translation.", "labels": [], "entities": [{"text": "dialogue modeling", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.9590240716934204}, {"text": "machine translation", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.736085832118988}]}, {"text": "The main challenge comes from the high degrees of freedom of outputs (responses).", "labels": [], "entities": []}, {"text": "This paper presents neural conversational models that have general mechanisms for handling a variety of situations that affect our responses.", "labels": [], "entities": []}, {"text": "Response selection tests on massive dialogue data we have collected from Twitter confirmed the effectiveness of the proposed models with situations derived from utterances, users or time.", "labels": [], "entities": []}], "introductionContent": [{"text": "The increasing amount of dialogue data in social media has opened the door to data-driven modeling of non-task-oriented, or chat, dialogues).", "labels": [], "entities": []}, {"text": "The data-driven models assume a response generation as a sequence to sequence mapping task, and recent ones are based on neural SEQ2SEQ models (.", "labels": [], "entities": [{"text": "response generation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7053988873958588}]}, {"text": "However, the adequacy of responses generated by these neural models is somewhat insufficient, in contrast to the acknowledged success of the neural SEQ2SEQ models in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 166, "end_pos": 185, "type": "TASK", "confidence": 0.7632046639919281}]}, {"text": "The contrasting outcomes in machine translation and chat dialogue modeling can be explained by the difference in the degrees of freedom on output fora given input.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7901038527488708}, {"text": "chat dialogue modeling", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.724209596713384}]}, {"text": "An appropriate response to a given utterance is not monolithic in chat dialogue.", "labels": [], "entities": []}, {"text": "Nevertheless, since only one ground truth response is provided in the actual dialogue data, the supervised systems will hesitate when choosing from the vast range of possible responses.", "labels": [], "entities": []}, {"text": "So, how do humans decide how to respond?", "labels": [], "entities": []}, {"text": "We converse with others while (implicitly) considering not only the utterance but also other various conversational situations ( \u00a7 2) such as time, place, and the current context of conversation and even our relationship with the addressee.", "labels": [], "entities": []}, {"text": "For example, when a friend says \"I feel so sleepy.\" in the morning, a probable response could be \"Were you up all night?\").", "labels": [], "entities": []}, {"text": "If the friend says the same thing at midnight, you might say \"It's time to go to bed.\"", "labels": [], "entities": []}, {"text": "Or if the friend is driving a car with you, you might answer \"If you fall asleep, we'll die.\"", "labels": [], "entities": []}, {"text": "Modeling situations behind conversations has been an open problem in chat dialogue modeling, and this difficulty has partly forced us to focus on task-oriented dialogue systems, the response of which has a low degree of freedom thanks to domain and goal specificity.", "labels": [], "entities": [{"text": "Modeling situations behind conversations", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.850133404135704}, {"text": "chat dialogue modeling", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.7218182484308878}]}, {"text": "Although a few studies have tried to exploit conversational situations such as speakers' emo-tions ( or personal characteristics () and topics (, the methods are specially designed for and evaluated using specific types of situations.", "labels": [], "entities": []}, {"text": "In this study, we explore neural conversational models that have general mechanisms to incorporate various types of situations behind chat conversations ( \u00a7 3.2).", "labels": [], "entities": []}, {"text": "These models take into account situations on the speaker's side and the addressee's side (or those who respond) when encoding utterances and decoding its responses, respectively.", "labels": [], "entities": []}, {"text": "To capture the conversational situations, we design two mechanisms that differ in how strong of an effect a given situation has on generating responses.", "labels": [], "entities": []}, {"text": "In experiments, we examined the proposed conversational models by incorporating three types of concrete conversational situations ( \u00a7 2): utterance, speaker/addressee (profiles), and time (season), respectively.", "labels": [], "entities": []}, {"text": "Although the models are capable of generating responses, we evaluate the models with a response selection test to avoid known issues in automatic evaluation metrics of generated responses ().", "labels": [], "entities": []}, {"text": "Experimental results obtained using massive dialogue data from Twitter showed that modeling conversational situations improved the relevance of responses ( \u00a7 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our situation-aware neural conversational models on massive dialogue data obtained from Twitter.", "labels": [], "entities": []}, {"text": "We compare our models ( \u00a7 3.2) with SEQ2SEQ baseline ( \u00a7 3.1) using a response selection test instead of evaluating generated responses, since recently pointed out several problems of existing metrics such as BLEU () for evaluating generated responses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 209, "end_pos": 213, "type": "METRIC", "confidence": 0.9983088970184326}]}], "tableCaptions": [{"text": " Table 1: Statistics of our dialogue datasets (train- ing, validation, and test portions are merged here).", "labels": [], "entities": []}, {"text": " Table 2: Hyperparameters for training.", "labels": [], "entities": []}, {"text": " Table 3: Results of the response selection test.", "labels": [], "entities": [{"text": "response selection", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9223805069923401}]}]}