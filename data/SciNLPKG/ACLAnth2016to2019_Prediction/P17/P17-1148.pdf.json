{"title": [{"text": "Learning Semantic Correspondences in Technical Documentation", "labels": [], "entities": []}], "abstractContent": [{"text": "We consider the problem of translating high-level textual descriptions to formal representations in technical documentation as part of an effort to model the meaning of such documentation.", "labels": [], "entities": [{"text": "translating high-level textual descriptions to formal representations in technical documentation", "start_pos": 27, "end_pos": 123, "type": "TASK", "confidence": 0.7987402141094208}]}, {"text": "We focus specifically on the problem of learning translational correspondences between text descriptions and grounded representations in the target documentation, such as formal representation of functions or code templates.", "labels": [], "entities": [{"text": "learning translational correspondences between text descriptions", "start_pos": 40, "end_pos": 104, "type": "TASK", "confidence": 0.7343910386164983}]}, {"text": "Our approach exploits the parallel nature of such documentation, or the tight coupling between high-level text and the low-level representations we aim to learn.", "labels": [], "entities": []}, {"text": "Data is collected by mining technical documents for such parallel text-representation pairs, which we use to train a simple semantic parsing model.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.7472425997257233}]}, {"text": "We report new baseline results on sixteen novel datasets, including the standard library documentation for nine popular programming languages across seven natural languages, and a small collection of Unix utility manuals.", "labels": [], "entities": []}], "introductionContent": [{"text": "Technical documentation in the computer domain, such as source code documentation and other howto manuals, provide high-level descriptions of how lower-level computer programs and utilities work.", "labels": [], "entities": []}, {"text": "Often these descriptions are coupled with formal representations of these lower-level features, expressed in the target programming languages.", "labels": [], "entities": []}, {"text": "For example,.1 shows the source code documentation (in red/bold) for the max function in the Java programming language paired with the representation of this function in the underlying Java language (in black).", "labels": [], "entities": []}, {"text": "This formal representation captures the name of the function, the return value, the types of arguments the function takes, among other details related to the function's place and visibility in the overall source code collection or API.", "labels": [], "entities": []}, {"text": "Given the high-level nature of the textual annotations, modeling the meaning of any given description is not an easy task, as it involves much more information than what is directly provided in the associated documentation.", "labels": [], "entities": []}, {"text": "For example, capturing the meaning of the description the greater of might require having a background theory about quantity/numbers and relations between different quantities.", "labels": [], "entities": []}, {"text": "A first step towards capturing the meaning, however, is learning to translate this description to symbols in the target representation, in this case to the max symbol.", "labels": [], "entities": []}, {"text": "By doing this translation to a formal language, modeling and learning the subsequent semantics becomes easier since we are eliminating the ambiguity of ordinary lan-  guage.", "labels": [], "entities": []}, {"text": "Similarly, we would want to first translate the description two long values, which specifies the number and type of argument taken by this function, to the sequence long a,long b.", "labels": [], "entities": []}, {"text": "By focusing on translation, we can create new datasets by mining these types of source code collections for sets of parallel text-representation pairs.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9711588621139526}]}, {"text": "Given the wide variety of available programming languages, many such datasets can be constructed, each offering new challenges related to differences in the formal representations used by different programming languages..2 shows example documentation for the Clojure programming language, which is part of the Lisp family of languages.", "labels": [], "entities": []}, {"text": "In this case, the description Returns random probability of should be translated to the function name random-sample since it describes what the overall function does.", "labels": [], "entities": []}, {"text": "Similarly, the argument descriptions from coll and of prob should translate to coll and prob.", "labels": [], "entities": []}, {"text": "Given the large community of programmers around the world, many source code collections are available in languages other than English.", "labels": [], "entities": []}, {"text": "shows an example entry from the French version of the PHP standard library, which was translated by volunteer developers.", "labels": [], "entities": [{"text": "PHP standard library", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.8217771848042806}]}, {"text": "Having multilingual data raises new challenges, and broadens the scope of investigations into this type of semantic translation.", "labels": [], "entities": [{"text": "semantic translation", "start_pos": 107, "end_pos": 127, "type": "TASK", "confidence": 0.7539794743061066}]}, {"text": "Other types of technical documentation, such as utility manuals, exhibit similar features.", "labels": [], "entities": []}, {"text": "shows an example manual in the domain of Unix utilities.", "labels": [], "entities": []}, {"text": "The textual description in red/bold describes an example use of the dappprof utility paired with formal representations in the form of executable code.", "labels": [], "entities": []}, {"text": "As with the previous examples, such formal representations do not capture the full meaning of the different descriptions, but serve as a convenient operationalization, or translational semantics, of the meaning in Unix.", "labels": [], "entities": []}, {"text": "Print elapsed time, for example, roughly describes what the dappprof utility does, whereas PID 1871 describes the second half of the code sequence.", "labels": [], "entities": [{"text": "PID 1871", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.6818039566278458}]}, {"text": "In both types of technical documentation, information is not limited to raw pairs of descriptions and representations, but can include other information and clues that are useful for learning.", "labels": [], "entities": []}, {"text": "Java function annotations include textual descriptions of individual arguments and return values (shown in green).", "labels": [], "entities": []}, {"text": "Taxonomic information and pointers to related functions or utilities are also annotated (e.g., the @see section in, or SEE ALSO section in).", "labels": [], "entities": [{"text": "SEE", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9573969841003418}]}, {"text": "Structural information about code sequences, and the types of abstract arguments these sequences take, are described in the SYNOPSIS section of the Unix manual.", "labels": [], "entities": []}, {"text": "This last piece of information allows us to generate abstract code templates, and generalize individual arguments.", "labels": [], "entities": []}, {"text": "For example, the raw argument 1871 in the sequence dappprof -p 1871 can be typed as a PID instance, and an argument of the -p flag.", "labels": [], "entities": []}, {"text": "Given this type of data, a natural experiment is to see whether we can build programs that translate high-level textual descriptions to correct formal representations.", "labels": [], "entities": []}, {"text": "We aim to learn these translations using raw text-meaning pairs as the sole supervision.", "labels": [], "entities": []}, {"text": "Our focus is on learning function translations or representations within nine programming language APIs, each varying in size, representation style, and source natural language.", "labels": [], "entities": [{"text": "learning function translations or representations", "start_pos": 16, "end_pos": 65, "type": "TASK", "confidence": 0.7540250658988953}]}, {"text": "To our knowledge, our work is the first to look at translating source code descriptions to formal representations using such a wide variety of programming and natural languages.", "labels": [], "entities": [{"text": "translating source code descriptions", "start_pos": 51, "end_pos": 87, "type": "TASK", "confidence": 0.8282541036605835}]}, {"text": "In total, we introduce fourteen new datasets in the source code domain that include seven natural languages, and report new results for an existing dataset.", "labels": [], "entities": []}, {"text": "As well, we look at learning simple code templates using a small collection of English Unix manuals.", "labels": [], "entities": []}, {"text": "The main goal of this paper is to establish strong baselines results on these resources, which we hope can be used for benchmarking and developing new semantic parsing methods.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 151, "end_pos": 167, "type": "TASK", "confidence": 0.6992017030715942}]}, {"text": "We achieved initial baselines using the language modeling and translation approach of.", "labels": [], "entities": []}, {"text": "We also show that modest improvements can be achieved by using a more conventional discriminative model) that, in part, exploits document-level features from the technical documentation sets.", "labels": [], "entities": []}], "datasetContent": [{"text": "Source code documentation Our source code documentation collection consists of the standard library for nine programming languages, which are listed in.", "labels": [], "entities": []}, {"text": "We also use the translated version of the PHP collection for six additional languages, the details of which are shown in.", "labels": [], "entities": [{"text": "PHP collection", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.8527802228927612}]}, {"text": "The Java dataset was first used in DC, while we extracted all other datasets for this work.", "labels": [], "entities": [{"text": "Java dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.727236196398735}, {"text": "DC", "start_pos": 35, "end_pos": 37, "type": "DATASET", "confidence": 0.7912907004356384}]}, {"text": "The size of the different datasets are detailed in both figures.", "labels": [], "entities": []}, {"text": "The number of pairs is the number of single sentences paired with function representations, which constitutes the core part of these datasets.", "labels": [], "entities": []}, {"text": "The number of descriptions is the number of additional textual descriptions provided in the overall document, such as descriptions of parameters or return values.", "labels": [], "entities": []}, {"text": "Figure 6: The non-English PHP datasets.", "labels": [], "entities": [{"text": "PHP datasets", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.683853417634964}]}, {"text": "We also quantify the different datasets in terms of unique symbols in the target representations, shown as Symbols.", "labels": [], "entities": []}, {"text": "All function representations and code sequences are linearized, and in some cases further tokenized, for example, by converting out of camel case or removing underscores.", "labels": [], "entities": []}, {"text": "Man pages The collection of man pages is from and includes 921 text-code pairs that span 330 Unix utilities and man pages.", "labels": [], "entities": []}, {"text": "Using information from the synopsis and parameter declarations, the target code representations are abstracted by type.", "labels": [], "entities": []}, {"text": "The extra descriptions are extracted from parameter descriptions, as shown in the DESCRIPTION section in, as well as from the NAME sections of each manual.", "labels": [], "entities": [{"text": "DESCRIPTION", "start_pos": 82, "end_pos": 93, "type": "METRIC", "confidence": 0.7292641997337341}]}, {"text": "For evaluation, we split our datasets into separate training, validation and test sets.", "labels": [], "entities": []}, {"text": "For Java, we reserve 60% of the data for training and the remaining 40% for validation (20%) and testing (20%).", "labels": [], "entities": []}, {"text": "For all other datasets, we use a 70%-30% split.", "labels": [], "entities": []}, {"text": "From a retrieval perspective, these left out descriptions are meant to mimic unseen queries to our model.", "labels": [], "entities": []}, {"text": "After training our models, we evaluate on these held out sets by ranking all known components in each resource using Algorithm 1.", "labels": [], "entities": []}, {"text": "A predicted component is counted as correct if it matches exactly a gold component.", "labels": [], "entities": []}, {"text": "Following DC, we report the accuracy of predicting the correct representation at the first position in the ranked list (Accuracy @1) and within the top 10 positions (Accuracy @10).", "labels": [], "entities": [{"text": "DC", "start_pos": 10, "end_pos": 12, "type": "DATASET", "confidence": 0.8156609535217285}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9995262622833252}, {"text": "Accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9965998530387878}, {"text": "Accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9956628680229187}]}, {"text": "We also report the mean reciprocal rank MRR, or the multiplicative inverse of the rank of the correct answer.", "labels": [], "entities": [{"text": "mean reciprocal rank MRR", "start_pos": 19, "end_pos": 43, "type": "METRIC", "confidence": 0.7236481830477715}]}, {"text": "Baselines For comparison, we trained a bag-ofwords classifier (the BoW Model in).", "labels": [], "entities": [{"text": "BoW Model", "start_pos": 67, "end_pos": 76, "type": "DATASET", "confidence": 0.9188838005065918}]}, {"text": "This model uses the occurrence of word-component symbol pairs as binary features, and aims to see if word co-occurrence alone is sufficient to for ranking representations.", "labels": [], "entities": []}, {"text": "Since our discriminative models use more data than the baseline models, which therefore make the results not directly comparable, we train a more comparable translation model, shown as M1 Descr.", "labels": [], "entities": []}, {"text": "in, by adding the additional textual data (i.e. parameter and return or module descriptions) to the models' parallel training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test results according to the table below.", "labels": [], "entities": []}]}