{"title": [{"text": "Robust Incremental Neural Semantic Graph Parsing", "labels": [], "entities": [{"text": "Robust Incremental Neural Semantic Graph Parsing", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7419949471950531}]}], "abstractContent": [{"text": "Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.6377193828423818}]}, {"text": "Yet statistical parsing has focussed almost exclusively on bilexical dependencies or domain-specific logical forms.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8048452734947205}]}, {"text": "We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS).", "labels": [], "entities": [{"text": "Minimal Recursion Semantics (MRS)", "start_pos": 119, "end_pos": 152, "type": "TASK", "confidence": 0.7626746247212092}]}, {"text": "The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments.", "labels": [], "entities": []}, {"text": "Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser.", "labels": [], "entities": [{"text": "MRS", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.8420537114143372}, {"text": "Abstract Meaning Representation (AMR)", "start_pos": 88, "end_pos": 125, "type": "TASK", "confidence": 0.6674384077390035}]}, {"text": "Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.", "labels": [], "entities": [{"text": "Smatch score", "start_pos": 20, "end_pos": 32, "type": "METRIC", "confidence": 0.8824023008346558}, {"text": "AMR parsing", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.6892205327749252}]}], "introductionContent": [{"text": "An important goal of Natural Language Understanding (NLU) is to parse sentences to structured, interpretable meaning representations that can be used for query execution, inference and reasoning.", "labels": [], "entities": [{"text": "Natural Language Understanding (NLU)", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.78767462571462}]}, {"text": "Recently end-to-end models have outperformed traditional pipeline approaches, predicting syntactic or semantic structure as intermediate steps, on NLU tasks such as sentiment analysis and semantic relatedness (, question answering (  and textual entailment).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 165, "end_pos": 183, "type": "TASK", "confidence": 0.9571443796157837}, {"text": "question answering", "start_pos": 212, "end_pos": 230, "type": "TASK", "confidence": 0.8264990150928497}]}, {"text": "However the linguistic structure used in applications has predominantly been shallow, restricted to bilexical dependencies or trees.", "labels": [], "entities": []}, {"text": "In this paper we focus on robust parsing into linguistically deep representations.", "labels": [], "entities": []}, {"text": "The main representation that we use is Minimal Recursion Semantics (MRS) (, which serves as the semantic representation of the English Resource Grammar (ERG).", "labels": [], "entities": [{"text": "Minimal Recursion Semantics (MRS", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.731350588798523}, {"text": "English Resource Grammar (ERG)", "start_pos": 127, "end_pos": 157, "type": "DATASET", "confidence": 0.8093154430389404}]}, {"text": "Existing parsers for full MRS (as opposed to bilexical semantic graphs derived from, but simplifying MRS) are grammar-based, performing disambiguation with a maximum entropy model (; this approach has high precision but incomplete coverage.", "labels": [], "entities": [{"text": "MRS", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8843682408332825}, {"text": "precision", "start_pos": 206, "end_pos": 215, "type": "METRIC", "confidence": 0.9965099692344666}]}, {"text": "Our main contribution is to develop a fast and robust parser for full MRS-based semantic graphs.", "labels": [], "entities": [{"text": "MRS-based semantic graphs", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.886163572470347}]}, {"text": "We exploit the power of global conditioning enabled by deep learning to predict linguistically deep graphs incrementally.", "labels": [], "entities": []}, {"text": "The model does not have access to the underlying ERG or syntactic structures from which the MRS analyses were originally derived.", "labels": [], "entities": [{"text": "MRS", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9517597556114197}]}, {"text": "We develop parsers for two graph-based conversions of MRS, Elementary Dependency Structure (EDS)) and Dependency MRS (DMRS)), of which the latter is inter-convertible with MRS.", "labels": [], "entities": []}, {"text": "Abstract Meaning Representation (AMR) () is a graph-based semantic representation that shares the goals of MRS.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8456986745198568}, {"text": "MRS", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9627429246902466}]}, {"text": "Aside from differences in the choice of which linguistic phenomena are annotated, MRS is a compositional representation explicitly coupled with the syntactic structure of the sentence, while AMR does not assume compositionality or alignment with the sentence structure.", "labels": [], "entities": [{"text": "MRS", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.640037477016449}]}, {"text": "Recently a number of AMR parsers have been developed), but corpora are still under active development and low inter-annotator agreement places on upper bound of 83% F1 on expected parser performance (.", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.789021372795105}, {"text": "F1", "start_pos": 165, "end_pos": 167, "type": "METRIC", "confidence": 0.9988534450531006}]}, {"text": "We apply our model to AMR parsing by introducing structure that is present explicitly in MRS but not in AMR.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9745970070362091}]}, {"text": "Parsers based on RNNs have achieved state-ofthe-art performance for dependency parsing and constituency parsing (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8985261917114258}, {"text": "constituency parsing", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.8831493258476257}]}, {"text": "One of the main reasons for the prevalence of bilexical dependencies and tree-based representations is that they can be parsed with efficient and wellunderstood algorithms.", "labels": [], "entities": []}, {"text": "However, one of the key advantages of deep learning is the ability to make predictions conditioned on unbounded contexts encoded with RNNs; this enables us to predict more complex structures without increasing algorithmic complexity.", "labels": [], "entities": []}, {"text": "In this paper we show how to perform linguistically deep parsing with RNNs.", "labels": [], "entities": [{"text": "linguistically deep parsing", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6628395120302836}]}, {"text": "Our parser is based on a transition system for semantic graphs.", "labels": [], "entities": []}, {"text": "However, instead of generating arcs over an ordered, fixed set of nodes (the words in the sentence), we generate the nodes and their alignments jointly with the transition actions.", "labels": [], "entities": []}, {"text": "We use a graph-based variant of the arc-eager transition-system.", "labels": [], "entities": []}, {"text": "The sentence is encoded with a bidirectional RNN.", "labels": [], "entities": []}, {"text": "The transition sequence, seen as a graph linearization, can be predicted with any encoder-decoder model, but we show that using hard attention, predicting the alignments with a pointer network and conditioning explicitly on stack-based features improves performance.", "labels": [], "entities": []}, {"text": "In order to deal with data sparsity candidate lemmas are predicted as a pre-processing step, so that the RNN decoder predicts unlexicalized node labels.", "labels": [], "entities": []}, {"text": "We evaluate our parser on DMRS, EDS and AMR graphs.", "labels": [], "entities": [{"text": "DMRS", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.8379175662994385}, {"text": "AMR graphs", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.7827828526496887}]}, {"text": "We show that our model architecture improves performance from 79.68% to 84.16% F1 over an attention-based encoderdecoder baseline.", "labels": [], "entities": [{"text": "F1", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9998086094856262}]}, {"text": "Although our parser is less accurate that a high-precision grammar-based parser on a test set of sentences parsable by that grammar, incremental prediction and GPU batch processing enables it to parse 529 tokens per second, against 7 tokens per second for the grammarbased parser.", "labels": [], "entities": []}, {"text": "On AMR parsing our model obtains 60.11% Smatch, an improvement of 8% over an existing neural AMR parser.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.9455365240573883}, {"text": "Smatch", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9992040991783142}, {"text": "AMR parser", "start_pos": 93, "end_pos": 103, "type": "TASK", "confidence": 0.8143355548381805}]}], "datasetContent": [{"text": "Dridan and Oepen (2011) proposed an evaluation metric called Elementary Dependency Matching (EDM) for MRS-based graphs.", "labels": [], "entities": [{"text": "Elementary Dependency Matching (EDM)", "start_pos": 61, "end_pos": 97, "type": "METRIC", "confidence": 0.6071275224288305}, {"text": "MRS-based graphs", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.9121802747249603}]}, {"text": "EDM computes the F1-score of tuples of predicates and arguments.", "labels": [], "entities": [{"text": "EDM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8323469161987305}, {"text": "F1-score", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9985566735267639}]}, {"text": "A predicate tuple consists of the label and character span of a predicate, while an argument tuple consists of the character spans of the head and dependent nodes of the relation, together with the argument label.", "labels": [], "entities": []}, {"text": "In order to tolerate subtle tokenization differences with respect to punctuation, we allow span pairs whose ends differ by one character to be matched.", "labels": [], "entities": []}, {"text": "The Smatch metric, proposed for evaluating AMR graphs, also measures graph overlap, but does not rely on sentence alignments to determine the correspondences between graph nodes.", "labels": [], "entities": [{"text": "AMR graphs", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.8777943551540375}]}, {"text": "Smatch is instead computed by performing inference over graph alignments to estimate the maximum F1-score obtainable from a one-to-one matching between the predicted and gold graph nodes.: DMRS development set results for attention-based encoder-decoder models with alignments encoded in the linearization, for topdown (TD) and arc-eager (AE) linearizations, and lexicalized and unlexicalized predicate prediction.", "labels": [], "entities": [{"text": "Smatch", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9712475538253784}, {"text": "F1-score", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9969788789749146}, {"text": "predicate prediction", "start_pos": 393, "end_pos": 413, "type": "TASK", "confidence": 0.703511968255043}]}], "tableCaptions": [{"text": " Table 1: DMRS development set results for  attention-based encoder-decoder models with  alignments encoded in the linearization, for top- down (TD) and arc-eager (AE) linearizations, and  lexicalized and unlexicalized predicate prediction.", "labels": [], "entities": [{"text": "predicate prediction", "start_pos": 219, "end_pos": 239, "type": "TASK", "confidence": 0.6981322765350342}]}, {"text": " Table 2: DMRS development set results of  encoder-decoder models with pointer-based align- ment prediction, delexicalized predicates and hard  or soft attention.", "labels": [], "entities": []}, {"text": " Table 3: DMRS parsing test set results, compar- ing the standard top-down attention-based and arc- eager stack-based RNN models to the grammar- based ACE parser.", "labels": [], "entities": [{"text": "DMRS parsing", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.662985697388649}]}, {"text": " Table 4: EDS parsing test set results.", "labels": [], "entities": [{"text": "EDS parsing", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.71768718957901}]}, {"text": " Table 5: Development set results for AMR pars- ing. All the models except the first predict align- ments with pointer networks.", "labels": [], "entities": [{"text": "AMR pars- ing", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.8849014341831207}]}]}