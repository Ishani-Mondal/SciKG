{"title": [{"text": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems", "labels": [], "entities": [{"text": "Program Induction by Rationale Generation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.6051366746425628}, {"text": "Learning to Solve and Explain Algebraic Word", "start_pos": 43, "end_pos": 87, "type": "TASK", "confidence": 0.7529920850481305}]}], "abstractContent": [{"text": "Solving algebraic word problems requires executing a series of arithmetic operations-a program-to obtain a final answer.", "labels": [], "entities": []}, {"text": "However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge.", "labels": [], "entities": []}, {"text": "To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps.", "labels": [], "entities": []}, {"text": "Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones.", "labels": [], "entities": []}, {"text": "To evaluate our approach, we have created anew 100,000-sample dataset of questions, answers and rationales.", "labels": [], "entities": []}, {"text": "Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Behaving intelligently often requires mathematical reasoning.", "labels": [], "entities": []}, {"text": "Shopkeepers calculate change, tax, and sale prices; agriculturists calculate the proper amounts of fertilizers, pesticides, and water for their crops; and managers analyze productivity.", "labels": [], "entities": []}, {"text": "Even determining whether you have enough money to pay fora list of items requires applying addition, multiplication, and comparison.", "labels": [], "entities": []}, {"text": "Solving these tasks is challenging as it involves recognizing how goals, entities, and quantities in the real-world map onto a mathematical formalization, computing the solution, and mapping the solution back onto the world.", "labels": [], "entities": []}, {"text": "As a proxy for the richness of the real world, a series of papers have used natural language specifications of algebraic word problems, and solved these by either learning to fill in templates that can be solved with equation solvers () or inferring and modeling operation sequences (programs) that lead to the final answer (.", "labels": [], "entities": []}, {"text": "In this paper, we learn to solve algebraic word problems by inducing and modeling programs that generate not only the answer, but an answer rationale, a natural language explanation interspersed with algebraic expressions justifying the overall solution.", "labels": [], "entities": []}, {"text": "Such rationales are what examiners require from students in order to demonstrate understanding of the problem solution; they play the very same role in our task.", "labels": [], "entities": []}, {"text": "Not only do natural language rationales enhance model interpretability, but they provide a coarse guide to the structure of the arithmetic programs that must be executed.", "labels": [], "entities": []}, {"text": "In fact the learner we propose (which relies on a heuristic search; \u00a74) fails to solve this task without modeling the rationales-the search space is too unconstrained.", "labels": [], "entities": []}, {"text": "This work is thus related to models that can explain or rationalize their decisions.", "labels": [], "entities": []}, {"text": "However, the use of rationales in this work is quite different from the role they play inmost prior work, where interpretation models are trained to generate plausible sounding (but not necessarily accurate) posthoc descriptions of the decision making process they used.", "labels": [], "entities": []}, {"text": "In this work, the rationale is generated as a latent variable that gives rise to the answer-it is thus a more faithful representation of the steps used in computing the answer.", "labels": [], "entities": []}, {"text": "This paper makes three contributions.", "labels": [], "entities": []}, {"text": "First, we have created anew dataset with more than 100,000 algebraic word problems that includes both answers and natural language answer rationales ( \u00a72).", "labels": [], "entities": []}, {"text": "Rationale: To solve this easiest way is just put the value and see that if it equals or not. with option A. p(a) = a 2 and from the dataset.", "labels": [], "entities": []}, {"text": "Second, we propose a sequence to sequence model that generates a sequence of instructions that, when executed, generates the rationale; only after this is the answer chosen ( \u00a73).", "labels": [], "entities": []}, {"text": "Since the target program is not given in the training data (most obviously, its specific form will depend on the operations that are supported by the program interpreter); the third contribution is thus a technique for inferring programs that generate a rationale and, ultimately, the answer.", "labels": [], "entities": []}, {"text": "Even constrained by a text rationale, the search space of possible programs is quite large, and we employ a heuristic search to find plausible next steps to guide the search for programs ( \u00a74).", "labels": [], "entities": []}, {"text": "Empirically, we are able to show that state-of-the-art sequence to sequence models are unable to perform above chance on this task, but that our model doubles the accuracy of the baseline ( \u00a76).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9993854761123657}]}], "datasetContent": [{"text": "We built a dataset with 100,000 problems with the annotations shown in.", "labels": [], "entities": []}, {"text": "Each question is decomposed in four parts, two inputs and two outputs: the description of the problem, which we will denote as the question, and the possible (multiple choice) answer options, denoted as options.", "labels": [], "entities": []}, {"text": "Our goal is to generate the description of the rationale used to reach the correct answer, denoted as rationale and the correct option label.", "labels": [], "entities": []}, {"text": "Problem 1 illustrates an example of an algebra problem, which must be translated into an expression (i.e., (27x + 17y)/(x + y) = 23) and then the desired quantity (x/y) solved for.", "labels": [], "entities": []}, {"text": "Problem 2 is an example that could be solved by multi-step arithmetic operations proposed in (.", "labels": [], "entities": []}, {"text": "Finally, Problem 3 describes a problem that is solved by testing each of the options, which has not been addressed in the past.", "labels": [], "entities": []}, {"text": "We apply our model to the task of generating rationales for solutions to math problems, evaluating it on both the quality of the rationale and the ability of the model to obtain correct answers.", "labels": [], "entities": []}, {"text": "The evaluation of the rationales is performed with average sentence level perplexity and BLEU-4 ().", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9995637536048889}]}, {"text": "When a model cannot generate a token for perplexity computation, we predict unknown token.", "labels": [], "entities": []}, {"text": "This benefits the baselines as they are less expressive.", "labels": [], "entities": []}, {"text": "As the perplexity of our model is dependent on the latent program that is generated, we force decode our model to generate the rationale, while maximizing the probability of the program.", "labels": [], "entities": []}, {"text": "This is analogous to the method used to obtain sample programs described in Section 4, but we choose the most likely instructions at each timestamp instead of sampling.", "labels": [], "entities": []}, {"text": "Finally, the correctness of the answer is evaluated by computing the percentage of the questions, where the chosen option matches the correct one.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Descriptive statistics of our dataset.", "labels": [], "entities": []}, {"text": " Table 3: Results over the test set measured in Per- plexity, BLEU and Accuracy.", "labels": [], "entities": [{"text": "Per- plexity", "start_pos": 48, "end_pos": 60, "type": "METRIC", "confidence": 0.9493894775708517}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9993017911911011}, {"text": "Accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9995261430740356}]}]}