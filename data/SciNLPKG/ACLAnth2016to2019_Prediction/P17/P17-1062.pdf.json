{"title": [{"text": "Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning", "labels": [], "entities": []}], "abstractContent": [{"text": "End-to-end learning of recurrent neural networks (RNNs) is an attractive solution for dialog systems; however, current techniques are data-intensive and require thousands of dialogs to learn simple behaviors.", "labels": [], "entities": [{"text": "End-to-end learning of recurrent neural networks (RNNs)", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.8386686775419447}]}, {"text": "We introduce Hybrid Code Networks (HCNs), which combine an RNN with domain-specific knowledge encoded as software and system action templates.", "labels": [], "entities": []}, {"text": "Compared to existing end-to-end approaches, HCNs considerably reduce the amount of training data required, while retaining the key benefit of inferring a latent representation of dialog state.", "labels": [], "entities": []}, {"text": "In addition, HCNs can be optimized with supervised learning, reinforcement learning, or a mixture of both.", "labels": [], "entities": []}, {"text": "HCNs attain state-of-the-art performance on the bAbI dialog dataset (Bordes and Weston, 2016), and outperform two commercially deployed customer-facing dialog systems.", "labels": [], "entities": [{"text": "bAbI dialog dataset", "start_pos": 48, "end_pos": 67, "type": "DATASET", "confidence": 0.9277826547622681}]}], "introductionContent": [{"text": "Task-oriented dialog systems help a user to accomplish some goal using natural language, such as making a restaurant reservation, getting technical support, or placing a phonecall.", "labels": [], "entities": []}, {"text": "Historically, these dialog systems have been built as a pipeline, with modules for language understanding, state tracking, action selection, and language generation.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.7181637138128281}, {"text": "state tracking", "start_pos": 107, "end_pos": 121, "type": "TASK", "confidence": 0.7771130204200745}, {"text": "action selection", "start_pos": 123, "end_pos": 139, "type": "TASK", "confidence": 0.7674035131931305}, {"text": "language generation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7560827136039734}]}, {"text": "However, dependencies between modules introduce considerable complexity -for example, it is often unclear how to define the dialog state and what history to maintain, yet action selection relies exclusively on the state for input.", "labels": [], "entities": []}, {"text": "Moreover, training each module requires specialized labels.", "labels": [], "entities": []}, {"text": "* Currently at JPMorgan Chase Recently, end-to-end approaches have trained recurrent neural networks (RNNs) directly on text transcripts of dialogs.", "labels": [], "entities": []}, {"text": "A key benefit is that the RNN infers a latent representation of state, obviating the need for state labels.", "labels": [], "entities": []}, {"text": "However, end-to-end methods lack a general mechanism for injecting domain knowledge and constraints.", "labels": [], "entities": []}, {"text": "For example, simple operations like sorting a list of database results or updating a dictionary of entities can expressed in a few lines of software, yet may take thousands of dialogs to learn.", "labels": [], "entities": [{"text": "sorting a list of database results", "start_pos": 36, "end_pos": 70, "type": "TASK", "confidence": 0.8777388234933218}]}, {"text": "Moreover, in some practical settings, programmed constraints are essential -for example, a banking dialog system would require that a user is logged in before they can retrieve account information.", "labels": [], "entities": []}, {"text": "This paper presents a model for end-to-end learning, called Hybrid Code Networks (HCNs) which addresses these problems.", "labels": [], "entities": []}, {"text": "In addition to learning an RNN, HCNs also allow a developer to express domain knowledge via software and action templates.", "labels": [], "entities": []}, {"text": "Experiments show that, compared to existing recurrent end-to-end techniques, HCNs achieve the same performance with considerably less training data, while retaining the key benefit of end-to-end trainability.", "labels": [], "entities": []}, {"text": "Moreover, the neural network can be trained with supervised learning or reinforcement learning, by changing the gradient update applied.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the model, and Section 3 compares the model to related work.", "labels": [], "entities": []}, {"text": "Section 4 applies HCNs to the bAbI dialog dataset.", "labels": [], "entities": [{"text": "bAbI dialog dataset", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.9227199157079061}]}, {"text": "Section 5 then applies the method to real customer support domains at our company.", "labels": [], "entities": []}, {"text": "Section 6 illustrates how HCNs can be optimized with reinforcement learning, and Section 7 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we compare HCNs to existing approaches on the public \"bAbI dialog\" dataset.", "labels": [], "entities": [{"text": "bAbI dialog\" dataset", "start_pos": 70, "end_pos": 90, "type": "DATASET", "confidence": 0.8223904967308044}]}, {"text": "This dataset includes two end-to-end dialog learning tasks, in the restaurant domain, called task5 and task6.", "labels": [], "entities": []}, {"text": "2 Task5 consists of synthetic, simulated dialog data, with highly regular user behavior and constrained vocabulary.", "labels": [], "entities": []}, {"text": "Dialogs include a database access action which retrieves relevant restaurants from a database, with results included in the dialog transcript.", "labels": [], "entities": []}, {"text": "We test on the \"OOV\" variant of Task5, which includes entity values not observed in the training set.", "labels": [], "entities": [{"text": "OOV", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9177514314651489}]}, {"text": "Task6 draws on human-computer dialog data from the second dialog state tracking challenge (DSTC2), where usability subjects (crowd-workers) interacted with several variants of a spoken dialog system ().", "labels": [], "entities": [{"text": "dialog state tracking challenge (DSTC2)", "start_pos": 58, "end_pos": 97, "type": "TASK", "confidence": 0.7267627503190722}]}, {"text": "Since the database from DSTC2 was not provided, database calls have been inferred from the data and inserted into the dialog transcript.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.920857846736908}]}, {"text": "Example dialogs are provided in the Appendix Sections A.2 and A.3.", "labels": [], "entities": [{"text": "Appendix Sections A.2", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.7317947347958883}, {"text": "A.3", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.5658361315727234}]}, {"text": "To apply HCNs, we wrote simple domain-specific software, as follows.", "labels": [], "entities": []}, {"text": "First, for entity extraction (step 4 in), we used a simple string match, with a pre-defined list of entity names -i.e., the list of restaurants available in the database.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7871001958847046}]}, {"text": "Second, in the context update (step 5), we wrote simple logic for tracking entities: when an entity is recognized in the user input, it is retained by the software, over-writing any previously stored value.", "labels": [], "entities": []}, {"text": "For example, if the price \"cheap\" is recognized in the first turn, it is retained as price=cheap.", "labels": [], "entities": []}, {"text": "If \"expensive\" is then recognized in the third turn, it over-writes \"cheap\" so the code now holds price=expensive.", "labels": [], "entities": []}, {"text": "Third, system actions were templatized: for example, system actions of the form \"prezzo is a nice restaurant in the west of town in the moderate price range\" all map to the template \"<name> is a nice restaurant in the <location> of town in the <price> price range\".", "labels": [], "entities": []}, {"text": "This results in 16 templates for Task5 and 58 for Task6.", "labels": [], "entities": []}, {"text": "Fourth, when database results are received into the entity state, they are sorted by rating.", "labels": [], "entities": []}, {"text": "Finally, an action mask was created which encoded common-sense dependencies.", "labels": [], "entities": []}, {"text": "These are implemented as simple if-then rules based on the presence of entity values: for example, only allow an API call if pre-conditions are met; only offer a restaurant if database results have already been received; do not ask for an entity if it is already known; etc.", "labels": [], "entities": []}, {"text": "For Task6, we noticed that the system can say that no restaurants match the current query without consulting the database (for an example dialog, see Section A.3 in the Appendix).", "labels": [], "entities": []}, {"text": "Ina practical system this information would be retrieved from the database and not encoded in the RNN.", "labels": [], "entities": [{"text": "RNN", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.8938382863998413}]}, {"text": "So, we mined the training data and built a table of search queries known to yield no results.", "labels": [], "entities": []}, {"text": "We also added context features that indicated the state of the database -for example, whether there were any restaurants matching the current query.", "labels": [], "entities": []}, {"text": "The complete set of context features is given in Appendix Section A.4.", "labels": [], "entities": [{"text": "Appendix Section A.4", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.7025161882241567}]}, {"text": "Altogether this code consisted of about 250 lines of Python.", "labels": [], "entities": []}, {"text": "We then trained an HCN on the training set, employing the domain-specific software described above.", "labels": [], "entities": []}, {"text": "We selected an LSTM for the recurrent layer, with the AdaDelta optimizer.", "labels": [], "entities": []}, {"text": "We used the development set to tune the number of hid-den units, and the number of epochs (12).", "labels": [], "entities": []}, {"text": "Utterance embeddings were formed by averaging word embeddings, using a publicly available 300-dimensional word embedding model trained using word2vec on web data ().", "labels": [], "entities": []}, {"text": "The word embeddings were static and not updated during LSTM training.", "labels": [], "entities": [{"text": "LSTM training", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.8296740055084229}]}, {"text": "In training, each dialog formed one minibatch, and updates were done on full rollouts (i.e., non-truncated back propagation through time).", "labels": [], "entities": []}, {"text": "The training loss was categorical cross-entropy.", "labels": [], "entities": []}, {"text": "Further low-level implementation details are in the Appendix Section A.1.", "labels": [], "entities": [{"text": "Appendix Section A.1", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.7745004494984945}]}, {"text": "We ran experiments with four variants of our model: with and without the utterance embeddings, and with and without the action mask, steps 3 and 6 respectively).", "labels": [], "entities": []}, {"text": "Following past work, we report average turn accuracy -i.e., for each turn in each dialog, present the (true) history of user and system actions to the network and obtain the network's prediction as a string of characters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.7671682834625244}]}, {"text": "The turn is correct if the string matches the reference exactly, and incorrect if not.", "labels": [], "entities": []}, {"text": "We also report dialog accuracy, which indicates if all turns in a dialog are correct.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.992817759513855}]}, {"text": "We compare to four past end-to-end approaches (.", "labels": [], "entities": []}, {"text": "We emphasize that past approaches have applied purely sequence-to-sequence models, or (as a baseline) purely programmed rules.", "labels": [], "entities": []}, {"text": "By contrast, Hybrid Code Networks area hybrid of hand-coded rules and learned models.", "labels": [], "entities": []}, {"text": "Since Task5 is synthetic data generated using rules, it is possible to obtain perfect accuracy using rules (line 1).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9984012246131897}]}, {"text": "The addition of domain knowledge greatly simplifies the learning task and enables HCNs to also attain perfect accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9889323711395264}]}, {"text": "On Task6, rules alone fare poorly, whereas HCNs outperform past learned models.", "labels": [], "entities": []}, {"text": "We next examined learning curves, training with increasing numbers of dialogs.", "labels": [], "entities": []}, {"text": "To guard against bias in the ordering of the training set, we averaged over 5 runs, randomly permuting the order of the training dialogs in each run.", "labels": [], "entities": []}, {"text": "In Task5, the action mask and utterance embeddings substantially reduce the number of training dialogs required (note the horizontal axis scale is logarithmic).", "labels": [], "entities": []}, {"text": "For Task6, the bene-.", "labels": [], "entities": []}, {"text": "Results for \"Rules\" taken from Bordes and Weston (2016).", "labels": [], "entities": [{"text": "Bordes and Weston (2016)", "start_pos": 31, "end_pos": 55, "type": "DATASET", "confidence": 0.8779750168323517}]}, {"text": "Note that, unlike cited past work, HCNs make use of domainspecific procedural knowledge.", "labels": [], "entities": []}, {"text": "fits of the utterance embeddings are less clear.", "labels": [], "entities": []}, {"text": "An error analysis showed that there are several systematic differences between the training and testing sets.", "labels": [], "entities": []}, {"text": "Indeed, DSTC2 intentionally used different dialog policies for the training and test sets, whereas our goal is to mimic the policy in the training set.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.9307487607002258}]}, {"text": "Nonetheless, these tasks are the best public benchmark we are aware of, and HCNs exceed performance of existing sequence-to-sequence models.", "labels": [], "entities": []}, {"text": "In addition, they match performance of past models using an order of magnitude less data (200 vs. 1618 dialogs), which is crucial in practical settings where collecting realistic dialogs fora new domain can be expensive.", "labels": [], "entities": []}, {"text": "We now turn to comparing with purely handcrafted approaches.", "labels": [], "entities": []}, {"text": "To do this, we obtained logs from our company's text-based customer support dialog system, which uses a sophisticated rulebased dialog manager.", "labels": [], "entities": []}, {"text": "Data from this system is attractive for evaluation because it is used by real customers -not usability subjects -and because its rule-based dialog manager was developed by customer support professionals at our company, and not the authors.", "labels": [], "entities": []}, {"text": "This data is not publicly available, but we are unaware of suitable humancomputer dialog data in the public domain which uses rules.", "labels": [], "entities": []}, {"text": "Customers start using the dialog system by entering a brief description of their problem, such as \"I need to update my operating system\".", "labels": [], "entities": []}, {"text": "They are then routed to one of several hundred domains, where each domain attempts to resolve a particular problem.", "labels": [], "entities": []}, {"text": "In this study, we collected humancomputer transcripts for the high-traffic domains \"reset password\" and \"cannot access account\".", "labels": [], "entities": []}, {"text": "We labeled the dialog data as follows.", "labels": [], "entities": []}, {"text": "First, we enumerated unique system actions observed in the data.", "labels": [], "entities": []}, {"text": "Then, for each dialog, starting from the beginning, we examined each system action, and determined whether it was \"correct\".", "labels": [], "entities": []}, {"text": "Here, correct means that it was the most appropriate action among the set of existing system actions, given the history of that dialog.", "labels": [], "entities": []}, {"text": "If multiple actions were arguably appropriate, we broke ties in favor of the existing rule-based dialog manager.", "labels": [], "entities": []}, {"text": "Example dialogs are provided in the Appendix Sections A.5 and A.6.", "labels": [], "entities": [{"text": "Appendix Sections A.5", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.7171563704808553}, {"text": "A.6", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.6143246293067932}]}, {"text": "If a system action was labeled as correct, we left it as-is and continued to the next system action.", "labels": [], "entities": []}, {"text": "If the system action was not correct, we replaced it with the correct system action, and discarded the rest of the dialog, since we do not know how the user would have replied to this new system action.", "labels": [], "entities": []}, {"text": "The resulting dataset contained a mixture of complete and partial dialogs, containing only correct system actions.", "labels": [], "entities": []}, {"text": "We partitioned this set into training and test dialogs.", "labels": [], "entities": []}, {"text": "Basic statistics of the data are shown in.", "labels": [], "entities": []}, {"text": "In this domain, no entities were relevant to the control flow, and there was no obvious mask logic since any question could follow any question.", "labels": [], "entities": []}, {"text": "Therefore, we wrote no domain-specific software for this instance of the HCN, and relied purely on the recurrent neural network to drive the conversation.", "labels": [], "entities": []}, {"text": "The architecture and training of the RNN was the same as in Section 4, except that here we did not have enough data fora validation set, so we instead trained until we either achieved 100% accuracy on the training set or reached 200 epochs.", "labels": [], "entities": [{"text": "RNN", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.7440841197967529}, {"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9958386421203613}]}, {"text": "To evaluate, we observe that conventional measures like average dialog accuracy unfairly penalize the system used to collect the dialogs -in our case, the rule-based system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.8713254928588867}]}, {"text": "If the system used for collection makes an error at turn t, the labeled dialog only includes the sub-dialog up to turn t, and the system being evaluated off-line is only evaluated on that sub-dialog.", "labels": [], "entities": [{"text": "collection", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.9644747376441956}]}, {"text": "In other words, in our case, reporting dialog accuracy would favor the HCN because it would be evaluated on fewer turns than the rule-based system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9398026466369629}, {"text": "HCN", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.9285523891448975}]}, {"text": "We therefore  use a comparative measure that examines which method produces longer continuous sequences of correct system actions, starting from the beginning of the dialog.", "labels": [], "entities": []}, {"text": "Specifically, we report \u2206P = , where C(HCN-win) is the number of test dialogs where the rule-based approach output a wrong action before the HCN; C(rule-win) is the number of test dialogs where the HCN output a wrong action before the rulebased approach; and C(all) is the number of dialogs in the test set.", "labels": [], "entities": []}, {"text": "When \u2206P > 0, there are more dialogs in which HCNs produce longer continuous sequences of correct actions starting from the beginning of the dialog.", "labels": [], "entities": []}, {"text": "We run all experiments 5 times, each time shuffling the order of the training set.", "labels": [], "entities": []}, {"text": "HCNs exceed performance of the existing rule-based system after about 30 dialogs.", "labels": [], "entities": [{"text": "HCNs", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8061202168464661}]}, {"text": "In these domains, we have a further source of knowledge: the rule-based dialog managers themselves can be used to generate example \"sunnyday\" dialogs, where the user provides purely expected inputs.", "labels": [], "entities": []}, {"text": "From each rule-based controller, synthetic dialogs were sampled to cover each expected user response at least once, and added to the set of labeled real dialogs.", "labels": [], "entities": []}, {"text": "This resulted in 75 dialogs for the \"Forgot password\" domain, and 325 for the \"Can't access account\" domain.", "labels": [], "entities": []}, {"text": "Training was repeated as described above.", "labels": [], "entities": []}, {"text": "Results are also included in, with the suffix \"sampled\".", "labels": [], "entities": []}, {"text": "In the \"Can't access account\" domain, the sampled dialogs yield a large improvement, probably because the flowchart for this domain is large, so the sampled dialogs increase coverage.", "labels": [], "entities": []}, {"text": "The gain in the \"forgot password\" domain is present but smaller.", "labels": [], "entities": []}, {"text": "In summary, HCNs can out-perform  \u2206P , where \u2206P is the fraction of test dialogs where HCNs produced longer initial correct sequences of system actions than the rules, minus the fraction where rules produced longer initial correct sequences than the HCNs.", "labels": [], "entities": []}, {"text": "\"embed\" indicates whether utterance embeddings were included; \"sampled\" indicates whether dialogs sampled from the rule-based controller were included in the training set.", "labels": [], "entities": []}, {"text": "production-grade rule-based systems with a reasonable number of labeled dialogs, and adding synthetic \"sunny-day\" dialogs improves performance further.", "labels": [], "entities": []}, {"text": "Moreover, unlike existing pipelined approaches to dialog management that rely on an explicit state tracker, this HCN used no explicit state tracker, highlighting an advantage of the model.", "labels": [], "entities": [{"text": "dialog management", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.901465505361557}]}], "tableCaptions": [{"text": " Table 1: Results on bAbI dialog Task5-OOV and Task6 (", "labels": [], "entities": []}, {"text": " Table 2: Basic statistics of labeled customer sup- port dialogs. Test accuracy refers to whole-dialog  accuracy of the existing rule-based system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9043847322463989}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.8774367570877075}]}, {"text": " Table 4: Binary context features used to convey entity and database state in Section 4.", "labels": [], "entities": []}]}