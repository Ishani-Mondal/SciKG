{"title": [{"text": "English Event Detection With Translated Language Features", "labels": [], "entities": [{"text": "English Event Detection", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5514993766943613}]}], "abstractContent": [{"text": "We propose novel radical features from automatic translation for event extraction.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7918241024017334}]}, {"text": "Event detection is a complex language processing task for which it is expensive to collect training data, making generali-sation challenging.", "labels": [], "entities": [{"text": "Event detection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8784823417663574}]}, {"text": "We derive meaningful subword features from automatic translations into target language.", "labels": [], "entities": []}, {"text": "Results suggest this method is particularly useful when using languages with writing systems that facilitate easy decomposition into subword features, e.g., logograms and Cangjie.", "labels": [], "entities": []}, {"text": "The best result combines logogram features from Chinese and Japanese with syllable features from Korean, providing an additional 3.0 points f-score when added to state-of-the-art generalisation features on the TAC KBP 2015 Event Nugget task.", "labels": [], "entities": [{"text": "TAC KBP 2015 Event Nugget task", "start_pos": 210, "end_pos": 240, "type": "DATASET", "confidence": 0.8412323792775472}]}], "introductionContent": [{"text": "Event trigger detection is the task of identifying the mention that predicates the occurrence of an event and assigning it an event type (e.g., attack).", "labels": [], "entities": [{"text": "Event trigger detection", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7585149705410004}]}, {"text": "Typical training data for event trigger detection includes fewer than 200 annotated documents ().", "labels": [], "entities": [{"text": "event trigger detection", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.8660702705383301}]}, {"text": "Yet systems attempt to identify many event types (e.g., for the data used here), making data sparsity a particular challenge).", "labels": [], "entities": []}, {"text": "Existing approaches use two main strategies for handling data sparsity.", "labels": [], "entities": []}, {"text": "One strategy is to use lexical databases.", "labels": [], "entities": []}, {"text": "Lexical databases have become a standard feature set for event detection.", "labels": [], "entities": [{"text": "event detection", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.7796841561794281}]}, {"text": "They make it easy to include synonyms and word-class information through hypernym relations.", "labels": [], "entities": []}, {"text": "However, they require substantial human effort to build and can have low coverage.", "labels": [], "entities": []}, {"text": "Another approach is to induce word-class information through clustering.", "labels": [], "entities": []}, {"text": "Here cluster co-membership can be used to find synonyms and cluster identifiers provide abstracted word-class information.", "labels": [], "entities": []}, {"text": "We propose novel semantic features for English event detection derived from automatic translations into thirteen languages.", "labels": [], "entities": [{"text": "English event detection", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.7163916925589243}]}, {"text": "In particular, we explore the use of Cangjie 1 radicals in Chinese and Japanese.", "labels": [], "entities": []}, {"text": "Where characters represent concepts, they have often been composed of smaller pictographic units, called radicals.", "labels": [], "entities": []}, {"text": "For example: \u660e(bright) is composed of two radicals \u65e5,\u6708(sun, moon) with corresponding Latin letter sequence \"AB\".", "labels": [], "entities": [{"text": "\u660e(", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9107926487922668}, {"text": "AB", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.6995697617530823}]}, {"text": "While this composition is often not productive, we hypothesise that the recurrence of some radicals among related concepts' logograms maybe exploited to identify semantic affinity.", "labels": [], "entities": []}, {"text": "Results suggest that (1) translated language features are especially useful if the target language has a writing system facilitating easy decomposition into useful subword features; (2) logograms (e.g., Chinese, Japanese), radicals (e.g., Chinese, Japanese) and syllables (e.g., Japanese, Korean) prove beneficial and complementary; and (3) Chinese characters are particularly useful, comparable to WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 399, "end_pos": 406, "type": "DATASET", "confidence": 0.9626675248146057}]}, {"text": "Adding the best translated language features to the final system improves F1 by 3.0 points over a state-of-the-art feature set on the TAC KBP 2015 nugget type detection task.", "labels": [], "entities": [{"text": "F1", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9997033476829529}, {"text": "TAC KBP 2015 nugget type detection task", "start_pos": 134, "end_pos": 173, "type": "TASK", "confidence": 0.8297829372542245}]}], "datasetContent": [{"text": "We use the TAC KBP 2015 English event nugget data () for the experiments.", "labels": [], "entities": [{"text": "TAC KBP 2015 English event nugget data", "start_pos": 11, "end_pos": 49, "type": "DATASET", "confidence": 0.9313480172838483}]}, {"text": "Development experiments use the training data (LDC2015E73) and the evaluation data (LDC2015R26) is held out for final results.", "labels": [], "entities": [{"text": "LDC2015E73", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.6724821925163269}]}, {"text": "The development corpus contains a total of 158 documents from two genres: 81 newswire documents and 77 discussion forum documents.", "labels": [], "entities": []}, {"text": "We split this into 80% for training and 20% for development testing.", "labels": [], "entities": []}, {"text": "We use Google Translate to obtain sentence and word translations into target languages and derive translated language features to help with the English task.", "labels": [], "entities": []}, {"text": "Evaluation uses the official scorer from the shared task, where a trigger is counted as correct if both the trigger span and its event subtype are correctly identified.", "labels": [], "entities": []}, {"text": "Comparing languages First, we explore how translated language features perform across the thirteen languages.", "labels": [], "entities": []}, {"text": "shows how much each target language improves BASE on development data.", "labels": [], "entities": [{"text": "BASE", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9351304173469543}]}, {"text": "We include all word, stem, character and Cangjie features as available for each language.", "labels": [], "entities": []}, {"text": "Chinese, Japanese and Korean standout, with improvements as high as 19.17 points f-score due mostly to large increases in recall.", "labels": [], "entities": [{"text": "f-score", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9782694578170776}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9989239573478699}]}, {"text": "These results suggest that languages with writing systems that facilitate easy decomposition into meaningful subword features are particularly useful.", "labels": [], "entities": []}, {"text": "Combining languages Next, we test whether system performance can be further improved using TRANS features from multiple languages.", "labels": [], "entities": []}, {"text": "We add target languages one at a time in order of individual performance, and find that Traditional Chinese, Japanese and Korean to Simplified Chinese together improve F1 by 2.5 points.", "labels": [], "entities": [{"text": "F1", "start_pos": 168, "end_pos": 170, "type": "METRIC", "confidence": 0.999476969242096}]}, {"text": "This combined feature set is used in the remaining analysis and experimental results.", "labels": [], "entities": []}, {"text": "Error analysis We explore characteristic errors for BASE+LEX versus BASE+TRANS for the attack event on evaluation data.", "labels": [], "entities": [{"text": "BASE+LEX", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.802439292271932}, {"text": "BASE", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9664539694786072}, {"text": "TRANS", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.5201776623725891}]}, {"text": "We randomly sample twenty instances where one is correct and the other is incorrect.", "labels": [], "entities": []}, {"text": "Of six LEX FN errors, two are triggers not seen in the training data, e.g., 'wages' (Transfer-Money), and 'resignation' (End-Position).", "labels": [], "entities": [{"text": "LEX FN", "start_pos": 7, "end_pos": 13, "type": "TASK", "confidence": 0.5685109794139862}]}, {"text": "In other cases, there seem to be too few training instances, e.g., 'pardoning' (Pardon) only appears once in the training data.", "labels": [], "entities": [{"text": "'pardoning' (Pardon)", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.5628468493620554}]}, {"text": "The TRANS FN error is due to a bad translation in which 'strike' (Attack) is a translated to the 'work stoppage' sense instead of the 'forceful hit' sense.", "labels": [], "entities": [{"text": "TRANS FN error", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.7506460547447205}]}, {"text": "For both systems, most FP errors correspond to cases with challenging ambiguity.", "labels": [], "entities": [{"text": "FP", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.7714383006095886}]}, {"text": "For instance, both systems label 'appeal' as Justice.Appeal event in two sentences where the word 'appeal' means 'ask for aid', instead of 'taking a court case to a higher court'.", "labels": [], "entities": []}, {"text": "The translation was incorrect in this case.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.902396559715271}]}, {"text": "Similarly, 'report' appears six times in the training data as three different event types (Broadcast, Correspondence, Move-Person).", "labels": [], "entities": []}, {"text": "Long-tail generalisation shows typelevel results for BASE+LEX and BASE+TRANS compared to BASE alone.", "labels": [], "entities": [{"text": "BASE+LEX", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.729434072971344}, {"text": "BASE+TRANS", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.5699145098527273}, {"text": "BASE", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.6009212136268616}]}, {"text": "The generalisation feature sets outperform the baseline for all but three of the 38 event types.", "labels": [], "entities": []}, {"text": "For Pardon, BASE obtains 97 F1 so there is little room for improvement.", "labels": [], "entities": [{"text": "Pardon", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.6414784789085388}, {"text": "BASE", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9183125495910645}, {"text": "F1", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.913638174533844}]}, {"text": "For Execute, LEX features have no effect while TRANS doubles BASE F1.", "labels": [], "entities": [{"text": "LEX", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9904062151908875}, {"text": "BASE F1", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9059174060821533}]}, {"text": "Contact is the only type where generalisation features are harmful.", "labels": [], "entities": []}, {"text": "Ignoring ties, BASE+TRANS performs best on more types (13) than BASE+LEX (11).", "labels": [], "entities": [{"text": "BASE+TRANS", "start_pos": 15, "end_pos": 25, "type": "TASK", "confidence": 0.41652806599934894}, {"text": "BASE+LEX", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.4377419054508209}]}, {"text": "TRANS appears to help more with long-tail entity types that have fewer training instances (e.g., Bankruptcy, Appeal, Born).", "labels": [], "entities": []}, {"text": "Encouragingly, this  analysis also suggests that LEX and TRANS can be complementary, with LEX doing particularly well on some types (e.g., Trial-Hearing, Correspond) and TRANS doing particularly well on others (e.g., Transfer-Money, Release-Parole).", "labels": [], "entities": []}, {"text": "This is 20.6 points higher than the baseline features alone, and improves both the precision of LEX and the recall of TRANS.", "labels": [], "entities": [{"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.999748170375824}, {"text": "LEX", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.8939818143844604}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9996082186698914}, {"text": "TRANS", "start_pos": 118, "end_pos": 123, "type": "DATASET", "confidence": 0.7179412841796875}]}], "tableCaptions": [{"text": " Table 2: Comparing instance count in training (Trn) and test  (Tst) to F1 for BASE (BA), LEX (LX) and TRANS (TR).", "labels": [], "entities": [{"text": "F1", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9994675517082214}, {"text": "BASE (BA)", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9298867583274841}, {"text": "LEX", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.8656389713287354}, {"text": "TRANS", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9320557713508606}]}, {"text": " Table 3: Final results comparing translated language fea- tures (TRANS) to benchmark lexical generalisation features  (LEX). BASE+LEX is our implementation of the core Hong  et al. classifier. TAC KPB 2015 #1 corresponds to reported re- sults for Hong et al. including semi-supervised learning. TAC  KPB 2015 shared task has 38 runs submitted from 14 teams.", "labels": [], "entities": [{"text": "BASE+LEX", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9019292990366617}, {"text": "TAC  KPB 2015 shared task", "start_pos": 296, "end_pos": 321, "type": "DATASET", "confidence": 0.8279102087020874}]}]}