{"title": [{"text": "A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments", "labels": [], "entities": [{"text": "Evaluating Summarizers", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.8460386991500854}]}], "abstractContent": [{"text": "We present anew framework for evaluating extractive summarizers, which is based on a principled representation as optimization problem.", "labels": [], "entities": []}, {"text": "We prove that every ex-tractive summarizer can be decomposed into an objective function and an optimization technique.", "labels": [], "entities": []}, {"text": "We perform a comparative analysis and evaluation of several objective functions embedded in well-known summarizers regarding their correlation with human judgments.", "labels": [], "entities": []}, {"text": "Our comparison of these correlations across two datasets yields surprising insights into the role and performance of objective functions in the different summarizers.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of extractive summarization (ES) can naturally be cast as a discrete optimization problem where the text source is considered as a set of sentences and the summary is created by selecting an optimal subset of the sentences under a length constraint.", "labels": [], "entities": [{"text": "extractive summarization (ES)", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.8327627182006836}]}, {"text": "In this work, we go one step further and mathematically prove that ES is equivalent to the problem of choosing (i) an objective function \u03b8 for scoring system summaries, and (ii) an optimizer O.", "labels": [], "entities": [{"text": "ES", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.6209109425544739}]}, {"text": "We use (\u03b8, O) to denote the resulting decomposition of any extractive summarizer.", "labels": [], "entities": []}, {"text": "Our proposed decomposition enables a principled analysis and evaluation of existing summarizers, and addresses a major issue in the current evaluation of ES.", "labels": [], "entities": [{"text": "ES", "start_pos": 154, "end_pos": 156, "type": "TASK", "confidence": 0.5279549360275269}]}, {"text": "This issue concerns the traditional \"intrinsic\" evaluation comparing system summaries against human reference summaries.", "labels": [], "entities": []}, {"text": "This kind of evaluation is actually an end-to-end evaluation of summarization systems which is performed after \u03b8 has been optimized by O. This is highly problematic from an evaluation point of view, because first, \u03b8 is typically not optimized exactly, and second, there might be side-effects caused by the particular optimization technique O, e.g., a sentence extracted to maximize \u03b8 might be suitable because of other properties not included in \u03b8.", "labels": [], "entities": []}, {"text": "Moreover, the commonly used evaluation metric ROUGE yields a noisy surrogate evaluation (despite its good correlation with human judgments) compared to the much more meaningful evaluation based on human judgments.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9735285043716431}]}, {"text": "As a result, the current end-toend evaluation does not provide any insights into the task of automatic summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 103, "end_pos": 116, "type": "TASK", "confidence": 0.9110267758369446}]}, {"text": "The (\u03b8, O) decomposition we propose addresses this issue: it enables a well-defined and principled evaluation of extractive summarizers on the level of their components \u03b8 and O. In this work, we focus on the analysis and evaluation of \u03b8, because \u03b8 is a model of the quality indicators of a summary, and thus crucial in order to understand the properties of \"good\" summaries.", "labels": [], "entities": []}, {"text": "Specifically, we compare \u03b8 functions of different summarizers by measuring the correlation of their \u03b8 functions with human judgments.", "labels": [], "entities": []}, {"text": "Our goal is to provide an evaluation framework which the research community could build upon in future research to identify the best possible \u03b8 and use it in optimization-based systems.", "labels": [], "entities": []}, {"text": "We believe that the identification of such a \u03b8 is the central question of summarization, because this optimal \u03b8 would represent an optimal definition of summary quality both from an algorithmic point of view and from the human perspective.", "labels": [], "entities": [{"text": "summarization", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.9845139384269714}]}, {"text": "In summary, our contribution is twofold: (i) We present a novel and principled evaluation framework for ES which allows evaluating the objective function and the optimization technique separately and independently.", "labels": [], "entities": [{"text": "ES", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.8024759888648987}]}, {"text": "(ii) We compare wellknown summarization systems regarding their implicit choices of \u03b8 by measuring the correlation of their \u03b8 functions with human judgments on two datasets from the Text Analysis Conference (TAC).", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC)", "start_pos": 182, "end_pos": 212, "type": "TASK", "confidence": 0.7852125068505605}]}, {"text": "Our comparative evaluation yields surprising results and shows that extractive summarization is not solved yet.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.7699044942855835}]}, {"text": "The code used in our experiments, including a general evaluation tool is available at github.com/UKPLab/acl2017-theta_ evaluation_summarization.", "labels": [], "entities": [{"text": "UKPLab", "start_pos": 97, "end_pos": 103, "type": "DATASET", "confidence": 0.9662212133407593}]}], "datasetContent": [{"text": "Now we compare the summarizers analyzed above by measuring the correlation of their \u03b8 functions with human judgments.", "labels": [], "entities": []}, {"text": "Datasets We use two multi-document summarization datasets from the Text Analysis Conference (TAC) shared task: TAC-2008 and TAC-2009.", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC) shared task", "start_pos": 67, "end_pos": 109, "type": "TASK", "confidence": 0.786194883286953}, {"text": "TAC-2008", "start_pos": 111, "end_pos": 119, "type": "DATASET", "confidence": 0.8459843397140503}, {"text": "TAC-2009", "start_pos": 124, "end_pos": 132, "type": "DATASET", "confidence": 0.8781813979148865}]}, {"text": "1 TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively.", "labels": [], "entities": [{"text": "TAC-2008", "start_pos": 2, "end_pos": 10, "type": "DATASET", "confidence": 0.8626782894134521}, {"text": "TAC-2009", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.8694999814033508}]}, {"text": "Each topic consists of 10 news articles to be summarized in a maximum of 100 words.", "labels": [], "entities": []}, {"text": "We use only the so-called initial summaries (A summaries), but not the update part.", "labels": [], "entities": []}, {"text": "For each topic, there are 4 human reference summaries along with a manually created Pyramid set.", "labels": [], "entities": []}, {"text": "In both editions, all system summaries and the 4 reference summaries were manually evaluated by NIST assessors for readability, content selection (with Pyramid) and overall responsiveness.", "labels": [], "entities": [{"text": "content selection", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.681937038898468}]}, {"text": "At the time of the shared tasks, 57 systems were submitted to For our experiments, we use the Pyramid and the responsiveness annotations.", "labels": [], "entities": []}, {"text": "System Comparison For each \u03b8, we compute the scores of all system and all manual summaries for any given topic.", "labels": [], "entities": []}, {"text": "These scores are compared with the human scores.", "labels": [], "entities": []}, {"text": "We include the manual summaries in our computation because this yields a more diverse set of summaries with a wider range of scores.", "labels": [], "entities": [{"text": "summaries", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.9539071321487427}]}, {"text": "Since an ideal summarizer would create summaries as well as humans, an ideal \u03b8 would also be able to correctly score human summaries with high scores.", "labels": [], "entities": [{"text": "summarizer", "start_pos": 15, "end_pos": 25, "type": "TASK", "confidence": 0.9561323523521423}, {"text": "summaries", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.9629088044166565}]}, {"text": "For comparison, we also report the correlation between pyramid and responsiveness.", "labels": [], "entities": [{"text": "pyramid", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9792817831039429}]}, {"text": "Correlations are measured with 3 metrics: Pear-son's r, Spearman's \u03c1 and Normalized Discounted Cumulative Gain (Ndcg).", "labels": [], "entities": [{"text": "Pear-son's r", "start_pos": 42, "end_pos": 54, "type": "METRIC", "confidence": 0.5999594132105509}, {"text": "Spearman's \u03c1", "start_pos": 56, "end_pos": 68, "type": "METRIC", "confidence": 0.7823870579401652}, {"text": "Normalized Discounted Cumulative Gain (Ndcg)", "start_pos": 73, "end_pos": 117, "type": "METRIC", "confidence": 0.8744927218982151}]}, {"text": "Pearson's r is a value correlation metric which depicts linear relationships between the scores produced by \u03b8 and the human judgments.", "labels": [], "entities": []}, {"text": "Spearman's \u03c1 is a rank correlation metric which compares the ordering of systems induced by \u03b8 and the ordering of systems induced by human judgments.", "labels": [], "entities": []}, {"text": "Ndcg is a metric that compares ranked lists and puts more emphasis on the top elements by logarithmic decay weighting.", "labels": [], "entities": []}, {"text": "Intuitively, it captures how well \u03b8 can recognize the best summaries.", "labels": [], "entities": []}, {"text": "The optimization scenario benefits from high Ndcg scores because only summaries with high \u03b8 scores are extracted.", "labels": [], "entities": []}, {"text": "Previous work on correlation analysis averaged scores over topics for each system and then computed the correlation between averaged scores ().", "labels": [], "entities": [{"text": "correlation analysis", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.9663224220275879}]}, {"text": "An alternative and more natural option which we use here is to compute the correlation for each topic and average these correlations over topics (CORRELATION-AVERAGE).", "labels": [], "entities": [{"text": "CORRELATION-AVERAGE", "start_pos": 146, "end_pos": 165, "type": "METRIC", "confidence": 0.9827700853347778}]}, {"text": "Since we want to estimate how well \u03b8 functions measure the quality of summaries, we find the summary level averaging more meaningful.", "labels": [], "entities": []}, {"text": "Analysis The results of our correlation analysis are presented in.", "labels": [], "entities": []}, {"text": "In our (\u03b8, O) formulation, the end-to-end approach maps a set of documents to exactly one summary selected by the system.", "labels": [], "entities": []}, {"text": "We call the (classical and well known) evaluation of this single summary end-to-end evaluation because it measures the end product of the system.", "labels": [], "entities": []}, {"text": "This is in contrast to our proposed evaluation of the assumption made by individual summarizers shown in.", "labels": [], "entities": []}, {"text": "A system summary was extracted by a given system because it was high scoring using its \u03b8, but we ask the question whether optimizing this \u03b8 made sense in the first place.", "labels": [], "entities": []}, {"text": "We first observe that scores are relatively low.", "labels": [], "entities": []}, {"text": "Summarization is not a solved problem and the systems we investigated cannot identify correctly what makes a good summary.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9767594933509827}]}, {"text": "This is in contrast to the picture in the classical end-to-end evaluation with ROUGE where state-of-the-art systems score relatively high.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.8850499391555786}]}, {"text": "Some Ndcg scores are higher (for TAC-2008) which explains why these systems can extract relatively good summaries in the end-toend evaluation.", "labels": [], "entities": [{"text": "TAC-2008", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.6779130101203918}]}, {"text": "In this classical evaluation, only the single best summary is evaluated, which means that a system does not need to be able to rank all: Correlation of \u03b8 functions with human judgments across various systems.", "labels": [], "entities": []}, {"text": "We see that systems with high end-to-end ROUGE scores (according to) do not necessarily have a good model of summary quality.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.960351824760437}]}, {"text": "Indeed, the best performing \u03b8 functions are not part of the systems performing best with ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9059295654296875}]}, {"text": "For example, ICSI is the best system according to ROUGE, but it is not clear that it has the best model of summary quality.", "labels": [], "entities": [{"text": "ICSI", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.5614505410194397}, {"text": "ROUGE", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.7652650475502014}]}, {"text": "In TAC-2009, LexRank, LSA and the heuristic Edmundson have better correlations with human judgments.", "labels": [], "entities": [{"text": "TAC-2009", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.8086825609207153}, {"text": "LexRank", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.907949686050415}]}, {"text": "The difference with end-to-end evaluation might stem from the fact that ICSI solves the optimization problem exactly, while LexRank and Edmundson use greedy optimizers.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 124, "end_pos": 131, "type": "DATASET", "confidence": 0.9511030316352844}]}, {"text": "There might also be some side-effects from which ICSI profits: extracting sentences to improve \u03b8 might lead to accidentally selecting suitable sentences, because \u03b8 can merely correlate well with properties of good summaries, while not modeling these properties itself.", "labels": [], "entities": []}, {"text": "It is worth noting that systems perform differently on TAC2009 and TAC2008.", "labels": [], "entities": [{"text": "TAC2009", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.9398471713066101}, {"text": "TAC2008", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9525704979896545}]}, {"text": "There are several differences between TAC2008 and TAC2009 like redundancy level or guidelines for annotations; for example, responsiveness is scored out of 5 in 2008 and out of 10 in 2009.", "labels": [], "entities": [{"text": "TAC2008", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.5461477041244507}, {"text": "TAC2009", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.8111200928688049}]}, {"text": "The LSA summarizer ranks among the best systems in TAC2009 with pearson's r but is closer to the worst systems in TAC2008.", "labels": [], "entities": [{"text": "LSA summarizer", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.766165018081665}, {"text": "TAC2009", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.9421457052230835}, {"text": "pearson's r", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.7133090098698934}, {"text": "TAC2008", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.9704771041870117}]}, {"text": "While this is difficult to explain we hypothesize that the model of summary quality from LSA is sensitive to the slight variations and therefore not robust.", "labels": [], "entities": []}, {"text": "In general, any system which claims to have a better \u03b8 than previous works should indeed report results on several datasets to ensure robustness and generality.", "labels": [], "entities": []}, {"text": "Interestingly, we observe that the correlation between Pyramid and responsiveness is better than in any system, but still not particularly high.", "labels": [], "entities": [{"text": "Pyramid", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9888417720794678}]}, {"text": "Responsiveness is an overall annotation while Pyramid is a manual measure of content only.", "labels": [], "entities": [{"text": "Pyramid", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.7119808793067932}]}, {"text": "These results confirm the intuition that humans take into account much more aspects when evaluating summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 100, "end_pos": 109, "type": "TASK", "confidence": 0.93996262550354}]}], "tableCaptions": [{"text": " Table 1: Correlation of \u03b8 functions with human judgments across various systems.", "labels": [], "entities": []}]}