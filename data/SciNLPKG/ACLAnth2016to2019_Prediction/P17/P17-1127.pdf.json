{"title": [{"text": "Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we study how to improve the domain adaptability of a deletion-based Long Short-Term Memory (LSTM) neu-ral network model for sentence compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 139, "end_pos": 159, "type": "TASK", "confidence": 0.7738218009471893}]}, {"text": "We hypothesize that syntactic information helps in making such models more robust across domains.", "labels": [], "entities": []}, {"text": "We propose two major changes to the model: using explicit syntactic features and introducing syntactic constraints through Integer Linear Programming (ILP).", "labels": [], "entities": []}, {"text": "Our evaluation shows that the proposed model works better than the original model as well as a traditional non-neural-network-based model in a cross-domain setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence compression is the task of compressing long, verbose sentences into short, concise ones.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9248673021793365}]}, {"text": "It can be used as a component of a text summarization system.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7865324020385742}]}, {"text": "shows two example input sentences and the compressed sentences written by human.", "labels": [], "entities": []}, {"text": "The task has been studied for almost two decades.", "labels": [], "entities": []}, {"text": "Early work on this task mostly relies on syntactic information such as constituency-based parse trees to help decide what to prune from a sentence or how to re-write a sentence.", "labels": [], "entities": []}, {"text": "Recently, there has been much interest in applying neural network models to solve the problem, where little or no linguistic analysis is performed except for tokenization (.", "labels": [], "entities": []}, {"text": "Although neural network-based models have achieved good performance on this task recently, they tend to suffer from two problems: (1) They require a large amount of data for training.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate three settings of our method: BiLSTM: In this setting, we use only the base bi-LSTM model without incorporating any syntactic feature.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.970954954624176}]}, {"text": "BiLSTM+SynFeat: In this setting, we combine word embeddings with POS embeddings and de-pendency embeddings as input to the bi-LSTM model and use the predictions of y from the bi-LSTM model.", "labels": [], "entities": []}, {"text": "BiLSTM+SynFeat+ILP: In this setting, on top of BiLSTM+SynFeat, we solve the ILP problem as described in Section 2.4 to predict the final label sequence y.", "labels": [], "entities": []}, {"text": "In the experiments, our model was trained using the Adam () algorithm with a learning rate initialized at 0.001.", "labels": [], "entities": []}, {"text": "The dimension of the hidden layers of bi-LSTM is 100.", "labels": [], "entities": []}, {"text": "Word embeddings are initialized from GloVe 100-dimensional pre-trained embeddings ().", "labels": [], "entities": []}, {"text": "POS and dependency embeddings are randomly initialized with 40-dimensional vectors.", "labels": [], "entities": []}, {"text": "The embeddings are all updated during training.", "labels": [], "entities": []}, {"text": "Dropping probability for dropout layers between stacked LSTM layers is 0.5.", "labels": [], "entities": [{"text": "Dropping probability", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.9581491947174072}]}, {"text": "The batch size is set as 30.", "labels": [], "entities": []}, {"text": "For the ILP part, \u03bb is set to 0.5, \u03b2 and \u03b3 are turned by the validation data and finally they are set to 0.7 and 0.2, respectively.", "labels": [], "entities": []}, {"text": "We utilize an open source ILP solver 4 in our method.", "labels": [], "entities": [{"text": "ILP solver", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.7205098569393158}]}, {"text": "We compare our methods with a few baselines: LSTM: This is the basic LSTM-based deletion method proposed by).", "labels": [], "entities": []}, {"text": "We report both the performance they achieved using close to two million training sentence pairs and the performance of our re-implementation of their model trained on the 8,000 sentence pairs.", "labels": [], "entities": []}, {"text": "LSTM+: This is advanced version of the model proposed by, where the authors incorporated some dependency parse tree information into the LSTM model and used the prediction on the previous word to help the prediction on the current word.", "labels": [], "entities": []}, {"text": "Traditional ILP: This is the ILP-based method proposed by.", "labels": [], "entities": []}, {"text": "This method does not use neural network models and is an unsupervised method that relies heavily on the syntactic structures of the input sentences . Abstractive seq2seq: This is an abstractive sequence-to-sequence model trained on 3.8 million Gigaword title-article pairs as described in Section 1.", "labels": [], "entities": []}, {"text": "With the two datasets Google News and BNC News that have the ground truth compressed sentences, we can perform automatic evaluation.", "labels": [], "entities": [{"text": "Google News", "start_pos": 22, "end_pos": 33, "type": "DATASET", "confidence": 0.8757649958133698}, {"text": "BNC News", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9373897016048431}]}, {"text": "We first split the Google News dataset into a training set, a validation set and a test set.", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 19, "end_pos": 38, "type": "DATASET", "confidence": 0.9397484660148621}]}, {"text": "We took the first 1,000 sentence pairs from Google News as the test set, following the same practice as.", "labels": [], "entities": []}, {"text": "We then use 8,000 of the remaining sentence pairs for training and the other 1,000 sentence pairs for validation.", "labels": [], "entities": []}, {"text": "For the NBC News dataset, we use it only as a test set, applying the sentence compression models trained from the 8,000 sentence pairs from Google News.", "labels": [], "entities": [{"text": "NBC News dataset", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.9642045100529989}]}, {"text": "We use the ground truth compressed sentences to compute accuracy and F1 scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9993698000907898}, {"text": "F1 scores", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9834475219249725}]}, {"text": "Accuracy is defined as the percentage of tokens for which the predicted label y i is correct.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.995142936706543}]}, {"text": "F1 scores are derived from precision and recall values, where precision is defined as the percentage of retained words that overlap with the ground truth, and recall is defined as the percentage of words in the ground truth compressed sentences that overlap with the generated compressed sentences.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9666211009025574}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9990198612213135}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9845566749572754}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9988717436790466}, {"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9989757537841797}]}, {"text": "We report both in-domain performance and cross-domain performance in.", "labels": [], "entities": []}, {"text": "From the table, we have the following observations: (1) For the abstractive sequence-to-sequence model, it was trained on the Gigaword data, so for both Google News and NBC News, the performance shown is cross-domain performance.", "labels": [], "entities": [{"text": "Gigaword data", "start_pos": 126, "end_pos": 139, "type": "DATASET", "confidence": 0.9662316739559174}]}, {"text": "We can see that indeed this abstractive method performed poorly in cross-domain settings.", "labels": [], "entities": []}, {"text": "In the in-domain setting, with the same amount of training data (8,000), our BiLSTM method with syntactic features (BiLSTM+SynFeat and BiLSTM+SynFeat+ILP) performs similarly to or better than the LSTM+ method proposed by, in terms of both F1 and accuracy.", "labels": [], "entities": [{"text": "F1", "start_pos": 239, "end_pos": 241, "type": "METRIC", "confidence": 0.9997381567955017}, {"text": "accuracy", "start_pos": 246, "end_pos": 254, "type": "METRIC", "confidence": 0.9989892840385437}]}, {"text": "This shows that our method is comparable to the LSTM+ method in the in-domain setting.", "labels": [], "entities": []}, {"text": "In the in-domain setting, even compared with the We use an open source implementation: https:// github.com/cnap/sentence-compression.", "labels": [], "entities": []}, {"text": "performance of LSTM+ trained on 2 million sentence pairs, our method trained on 8,000 sentence pairs does not perform substantially worse.", "labels": [], "entities": []}, {"text": "In the out-of-domain setting, our BiLSTM+SynFeat and BiLSTM+SynFeat+ILP methods clearly outperform the LSTM and LSTM+ methods.", "labels": [], "entities": []}, {"text": "This shows that by incorporating more syntactic features, our methods learn a sentence compression model that is less domain-dependent.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.7105063498020172}]}, {"text": "The Traditional ILP method also works better than the LSTM and LSTM+ methods in the out-of-domain setting.", "labels": [], "entities": []}, {"text": "This is probably because the Traditional ILP method relies heavily on syntax, which is less domain-dependent compared with lexical patterns.", "labels": [], "entities": []}, {"text": "But the Traditional ILP method performs worse in the in-domain setting than both the LSTM and LSTM+ methods and our methods.", "labels": [], "entities": []}, {"text": "Overall, shows that our proposed method combines both the strength of neural network models in the in-domain setting and the strength of the syntax-based methods in the crossdomain setting.", "labels": [], "entities": []}, {"text": "Therefore, our method works reasonably well for both in-domain and out-ofdomain data.", "labels": [], "entities": []}, {"text": "We also notice that on Google News, adding the ILP layer decreased the sentence compression performance.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.7691194713115692}]}, {"text": "After some analysis, we think the reason is that some of the constraints used in the ILP layer have led to less deletion but the ground truth compressed sentences in the Google News data tend to be shorter compared with those in the NBC News data.", "labels": [], "entities": [{"text": "Google News data", "start_pos": 170, "end_pos": 186, "type": "DATASET", "confidence": 0.8684984842936198}, {"text": "NBC News data", "start_pos": 233, "end_pos": 246, "type": "DATASET", "confidence": 0.898123562335968}]}, {"text": "We also conduct additional experiments to seethe effect of the training data size on our meth-: Automatic evaluation of our sentence compression methods.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.7180375754833221}]}, {"text": "CR standards for compression rate and is defined as the average percentage of words that are retained after compression.", "labels": [], "entities": [{"text": "CR", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.589139997959137}]}, {"text": "ods and the LSTM+ method.", "labels": [], "entities": []}, {"text": "shows the F1 scores on the in-domain Google News data and the out-of-domain NBC News data when we train the models using different amounts of sentence pairs.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9996300935745239}, {"text": "Google News data", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.9231248696645101}, {"text": "NBC News data", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.9565048813819885}]}, {"text": "We can see that in the in-domain setting, our method does not have any advantage over the LSTM+ method.", "labels": [], "entities": []}, {"text": "But in the cross-domain setting, our method that uses ILP to impose syntax-based constraints clearly performs better than LSTM+ when the amount of training data is relatively small.", "labels": [], "entities": []}, {"text": "The evaluation above does not look at the readability of the compressed sentences.", "labels": [], "entities": []}, {"text": "In order to evaluate whether sentences generated by our method are readable, we adopt the manual evaluation procedure by to compare our method with LSTM+ and Traditional ILP in terms of readability and informativeness.", "labels": [], "entities": []}, {"text": "We asked two raters to score a randomly selected set of 100 sentences from the Research Papers dataset.", "labels": [], "entities": [{"text": "Research Papers dataset", "start_pos": 79, "end_pos": 102, "type": "DATASET", "confidence": 0.9524921973546346}]}, {"text": "The compressed sentences were randomly ordered and presented to the human raters to avoid any bias.", "labels": [], "entities": []}, {"text": "The raters were asked to score the sentences on a five-point scale in terms of both readability and informativeness.", "labels": [], "entities": []}, {"text": "We show the average scores of the three methods we compare in.", "labels": [], "entities": []}, {"text": "We can see that our BiLSTM+SynFeat+ILP method clearly outperforms the two baseline methods in the manual evaluation.", "labels": [], "entities": []}, {"text": "We also show a small sample of input sentences from the Research Papers dataset and the automatically compressed sentences by different methods in.", "labels": [], "entities": [{"text": "Research Papers dataset", "start_pos": 56, "end_pos": 79, "type": "DATASET", "confidence": 0.9636286894480387}]}, {"text": "As we can see from the table, a general weakness of the LSTM+ method is that the compressed sentences may not be grammatical.", "labels": [], "entities": []}, {"text": "In comparison, our method does better in terms of preserving grammaticality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Automatic evaluation of our sentence compression methods. CR standards for compression rate  and is defined as the average percentage of words that are retained after compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.7460258305072784}, {"text": "CR", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9875118732452393}]}, {"text": " Table 2: Some input sentences from the Research Papers dataset and the automatically compressed  sentences using different methods. T: Traditional ILP method. S: LSTM+. B: BiLSTM+SynFeat+ILP.", "labels": [], "entities": [{"text": "Research Papers dataset", "start_pos": 40, "end_pos": 63, "type": "DATASET", "confidence": 0.8955321709314982}]}]}