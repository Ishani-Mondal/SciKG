{"title": [], "abstractContent": [{"text": "Distributed word representations are widely used for modeling words in NLP tasks.", "labels": [], "entities": []}, {"text": "Most of the existing models generate one representation per word and do not consider different meanings of a word.", "labels": [], "entities": []}, {"text": "We present two approaches to learn multiple topic-sensitive representations per word by using Hierarchical Dirichlet Process.", "labels": [], "entities": []}, {"text": "We observe that by modeling topics and integrating topic distributions for each document we obtain representations that are able to distinguish between different meanings of a given word.", "labels": [], "entities": []}, {"text": "Our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task.", "labels": [], "entities": [{"text": "lexical substitution task", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.7786143819491068}]}], "introductionContent": [{"text": "Word representations in the form of dense vectors, or word embeddings, capture semantic and syntactic information () and are widely used in many NLP tasks ().", "labels": [], "entities": []}, {"text": "Most of the existing models generate one representation per word and do not distinguish between different meanings of a word.", "labels": [], "entities": []}, {"text": "However, many tasks can benefit from using multiple representations per word to capture polysemy.", "labels": [], "entities": []}, {"text": "There have been several attempts to build repositories for word senses, but this is laborious and limited to few languages.", "labels": [], "entities": []}, {"text": "Moreover, defining a universal set of word senses is challenging as polysemous words can exist at many levels of granularity.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a model that uses a nonparametric Bayesian model, Hierarchical Dirichlet Process (HDP), to learn multiple topicsensitive representations per word.", "labels": [], "entities": []}, {"text": "show that HDP is effective in learning topics yielding state-of-the-art performance for sense induction.", "labels": [], "entities": [{"text": "sense induction", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.9283576011657715}]}, {"text": "However, they assume that topics and senses are interchangeable, and train individual models per word making it difficult to scale to large data.", "labels": [], "entities": []}, {"text": "Our approach enables us to use HDP to model senses effectively using large unannotated training data.", "labels": [], "entities": []}, {"text": "We investigate to what extent distributions over word senses can be approximated by distributions over topics without assuming these concepts to be identical.", "labels": [], "entities": []}, {"text": "The contributions of this paper are: (i) We propose three unsupervised, language-independent approaches to approximate senses with topics and learn multiple topic-sensitive embeddings per word.", "labels": [], "entities": []}, {"text": "(ii) We show that in the Lexical Substitution ranking task our models outperform two competitive baselines.", "labels": [], "entities": [{"text": "Lexical Substitution ranking", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.8394708236058553}]}], "datasetContent": [{"text": "In this section we present the setup for our experiments and empirically evaluate our approach on the context-aware word similarity and lexical substitution tasks.", "labels": [], "entities": [{"text": "word similarity and lexical substitution tasks", "start_pos": 116, "end_pos": 162, "type": "TASK", "confidence": 0.71356267730395}]}, {"text": "All word representations are learned on the English Wikipedia corpus containing 4.8M documents (1B tokens).", "labels": [], "entities": [{"text": "English Wikipedia corpus", "start_pos": 44, "end_pos": 68, "type": "DATASET", "confidence": 0.9215070406595866}]}, {"text": "The topics are learned on a 100K-document subset of this corpus using the HDP implementation of.", "labels": [], "entities": []}, {"text": "Once the topics have been learned, we run HDP on the whole corpus to obtain the word-topic labeling (see Section 2.1) and the document-level topic distributions (Section 2.2).", "labels": [], "entities": []}, {"text": "We train each model vari-  ant with window size c \" 10 and different embedding sizes (100, 300, 600) initialized randomly.", "labels": [], "entities": []}, {"text": "We compare our models to several baselines: Skipgram (SGE) and the best-performing multisense embeddings model per word type (MSSG) ().", "labels": [], "entities": []}, {"text": "All model variants are trained on the same training data with the same settings, following suggestions by and . For MSSG we use the best performing similarity measure (avgSimC) as proposed by.", "labels": [], "entities": [{"text": "similarity measure (avgSimC)", "start_pos": 148, "end_pos": 176, "type": "METRIC", "confidence": 0.919354784488678}]}], "tableCaptions": [{"text": " Table 2: Spearman's rank correlation performance  for the Word Similarity task on SCWS.", "labels": [], "entities": [{"text": "Word Similarity task", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.8182541330655416}]}, {"text": " Table 3: GAP scores on LS-SE07 and LS-CIC  sets. For SGE + C we use the context embeddings  to disambiguate the substitutions. Improvements  over the best baseline (MSSG) are marked at  p \u0103 .01 and at p \u0103 .05.", "labels": [], "entities": [{"text": "GAP", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.619806706905365}, {"text": "\u0103", "start_pos": 189, "end_pos": 190, "type": "METRIC", "confidence": 0.9674063920974731}, {"text": "\u0103", "start_pos": 204, "end_pos": 205, "type": "METRIC", "confidence": 0.9778319597244263}]}, {"text": " Table 4: GAP scores on the candidate ranking task on LS-SE07 for different part-of-speech categories.", "labels": [], "entities": [{"text": "GAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9666349291801453}]}]}