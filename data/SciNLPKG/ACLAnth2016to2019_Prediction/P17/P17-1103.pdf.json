{"title": [{"text": "Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses", "labels": [], "entities": [{"text": "Learning to Evaluate Dialogue Responses", "start_pos": 34, "end_pos": 73, "type": "TASK", "confidence": 0.7521432876586914}]}], "abstractContent": [{"text": "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem.", "labels": [], "entities": []}, {"text": "Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality.", "labels": [], "entities": []}, {"text": "Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations.", "labels": [], "entities": []}, {"text": "In response to this challenge, we formulate automatic dialogue evaluation as a learning problem.", "labels": [], "entities": [{"text": "automatic dialogue evaluation", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.60569895307223}]}, {"text": "We present an evaluation model (ADEM) that learns to predict human-like scores to input responses, using anew dataset of human response scores.", "labels": [], "entities": []}, {"text": "We show that the ADEM model's predictions correlate significantly, and at a level much higher than word-overlap met-rics such as BLEU, with human judgements at both the utterance and system-level.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9962847232818604}]}, {"text": "We also show that ADEM can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation.", "labels": [], "entities": [{"text": "automatic dialogue evaluation", "start_pos": 114, "end_pos": 143, "type": "TASK", "confidence": 0.6455660363038381}]}], "introductionContent": [{"text": "Building systems that can naturally and meaningfully converse with humans has been a central goal of artificial intelligence since the formulation of the Turing test.", "labels": [], "entities": []}, {"text": "Research on one type of such systems, sometimes referred to as non-task-oriented dialogue systems, goes back to the mid-60s with Weizenbaum's famous program ELIZA: a rule-based system mimicking a Rogerian psychotherapist by persistently either rephrasing statements or asking questions (Weizenbaum, * Indicates equal contribution.", "labels": [], "entities": [{"text": "Indicates equal contribution", "start_pos": 301, "end_pos": 329, "type": "METRIC", "confidence": 0.9070327083269755}]}], "datasetContent": [{"text": "Model (ADEM) To overcome the problems of evaluation with wordoverlap metrics, we aim to construct a dialogue evaluation model that: (1) captures semantic similarity beyond word overlap statistics, and (2) exploits both the context and the reference response to calculate its score for the model response.", "labels": [], "entities": []}, {"text": "We call this evaluation model ADEM.", "labels": [], "entities": []}, {"text": "ADEM learns distributed representations of the context, model response, and reference response using a hierarchical RNN encoder.", "labels": [], "entities": []}, {"text": "Given the dialogue context c, reference response r, and model respons\u00ea r, ADEM first encodes each of them into vectors (c, \u02c6 r, and r, respectively) using the RNN encoder.", "labels": [], "entities": []}, {"text": "Then, ADEM computes the score using a dot-product between the vector representations of c, r, and\u02c6rand\u02c6 and\u02c6r in a linearly transformed space: : where M, N \u2208 Rn are learned matrices initialized to the identity, and \u03b1, \u03b2 are scalar constants used to initialize the model's predictions in the range.", "labels": [], "entities": []}, {"text": "The model is shown in.", "labels": [], "entities": []}, {"text": "The matrices M and N can be interpreted as linear projections that map the model respons\u00ea r into the space of contexts and reference responses, respectively.", "labels": [], "entities": []}, {"text": "The model gives high scores to responses that have similar vector representations to the context and reference response after this projection.", "labels": [], "entities": []}, {"text": "The model is end-to-end differentiable; all the parameters can be learned by backpropagation.", "labels": [], "entities": []}, {"text": "In our implementation, the parameters \u03b8 = {M, N } of the model are trained to minimize the squared error between the model predictions and the human score, with L2-regularization: (2) where \u03b3 is a scalar constant.", "labels": [], "entities": []}, {"text": "The simplicity of our model leads to both accurate predictions and fast evaluation (see supp.", "labels": [], "entities": []}, {"text": "material), which is important to allow rapid prototyping of dialogue systems.", "labels": [], "entities": []}, {"text": "The hierarchical RNN encoder in our model consists of two layers of RNNs).", "labels": [], "entities": []}, {"text": "The lower-level RNN, the utterance-level encoder, takes as input words from the dialogue, and produces a vector output at the end of each utterance.", "labels": [], "entities": []}, {"text": "The context-level encoder takes the representation of each utterance as input and outputs a vector representation of the context.", "labels": [], "entities": []}, {"text": "This hierarchical structure is useful for incorporating information from early utterances in the context (.", "labels": [], "entities": []}, {"text": "Following previous work, we take the last hidden state of the context-level encoder as the vector representation of the input utterance or context.", "labels": [], "entities": []}, {"text": "The parameters of the RNN encoder are pretrained and are not learned from the human scores.", "labels": [], "entities": []}, {"text": "An important point is that the ADEM procedure above is not a dialogue retrieval model: the fundamental difference is that ADEM has access to the reference response.", "labels": [], "entities": []}, {"text": "Thus, ADEM can compare a model's response to a known good response, which is significantly easier than inferring response quality from solely the context.", "labels": [], "entities": []}, {"text": "Pre-training with VHRED We would like an evaluation model that can make accurate predictions from few labeled examples, since these examples are expensive to obtain.", "labels": [], "entities": [{"text": "VHRED", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.8626585006713867}]}, {"text": "We therefore employ semi-supervised learning, and use a pre-training procedure to learn the parameters of the encoder.", "labels": [], "entities": []}, {"text": "In particular, we train the encoder as part of a neural dialogue model; we attach a third decoder RNN that takes the output of the encoder as input, and train it to predict the next utterance of a dialogue conditioned on the context.", "labels": [], "entities": []}, {"text": "The dialogue model we employ for pre-training is the latent variable hierarchical recurrent encoderdecoder (VHRED) model), shown in.", "labels": [], "entities": []}, {"text": "The VHRED model is an extension of the original hierarchical recurrent encoderdecoder (HRED) model (Serban et al., 2016a) with a turn-level stochastic latent variable.", "labels": [], "entities": []}, {"text": "The dialogue context is encoded into a vector using our hierarchical encoder, and the VHRED then samples a Gaus- sian variable that is used to condition the decoder (see supplemental material for further details).", "labels": [], "entities": [{"text": "VHRED", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.5645747780799866}]}, {"text": "After training VHRED, we use the last hidden state of the context-level encoder, when c, r, and\u02c6rand\u02c6 and\u02c6r are fed as input, as the vector representations for c, r, and\u02c6rand\u02c6and\u02c6r, respectively.", "labels": [], "entities": [{"text": "VHRED", "start_pos": 15, "end_pos": 20, "type": "DATASET", "confidence": 0.817704975605011}]}, {"text": "We use representations from the VHRED model as it produces more diverse and coherent responses compared to HRED.", "labels": [], "entities": [{"text": "VHRED", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.837712824344635}]}, {"text": "In order to reduce the effective vocabulary size, we use byte pair encoding (BPE), which splits each word into sub-words or characters.", "labels": [], "entities": []}, {"text": "We also use layer normalization () for the hierarchical encoder, which we found worked better at the task of dialogue generation than the related recurrent batch normalization (.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.7696480453014374}]}, {"text": "To train the VHRED model, we employed several of the same techniques found in (Serban et al., 2016b) and (Bowman et al., 2016): we drop words in the decoder with a fixed rate of 25%, and we anneal the KL-divergence term linearly from 0 to 1 over the first 60,000 batches.", "labels": [], "entities": []}, {"text": "We use Adam as our optimizer (.", "labels": [], "entities": []}, {"text": "When training ADEM, we also employ a subsampling procedure based on the model response length.", "labels": [], "entities": []}, {"text": "In particular, we divide the training examples into bins based on the number of words in a response and the score of that response.", "labels": [], "entities": []}, {"text": "We then over-sample from bins across the same score to ensure that ADEM does not use response length to predict the score.", "labels": [], "entities": []}, {"text": "This is because humans have a tendency to give a higher rating to shorter responses than to longer responses (), as shorter responses are often more generic and thus are more likely to be suitable to the context.", "labels": [], "entities": []}, {"text": "Indeed, the test set Pearson correlation between response length and human score is 0.27.", "labels": [], "entities": [{"text": "Pearson correlation between response length", "start_pos": 21, "end_pos": 64, "type": "METRIC", "confidence": 0.726413756608963}]}, {"text": "For training VHRED, we use a context embedding size of 2000.", "labels": [], "entities": [{"text": "VHRED", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.7621544003486633}]}, {"text": "However, we found the ADEM model learned more effectively when this embedding size was reduced.", "labels": [], "entities": []}, {"text": "Thus, after training VHRED, we use principal component analysis (PCA) to reduce the dimensionality of the context, model response, and reference response embeddings ton.", "labels": [], "entities": [{"text": "VHRED", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.8117378950119019}]}, {"text": "We found experimentally that n = 50 provided the best performance.", "labels": [], "entities": []}, {"text": "When training our models, we conduct early stopping on a separate validation set.", "labels": [], "entities": []}, {"text": "For the evaluation dataset, we split the train/ validation/ test sets such that there is no context overlap (i.e. the contexts in the test set are unseen during training).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the dialogue response evalua- tion dataset. Each example is in the form (context,  model response, reference response, human score).", "labels": [], "entities": [{"text": "dialogue response evalua- tion dataset", "start_pos": 28, "end_pos": 66, "type": "DATASET", "confidence": 0.7071028451124827}]}, {"text": " Table 2: Correlation between metrics and human judgements, with p-values shown in brackets. 'ADEM  (T2V)' indicates ADEM with tweet2vec embeddings (", "labels": [], "entities": [{"text": "ADEM  (T2V)'", "start_pos": 94, "end_pos": 106, "type": "METRIC", "confidence": 0.9026999473571777}]}, {"text": " Table 3: System-level correlation, with the p-value  in brackets.", "labels": [], "entities": [{"text": "correlation", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.7274833917617798}]}, {"text": " Table 4: Correlation for ADEM when various model responses are removed from the training set. The  left two columns show performance on the entire test set, and the right two columns show performance  on responses only from the dialogue model not seen during training. The last row (25% at random)  corresponds to the ADEM model trained on all model responses, but with the same amount of training data  as the model above (i.e. 25% less data than the full training set).", "labels": [], "entities": [{"text": "ADEM", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.928225040435791}]}, {"text": " Table 5: Examples of scores given by the ADEM model.", "labels": [], "entities": []}]}