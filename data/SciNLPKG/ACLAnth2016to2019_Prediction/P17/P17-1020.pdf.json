{"title": [{"text": "Coarse-to-Fine Question Answering for Long Documents", "labels": [], "entities": [{"text": "Coarse-to-Fine Question Answering", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6714028914769491}]}], "abstractContent": [{"text": "We present a framework for question answering that can efficiently scale to longer documents while maintaining or even improving performance of state-of-the-art models.", "labels": [], "entities": [{"text": "question answering", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8821902275085449}]}, {"text": "While most successful approaches for reading comprehension rely on recurrent neural networks (RNNs), running them overlong documents is prohibitively slow because it is difficult to parallelize over sequences.", "labels": [], "entities": [{"text": "reading comprehension", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.8751461803913116}]}, {"text": "Inspired by how people first skim the document, identify relevant parts, and carefully read these parts to produce an answer, we combine a coarse, fast model for selecting relevant sentences and a more expensive RNN for producing the answer from those sentences.", "labels": [], "entities": []}, {"text": "We treat sentence selection as a latent variable trained jointly from the answer only using reinforcement learning.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7224611788988113}]}, {"text": "Experiments demonstrate the state of the art performance on a challenging subset of the WIKIREADING dataset (Hewlett et al., 2016) and on anew dataset, while speeding up the model by 3.5x-6.7x.", "labels": [], "entities": [{"text": "WIKIREADING dataset", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.9690554440021515}]}], "introductionContent": [{"text": "Reading a document and answering questions about its content are among the hallmarks of natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.6686081190903982}]}, {"text": "Recently, interest in question answering (QA) from unstructured documents has increased along with the availability of large scale datasets for reading comprehension (.", "labels": [], "entities": [{"text": "question answering (QA) from unstructured documents", "start_pos": 22, "end_pos": 73, "type": "TASK", "confidence": 0.8615957424044609}]}, {"text": "Current state-of-the-art approaches for QA over documents are based on recurrent neural networks \u2020 Work done while the authors were at Google.", "labels": [], "entities": [{"text": "QA over documents", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.9268013040224711}]}, {"text": "(RNNs) that encode the document and the question to determine the answer (.", "labels": [], "entities": []}, {"text": "While such models have access to all the relevant information, they are slow because the model needs to be run sequentially over possibly thousands of tokens, and the computation is not parallelizable.", "labels": [], "entities": []}, {"text": "In fact, such models usually truncate the documents and consider only a limited number of tokens (.", "labels": [], "entities": []}, {"text": "Inspired by studies on how people answer questions by first skimming the document, identifying relevant parts, and carefully reading these parts to produce an answer, we propose a coarse-to-fine model for question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 205, "end_pos": 223, "type": "TASK", "confidence": 0.8825942873954773}]}, {"text": "Our model takes a hierarchical approach (see), where first a fast model is used to select a few sentences from the document that are relevant for answering the question ().", "labels": [], "entities": []}, {"text": "Then, a slow RNN is employed to produce the final answer from the selected sentences.", "labels": [], "entities": []}, {"text": "The RNN is run over a fixed number of tokens, regardless of the length of the document.", "labels": [], "entities": []}, {"text": "Empirically, our model encodes the d: s1: The 2011 Joplin tornado was a catastrophic EF5-rated multiple-vortex tornado that struck Joplin, Missouri . .", "labels": [], "entities": []}, {"text": "s4: It was the third tornado to strike Joplin since: Overall, the tornado killed 158 people . .", "labels": [], "entities": []}, {"text": "., injured some 1,150 others, and caused damages . .", "labels": [], "entities": []}, {"text": "x: how many people died in joplin mo tornado y: 158 people text up to 6.7 times faster than the base model, which reads the first few paragraphs, while having access to four times more tokens.", "labels": [], "entities": []}, {"text": "A defining characteristic of our setup is that an answer does not necessarily appear verbatim in the input (the genre of a movie can be determined even if not mentioned explicitly).", "labels": [], "entities": []}, {"text": "Furthermore, the answer often appears multiple times in the document in spurious contexts (the year '2012' can appear many times while only once in relation to the question).", "labels": [], "entities": []}, {"text": "Thus, we treat sentence selection as a latent variable that is trained jointly with the answer generation model from the answer only using reinforcement learning.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7576076090335846}, {"text": "answer generation", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.6867623925209045}]}, {"text": "Treating sentence selection as a latent variable has been explored in classification (, however, to our knowledge, has not been applied for question answering.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.695056676864624}, {"text": "question answering", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.828264981508255}]}, {"text": "We find that jointly training sentence selection and answer generation is especially helpful when locating the sentence containing the answer is hard.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7115375846624374}, {"text": "answer generation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.8347330689430237}]}, {"text": "We evaluate our model on the WIKIREAD-ING dataset, focusing on examples where the document is long and sentence selection is challenging, and on anew dataset called WIKISUGGEST that contains more natural questions gathered from a search engine.", "labels": [], "entities": [{"text": "WIKIREAD-ING dataset", "start_pos": 29, "end_pos": 49, "type": "DATASET", "confidence": 0.9514070749282837}, {"text": "sentence selection", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7233477234840393}, {"text": "WIKISUGGEST", "start_pos": 165, "end_pos": 176, "type": "DATASET", "confidence": 0.9203935861587524}]}, {"text": "To conclude, we present a modular framework and learning procedure for QA overlong text.", "labels": [], "entities": [{"text": "QA overlong text", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.8341269691785177}]}, {"text": "It captures a limited form of document structure such as sentence boundaries and deals with long documents or potentially multiple documents.", "labels": [], "entities": []}, {"text": "Experiments show improved performance compared to the state of the art on the subset of WIKIREADING, comparable performance on other datasets, and a 3.5x-6.7x speedup in document encoding, while allowing access to much longer documents.: Statistics on string matches of the answer y * in the document.", "labels": [], "entities": [{"text": "WIKIREADING", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.8643106818199158}, {"text": "document encoding", "start_pos": 170, "end_pos": 187, "type": "TASK", "confidence": 0.6956951022148132}]}, {"text": "The third column only considers examples with answer match.", "labels": [], "entities": []}, {"text": "Often the answer string is missing or appears many times while it is relevant to query only once.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experimental Setup We used 70% of the data for training, 10% for development, and 20% for testing in all datasets.", "labels": [], "entities": []}, {"text": "We used the first 35 sentences in each document as input to the hierarchical models, where each sentence has a maximum length of 35 tokens.", "labels": [], "entities": []}, {"text": "Similar to, we add the first five words in the document (typically the title) at the end of each sentence sequence for WIKISUGGEST.", "labels": [], "entities": [{"text": "WIKISUGGEST", "start_pos": 119, "end_pos": 130, "type": "DATASET", "confidence": 0.882072389125824}]}, {"text": "We add the sentence index as a one hot vector to the sentence representation.", "labels": [], "entities": []}, {"text": "We coarsely tuned and fixed most hyperparameters for all models.", "labels": [], "entities": []}, {"text": "The word embedding dimension is set to 256 for both sentence selection and answer generation models.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7114278823137283}, {"text": "answer generation", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7407659143209457}]}, {"text": "We used the decay rate of 0.8 for curriculum learning.", "labels": [], "entities": [{"text": "decay rate", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9519738554954529}]}, {"text": "Hidden dimension is fixed at 128, batch size at 128, GRU state cell at 512, and vocabulary size at 100K.", "labels": [], "entities": []}, {"text": "For CNN sentence selection model, we used 100 filters and set filter width as five.", "labels": [], "entities": [{"text": "CNN sentence selection", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8342554569244385}]}, {"text": "The initial learning rate and gradient clipping coefficients for each model are tuned on the development set.", "labels": [], "entities": []}, {"text": "The ranges for learning rates were 0.00025, 0.0005, 0.001, 0.002, 0.004 and 0.5, 1.0 for gradient clipping coefficient.", "labels": [], "entities": []}, {"text": "We halved the learning rate every 25k steps.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 14, "end_pos": 27, "type": "METRIC", "confidence": 0.9415338635444641}]}, {"text": "We use the Adam ( optimizer and TensorFlow framework (.", "labels": [], "entities": []}, {"text": "Evaluation Metrics Our main evaluation metric is answer accuracy, the proportion of questions answered correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.8916541934013367}]}, {"text": "For sentence selection, since we do not know which sentence contains the answer, we report approximate accuracy by matching sentences that contain the answer string (y * ).", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8025858104228973}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9986981153488159}]}, {"text": "For the soft attention model, we treat the sentence with the highest probability as the predicted sentence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on string matches of the answer y  *  in the  document. The third column only considers examples with  answer match. Often the answer string is missing or appears  many times while it is relevant to query only once.", "labels": [], "entities": []}, {"text": " Table 3: Answer prediction accuracy on the test set. K is the  number of sentences in the document summary.", "labels": [], "entities": [{"text": "Answer prediction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8879618346691132}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9339461326599121}, {"text": "K", "start_pos": 54, "end_pos": 55, "type": "METRIC", "confidence": 0.9708737134933472}]}, {"text": " Table 4: Approximate sentence selection accuracy on the de- velopment set for all models. We use ORACLE to find a proxy  gold sentence and report the proportion of times each model  selects the proxy sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9871721863746643}, {"text": "ORACLE", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9962454438209534}]}, {"text": " Table 5: Manual error analysis on 50 errors from the devel- opment set for REINFORCE (K=1).", "labels": [], "entities": [{"text": "error analysis", "start_pos": 17, "end_pos": 31, "type": "METRIC", "confidence": 0.884505569934845}, {"text": "REINFORCE", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9916403293609619}]}, {"text": " Table 6: Example outputs from REINFORCE (K=1) with BOW sentence selection model. First column: sentence index (l).  Second column: attention distribution p \u03b8 (s l |d, x). Last column: text s l .", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.8641837239265442}, {"text": "BOW sentence selection", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.6211857895056406}]}]}