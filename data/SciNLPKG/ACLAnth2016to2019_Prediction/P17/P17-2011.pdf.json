{"title": [{"text": "An Analysis of Action Recognition Datasets for Language and Vision Tasks", "labels": [], "entities": [{"text": "Action Recognition", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.6892296671867371}]}], "abstractContent": [{"text": "A large amount of recent research has focused on tasks that combine language and vision, resulting in a proliferation of datasets and methods.", "labels": [], "entities": []}, {"text": "One such task is action recognition, whose applications include image annotation, scene understanding and image retrieval.", "labels": [], "entities": [{"text": "action recognition", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8643887341022491}, {"text": "scene understanding", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7539477944374084}, {"text": "image retrieval", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.7805557250976562}]}, {"text": "In this survey, we categorize the existing approaches based on how they conceptualize this problem and provide a detailed review of existing datasets, highlighting their diversity as well as advantages and disadvantages.", "labels": [], "entities": []}, {"text": "We focus on recently developed datasets which link visual information with linguistic resources and provide a fine-grained syntactic and semantic analysis of actions in images.", "labels": [], "entities": []}], "introductionContent": [{"text": "Action recognition is the task of identifying the action being depicted in a video or still image.", "labels": [], "entities": [{"text": "Action recognition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8524712324142456}]}, {"text": "The task is useful fora range of applications such as generating descriptions, image/video retrieval, surveillance, and human-computer interaction.", "labels": [], "entities": [{"text": "image/video retrieval", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.6548840925097466}]}, {"text": "It has been widely studied in computer vision, often on videos, where motion and temporal information provide cues for recognizing actions).", "labels": [], "entities": []}, {"text": "However, many actions are recognizable from still images, seethe examples in.", "labels": [], "entities": []}, {"text": "Due to the absence of motion cues and temporal features) action recognition from stills is more challenging.", "labels": [], "entities": [{"text": "action recognition", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.8326751291751862}]}, {"text": "Most of the existing work can be categorized into four tasks: (a) action classification (AC); (b) determining human-object interaction (HOI); (c) visual verb sense disambiguation (VSD); and (d) visual semantic role labeling (VSRL).", "labels": [], "entities": [{"text": "action classification (AC)", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.7971356749534607}, {"text": "visual verb sense disambiguation (VSD)", "start_pos": 146, "end_pos": 184, "type": "TASK", "confidence": 0.7093297157968793}, {"text": "visual semantic role labeling (VSRL)", "start_pos": 194, "end_pos": 230, "type": "TASK", "confidence": 0.6957946377141135}]}, {"text": "In we illustrate each of these riding horse running playing guitar jumping: Examples of actions in still images tasks and show how they are related to each other.", "labels": [], "entities": []}, {"text": "Until recently, action recognition was studied as action classification on small-scale datasets with a limited number of predefined actions labels.", "labels": [], "entities": [{"text": "action recognition", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.8907318711280823}, {"text": "action classification", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.7412571907043457}]}, {"text": "Often the labels inaction classification tasks are verb phrases or a combination of verb and object such as playing baseball, riding horse.", "labels": [], "entities": []}, {"text": "These datasets have helped in building models and understanding which aspects of an image are important for classifying actions, but most methods are not scalable to larger numbers of actions.", "labels": [], "entities": []}, {"text": "Action classification models are trained on images annotated with mutually exclusive labels, i.e., the assumption is that only a single label is relevant fora given image.", "labels": [], "entities": [{"text": "Action classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8236262798309326}]}, {"text": "This ignores the fact that actions such as holding bicycle and riding bicycle can co-occur in the same image.", "labels": [], "entities": []}, {"text": "To address these issues and also to understand the range of possible interactions between humans and objects, the human-object interaction (HOI) detection task has been proposed, in which all possible interactions between a human and a given object have to be identified (.", "labels": [], "entities": [{"text": "human-object interaction (HOI) detection task", "start_pos": 114, "end_pos": 159, "type": "TASK", "confidence": 0.7185379607336861}]}, {"text": "However, both action classification and HOI detection do not consider the ambiguity that arises when verbs are used as labels, e.g., the verb play has multiple meanings in different contexts.", "labels": [], "entities": [{"text": "action classification", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.771495133638382}, {"text": "HOI detection", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.8912891447544098}]}, {"text": "On the other hand, action labels consisting of verbobject pairs can miss important generalizations: riding horse and riding elephant both instantiate the same verb semantics, i.e., riding animal.", "labels": [], "entities": []}, {"text": "Thirdly, existing action labels miss generalizations across verbs, e.g., the fact that fixing bike and repairing bike are semantically equivalent, in spite of the use of different verbs.", "labels": [], "entities": []}, {"text": "These observations have led authors to argue that actions should be analyzed at the level of verb senses.", "labels": [], "entities": []}, {"text": "propose the new task of visual verb sense disambiguation (VSD), in which a verbimage pair is annotated with a verb sense taken from an existing lexical database (OntoNotes in this case).", "labels": [], "entities": [{"text": "visual verb sense disambiguation (VSD)", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.7875938415527344}]}, {"text": "While VSD handles distinction between different verb senses, it does not identify or localize the objects that participate in the action denoted by the verb.", "labels": [], "entities": [{"text": "VSD", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.770757257938385}]}, {"text": "Recent work) has filled this gap by proposing the task of visual semantic role labeling (VSRL), in which images are labeled with verb frames, and the objects that fill the semantic roles of the frame are identified in the image.", "labels": [], "entities": [{"text": "visual semantic role labeling (VSRL)", "start_pos": 58, "end_pos": 94, "type": "TASK", "confidence": 0.7713784234864371}]}, {"text": "In this paper, we provide a unified view of action recognition tasks, pointing out their strengths and weaknesses.", "labels": [], "entities": [{"text": "action recognition tasks", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.8449687361717224}]}, {"text": "We survey existing literature and provide insights into existing datasets and models for action recognition tasks.", "labels": [], "entities": [{"text": "action recognition tasks", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.8581130305926005}]}], "datasetContent": [{"text": "We give an overview of commonly used datasets for action recognition tasks in and group them according to subtask.", "labels": [], "entities": [{"text": "action recognition tasks", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.845523734887441}]}, {"text": "We observe that the number of verbs covered in these datasets is often smaller than the number of action labels reported (see, columns #V and #L) and in many cases the action label involves object reference.", "labels": [], "entities": []}, {"text": "A few of the first action recognition datasets such as the Ikizler and Willow datasets) had action labels such as throwing and running; they were taken from the sports domain and exhibited diversity in camera viewpoint, background and resolution.", "labels": [], "entities": [{"text": "action recognition", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7423851191997528}, {"text": "Ikizler and Willow datasets", "start_pos": 59, "end_pos": 86, "type": "DATASET", "confidence": 0.8148558288812637}, {"text": "resolution", "start_pos": 235, "end_pos": 245, "type": "METRIC", "confidence": 0.9701815843582153}]}, {"text": "Then datasets were created to capture variation inhuman poses in the sports domain for actions such as tennis serve and cricket bowling; typically features based on poses and body parts were used to build models ().", "labels": [], "entities": [{"text": "tennis serve", "start_pos": 103, "end_pos": 115, "type": "TASK", "confidence": 0.7408597469329834}]}, {"text": "Further datasets were created based on the intuition that object information helps in modeling action recognition, which resulted in the use of action labels such as riding horse or riding bike).", "labels": [], "entities": [{"text": "modeling action recognition", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.6313208838303884}]}, {"text": "Not only were most of these datasets domain specific, but the labels were also manually selected and mutually exclusive, i.e., two actions cannot co-occur in the same image.", "labels": [], "entities": []}, {"text": "Also, most of these datasets do not localize objects or identify their semantic roles.", "labels": [], "entities": []}, {"text": "With the exception of a few datasets such as COCO-a, VerSe, imSitu all action recognition datasets have manually picked labels or focus on covering actions in specific domains such as sports.", "labels": [], "entities": [{"text": "COCO-a", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.9277375936508179}, {"text": "VerSe", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.8885880708694458}, {"text": "action recognition", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.6998347789049149}]}, {"text": "Alternatively, many datasets only cover actions relevant to specific object categories such as musical instruments, animals and vehicles.", "labels": [], "entities": []}, {"text": "In the real world, people interact with many more objects and perform actions relevant to a wide range of domains such as personal care, household activities, or socializing.", "labels": [], "entities": []}, {"text": "This limits the diversity and coverage of existing action recognition datasets.", "labels": [], "entities": [{"text": "action recognition", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7120870053768158}]}, {"text": "Recently proposed datasets partly handle this issue by using generic linguistic resources to extend the vocabulary of verbs inaction labels.", "labels": [], "entities": []}, {"text": "The diversity issue has also been highlighted and addressed in recent video action recognition datasets, which include generic household activities.", "labels": [], "entities": [{"text": "video action recognition", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.6452174087365469}]}, {"text": "An analysis of various image description and question answering datasets by shows the bias in the distribution of word categories.", "labels": [], "entities": [{"text": "image description and question answering", "start_pos": 23, "end_pos": 63, "type": "TASK", "confidence": 0.6587504148483276}]}, {"text": "Image description datasets have a higher distribution of nouns compared to other word categories, indicating that the descriptions are object specific, limiting their usefulness for action-based tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of various existing action recognition datasets. #L denotes number of action labels  in the dataset; #V denotes number of verbs covered in the dataset; Obj indicates number of objects  annotated; Sen indicates whether sense ambiguity is explicitly handled; Des indicates whether image  descriptions are included; Cln indicates whether dataset is manually verified; ML indicates the possibility  of multiple labels per image; Resource indicates linguistic resource used to label actions.", "labels": [], "entities": [{"text": "Obj", "start_pos": 173, "end_pos": 176, "type": "METRIC", "confidence": 0.9888217449188232}]}]}