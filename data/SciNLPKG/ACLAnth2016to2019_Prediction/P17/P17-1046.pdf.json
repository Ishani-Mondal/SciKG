{"title": [{"text": "Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots", "labels": [], "entities": [{"text": "Sequential Matching Network", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8757242759068807}, {"text": "Multi-turn Response Selection", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.691290815671285}]}], "abstractContent": [{"text": "We study response selection for multi-turn conversation in retrieval-based chat-bots.", "labels": [], "entities": [{"text": "response selection", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7648512125015259}]}, {"text": "Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally , which may lose relationships among utterances or important contextual information.", "labels": [], "entities": []}, {"text": "We propose a sequential matching network (SMN) to address both problems.", "labels": [], "entities": [{"text": "sequential matching network (SMN)", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.8258331418037415}]}, {"text": "SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations.", "labels": [], "entities": [{"text": "SMN", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9652873873710632}]}, {"text": "The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among utterances.", "labels": [], "entities": []}, {"text": "The final matching score is calculated with the hidden states of the RNN.", "labels": [], "entities": [{"text": "RNN", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.9051382541656494}]}, {"text": "An empirical study on two public data sets shows that SMN can significantly outperform state-of-the-art methods for response selection in multi-turn conversation.", "labels": [], "entities": [{"text": "SMN", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9869747161865234}, {"text": "response selection", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.7710517942905426}]}], "introductionContent": [{"text": "Conversational agents include task-oriented dialog systems and non-task-oriented chatbots.", "labels": [], "entities": []}, {"text": "Dialog systems focus on helping people complete specific tasks in vertical domains (, while chatbots aim to naturally and meaningfully converse with humans on open domain topics ().", "labels": [], "entities": []}, {"text": "Existing work on building chatbots includes generation -based methods and retrieval-based methods.", "labels": [], "entities": []}, {"text": "Retrieval based chatbots enjoy the advantage of informative and fluent responses, because they select a proper response for * Corresponding Author Context utterance 1 Human: How are you doing?", "labels": [], "entities": []}, {"text": "utterance 2 ChatBot: I am going to hold a drum class in Shanghai.", "labels": [], "entities": []}, {"text": "The location is near Lujiazui.", "labels": [], "entities": [{"text": "Lujiazui", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9715457558631897}]}, {"text": "utterance 3 Human: Interesting!", "labels": [], "entities": []}, {"text": "Do you have coaches who can help me practice drum?", "labels": [], "entities": []}, {"text": "utterance 4 ChatBot: Of course.", "labels": [], "entities": [{"text": "Of", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.8973230123519897}]}, {"text": "utterance 5 Human: Can I have a free first lesson?", "labels": [], "entities": []}, {"text": "Response Candidates response 1 Sure.", "labels": [], "entities": [{"text": "Sure", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9741587042808533}]}, {"text": "Have you ever played drum before?", "labels": [], "entities": []}, {"text": "response 2 What lessons do you want?: An example of multi-turn conversation the current conversation from a repository with response selection algorithms.", "labels": [], "entities": []}, {"text": "While most existing work on retrieval-based chatbots studies response selection for single-turn conversation () which only considers the last input message, we consider the problem in a multi-turn scenario.", "labels": [], "entities": []}, {"text": "Ina chatbot, multi-turn response selection takes a message and utterances in its previous turns as input and selects a response that is natural and relevant to the whole context.", "labels": [], "entities": [{"text": "multi-turn response selection", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.5894577503204346}]}, {"text": "The key to response selection lies in inputresponse matching.", "labels": [], "entities": [{"text": "response selection", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8280049860477448}]}, {"text": "Different from single-turn conversation, multi-turn conversation requires matching between a response and a conversation context in which one needs to consider not only the matching between the response and the input message but also matching between responses and utterances in previous turns.", "labels": [], "entities": []}, {"text": "The challenges of the task include (1) how to identify important information (words, phrases, and sentences) in context, which is crucial to selecting a proper response and leveraging relevant information in matching; and (2) how to model relationships among the utterances in the context.", "labels": [], "entities": []}, {"text": "illustrates the challenges with an example.", "labels": [], "entities": []}, {"text": "First, \"hold a drum class\" and \"drum\" in context are very important.", "labels": [], "entities": []}, {"text": "Without them, one may find responses relevant to the message (i.e., the fifth utterance of the context) but nonsense in the context (e.g., \"what lessons do you want?\").", "labels": [], "entities": []}, {"text": "Second, the message highly depends on the second utterance in the context, and the order of the utterances matters in response selection: exchanging the third utterance and the fifth utterance may lead to different responses.", "labels": [], "entities": []}, {"text": "Existing work, however, either ignores relationships among utterances when concatenating them together (, or loses important information in context in the process of converting the whole context to a vector without enough supervision from responses (e.g., by a hierarchical RNN ( ).", "labels": [], "entities": []}, {"text": "We propose a sequential matching network (SMN), anew context based matching model that can tackle both challenges in an end-to-end way.", "labels": [], "entities": [{"text": "sequential matching network (SMN)", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.8243539780378342}]}, {"text": "The reason that existing models lose important information in the context is that they first represent the whole context as a vector and then match the context vector with a response vector.", "labels": [], "entities": []}, {"text": "Thus, responses in these models connect with the context until the final step in matching.", "labels": [], "entities": []}, {"text": "To avoid information loss, SMN matches a response with each utterance in the context at the beginning and encodes important information in each pair into a matching vector.", "labels": [], "entities": [{"text": "information loss", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7488315999507904}, {"text": "SMN", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9766125679016113}]}, {"text": "The matching vectors are then accumulated in the utterances' temporal order to model their relationships.", "labels": [], "entities": []}, {"text": "The final matching degree is computed with the accumulation of the matching vectors.", "labels": [], "entities": []}, {"text": "Specifically, for each utterance-response pair, the model constructs a word-word similarity matrix and a sequence-sequence similarity matrix by the word embeddings and the hidden states of a recurrent neural network with gated recurrent units (GRU) () respectively.", "labels": [], "entities": []}, {"text": "The two matrices capture important matching information in the pair on a word level and a segment (word subsequence) level respectively, and the information is distilled and fused as a matching vector through an alternation of convolution and pooling operations on the matrices.", "labels": [], "entities": []}, {"text": "By this means, important information from multiple levels of granularity in context is recognized under sufficient supervision from the response and carried into matching with minimal loss.", "labels": [], "entities": []}, {"text": "The matching vectors are then uploaded to another GRU to form a matching score for the context and the response.", "labels": [], "entities": []}, {"text": "The GRU accumulates the pair matching in its hidden states in the chronological order of the utterances in context.", "labels": [], "entities": [{"text": "GRU", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.513949990272522}]}, {"text": "It models relationships and dependencies among the utterances in a matching fashion and has the utterance order supervise the accumulation of pair matching.", "labels": [], "entities": []}, {"text": "The matching degree of the context and the response is computed by a logit model with the hidden states of the GRU.", "labels": [], "entities": [{"text": "GRU", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.8856666684150696}]}, {"text": "SMN extends the powerful \"2D\" matching paradigm in text pair matching for single-turn conversation to context based matching for multi-turn conversation, and enjoys the advantage of both important information in utterance-response pairs and relationships among utterances being sufficiently preserved and leveraged in matching.", "labels": [], "entities": [{"text": "SMN", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9234293699264526}, {"text": "text pair matching", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.6853328943252563}]}, {"text": "We test our model on the Ubuntu dialogue corpus ( which is a large scale publicly available English data set for research in multi-turn conversation.", "labels": [], "entities": [{"text": "Ubuntu dialogue corpus", "start_pos": 25, "end_pos": 47, "type": "DATASET", "confidence": 0.8941157062848409}]}, {"text": "The results show that our model can significantly outperform state-ofthe-art methods, and improvement to the best baseline model on R 10 @1 is over 6%.", "labels": [], "entities": [{"text": "R 10", "start_pos": 132, "end_pos": 136, "type": "DATASET", "confidence": 0.7981019020080566}]}, {"text": "In addition to the Ubuntu corpus, we create a human-labeled Chinese data set, namely the Douban Conversation Corpus, and test our model on it.", "labels": [], "entities": [{"text": "Ubuntu corpus", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.9400002360343933}, {"text": "Chinese data set", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.8008888363838196}, {"text": "Douban Conversation Corpus", "start_pos": 89, "end_pos": 115, "type": "DATASET", "confidence": 0.7905592918395996}]}, {"text": "In contrast to the Ubuntu corpus in which data is collected from a specific domain and negative candidates are randomly sampled, conversations in this data come from the open domain, and response candidates in this data set are collected from a retrieval engine and labeled by three human judges.", "labels": [], "entities": [{"text": "Ubuntu corpus", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.8967369794845581}]}, {"text": "On this data, our model improves the best baseline model by 3% on R 10 @1 and 4% on P@1.", "labels": [], "entities": []}, {"text": "As far as we know, Douban Conversation Corpus is the first human-labeled data set for multi-turn response selection and could be a good complement to the Ubuntu corpus.", "labels": [], "entities": [{"text": "Douban Conversation Corpus", "start_pos": 19, "end_pos": 45, "type": "DATASET", "confidence": 0.7836387157440186}, {"text": "multi-turn response selection", "start_pos": 86, "end_pos": 115, "type": "TASK", "confidence": 0.7150539755821228}, {"text": "Ubuntu corpus", "start_pos": 154, "end_pos": 167, "type": "DATASET", "confidence": 0.9355069100856781}]}, {"text": "We have released Douban Conversation Corups and our source code at https://github.com/MarkWuNLP/ MultiTurnResponseSelection Our contributions in this paper are three-folds: (1) the proposal of anew context based matching model for multi-turn response selection in retrieval-based chatbots; (2) the publication of a large human-labeled data set to research communities; (3) empirical verification of the effectiveness of the model on public data sets.", "labels": [], "entities": [{"text": "multi-turn response selection", "start_pos": 231, "end_pos": 260, "type": "TASK", "confidence": 0.6670577923456827}]}], "datasetContent": [{"text": "We tested our model on a publicly available English data set and a Chinese data set published with this paper.", "labels": [], "entities": [{"text": "English data set", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.7937981386979421}, {"text": "Chinese data set", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.8379304806391398}]}], "tableCaptions": [{"text": " Table 2: Statistics of Douban Conversation Corpus", "labels": [], "entities": []}, {"text": " Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation results of model ablation.", "labels": [], "entities": []}]}