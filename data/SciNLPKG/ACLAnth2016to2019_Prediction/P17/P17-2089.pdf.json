{"title": [{"text": "Sentence Embedding for Neural Machine Translation Domain Adaptation", "labels": [], "entities": [{"text": "Sentence Embedding", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8662469685077667}, {"text": "Neural Machine Translation Domain Adaptation", "start_pos": 23, "end_pos": 67, "type": "TASK", "confidence": 0.8322677135467529}]}], "abstractContent": [{"text": "Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able to improve translation performance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7707432806491852}]}, {"text": "Recently Neural Machine Translation (NMT) has become prominent in the field.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.8061548074086508}]}, {"text": "However, most of the existing domain adaptation methods only focus on phrase-based machine translation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7460404634475708}, {"text": "phrase-based machine translation", "start_pos": 70, "end_pos": 102, "type": "TASK", "confidence": 0.7244394818941752}]}, {"text": "In this paper, we exploit the NMT's internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data.", "labels": [], "entities": [{"text": "NMT", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.8612582683563232}]}, {"text": "The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points.", "labels": [], "entities": [{"text": "IWSLT English-French and NIST Chinese-English tasks", "start_pos": 40, "end_pos": 91, "type": "DATASET", "confidence": 0.7775241136550903}, {"text": "NMT", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.8992430567741394}, {"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9963757395744324}, {"text": "BLEU", "start_pos": 253, "end_pos": 257, "type": "METRIC", "confidence": 0.9938831329345703}]}], "introductionContent": [{"text": "Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks ().", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.7502201497554779}]}, {"text": "An ever increasing amount of data is becoming available for NMT training.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.9574600458145142}]}, {"text": "However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance.", "labels": [], "entities": [{"text": "NMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.9520241022109985}]}, {"text": "Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there are only a few works concerning NMT adaptation ().", "labels": [], "entities": [{"text": "NMT adaptation", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.9847254157066345}]}, {"text": "Most traditional adaptation methods focus on Phrase-Based Statistical Machine Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection  as follows.", "labels": [], "entities": [{"text": "Phrase-Based Statistical Machine Translation (PBSMT)", "start_pos": 45, "end_pos": 97, "type": "TASK", "confidence": 0.6930728810174125}, {"text": "model adaptation", "start_pos": 166, "end_pos": 182, "type": "TASK", "confidence": 0.7528673410415649}, {"text": "data selection", "start_pos": 187, "end_pos": 201, "type": "TASK", "confidence": 0.688324049115181}]}, {"text": "For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7769609987735748}]}, {"text": "These models are then combined to achieve the best performance.", "labels": [], "entities": []}, {"text": "Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation.", "labels": [], "entities": [{"text": "NMT adaptation", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.964307576417923}]}, {"text": "Recently, an NMT adaptation method () was proposed.", "labels": [], "entities": [{"text": "NMT adaptation", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.8624342679977417}]}, {"text": "The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data.", "labels": [], "entities": []}, {"text": "Empirical results show their method can improve NMT performance, and this approach provides a natural baseline.", "labels": [], "entities": [{"text": "NMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.973251461982727}]}, {"text": "For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores.", "labels": [], "entities": []}, {"text": "A language model can be used to score sentences (, as well as joint models, and more recently Convolutional Neural Network (CNN) models).", "labels": [], "entities": []}, {"text": "These methods select useful sentences from the whole corpus, so they can be directly applied to NMT.", "labels": [], "entities": []}, {"text": "However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural translation process.", "labels": [], "entities": [{"text": "PBSMT", "start_pos": 53, "end_pos": 58, "type": "TASK", "confidence": 0.8811655640602112}]}, {"text": "For NMT sentences selection, our hypothesis is that the NMT system itself can be used to score each sentence in the training data.", "labels": [], "entities": [{"text": "NMT sentences selection", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8423856496810913}]}, {"text": "Specifically, an NMT system embeds the source sentence into a vector representation 1 and we can use these vectors to measure a sentence pair's similarity to the in-domain corpus.", "labels": [], "entities": []}, {"text": "In comparison with the CNN or other sentence embedding methods, this method can directly make use of information induced by the NMT system information itself.", "labels": [], "entities": []}, {"text": "In addition, the proposed sentence selection method can be used in conjunction with the NMT further training method ().", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8828906416893005}, {"text": "NMT further training", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.6088364919026693}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics on data sets.", "labels": [], "entities": []}, {"text": " Table 2: IWSLT EN-FR results. Luong and  Manning (2015)'s further (shorted as f ur in  Tables 2 and 3) training method can only be  applied to NMT.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8064593076705933}]}, {"text": " Table 3: NIST ZH-EN results.", "labels": [], "entities": [{"text": "NIST ZH-EN", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.8037281632423401}]}]}