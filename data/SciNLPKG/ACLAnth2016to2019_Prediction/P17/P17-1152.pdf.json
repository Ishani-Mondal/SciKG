{"title": [{"text": "Enhanced LSTM for Natural Language Inference", "labels": [], "entities": [{"text": "Enhanced LSTM", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6450880467891693}, {"text": "Natural Language Inference", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.6710094014803568}]}], "abstractContent": [{"text": "Reasoning and inference are central to human and artificial intelligence.", "labels": [], "entities": []}, {"text": "Modeling inference inhuman language is very challenging.", "labels": [], "entities": []}, {"text": "With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective.", "labels": [], "entities": []}, {"text": "In this paper, we present anew state-of-the-art result , achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.999534010887146}, {"text": "Stanford Natural Language Inference Dataset", "start_pos": 96, "end_pos": 139, "type": "DATASET", "confidence": 0.9095481276512146}]}, {"text": "Unlike the previous top models that use very complicated network architec-tures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models.", "labels": [], "entities": []}, {"text": "Based on this, we further show that by explicitly considering recursive ar-chitectures in both local inference model-ing and inference composition, we achieve additional improvement.", "labels": [], "entities": []}, {"text": "Particularly, incorporating syntactic parsing information contributes to our best result-it further improves the performance even when added to the already very strong model.", "labels": [], "entities": [{"text": "syntactic parsing information", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.749638299147288}]}], "introductionContent": [{"text": "Reasoning and inference are central to both human and artificial intelligence.", "labels": [], "entities": []}, {"text": "Modeling inference inhuman language is notoriously challenging but is a basic problem towards true natural language understanding, as pointed out by, \"a necessary (if not sufficient) condition for true natural language understanding is a mastery of open-domain natural language inference.\"", "labels": [], "entities": [{"text": "Modeling inference inhuman language", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9018073529005051}, {"text": "natural language understanding", "start_pos": 99, "end_pos": 129, "type": "TASK", "confidence": 0.8416090408960978}]}, {"text": "The previous work has included extensive research on recognizing textual entailment.", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.8550152977307638}]}, {"text": "Specifically, natural language inference (NLI) is concerned with determining whether a naturallanguage hypothesis h can be inferred from a premise p, as depicted in the following example from, where the hypothesis is regarded to be entailed from the premise.", "labels": [], "entities": [{"text": "natural language inference (NLI)", "start_pos": 14, "end_pos": 46, "type": "TASK", "confidence": 0.8167208830515543}]}, {"text": "p: Several airlines polled saw costs grow more than expected, even after adjusting for inflation.", "labels": [], "entities": []}, {"text": "h: Some of the companies in the poll reported cost increases.", "labels": [], "entities": []}, {"text": "The most recent years have seen advances in modeling natural language inference.", "labels": [], "entities": [{"text": "natural language inference", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.6080868045488993}]}, {"text": "An important contribution is the creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) dataset", "start_pos": 82, "end_pos": 132, "type": "DATASET", "confidence": 0.7419913485646248}]}, {"text": "The corpus has 570,000 human-written English sentence pairs manually labeled by multiple human subjects.", "labels": [], "entities": []}, {"text": "This makes it feasible to train more complex inference models.", "labels": [], "entities": []}, {"text": "Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (.", "labels": [], "entities": []}, {"text": "While some previous top-performing models use rather complicated network architectures to achieve the state-of-the-art results, we demonstrate in this paper that enhancing sequential inference models based on chain models can outperform all previous results, suggesting that the potentials of such sequential inference approaches have not been fully exploited yet.", "labels": [], "entities": []}, {"text": "More specifically, we show that our sequential inference model achieves an accuracy of 88.0% on the SNLI benchmark.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9996107220649719}, {"text": "SNLI benchmark", "start_pos": 100, "end_pos": 114, "type": "DATASET", "confidence": 0.8063743710517883}]}, {"text": "Exploring syntax for NLI is very attractive to us.", "labels": [], "entities": []}, {"text": "In many problems, syntax and semantics interact closely, including in semantic composition, among others.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.7107183784246445}]}, {"text": "Complicated tasks such as natural language inference could well involve both, which has been discussed in the context of recognizing textual entailment (RTE) (.", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE)", "start_pos": 121, "end_pos": 157, "type": "TASK", "confidence": 0.8276460071404775}]}, {"text": "In this paper, we are interested in exploring this within the neural network frameworks, with the presence of relatively large training data.", "labels": [], "entities": []}, {"text": "We show that by explicitly encoding parsing information with recursive networks in both local inference modeling and inference composition and by incorporating it into our framework, we achieve additional improvement, increasing the performance to anew state of the art with an 88.6% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 284, "end_pos": 292, "type": "METRIC", "confidence": 0.9958897233009338}]}], "datasetContent": [{"text": "Data The Stanford Natural Language Inference (SNLI) corpus) focuses on three basic relationships between a premise and a potential hypothesis: the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral).", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) corpus", "start_pos": 9, "end_pos": 58, "type": "DATASET", "confidence": 0.5211191922426224}]}, {"text": "The original SNLI corpus contains also \"the other\" category, which includes the sentence pairs lacking consensus among multiple human annotators.", "labels": [], "entities": [{"text": "SNLI corpus", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.7993001639842987}]}, {"text": "As in the related work, we remove this category.", "labels": [], "entities": []}, {"text": "We used the same split as in and other previous work.", "labels": [], "entities": []}, {"text": "The parse trees used in this paper are produced by the Stanford PCFG Parser 3.5.3) and they are delivered as part of the SNLI corpus.", "labels": [], "entities": [{"text": "Stanford PCFG Parser 3.5.3", "start_pos": 55, "end_pos": 81, "type": "DATASET", "confidence": 0.9230877459049225}, {"text": "SNLI corpus", "start_pos": 121, "end_pos": 132, "type": "DATASET", "confidence": 0.7926298975944519}]}, {"text": "We use classification accuracy as the evaluation metric, as in related work.", "labels": [], "entities": [{"text": "classification", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.9320780038833618}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.7803740501403809}]}, {"text": "Training We use the development set to select models for testing.", "labels": [], "entities": []}, {"text": "To help replicate our results, we publish our code 1 . Below, we list our training details.", "labels": [], "entities": []}, {"text": "We use the Adam method) for optimization.", "labels": [], "entities": []}, {"text": "The first momentum is set to be 0.9 and the second 0.999.", "labels": [], "entities": []}, {"text": "The initial learning rate is 0.0004 and the batch size is 32.", "labels": [], "entities": []}, {"text": "All hidden states of LSTMs, tree-LSTMs, and word embeddings have 300 dimensions.", "labels": [], "entities": []}, {"text": "We use dropout with a rate of 0.5, which is applied to all feedforward connections.", "labels": [], "entities": []}, {"text": "We use pre-trained 300-D Glove 840B vectors) to initialize our word embeddings.", "labels": [], "entities": []}, {"text": "Out-of-vocabulary (OOV) words are initialized randomly with Gaussian samples.", "labels": [], "entities": []}, {"text": "All vectors including word embedding are updated during training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracies of the models on SNLI. Our final model achieves the accuracy of 88.6%, the best  result observed on SNLI, while our enhanced sequential encoding model attains an accuracy of 88.0%,  which also outperform the previous models.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9787529706954956}, {"text": "SNLI", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.8672245740890503}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9992572665214539}, {"text": "SNLI", "start_pos": 121, "end_pos": 125, "type": "DATASET", "confidence": 0.9173840880393982}, {"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9982328414916992}]}, {"text": " Table 2: Ablation performance of the models.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9972683191299438}]}]}