{"title": [{"text": "A Local Detection Approach for Named Entity Recognition and Mention Detection", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.7399961153666178}, {"text": "Mention Detection", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7926515340805054}]}], "abstractContent": [{"text": "In this paper, we study a novel approach for named entity recognition (NER) and mention detection (MD) in natural language processing.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.768698051571846}, {"text": "mention detection (MD)", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.8059708893299102}]}, {"text": "Instead of treating NER as a sequence labeling problem, we propose anew local detection approach, which relies on the recent fixed-size ordinally forgetting encoding (FOFE) method to fully encode each sentence fragment and its left/right contexts into a fixed-size representation.", "labels": [], "entities": [{"text": "NER", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9288305640220642}, {"text": "local detection", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.8050746321678162}]}, {"text": "Subsequently, a simple feedforward neural network (FFNN) is learned to either reject or predict entity label for each individual text fragment.", "labels": [], "entities": []}, {"text": "The proposed method has been evaluated in several popular NER and MD tasks, including CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) tasks.", "labels": [], "entities": [{"text": "CoNLL 2003 NER task", "start_pos": 86, "end_pos": 105, "type": "DATASET", "confidence": 0.8998455703258514}, {"text": "TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) tasks", "start_pos": 126, "end_pos": 190, "type": "TASK", "confidence": 0.7427397936582565}]}, {"text": "Our method has yielded pretty strong performance in all of these examined tasks.", "labels": [], "entities": []}, {"text": "This local detection approach has shown many advantages over the traditional sequence labeling methods.", "labels": [], "entities": [{"text": "local detection", "start_pos": 5, "end_pos": 20, "type": "TASK", "confidence": 0.7588786780834198}]}], "introductionContent": [{"text": "Natural language processing (NLP) plays an important role in artificial intelligence, which has been extensively studied for many decades.", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7642235855261484}]}, {"text": "Conventional NLP techniques include the rule-based symbolic approaches widely used about two decades ago, and the more recent statistical approaches relying on feature engineering and statistical models.", "labels": [], "entities": []}, {"text": "In the recent years, deep learning approach has achieved huge successes in many applications, ranging from speech recognition to image classification.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.8132884502410889}, {"text": "image classification", "start_pos": 129, "end_pos": 149, "type": "TASK", "confidence": 0.8186586201190948}]}, {"text": "It is drawing increasing attention in the NLP community.", "labels": [], "entities": []}, {"text": "In this paper, we are interested in a fundamental problem in NLP, namely named entity recognition (NER) and mention detection (MD).", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.7317712704340616}, {"text": "mention detection (MD)", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.7621844053268433}]}, {"text": "NER and MD are very challenging tasks in NLP, laying the foundation of almost every NLP application.", "labels": [], "entities": [{"text": "MD", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.9132040143013}]}, {"text": "NER and MD are tasks of identifying entities (named and/or nominal) from raw text, and classifying the detected entities into one of the pre-defined categories such as person (PER), organization (ORG), location (LOC), etc.", "labels": [], "entities": []}, {"text": "Some tasks focus on named entities only, while the others also detect nominal mentions.", "labels": [], "entities": []}, {"text": "Moreover, nested mentions may need to be extracted too.", "labels": [], "entities": []}, {"text": "For example,P ER and herP ER N studied in where Toronto is a LOC entity, embedded in another longer ORG entity University of Toronto.", "labels": [], "entities": [{"text": "herP ER N", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.7609061002731323}, {"text": "ORG entity University of Toronto", "start_pos": 100, "end_pos": 132, "type": "DATASET", "confidence": 0.8267816424369812}]}, {"text": "Similar to many other NLP problems, NER and MD is formulated as a sequence labeling problem, where a tag is sequentially assigned to each word in the input sentence.", "labels": [], "entities": []}, {"text": "It has been extensively studied in the NLP community.", "labels": [], "entities": []}, {"text": "The core problem is to model the conditional probability of an output sequence given an arbitrary input sequence.", "labels": [], "entities": []}, {"text": "Many hand-crafted features are combined with statistical models, such as conditional random fields (CRFs), to compute conditional probabilities.", "labels": [], "entities": []}, {"text": "More recently, some popular neural networks, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs), are proposed to solve sequence labelling problems.", "labels": [], "entities": []}, {"text": "In the inference stage, the learned models compute the conditional probabilities and the output sequence is generated by the Viterbi decoding algorithm.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel local detection approach for solving NER and MD problems.", "labels": [], "entities": [{"text": "local detection", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.778931200504303}, {"text": "NER and MD", "start_pos": 71, "end_pos": 81, "type": "TASK", "confidence": 0.7727952798207601}]}, {"text": "The idea can be easily extended to many other se-quence labeling problems, such as chunking, partof-speech tagging (POS).", "labels": [], "entities": [{"text": "partof-speech tagging (POS)", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.8209554135799408}]}, {"text": "Instead of globally modeling the whole sequence in training and jointly decode the entire output sequence in test, our method examines all word segments (up to a certain length) in a sentence.", "labels": [], "entities": []}, {"text": "A word segment will be examined individually based on the underlying segment itself and its left and right contexts in the sentence so as to determine whether this word segment is a valid named entity and the corresponding label if it is.", "labels": [], "entities": []}, {"text": "This approach conforms to the way human resolves an NER problem.", "labels": [], "entities": [{"text": "NER problem", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.8829353451728821}]}, {"text": "Given any word fragment and its contexts in a sentence or paragraph, people accurately determine whether this word segment is a named entity or not.", "labels": [], "entities": []}, {"text": "People rarely conduct a global decoding over the entire sentence to make such a decision.", "labels": [], "entities": []}, {"text": "The key to making an accurate local decision for each individual fragment is to have full access to the fragment itself as well as its complete contextual information.", "labels": [], "entities": []}, {"text": "The main pitfall to implement this idea is that we cannot easily encode the segment and its contexts in models since they are of varying lengths in natural languages.", "labels": [], "entities": []}, {"text": "Many feature engineering techniques have been proposed but all of these methods will inevitably lead to information loss.", "labels": [], "entities": [{"text": "information loss", "start_pos": 104, "end_pos": 120, "type": "TASK", "confidence": 0.7627657055854797}]}, {"text": "In this work, we propose to use a recent fixed-size encoding method, namely fixed-size ordinally forgetting encoding (FOFE) (, to solve this problem.", "labels": [], "entities": [{"text": "fixed-size ordinally forgetting encoding (FOFE)", "start_pos": 76, "end_pos": 123, "type": "METRIC", "confidence": 0.6032838353088924}]}, {"text": "The FOFE method is a simple recursive encoding method.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.823842465877533}]}, {"text": "FOFE theoretically guarantees (almost) unique and lossless encoding of any variable-length sequence.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.4870622158050537}]}, {"text": "The left and the right contexts for each word segment are encoded by FOFE method, and then a simple neural network can be trained to make a precise recognition for each individual word segment based on the fixed-size presentation of the contextual information.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9471637606620789}]}, {"text": "This FOFE-based local detection approach is more appealing to NER and MD.", "labels": [], "entities": [{"text": "FOFE-based local detection", "start_pos": 5, "end_pos": 31, "type": "TASK", "confidence": 0.7306430339813232}, {"text": "NER", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.6812131404876709}]}, {"text": "Firstly, feature engineering is almost eliminated.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.8563886284828186}]}, {"text": "Secondly, under this local detection framework, nested mention is handled with little modification.", "labels": [], "entities": []}, {"text": "Next, it makes better use of partially-labeled data available from many application scenarios.", "labels": [], "entities": []}, {"text": "Sequence labeling model requires all entities in a sentence to be labeled.", "labels": [], "entities": [{"text": "Sequence labeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.896207720041275}]}, {"text": "If only some (not all) entities are labeled, it is not effective to learn a sequence labeling model.", "labels": [], "entities": []}, {"text": "However, every single labeled entity, along with its contexts, maybe used to learn the proposed model.", "labels": [], "entities": []}, {"text": "At last, due to the simplicity of FOFE, simple neural networks, such as multilayer perceptrons, are sufficient for recognition.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 34, "end_pos": 38, "type": "TASK", "confidence": 0.352658748626709}, {"text": "recognition", "start_pos": 115, "end_pos": 126, "type": "TASK", "confidence": 0.9626184701919556}]}, {"text": "These models are much faster to train and easier to tune.", "labels": [], "entities": []}, {"text": "In the test stage, all possible word segments from a sentence maybe packed into a mini-batch, jointly recognized in parallel on GPUs.", "labels": [], "entities": []}, {"text": "This leads to a very fast decoding process as well.", "labels": [], "entities": []}, {"text": "In this paper, we have applied this FOFE-based local detection approach to several popular NER and MD tasks, including the CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Trilingual Entity Discovery and Linking (EDL) tasks.", "labels": [], "entities": [{"text": "FOFE-based local detection", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.7491294244925181}, {"text": "CoNLL 2003 NER task", "start_pos": 123, "end_pos": 142, "type": "DATASET", "confidence": 0.9317710995674133}, {"text": "Trilingual Entity Discovery and Linking (EDL) tasks", "start_pos": 175, "end_pos": 226, "type": "TASK", "confidence": 0.7949692275789049}]}, {"text": "Our proposed method has yielded strong performance in all of these examined tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the effectiveness of our proposed methods on several popular NER and MD tasks, including CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Trilingual Entity Discovery and Linking (EDL) tasks.", "labels": [], "entities": [{"text": "CoNLL 2003 NER task", "start_pos": 118, "end_pos": 137, "type": "DATASET", "confidence": 0.893979087471962}, {"text": "TAC-KBP2016 Trilingual Entity Discovery and Linking (EDL) tasks", "start_pos": 158, "end_pos": 221, "type": "TASK", "confidence": 0.759220540523529}]}, {"text": "We have made our codes available at https:// github.com/xmb-cipher/fofe-ner for readers to reproduce the results in this paper.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Effect of various FOFE feature combinations on the CoNLL2003 test data.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9819626212120056}, {"text": "CoNLL2003 test data", "start_pos": 61, "end_pos": 80, "type": "DATASET", "confidence": 0.9685389796892802}]}, {"text": " Table 2: Performance (F 1 score) comparison among various neural models reported on the CoNLL  dataset, and the different features used in these methods.", "labels": [], "entities": [{"text": "F 1 score)", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.921372264623642}, {"text": "CoNLL  dataset", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.963916540145874}]}, {"text": " Table 4: Entity Discovery Performance of our  method on the KBP2015 EDL evaluation data,  with comparison to the best systems in KBP2015  official evaluation.", "labels": [], "entities": [{"text": "Entity Discovery", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6765556484460831}, {"text": "KBP2015 EDL evaluation data", "start_pos": 61, "end_pos": 88, "type": "DATASET", "confidence": 0.9516797512769699}, {"text": "KBP2015  official evaluation", "start_pos": 130, "end_pos": 158, "type": "DATASET", "confidence": 0.8692827820777893}]}, {"text": " Table 5: Official entity discovery performance of our methods on KBP2016 trilingual EDL track. Neither  KBP2015 nor in-house data labels nominal mentions. Nominal mentions in Spanish are totally ignored  since no training data is found for them.", "labels": [], "entities": [{"text": "KBP2016 trilingual EDL track", "start_pos": 66, "end_pos": 94, "type": "DATASET", "confidence": 0.8877503126859665}, {"text": "KBP2015", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.9572544097900391}]}, {"text": " Table 6: Our entity discovery official performance  (English only) in KBP2016 is shown as a compar- ison of three models trained by different combina- tions of training data sets.", "labels": [], "entities": [{"text": "KBP2016", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9552202820777893}]}]}