{"title": [{"text": "Neural Machine Translation via Binary Code Prediction", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7887261708577474}]}], "abstractContent": [{"text": "In this paper, we propose anew method for calculating the output layer in neural machine translation systems.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.6948428750038147}]}, {"text": "The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case.", "labels": [], "entities": []}, {"text": "In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes.", "labels": [], "entities": []}, {"text": "Experiments on two English \u2194 Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.", "labels": [], "entities": [{"text": "English \u2194 Japanese bidirectional translation tasks", "start_pos": 19, "end_pos": 69, "type": "TASK", "confidence": 0.707106371720632}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9993941783905029}]}], "introductionContent": [{"text": "When handling broad or open domains, machine translation systems usually have to handle a large vocabulary as their inputs and outputs.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7622780799865723}]}, {"text": "This is particularly a problem in neural machine translation (NMT) models ), such as the attention-based models ( shown in.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.8381630579630533}]}, {"text": "In these models, the output layer is required to generate a specific word from an internal vector, and a large vocabulary size tends to require a large amount of computation to predict each of the candidate word probabilities.", "labels": [], "entities": []}, {"text": "Because this is a significant problem for neural language and translation models, there area number of methods proposed to resolve this problem, which we detail in Section 2.2.", "labels": [], "entities": [{"text": "neural language and translation models", "start_pos": 42, "end_pos": 80, "type": "TASK", "confidence": 0.6627303540706635}]}, {"text": "However, none of these previous methods simultaneously satisfies the following desiderata, all of which, we argue, are desirable for practical use in NMT systems: Memory efficiency: The method should not require large memory to store the parameters and calculated vectors to maintain scalability in resource-constrained environments.", "labels": [], "entities": []}, {"text": "Time efficiency: The method should be able to train the parameters efficiently, and possible to perform decoding efficiently with choosing the candidate words from the full probability distribution.", "labels": [], "entities": [{"text": "Time", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8935609459877014}]}, {"text": "In particular, the method should be performed fast on general CPUs to suppress physical costs of computational resources for actual production systems.", "labels": [], "entities": []}, {"text": "Compatibility with parallel computation: It should be easy for the method to be minibatched and optimized to run efficiently on GPUs, which are essential for training large NMT models.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method that satisfies all of these conditions: requires significantly less memory, fast, and is easy to implement minibatched on GPUs.", "labels": [], "entities": []}, {"text": "The method works by not predicting a softmax over the entire output vocab-ulary, but instead by encoding each vocabulary word as a vector of binary variables, then independently predicting the bits of this binary representation.", "labels": [], "entities": []}, {"text": "In order to represent a vocabulary size of 2 n , the binary representation need only beat least n bits long, and thus the amount of computation and size of parameters required to select an output word is only O(log V ) in the size of the vocabulary V , a great reduction from the standard linear increase of O(V ) seen in the original softmax.", "labels": [], "entities": [{"text": "O", "start_pos": 308, "end_pos": 309, "type": "METRIC", "confidence": 0.9690259695053101}]}, {"text": "While this idea is simple and intuitive, we found that it alone was not enough to achieve competitive accuracy with real NMT models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9958089590072632}]}, {"text": "Thus we make two improvements: First, we propose a hybrid model, where the high frequency words are predicted by a standard softmax, and low frequency words are predicted by the proposed binary codes separately.", "labels": [], "entities": []}, {"text": "Second, we propose the use of convolutional error correcting codes with Viterbi decoding, which add redundancy to the binary representation, and even in the face of localized mistakes in the calculation of the representation, are able to recover the correct word.", "labels": [], "entities": []}, {"text": "In experiments on two translation tasks, we find that the proposed hybrid method with error correction is able to achieve results that are competitive with standard softmax-based models while reducing the output layer to a fraction of its original size.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.9035698771476746}]}], "datasetContent": [{"text": "We examined the performance of the proposed methods on two English-Japanese bidirectional translation tasks which have different translation difficulties: ASPEC (Nakazawa et al., 2016) and BTEC.", "labels": [], "entities": [{"text": "ASPEC", "start_pos": 155, "end_pos": 160, "type": "METRIC", "confidence": 0.49889612197875977}, {"text": "BTEC", "start_pos": 189, "end_pos": 193, "type": "DATASET", "confidence": 0.9173752069473267}]}, {"text": "describes details of two corpora.", "labels": [], "entities": []}, {"text": "To prepare inputs for training, we used tokenizer.perl in Moses ( and) for English/Japanese tokenizations respectively, applied lowercase.perl from Moses, and replaced out-of-vocabulary words such that rank(w) > V \u2212 3 into the UNK marker.", "labels": [], "entities": [{"text": "UNK", "start_pos": 227, "end_pos": 230, "type": "DATASET", "confidence": 0.8410934805870056}]}, {"text": "We implemented each NMT model using C++ in the DyNet framework ( and trained/tested on 1 GPU (GeForce GTX TITAN X).", "labels": [], "entities": [{"text": "DyNet framework", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9005502760410309}, {"text": "GeForce GTX TITAN X", "start_pos": 94, "end_pos": 113, "type": "DATASET", "confidence": 0.849711999297142}]}, {"text": "Each testis also performed on CPUs to compare its processing time.", "labels": [], "entities": []}, {"text": "We used a bidirectional RNN-based encoder applied in, unidirectional decoder with the same style of (, and the concat global attention model also proposed in.", "labels": [], "entities": []}, {"text": "Each recurrent unit is constructed using a 1-layer LSTM (input/forget/output gates and nonpeepholes) () with 30% dropout () for the input/output vectors of the LSTMs.", "labels": [], "entities": []}, {"text": "All word embeddings, recurrent states and model-specific hidden states are designed with 512-dimentional vectors.", "labels": [], "entities": []}, {"text": "Only output layers and loss functions are replaced, and other network architectures are identical for the conventional/proposed models.", "labels": [], "entities": []}, {"text": "We used the Adam optimizer () with fixed hyperparameters \u03b1 = 0.001, \u03b2 1 = 0.9 \u03b2 2 = 0.999, \u03b5 = 10 \u22128 , and mini-batches with 64 sentences sorted according to their sequence lengths.", "labels": [], "entities": []}, {"text": "For evaluating the quality of each model, we calculated case-insensitive BLEU () every 1000 mini-batches.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9893931150436401}]}, {"text": "lists summaries of all methods we examined in experiments.", "labels": [], "entities": []}, {"text": "shows the BLEU on the test set (bold and italic faces indicate the best and second places in each task), number of bits B (or B ) for the binary code, actual size of the output layer # out , number of parameters in the output layer # W,\u03b2 , as well as the ratio of # W,\u03b2 or amount of whole parameters compared with Softmax, and averaged processing time at training (per mini-batch on GPUs) and test (per sentence on GPUs/CPUs), respectively.(a) and 5(b) shows training curves up to 180,000 epochs about some English\u2192Japanese settings.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9982208609580994}]}, {"text": "To relax instabilities of translation qualities while training (as shown in(a) and 5(b)), each BLEU in is calculated by averaging actual test BLEU of 5 consecutive results  around the epoch that has the highest dev BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9980283379554749}, {"text": "BLEU", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9711065888404846}, {"text": "BLEU", "start_pos": 215, "end_pos": 219, "type": "METRIC", "confidence": 0.9639385938644409}]}], "tableCaptions": [{"text": " Table 1: Details of the corpus.  Name  ASPEC BTEC  Languages  En \u2194 Ja", "labels": [], "entities": [{"text": "ASPEC BTEC  Languages  En \u2194 Ja", "start_pos": 40, "end_pos": 70, "type": "DATASET", "confidence": 0.8110753297805786}]}, {"text": " Table 3: Comparison of BLEU, size of output layers, number of parameters and processing time.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9947951436042786}]}]}