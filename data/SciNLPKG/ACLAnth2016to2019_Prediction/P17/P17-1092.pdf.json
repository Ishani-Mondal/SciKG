{"title": [{"text": "Neural Discourse Structure for Text Categorization", "labels": [], "entities": [{"text": "Neural Discourse Structure", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7383493781089783}, {"text": "Text Categorization", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7222669720649719}]}], "abstractContent": [{"text": "We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization.", "labels": [], "entities": []}, {"text": "Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task.", "labels": [], "entities": [{"text": "RST", "start_pos": 188, "end_pos": 191, "type": "TASK", "confidence": 0.9335362911224365}]}, {"text": "Experiments consider variants of the approach and illustrate its strengths and weaknesses.", "labels": [], "entities": []}], "introductionContent": [{"text": "Advances in text categorization have the potential to improve systems for analyzing sentiment, inferring authorship or author attributes, making predictions, and many more.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7295630276203156}, {"text": "analyzing sentiment", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7562807500362396}]}, {"text": "Several past researchers have noticed that methods that reason about the relative salience or importance of passages within a text can lead to improvements).", "labels": [], "entities": []}, {"text": "Latent variables (, structured-sparse regularizers, and neural attention models () have all been explored.", "labels": [], "entities": []}, {"text": "Discourse structure, which represents the organization of a text as a tree (for an example, see), might provide cues for the importance of different parts of a text.", "labels": [], "entities": []}, {"text": "Some promising results on sentiment classification tasks support this idea: and applied hand-crafted weighting schemes to the sentences in a document, based on its discourse structure, and showed benefit to sentiment polarity classification.", "labels": [], "entities": [{"text": "sentiment classification tasks", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.9199676314989725}, {"text": "sentiment polarity classification", "start_pos": 207, "end_pos": 240, "type": "TASK", "confidence": 0.7674771944681803}]}, {"text": "In this paper, we investigate the value of discourse structure for text categorization more broadly, considering five tasks, through the use of a recursive neural network built on an automatically-derived document parse from a topperforming, open-source discourse parser, DPLP).", "labels": [], "entities": [{"text": "text categorization", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7291164547204971}]}, {"text": "Our models learn to weight the importance of a document's sentences, based on their positions and relations in the discourse tree.", "labels": [], "entities": []}, {"text": "We introduce anew, unnormalized attention mechanism to this end.", "labels": [], "entities": []}, {"text": "Experimental results show that variants of our model outperform prior work on four out of five tasks considered.", "labels": [], "entities": []}, {"text": "Our method unsurprisingly underperforms on the fifth task, making predictions about legislative bills-a genre in which discourse conventions are quite different from those in the discourse parser's training data.", "labels": [], "entities": []}, {"text": "Further experiments show the effect of discourse parse quality on text categorization performance, suggesting that future improvements to discourse parsing will payoff for text categorization, and validate our new attention mechanism.", "labels": [], "entities": [{"text": "discourse parse", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7013992667198181}, {"text": "discourse parsing", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.7282919883728027}, {"text": "text categorization", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.7587428987026215}]}, {"text": "Our implementation is available at https:// github.com/jiyfeng/disco4textcat.", "labels": [], "entities": []}], "datasetContent": [{"text": "We selected five datasets of different sizes and corresponding to varying categorization tasks.", "labels": [], "entities": []}, {"text": "Some information about these datasets is summarized in.", "labels": [], "entities": []}, {"text": "Sentiment analysis on Yelp reviews.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8749403357505798}, {"text": "Yelp reviews", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.8891536593437195}]}, {"text": "Originally from the Yelp Dataset Challenge in 2015, this dataset contains 1.5 million examples.", "labels": [], "entities": [{"text": "Yelp Dataset Challenge in 2015", "start_pos": 20, "end_pos": 50, "type": "DATASET", "confidence": 0.9686200022697449}]}, {"text": "We used the preprocessed dataset from, which has 650,000 training and 50,000 test examples.", "labels": [], "entities": []}, {"text": "The task is to predict an ordinal rating (1-5) from the text of the review.", "labels": [], "entities": []}, {"text": "To select the best combination of hyperparameters, we randomly sampled 10% training examples as the development data.", "labels": [], "entities": []}, {"text": "We compared with hierarchical attention networks (, which use the normalized attention mechanism on both word and sentence layers with a flat document structure, and provide the state-of-the-art result on this corpus.", "labels": [], "entities": []}, {"text": "The corpus was originally collected by, and the data split we used was constructed by.", "labels": [], "entities": []}, {"text": "The goal is to predict the vote (\"yea\" or \"nay\") for the speaker of each speech segment.", "labels": [], "entities": [{"text": "predict the vote (\"yea", "start_pos": 15, "end_pos": 37, "type": "METRIC", "confidence": 0.7873538732528687}]}, {"text": "The most recent work on this corpus is from, which proposed structured regularization methods based on linguistic components, e.g., sentences, topics, and syntactic parses.", "labels": [], "entities": []}, {"text": "Each regularization method induces a linguistic bias to improve text classification accuracy, where the best result we repeated here is from the model with sentence regularizers.", "labels": [], "entities": [{"text": "text classification", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.747722327709198}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9221343398094177}]}, {"text": "This classic movie review corpus was constructed by and includes 1,000 positive and 1,000 negative reviews.", "labels": [], "entities": []}, {"text": "On this corpus, we used the standard tenfold data split for cross validation and reported the average accuracy across folds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9973281621932983}]}, {"text": "We compared with the work from both Congressional bill corpus.", "labels": [], "entities": []}, {"text": "This corpus, collected by, includes 51,762 legislative bills from the 103rd to 111th U.S. Congresses.", "labels": [], "entities": []}, {"text": "The task is to predict whether a bill will survive based on its content.", "labels": [], "entities": []}, {"text": "We randomly sampled 10% training examples as development data to search for the best hyperparameters.", "labels": [], "entities": []}, {"text": "To our knowledge, the best published results are due to, which is the same baseline as for the congressional floor debates corpus.", "labels": [], "entities": [{"text": "congressional floor debates corpus", "start_pos": 95, "end_pos": 129, "type": "DATASET", "confidence": 0.6111506372690201}]}, {"text": "We evaluated all variants of our model on the five datasets presented in section 5, comparing in each case to the published state of the art as well as the most relevant works.", "labels": [], "entities": []}, {"text": "See  This finding demonstrates the benefit of explicit discourse structure-even the output from an imperfect parser-for text categorization in some genres.", "labels": [], "entities": []}, {"text": "This benefit is supported by both UN-LABELED and FULL, since both of them use discourse structures of texts.", "labels": [], "entities": [{"text": "UN-LABELED", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.6866441369056702}, {"text": "FULL", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.961479663848877}]}, {"text": "The advantage of using discourse information varies on different genres and different corpus sizes.", "labels": [], "entities": []}, {"text": "Even though the discourse parser is trained on news text, it still offers benefit to restaurant and movie reviews and to the genre of congressional debates.", "labels": [], "entities": [{"text": "discourse parser", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7752342224121094}]}, {"text": "Even for news text, if the training dataset is small (e.g., MFC), a lighter-weight variant of discourse (UNLABELED) is preferred.", "labels": [], "entities": [{"text": "UNLABELED", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.7918611764907837}]}, {"text": "Legislative bills, which have technical legal content and highly specialized conventions (see the supplementary material for an example), are arguably the most distant genre from news among those we considered.", "labels": [], "entities": []}, {"text": "On that task, we see discourse working against accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9917603731155396}]}, {"text": "Note that the corpus of bills is more than ten times larger than three cases where our UNLABELED model outperformed past methods, suggesting that the drop in performance is not due to lack of data.", "labels": [], "entities": []}, {"text": "It is also important to notice that the ROOT model performs quite poorly in all cases.", "labels": [], "entities": [{"text": "ROOT", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.40846124291419983}]}, {"text": "This implies that discourse structure is not simply helping by finding a single EDU upon which to make the categorization decision.", "labels": [], "entities": []}, {"text": "shows some example texts from the Yelp Review corpus with their discourse structures produced by DPLP, where the weights were generated with the FULL model. are two successful examples of the FULL model.", "labels": [], "entities": [{"text": "Yelp Review corpus", "start_pos": 34, "end_pos": 52, "type": "DATASET", "confidence": 0.9665902455647787}, {"text": "FULL", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.8760959506034851}]}, {"text": "shows a simple case with respect to the discourse structure. is slightly different-the text in this example may have more than one reasonable discourse structure, e.g., 2D could be a child of 2C instead of 2A.", "labels": [], "entities": []}, {"text": "In both cases, discourse structures help the FULL model bias to the important sentences., on the other hand, presents a negative example, where DPLP failed to identify the most salient sentence 3F . In addition, the weights produced by the FULL model do not make much sense, which we suspect the model was confused by the structure.(c) also presents a manually-constructed discourse structure on the same text for reference.", "labels": [], "entities": [{"text": "FULL", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.7388225197792053}]}, {"text": "A more accurate prediction is expected if we use this manuallyconstructed discourse structure, because it has the appropriate dependency between sentences.", "labels": [], "entities": []}, {"text": "In addition, the annotated discourse relations are able to select the right relation-specific composition matrices in FULL model, which are consistent with the training examples.", "labels": [], "entities": [{"text": "FULL", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.6103342771530151}]}, {"text": "A natural question is whether further improvements to RST discourse parsing would lead to even greater gains in text categorization.", "labels": [], "entities": [{"text": "RST discourse parsing", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.93973708152771}]}, {"text": "While advances in discourse parsing are beyond the scope of this paper, we can gain some insight by exploring degradation to the DPLP parser.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.6758185774087906}, {"text": "DPLP parser", "start_pos": 129, "end_pos": 140, "type": "TASK", "confidence": 0.6715366840362549}]}, {"text": "An easy way to do this is to train it on subsets of the RST discourse treebank.", "labels": [], "entities": [{"text": "RST discourse treebank", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.7403194308280945}]}, {"text": "We repeated the conditions described above for our FULL model, training DPLP on 25%, 50%, and 75% of the training set (randomly selected in each case) before re-parsing the data for the sentiment analysis task.", "labels": [], "entities": [{"text": "FULL", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.4961903393268585}, {"text": "sentiment analysis task", "start_pos": 186, "end_pos": 209, "type": "TASK", "confidence": 0.9470848441123962}]}, {"text": "We did not repeat the hyperparameter search.", "labels": [], "entities": []}, {"text": "In, we plot accuracy of the classifier (y-axis) against the F 1 performance of the discourse parser (x-axis).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9991905093193054}, {"text": "F 1", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9614787697792053}]}, {"text": "Unsurprisingly, lower parsing performance implies lower classification accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.9482584595680237}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9565033316612244}]}, {"text": "Notably, if the RST discourse treebank were reduced to 25% of its size, our method would underperform the discourseignorant model of.", "labels": [], "entities": [{"text": "RST discourse treebank", "start_pos": 16, "end_pos": 38, "type": "DATASET", "confidence": 0.7846049070358276}]}, {"text": "While we cannot extrapolate with certainty, these findings suggest that further improvements to discourse parsing, through larger annotated datasets or improved models, could lead to greater gains.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.7186205983161926}]}, {"text": "In section 3, we contrasted our new attention mechanism (Equation 2), which is inspired by RST's lack of \"competition\" for salience among satellites, with the attention mechanism used in machine translation (Bahdanau et al., 2015).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.7432399094104767}]}, {"text": "We consider here a variant of our model with normalized attention: The result here is a vector \u03b1 i , with one element for each child node j \u2208 children(i), and which sums to one.", "labels": [], "entities": []}, {"text": "On Yelp dateset, this variant of the FULL model achieves 70.3% accuracy (1.5% absolute behind our FULL model), giving empirical support to our theoretically-motivated design decision not to normalize attention.", "labels": [], "entities": [{"text": "Yelp dateset", "start_pos": 3, "end_pos": 15, "type": "DATASET", "confidence": 0.9608978033065796}, {"text": "FULL", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.655942440032959}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.999183714389801}]}, {"text": "Of course, further architecture improvements may yet be possible.", "labels": [], "entities": []}, {"text": "Our findings in this work show the benefit of using discourse structure for text categorization.", "labels": [], "entities": []}, {"text": "Although discourse structure strongly improves the performance on most of corpora in our experiments, its benefit is limited particularly by two factors: (1) the state-of-the-art performance on RST discourse parsing; and (2) domain mismatch between the training corpus fora discourse parser and the domain where the discourse parser is used.", "labels": [], "entities": [{"text": "RST discourse parsing", "start_pos": 194, "end_pos": 215, "type": "TASK", "confidence": 0.9076962272326151}]}, {"text": "For the first factor, discourse parsing is still an active research topic in NLP, and may yet improve.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7924362421035767}]}, {"text": "The second factor suggests exploring domain adaptation methods or even direct discourse annotation for genres of interest.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Information about the five datasets used in our experiments. To compare with prior work, we  use different experimental settings. For Yelp and Bill corpora, we use 10% of the training examples as  development data. For MFC and Movies corpora, we use 10-fold cross validation and report averages  across all folds.", "labels": [], "entities": []}, {"text": " Table 2: Test-set accuracy across five datasets. Results from prior work are reprinted from the corre- sponding publications. Boldface marks performance stronger than the previous state of the art.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9969340562820435}]}]}