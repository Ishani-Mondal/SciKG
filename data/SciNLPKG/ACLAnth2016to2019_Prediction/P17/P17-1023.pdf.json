{"title": [{"text": "Obtaining referential word meanings from visual and distributional information: Experiments on object naming", "labels": [], "entities": [{"text": "Obtaining referential word meanings", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8947319537401199}, {"text": "object naming", "start_pos": 95, "end_pos": 108, "type": "TASK", "confidence": 0.7914120852947235}]}], "abstractContent": [{"text": "We investigate object naming, which is an important sub-task of referring expression generation on real-world images.", "labels": [], "entities": [{"text": "object naming", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.7601515054702759}, {"text": "referring expression generation", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.8173121611277262}]}, {"text": "As opposed to mutually exclusive labels used in object recognition, object names are more flexible, subject to communicative preferences and semantically related to each other.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.742838054895401}]}, {"text": "Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings.", "labels": [], "entities": [{"text": "referential word meaning", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.6738830010096232}]}, {"text": "We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training.", "labels": [], "entities": []}, {"text": "We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space.", "labels": [], "entities": [{"text": "zero-shot learning", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7985745668411255}]}, {"text": "Ina standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning.", "labels": [], "entities": [{"text": "object naming task", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.8015974760055542}]}], "introductionContent": [{"text": "Expressions referring to objects in visual scenes typically include a word naming the type of the object: E.g., house in, or, as a very general type, thingy in (d).", "labels": [], "entities": []}, {"text": "Determining such a name is a crucial step for referring expression generation (REG) systems, as many other decisions concerning, e.g., the selection of attributes follow from it ().", "labels": [], "entities": [{"text": "referring expression generation (REG)", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.8427403072516123}]}, {"text": "For along time, however, research on REG mostly assumed the availability of symbolic representations of ref-(a)\"house\" (b)\"buildings\" (c)\"large structure\" (d)\"roof thingy\" Figure 1: Examples of object names in the REFERIT corpus referring to instances of buildings erent and scene, and sidestepped questions about how speakers actually choose these names, due to the lack of models capable of capturing what a word like house refers to in the real world.", "labels": [], "entities": []}, {"text": "Recent advances in image processing promise to fill this gap, with state-of-the-art computer vision systems being able to classify images into thousands of different categories (e.g.).", "labels": [], "entities": [{"text": "image processing", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.782236248254776}]}, {"text": "However, classification is not naming (.", "labels": [], "entities": [{"text": "classification", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.9675806760787964}]}, {"text": "Standard object classification schemes are inherently \"flat\", and treat object labels as mutually exclusive.", "labels": [], "entities": [{"text": "object classification", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.7410746812820435}]}, {"text": "A state-of-the-art object recognition system would be trained to classify the object in e.g. (a) as either house or building, ignoring the lexical similarity between these two names.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7422290146350861}]}, {"text": "In contrast, humans seem to be more flexible as to the chosen level of generality.", "labels": [], "entities": []}, {"text": "Depending on the prototypicality of the object to name, and possibly other visual properties, a general name might be more or less appropriate.", "labels": [], "entities": []}, {"text": "For instance, a robin can be named bird, but a penguin is better referred 243 to as \"penguin\"; along the same lines, the rather unusual building in (c) that is not easy to otherwise categorise was named \"structure\".", "labels": [], "entities": []}, {"text": "Other work at the intersection of image and language processing has investigated models that learn to directly associate visual objects with a continuous representation of word meaning, i.e. through cross-modal transfer into distributional vector spaces (.", "labels": [], "entities": []}, {"text": "Here, the idea is to exploit a powerful model of lexical similarity induced from large amounts text for being able to capture inherent lexical relations between object categories.", "labels": [], "entities": []}, {"text": "Thus, under the assumption that such semantic spaces represent, in some format least, taxonomic knowledge, this makes labels on different levels of specificity available fora given object.", "labels": [], "entities": []}, {"text": "Moreover, if the mapping is sufficiently general, it should be able to map objects to an appropriate label, even if during training of the mapping this label has not been seen (zero-shot learning).", "labels": [], "entities": []}, {"text": "While cross-modal transfer seems to be a conceptually attractive model for learning object names, it is based on an important assumption that, in our view, has not received sufficient attention in previous works: it assumes that a given distributional vector space constitutes an optimal target representation that visual instances of objects can be mapped to.", "labels": [], "entities": [{"text": "cross-modal transfer", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.7703219056129456}]}, {"text": "However, distributional representations of word meaning are known to capture a rather fuzzy notion of lexical similarity, e.g. car is similar to van and to street.", "labels": [], "entities": []}, {"text": "A cross-modal transfer model is \"forced\" to learn to map objects into the same area in the semantic space if their names are distributionally similar, but regardless of their actual visual similarity.", "labels": [], "entities": [{"text": "cross-modal transfer", "start_pos": 2, "end_pos": 22, "type": "TASK", "confidence": 0.7084694355726242}]}, {"text": "Indeed, we have found in a recent study that the contribution of distributional information to learning referential word meanings is restricted to certain types of words and does not generalize across the vocabulary (.", "labels": [], "entities": []}, {"text": "The goal of this work is to learn a model of referential word meaning that makes accurate object naming predictions and goes beyond treating words as independent, mutually exclusive labels in a flat classification scheme.", "labels": [], "entities": [{"text": "referential word meaning", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.6800771752993265}, {"text": "object naming predictions", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.8323613007863363}]}, {"text": "We extend upon work on learning models of referential word use from corpora of images paired with referring expressions () that treats words as individual predictors capturing referential appropriateness.", "labels": [], "entities": []}, {"text": "We explore different ways of linking these predictors to distributional knowledge, during application and during training.", "labels": [], "entities": []}, {"text": "We find that these different models achieve very similar performance in a standard object naming task, though experiments on model combination suggest that they capture complementary aspects of referential meaning.", "labels": [], "entities": [{"text": "object naming task", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8033482531706492}]}, {"text": "Ina zero-shot setup of an object naming task, we find that combining lexical and visual information during training is most beneficial, outperforming variants of cross-modal transfer.", "labels": [], "entities": [{"text": "object naming task", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7943574090798696}, {"text": "cross-modal transfer", "start_pos": 162, "end_pos": 182, "type": "TASK", "confidence": 0.7118786424398422}]}], "datasetContent": [{"text": "This Section reports on experiments in a standard setup of the object naming task where all object names are paired with visual instances of their referents during training.", "labels": [], "entities": [{"text": "object naming task", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.8593714634577433}]}, {"text": "Ina comparable task, i.e. object recognition with known object categories, cross-modal projection or transfer approaches have been reported to perform worse than standard object classification methods (.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.81736820936203}, {"text": "cross-modal projection or transfer", "start_pos": 75, "end_pos": 109, "type": "TASK", "confidence": 0.7493399828672409}]}, {"text": "This seems to suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the real world and that referential meaning should potentially be learned from visual object representation only.", "labels": [], "entities": []}, {"text": "Some previous work on zero-shot image labeling assumes additional components that first identify whether an image should be labelled by a known or unknown word ( ).", "labels": [], "entities": [{"text": "zero-shot image labeling", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.6043895681699117}]}, {"text": "We follow and let the model decide whether to refer to an object by a known or unknown name.", "labels": [], "entities": []}, {"text": "Related to that, distinct evaluation procedures have been used in the literature on zero-shot learning: Testing on full vocabulary A realistic way to test zero-shot learning performance is to consider all words from a given vocabulary during testing, though the testset only contains instances of objects that have been named with a 'zero-shot word' (for which no visual instances were seen during training).", "labels": [], "entities": []}, {"text": "Accuracies in this setup reflect how well the model is able to generalize, i.e. how often it decides to deviate from the words it was trained on, and (implicitly) predicts that the given object requires a \"new\" name.", "labels": [], "entities": []}, {"text": "In case of the (i) hypernym and (ii) singular/plural test set, this accuracy also reflects to what extent the model is able to detect cases where (i) a more general or vague term is needed, where (ii) an unknown singular/plural counterpart of a known object type occurs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9991944432258606}]}, {"text": "Testing on disjoint vocabulary Alternatively, the model's vocabulary can be restricted during testing to zero-shot words only, such that names encountered during training and testing are disjoint, see e.g. (. This setup factors out the generalization problem, and assesses to what extent a model is able to capture the referential meaning of a word that does not have instances in the training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracies in object naming", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9928459525108337}, {"text": "object naming", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9002573490142822}]}, {"text": " Table 2: Object naming acc., combined models", "labels": [], "entities": [{"text": "Object naming", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.757893443107605}]}, {"text": " Table 3: Cosine similarities between word2vec  embeddings of nouns generated in the top k", "labels": [], "entities": []}, {"text": " Table 4: Accuracies in zero-shot object naming on different vocabulary splits", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9448270201683044}, {"text": "zero-shot object naming", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.686223308245341}]}]}