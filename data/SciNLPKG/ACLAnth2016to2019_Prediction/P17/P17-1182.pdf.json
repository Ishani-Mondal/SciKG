{"title": [{"text": "One-Shot Neural Cross-Lingual Transfer for Paradigm Completion", "labels": [], "entities": [{"text": "Neural Cross-Lingual Transfer", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.5931214888890585}, {"text": "Paradigm Completion", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8485960066318512}]}], "abstractContent": [{"text": "We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task.", "labels": [], "entities": [{"text": "paradigm completion", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7466627657413483}]}, {"text": "We use labeled data from a high-resource language to increase performance on a low-resource language.", "labels": [], "entities": []}, {"text": "In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9989247918128967}]}, {"text": "We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge .", "labels": [], "entities": []}], "introductionContent": [{"text": "Low-resource natural language processing remains an open problem for many tasks of interest.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.7525965968767802}]}, {"text": "Furthermore, for most languages in the world, highcost linguistic annotation and resource creation are unlikely to be undertaken in the near future.", "labels": [], "entities": [{"text": "highcost linguistic annotation", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.5327954192956289}]}, {"text": "In the case of morphology, out of the 7000 currently spoken languages, only about 200 have computer-readable annotations) -although morphology is easy to annotate compared to syntax and semantics.", "labels": [], "entities": []}, {"text": "Transfer learning is one solution to this problem: it exploits annotations in a high-resource language to train a system fora low-resource language.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9362368285655975}]}, {"text": "In this work, we present a method for cross-lingual transfer of inflectional morphology using an encoder-decoder recurrent neural network (RNN).", "labels": [], "entities": [{"text": "cross-lingual transfer of inflectional morphology", "start_pos": 38, "end_pos": 87, "type": "TASK", "confidence": 0.8104277610778808}]}, {"text": "This allows for the development of tools for computational morphology with limited annotated data.", "labels": [], "entities": []}, {"text": "In many languages, individual lexical entries maybe realized as distinct inflections of a single lemma depending on the syntactic context.", "labels": [], "entities": []}, {"text": "For example, the 3SgPresInd of the English verbal lemma to bring is brings.", "labels": [], "entities": []}, {"text": "In morphologically rich languages, a lemma can have hundreds of individual forms.", "labels": [], "entities": []}, {"text": "Thus, both generation and analysis of such morphological inflections are active areas of research in NLP and morphological processing has been shown to be a boon to several other down-stream applications, e.g., machine translation (, speech recognition (, parsing (Seeker and C \u00b8 etino\u02d8 glu, 2015), keyword spotting () and word embeddings (), inter alia.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 211, "end_pos": 230, "type": "TASK", "confidence": 0.8449012935161591}, {"text": "speech recognition", "start_pos": 234, "end_pos": 252, "type": "TASK", "confidence": 0.7578117847442627}, {"text": "parsing (Seeker and C \u00b8 etino\u02d8 glu, 2015)", "start_pos": 256, "end_pos": 297, "type": "TASK", "confidence": 0.6567612936099371}, {"text": "keyword spotting", "start_pos": 299, "end_pos": 315, "type": "TASK", "confidence": 0.7484644055366516}]}, {"text": "In this work, we focus on paradigm completion, a form of morphological generation that maps a given lemma to a target inflection, e.g., (bring, Past) \u2192 brought (with Past being the target tag).", "labels": [], "entities": [{"text": "paradigm completion", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7640674412250519}]}, {"text": "RNN sequence-to-sequence models are the state of the art for paradigm completion).", "labels": [], "entities": [{"text": "paradigm completion", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7167586833238602}]}, {"text": "However, these models require a large amount of data to achieve competitive performance; this makes them unsuitable for out-of-thebox application to paradigm completion in the low-resource scenario.", "labels": [], "entities": []}, {"text": "To mitigate this, we consider transfer learning: we train an end-to-end neural system jointly with limited data from a lowresource language and a larger amount of data from a high-resource language.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.9184129238128662}]}, {"text": "This technique allows the model to apply knowledge distilled from the high-resource training data to the low-resource language as needed.", "labels": [], "entities": []}, {"text": "We conduct experiments on 21 language pairs from four language families, emulating a lowresource setting.", "labels": [], "entities": []}, {"text": "Our results demonstrate successful transfer of morphological knowledge.", "labels": [], "entities": []}, {"text": "We show improvements inaccuracy and edit distance of up to 58% (accuracy) and 4.62 (edit distance) over the same model with only in-domain language data on the paradigm completion task.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.873204916715622}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9985241293907166}, {"text": "edit distance", "start_pos": 84, "end_pos": 97, "type": "METRIC", "confidence": 0.8916557729244232}]}, {"text": "We further obtain up to 44% (resp. 14%) improvement inaccuracy for the one-shot (resp. zero-shot) setting, i.e., one (resp. zero) in-domain language sample per target tag.", "labels": [], "entities": []}, {"text": "We also show that the effectiveness of morphological transfer depends on language relatedness, measured by lexical similarity.", "labels": [], "entities": [{"text": "morphological transfer", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.8477157950401306}]}], "datasetContent": [{"text": "We run four experiments on 21 distinct pairings of languages to show the feasibility of morphological transfer and analyze our method.", "labels": [], "entities": [{"text": "morphological transfer", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.7368952929973602}]}, {"text": "We first discuss details common to all experiments.", "labels": [], "entities": []}, {"text": "We keep hyperparameters during all experiments (and for all languages) fixed to the following values.", "labels": [], "entities": []}, {"text": "Encoder and decoder RNNs each have 100 hidden units and the size of all subtag, character and language embeddings is 300.", "labels": [], "entities": []}, {"text": "For training we use ADADELTA) with minibatch size 20.", "labels": [], "entities": [{"text": "ADADELTA", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9827978610992432}]}, {"text": "All models are trained for 300 epochs.", "labels": [], "entities": []}, {"text": "Following, we initialize all weights in the encoder, decoder and the embeddings except for the GRU weights in the decoder to the identity matrix.", "labels": [], "entities": []}, {"text": "Biases are initialized to zero.", "labels": [], "entities": [{"text": "Biases", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.941474974155426}]}, {"text": "Evaluation metrics: (i) 1-best accuracy: the percentage of predictions that match the true answer exactly; (ii) average edit distance between prediction and true answer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.8188861012458801}, {"text": "edit distance", "start_pos": 120, "end_pos": 133, "type": "METRIC", "confidence": 0.9414401054382324}]}, {"text": "The two metrics differ in that accuracy gives no partial credit and incorrect answers maybe drastically different from the annotated form without incurring additional penalty.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9992092251777649}]}, {"text": "In contrast, edit distance gives partial credit for forms that are closer to the true answer.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 13, "end_pos": 26, "type": "METRIC", "confidence": 0.7847069203853607}]}], "tableCaptions": [{"text": " Table 3: Accuracy (acc; the higher the better; indicated by \u2191) and edit distance (ED; the lower the better;  indicated by \u2193) of cross-lingual transfer learning for paradigm completion. The target language is indicated  by \"\u2192\", e.g., it is Spanish for \"\u2192ES\". Sources are indicated in the row \"source\"; \"0\" is the monolingual  case. Except for Estonian, we train on n s = 12,000 source samples and n t \u2208 {50, 200} target samples  (as indicated by the row). There are two baselines in the table. (i) \"0\": no transfer, i.e., we consider only  in-domain data; (ii) \"AR\": the Semitic language Arabic is unrelated to all target languages and functions  as a dummy language that is unlikely to provide relevant information. All languages are denoted using the  official codes (SME=Northern Sami).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988347887992859}, {"text": "edit distance (ED", "start_pos": 68, "end_pos": 85, "type": "METRIC", "confidence": 0.7536008358001709}, {"text": "cross-lingual transfer learning", "start_pos": 129, "end_pos": 160, "type": "TASK", "confidence": 0.7779654463132223}, {"text": "paradigm completion", "start_pos": 165, "end_pos": 184, "type": "TASK", "confidence": 0.7197427153587341}, {"text": "AR", "start_pos": 562, "end_pos": 564, "type": "METRIC", "confidence": 0.9361210465431213}]}, {"text": " Table 4: Results for transfer from pairs of source  languages to ES. Results from single languages are  repeated for comparison.", "labels": [], "entities": []}, {"text": " Table 5: Results for one-shot and zero-shot transfer  learning. Formatting is the same as for Tab. 3. We  still use n s = 12000 source samples. In the one- shot (resp. zero-shot) case, we observe exactly one  form (resp. zero forms) for each tag in the target  language at training time.", "labels": [], "entities": [{"text": "zero-shot transfer  learning", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.765602191289266}]}, {"text": " Table 6: Results for ciphering. \"0\u2192ES\" and \"orig\"  are original results, copied from Tab. 3; \"ciph\" is  the result after the cipher has been applied.", "labels": [], "entities": [{"text": "Tab. 3", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.9414814313252767}]}]}