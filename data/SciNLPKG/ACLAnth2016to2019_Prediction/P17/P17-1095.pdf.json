{"title": [{"text": "Bayesian Modeling of Lexical Resources for Low-Resource Settings", "labels": [], "entities": [{"text": "Bayesian Modeling of Lexical Resources", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7146925747394561}]}], "abstractContent": [{"text": "Lexical resources such as dictionaries and gazetteers are often used as auxiliary data for tasks such as part-of-speech induction and named-entity recognition.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.7325620055198669}, {"text": "named-entity recognition", "start_pos": 134, "end_pos": 158, "type": "TASK", "confidence": 0.745978057384491}]}, {"text": "However, discriminative training with lexical features requires annotated data to reliably estimate the lexical feature weights and may result in overfitting the lexical features at the expense of features which generalize better.", "labels": [], "entities": []}, {"text": "In this paper, we investigate a more robust approach: we stipulate that the lexicon is the result of an assumed generative process.", "labels": [], "entities": []}, {"text": "Practically, this means that we may treat the lexical resources as observations under the proposed generative model.", "labels": [], "entities": []}, {"text": "The lexical resources provide training data for the generative model without requiring separate data to estimate lexical feature weights.", "labels": [], "entities": []}, {"text": "We evaluate the proposed approach in two settings: part-of-speech induction and low-resource named-entity recognition.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.7764489948749542}, {"text": "low-resource named-entity recognition", "start_pos": 80, "end_pos": 117, "type": "TASK", "confidence": 0.6998217304547628}]}], "introductionContent": [{"text": "Dictionaries and gazetteers are useful in many natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.6285633544127146}]}, {"text": "These lexical resources maybe derived from freely available sources (such as Wikidata and Wiktionary) or constructed fora particular domain.", "labels": [], "entities": []}, {"text": "Lexical resources are typically used to complement existing annotations fora given task (Ando and.", "labels": [], "entities": []}, {"text": "In this paper, we focus instead on low-resource settings where task annotations are unavailable or scarce.", "labels": [], "entities": []}, {"text": "Specifically, we use lexical resources to guide part-of-speech induction ( \u00a74) and to bootstrap named-entity recognizers in low-resource languages ( \u00a75).", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.7221508920192719}, {"text": "bootstrap named-entity recognizers", "start_pos": 86, "end_pos": 120, "type": "TASK", "confidence": 0.7068367203076681}]}, {"text": "Given their success, it is perhaps surprising that incorporating gazetteers or dictionaries into discriminative models (e.g. conditional random fields) may sometimes hurt performance.", "labels": [], "entities": []}, {"text": "This phenomena is called weight under-training, in which lexical features-which detect whether a name is listed in the dictionary or gazetteer-are given excessive weight at the expense of other useful features such as spelling features that would generalize to unlisted names (;).", "labels": [], "entities": []}, {"text": "Furthermore, discriminative training with lexical features requires sufficient annotated training data, which poses challenges for the unsupervised and low-resource settings we consider here.", "labels": [], "entities": []}, {"text": "Our observation is that Bayesian modeling provides a principled solution.", "labels": [], "entities": []}, {"text": "The lexicon is itself a dataset that was generated by some process.", "labels": [], "entities": []}, {"text": "Practically, this means that lexicon entries (words or phrases) maybe treated as additional observations.", "labels": [], "entities": []}, {"text": "As a result, these entries provide information about how names are spelled.", "labels": [], "entities": []}, {"text": "The presence of the lexicon therefore now improves training of the spelling features, rather than competing with the spelling features to help explain the labeled corpus.", "labels": [], "entities": []}, {"text": "A downside is that generative models are typically less feature-rich than their globally normalized discriminative counterparts (e.g. conditional random fields).", "labels": [], "entities": []}, {"text": "In designing our approach-the hierarchical sequence memoizer (HSM)-we aim to be reasonably expressive while retaining practically useful inference algorithms.", "labels": [], "entities": []}, {"text": "We propose a Bayesian nonparametric model to serve as a generative distribution responsible for both lexicon and corpus data.", "labels": [], "entities": []}, {"text": "The proposed model memoizes previously used lexical entries (words or phrases) but backs off to a character-level distribution when generating novel types.", "labels": [], "entities": []}, {"text": "We propose an efficient inference algorithm for the proposed model using particle Gibbs sampling ( \u00a73).", "labels": [], "entities": []}, {"text": "Our code is available at https://github.com/noa/bayesner.", "labels": [], "entities": []}], "datasetContent": [{"text": "We follow the experimental procedure described in, and use their released code and data to compare to their best model: a second-order maximum entropy Markov model parametrized with log-linear features (SHMM-ME).", "labels": [], "entities": []}, {"text": "This model uses hand-crafted features designed to distinguish between different parts-of-speech, and it has special handling for rare words.", "labels": [], "entities": []}, {"text": "This approach is surprisingly effective and outperforms alternate approaches such as cross-lingual transfer ().", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.7091964930295944}]}, {"text": "However, it also has limitations, since words that do not appear in the dictionary will be unconstrained, and spurious or incorrect lexical entries may lead to propagation of errors.", "labels": [], "entities": []}, {"text": "The lexicons are taken from the Wiktionary project; their size and coverage are documented by (.", "labels": [], "entities": [{"text": "Wiktionary project", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.8897523880004883}]}, {"text": "We evaluate our model on multi-lingual data released as part of the CoNLL 2007 and CoNLL-X shared tasks.", "labels": [], "entities": [{"text": "CoNLL 2007 and CoNLL-X shared tasks", "start_pos": 68, "end_pos": 103, "type": "DATASET", "confidence": 0.8338428239027659}]}, {"text": "In particular, we use the same set of languages as.", "labels": [], "entities": []}, {"text": "For our method, we impute the parts-of-speech by running particle Gibbs for 100 epochs, where one epoch consists of resampling the states fora each sentence in the corpus.", "labels": [], "entities": []}, {"text": "The final sampler state is then taken as a 1-best tagging of the unlabeled data.", "labels": [], "entities": []}, {"text": "The results are reported in.", "labels": [], "entities": []}, {"text": "We find that our hierarchical sequence memoizer (HSM) matches or exceeds the performance of the baseline (SHMM-ME) for nearly all the tested languages, particularly for morphologically rich languages such as German where the spelling distributions H y may capture regularities.", "labels": [], "entities": []}, {"text": "It is interesting to note that our model performs worse relative to the baseline for English; one possible explanation is that the baseline uses hand-engineered features whereas ours does not, and these features may have been tuned using English data for validation.", "labels": [], "entities": []}, {"text": "Our generative model is supposed to exploit lexicons well.", "labels": [], "entities": []}, {"text": "To see what is lost from using a generative model, we also compared with on standard supervised tagging without any lexicons.", "labels": [], "entities": []}, {"text": "Even here our generative model is very competive, losing only on English and Swedish.", "labels": [], "entities": [{"text": "generative", "start_pos": 14, "end_pos": 24, "type": "TASK", "confidence": 0.9657678008079529}]}, {"text": "In this section we report supervised NER experiments on two low-resource languages: Turkish and Uzbek.", "labels": [], "entities": [{"text": "NER", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9629080295562744}]}, {"text": "We vary both the amount of supervision as well as the size of the lexical resources.", "labels": [], "entities": []}, {"text": "A challenge when evaluating the performance of a model with small amounts of training data is that there maybe high-variance in the results.", "labels": [], "entities": []}, {"text": "In order to have more confidence in our results, we perform bootstrap resampling experiments in which the training set, evaluation set, and lexical resources are randomized across several replications of the same experiment (for each of the data conditions).", "labels": [], "entities": []}, {"text": "We use 10 replications for each of the data conditions reported in, and report both the mean performance and 95% confidence intervals.", "labels": [], "entities": []}, {"text": "We use the Stanford NER system with a standard set of language-independent features ().", "labels": [], "entities": [{"text": "Stanford NER system", "start_pos": 11, "end_pos": 30, "type": "DATASET", "confidence": 0.9070842663447062}]}, {"text": "8 . This model is a conditional random field (CRF) with feature templates which include character n-grams as well as word shape features.", "labels": [], "entities": []}, {"text": "Crucially, we also incorporate lexical features.", "labels": [], "entities": []}, {"text": "The CRF parameters are regularized using an L1 penalty and optimized via Orthant-wise limited-memory quasi-Newton optimization.", "labels": [], "entities": []}, {"text": "For both our proposed method and the discriminative baseline, we use a fixed set of hyperparameters (i.e. we do not use a separate validation set for tuning each data condition).", "labels": [], "entities": []}, {"text": "In order to make a fair comparison to the CRF, we use our sampler for forward inference only, without resampling on the test data.", "labels": [], "entities": []}, {"text": "We show learning curves as a function of supervised training corpus size.", "labels": [], "entities": []}, {"text": "shows that our generative model strongly beats the baseline in this low-data regime.", "labels": [], "entities": [{"text": "generative", "start_pos": 15, "end_pos": 25, "type": "TASK", "confidence": 0.9761359691619873}]}, {"text": "In particular, when there is little annotated training data, our proposed generative model can compensate by exploiting the lexicon, while the discriminative baseline scores terribly.", "labels": [], "entities": []}, {"text": "The performance gap decreases with larger supervised corpora, which is consistent with prior results comparing generative and discriminative training).", "labels": [], "entities": []}, {"text": "In, we show the effect of the lexicon's size: as expected, larger lexicons are better.", "labels": [], "entities": []}, {"text": "The generative approach significantly outperforms the discriminative baseline at any lexicon size, although its advantage drops for smaller lexicons or larger training corpora.", "labels": [], "entities": []}, {"text": "In we found that increasing the pseudocount c consistently decreases performance, so we used c = 1 in our other experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Part-of-speech induction results in multiple languages.", "labels": [], "entities": [{"text": "Part-of-speech induction", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8159249722957611}]}]}