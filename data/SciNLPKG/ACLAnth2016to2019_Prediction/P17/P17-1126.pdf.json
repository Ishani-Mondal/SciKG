{"title": [{"text": "Learning to Generate Market Comments from Stock Prices", "labels": [], "entities": [{"text": "Learning to Generate Market Comments from Stock Prices", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.7581381276249886}]}], "abstractContent": [{"text": "This paper presents a novel encoder-decoder model for automatically generating market comments from stock prices.", "labels": [], "entities": [{"text": "automatically generating market comments from stock prices", "start_pos": 54, "end_pos": 112, "type": "TASK", "confidence": 0.6981041303702763}]}, {"text": "The model first encodes both short-and long-term series of stock prices so that it can mention short-and long-term changes in stock prices.", "labels": [], "entities": []}, {"text": "In the decoding phase, our model can also generate a numerical value by selecting an appropriate arithmetic operation such as subtraction or rounding, and applying it to the input stock prices.", "labels": [], "entities": []}, {"text": "Empirical experiments show that our best model generates market comments at the fluency and the informativeness approaching human-generated reference texts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Various industries such as finance, pharmaceuticals, and telecommunications have been increasingly providing opportunities to treat various types of large-scale numerical time-series data.", "labels": [], "entities": []}, {"text": "Such data are hard for non-specialists to interpret in detail and time-consuming even for specialists to construe.", "labels": [], "entities": []}, {"text": "As a result, there has been a growing interest in automatically generating concise descriptions of such data, i.e., data summarization.", "labels": [], "entities": [{"text": "data summarization", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.6463556438684464}]}, {"text": "This interest in data summarization is encouraged by the recent development of neural network-based text generation methods.", "labels": [], "entities": [{"text": "data summarization", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.6550923585891724}, {"text": "text generation", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.7313725650310516}]}, {"text": "Given an appropriate architecture, a neural network can generate a sentence that is mostly grammatical and semantically reasonable.", "labels": [], "entities": []}, {"text": "In this study, we focus on the task of generating market comments from a time-series of stock prices.", "labels": [], "entities": []}, {"text": "We adopt an encoder-decoder model) and exploit its capability to learn to capture the behavior of the input and generate a description of it.", "labels": [], "entities": []}, {"text": "Although encoderdecoder models can learn to do this, they need to be  provided with an appropriate network-architecture and necessary information.", "labels": [], "entities": []}, {"text": "We use to illustrate the characteristic problems of comment generation for time-series of stock prices.", "labels": [], "entities": [{"text": "comment generation", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7704246640205383}]}, {"text": "The figure shows the Nikkei Stock Average (Nikkei 225, or simply Nikkei), which is a stock market index calculated from 225 selected issues, on some consecutive trading days accompanied by the market comments made at some specific time points in the span.", "labels": [], "entities": [{"text": "Nikkei Stock Average (Nikkei 225", "start_pos": 21, "end_pos": 53, "type": "DATASET", "confidence": 0.8568237523237864}]}, {"text": "The first problem is that market comments do not merely describe the increase and decrease of the price.", "labels": [], "entities": []}, {"text": "They also often describe how the price changes compared with the previous period, such as \"continues to fall\" in (3) of, \"turns to rise\" in (2), and \"rebound\" in (6).", "labels": [], "entities": []}, {"text": "Market comments sometimes describe the change in price compared with the prices in the previous week.", "labels": [], "entities": []}, {"text": "The second problem is that market comments also contain expressions that depend on their delivery time: e.g., \"opens with\" in (1), \"closing price of the morning session\" in (3), and \"beginning of the afternoon session\" in (4).", "labels": [], "entities": []}, {"text": "The third problem is that market comments typically contain numerical values, which often cannot be copied from the input prices.", "labels": [], "entities": []}, {"text": "Such numerical values probably cannot be generated as other words are generated by the standard decoder.", "labels": [], "entities": []}, {"text": "This difficulty can be easily understood as analogous with the difficulty of generating named entities by encoder-decoder models.", "labels": [], "entities": []}, {"text": "To derive such values, the model needs arithmetic operations such as subtraction as in examples and (6) mentioning the difference in price and rounding as in example (5).", "labels": [], "entities": []}, {"text": "To address these problems, we present a novel encoder-decoder model to automatically generate market comments from stock prices.", "labels": [], "entities": []}, {"text": "To address the first problem of capturing various types of change in different time scales, the model first encodes data consisting of both short-and long-term time-series, where a multi-layer perceptron, a recurrent neural network, or a convolutional network is adopted as a basic encoder.", "labels": [], "entities": []}, {"text": "In the decoding phase, we feed our model with the delivery time of the market comment to generate the expressions depending on time of day to address the second problem.", "labels": [], "entities": []}, {"text": "To address the third problem regarding with numerical values mentioned in the generated text, we allow our model to choose an arithmetic operation such as subtraction or rounding instead of generating a word.", "labels": [], "entities": []}, {"text": "The proposed methods are evaluated on the task of generating Japanese market comments on the Nikkei Stock Average.", "labels": [], "entities": [{"text": "Nikkei Stock Average", "start_pos": 93, "end_pos": 113, "type": "DATASET", "confidence": 0.9786717295646667}]}, {"text": "Automatic evaluation with BLEU score) and F-score of time-dependent expressions reveals that our model outperforms a baseline encoder-decoder model significantly.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9833775758743286}, {"text": "F-score", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.998276948928833}]}, {"text": "Furthermore, human assessment and error analysis prove that our best model generates characteristic expressions discussed above almost perfectly, approaching the fluency and the informativeness of human-generated market comments.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the five-minute chart of Nikkei 225 from March 2013 to October 2016 as numerical timeseries data, which were collected from IBI-Square Stocks 1 , and 7,351 descriptions as market comments, which are written in Japanese and provided by Nikkei QUICK News.", "labels": [], "entities": [{"text": "Nikkei 225 from March 2013", "start_pos": 33, "end_pos": 59, "type": "DATASET", "confidence": 0.9684327244758606}, {"text": "IBI-Square Stocks 1", "start_pos": 132, "end_pos": 151, "type": "DATASET", "confidence": 0.9643713434537252}, {"text": "Nikkei QUICK News", "start_pos": 243, "end_pos": 260, "type": "DATASET", "confidence": 0.8949160575866699}]}, {"text": "We divided the dataset into three parts: 5,880 for training, 730 for validation, and 741 for testing.", "labels": [], "entities": []}, {"text": "For a human evaluation, we randomly selected 100 comments and their time-series data included in the test set.", "labels": [], "entities": []}, {"text": "We set N = 62, which is the number of time steps for stock prices for one trading day, and M = 7, which is the number of the time steps for closing prices of the preceding trading days.", "labels": [], "entities": [{"text": "M", "start_pos": 91, "end_pos": 92, "type": "METRIC", "confidence": 0.9941378831863403}]}, {"text": "We used Adam ( for optimization with a learning rate of 0.001 and a mini-batch size of 100.", "labels": [], "entities": []}, {"text": "The dimensions of word embeddings, time embeddings, and hidden states for both the encoder and decoder are set to 128, 64, and 256, respectively.", "labels": [], "entities": []}, {"text": "For CNN, we used a single convolutional layer and set the filter size to 3.", "labels": [], "entities": [{"text": "CNN", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.917648434638977}]}, {"text": "In the experiments, we conducted three types of evaluation: two for automatic evaluation, and one for human evaluation.", "labels": [], "entities": []}, {"text": "For one automatic evaluation, we used BLEU () to measure the matching degree between the market comments written by humans as references and output comments generated by our model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9986635446548462}]}, {"text": "We applied paired bootstrap resampling) fora significance test.", "labels": [], "entities": []}, {"text": "For the other automatic evaluation metric, we calculate F-measures for time-dependent expressions, using market comments written by humans as references, to investigate whether our model can correctly output timedependent expressions such as \"open with\" and describe how the price changes compared with the previous period referring to the series of preceding prices such as \"continual fall\".", "labels": [], "entities": [{"text": "F-measures", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9868702292442322}]}, {"text": "Specifically, we calculate F-measures for 13 expressions shown in.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9734373688697815}]}, {"text": "For the human evaluation, we recruited a specialist in financial engineering as a judge to evaluate the quality of generated market comments.", "labels": [], "entities": []}, {"text": "To evaluate the difference in the quality of generated comments between our models and human, we showed both system-generated and humangenerated market comments together with their   time-series data consisting of x short and x long , without letting the judge know which comment is generated by which method.", "labels": [], "entities": []}, {"text": "We asked the judge to give each market comment two scores: one for informativeness and one for fluency.", "labels": [], "entities": []}, {"text": "Both scores have two levels, 0 or 1, where 1 indicates high informativeness or fluency.", "labels": [], "entities": []}, {"text": "For informativeness, the judge used both generated comments and their input stock prices to rate the comments.", "labels": [], "entities": []}, {"text": "Specifically, if the judge deem that a generated comment describes an important price movement or an outline of the movement properly, such comments are considered to be informative.", "labels": [], "entities": []}, {"text": "For fluency, the judge read only the generated comments and rate them in terms of readability, regardless of their content of the comment.", "labels": [], "entities": []}, {"text": "In addition, since some of the market comments written by humans sometimes include external information such as \"Nikkei opens with a continual fall as yen pressures exporters\", we also asked the judge to ignore the correctness of external information mentioned in comments, for the sake of fairness in comparison, because external information cannot be retrieved from the time-series data.", "labels": [], "entities": []}, {"text": "To assess the effectiveness of the techniques we introduced, we conducted experiments with 11 models.", "labels": [], "entities": []}, {"text": "shows an overview of the models: BLEU scores on the test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9988157749176025}]}, {"text": "Differences between the best model, mlp-enc, and other models are statistically significant at p < 0.05.", "labels": [], "entities": []}, {"text": "We compared three types of models: a baseline, full models (e.g., mlp-enc), and ablated models (e.g., -short).", "labels": [], "entities": []}, {"text": "For example, -short is a model that does not use the short-term time series.", "labels": [], "entities": []}, {"text": "shows the BLEU scores on the test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9980375170707703}]}, {"text": "presents the F-measure of the models for each phrase.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9964737296104431}]}, {"text": "We also present output examples with human-generated market comments (Human) for reference in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: BLEU scores on the test set. Differences  between the best model, mlp-enc, and other models  are statistically significant at p < 0.05.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990857839584351}]}, {"text": " Table 4: Results of human evaluation. Each score  indicates number of market comments judged to  be level-1. External shows number of market com- ments including external information.", "labels": [], "entities": []}]}