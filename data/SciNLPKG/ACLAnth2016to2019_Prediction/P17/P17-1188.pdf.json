{"title": [{"text": "Learning Character-level Compositionality with Visual Features", "labels": [], "entities": [{"text": "Learning Character-level Compositionality", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.5655938784281412}]}], "abstractContent": [{"text": "Previous work has modeled the composi-tionality of words by creating character-level models of meaning, reducing problems of sparsity for rare words.", "labels": [], "entities": []}, {"text": "However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts.", "labels": [], "entities": []}, {"text": "In this paper, we model this effect by creating embeddings for characters based on their visual characteristics , creating an image for the character and running it through a convolutional neural network to produce a visual character embedding.", "labels": [], "entities": []}, {"text": "Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.8444761037826538}]}, {"text": "Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry semantic content, resulting in embeddings that are coherent in visual space.", "labels": [], "entities": []}], "introductionContent": [{"text": "Compositionality-the fact that the meaning of a complex expression is determined by its structure and the meanings of its constituents-is a hallmark of every natural language (.", "labels": [], "entities": []}, {"text": "Recently, neural models have provided a powerful tool for learning how to compose words together into a meaning representation of whole sentences for many downstream tasks.", "labels": [], "entities": []}, {"text": "This is done using models of various levels of sophistication, from simpler bag-of-words) and linear recurrent neural network (RNN) models, to more sophisticated models using tree- structured) or convolutional networks ().", "labels": [], "entities": []}, {"text": "In fact, a growing body of evidence shows that it is essential to look below the word-level and consider compositionality within words themselves.", "labels": [], "entities": []}, {"text": "For example, several works have proposed models that represent words by composing together the characters into a representation of the word itself (.", "labels": [], "entities": []}, {"text": "Additionally, for languages with productive word formation (such as agglutination and compounding), models calculating morphologysensitive word representations have been found effective ().", "labels": [], "entities": []}, {"text": "These models help to learn more robust representations for rare words by exploiting morphological patterns, as opposed to models that operate purely on the lexical level as the atomic units.", "labels": [], "entities": []}, {"text": "For many languages, compositionality stops at the character-level: characters are atomic units of meaning or pronunciation in the language, and no further decomposition can be done.: By-category statistics for the Wikipedia dataset.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 214, "end_pos": 231, "type": "DATASET", "confidence": 0.9545454978942871}]}, {"text": "Note that Food is the abbreviation for \"Food and Culture\" and Religion is the abbreviation for \"Religion and Belief\".", "labels": [], "entities": []}, {"text": "be derived from the sum of its parts, is very much a reality.", "labels": [], "entities": []}, {"text": "Perhaps the most compelling example of compositionality of sub-character units can be found in logographic writing systems such as the Han and Kanji characters used in Chinese and Japanese, respectively.", "labels": [], "entities": []}, {"text": "2 As shown on the left side of, each part of a Chinese character (called a \"radical\") potentially contributes to the meaning (i.e.,) or pronunciation (i.e.,) of the overall character.", "labels": [], "entities": []}, {"text": "This is similar to how English characters combine into the meaning or pronunciation of an English word.", "labels": [], "entities": []}, {"text": "Even in languages with phonemic orthographies, where each character corresponds to a pronunciation instead of a meaning, there are cases where composition occurs.(c) and (d) show the examples of Korean and German, respectively, where morphological inflection can cause single characters to make changes where some but not all of the component parts are shared.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the feasibility of modeling the compositionality of characters in away similar to how humans do: by visually observing the character and using the features of its shape to learn a representation encoding its meaning.", "labels": [], "entities": []}, {"text": "Our method is relatively simple, and generalizable to a wide variety of languages: we first transform each character from its Unicode representation to a rendering of its shape as an image, then calculate a representation of the image using Convolutional Neural Networks (CNNs) ().", "labels": [], "entities": []}, {"text": "These features then serve as inputs to a down-stream processing task and trained in an end-to-end manner, which first calculates a loss function, then back-propagates the loss back to the CNN.", "labels": [], "entities": []}, {"text": "As demonstrated by our motivating examples in, in logographic languages character-level semantic or phonetic similarity is often indicated by visual cues; we conjecture that CNNs can appropriately model these visual patterns.", "labels": [], "entities": []}, {"text": "Consequently, characters with similar visual appearances will be biased to have similar embeddings, allowing our model to handle rare characters effectively, just as character-level models have been effective for rare words.", "labels": [], "entities": []}, {"text": "To evaluate our model's ability to learn representations, particularly for rare characters, we perform experiments on a downstream task of classifying Wikipedia titles for three Asian languages: Chinese, Japanese, and Korean.", "labels": [], "entities": []}, {"text": "We show that our proposed framework outperforms a baseline model that uses standard character embeddings for instances containing rare characters.", "labels": [], "entities": []}, {"text": "A qualitative analysis of the characteristics of the learned embeddings of our model demonstrates that visually similar characters share similar embeddings.", "labels": [], "entities": []}, {"text": "We also show that the learned representations are particularly effective under low-resource scenarios and complementary with standard character embeddings; combining the two representations through three different fusion methods) leads to consistent improvements over the strongest baseline without visual features.", "labels": [], "entities": []}], "datasetContent": [{"text": "As the labels we would like to predict, we use 12 different main categories from the Wikipedia web page: Geography, Sports, Arts, Military, Economics, Transportation, Health Science, Education, Food Culture, Religion and Belief, Agriculture and Electronics.", "labels": [], "entities": []}, {"text": "Wikipedia has a hierarchical structure, where each of these main categories has a number of subcategories, and each subcategory has its own subcategories, etc.", "labels": [], "entities": []}, {"text": "We traverse this hierarchical structure, adding each main category tag to all of its descendants in this subcategory tree structure.", "labels": [], "entities": []}, {"text": "In the case that a particular article is the descendant of multiple main categories, we favor the main category that minimizes the depth of the article in the tree (e.g., if an article is two steps away from Sports and three steps away from Arts, it will receive the \"Sports\" label).", "labels": [], "entities": []}, {"text": "We also perform some rudimentary filtering, removing pages that match the regular expression \".*:.*\", which catches special pages such as \"title:agriculture\".", "labels": [], "entities": []}, {"text": "In this section, we compare our proposed VISUAL model with the baseline LOOKUP model through three different sets of experiments.", "labels": [], "entities": [{"text": "VISUAL", "start_pos": 41, "end_pos": 47, "type": "TASK", "confidence": 0.7788897156715393}, {"text": "LOOKUP", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9578677415847778}]}, {"text": "First, we examine whether our model is capable of classifying text and achieving similar performance as the baseline model.", "labels": [], "entities": []}, {"text": "Next, we examine the hypothesis that our model will outperform the baseline model when dealing with low frequency characters.", "labels": [], "entities": []}, {"text": "Finally, we examine the fusion methods described in Section 4.", "labels": [], "entities": []}, {"text": "The dimension of the embeddings and batch size for both models are set to dc = 128 and B = 400, respectively.", "labels": [], "entities": [{"text": "B", "start_pos": 87, "end_pos": 88, "type": "METRIC", "confidence": 0.9687000513076782}]}, {"text": "We build our proposed model using Torch), and use Adam () with a learning rate \u03b7 = 0.001 for stochastic optimization.", "labels": [], "entities": [{"text": "learning rate \u03b7", "start_pos": 65, "end_pos": 80, "type": "METRIC", "confidence": 0.8671960234642029}]}, {"text": "The length of each instance is cutoff or padded to 10 characters for batch training.", "labels": [], "entities": []}, {"text": "In our second experiment, we consider two smaller training sizes (i.e., 50% and 12.5% of the full training size) indicated by green and red lines in.", "labels": [], "entities": []}, {"text": "We performed this experiment under the hypothesis that because the proposed method was more robust to infrequent characters, the proposed model may perform better in low-resourced scenarios.", "labels": [], "entities": []}, {"text": "If this is the case, the intersection point of the two models will shift right because of the increase of the number of instances with low average character frequency.: Classification results for the LOOKUP / VISUAL of the k lowest frequency instances across four datasets.", "labels": [], "entities": [{"text": "LOOKUP", "start_pos": 200, "end_pos": 206, "type": "METRIC", "confidence": 0.9942976832389832}, {"text": "VISUAL", "start_pos": 209, "end_pos": 215, "type": "METRIC", "confidence": 0.5532269477844238}]}, {"text": "The 100 lowest frequency instances for traditional and simplified Chinese and Korean were both significant (p-value < 0.05).", "labels": [], "entities": []}, {"text": "Those for Japanese were not (p-value = 0.13); likely because there was less variety than Chinese and more data than Korean.", "labels": [], "entities": []}, {"text": "As we can see in, the intersection point for 100% training data lies between the intersection point for 50% training data and 12.5%.", "labels": [], "entities": [{"text": "intersection", "start_pos": 22, "end_pos": 34, "type": "METRIC", "confidence": 0.9617234468460083}]}, {"text": "This disagrees with our hypothesis; this is likely because while the number of low-frequency characters increases, smaller amounts of data also adversely impact the ability of CNN to learn useful visual features, and thus there is not a clear gain nor loss when using the proposed method.", "labels": [], "entities": []}, {"text": "As a more extreme test of the ability of our proposed framework to deal with the unseen char-zh trad zh simp ja  acters in the test set, we use traditional Chinese as our training data and simplified Chinese as our testing data.", "labels": [], "entities": []}, {"text": "The model was able to achieve around 40% classification accuracy when we use the full training set, compared to 55%, which is achieved by the model trained on simplified Chinese.", "labels": [], "entities": [{"text": "classification", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.9135127067565918}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9714997410774231}]}, {"text": "This result demonstrates that the model is able to transfer between similar scripts, similarly to how most Chinese speakers can guess the meaning of the text, even if it is written in the other script.", "labels": [], "entities": []}, {"text": "Results of different fusion methods can be found in Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.9507051408290863}]}, {"text": "5. The results show that late fusion gives the best performance among all the fusion schemes combining the LOOKUP model and the proposed VISUAL model.", "labels": [], "entities": [{"text": "LOOKUP", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.8725815415382385}]}, {"text": "Early fusion achieves small improvements for all languages except Japanese, where it displays a slight drop.", "labels": [], "entities": []}, {"text": "Unsurprisingly, fallback fusion performs better than the LOOKUP model and the VISUAL model alone, since it directly targets the weakness of the LOOKUP model (e.g., rare characters) and replaces the results with the VISUAL model.", "labels": [], "entities": [{"text": "fallback fusion", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.7107614427804947}]}, {"text": "These results show that simple integration, no matter which schemes we use, is beneficial, demonstrating that both methods are capturing complementary information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: By-category statistics for the Wikipedia dataset. Note that Food is the abbreviation for \"Food  and Culture\" and Religion is the abbreviation for \"Religion and Belief\".", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.9475680589675903}]}, {"text": " Table 2: Architecture of the CNN used in the ex- periments. All the convolutional layers have 32  3\u00d73 filters.", "labels": [], "entities": []}, {"text": " Table 3: The classification results of the LOOKUP  / VISUAL models for different percentages of full  training size.", "labels": [], "entities": [{"text": "LOOKUP", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.6817440390586853}]}, {"text": " Table 4: Classification results for the LOOKUP  / VISUAL of the k lowest frequency instances  across four datasets. The 100 lowest frequency in- stances for traditional and simplified Chinese and  Korean were both significant (p-value < 0.05).", "labels": [], "entities": [{"text": "LOOKUP", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9907316565513611}, {"text": "VISUAL", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.7406927943229675}]}, {"text": " Table 5: Experiment results for three different fu- sion methods across 4 datasets. The late fusion  model was better (p-value < 0.001) across four  datasets.", "labels": [], "entities": []}]}