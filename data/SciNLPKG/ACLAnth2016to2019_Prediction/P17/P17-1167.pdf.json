{"title": [{"text": "Search-based Neural Structured Learning for Sequential Question Answering", "labels": [], "entities": [{"text": "Sequential Question Answering", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.7836809158325195}]}], "abstractContent": [{"text": "Recent work in semantic parsing for question answering has focused on long and complicated questions, many of which would seem unnatural if asked in a normal conversation between two humans.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.803406685590744}, {"text": "question answering", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8230692446231842}]}, {"text": "In an effort to explore a conversational QA setting, we present a more realistic task: answering sequences of simple but interrelated questions.", "labels": [], "entities": [{"text": "answering sequences of simple but interrelated questions", "start_pos": 87, "end_pos": 143, "type": "TASK", "confidence": 0.8311283588409424}]}, {"text": "We collect a dataset of 6,066 question sequences that inquire about semi-structured tables from Wikipedia, with 17,553 question-answer pairs in total.", "labels": [], "entities": []}, {"text": "To solve this sequential question answering task, we propose a novel dynamic neural semantic parsing framework trained using a weakly supervised reward-guided search.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7665171523888906}, {"text": "dynamic neural semantic parsing", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.7285862118005753}]}, {"text": "Our model effectively leverages the sequential context to outperform state-of-the-art QA systems that are designed to answer highly complex questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsing, which maps natural language text to meaning representations informal logic, has emerged as a key technical component for building question answering systems.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8509205281734467}, {"text": "question answering", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.8247358500957489}]}, {"text": "Once a natural language question has been mapped to a formal query, its answer can be retrieved by executing the query on a back-end structured database.", "labels": [], "entities": []}, {"text": "One of the main focuses of semantic parsing research is how to address compositionality in language, and complicated questions have been specifically targeted in the design of a recently-released QA dataset.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7463065385818481}, {"text": "QA dataset", "start_pos": 196, "end_pos": 206, "type": "DATASET", "confidence": 0.6818042248487473}]}, {"text": "Take for example the following question: \"of those actresses who won a Tony after 1960, which one took the most amount of years after winning the Tony to * Work done during an internship at Microsoft Research win an Oscar?\"", "labels": [], "entities": []}, {"text": "The corresponding logical form is highly compositional; in order to answer it, many sub-questions must be implicitly answered in the process (e.g., \"who won a Tony after 1960?\").", "labels": [], "entities": [{"text": "who won a Tony after 1960?\")", "start_pos": 149, "end_pos": 177, "type": "TASK", "confidence": 0.6551935076713562}]}, {"text": "While we agree that semantic parsers should be able to answer very complicated questions, in reality these questions are rarely issued by users.", "labels": [], "entities": []}, {"text": "Because users can interact with a QA system repeatedly, there is no need to assume a single-turn QA setting where the exact question intent has to be captured with just one complex question.", "labels": [], "entities": []}, {"text": "The same intent can be more naturally expressed through a sequence of simpler questions, as shown below: 1.", "labels": [], "entities": []}, {"text": "What actresses won a Tony after 1960?", "labels": [], "entities": []}, {"text": "2. Of those, who later won an Oscar?", "labels": [], "entities": []}, {"text": "3. Who had the biggest gap between their two award wins?", "labels": [], "entities": []}, {"text": "Decomposing complicated intents into multiple related but simpler questions is arguably a more effective strategy to explore a topic of interest, and it reduces the cognitive burden on both the person who asks the question and the one who answers it.", "labels": [], "entities": []}, {"text": "In this work, we study semantic parsing for answering sequences of simple related questions.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.780107170343399}, {"text": "answering sequences of simple related questions", "start_pos": 44, "end_pos": 91, "type": "TASK", "confidence": 0.8203300933043162}]}, {"text": "We collect a dataset of question sequences called SequentialQA (SQA; Section 2) by asking crowdsourced workers to decompose complicated questions sampled from the WikiTableQuestions dataset) into multiple easier ones.", "labels": [], "entities": [{"text": "WikiTableQuestions dataset", "start_pos": 163, "end_pos": 189, "type": "DATASET", "confidence": 0.8802327811717987}]}, {"text": "SQA, which contains 6,066 question sequences with 17,553 total question-answer pairs, is to the best of our knowledge the first semantic parsing dataset for sequential question answering.", "labels": [], "entities": [{"text": "SQA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7197047472000122}, {"text": "sequential question answering", "start_pos": 157, "end_pos": 186, "type": "TASK", "confidence": 0.7375640869140625}]}, {"text": "Section 3 describes our novel dynamic neural semantic parsing framework (DynSP), a weakly su-1 For instance, there are only 3.75% questions with more than 15 words in.", "labels": [], "entities": [{"text": "dynamic neural semantic parsing", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.68027663230896}]}, {"text": "2 Studies have shown increased sentence complexity links to longer reading times  pervised structured-output learning approach based on reward-guided search that is designed for solving sequential QA.", "labels": [], "entities": []}, {"text": "We demonstrate in Section 4 that DynSP achieves higher accuracies than existing systems on SQA, and we offer a qualitative analysis of question types that our method answers effectively, as well as those on which it struggles.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9829115867614746}]}], "datasetContent": [{"text": "We collect the SequentialQA (SQA) dataset via crowdsourcing by leveraging WikiTableQuestions (Pasupat and Liang, 2015, henceforth WTQ), which contains highly compositional questions associated with HTML tables from Wikipedia.", "labels": [], "entities": []}, {"text": "Each crowdsourcing task contains along, complex question originally from WTQ as the question intent.", "labels": [], "entities": [{"text": "WTQ", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.9624184966087341}]}, {"text": "The workers are asked to compose a sequence of simpler questions that lead to the final intent; an example of this process is shown in.", "labels": [], "entities": []}, {"text": "To simplify the task for workers, we only use questions from WTQ whose answers are cells in the table, which excludes those involving arithmetic and counting.", "labels": [], "entities": [{"text": "WTQ", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.90822434425354}]}, {"text": "We likewise also restrict the questions our workers can write to those answerable by only table cells.", "labels": [], "entities": []}, {"text": "These restrictions speed the annotation process because workers can just click on the table to answer their question.", "labels": [], "entities": []}, {"text": "They also allow us to collect answer coordinates (row and column in the table) as opposed to answer text, which removes many normalization issues for answer string matching in evaluation.", "labels": [], "entities": [{"text": "answer string matching", "start_pos": 150, "end_pos": 172, "type": "TASK", "confidence": 0.625953863064448}]}, {"text": "Finally, we only use long questions that contain nine or more words as intents; shorter questions tend to be simpler and are thus less amenable to decomposition.", "labels": [], "entities": []}, {"text": "Since the questions in SQA are decomposed from those in WTQ, we compare our method, DynSP, to two existing semantic parsers designed for WTQ: (1) the floating parser (FP) of, and (2) the neural programmer (NP) of.", "labels": [], "entities": []}, {"text": "We describe below each system's configurations in more detail and qualitatively compare and contrast their performance on SQA.", "labels": [], "entities": []}, {"text": "Floating parser: The floating parser) maps questions to logical forms and then executes them on the table to retrieve the answers.", "labels": [], "entities": []}, {"text": "It was designed specifically for the WTQ task (achieving 37.0% accuracy on the WTQ test set) and differs from other semantic parsers by not anchoring predicates to tokens in the question, relying instead on typing constraints to reduce the search space.", "labels": [], "entities": [{"text": "WTQ task", "start_pos": 37, "end_pos": 45, "type": "TASK", "confidence": 0.7296873927116394}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9980995059013367}, {"text": "WTQ test set", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.9659393231074015}]}, {"text": "Using FP as-is results in poor performance on SQA because the system is configured for questions with single answers, while SQA contains many questions with multiple-cell answers.", "labels": [], "entities": []}, {"text": "We address this issue by removing a pruning hyperparameter (tooManyValues) and features that add bias on the denotation size.", "labels": [], "entities": []}, {"text": "Neural programmer: The neural programmer proposed by has shown promising results on WTQ, achieving accuracies on par with those of FP.", "labels": [], "entities": [{"text": "WTQ", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.6840423345565796}]}, {"text": "Similar to our method, NP contains specialized neural modules that perform discrete operations such as argmax and argmin, and it is able to chain together multiple modules to answer a single question.", "labels": [], "entities": []}, {"text": "However, module selection in NP is computed via soft attention (, and information is propagated from one module to the next using a recurrent neural network.", "labels": [], "entities": []}, {"text": "Since module selection is not tied to a pre-defined parse language like DynSP, NP simply runs fora fixed number of recurrent timesteps per question rather than growing a parse until it is complete.", "labels": [], "entities": [{"text": "module selection", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.7425743639469147}]}, {"text": "Comparing the baseline systems: FP and NP exemplify two very different paradigms for designing a semantic parsing system to answer questions using structured data.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 97, "end_pos": 113, "type": "TASK", "confidence": 0.7572093307971954}]}, {"text": "FP is a feature-rich system that aims to output the correct semantic parse (in a logical parse language) fora given question.", "labels": [], "entities": [{"text": "FP", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.760822594165802}]}, {"text": "On the other hand, the end-to-end neural network of NP relies on its modular architectures to output a probability distribution over cells in a table given a question.", "labels": [], "entities": []}, {"text": "While NP can learn more powerful neural matching functions between questions and tables than FP's simpler feature-based matching, NP cannot produce a complete, discrete semantic parse, which means that its actions can only be interpreted coarsely by looking at the order of the modules selected at each timestep.", "labels": [], "entities": []}, {"text": "Furthermore, FP's design theoretically allows it to operate on partial tables indirectly through an API, which is necessary if tables are large and stored in a backend database, while NP requires upfront access to the full tables to facilitate end-to-end model differentiability.", "labels": [], "entities": []}, {"text": "Even though FP and NP are powerful systems designed for the more difficult, compositional questions in WTQ, our method outperforms both systems on SQA when we consider all questions within a sequence independently of each other (a fair comparison), demonstrating the power of our search-based semantic parsing framework.", "labels": [], "entities": [{"text": "search-based semantic parsing", "start_pos": 280, "end_pos": 309, "type": "TASK", "confidence": 0.7209844787915548}]}, {"text": "More interestingly, when we leverage the sequential information by including the subsequent action, our method improves almost 3% in absolute accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.8657984137535095}]}, {"text": "DynSP combines the best parts of both FP and NP.", "labels": [], "entities": [{"text": "DynSP", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7927547097206116}, {"text": "FP", "start_pos": 38, "end_pos": 40, "type": "DATASET", "confidence": 0.5730807185173035}]}, {"text": "Given a question, we try to generate its correct semantic parse in a formal language that can be predefined by the choice of structured data source (e.g., SQL).", "labels": [], "entities": []}, {"text": "However, we push the burden of feature engineering to neural networks as in NP.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7387632727622986}]}, {"text": "Our framework is easier to extend to the sequential setting of SQA than either baseline system, requiring just the additional subsequent action.", "labels": [], "entities": [{"text": "SQA", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.8534064888954163}]}, {"text": "FP's reliance on a hand-designed grammar necessitates extra rules that operate over partial tables from the previous question, which if added would blowup the search space.", "labels": [], "entities": []}, {"text": "Meanwhile, modifying NP to handle sequential QA is non-trivial due to soft module and answer selection; it is not immediately clear how to constrain predictions for one question based on the probability distribution over table cells from the previous question in the sequence.", "labels": [], "entities": []}, {"text": "To more fairly compare DynSP to the baseline systems, we also experiment with a \"concatenated questions\" setting, which allows the baselines to access sequential context.", "labels": [], "entities": []}, {"text": "Here, we treat concatenated question prefixes of a sequence as additional training examples, where a question prefix includes all questions prior to the current question in the sequence.", "labels": [], "entities": []}, {"text": "For example, suppose the question sequence is: 1.", "labels": [], "entities": []}, {"text": "what are all of the teams?", "labels": [], "entities": []}, {"text": "2. of those, which won championships?", "labels": [], "entities": []}, {"text": "For the second question, in addition to the original question-answer pair, we add the concatenated question sequence \"what are all of the teams? of those, which won championships?\" paired with the second question's answer.", "labels": [], "entities": []}, {"text": "We refer to these concatenated question baselines as FP + and NP + .", "labels": [], "entities": [{"text": "FP", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.7089312076568604}]}], "tableCaptions": [{"text": " Table 2: Accuracies of all systems on SQA; the  models in the first half of the table treat questions  independently, while those in the second half con- sider sequential context. Our method outperforms  existing ones both in terms of overall accuracy as  well as sequence accuracy.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9934254884719849}, {"text": "SQA", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8904617428779602}, {"text": "accuracy", "start_pos": 244, "end_pos": 252, "type": "METRIC", "confidence": 0.9942578077316284}, {"text": "accuracy", "start_pos": 274, "end_pos": 282, "type": "METRIC", "confidence": 0.8191071152687073}]}]}