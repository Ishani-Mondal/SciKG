{"title": [{"text": "Learning attention for historical text normalization by learning to pronounce", "labels": [], "entities": [{"text": "historical text normalization", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.6843293309211731}]}], "abstractContent": [{"text": "Automated processing of historical texts often relies on pre-normalization to modern word forms.", "labels": [], "entities": []}, {"text": "Training encoder-decoder architectures to solve such problems typically requires a lot of training data, which is not available for the named task.", "labels": [], "entities": []}, {"text": "We address this problem by using several novel encoder-decoder architectures, including a multi-task learning (MTL) architecture using a grapheme-to-phoneme dictionary as auxiliary data, pushing the state-of-the-art by an absolute 2% increase in performance.", "labels": [], "entities": []}, {"text": "We analyze the induced models across 44 different texts from Early New High German.", "labels": [], "entities": []}, {"text": "Interestingly, we observe that, as previously conjectured, multi-task learning can learn to focus attention during decoding, in ways remarkably similar to recently proposed attention mechanisms.", "labels": [], "entities": []}, {"text": "This, we believe, is an important step toward understanding how MTL works.", "labels": [], "entities": [{"text": "MTL", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.982249915599823}]}], "introductionContent": [{"text": "There is a growing interest in automated processing of historical documents, as evidenced by the growing field of digital humanities and the increasing number of digitally available collections of historical documents.", "labels": [], "entities": []}, {"text": "A common approach to deal with the high amount of variance often found in this type of data is to perform spelling normalization, which is the mapping of historical spelling variants to standardized/modernized forms (e.g. vnd \u2192 und 'and').", "labels": [], "entities": [{"text": "spelling normalization", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7858805358409882}]}, {"text": "Training data for supervised learning of historical text normalization is typically scarce, making it a challenging task for neural architectures, which typically require large amounts of labeled data.", "labels": [], "entities": [{"text": "historical text normalization", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.583943784236908}]}, {"text": "Nevertheless, we explore framing the spelling normalization task as a character-based sequence-to-sequence transduction problem, and use encoder-decoder recurrent neural networks (RNNs) to induce our transduction models.", "labels": [], "entities": [{"text": "spelling normalization task", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.888819952805837}]}, {"text": "This is similar to models that have been proposed for neural machine translation (e.g.,), so essentially, our approach could also be considered a specific case of character-based neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.6755268971125284}, {"text": "character-based neural machine translation", "start_pos": 163, "end_pos": 205, "type": "TASK", "confidence": 0.631254568696022}]}, {"text": "By basing our model on individual characters as input, we keep the vocabulary size small, which in turn reduces the model's complexity and the amount of data required to train it effectively.", "labels": [], "entities": []}, {"text": "Using an encoder-decoder architecture removes the need for an explicit character alignment between historical and modern wordforms.", "labels": [], "entities": []}, {"text": "Furthermore, we explore using an auxiliary task for which data is more readily available, namely grapheme-tophoneme mapping (word pronunciation), to regularize the induction of the normalization models.", "labels": [], "entities": [{"text": "word pronunciation", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.6624618023633957}]}, {"text": "We propose several architectures, including multi-task learning architectures taking advantage of the auxiliary data, and evaluate them across 44 small datasets from Early New High German.", "labels": [], "entities": [{"text": "Early New High German", "start_pos": 166, "end_pos": 187, "type": "DATASET", "confidence": 0.8877227157354355}]}, {"text": "Contributions Our contributions are as follows: \u2022 We are, to the best of our knowledge, the first to propose and evaluate encoder-decoder architectures for historical text normalization.", "labels": [], "entities": [{"text": "historical text normalization", "start_pos": 156, "end_pos": 185, "type": "TASK", "confidence": 0.6298104723294576}]}, {"text": "\u2022 We evaluate several such architectures across 44 datasets of Early New High German.", "labels": [], "entities": [{"text": "Early New High German", "start_pos": 63, "end_pos": 84, "type": "DATASET", "confidence": 0.917784720659256}]}, {"text": "\u2022 We show that such architectures benefit from bidirectional encoding, beam search, and attention.", "labels": [], "entities": [{"text": "beam search", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.8087427914142609}]}, {"text": "\u2022 We also show that MTL with pronunciation as an auxiliary task improves the performance of architectures without attention.", "labels": [], "entities": [{"text": "MTL", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9723272919654846}]}, {"text": "\u2022 We analyze the above architectures and show that the MTL architecture learns attention from the auxiliary task, making the attention mechanism largely redundant.", "labels": [], "entities": []}, {"text": "\u2022 We make our implementation publicly available at https://bitbucket.org/ mbollmann/acl2017.", "labels": [], "entities": []}, {"text": "In sum, we both push the state-of-the-art in historical text normalization and present an analysis that, we believe, brings us a step further in understanding the benefits of multi-task learning.", "labels": [], "entities": [{"text": "historical text normalization", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.6312330464522043}]}], "datasetContent": [{"text": "Normalization For the normalization task, we use a total of 44 texts from the Anselm corpus) of Early New High German.", "labels": [], "entities": [{"text": "normalization", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.967930018901825}, {"text": "Anselm corpus) of Early New High German", "start_pos": 78, "end_pos": 117, "type": "DATASET", "confidence": 0.8746702075004578}]}, {"text": "The corpus is a collection of manuscripts and prints of the same core text, a religious treatise.", "labels": [], "entities": []}, {"text": "Although the texts are semi-parallel and share some vocabulary, they were written in different time periods (between the 14th and 16th century) as well as different dialectal regions, and show quite diverse spelling characteristics.", "labels": [], "entities": []}, {"text": "For example, the modern German word Frau 'woman' can be spelled as fraw/vraw (Me), frawe (N2), frauwe (St), fra\u00fcwe (B2), frow (Stu), vrowe (Ka), vorwe (Sa), or vrouwe (B), among others.", "labels": [], "entities": []}, {"text": "All texts in the Anselm corpus are manually annotated with gold-standard normalizations following guidelines described in.", "labels": [], "entities": [{"text": "Anselm corpus", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.9293095171451569}]}, {"text": "For our experiments, we excluded texts from the corpus that are shorter than 4,000 tokens, as well as a few for which annotations were not yet available at the time of writing (mostly Low German and Dutch versions).", "labels": [], "entities": []}, {"text": "Nonetheless, the remaining 44 texts are still quite short for machine-learning standards, ranging from about 4,200 to 13,200 tokens, with an average length of 7,350 tokens.", "labels": [], "entities": []}, {"text": "For all texts, we removed tokens that consisted solely of punctuation characters.", "labels": [], "entities": []}, {"text": "We also lowercase all characters, since it helps keep the size of the vocabulary low, and uppercasing of words is usually not very consistent in historical texts.", "labels": [], "entities": []}, {"text": "Tokenization was not an issue for pre-processing these texts, since modern token boundaries have already been marked by the transcribers.", "labels": [], "entities": [{"text": "Tokenization", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9061099290847778}]}, {"text": "1 https://www.linguistics.rub.de/ anselm/ 2 We refer to individual texts using the same internal IDs that are found in the Anselm corpus (cf. the website).", "labels": [], "entities": [{"text": "Anselm corpus", "start_pos": 123, "end_pos": 136, "type": "DATASET", "confidence": 0.8945430219173431}]}, {"text": "Grapheme-to-phoneme mappings We use learning to pronounce as our auxiliary task.", "labels": [], "entities": [{"text": "Grapheme-to-phoneme mappings", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7406929135322571}]}, {"text": "This task consists of learning mappings from sequences of graphemes to the corresponding sequences of phonemes.", "labels": [], "entities": []}, {"text": "We use the German part of the CELEX lexical database (, particularly the database of phonetic transcriptions of German wordforms.", "labels": [], "entities": [{"text": "CELEX lexical database", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.9558572570482889}]}, {"text": "The database contains a total of 365,530 wordforms with transcriptions in DISC format, which assigns one character to each distinct phonological segment (including affricates and diphthongs).", "labels": [], "entities": []}, {"text": "For example, the word Jungfrau 'virgin' is represented as 'jUN-frB.", "labels": [], "entities": [{"text": "Jungfrau", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.9386876225471497}]}, {"text": "We split up each text into three parts, using 1,000 tokens each fora test set and a development set (that is not currently used), and the remainder of the text (between 2,000 and 11,000 tokens) for training.", "labels": [], "entities": []}, {"text": "We then train and evaluate on each of the 43 texts (excluding the B text that was used for hyper-parameter tuning) individually.", "labels": [], "entities": []}, {"text": "Baselines We compare our architectures to several competitive baselines.", "labels": [], "entities": []}, {"text": "Our first baseline is an averaged perceptron model trained to predict output character n-grams for each input character, after using Levenshtein alignment with generated segment distances (: Average word accuracy across 43 texts from the Anselm dataset, evaluated on the first 1,000 tokens of each text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 204, "end_pos": 212, "type": "METRIC", "confidence": 0.5976988077163696}, {"text": "Anselm dataset", "start_pos": 238, "end_pos": 252, "type": "DATASET", "confidence": 0.9126594960689545}]}, {"text": "Evaluation on the base encoder-decoder model (Sec. 3.1) with greedy search, beam search (k = 5) and/or lexical filtering (Sec. 3.3), with attentional decoder (Sec.", "labels": [], "entities": []}, {"text": "3.4), and the multi-task learning (MTL) model using grapheme-to-phoneme mappings (Sec. 3.5).", "labels": [], "entities": [{"text": "multi-task learning (MTL)", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.6652933359146118}]}, {"text": "deep bi-LSTM sequential tagger, following Bollmann and S\u00f8gaard (2016).", "labels": [], "entities": []}, {"text": "We evaluate this tagger using both standard and multi-task learning.", "labels": [], "entities": []}, {"text": "Finally, we compare our model to the rule-based and Levenshtein-based algorithms provided by the Norma tool (Bollmann, 2012).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average word accuracy across 43 texts from the Anselm dataset, evaluated on the first 1,000 to- kens of each text. Evaluation on the base encoder-decoder model (Sec. 3.1) with greedy search, beam  search (k = 5) and/or lexical filtering (Sec. 3.3), with attentional decoder (Sec. 3.4), and the multi-task  learning (MTL) model using grapheme-to-phoneme mappings (Sec. 3.5).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9362109899520874}, {"text": "Anselm dataset", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.9711528718471527}]}, {"text": " Table 3: Word accuracy on the Anselm dataset, evaluated on the first 1,000 tokens, using the baseline  models (cf. Sec. 4): the Norma tool", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.954532265663147}, {"text": "Anselm dataset", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.940213143825531}]}, {"text": " Table 4: Word accuracy on the Anselm dataset, evaluated on the first 1,000 tokens, using our base  encoder-decoder model (Sec. 3) and the multi-task model. G = greedy decoding, B = beam-search de- coding (with beam size 5), F = lexical filter, A = attentional model. Best results (also taking into account  the baseline results from", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9568229913711548}, {"text": "Anselm dataset", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.9561079740524292}]}]}