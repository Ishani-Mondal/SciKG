{"title": [{"text": "A* CCG Parsing with a Supertag and Dependency Factored Model", "labels": [], "entities": [{"text": "A", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9339778423309326}, {"text": "Parsing", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.820751965045929}]}], "abstractContent": [{"text": "We propose anew A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs.", "labels": [], "entities": [{"text": "A* CCG parsing", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.5382557883858681}]}, {"text": "Our factored model allows the precomputation of all probabilities and runs very efficiently, while mod-eling sentence structures explicitly via dependencies.", "labels": [], "entities": []}, {"text": "Our model achieves the state-of-the-art results on English and Japanese CCG parsing.", "labels": [], "entities": [{"text": "English and Japanese CCG parsing", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.5509809076786041}]}], "introductionContent": [{"text": "Supertagging in lexicalized grammar parsing is known as almost parsing, in that each supertag is syntactically informative and most ambiguities are resolved once a correct supertag is assigned to every word.", "labels": [], "entities": [{"text": "Supertagging in lexicalized grammar parsing", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7065365672111511}]}, {"text": "Recently this property is effectively exploited in A* Combinatory Categorial Grammar (CCG;) parsing (, in which the probability of a CCG tree yon a sentence x of length N is the product of the probabilities of supertags (categories) c i (locally factored model): By not modeling every combinatory rule in a derivation, this formulation enables us to employ efficient A* search (see Section 2), which finds the most probable supertag sequence that can build a well-formed CCG tree.", "labels": [], "entities": [{"text": "A* Combinatory Categorial Grammar (CCG;) parsing", "start_pos": 51, "end_pos": 99, "type": "TASK", "confidence": 0.7049110730489095}]}, {"text": "Although much ambiguity is resolved with this supertagging, some ambiguity still remains.: CCG trees that are equally likely under Eq.", "labels": [], "entities": []}, {"text": "1. Our model resolves this ambiguity by modeling the head of every word (dependencies).", "labels": [], "entities": []}, {"text": "parses are derived from the same supertags.s approach to this problem is resorting to some deterministic rule.", "labels": [], "entities": []}, {"text": "For example,  employ the attach low heuristics, which is motivated by the right-branching tendency of English, and always prioritizes (b) for this type of ambiguity.", "labels": [], "entities": []}, {"text": "Though for English it empirically works well, an obvious limitation is that it does not always derive the correct parse; consider a phrase \"a house in Paris with a garden\", for which the correct parse has the structure corresponding to (a) instead.", "labels": [], "entities": []}, {"text": "In this paper, we provide away to resolve these remaining ambiguities under the locally factored model, by explicitly modeling bilexical dependencies as shown in.", "labels": [], "entities": []}, {"text": "Our joint model is still locally factored so that an efficient A* search can be applied.", "labels": [], "entities": []}, {"text": "The key idea is to predict the head of every word independently as in Eq.", "labels": [], "entities": []}, {"text": "1 with a strong unigram model, for which we utilize the scoring model in the recent successful graph-based dependency parsing on LSTMs.", "labels": [], "entities": [{"text": "graph-based dependency parsing", "start_pos": 95, "end_pos": 125, "type": "TASK", "confidence": 0.676224927107493}]}, {"text": "Specif-ically, we extend the bi-directional LSTM (bi-LSTM) architecture of  predicting the supertag of a word to predict the head of it at the same time with a bilinear transformation.", "labels": [], "entities": []}, {"text": "The importance of modeling structures beyond supertags is demonstrated in the performance gain in , which adds a recursive component to the model of Eq.", "labels": [], "entities": []}, {"text": "1. Unfortunately, this formulation loses the efficiency of the original one since it needs to compute a recursive neural network every time it searches fora new node.", "labels": [], "entities": []}, {"text": "Our model does not resort to the recursive networks while modeling tree structures via dependencies.", "labels": [], "entities": []}, {"text": "We also extend the tri-training method of  to learn our model with dependencies from unlabeled data.", "labels": [], "entities": []}, {"text": "On English CCGbank test data, our model with this technique achieves 88.8% and 94.0% in terms of labeled and unlabeled F1, which mark the best scores so far.", "labels": [], "entities": [{"text": "English CCGbank test data", "start_pos": 3, "end_pos": 28, "type": "DATASET", "confidence": 0.93558070063591}, {"text": "F1", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.9781967997550964}]}, {"text": "Besides English, we provide experiments on Japanese CCG parsing.", "labels": [], "entities": [{"text": "Japanese CCG parsing", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.5518943170706431}]}, {"text": "Japanese employs freer word order dominated by the case markers and a deterministic rule such as the attach low method may notwork well.", "labels": [], "entities": []}, {"text": "We show that this is actually the case; our method outperforms the simple application of  in a large margin, 10.0 points in terms of clause dependency accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.944856584072113}]}], "datasetContent": [{"text": "We perform experiments on English and Japanese CCGbanks.", "labels": [], "entities": []}, {"text": "We follow the standard data splits and use Sections 02-21 for training, Section 00 for development, and Section 23 for final evaluation.", "labels": [], "entities": []}, {"text": "We report labeled and unlabeled F1 of the extracted CCG semantic dependencies obtained using generate program supplied with C&C parser.", "labels": [], "entities": [{"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.954437255859375}]}, {"text": "For our models, we adopt the pruning strategies in and allow at most 50 categories per word, use a variable-width beam with \u03b2 = 0.00001, and utilize a tag dictionary, which maps frequent words to the possible supertags . Unless otherwise stated, we only allow normal form parses, choosing the same subset of the constraints as.", "labels": [], "entities": []}, {"text": "We use as word representation the concatenation of word vectors initialized to GloVe 8, and randomly initialized prefix and suffix vectors of the length 1 to 4, which is inspired by . All affixes appearing less than two times in the training data are mapped to \"UNK\".", "labels": [], "entities": [{"text": "UNK", "start_pos": 262, "end_pos": 265, "type": "DATASET", "confidence": 0.8794030547142029}]}, {"text": "Other model configurations are: 4-layer biLSTMs with left and right 300-dimensional LSTMs, 1-layer 100-dimensional MLPs with ELU non-linearity) for all M LP dep child , M LP dep head , M LP tag child and M LP tag head , and the Adam optimizer with \u03b2 1 = 0.9, \u03b2 2 = 0.9, L2 norm (1e \u22126 ), and learning rate decay with the ratio 0.75 for every 2,500 iteration starting from 2e \u22123 , which is shown to be effective for training the biaffine parser).", "labels": [], "entities": [{"text": "learning rate decay", "start_pos": 292, "end_pos": 311, "type": "METRIC", "confidence": 0.9066054622332255}]}, {"text": "We follow the default train/dev/test splits of Japanese CCGbank (.", "labels": [], "entities": [{"text": "Japanese CCGbank", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.823112428188324}]}, {"text": "For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg 9, and our implementation of the supertag-factored model using bi-LSTMs.", "labels": [], "entities": []}, {"text": "For Japanese, we use as word representation the concatenation of word vectors initialized to Japanese Wikipedia Entity Vector 10 , and 100-dimensional vectors computed from randomly initialized 50-dimensional character embeddings through convolution).", "labels": [], "entities": []}, {"text": "We do not use affix vectors as affixes are less informative in Japanese.", "labels": [], "entities": []}, {"text": "All characters appearing less than two times are mapped to \"UNK\".", "labels": [], "entities": [{"text": "UNK", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.7077073454856873}]}, {"text": "We use the same parameter settings as English for bi-LSTMs, MLPs, and optimization.", "labels": [], "entities": []}, {"text": "One issue in Japanese experiments is evaluation.", "labels": [], "entities": []}, {"text": "The Japanese CCGbank is encoded in a different format than the English bank, and no standalone script for extracting semantic dependencies is available yet.", "labels": [], "entities": [{"text": "Japanese CCGbank", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8506287336349487}]}, {"text": "For this reason, we evaluate the parser outputs by converting them to bunsetsu   dependencies, the syntactic representation ordinary used in Japanese NLP ().", "labels": [], "entities": []}, {"text": "Given a CCG tree, we obtain this by first segment a sentence into bunsetsu (chunks) using CaboCha and extract dependencies that cross a bunsetsu boundary after obtaining the word-level, head final dependencies as in.", "labels": [], "entities": []}, {"text": "For example, the sentence in is segmented as \"Boku wa | eigo wo | hanashi tai\", from which we extract two dependencies (Boku wa) \u2190 (hanashi tai) and (eigo wo) \u2190 (hanashi tai).", "labels": [], "entities": [{"text": "Boku", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9566256403923035}]}, {"text": "We perform this conversion for both gold and output CCG trees and calculate the (unlabeled) attachment accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.898579478263855}]}, {"text": "Though this is imperfect, it can detect important parse errors such as attachment errors and thus can be a good proxy for the performance as a CCG parser.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parsing results (F1) on English develop- ment set. \"w/o dep\" means that the model discards  dependency components at prediction.", "labels": [], "entities": [{"text": "F1", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.8180614113807678}]}, {"text": " Table 2: Parsing results (F1) on English develop- ment set when excluding the normal form con- straints. # violations is the number of combina- tions violating the constraints on the outputs.", "labels": [], "entities": [{"text": "F1", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9244469404220581}, {"text": "violations", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.766798198223114}]}, {"text": " Table 3: Parsing results (F1) on English test  set (Section 23).", "labels": [], "entities": [{"text": "F1", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9319830536842346}, {"text": "English test  set", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.9543296694755554}]}, {"text": " Table 4: Results of the efficiency experiment,  where each number is the number of sentences  processed per second. We compare our proposed  parser against neuralccg and our reimplemen- tation of EasySRL.", "labels": [], "entities": [{"text": "EasySRL", "start_pos": 197, "end_pos": 204, "type": "DATASET", "confidence": 0.9377657175064087}]}, {"text": " Table 5: Results of Japanese CCGbank.", "labels": [], "entities": [{"text": "Japanese CCGbank", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.908227950334549}]}]}