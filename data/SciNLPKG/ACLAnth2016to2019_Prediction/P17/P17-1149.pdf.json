{"title": [{"text": "Bridging Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding", "labels": [], "entities": []}], "abstractContent": [{"text": "Integrating text and knowledge into a unified semantic space has attracted significant research interests recently.", "labels": [], "entities": []}, {"text": "However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities.", "labels": [], "entities": []}, {"text": "In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base.", "labels": [], "entities": []}, {"text": "In addition, we further design an efficient language model based approach to disam-biguate each mention to a specific sense.", "labels": [], "entities": []}, {"text": "In experiments, both qualitative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings.", "labels": [], "entities": []}, {"text": "Using entity linking as a study case, we apply our disambigua-tion method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve state-of-the-art performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "Jointly learning text and knowledge representations in a unified vector space greatly benefits many Natural Language Processing (NLP) tasks, such as knowledge graph completion (;, relation extraction ( , word sense disambiguation (, entity classification () and linking (.", "labels": [], "entities": [{"text": "knowledge graph completion", "start_pos": 149, "end_pos": 175, "type": "TASK", "confidence": 0.6185499827067057}, {"text": "relation extraction", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.7841878235340118}, {"text": "word sense disambiguation", "start_pos": 204, "end_pos": 229, "type": "TASK", "confidence": 0.6669475336869558}, {"text": "entity classification", "start_pos": 233, "end_pos": 254, "type": "TASK", "confidence": 0.7143953591585159}]}, {"text": "Existing work can be roughly divided into two categories.", "labels": [], "entities": []}, {"text": "One is encoding words and entities into a unified vector space using Deep Neural * Corresponding author.", "labels": [], "entities": []}, {"text": "These methods suffer from the problems of expensive training and great limitations on the size of word and entity vocabulary.", "labels": [], "entities": []}, {"text": "The other is to learn word and entity embeddings separately, and then align similar words and entities into a common space with the help of Wikipedia hyperlinks, so that they share similar representations (.", "labels": [], "entities": []}, {"text": "However, there are two major problems arising from directly integrating word and entity embeddings into a unified semantic space.", "labels": [], "entities": []}, {"text": "First, mention phrases are highly ambiguous and can refer to multiple entities in the common space.", "labels": [], "entities": []}, {"text": "As shown in, the same mention independence day (m 1 ) can either refer to a holiday: Independence Day (US) or a film: Independence Day (film).", "labels": [], "entities": [{"text": "Independence Day (US)", "start_pos": 85, "end_pos": 106, "type": "DATASET", "confidence": 0.8550094366073608}, {"text": "Independence Day (film)", "start_pos": 118, "end_pos": 141, "type": "DATASET", "confidence": 0.9004825830459595}]}, {"text": "Second, an entity often has various aliases when mentioned in various contexts, which implies a much larger size of mention vocabulary compared with entities.", "labels": [], "entities": []}, {"text": "For example, in, the documents d 2 and d 3 describes the same entity Independence Day (US) (e 2 ) with distinct mentions: independence day and July 4th.", "labels": [], "entities": [{"text": "Independence Day (US)", "start_pos": 69, "end_pos": 90, "type": "DATASET", "confidence": 0.8868306159973145}]}, {"text": "We observe tens of millions of mentions referring to 5 millions of entities in Wikipedia.", "labels": [], "entities": []}, {"text": "To address these issues, we propose to learn multiple embeddings for mentions inspired by the Word Sense Disambiguation (WSD) task.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD) task", "start_pos": 94, "end_pos": 130, "type": "TASK", "confidence": 0.7783067779881614}]}, {"text": "The basic idea behind it is to consider entities in KBs that can provide a meaning repository of mentions (i.e. words or phrases) in texts.", "labels": [], "entities": []}, {"text": "That is, each mention has one or multiple meanings, namely mention senses, and each sense corresponds to an entity.", "labels": [], "entities": []}, {"text": "Furthermore, we assume that different mentions referring to the same entity express the same meaning and share a common mention sense embedding, which largely reduces the size of mention vocabulary to be learned.", "labels": [], "entities": []}, {"text": "For example, the mentions Independence Day ind 2 and July 4th ind 3 have a common mention sense embedding during training since they refer to the same holiday.", "labels": [], "entities": [{"text": "Independence Day ind 2", "start_pos": 26, "end_pos": 48, "type": "DATASET", "confidence": 0.9378572851419449}]}, {"text": "Thus, text and knowledge are bridged via mention sense.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel MultiPrototype Mention Embedding (MPME) model, which jointly learns the representations of words, entities, and mentions at sense level.", "labels": [], "entities": []}, {"text": "Different mention senses are distinguished by taking advantage of both textual context information and knowledge of reference entities.", "labels": [], "entities": []}, {"text": "Following the frameworks in (, we use separate models to learn the representations for words, entities and mentions, and further align them by a unified optimization objective.", "labels": [], "entities": []}, {"text": "Extending from skip-gram model and CBOW model, our model can be trained efficiently (,b) from a large scale corpus.", "labels": [], "entities": []}, {"text": "In addition, we also design a language model based approach to determine the sense for each mention in a document based on multi-prototype mention embeddings.", "labels": [], "entities": []}, {"text": "For evaluation, we first provide qualitative analysis to verify the effectiveness of MPME to bridge text and knowledge representations at the sense level.", "labels": [], "entities": []}, {"text": "Then, separate tasks for words and entities show improvements by using our word, entity and mention representations.", "labels": [], "entities": []}, {"text": "Finally, using entity linking as a case study, experimental results on the benchmark dataset demonstrate the effectiveness of our embedding model as well as the disambiguation method.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.7188673764467239}]}], "datasetContent": [{"text": "Setup We choose Wikipedia, the March 2016 dump, as training corpus, which contains nearly 75 millions of anchors, 180 millions of edges among entities and 1.8 billions of tokens after preprocessing.", "labels": [], "entities": []}, {"text": "We then train MPME 2 for 1.5 millions of words, 5 millions of entities and 1.7 millions of mentions.", "labels": [], "entities": []}, {"text": "The entire training process in 10 iterations costs nearly 8 hours on the server with 64 core CPU and 188GB memory.", "labels": [], "entities": []}, {"text": "We use the default settings in word2vec 3 , and set our embedding dimension as 200 and context window size as 5.", "labels": [], "entities": []}, {"text": "For each positive example, we sample 5 negative examples 4 . Baseline Methods As far as we know, this is the first work to deal with mention ambiguity in the integration of text and knowledge representations, so there is no exact baselines for comparison.", "labels": [], "entities": []}, {"text": "We use the method in () as a baseline, marked as ALIGN 5 , because (1) this is the most similar work that directly aligns word and entity embeddings.", "labels": [], "entities": [{"text": "ALIGN", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.8959859013557434}]}, {"text": "(2) it achieves the state-of-the-art performance in entity linking task.", "labels": [], "entities": []}, {"text": "To investigate the effect of multi-prototype, we degrade our method to single-prototype as another baseline, which means to use one sense to represent all mentions with the same phrase, namely Single-Prototype Mention Embedding (SPME).", "labels": [], "entities": []}, {"text": "For example, SPME only learns one unique sense vector for Independence Day whatever it denotes a holiday or a film.", "labels": [], "entities": [{"text": "SPME", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.968394935131073}]}], "tableCaptions": [{"text": " Table 2: Entity Relatedness.  NDCG  MAP  @1  @5  @10  ALIGN  0.416 0.432 0.472 0.410  Entity2vec 0.593 0.595 0.636 0.566  SPME  0.593 0.594 0.636 0.566  MPME  0.613 0.613 0.654 0.582", "labels": [], "entities": [{"text": "Entity Relatedness", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8153918385505676}, {"text": "NDCG", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.8043083548545837}, {"text": "MAP", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.4145215153694153}, {"text": "ALIGN", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9572407603263855}]}, {"text": " Table 3: Word Analogical Reasoning.", "labels": [], "entities": [{"text": "Word Analogical Reasoning", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6733619769414266}]}, {"text": " Table 4: Performance of Supervised Method  ALIGN SPME MPME  Micro P@1  0.828  0.820  0.851  Macro P@1  0.862  0.844  0.881", "labels": [], "entities": [{"text": "ALIGN SPME MPME  Micro P@1  0.828  0.820  0.851  Macro P@1  0.862  0.844  0.881", "start_pos": 44, "end_pos": 123, "type": "METRIC", "confidence": 0.7722601873033187}]}, {"text": " Table 5: Performance of Unsupervised Methods", "labels": [], "entities": []}]}