{"title": [{"text": "Disfluency Detection using a Noisy Channel Model and a Deep Neural Language Model", "labels": [], "entities": [{"text": "Disfluency Detection", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9112105369567871}]}], "abstractContent": [{"text": "This paper presents a model for disflu-ency detection in spontaneous speech transcripts called LSTM Noisy Channel Model.", "labels": [], "entities": [{"text": "disflu-ency detection", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.7201686501502991}]}, {"text": "The model uses a Noisy Channel Model (NCM) to generate n-best candidate dis-fluency analyses and a Long Short-Term Memory (LSTM) language model to score the underlying fluent sentences of each analysis.", "labels": [], "entities": []}, {"text": "The LSTM language model scores, along with other features, are used in a MaxEnt reranker to identify the most plausible analysis.", "labels": [], "entities": [{"text": "MaxEnt reranker", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.9001914262771606}]}, {"text": "We show that using an LSTM language model in the reranking process of noisy channel disfluency model improves the state-of-the-art in disfluency detection.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 134, "end_pos": 154, "type": "TASK", "confidence": 0.9197263121604919}]}], "introductionContent": [{"text": "Disfluency is a characteristic of spontaneous speech which is not present in written text.", "labels": [], "entities": [{"text": "Disfluency", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9281643033027649}]}, {"text": "Disfluencies are informally defined as interruptions in the normal flow of speech that occur in different forms, including false starts, corrections, repetitions and filled pauses.", "labels": [], "entities": []}, {"text": "According to definition, the basic pattern of speech disfluencies contains three parts: reparandum 1 , interregnum and repair.", "labels": [], "entities": [{"text": "speech disfluencies", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7493204474449158}, {"text": "interregnum", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.9602867364883423}]}, {"text": "Example 1 illustrates a disfluent structure, where the reparandum to Boston is the part of the utterance that is replaced, the interregnum uh, I mean is an optional part of a disfluent structure that consists of a filled pause uh and a discourse marker I mean and the repair to Denver replaces the reparandum.", "labels": [], "entities": []}, {"text": "The fluent version of Example 1 is obtained by deleting Reparandum is sometimes called edit.", "labels": [], "entities": [{"text": "Reparandum", "start_pos": 56, "end_pos": 66, "type": "DATASET", "confidence": 0.77605140209198}]}, {"text": "While disfluency rate varies with the context, age and gender of speaker, reported disfluencies once in every 17 words.", "labels": [], "entities": []}, {"text": "Such frequency is high enough to reduce the readability of speech transcripts.", "labels": [], "entities": [{"text": "frequency", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9640020728111267}]}, {"text": "Moreover, disfluencies pose a major challenge to natural language processing tasks, such as dialogue systems, that rely on speech transcripts (.", "labels": [], "entities": []}, {"text": "Since such systems are usually trained on fluent, clean corpora, it is important to apply a speech disfluency detection system as a pre-processor to find and remove disfluencies from input data.", "labels": [], "entities": [{"text": "speech disfluency detection", "start_pos": 92, "end_pos": 119, "type": "TASK", "confidence": 0.7427036166191101}]}, {"text": "By disfluency detection, we usually mean identifying and deleting reparandum words.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.7436635196208954}]}, {"text": "Filled pauses and discourse markers belong to a closed set of words, so they are trivial to detect . In this paper, we introduce anew model for detecting restart and repair disfluencies in spontaneous speech transcripts called LSTM Noisy Channel Model (LSTM-NCM).", "labels": [], "entities": []}, {"text": "The model uses a Noisy Channel Model (NCM) to generate n-best candidate disfluency analyses, and a Long Short-Term Memory (LSTM) language model to rescore the NCM analyses.", "labels": [], "entities": []}, {"text": "The language model scores are used as features in a MaxEnt reranker to select the most plausible analysis.", "labels": [], "entities": [{"text": "MaxEnt reranker", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.8985316753387451}]}, {"text": "We show that this novel approach improves the current state-of-the-art.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: F-scores on the dev set for a variety of  LSTM language models.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9846700429916382}]}, {"text": " Table 3: Expected error rates on the dev set for a  variey of LSTM language models.", "labels": [], "entities": [{"text": "Expected error rates", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8177845478057861}]}, {"text": " Table 4: F-score for 4-gram, LSTM and combina- tion of both language models.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9948326349258423}]}, {"text": " Table 5: Expected error rates for 4-gram, LSTM  and combination of both language models.", "labels": [], "entities": [{"text": "Expected error rates", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8760586380958557}]}, {"text": " Table 6: Comparison of the LSTM-NCM to state- of-the-art methods on the dev set. *Models have  used richer input.", "labels": [], "entities": []}]}