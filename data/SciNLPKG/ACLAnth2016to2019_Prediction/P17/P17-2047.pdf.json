{"title": [{"text": "EviNets: Neural Networks for Combining Evidence Signals for Factoid Question Answering", "labels": [], "entities": [{"text": "Factoid Question Answering", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.5876854856808981}]}], "abstractContent": [{"text": "A critical task for question answering is the final answer selection stage, which has to combine multiple signals available about each answer candidate.", "labels": [], "entities": [{"text": "question answering", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.9092622101306915}, {"text": "answer selection", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.6398836970329285}]}, {"text": "This paper proposes EviNets: a novel neural network architecture for factoid question answering.", "labels": [], "entities": [{"text": "factoid question answering", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.7647894819577535}]}, {"text": "EviNets scores candidate answer entities by combining the available supporting evidence, e.g., structured knowledge bases and unstructured text documents.", "labels": [], "entities": []}, {"text": "EviNets represents each piece of evidence with a dense embeddings vector, scores their relevance to the question, and aggregates the support for each candidate to predict their final scores.", "labels": [], "entities": []}, {"text": "Each of the components is generic and allows plugging in a variety of models for semantic similarity scoring and information aggregation.", "labels": [], "entities": [{"text": "semantic similarity scoring", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.768374482790629}, {"text": "information aggregation", "start_pos": 113, "end_pos": 136, "type": "TASK", "confidence": 0.7752552032470703}]}, {"text": "We demonstrate the effectiveness of EviNets in experiments on the existing TREC QA and WikiMovies benchmarks, and on the new Yahoo!", "labels": [], "entities": [{"text": "TREC QA and WikiMovies benchmarks", "start_pos": 75, "end_pos": 108, "type": "DATASET", "confidence": 0.7773193120956421}]}, {"text": "Answers dataset introduced in this paper.", "labels": [], "entities": [{"text": "Answers dataset", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.6666194796562195}]}, {"text": "EviNets can be extended to other information types and could facilitate future work on combining evidence signals for joint reasoning in question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.8171020150184631}]}], "introductionContent": [{"text": "Most of the recent works in Question Answering (QA) have focused on the problem of semantic matching between a question and candidate answer sentences.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.8873983263969422}, {"text": "semantic matching between a question and candidate answer sentences", "start_pos": 83, "end_pos": 150, "type": "TASK", "confidence": 0.8409864902496338}]}, {"text": "The datasets used in these works, such as Answer Sentence Selection Dataset () and WikiQA (, typically contain a relatively small set of sentences, and the task is to select those that state the answer to the question.", "labels": [], "entities": [{"text": "Answer Sentence Selection Dataset", "start_pos": 42, "end_pos": 75, "type": "DATASET", "confidence": 0.7357447445392609}, {"text": "WikiQA", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.8998225331306458}]}, {"text": "However, for many questions, a single sentence does not provide sufficient information, and it may not be reliable in isolation.", "labels": [], "entities": []}, {"text": "At the same time, the redundancy of information in large corpora, such as the Web, has been shown useful to improve information retrieval approaches to QA).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.7128304839134216}]}, {"text": "This work focuses on factoid questions, which can be answered with an entity, i.e., an object in a Knowledge Base (KB) such as Freebase.", "labels": [], "entities": []}, {"text": "Knowledge Base Question Answering (KBQA) techniques, such as; ;, can be used to answer some of the user questions directly from a KB.", "labels": [], "entities": [{"text": "Knowledge Base Question Answering (KBQA)", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6886948645114899}]}, {"text": "However, KBs are inherently incomplete (), and do not have sufficient information to answer many other questions).", "labels": [], "entities": []}, {"text": "Previous, feature-engineering, approaches for combining different data sources to improve answer retrieval were shown to be quite effective for QA (.", "labels": [], "entities": [{"text": "answer retrieval", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.8479989469051361}]}, {"text": "Alternatively, Memory Networks ( and their extensions () use embeddings to represent relevant data as memories, and summarize them into a single vector, therefore losing information about answers provenances.", "labels": [], "entities": []}, {"text": "In this paper, we introduce EviNets, a novel neural network architecture for factoid question answering, which provides a unified framework for aggregating evidence, supporting answer candidates.", "labels": [], "entities": [{"text": "factoid question answering", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.7052284876505533}]}, {"text": "Given a question, EviNets retrieves a set of relevant pieces of information, e.g., sentences from a corpora or knowledge base triples, and extracts mentioned entities as candidate answers.", "labels": [], "entities": []}, {"text": "All the evidence signals are then embedded into the same vector space, scored and aggregated using multiple strategies for each answer candidate.", "labels": [], "entities": []}, {"text": "Experiments on the TREC QA, WikiMovies and new Yahoo!", "labels": [], "entities": [{"text": "TREC QA", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9066572189331055}, {"text": "WikiMovies", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.8974609375}]}, {"text": "Answers datasets demonstrate the effectiveness of EviNets, and its ability to handle both unstructured text and structured KB triples.", "labels": [], "entities": []}], "datasetContent": [{"text": "The TREC QA dataset is composed of factoid questions, which can be answered with an entity, and were used in TREC 8-12 question answering tracks.", "labels": [], "entities": [{"text": "TREC QA dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9255572358767191}, {"text": "TREC 8-12 question answering", "start_pos": 109, "end_pos": 137, "type": "TASK", "confidence": 0.7347151041030884}]}, {"text": "Similarly to we used web search (using the Microsoft Bing Web Search API) to retrieve top 50 documents, parsed them, extracted sentences and ranked them using tf-idf similarity to the question.", "labels": [], "entities": []}, {"text": "To compare our results with the existing state-of-the-art, we used the same set of candidate entities as used by the QuASE model.", "labels": [], "entities": []}, {"text": "We note that the extracted evidence differs between the models, and we were unable to match some of the candidates to our sentences.", "labels": [], "entities": []}, {"text": "For text+kb experiment, just as QuASE, we used entity descriptions and types from Freebase knowledge base.", "labels": [], "entities": [{"text": "Freebase knowledge base", "start_pos": 82, "end_pos": 105, "type": "DATASET", "confidence": 0.9481593171755472}]}, {"text": "EviNets achieves competitive results on the dataset, beating KV MemN2N by 13% in F1 score, and, unlike QuASE, does not rely on expensive feature engineering and does not require any external resources to train.", "labels": [], "entities": [{"text": "EviNets", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8586390614509583}, {"text": "KV MemN2N", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.8639508187770844}, {"text": "F1 score", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9851788878440857}]}, {"text": "The WikiMovies dataset contains questions in the movies domain along with relevant Wikipedia passages and OMDb knowledge base.", "labels": [], "entities": [{"text": "WikiMovies dataset", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9645399451255798}]}, {"text": "Since KVMemN2N already achieves an almost perfect result answering the questions using the KB, we focus on using the provided movie articles from Wikipedia.", "labels": [], "entities": [{"text": "KVMemN2N", "start_pos": 6, "end_pos": 14, "type": "DATASET", "confidence": 0.8630470037460327}]}, {"text": "We followed the preprocessing procedures described in.", "labels": [], "entities": []}, {"text": "Unlike TREC QA, where there are often multiple relevant supporting pieces of evidence, answers in the WikiMovies dataset usually have a single relevant sentence, which, however, mentions multi-.", "labels": [], "entities": [{"text": "TREC QA", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.5262914746999741}, {"text": "WikiMovies dataset", "start_pos": 102, "end_pos": 120, "type": "DATASET", "confidence": 0.9246256649494171}]}, {"text": "As we can see, with the same setup using individual sentences as evidence/memories EviNets significantly outperforms the KV MemN2N model by 27%.", "labels": [], "entities": []}, {"text": "It is important to emphasize that the best-reported results of memory networks were obtained using entitycentered windows as memories, which requires special pre-processing and increases the number of memories.", "labels": [], "entities": []}, {"text": "Additionally, these models used all of the KB entities as candidate answers, whereas EviNets relies only on the mentioned ones, which is a more scalable scenario for open-domain question answering, where it is not realistic to score millions of candidate answers in real-time.", "labels": [], "entities": [{"text": "open-domain question answering", "start_pos": 166, "end_pos": 196, "type": "TASK", "confidence": 0.6148772339026133}]}, {"text": "recently released a dataset with search queries, which lead to clicks on factoid Yahoo!", "labels": [], "entities": []}, {"text": "Answers questions, identified as questions with the best answer containing less than 3 words and a Wikipedia page as the specified source of information . This dataset contains 15K queries, which correspond to 4725 unique Yahoo!", "labels": [], "entities": []}, {"text": "We took these questions, and mapped answers to KB entities using the TagMe entity linking library  which no answer entities with a good confidence 4 were identified, e.g., date answers, and randomly split the rest into training, development and test sets, with 2711 questions in total.", "labels": [], "entities": []}, {"text": "Similarly to the TREC QA experiments, we extracted textual evidence using Bing Web Search API, by retrieving top 50 relevant documents, extracting the main content blocks, and splitting them into sentences.", "labels": [], "entities": [{"text": "TREC QA", "start_pos": 17, "end_pos": 24, "type": "DATASET", "confidence": 0.6319032311439514}]}, {"text": "We applied the TagMe entity linker to the extracted sentences, and considered all entities of mentions with the confidence score above the 0.2 threshold as candidate answers.", "labels": [], "entities": []}, {"text": "For candidate entities we also retrieved relevant KB triples, such as entity types and descriptions, which extended the original pool of evidences.", "labels": [], "entities": []}, {"text": "summarizes the results of EviNets and some baseline methods on the created Yahoo!", "labels": [], "entities": []}, {"text": "As we can see, knowledge base data is not enough to answer most of these questions, and a state-of-the-art KBQA system Aqqu gets only 0.116 precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9975323677062988}]}, {"text": "Adding textual data helps significantly, and Text2KB improves the precision to 0.17, which roughly matches the results of the AskMSR system, that ranks candidate entities by their popularity in the retrieved documents.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9995614886283875}]}, {"text": "Using text along with KB evidence gave higher performance metrics, boosting F1 from 0.271 to 0.291.", "labels": [], "entities": [{"text": "KB", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.7585175633430481}, {"text": "F1", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9997674822807312}]}, {"text": "EviNets significantly improves over the baseline approaches, beating AskMSR by 28% and KV MemN2N by almost 80% in F1 score.", "labels": [], "entities": [{"text": "EviNets", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8387656211853027}, {"text": "AskMSR", "start_pos": 69, "end_pos": 75, "type": "DATASET", "confidence": 0.8831413388252258}, {"text": "F1 score", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9790403544902802}]}], "tableCaptions": [{"text": " Table 3: Precision, Recall and F1 of different  methods on TREC QA dataset. Improvements  over KV MemN2N are statistically significant.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9994044303894043}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9988018274307251}, {"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9992119073867798}, {"text": "TREC QA dataset", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.8934580286343893}, {"text": "KV MemN2N", "start_pos": 96, "end_pos": 105, "type": "DATASET", "confidence": 0.7478882670402527}]}, {"text": " Table 4: Accuracy of EviNets and baseline mod- els on the WikiMovies dataset. The results marked  * are obtained using a different setup, i.e., they  use pre-processed entity window memories, and  the whole set of entities as candidates.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9708922505378723}, {"text": "WikiMovies dataset", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.9369404315948486}]}, {"text": " Table 5: Precision, Recall and F1 of different  methods on Yahoo! Answers factoid QA dataset.  The Oracle assumes candidate answers are ranked  perfectly and its performance is limited by the ini- tial retrieval step.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9988948702812195}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9972288012504578}, {"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9992554783821106}, {"text": "Yahoo! Answers factoid QA dataset", "start_pos": 60, "end_pos": 93, "type": "DATASET", "confidence": 0.9102753599484762}]}]}