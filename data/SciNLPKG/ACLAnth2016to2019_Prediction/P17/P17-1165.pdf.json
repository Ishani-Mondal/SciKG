{"title": [{"text": "Topical Coherence in LDA-based Models through Induced Segmentation", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words.", "labels": [], "entities": []}, {"text": "The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment.", "labels": [], "entities": []}, {"text": "In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments.", "labels": [], "entities": []}, {"text": "We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks.", "labels": [], "entities": []}, {"text": "Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification.", "labels": [], "entities": [{"text": "Micro F1 measure", "start_pos": 254, "end_pos": 270, "type": "METRIC", "confidence": 0.6640312472979227}, {"text": "text classification", "start_pos": 275, "end_pos": 294, "type": "TASK", "confidence": 0.8400075733661652}]}], "introductionContent": [{"text": "Since the seminal works of and, there have been several developments in probabilistic topic models.", "labels": [], "entities": []}, {"text": "Many extensions have indeed been proposed for different applications, including ad-hoc information retrieval (), clustering search results () and driving faceted browsing.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.7212311327457428}, {"text": "clustering search results", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.9048467675844828}]}, {"text": "In most of these studies, the initial exchangeability assumptions of PLSA and LDA, stipulating that words within a document are interdependent, has led to incoherent topic assignments within semantically meaningful text units, even though the importance of having topically coherent phrases is generally admitted ().", "labels": [], "entities": []}, {"text": "More recently, () has shown that binding topics, so as to obtain more coherent topic assignments, within such text segments as noun phrases improves the performance (e.g. in terms of perplexity) of LDAbased models.", "labels": [], "entities": []}, {"text": "The question nevertheless remains as to which segmentation one should rely on.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.9630938172340393}]}, {"text": "Furthermore, text segments can refer to topics that are barely present in other parts of the document.", "labels": [], "entities": []}, {"text": "For example, the segment \"the Kurdish regional capital\" in the sentence 1 \"A thousand protesters took to the main street in Erbil, the Kurdish regional capital, to condemn anew law requiring all public demonstrations to have government permits.\" refers to geography in a document that is mainly devoted to politics.", "labels": [], "entities": []}, {"text": "Relying on a single topic distribution, as done inmost previous studies including (), may prevent one from capturing those segment specific topics.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel LDA-based model that automatically segments documents into topically coherent sequences of words.", "labels": [], "entities": []}, {"text": "The coherence between topics is ensured through copulas) that bind the topics associated to the words of a segment.", "labels": [], "entities": []}, {"text": "In addition, this model relies on both document and segment specific topic distri-butions so as to capture fine grained differences in topic assignments.", "labels": [], "entities": []}, {"text": "A simple switching mechanism is used to select the appropriate distribution (document or segment specific) for assigning a topic to a word.", "labels": [], "entities": [{"text": "assigning a topic to a word", "start_pos": 111, "end_pos": 138, "type": "TASK", "confidence": 0.8016150792439779}]}, {"text": "We show that this model naturally encompasses other state-of-the-art LDA-based models proposed to accomplish the same task, and that it outperforms these models over six publicly available collections in terms of perplexity, Normalized Pointwise Mutual Information (NPMI), a measure used to assess the coherence of topics with documents, and the Micro F1-measure in a text classification context.", "labels": [], "entities": [{"text": "Micro", "start_pos": 346, "end_pos": 351, "type": "DATASET", "confidence": 0.8412051796913147}, {"text": "F1-measure", "start_pos": 352, "end_pos": 362, "type": "METRIC", "confidence": 0.5034095644950867}, {"text": "text classification", "start_pos": 368, "end_pos": 387, "type": "TASK", "confidence": 0.7008540630340576}]}], "datasetContent": [{"text": "We conducted a number of experiments aimed at studying the impact of simultaneously segmenting and assigning topics to words within segments using the proposed segLDAcop model.", "labels": [], "entities": []}, {"text": "The first two collections were considered in (Balikas et al., 2016a), we followed their setup by considering 3 subsets of Wikipedia with different number of classes (namely, Wiki0, Wiki1 and Wiki2).", "labels": [], "entities": []}, {"text": "The Reuters dataset comes from Reuters-21578, Distribution 1.0 as investigated in ( and the NYT dataset is collected from full text of New York Times global news, from January 1st to December 31st, 2011.", "labels": [], "entities": [{"text": "Reuters dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9818804562091827}, {"text": "Reuters-21578", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.9508001208305359}, {"text": "NYT dataset", "start_pos": 92, "end_pos": 103, "type": "DATASET", "confidence": 0.9874788522720337}]}, {"text": "These collections were processed following () by removing a standard list of 50 stop words, lemmatizing, lowercasing and keeping only words made of letters.", "labels": [], "entities": []}, {"text": "To deal with relatively homogeneous collections, we also removed documents that are too long.", "labels": [], "entities": []}, {"text": "The statistics of these datasets, as well as the admissible maximal length for documents, in terms of the number of words they contain, can be found in.", "labels": [], "entities": []}, {"text": "Settings: We compared our models (segLDAcop p=0 , segLDAcop \u03bb=0 , segLDAcop) with three models, namely the standard LDA model, and two previously introduced models aiming at binding topics within segments: 1.", "labels": [], "entities": []}, {"text": "LDA: Standard Latent Dirichlet Allocation implemented using collapsed Gibbs sampling inference (  Both senLDA and copLDA implementations, can be found in https://github.com/ balikasg/topicModelling.", "labels": [], "entities": [{"text": "copLDA", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.954383134841919}]}, {"text": "In all models \u03b1 and \u03b2 play asymmetric role and are respectively fixed to 1/K, following).", "labels": [], "entities": []}, {"text": "For copula based models, \u03bb is set to 5, following).", "labels": [], "entities": []}, {"text": "As already discussed, p is set to 0 for segLDAcop p=0 ; it is set to 0.5 for segLDAcop so as not to privilege a priori one topic distribution (document or segment specific) over the other.", "labels": [], "entities": []}, {"text": "For sampling from Frank's copula, we relied on the R copula package . We chose L (the maximum length of a segment) using line search for L \u2208 and used L = 3 in all our experiments.", "labels": [], "entities": []}, {"text": "Finally, to illustrate the behaviors of the different models with different number of topics, we present here the results obtained with K = 20 and K = 100.", "labels": [], "entities": []}, {"text": "We now compare the different models along three main dimensions: perplexity, use of topic Our complete code will be available for research purposes.", "labels": [], "entities": []}, {"text": "representations for classification and topic coherence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Perplexity with respect to different number of topics (20 and 100).", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9517539739608765}]}, {"text": " Table 3: MiF score (percent) with respect to different number of topics (20 and 100).", "labels": [], "entities": [{"text": "MiF score", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9500719308853149}]}]}