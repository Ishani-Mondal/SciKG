{"title": [{"text": "Neural Word Segmentation with Rich Pretraining", "labels": [], "entities": [{"text": "Neural Word Segmentation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8029876550038656}]}], "abstractContent": [{"text": "Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings.", "labels": [], "entities": [{"text": "Neural word segmentation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8342617948849996}]}, {"text": "On the other hand, statistical segmentation research has exploited richer sources of external information , such as punctuation, automatic seg-mentation and POS.", "labels": [], "entities": [{"text": "statistical segmentation", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.8114572167396545}]}, {"text": "We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submod-ule using rich external sources.", "labels": [], "entities": [{"text": "neural word segmentation", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.6747692028681437}]}, {"text": "Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7328523248434067}]}, {"text": "Neural network models have been exploited due to their strength in non-sparse representation learning and non-linear power in feature combination, which have led to advances in many NLP tasks.", "labels": [], "entities": []}, {"text": "So far, neural word segmentors have given comparable accuracies to the best statictical models.", "labels": [], "entities": [{"text": "neural word segmentors", "start_pos": 8, "end_pos": 30, "type": "TASK", "confidence": 0.7042784094810486}]}, {"text": "With respect to non-sparse representation, character embeddings have been exploited as a foundation of neural word segmentors.", "labels": [], "entities": [{"text": "neural word segmentors", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.7001275916894277}]}, {"text": "They serve to reduce sparsity of character ngrams, allowing, for example, \"(cat) (lie) (in) (corner)\" to be connected with \"(dog) (sit) (in) * Equal contribution.", "labels": [], "entities": []}, {"text": "(corner)\" (, which is infeasible by using sparse one-hot character features.", "labels": [], "entities": []}, {"text": "In addition to character embeddings, distributed representations of character bigrams) and words) have also been shown to improve segmentation accuracies.", "labels": [], "entities": []}, {"text": "With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (), as well as LSTMs on characters) and words).", "labels": [], "entities": [{"text": "segmentation disambiguation", "start_pos": 116, "end_pos": 143, "type": "TASK", "confidence": 0.898772269487381}]}, {"text": "For structured learning and inference, CRF has been used for character sequence labelling models () and structural beam search has been used for word-based segmentors.", "labels": [], "entities": [{"text": "character sequence labelling", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.667936901251475}]}, {"text": "Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings overlarge Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (.", "labels": [], "entities": [{"text": "segmentation accuracies", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.672074630856514}, {"text": "parsing", "start_pos": 205, "end_pos": 212, "type": "TASK", "confidence": 0.9765422344207764}]}, {"text": "Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a standard practice in neural NLP.", "labels": [], "entities": [{"text": "Pretraining", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.8624399900436401}]}, {"text": "On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (), and making use of selfpredictions (.", "labels": [], "entities": [{"text": "statistical segmentation", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.7628317475318909}]}, {"text": "It has also utilised heterogenous annotations such as POS ( [] SEP state4 [] APP state5 [] APP state6 [] SEP state7 [] APP state8 [ ] FIN state9 \u03c6 [ ] --: A transition based word segmentation example. standards).", "labels": [], "entities": [{"text": "POS", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.6965079307556152}, {"text": "FIN state9 \u03c6", "start_pos": 134, "end_pos": 146, "type": "DATASET", "confidence": 0.7931457956631979}, {"text": "word segmentation", "start_pos": 174, "end_pos": 191, "type": "TASK", "confidence": 0.7258134186267853}]}, {"text": "To our knowledge, such rich external information has not been systematically investigated for neural segmentation.", "labels": [], "entities": [{"text": "neural segmentation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7067030072212219}]}, {"text": "We fill this gap by investigating rich external pretraining for neural segmentation.", "labels": [], "entities": [{"text": "neural segmentation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.6802712976932526}]}, {"text": "Following and, we adopt a globally optimised beam-search framework for neural structured prediction (, which allows word information to be modelled explicitly.", "labels": [], "entities": [{"text": "neural structured prediction", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.7054198185602824}]}, {"text": "Different from previous work, we make our model conceptually simple and modular, so that the most important sub module, namely a five-character window context, can be pretrained using external data.", "labels": [], "entities": []}, {"text": "We adopt a multi-task learning strategy), casting each external source of information as a auxiliary classification task, sharing a five-character window network.", "labels": [], "entities": []}, {"text": "After pretraining, the character window network is used to initialize the corresponding module in our segmentor.", "labels": [], "entities": []}, {"text": "Results on 6 different benchmarks show that our method outperforms the best statistical and neural segmentation models consistently, giving the best reported results on 5 datasets in different domains and genres.", "labels": [], "entities": []}, {"text": "Our implementation is based on LibN3L 1 ().", "labels": [], "entities": [{"text": "LibN3L 1", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.9119082093238831}]}, {"text": "Code and models can be downloaded from http://gitHub.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use Chinese Treebank 6.0 (CTB6) () as our main dataset.", "labels": [], "entities": [{"text": "Chinese Treebank 6.0 (CTB6)", "start_pos": 7, "end_pos": 34, "type": "DATASET", "confidence": 0.9720291296641032}]}, {"text": "Training, development and test set splits follow previous work ().", "labels": [], "entities": []}, {"text": "In order to verify the robustness of our model, we additionally use SIGHAN 2005 bake-off ( , automatically segmented using ZPar 0.6 off-theshelf (, the statictics of which are shown in.", "labels": [], "entities": [{"text": "SIGHAN 2005 bake-off", "start_pos": 68, "end_pos": 88, "type": "DATASET", "confidence": 0.8287506699562073}]}, {"text": "For pretraining character representations, we extract punctuation classification data from the Gigaword corpus, and use the word-based ZPar and a standard character-based CRF model () to obtain automatic segmentation results.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 95, "end_pos": 110, "type": "DATASET", "confidence": 0.9625658094882965}]}, {"text": "We compare pretraining using ZPar results only and using results that both segmentors agree on.", "labels": [], "entities": []}, {"text": "For heterogenous segmentation corpus and POS data, we use a People's Daily corpus of 5 months . Statistics are listed in.", "labels": [], "entities": [{"text": "POS data", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.6491384506225586}, {"text": "People's Daily corpus", "start_pos": 60, "end_pos": 81, "type": "DATASET", "confidence": 0.8975648134946823}]}, {"text": "The standard word precision, recall and F1 measure) are used to evaluate segmentation performances.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.7125164270401001}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9991928935050964}, {"text": "F1 measure", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9926957190036774}, {"text": "segmentation", "start_pos": 73, "end_pos": 85, "type": "TASK", "confidence": 0.959548830986023}]}, {"text": "We adopt commonly used values for most hyperparameters, but tuned the sizes of hidden layers on the development set.", "labels": [], "entities": []}, {"text": "The values are summarized in.", "labels": [], "entities": []}, {"text": "We perform development experiments to verify the usefulness of various context representations, network configurations and different pretraining methods, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Influence of character contexts.", "labels": [], "entities": []}, {"text": " Table 5: Influence of word contexts.", "labels": [], "entities": []}, {"text": " Table 6: Influence of pretraining.", "labels": [], "entities": [{"text": "pretraining", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.8553522229194641}]}, {"text": " Table 7: Main results on CTB6.", "labels": [], "entities": [{"text": "CTB6", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.9523276686668396}]}, {"text": " Table 8: Main results on other test datasets.", "labels": [], "entities": []}]}