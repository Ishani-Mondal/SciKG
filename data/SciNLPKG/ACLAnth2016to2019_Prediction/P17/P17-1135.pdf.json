{"title": [{"text": "Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection", "labels": [], "entities": [{"text": "Cross-Lingual Named Entity Recognition", "start_pos": 18, "end_pos": 56, "type": "TASK", "confidence": 0.6568477153778076}]}], "abstractContent": [{"text": "The state-of-the-art named entity recognition (NER) systems are supervised machine learning models that require large amounts of manually annotated data to achieve high accuracy.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.8199646472930908}, {"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9710321426391602}]}, {"text": "However, annotating NER data by human is expensive and time-consuming, and can be quite difficult fora new language.", "labels": [], "entities": []}, {"text": "In this paper , we present two weakly supervised approaches for cross-lingual NER with no human annotation in a target language.", "labels": [], "entities": [{"text": "NER", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.6826435923576355}]}, {"text": "The first approach is to create automatically labeled NER data fora target language via annotation projection on comparable corpora, where we develop a heuris-tic scheme that effectively selects good-quality projection-labeled data from noisy data.", "labels": [], "entities": []}, {"text": "The second approach is to project distributed representations of words (word embeddings) from a target language to a source language, so that the source-language NER system can be applied to the target language without retraining.", "labels": [], "entities": []}, {"text": "We also design two co-decoding schemes that effectively combine the outputs of the two projection-based approaches.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the proposed approaches on both in-house and open NER data for several target languages.", "labels": [], "entities": []}, {"text": "The results show that the combined systems outperform three other weakly supervised approaches on the CoNLL data.", "labels": [], "entities": [{"text": "CoNLL data", "start_pos": 102, "end_pos": 112, "type": "DATASET", "confidence": 0.9701523780822754}]}], "introductionContent": [{"text": "Named entity recognition (NER) is a fundamental information extraction task that automatically detects named entities in text and classifies them into pre-defined entity types such as PERSON, ORGANIZATION, GPE (GeoPolitical Entities), EVENT, LOCATION, TIME, DATE, etc.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8101397057374319}, {"text": "information extraction task", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.8126562237739563}, {"text": "EVENT", "start_pos": 235, "end_pos": 240, "type": "METRIC", "confidence": 0.8698457479476929}, {"text": "TIME", "start_pos": 252, "end_pos": 256, "type": "METRIC", "confidence": 0.9105650782585144}, {"text": "DATE", "start_pos": 258, "end_pos": 262, "type": "METRIC", "confidence": 0.8654446601867676}]}, {"text": "NER provides essential inputs for many information extraction applications, including relation extraction, entity linking, question answering and text mining.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.750396192073822}, {"text": "relation extraction", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.8655306696891785}, {"text": "entity linking", "start_pos": 107, "end_pos": 121, "type": "TASK", "confidence": 0.8179251253604889}, {"text": "question answering", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.9113844931125641}, {"text": "text mining", "start_pos": 146, "end_pos": 157, "type": "TASK", "confidence": 0.8115123212337494}]}, {"text": "Building fast and accurate NER systems is a crucial step towards enabling large-scale automated information extraction and knowledge discovery on the huge volumes of electronic documents existing today.", "labels": [], "entities": [{"text": "NER", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9666876196861267}, {"text": "information extraction", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.7290993332862854}, {"text": "knowledge discovery", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7270340919494629}]}, {"text": "The state-of-the-art NER systems are supervised machine learning models (, including maximum entropy Markov models (MEMMs) (), conditional random fields (CRFs) () and neural networks.", "labels": [], "entities": []}, {"text": "To achieve high accuracy, a NER system needs to be trained with a large amount of manually annotated data, and is often supplied with language-specific resources (e.g., gazetteers, word clusters, etc.).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9923322200775146}]}, {"text": "Annotating NER data by human is rather expensive and timeconsuming, and can be quite difficult fora new language.", "labels": [], "entities": [{"text": "Annotating NER", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7710334658622742}]}, {"text": "This creates a big challenge in building NER systems of multiple languages for supporting multilingual information extraction applications.", "labels": [], "entities": [{"text": "multilingual information extraction", "start_pos": 90, "end_pos": 125, "type": "TASK", "confidence": 0.6614385942618052}]}, {"text": "The difficulty of acquiring supervised annotation raises the following question: given a welltrained NER system in a source language (e.g., English), how can one go about extending it to anew language with decent performance and no human annotation in the target language?", "labels": [], "entities": []}, {"text": "There are mainly two types of approaches for building weakly supervised cross-lingual NER systems.", "labels": [], "entities": []}, {"text": "The first type of approaches create weakly labeled NER training data in a target language.", "labels": [], "entities": [{"text": "NER training", "start_pos": 51, "end_pos": 63, "type": "TASK", "confidence": 0.8795856535434723}]}, {"text": "One way to create weakly labeled data is through annotation projection on aligned parallel corpora or translations between a source language and a target language, e.g.,).", "labels": [], "entities": []}, {"text": "Another way is to utilize the text and structure of Wikipedia to generate weakly labeled multilingual training annotations, e.g.,.", "labels": [], "entities": []}, {"text": "The second type of approaches are based on direct model transfer, e.g.,.", "labels": [], "entities": []}, {"text": "The basic idea is to train a single NER system in the source language with language independent features, so the system can be applied to other languages using those universal features.", "labels": [], "entities": []}, {"text": "In this paper, we make the following contributions to weakly supervised cross-lingual NER with no human annotation in the target languages.", "labels": [], "entities": []}, {"text": "First, for the annotation projection approach, we develop a heuristic, language-independent data selection scheme that seeks to select good-quality projection-labeled NER data from comparable corpora.", "labels": [], "entities": []}, {"text": "Experimental results show that the data selection scheme can significantly improve the accuracy of the target-language NER system when the alignment quality is low and the projectionlabeled data are noisy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9990769624710083}]}, {"text": "Second, we propose anew approach for direct NER model transfer based on representation projection.", "labels": [], "entities": [{"text": "NER model transfer", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9062373042106628}, {"text": "representation projection", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.7098401188850403}]}, {"text": "It projects word representations in vector space (word embeddings) from a target language to a source language, to create a universal representation of the words in different languages.", "labels": [], "entities": []}, {"text": "Under this approach, the NER system trained for the source language can be directly applied to the target language without the need for re-training.", "labels": [], "entities": []}, {"text": "Finally, we design two co-decoding schemes that combine the outputs (views) of the two projection-based systems to produce an output that is more accurate than the outputs of individual systems.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the proposed approaches on both in-house and open NER data sets fora number of target languages.", "labels": [], "entities": []}, {"text": "The results show that the combined systems outperform the state-of-the-art cross-lingual NER approaches proposed in, and  on the CoNLL NER test data.", "labels": [], "entities": [{"text": "NER", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.8395410776138306}, {"text": "CoNLL NER test data", "start_pos": 129, "end_pos": 148, "type": "DATASET", "confidence": 0.9627991169691086}]}, {"text": "We organize the paper as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we introduce three NER models that are used in the paper.", "labels": [], "entities": []}, {"text": "In Section 3 we present an annotation projection approach with effective data selection.", "labels": [], "entities": []}, {"text": "In Section 4 we propose a representation projection approach for direct NER model transfer.", "labels": [], "entities": [{"text": "NER model transfer", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.9059533079465231}]}, {"text": "In Section 5 we describe two co-decoding schemes that effectively combine the outputs of two projection-based approaches.", "labels": [], "entities": []}, {"text": "In Section 6 we evaluate the performance of the proposed approaches.", "labels": [], "entities": []}, {"text": "We describe related work in Section 7 and conclude the paper in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the performance of the proposed approaches for cross-lingual NER, including the 2 projection-based approaches and the 2 co-decoding schemes for combining them: (1) The annotation projection (AP) approach with heuristic data selection; (2) The representation projection approach (with two neural network architectures NN1 and NN2); (3) The exclude-O confidence-based co-decoding scheme; (4) The rank-based co-decoding scheme.", "labels": [], "entities": [{"text": "cross-lingual NER", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.5833753347396851}]}, {"text": "In, we show the results of different approaches for the in-house NER data.", "labels": [], "entities": [{"text": "NER", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9084913730621338}]}, {"text": "For annotation projection, the source (English) NER system is a linear-chain CRF model trained with 328K tokens of human-annotated English newswire data.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8790400624275208}]}, {"text": "The target-language NER systems are 2nd-order MEMM models trained with 1.3M, 1.5M, 2.6M and 1.5M tokens of projection-labeled data for Japanese, Korean, German and Portuguese, respectively.", "labels": [], "entities": []}, {"text": "The projection-labeled data are selected using the heuristic data selection scheme (see).", "labels": [], "entities": []}, {"text": "For representation projection, the source (English) NER systems are neural network models with architectures NN1 and NN2 (see), both trained with 328K tokens of humanannotated English newswire data.", "labels": [], "entities": [{"text": "representation projection", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.9395938813686371}]}, {"text": "The results show that the annotation projection (AP) approach has a relatively high precision and low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9993705153465271}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9988974332809448}]}, {"text": "For representation projection, neural network model NN2 (with a smoothing layer) is better than NN1, and NN2 tends to have a more balanced precision and recall.", "labels": [], "entities": [{"text": "representation projection", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.92420494556427}, {"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9990286827087402}, {"text": "recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9983224272727966}]}, {"text": "The rank-based codecoding scheme is more effective for combining the two projection-based approaches.", "labels": [], "entities": []}, {"text": "In particular, the rank-based scheme that combines AP and NN2 achieves the highest F 1 score among all the weakly supervised approaches for Korean, German and Portuguese (second highest F 1 score for Japanese), and it improves over the best of the two  projection-based systems by 2.2 to 7.4 F 1 score.", "labels": [], "entities": [{"text": "AP", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9311841726303101}, {"text": "F 1 score", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9915252923965454}, {"text": "F 1 score", "start_pos": 186, "end_pos": 195, "type": "METRIC", "confidence": 0.9214490056037903}, {"text": "F 1", "start_pos": 292, "end_pos": 295, "type": "METRIC", "confidence": 0.9675562083721161}]}, {"text": "We also provide the performance of supervised learning where the NER system is trained with human-annotated data in the target language (with size shown in the bracket).", "labels": [], "entities": []}, {"text": "While the performance of the weakly supervised systems is not as good as supervised learning, it is important to build weakly supervised systems with decent performance when supervised annotation is unavailable.", "labels": [], "entities": []}, {"text": "Even if supervised annotation is feasible, the weakly supervised systems can be used to pre-annotate the data, and we observed that pre-annotation can improve the annotation speed by 40%-60%, which greatly reduces the annotation cost.", "labels": [], "entities": []}, {"text": "For the CoNLL data, the source (English) NER system for annotation projection is a linearchain CRF model trained with the CoNLL English training data (203K tokens), and the targetlanguage NER systems are 2nd-order MEMM models trained with 1.3M, 7.0M and 1.2M tokens of projection-labeled data for Spanish, Dutch and German, respectively.", "labels": [], "entities": [{"text": "CoNLL data", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.9759677648544312}, {"text": "annotation projection", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.7783362567424774}, {"text": "CoNLL English training data", "start_pos": 122, "end_pos": 149, "type": "DATASET", "confidence": 0.8994012475013733}]}, {"text": "The projection-labeled data are selected using the heuristic data selection scheme, where the threshold parameters q and n are determined via coordinate search based on the CoNLL development sets.", "labels": [], "entities": [{"text": "CoNLL development sets", "start_pos": 173, "end_pos": 195, "type": "DATASET", "confidence": 0.9381850957870483}]}, {"text": "Compared with no data selection, the data selection scheme improves the annotation projection approach by 2.7/2.0/2.7 F 1 score on the Spanish/Dutch/German development data.", "labels": [], "entities": [{"text": "Spanish/Dutch/German development data", "start_pos": 135, "end_pos": 172, "type": "DATASET", "confidence": 0.6706869304180145}]}, {"text": "In addition to standard NER features such as n-gram word features, word type features, prefix and suffix features, the target-language NER systems also use the multilingual Wikipedia entity type mappings developed in to generate dictionary features and as decoding constraints, which improve the annotation projection approach by 3.0/5.4/7.9 F 1 score on the Spanish/Dutch/German development data.", "labels": [], "entities": [{"text": "Spanish/Dutch/German development data", "start_pos": 359, "end_pos": 396, "type": "DATASET", "confidence": 0.6489698759147099}]}, {"text": "For representation projection, the source (English) NER systems are neural network models (NN1 and NN2) trained with the CoNLL English training data.", "labels": [], "entities": [{"text": "representation projection", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.9529885947704315}, {"text": "CoNLL English training data", "start_pos": 121, "end_pos": 148, "type": "DATASET", "confidence": 0.9557088017463684}]}, {"text": "Compared with the standard CBOW word2vec model, the concatenated variant improves the representation projection approach (NN1) by 8.9/11.4/6.8 F 1 score on the Spanish/Dutch/German development data, as well as by 2.0 F 1 score on English.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9449956019719442}, {"text": "Spanish/Dutch/German development data", "start_pos": 160, "end_pos": 197, "type": "DATASET", "confidence": 0.5887646930558341}, {"text": "F 1 score", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9701109925905863}]}, {"text": "In addition, the frequency-weighted cross-lingual word embedding projection (7) improves the representation projection approach (NN1) by 2.2/6.3/3.7 F 1 score on the Spanish/Dutch/German development data, compared with using uniform weights on the same data.", "labels": [], "entities": [{"text": "Spanish/Dutch/German development data", "start_pos": 166, "end_pos": 203, "type": "DATASET", "confidence": 0.5676743303026471}]}, {"text": "We do observe, however, that using uniform weights when keeping only the most frequent translation of a word instead of all word pairs above a threshold in the training dictionary, leads to performance similar to that of the frequencyweighted projection.", "labels": [], "entities": []}, {"text": "In we show the results for the CoNLL development data.", "labels": [], "entities": [{"text": "CoNLL development data", "start_pos": 31, "end_pos": 53, "type": "DATASET", "confidence": 0.9138225118319193}]}, {"text": "For representation projection, NN1 is better than NN2.", "labels": [], "entities": [{"text": "representation projection", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.9130918681621552}]}, {"text": "Both the annotation projection approach and NN1 tend to have a high precision.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.6253805905580521}, {"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9972512125968933}]}, {"text": "In this case, the exclude-O confidencebased co-decoding scheme that combines AP and NN1 achieves the highest F 1 score for Spanish and Dutch (second highest F 1 score for German), and improves over the best of the two projection-based systems by 1.5 to 3.4 F 1 score.", "labels": [], "entities": [{"text": "AP", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9823811650276184}, {"text": "F 1 score", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9831236203511556}, {"text": "F 1 score", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.8993107279141744}, {"text": "F 1 score", "start_pos": 257, "end_pos": 266, "type": "METRIC", "confidence": 0.9602149724960327}]}, {"text": "In we compare our top systems (confidence or rank-based co-decoding of AP and NN1, determined by the development data) with the best results of the cross-lingual NER approaches proposed in", "labels": [], "entities": [{"text": "NER", "start_pos": 162, "end_pos": 165, "type": "TASK", "confidence": 0.9175125360488892}]}], "tableCaptions": [{"text": " Table 1: A snapshot of the frequency table where  the target language is Portuguese. Estados Unidos  means United States. The correct NER tag for Es- tados Unidos is GPE which has the highest relative  frequency in the weakly labeled data.", "labels": [], "entities": [{"text": "GPE", "start_pos": 167, "end_pos": 170, "type": "METRIC", "confidence": 0.9785501956939697}]}, {"text": " Table 3: In-house NER data: Precision, Recall and  F 1 score on exact phrasal matches. The highest F 1  score among all the weakly supervised approaches  is shown in bold. Same for Tables 4 and 5.", "labels": [], "entities": [{"text": "NER", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.905456006526947}, {"text": "Precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9967724680900574}, {"text": "Recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9904826879501343}, {"text": "F 1 score", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.985936184724172}, {"text": "F 1  score", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9667316476504008}]}, {"text": " Table 4: CoNLL NER development data.", "labels": [], "entities": [{"text": "CoNLL NER development data", "start_pos": 10, "end_pos": 36, "type": "DATASET", "confidence": 0.8423891067504883}]}, {"text": " Table 5: CoNLL NER test data.", "labels": [], "entities": [{"text": "CoNLL NER test data", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.8607698380947113}]}]}