{"title": [{"text": "Leveraging Knowledge Bases in LSTMs for Improving Machine Reading", "labels": [], "entities": [{"text": "Improving Machine Reading", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.9025766650835673}]}], "abstractContent": [{"text": "This paper focuses on how to take advantage of external knowledge bases (KBs) to improve recurrent neural networks for machine reading.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 119, "end_pos": 134, "type": "TASK", "confidence": 0.8190211057662964}]}, {"text": "Traditional methods that exploit knowledge from KBs encode knowledge as discrete indicator features.", "labels": [], "entities": []}, {"text": "Not only do these features generalize poorly, but they require task-specific feature engineering to achieve good performance.", "labels": [], "entities": []}, {"text": "We propose KBLSTM, a novel neural model that leverages continuous representations of KBs to enhance the learning of recurrent neural networks for machine reading.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 146, "end_pos": 161, "type": "TASK", "confidence": 0.7638773322105408}]}, {"text": "To effectively integrate background knowledge with information from the currently processed text, our model employs an attention mechanism with a sentinel to adaptively decide whether to attend to background knowledge and which information from KBs is useful.", "labels": [], "entities": []}, {"text": "Experimental results show that our model achieves accuracies that surpass the previous state-of-the-art results for both entity extraction and event extraction on the widely used ACE2005 dataset.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.7634448409080505}, {"text": "event extraction", "start_pos": 143, "end_pos": 159, "type": "TASK", "confidence": 0.6990218460559845}, {"text": "ACE2005 dataset", "start_pos": 179, "end_pos": 194, "type": "DATASET", "confidence": 0.9806619584560394}]}], "introductionContent": [{"text": "Recurrent neural networks (RNNs), a neural architecture that can operate over text sequentially, have shown great success in addressing a wide range of natural language processing problems, such as parsing (, named entity recognition (, and semantic role labeling ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 198, "end_pos": 205, "type": "TASK", "confidence": 0.9667593836784363}, {"text": "named entity recognition", "start_pos": 209, "end_pos": 233, "type": "TASK", "confidence": 0.6200564404328665}, {"text": "semantic role labeling", "start_pos": 241, "end_pos": 263, "type": "TASK", "confidence": 0.6658506989479065}]}, {"text": "These neural networks are typically trained end-to-end where the input is only text or a sequence of words and a lot of background knowledge is disregarded.", "labels": [], "entities": []}, {"text": "The importance of background knowledge in natural language understanding has long been recognized.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.6586905717849731}]}, {"text": "Earlier NLP systems mostly exploited restricted linguistic knowledge such as manually-encoded morphological and syntactic patterns.", "labels": [], "entities": []}, {"text": "With the advanced development of knowledge base construction, large amounts of semantic knowledge become available, ranging from manually annotated semantic networks like WordNet 1 to semi-automatically or automatically constructed knowledge graphs like DBPedia 2 and NELL 3 . While traditional approaches have exploited the use of these knowledge bases (KBs) in NLP tasks, they require a lot of task-specific engineering to achieve good performance.", "labels": [], "entities": []}, {"text": "One way to leverage KBs in recurrent neural networks is by augmenting the dense representations of the networks with the symbolic features derived from KBs.", "labels": [], "entities": []}, {"text": "This is not ideal as the symbolic features have poor generalization ability.", "labels": [], "entities": []}, {"text": "In addition, they can be highly sparse, e.g., using WordNet synsets can easily produce millions of indicator features, leading to high computational cost.", "labels": [], "entities": []}, {"text": "Furthermore, the usefulness of knowledge features varies across contexts, as general KBs involve polysemy, e.g., \"Clinton\" can refer to a person or a town.", "labels": [], "entities": []}, {"text": "Incorporating KBs irrespective of the textual context could mislead the machine reading process.", "labels": [], "entities": []}, {"text": "Can we train a recurrent neural network that learns to adaptively leverage knowledge from KBs to improve machine reading?", "labels": [], "entities": [{"text": "machine reading", "start_pos": 105, "end_pos": 120, "type": "TASK", "confidence": 0.7696672081947327}]}, {"text": "In this paper, we propose KBLSTM, an extension to bidirec-tional Long Short-Term Memory neural networks (BiLSTMs)) that is capable of leveraging symbolic knowledge from KBs as it processes each word in the text.", "labels": [], "entities": []}, {"text": "At each time step, the model retrieves KB concepts that are potentially related to the current word.", "labels": [], "entities": []}, {"text": "Then, an attention mechanism is employed to dynamically model their semantic relevance to the reading context.", "labels": [], "entities": []}, {"text": "Furthermore, we introduce a sentinel component in BiLSTMs that allows flexibility in deciding whether to attend to background knowledge or not.", "labels": [], "entities": []}, {"text": "This is crucial because in some cases the text context should override the context-independent background knowledge available in general KBs.", "labels": [], "entities": []}, {"text": "In this work, we leverage two general, readily available knowledge bases: WordNet (WordNet, 2010) and NELL ( . WordNet is a manually created lexical database that organizes a large number of English words into sets of synonyms (i.e. synsets) and records conceptual relations (e.g., hypernym, part of) among them.", "labels": [], "entities": [{"text": "WordNet (WordNet, 2010", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.8176177859306335}, {"text": "NELL", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.49051356315612793}, {"text": "WordNet is a manually created lexical database that organizes a large number of English words into sets of synonyms (i.e. synsets) and records conceptual relations (e.g., hypernym, part of) among them", "start_pos": 111, "end_pos": 311, "type": "Description", "confidence": 0.8193064750851812}]}, {"text": "NELL is an automatically constructed webbased knowledge base that stores beliefs about entities.", "labels": [], "entities": []}, {"text": "It is organized based on an ontology of hundreds of semantic categories (e.g., person, fruit, sport) and relations (e.g., personPlaysInstrument).", "labels": [], "entities": []}, {"text": "We learn distributed representations (i.e., embeddings) of WordNet and NELL concepts using knowledge graph embedding methods.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9352944493293762}]}, {"text": "We then integrate these learned embeddings with the state vectors of the BiLSTM network to enable knowledge-aware predictions.", "labels": [], "entities": [{"text": "BiLSTM network", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.8878856301307678}]}, {"text": "We evaluate the proposed model on two core information extraction tasks: entity extraction and event extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.7213484346866608}, {"text": "entity extraction", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7970081269741058}, {"text": "event extraction", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7323033064603806}]}, {"text": "For entity extraction, the model needs to recognize all mentions of entities such as person, organization, location, and other things from text.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7499074041843414}]}, {"text": "For event extraction, the model is required to identify event mentions or event triggers 4 that express certain types of events, e.g., elections, attacks, and travels.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7666345536708832}]}, {"text": "Both tasks are challenging and often require the combination of background knowledge and the text context for accurate prediction.", "labels": [], "entities": []}, {"text": "For example, in the sentence \"Maigret left viewers in tears.\", knowing that \"Maigret\" can refer to a TV show can greatly help disambiguate its meaning.", "labels": [], "entities": []}, {"text": "However, knowledge bases may hurt performance if used blindly.", "labels": [], "entities": []}, {"text": "For example, in the sentence \"Santiago is charged with murder.\", methods that rely heavily on KBs are likely to interpret \"Santiago\" as a location due to the popular use of Santiago as a city.", "labels": [], "entities": []}, {"text": "Similarly for events, the same word can trigger different types of events, for example, \"release\" can be used to describe different events ranging from book publishing to parole.", "labels": [], "entities": []}, {"text": "It is important for machine learning models to learn to decide which knowledge from KBs is relevant given the context.", "labels": [], "entities": []}, {"text": "Extensive experiments demonstrate that our KBLSTM models effectively leverage background knowledge from KBs in training BiLSTM networks for machine reading.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 140, "end_pos": 155, "type": "TASK", "confidence": 0.8275243937969208}]}, {"text": "They achieve significant improvement on both entity and event extraction compared to traditional feature-based methods and LSTM networks that disregard knowledge in KBs, resulting in new state-of-the-art results for entity extraction and event extraction on the widely used ACE2005 dataset.", "labels": [], "entities": [{"text": "entity and event extraction", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.668330729007721}, {"text": "entity extraction", "start_pos": 216, "end_pos": 233, "type": "TASK", "confidence": 0.7598420977592468}, {"text": "event extraction", "start_pos": 238, "end_pos": 254, "type": "TASK", "confidence": 0.7048603892326355}, {"text": "ACE2005 dataset", "start_pos": 274, "end_pos": 289, "type": "DATASET", "confidence": 0.9847235381603241}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Entity extraction results on the ACE2005  test set with gold-standard mention boundaries.", "labels": [], "entities": [{"text": "Entity extraction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8024163246154785}, {"text": "ACE2005  test set", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.9788312911987305}]}, {"text": " Table 2: Ablation results with different KBs.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9930034279823303}]}, {"text": " Table 3: Entity extraction results on the ACE2005  test set.", "labels": [], "entities": [{"text": "Entity extraction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7685684561729431}, {"text": "ACE2005  test set", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.9830736716588339}]}, {"text": " Table 4: Entity extraction results on the OntoNotes  5.0 test set.", "labels": [], "entities": [{"text": "Entity extraction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7481616735458374}, {"text": "OntoNotes  5.0 test set", "start_pos": 43, "end_pos": 66, "type": "DATASET", "confidence": 0.9292039424180984}]}, {"text": " Table 5: event extraction results on the ACE2005  test set.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7483685910701752}, {"text": "ACE2005  test set", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.9835833509763082}]}]}