{"title": [{"text": "VERB PHYSICS: Relative Physical Knowledge of Actions and Objects", "labels": [], "entities": [{"text": "VERB PHYSICS", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.655742734670639}, {"text": "Relative Physical Knowledge of Actions and Objects", "start_pos": 14, "end_pos": 64, "type": "TASK", "confidence": 0.7965307576315743}]}], "abstractContent": [{"text": "Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious , e.g., \"My house is bigger than me.\"", "labels": [], "entities": []}, {"text": "However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world.", "labels": [], "entities": []}, {"text": "For example, a statement like, \"Tyler entered his house\" implies that his house is bigger than Tyler.", "labels": [], "entities": []}, {"text": "In this paper, we present an approach to infer relative physical knowledge of actions and objects along five dimensions (e.g., size, weight, and strength) from un-structured natural language text.", "labels": [], "entities": []}, {"text": "We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.7137354910373688}]}, {"text": "Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different types of knowledge improves performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reading and reasoning about natural language text often requires trivial knowledge about everyday physical actions and objects.", "labels": [], "entities": [{"text": "Reading and reasoning about natural language text", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7696255700928825}]}, {"text": "For example, given a sentence \"Shanice could fit the trophy into the suitcase,\" we can trivially infer that the trophy must be smaller than the suitcase even though it is not stated explicitly.", "labels": [], "entities": []}, {"text": "This reasoning requires knowledge about the action \"fit\"-in particular, typical preconditions that need to be satisfied in order to perform the action.", "labels": [], "entities": []}, {"text": "In addition, reasoning about the applicability of various physical actions in a given situation often requires background knowledge about objects in the world, for example, that people are usually smaller than houses, that cars generally move faster than humans walk, or that a brick probably is heavier than a feather.", "labels": [], "entities": []}, {"text": "In fact, the potential use of such knowledge about everyday actions and objects can go beyond language understanding and reasoning.", "labels": [], "entities": []}, {"text": "Many open challenges in computer vision and robotics may also benefit from such knowledge, as shown in recent work that requires visual reasoning and entailment).", "labels": [], "entities": []}, {"text": "Ideally, an AI system should acquire such knowledge through direct physical interactions with the world.", "labels": [], "entities": []}, {"text": "However, such a physically interactive system does not seem feasible in the foreseeable future.", "labels": [], "entities": []}, {"text": "In this paper, we present an approach to acquire trivial physical knowledge from unstructured natural language text as an alternative knowledge source.", "labels": [], "entities": []}, {"text": "In particular, we focus on acquiring relative physical knowledge of actions and objects organized along five dimensions: size, weight, strength, rigidness, and speed.", "labels": [], "entities": [{"text": "speed", "start_pos": 160, "end_pos": 165, "type": "METRIC", "confidence": 0.9736821055412292}]}, {"text": "illustrates example knowledge of (1) relative physical relations of object pairs and (2) physical implications of actions when applied to those object pairs.", "labels": [], "entities": []}, {"text": "While natural language text is a rich source to obtain broad knowledge about the world, compiling trivial commonsense knowledge from unstructured text is a nontrivial feat.", "labels": [], "entities": []}, {"text": "The central challenge lies in reporting bias: people rarely states the obvious, since it goes against Grice's conversational maxim on the quantity of information.", "labels": [], "entities": []}, {"text": "In this work, we demonstrate that it is possible to overcome reporting bias and still extract the unspoken knowledge from language.", "labels": [], "entities": []}, {"text": "The key insight is this: there is consistency in the way people describe how they interact with the world, which provides vital clues to reverse engineer the common knowledge shared among people.", "labels": [], "entities": []}, {"text": "More concretely, we frame knowledge acquisition as joint inference over two closely related puzzles: inferring relative physical knowledge about object pairs while simultaneously reasoning about physical implications of actions.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.7131534069776535}]}, {"text": "Importantly, four of five dimensions of knowledge in our study-weight, strength, rigidness, and speed-are either not visual or not easily recognizable by image recognition using currently available computer vision techniques.", "labels": [], "entities": [{"text": "speed-are", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9863644242286682}]}, {"text": "Thus, our work provides unique value to complement recent attempts to acquire commonsense knowledge from web images (.", "labels": [], "entities": []}, {"text": "In sum, our contributions are threefold: \u2022 We introduce anew task in the domain of commonsense knowledge extraction from language, focusing on the physical implications of actions and the relative physical relations among objects, organized along five dimensions.", "labels": [], "entities": [{"text": "commonsense knowledge extraction from language", "start_pos": 83, "end_pos": 129, "type": "TASK", "confidence": 0.8420592427253724}]}, {"text": "\u2022 We propose a model that can infer relations over grounded object pairs together with first order relations implied by physical verbs.", "labels": [], "entities": []}, {"text": "\u2022 We develop anew dataset VERBPHYSICS that compiles crowdsourced knowledge of actions and objects.", "labels": [], "entities": []}, {"text": "1 The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first provide the formal definition of knowledge we aim to learn in Section 2.", "labels": [], "entities": []}, {"text": "We then describe our data collection in Section 3 and present our inference model in Section 4.", "labels": [], "entities": []}, {"text": "Empirical results are given in Section 5 and discussed in Section 6.", "labels": [], "entities": []}, {"text": "We review related work in Section 7 and conclude in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "Factor Graph Construction: We first need to pick a set of frames and objects to determine our set of random variables.", "labels": [], "entities": [{"text": "Factor Graph Construction", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.670464406410853}]}, {"text": "The frames are simply the subset of the frames that were crowdsourced in the given configuration (e.g., seed + dev), with \"soft 1\" unary seed factors (the gold label indexed row of the binary factor matrix) given only to those in the seed set.", "labels": [], "entities": []}, {"text": "The same selection criteria and seed factors are applied to the crowdsourced object pairs.", "labels": [], "entities": []}, {"text": "For lexical similarity factors (\u03c8 v , \u03c8 o ), we pick connections based on the cosine similarity scores of GloVe vectors thresholded above a value chosen based on development set performance.", "labels": [], "entities": []}, {"text": "Attribute similarity factors (\u03c8 a ) are chosen based on sets of frames that reach largely the same decisions on the seed data (95%: Example model predictions on dev set frames.", "labels": [], "entities": []}, {"text": "The model's confidence is shown by the bars on the right.", "labels": [], "entities": [{"text": "confidence", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9791865348815918}]}, {"text": "The correct relation is highlighted in orange (6-10 are failure cases for the model).", "labels": [], "entities": []}, {"text": "If there are two blanks, the relation is between them.", "labels": [], "entities": []}, {"text": "If there is only one blank, the relation is between PERSON and the blank.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9949619770050049}]}, {"text": "Note that receives miniscule weight because it is never the correct value for frames in the seed set.", "labels": [], "entities": []}, {"text": "factors (\u03c8 s ) are picked by using a threshold (also tuned on the development set) of pointwise mutual information (PMI) between the frames and the object pairs' occurrences in the Google Syntax Ngram corpus.", "labels": [], "entities": [{"text": "pointwise mutual information (PMI)", "start_pos": 86, "end_pos": 120, "type": "METRIC", "confidence": 0.6891571978727976}, {"text": "Google Syntax Ngram corpus", "start_pos": 181, "end_pos": 207, "type": "DATASET", "confidence": 0.716237910091877}]}, {"text": "For each task, we consider the set of factors to include in each model a hyperparameter, which is also tuned on the development set.", "labels": [], "entities": []}, {"text": "Baselines: Baselines include making a RAN-DOM choice, picking between >, <, and ), picking the MAJORITY label, and a maximum entropy classifier based on the embedding representations (EMB-MAXENT) defined in Section 4.6.", "labels": [], "entities": [{"text": "MAJORITY", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.7799389958381653}]}, {"text": "Inferring Knowledge of Actions: Our first experiment is to predict knowledge implied by new frames.", "labels": [], "entities": []}, {"text": "In this task, 5% of the frames are available as seed knowledge.", "labels": [], "entities": []}, {"text": "We experiment with two different sets of seed knowledge for the object pair data: OUR MODEL (A) uses only 5% of the object pair data as seed, and OUR MODEL (B) uses 20%.", "labels": [], "entities": [{"text": "OUR MODEL", "start_pos": 82, "end_pos": 91, "type": "DATASET", "confidence": 0.4949783533811569}]}, {"text": "The full results for the baseline methods and our model are given in the upper half of: Ablation results on size attribute for the frame prediction task on the development dataset for OUR MODEL (A) (5% of the object pairs as seed data).", "labels": [], "entities": [{"text": "Ablation", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9986506104469299}, {"text": "frame prediction task", "start_pos": 131, "end_pos": 152, "type": "TASK", "confidence": 0.778614471356074}]}, {"text": "We find that different graph configurations improve performance for different tasks and data amounts.", "labels": [], "entities": []}, {"text": "In this setting, frame and attribute similarity factors hindered performance.", "labels": [], "entities": []}, {"text": "Ablations are given in, and sample correct predictions from the development set are shown in examples 1-5 of.", "labels": [], "entities": [{"text": "Ablations", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9815917015075684}]}, {"text": "Inferring Knowledge of Objects: Our second experiment is to predict the correct relations of new object pairs.", "labels": [], "entities": []}, {"text": "The data for this task is the inverse of before: 5% of the object pairs are available as seed knowledge, and we experiment with both 5% (OUR MODEL (A)) and 20% (OUR MODEL (B)) frames given as seed data.", "labels": [], "entities": []}, {"text": "Again, both are independently tuned on the development data.", "labels": [], "entities": []}, {"text": "Results for this task are presented in the lower half of.", "labels": [], "entities": []}, {"text": "While OUR MODEL (A) is competitive with the strongest baseline, introducing the additional frame data allows OUR MODEL (B) to reach the highest accuracy.", "labels": [], "entities": [{"text": "OUR MODEL", "start_pos": 6, "end_pos": 15, "type": "DATASET", "confidence": 0.5230568200349808}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9964074492454529}]}], "tableCaptions": [{"text": " Table 2: Accuracy of baselines and our model on both tasks. Top: frame prediction task; bottom:  object pair prediction task. In both tasks 5% of in-domain data (frames or object pairs, respectively) are  available as seed data. We compare providing the other type of data (object pairs or frames, respectively)  as seed knowledge, trying 5% (OUR MODEL (A)) and 20% (OUR MODEL (B)).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9610462784767151}, {"text": "frame prediction task", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.7271660168965658}, {"text": "object pair prediction task", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.7035957351326942}, {"text": "OUR MODEL (A))", "start_pos": 344, "end_pos": 358, "type": "METRIC", "confidence": 0.8970602869987487}, {"text": "OUR MODEL (B))", "start_pos": 368, "end_pos": 382, "type": "METRIC", "confidence": 0.9222161173820496}]}, {"text": " Table 3: Ablation results on size attribute for the  frame prediction task on the development dataset  for OUR MODEL (A) (5% of the object pairs as  seed data). We find that different graph configura- tions improve performance for different tasks and  data amounts. In this setting, frame and attribute  similarity factors hindered performance.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9952204823493958}, {"text": "frame prediction task", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.7667924364407858}, {"text": "OUR MODEL (A)", "start_pos": 108, "end_pos": 121, "type": "METRIC", "confidence": 0.6501241683959961}]}]}