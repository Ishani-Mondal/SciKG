{"title": [{"text": "Abstractive Document Summarization with a Graph-Based Attentional Neural Model", "labels": [], "entities": [{"text": "Abstractive Document Summarization", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7062410910924276}]}], "abstractContent": [{"text": "ive summarization is the ultimate goal of document summarization research, but previously it is less investigated due to the immaturity of text generation techniques.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7284780442714691}, {"text": "text generation", "start_pos": 139, "end_pos": 154, "type": "TASK", "confidence": 0.7111933529376984}]}, {"text": "Recently impressive progress has been made to abstractive sentence sum-marization using neural models.", "labels": [], "entities": [{"text": "abstractive sentence sum-marization", "start_pos": 46, "end_pos": 81, "type": "TASK", "confidence": 0.6273889740308126}]}, {"text": "Unfortunately , attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets.", "labels": [], "entities": [{"text": "abstractive document summarization", "start_pos": 28, "end_pos": 62, "type": "TASK", "confidence": 0.633639524380366}]}, {"text": "In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework.", "labels": [], "entities": [{"text": "neural abstractive document summarization", "start_pos": 45, "end_pos": 86, "type": "TASK", "confidence": 0.7462644279003143}]}, {"text": "The intuition is to address the saliency factor of summarization, which has been overlooked by prior works.", "labels": [], "entities": [{"text": "summarization", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9815701246261597}]}, {"text": "Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models.", "labels": [], "entities": []}, {"text": "The data-driven neural abstractive method is also competitive with state-of-the-art ex-tractive methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Document summarization is a task to generate a fluent, condensed summary fora document, and keep important information.", "labels": [], "entities": [{"text": "Document summarization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8909185230731964}]}, {"text": "As a useful technique to alleviate the information overload people are facing today, document summarization has been extensively investigated.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.6423632055521011}]}, {"text": "Efforts on document summarization can be categorized to extractive and abstractive methods.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.5998732447624207}]}, {"text": "Extractive methods produce the summary of a document by extracting sentences from the original document.", "labels": [], "entities": []}, {"text": "They have the advantage of producing fluent sentences and preserving the meaning of original documents, but also inevitably face the drawbacks of information redundancy and incoherence between sentences.", "labels": [], "entities": []}, {"text": "Moreover, extraction is far from the way humans write summaries.", "labels": [], "entities": [{"text": "extraction", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9566437602043152}]}, {"text": "On the contrary, abstractive methods are able to generate better summaries with the use of arbitrary words and expressions, but generating abstractive summaries is much more difficult in practice.", "labels": [], "entities": [{"text": "summaries", "start_pos": 65, "end_pos": 74, "type": "TASK", "confidence": 0.9757879376411438}]}, {"text": "Abstractive summarization involves sophisticated techniques including meaning representation, content organization, and surface realization.", "labels": [], "entities": [{"text": "Abstractive summarization", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7522480487823486}, {"text": "meaning representation", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.7241118997335434}, {"text": "surface realization", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.8015095889568329}]}, {"text": "Each of these techniques has large space to be improved (.", "labels": [], "entities": []}, {"text": "Due to the immaturity of natural language generation techniques, fully abstractive approaches are still at the beginning and cannot always ensure grammatical abstracts.", "labels": [], "entities": []}, {"text": "Recent neural networks enable an end-to-end framework for natural language generation.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6720465918382009}]}, {"text": "Success has been witnessed on tasks like machine translation and image captioning, together with the abstractive sentence summarization.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.800685703754425}, {"text": "image captioning", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7323049306869507}, {"text": "abstractive sentence summarization", "start_pos": 101, "end_pos": 135, "type": "TASK", "confidence": 0.5739784439404806}]}, {"text": "Unfortunately, the extension of sentence abstractive methods to the document summarization task is not straightforward.", "labels": [], "entities": [{"text": "document summarization task", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.7736643850803375}]}, {"text": "Encoding and decoding fora long sequence of multiple sentences, currently still lack satisfactory solutions (.", "labels": [], "entities": []}, {"text": "Recent abstractive document summarization models are yet notable to achieve convincing performance, with a considerable gap from extractive methods.", "labels": [], "entities": [{"text": "abstractive document summarization", "start_pos": 7, "end_pos": 41, "type": "TASK", "confidence": 0.630366325378418}]}, {"text": "In this paper, we review the key factors of document summarization, i.e., the saliency, fluency, coherence, and novelty requirements of the generated summary.", "labels": [], "entities": []}, {"text": "Fluency is what neural generation models are naturally good at, but the other factors are less considered in previous neural abstractive models.", "labels": [], "entities": []}, {"text": "A recent study starts to consider the factor of novelty, using a distraction mechanism to avoid redundancy.", "labels": [], "entities": [{"text": "novelty", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.8918567299842834}]}, {"text": "As far as we know, however, saliency has not been addressed by existing neural abstractive models, despite its importance for summary generation.", "labels": [], "entities": [{"text": "summary generation", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.8518743515014648}]}, {"text": "In this work, we study how neural summarization models can discover the salient information of a document.", "labels": [], "entities": []}, {"text": "Inspired by the graph-based extractive summarization methods, we introduce a novel graph-based attention mechanism in the encoderdecoder framework.", "labels": [], "entities": [{"text": "graph-based extractive summarization", "start_pos": 16, "end_pos": 52, "type": "TASK", "confidence": 0.5733421246210734}]}, {"text": "Moreover, we investigate the challenges of accepting and generating long sequences for sequence-to-sequence (seq2seq) models, and propose anew hierarchical decoding algorithm with a reference mechanism to generate the abstractive summaries.", "labels": [], "entities": []}, {"text": "The proposed method is able to tackle the constraints of saliency, nonredundancy, information correctness, and fluency under a unified framework.", "labels": [], "entities": [{"text": "information correctness", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.8035975992679596}]}, {"text": "We conduct experiments on two large-scale corpora with human generated summaries.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that our approach consistently outperforms previous neural abstractive summarization models, and is also competitive with state-of-the-art extractive methods.", "labels": [], "entities": [{"text": "neural abstractive summarization", "start_pos": 85, "end_pos": 117, "type": "TASK", "confidence": 0.6692374050617218}]}, {"text": "We organize the paper as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces related work.", "labels": [], "entities": []}, {"text": "Section 3 describes our method.", "labels": [], "entities": []}, {"text": "In Section 4 we present the experiments and have discussion.", "labels": [], "entities": []}, {"text": "Finally in Section 5 we conclude this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on two large-scale corpora of CNN and DailyMail, which have been widely used in neural document summarization tasks.", "labels": [], "entities": [{"text": "CNN", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.9528493881225586}, {"text": "DailyMail", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.8412113189697266}, {"text": "neural document summarization tasks", "start_pos": 103, "end_pos": 138, "type": "TASK", "confidence": 0.6779865846037865}]}, {"text": "The corpora are originally constructed in () by collecting human generated abstractive highlights from the news stories in the CNN and DailyMail website.", "labels": [], "entities": [{"text": "CNN and DailyMail website", "start_pos": 127, "end_pos": 152, "type": "DATASET", "confidence": 0.8358364999294281}]}, {"text": "The statistics and split of the two datasets are listed in.", "labels": [], "entities": []}, {"text": "We adopt the widely used ROUGE) toolkit for evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9362810254096985}]}, {"text": "We first compare with the reported results in) including various traditional extractive methods and a state-of-the-art abstractive model (Distraction-M3) on the CNN dataset, as shown in.", "labels": [], "entities": [{"text": "CNN dataset", "start_pos": 161, "end_pos": 172, "type": "DATASET", "confidence": 0.9728067815303802}]}, {"text": "Uni-GRU is a non-hierarchical seq2seq baseline model.", "labels": [], "entities": []}, {"text": "In we compare our method with the results of state-of-the-art neural summarization methods reported in recent papers.", "labels": [], "entities": [{"text": "neural summarization", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.6044415235519409}]}, {"text": "Extractive models include NN-SE ( and SummaRuNNer (, while SummaRuNNer-abs is also an extractive model similar to SummaRuNNer but is trained directly on the abstractive summaries.", "labels": [], "entities": []}, {"text": "Moreover, we include several baselines for comparison, including the baselines reported in although they are tested on 500 samples of the test set.", "labels": [], "entities": []}, {"text": "LREG is a feature based method using linear regression.", "labels": [], "entities": [{"text": "LREG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6954371929168701}]}, {"text": "NN-ABS is a neural abstractive baseline which is a simple hierarchical extension of.", "labels": [], "entities": []}, {"text": "NN-WE is the abstractive model which restricts the generation of words from the original document.", "labels": [], "entities": [{"text": "NN-WE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8004826307296753}]}, {"text": "Lead-3 is a strong extractive baseline that uses the lead three sentences as the summary.", "labels": [], "entities": []}, {"text": "In we compare our model with the abstractive attentional encoder-decoder models in   (, which leverage several effective techniques and achieve state-of-the-art performance on sentence abstractive summarization tasks.", "labels": [], "entities": [{"text": "sentence abstractive summarization", "start_pos": 176, "end_pos": 210, "type": "TASK", "confidence": 0.7494147519270579}]}, {"text": "The words-lvt2k and words-lvt2k-ptr are flat models and words-lvt2k-hieratt is a hierarchical extension.", "labels": [], "entities": []}, {"text": "Results in show our abstractive method is able to outperform traditional extractive methods and the distraction-based abstractive model.", "labels": [], "entities": []}, {"text": "The results in show that our method has considerable improvement over neural abstractive baselines, and is able to outperform stateof-the-art neural extractive methods.", "labels": [], "entities": []}, {"text": "An interesting observation is the results of the hierarchical model in are lower than the flat models, which may demonstrate the difficulty fora traditional attention model to identify the important information in a document.", "labels": [], "entities": []}, {"text": "We also conducted human evaluation on 20 random samples from the DailyMail test set and compared the summaries generated by our method with the outputs of Lead-3, NN-SE (Cheng and", "labels": [], "entities": [{"text": "DailyMail test set", "start_pos": 65, "end_pos": 83, "type": "DATASET", "confidence": 0.991133987903595}]}], "tableCaptions": [{"text": " Table 1: The statistics of the two datasets. D.L.  and S.L. indicate the average number of sentences  in the document and summary, respectively.", "labels": [], "entities": []}, {"text": " Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.", "labels": [], "entities": [{"text": "CNN test set", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9778937896092733}, {"text": "F1 variants of Rouge", "start_pos": 72, "end_pos": 92, "type": "DATASET", "confidence": 0.8794002085924149}]}, {"text": " Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.", "labels": [], "entities": [{"text": "DailyMail test  set", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.978270689646403}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.8191207051277161}]}, {"text": " Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.", "labels": [], "entities": [{"text": "CNN/DailyMail test set", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.9356400966644287}, {"text": "F1", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.9846257567405701}]}, {"text": " Table 5: Human evaluation results.", "labels": [], "entities": []}, {"text": " Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).", "labels": [], "entities": [{"text": "CNN test set", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.9729954401652018}]}]}