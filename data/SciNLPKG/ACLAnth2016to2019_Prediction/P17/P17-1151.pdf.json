{"title": [], "abstractContent": [{"text": "Word embeddings provide point representations of words containing useful semantic information.", "labels": [], "entities": []}, {"text": "We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, en-tailment, and rich uncertainty information.", "labels": [], "entities": []}, {"text": "To learn these distributions, we propose an energy-based max-margin objective.", "labels": [], "entities": []}, {"text": "We show that the resulting approach captures uniquely expressive semantic information , and outperforms alternatives, such as word2vec skip-grams, and Gaus-sian embeddings, on benchmark datasets such as word similarity and entailment.", "labels": [], "entities": []}], "introductionContent": [{"text": "To model language, we must represent words.", "labels": [], "entities": []}, {"text": "We can imagine representing every word with a binary one-hot vector corresponding to a dictionary position.", "labels": [], "entities": []}, {"text": "But such a representation contains no valuable semantic information: distances between word vectors represent only differences in alphabetic ordering.", "labels": [], "entities": []}, {"text": "Modern approaches, by contrast, learn to map words with similar meanings to nearby points in a vector space, from large datasets such as Wikipedia.", "labels": [], "entities": []}, {"text": "These learned word embeddings have become ubiquitous in predictive tasks.", "labels": [], "entities": [{"text": "predictive tasks", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.9090543985366821}]}, {"text": "recently proposed an alternative view, where words are represented by a whole probability distribution instead of a deterministic point vector.", "labels": [], "entities": []}, {"text": "Specifically, they model each word by a Gaussian distribution, and learn its mean and covariance matrix from data.", "labels": [], "entities": []}, {"text": "This approach generalizes any deterministic point embedding, which can be fully captured by the mean vector of the Gaussian distribution.", "labels": [], "entities": []}, {"text": "Moreover, the full distribution provides much richer information than point estimates for characterizing words, representing probability mass and uncertainty across a set of semantics.", "labels": [], "entities": []}, {"text": "However, since a Gaussian distribution can have only one mode, the learned uncertainty in this representation can be overly diffuse for words with multiple distinct meanings (polysemies), in order for the model to assign some density to any plausible semantics.", "labels": [], "entities": []}, {"text": "Moreover, the mean of the Gaussian can be pulled in many opposing directions, leading to a biased distribution that centers its mass mostly around one meaning while leaving the others not well represented.", "labels": [], "entities": []}, {"text": "In this paper, we propose to represent each word with an expressive multimodal distribution, for multiple distinct meanings, entailment, heavy tailed uncertainty, and enhanced interpretability.", "labels": [], "entities": []}, {"text": "For example, one mode of the word 'bank' could overlap with distributions for words such as 'finance' and 'money', and another mode could overlap with the distributions for 'river' and 'creek'.", "labels": [], "entities": []}, {"text": "It is our contention that such flexibility is critical for both qualitatively learning about the meanings of words, and for optimal performance on many predictive tasks.", "labels": [], "entities": []}, {"text": "In particular, we model each word with a mixture of Gaussians (Section 3.1).", "labels": [], "entities": []}, {"text": "We learn all the parameters of this mixture model using a maximum margin energy-based ranking objective), where the energy function describes the affinity between a pair of words.", "labels": [], "entities": []}, {"text": "For analytic tractability with Gaussian mixtures, we use the inner product between probability distributions in a Hilbert space, known as the expected likelihood kernel (), as our energy function (Section 3.4).", "labels": [], "entities": []}, {"text": "Additionally, we propose transformations for numerical stability and initialization A.2, resulting in a robust, straightforward, and scalable learning procedure, capable of training on a corpus with billions of words in days.", "labels": [], "entities": [{"text": "numerical stability", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8019247651100159}, {"text": "initialization A.2", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7415901720523834}]}, {"text": "We show that the model is able to automatically discover multiple meanings for words, and significantly outperform other alternative methods across several tasks such as word similarity and entailment (Section 4.4, 4.5, 4.7).", "labels": [], "entities": [{"text": "word similarity", "start_pos": 170, "end_pos": 185, "type": "TASK", "confidence": 0.7088000178337097}]}, {"text": "We have made code available at http://github.com/ benathi/word2gm, where we implement our model in Tensorflow).", "labels": [], "entities": []}], "datasetContent": [{"text": "We have introduced a model for multi-prototype embeddings, which expressively captures word meanings with whole probability distributions.", "labels": [], "entities": []}, {"text": "We show that our combination of energy and objective functions, proposed in Section 3, enables one to learn interpretable multimodal distributions through unsupervised training, for describing words with multiple distinct meanings.", "labels": [], "entities": []}, {"text": "By representing multiple distinct meanings, our model also reduces the unnecessarily large variance of a Gaussian embedding model, and has improved results on word entailment tasks.", "labels": [], "entities": [{"text": "word entailment tasks", "start_pos": 159, "end_pos": 180, "type": "TASK", "confidence": 0.8145918250083923}]}, {"text": "To learn the parameters of the proposed mixture model, we train on a concatenation of two datasets: UKWAC (2.5 billion tokens) and Wackypedia (1 billion tokens) ().", "labels": [], "entities": [{"text": "UKWAC", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.9253467917442322}]}, {"text": "We discard words that occur fewer than 100 times in the corpus, which results in a vocabulary size of 314, 129 words.", "labels": [], "entities": []}, {"text": "Our word sampling scheme, described at the end of Section 4.3, is similar to that of word2vec with one negative context word for each positive context word.", "labels": [], "entities": []}, {"text": "After training, we obtain learned parameters for each word w.", "labels": [], "entities": []}, {"text": "We treat the mean vector \u00b5 w,i as the embedding of the i th mixture component with the covariance matrix \u03a3 w,i representing its subtlety and uncertainty.", "labels": [], "entities": []}, {"text": "We perform qualitative evaluation to show that our embeddings learn meaningful multi-prototype representations and compare to existing models using a quantitative evaluation on word similarity datasets and word entailment.", "labels": [], "entities": []}, {"text": "We name our model as Word to Gaussian Mixture (w2gm) in constrast to Word to Gaussian (w2g)).", "labels": [], "entities": []}, {"text": "Unless stated otherwise, w2g refers to our implementation of w2gm model with one mixture component.", "labels": [], "entities": []}, {"text": "In, we show examples of polysemous words and their nearest neighbors in the embedding space to demonstrate that our trained embeddings capture multiple word senses.", "labels": [], "entities": []}, {"text": "For instance, a word such as 'rock' that could mean either 'stone' or 'rock music' should have each of its meanings represented by a distinct Gaussian component.", "labels": [], "entities": []}, {"text": "Our results fora mixture of two Gaussians model confirm this hypothesis, where we observe that the 0 th component of 'rock' being related to ('basalt', 'boulders') and the 1 st component being related to ('indie', 'funk', 'hip-hop').", "labels": [], "entities": []}, {"text": "Similarly, the word bank has its 0 th component representing the riverbank and the 1 st component representing the financial bank.", "labels": [], "entities": []}, {"text": "By contrast, in (bottom), see that for Gaussian embeddings with one mixture component, nearest neighbors of polysemous words are predominantly related to a single meaning.", "labels": [], "entities": []}, {"text": "For instance, 'rock' mostly has neighbors related to rock music and 'bank' mostly related to the financial bank.", "labels": [], "entities": []}, {"text": "The alternative meanings of these polysemous words are not well represented in the embeddings.", "labels": [], "entities": []}, {"text": "As a numerical example, the cosine similarity between 'rock' and 'stone' for the Gaussian representation of is only 0.029, much lower than the cosine similarity 0.586 between the 0 th component of 'rock' and 'stone' in our multimodal representation.", "labels": [], "entities": []}, {"text": "In cases where a word only has a single popular meaning, the mixture components can be fairly close; for instance, one component of 'stone' is close to and the other to, which reflects subtle variations in meanings.", "labels": [], "entities": []}, {"text": "In general, the mixture can give properties such as heavy tails and more interesting unimodal characterizations of uncertainty than could be described by a single Gaussian.", "labels": [], "entities": []}, {"text": "Embedding Visualization We provide an interactive visualization as part of our code repository: https://github.com/benathi/ word2gm#visualization that allows realtime queries of words' nearest neighbors (in the embeddings tab) for K = 1, 2, 3 components.", "labels": [], "entities": []}, {"text": "We use a notation similar to that of, where a token w:i represents the component i of a word w.", "labels": [], "entities": []}, {"text": "For instance, if in the K = 2 link we search for bank:0, we obtain the nearest neigh-bors such as river:1, confluence:0, waterway:1, which indicates that the 0 th component of 'bank' has the meaning 'river bank'.", "labels": [], "entities": []}, {"text": "On the other hand, searching for bank:1 yields nearby words such as banking:1, banker:0, ATM:0, indicating that this component is close to the 'financial bank'.", "labels": [], "entities": []}, {"text": "We also have a visualization of a unimodal (w2g) for comparison in the K = 1 link.", "labels": [], "entities": []}, {"text": "In addition, the embedding link for our Gaussian mixture model with K = 3 mixture components can learn three distinct meanings.", "labels": [], "entities": []}, {"text": "For instance, each of the three components of 'cell' is close to ('keypad', 'digits'), ('incarcerated', 'inmate') or ('tissue', 'antibody'), indicating that the distribution captures the concept of 'cellphone', 'jail cell', or 'biological cell', respectively.", "labels": [], "entities": []}, {"text": "Due to the limited number of words with more than 2 meanings, our model with K = 3 does not generally offer substantial performance differences to our model with K = 2; hence, we do not further display K = 3 results for compactness.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Spearman correlation for word similarity datasets. The models sg, w2g, w2gm denote  word2vec skip-gram, Gaussian embedding, and Gaussian mixture embedding (K=2). The measures  mc, el, me denote maximum cosine similarity, expected likelihood kernel, and minimum Euclidean  distance. For each of w2g and w2gm, we underline the similarity metric with the best score. For each  dataset, we boldface the score with the best performance across all models. The correlation scores for  sg * , w2g * are taken from", "labels": [], "entities": []}, {"text": " Table 3: Spearman's correlation (\u03c1) on WordSim- 353 datasets for our Word to Gaussian Mixture  embeddings, as well as the multi-prototype em- bedding by Huang et al. (2012) and the MSSG  model by", "labels": [], "entities": [{"text": "Spearman's correlation (\u03c1)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.7430899937947592}, {"text": "WordSim- 353 datasets", "start_pos": 40, "end_pos": 61, "type": "DATASET", "confidence": 0.9365079551935196}]}, {"text": " Table 4: Spearman's correlation \u03c1 on dataset  SCWS. We show the results for single proto- type (top) and multi-prototype (bottom) The suffix  -(S,M) refers to single and multiple prototype  models, respectively.", "labels": [], "entities": [{"text": "Spearman's correlation \u03c1", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.6258350536227226}]}, {"text": " Table 5: Entailment results for models w2g and  w2gm with window size 5 and 10. The metrics  used are the maximum cosine similarity, or the  maximum negative KL divergence. We calculate  the best average precision as well as the best F1  score. In most cases, w2gm outperforms w2g for  describing entailment.", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 115, "end_pos": 132, "type": "METRIC", "confidence": 0.8425727486610413}, {"text": "precision", "start_pos": 205, "end_pos": 214, "type": "METRIC", "confidence": 0.9613196849822998}, {"text": "F1  score", "start_pos": 235, "end_pos": 244, "type": "METRIC", "confidence": 0.9796615540981293}]}]}