{"title": [{"text": "A Syntactic Neural Model for General-Purpose Code Generation", "labels": [], "entities": [{"text": "General-Purpose Code Generation", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.6566492021083832}]}], "abstractContent": [{"text": "We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python.", "labels": [], "entities": [{"text": "parsing natural language descriptions", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.8968327045440674}]}, {"text": "Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language.", "labels": [], "entities": [{"text": "language generation", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7508794367313385}]}, {"text": "Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7747240364551544}]}, {"text": "Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions , achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 207, "end_pos": 223, "type": "TASK", "confidence": 0.7444669604301453}]}], "introductionContent": [{"text": "Every programmer has experienced the situation where they know what they want to do, but do not have the ability to turn it into a concrete implementation.", "labels": [], "entities": []}, {"text": "For example, a Python programmer may want to \"sort my list in descending order,\" but not be able to come up with the proper syntax sorted(my list, reverse=True) to realize his intention.", "labels": [], "entities": []}, {"text": "To resolve this impasse, it is common for programmers to search the web in natural language (NL), find an answer, and modify it into the desired form (.", "labels": [], "entities": []}, {"text": "However, this is time-consuming, and thus the software engineering literature is ripe with methods to directly generate code from NL descriptions, mostly with hand-engineered methods highly tailored to specific programming languages.", "labels": [], "entities": []}, {"text": "In parallel, the NLP community has developed methods for data-driven semantic parsing, which attempt to map NL to structured logical forms executable by computers.", "labels": [], "entities": [{"text": "data-driven semantic parsing", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.7037900487581888}]}, {"text": "These logical forms can be general-purpose meaning representations, formalisms for querying knowledge bases) and instructions for robots or personal assistants), among others.", "labels": [], "entities": []}, {"text": "While these methods have the advantage of being learnable from data, compared to the programming languages (PLs) in use by programmers, the domain-specific languages targeted by these works have a schema and syntax that is relatively simple.", "labels": [], "entities": []}, {"text": "Recently,  have proposed a data-driven code generation method for high-level, general-purpose PLs like Python and Java.", "labels": [], "entities": [{"text": "data-driven code generation", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.6722978750864664}]}, {"text": "This work treats code generation as a sequence-tosequence modeling problem, and introduce methods to generate words from character-level models, and copy variable names from input descriptions.", "labels": [], "entities": [{"text": "code generation", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.7488294541835785}]}, {"text": "However, unlike most work in semantic parsing, it does not consider the fact that code has to be well-defined programs in the target syntax.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7348478585481644}]}, {"text": "In this work, we propose a data-driven syntaxbased neural network model tailored for generation of general-purpose PLs like Python.", "labels": [], "entities": []}, {"text": "In order to capture the strong underlying syntax of the PL, we define a model that transduces an NL statement into an Abstract Syntax Tree (AST;, \u00a7 2) for the target PL.", "labels": [], "entities": []}, {"text": "ASTs can be deterministically generated for all well-formed programs using standard parsers provided by the PL, and thus give us away to obtain syntax information with minimal engineering.", "labels": [], "entities": [{"text": "ASTs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9036317467689514}]}, {"text": "Once we generate an AST, we can use deterministic generation tools to convert the AST into surface code.", "labels": [], "entities": [{"text": "AST", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.950191855430603}]}, {"text": "We hypothesize: Example production rules for common Python statements that such a structured approach has two benefits.", "labels": [], "entities": []}, {"text": "First, we hypothesize that structure can be used to constrain our search space, ensuring generation of well-formed code.", "labels": [], "entities": []}, {"text": "To this end, we propose a syntax-driven neural code generation model.", "labels": [], "entities": [{"text": "syntax-driven neural code generation", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.6083941459655762}]}, {"text": "The backbone of our approach is a grammar model ( \u00a7 3) which formalizes the generation story of a derivation AST into sequential application of actions that either apply production rules ( \u00a7 3.1), or emit terminal tokens ( \u00a7 3.2).", "labels": [], "entities": []}, {"text": "The underlying syntax of the PL is therefore encoded in the grammar model a priori as the set of possible actions.", "labels": [], "entities": []}, {"text": "Our approach frees the model from recovering the underlying grammar from limited training data, and instead enables the system to focus on learning the compositionality among existing grammar rules.", "labels": [], "entities": []}, {"text": "have noted that this imposition of structure on neural models is useful for semantic parsing, and we expect this to be even more important for general-purpose PLs where the syntax trees are larger and more complex.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.790795624256134}]}, {"text": "Second, we hypothesize that structural information helps to model information flow within the neural network, which naturally reflects the recursive structure of PLs.", "labels": [], "entities": []}, {"text": "To test this, we extend a standard recurrent neural network (RNN) decoder to allow for additional neural connections which reflect the recursive structure of an AST.", "labels": [], "entities": [{"text": "AST", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.9280388355255127}]}, {"text": "As an example, when expanding the node ? in, we make use of the information from both its parent and left sibling (the dashed rectangle).", "labels": [], "entities": []}, {"text": "This enables us to locally pass information of relevant code segments via neural network connections, resulting in more confident predictions.", "labels": [], "entities": []}, {"text": "Experiments ( \u00a7 5) on two Python code generation tasks show 11.7% and 9.3% absolute improvements inaccuracy against the state-of-the-art system ( . Our model also gives competitive performance on a standard semantic parsing benchmark 1 . 1 Implementation available at https://github.", "labels": [], "entities": [{"text": "Python code generation tasks", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.7098811939358711}, {"text": "semantic parsing", "start_pos": 207, "end_pos": 223, "type": "TASK", "confidence": 0.7362633347511292}, {"text": "Implementation", "start_pos": 240, "end_pos": 254, "type": "METRIC", "confidence": 0.9381290078163147}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Statistics of datasets and associated grammars", "labels": [], "entities": []}, {"text": " Table 3: Results on two Python code generation tasks.   \u2020 Results previously reported in Ling et al. (2016).", "labels": [], "entities": [{"text": "Python code generation tasks", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.6777025610208511}]}, {"text": " Table 4: Results on the noise-filtered IFTTT test set of \">3  agree with gold annotations\" (averaged over three runs), our  model performs competitively among neural models.", "labels": [], "entities": [{"text": "IFTTT test set", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.7538134356339773}]}]}