{"title": [{"text": "English Multiword Expression-aware Dependency Parsing Including Named Entities", "labels": [], "entities": [{"text": "Multiword Expression-aware Dependency Parsing Including Named Entities", "start_pos": 8, "end_pos": 78, "type": "TASK", "confidence": 0.6719745482717242}]}], "abstractContent": [{"text": "Because syntactic structures and spans of multiword expressions (MWEs) are independently annotated in many English syntactic corpora, they are generally inconsistent with respect to one another, which is harmful to the implementation of an aggregate system.", "labels": [], "entities": [{"text": "syntactic structures and spans of multiword expressions (MWEs)", "start_pos": 8, "end_pos": 70, "type": "TASK", "confidence": 0.6573153614997864}]}, {"text": "In this work, we construct a corpus that ensures consistency between dependency structures and MWEs, including named entities.", "labels": [], "entities": []}, {"text": "Further, we explore models that predict both MWE-spans and an MWE-aware dependency structure.", "labels": [], "entities": []}, {"text": "Experimental results show that our joint model using additional MWE-span features achieves an MWE recognition improvement of 1.35 points over a pipeline model.", "labels": [], "entities": [{"text": "MWE recognition", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.6802312433719635}]}], "introductionContent": [{"text": "To solve complex Natural Language Processing (NLP) tasks that require deep syntactic analysis, various levels of annotation such as parse trees and named entities (NEs) must be consistent with one another (.", "labels": [], "entities": []}, {"text": "Otherwise, it is usually impossible to combine these pieces of information effectively.", "labels": [], "entities": []}, {"text": "However, the standard syntactic corpus of English, Penn Treebank, is not concerned with consistency between syntactic trees and spans of multiword expressions.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9877147376537323}]}, {"text": "In Penn Treebank, that is, an MWE-span does not always correspond to a span dominated by a single non-terminal node.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 3, "end_pos": 16, "type": "DATASET", "confidence": 0.9938015639781952}]}, {"text": "Therefore, word-based dependency structures converted from Penn Treebank are generally inconsistent with MWE-spans ().", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.9947588741779327}, {"text": "MWE-spans", "start_pos": 105, "end_pos": 114, "type": "DATASET", "confidence": 0.7402362823486328}]}, {"text": "To mitigate this inconsistency, estab-(a) a word-based dependency structure (b) an MWE-aware dependency structure: A word-based and an MWE-aware dependency structure.", "labels": [], "entities": []}, {"text": "In the former, a span of an MWE (\"a number of\") does not correspond to any subtree.", "labels": [], "entities": []}, {"text": "The MWE is represented as a single node in the latter structure.", "labels": [], "entities": []}, {"text": "lishes each span of functional MWEs 1 as a subtree of a phrase structure in the Wall Street Journal portion of Ontonotes ().", "labels": [], "entities": [{"text": "Wall Street Journal portion of Ontonotes", "start_pos": 80, "end_pos": 120, "type": "DATASET", "confidence": 0.9346038103103638}]}, {"text": "To pursue this direction further, we construct a corpus such that dependency structures are consistent with MWEs, by extending's corpus 2 . As is the case with their corpus, each MME is a syntactic unit in an MWE-aware dependency structure from our corpus.", "labels": [], "entities": []}, {"text": "Moreover, our corpus includes not only functional MWEs but also NEs.", "labels": [], "entities": []}, {"text": "Because NEs are highly productive and occur more frequently than functional MWEs, they are difficult to cover in a dictionary.", "labels": [], "entities": []}, {"text": "Consistency between NE-spans and phrase structures is not guaranteed because they are independently annotated inmost syntactic corpora.", "labels": [], "entities": []}, {"text": "By functional MWEs, we mean MWEs that function either as prepositions, conjunctions, determiners, pronouns, or adverbs.", "labels": [], "entities": []}, {"text": "We release our dependency corpus at https: //github.com/naist-cl-parsing/ mwe-aware-dependency.", "labels": [], "entities": []}, {"text": "MWE-aware phrase structures will be distributed from LDC as apart of LDC2017T01.", "labels": [], "entities": [{"text": "LDC2017T01", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.9348235130310059}]}, {"text": "For instance, in, an NE-span is \"Board of Investment,\" which is inconsistent with the syntactic tree.", "labels": [], "entities": []}, {"text": "Therefore, we resolve this inconsistency by modifying phrase structures locally and establishing each NE as a subtree.", "labels": [], "entities": []}], "datasetContent": [{"text": "We split the Wall Street Journal (WSJ) portion of Ontonotes, using sections 2-21 for training, and section 23 for testing.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) portion of Ontonotes", "start_pos": 13, "end_pos": 59, "type": "DATASET", "confidence": 0.9189941949314542}]}, {"text": "For all models, we used . For the pipeline model, we converted each concatenated token corresponding to an MWE into a head-initial structure and compared this with the gold tree.", "labels": [], "entities": []}, {"text": "For the joint model, we directly compared a predicted tree with the gold tree.", "labels": [], "entities": []}, {"text": "To evaluate MWE recognition, we used the F-measure for untagged / tagged MWEs (FUM/FTM) . For the pipeline model, we compared the gold MWEs with predictions by CRF.", "labels": [], "entities": [{"text": "MWE recognition", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.984372079372406}, {"text": "F-measure", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9946045279502869}, {"text": "FUM/FTM)", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.8350394368171692}, {"text": "CRF", "start_pos": 160, "end_pos": 163, "type": "DATASET", "confidence": 0.9123885631561279}]}, {"text": "For the proposed joint model, we compared the gold MWEs with predicted MWE-spans and We used 20-way jackknifing for the training split.", "labels": [], "entities": []}, {"text": "The test split was automatically tagged by the POS tagger trained on the training split.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.5923890620470047}]}, {"text": "We used 20-way jackknifing for the training split.", "labels": [], "entities": [{"text": "training split", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.7144789695739746}]}, {"text": "The test split was automatically tagged by the sequential labeler trained on the training split.", "labels": [], "entities": []}, {"text": "When calculating UAS/LAS, we removed punctuation.", "labels": [], "entities": [{"text": "UAS/LAS", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.46999139587084454}]}, {"text": "13 FUM only focuses on MWE-spans, whereas FTM focuses on both MWE-spans and MWE POS tags.", "labels": [], "entities": [{"text": "13", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8329017758369446}, {"text": "FUM", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.4784766733646393}, {"text": "FTM", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.9106041193008423}]}, {"text": "MWE POS tags represented as dependency labels.", "labels": [], "entities": [{"text": "MWE POS tags", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.7471175392468771}]}, {"text": "We present the experimental results in.", "labels": [], "entities": []}, {"text": "Comparing the joint model with the pipeline model, there is not much difference between these models regarding UAS / LAS for all sentences.", "labels": [], "entities": [{"text": "UAS / LAS", "start_pos": 111, "end_pos": 120, "type": "TASK", "confidence": 0.47262770930926007}]}, {"text": "However, the former is 2.13 / 0.48 points worse than the latter in terms of UAS / LAS regarding the first tokens of MWEs (1269 in 34,526 tokens), and 2.37 / 2.53 points worse than the latter regarding FUM / FTM.", "labels": [], "entities": [{"text": "UAS / LAS", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.6812337239583334}, {"text": "FUM / FTM", "start_pos": 201, "end_pos": 210, "type": "DATASET", "confidence": 0.7026457587877909}]}, {"text": "These results suggest that the performance of the joint model with no additional features at predicting dependencies inside and around MWEs is worse than the pipeline model.", "labels": [], "entities": []}, {"text": "One of the reasons for this is that the exploitation of headinitial structures in the joint model) involves the addition of MWE-specific labels.", "labels": [], "entities": []}, {"text": "This results in an increase in the total number of dependency labels from 41 to 50.", "labels": [], "entities": []}, {"text": "Because of this broader output space, more search errors can occur in the joint model compared with the pipeline model.", "labels": [], "entities": []}, {"text": "Moreover, a breakdown by type of MWE shows that most differences in performance between these two models are related to functional MWEs.", "labels": [], "entities": []}, {"text": "These results suggest that constraints regarding functional MWEs during parsing (3.2) are harmful to the joint model with no additional features in terms of its performance with respect to functional MWEs.", "labels": [], "entities": [{"text": "parsing", "start_pos": 72, "end_pos": 79, "type": "TASK", "confidence": 0.9436758160591125}]}, {"text": "By adding MWE-specific features to the joint model, however, we observe at least a 2.52 / 3.00 point improvement in terms of UAS / LAS regarding the first tokens of MWEs, and a 2.90 / 2.99 point improvement regarding FUM / FTM.", "labels": [], "entities": [{"text": "UAS / LAS", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.8946274320284525}, {"text": "FUM / FTM", "start_pos": 217, "end_pos": 226, "type": "DATASET", "confidence": 0.46976996461550397}]}, {"text": "As a result, we obtain a 1.35 / 1.28 point improvement with joint(+pred span) compared with the pipeline model in terms of FUM / FTM.", "labels": [], "entities": [{"text": "pred span", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.966059148311615}, {"text": "FUM", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.476265549659729}, {"text": "FTM", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.40944844484329224}]}, {"text": "A breakdown by type of MWE shows that the addition of MWEspecific features leads to a performance improvement, especially for functional MWEs.", "labels": [], "entities": []}, {"text": "These results suggest that MWE-specific features are effective at both MWE recognition through dependency parsing and the prediction of dependencies connecting inside and outside of MWEs.", "labels": [], "entities": [{"text": "MWE recognition", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.9806181490421295}, {"text": "dependency parsing", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.812333732843399}]}, {"text": "Comparing the joint(+pred span) with the joint(+dict), the former is 0.40 / 0.55 points better than the latter in terms of UAS / LAS regarding the first tokens of MWEs, and 0.82 / 0.82 points better than the latter regarding FUM / FTM.", "labels": [], "entities": [{"text": "UAS / LAS", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.6799487471580505}, {"text": "FUM / FTM", "start_pos": 225, "end_pos": 234, "type": "DATASET", "confidence": 0.7492121656735738}]}, {"text": "We can attribute this gain in performance to the additional features extracted from more accurate predictions of MWE-spans and MWE POS tags by CRF than those by dictionary matching.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Histogram tabling the consistency be- tween MWE-spans and phrase structures. annotations on Ontonotes 4 into phrase structures  such that functional MWEs are established as sub- trees. Subsequently, we convert phrase structures  to dependency structures. We construct our corpus  by extending Kato et al. (2016)'s corpus 5 , which  is itself built on a corpus by Shigeto et al. (2013).  Regarding MWE annotations, Shigeto et al. (2013)  first constructed an MWE dictionary by extract- ing functional MWEs from the English-language  Wiktionary 6 , and classified their occurrences in  Ontonotes into either MWE or literal usage. Kato  et al. (2016) integrated these MWE annotations  into phrase structures and established functional  MWEs as subtrees.", "labels": [], "entities": []}, {"text": " Table 3: Experimental results on the test set.", "labels": [], "entities": []}, {"text": " Table 4: Breakdown of experimental results by type of MWE. Note that UAS / LAS are calculated  regarding first tokens of MWEs. For NEs, the FTM is the same as the FUM because each NE always  takes NNP as an MWE-level POS tag, and is not repeated.", "labels": [], "entities": [{"text": "UAS / LAS", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.8001131812731425}, {"text": "FTM", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.9967034459114075}, {"text": "FUM", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.8419605493545532}]}]}