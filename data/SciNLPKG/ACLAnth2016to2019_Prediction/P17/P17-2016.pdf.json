{"title": [{"text": "Sentence Alignment Methods for Improving Text Simplification Systems", "labels": [], "entities": [{"text": "Sentence Alignment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9473844468593597}, {"text": "Improving Text Simplification", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.9004411101341248}]}], "abstractContent": [{"text": "We provide several methods for sentence-alignment of texts with different complexity levels.", "labels": [], "entities": []}, {"text": "Using the best of them, we sentence-align the Newsela corpora, thus providing large training materials for automatic text simplification (ATS) systems.", "labels": [], "entities": [{"text": "text simplification (ATS)", "start_pos": 117, "end_pos": 142, "type": "TASK", "confidence": 0.8218832194805146}]}, {"text": "We show that using this dataset, even the standard phrase-based statistical machine translation models for ATS can outper-form the state-of-the-art ATS systems.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 51, "end_pos": 95, "type": "TASK", "confidence": 0.6108386218547821}, {"text": "ATS", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9051035046577454}]}], "introductionContent": [{"text": "Automated text simplification (ATS) tries to automatically transform (syntactically, lexically and/or semantically) complex sentences into their simpler variants without significantly altering the original meaning.", "labels": [], "entities": [{"text": "Automated text simplification (ATS)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7696852137645086}]}, {"text": "It has attracted much attention recently as it could make texts more accessible to wider audiences, and used as a pre-processing step, improve performances of various NLP tasks and systems.", "labels": [], "entities": []}, {"text": "However, the state-of-the-art ATS systems still do not reach satisfying performances and require some human post-editing.", "labels": [], "entities": [{"text": "ATS", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9128186702728271}]}, {"text": "While the best supervised approaches generally lead to grammatical output with preserved original meaning, they are overcautious, making almost no changes to the input sentences, probably due to the limited size or bad quality of parallel TS corpora used for training.", "labels": [], "entities": []}, {"text": "The largest existing sentence-aligned TS dataset for English is the English Wikipedia -Simple English Wikipedia (EW-SEW) dataset, which contains 160-280,000 sentence pairs, depending on whether we want to model only traditional sentence rewritings or also to model content reduction and stronger paraphrasing (.", "labels": [], "entities": [{"text": "English Wikipedia -Simple English Wikipedia (EW-SEW) dataset", "start_pos": 68, "end_pos": 128, "type": "DATASET", "confidence": 0.7441906034946442}, {"text": "content reduction", "start_pos": 265, "end_pos": 282, "type": "TASK", "confidence": 0.7095869332551956}]}, {"text": "For Spanish, the largest existing parallel TS corpus contains only 1,000 sentence pairs thus impeding the use of fully supervised approaches.", "labels": [], "entities": []}, {"text": "The best unsupervised lexical simplification (LS) systems for English which leverage word-embeddings seem to perform more lexical substitutions but at the cost of having less grammatical output and more often changed meaning.", "labels": [], "entities": [{"text": "unsupervised lexical simplification (LS)", "start_pos": 9, "end_pos": 49, "type": "TASK", "confidence": 0.7844265003999075}]}, {"text": "However, there have been no direct comparisons of supervised and unsupervised state-of-the-art approaches so far.", "labels": [], "entities": []}, {"text": "The Newsela corpora 1 offers over 2,000 original news articles in English and around 250 in Spanish, manually simplified to 3-4 different complexity levels following strict guidelines (.", "labels": [], "entities": []}, {"text": "Although it was suggested that it has better quality than the EW-SEW corpus (), Newsela has not yet been used for training end-to-end ATS systems, due to the lack of its sentence (and paragraph) alignments.", "labels": [], "entities": [{"text": "EW-SEW corpus", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.8986515700817108}, {"text": "Newsela", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.861840546131134}, {"text": "ATS", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.8190216422080994}]}, {"text": "Such alignments, between various text complexity levels, would offer large training datasets for modelling different levels of simplification, i.e. 'mild' simplifications (using the alignments from the neighbouring levels) and 'heavy' simplifications (using the alignments of level pairs: 0-3, 0-4, 1-4).", "labels": [], "entities": []}, {"text": "We: (1) provide several methods for paragraph-and sentence alignment of parallel texts, and for assessing similarity level between pairs of text snippets, as freely avail-able software; 2 (2) compare the performances of lexically-and semantically-based alignment methods across various text complexity levels; (3) test the hypothesis that the original order of information is preserved during manual simplification () by offering customized MST-LIS alignment strategy (Section 3.1); and (4) show that the new sentence-alignments lead to the state-of-the-art ATS systems even in a basic phrase-based statistical machine translation (PB-SMT) approach to text simplifications.", "labels": [], "entities": [{"text": "paragraph-and sentence alignment of parallel texts", "start_pos": 36, "end_pos": 86, "type": "TASK", "confidence": 0.7747117231289545}, {"text": "MST-LIS alignment", "start_pos": 441, "end_pos": 458, "type": "TASK", "confidence": 0.8729868531227112}, {"text": "phrase-based statistical machine translation (PB-SMT)", "start_pos": 586, "end_pos": 639, "type": "TASK", "confidence": 0.7085880466869899}]}], "datasetContent": [{"text": "To compare the performances of different alignment methods, we randomly selected 10 original texts (Level 0) and their corresponding simpler versions at Levels 1, 3 and 4.", "labels": [], "entities": []}, {"text": "Instead of creating a 'gold standard' and then automatically evaluating the performances, we asked two annotators to rate each pair of automatically aligned paragraphs and sentences -by each of the possible six alignment methods and the HMM-based method (Bott and Saggion, 2011) -for three pairs of text complexity levels (0-1, 0-4, and 3-4) on a 0-2 scale, where: 0 -no semantic overlap in the content; 1 -partial semantic overlap (partial matches); 2 -same semantic content (good matches).", "labels": [], "entities": []}, {"text": "This resulted in a total of 1526 paragraph-and 1086 sentence-alignments for the 0-1 pairs, and 1218 paragraph-and 1266 sentence-alignments for the 0-4 and 3-4 pairs.", "labels": [], "entities": []}, {"text": "In the context of TS, both good-and partial matches are important.", "labels": [], "entities": [{"text": "TS", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9858285188674927}]}, {"text": "While full semantic overlap models full paraphrases ('1-1' alignments), partial overlap models sentence splitting (\"1-n\" alignments), deleting irrelevant sentence parts, adding explanations, or summarizing ('n-m' alignments).", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7388721257448196}, {"text": "summarizing", "start_pos": 194, "end_pos": 205, "type": "TASK", "confidence": 0.9701416492462158}]}, {"text": "Several examples of full and partial matches from the EW-SEW dataset (Hwang et al., 2015) are given in.", "labels": [], "entities": [{"text": "EW-SEW dataset", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.9564668536186218}]}, {"text": "We expect that the automatic-alignment task is the easiest between the 0-1 text complexity levels, and much more difficult between the 0-4 levels (Level 4 is obtained after four stages of simplification and thus contains stronger paraphrases and less lexical overlap with Level 0 than Level 1 has).", "labels": [], "entities": []}, {"text": "We also explore whether the task is equally difficult whenever we align two neighbouring levels, or the difficulty of the task depends on the level complexity (0-1 vs. 3-4).", "labels": [], "entities": []}, {"text": "The obtained interannotator agreement, weighted Cohen's \u03ba (on 400 double-annotated instances) was between 0.71 and 0.74 depending on the task and levels.", "labels": [], "entities": []}, {"text": "The results of the manual analysis showed that: (1) all applied methods significantly (p < 0.001) outperformed the HMM method on both paragraph-and sentence-alignment tasks; 5 (2) the methods which do not assume hypothesis H1 (C3G, CWASA, and WAVG) led to (not significantly) higher percentage of correct alignments than their counterparts which do assume 98  H1 (C3G*, CWASA*, WAVG*); (3) the difference in the performances of the lexical approach (C3G) and semantic approaches (CWASA and WAVG) was significant only in the 0-4 sentencealignment task, where CWASA performed significantly worse (p < 0.001) than the other two methods, and in the 0-4 paragraph-alignment task, where WAVG performed significantly worse than C3G; (4) the 2-step C3G alignment-method (C3G-2s), which first aligns paragraphs using the best paragraph-alignment method (C3G) and then within each paragraph align sentences with the best sentence-alignment method (C3G), led to more good+partial alignments than the 'direct' sentence-alignment C3G method.", "labels": [], "entities": []}, {"text": "Finally, we test our new English Newsela (C3G-2s) sentence-alignments (both for the neighbouring levels -neighb. and for all levels -all) and Newsela sentence-alignments for neighboring levels obtained with HMM-method 6 (Bott and Saggion, 2011) in the ATS task using standard PB-SMT models 7 in the Moses toolkit (.", "labels": [], "entities": [{"text": "ATS task", "start_pos": 252, "end_pos": 260, "type": "TASK", "confidence": 0.8338269293308258}]}, {"text": "We vary the training dataset and the corpus used to build language models (LMs), while keeping always the same 2,000 sentence pairs for tuning () and the first 70 sentence Given that the performance of the HMM-method was poor for non-neighboring levels.", "labels": [], "entities": []}, {"text": "GIZA++ implementation of the IBM word alignment model 4 (, refinement and phraseextraction heuristics (, the minimum error rate training for tuning, and 5-gram LMs with Kneser-Ney smoothing trained with SRILM).", "labels": [], "entities": [{"text": "IBM word alignment", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.5672913491725922}, {"text": "minimum error rate training", "start_pos": 109, "end_pos": 136, "type": "METRIC", "confidence": 0.7427545264363289}, {"text": "SRILM", "start_pos": 203, "end_pos": 208, "type": "METRIC", "confidence": 0.5368959903717041}]}, {"text": "pairs of their test set 8 for our human evaluation.", "labels": [], "entities": []}, {"text": "Using that particular test set allow us to compare our (PBSMT) systems with the output of the stateof-the-art syntax-based MT (SBMT) system for TS ( which is not freely available.", "labels": [], "entities": []}, {"text": "We compare: (1) the performance of the standard PBSMT model which uses only the already available EW-SEW dataset () with the performances of the same PBSMT models but this time using the combination of the EW-SEW dataset and our newly-created Newsela datasets; (2) the latter PBSMT models (which use both EW-SEW and new Newsela datasets) against the state-of-the-art supervised ATS system (, and one of the recently proposed unsupervised lexical simplification systems, the LightLS system (Glava\u0161 and\u0160tajnerand\u02c7and\u0160tajner, 2015).", "labels": [], "entities": [{"text": "EW-SEW dataset", "start_pos": 98, "end_pos": 112, "type": "DATASET", "confidence": 0.7692873477935791}, {"text": "EW-SEW dataset", "start_pos": 206, "end_pos": 220, "type": "DATASET", "confidence": 0.8374695777893066}, {"text": "Newsela datasets", "start_pos": 243, "end_pos": 259, "type": "DATASET", "confidence": 0.9703004360198975}, {"text": "Newsela datasets", "start_pos": 320, "end_pos": 336, "type": "DATASET", "confidence": 0.9182628691196442}, {"text": "LightLS system (Glava\u0161 and\u0160tajnerand\u02c7and\u0160tajner, 2015)", "start_pos": 474, "end_pos": 528, "type": "DATASET", "confidence": 0.9178385511040688}]}, {"text": "We perform three types of human evaluation on the outputs of all systems.", "labels": [], "entities": []}, {"text": "First, we count the total number of changes made by each system (Total), counting the change of a whole phrase (e.g. \"become defunct\" \u2192 \"was dissolved\") as one change.", "labels": [], "entities": []}, {"text": "We mark as Correct those changes that preserve the original meaning and grammaticality of the sentence (assessed by two native English speakers) and, at the same time, make the sentence easier to understand (assessed by two non-native fluent English speakers).", "labels": [], "entities": []}, {"text": "Second, three native English speakers rate the grammaticality (G) and meaning preservation (M) of each sentence with at least one change on a 1-5 Likert scale (1 -very bad; 5 -very good).", "labels": [], "entities": [{"text": "grammaticality (G) and meaning preservation (M)", "start_pos": 47, "end_pos": 94, "type": "METRIC", "confidence": 0.7250130116939545}, {"text": "Likert scale", "start_pos": 146, "end_pos": 158, "type": "METRIC", "confidence": 0.9320472776889801}]}, {"text": "Third, the three nonnative fluent English speakers were shown original (reference) sentences and target (output) sentences (one pair at the time) and asked whether the target sentence is: +2 -much simpler; +1 -somewhat simpler; 0 -equally difficult; -1 -somewhat more difficult; -2 -much more difficult, than the reference sentence.", "labels": [], "entities": []}, {"text": "While the correctness of changes takes into account the influence of each individual change on grammaticality, meaning and simplicity of a sentence, the Scores (G and M) and Rank (S) take into account the mutual influence of all changes within a sentence.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 123, "end_pos": 133, "type": "METRIC", "confidence": 0.9980352520942688}, {"text": "Rank (S)", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9021580815315247}]}, {"text": "Adding our sentence-aligned Newsela corpus  He advocates applying a user-centered design process in product development cycles and also works towards popularizing interaction design as a mainstream discipline.", "labels": [], "entities": [{"text": "Newsela corpus", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.868325412273407}]}, {"text": "PBSMT (Newsela neighb. C3G-2s + Wiki) He advocates a user-centered design process in product development cycles and also works for popularizing interaction design as a mainstream discipline.", "labels": [], "entities": [{"text": "PBSMT (Newsela neighb. C3G-2s + Wiki", "start_pos": 0, "end_pos": 36, "type": "DATASET", "confidence": 0.8264785792146411}]}, {"text": "PBSMT (Newsela all C3G-2s + Wiki) He supports a user-centered design process in product development cycles and also works for popularizing interaction design as a mainstream discipline.", "labels": [], "entities": [{"text": "PBSMT", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8672953248023987}]}, {"text": "PBSMT (Newsela HMM neighb. + Wiki) He advocates a user-centered design process in product development cycles and also works towards popularizing interaction design as a mainstream discipline.", "labels": [], "entities": [{"text": "PBSMT (Newsela HMM neighb. + Wiki", "start_pos": 0, "end_pos": 33, "type": "DATASET", "confidence": 0.8291663442339215}]}, {"text": "PBSMT (Wiki) He advocates applying a user-centered design process in product development cycles and also works towards popularizing interaction design as a mainstream discipline.", "labels": [], "entities": [{"text": "PBSMT", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8827012181282043}]}, {"text": "SBMT ( He advocates using a user-centered design process in product development cycles and also works for popularizing trade design as a whole field.", "labels": [], "entities": [{"text": "trade design", "start_pos": 119, "end_pos": 131, "type": "TASK", "confidence": 0.707696795463562}]}], "tableCaptions": [{"text": " Table 2: Percentage of good+partial sentence-and  paragraph-alignments on the English Newsela cor- pus. All results are significantly better (p < 0.001,  Wilcoxon's signed rank test) than those obtained  by the HMM method (Bott and Saggion, 2011).  The best scores are in bold.", "labels": [], "entities": [{"text": "English Newsela cor- pus", "start_pos": 79, "end_pos": 103, "type": "DATASET", "confidence": 0.8335717678070068}]}, {"text": " Table 3: Extrinsic evaluation (PBSMT-based automatic text simplification systems vs. state of the art).", "labels": [], "entities": [{"text": "Extrinsic evaluation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6682952344417572}]}]}