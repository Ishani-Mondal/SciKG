{"title": [{"text": "Joint CTC/attention decoding for end-to-end speech recognition", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7061757743358612}]}], "abstractContent": [{"text": "End-to-end automatic speech recognition (ASR) has become a popular alternative to conventional DNN/HMM systems because it avoids the need for linguistic resources such as pronunciation dictionary, tokenization, and context-dependency trees, leading to a greatly simplified model-building process.", "labels": [], "entities": [{"text": "End-to-end automatic speech recognition (ASR)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7215194872447422}]}, {"text": "There are two major types of end-to-end archi-tectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connection-ist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming.", "labels": [], "entities": [{"text": "ASR", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9886167645454407}, {"text": "connection-ist temporal classification (CTC)", "start_pos": 188, "end_pos": 232, "type": "TASK", "confidence": 0.76066425939401}]}, {"text": "This paper proposes a joint decoding algorithm for end-to-end ASR with a hybrid CTC/attention architecture, which effectively utilizes both advantages in decoding.", "labels": [], "entities": [{"text": "ASR", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9700905680656433}]}, {"text": "We have applied the proposed method to two ASR benchmarks (spontaneous Japanese and Mandarin Chi-nese), and showing the comparable performance to conventional state-of-the-art DNN/HMM ASR systems without linguistic resources.", "labels": [], "entities": [{"text": "ASR", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9835561513900757}]}], "introductionContent": [{"text": "Automatic speech recognition (ASR) is currently a mature set of technologies that have been widely deployed, resulting in great success in interface applications such as voice search.", "labels": [], "entities": [{"text": "Automatic speech recognition (ASR)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8423686722914377}, {"text": "voice search", "start_pos": 170, "end_pos": 182, "type": "TASK", "confidence": 0.7250890135765076}]}, {"text": "A typical ASR system is factorized into several modules including acoustic, lexicon, and language models based on a probabilistic noisy channel model.", "labels": [], "entities": [{"text": "ASR", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9882069230079651}]}, {"text": "Over the last decade, dramatic improvements in acoustic and language models have been driven by machine learning techniques known as deep learning).", "labels": [], "entities": []}, {"text": "However, current systems lean heavily on the scaffolding of complicated legacy architectures that grew up around traditional techniques.", "labels": [], "entities": []}, {"text": "For example, when we build an acoustic model from scratch, we have to first build hidden Markov model (HMM) and Gaussian mixture model (GMM) followed by deep neural networks (DNN).", "labels": [], "entities": []}, {"text": "In addition, the factorization of acoustic, lexicon, and language models is derived by conditional independence assumptions (especially Markov assumptions), although the data do not necessarily follow such assumptions leading to model misspecification.", "labels": [], "entities": []}, {"text": "This factorization form also yields a local optimum since the above modules are optimized separately.", "labels": [], "entities": []}, {"text": "Further, to well factorize acoustic and language models, the system requires linguistic knowledge based on a lexicon model, which is usually based on a hand-crafted pronunciation dictionary to map word to phoneme sequence.", "labels": [], "entities": []}, {"text": "In addition to the pronunciation dictionary issue, some languages, which do not explicitly have a word boundary, need languagespecific tokenization modules () for language modeling.", "labels": [], "entities": []}, {"text": "Finally, inference/decoding has to be performed by integrating all modules resulting in complex decoding.", "labels": [], "entities": []}, {"text": "Consequently, it is quite difficult for non-experts to use/develop ASR systems for new applications, especially for new languages.", "labels": [], "entities": [{"text": "ASR", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9764100313186646}]}, {"text": "End-to-end ASR has the goal of simplifying the above module-based architecture into a singlenetwork architecture within a deep learning framework, in order to address the above issues.", "labels": [], "entities": [{"text": "ASR", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9833323955535889}]}, {"text": "There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming ().", "labels": [], "entities": [{"text": "ASR", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9893980026245117}, {"text": "connectionist temporal classification (CTC)", "start_pos": 187, "end_pos": 230, "type": "TASK", "confidence": 0.7741154332955679}]}, {"text": "The attention-based end-to-end method solves the ASR problem as a sequence mapping from speech feature sequences to text by using encoderdecoder architecture.", "labels": [], "entities": [{"text": "ASR problem", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.913070410490036}]}, {"text": "The decoder network uses an attention mechanism to find an alignment between each element of the output sequence and the hidden states generated by the acoustic encoder network for each frame of acoustic input.", "labels": [], "entities": []}, {"text": "This basic temporal attention mechanism is too flexible in the sense that it allows extremely non-sequential alignments.", "labels": [], "entities": []}, {"text": "This maybe fine for applications such as machine translation where input and output word order are different (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8160981237888336}]}, {"text": "However, in speech recognition, the feature inputs and corresponding letter outputs generally proceed in the same order.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7481468021869659}]}, {"text": "Another problem is that the input and output sequences in ASR can have very different lengths, and these vary greatly from case to case, depending on the speaking rate and writing system, making it more difficult to track the alignment.", "labels": [], "entities": [{"text": "ASR", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9613476395606995}]}, {"text": "However, an advantage is that the attention mechanism does not require any conditional independence assumptions, and could address all the problems cited above.", "labels": [], "entities": []}, {"text": "Although the alignment problems of attention-based mechanisms have been partially addressed in) using various mechanisms, here we propose more rigorous constraints by using CTC-based alignment to guide the decoding.", "labels": [], "entities": []}, {"text": "CTC permits an efficient computation of a strictly monotonic alignment using dynamic programming () although it requires language models and graph-based decoding ( except in the case of huge training data (.", "labels": [], "entities": [{"text": "CTC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8460255265235901}]}, {"text": "We propose to take advantage of the constrained CTC alignment in a hybrid CTC/attention based system during decoding.", "labels": [], "entities": []}, {"text": "The proposed method adopts a CTC/attention hybrid architecture, which was originally designed to regularize an attention-based encoder network by additionally using a CTC during training ().", "labels": [], "entities": []}, {"text": "The proposed method extends the architecture to perform one-pass/rescoring joint decoding, where hypotheses of attention-based ASR are boosted by scores obtained by using CTC outputs.", "labels": [], "entities": [{"text": "one-pass/rescoring joint decoding", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.5802917301654815}, {"text": "ASR", "start_pos": 127, "end_pos": 130, "type": "TASK", "confidence": 0.7986763715744019}]}, {"text": "This greatly reduces irregular alignments without any heuristic search techniques.", "labels": [], "entities": []}, {"text": "The proposed method is applied to Japanese and Mandarin ASR tasks, which require extra linguistic resources including morphological analyzer () or word segmentation () in addition to pronunciation dictionary to provide accurate lexicon and language models in conventional DNN/HMM ASR.", "labels": [], "entities": [{"text": "ASR tasks", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.8066899478435516}, {"text": "word segmentation", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.6762825101613998}]}, {"text": "Surprisingly, the method achieved performance comparable to, and in some cases superior to, several state-of-the-art DNN/HMM ASR systems, without using the above linguistic resources.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used Japanese and Mandarin Chinese ASR benchmarks to show the effectiveness of the proposed joint CTC/attention decoding approach.", "labels": [], "entities": [{"text": "ASR", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.8575495481491089}, {"text": "CTC/attention decoding", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.8061496764421463}]}, {"text": "The main reason for choosing these two languages is that those ideogram languages have relatively shorter lengths for letter sequences than those in alphabet languages, which reduces computational complexities greatly, and makes it easy to handle context information in a decoder network.", "labels": [], "entities": []}, {"text": "Our preliminary investigation shows that Japanese and Mandarin Chinese end-to-end ASR can be easily scaled up, and shows state-of-the-art performance without using various tricks developed in English tasks.", "labels": [], "entities": [{"text": "ASR", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.8857104778289795}]}, {"text": "Also, we would like to emphasize that the system did not use language-specific processing (e.g., morphological analyzer, Pinyin dictionary), and simply used all appeared characters in their transcriptions including Japanese syllable and Kanji, Chinese, Arabic number, and alphabet characters, as they are.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Character error rate (CER) for conventional attention and hybrid CTC/attention end-to-end  ASR. Corpus of Spontaneous Japanese speech recognition (CSJ) task.", "labels": [], "entities": [{"text": "Character error rate (CER)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.8897256851196289}, {"text": "CTC/attention end-to-end  ASR", "start_pos": 75, "end_pos": 104, "type": "TASK", "confidence": 0.5960896253585816}, {"text": "Corpus of Spontaneous Japanese speech recognition (CSJ)", "start_pos": 106, "end_pos": 161, "type": "TASK", "confidence": 0.6426259709729089}]}, {"text": " Table 2: Character error rate (CER) for conventional attention and hybrid CTC/attention end-to-end  ASR. HKUST Mandarin Chinese conversational telephone speech recognition (MTS) task.", "labels": [], "entities": [{"text": "Character error rate (CER)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.9097673694292704}, {"text": "CTC/attention end-to-end  ASR", "start_pos": 75, "end_pos": 104, "type": "TASK", "confidence": 0.523870712518692}, {"text": "HKUST Mandarin Chinese conversational telephone speech recognition (MTS)", "start_pos": 106, "end_pos": 178, "type": "TASK", "confidence": 0.855941116809845}]}, {"text": " Table 3: RTF versus CER for the one-pass and  rescoring methods.", "labels": [], "entities": [{"text": "RTF", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7855013608932495}, {"text": "CER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9210525751113892}]}]}