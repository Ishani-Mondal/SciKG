{"title": [{"text": "Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings", "labels": [], "entities": [{"text": "Revisiting Recurrent Networks", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8279051582018534}]}], "abstractContent": [{"text": "We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al.", "labels": [], "entities": []}, {"text": "While they found LSTM recurrent networks to un-derperform word averaging, we present several developments that together produce the opposite conclusion.", "labels": [], "entities": [{"text": "word averaging", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.7351864874362946}]}, {"text": "These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively.", "labels": [], "entities": []}, {"text": "These improve LSTMs in both transfer learning and supervised settings.", "labels": [], "entities": [{"text": "LSTMs", "start_pos": 14, "end_pos": 19, "type": "TASK", "confidence": 0.9699822068214417}]}, {"text": "We also introduce anew recurrent architecture , the GATED RECURRENT AVERAGING NETWORK, that is inspired by averaging and LSTMs while outperforming them both.", "labels": [], "entities": [{"text": "GATED RECURRENT AVERAGING NETWORK", "start_pos": 52, "end_pos": 85, "type": "METRIC", "confidence": 0.4801168441772461}]}, {"text": "We analyze our learned models , finding evidence of preferences for particular parts of speech and dependency relations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modeling sentential compositionality is a fundamental aspect of natural language semantics.", "labels": [], "entities": [{"text": "Modeling sentential compositionality", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6059050858020782}]}, {"text": "Researchers have proposed abroad range of compositional functional architectures) and evaluated them on a large variety of applications.", "labels": [], "entities": []}, {"text": "Our goal is to learn a generalpurpose sentence embedding function that can be used unmodified for measuring semantic textual similarity (STS) () and can also serve as a useful initialization for downstream tasks.", "labels": [], "entities": [{"text": "textual similarity (STS)", "start_pos": 117, "end_pos": 141, "type": "METRIC", "confidence": 0.5945547640323638}]}, {"text": "We wish to learn this embedding function such that sentences with high semantic similarity have high cosine similarity in the embedding space.", "labels": [], "entities": []}, {"text": "In particular, we focus on the setting of, in which models are trained on noisy paraphrase pairs and evaluated on both STS and supervised semantic tasks.", "labels": [], "entities": []}, {"text": "Surprisingly, Wieting et al. found that simple embedding functions-those based on averaging word vectors-outperform more powerful architectures based on long short-term memory (LSTM).", "labels": [], "entities": []}, {"text": "In this paper, we revisit their experimental setting and present several techniques that together improve the performance of the LSTM to be superior to word averaging.", "labels": [], "entities": [{"text": "word averaging", "start_pos": 152, "end_pos": 166, "type": "TASK", "confidence": 0.8281879127025604}]}, {"text": "We first change data sources: rather than train on noisy phrase pairs from the Paraphrase Database (PPDB;, we use noisy sentence pairs obtained automatically by aligning Simple English to standard English Wikipedia ().", "labels": [], "entities": []}, {"text": "Even though this data was intended for use by text simplification systems, we find it to be efficient and effective for learning sentence embeddings, outperforming much larger sets of examples from PPDB.", "labels": [], "entities": []}, {"text": "We then show how we can modify and regularize the LSTM to further improve its performance.", "labels": [], "entities": []}, {"text": "The main modification is to simply average the hidden states instead of using the final one.", "labels": [], "entities": []}, {"text": "For regularization, we experiment with two kinds of dropout and also with randomly scrambling the words in each input sequence.", "labels": [], "entities": [{"text": "regularization", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9805086255073547}]}, {"text": "We find that these techniques help in the transfer learning setting and on two supervised semantic similarity datasets as well.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.8960088789463043}]}, {"text": "Further gains are obtained on the supervised tasks by initializing with our models from the transfer setting.", "labels": [], "entities": []}, {"text": "Inspired by the strong performance of both averaging and LSTMs, we introduce a novel recurrent neural network architecture which we call the GATED RECURRENT AVERAGING NETWORK (GRAN).", "labels": [], "entities": [{"text": "GATED RECURRENT AVERAGING NETWORK (GRAN)", "start_pos": 141, "end_pos": 181, "type": "METRIC", "confidence": 0.7216707255159106}]}, {"text": "The GRAN outperforms averaging and the LSTM in both the transfer and supervised learning settings, forming a promising new recurrent architecture for semantic modeling.", "labels": [], "entities": [{"text": "GRAN", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9662667512893677}, {"text": "semantic modeling", "start_pos": 150, "end_pos": 167, "type": "TASK", "confidence": 0.8584990203380585}]}], "datasetContent": [{"text": "Our experiments are designed to address the empirical question posed by: why do LSTMs underperform AVG for transfer learning?", "labels": [], "entities": []}, {"text": "In Sections 4.1.2-4.2, we make progress on this question by presenting methods that bridge the gap between the two models in the transfer setting.", "labels": [], "entities": []}, {"text": "We then apply these same techniques to improve performance in the supervised setting, described in Section 4.3.", "labels": [], "entities": []}, {"text": "In both settings we also evaluate our novel GRAN architecture, finding it to consistently outperform both AVG and the LSTM.", "labels": [], "entities": [{"text": "AVG", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.5155855417251587}]}, {"text": "We train on large sets of noisy paraphrase pairs and evaluate on a diverse set of 22 textual similarity datasets, including all datasets from every SemEval semantic textual similarity (STS) task from 2012 to 2015.", "labels": [], "entities": [{"text": "SemEval semantic textual similarity (STS) task from 2012", "start_pos": 148, "end_pos": 204, "type": "TASK", "confidence": 0.8455910086631775}]}, {"text": "We also evaluate on the SemEval 2015 Twitter task () and the SemEval 2014 SICK Semantic Relatedness task ().", "labels": [], "entities": [{"text": "SemEval 2015 Twitter task", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.5213274583220482}, {"text": "SemEval 2014 SICK Semantic Relatedness task", "start_pos": 61, "end_pos": 104, "type": "TASK", "confidence": 0.6579357584317526}]}, {"text": "Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent.", "labels": [], "entities": [{"text": "STS tasks", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.8731101155281067}]}, {"text": "We report the average Pearson's rover these 22 sentence similarity tasks.", "labels": [], "entities": [{"text": "Pearson's rover", "start_pos": 22, "end_pos": 37, "type": "DATASET", "confidence": 0.9193989237149557}]}, {"text": "Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others.", "labels": [], "entities": [{"text": "machine translation outputs, web forums, news headlines, image and video captions", "start_pos": 112, "end_pos": 193, "type": "TASK", "confidence": 0.6313144798789706}]}, {"text": "Further details are provided in the official task descriptions ().", "labels": [], "entities": []}, {"text": "We first investigate how different sources of training data affect the results.", "labels": [], "entities": []}, {"text": "We try two data sources.", "labels": [], "entities": []}, {"text": "The first is phrase pairs from the Paraphrase Database (PPDB).", "labels": [], "entities": [{"text": "Paraphrase Database (PPDB)", "start_pos": 35, "end_pos": 61, "type": "DATASET", "confidence": 0.7783355712890625}]}, {"text": "PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones.", "labels": [], "entities": []}, {"text": "The pairs in PPDB are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases.", "labels": [], "entities": []}, {"text": "PPDB is derived automatically from naturally-occurring bilingual text, and versions of PPDB have been released for many languages without the need for any manual annotation ().", "labels": [], "entities": []}, {"text": "The second source of data is a set of sentence pairs automatically extracted from Simple English Wikipedia and English Wikipedia articles by embeddings () to initialize the word embedding matrix (W w ) for all models.", "labels": [], "entities": []}, {"text": "For all experiments, we fix the mini-batch size to 100, and \u03bb c to 0.", "labels": [], "entities": []}, {"text": "We tune the margin \u03b4 over {0.4, 0.6, 0.8} and \u03bb w over {10 \u22124 , 10 \u22125 , 10 \u22126 , 10 \u22127 , 10 \u22128 , 0}.", "labels": [], "entities": [{"text": "margin \u03b4", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.8906518220901489}]}, {"text": "We train AVG for 7 epochs, and the LSTM for 3, since it converges much faster and does not benefit from 7 epochs.", "labels": [], "entities": [{"text": "AVG", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.5506303310394287}, {"text": "LSTM", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.7801728248596191}]}, {"text": "For optimization we use Adam () with a learning rate of 0.001.", "labels": [], "entities": []}, {"text": "We use the 2016 STS tasks ( for model selection, where we average the Pearson's rover its 5 datasets.", "labels": [], "entities": [{"text": "2016 STS tasks", "start_pos": 11, "end_pos": 25, "type": "DATASET", "confidence": 0.6239896516005198}, {"text": "model selection", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.765831708908081}, {"text": "Pearson's rover its 5 datasets", "start_pos": 70, "end_pos": 100, "type": "DATASET", "confidence": 0.8848085502783457}]}, {"text": "We refer to this type of model selection as test.", "labels": [], "entities": []}, {"text": "For evaluation, we report the average Pearson's rover the 22 other sentence similarity tasks.", "labels": [], "entities": [{"text": "Pearson's rover", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.724705696105957}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We first note that, when training on PPDB, we find the same result as: AVG outperforms the LSTM by more than 13 points.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.8331130146980286}, {"text": "AVG", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9960165619850159}]}, {"text": "However, when training both on sentence pairs, the gap shrinks to about 9 points.", "labels": [], "entities": []}, {"text": "It appears that part of the inferior performance for the LSTM in prior work was due to training on phrase pairs rather than on sentence pairs.", "labels": [], "entities": []}, {"text": "The AVG model also benefits from training on sentences, but not nearly as much as the LSTM.", "labels": [], "entities": []}, {"text": "Our hypothesis explaining this result is that in PPDB, the phrase pairs are short fragments of text which are not necessarily constituents or phrases in any syntactic sense.", "labels": [], "entities": []}, {"text": "Therefore, the sentences in the STS test sets are quite different from the fragments seen during training.", "labels": [], "entities": [{"text": "STS test sets", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.7964268922805786}]}, {"text": "We hypothesize that while word-averaging is relatively unaffected by this difference, the recurrent models are much more sensitive to overall characteristics of the word sequences, and the difference between train and test matters much more.", "labels": [], "entities": []}, {"text": "These results also suggest that the SimpWiki data, even though it was developed for text simplification, maybe useful for other researchers working on semantic textual similarity tasks.", "labels": [], "entities": [{"text": "SimpWiki data", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.7972909212112427}, {"text": "text simplification", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7664526700973511}]}, {"text": "We next compare LSTM and LSTMAVG.", "labels": [], "entities": []}, {"text": "The latter consists of averaging the hidden vectors of the LSTM rather than using the final hidden vector as in prior work ().", "labels": [], "entities": []}, {"text": "We hypothesize that the LSTM may put more emphasis on the words at the end of the sentence than those at the beginning.", "labels": [], "entities": []}, {"text": "By averaging the hidden states, the impact of all words in the sequence is better taken into account.", "labels": [], "entities": []}, {"text": "Averaging also makes the LSTM more like AVG, which we know to perform strongly in this setting.", "labels": [], "entities": [{"text": "AVG", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9487026333808899}]}, {"text": "The results on AVG and the LSTM models are shown in.", "labels": [], "entities": [{"text": "AVG", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.5999481081962585}]}, {"text": "When training on PPDB, moving from LSTM to LSTMAVG improves performance by 10 points, closing most of the gap with AVG.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.9163775444030762}, {"text": "LSTM", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.8610734939575195}, {"text": "LSTMAVG", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.7869837284088135}, {"text": "AVG", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.8150084614753723}]}, {"text": "We also find that LSTMAVG improves by moving from PPDB to SimpWiki, though in both cases it still lags behind AVG.", "labels": [], "entities": [{"text": "AVG", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.8916151523590088}]}, {"text": "ing and test sentences, adding both, and adding neither.", "labels": [], "entities": []}, {"text": "We treated adding these tags as hyperparameters and tuned over these four settings along with the other hyperparameters in the original experiment.", "labels": [], "entities": []}, {"text": "Interestingly, we found that adding these tags, especially EOS, had a large effect on the LSTM when training on SimpWiki, improving performance by 6 points.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 90, "end_pos": 94, "type": "TASK", "confidence": 0.7568374276161194}]}, {"text": "When training on PPDB, adding EOS tags only improved performance by 1.6 points.", "labels": [], "entities": []}, {"text": "The addition of the tags had a smaller effect on LSTMAVG.", "labels": [], "entities": []}, {"text": "Adding EOS tags improved performance by 0.3 points on SimpWiki and adding SOS tags on PPDB improved performance by 0.9 points.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.9155360460281372}]}, {"text": "as well as several additional regularization methods we describe below.", "labels": [], "entities": []}, {"text": "We try two forms of dropout.", "labels": [], "entities": []}, {"text": "The first is just standard dropout () on the word embeddings.", "labels": [], "entities": []}, {"text": "The second is \"word dropout\", which drops out entire word embeddings with some probability.", "labels": [], "entities": []}, {"text": "We also experiment with scrambling the inputs.", "labels": [], "entities": []}, {"text": "For a given mini-batch, we go through each sentence pair and, with some probability, we shuffle the words in each sentence in the pair.", "labels": [], "entities": []}, {"text": "When scrambling a sentence pair, we always shuffle both sentences in the pair.", "labels": [], "entities": []}, {"text": "We do this before selecting negative examples for the mini-batch.", "labels": [], "entities": []}, {"text": "The motivation for scrambling is to make it more difficult for the LSTM to memorize the sequences in the training data, forcing it to focus more on the identities of the words and lesson word order.", "labels": [], "entities": []}, {"text": "Hence it will be expected to behave more like the word averaging model.", "labels": [], "entities": [{"text": "word averaging", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.7060821503400803}]}, {"text": "We also experiment with combining scrambling and dropout.", "labels": [], "entities": []}, {"text": "In this setting, we tune over scrambling with either word dropout or dropout.", "labels": [], "entities": []}, {"text": "The settings for these experiments are largely the same as those of the previous section with the exception that we tune \u03bb w over a smaller set of values: {10 \u22125 , 0}.", "labels": [], "entities": []}, {"text": "When using L 2 regularization, we tune \u03bb cover {10 \u22123 , 10 \u22124 , 10 \u22125 , 10 \u22126 }.", "labels": [], "entities": []}, {"text": "When using dropout, we tune the dropout rate over {0.2, 0.4, 0.6}.", "labels": [], "entities": []}, {"text": "When using scrambling, we tune the scrambling rate over {0.25, 0.5, 0.75}.", "labels": [], "entities": []}, {"text": "We also include a bidirectional model (\"Bi\") for both LSTMAVG and the GATED RECURRENT AVERAG-ING NETWORK.", "labels": [], "entities": [{"text": "Bi", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9713934659957886}, {"text": "GATED RECURRENT AVERAG-ING NETWORK", "start_pos": 70, "end_pos": 104, "type": "TASK", "confidence": 0.42832109332084656}]}, {"text": "We tune over two ways to combine the forward and backward hidden states; the first simply adds them together and the second uses a single feedforward layer with a tanh activation.", "labels": [], "entities": []}, {"text": "We try two approaches for model selection.", "labels": [], "entities": [{"text": "model selection", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7540580630302429}]}, {"text": "The first, test , is the same as was done in Section 4.1.2, where we use the average Pearson's r on the 5 2016 STS datasets.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.954207162062327}, {"text": "5 2016 STS datasets", "start_pos": 104, "end_pos": 123, "type": "DATASET", "confidence": 0.6168543547391891}]}, {"text": "The second tunes based on the average Pearson's r of all 22 datasets in our evaluation.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.9851232171058655}]}, {"text": "We refer to this as oracle.", "labels": [], "entities": []}, {"text": "The results are shown in   bling input sequences is very effective in improving the result of the LSTM, while neither type of dropout improves AVG.", "labels": [], "entities": [{"text": "AVG", "start_pos": 143, "end_pos": 146, "type": "METRIC", "confidence": 0.9946120977401733}]}, {"text": "Moreover, averaging the hidden states of the LSTM is the most effective modification to the LSTM in improving performance.", "labels": [], "entities": []}, {"text": "All of these modifications can be combined to significantly improve the LSTM, finally allowing it to overtake AVG.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 72, "end_pos": 76, "type": "TASK", "confidence": 0.6152953505516052}, {"text": "AVG", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.7345913648605347}]}, {"text": "In, we compare the various GRAN architectures.", "labels": [], "entities": [{"text": "GRAN", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.7809275388717651}]}, {"text": "We find that the GRAN provides a small improvement over the best LSTM configuration, possibly because of its similarity to AVG.", "labels": [], "entities": [{"text": "GRAN", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9808323383331299}, {"text": "AVG", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.7807771563529968}]}, {"text": "It also outperforms the other GRAN models, despite being the simplest.", "labels": [], "entities": [{"text": "GRAN", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.8428161144256592}]}, {"text": "In, we show results on all individual STS evaluation datasets after using STS 2016 for model selection (unidirectional models only).", "labels": [], "entities": [{"text": "STS evaluation datasets", "start_pos": 38, "end_pos": 61, "type": "DATASET", "confidence": 0.6806652545928955}]}, {"text": "The LSTMAVG and GATED RECURRENT AVERAGING NETWORK are more closely correlated in performance, in terms of Spearman's \u03c1 and Pearson'r r, than either is to AVG.", "labels": [], "entities": [{"text": "LSTMAVG", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.8018524050712585}, {"text": "GATED RECURRENT AVERAGING NETWORK", "start_pos": 16, "end_pos": 49, "type": "METRIC", "confidence": 0.5691702663898468}, {"text": "Spearman's \u03c1", "start_pos": 106, "end_pos": 118, "type": "METRIC", "confidence": 0.7418597340583801}, {"text": "Pearson'r r", "start_pos": 123, "end_pos": 134, "type": "METRIC", "confidence": 0.913657009601593}, {"text": "AVG", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.7431972026824951}]}, {"text": "But they do differ significantly in some datasets, most notably in those comparing machine translation output with its ref-  Upon examination, we found that these datasets, especially 2013 OnWN, contain examples of low similarity with high word overlap.", "labels": [], "entities": [{"text": "OnWN", "start_pos": 189, "end_pos": 193, "type": "DATASET", "confidence": 0.5253565311431885}]}, {"text": "For example, the pair the act of preserving or protecting something., the act of decreasing or reducing something. from 2013 OnWN has a gold similarity score of 0.4.", "labels": [], "entities": [{"text": "OnWN", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.9481824636459351}, {"text": "gold similarity score", "start_pos": 136, "end_pos": 157, "type": "METRIC", "confidence": 0.7513647675514221}]}, {"text": "It appears that AVG was fooled by the high amount of word overlap in such pairs, while the other two models were better able to recognize the semantic differences.", "labels": [], "entities": [{"text": "AVG", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.4870200753211975}]}], "tableCaptions": [{"text": " Table 1: Test results on SemEval semantic textual  similarity datasets (Pearson's r \u00d7 100) when train- ing on different sources of data: phrase pairs from  PPDB or simple-to-standard English Wikipedia  sentence pairs from Coster and Kauchak (2011).", "labels": [], "entities": [{"text": "SemEval semantic textual  similarity", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.8349495977163315}]}, {"text": " Table 2: Results on SemEval textual similarity  datasets (Pearson's r \u00d7 100) when experimenting  with different regularization techniques.", "labels": [], "entities": [{"text": "SemEval textual similarity", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.8112306992212931}, {"text": "Pearson's r \u00d7 100", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.8847321271896362}]}, {"text": " Table 3: Results on SemEval textual similarity  datasets (Pearson's r \u00d7 100) for the GRAN ar- chitectures. The first row, marked as (no reg.) is  the GRAN without any regularization. The other  rows show the result of the various GRAN models  using dropout and scrambling.", "labels": [], "entities": [{"text": "SemEval textual similarity  datasets", "start_pos": 21, "end_pos": 57, "type": "DATASET", "confidence": 0.648797057569027}, {"text": "GRAN", "start_pos": 151, "end_pos": 155, "type": "DATASET", "confidence": 0.6800090074539185}]}, {"text": " Table 4: Results on SemEval textual similarity  datasets (Pearson's r \u00d7 100). The highest score in  each row is in boldface.", "labels": [], "entities": [{"text": "SemEval textual similarity  datasets", "start_pos": 21, "end_pos": 57, "type": "DATASET", "confidence": 0.7807173430919647}, {"text": "Pearson's r \u00d7 100", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.8711495995521545}]}, {"text": " Table 5: Results from supervised training on  the STS and SICK datasets (Pearson's r \u00d7 100).  The last column is the average result on the two  datasets.", "labels": [], "entities": [{"text": "SICK datasets", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.7724699676036835}, {"text": "Pearson's r \u00d7 100)", "start_pos": 74, "end_pos": 92, "type": "METRIC", "confidence": 0.6755632410446802}]}, {"text": " Table 6: Results from supervised training on the  STS and SICK datasets (Pearson's r \u00d7 100) for  the GRAN architectures. The last column is the  average result on the two datasets.", "labels": [], "entities": [{"text": "SICK datasets", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.755961537361145}]}, {"text": " Table 7: Illustrative sentence pairs from the STS datasets showing errors made by LSTMAVG and  AVG. The last three columns show the gold similarity score, the similarity score of LSTMAVG, and the  similarity score of AVG. Boldface indicates smaller error compared to gold scores.", "labels": [], "entities": [{"text": "STS datasets", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.9150120913982391}, {"text": "gold similarity score", "start_pos": 133, "end_pos": 154, "type": "METRIC", "confidence": 0.8083494305610657}, {"text": "similarity score", "start_pos": 160, "end_pos": 176, "type": "METRIC", "confidence": 0.9616059362888336}, {"text": "similarity score", "start_pos": 198, "end_pos": 214, "type": "METRIC", "confidence": 0.9716165959835052}]}, {"text": " Table 8: Impact of initializing and regularizing  toward universal models (Pearson's r \u00d7100) in su- pervised training.", "labels": [], "entities": []}, {"text": " Table 9: POS tags and dependency labels with  highest and lowest average GATED RECURRENT  AVERAGING NETWORK gate L 1 norms. The lists  are ordered from highest norm to lowest in the top  10 columns, and lowest to highest in the bottom  10 columns.", "labels": [], "entities": [{"text": "GATED RECURRENT  AVERAGING NETWORK gate L 1 norms", "start_pos": 74, "end_pos": 123, "type": "METRIC", "confidence": 0.7621702551841736}]}, {"text": " Table 10: Average L 1 norms for adjectives (JJ)  with selected dependency labels.", "labels": [], "entities": [{"text": "Average L 1 norms", "start_pos": 11, "end_pos": 28, "type": "METRIC", "confidence": 0.855398952960968}]}, {"text": " Table 11: Average L 1 norms for words with the  tag VBG with selected dependency labels.", "labels": [], "entities": [{"text": "Average L 1 norms", "start_pos": 11, "end_pos": 28, "type": "METRIC", "confidence": 0.919868141412735}, {"text": "VBG", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.8585937023162842}]}]}