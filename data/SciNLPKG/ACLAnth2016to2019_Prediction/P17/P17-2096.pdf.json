{"title": [{"text": "Fast and Accurate Neural Word Segmentation for Chinese", "labels": [], "entities": [{"text": "Accurate", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9897241592407227}, {"text": "Neural Word Segmentation", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.5415289302666982}]}], "abstractContent": [{"text": "Neural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 129, "end_pos": 154, "type": "TASK", "confidence": 0.6016262571016947}]}, {"text": "However, both training and working procedures of the current neural models are computationally inefficient.", "labels": [], "entities": []}, {"text": "This paper presents a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks.", "labels": [], "entities": [{"text": "greedy neural word segmenter", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.7188062071800232}]}, {"text": "Our segmenter is truly end-to-end, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word segmentation is a fundamental task for processing most east Asian languages, typically Chinese.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7111786752939224}]}, {"text": "Almost all practical Chinese processing applications essentially rely on Chinese word segmentation (CWS), e.g.,.", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.7767049570878347}]}, {"text": "Since, most methods formalize this task as a sequence labeling problem.", "labels": [], "entities": [{"text": "sequence labeling problem", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.7079437971115112}]}, {"text": "Ina supervised learning fashion, sequence labeling may adopt various models such as Maximum Entropy (ME) () and Conditional Random Fields (CRF) ().", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7199808359146118}, {"text": "Maximum Entropy (ME)", "start_pos": 84, "end_pos": 104, "type": "METRIC", "confidence": 0.861717414855957}]}, {"text": "However, these models rely heavily on hand-crafted features.", "labels": [], "entities": []}, {"text": "To minimize the efforts in feature engineering, neural word segmentation has been actively studied recently.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.9210769236087799}, {"text": "neural word segmentation", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.668020486831665}]}, {"text": "first adapted the sliding-window based sequence labeling) with character embeddings as input.", "labels": [], "entities": []}, {"text": "A number of other researchers have attempted to improve the segmenter of () by augmenting it with additional complexity.", "labels": [], "entities": []}, {"text": "proposed to model ngram features via a gated recursive neural network (GRNN).", "labels": [], "entities": []}, {"text": "used a Long shortterm memory network (LSTM)) to capture long-distance context.", "labels": [], "entities": []}, {"text": "integrated both GRNN and LSTM for deeper feature extraction.", "labels": [], "entities": [{"text": "GRNN", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.6124224066734314}, {"text": "feature extraction", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7309227585792542}]}, {"text": "Besides sequence labeling schemes, proposed a transition-based framework.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.6353621482849121}]}, {"text": "used a zero-order semi-CRF based model.", "labels": [], "entities": []}, {"text": "However, these two models rely on either traditional discrete features or nonneural-network components for performance enhancement, their performance drops rapidly when solely depending on neural models.", "labels": [], "entities": []}, {"text": "Most closely related to this work, proposed to score candidate segmented outputs directly, employing a gated combination neural network over characters for word representation generation and an LSTM scoring model for segmentation result evaluation.", "labels": [], "entities": [{"text": "word representation generation", "start_pos": 156, "end_pos": 186, "type": "TASK", "confidence": 0.7942007382710775}, {"text": "segmentation result evaluation", "start_pos": 217, "end_pos": 247, "type": "TASK", "confidence": 0.8906468947728475}]}, {"text": "Despite the active progress of most existing works in terms of accuracy, their computational needs have been significantly increased to the extent that training a neural segmenter usually takes days even using cutting-edge hardwares.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9984935522079468}]}, {"text": "Meanwhile, different applications often require diverse segmenters and offer large-scale incoming data.", "labels": [], "entities": []}, {"text": "The efficiency of a word segmenter either for training and decoding is crucial in practice.", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.7292607724666595}]}, {"text": "In this paper, we propose a simple yet accurate neu-ral word segmenter who searches greedily during both training and working to overcome the existing efficiency obstacle.", "labels": [], "entities": [{"text": "neu-ral word segmenter", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.5341230233510336}]}, {"text": "Our evaluation will be performed on Chinese benchmark datasets.", "labels": [], "entities": [{"text": "Chinese benchmark datasets", "start_pos": 36, "end_pos": 62, "type": "DATASET", "confidence": 0.884528120358785}]}], "datasetContent": [{"text": "We conduct experiments on two popular benchmark datasets, namely PKU and MSR, from the second international Chinese word segmentation bakeoff () (.", "labels": [], "entities": [{"text": "PKU", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8867424130439758}]}, {"text": "Throughout this paper, we use the same model setting as shown in.", "labels": [], "entities": []}, {"text": "These numbers are tuned on development sets.", "labels": [], "entities": []}, {"text": "We follow () to train model parameters.", "labels": [], "entities": []}, {"text": "The learning rate at epoch t is set as \u03b7 t = 0.2/(1 + \u03b3t), where \u03b3 = 0.1 for PKU dataset and \u03b3 = 0.2 for MSR dataset.", "labels": [], "entities": [{"text": "PKU dataset", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.9296759963035583}, {"text": "MSR dataset", "start_pos": 105, "end_pos": 116, "type": "DATASET", "confidence": 0.9415232837200165}]}, {"text": "The character embeddings are either randomly initialized or pre-trained by) toolkit on Chinese Wikipedia corpus (which will be indicated by +pre-train in tables.), while the word embeddings are always randomly initialized.", "labels": [], "entities": [{"text": "Chinese Wikipedia corpus", "start_pos": 87, "end_pos": 111, "type": "DATASET", "confidence": 0.8819828629493713}]}, {"text": "The beam size is kept the same during training and working.", "labels": [], "entities": [{"text": "beam size", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9349818825721741}]}, {"text": "By default, early update strategy is adopted and the word table H is top half of in-vocabulary (IV) words by frequency.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The effect of different update methods.  #epochs denotes the number of training epochs to  convergence.", "labels": [], "entities": []}, {"text": " Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4", "labels": [], "entities": []}, {"text": " Table 5: Results on all four Bakeoff-2005 datasets.", "labels": [], "entities": [{"text": "Bakeoff-2005 datasets", "start_pos": 30, "end_pos": 51, "type": "DATASET", "confidence": 0.9343864321708679}]}]}