{"title": [{"text": "Flexible and Creative Chinese Poetry Generation Using Neural Memory", "labels": [], "entities": [{"text": "Flexible and Creative Chinese Poetry Generation", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.6167287329832712}]}], "abstractContent": [{"text": "It has been shown that Chinese poems can be successfully generated by sequence-to-sequence neural models, particularly with the attention mechanism.", "labels": [], "entities": []}, {"text": "A potential problem of this approach, however , is that neural models can only learn abstract rules, while poem generation is a highly creative process that involves not only rules but also innovations for which pure statistical models are not appropriate in principle.", "labels": [], "entities": [{"text": "poem generation", "start_pos": 107, "end_pos": 122, "type": "TASK", "confidence": 0.8209984004497528}]}, {"text": "This work proposes a memory-augmented neural model for Chi-nese poem generation, where the neural model and the augmented memory work together to balance the requirements of linguistic accordance and aesthetic innovation , leading to innovative generations that are still rule-compliant.", "labels": [], "entities": [{"text": "Chi-nese poem generation", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.6296634177366892}]}, {"text": "In addition, it is found that the memory mechanism provides interesting flexibility that can be used to generate poems with different styles.", "labels": [], "entities": []}], "introductionContent": [{"text": "Classical Chinese poetry is a special cultural heritage with over 2,000 years of history and is still fascinating us today.", "labels": [], "entities": []}, {"text": "Among the various genres, perhaps the most popular one is the quatrain, a special style with a strict structure (four lines with five or seven characters per line), a regulated rhythmical form (the last characters in the second and fourth lines must follow the same rhythm), and a required tonal pattern (tones of characters in some positions should satisfy a predefined regulation)).", "labels": [], "entities": []}, {"text": "This genre flourished mostly in the Tang Dynasty, and so are often called Corresponding author: Dong Wang; RM 1-303, FIT BLDG, Tsinghua University, Beijing (100084), P.R. China.", "labels": [], "entities": [{"text": "FIT BLDG", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.7356869578361511}, {"text": "P.R. China", "start_pos": 166, "end_pos": 176, "type": "DATASET", "confidence": 0.9083075523376465}]}, {"text": "An example of a quatrain written by Wei Wang, a famous poet in the Tang Dynasty, is shown in.", "labels": [], "entities": []}, {"text": "Due to the stringent restrictions in both rhythm and tone, it is not trivial to create a fully rule-compliant quatrain.", "labels": [], "entities": []}, {"text": "More importantly, besides such strict regulations, a good quatrain should also read fluently, hold a consistent theme, and express a unique affection.", "labels": [], "entities": []}, {"text": "Therefore, poem generation is widely recognized as a very intelligent activity and can be performed only by knowledgeable people with a lot of training.", "labels": [], "entities": [{"text": "poem generation", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.9179130494594574}]}], "datasetContent": [{"text": "This section describes the experiments and results carried out in this paper.", "labels": [], "entities": []}, {"text": "Here, The baseline system was a reproduction of the Attention-based system presented in ().", "labels": [], "entities": []}, {"text": "the model in This system has been shown to be rather flexi- ble and powerful: it can generate different genres of Chinese poems, and when generating quatrains it has been shown to be able to fool human experts in many cases () and the authors had did a thorough comparison with competitive methods mentioned in the related work of this paper.", "labels": [], "entities": []}, {"text": "We obtained the database and the source code (in theano), and reproduced their system using Tensorflow from Google 2 . We didn't make comparisons with some previous methods such as NNLM, SMT, RNNPG as they had been fully compared in () and all of them were much worse than the attention-based system.", "labels": [], "entities": [{"text": "NNLM", "start_pos": 181, "end_pos": 185, "type": "DATASET", "confidence": 0.8936011791229248}]}, {"text": "Another reason was that the experts were not happy to evaluate poems with clearly bad quality.", "labels": [], "entities": []}, {"text": "We also reproduced the model in () with the help of the first author.", "labels": [], "entities": []}, {"text": "However, since their implementation did not involve any restrictions on rhythm and tone, the experts were reluctant to recognize them as good poems.", "labels": [], "entities": []}, {"text": "With a larger dataset (e.g., 1 Million poems), it is assumed that the rhythm and tone can be learned and their system would be good in both fluency and rule compliance.", "labels": [], "entities": []}, {"text": "It should be also emphasized that the memory approach proposed in this paper is a general technique and is complementary to other efforts such as the planning approach () and the recursive approach.", "labels": [], "entities": []}, {"text": "Based on the baseline system, we built the memory-augmented model, and conducted two experiments to demonstrate its power.", "labels": [], "entities": []}, {"text": "The first is an innovation experiment which employs memory to promote or regularize the generation of innovative poems, and the second is a style-transfer experiment which employs memory to generate flexible poems in different styles.", "labels": [], "entities": []}, {"text": "We invited 34 experts to participate in the experiments, and all of them have rich experience not only evaluating poems, but also in writing them.", "labels": [], "entities": []}, {"text": "Most of the experts are from prestigious institutes, including Peking university and the Chinese Academy of Social Science (CASS).", "labels": [], "entities": [{"text": "Chinese Academy of Social Science (CASS)", "start_pos": 89, "end_pos": 129, "type": "DATASET", "confidence": 0.6488295719027519}]}, {"text": "Following the suggestions of the experts, we use five metrics to evaluate the generation, as listed below: \u2022 Compliance: if regulations on tones and rhymes are satisfied; \u2022 Fluency: if the sentences read fluently and convey reasonable meaning; \u2022 Theme consistency: if the entire poem adheres to a single theme; \u2022 Aesthetic innovation: if the quatrain stimulates any aesthetic feeling with elaborate innovation; \u2022 Scenario consistency: if the scenario remains consistent.", "labels": [], "entities": []}, {"text": "The baseline system was built with two customized datasets.", "labels": [], "entities": []}, {"text": "The first dataset is a Chinese po-em corpus (CPC), which we used in this work to train the embeddings of Chinese characters.", "labels": [], "entities": [{"text": "Chinese po-em corpus (CPC)", "start_pos": 23, "end_pos": 49, "type": "DATASET", "confidence": 0.7418863326311111}]}, {"text": "Our CPC dataset contains 284,899 traditional Chinese poems in various genres, including Tang quatrains, Song Iambics, Yuan Songs, and Ming and Qing poems.", "labels": [], "entities": [{"text": "CPC dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9780188500881195}]}, {"text": "This large quantity of data ensures reliable learning for the semantic content of most Chinese characters.", "labels": [], "entities": []}, {"text": "Our second dataset is a Chinese quatrain corpus (CQC) that we have collected from the internet, which consists of 13, 299 5-char quatrains and 65, 560 7-char quatrains.", "labels": [], "entities": [{"text": "Chinese quatrain corpus (CQC)", "start_pos": 24, "end_pos": 53, "type": "DATASET", "confidence": 0.8671009341875712}]}, {"text": "This corpus was used to train the attention-based RNN baseline.", "labels": [], "entities": []}, {"text": "We filtered out the poems whose characters are all low-frequency (less than 100 counts in the database).", "labels": [], "entities": []}, {"text": "After the filtering, the remaining corpus contains 9,195 5-char quatrains and 49,162 7-char quatrains.", "labels": [], "entities": []}, {"text": "We used 9,000 5-char and 49,000 7-char quatrains to train the attention model, and the rest for validation.", "labels": [], "entities": []}, {"text": "Another two datasets were created for use in the memory-augmented system.", "labels": [], "entities": []}, {"text": "Our first dataset, MEM-I, contains 500 quatrains randomly selected from our CQC corpus.", "labels": [], "entities": [{"text": "CQC corpus", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.9399015009403229}]}, {"text": "This dataset was used to produce the memory in the innovation experiment; the second dataset, MEM-S, contains 300 quatrains with clear styles, including 100 pastoral, 100 battlefield and 100 romantic quatrains.", "labels": [], "entities": [{"text": "MEM-S", "start_pos": 94, "end_pos": 99, "type": "DATASET", "confidence": 0.536865234375}]}, {"text": "It was used to generate memory with different styles in the style-transfer experiment.", "labels": [], "entities": []}, {"text": "All the datasets will be released online 3 .  We invited 34 experts to evaluate the quality of the poem generation.", "labels": [], "entities": [{"text": "poem generation", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.674612820148468}]}, {"text": "In the innovation experiment, the evaluation consisted of a comparison between different systems and configurations in terms of the five metrics.", "labels": [], "entities": []}, {"text": "The innovation questions presented the expert with two poems, and asked them to judge which of the poems was better in terms of the five metrics; in the style-transfer experiment, the evaluation was performed by identifying the style of a generated poem.", "labels": [], "entities": []}, {"text": "The evaluation was conducted online, with each questionnaire containing 11 questions focusing on innovation and 4 questions concerned with style-transfer.", "labels": [], "entities": []}, {"text": "Each of the style-transfer questions presented the expert with a single poem and asked them to score it between 1 to 5, with a larger score being better, in terms of compliance, aesthetic innovation, scenario consistency, and fluency.", "labels": [], "entities": [{"text": "compliance", "start_pos": 166, "end_pos": 176, "type": "METRIC", "confidence": 0.9849823117256165}, {"text": "consistency", "start_pos": 209, "end_pos": 220, "type": "METRIC", "confidence": 0.9265297651290894}]}, {"text": "They were also asked to specify the style of the poem.", "labels": [], "entities": []}, {"text": "Using the poems generated by our systems, we generated many different questions of both types, and then created a number of online questionnaires that randomly selected from these questions.", "labels": [], "entities": []}, {"text": "This meant that as discussed above, each questionnaire had 11 randomly selected innovation questions, and 4 randomly selected style transfer questions.", "labels": [], "entities": []}, {"text": "Each question was only used once, meaning that it was not duplicated on multiple questionnaires, and so each questionnaire was different.", "labels": [], "entities": []}, {"text": "Experts could choose to answer multiple questionnaires if they wished, as each one was different.", "labels": [], "entities": []}, {"text": "From the 34 experts, we collected 69 completed questionnaires, which equals to 759 innovation questions and 276 style-transfer questions.", "labels": [], "entities": []}, {"text": "This experiment focuses on the contribution of memory for innovative poem generation.", "labels": [], "entities": [{"text": "poem generation", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.7355598658323288}]}, {"text": "We experimented with two configurations: one is with a one-iteration model (C 1 ) and the other is with an overfitted model (C \u221e ).", "labels": [], "entities": []}, {"text": "The memory was generated from the 500 quatrains in MEM-I, and the weighting factor was defined empirically as 16 for C 1 and 49 for C \u221e . The topics of the generation were 160 keywords randomly selected from Shixuhanyinge (Liu, 1735).", "labels": [], "entities": [{"text": "MEM-I", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.8692337274551392}]}, {"text": "Given a pair of poems generated by two different configurations using the same topic, the experts were asked to choose which one they preferred.", "labels": [], "entities": []}, {"text": "The evaluation is therefore pair-wised, and each pair of configurations contains at least 180 evaluations.", "labels": [], "entities": []}, {"text": "The results are shown in, where the preference ratio for each pair of configurations was tested in terms of the 5 metrics.", "labels": [], "entities": []}, {"text": "From the first row of, we observe that the experts have a clear preference for the poems generated by the C 1 model, the one that can produce fluent yet uninteresting poems.", "labels": [], "entities": []}, {"text": "In particular, the 'aesthetic innovation' score for C \u221e is not better than C 1 , which was different from what we expected.", "labels": [], "entities": []}, {"text": "Informal offline discussions with the poetry experts found that the experts identified some innovative expression in the C \u221e condition, but most of the them was regarded as being nonsense in the opinion of many of the experts.", "labels": [], "entities": []}, {"text": "In comparison to sparking innovation, fluency and being meaningful is more important not only for non-expert readers, but also for professional poet-: Preference ratios for systems with or without overfitting and with or without memory augmentation. s. In other words, only meaningful innovation is regarded as innovation, and irrational innovation is simply treated as junk.", "labels": [], "entities": []}, {"text": "From the second and third rows of, it can be seen that involving memory significantly improves both C 1 and C \u221e , particularly for C \u221e . For C 1 , the most substantial improvement is observed in terms of 'Aesthetic innovation', which is consistent with our argument that memory can help encourage innovation for this model.", "labels": [], "entities": [{"text": "Aesthetic innovation", "start_pos": 205, "end_pos": 225, "type": "METRIC", "confidence": 0.9609837532043457}]}, {"text": "For C \u221e , 'Fluency' seems to be the most improved metric.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9919825196266174}]}, {"text": "This is also consistent with our argument that involving memory constrains over-innovation for over-fitted models.", "labels": [], "entities": []}, {"text": "The last row of is an extra experiment that investigates if C \u221e is regularized well enough after introducing the memory.", "labels": [], "entities": []}, {"text": "It seems that with the regularization, the overfitting problem is largely solved, and the generation is nearly as fluent and consistent as the C 1 condition.", "labels": [], "entities": []}, {"text": "Interestingly, the score for aesthetic innovation is also significantly improved.", "labels": [], "entities": []}, {"text": "Since the regularization is not supposed to boost innovation, this seems confusing at first glance (in comparison to the result on the same metric in the first row), but this is probably because the increased fluency and consistency makes the innovation more appreciated, therefore doubly confirming our argument that true innovation should be reasonable and meaningful.", "labels": [], "entities": [{"text": "consistency", "start_pos": 221, "end_pos": 232, "type": "METRIC", "confidence": 0.9655852913856506}]}, {"text": "In the second experiment, the memory mechanism is used to generate poems in different styles.", "labels": [], "entities": []}, {"text": "We chose three styles: pastoral, battlefield, and romantic.", "labels": [], "entities": []}, {"text": "A style-specific memory, which we call style memory, was constructed for each style by the corresponding quatrains in the MEM-S dataset.", "labels": [], "entities": [{"text": "MEM-S dataset", "start_pos": 122, "end_pos": 135, "type": "DATASET", "confidence": 0.8925957977771759}]}, {"text": "The system with one-iteration model C 1 was used as the baseline.", "labels": [], "entities": []}, {"text": "Two sets of topics were used in the experiment, one is general and the other is style-biased.", "labels": [], "entities": []}, {"text": "The experiments then investigate if the memory mechanism can produce a clear style if the topic is general, and can transfer to a different style if the topic is style-biased already.", "labels": [], "entities": []}, {"text": "The experts were asked to specify the style from four options including the three defined above and a 'unclear style' option.", "labels": [], "entities": []}, {"text": "In addition, the experts were asked to score the poems in terms of compliance, fluency, aesthetic innovation, and scenario consistency, which we can use to check if the style transfer impacts the quality of the poem generation.", "labels": [], "entities": [{"text": "compliance", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9852900505065918}, {"text": "consistency", "start_pos": 123, "end_pos": 134, "type": "METRIC", "confidence": 0.7453288435935974}]}, {"text": "Note that we did not ask for the theme consistency to be scored in this experiment because the topic words were not presented to the experts, in order to prevent the topic affecting their judgment regarding the style.", "labels": [], "entities": []}, {"text": "The score ranges from 1 to 5, with a larger score being better.", "labels": [], "entities": []}, {"text": "presents the results with the general topics.", "labels": [], "entities": []}, {"text": "The numbers show the probabilities that the poems generated by a particular system were labeled as having various styles.", "labels": [], "entities": []}, {"text": "Since the topics are unbiased in types, the generation of the baseline system is assumed to be with unclear styles.", "labels": [], "entities": []}, {"text": "For other systems, the style of the generation is assumed to be the same as the style of their memories.", "labels": [], "entities": []}, {"text": "The results in clearly demonstrates these assumptions.", "labels": [], "entities": []}, {"text": "The tendency that romantic poems are recognized as pastoral poems is a little surprising.", "labels": [], "entities": []}, {"text": "Further analysis shows that experts tend to recognize romantic poems as pastoral poems only if there are any related symbols such as trees, mountain, river.", "labels": [], "entities": []}, {"text": "These words are very general in Chinese quatrains.", "labels": [], "entities": []}, {"text": "The indicator words of romantic poems such as skirt, rouge, and singing are not as popular and their indication power is not as strong, leading to less labeling of romantic poems, as shown in the results.", "labels": [], "entities": []}, {"text": "We also tested transferring from one style to another.", "labels": [], "entities": []}, {"text": "This was achieved by generating poems with some style-biased topics, and then using a style memory to force the generation to change the style.", "labels": [], "entities": []}, {"text": "Our experiments show that in 73% cases the style can be successfully transferred.", "labels": [], "entities": [{"text": "style", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9203684329986572}]}, {"text": "Finally, the scores of the poems generated with and without the style memories are shown in, where the poems generated with both general and style-biased topics are accounted for.", "labels": [], "entities": []}, {"text": "It can be seen that overall, the style transfer may degrade fluency a little.", "labels": [], "entities": [{"text": "style transfer", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.8069475889205933}]}, {"text": "This is understandable, as enforcing a particular style has to break the optimal generation with the baseline, which is assumed to be good at generating fluent poems.", "labels": [], "entities": []}, {"text": "Nevertheless the sacrifice is not significant.: Averaged scores for systems with or without style memory.", "labels": [], "entities": [{"text": "Averaged scores", "start_pos": 48, "end_pos": 63, "type": "METRIC", "confidence": 0.9522435665130615}]}, {"text": "to shows example poems generated by the system C 1 , C 1 +Mem and C 1 +Style Mem where the style in this case is set to be romantic.", "labels": [], "entities": []}, {"text": "The three poems were generated with the same, very general, topic ('g(oneself)').", "labels": [], "entities": []}, {"text": "More examples are given in the supporting material.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Preference ratios for systems with or without overfitting and with or without memory augmen- tation.", "labels": [], "entities": [{"text": "Preference", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9624788761138916}]}, {"text": " Table 3: Probability that poems generated by each  configuration with general topics are labeled as  various styles.", "labels": [], "entities": []}, {"text": " Table 4: Averaged scores for systems with or with- out style memory.", "labels": [], "entities": [{"text": "Averaged scores", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.95997354388237}]}]}