{"title": [{"text": "Data-Driven Broad-Coverage Grammars for Opinionated Natural Language Generation (ONLG)", "labels": [], "entities": [{"text": "Opinionated Natural Language Generation (ONLG)", "start_pos": 40, "end_pos": 86, "type": "TASK", "confidence": 0.7491402455738613}]}], "abstractContent": [{"text": "Opinionated natural language generation (ONLG) is anew, challenging, NLG task in which we aim to automatically generate human-like, subjective, responses to opinionated articles online.", "labels": [], "entities": [{"text": "Opinionated natural language generation (ONLG)", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7435537874698639}]}, {"text": "We present a data-driven architecture for ONLG that generates subjective responses triggered by users' agendas, based on automatically acquired wide-coverage generative grammars.", "labels": [], "entities": []}, {"text": "We compare three types of grammatical representations that we design for ONLG.", "labels": [], "entities": [{"text": "ONLG", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.6602287292480469}]}, {"text": "The grammars interleave different layers of linguistic information, and are induced from anew, enriched dataset we developed.", "labels": [], "entities": []}, {"text": "Our evaluation shows that generation with Relational-Realizational (Tsar-faty and Sima'an, 2008) inspired grammar gets better language model scores than lexicalized grammars\u00e0grammars`grammars\u00e0 la Collins (2003), and that the latter gets better human-evaluation scores.", "labels": [], "entities": []}, {"text": "We also show that conditioning the generation on topic models makes generated responses more relevant to the document content.", "labels": [], "entities": []}], "introductionContent": [{"text": "Interaction in social media has become increasingly prevalent nowadays.", "labels": [], "entities": []}, {"text": "It fundamentally changes the way businesses and consumers behave, it is instrumental to the success of individuals and businesses) and it also affects political regimes.", "labels": [], "entities": []}, {"text": "In particular, automatic interaction in natural language in social media is now a common theme, as seen in the rapid popularization of chat applications, chat-bots, and \"smart agents\" aiming to conduct human-like interactions in natural language.", "labels": [], "entities": []}, {"text": "So far, generation of human-like interaction in general has been addressed mostly commercially, where there is a movement towards online response automation, and movement away from script-based interaction towards interactive chat bots ().", "labels": [], "entities": []}, {"text": "These efforts provide an automated one-size-fits-all type of interaction, with no particular expression of particular sentiments, topics, or opinions.", "labels": [], "entities": []}, {"text": "In academia, work on generating human-like interaction focused so far on generating responses to tweets) or taking turns in short dialogs (.", "labels": [], "entities": []}, {"text": "However, the architectures assumed in these studies implement sequence to sequence (seq2seq) mappings, which do not take into account topics, sentiments or agendas of the intended responders.", "labels": [], "entities": []}, {"text": "Many real-world tasks and applications would benefit from automatic interaction that is generated intendedly based on a certain user profile or agenda.", "labels": [], "entities": []}, {"text": "For instance, this can help promoting apolitical candidate or asocial idea in social media, aiding people forming and expressing opinions on specific topics, or, in human-computer interfaces (HCI), making the computer-side generated utterances more meaningful, and ultimately more human-like (assuming that human-like interaction is very often affected by opinion, agenda, style, etc.).", "labels": [], "entities": []}, {"text": "In this work we address the opinionated natural language generation (ONLG) task, in which we aim to automatically generate human-like responses to opinionated articles.", "labels": [], "entities": [{"text": "opinionated natural language generation (ONLG)", "start_pos": 28, "end_pos": 74, "type": "TASK", "confidence": 0.7321027091571263}]}, {"text": "These responses address particular topics and reflect diverse sentiments towards them, in accordance to predefined user agendas.", "labels": [], "entities": []}, {"text": "This is an open-ended and unstructured generation challenge, which is closely tied to the communicative goals of actual human responders.", "labels": [], "entities": []}, {"text": "In previous work we addressed the ONLG challenge using a template-based approach).", "labels": [], "entities": [{"text": "ONLG", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.5179672837257385}]}, {"text": "The proposed system generated subjective responses to articles, driven by user agendas.", "labels": [], "entities": []}, {"text": "While the evaluation showed promising results in human-likeness and relevance ratings, the template-based system suffers from low output variety, which leads to a learning effect that reduced the perceived human-likeness of generated responses overtime.", "labels": [], "entities": []}, {"text": "In this work we tackle ONLG from a datadriven perspective, aiming to circumvent such learning effects and repetitive patterns in templatebased generation.", "labels": [], "entities": [{"text": "ONLG", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.7356963157653809}]}, {"text": "Here, we approach generation via automatically inducing broad-coverage generative grammars from a large corpus, and using them for response generation.", "labels": [], "entities": [{"text": "response generation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.798067569732666}]}, {"text": "More specifically, we define a grammar-based generation architecture and design different grammatical representations suitable for the ONLG task.", "labels": [], "entities": [{"text": "ONLG task", "start_pos": 135, "end_pos": 144, "type": "TASK", "confidence": 0.6478375792503357}]}, {"text": "Our grammars interleave different layers of linguistic information -including phrase-structure and dependency labels, lexical items, and levels of sentiment -with the goal of making responses both human-like and relevant.", "labels": [], "entities": []}, {"text": "In classical NLG terms, these grammars offer the opportunity for both micro-planning and surface realization) to unfold together.", "labels": [], "entities": []}, {"text": "We implement a generator and a search strategy to carryout the generation, and sort through possible candidates to get the best ones.", "labels": [], "entities": []}, {"text": "We evaluate the generated responses and the underlying grammars using automated metrics as well as human evaluation inspired by the Turing test (cf. and).", "labels": [], "entities": []}, {"text": "Our evaluation shows that while relational realizational (RR) inspired grammars) get good language model scores, simple head-driven lexicalized grammars\u00e0 grammars`grammars\u00e0 la Collins (2003) get better human rating and are more sensitive to sentiment.", "labels": [], "entities": [{"text": "relational realizational (RR) inspired grammars", "start_pos": 32, "end_pos": 79, "type": "TASK", "confidence": 0.7927208372524807}]}, {"text": "Furthermore, we show that incorporating topic models into the grammar-based generation makes the generated responses more relevant to the document content.", "labels": [], "entities": []}, {"text": "Finally, our human evaluations results show no learning effect.", "labels": [], "entities": []}, {"text": "That is, human raters are unable to discover in the generated responses typical structures that would lead them to consider the responses machine-generated.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "ln Section 2 we discuss the formal model, and in Section 3 we present the proposed end-toend ONLG architecture.", "labels": [], "entities": [{"text": "ONLG", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9306122064590454}]}, {"text": "In Section 4 we introduce the grammars we define, and we describe how we use them for generation in Section 5.", "labels": [], "entities": []}, {"text": "We follow that with our empirical evaluation in Section 6.", "labels": [], "entities": []}, {"text": "In Section 7 we discuss related and future work, and in Section 8 we summarize conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "We aim to evaluate the grammars' applicability to the ONLG task.", "labels": [], "entities": [{"text": "ONLG", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.6594926714897156}]}, {"text": "Set in an open domain, it is not trivial to find a \"gold-standard\" for this task, or even a method to obtain one.", "labels": [], "entities": []}, {"text": "Our evaluation thus follows two tracks: an automated assessment track, where we quantitatively assess the responses, and a Turing-like test similar to that of, where we aim to gauge human-likeness and response relevance.", "labels": [], "entities": []}, {"text": "Here we use Microsoft's WebLM API which is part of the Microsoft Oxford Project (Microsoft, 2011).", "labels": [], "entities": [{"text": "Microsoft Oxford Project (Microsoft, 2011)", "start_pos": 55, "end_pos": 97, "type": "DATASET", "confidence": 0.8620014935731888}]}, {"text": "We collected anew corpus of news articles and corresponding user comments from the NY-Times R website, using their open Community API.", "labels": [], "entities": [{"text": "NY-Times R website", "start_pos": 83, "end_pos": 101, "type": "DATASET", "confidence": 0.9252855181694031}]}, {"text": "We focus on sports news, which gave us 3,583 news articles and 13,100 user comments, or 55,700 sentences.", "labels": [], "entities": []}, {"text": "The articles are then used for training a topic model using the Mallet library).", "labels": [], "entities": [{"text": "Mallet library", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.9916556775569916}]}, {"text": "Next, we use the comments in the corpus to induce the grammars.", "labels": [], "entities": []}, {"text": "To obtain our Base representation we parse the sentences using the Stanford CoreNLP suite () which can provide both phrase-structure and sentiment annotation.", "labels": [], "entities": [{"text": "Stanford CoreNLP suite", "start_pos": 67, "end_pos": 89, "type": "DATASET", "confidence": 0.9382434089978536}]}, {"text": "To obtain our Lexicalized representation we follow the same procedure, this time also using a head-finder which locates the headword for each non-terminal.", "labels": [], "entities": []}, {"text": "To obtain the Relational-Realizational representation we followed the algorithm described in, which, given both a constituency parse and a dependency parse of a sentence, unifies them into a lexicalized and functional phrasestructure.", "labels": [], "entities": []}, {"text": "The merging is based on matching spans over words within the sentence.", "labels": [], "entities": []}, {"text": "In each, the system generates sentences with one grammar (G \u2208 {Base, Lex, RR}) and one scoring scheme (with/without topic model scores).", "labels": [], "entities": []}, {"text": "The results of each simulation are 5,000 responses for each variant of the system, consisting of 1,000 sentences for each sentiment class, s \u2208 {\u22122, \u22121, 0, 1, 2}.", "labels": [], "entities": []}, {"text": "The same 5000 generated sentences were used in all experiments.", "labels": [], "entities": []}, {"text": "We set the generator for trees of maximum depth of 13 which can yield up to 4096 words.", "labels": [], "entities": []}, {"text": "In reality, the realization was of much shorter sentences.", "labels": [], "entities": []}, {"text": "Examples for generated responses are given in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Responses generated by the system with  the different grammars and sentiment levels.", "labels": [], "entities": []}, {"text": " Table 2: Mean and 95% Confidence Interval (CI)  of language model scores, and measures of com- pactness and sentiment agreement. The last row,  HUMAN refers to the collected human responses.", "labels": [], "entities": [{"text": "Mean and 95% Confidence Interval (CI)", "start_pos": 10, "end_pos": 47, "type": "METRIC", "confidence": 0.8092432816823324}, {"text": "HUMAN", "start_pos": 145, "end_pos": 150, "type": "METRIC", "confidence": 0.9579168558120728}]}, {"text": " Table 5: Mean and 95% Confidence Interval (CI)  for human-likeness ratings (scaling 1:low-7:high).", "labels": [], "entities": [{"text": "Mean and 95% Confidence Interval (CI)", "start_pos": 10, "end_pos": 47, "type": "METRIC", "confidence": 0.8364134165975783}]}, {"text": " Table 6: Regression analysis of the human survey.", "labels": [], "entities": [{"text": "Regression", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9109110236167908}]}]}