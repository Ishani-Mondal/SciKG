{"title": [{"text": "Semi-supervised Multitask Learning for Sequence Labeling", "labels": [], "entities": [{"text": "Sequence Labeling", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.9290783703327179}]}], "abstractContent": [{"text": "We propose a sequence labeling framework with a secondary training objective , learning to predict surrounding words for every word in the dataset.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.6517229229211807}]}, {"text": "This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9962804913520813}]}, {"text": "The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging.", "labels": [], "entities": [{"text": "error detection in learner texts", "start_pos": 77, "end_pos": 109, "type": "TASK", "confidence": 0.8014692842960358}, {"text": "named entity recognition", "start_pos": 111, "end_pos": 135, "type": "TASK", "confidence": 0.6031339168548584}]}, {"text": "The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unan-notated data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Accurate and efficient sequence labeling models have a wide range of applications, including named entity recognition (NER), part-of-speech (POS) tagging, error detection and shallow parsing.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 93, "end_pos": 123, "type": "TASK", "confidence": 0.7654216637214025}, {"text": "part-of-speech (POS) tagging", "start_pos": 125, "end_pos": 153, "type": "TASK", "confidence": 0.6158624112606048}, {"text": "error detection", "start_pos": 155, "end_pos": 170, "type": "TASK", "confidence": 0.706835150718689}, {"text": "shallow parsing", "start_pos": 175, "end_pos": 190, "type": "TASK", "confidence": 0.7781118452548981}]}, {"text": "Specialised approaches to sequence labeling often include extensive feature engineering, such as integrated gazetteers, capitalisation features, morphological information and POS tags.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7402811944484711}]}, {"text": "However, recent work has shown that neural network architectures are able to achieve comparable or improved performance, while automatically discovering useful features fora specific task and only requiring a sequence of tokens as input.", "labels": [], "entities": []}, {"text": "This feature discovery is usually driven by an objective function based on predicting the annotated labels for each word, without much incentive to learn more general language features from the available text.", "labels": [], "entities": [{"text": "feature discovery", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.7873096168041229}]}, {"text": "In many sequence labeling tasks, the relevant labels in the dataset are very sparse and most of the words contribute very little to the training process.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.747766762971878}]}, {"text": "For example, in the CoNLL 2003 NER dataset) only 17% of the tokens represent an entity.", "labels": [], "entities": [{"text": "CoNLL 2003 NER dataset", "start_pos": 20, "end_pos": 42, "type": "DATASET", "confidence": 0.9382598251104355}]}, {"text": "This ratio is even lower for error detection, with only 14% of all tokens being annotated as an error in the FCE dataset).", "labels": [], "entities": [{"text": "error detection", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.6758940517902374}, {"text": "FCE dataset", "start_pos": 109, "end_pos": 120, "type": "DATASET", "confidence": 0.9750655591487885}]}, {"text": "The sequence labeling models are able to learn this bias in the label distribution without obtaining much additional information from words that have the majority label (O for outside of an entity; C for correct word).", "labels": [], "entities": []}, {"text": "Therefore, we propose an additional training objective which allows the models to make more extensive use of the available data.", "labels": [], "entities": []}, {"text": "The task of language modeling offers an easily accessible objective -learning to predict the next word in the sequence requires only plain text as input, without relying on any particular annotation.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7248836755752563}, {"text": "predict the next word in the sequence", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.788348308631352}]}, {"text": "Neural language modeling architectures also have many similarities to common sequence labeling frameworks: words are first mapped to distributed embeddings, followed by a recurrent neural network (RNN) module for composing word sequences into an informative context representation ().", "labels": [], "entities": [{"text": "Neural language modeling", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8087104757626852}]}, {"text": "Compared to any sequence labeling dataset, the task of language modeling has a considerably larger and more varied set of possible options to predict, making better use of each available word and encouraging the model to learn more general language features for accurate composition.", "labels": [], "entities": []}, {"text": "In this paper, we propose a neural sequence labeling architecture that is also optimised as a language model, predicting surrounding words in the dataset in addition to assigning labels to each token.", "labels": [], "entities": []}, {"text": "Specific sections of the network are op-timised as a forward-or backward-moving language model, while the label predictions are performed using context from both directions.", "labels": [], "entities": []}, {"text": "This secondary unsupervised objective encourages the framework to learn richer features for semantic composition without requiring additional training data.", "labels": [], "entities": []}, {"text": "We evaluate the sequence labeling model on 10 datasets from the fields of NER, POS-tagging, chunking and error detection in learner texts.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.6321359425783157}, {"text": "NER", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.5843257308006287}, {"text": "error detection", "start_pos": 105, "end_pos": 120, "type": "TASK", "confidence": 0.6700330078601837}]}, {"text": "Our experiments show that by including the unsupervised objective into the training process, the sequence labeling model achieves consistent performance improvements on all the benchmarks.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.6046020090579987}]}, {"text": "This multitask training framework gives the largest improvements on error detection datasets, outperforming the previous state-of-the-art architecture.", "labels": [], "entities": [{"text": "error detection", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.667031928896904}]}], "datasetContent": [{"text": "The proposed architecture was evaluated on 10 different sequence labeling datasets, covering the tasks of error detection, NER, chunking, and POStagging.", "labels": [], "entities": [{"text": "error detection", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.6966679990291595}, {"text": "NER", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.8654717206954956}, {"text": "chunking", "start_pos": 128, "end_pos": 136, "type": "TASK", "confidence": 0.9224557876586914}]}, {"text": "The word embeddings in the model were initialised with publicly available pretrained vectors, created using word2vec ( ).", "labels": [], "entities": []}, {"text": "For general-domain datasets we used 300-dimensional embeddings trained on Google News.", "labels": [], "entities": []}, {"text": "For biomedical datasets, the word embeddings were initialised with 200-dimensional vectors trained on PubMed and PMC.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 102, "end_pos": 108, "type": "DATASET", "confidence": 0.9760613441467285}, {"text": "PMC", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.8835747241973877}]}, {"text": "The neural network framework was implemented using Theano (Al-Rfou et al., 2016) and we make the code publicly available online.", "labels": [], "entities": [{"text": "Theano (Al-Rfou et al., 2016)", "start_pos": 51, "end_pos": 80, "type": "DATASET", "confidence": 0.891221784055233}]}, {"text": "For most of the hyperparameters, we follow the settings by  in order to facilitate direct comparison with previous work.", "labels": [], "entities": []}, {"text": "The LSTM hidden layers are set to size 200 in each direction for both word-and character-level components.", "labels": [], "entities": []}, {"text": "All digits in the text were replaced with the character 0; any words that occurred only once in the training data were replaced by an OOV token.", "labels": [], "entities": []}, {"text": "In order to reduce computational complexity in these experiments, the language modeling objective predicted only the 7,500 most frequent words, with an extra token covering all the other words.", "labels": [], "entities": []}, {"text": "Sentences were grouped into batches of size 64 and parameters were optimised using AdaDelta) with default learning rate 1.0.", "labels": [], "entities": [{"text": "AdaDelta", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.8729250431060791}]}, {"text": "Training was stopped when performance on the development set had not improved for 7 epochs.", "labels": [], "entities": []}, {"text": "Performance on the development set was also used to select the best model, which was then evaluated on the test set.", "labels": [], "entities": []}, {"text": "In order to avoid any outlier results due to randomness in the model initialisa- Based on development experiments, we found that error detection was the only task that did not benefit from having a CRF module at the output layer -since the labels are very sparse and the dataset contains only 2 possible labels, explicitly modeling state transitions does not improve performance on this task.", "labels": [], "entities": [{"text": "error detection", "start_pos": 129, "end_pos": 144, "type": "TASK", "confidence": 0.7064353674650192}]}, {"text": "Therefore, we use a softmax output for error detection experiments and CRF on all other datasets.", "labels": [], "entities": [{"text": "error detection", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.657415121793747}, {"text": "CRF", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.7560840249061584}]}, {"text": "The publicly available sequence labeling system by  is used as the baseline.", "labels": [], "entities": []}, {"text": "During development we found that applying dropout () on word embeddings improves performance on nearly all datasets, compared to this baseline.", "labels": [], "entities": []}, {"text": "Therefore, elementwise dropout was applied to each of the input embeddings with probability 0.5 during training, and the weights were multiplied by 0.5 during testing.", "labels": [], "entities": []}, {"text": "In order to separate the effects of this modification from the newly proposed optimisation method, we report results for three different systems: 1) the publicly available baseline, 2) applying dropout on top of the baseline system, and 3) applying both dropout and the novel multitask objective from Section 3.", "labels": [], "entities": []}, {"text": "Based on development experiments we set the value of \u03b3, which controls the importance of the language modeling objective, to 0.1 for all experiments throughout training.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7682391405105591}]}, {"text": "Since context prediction is not part of the main evaluation of sequence labeling systems, we expected the additional objective to mostly benefit early stages of training, whereas the model would later need to specialise only towards assigning labels.", "labels": [], "entities": [{"text": "context prediction", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.8431787490844727}]}, {"text": "Therefore, we also performed experiments on the development data where the value of \u03b3 was gradually decreased, but found that a small static value performed comparably well or even better.", "labels": [], "entities": []}, {"text": "These experiments indicate that the language modeling objective helps the network learn general-purpose features that are useful for sequence labeling even in the later stages of training.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.709576278924942}, {"text": "sequence labeling", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.701402485370636}]}], "tableCaptions": [{"text": " Table 1: Precision, Recall and F 0.5 score of alternative sequence labeling architectures on error detection  datasets. Dropout and LMcost modifications are added incrementally to the baseline.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9991501569747925}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9964660406112671}, {"text": "F 0.5", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.979017823934555}]}, {"text": " Table 2: Performance of alternative sequence labeling architectures on NER and chunking datasets,  measured using CoNLL standard entity-level F 1 score.", "labels": [], "entities": [{"text": "CoNLL standard entity-level F 1 score", "start_pos": 115, "end_pos": 152, "type": "METRIC", "confidence": 0.7564264237880707}]}, {"text": " Table 3: Accuracy of different sequence labeling architectures on POS-tagging datasets.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9664736986160278}, {"text": "POS-tagging datasets", "start_pos": 67, "end_pos": 87, "type": "DATASET", "confidence": 0.7997844815254211}]}]}