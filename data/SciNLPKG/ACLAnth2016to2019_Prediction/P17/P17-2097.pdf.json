{"title": [{"text": "Pay Attention to the Ending: Strong Neural Baselines for the ROC Story Cloze Task", "labels": [], "entities": [{"text": "ROC Story Cloze", "start_pos": 61, "end_pos": 76, "type": "DATASET", "confidence": 0.9260654250780741}]}], "abstractContent": [{"text": "We consider the ROC story cloze task (Mostafazadeh et al., 2016) and present several findings.", "labels": [], "entities": [{"text": "ROC story cloze", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.5675636927286783}]}, {"text": "We develop a model that uses hierarchical recurrent networks with attention to encode the sentences in the story and score candidate endings.", "labels": [], "entities": []}, {"text": "By discarding the large training set and only training on the validation set, we achieve an accuracy of 74.7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9996894598007202}]}, {"text": "Even when we discard the story plots (sentences before the ending) and only train to choose the better of two endings, we can still reach 72.5%.", "labels": [], "entities": []}, {"text": "We then analyze this \"ending-only\" task setting.", "labels": [], "entities": []}, {"text": "We estimate human accuracy to be 78% and find several types of clues that lead to this high accuracy, including those related to sentiment, negation, and general ending likelihood regardless of the story context.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9898613691329956}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9969610571861267}, {"text": "negation", "start_pos": 140, "end_pos": 148, "type": "TASK", "confidence": 0.7680423259735107}]}], "introductionContent": [{"text": "The ROC story cloze task () tests a system's ability to choose the more plausible of two endings to a story.", "labels": [], "entities": [{"text": "ROC story cloze", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.5844223996003469}]}, {"text": "The incorrect ending is written to still fit the world of the story, e.g., the protagonist typically appears in both endings.", "labels": [], "entities": []}, {"text": "The task is designed to test for \"commonsense\" knowledge, where the difference between the two endings lies in the plausibility of the characters' actions.", "labels": [], "entities": []}, {"text": "The best system of achieves 58.5% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9996752738952637}]}, {"text": "The ROC training and evaluation data differ in a key way.", "labels": [], "entities": []}, {"text": "The training set contains 5-sentence stories.", "labels": [], "entities": []}, {"text": "But the evaluation datasets (the validation and test sets) contain both a correct ending and an incorrect ending.", "labels": [], "entities": []}, {"text": "This means that the task is one of outlier detection: systems must estimate the density of correct endings in the training data and then detect which of the two endings is an outlier.", "labels": [], "entities": [{"text": "outlier detection", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7065504938364029}]}, {"text": "This becomes difficult when the evaluation contains distractors that are still somewhat plausible.", "labels": [], "entities": []}, {"text": "For example, a model may place mass on stories that consistently mention the same characters, but this will not be useful for the task because even the incorrect ending uses the correct character names.", "labels": [], "entities": []}, {"text": "In this paper, we discard the 50k training stories and train only on the 1871-story validation set.", "labels": [], "entities": [{"text": "1871-story validation set", "start_pos": 73, "end_pos": 98, "type": "DATASET", "confidence": 0.7110582292079926}]}, {"text": "We develop several neural models based on recurrent networks, comparing flat and hierarchical models for encoding the sentences in the story.", "labels": [], "entities": []}, {"text": "We also use an attention mechanism based on the ending to identify useful parts of the plot.", "labels": [], "entities": []}, {"text": "Our final model achieves 74.7% on the test set, outperforming all systems of and approaching the state of the art results of concurrent work ().", "labels": [], "entities": []}, {"text": "We then discard the first four sentences of each story and use our model to score endings alone.", "labels": [], "entities": []}, {"text": "We achieve 72.5% on the test set, outperforming most prior work without even looking at the story plots.", "labels": [], "entities": [{"text": "72.5", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9541833996772766}]}, {"text": "We do a small-scale manual study of this ending-only task, finding that humans can identify the better ending in approximately 78% of cases.", "labels": [], "entities": []}, {"text": "We report several reasons for the high accuracy of this ending-only setting, including some that are readily captured by automatic methods, such as sentiment analysis and the presence of negation words, as well as others that are more difficult, like those derived from world knowledge.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9988991022109985}, {"text": "sentiment analysis", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.9676470756530762}]}, {"text": "Our results and analysis, combined with the similar concurrent observations of, suggest that any meaningful system for the ROC task must outperform the best ending-only baselines.", "labels": [], "entities": [{"text": "ROC task", "start_pos": 123, "end_pos": 131, "type": "TASK", "confidence": 0.9249597787857056}]}], "datasetContent": [{"text": "We refer to a 5-sentence sequence as a story, the incomplete 4-sentence sequence as a plot, and the fifth sentence as an ending.", "labels": [], "entities": []}, {"text": "The ROC story corpus () contains training, validation, and test sets.", "labels": [], "entities": [{"text": "ROC story corpus", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8189614017804464}]}, {"text": "The training set contains 5-sentence stories.", "labels": [], "entities": []}, {"text": "The validation and test sets contain 4-sentence plots followed by two candidate endings, with only one correct.", "labels": [], "entities": []}, {"text": "evaluated several methods for solving the task.", "labels": [], "entities": []}, {"text": "Since the training set does not contain incorrect endings, their methods are based on computing similarity between the plot and ending.", "labels": [], "entities": []}, {"text": "Their best results were obtained with the Deep Structured Semantic Model (DSSM) () which represents texts using character trigram counts followed by neural network layers and a similarity function.", "labels": [], "entities": []}, {"text": "Concurrently with our work, the LSDSem 2017 shared task was held (, focusing on the ROC story cloze task.", "labels": [], "entities": [{"text": "LSDSem 2017 shared task", "start_pos": 32, "end_pos": 55, "type": "DATASET", "confidence": 0.8512724488973618}, {"text": "ROC story cloze task", "start_pos": 84, "end_pos": 104, "type": "DATASET", "confidence": 0.7819710969924927}]}, {"text": "Several of the participants made similar observations to what we describe here, namely that supervised learning on the validation set is more effective than learning directly from the training set, as well as noting certain biases in the endings ().", "labels": [], "entities": []}, {"text": "We shuffle and split the validation set into 5 folds and do 5-fold cross validation.", "labels": [], "entities": [{"text": "validation", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.9712981581687927}]}, {"text": "For modeling decisions, we tune based on the average accuracy of the held-out folds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9988864064216614}]}, {"text": "For final experiments, we choose the fold with the best held-out accuracy and report its test set accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9978768825531006}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9910815954208374}]}, {"text": "We use Adam () for optimization with learning rate 0.001 and mini-batch size 50.", "labels": [], "entities": []}, {"text": "We use pretrained 300-dimensional GloVe embeddings trained on Wikipedia and Gigaword) and keep them fixed during training.", "labels": [], "entities": []}, {"text": "We use L 2 regularization for the score feed-forward network, which has a single hidden layer of size 512.", "labels": [], "entities": []}, {"text": "We use 300 for the LSTM hidden vector dimensionality for both encoders.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracies (%) averaged over held-out  folds of 5-fold cross validation. Comparing hier- archical (HIER) and non-hierarchical (FLAT) en- coders, and encoding story (ENCSTORY) vs. sepa- rately encoding plot and ending (ENCPLOTEND).  No attention is used.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.999261200428009}]}, {"text": " Table 2: For the ENCSTORY and ENCPLOTEND  models, showing the contribution of adding atten- tion (+ATT). All models use the HIER encoder.", "labels": [], "entities": [{"text": "ATT", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.8774827718734741}, {"text": "HIER encoder", "start_pos": 125, "end_pos": 137, "type": "DATASET", "confidence": 0.8758864998817444}]}, {"text": " Table 3: Final results.  *  = estimate from 100; see  Section 6.1.  \u2021 = from Mostafazadeh et al. (2016).", "labels": [], "entities": []}, {"text": " Table 4: Ending selection rules exhibiting biases  in endings. Final column shows correlation be- tween each feature and the score of our model.", "labels": [], "entities": []}]}