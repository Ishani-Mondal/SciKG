{"title": [{"text": "Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks", "labels": [], "entities": [{"text": "Abstract Meaning Representation Parsing", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7032125145196915}, {"text": "Recurrent Neural Networks", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.5036680201689402}]}], "abstractContent": [{"text": "We present a system which parses sentences into Abstract Meaning Representations , improving state-of-the-art results for this task by more than 5%.", "labels": [], "entities": [{"text": "parses sentences into Abstract Meaning Representations", "start_pos": 26, "end_pos": 80, "type": "TASK", "confidence": 0.7198316752910614}]}, {"text": "AMR graphs represent semantic content using linguistic properties such as semantic roles, coref-erence, negation, and more.", "labels": [], "entities": []}, {"text": "The AMR parser does not rely on a syntactic pre-parse, or heavily engineered features, and uses five recurrent neural networks as the key architectural components for inferring AMR graphs.", "labels": [], "entities": [{"text": "AMR parser", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.70041424036026}]}], "introductionContent": [{"text": "Semantic analysis is the process of extracting meaning from text, revealing key ideas such as \"who did what to whom, when, how, and where?\", and is considered to be one of the most complex tasks in natural language processing.", "labels": [], "entities": [{"text": "Semantic analysis is the process of extracting meaning from text, revealing key ideas such as \"who did what to whom, when, how, and where?\"", "start_pos": 0, "end_pos": 139, "type": "Description", "confidence": 0.8022809147834777}]}, {"text": "Historically, an important consideration has been the definition of the output of the task -how can the concepts in a sentence be captured in a general, consistent and expressive manner that facilitates downstream semantic processing?", "labels": [], "entities": []}, {"text": "Over the years many formalisms have been proposed as suitable target representations including variants of first order logic, semantic networks, and frame-based slot-filler notations.", "labels": [], "entities": []}, {"text": "Such representations have found a place in many semantic applications but there is no clear consensus as to the best representation.", "labels": [], "entities": []}, {"text": "However, with the rise of supervised machine learning techniques, anew requirement has come to the fore: the ability of human annotators to quickly and reliably generate semantic representations as training data.", "labels": [], "entities": []}, {"text": "Abstract Meaning Representation (AMR) (Banarescu et al., 2012) 1 was developed to provide 1 http://amr.isi.edu/language.html a computationally useful and expressive representation that could be reliably generated by human annotators.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR) (Banarescu et al., 2012)", "start_pos": 0, "end_pos": 62, "type": "TASK", "confidence": 0.7703295716872582}]}, {"text": "Sentence meanings in AMR are represented in the form of graphs consisting of concepts (nodes) connected by labeled relations (edges).", "labels": [], "entities": [{"text": "Sentence meanings", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8827451467514038}, {"text": "AMR", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.6769379377365112}]}, {"text": "AMR graphs include a number of traditional NLP representations including named entities (, word senses (), coreference relations, and predicate-argument structures).", "labels": [], "entities": []}, {"text": "More recent innovations include wikification of named entities and normalization of temporal expressions.", "labels": [], "entities": []}, {"text": "provides an insightful discussion of the relationship between AMR and other formal representations including first order logic.", "labels": [], "entities": [{"text": "AMR", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9037721753120422}]}, {"text": "The process of creating AMR's for sentences is called AMR Parsing and was first introduced in (.", "labels": [], "entities": [{"text": "AMR Parsing", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.7253341376781464}]}, {"text": "A key factor driving the development of AMR systems has been the increasing availability of training resources in the form of corpora where each sentence is paired with a corresponding AMR representation 2 . A consistent framework for evaluating AMR parsers was defined by the Semeval-2016 Meaning Representation Parsing Task . Standard training, development and test splits for the AMR Annotation Release 1 corpus are provided, as well as an additional out-of-domain test dataset, for system comparisons.", "labels": [], "entities": [{"text": "Semeval-2016 Meaning Representation Parsing Task", "start_pos": 277, "end_pos": 325, "type": "TASK", "confidence": 0.651461946964264}, {"text": "AMR Annotation Release 1 corpus", "start_pos": 383, "end_pos": 414, "type": "DATASET", "confidence": 0.6611191987991333}]}, {"text": "Viewed as a structured prediction task, AMR parsing poses some difficult challenges not faced by other related language processing tasks including part of speech tagging, syntactic parsing or se-  name (a) An AMR graphical depiction of the meaning of the sentence France plans further nuclear cooperation with numerous countries . Concepts are represented as ovals, and relations are the directed connections between them.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9722837507724762}, {"text": "speech tagging", "start_pos": 155, "end_pos": 169, "type": "TASK", "confidence": 0.729479655623436}, {"text": "syntactic parsing", "start_pos": 171, "end_pos": 188, "type": "TASK", "confidence": 0.7570863366127014}]}, {"text": "Predicate concepts are labelled with their PropBank sense, and semantic roles are indicated by \"Arg\" relations.", "labels": [], "entities": [{"text": "Arg", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9681202173233032}]}, {"text": "Non-Arg relations like name or mod are called \"Nargs\" in this paper.", "labels": [], "entities": []}, {"text": "Note the shaded section, which shows an example of a subgraph, containing related concepts and relations.", "labels": [], "entities": []}, {"text": "In the example, the subgraph represents \"France\" which includes the category country and a shortened link to the France wiki page.", "labels": [], "entities": [{"text": "France wiki page", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.944794774055481}]}, {"text": "(b) General Architecture for the AMR Parser, which creates an AMR based on the words in a sentence.", "labels": [], "entities": [{"text": "AMR Parser", "start_pos": 33, "end_pos": 43, "type": "TASK", "confidence": 0.58660688996315}]}, {"text": "The 5 B-LSTM networks infer structures of the AMR.", "labels": [], "entities": [{"text": "AMR", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.8456112146377563}]}, {"text": "For example, the SG network infers subgraphs, which are mostly single concept, like \"plan-01\" or \"further\", but can also be like the more complex shaded \"France\" subgraph in the example.", "labels": [], "entities": []}, {"text": "Other B-LSTM networks are used to infer predicate argument relations (Args), other relations (Nargs), attributes like \"TOP\" (Attr) and name categories like \"country\" for France (Ncat).", "labels": [], "entities": [{"text": "TOP", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9009941816329956}]}, {"text": "The prediction task in these settings can be cast as per-token labeling tasks (i.e. IOB tags) or as a sequence of discrete parser actions, as in transition-based (shift-reduce) approaches to dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 191, "end_pos": 209, "type": "TASK", "confidence": 0.8208130598068237}]}, {"text": "The first challenge is that AMR representations are by design abstracted away from their associated surface forms.", "labels": [], "entities": [{"text": "AMR representations", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.9203466773033142}]}, {"text": "AMR corpora pair sentences with their corresponding representations, without providing an explicit annotation, or alignment, that links the parts of the representation to their corresponding elements of the sentence.", "labels": [], "entities": []}, {"text": "Not surprisingly, this complicates training, decoding and evaluation.", "labels": [], "entities": []}, {"text": "The second challenge is the fact that, as noted earlier, the AMR parsing task is an amalgam of predicate identification and classification, entity recognition, co-reference, word sense disambiguation and semantic role labeling -each of which relies on the others for successful analysis.", "labels": [], "entities": [{"text": "AMR parsing task", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.9496258894602457}, {"text": "predicate identification and classification", "start_pos": 95, "end_pos": 138, "type": "TASK", "confidence": 0.8434314876794815}, {"text": "entity recognition", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.7647019326686859}, {"text": "word sense disambiguation", "start_pos": 174, "end_pos": 199, "type": "TASK", "confidence": 0.6855220198631287}, {"text": "semantic role labeling", "start_pos": 204, "end_pos": 226, "type": "TASK", "confidence": 0.620988001426061}]}, {"text": "The architecture and system presented in the following sections is largely motivated by these two challenges.", "labels": [], "entities": []}], "datasetContent": [{"text": "Semantic graph comparison can be tricky because direct graph alignment fails in the presence of just a few miscompares.", "labels": [], "entities": [{"text": "Semantic graph comparison", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7693099776903788}, {"text": "direct graph alignment", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.6924074292182922}]}, {"text": "A practical graph comparison program called Smatch) is used to consistently evaluate AMR parsers.", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.8575077056884766}]}, {"text": "The smatch python script provides an F1 evaluation metric for whole-sentence semantic graph analysis by comparing sets of triples which describe portions of the graphs, and uses a hill climbing algorithm for efficiency.", "labels": [], "entities": [{"text": "F1", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.9957517385482788}, {"text": "whole-sentence semantic graph analysis", "start_pos": 62, "end_pos": 100, "type": "TASK", "confidence": 0.6263928860425949}]}, {"text": "All networks, including SG, were trained using stochastic gradient descent (SGD) with a fixed learning rate.", "labels": [], "entities": []}, {"text": "We tried sentence level loglikelihood, which trains a viterbi decoder, as a training objective, but found no improvement over word-level likelihood (cross entropy).", "labels": [], "entities": []}, {"text": "After all LSTM and linear layers, we added dropout to minimize overfitting ( and batch normalization to reduce sensitivity to learning rates and initialization.", "labels": [], "entities": []}, {"text": "For each of the five networks, we used the LDC2015E86 training split to train parameters, and periodically interrupted training to run the dev split (forward) in order to monitor performance.", "labels": [], "entities": [{"text": "LDC2015E86 training split", "start_pos": 43, "end_pos": 68, "type": "DATASET", "confidence": 0.8742359280586243}]}, {"text": "The model parameters which resulted in best dev performance were saved as the final model.", "labels": [], "entities": []}, {"text": "The test split was used as the \"in domain\" data set to assess the fully assembled parser.", "labels": [], "entities": []}, {"text": "The inferred AMR's were then evaluated using the smatch program to produce an F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.979237824678421}]}, {"text": "An evaluation dataset was provided for Semeval 2016 task 8, which is significantly different from the LDC2015E86 split dataset.", "labels": [], "entities": [{"text": "Semeval 2016 task 8", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7319879084825516}, {"text": "LDC2015E86 split dataset", "start_pos": 102, "end_pos": 126, "type": "DATASET", "confidence": 0.9061549703280131}]}, {"text": "((2016) describes the eval dataset as \"quite difficult to parse, particularly due to creative approaches to word representation in the web forum portion\").", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SG Network Example Output", "labels": [], "entities": [{"text": "SG Network Example Output", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8066039234399796}]}, {"text": " Table 3: Smatch F1 results for our parser and top 5 parsers from semeval 2016 task 8.", "labels": [], "entities": [{"text": "F1", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.895256519317627}]}]}