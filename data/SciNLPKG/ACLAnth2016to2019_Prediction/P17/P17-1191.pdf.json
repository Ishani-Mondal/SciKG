{"title": [{"text": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment", "labels": [], "entities": [{"text": "Prepositional Phrase Attachment", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.7530636390050253}]}], "abstractContent": [{"text": "Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language.", "labels": [], "entities": []}, {"text": "Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9300693273544312}]}, {"text": "We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters.", "labels": [], "entities": [{"text": "predicting prepositional phrase (PP) attachments", "start_pos": 60, "end_pos": 108, "type": "TASK", "confidence": 0.8802092926842826}]}, {"text": "We show that using context-sensitive em-beddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.999678373336792}, {"text": "PP attachment", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.7324710190296173}, {"text": "errors", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9811627864837646}]}], "introductionContent": [{"text": "Type-level word embeddings map a word type (i.e., a surface form) to a dense vector of real numbers such that similar word types have similar embeddings.", "labels": [], "entities": []}, {"text": "When pre-trained on a large corpus of unlabeled text, they provide an effective mechanism for generalizing statistical models to words which do not appear in the labeled training data fora downstream task.", "labels": [], "entities": []}, {"text": "In accordance with standard terminology, we make the following distinction between types and tokens in this paper: By word types, we mean the surface form of the word, whereas by tokens we mean the instantiation of the surface form in a context.", "labels": [], "entities": []}, {"text": "For example, the same word type 'pool' occurs as two different tokens in the sentences \"He sat by the pool,\" and \"He played a game of pool.\"", "labels": [], "entities": []}, {"text": "Most word embedding models define a single vector for each word type.", "labels": [], "entities": []}, {"text": "However, a fundamental flaw in this design is their inability to distinguish between different meanings and abstractions of the same word.", "labels": [], "entities": []}, {"text": "In the two sentences shown above, the word 'pool' has different meanings, but the same representation is typically used for both of them.", "labels": [], "entities": []}, {"text": "Similarly, the fact that 'pool' and 'lake' are both kinds of water bodies is not explicitly incorporated inmost type-level embeddings.", "labels": [], "entities": []}, {"text": "Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g.,, potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space.", "labels": [], "entities": []}, {"text": "Previous work partially addresses these problems by estimating concept embeddings in WordNet (e.g.,, or improving word representations using information from knowledge graphs (e.g.,.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.8608191013336182}]}, {"text": "However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings.", "labels": [], "entities": []}, {"text": "In this work, we represent a word token in a given context by estimating a context-sensitive probability distribution over relevant concepts in WordNet and use the expected value (i.e., weighted sum) of the concept embeddings as the token representation (see \u00a72).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 144, "end_pos": 151, "type": "DATASET", "confidence": 0.9658493995666504}]}, {"text": "We take a task-centric approach towards doing this, and learn the token representations jointly with the task-specific parameters.", "labels": [], "entities": []}, {"text": "In addition to providing context-sensitive token embeddings, the proposed method implicitly regularizes the embeddings of related words by forcing related words to share similar concept embeddings.", "labels": [], "entities": []}, {"text": "As a result, the representation of a rare word which does not appear in the training data fora downstream task benefits from all the updates to related words which share one or more concept embeddings.: An example grounding for the word 'pool'.", "labels": [], "entities": []}, {"text": "Solid arrows represent possible senses and dashed arrows represent hypernym relations.", "labels": [], "entities": []}, {"text": "Note that the same set of concepts are used to ground the word 'pool' regardless of its context.", "labels": [], "entities": []}, {"text": "Other WordNet senses for 'pool' were removed from the figure for simplicity.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 6, "end_pos": 13, "type": "DATASET", "confidence": 0.9340356588363647}]}, {"text": "Our approach to context-sensitive embeddings assumes the availability of a lexical ontology.", "labels": [], "entities": []}, {"text": "While this work relies on WordNet, and we exploit the order of senses given by WordNet, our model is, in principle applicable to any ontology, with appropriate modifications.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9651342630386353}, {"text": "WordNet", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.9647519588470459}]}, {"text": "In this work, we do not assume the inputs are sense tagged.", "labels": [], "entities": []}, {"text": "We use the proposed embeddings to predict prepositional phrase (PP) attachments (see \u00a73), a challenging problem which emphasizes the selectional preferences between words in the PP and each of the candidate head words.", "labels": [], "entities": [{"text": "prepositional phrase (PP) attachments", "start_pos": 42, "end_pos": 79, "type": "TASK", "confidence": 0.6890121301015218}]}, {"text": "Our empirical results and detailed analysis (see \u00a74) show that the proposed embeddings effectively use WordNet to improve the accuracy of PP attachment predictions.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9609469175338745}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9983308911323547}, {"text": "PP attachment predictions", "start_pos": 138, "end_pos": 163, "type": "TASK", "confidence": 0.8693805734316508}]}], "datasetContent": [{"text": "We used the English PP attachment dataset created and made available by.", "labels": [], "entities": [{"text": "English PP attachment dataset", "start_pos": 12, "end_pos": 41, "type": "DATASET", "confidence": 0.5916036367416382}]}, {"text": "The training and test splits contain 33,359 and 1951 labeled examples respectively.", "labels": [], "entities": []}, {"text": "As explained in \u00a73.1, the input for each example is 1) an ordered list of candidate head words, 2) the preposition, and 3) the direct dependent of the preposition.", "labels": [], "entities": []}, {"text": "The head words are either nouns or verbs and the dependent is always a noun.", "labels": [], "entities": []}, {"text": "All examples in this dataset have at least two candidate head words.", "labels": [], "entities": []}, {"text": "As discussed in, this dataset is a more realistic PP attachment task than the RRR dataset (.", "labels": [], "entities": [{"text": "PP attachment task", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8895937403043112}, {"text": "RRR dataset", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.8367231488227844}]}, {"text": "The RRR dataset is a binary classification task with exactly two headword candidates in all examples.", "labels": [], "entities": [{"text": "RRR dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7428200393915176}]}, {"text": "The context for each example in the RRR dataset is also limited which defeats the purpose of our context-sensitive embeddings.", "labels": [], "entities": [{"text": "RRR dataset", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.8343948125839233}]}], "tableCaptions": [{"text": " Table 1: Results on Belinkov et al. (2014)'s PPA test set. HPCD (full) is from the original paper, and  it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted (Faruqui et al., 2015) to WordNet  3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend (Rothe and  Sch\u00fctze, 2015) on GloVe.", "labels": [], "entities": [{"text": "PPA test set", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.8103675444920858}, {"text": "HPCD", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.8885502815246582}]}, {"text": " Table 2: Results from RBG dependency parser  with features coming from various PP attachment  predictors and oracle attachments.", "labels": [], "entities": [{"text": "RBG dependency parser", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.808779756228129}]}]}