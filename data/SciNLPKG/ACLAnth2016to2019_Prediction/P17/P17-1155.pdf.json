{"title": [{"text": "Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation", "labels": [], "entities": [{"text": "Sarcasm SIGN", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.7539135813713074}, {"text": "Sentiment Based Monolingual Machine Translation", "start_pos": 40, "end_pos": 87, "type": "TASK", "confidence": 0.5885064125061035}]}], "abstractContent": [{"text": "Sarcasm is a form of speech in which speakers say the opposite of what they truly mean in order to convey a strong sentiment.", "labels": [], "entities": [{"text": "Sarcasm is a form of speech in which speakers say the opposite of what they truly mean in order to convey a strong sentiment", "start_pos": 0, "end_pos": 124, "type": "Description", "confidence": 0.7448780611157417}]}, {"text": "In other words, \"Sarcasm is the giant chasm between what I say, and the person who doesn't get it.\".", "labels": [], "entities": []}, {"text": "In this paper we present the novel task of sarcasm interpretation, defined as the generation of a non-sarcastic utterance conveying the same message as the original sarcastic one.", "labels": [], "entities": [{"text": "sarcasm interpretation", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.8945605456829071}]}, {"text": "We introduce a novel dataset of 3000 sarcastic tweets, each interpreted by five human judges.", "labels": [], "entities": []}, {"text": "Addressing the task as mono-lingual machine translation (MT), we experiment with MT algorithms and evaluation measures.", "labels": [], "entities": [{"text": "mono-lingual machine translation (MT)", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.8258697589238485}, {"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.968578577041626}]}, {"text": "We then present SIGN: an MT based sarcasm interpretation algorithm that targets sentiment words, a defining element of textual sarcasm.", "labels": [], "entities": [{"text": "MT based sarcasm interpretation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.7339167892932892}]}, {"text": "We show that while the scores of n-gram based automatic measures are similar for all interpretation models, SIGN's interpretations are scored higher by humans for adequacy and sentiment polarity.", "labels": [], "entities": []}, {"text": "We conclude with a discussion on future research directions for our new task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sarcasm is a sophisticated form of communication in which speakers convey their message in an indirect way.", "labels": [], "entities": []}, {"text": "It is defined in the MerriamWebster dictionary as the use of words that mean the opposite of what Our dataset, consisting of 3000 sarcastic tweets each augmented with five interpretations, is available in the project page: https://github.com/Lotemp/ SarcasmSIGN.", "labels": [], "entities": [{"text": "MerriamWebster dictionary", "start_pos": 21, "end_pos": 46, "type": "DATASET", "confidence": 0.9868586659431458}]}, {"text": "The page also contains the sarcasm interpretation guidelines, the code of the SIGN algorithms and other materials related to this project.", "labels": [], "entities": [{"text": "sarcasm interpretation", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.8377810418605804}]}, {"text": "one would really want to say in order to insult someone, to show irritation, or to be funny.", "labels": [], "entities": []}, {"text": "Considering this definition, it is not surprising to find frequent use of sarcastic language in opinionated user generated content, in environments such as Twitter, Facebook, Reddit and many more.", "labels": [], "entities": []}, {"text": "In textual communication, knowledge about the speaker's intent is necessary in order to fully understand and interpret sarcasm.", "labels": [], "entities": [{"text": "interpret sarcasm", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.8058523535728455}]}, {"text": "Consider, for example, the sentence \"what a wonderful day\".", "labels": [], "entities": []}, {"text": "A literal analysis of this sentence demonstrates a positive experience, due to the use of the word wonderful.", "labels": [], "entities": []}, {"text": "However, if we knew that the sentence was meant sarcastically, wonderful would turn into a word of a strong negative sentiment.", "labels": [], "entities": []}, {"text": "In spoken language, sarcastic utterances are often accompanied by a certain tone of voice which points out the intent of the speaker, whereas in textual communication, sarcasm is inherently ambiguous, and its identification and interpretation maybe challenging even for humans.", "labels": [], "entities": [{"text": "identification and interpretation", "start_pos": 209, "end_pos": 242, "type": "TASK", "confidence": 0.7602543036142985}]}, {"text": "In this paper we present the novel task of interpretation of sarcastic utterances.", "labels": [], "entities": [{"text": "interpretation of sarcastic utterances", "start_pos": 43, "end_pos": 81, "type": "TASK", "confidence": 0.8861642628908157}]}, {"text": "We define the purpose of the interpretation task as the capability to generate a non-sarcastic utterance that captures the meaning behind the original sarcastic text.", "labels": [], "entities": []}, {"text": "Our work currently targets the Twitter domain since it is a medium in which sarcasm is prevalent, and it allows us to focus on the interpretation of tweets marked with the content tag #sarcasm.", "labels": [], "entities": []}, {"text": "And so, for example, given the tweet \"how I love Mondays.", "labels": [], "entities": []}, {"text": "#sarcasm\" we would like our system to generate interpretations such as \"how I hate Mondays\" or \"I really hate Mondays\".", "labels": [], "entities": []}, {"text": "In order to learn such interpretations, we constructed a parallel corpus of 3000 sarcastic tweets, each of which has five non-sarcastic interpretations (Section 3).", "labels": [], "entities": []}, {"text": "Our task is complex since sarcasm can be expressed in many forms, it is ambiguous in nature and its understanding may require world knowl-edge.", "labels": [], "entities": []}, {"text": "Following are several examples taken from our corpus: 1.", "labels": [], "entities": []}, {"text": "loving life so much right now.", "labels": [], "entities": []}, {"text": "Great, a choice between two excellent candidates, Donald Trump or Hillary Clinton.", "labels": [], "entities": []}, {"text": "#sarcasm In example (1) it is quite straightforward to seethe exaggerated positive sentiment used in order to convey strong negative feelings.", "labels": [], "entities": []}, {"text": "Examples (2) and (3), however, do not contain any excessive sentiment.", "labels": [], "entities": []}, {"text": "Instead, previous knowledge is required if one wishes to fully understand and interpret what went wrong with California, or who Hillary Clinton and Donald Trump are.", "labels": [], "entities": []}, {"text": "Since sarcasm is a refined and indirect form of speech, its interpretation maybe challenging for certain populations.", "labels": [], "entities": []}, {"text": "For example, studies show that children with deafness, autism or Asperger's Syndrome struggle with non literal communication such as sarcastic language.", "labels": [], "entities": [{"text": "Asperger", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9780220985412598}]}, {"text": "Moreover, since sarcasm transforms the polarity of an apparently positive or negative expression into its opposite, it poses a challenge for automatic systems for opinion mining, sentiment analysis and extractive summarization ().", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 163, "end_pos": 177, "type": "TASK", "confidence": 0.8598555326461792}, {"text": "sentiment analysis", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.9497382342815399}, {"text": "extractive summarization", "start_pos": 202, "end_pos": 226, "type": "TASK", "confidence": 0.6269005537033081}]}, {"text": "Extracting the honest meaning behind the sarcasm may alleviate such issues.", "labels": [], "entities": [{"text": "Extracting the honest meaning behind the sarcasm", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8243997267314366}]}, {"text": "In order to design an automatic sarcasm interpretation system, we first rely on previous work in established similar tasks (section 2), particularly machine translation (MT), borrowing algorithms as well as evaluation measures.", "labels": [], "entities": [{"text": "sarcasm interpretation", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.8161446154117584}, {"text": "machine translation (MT)", "start_pos": 149, "end_pos": 173, "type": "TASK", "confidence": 0.8189041554927826}]}, {"text": "In section 4 we discuss the automatic evaluation measures we apply in our work and present human based measures for: (a) the fluency of a generated nonsarcastic utterance, (b) its adequacy as interpretation of the original sarcastic tweet's meaning, and (c) whether or not it captures the sentiment of the original tweet.", "labels": [], "entities": [{"text": "interpretation of the original sarcastic tweet's meaning", "start_pos": 192, "end_pos": 248, "type": "TASK", "confidence": 0.6877083778381348}]}, {"text": "Then, in section 5, we explore the performance of prominent phrase-based and neural MT systems on our task in development data experiments.", "labels": [], "entities": []}, {"text": "We next present the Sarcasm SIGN (Sarcasm Sentimental Interpretation GeNerator, section 6), our novel MT based algorithm which puts a special emphasis on sentiment words.", "labels": [], "entities": [{"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9672524333000183}]}, {"text": "Lastly, in Section 7 we assess the performance of the various algorithms and show that while they perform similarly in terms of automatic MT evaluation, SIGN is superior according to the human measures.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 138, "end_pos": 151, "type": "TASK", "confidence": 0.9301726818084717}, {"text": "SIGN", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.918410062789917}]}, {"text": "We conclude with a discussion on future research directions for our task, regarding both algorithms and evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "As mentioned above, in certain cases world knowledge is mandatory in order to correctly evaluate sarcasm interpretations.", "labels": [], "entities": [{"text": "evaluate sarcasm interpretations", "start_pos": 88, "end_pos": 120, "type": "TASK", "confidence": 0.6826141873995463}]}, {"text": "For example, in the case of the second sarcastic tweet in table 1, we need to know that 2:30 is considered a late hour so that staying up till 2:30 and staying up late would be considered equivalent despite the lexical difference.", "labels": [], "entities": []}, {"text": "Furthermore, we notice that transforming a sarcastic utterance into anon sarcastic one often requires to change a small number of words.", "labels": [], "entities": []}, {"text": "For example, a single word change in the sarcastic tweet \"How I love Mondays.", "labels": [], "entities": []}, {"text": "#sarcasm\" leads to the non-sarcastic utterance How I hate Mondays.", "labels": [], "entities": []}, {"text": "This is not typical for MT, where usually the entire source sentence is translated to anew sentence in the target language and we would expect lexical similarity between the machine generated translation and the human reference it is compared to.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9916822910308838}]}, {"text": "This raises a doubt as to whether n-gram based MT evaluation measures such as the aforementioned are suitable for our task.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9043335318565369}]}, {"text": "We hence asses the quality of an interpretation using automatic evaluation measures from the tasks of MT, paraphrasing, and summarization (Section 2), and compare these measures to human-based measures.", "labels": [], "entities": [{"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9886460304260254}, {"text": "summarization", "start_pos": 124, "end_pos": 137, "type": "TASK", "confidence": 0.9782376289367676}]}, {"text": "Automatic Measures We use BLEU and ROUGE as measures of n-gram precision and recall, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9992855191230774}, {"text": "ROUGE", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9972620010375977}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9845646619796753}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9996832609176636}]}, {"text": "We report scores of ROUGE-1, ROUGE-2 and ROUGE-L (recall based on unigrams, bigrams and longest common subsequence between candidate and reference, respectively).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9934682846069336}, {"text": "ROUGE-2", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9501846432685852}, {"text": "ROUGE-L", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9914604425430298}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9980261921882629}]}, {"text": "In order to asses the n-gram novelty of interpretations (i.e, difference from the source), we report PINC and PINC * sigmoid(BLEU) (see Section 2).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.8302463889122009}]}, {"text": "Human judgments We employed an additional group of five Fiverr workers and asked them to score each generated interpretations with two scores on a 1-7 scale, 7 being the best.", "labels": [], "entities": []}, {"text": "The scores  We experiment with SIGN and the Moses and RNN baselines at the same setup of section 5.", "labels": [], "entities": [{"text": "SIGN", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.6157921552658081}, {"text": "RNN baselines", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.6040114760398865}]}, {"text": "We report test set results for automatic and human measures, in respectively.", "labels": [], "entities": []}, {"text": "As in the development data experiments, the RNN presents critically low adequacy scores of 2.11 across the entire test set and of 1.89 in cases where the interpretation and the tweet differ.", "labels": [], "entities": []}, {"text": "This, along with its low fluency scores (5.74 and 5.43 respectively) and its very low BLEU and ROUGE scores make us deem this model immature for our task and dataset, hence we exclude it from this section's tables and do not discuss it further.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9994822144508362}, {"text": "ROUGE", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9503527879714966}]}, {"text": "In terms of automatic evaluation, SIGN and Moses do not perform significantly different.", "labels": [], "entities": []}, {"text": "When it comes to human evaluation (Table 6) however, SIGN-context presents substantial gains.", "labels": [], "entities": [{"text": "SIGN-context", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.8107749223709106}]}, {"text": "While for fluency Moses and SIGN-context perform similarly, SIGN-context performs much better in terms of adequacy and the percentage of tweets with the correct sentiment.", "labels": [], "entities": []}, {"text": "The differences are substantial as well as statistically significant: adequacy of 3.61 for SIGN-context compared to 2.55 of Moses, and correct sentiment for 46.2% of the SIGN interpretations, compared to only 25.7% of the Moses interpretations.", "labels": [], "entities": [{"text": "adequacy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9763650298118591}, {"text": "correct sentiment", "start_pos": 135, "end_pos": 152, "type": "METRIC", "confidence": 0.9718477725982666}]}, {"text": "further provides an initial explanation to the improvement of SIGN over Moses: Moses tends to keep interpretations identical to the original sarcastic tweet, altering them in only 42.3% of the cases, 10 while SIGN-context's interpretations differ from the original sarcastic tweet in 68.5% of the cases, which comes closer to the 73.8% in the gold standard human interpretations.", "labels": [], "entities": []}, {"text": "If for each of the algorithms we only regard to interpretations that differ from the original sarcastic tweet, the differences between the models are less substantial.", "labels": [], "entities": []}, {"text": "Nonetheless, SIGN-context still presents improvement by correctly changing sentiment in 67.5% of the cases compared to 60.8% for Moses.", "labels": [], "entities": [{"text": "SIGN-context", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.7295528650283813}]}, {"text": "Both tables consistently show that the contextbased selection strategy of SIGN outperforms the centroid alternative.", "labels": [], "entities": []}, {"text": "This makes sense as, being context-ignorant, SIGN-centroid might produce non-fluent or inadequate interpretations fora given context.", "labels": [], "entities": []}, {"text": "For example, the tweet \"Also gotta move a piano as well.", "labels": [], "entities": []}, {"text": "joy #sarcasm\" is changed to \"Also gotta move a piano as well.", "labels": [], "entities": []}, {"text": "bummer\" by SIGN-context, while SIGN-centroid changes it to the less appropriate \"Also gotta move a piano as well.", "labels": [], "entities": []}, {"text": "Nonetheless, even this naive de-clustering approach substantially improves adequacy and sentiment accuracy over Moses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9636657238006592}]}, {"text": "Finally, comparison to SIGN-oracle reveals that the context selection strategy is not far from human performance with respect to both automatic and human evaluation measures.", "labels": [], "entities": []}, {"text": "Still, some gain can be achieved, especially for the human measures on tweets that were changed at interpreta- We elaborate on this in section 8. tion.", "labels": [], "entities": []}, {"text": "This indicates that SIGN can improve mostly through a better clustering of sentiment words, rather than through a better selection strategy.", "labels": [], "entities": [{"text": "SIGN", "start_pos": 20, "end_pos": 24, "type": "TASK", "confidence": 0.9137523174285889}]}], "tableCaptions": [{"text": " Table 3: Development data results for MT models.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9825947880744934}]}, {"text": " Table 5: Test data results with automatic evaluation measures.", "labels": [], "entities": []}, {"text": " Table 6: Test set results with human measures.  %changed provides the fraction of tweets that  were changed during interpretation (i.e. the tweet  and its interpretation are not identical). In cases  where one of our models presents significant im- provement over Moses, the results are decorated  with a star. Statistical significance is tested with  the paired t-test for fluency and adequacy, and with  the McNemar paired test for labeling disagree- ments (", "labels": [], "entities": [{"text": "McNemar paired test", "start_pos": 411, "end_pos": 430, "type": "METRIC", "confidence": 0.6030237674713135}, {"text": "labeling disagree- ments", "start_pos": 435, "end_pos": 459, "type": "TASK", "confidence": 0.8015826195478439}]}]}