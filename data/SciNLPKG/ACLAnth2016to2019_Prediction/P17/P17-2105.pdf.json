{"title": [], "abstractContent": [{"text": "We develop a language-independent, deep learning-based approach to the task of morphological disambiguation.", "labels": [], "entities": [{"text": "morphological disambiguation", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.7706258893013}]}, {"text": "Guided by the intuition that the correct analysis should be \"most similar\" to the context, we propose dense representations for morphological analyses and surface context and a simple yet effective way of combining the two to perform disambiguation.", "labels": [], "entities": []}, {"text": "Our approach improves on the language-dependent state of the art for two agglu-tinative languages (Turkish and Kazakh) and can be potentially applied to other morphologically complex languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphological disambiguation (MD) is along standing problem in processing morphologically complex languages (MCL).", "labels": [], "entities": [{"text": "Morphological disambiguation (MD)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8911133885383606}]}, {"text": "POS tagging is a somewhat related problem, however in MD, in addition to POS tags, one typically has to predict lemmata (roots hereinafter) that surface forms stem from and morphemes).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8032743632793427}]}, {"text": "Thus, if one counts analyses as tags, MD can be cast as a tagging problem with an extremely large tagset.", "labels": [], "entities": [{"text": "MD", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.696814239025116}]}, {"text": "This fact discourages direct application of the state of the art approaches designed for small fixed tagsets.", "labels": [], "entities": []}, {"text": "To develop a language independent dense representation of the analyses, we segment 2 an analysis We use the term morpheme for its universal recognition within the community.", "labels": [], "entities": []}, {"text": "A more appropriate term might be grammeme, i.e. a value of grammatical category.", "labels": [], "entities": []}, {"text": "2 Such a segmentation is denoted by the squared brackets numbered in the respective order (cf. Turkish example). into (i) the root, (ii) its POS and (iii) the morpheme chain (MC).", "labels": [], "entities": [{"text": "POS", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.9507546424865723}]}, {"text": "We then proceed to jointly learn the embeddigns for the root and the POS segments and to combine them and the MC segment representation into a single dense representation.", "labels": [], "entities": []}, {"text": "MC segments are represented as binary vectors that, fora given analysis, encode presence or absence of each morpheme found in the train set.", "labels": [], "entities": []}, {"text": "This ensures language independence and contrasts previous work (at least on Turkish and Kazakh), where only certain morphemes are chosen as features depending on their position () or presence) in an analysis, or the authors' intuition (.", "labels": [], "entities": []}, {"text": "Apart from the sparseness of analyses distribution MCL notoriously raise free word order and long dependency issues.", "labels": [], "entities": []}, {"text": "Thus, decoding analysis sequences using only the leftmost context may not be enough.", "labels": [], "entities": []}, {"text": "To address this we leverage the rightmost context as well.", "labels": [], "entities": []}, {"text": "We model the left-and rightmost surface context in two ways: using (i) BiL-STM () with a character-based sub-layer ( and (ii) with a feed forward network on word embeddings.", "labels": [], "entities": []}, {"text": "We then entertain the idea that given a word with multiple analyses and its surface context, the correct analysis might be \"closer\" to the context.", "labels": [], "entities": []}, {"text": "Following our intuition, we have tried computing the distance between the analysis and the context representations, and a simple dot product (as in unnormalized cosine similarity) has yielded the best performance.", "labels": [], "entities": []}, {"text": "We evaluate our approach on Turkish and Kazakh data sets, using several baselines (including the state of the art methods for both languages) and a variety of settings and metrics.", "labels": [], "entities": [{"text": "Turkish and Kazakh data sets", "start_pos": 28, "end_pos": 56, "type": "DATASET", "confidence": 0.6291366577148437}]}, {"text": "In terms of general accuracy our approach has achieved a nearly 1% improvement over the state of the art for Turkish and a marginal improvement for Kazakh.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.981086015701294}]}, {"text": "Our contribution amounts to the following: (i) a general MD framework for MCL that can be analyzed in <root, POS, MC> triplets; (ii) improvement on language-dependent state of the art for Turkish and Kazakh.", "labels": [], "entities": []}], "datasetContent": [{"text": "As described in the previous section, each of our models has two settings: the one that does not incorporate surrounding morphological context and the one that does (the starred one).", "labels": [], "entities": []}, {"text": "In addition to that we use pre-trained embeddings, by training word2vec () skip-gram model on Wikipedia texts.", "labels": [], "entities": []}, {"text": "This setting is denoted by a double dagger ( \u2021).", "labels": [], "entities": [{"text": "double dagger", "start_pos": 29, "end_pos": 42, "type": "METRIC", "confidence": 0.7765763401985168}]}, {"text": "We perform a single run evaluation in terms of token-and sentence-based accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9756850004196167}]}, {"text": "We consider four types of tokens: (i) all tokens; (ii) ambiguous tokens (the ones with at least two analyses); (iii) OOV tokens; (iv) ambiguous OOV tokens.", "labels": [], "entities": []}, {"text": "Thus, we use a total of five metrics.", "labels": [], "entities": []}, {"text": "In terms of strictness we deem correct only the predictions that match the golden truth completely, i.e. in root, POS and MC (up to a single morpheme tag).", "labels": [], "entities": [{"text": "POS", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.8633724451065063}]}, {"text": "Note that all of the baselines are language dependent to a certain degree, with MANN being the least dependent and HMMCG the most.", "labels": [], "entities": [{"text": "HMMCG", "start_pos": 115, "end_pos": 120, "type": "DATASET", "confidence": 0.8253170251846313}]}, {"text": "The latter baseline employs handengineered constraint grammar rules to perform initial disambiguation, followed by application of the HMM tagger, which cherry-picks the most informative grammatical features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpora statistics: \u2206AT denotes the av- erage number of analysis per token.", "labels": [], "entities": [{"text": "AT", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9149158596992493}, {"text": "av- erage number of analysis", "start_pos": 46, "end_pos": 74, "type": "METRIC", "confidence": 0.7746793329715729}]}, {"text": " Table 2: Results: here, tok. acc. and tok. amb. acc. denote the accuracy over all and ambiguous tokens  respectively. Same goes for OOV acc. and OOV amb. acc.. Sentence accuracy is denoted as sen. acc..", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9990082383155823}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9814175963401794}]}]}