{"title": [{"text": "Representations of language in a model of visually grounded speech signal", "labels": [], "entities": [{"text": "Representations of language", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.879327654838562}]}], "abstractContent": [{"text": "We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space.", "labels": [], "entities": [{"text": "speech perception", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7130346745252609}]}, {"text": "We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal.", "labels": [], "entities": []}, {"text": "We carryout an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease .", "labels": [], "entities": []}], "introductionContent": [{"text": "Speech recognition is one of the success stories of language technology.", "labels": [], "entities": [{"text": "Speech recognition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8395760953426361}]}, {"text": "It works remarkably well in a range of practical settings.", "labels": [], "entities": []}, {"text": "However, this success relies on the use of very heavy supervision where the machine is fed thousands of hours of painstakingly transcribed audio speech signal.", "labels": [], "entities": []}, {"text": "Humans are able to learn to recognize and understand speech from notably weaker and noisier supervision: they manage to learn to extract structure and meaning from speech by simply being exposed to utterances situated and grounded in their daily sensory experience.", "labels": [], "entities": []}, {"text": "Modeling and emulating this remarkable skill has been the goal of numerous studies; however in the overwhelming majority of cases researchers used severely simplified settings where either the language input or the extralinguistic sensory input, or both, are small scale and symbolically represented.", "labels": [], "entities": []}, {"text": "Section 2 provides a brief overview of this research.", "labels": [], "entities": []}, {"text": "More recently several lines of work have moved towards more realistic inputs while modeling or emulating language acquisition in a grounded setting.", "labels": [], "entities": []}, {"text": "use the image captioning dataset MS COCO () to mimic the setting of grounded language learning: the sensory input consists of images of natural scenes, while the language input are phonetically transcribed descriptions of these scenes.", "labels": [], "entities": [{"text": "image captioning dataset MS COCO", "start_pos": 8, "end_pos": 40, "type": "DATASET", "confidence": 0.7141446232795715}]}, {"text": "The use of such moderately large and low-level data allows the authors to train a multi-layer recurrent neural network model, and to explore the nature and localization of the emerging hierarchy of linguistic representations learned in the process.", "labels": [], "entities": []}, {"text": "Furthermore, in a series of recent studies;; use image captioning datasets to model learning to understand spoken language from visual context with convolutional neural network models.", "labels": [], "entities": []}, {"text": "Finally, there is a small but growing body of work dedicated to elucidating the nature of representations learned by neural networks from language data (see Section 2.2 fora brief overview).", "labels": [], "entities": []}, {"text": "In the current work we build on these three strands of research and contribute the following advances: \u2022 We use a multi-layer gated recurrent neural network to properly model the temporal nature of speech signal and substantially improve performance compared to the convolutional architecture from Harwath and Glass (2015); \u2022 We carryout an in-depth analysis of the representations used by different components of the trained model and correlate them to representations learned by a text-based model and to human patterns of judgment on linguistic stimuli.", "labels": [], "entities": []}, {"text": "This analysis is especially novel fora model with speech signal as input.", "labels": [], "entities": []}, {"text": "The general pattern of findings in our analysis is as follows: The model learns to extract from the acoustic input both form-related and semanticsrelated information, and encodes it in the activations of the hidden layers.", "labels": [], "entities": []}, {"text": "Encoding of semantic aspects tends to become richer as we go up the hierarchy of layers.", "labels": [], "entities": []}, {"text": "Meanwhile, encoding of formrelated aspects of the language input, such as utterance length or the presence of specific words, tends to initially increase and then decay.", "labels": [], "entities": []}, {"text": "We release the code for our models and analyses as open source, available at https://github.com/gchrupala/visually-groundedspeech.", "labels": [], "entities": []}, {"text": "We also release a dataset of synthetically spoken image captions based on MS COCO, available at https://doi.org/10.5281/zenodo.400926.", "labels": [], "entities": [{"text": "synthetically spoken image captions", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.7643172442913055}, {"text": "MS COCO", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9144706726074219}]}], "datasetContent": [{"text": "Our main goal is to analyze the emerging representations from different components of the model and to examine the linguistic knowledge they encode.", "labels": [], "entities": []}, {"text": "For this purpose, we employ a number of tasks that cover the spectrum from fully formbased to fully semantic.", "labels": [], "entities": []}, {"text": "In Section 4.2 we assess the effectiveness of our architecture by evaluating it on the task of ranking images given an utterance.", "labels": [], "entities": []}, {"text": "Sections 4.3 to 4.6 present our analyses.", "labels": [], "entities": []}, {"text": "In Sections 4.3 and 4.4 we define auxiliary tasks to investigate to what extent the network encodes information about the surface form of an utterance from the speech input.", "labels": [], "entities": []}, {"text": "In Section 4.5 and 4.6 we focus on where semantic information is encoded in the model.", "labels": [], "entities": []}, {"text": "In the analyses, we use the following features: Utterance embeddings: the weighted sum of the unit activations on the last layer, as calculated by Equation (3).", "labels": [], "entities": [{"text": "Utterance", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.965833842754364}]}, {"text": "Average unit activations: hidden layer activations averaged overtime and L2-normalized for each hidden layer.", "labels": [], "entities": []}, {"text": "Average input vectors: the MFCC vectors averaged overtime.", "labels": [], "entities": [{"text": "overtime", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9856029748916626}]}, {"text": "We use this feature to examine how much information can be extracted from the input signal only.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Image retrieval performance on Flickr8K.  R@N stands for recall at N; \u02dc  r stands for median  rank of the correct image.", "labels": [], "entities": [{"text": "Image retrieval", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7818594872951508}, {"text": "Flickr8K", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.9666566848754883}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9983946681022644}]}, {"text": " Table 2:  Image retrieval performance on  MS COCO. R@N stands for recall at N; \u02dc  r stands  for median rank of the correct image.", "labels": [], "entities": [{"text": "Image retrieval", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.7893121242523193}, {"text": "MS COCO", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.8650617003440857}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9985880255699158}]}]}