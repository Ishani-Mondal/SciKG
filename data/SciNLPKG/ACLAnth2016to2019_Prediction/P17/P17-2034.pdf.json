{"title": [{"text": "A Corpus of Natural Language for Visual Reasoning", "labels": [], "entities": []}], "abstractContent": [{"text": "We present anew visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences.", "labels": [], "entities": []}, {"text": "We describe a method of crowd-sourcing linguistically-diverse data, and present an analysis of our data.", "labels": [], "entities": []}, {"text": "The data demonstrates abroad set of linguistic phenomena , requiring visual and set-theoretic reasoning.", "labels": [], "entities": []}, {"text": "We experiment with various models, and show the data presents a strong challenge for future research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding complex compositional language in context is a challenge shared by many tasks.", "labels": [], "entities": []}, {"text": "Visual question answering and robot instruction systems require reasoning about sets of objects, quantities, comparisons, and spatial relations; for example, when instructing home assistance or assembly-line robots to manipulate objects in cluttered environments.", "labels": [], "entities": [{"text": "Visual question answering", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6214350958665212}]}, {"text": "This reasoning requires robust language understanding, and is only partially addressed by existing datasets.", "labels": [], "entities": []}, {"text": "VQA (, while lexically and visually diverse, includes relatively short sentences with limited coverage of such phenomena.", "labels": [], "entities": [{"text": "VQA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.91057288646698}]}, {"text": "CLEVR) and SHAPES (), in contrast, display complex compositional structure, but include only synthetic language.", "labels": [], "entities": []}, {"text": "In this paper, we introduce the Cornell Natural Language Visual Reasoning (NLVR) corpus and task.", "labels": [], "entities": [{"text": "Cornell Natural Language Visual Reasoning (NLVR) corpus", "start_pos": 32, "end_pos": 87, "type": "DATASET", "confidence": 0.695339153210322}]}, {"text": "We define the binary prediction task of judging if a statement is true for an image or not, and introduce a corpus of annotated pairs of natural language statements and synthetic images.", "labels": [], "entities": [{"text": "binary prediction task", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.7979247371355692}]}, {"text": "Collecting this kind of language presents two challenges.", "labels": [], "entities": []}, {"text": "First, we must design environments to  support such descriptions.", "labels": [], "entities": []}, {"text": "We use simple visual environments displaying objects with complex visual relations between them.", "labels": [], "entities": []}, {"text": "The second challenge is eliciting complex descriptions displaying a range of syntactic and semantic phenomena.", "labels": [], "entities": []}, {"text": "We use a twostage crowdsourcing process.", "labels": [], "entities": []}, {"text": "In the first stage, we present sets of images and ask workers to write descriptive statements that distinguish them.", "labels": [], "entities": []}, {"text": "Using synthetic images with abstract shapes allows us to control the potential distinctions between them; for example, by discouraging simple statements about object existence.", "labels": [], "entities": []}, {"text": "In the second stage, we ask workers to label the truth value for the sentences and images generated in the first stage.", "labels": [], "entities": []}, {"text": "Our data includes 92,244 sentence-image pairs with 3,962 unique sentences.", "labels": [], "entities": []}, {"text": "We include both images and the structured representation used to generate them to support research using both raw visual information and structured data.", "labels": [], "entities": []}, {"text": "To assess the difficulty of NLVR, we experiment with multiple baselines.", "labels": [], "entities": []}, {"text": "The best model using images achieves an accuracy of 66.12, demonstrating remaining challenges in the data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9996247291564941}]}, {"text": "We also analyze the language in our data for presence of certain linguistic phenomena, and compare this analysis with related datasets.", "labels": [], "entities": []}, {"text": "The data and leaderboard are available at http://lic.nlp.cornell.edu/nlvr.", "labels": [], "entities": []}], "datasetContent": [{"text": "Several datasets have been created to study visual reasoning and language.", "labels": [], "entities": []}, {"text": "VQA () includes crowdsourced questions and answers for photographs and abstract scenes, and has been studied extensively (e.g.,.", "labels": [], "entities": [{"text": "VQA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7848203182220459}]}, {"text": "In contrast to VQA, we use synthetic images and emphasize representing abroad range of language phenomena.", "labels": [], "entities": []}, {"text": "Our motivation is similar to that of SHAPES) and CLEVR.", "labels": [], "entities": [{"text": "CLEVR", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.795307993888855}]}, {"text": "Both datasets also use synthetic images and emphasize representing diverse spatial language.", "labels": [], "entities": []}, {"text": "However, unlike our approach, they include only automatically generated language.", "labels": [], "entities": []}, {"text": "Visual reasoning has also been addressed in instructional language corpora (e.g.,, where executable instructions are grounded in manipulable environments.", "labels": [], "entities": [{"text": "Visual reasoning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7555451393127441}]}, {"text": "The language we observe is similar to the type of language studied for understanding and generation of referential expressions (.", "labels": [], "entities": [{"text": "understanding and generation of referential expressions", "start_pos": 71, "end_pos": 126, "type": "TASK", "confidence": 0.7911953379710516}]}, {"text": "Our task is related to caption generation, which has been studied extensively (e.g.,) with MSCOCO () and Flickr30K (.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.9798473715782166}, {"text": "Flickr30K", "start_pos": 105, "end_pos": 114, "type": "DATASET", "confidence": 0.8757699728012085}]}, {"text": "In contrast to caption generation, our task does not require approximate metrics like BLEU.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.9652107357978821}, {"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9937068819999695}]}, {"text": "Several existing datasets focus on natural language querying of structured representations, including GeoQuery and WikiTables (.", "labels": [], "entities": []}, {"text": "Our work is complementary to these resources.", "labels": [], "entities": []}, {"text": "While our corpus was collected using images, we also provide structured representations.", "labels": [], "entities": []}, {"text": "When used with these representations, our corpus is similar to WikiTables, where questions are paired with small web tables.", "labels": [], "entities": []}, {"text": "Instead of web tables, we use object sets and focus on visual language.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Qualitative and empirical analysis of our data and VQA (Antol et al., 2015). We analyze 200 sentences  for each dataset. The data is categorized to semantic and syntactic categories. We use the terms hard and soft  cardinality to differentiate between language using exact numerical values and ranges. For each dataset, we show  the percentage of the samples analyzed that demonstrate the phenomena.", "labels": [], "entities": [{"text": "VQA", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.8212240934371948}]}, {"text": " Table 3: Mean accuracy and standard deviation results. We report accuracy for the train, development, and both  test sets. Three systems use the structured representation. Two systems (and Image Only) use the raw image.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9860672354698181}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.6944036483764648}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9988946318626404}]}]}