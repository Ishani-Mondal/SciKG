{"title": [{"text": "Blind phoneme segmentation with temporal prediction errors", "labels": [], "entities": [{"text": "Blind phoneme segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6373688876628876}, {"text": "temporal prediction", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.6873632818460464}]}], "abstractContent": [{"text": "Phonemic segmentation of speech is a critical step of speech recognition systems.", "labels": [], "entities": [{"text": "Phonemic segmentation of speech", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.848533883690834}, {"text": "speech recognition", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7378058731555939}]}, {"text": "We propose a novel unsupervised algorithm based on sequence prediction models such as Markov chains and recurrent neural networks.", "labels": [], "entities": []}, {"text": "Our approach consists in analyzing the error profile of a model trained to predict speech features frame-by-frame.", "labels": [], "entities": []}, {"text": "Specifically, we try to learn the dynamics of speech in the MFCC space and hypothesize boundaries from local maxima in the prediction error.", "labels": [], "entities": [{"text": "MFCC space", "start_pos": 60, "end_pos": 70, "type": "DATASET", "confidence": 0.8716092109680176}]}, {"text": "We evaluate our system on the TIMIT dataset, with improvements over similar methods.", "labels": [], "entities": [{"text": "TIMIT dataset", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.9206542372703552}]}], "introductionContent": [{"text": "One of the main difficulty of speech processing as opposed to text processing is the continuous, time-dependent nature of the signal.", "labels": [], "entities": [{"text": "speech processing", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.6988314688205719}]}, {"text": "As a consequence, pre-segmentation of the speech signal into words or sub-words units such as phonemes, syllables or words is an essential first step of a variety of speech recognition tasks.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.7167164385318756}]}, {"text": "Segmentation in phonemes is useful fora number of applications (annotation of speech for the purpose of phonetic analysis, computation of speech rate, keyword spotting, etc), and can be done in two ways.", "labels": [], "entities": [{"text": "phonetic analysis", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7048170566558838}, {"text": "keyword spotting", "start_pos": 151, "end_pos": 167, "type": "TASK", "confidence": 0.7131128162145615}]}, {"text": "Supervised methods are based on an existing phoneme or word recognition system, which is used to decode the incoming speech into phonemes.", "labels": [], "entities": []}, {"text": "Phonemes boundaries can then be extracted as a by-product of the alignment of the phoneme models with the speech.", "labels": [], "entities": []}, {"text": "Unsupervised methods (also called blind segmentation) consist in finding phonemes boundaries using the acoustic signals only.", "labels": [], "entities": [{"text": "blind segmentation", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7817160189151764}]}, {"text": "Supervised methods depend * This work was done when the author was an intern at LSCP / ENS / EHESS / CNRS on the training of acoustic and language models, which requires access to large amounts of linguistic resources (annotated speech, phonetic dictionary, text).", "labels": [], "entities": [{"text": "LSCP / ENS / EHESS / CNRS", "start_pos": 80, "end_pos": 105, "type": "DATASET", "confidence": 0.8118700810841152}]}, {"text": "Unsupervised methods do not require these resources and are therefore appropriate for so-called under-resourced languages, such as endangered languages, or languages without consistent orthographies.", "labels": [], "entities": []}, {"text": "We propose a blind phoneme segmentation method based on short term statistical properties of the speech signal.", "labels": [], "entities": [{"text": "blind phoneme segmentation", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.632175495227178}]}, {"text": "We designate peaks in the error curve of a model trained to predict speech frame by frame as potential boundaries.", "labels": [], "entities": []}, {"text": "Three different models are tested.", "labels": [], "entities": []}, {"text": "The first is an approximated Markov model of the transition probabilities between categorical speech features.", "labels": [], "entities": []}, {"text": "We then replace it by a recurrent neural network operating on the same categorical features.", "labels": [], "entities": []}, {"text": "Finally, a recurrent neural network is directly trained to predict the raw speech features.", "labels": [], "entities": []}, {"text": "This last model is especially interesting in that it couples our statistical approach with more common spectral transition based methods for instance).", "labels": [], "entities": []}, {"text": "We first describe the various models used and the pre-and post-processing procedures, before presenting and discussing our results in the light of previous work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our methods on the TIMIT dataset.", "labels": [], "entities": [{"text": "TIMIT dataset", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9366007447242737}]}, {"text": "The TIMIT dataset consists of 6300 utterances (\u223c 5.4 hours) from 630 speakers spanning 8 dialects of the English language.", "labels": [], "entities": [{"text": "TIMIT dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8253510892391205}]}, {"text": "The corpus was divided into a training and test set according to the standard split.", "labels": [], "entities": []}, {"text": "The training set contains 4620 utterances (172,460 boundaries) and the test set 1680 (65,825 boundaries).", "labels": [], "entities": []}, {"text": "The performance evaluation of our system is based on precision (P ), recall (R) and F -score, defined as the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "precision (P )", "start_pos": 53, "end_pos": 67, "type": "METRIC", "confidence": 0.9507695287466049}, {"text": "recall (R)", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9666894823312759}, {"text": "F -score", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9944152434666952}, {"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9984899759292603}, {"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9914658069610596}]}, {"text": "A drawback of this metric is that high recall, low precision results, such as the ones produces by hypothesizing a boundary every 5 ms (P : 58%, R : 91%) yield high F -score (70%).", "labels": [], "entities": [{"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9984286427497864}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9931405186653137}, {"text": "F -score", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9940976699193319}]}, {"text": "Other metrics have been designed to tackle this issue.", "labels": [], "entities": []}, {"text": "One such example is the R-value : Where OS = RP \u2212 1 is the over-segmentation measure.", "labels": [], "entities": []}, {"text": "The R value represents how close the segmentation is from the ideal 0 OS, 1 R point and the P=1 line in the R, OS space.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.9680846929550171}]}, {"text": "Further details can be found in  Determining whether gold boundary is detected or not is a crucial part of the evaluation procedure.", "labels": [], "entities": [{"text": "Determining", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9398648738861084}]}, {"text": "On our test set for instance, which contains 65,825 gold boundaries partitioned into 1,680 files, adding or removing one correctly detected boundary per utterance leads to a change of \u00b1 2.5% in precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 194, "end_pos": 203, "type": "METRIC", "confidence": 0.9981192946434021}]}, {"text": "This means that minor changes in the evaluation process (such as removing the trailing silence parts of each file, removing the opening and closing boundary) yield non-trivial variations in the end result.", "labels": [], "entities": []}, {"text": "A common condition fora gold boundary to be considered as 'correctly detected' is to have a proposed boundary within a 20 ms distance on either side.", "labels": [], "entities": []}, {"text": "Without any other specification, this means that a proposed boundary maybe matched to several gold boundaries, provided these are within 40 ms from each other, leading to an increase of up to 4% F-score in some of our results (74%-78%).", "labels": [], "entities": [{"text": "F-score", "start_pos": 195, "end_pos": 202, "type": "METRIC", "confidence": 0.9996862411499023}]}, {"text": "Unfortunately this point is seldom detailed in the literature.", "labels": [], "entities": []}, {"text": "We decided to use the procedure described in to match gold boundaries and hypothesized boundaries : overlapping tolerance windows are cropped in the middle of the two boundaries.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Final results (in%) evaluated with  cropped tolerance windows", "labels": [], "entities": []}, {"text": " Table 2: Final results (in%) evaluated with over- lapping tolerance windows. The scores reported  for Rasanen (2014) are the paper results.", "labels": [], "entities": [{"text": "Rasanen (2014)", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.6883787661790848}]}]}