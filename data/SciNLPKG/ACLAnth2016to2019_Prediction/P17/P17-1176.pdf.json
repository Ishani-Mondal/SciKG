{"title": [{"text": "A Teacher-Student Framework for Zero-Resource Neural Machine Translation", "labels": [], "entities": [{"text": "Zero-Resource Neural Machine Translation", "start_pos": 32, "end_pos": 72, "type": "TASK", "confidence": 0.6423682570457458}]}], "abstractContent": [{"text": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains.", "labels": [], "entities": [{"text": "end-to-end neural machine translation (NMT)", "start_pos": 6, "end_pos": 49, "type": "TASK", "confidence": 0.797784081527165}]}, {"text": "In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language.", "labels": [], "entities": []}, {"text": "Based on the assumption, our method is able to train a source-to-target NMT model (\"stu-dent\") without parallel corpora available guided by an existing pivot-to-target NMT model (\"teacher\") on a source-pivot parallel corpus.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9989006519317627}]}], "introductionContent": [{"text": "Neural machine translation (NMT), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8360887169837952}]}, {"text": "Although NMT has achieved state-of-the-art translation performance on resource-rich language pairs such as English-French and German-English (, it still suffers from the unavailability of large-scale parallel corpora for translating low-resource languages.", "labels": [], "entities": [{"text": "NMT", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.7549795508384705}]}, {"text": "Due to the large parameter space, neural models usually learn poorly from low-count events, resulting in a poor choice for low-resource language pairs.", "labels": [], "entities": []}, {"text": "Zoph et * Corresponding author: Yang al. indicate that NMT obtains much worse translation quality than a statistical machine translation (SMT) system on low-resource languages.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 105, "end_pos": 142, "type": "TASK", "confidence": 0.7861532717943192}]}, {"text": "As a result, a number of authors have endeavored to explore methods for translating language pairs without parallel corpora available.", "labels": [], "entities": []}, {"text": "These methods can be roughly divided into two broad categories: multilingual and pivot-based.", "labels": [], "entities": []}, {"text": "present a multi-way, multilingual model with shared attention to achieve zeroresource translation.", "labels": [], "entities": [{"text": "zeroresource translation", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.6977696120738983}]}, {"text": "They fine-tune the attention part using pseudo bilingual sentences for the zeroresource language pair.", "labels": [], "entities": []}, {"text": "Another direction is to develop a universal NMT model in multilingual scenarios.", "labels": [], "entities": []}, {"text": "They use parallel corpora of multiple languages to train one single model, which is then able to translate a language pair without parallel corpora available.", "labels": [], "entities": []}, {"text": "Although these approaches prove to be effective, the combination of multiple languages in modeling and training leads to increased complexity compared with standard NMT.", "labels": [], "entities": []}, {"text": "Another direction is to achieve source-to-target NMT without parallel data via a pivot, which is either text () or image (.", "labels": [], "entities": []}, {"text": "propose a pivot-based method for zeroresource NMT: it first translates the source language to a pivot language, which is then translated to the target language.", "labels": [], "entities": []}, {"text": "show that using multimedia information as pivot also benefits zero-resource translation.", "labels": [], "entities": [{"text": "zero-resource translation", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.7276268601417542}]}, {"text": "However, pivot-based approaches usually need to divide the decoding process into two steps, which is not only more computationally expensive, but also potentially suffers from the error propagation problem (.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 180, "end_pos": 197, "type": "TASK", "confidence": 0.6653366386890411}]}, {"text": "In this paper, we propose anew method for zero-resource neural machine translation.", "labels": [], "entities": [{"text": "zero-resource neural machine translation", "start_pos": 42, "end_pos": 82, "type": "TASK", "confidence": 0.5994927808642387}]}, {"text": "Our Figure 1: (a) The pivot-based approach and (b) the teacher-student approach to zero-resource neural machine translation.", "labels": [], "entities": [{"text": "zero-resource neural machine translation", "start_pos": 83, "end_pos": 123, "type": "TASK", "confidence": 0.6017596498131752}]}, {"text": "X, Y, and Z denote source, target, and pivot languages, respectively.", "labels": [], "entities": []}, {"text": "We use a dashed line to denote that there is a parallel corpus available for the connected language pair.", "labels": [], "entities": []}, {"text": "Solid lines with arrows represent translation directions.", "labels": [], "entities": []}, {"text": "The pivot-based approach leverages a pivot to achieve indirect source-to-target translation: it first translates x into z, which is then translated into y.", "labels": [], "entities": []}, {"text": "Our training algorithm is based on the translation equivalence assumption: if x is a translation of z, then P (y|x; \u03b8 x\u2192y ) should be close to P (y|z; \u03b8 z\u2192y ).", "labels": [], "entities": []}, {"text": "Our approach directly trains the intended source-totarget model P (y|x; \u03b8 x\u2192y ) (\"student\") on a source-pivot parallel corpus, with the guidance of an existing pivot-to-target model P (y|z; \u02c6 \u03b8 z\u2192y ) (\"teacher\").", "labels": [], "entities": []}, {"text": "method assumes that parallel sentences should have close probabilities of generating a sentence in a third language.", "labels": [], "entities": []}, {"text": "To train a source-to-target NMT model without parallel corpora (\"student\"), we leverage an existing pivot-to-target NMT model (\"teacher\") to guide the learning process of the student model on a source-pivot parallel corpus.", "labels": [], "entities": []}, {"text": "Compared with pivot-based approaches (Cheng et al., 2016a), our method allows direct parameter estimation of the intended NMT model, without the need to divide decoding into two steps.", "labels": [], "entities": []}, {"text": "This strategy not only improves efficiency but also avoids error propagation in decoding.", "labels": [], "entities": []}, {"text": "Experiments on the Europarl and WMT datasets show that our approach achieves significant improvements in terms of both translation quality and decoding efficiency over a baseline pivot-based approach to zero-resource NMT on Spanish-French and German-French translation tasks.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.9892596006393433}, {"text": "WMT datasets", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.8789190948009491}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Data statistics. For the Europarl corpus,  we evaluate our approach on Spanish-French (Es- Fr) and German-French (De-Fr) translation tasks.  For the WMT corpus, we evaluate approach on the  Spanish-French (Es-Fr) translation task. English  is used as a pivot language in all experiments.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.9912265241146088}, {"text": "WMT corpus", "start_pos": 159, "end_pos": 169, "type": "DATASET", "confidence": 0.7881392240524292}]}, {"text": " Table 2: Verification of sentence-level and word-level assumptions by evaluating approximated KL di- vergence from the source-to-target model to the pivot-to-target model over training iterations of the  source-to-target model. The pivot-to-target model is trained and kept fixed.", "labels": [], "entities": [{"text": "KL di- vergence", "start_pos": 95, "end_pos": 110, "type": "METRIC", "confidence": 0.6629592776298523}]}, {"text": " Table 3: Comparison with previous work on Spanish-French and German-French translation tasks from  the Europarl corpus. English is treated as the pivot language. The likelihood method uses 100K parallel  source-target sentences, which are not available for other methods.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 104, "end_pos": 119, "type": "DATASET", "confidence": 0.9915141761302948}]}, {"text": " Table 5: Comparison with previous work on Spanish-French translation in a zero-resource scenario over  the WMT corpus. The BLEU scores are case sensitive.  \u2020: the method depends on two-step decoding.", "labels": [], "entities": [{"text": "WMT corpus", "start_pos": 108, "end_pos": 118, "type": "DATASET", "confidence": 0.923175185918808}, {"text": "BLEU", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9992374181747437}]}]}