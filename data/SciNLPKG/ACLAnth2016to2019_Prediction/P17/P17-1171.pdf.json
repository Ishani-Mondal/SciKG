{"title": [{"text": "Reading Wikipedia to Answer Open-Domain Questions", "labels": [], "entities": [{"text": "Reading Wikipedia to Answer Open-Domain Questions", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.6719511946042379}]}], "abstractContent": [{"text": "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article.", "labels": [], "entities": [{"text": "open-domain question answering", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.6148625512917837}]}, {"text": "This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles).", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.6939481943845749}]}, {"text": "Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs.", "labels": [], "entities": [{"text": "TF-IDF matching", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.6432430893182755}]}, {"text": "Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.7958305776119232}]}], "introductionContent": [{"text": "This paper considers the problem of answering factoid questions in an open-domain setting using Wikipedia as the unique knowledge source, such as one does when looking for answers in an encyclopedia.", "labels": [], "entities": [{"text": "answering factoid questions", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.8225533167521158}]}, {"text": "Wikipedia is a constantly evolving source of detailed information that could facilitate intelligent machines -if they are able to leverage its power.", "labels": [], "entities": []}, {"text": "Unlike knowledge bases (KBs) such as Freebase ( or DBPedia (, which are easier for computers to process but too sparsely populated for open-domain question answering), Wikipedia contains up-to-date knowledge that humans are interested in.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.9657305479049683}, {"text": "open-domain question answering", "start_pos": 135, "end_pos": 165, "type": "TASK", "confidence": 0.6958000659942627}]}, {"text": "It is designed, however, for humans -not machines -to read.", "labels": [], "entities": []}, {"text": "Using Wikipedia articles as the knowledge source causes the task of question answering (QA) to combine the challenges of both large-scale open-domain QA and of machine comprehension of text.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.8483698308467865}]}, {"text": "In order to answer any question, one must first retrieve the few relevant articles among more than 5 million items, and then scan them carefully to identify the answer.", "labels": [], "entities": []}, {"text": "We term this setting, machine reading at scale (MRS).", "labels": [], "entities": [{"text": "MRS", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.7354171276092529}]}, {"text": "Our work treats Wikipedia as a collection of articles and does not rely on its internal graph structure.", "labels": [], "entities": []}, {"text": "As a result, our approach is generic and could be switched to other collections of documents, books, or even daily updated newspapers.", "labels": [], "entities": []}, {"text": "Large-scale QA systems like IBM's DeepQA () rely on multiple sources to answer: besides Wikipedia, it is also paired with KBs, dictionaries, and even news articles, books, etc.", "labels": [], "entities": []}, {"text": "As a result, such systems heavily rely on information redundancy among the sources to answer correctly.", "labels": [], "entities": []}, {"text": "Having a single knowledge source forces the model to be very precise while searching for an answer as the evidence might appear only once.", "labels": [], "entities": []}, {"text": "This challenge thus encourages research in the ability of a machine to read, a key motivation for the machine comprehension subfield and the creation of datasets such as SQuAD (, CNN/Daily Mail () and CBT (.", "labels": [], "entities": [{"text": "CNN/Daily Mail", "start_pos": 179, "end_pos": 193, "type": "DATASET", "confidence": 0.9048280417919159}, {"text": "CBT", "start_pos": 201, "end_pos": 204, "type": "DATASET", "confidence": 0.879948616027832}]}, {"text": "However, those machine comprehension resources typically assume that a short piece of relevant text is already identified and given to the model, which is not realistic for building an opendomain QA system.", "labels": [], "entities": []}, {"text": "In sharp contrast, methods that use KBs or information retrieval over documents have to employ search as an integral part of the solution.", "labels": [], "entities": []}, {"text": "Instead MRS is focused on simultaneously maintaining the challenge of machine comprehension, which requires the deep understanding of text, while keeping the realistic constraint of searching over a large open resource.", "labels": [], "entities": [{"text": "MRS", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9763656854629517}]}, {"text": "In this paper, we show how multiple existing QA datasets can be used to evaluate MRS by requiring an open-domain system to perform well on all of them at once.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.7506257593631744}, {"text": "MRS", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.9482606649398804}]}, {"text": "We develop DrQA, a strong system for question answering from Wikipedia composed of: (1) Document Retriever, a module using bigram hashing and TF-IDF matching designed to, given a question, efficiently return a subset of relevant articles and (2) Document Reader, a multi-layer recurrent neural network machine comprehension model trained to detect answer spans in those few returned documents.", "labels": [], "entities": [{"text": "DrQA", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.9310316443443298}, {"text": "question answering", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7466962933540344}, {"text": "Document Reader", "start_pos": 246, "end_pos": 261, "type": "TASK", "confidence": 0.6871182322502136}]}, {"text": "gives an illustration of DrQA.", "labels": [], "entities": [{"text": "DrQA", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.9664283990859985}]}, {"text": "Our experiments show that Document Retriever outperforms the built-in Wikipedia search engine and that Document Reader reaches state-of-theart results on the very competitive SQuAD benchmark (.", "labels": [], "entities": [{"text": "SQuAD benchmark", "start_pos": 175, "end_pos": 190, "type": "DATASET", "confidence": 0.7241230458021164}]}, {"text": "Finally, our full system is evaluated using multiple benchmarks.", "labels": [], "entities": []}, {"text": "In particular, we show that performance is improved across all datasets through the use of multitask learning and distant supervision compared to single task training.", "labels": [], "entities": []}], "datasetContent": [{"text": "SQuAD is one of the largest general purpose QA datasets currently available.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8048331141471863}, {"text": "QA datasets", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.6733754426240921}]}, {"text": "SQuAD questions have been collected via a process involving showing a paragraph to each human annotator and asking them to write a question.", "labels": [], "entities": []}, {"text": "As a result, their distribution is quite specific.", "labels": [], "entities": []}, {"text": "We hence propose to train and evaluate our system on other datasets developed for open-domain QA that have been constructed in different ways (not necessarily in the context of answering from Wikipedia).", "labels": [], "entities": []}, {"text": "CuratedTREC This dataset is based on the benchmarks from the TREC QA tasks that have been curated by Baudi\u0161 and\u0160ediv`yand\u02c7and\u0160ediv`and\u0160ediv`y (2015).", "labels": [], "entities": [{"text": "TREC QA tasks", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.6136799256006876}]}, {"text": "We use the large version, which contains a total of 2,180 questions extracted from the datasets from WebQuestions Introduced in (, this dataset is built to answer questions from the Freebase KB.", "labels": [], "entities": [{"text": "Freebase KB", "start_pos": 182, "end_pos": 193, "type": "DATASET", "confidence": 0.9726198017597198}]}, {"text": "It was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk.", "labels": [], "entities": []}, {"text": "We convert each answer to text by using entity names so that the dataset does not reference Freebase IDs and is purely made of plain text question-answer pairs.", "labels": [], "entities": []}, {"text": "WikiMovies This dataset, introduced in (, contains 96k question-answer pairs in the domain of movies.", "labels": [], "entities": []}, {"text": "Originally created from the OMDb and MovieLens databases, the examples are built such that they can also be answered by using a subset of Wikipedia as the knowledge source (the title and the first section of articles from the movie domain).", "labels": [], "entities": []}, {"text": "This section first presents evaluations of our Document Retriever and Document Reader modules separately, and then describes tests of their combination, DrQA, for open-domain QA on the full Wikipedia.", "labels": [], "entities": [{"text": "DrQA", "start_pos": 153, "end_pos": 157, "type": "DATASET", "confidence": 0.8747329115867615}]}, {"text": "Next we evaluate our Document Reader component on the standard SQuAD evaluation.", "labels": [], "entities": []}, {"text": "Implementation details We use 3-layer bidirectional LSTMs with h = 128 hidden units for both paragraph and question encoding.", "labels": [], "entities": []}, {"text": "We apply the Stanford CoreNLP toolkit ( ) for tokenization and also generating lemma, partof-speech, and named entity tags.", "labels": [], "entities": [{"text": "Stanford CoreNLP toolkit", "start_pos": 13, "end_pos": 37, "type": "DATASET", "confidence": 0.9041668772697449}, {"text": "tokenization", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.966072142124176}]}, {"text": "Lastly, all the training examples are sorted by the length of paragraph and divided into minibatches of 32 examples each.", "labels": [], "entities": []}, {"text": "We use Adamax for optimization as described in.", "labels": [], "entities": []}, {"text": "Dropout with p = 0.3 is applied to word embeddings and all the hidden units of LSTMs.", "labels": [], "entities": []}, {"text": "presents our evaluation results on both development and test sets.", "labels": [], "entities": []}, {"text": "SQuAD has been a very competitive machine comprehension benchmark since its creation and we only list the best-performing systems in the table.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7138064503669739}]}, {"text": "Our system (single model) can achieve 70.0% exact match and 79.0% F1 scores on the test set, which surpasses all the published results and can match the top performance on the SQuAD leaderboard at the time of writing.", "labels": [], "entities": [{"text": "exact", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9959232807159424}, {"text": "match", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.6321752667427063}, {"text": "F1 scores", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9827643930912018}, {"text": "SQuAD leaderboard", "start_pos": 176, "end_pos": 193, "type": "DATASET", "confidence": 0.9153984487056732}]}, {"text": "Additionally, we think that our model is conceptually simpler than most of the existing systems.", "labels": [], "entities": []}, {"text": "We conducted an ablation analysis on the feature vector of paragraph tokens.", "labels": [], "entities": []}, {"text": "As shown in all the features contribute to the performance of our final system.", "labels": [], "entities": []}, {"text": "Without the aligned question embedding feature (only word embedding and a few manual features), our system is still able to achieve F1 over 77%.", "labels": [], "entities": [{"text": "F1", "start_pos": 132, "end_pos": 134, "type": "METRIC", "confidence": 0.9996438026428223}]}, {"text": "More interestingly, if we remove both f aligned and f exact match , the performance drops dramatically, so we conclude that both features play a similar but complementary role in the feature representation related to the paraphrased nature of a question vs. the context around an answer.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of questions for each dataset  used in this paper. DS: distantly supervised train- ing data.  *  : These training sets are not used as  is because no paragraph is associated with each  question.  \u2020 : Corresponds to SQuAD development  set.", "labels": [], "entities": [{"text": "SQuAD development  set", "start_pos": 232, "end_pos": 254, "type": "DATASET", "confidence": 0.7647380630175272}]}, {"text": " Table 3: Document retrieval results. % of ques- tions for which the answer segment appears in one  of the top 5 pages returned by the method.", "labels": [], "entities": [{"text": "Document retrieval", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8690308630466461}]}, {"text": " Table 4: Evaluation results on the SQuAD dataset (single model only).  \u2020 : Test results reflect the SQuAD  leaderboard (https://stanford-qa.com) as of Feb 6, 2017.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9181930124759674}, {"text": "SQuAD  leaderboard", "start_pos": 101, "end_pos": 119, "type": "DATASET", "confidence": 0.8818672597408295}]}]}