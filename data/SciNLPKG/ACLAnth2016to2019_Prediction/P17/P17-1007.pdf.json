{"title": [{"text": "Skip-Gram -Zipf + Uniform = Vector Additivity", "labels": [], "entities": []}], "abstractContent": [{"text": "In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation.", "labels": [], "entities": [{"text": "word analogy questions", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.851669708887736}, {"text": "caption generation", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.9148841500282288}]}, {"text": "An unexpected \"sideeffect\" of such models is that their vectors often exhibit compositionality, i.e., adding two word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., \"man\" + \"royal\" = \"king\".", "labels": [], "entities": []}, {"text": "This work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model.", "labels": [], "entities": []}, {"text": "In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus.", "labels": [], "entities": [{"text": "additive compositionality", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.7937202751636505}]}, {"text": "As a corollary, it explains the success of vector calculus in solving word analogies.", "labels": [], "entities": [{"text": "solving word analogies", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.730099489291509}]}, {"text": "When these assumptions do not hold, this work describes the correct non-linear composition operator.", "labels": [], "entities": []}, {"text": "Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby: the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies.", "labels": [], "entities": []}, {"text": "This shows that SkipGram embeddings are optimal in the sense of Globerson and Tishby and, further, implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.", "labels": [], "entities": []}], "introductionContent": [{"text": "The strategy of representing words as vectors has along history in computational linguistics and machine learning.", "labels": [], "entities": []}, {"text": "The general idea is to find a map from words to vectors such that wordsimilarity and vector-similarity are in correspondence.", "labels": [], "entities": []}, {"text": "Whilst vector-similarity can be readily quantified in terms of distances and angles, quantifying word-similarity is a more ambiguous task.", "labels": [], "entities": []}, {"text": "A key insight in that regard is to posit that the meaning of a word is captured by \"the company it keeps\" and, therefore, that two words that keep company with similar words are likely to be similar themselves.", "labels": [], "entities": []}, {"text": "In the simplest case, one seeks vectors whose inner products approximate the co-occurrence frequencies.", "labels": [], "entities": []}, {"text": "In more sophisticated methods cooccurrences are reweighed to suppress the effect of more frequent words () and/or to emphasize pairs of words whose co-occurrence frequency maximally deviates from the independence assumption).", "labels": [], "entities": []}, {"text": "An alternative to seeking word-embeddings that reflect co-occurrence statistics is to extract the vectorial representation of words from non-linear statistical language models, specifically neural networks.", "labels": [], "entities": []}, {"text": "() already proposed (i) associating with each vocabulary word a feature vector, (ii) expressing the probability function of word sequences in terms of the feature vectors of the words in the sequence, and (iii) learning simultaneously the vectors and the parameters of the probability function.", "labels": [], "entities": []}, {"text": "This approach came into prominence recently through works of Mikolov et al.", "labels": [], "entities": []}, {"text": "(see below) whose main departure from () was to follow the suggestion of and tradeaway the expressive capacity of general neuralnetwork models for the scalability (to very large corpora) afforded by (the more restricted class of) log-linear models.", "labels": [], "entities": []}, {"text": "An unexpected side effect of deriving wordembeddings via neural networks is that the wordvectors produced appear to enjoy (approximate) additive compositionality: adding two wordvectors often results in a vector whose nearest word-vector belongs to the word capturing the composition of the added words, e.g., \"man\" + \"royal\" = \"king\" ().", "labels": [], "entities": []}, {"text": "This unexpected property allows one to use these vectors to answer word-analogy questions algebraically, e.g., answering the question \"Man is to king as woman is to \" by returning the word whose word-vector is nearest to the vector In this work we focus on explaining the source of this phenomenon for the most prominent such model, namely the Skip-Gram model introduced in).", "labels": [], "entities": []}, {"text": "The Skip-Gram model learns vector representations of words based on their patterns of co-occurrence in the training corpus as follows: it assigns to each word c in the vocabulary V , a \"context\" and a \"target\" vector, respectively u c and v c , which are to be used in order to predict the words that appear around each occurrence of c within a window of \u2206 tokens.", "labels": [], "entities": []}, {"text": "Specifically, the log probability of any target word w to occur at any position within distance \u2206 of a context word c is taken to be proportional to the inner product between u c and v w , i.e., letting n = |V |, Further, Skip-Gram assumes that the conditional probability of each possible set of words in a window around a context word c factorizes as the product of the respective conditional probabilities: (Mikolov et al., 2013a) proposed learning the Skip-Gram parameters on a training corpus by using maximum likelihood estimation under (1) and (2).", "labels": [], "entities": []}, {"text": "Thus, if w i denotes the i-th word in the training corpus and T the length of the corpus, we seek the word vectors that maximize As mentioned, the normalized context vectors obtained from maximizing under and exhibit additive compositionality.", "labels": [], "entities": []}, {"text": "For example, the cosine distance between the sum of the context vectors of the words \"Vietnam\" and \"capital\" and the context vector of the word \"Hanoi\" is small.", "labels": [], "entities": []}, {"text": "While there has been much interest in using algebraic operations on word vectors to carryout semantic operations like composition, and mathematically-flavored explanations have been offered (e.g., in the recent work), the only published work which attempts a rigorous theoretical understanding of this phenomenon is (.", "labels": [], "entities": []}, {"text": "This work guarantees that word vectors can be recovered by factorizing the so-called PMI matrix, and that algebraic operations on these word vectors can be used to solve analogies, under certain conditions on the process that generated the training corpus.", "labels": [], "entities": []}, {"text": "Specifically, the word vectors must be known a priori, before their recovery, and to have been generated by randomly scaling uniformly sampled vectors from the unit sphere 1 . Further, the ith word in the corpus must have been selected with probability proportional toe u T w c i , where the \"discourse\" vector c i governs the topic of the corpus at the ith word.", "labels": [], "entities": []}, {"text": "Finally, the discourse vector is assumed to evolve according to a random walk on the unit sphere that has a uniform stationary distribution.", "labels": [], "entities": []}, {"text": "By way of contrast, our results assume nothing a priori about the properties of the word vectors.", "labels": [], "entities": []}, {"text": "In fact, the connection we establish between the Skip-Gram and the Sufficient Dimensionality Reduction model of ( shows that the word vectors learned by Skip-Gram are information-theoretically optimal.", "labels": [], "entities": []}, {"text": "Further, the context word c in the Skip-Gram model essentially serves the role that the discourse vector does in the PMI model of (: the words neighboring care selected with probability proportional toe u Tc vw . We find the exact non-linear composition operator when no assumptions are made on the context word.", "labels": [], "entities": []}, {"text": "When an analogous assumption to that of () is made, that the context words are uniformly distributed, we prove that the composition operator reduces to vector addition.", "labels": [], "entities": []}, {"text": "While our primary motivation has been to provide a better theoretical understanding of word compositionality in the popular Skip-Gram model, our connection with the SDR method illuminates a much more general point about the practical applicability of the Skip-Gram model.", "labels": [], "entities": [{"text": "word compositionality", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.7161256372928619}]}, {"text": "In particular, it addresses the question of whether, fora given corpus, fitting a Skip-Gram model will give good embeddings.", "labels": [], "entities": []}, {"text": "Even if we are making reasonable linguistic assumptions about how to model words and the interdependencies of words in a corpus, it's not clear that these have to hold universally on all corpuses to which we apply Skip-Gram.", "labels": [], "entities": []}, {"text": "However, the fact that when we fit a Skip-Gram model we are fitting an SDR model (up to frequency information), and the fact that SDR models are information-theoretically optimal in a certain sense, argues that regardless of whether the Skip-Gram assumptions hold, Skip-Gram always gives us optimal features in the following sense: the learned context embeddings and target embeddings preserve the maximal amount of mutual information between any pair of random variables X and Y consistent with the observed co-occurence matrix, where Y is the target word and X is the predictor word (in a min-max sense, since there are many ways of coupling X and Y , each of which may have different amounts of mutual information).", "labels": [], "entities": []}, {"text": "Importantly, this statement requires no assumptions on the distribution P (X, Y ).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}