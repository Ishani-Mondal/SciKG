{"title": [{"text": "Semi-supervised sequence tagging with bidirectional language models", "labels": [], "entities": [{"text": "Semi-supervised sequence tagging", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5754951139291128}]}], "abstractContent": [{"text": "Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network archi-tectures for NLP tasks.", "labels": [], "entities": []}, {"text": "However, inmost cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate a general semi-supervised approach for adding pre-trained context embeddings from bidi-rectional language models to NLP systems and apply it to sequence labeling tasks.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 174, "end_pos": 197, "type": "TASK", "confidence": 0.7096800804138184}]}, {"text": "We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.8065938850243887}]}], "introductionContent": [{"text": "Due to their simplicity and efficacy, pre-trained word embedding have become ubiquitous in NLP systems.", "labels": [], "entities": []}, {"text": "Many prior studies have shown that they capture useful semantic and syntactic information () and including them in NLP systems has been shown to be enormously helpful fora variety of downstream tasks).", "labels": [], "entities": []}, {"text": "However, in many NLP tasks it is essential to represent not just the meaning of a word, but also the word in context.", "labels": [], "entities": []}, {"text": "For example, in the two phrases \"A Central Bank spokesman\" and \"The Central African Republic\", the word 'Central' is used as part of both an Organization and Location.", "labels": [], "entities": []}, {"text": "Accordingly, current state of the art sequence tagging models typically include a bidirectional recurrent neural network (RNN) that encodes token sequences into a context sensitive representation before making token specific predictions (.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7530231475830078}]}, {"text": "Although the token representation is initialized with pre-trained embeddings, the parameters of the bidirectional RNN are typically learned only on labeled data.", "labels": [], "entities": []}, {"text": "Previous work has explored methods for jointly learning the bidirectional RNN with supplemental labeled data from other tasks (e.g.,.", "labels": [], "entities": []}, {"text": "In this paper, we explore an alternate semisupervised approach which does not require additional labeled data.", "labels": [], "entities": []}, {"text": "We use a neural language model (LM), pre-trained on a large, unlabeled corpus to compute an encoding of the context at each position in the sequence (hereafter an LM embedding) and use it in the supervised sequence tagging model.", "labels": [], "entities": [{"text": "supervised sequence tagging", "start_pos": 195, "end_pos": 222, "type": "TASK", "confidence": 0.6583049297332764}]}, {"text": "Since the LM embeddings are used to compute the probability of future words in a neural LM, they are likely to encode both the semantic and syntactic roles of words in context.", "labels": [], "entities": []}, {"text": "Our main contribution is to show that the context sensitive representation captured in the LM embeddings is useful in the supervised sequence tagging setting.", "labels": [], "entities": [{"text": "supervised sequence tagging", "start_pos": 122, "end_pos": 149, "type": "TASK", "confidence": 0.6039717296759287}]}, {"text": "When we include the LM embeddings in our system overall performance increases from 90.87% to 91.93% F 1 for the CoNLL 2003 NER task, a more then 1% absolute F1 increase, and a substantial improvement over the previous state of the art.", "labels": [], "entities": [{"text": "F 1", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.994668573141098}, {"text": "CoNLL 2003 NER task", "start_pos": 112, "end_pos": 131, "type": "DATASET", "confidence": 0.8842754513025284}, {"text": "F1", "start_pos": 157, "end_pos": 159, "type": "METRIC", "confidence": 0.9879014492034912}]}, {"text": "We also establish anew state of the art result (96.37% F 1 ) for the CoNLL 2000 Chunking task.", "labels": [], "entities": [{"text": "F 1 )", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.970513919989268}, {"text": "CoNLL 2000 Chunking task", "start_pos": 69, "end_pos": 93, "type": "DATASET", "confidence": 0.8315633684396744}]}, {"text": "As a secondary contribution, we show that using both forward and backward LM embeddings boosts performance over a forward only LM.", "labels": [], "entities": []}, {"text": "We also demonstrate that domain specific pre-training is not necessary by applying a LM trained in the news domain to scientific papers.", "labels": [], "entities": []}, {"text": "2 Language model augmented sequence taggers (TagLM)", "labels": [], "entities": [{"text": "Language model augmented sequence taggers", "start_pos": 2, "end_pos": 43, "type": "TASK", "confidence": 0.5885319113731384}]}], "datasetContent": [{"text": "We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task) and the).", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.6700921952724457}, {"text": "CoNLL 2003 NER task", "start_pos": 77, "end_pos": 96, "type": "DATASET", "confidence": 0.925568550825119}]}, {"text": "We report the official evaluation metric (micro-averaged F 1 ).", "labels": [], "entities": [{"text": "micro-averaged F 1", "start_pos": 42, "end_pos": 60, "type": "METRIC", "confidence": 0.7805210550626119}]}, {"text": "In both cases, we use the BIOES labeling scheme for the output tags, following previous work which showed it outperforms other options (e.g., we trained on both the train and development sets after tuning hyperparameters on the development set.", "labels": [], "entities": [{"text": "BIOES", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9572651386260986}]}, {"text": "The hyperparameters for our baseline model are similar to.", "labels": [], "entities": []}, {"text": "We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character encoder.", "labels": [], "entities": []}, {"text": "The sequence layer uses two bidirectional GRUs with 300 hidden units each.", "labels": [], "entities": []}, {"text": "For regularization, we add 25% dropout to the input of each GRU, but not to the recurrent connections.", "labels": [], "entities": [{"text": "regularization", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9802234172821045}]}, {"text": "The CoNLL 2000 chunking task uses sections 15-18 from the Wall Street Journal corpus for training and section 20 for testing.", "labels": [], "entities": [{"text": "CoNLL 2000 chunking task", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.7546401768922806}, {"text": "Wall Street Journal corpus", "start_pos": 58, "end_pos": 84, "type": "DATASET", "confidence": 0.9755034446716309}]}, {"text": "It defines 11 syntactic chunk types (e.g., NP, VP, ADJP) in addition to other.", "labels": [], "entities": []}, {"text": "We randomly sampled 1000 sentences from the training set as a held-out development set.", "labels": [], "entities": []}, {"text": "The baseline sequence tagger uses 30 dimensional character embeddings and a CNN with 30 filters of width 3 characters followed by a tanh non-linearity for the token character encoder.", "labels": [], "entities": [{"text": "CNN", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.8800175189971924}]}, {"text": "The sequence layer uses two bidirectional LSTMs with 200 hidden units.", "labels": [], "entities": []}, {"text": "Following we added 50% dropout to the character embeddings, the input to each LSTM layer (but not recurrent connections) and to the output of the final LSTM layer.", "labels": [], "entities": []}, {"text": "The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark (), a publicly available benchmark for largescale language modeling.", "labels": [], "entities": [{"text": "largescale language modeling", "start_pos": 129, "end_pos": 157, "type": "TASK", "confidence": 0.6928152640660604}]}, {"text": "The training split has approximately 800 million tokens, about a 4000X increase over the number training tokens in the CoNLL datasets.", "labels": [], "entities": [{"text": "CoNLL datasets", "start_pos": 119, "end_pos": 133, "type": "DATASET", "confidence": 0.979005753993988}]}, {"text": "explored several model architectures and released their best single model and training recipes.", "labels": [], "entities": []}, {"text": "Following, they used linear projection layers at the output of each LSTM layer to reduce the computation time but still maintain a large LSTM state.", "labels": [], "entities": []}, {"text": "Their single best model took three weeks to train on 32 GPUs and achieved 30.0 test perplexity.", "labels": [], "entities": []}, {"text": "It uses a character CNN with 4096 filters for input, followed by two stacked LSTMs, each with 8192 hidden units and a 1024 dimensional projection layer.", "labels": [], "entities": []}, {"text": "We use CNN-BIG-LSTM to refer to this language model in our results.", "labels": [], "entities": [{"text": "CNN-BIG-LSTM", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.9195871949195862}]}, {"text": "In addition to CNN-BIG-LSTM from, we used the same corpus to train two additional language models with fewer parameters: forward LSTM-2048-512 and backward LSTM-2048-512.", "labels": [], "entities": [{"text": "CNN-BIG-LSTM", "start_pos": 15, "end_pos": 27, "type": "DATASET", "confidence": 0.891353964805603}]}, {"text": "Both language models use token embeddings as input to a single layer LSTM with 2048 units and a 512 dimension projection layer.", "labels": [], "entities": []}, {"text": "We closely followed the procedure outlined in  Training.", "labels": [], "entities": []}, {"text": "All experiments use the Adam optimizer () with gradient norms clipped at 5.0.", "labels": [], "entities": []}, {"text": "In all experiments, we fine tune the pre-trained Senna word embeddings but fix all weights in the pre-trained language models.", "labels": [], "entities": []}, {"text": "In addition to explicit dropout regularization, we also use early stopping to prevent over-fitting and use the following process to determine when to stop training.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 60, "end_pos": 74, "type": "METRIC", "confidence": 0.9393778443336487}]}, {"text": "We first train with a constant learning rate \u03b1 = 0.001 on the training data and monitor the development set performance at each epoch.", "labels": [], "entities": [{"text": "learning rate \u03b1", "start_pos": 31, "end_pos": 46, "type": "METRIC", "confidence": 0.828294555346171}]}, {"text": "Then, at the epoch with the highest development performance, we start a simple learning rate annealing schedule: decrease \u03b1 an order of magnitude (i.e., divide by ten), train for five epochs, decrease \u03b1 an order of magnitude again, train for five more epochs and stop.", "labels": [], "entities": []}, {"text": "Following, we train each final model configuration ten times with different random seeds and report the mean and standard deviation F 1 . It is important to estimate the variance of model performance since the test data sets are relatively small.", "labels": [], "entities": [{"text": "standard deviation F", "start_pos": 113, "end_pos": 133, "type": "METRIC", "confidence": 0.8788954814275106}]}, {"text": "compare results from TagLM with previously published state of the art results without additional labeled data or task specific gazetteers.", "labels": [], "entities": [{"text": "TagLM", "start_pos": 21, "end_pos": 26, "type": "TASK", "confidence": 0.8690446615219116}]}, {"text": "compare results of TagLM to other systems that include additional labeled data or gazetteers.", "labels": [], "entities": [{"text": "TagLM", "start_pos": 19, "end_pos": 24, "type": "TASK", "confidence": 0.8379032611846924}]}, {"text": "In both tasks, TagLM establishes anew state of the art using bidirectional LMs (the forward CNN-BIG-LSTM and the backward LSTM-2048-512).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test set F 1 comparison on CoNLL 2003  NER task, using only CoNLL 2003 data and unla- beled text.", "labels": [], "entities": [{"text": "CoNLL 2003  NER task", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.9188328683376312}, {"text": "CoNLL 2003 data", "start_pos": 70, "end_pos": 85, "type": "DATASET", "confidence": 0.9375876188278198}]}, {"text": " Table 2: Test set F 1 comparison on CoNLL 2000  Chunking task using only CoNLL 2000 data and  unlabeled text.", "labels": [], "entities": [{"text": "CoNLL 2000  Chunking task", "start_pos": 37, "end_pos": 62, "type": "DATASET", "confidence": 0.8899706155061722}, {"text": "CoNLL 2000 data", "start_pos": 74, "end_pos": 89, "type": "DATASET", "confidence": 0.9390364289283752}]}, {"text": " Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).", "labels": [], "entities": [{"text": "CoNLL 2003 NER", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.9302450021107992}]}, {"text": " Table 4: Improvements in test set F 1 in CoNLL 2000 Chunking when including additional labeled data  (except the case of TagLM where we do not use additional labeled data).", "labels": [], "entities": [{"text": "CoNLL 2000 Chunking", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.8315487106641134}]}, {"text": " Table 6: Comparison of CoNLL-2003 test set F 1 for different language model combinations. All lan- guage models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256  *  which  was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.", "labels": [], "entities": [{"text": "CoNLL-2003 test set F 1", "start_pos": 24, "end_pos": 47, "type": "DATASET", "confidence": 0.8562411308288574}, {"text": "NER CoNLL 2003 dataset", "start_pos": 253, "end_pos": 275, "type": "DATASET", "confidence": 0.928103968501091}]}]}