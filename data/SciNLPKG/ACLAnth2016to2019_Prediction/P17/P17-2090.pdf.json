{"title": [{"text": "Data Augmentation for Low-Resource Neural Machine Translation", "labels": [], "entities": [{"text": "Data Augmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6916557848453522}, {"text": "Low-Resource Neural Machine Translation", "start_pos": 22, "end_pos": 61, "type": "TASK", "confidence": 0.6362836435437202}]}], "abstractContent": [{"text": "The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.8072211941083273}]}, {"text": "For low-resource language pairs this is not the case, resulting in poor translation quality.", "labels": [], "entities": []}, {"text": "Inspired by work in computer vision , we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts.", "labels": [], "entities": []}, {"text": "Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9971210360527039}, {"text": "BLEU", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9960507750511169}]}], "introductionContent": [{"text": "In computer vision, data augmentation techniques are widely used to increase robustness and improve learning of objects with a limited number of training examples.", "labels": [], "entities": []}, {"text": "In image processing the training data is augmented by, for instance, horizontally flipping, random cropping, tilting, and altering the RGB channels of the original images ().", "labels": [], "entities": [{"text": "image processing", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.8288523256778717}]}, {"text": "Since the content of the new image is still the same, the label of the original image is preserved (see top of).", "labels": [], "entities": []}, {"text": "While data augmentation has become a standard technique to train deep networks for image processing, it is not a common practice in training networks for NLP tasks such as Machine Translation.", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.7616903185844421}, {"text": "image processing", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.7905285358428955}, {"text": "Machine Translation", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.810932993888855}]}, {"text": "Neural Machine Translation (NMT) () is a sequence-to-sequence architecture where an encoder builds up a representation of the source sentence and a decoder, using the previous A boy is holding a bat.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7792828679084778}]}, {"text": "A boy is holding a backpack.", "labels": [], "entities": []}, {"text": "A boy is holding a bat.", "labels": [], "entities": []}, {"text": "A boy is holding a bat.", "labels": [], "entities": []}, {"text": "Ein Junge h\u00e4lt einen Schl\u00e4ger.", "labels": [], "entities": []}, {"text": "Ein Junge h\u00e4lt einen Rucksack.", "labels": [], "entities": []}, {"text": "LSTM hidden states and an attention mechanism, generates the target translation.", "labels": [], "entities": []}, {"text": "To train a model with reliable parameter estimations, these networks require numerous instances of sentence translation pairs with words occurring in diverse contexts, which is typically not available in low-resource language pairs.", "labels": [], "entities": [{"text": "sentence translation pairs", "start_pos": 99, "end_pos": 125, "type": "TASK", "confidence": 0.7891281048456827}]}, {"text": "As a result NMT falls short of reaching state-of-the-art performances for these language pairs (.", "labels": [], "entities": [{"text": "NMT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.6833963394165039}]}, {"text": "The solution is to either manually annotate more data or perform unsupervised data augmentation.", "labels": [], "entities": []}, {"text": "Since manual annotation of data is timeconsuming, data augmentation for low-resource language pairs is a more viable approach.", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7231552600860596}]}, {"text": "Recently proposed a method to back-translate sentences from monolingual data and augment the bitext with the resulting pseudo parallel corpora.", "labels": [], "entities": []}, {"text": "In this paper, we propose a simple yet effective approach, translation data augmentation (TDA), that augments the training data by altering existing sentences in the parallel corpus, similar in spirit to the data augmentation approaches in computer vision (see).", "labels": [], "entities": [{"text": "translation data augmentation (TDA)", "start_pos": 59, "end_pos": 94, "type": "TASK", "confidence": 0.7157422800858816}]}, {"text": "In order for the augmentation process in this scenario to be label-preserving, any change to a sentence in one language must pre-serve the meaning of the sentence, requiring sentential paraphrasing systems which are not available for many language pairs.", "labels": [], "entities": []}, {"text": "Instead, we propose a weaker notion of label preservation that allows to alter both source and target sentences at the same time as long as they remain translations of each other.", "labels": [], "entities": [{"text": "label preservation", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7431101202964783}]}, {"text": "While our approach allows us to augment data in numerous ways, we focus on augmenting instances involving low-frequency words, because the parameter estimation of rare words is challenging, and further exacerbated in a lowresource setting.", "labels": [], "entities": []}, {"text": "We simulate a low-resource setting as done in the literature () and obtain substantial improvements for translating English\u00d1German and German\u00d1English.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate the utility of our approach in a simulated low-resource NMT scenario.", "labels": [], "entities": []}, {"text": "To simulate a low-resource setting we randomly sample 10% of the English\u00d8German WMT15 training data and report results on newstest).", "labels": [], "entities": [{"text": "English\u00d8German WMT15 training data", "start_pos": 65, "end_pos": 99, "type": "DATASET", "confidence": 0.8913328647613525}, {"text": "newstest", "start_pos": 122, "end_pos": 130, "type": "DATASET", "confidence": 0.9397256970405579}]}, {"text": "For reference we also provide the result of our baseline system on the full data.", "labels": [], "entities": []}, {"text": "As NMT system we use a 4-layer attentionbased encoder-decoder model as described in) trained with hidden dimension 1000, batch size 80 for 20 epochs.", "labels": [], "entities": []}, {"text": "In all experiments the NMT vocabulary is limited to the most common 30K words in both languages.", "labels": [], "entities": []}, {"text": "Note that data augmentation does not introduce new words to the vocabulary.", "labels": [], "entities": []}, {"text": "In all experiments we preprocess source and target language data with Bytepair encoding (BPE) (Sennrich et al., 2016b) using 30K merge operations.", "labels": [], "entities": [{"text": "Bytepair encoding (BPE)", "start_pos": 70, "end_pos": 93, "type": "METRIC", "confidence": 0.8803846716880799}]}, {"text": "In the augmentation experiments BPE is performed after data augmentation.", "labels": [], "entities": [{"text": "BPE", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.5531701445579529}]}, {"text": "For the LMs needed for data augmentation, we train 2-layer LSTM networks in forward and backward directions on the monolingual data provided for the same task (3.5B and 0.9B tokens in English and German respectively) with embedding size 64 and hidden size 128.", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.6803639084100723}]}, {"text": "We set the rare word threshold R to 100, top K words to 1000 and maximum number N of augmentations per rare word to 500.", "labels": [], "entities": []}, {"text": "In all experiments we use the English LM for the rare word substitutions, and the German LM to choose the optimal word translation in context.", "labels": [], "entities": []}, {"text": "Since our approach is not label preserving we only perform augmentation during training and do not alter source sentences during testing.", "labels": [], "entities": []}, {"text": "We also compare our approach to by back-translating monolingual data and adding it to the parallel training data.", "labels": [], "entities": []}, {"text": "Specifically, we back-translate sentences from the target side of WMT'15 that are not included in our low-resource baseline with two settings: keeping a one-to-one ratio of back-translated versus original data (1 : 1) following the authors' suggestion, or using three times more back-translated data (3 : 1).", "labels": [], "entities": [{"text": "WMT'15", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.9204241037368774}]}, {"text": "We measure translation quality by singlereference case-insensitive BLEU () computed with the multi-bleu.perl script from Moses.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9680411219596863}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9819018244743347}]}], "tableCaptions": [{"text": " Table 2: Translation performance (BLEU) on German-English and English-German WMT test sets", "labels": [], "entities": [{"text": "Translation performance (BLEU)", "start_pos": 10, "end_pos": 40, "type": "METRIC", "confidence": 0.757325166463852}, {"text": "German-English and English-German WMT test sets", "start_pos": 44, "end_pos": 91, "type": "DATASET", "confidence": 0.7051393141349157}]}, {"text": " Table 3: An example from newstest2014 illustrating the effect of augmenting rare words on generation  during test time. The translation of the baseline does not include the rare word centimetres, however, the  translation of our TDA model generates the rare word and produces a more fluent sentence. Instances of  the augmentation of the word centimetres in training data are also provided.", "labels": [], "entities": []}]}