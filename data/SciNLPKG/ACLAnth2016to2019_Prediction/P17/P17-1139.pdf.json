{"title": [{"text": "Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization", "labels": [], "entities": [{"text": "Prior Knowledge Integration", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6723784605662028}, {"text": "Neural Machine Translation", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.7521576484044393}]}], "abstractContent": [{"text": "Although neural machine translation has made significant progress recently, how to integrate multiple overlapping, arbitrary prior knowledge sources remains a challenge.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.706326981385549}]}, {"text": "In this work, we propose to use posterior regularization to provide a general framework for integrating prior knowledge into neural machine translation.", "labels": [], "entities": [{"text": "posterior regularization", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.6282794177532196}, {"text": "neural machine translation", "start_pos": 125, "end_pos": 151, "type": "TASK", "confidence": 0.7228672901789347}]}, {"text": "We represent prior knowledge sources as features in a log-linear model, which guides the learning process of the neural translation model.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7798991501331329}]}, {"text": "Experiments on Chinese-English translation show that our approach leads to significant improvements.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.6425765305757523}]}], "introductionContent": [{"text": "The past several years have witnessed the rapid development of neural machine translation (NMT), which aims to model the translation process using neural networks in an end-to-end manner.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.848855217297872}]}, {"text": "With the capability of capturing long-distance dependencies due to the gating) and attention mechanisms, NMT has shown remarkable superiority over conventional statistical machine translation (SMT) across a variety of natural languages).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 160, "end_pos": 197, "type": "TASK", "confidence": 0.7896193166573843}]}, {"text": "Despite the apparent success, NMT still suffers from one significant drawback: it is difficult to integrate prior knowledge into neural networks.", "labels": [], "entities": [{"text": "NMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9314234256744385}]}, {"text": "On one hand, neural networks use continuous realvalued vectors to represent all language structures involved in the translation process.", "labels": [], "entities": []}, {"text": "While these vector representations prove to be capable of capturing translation regularities implicitly (Sutskever * Corresponding author: Yang Liu.", "labels": [], "entities": []}, {"text": "et al.,), it is hard to interpret each hidden state in neural networks from a linguistic perspective.", "labels": [], "entities": []}, {"text": "On the other hand, prior knowledge in machine translation is usually represented in discrete symbolic forms such as dictionaries and rules) that explicitly encode translation regularities.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7453464567661285}]}, {"text": "It is difficult to transform prior knowledge represented in discrete forms to continuous representations required by neural networks.", "labels": [], "entities": []}, {"text": "Therefore, a number of authors have endeavored to integrate prior knowledge into NMT in recent years, either by modifying model architectures ( or by modifying training objectives (.", "labels": [], "entities": []}, {"text": "For example, to address the over-translation and under-translation problems widely observed in NMT,  directly extend standard NMT to model the coverage constraint that each source phrase should be translated into exactly one target phrase ().", "labels": [], "entities": []}, {"text": "Alternatively, and propose to control the fertilities of source words by appending additional additive terms to training objectives.", "labels": [], "entities": []}, {"text": "Although these approaches have demonstrated clear benefits of incorporating prior knowledge into NMT, how to combine multiple overlapping, arbitrary prior knowledge sources still remains a major challenge.", "labels": [], "entities": []}, {"text": "It is difficult to achieve this end by directly modifying model architectures because neural networks usually impose strong independence assumptions between hidden states.", "labels": [], "entities": []}, {"text": "As a result, extending a neural model requires that the interdependence of information sources be modeled explicitly (, making it hard to extend.", "labels": [], "entities": []}, {"text": "While this drawback can be partly alleviated by appending additional additive terms to training objectives (, these terms are restricted to a limited number of simple constraints.", "labels": [], "entities": []}, {"text": "In this work, we propose a general framework for integrating multiple overlapping, arbitrary prior knowledge sources into NMT using posterior regularization (.", "labels": [], "entities": []}, {"text": "Our framework is capable of incorporating indirect supervision via posterior distributions of neural translation models.", "labels": [], "entities": []}, {"text": "To represent prior knowledge sources as arbitrary real-valued features, we define the posterior distribution as a loglinear model instead of a constrained posterior set ().", "labels": [], "entities": []}, {"text": "This treatment not only leads to a simpler and more efficient training algorithm but also achieves better translation performance.", "labels": [], "entities": []}, {"text": "Experiments show that our approach is able to incorporate a variety of features and achieves significant improvements over posterior regularization using constrained posterior sets on NIST Chinese-English datasets.", "labels": [], "entities": [{"text": "NIST Chinese-English datasets", "start_pos": 184, "end_pos": 213, "type": "DATASET", "confidence": 0.9421749114990234}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of BLEU scores on the Chinese-English datasets. RNNSEARCH is an attention- based neural machine translation model (Bahdanau et al., 2015) that does not incorporate prior knowl- edge. CPR extends RNNSEARCH by introducing coverage penalty refinement (Eq.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9984726309776306}, {"text": "Chinese-English datasets", "start_pos": 43, "end_pos": 67, "type": "DATASET", "confidence": 0.7116295397281647}, {"text": "attention- based neural machine translation", "start_pos": 85, "end_pos": 128, "type": "TASK", "confidence": 0.7228399813175201}, {"text": "coverage penalty refinement", "start_pos": 241, "end_pos": 268, "type": "METRIC", "confidence": 0.9202044208844503}]}, {"text": " Table 2: Effect of reranking on translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9629803895950317}]}]}