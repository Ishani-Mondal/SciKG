{"title": [{"text": "Feature Hashing for Language and Dialect Identification", "labels": [], "entities": [{"text": "Language and Dialect Identification", "start_pos": 20, "end_pos": 55, "type": "TASK", "confidence": 0.6828790307044983}]}], "abstractContent": [{"text": "We evaluate feature hashing for language identification (LID), a method not previously used for this task.", "labels": [], "entities": [{"text": "language identification (LID)", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.8136262774467469}]}, {"text": "Using a standard dataset, we first show that while feature performance is high, LID data is highly dimensional and mostly sparse (>99.5%) as it includes large vocabularies for many languages; memory requirements grow as languages are added.", "labels": [], "entities": []}, {"text": "Next we apply hash-ing using various hash sizes, demonstrating that there is no performance loss with dimensionality reductions of up to 86%.", "labels": [], "entities": []}, {"text": "We also show that using an ensemble of low-dimension hash-based classifiers further boosts performance.", "labels": [], "entities": []}, {"text": "Feature hashing is highly useful for LID and holds great promise for future work in this area.", "labels": [], "entities": [{"text": "Feature hashing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6738461554050446}, {"text": "LID", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.7252808809280396}]}], "introductionContent": [{"text": "Language Identification (LID) is the task of determining the language of a text, at the document, sub-document or even sentence level.", "labels": [], "entities": [{"text": "Language Identification (LID) is the task of determining the language of a text, at the document, sub-document or even sentence level", "start_pos": 0, "end_pos": 133, "type": "Description", "confidence": 0.7191671848297119}]}, {"text": "LID is a fundamental preprocessing task in NLP and is also used in lexicography, machine translation and information retrieval.", "labels": [], "entities": [{"text": "LID", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9211090207099915}, {"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.8192352652549744}, {"text": "information retrieval", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.8041591048240662}]}, {"text": "It is widely used for filtering to select documents in a specific language; e.g. LID can filter webpages or tweets by language.", "labels": [], "entities": []}, {"text": "Although LID has been widely studied, several open issues remain ().", "labels": [], "entities": [{"text": "LID", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.7692334651947021}]}, {"text": "Current goals include developing models that can identify thousands of languages; extending the task to more fine-grained dialect identification; and making LID functionality more readily available to users/developers.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.761411726474762}]}, {"text": "A common challenge among these goals is dealing with high dimensional feature spaces.", "labels": [], "entities": []}, {"text": "LID differs from traditional text categorization tasks in some important aspects.", "labels": [], "entities": []}, {"text": "Standard tasks, such as topic classification, are usually performed within a single language, and the maximum feature space size is a function of the single language's vocabulary.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.8630251288414001}]}, {"text": "However, LID must deal with vocabulary from many languages and the feature space grows prodigiously.", "labels": [], "entities": []}, {"text": "This raises immediate concerns about memory requirements for such systems and portends implementation issues for applying the systems to dozens, hundreds or even thousands of languages.", "labels": [], "entities": []}, {"text": "Recent LID work has reported results on datasets including over 1,300 languages, albeit using small samples.", "labels": [], "entities": []}, {"text": "Such models are going to include an extraordinarily large feature space, and individual vectors for each sample are going to be extremely sparse.", "labels": [], "entities": []}, {"text": "LID is usually done using n-grams and as the number of languages and/or n gets larger, the feature space will become prohibitively large or impractical for real-world use.", "labels": [], "entities": []}, {"text": "For high dimensional input, traditional dimensionality reduction methods (e.g. PCA, LDA) can be computationally expensive.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.7305296510457993}]}, {"text": "Feature selection methods, e.g. those using entropy, are simpler but still expensive.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.786871075630188}]}, {"text": "Recently, feature hashing has been shown to be a very effective dimensionality reduction method.", "labels": [], "entities": [{"text": "feature hashing", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7865017950534821}, {"text": "dimensionality reduction", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.7236284762620926}]}, {"text": "It has proven to be useful in numerous machine learning applications, particularly for handling extremely high dimensional data.", "labels": [], "entities": []}, {"text": "It also provides numerous other benefits, which we describe in \u00a72.1.", "labels": [], "entities": []}, {"text": "Although hashing could be tremendously useful for LID, to our knowledge no such experiments have been reported to date.", "labels": [], "entities": [{"text": "LID", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9534383416175842}]}, {"text": "It is unclear how collisions of features from different languages would affect its application for LID.", "labels": [], "entities": []}, {"text": "Accordingly, the aims of the present work are to: (1) evaluate the effectiveness of hashing for LID; (2) compare its performance to the standard n-gram approach; (3) assess the role of hash size (and collision rate) on accuracy for different feature types; and (4) determine if ensemble methods can boost performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 219, "end_pos": 227, "type": "METRIC", "confidence": 0.9971555471420288}]}], "datasetContent": [{"text": "Our methodology is based on the results of 2016 DSL Shared Task () and we use their dataset.", "labels": [], "entities": [{"text": "DSL Shared Task", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.7504589557647705}]}, {"text": "The DSL task is performed at the sentence level, making it more challenging.", "labels": [], "entities": []}, {"text": "Participants applied various methods, but the task organizers note that linear classifiers, particularly SVMs, were the most successful (.", "labels": [], "entities": []}, {"text": "This is unsurprising as SVMs have been very successful for text classification and we adopt this method.", "labels": [], "entities": [{"text": "text classification", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8390909433364868}]}, {"text": "The data is balanced across classes, so accuracy is used as the evaluation metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9993539452552795}]}], "tableCaptions": [{"text": " Table 1: Test set accuracy for hashed features at each hash size. Baseline is accuracy without hashing.  Best result (w/ smallest hash) per row in bold. Last column is the best result's reduction in dimensionality.  We observe that every feature matches its baseline at a hash size smaller than its full feature space.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9808180332183838}, {"text": "Baseline", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9841192364692688}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9994527697563171}]}, {"text": " Table 1.  We see very significant reductions of up to 86%  in dimensionality without any performance loss.  Character 4-grams yield very competitive results  (0.88) with a large feature space reduction of 82%  using a hash size of 2 16 .", "labels": [], "entities": []}]}