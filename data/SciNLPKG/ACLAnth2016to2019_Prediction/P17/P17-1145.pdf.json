{"title": [{"text": "Watset: Automatic Induction of Synsets from a Graph of Synonyms", "labels": [], "entities": [{"text": "Automatic Induction of Synsets from a Graph of Synonyms", "start_pos": 8, "end_pos": 63, "type": "TASK", "confidence": 0.8174958527088165}]}], "abstractContent": [{"text": "This paper presents anew graph-based approach that induces synsets using syn-onymy dictionaries and word embeddings.", "labels": [], "entities": []}, {"text": "First, we build a weighted graph of synonyms extracted from commonly available resources, such as Wiktionary.", "labels": [], "entities": []}, {"text": "Second, we apply word sense induction to deal with ambiguous words.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.7983252008756002}]}, {"text": "Finally, we cluster the disambiguated version of the ambiguous input graph into synsets.", "labels": [], "entities": []}, {"text": "Our meta-clustering approach lets us use an efficient hard clustering algorithm to perform a fuzzy clustering of the graph.", "labels": [], "entities": []}, {"text": "Despite its simplicity, our approach shows excellent results, outperforming five competitive state-of-the-art methods in terms of F-score on three gold standard datasets for English and Russian derived from large-scale manually constructed lexical resources.", "labels": [], "entities": [{"text": "F-score", "start_pos": 130, "end_pos": 137, "type": "METRIC", "confidence": 0.9975334405899048}]}], "introductionContent": [{"text": "A synset is a set of mutual synonyms, which can be represented as a clique graph where nodes are words and edges are synonymy relations.", "labels": [], "entities": []}, {"text": "Synsets represent word senses and are building blocks of WordNet and similar resources such as thesauri and lexical ontologies.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9283691644668579}]}, {"text": "These resources are crucial for many natural language processing applications that require commonsense reasoning, such as information retrieval) and question answering (;).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 122, "end_pos": 143, "type": "TASK", "confidence": 0.7750105261802673}, {"text": "question answering", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.9045648276805878}]}, {"text": "However, for most languages, no manually-constructed resource is available that is comparable to the English WordNet in terms of coverage and quality.", "labels": [], "entities": []}, {"text": "For instance, present a comparative analysis of lexical resources available for the Russian language concluding that there is no resource compared to WordNet in terms of coverage and quality for Russian.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 150, "end_pos": 157, "type": "DATASET", "confidence": 0.9466440677642822}]}, {"text": "This lack of linguistic resources for many languages urges the development of new methods for automatic construction of WordNetlike resources.", "labels": [], "entities": []}, {"text": "The automatic methods foster construction and use of the new lexical resources.", "labels": [], "entities": []}, {"text": "Wikipedia 1 , Wiktionary 2 , OmegaWiki 3 and other collaboratively-created resources contain a large amount of lexical semantic informationyet designed to be human-readable and not formally structured.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9291173219680786}]}, {"text": "While semantic relations can be automatically extracted using tools such as DKPro JWKTL and Wikokit , words in these relations are not disambiguated.", "labels": [], "entities": [{"text": "DKPro JWKTL", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.7753918170928955}]}, {"text": "For instance, the synonymy pairs (bank, streambank) and (bank, banking company) will be connected via the word \"bank\", while they refer to the different senses.", "labels": [], "entities": []}, {"text": "This problem stems from the fact that articles in Wiktionary and similar resources list undisambiguated synonyms.", "labels": [], "entities": []}, {"text": "They are easy to disambiguate for humans while reading a dictionary article, but can be a source of errors for language processing systems.", "labels": [], "entities": []}, {"text": "The contribution of this paper is a novel approach that resolves ambiguities in the input graph to perform fuzzy clustering.", "labels": [], "entities": [{"text": "fuzzy clustering", "start_pos": 107, "end_pos": 123, "type": "TASK", "confidence": 0.6519892662763596}]}, {"text": "The method takes as an input synonymy relations between potentially ambiguous terms available in human-readable dictionaries and transforms them into a machine readable representation in the form of disambiguated synsets.", "labels": [], "entities": []}, {"text": "Our method, called WATSET, is based on anew local-global meta-algorithm for fuzzy graph clustering.", "labels": [], "entities": [{"text": "fuzzy graph clustering", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.6302849352359772}]}, {"text": "The underlying principle is to discover the word senses based on a local graph cluster-ing, and then to induce synsets using global sense clustering.", "labels": [], "entities": []}, {"text": "We show that our method outperforms other methods for synset induction.", "labels": [], "entities": [{"text": "synset induction", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.8697874844074249}]}, {"text": "The induced resource eliminates the need in manual synset construction and can be used to build WordNet-like semantic networks for under-resourced languages.", "labels": [], "entities": [{"text": "synset construction", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7682964503765106}]}, {"text": "An implementation of our method along with induced lexical resources is available online.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct our experiments on resources from two different languages.", "labels": [], "entities": []}, {"text": "We evaluate our approach on two datasets for English to demonstrate its performance on a resource-rich language.", "labels": [], "entities": []}, {"text": "Additionally, we evaluate it on two Russian datasets since Russian is a good example of an under-resourced language with a clear need for synset induction.", "labels": [], "entities": [{"text": "synset induction", "start_pos": 138, "end_pos": 154, "type": "TASK", "confidence": 0.6900857985019684}]}, {"text": "For each language, we used two differently constructed lexical semantic resources listed in  appears to be de facto gold standard in similar tasks.", "labels": [], "entities": []}, {"text": "We used WordNet 3.1 to derive the synonymy pairs from synsets.", "labels": [], "entities": []}, {"text": "Additionally, we use BabelNet 9 , a large-scale multilingual semantic network constructed automatically using WordNet, Wikipedia and other resources.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.9409588575363159}]}, {"text": "We retrieved all the synonymy pairs from the BabelNet 3.7 synsets marked as English.", "labels": [], "entities": [{"text": "BabelNet 3.7 synsets", "start_pos": 45, "end_pos": 65, "type": "DATASET", "confidence": 0.8862545092900594}]}, {"text": "As a lexical ontology for Russian, we use RuWordNet 10 (Loukachevitch et al., 2016), containing both general vocabulary and domainspecific synsets related to sport, finance, economics, etc.", "labels": [], "entities": []}, {"text": "Up to a half of the words in this resource are multi-word expressions (, which is due to the coverage of domainspecific vocabulary.", "labels": [], "entities": []}, {"text": "RuWordNet is a WordNetlike version of the RuThes thesaurus that is constructed in the traditional way, namely by a small group of expert lexicographers.", "labels": [], "entities": [{"text": "RuWordNet", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9188851118087769}]}, {"text": "In addition, we use Yet Another RussNet 11 (YARN) by as another gold standard for Russian.", "labels": [], "entities": [{"text": "Yet Another RussNet 11 (YARN)", "start_pos": 20, "end_pos": 49, "type": "DATASET", "confidence": 0.7278207838535309}]}, {"text": "The resource is constructed using crowdsourcing and mostly covers general vocabulary.", "labels": [], "entities": []}, {"text": "Particularly, non-expert users are allowed to edit synsets in a collaborative way loosely supervised by a team of project curators.", "labels": [], "entities": []}, {"text": "Due to the ongoing development of the re-source, we selected as the gold standard only those synsets that were edited at least eight times in order to filter out noisy incomplete synsets.: Statistics of the gold standard datasets.", "labels": [], "entities": [{"text": "gold standard datasets", "start_pos": 207, "end_pos": 229, "type": "DATASET", "confidence": 0.8271950085957845}]}, {"text": "To evaluate the quality of the induced synsets, we transformed them into binary synonymy relations and computed precision, recall, and F-score on the basis of the overlap of these binary relations with the binary relations from the gold standard datasets.", "labels": [], "entities": [{"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9988527297973633}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9995161294937134}, {"text": "F-score", "start_pos": 135, "end_pos": 142, "type": "METRIC", "confidence": 0.9991683959960938}]}, {"text": "Given a synset containing n words, we generate a set of n(n\u22121) 2 pairs of synonyms.", "labels": [], "entities": []}, {"text": "The F-score calculated this way is known as Paired F-score (.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.99774169921875}, {"text": "Paired", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9693738222122192}, {"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.514060914516449}]}, {"text": "The advantage of this measure compared to other cluster evaluation measures, such as Fuzzy B-Cubed (, is its straightforward interpretability.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the gold standard datasets.", "labels": [], "entities": [{"text": "gold standard datasets", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.8767347733179728}]}, {"text": " Table 2: Statistics of the input datasets.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of the synset induction methods on datasets for English. All methods rely on the  similarity edge weighting (sim); best configurations of each method in terms of F-scores are shown for  each dataset. Results are sorted by F-score on BabelNet, top three values of each metric are boldfaced.", "labels": [], "entities": [{"text": "similarity edge weighting (sim)", "start_pos": 103, "end_pos": 134, "type": "METRIC", "confidence": 0.8986229101816813}, {"text": "F-scores", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9664182662963867}, {"text": "F-score", "start_pos": 243, "end_pos": 250, "type": "METRIC", "confidence": 0.9305349588394165}]}, {"text": " Table 4: Results on Russian sorted by F-score on YARN, top three values of each metric are boldfaced.", "labels": [], "entities": [{"text": "F-score", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9933620691299438}, {"text": "YARN", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.5402030944824219}]}]}