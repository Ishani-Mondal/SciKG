{"title": [], "abstractContent": [{"text": "Motivated by concerns for user privacy , we design a steganographic system (\"stegosystem\") that enables two users to exchange encrypted messages without an adversary detecting that such an exchange is taking place.", "labels": [], "entities": []}, {"text": "We propose anew linguistic stegosystem based on a Long Short-Term Memory (LSTM) neural network.", "labels": [], "entities": []}, {"text": "We demonstrate our approach on the Twit-ter and Enron email datasets and show that it yields high-quality steganographic text while significantly improving capacity (encrypted bits per word) relative to the state-of-the-art.", "labels": [], "entities": [{"text": "Twit-ter and Enron email datasets", "start_pos": 35, "end_pos": 68, "type": "DATASET", "confidence": 0.7726179957389832}]}], "introductionContent": [{"text": "The business model behind modern communication systems (email services or messaging services provided by social networks) is incompatible with end-to-end message encryption.", "labels": [], "entities": [{"text": "end-to-end message encryption", "start_pos": 143, "end_pos": 172, "type": "TASK", "confidence": 0.767114500204722}]}, {"text": "The providers of these services can afford to offer them free of charge because most of their users agree to receive \"targeted ads\" (ads that are especially chosen to appeal to each user, based on the needs the user has implied through their messages).", "labels": [], "entities": []}, {"text": "This model works as long as users communicate mostly in the clear, which enables service providers to make informed guesses about user needs.", "labels": [], "entities": []}, {"text": "This situation does not prevent users from encrypting a few sensitive messages, but it does takeaway some of the benefits of confidentiality.", "labels": [], "entities": []}, {"text": "For instance, imagine a scenario where two users want to exchange forbidden ideas or organize forbidden events under an authoritarian regime; in a world where most communication happens in the clear, encrypting a small fraction of messages automatically makes these messages-and the users who exchange them-suspicious.", "labels": [], "entities": []}, {"text": "With this motivation in mind, we want to design a system that enables two users to exchange encrypted messages, such that a passive adversary that reads the messages can determine neither the original content of the messages nor the fact that the messages are encrypted.", "labels": [], "entities": []}, {"text": "We build on linguistic steganography, i.e., the science of encoding a secret piece of information (\"payload\") into apiece of text that looks like natural language (\"stegotext\").", "labels": [], "entities": []}, {"text": "We propose a novel stegosystem, based on a neural network, and demonstrate that it combines high quality of output (i.e., the stegotext indeed looks like natural language) with the highest capacity (number of bits encrypted per word) published in literature.", "labels": [], "entities": []}, {"text": "In the rest of the paper, we describe existing linguistic stegosystems along with ours ( \u00a72), provide details on our system ( \u00a73), present preliminary experimental results on Twitter and email messages ( \u00a74), and conclude with future directions ( \u00a75).", "labels": [], "entities": []}], "datasetContent": [{"text": "We use perplexity to quantify stegotext quality; and capacity (i.e., encrypted bits per output word) to quantify its efficiency in carrying secret information.", "labels": [], "entities": []}, {"text": "In Section 4, we also discuss our stegotext quality as empirically perceived by us as human readers.", "labels": [], "entities": []}, {"text": "Perplexity is a standard metric for the quality of language models (Martin and Jurafsky, 2000), and it is defined as the average per-word log-probability on the valid data set: Instead, we measure the probability of w i by taking the average of p[w i ] overall possible secret bit blocks B, under the assumption that bit blocks are distributed uniformly.", "labels": [], "entities": []}, {"text": "By the Law of Large Numbers), if we perform many stegotext-generating trials using different random secret data as input, the probability of each word will tend to the expected value, Capacity.", "labels": [], "entities": [{"text": "Capacity", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9785459041595459}]}, {"text": "Our system's capacity is the number of encrypted bits per output word.", "labels": [], "entities": []}, {"text": "Without common tokens, capacity is always |B| bits/word (since each bit block of size |B| is always mapped to one output word).", "labels": [], "entities": []}, {"text": "In the common-token variant, capacity decreases because the output includes common tokens that do not carry any secret information; in particular, if the fraction of common tokens is p, then capacity is (1 \u2212 p) \u00b7 |B|.", "labels": [], "entities": []}, {"text": "In this section, we present our preliminary experimental evaluation: our Twitter and email datasets ( \u00a74.1), details about the LSTMs used to produce our results ( \u00a74.2), and finally a discussion of our results ( \u00a74.3).", "labels": [], "entities": []}, {"text": "Tweets and emails are among the most popular media of open communication and therefore provide very realistic environments for hiding information.", "labels": [], "entities": []}, {"text": "We thus trained our LSTMs on those two domains, Twitter messages and Enron emails), which vary greatly in message length and vocabulary size.", "labels": [], "entities": []}, {"text": "For Twitter, we used the NLTK tokenizer to tokenize tweets) into words and punctuation marks.", "labels": [], "entities": []}, {"text": "We normalized the content by replacing usernames and URLs with a username token (<user>) and a URL token (<url>), respectively.", "labels": [], "entities": []}, {"text": "We used 600 thousand tweets with a total of 45 million words and a vocabulary of size 225 thousand.", "labels": [], "entities": []}, {"text": "For Enron, we cleaned and extracted email message bodies () from the Enron dataset, and we tokenized the messages into words and punctuation marks.", "labels": [], "entities": [{"text": "Enron dataset", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.924263209104538}]}, {"text": "We took the first 100MB of the resulting messages, with 16.8 million tokens and a vocabulary size of 406 thousand.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: An increase of of capacity correlates  with an increase of perplexity, which implies that  there is a negative correlation between capacity  and text quality. After adding common tokens,  there is a significant reduction in perplexity (ppl),  at the expense of a lower capacity (bits per word).", "labels": [], "entities": []}]}