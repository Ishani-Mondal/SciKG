{"title": [{"text": "From Characters to Words to in Between: Do We Capture Morphology?", "labels": [], "entities": []}], "abstractContent": [{"text": "Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams.", "labels": [], "entities": []}, {"text": "While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies.", "labels": [], "entities": []}, {"text": "On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled.", "labels": [], "entities": [{"text": "language modeling task", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.7935641010602316}]}, {"text": "Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others.", "labels": [], "entities": []}, {"text": "But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses , even when learned from an order of magnitude more data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Continuous representations of words learned by neural networks are central to many NLP tasks ().", "labels": [], "entities": []}, {"text": "However, directly mapping a finite set of word types to a continuous representation has well-known limitations.", "labels": [], "entities": []}, {"text": "First, it makes a closed vocabulary assumption, enabling only generic out-of-vocabulary handling.", "labels": [], "entities": []}, {"text": "Second, it cannot exploit systematic functional relationships in learning.", "labels": [], "entities": []}, {"text": "For example, cat and cats stand in the same relationship as dog and dogs.", "labels": [], "entities": []}, {"text": "While this relationship might be discovered for these specific frequent words, it does not help us learn that the same relationship also holds for the much rarer words sloth and sloths.", "labels": [], "entities": []}, {"text": "These functional relationships reflect the fact that words are composed from smaller units of meaning, or morphemes.", "labels": [], "entities": []}, {"text": "For instance, cats consists of two morphemes, cat and -s, with the latter shared by the words dogs and tarsiers.", "labels": [], "entities": []}, {"text": "Modeling this effect is crucial for languages with rich morphology, where vocabulary sizes are larger, many more words are rare, and many more such functional relationships exist.", "labels": [], "entities": []}, {"text": "Hence, some models produce word representations as a function of subword units obtained from morphological segmentation or analysis (.", "labels": [], "entities": []}, {"text": "A downside of these models is that they depend on morphological segmenters or analyzers.", "labels": [], "entities": []}, {"text": "Morphemes typically have similar orthographic representations across words.", "labels": [], "entities": []}, {"text": "For example, the morpheme -s is realized as -es in finches.", "labels": [], "entities": []}, {"text": "Since this variation is limited, the general relationship between morphology and orthography can be exploited by composing the representations of characters (, character n-grams (), bytes, or combinations thereof).", "labels": [], "entities": []}, {"text": "These models are compact, can represent rare and unknown words, and do not require morphological analyzers.", "labels": [], "entities": []}, {"text": "They raise a provocative question: Does NLP benefit from models of morphology, or can they be replaced entirely by models of characters?", "labels": [], "entities": []}, {"text": "The relative merits of word, subword. and character-level models are not fully understood because each new model has been compared on different tasks and datasets, and often compared against word-level models.", "labels": [], "entities": []}, {"text": "A number of questions remain open: 1.", "labels": [], "entities": []}, {"text": "How do representations based on morphemes compare with those based on characters?", "labels": [], "entities": []}, {"text": "2. What is the best way to compose subword representations?", "labels": [], "entities": []}, {"text": "3. Do character-level models capture morphology in terms of predictive utility?", "labels": [], "entities": []}, {"text": "4. How do different representations interact with languages of different morphological typologies?", "labels": [], "entities": []}, {"text": "The last question is raised by Bender (2013): languages are typologically diverse, and the behavior of a model on one language may not generalize to others.", "labels": [], "entities": []}, {"text": "Character-level models implicitly assume concatenative morphology, but many widely-spoken languages feature nonconcatenative morphology, and it is unclear how such models will behave on these languages.", "labels": [], "entities": []}, {"text": "To answer these questions, we performed a systematic comparison across different models for the simple and ubiquitous task of language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.7840799987316132}]}, {"text": "We present experiments that vary (1) the type of subword unit; (2) the composition function; and (3) morphological typology.", "labels": [], "entities": []}, {"text": "To understand the extent to which character-level models capture true morphological regularities, we present oracle experiments using human morphological annotations instead of automatic morphological segments.", "labels": [], "entities": []}, {"text": "Our results show that: 1.", "labels": [], "entities": []}, {"text": "For most languages, character-level representations outperform the standard word representations.", "labels": [], "entities": []}, {"text": "Most interestingly, a previously unstudied combination of character trigrams composed with bi-LSTMs performs best on the majority of languages.", "labels": [], "entities": []}, {"text": "2. Bi-LSTMs and CNNs are more effective composition functions than addition.", "labels": [], "entities": []}, {"text": "3. Character-level models learn functional relationships between orthographically similar words, but don't (yet) match the predictive accuracy of models with access to true morphological analyses.", "labels": [], "entities": []}, {"text": "4. Character-level models are effective across a range of morphological typologies, but orthography influences their effectiveness.", "labels": [], "entities": []}, {"text": "word tries morphemes try+s morphs tri+es morph.", "labels": [], "entities": []}, {"text": "analysis try+VB+3rd+SG+Pres: The morphemes, morphs, and morphological analysis of tries.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments on ten languages (  To ensure that we compared models and not implementations, we reimplemented all models in a single framework using Tensorflow (.", "labels": [], "entities": []}, {"text": "We use a common setup for all experiments based on that of ,, and.", "labels": [], "entities": []}, {"text": "In preliminary experiments, we confirmed that our models produced similar patterns of perplexities for the reimplemented word and character LSTM The Arabic and Hebrew dataset are unvocalized.", "labels": [], "entities": []}, {"text": "Japanese mixes Kanji, Katakana, Hiragana, and Latin characters (for foreign words).", "labels": [], "entities": []}, {"text": "Hence, a Japanese character can correspond to a character, syllable, or word.", "labels": [], "entities": []}, {"text": "The preprocessed dataset is already word-segmented.", "labels": [], "entities": []}, {"text": "Our implementation of these models can be found at https://github.com/claravania/subword-lstm-lm models of . Even following detailed discussion with Ling (p.c.), we were unable to reproduce their perplexities exactly-our English reimplementation gives lower perplexities; our Turkish higher-but we do reproduce their general result that character bi-LSTMs outperform word models.", "labels": [], "entities": []}, {"text": "We suspect that different preprocessing and the stochastic learning explains differences in perplexities.", "labels": [], "entities": []}, {"text": "Our final model with biLSTMs composition follows as it gives us the same perplexity results for our preliminary experiments on the Penn Treebank dataset), preprocessed by.", "labels": [], "entities": [{"text": "Penn Treebank dataset", "start_pos": 131, "end_pos": 152, "type": "DATASET", "confidence": 0.995979388554891}]}, {"text": "Our LSTM-LM uses two hidden layers with 200 hidden units and representation vectors for words, characters, and morphs all have dimension 200.", "labels": [], "entities": []}, {"text": "All parameters are initialized uniformly at random from -0.1 to 0.1, trained by stochastic gradient descent with mini-batch size of 32, time steps of 20, for 50 epochs.", "labels": [], "entities": []}, {"text": "To avoid overfitting, we apply dropout with probability 0.5 on the input-tohidden layer and all of the LSTM cells (including those in the bi-LSTM, if used).", "labels": [], "entities": []}, {"text": "For all models which do not use bi-LSTM composition, we start with a learning rate of 1.0 and decrease it by half if the validation perplexity does not decrease by 0.1 after 3 epochs.", "labels": [], "entities": []}, {"text": "For models with bi-LSTMs composition, we use a constant learning rate of 0.2 and stop training when validation perplexity does not improve after 3 epochs.", "labels": [], "entities": []}, {"text": "For the character CNN model, we use the same settings as the small model of.", "labels": [], "entities": []}, {"text": "To make our results comparable to , for each language we limit the output vocabulary to the most frequent 5,000 training words plus an unknown word token.", "labels": [], "entities": []}, {"text": "To learn to predict unknown words, we follow : in training, words that occur only once are stochastically replaced with the unknown token with probability 0.5.", "labels": [], "entities": []}, {"text": "To evaluate the models, we compute perplexity on the test data.", "labels": [], "entities": []}, {"text": "For these languages, character trigrams composed with bi-LSTMs outperformed all other models, particularly for Czech and Russian (up to 20%), which is unsurprising since both are morphologically richer than English.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Language model perplexities on test. The best model for each language is highlighted in bold  and the improvement of this model over the word-level model is shown in the final column.", "labels": [], "entities": []}, {"text": " Table 6: Perplexity results using hand-annotated  morphological analyses (cf.", "labels": [], "entities": []}, {"text": " Table 7: Perplexity results on the Czech develop- ment data, varying training data size. Perplexity  using ~1M tokens annotated data is 28.83.", "labels": [], "entities": [{"text": "Czech develop- ment data", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.8433910727500915}]}, {"text": " Table 8: Average perplexities of words that occur  after nouns and verbs. Frequent words occur more  than ten times in the training data; rare words oc- cur fewer times than this. The best perplexity is in  bold while the second best is underlined.", "labels": [], "entities": []}, {"text": " Table 9: Percentage of full reduplication on the  type and token level.", "labels": [], "entities": []}, {"text": " Table 10: Average perplexities of words that occur  after reduplicated words in the test set.", "labels": [], "entities": []}]}