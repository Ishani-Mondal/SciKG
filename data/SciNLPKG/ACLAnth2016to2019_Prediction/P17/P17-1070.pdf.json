{"title": [{"text": "A Nested Attention Neural Hybrid Model for Grammatical Error Correction", "labels": [], "entities": [{"text": "Nested Attention Neural Hybrid", "start_pos": 2, "end_pos": 32, "type": "TASK", "confidence": 0.8100855052471161}, {"text": "Grammatical Error Correction", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.8106536467870077}]}], "abstractContent": [{"text": "Grammatical error correction (GEC) systems strive to correct both global errors in word order and usage, and local errors in spelling and inflection.", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7969257632891337}]}, {"text": "Further developing upon recent work on neural machine translation, we propose anew hybrid neural model with nested attention layers for GEC.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.6825179755687714}, {"text": "GEC", "start_pos": 136, "end_pos": 139, "type": "TASK", "confidence": 0.5488234162330627}]}, {"text": "Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information , and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL-14 benchmark dataset.", "labels": [], "entities": [{"text": "CoNLL-14 benchmark dataset", "start_pos": 241, "end_pos": 267, "type": "DATASET", "confidence": 0.9529417157173157}]}, {"text": "Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (;.", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.7822180539369583}, {"text": "machine translation (MT)", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.7755674958229065}]}, {"text": "Such systems, which are based on phrasebased MT models that are typically trained on large sets of sentence-correction pairs, can correct global errors such as word order and usage and local errors in spelling and inflection.", "labels": [], "entities": []}, {"text": "The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms.", "labels": [], "entities": []}, {"text": "Recently, neural machine translation (NMT) systems have achieved substantial improvements in translation quality over phrase-based MT systems.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8269329071044922}]}, {"text": "Thus, there is growing interest in applying neural systems to GEC.", "labels": [], "entities": [{"text": "GEC", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.8547264933586121}]}, {"text": "In this paper, we significantly extend previous work, and explore new neural models to meet the unique challenges of GEC.", "labels": [], "entities": [{"text": "GEC", "start_pos": 117, "end_pos": 120, "type": "TASK", "confidence": 0.7542466521263123}]}, {"text": "The core component of most NMT systems is a sequence-to-sequence (S2S) model which encodes a sequence of source words into a vector and then generates a sequence of target words from the vector.", "labels": [], "entities": []}, {"text": "Unlike the phrase-based MT models, the S2S model can capture long-distance, or even global, word dependencies, which are crucial to correcting global grammatical errors and helping users achieve native speaker fluency ().", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.8121287822723389}]}, {"text": "Thus, the S2S model is expected to perform better on GEC than phrase-based models.", "labels": [], "entities": [{"text": "GEC", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.8551152348518372}]}, {"text": "However, as we will show in this paper, to achieve the best performance on GEC, we still need to extend the standard S2S model to address several task-specific challenges, which we will describe below.", "labels": [], "entities": [{"text": "GEC", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.9463983178138733}]}, {"text": "First, a GEC model needs to deal with an extremely large vocabulary that consists of a large number of words and their (mis)spelling variations.", "labels": [], "entities": [{"text": "GEC", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9072238206863403}]}, {"text": "Second, the GEC model needs to capture structure at different levels of granularity in order to correct errors of different types.", "labels": [], "entities": []}, {"text": "For example, while correcting spelling and local grammar errors requires only word-level or sub-word level information, e.g., violets \u2192 violates (spelling) or violate \u2192 violates (verb form), correcting errors in word order or usage requires global semantic relationships among phrases and words.", "labels": [], "entities": [{"text": "correcting spelling and local grammar errors", "start_pos": 19, "end_pos": 63, "type": "TASK", "confidence": 0.7720580597718557}]}, {"text": "Standard approaches in neural machine translation, also applied to grammatical error correction by, address the large vocabulary problem by restricting the vocabulary to a limited number of high-frequency words and re-sorting to standard word translation dictionaries to provide translations for the words that are out of the vocabulary (OOV).", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.7349569400151571}, {"text": "grammatical error correction", "start_pos": 67, "end_pos": 95, "type": "TASK", "confidence": 0.6828915874163309}]}, {"text": "However, this approach often fails to take into account the OOVs in context for making correction decisions, and does not generalize well to correcting words that are unseen in the parallel training data.", "labels": [], "entities": [{"text": "OOVs", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9398108720779419}]}, {"text": "An alternative approach, proposed by, applies a character-level sequence to sequence neural model.", "labels": [], "entities": []}, {"text": "Although the model eliminates the OOV issue, it cannot effectively leverage word-level information for GEC, even if it is used together with a separate word-based language model.", "labels": [], "entities": [{"text": "OOV", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.6757932901382446}]}, {"text": "Our solution to the challenges mentioned above is a novel, hybrid neural model with nested attention layers that infuse both word-level and character-level information.", "labels": [], "entities": []}, {"text": "The architecture of the model is illustrated in.", "labels": [], "entities": []}, {"text": "The word-level information is used for correcting global grammar and fluency errors while the character-level information is used for correcting local errors in spelling or inflected forms.", "labels": [], "entities": [{"text": "correcting global grammar and fluency errors", "start_pos": 39, "end_pos": 83, "type": "TASK", "confidence": 0.8320143322149912}]}, {"text": "Contextual information is crucial for GEC.", "labels": [], "entities": [{"text": "GEC", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.967886209487915}]}, {"text": "Using the proposed model, by combining embedding vectors and attention at both word and character levels, we model all contextual words, including OOVs, in a unified context vector representation.", "labels": [], "entities": []}, {"text": "In particular, as we will discuss in Section 5, the character-level attention layer captures most useful information for correcting local errors that involve small edits in orthography.", "labels": [], "entities": [{"text": "correcting local errors", "start_pos": 121, "end_pos": 144, "type": "TASK", "confidence": 0.8604055245717367}]}, {"text": "Our model differs substantially from the wordlevel S2S model of and the character-level S2S model of in the way we infuse information at both the word level and the character level.", "labels": [], "entities": []}, {"text": "We extend the wordcharacter hybrid model of, which was originally developed for machine translation, by introducing a character attention layer.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7455814778804779}]}, {"text": "This allows the model to learn substitution patterns at both the character level and the word level in an end-to-end fashion, using sentencecorrection pairs.", "labels": [], "entities": []}, {"text": "We validate the effectiveness of our model on the CoNLL-14 benchmark dataset (.", "labels": [], "entities": [{"text": "CoNLL-14 benchmark dataset", "start_pos": 50, "end_pos": 76, "type": "DATASET", "confidence": 0.9636240800221761}]}, {"text": "Results show that the proposed model outperforms all previous neural models for GEC, including the hybrid model of, which we apply to GEC for the first time.", "labels": [], "entities": [{"text": "GEC", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.6866731643676758}, {"text": "GEC", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.8654098510742188}]}, {"text": "When integrated with a large word-based n-gram language model, our GEC system achieves an F 0.5 of 45.15 on CoNLL-14, substantially exceeding the previ-", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.9933016002178192}, {"text": "CoNLL-14", "start_pos": 108, "end_pos": 116, "type": "DATASET", "confidence": 0.8900662064552307}]}], "datasetContent": [{"text": "We use standard publicly available datasets for training and evaluation.", "labels": [], "entities": []}, {"text": "One data source is the NUS Corpus of Learner English (NUCLE) (), which is provided as a training set for the CoNLL-13 and CoNLL-14 shared tasks.", "labels": [], "entities": [{"text": "NUS Corpus of Learner English (NUCLE)", "start_pos": 23, "end_pos": 60, "type": "DATASET", "confidence": 0.9537604600191116}, {"text": "CoNLL-13 and CoNLL-14 shared tasks", "start_pos": 109, "end_pos": 143, "type": "DATASET", "confidence": 0.7372633695602417}]}, {"text": "From the original corpus of size about 60K parallel sentences, we randomly selected close to 5K sentence pairs for use as a validation set, and 45K parallel sentences for use in training.", "labels": [], "entities": []}, {"text": "A second data source  is the Cambridge Learner Corpus (CLC), from which we extracted a substantially larger set of parallel sentences.", "labels": [], "entities": [{"text": "Cambridge Learner Corpus (CLC)", "start_pos": 29, "end_pos": 59, "type": "DATASET", "confidence": 0.9543651640415192}]}, {"text": "Finally, we used additional training examples from the Lang-8 Corpus of Learner English v1.0 ().", "labels": [], "entities": [{"text": "Lang-8 Corpus of Learner English v1.0", "start_pos": 55, "end_pos": 92, "type": "DATASET", "confidence": 0.9693005581696829}]}, {"text": "As Lang-8 data is crowd-sourced, we used heuristics to filter out noisy examples: we removed sentences longer than 100 words and sentence pairs where the correction was substantially shorter than the input text.", "labels": [], "entities": []}, {"text": "shows the number of sentence pairs from each source used for training.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the models on the standard sets from the CoNLL-14 shared task ().", "labels": [], "entities": [{"text": "CoNLL-14 shared task", "start_pos": 72, "end_pos": 92, "type": "DATASET", "confidence": 0.8193289041519165}]}, {"text": "We report final performance on the CoNLL-14 test set without alternatives, and analyze model performance on the CoNLL-13 development set (.", "labels": [], "entities": [{"text": "CoNLL-14 test set", "start_pos": 35, "end_pos": 52, "type": "DATASET", "confidence": 0.9742901921272278}, {"text": "CoNLL-13 development set", "start_pos": 112, "end_pos": 136, "type": "DATASET", "confidence": 0.9590600331624349}]}, {"text": "We use the development and validation sets for model selection.", "labels": [], "entities": [{"text": "model selection", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.713077038526535}]}, {"text": "The sizes of all datasets in number of sentences are shown in.", "labels": [], "entities": []}, {"text": "We report performance in F 0.5 -measure, as calculated by the m2scorer-the official implementation of the scoring metric in the shared task.", "labels": [], "entities": [{"text": "F 0.5 -measure", "start_pos": 25, "end_pos": 39, "type": "METRIC", "confidence": 0.9800728559494019}]}, {"text": "Given system outputs and gold-standard edits, m2scorer computes the F 0.5 measure of a set of system edits against a set of gold-standard edits.", "labels": [], "entities": [{"text": "F 0.5 measure", "start_pos": 68, "end_pos": 81, "type": "METRIC", "confidence": 0.9717523455619812}]}], "tableCaptions": [{"text": " Table 1: Overview of the datasets used.", "labels": [], "entities": [{"text": "Overview", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9185761213302612}]}, {"text": " Table 2: Training data by source.", "labels": [], "entities": [{"text": "Training data", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.7250640243291855}]}, {"text": " Table 3:  F 0.5 results on the CoNLL-13 and  CoNLL-14 test sets of main model architectures.", "labels": [], "entities": [{"text": "F", "start_pos": 11, "end_pos": 12, "type": "METRIC", "confidence": 0.9841386675834656}, {"text": "CoNLL-13 and  CoNLL-14 test sets", "start_pos": 32, "end_pos": 64, "type": "DATASET", "confidence": 0.8055872678756714}]}, {"text": " Table 5: F 0.5 results on the CoNLL-13 set of main  model architectures, on different segments of the  set according to whether the input contains OOVs.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9720512628555298}, {"text": "CoNLL-13 set", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.9237704575061798}]}, {"text": " Table 7: Precision, Recall and F 0.5 results on  CoNLL-13,on the \"small changes\" and \"large  changes\" portions of the OOV segment.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9994064569473267}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.998496413230896}, {"text": "F 0.5", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9888158738613129}, {"text": "CoNLL-13,on", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.8439199924468994}]}]}