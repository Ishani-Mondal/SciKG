{"title": [{"text": "Character Composition Model with Convolutional Neural Networks for Dependency Parsing on Morphologically Rich Languages", "labels": [], "entities": [{"text": "Character Composition", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7228816002607346}]}], "abstractContent": [{"text": "We present a transition-based dependency parser that uses a convolutional neural network to compose word representations from characters.", "labels": [], "entities": []}, {"text": "The character composition model shows great improvement over the word-lookup model, especially for parsing agglutinative languages.", "labels": [], "entities": [{"text": "character composition", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7372871935367584}, {"text": "parsing agglutinative languages", "start_pos": 99, "end_pos": 130, "type": "TASK", "confidence": 0.885397752126058}]}, {"text": "These improvements are even better than using pre-trained word embeddings from extra data.", "labels": [], "entities": []}, {"text": "On the SPMRL data sets, our system outperforms the previous best greedy parser (Ballesteros et al., 2015) by a margin of 3% on average.", "labels": [], "entities": [{"text": "SPMRL data sets", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.8545239567756653}]}], "introductionContent": [{"text": "As with many other NLP tasks, dependency parsing also suffers from the out-of-vocabulary (OOV) problem, and probably more than others since training data with syntactical annotation is usually scarce.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9009469449520111}]}, {"text": "This problem is particularly severe when the target is a morphologically rich language.", "labels": [], "entities": []}, {"text": "For example, in the SPMRL shared task data sets), 4 out of 9 treebanks contain more than 40% word types in the development set that are never seen in the training set.", "labels": [], "entities": [{"text": "SPMRL shared task data sets", "start_pos": 20, "end_pos": 47, "type": "DATASET", "confidence": 0.855834448337555}]}, {"text": "One way to tackle the OOV problem is to pretrain the word embeddings, e.g., with word2vec (, from a large set of unlabeled data.", "labels": [], "entities": [{"text": "OOV", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.814647376537323}]}, {"text": "This comes with two main advantages: (1) more word types, which means that the vocabulary is extended by the unlabeled data, so that some of the OOV words now have a learned representation; (2) more word tokens per type, which means that the syntactic and semantic similarities of the words are better modeled than only using the parser training data.", "labels": [], "entities": []}, {"text": "The parser is available at http://www.ims.", "labels": [], "entities": []}, {"text": "uni-stuttgart.de/institut/mitarbeiter/ xiangyu/index.en.html Pre-trained word embeddings can alleviate the OOV problem by expanding the vocabulary, but it does not model the morphological information.", "labels": [], "entities": []}, {"text": "Instead of looking up word embeddings, many researchers propose to compose the word representation from characters for various tasks, e.g., part-of-speech tagging (dos, named entity recognition (dos, language modeling (, machine translation).", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.6994323283433914}, {"text": "named entity recognition", "start_pos": 169, "end_pos": 193, "type": "TASK", "confidence": 0.6815681060155233}, {"text": "language modeling", "start_pos": 200, "end_pos": 217, "type": "TASK", "confidence": 0.6979065239429474}, {"text": "machine translation", "start_pos": 221, "end_pos": 240, "type": "TASK", "confidence": 0.7383589744567871}]}, {"text": "In particular, use a bidirectional long short-term memory (LSTM) character model for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.8153909742832184}]}, {"text": "present a convolutional neural network (CNN) character model for language modeling, but make no comparison among the character models, and state that \"it remains open as to which character composition model (i.e., LSTM or CNN) performs better\".", "labels": [], "entities": [{"text": "language modeling", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7142774015665054}]}, {"text": "We propose to apply the CNN model by in a greedy transition-based dependency parser with feed-forward neural networks.", "labels": [], "entities": []}, {"text": "This model requires no extra unlabeled data but performs better than using pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "Furthermore, it can be combined with word embeddings from the lookup table since they capture different aspects of word similarities.", "labels": [], "entities": []}, {"text": "Experimental results show that the CNN model works especially well on agglutinative languages, where the OOV rates are high.", "labels": [], "entities": [{"text": "OOV", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9775406718254089}]}, {"text": "On other morphologically rich languages, the CNN model also performs at least as good as the word-lookup model.", "labels": [], "entities": []}, {"text": "Furthermore, our CNN model outperforms both the original and our re-implementation of the bidirectional LSTM model by by a large margin.", "labels": [], "entities": []}, {"text": "It provides empirical evidence to the aforementioned open question, suggesting that the CNN is the better character composition model for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.8647358119487762}]}], "datasetContent": [{"text": "We conduct our experiments on the treebanks from the SPMRL 2014 shared task (), which includes 9 morphologically rich languages: Arabic, Basque, French, German, Hebrew, Hungarian, Korean, Polish, and Swedish.", "labels": [], "entities": [{"text": "SPMRL 2014 shared task", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.7981547564268112}]}, {"text": "All the treebanks are split into training, development, and test sets by the shared task organizers.", "labels": [], "entities": []}, {"text": "We use the fine-grained predicted POS tags provided by the organizers, and evaluate the labeled attachment scores (LAS) including punctuation.", "labels": [], "entities": [{"text": "labeled attachment scores (LAS)", "start_pos": 88, "end_pos": 119, "type": "METRIC", "confidence": 0.8323754568894705}]}, {"text": "We experiment with the CNN-based character composition model (CNN) along with several baselines.", "labels": [], "entities": [{"text": "CNN-based character composition", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.5259636143843333}]}, {"text": "The first baseline (WORD) uses the wordlookup model described in Section 2.1 with randomly initialized word embeddings.", "labels": [], "entities": []}, {"text": "The second baseline (W2V) uses pre-trained word embeddings by) with the CBOW model and default parameters on the unlabeled texts from the shared task organizers.", "labels": [], "entities": []}, {"text": "The third baseline (LSTM) uses a bidirectional LSTM as the character composition model following.", "labels": [], "entities": []}, {"text": "Appendix C lists the hyper-parameters of all the models.", "labels": [], "entities": []}, {"text": "Further analysis suggests that combining the character composition models with word-lookup models could be beneficial since they capture different aspects of word similarities (orthographic vs. syntactic/semantic).", "labels": [], "entities": []}, {"text": "We therefore experiment with four combined models in two groups: (1) randomly initialized word embeddings (LSTM+WORD vs. CNN+WORD), and (2) pre-trained word embeddings (LSTM+W2V vs. CNN+W2V).", "labels": [], "entities": []}, {"text": "The experimental results are shown in, with Int denoting internal comparisons (with three groups) and Ext denoting external comparisons, the highest LAS in each group is marked in boldface.", "labels": [], "entities": [{"text": "Int", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.908471941947937}, {"text": "LAS", "start_pos": 149, "end_pos": 152, "type": "METRIC", "confidence": 0.9945616126060486}]}], "tableCaptions": [{"text": " Table 1: LAS on the test sets, the best LAS in each group is marked in bold face.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9937214851379395}, {"text": "LAS", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9852654337882996}]}, {"text": " Table 2: LAS improvements by CNN and LSTM in the IV and OOV cases on the development sets.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9654471278190613}, {"text": "CNN", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.8751978874206543}]}]}