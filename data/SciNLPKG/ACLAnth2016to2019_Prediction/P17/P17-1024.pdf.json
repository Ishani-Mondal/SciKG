{"title": [{"text": "FOIL it! Find One mismatch between Image and Language caption", "labels": [], "entities": [{"text": "FOIL", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9716373682022095}, {"text": "Image and Language caption", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.5297698751091957}]}], "abstractContent": [{"text": "In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities.", "labels": [], "entities": []}, {"text": "To this end, we propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images with both correct and 'foil' captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake ('foil word').", "labels": [], "entities": [{"text": "MS-COCO dataset", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.9396756589412689}, {"text": "FOIL-COCO", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9928396940231323}]}, {"text": "We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption classification (correct vs. foil); b) foil word detection; c) foil word correction.", "labels": [], "entities": [{"text": "caption classification", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.878129631280899}, {"text": "foil word detection", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.6939259270826975}, {"text": "foil word correction", "start_pos": 173, "end_pos": 193, "type": "TASK", "confidence": 0.807331403096517}]}, {"text": "Humans , in contrast, have near-perfect performance on those tasks.", "labels": [], "entities": []}, {"text": "We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image.", "labels": [], "entities": [{"text": "FOIL-COCO", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.6194899082183838}]}], "introductionContent": [{"text": "Most human language understanding is grounded in perception.", "labels": [], "entities": [{"text": "human language understanding", "start_pos": 5, "end_pos": 33, "type": "TASK", "confidence": 0.7742557525634766}]}, {"text": "There is thus growing interest in combining information from language and vision in the NLP and AI communities.", "labels": [], "entities": []}, {"text": "So far, the primary testbeds of Language and Vision (LaVi) models have been 'Visual Question Answering' (VQA) (e.g.;;; ;) and 'Image Captioning' (IC) (e.g.;;;;;).", "labels": [], "entities": [{"text": "Visual Question Answering' (VQA)", "start_pos": 77, "end_pos": 109, "type": "TASK", "confidence": 0.7160293587616512}, {"text": "Image Captioning' (IC)", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.8325569579998652}]}, {"text": "Whilst some models have seemed extremely successful on those tasks, it remains unclear how the reported results should be interpreted and what those: Is the caption corrector foil (T1)?", "labels": [], "entities": [{"text": "caption corrector foil (T1)", "start_pos": 157, "end_pos": 184, "type": "METRIC", "confidence": 0.8144187281529108}]}, {"text": "If it is foil, where is the mistake (T2) and which is the word to correct the foil one (T3)?", "labels": [], "entities": [{"text": "foil", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9841618537902832}, {"text": "mistake (T2)", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.7905505746603012}]}, {"text": "There is an emerging feeling in the community that the VQA task should be revisited, especially as many current dataset can be handled by 'blind' models which use language input only, or by simple concatenation of language and vision features ().", "labels": [], "entities": [{"text": "VQA task", "start_pos": 55, "end_pos": 63, "type": "TASK", "confidence": 0.5381155610084534}]}, {"text": "In IC too, showed that, contrarily to what prior research had suggested, the task is far from been solved, since IC models are notable to distinguish between a correct and incorrect caption.", "labels": [], "entities": [{"text": "IC", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.7396177053451538}]}, {"text": "Such results indicate that in current datasets, language provides priors that make LaVi models successful without truly understanding and integrating language and vision.", "labels": [], "entities": []}, {"text": "But problems do not stop at biases.", "labels": [], "entities": []}, {"text": "also point out that current data 'conflate multiple sources of error, making it hard to pinpoint model weaknesses', thus highlighting the need for diagnostic datasets.", "labels": [], "entities": []}, {"text": "Thirdly, existing IC evaluation metrics are sensitive to n-gram overlap and there is a need for measures that better simulate human judgments.", "labels": [], "entities": [{"text": "IC evaluation", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.8919378817081451}]}, {"text": "Our paper tackles the identified issues by proposing an automatic method for creating a large dataset of real images with minimal language bias and some diagnostic abilities.", "labels": [], "entities": []}, {"text": "Our dataset, FOIL (Find One mismatch between Image and Language caption), consists of images associated with incorrect captions.", "labels": [], "entities": [{"text": "FOIL", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9935913681983948}, {"text": "Find One mismatch between Image and Language caption)", "start_pos": 19, "end_pos": 72, "type": "TASK", "confidence": 0.5464917851818932}]}, {"text": "The captions are produced by introducing one single error (or 'foil') per caption in existing, human-annotated data).", "labels": [], "entities": [{"text": "foil", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9133992195129395}]}, {"text": "This process results in a challenging error-detection/correction setting (because the caption is 'nearly' correct).", "labels": [], "entities": []}, {"text": "It also provides us with aground truth (we know where the error is) that can be used to objectively measure the performance of current models.", "labels": [], "entities": []}, {"text": "We propose three tasks based on widely accepted evaluation measures: we test the ability of the system to a) compute whether a caption is compatible with the image (T1); b) when it is incompatible, highlight the mismatch in the caption (T2); c) correct the mistake by replacing the foil word (T3).", "labels": [], "entities": []}, {"text": "The dataset presented in this paper (Section 3) is built on top of MS-COCO (, and contains 297,268 datapoints and 97,847 images.", "labels": [], "entities": [{"text": "MS-COCO", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9474769234657288}]}, {"text": "We will refer to it as FOIL-COCO.", "labels": [], "entities": [{"text": "FOIL-COCO", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9738929867744446}]}, {"text": "We evaluate two state-of-the-art VQA models: the popular one by, and the attention-based model by, and one popular IC model by ( . We show that those models perform close to chance level, while humans can perform the tasks accurately (Section 4).", "labels": [], "entities": []}, {"text": "Section 5 provides an analysis of our results, allowing us to diagnose three failures of LaVi models.", "labels": [], "entities": []}, {"text": "First, their coarse representations of language and visual input do not encode suitably structured information to spot mismatches between an utterance and the corresponding scene (tested by T1).", "labels": [], "entities": []}, {"text": "Second, their language representation is not finegrained enough to identify the part of an utterance that causes a mismatch with the image as it is (T2).", "labels": [], "entities": []}, {"text": "Third, their visual representation is also too poor to spot and name the visual area that corresponds to a captioning error (T3).", "labels": [], "entities": [{"text": "captioning error (T3)", "start_pos": 107, "end_pos": 128, "type": "METRIC", "confidence": 0.5871553182601928}]}], "datasetContent": [{"text": "In this section, we describe how we automatically generate FOIL-COCO datapoints, i.e. image, original and foil caption triples.", "labels": [], "entities": [{"text": "FOIL-COCO datapoints", "start_pos": 59, "end_pos": 79, "type": "DATASET", "confidence": 0.7434267699718475}]}, {"text": "We used the training and validation Microsoft's Common Objects in Context (MS-COCO) dataset () (2014 version) as our starting point.", "labels": [], "entities": [{"text": "MS-COCO) dataset", "start_pos": 75, "end_pos": 91, "type": "DATASET", "confidence": 0.5927751759688059}]}, {"text": "In MS-COCO, each image is described by at least five descriptions written by humans via Amazon Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 88, "end_pos": 116, "type": "DATASET", "confidence": 0.9085741738478342}]}, {"text": "The images contains 91 common object categories (e.g. dog, elephant, bird, . .", "labels": [], "entities": []}, {"text": ". and car, bicycle, airplane, . .", "labels": [], "entities": []}, {"text": "), from 11 supercategories (Animal, Vehicle, resp.), with 82 of them having more than 5K labeled instances.", "labels": [], "entities": []}, {"text": "In total there are 123,287 images with captions (82,783 for training and 40,504 for validation).", "labels": [], "entities": []}, {"text": "Our data generation process consists of four The MS-COCO test set is not available for download.", "labels": [], "entities": [{"text": "MS-COCO test set", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.9414171576499939}]}, {"text": "main steps, as described below.", "labels": [], "entities": []}, {"text": "The last two steps are illustrated in.", "labels": [], "entities": []}, {"text": "1. Generation of replacement word pairs We want to replace one noun in the original caption (the target) with an incorrect but similar word (the foil).", "labels": [], "entities": [{"text": "Generation of replacement word pairs", "start_pos": 3, "end_pos": 39, "type": "TASK", "confidence": 0.7507708191871643}]}, {"text": "To do this, we take the labels of MS-COCO categories, and we pair together words belonging to the same supercategory (e.g., bicycle::motorcycle, bicycle::car, bird::dog).", "labels": [], "entities": []}, {"text": "We use as our vocabulary 73 out of the 91 MS-COCO categories, leaving out those categories that are multiword expressions (e.g. traffic light).", "labels": [], "entities": []}, {"text": "We thus obtain 472 target::foil pairs.", "labels": [], "entities": []}, {"text": "2. Splitting of replacement pairs into training and testing To avoid the models learning trivial correlations due to replacement frequency, we randomly split, within each supercategory, the candidate target::foil pairs which are used to generate the captions of the training vs. test sets.", "labels": [], "entities": []}, {"text": "We obtain 256 pairs, built out of 72 target and 70 foil words, for the training set, and 216 pairs, containing 73 target and 71 foil words, for the test set.", "labels": [], "entities": []}, {"text": "3. Generation of foil captions We would like to generate foil captions by replacing only target words which refer to visually salient objects.", "labels": [], "entities": [{"text": "Generation of foil captions", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.7042683660984039}]}, {"text": "To this end, given an image, we replace only those target words that occur in more than one MS-COCO caption associated with that image.", "labels": [], "entities": []}, {"text": "Moreover, we want to use foils which are not visually present, i.e. that refer to visual content not present in the image.", "labels": [], "entities": []}, {"text": "Hence, given an image, we only replace a word with foils that are not among the labels (objects) annotated in MS-COCO for that image.", "labels": [], "entities": [{"text": "MS-COCO", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.927251398563385}]}, {"text": "We use the images from the MS-COCO training and validation sets to generate our training and test sets, respectively.", "labels": [], "entities": [{"text": "MS-COCO training and validation sets", "start_pos": 27, "end_pos": 63, "type": "DATASET", "confidence": 0.7997924208641052}]}, {"text": "We obtain 2,229,899 for training and 1,097,012 captions for testing.", "labels": [], "entities": [{"text": "captions", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9346997737884521}]}, {"text": "4. Mining the hardest foil caption for each image To eliminate possible visual-language dataset bias, out of all foil captions generated in step 3, we select only the hardest one.", "labels": [], "entities": []}, {"text": "For this purpose, we need to model the visual-language bias of the dataset.", "labels": [], "entities": []}, {"text": "To this end, we use Neuraltalk 3), one of the stateof-the-art image captioning systems, pre-trained on MS-COCO.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.7745896577835083}, {"text": "MS-COCO", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9369605779647827}]}, {"text": "Neuraltalk is based on an LSTM which takes as input an image and generates a sentence describing its content.", "labels": [], "entities": []}, {"text": "We obtain a neural network N that implicitly represents the visuallanguage bias through its weights.", "labels": [], "entities": []}, {"text": "We use N to approximate the conditional probability of a caption C given a dataset T and and an image I (P (C|I, T )).", "labels": [], "entities": []}, {"text": "This is obtained by simply using the loss l(C, N (I)) i.e., the error obtained by comparing the pseudo-ground truth C with the sentence predicted by N : P (C|I, T ) = 1 \u2212 l(C, N (I)) (we refer to) for more details on how l() is computed).", "labels": [], "entities": []}, {"text": "P (C|I, T ) is used to select the hardest foil among all the possible foil captions, i.e. the one with the highest probability according to the dataset bias learned by N . Through this process, we obtain 197,788 and 99,480 original::foil caption pairs for the training and test sets, respectively.", "labels": [], "entities": []}, {"text": "None of the target::foil word pairs are filtered out by this mining process.", "labels": [], "entities": []}, {"text": "The final FOIL-COCO dataset consists of 297,268 datapoints in training and 99,480 in test set).", "labels": [], "entities": [{"text": "FOIL-COCO dataset", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.8168883919715881}]}, {"text": "All the 11 MS-COCO supercategories are represented in our dataset and contain 73 categories from the 91 MS-COCO ones (4.8 categories per supercategory on average.)", "labels": [], "entities": []}, {"text": "Further details are reported in.", "labels": [], "entities": []}, {"text": "We conduct three tasks, as presented below: Task 1 (T1): Correct vs. foil classification Given an image and a caption, the model is asked to mark whether the caption is corrector wrong.", "labels": [], "entities": [{"text": "foil classification", "start_pos": 69, "end_pos": 88, "type": "METRIC", "confidence": 0.9028418660163879}]}, {"text": "The aim is to understand whether LaVi models can spot mismatches between their coarse representations of language and visual input.", "labels": [], "entities": []}, {"text": "Task 2 (T2): Foil word detection Given an image and a foil caption, the model has to detect the foil word.", "labels": [], "entities": [{"text": "Foil word detection", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7899229923884074}]}, {"text": "The aim is to evaluate the understanding of the system at the word level.", "labels": [], "entities": []}, {"text": "In order to systematically check the system's performance with different prior information, we test two different set-: The main aspects of the foil caption generation process.", "labels": [], "entities": [{"text": "foil caption generation", "start_pos": 144, "end_pos": 167, "type": "TASK", "confidence": 0.845765233039856}]}, {"text": "Left column: some of the original COCO captions associated with an image.", "labels": [], "entities": [{"text": "COCO captions", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.6181559264659882}]}, {"text": "In bold we highlight one of the target words (bicycle), chosen because it is mentioned by more than one annotator.", "labels": [], "entities": []}, {"text": "Middle column: For each original caption and each chosen target word, different foil captions are generated by replacing the target word with all possible candidate foil replacements.", "labels": [], "entities": []}, {"text": "Right column: A single caption is selected amongst all foil candidates.", "labels": [], "entities": []}, {"text": "We select the 'hardest' caption, according to Neuraltalk model, trained using only the original captions.", "labels": [], "entities": []}, {"text": "tings: the foil has to be selected amongst (a) only the nouns or (b) all content words in the caption.", "labels": [], "entities": []}, {"text": "Task 3 (T3): Foil word correction Given an image, a foil caption and the foil word, the model has to detect the foil and provide its correction.", "labels": [], "entities": [{"text": "Foil word correction", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.6805288195610046}]}, {"text": "The aim is to check whether the system's visual representation is fine-grained enough to be able to extract the information necessary to correct the error.", "labels": [], "entities": []}, {"text": "For efficiency reasons, we operationalise this task by asking models to select a correction from the set of target words, rather than the whole dataset vocabulary (viz. more than 10K words).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Composition of FOIL-COCO.", "labels": [], "entities": [{"text": "FOIL-COCO", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.5659496784210205}]}, {"text": " Table 2: T1: Accuracy for the classification task,  relatively to all image-caption pairs (overall) and  by type of caption (correct vs. foil); T2: Accu- racy for the foil word detection task, when the foil  is known to be among the nouns only or when it  is known to be among all the content words; T3:  Accuracy for the foil word correction task when  the correct word has to be chosen among any of  the target words.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9988264441490173}, {"text": "Accu- racy", "start_pos": 149, "end_pos": 159, "type": "METRIC", "confidence": 0.9580834905306498}, {"text": "foil word detection task", "start_pos": 168, "end_pos": 192, "type": "TASK", "confidence": 0.6590224429965019}, {"text": "Accuracy", "start_pos": 306, "end_pos": 314, "type": "METRIC", "confidence": 0.998310923576355}, {"text": "foil word correction task", "start_pos": 323, "end_pos": 348, "type": "TASK", "confidence": 0.7228676229715347}]}, {"text": " Table 3: Classification Accuracy of foil captions by Super Categories (T1). The No. of the objects and  the No. of foil captions refer to the test set. The training set has a similar distribution.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9564725756645203}, {"text": "foil captions", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.6052121073007584}]}, {"text": " Table 4: Easiest and hardest target::foil pairs: T1 (caption classification) and T2 (foil word detection).", "labels": [], "entities": [{"text": "foil", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9816676378250122}, {"text": "caption classification", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7599559128284454}, {"text": "foil word detection", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.6121051609516144}]}]}