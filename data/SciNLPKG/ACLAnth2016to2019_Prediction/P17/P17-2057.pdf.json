{"title": [{"text": "Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks", "labels": [], "entities": [{"text": "Question Answering on Knowledge Bases", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8206268191337586}]}], "abstractContent": [{"text": "Existing question answering methods infer answers either from a knowledge base or from raw text.", "labels": [], "entities": [{"text": "question answering", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7444235384464264}]}, {"text": "While knowledge base (KB) methods are good at answering composi-tional questions, their performance is often affected by the incompleteness of the KB.", "labels": [], "entities": []}, {"text": "Au contraire, web text contains millions of facts that are absent in the KB, however in an unstructured form.", "labels": [], "entities": []}, {"text": "Universal schema can support reasoning on the union of both structured KBs and unstructured text by aligning them in a common embedded space.", "labels": [], "entities": []}, {"text": "In this paper we extend universal schema to natural language question answering, employing memory networks to attend to the large body of facts in the combination of text and KB.", "labels": [], "entities": [{"text": "natural language question answering", "start_pos": 44, "end_pos": 79, "type": "TASK", "confidence": 0.5980585217475891}]}, {"text": "Our models can be trained in an end-to-end fashion on question-answer pairs.", "labels": [], "entities": []}, {"text": "Evaluation results on SPADES fill-in-the-blank question answering dataset show that exploiting universal schema for question answering is better than using either a KB or text alone.", "labels": [], "entities": [{"text": "SPADES fill-in-the-blank question answering", "start_pos": 22, "end_pos": 65, "type": "TASK", "confidence": 0.8302141726016998}, {"text": "question answering", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.7626418173313141}]}, {"text": "This model also outperforms the current state-of-the-art by 8.5 F 1 points.", "labels": [], "entities": [{"text": "F 1", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9340201020240784}]}], "introductionContent": [{"text": "Question Answering (QA) has been a longstanding goal of natural language processing.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9064728617668152}, {"text": "natural language processing", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.6537893613179525}]}, {"text": "Two main paradigms evolved in solving this problem: 1) answering questions on a knowledge base; and 2) answering questions using text.", "labels": [], "entities": [{"text": "answering questions using text", "start_pos": 103, "end_pos": 133, "type": "TASK", "confidence": 0.8454468101263046}]}, {"text": "Knowledge bases (KB) contains facts expressed in a fixed schema, facilitating compositional reasoning.", "labels": [], "entities": []}, {"text": "These attracted research ever since the early days of computer science, e.g., BASEBALL.", "labels": [], "entities": [{"text": "BASEBALL", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.6980393528938293}]}, {"text": "This problem has matured into learning semantic parsers from parallel question and logical form pairs (, to recent scaling of methods to work on very large KBs like Freebase using question and answer pairs).", "labels": [], "entities": [{"text": "learning semantic parsers from parallel question and logical form pairs", "start_pos": 30, "end_pos": 101, "type": "TASK", "confidence": 0.7728517115116119}]}, {"text": "However, a major drawback of this paradigm is that KBs are highly incomplete.", "labels": [], "entities": []}, {"text": "It is also an open question whether KB relational structure is expressive enough to represent world knowledge ( The paradigm of exploiting text for questions started in the early 1990s.", "labels": [], "entities": []}, {"text": "With the advent of web, access to text resources became abundant and cheap.", "labels": [], "entities": []}, {"text": "Initiatives like TREC QA competitions helped popularizing this paradigm ().", "labels": [], "entities": [{"text": "TREC QA", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.5402088612318039}]}, {"text": "With the recent advances in deep learning and availability of large public datasets, there has been an explosion of research in a very short time (.", "labels": [], "entities": []}, {"text": "Still, text representation is unstructured and does not allow the compositional reasoning which structured KB supports.", "labels": [], "entities": [{"text": "text representation", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7026175856590271}]}, {"text": "An important but under-explored QA paradigm is where KB and text are exploited together.", "labels": [], "entities": []}, {"text": "Such combination is attractive because text contains millions of facts not present in KB, and a KB's generative capacity represents infinite number of facts that are never seen in text.", "labels": [], "entities": []}, {"text": "However QA inference on this combination is challenging due to the structural non-uniformity of KB and text.", "labels": [], "entities": [{"text": "QA inference", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.8913059830665588}]}, {"text": "Distant supervision methods () address this problem partially by means of aligning text patterns with KB.", "labels": [], "entities": []}, {"text": "But the rich and ambiguous nature of language allows a fact to be expressed in many different forms which these models fail to capture.", "labels": [], "entities": []}, {"text": "Universal schema () avoids the alignment problem by jointly embedding KB facts k b :h a s _ cit y k b :h a s _ co mp an y k b :p re s id en t_ of a rg 2 is the fi rs tn on -w hit e p re s id en to fa rg  The contributions of the paper are as follows (a) We show that universal schema representation is a better knowledge source for QA than either KB or text alone, (b) On the SPADES dataset, containing real world fill-in-the-blank questions, we outperform state-of-the-art semantic parsing baseline, with 8.5 F 1 points.", "labels": [], "entities": [{"text": "SPADES dataset", "start_pos": 376, "end_pos": 390, "type": "DATASET", "confidence": 0.8556047677993774}, {"text": "semantic parsing", "start_pos": 474, "end_pos": 490, "type": "TASK", "confidence": 0.7598086595535278}, {"text": "F 1", "start_pos": 510, "end_pos": 513, "type": "METRIC", "confidence": 0.9469898045063019}]}, {"text": "(c) Our analysis shows how individual data sources help fill the weakness of the other, thereby improving overall performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use Freebase () as our KB, and ClueWeb () as our text source to build universal schema.", "labels": [], "entities": []}, {"text": "For evaluation, literature offers two options: 1) datasets for text-based question answering tasks such as answer sentence selection and reading comprehension; and 2) datasets for KB question answering.", "labels": [], "entities": [{"text": "text-based question answering tasks", "start_pos": 63, "end_pos": 98, "type": "TASK", "confidence": 0.7217791453003883}, {"text": "answer sentence selection", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.7385667165120443}, {"text": "KB question answering", "start_pos": 180, "end_pos": 201, "type": "TASK", "confidence": 0.715250551700592}]}, {"text": "Although the text-based question answering datasets are large in size, e.g.,) has over 100k questions, answers to these are often not entities but rather sentences which are not the focus of our work.", "labels": [], "entities": [{"text": "question answering", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.6993784010410309}]}, {"text": "Moreover these texts may not contain Freebase entities at all, making these skewed heavily towards text.", "labels": [], "entities": []}, {"text": "Coming to the alternative option,) is widely used for QA on Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9119186997413635}]}, {"text": "This dataset is curated such that all questions can be answered on Freebase alone.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.9565812349319458}]}, {"text": "But since our goal is to explore the impact of universal schema, testing on a dataset completely answerable on a KB is not ideal.", "labels": [], "entities": []}, {"text": "WikiMovies dataset () also has similar properties.", "labels": [], "entities": [{"text": "WikiMovies dataset", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9504677355289459}]}, {"text": "ours, however this is not publicly released during the submission time.", "labels": [], "entities": []}, {"text": "Instead, we use SPADES ( as our evaluation data which contains fill-in-the-blank cloze-styled questions created from ClueWeb.", "labels": [], "entities": [{"text": "SPADES", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.7651210427284241}]}, {"text": "This dataset is ideal to test our hypothesis for following reasons: 1) it is large with 93K sentences and 1.8M entities; and 2) since these are collected from Web, most sentences are natural.", "labels": [], "entities": []}, {"text": "A limitation of this dataset is that it contains only the sentences that have entities connected by at least one relation in Freebase, making it skewed towards Freebase as we will see.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 125, "end_pos": 133, "type": "DATASET", "confidence": 0.9633287787437439}, {"text": "Freebase", "start_pos": 160, "end_pos": 168, "type": "DATASET", "confidence": 0.9874376654624939}]}, {"text": "We use the standard train, dev and test splits for our experiments.", "labels": [], "entities": []}, {"text": "For text part of universal schema, we use the sentences present in the training set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: QA results on SPADES.", "labels": [], "entities": [{"text": "QA", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.5654414892196655}, {"text": "SPADES", "start_pos": 24, "end_pos": 30, "type": "TASK", "confidence": 0.7936177849769592}]}, {"text": " Table 3: Detailed results on SPADES.", "labels": [], "entities": [{"text": "SPADES", "start_pos": 30, "end_pos": 36, "type": "TASK", "confidence": 0.6484390497207642}]}]}