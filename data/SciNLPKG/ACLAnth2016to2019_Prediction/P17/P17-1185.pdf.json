{"title": [{"text": "Riemannian Optimization for Skip-Gram Negative Sampling", "labels": [], "entities": [{"text": "Sampling", "start_pos": 47, "end_pos": 55, "type": "TASK", "confidence": 0.6858288049697876}]}], "abstractContent": [{"text": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent.", "labels": [], "entities": [{"text": "Skip-Gram Negative Sampling (SGNS) word embedding", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7510665878653526}]}, {"text": "However, the optimization of SGNS objective can be viewed as a problem of searching fora good matrix with the low-rank constraint.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.9619444012641907}]}, {"text": "The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 115, "end_pos": 119, "type": "TASK", "confidence": 0.9301469326019287}]}, {"text": "In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors , such as the original method to train SGNS and SVD over SPPMI matrix.", "labels": [], "entities": []}], "introductionContent": [{"text": "where matrices U i \u2208 R n\u00d7d and V i \u2208 R m\u00d7d have d orthonormal columns and Si \u2208 R d\u00d7d . Then we need to perform two QR-decompositions to retract point X i + F (X i ) back to the manifold: In this way, we always keep the solution X i+1 = U i+1 S i+1 V i+1 on the manifold Md and in the form.", "labels": [], "entities": []}, {"text": "What is important, we only need to compute F (X i ), so the gradients with respect to U , Sand V are never computed explicitly, thus avoiding the subtle case where S is close to singular (so-called singular (critical) point on the manifold).", "labels": [], "entities": []}, {"text": "Indeed, the gradient with respect to U (while keeping the orthogonality constraints) can be written ( as: which means that the gradient will be large if S is close to singular.", "labels": [], "entities": []}, {"text": "The projector-splitting scheme is free from this problem.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate word embeddings via the word similarity task.", "labels": [], "entities": []}, {"text": "We use the following popular datasets for this purpose: \"wordsim-353\"); 3 datasets), \"simlex-999\" ( and \"men\" ().", "labels": [], "entities": []}, {"text": "Original \"wordsim-353\" dataset is a mixture of the word pairs for both word similarity and word relatedness tasks.", "labels": [], "entities": []}, {"text": "This dataset was split () into two intersecting parts: \"wordsim-sim\" (\"ws-sim\" in the tables) and \"wordsim-rel\" (\"wsrel\" in the tables) to separate the words from different tasks.", "labels": [], "entities": []}, {"text": "In our experiments, we use both of them on a par with the full version of \"wordsim-353\" (\"ws-full\" in the tables).", "labels": [], "entities": []}, {"text": "Each dataset contains word pairs together with assessor-assigned similarity scores for each pair.", "labels": [], "entities": []}, {"text": "As a quality measure, we use Spearman's correlation between these human ratings and cosine similarities for each pair.", "labels": [], "entities": []}, {"text": "We call this quality metric linguistic in our paper.", "labels": [], "entities": []}, {"text": "First of all, we compare the value of SGNS objective obtained by the methods.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.831012487411499}]}, {"text": "The comparison is demonstrated in.", "labels": [], "entities": []}, {"text": "We see that SGD-SGNS and SVD-SPPMI methods provide quite similar results, however, the proposed method obtains significantly better  SGNS values, what proves the feasibility of using Riemannian optimization framework in SGNS optimization problem.", "labels": [], "entities": [{"text": "SGNS optimization", "start_pos": 220, "end_pos": 237, "type": "TASK", "confidence": 0.9274048805236816}]}, {"text": "It is interesting to note that SVD-SPPMI method, which does not optimize SGNS objective directly, obtains better results than SGD-SGNS method, which aims at optimizing SGNS.", "labels": [], "entities": []}, {"text": "This fact additionally confirms the idea described in Section 2.2.2 that the independent optimization over parameters W and C may decrease the performance.", "labels": [], "entities": []}, {"text": "However, the target performance measure of embedding models is the correlation between semantic similarity and human assessment (Section 4.2).", "labels": [], "entities": []}, {"text": "presents the comparison of the methods in terms of it.", "labels": [], "entities": []}, {"text": "We see that our method outperforms the competitors on all datasets except for \"men\" dataset where it obtains slightly worse results.", "labels": [], "entities": []}, {"text": "Moreover, it is important that the higher dimension entails higher performance gain of our method in comparison to the competitors.", "labels": [], "entities": []}, {"text": "To understand how our model improves or degrades the performance in comparison to the baseline, we found several words, whose neighbors in terms of cosine distance change significantly.", "labels": [], "entities": []}, {"text": "Table 3 demonstrates neighbors of the words \"five\", \"he\" and \"main\" for both SVD-SPPMI and RO-SGNS models.", "labels": [], "entities": []}, {"text": "A neighbor is marked bold if we suppose that it has similar semantic meaning to the: Examples of the semantic neighbors from 11th to 20th obtained for the word \"usa\" by all three methods.", "labels": [], "entities": []}, {"text": "Top-10 neighbors for all three methods are exact names of states.", "labels": [], "entities": []}, {"text": "First of all, we notice that our model produces much better neighbors of the words describing digits or numbers (see word \"five\" as an example).", "labels": [], "entities": []}, {"text": "Similar situation happens for many other words, e.g. in case of \"main\" -the nearest neighbors contain 4 similar words for our model instead of 2 in case of SVD-SPPMI.", "labels": [], "entities": []}, {"text": "The neighbourhood of \"he\" contains less semantically similar words in case of our model.", "labels": [], "entities": []}, {"text": "However, it filters out irrelevant words, such as \"promptly\" and \"dumbledore\".", "labels": [], "entities": []}, {"text": "contains the nearest words to the word \"usa\" from 11th to 20th.", "labels": [], "entities": []}, {"text": "We marked names of USA states bold and did not represent top-10 nearest words as they are exactly names of states for all three models.", "labels": [], "entities": []}, {"text": "Some non-bold words are arguably relevant as they present large USA cities (\"akron\", \"burbank\", \"madison\") or geographical regions of several states (\"midwest\", \"northeast\", \"southwest\"), but there are also some completely irrelevant words (\"uk\", \"cities\", \"places\") presented by first two models.", "labels": [], "entities": []}, {"text": "Our experiments show that the optimal number of iterations K in the optimization procedure and step size \u03bb depend on the particular value of d.", "labels": [], "entities": []}, {"text": "Ford = 100, we have K = 7, \u03bb = 5 \u00b7 10 \u22125 , ford = 200, we have K = 8, \u03bb = 5 \u00b7 10 \u22125 , and ford = 500, we have K = 2, \u03bb = 10 \u22124 . Moreover, the best results were obtained when SVD-SPPMI embeddings were used as an initialization of Riemannian optimization process.", "labels": [], "entities": []}, {"text": "illustrates how the correlation between semantic similarity and human assessment scores changes through iterations of our method.", "labels": [], "entities": []}, {"text": "Optimal value of K is the same for both whole testing set and its 10-fold subsets chosen for cross-validation.", "labels": [], "entities": [{"text": "Optimal", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9632354974746704}]}, {"text": "The idea to stop optimization procedure on some iteration is also discussed in ().", "labels": [], "entities": []}, {"text": "Training of the same dimensional models (d = 500) on English Wikipedia corpus using SGD-SGNS, SVD-SPPMI, RO-SGNS took 20 minutes, 10 minutes and 70 minutes respectively.", "labels": [], "entities": [{"text": "English Wikipedia corpus", "start_pos": 53, "end_pos": 77, "type": "DATASET", "confidence": 0.9417888323465983}]}, {"text": "Our method works slower, but not significantly.", "labels": [], "entities": []}, {"text": "Moreover, since we were not focused on the code efficiency optimization, this time can be reduced.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of the methods in terms of the semantic similarity task. Each entry represents the  Spearman's correlation between predicted similarities and the manually assessed ones.", "labels": [], "entities": []}, {"text": " Table 3: Examples of the semantic neighbors obtained for words \"five\", \"he\" and \"main\".", "labels": [], "entities": []}, {"text": " Table 4: Examples of the semantic neighbors  from 11th to 20th obtained for the word \"usa\" by  all three methods. Top-10 neighbors for all three  methods are exact names of states.", "labels": [], "entities": []}]}