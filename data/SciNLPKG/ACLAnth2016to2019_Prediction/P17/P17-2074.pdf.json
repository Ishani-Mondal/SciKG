{"title": [{"text": "Best-Worst Scaling More Reliable than Rating Scales: A Case Study on Sentiment Intensity Annotation", "labels": [], "entities": [{"text": "Sentiment Intensity Annotation", "start_pos": 69, "end_pos": 99, "type": "TASK", "confidence": 0.9242674509684244}]}], "abstractContent": [{"text": "Rating scales area widely used method for data annotation; however, they present several challenges, such as difficulty in maintaining inter-and intra-annotator consistency.", "labels": [], "entities": [{"text": "data annotation", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7438740432262421}]}, {"text": "Best-worst scaling (BWS) is an alternative method of annotation that is claimed to produce high-quality annotations while keeping the required number of annotations similar to that of rating scales.", "labels": [], "entities": [{"text": "Best-worst scaling (BWS)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6115302085876465}]}, {"text": "However, the veracity of this claim has never been systematically established.", "labels": [], "entities": [{"text": "veracity", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9979996085166931}]}, {"text": "Here for the first time, we setup an experiment that directly compares the rating scale method with BWS.", "labels": [], "entities": [{"text": "BWS", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.7084753513336182}]}, {"text": "We show that with the same total number of annotations, BWS produces significantly more reliable results than the rating scale.", "labels": [], "entities": [{"text": "BWS", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9143779277801514}]}], "introductionContent": [{"text": "When manually annotating data with quantitative or qualitative information, researchers in many disciplines, including social sciences and computational linguistics, often rely on rating scales (RS).", "labels": [], "entities": []}, {"text": "A rating scale provides the annotator with a choice of categorical or numerical values that represent the measurable characteristic of the rated data.", "labels": [], "entities": []}, {"text": "For example, when annotating a word for sentiment, the annotator can be asked to choose among integer values from 1 to 9, with 1 representing the strongest negative sentiment, and 9 representing the strongest positive sentiment).", "labels": [], "entities": []}, {"text": "Another example is the Likert scale, which measures responses on asymmetric agree-disagree scale, from 'strongly disagree' to 'strongly agree').", "labels": [], "entities": [{"text": "Likert scale", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.6791206002235413}]}, {"text": "The annotations for an item from multiple respondents are usually averaged to obtain a real-valued score for that item.", "labels": [], "entities": []}, {"text": "Thus, for an Nitem set, if each item is to be annotated by five respondents, then the number of annotations required is 5N . While frequently used in many disciplines, the rating scale method has a number of limitations).", "labels": [], "entities": []}, {"text": "These include: \u2022 Inconsistencies in annotations by different annotators: one annotator might assign a score of 7 to the word good on a 1-to-9 sentiment scale, while another annotator can assign a score of 8 to the same word.", "labels": [], "entities": [{"text": "Inconsistencies", "start_pos": 17, "end_pos": 32, "type": "METRIC", "confidence": 0.9510766267776489}]}, {"text": "\u2022 Inconsistencies in annotations by the same annotator: an annotator might assign different scores to the same item when the annotations are spread overtime.", "labels": [], "entities": []}, {"text": "\u2022 Scale region bias: annotators often have a bias towards apart of the scale, for example, preference for the middle of the scale.", "labels": [], "entities": []}, {"text": "\u2022 Fixed granularity: in some cases, annotators might feel too restricted with a given rating scale and may want to place an item inbetween the two points on the scale.", "labels": [], "entities": []}, {"text": "On the other hand, a fine-grained scale may overwhelm the respondents and lead to even more inconsistencies in annotation.", "labels": [], "entities": []}, {"text": "Paired Comparisons) is a comparative annotation method, where respondents are presented with pairs of items and asked which item has more of the property of interest (for example, which is more positive).", "labels": [], "entities": []}, {"text": "The annotations can then be converted into a ranking of items by the property of interest, and one can even obtain real-valued scores indicating the degree to which an item is associated with the property of interest.", "labels": [], "entities": []}, {"text": "The paired comparison method does not suffer from the problems discussed above for the rating scale, but it requires a large number of annotations-order N 2 , where N is the number of items to be annotated.", "labels": [], "entities": []}, {"text": "Best-Worst Scaling (BWS) is a less-known, and more recently introduced, variant of comparative annotation.", "labels": [], "entities": [{"text": "Best-Worst Scaling (BWS)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6303190946578979}, {"text": "comparative annotation", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.8953086733818054}]}, {"text": "It was developed by, building on some groundbreaking research in the 1960s in mathematical psychology and psychophysics by Anthony A.", "labels": [], "entities": []}, {"text": "J. Marley and Duncan Luce.", "labels": [], "entities": [{"text": "J. Marley and Duncan Luce", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.7610565662384033}]}, {"text": "Annotators are presented with n items at a time (an n-tuple, where n > 1, and typically n = 4).", "labels": [], "entities": []}, {"text": "They are asked which item is the best (highest in terms of the property of interest) and which is the worst (lowest in terms of the property of interest).", "labels": [], "entities": []}, {"text": "When working on 4-tuples, best-worst annotations are particularly efficient because by answering these two questions, the results for five out of six item-item pair-wise comparisons become known.", "labels": [], "entities": []}, {"text": "All items to be rated are organized in a set of m 4-tuples (m \u2265 N , where N is the number of items) so that each item is evaluated several times in diverse 4-tuples.", "labels": [], "entities": []}, {"text": "Once them 4-tuples are annotated, one can compute real-valued scores for each of the items using a simple counting procedure.", "labels": [], "entities": []}, {"text": "The scores can be used to rank items by the property of interest.", "labels": [], "entities": []}, {"text": "BWS is claimed to produce high-quality annotations while still keeping the number of annotations small (1.5N -2N tuples need to be annotated) ().", "labels": [], "entities": [{"text": "BWS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.887203574180603}]}, {"text": "However, the veracity of this claim has never been systematically established.", "labels": [], "entities": [{"text": "veracity", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9979996085166931}]}, {"text": "In this paper, we pit the widely used rating scale squarely against BWS in a quantitative experiment to determine which method provides more reliable results.", "labels": [], "entities": [{"text": "BWS", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.959259569644928}]}, {"text": "We produce real-valued sentiment intensity ratings for 3,207 English terms (words and phrases) using both methods by aggregating responses from several independent annotators.", "labels": [], "entities": []}, {"text": "We show that BWS ranks terms more reliably, that is, when comparing the term rankings obtained from two groups of annotators for the same set of terms, the correlation between the two sets of ranks produced by BWS is significantly higher than the correlation for the two sets obtained with RS.", "labels": [], "entities": [{"text": "BWS", "start_pos": 210, "end_pos": 213, "type": "DATASET", "confidence": 0.7697150111198425}]}, {"text": "The difference in reliability is more marked when about 5N (or less) total annotations are obtained, which is the casein many NLP annotation projects (.", "labels": [], "entities": [{"text": "reliability", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9879789352416992}]}, {"text": "Furthermore, the reliability obtained by rating scale when using ten annotations per term is matched by BWS with only 3N total annotations (two annotations for each of the 1.5N 4-tuples).", "labels": [], "entities": [{"text": "reliability", "start_pos": 17, "end_pos": 28, "type": "METRIC", "confidence": 0.9671838283538818}, {"text": "BWS", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9295012950897217}]}, {"text": "The sparse prior work in natural language annotations that uses BWS involves the creation of datasets for relational similarity), word-sense disambiguation, and word-sentiment intensity).", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 130, "end_pos": 155, "type": "TASK", "confidence": 0.6896660774946213}]}, {"text": "However, none of these works has systematically compared BWS with the rating scale method.", "labels": [], "entities": [{"text": "BWS", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.5397984385490417}]}, {"text": "We hope that our findings will encourage the use of BWS more widely to obtain high-quality NLP annotations.", "labels": [], "entities": [{"text": "BWS", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.6580557227134705}]}, {"text": "All data from our experiments as well as scripts to generate BWS tuples, to generate item scores from BWS annotations, and for assessing reliability of the annotations are made freely available.", "labels": [], "entities": [{"text": "BWS", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.8364543318748474}, {"text": "BWS", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.8336123824119568}]}], "datasetContent": [{"text": "Both rating scale and BWS are less than perfect ways to capture the true word-sentiment intensities in the minds of native speakers of a language.", "labels": [], "entities": [{"text": "BWS", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9912630319595337}]}, {"text": "Since the \"true\" intensities are not known, determining which approach is better is non-trivial.", "labels": [], "entities": []}, {"text": "A useful measure of quality is reproducibilityif repeated independent manual annotations from multiple respondents result in similar sentiment scores, then one can be confident that the scores capture the true sentiment intensities.", "labels": [], "entities": []}, {"text": "Thus, we setup an experiment that compares BWS and RS in terms of how similar the results are on repeated independent annotations.", "labels": [], "entities": [{"text": "BWS", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9202284812927246}, {"text": "RS", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9006110429763794}]}, {"text": "It is expected that reproducibility improves with the number of annotations for both methods.", "labels": [], "entities": []}, {"text": "(Estimating a value often stabilizes as the sample size is increased.)", "labels": [], "entities": []}, {"text": "However, in rating scale annotation, each item is annotated individually whereas in BWS, groups of four items (4-tuples) are annotated together (and each item is present in multiple different 4-tuples).", "labels": [], "entities": [{"text": "BWS", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8031560778617859}]}, {"text": "To make the reproducibility evaluation fair, we ensure that the term scores are inferred from the same total number of annotations for both methods.", "labels": [], "entities": []}, {"text": "For an N -item set, let k rs be the number of times each item is annotated via a rating scale.", "labels": [], "entities": []}, {"text": "Then the total number of rating scale annotations is k rs N . For BWS, let the same N -item set be converted into m 4-tuples that are each annotated k bws times.", "labels": [], "entities": [{"text": "BWS", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.6901546716690063}]}, {"text": "Then the total number of BWS annotations is k bws m.", "labels": [], "entities": []}, {"text": "In our experiments, we compare results across BWS and rating scale at points when k rs N = k bws m.", "labels": [], "entities": [{"text": "BWS", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.47548985481262207}]}, {"text": "The cognitive complexity involved in answering a BWS question is different from that in a rating scale question.", "labels": [], "entities": [{"text": "answering a BWS question", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.6967047303915024}]}, {"text": "On the one hand, for BWS, the respondent has to consider four items at a time simultaneously.", "labels": [], "entities": [{"text": "BWS", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.576070249080658}]}, {"text": "On the other hand, even though a rating scale question explicitly involves only one item, the respondent must choose a score that places it appropriately with respect to other items.", "labels": [], "entities": []}, {"text": "Quantifying the degree of cognitive load of a BWS annotation vs. a rating scale annotation (especially in a crowdsourcing setting) is particularly challenging, and beyond the scope of this paper.", "labels": [], "entities": []}, {"text": "Here we explore the extent to which the rating scale method and BWS lead to the same resulting scores when the annotations are repeated, controlling for the total number of annotations.", "labels": [], "entities": [{"text": "BWS", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9513787627220154}]}], "tableCaptions": [{"text": " Table 1: Differences in final outcomes of BWS  and RS, for different total numbers of annotations.", "labels": [], "entities": [{"text": "BWS", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.8329234719276428}, {"text": "RS", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.6381765604019165}]}, {"text": " Table 2: Correlations between sentiment scores  produced by BWS and rating scale.", "labels": [], "entities": [{"text": "BWS", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.706770658493042}]}, {"text": " Table 3: Average SHR for BWS and rating scale  (RS) on different subsets of terms.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9762353897094727}, {"text": "SHR", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.5092382431030273}, {"text": "BWS", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.43234992027282715}, {"text": "rating scale  (RS)", "start_pos": 34, "end_pos": 52, "type": "METRIC", "confidence": 0.9452801585197449}]}]}