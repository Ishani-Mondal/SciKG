{"title": [{"text": "Predicting Depression for Japanese Blog Text", "labels": [], "entities": [{"text": "Predicting Depression", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7879820764064789}]}], "abstractContent": [{"text": "This study aims to predict clinical depression , a prevalent mental disorder, from blog posts written in Japanese by using machine learning approaches.", "labels": [], "entities": []}, {"text": "The study focuses on how data quality and various types of linguistic features (characters, tokens , and lemmas) affect prediction outcome.", "labels": [], "entities": []}, {"text": "Depression prediction achieved 95.5% accuracy using selected lemmas as features.", "labels": [], "entities": [{"text": "Depression prediction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7938137352466583}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.999459445476532}]}], "introductionContent": [{"text": "The World Health Organization (WHO) recognizes that depression is a leading cause of ill health and disability.", "labels": [], "entities": []}, {"text": "In Japan, it is also the most frequent reason for sick leave from work).", "labels": [], "entities": []}, {"text": "However, many people with depression may not be aware that their mood change and fatigue are due to depression.", "labels": [], "entities": []}, {"text": "In order to offer help for those who need it, we first need to identify them.", "labels": [], "entities": []}, {"text": "This study examines whether linguistic features in written texts can help predict whether the author is depressed by using a supervised machine learning approach.", "labels": [], "entities": []}, {"text": "Specifically, we examine the effectiveness of morphological (character ngrams), syntactic (token n-grams), and (syntactic-)semantic (lemmas of selected POS categories) features.", "labels": [], "entities": []}, {"text": "In addition, we remove the topic bias so that the methods can be used to predict depression in people who do not know they are depressed and thus do not write about depression.", "labels": [], "entities": []}, {"text": "The results show that lemmas from verb and adverb categories improve performance in classifying authors.", "labels": [], "entities": []}, {"text": "Additionally, the selected words include words not typically thought of as related to depression.", "labels": [], "entities": []}, {"text": "Thus, the study suggests that feature engineering should not be constrained by our notion of what would be related to certain mental conditions or personalities as changes in people's language use maybe very subtle.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.8327612280845642}]}, {"text": "Section 2 discusses previous work on author profiling and depression detection.", "labels": [], "entities": [{"text": "author profiling", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.8809186220169067}, {"text": "depression detection", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.7574685215950012}]}, {"text": "In Section 3, we describe data acquisition, topic modeling, and classifications with different features.", "labels": [], "entities": [{"text": "data acquisition", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7410495281219482}, {"text": "topic modeling", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.8235789239406586}]}, {"text": "Section 4 summarizes the results, and Section 5 discusses the results.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes the paper with a summary and an outlook.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of authors and documents and average word count before and after topic modeling", "labels": [], "entities": []}, {"text": " Table 2: Number of authors and documents and average word count in training and test sets", "labels": [], "entities": []}, {"text": " Table 3: Accuracies (%) for character n-grams. CharUni and CharN: Japanese character uni-and n- grams. RomUni and RomN: Romanized character uni-and n-grams. FS:Feature Selection. The value  in parentheses indicates the best value of n for n-gram and a method for feature selection (L:Logistic  Regression, S:SVM, U:Univariate)", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9984115362167358}, {"text": "FS", "start_pos": 158, "end_pos": 160, "type": "METRIC", "confidence": 0.9224798083305359}]}, {"text": " Table 4: Accuracies (%) for Token and Token n-grams. The value in parentheses indicates the value of n  for n-grams or model of feature selection (L:Logistic Regression, S:SVM, U:Univariate)", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9991330504417419}]}, {"text": " Table 5: Accuracies (%) for lemmas and lemmas of POS categories with the highest accuracy. The char- acter in parentheses shows a model of feature selection (L:Logistic Regression, S:SVM, U:Univariate)", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9991156458854675}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.997312605381012}]}]}