{"title": [{"text": "Towards a Seamless Integration of Word Senses into Downstream NLP Applications", "labels": [], "entities": [{"text": "Seamless Integration of Word Senses", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.8925084829330444}, {"text": "Downstream NLP", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.8254925012588501}]}], "abstractContent": [{"text": "Lexical ambiguity can impede NLP systems from accurate understanding of semantics.", "labels": [], "entities": []}, {"text": "Despite its potential benefits, the integration of sense-level information into NLP systems has remained understudied.", "labels": [], "entities": []}, {"text": "By incorporating a novel disambiguation algorithm into a state-of-the-art classification model, we create a pipeline to integrate sense-level information into downstream NLP applications.", "labels": [], "entities": []}, {"text": "We show that a simple disambiguation of the input text can lead to consistent performance improvement on multiple topic categoriza-tion and polarity detection datasets, particularly when the fine granularity of the underlying sense inventory is reduced and the document is sufficiently large.", "labels": [], "entities": []}, {"text": "Our results also point to the need for sense representation research to focus more on in vivo evaluations which target the performance in downstream NLP applications rather than artificial benchmarks.", "labels": [], "entities": [{"text": "sense representation", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.8508553802967072}]}], "introductionContent": [{"text": "As a general trend, most current Natural Language Processing (NLP) systems function at the word level, i.e. individual words constitute the most fine-grained meaning-bearing elements of their input.", "labels": [], "entities": []}, {"text": "The word level functionality can affect the performance of these systems in two ways: (1) it can hamper their efficiency in handling words that are not encountered frequently during training, such as multiwords, inflections and derivations, and (2) it can restrict their semantic understanding to the level of words, with all their ambiguities, and thereby prevent accurate capture of the intended meanings.", "labels": [], "entities": []}, {"text": "The first issue has recently been alleviated by techniques that aim to boost the generalisation power of NLP systems by resorting to sub-word or character-level information ().", "labels": [], "entities": []}, {"text": "The second limitation, however, has not yet been studied sufficiently.", "labels": [], "entities": []}, {"text": "A reasonable way to handle word ambiguity, and hence to tackle the second issue, is to semantify the input text: transform it from its surface-level semantics to the deeper level of word senses, i.e. their intended meanings.", "labels": [], "entities": []}, {"text": "We take a step in this direction by designing a pipeline that enables seamless integration of word senses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks.", "labels": [], "entities": []}, {"text": "To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.698613166809082}]}, {"text": "We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization.", "labels": [], "entities": [{"text": "polarity detection", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.8298746645450592}]}, {"text": "Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 136, "end_pos": 161, "type": "TASK", "confidence": 0.8115801215171814}]}, {"text": "We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced.", "labels": [], "entities": [{"text": "text classification", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7532451152801514}]}, {"text": "Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques.", "labels": [], "entities": [{"text": "WSD and sense representation", "start_pos": 87, "end_pos": 115, "type": "TASK", "confidence": 0.6241737306118011}]}], "datasetContent": [{"text": "We evaluated our model on two classification tasks: topic categorization (Section 5.2) and polarity detection (Section 5.3).", "labels": [], "entities": [{"text": "polarity detection", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7397812455892563}]}, {"text": "In the following section we present the common experimental setup.", "labels": [], "entities": []}, {"text": "Throughout all the experiments we used the classification model described in Section 4.", "labels": [], "entities": []}, {"text": "The general architecture of the model was the same for both tasks, with slight variations in hyperparameters given the different natures of the tasks, following the values suggested by and for the two tasks.", "labels": [], "entities": []}, {"text": "Hyperparameters were fixed across all configurations in the corresponding tasks.", "labels": [], "entities": [{"text": "Hyperparameters", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9246422648429871}]}, {"text": "The embedding layer was fixed to 300 dimensions, irrespective of the configuration, i.e. Random and Pre-trained.", "labels": [], "entities": []}, {"text": "For both tasks the evaluation was carried out by 10-fold cross-validation unless standard trainingtesting splits were available.", "labels": [], "entities": []}, {"text": "The disambiguation threshold \u03b8 (cf. Section 3) was tuned on the training portion of the corresponding data, over seven values in in steps of 0.5.", "labels": [], "entities": [{"text": "disambiguation threshold \u03b8", "start_pos": 4, "end_pos": 30, "type": "METRIC", "confidence": 0.8482278188069662}]}, {"text": "We used Keras (Chollet, 2015) and Theano (Team, 2016) for our model implementations.", "labels": [], "entities": [{"text": "Keras (Chollet, 2015)", "start_pos": 8, "end_pos": 29, "type": "DATASET", "confidence": 0.840687225262324}, {"text": "Theano (Team, 2016)", "start_pos": 34, "end_pos": 53, "type": "DATASET", "confidence": 0.8612924814224243}]}, {"text": "The integration of senses was carried out as described in Section 3.", "labels": [], "entities": []}, {"text": "For disambiguating with both WordNet and Wikipedia senses we relied on the joint semantic network of Wikipedia hyperlinks and WordNet via the mapping provided by BabelNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 126, "end_pos": 133, "type": "DATASET", "confidence": 0.9525555372238159}]}, {"text": "Pre-trained word and sense embeddings.", "labels": [], "entities": []}, {"text": "Throughout all the experiments we used Word2vec () embeddings, trained on the Google News corpus.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9306327104568481}, {"text": "Google News corpus", "start_pos": 78, "end_pos": 96, "type": "DATASET", "confidence": 0.8817084630330404}]}, {"text": "We truncated this set to its 250K most frequent words.", "labels": [], "entities": []}, {"text": "We also used WordNet 3.0 and the Wikipedia dump of November 2014 to compute the sense embeddings (see Section 4.1).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9259645342826843}, {"text": "Wikipedia dump of November 2014", "start_pos": 33, "end_pos": 64, "type": "DATASET", "confidence": 0.9389050006866455}]}, {"text": "As a result, we obtained a set of 757,262 sense embeddings in the same space as the pre-trained Word2vec word embeddings.", "labels": [], "entities": [{"text": "Word2vec word embeddings", "start_pos": 96, "end_pos": 120, "type": "DATASET", "confidence": 0.8774429559707642}]}, {"text": "We used DeConf ( as our pre-trained WordNet sense embeddings.", "labels": [], "entities": [{"text": "WordNet sense embeddings", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.8999314109484354}]}, {"text": "All vectors had a fixed dimensionality of 300.", "labels": [], "entities": []}, {"text": "In addition to WordNet senses, we experimented with supersenses (see Section 4.2) to check how reducing granularity would affect system performance.", "labels": [], "entities": []}, {"text": "For obtaining supersenses in a given text we relied on our disambiguation pipeline and simply clustered together senses belonging to the same WordNet supersense.", "labels": [], "entities": []}, {"text": "We report the results in terms of standard accuracy and F1 measures.", "labels": [], "entities": [{"text": "standard", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9351162314414978}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.8929035067558289}, {"text": "F1", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9997289776802063}]}, {"text": "For the polarity detection task we used five standard evaluation datasets.", "labels": [], "entities": [{"text": "polarity detection", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.9359181225299835}]}, {"text": "PL04 () is a polarity detection dataset composed of full movie reviews.", "labels": [], "entities": [{"text": "PL04", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8885628581047058}, {"text": "polarity detection", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7552325427532196}]}, {"text": "PL05 18 (Pang and), instead, is composed of short snippets from movie reviews.", "labels": [], "entities": [{"text": "PL05 18", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6564064472913742}]}, {"text": "RTC contains critic reviews from Rotten Tomatoes , divided into 436,000 training and 2,000 test instances.", "labels": [], "entities": [{"text": "RTC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7742404937744141}]}, {"text": "IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test.", "labels": [], "entities": [{"text": "IMDB (Maas et al., 2011)", "start_pos": 0, "end_pos": 24, "type": "DATASET", "confidence": 0.8139877021312714}]}, {"text": "Finally, we used the Stanford Sentiment dataset , which associates each review with a value that denotes its sentiment.", "labels": [], "entities": [{"text": "Stanford Sentiment dataset", "start_pos": 21, "end_pos": 47, "type": "DATASET", "confidence": 0.9361909826596578}]}, {"text": "To be consistent with the binary classification of the other datasets, we removed the neutral phrases according to the dataset's scale (between 0.4 and 0.6) and considered the reviews whose values were below 0.4 as negative and above 0.6 as positive.", "labels": [], "entities": []}, {"text": "This resulted in a binary polarity dataset of 119,783 phrases.", "labels": [], "entities": []}, {"text": "Unlike the previous four datasets, this dataset does not contain an even distribution of positive and negative labels.", "labels": [], "entities": []}, {"text": "lists accuracy performance of our classification model and all its variants on five polar-18 Both PL04 and PL05 were downloaded from http:// www.cs.cornell.edu/people/pabo/movie-review-data/ 19 http://www.rottentomatoes.com, showing that the addition of the recurrent layer to the model (cf. Section 4) was beneficial.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9995404481887817}, {"text": "PL04", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.7780788540840149}, {"text": "PL05", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.7862852811813354}]}, {"text": "However, interestingly, no consistent performance gain is observed in the polarity detection task, when the model is provided with disambiguated input, particularly for datasets with relatively short reviews.", "labels": [], "entities": [{"text": "polarity detection task", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.8447432319323221}]}, {"text": "We attribute this to the nature of the task.", "labels": [], "entities": []}, {"text": "Firstly, given that words rarely happen to be ambiguous with respect to their sentiment, the semantic sense distinctions provided by the disambiguation stage do not assist the classifier in better decision making, and instead introduce data sparsity.", "labels": [], "entities": []}, {"text": "Secondly, since the datasets mostly contain short texts, e.g., sentences or snippets, the disambiguation algorithm does not have sufficient context to make high-confidence judgements, resulting in fewer disambiguations or less reliable ones.", "labels": [], "entities": []}, {"text": "In the following section we perform a more in-depth analysis of the impact of document size on the performance of our sense-based models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the topic categorization datasets.", "labels": [], "entities": []}, {"text": " Table 2: Classification performance at the word, sense, and supersense levels with random and pre- trained embedding initialization. We show in bold those settings that improve the word-based model.", "labels": [], "entities": []}, {"text": " Table 3: Statistics of the polarity detection datasets.", "labels": [], "entities": [{"text": "polarity detection", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8248990774154663}]}, {"text": " Table 4: Accuracy performance on five polarity detection datasets. Given that polarity datasets are  balanced", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9977838397026062}]}]}