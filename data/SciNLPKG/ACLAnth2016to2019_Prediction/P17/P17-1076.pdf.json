{"title": [{"text": "A Minimal Span-Based Neural Constituency Parser", "labels": [], "entities": []}], "abstractContent": [{"text": "In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.8344322144985199}]}, {"text": "We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input.", "labels": [], "entities": []}, {"text": "We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 225, "end_pos": 238, "type": "DATASET", "confidence": 0.9946997761726379}, {"text": "F1", "start_pos": 246, "end_pos": 248, "type": "METRIC", "confidence": 0.7970117330551147}, {"text": "French Treebank", "start_pos": 280, "end_pos": 295, "type": "DATASET", "confidence": 0.9951371550559998}, {"text": "F1", "start_pos": 303, "end_pos": 305, "type": "METRIC", "confidence": 0.536024272441864}]}], "introductionContent": [{"text": "This paper presents a minimal but surprisingly effective span-based neural model for constituency parsing.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.9167463481426239}]}, {"text": "Recent years have seen a great deal of interest in parsing architectures that make use of recurrent neural network (RNN) representations of input sentences).", "labels": [], "entities": [{"text": "parsing", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.9682533144950867}]}, {"text": "Despite evidence that linear RNN decoders are implicitly able to respect some nontrivial well-formedness constraints on structured outputs, researchers have consistently found that the best performance is achieved by systems that explicitly require the decoder to generate well-formed tree structures.", "labels": [], "entities": []}, {"text": "There are two general approaches to ensuring this structural consistency.", "labels": [], "entities": []}, {"text": "The most common is to encode the output as a sequence of operations within a transition system which constructs trees incrementally.", "labels": [], "entities": []}, {"text": "This transforms the parsing problem back into a sequence-to-sequence problem, while making it easy to force the decoder to take only actions guaranteed to produce well-formed outputs.", "labels": [], "entities": [{"text": "parsing problem", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.9160761535167694}]}, {"text": "However, transition-based models do not admit fast dynamic programs and require careful feature engineering to support exact search-based inference (.", "labels": [], "entities": []}, {"text": "Moreover, models with recurrent state require complex training procedures to benefit from anything other than greedy decoding).", "labels": [], "entities": []}, {"text": "An alternative line of work focuses on chart parsers, which use log-linear or neural scoring potentials to parameterize a tree-structured dynamic program for maximization or marginalization (.", "labels": [], "entities": []}, {"text": "These models enjoy a number of appealing formal properties, including support for exact inference and structured loss functions.", "labels": [], "entities": []}, {"text": "However, previous chart-based approaches have required considerable scaffolding beyond a simple well-formedness potential, e.g. pre-specification of a complete context-free grammar for generating output structures and initial pruning of the output space with a weaker model ().", "labels": [], "entities": []}, {"text": "Additionally, we are unaware of any recent chartbased models that achieve results competitive with the best transition-based models.", "labels": [], "entities": []}, {"text": "In this work, we present an extremely simple chart-based neural parser based on independent scoring of labels and spans, and show how this model can be adapted to support a greedy topdown decoding procedure.", "labels": [], "entities": []}, {"text": "Our goal is to preserve the basic algorithmic properties of span-oriented (rather than transition-oriented) parse representations, while exploring the extent to which neural representational machinery can replace the additional structure required by existing chart parsers.", "labels": [], "entities": []}, {"text": "On the Penn Treebank, our approach outperforms a number of recent models for chart-based and transition-based parsing-including the state-ofthe-art models of and-achieving an F1 score of 91.79.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9953223764896393}, {"text": "transition-based parsing-including", "start_pos": 93, "end_pos": 127, "type": "TASK", "confidence": 0.5589829832315445}, {"text": "F1 score", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.98940709233284}]}, {"text": "We additionally obtain a strong F1 score of 82.23 on the French Treebank.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9848669171333313}, {"text": "French Treebank", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.9576736390590668}]}], "datasetContent": [{"text": "We first describe the general setup used for our experiments.", "labels": [], "entities": []}, {"text": "We use the Penn Treebank (Marcus et al., 1993) for our English experiments, with standard splits of sections 2-21 for training, section 22 for development, and section 23 for testing.", "labels": [], "entities": [{"text": "Penn Treebank (Marcus et al., 1993)", "start_pos": 11, "end_pos": 46, "type": "DATASET", "confidence": 0.9637944168514676}]}, {"text": "We use the French Treebank from the SPMRL 2014 shared task () with its provided splits for our French experiments.", "labels": [], "entities": [{"text": "French Treebank from the SPMRL 2014 shared task", "start_pos": 11, "end_pos": 58, "type": "DATASET", "confidence": 0.8832463175058365}]}, {"text": "No token preprocessing is performed, and only a single <UNK> token is used for unknown words attest time.", "labels": [], "entities": []}, {"text": "The inputs to our system are concatenations of 100-dimensional word embeddings and 50-dimensional part-of-speech embeddings.", "labels": [], "entities": []}, {"text": "In the case of the French Treebank, we also include 50-dimensional embeddings of each morphological tag.", "labels": [], "entities": [{"text": "French Treebank", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.9918577969074249}]}, {"text": "We use automatically predicted tags for training and testing, obtaining predicted part-ofspeech tags for the Penn Treebank using the Stanford tagger () with 10-way jackknifing, and using the provided predicted partof-speech and morphological tags for the French Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.9956615567207336}, {"text": "French Treebank", "start_pos": 255, "end_pos": 270, "type": "DATASET", "confidence": 0.9952069818973541}]}, {"text": "Words are replaced by <UNK> with probability 1/(1+freq(w)) during training, where freq(w) is the frequency of win the training data.", "labels": [], "entities": [{"text": "freq(w)", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9320776462554932}]}, {"text": "We use a two-layer bidirectional LSTM for our base span features.", "labels": [], "entities": []}, {"text": "Dropout with a ratio selected from {0.2, 0.3, 0.4} is applied to all non-recurrent: Development F1 scores on the Penn Treebank.", "labels": [], "entities": [{"text": "Development F1", "start_pos": 84, "end_pos": 98, "type": "METRIC", "confidence": 0.579245924949646}, {"text": "Penn Treebank", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.993902713060379}]}, {"text": "Each table corresponds to a particular choice of label loss (either the basic 0-1 loss or the structured Hamming label loss of Section 5.5) and labeling scheme (either the basic atomic scheme or the top-middle-bottom labeling scheme of Section 5.1).", "labels": [], "entities": [{"text": "Section 5.5", "start_pos": 127, "end_pos": 138, "type": "DATASET", "confidence": 0.8428211212158203}, {"text": "Section 5.1", "start_pos": 236, "end_pos": 247, "type": "DATASET", "confidence": 0.8874361515045166}]}, {"text": "The columns within each table correspond to different split scoring schemes: basic minimal scoring, the leftright scoring of Section 5.2, the concatenation scoring of Section 5.3, and the deep biaffine scoring of Section 5.4.", "labels": [], "entities": []}, {"text": "connections of the LSTM, including its inputs and outputs.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.7577254772186279}]}, {"text": "We tie the hidden dimension of the LSTM and all feedforward networks, selecting a size from {150, 200, 250}.", "labels": [], "entities": []}, {"text": "All parameters (including word and tag embeddings) are randomly initialized using Glorot initialization, and are tuned on development set performance.", "labels": [], "entities": []}, {"text": "We use the Adam optimizer () with its default settings for optimization, with a batch size of 10.", "labels": [], "entities": []}, {"text": "Our system is implemented in C++ using the DyNet neural network library (.", "labels": [], "entities": []}, {"text": "We begin by training the minimal version of our proposed chart and top-down parsers on the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.9896233081817627}]}, {"text": "Out of the box, we obtain test F1 scores of 91.69 for the chart parser and 91.58 for the topdown parser.", "labels": [], "entities": [{"text": "F1", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9464853405952454}]}, {"text": "The higher of these matches the recent state-of-the-art score of 91.7 reported by, demonstrating that our simple neural parsing system is already capable of achieving strong results.", "labels": [], "entities": []}, {"text": "Building on this, we explore the effects of different split scoring functions when using either the basic 0-1 label loss or the structured label loss discussed in Section 5.5.", "labels": [], "entities": []}, {"text": "Our results are presented in.", "labels": [], "entities": []}, {"text": "We observe that regardless of the label loss, the minimal and deep biaffine split scoring schemes perform a notch below the left-right and concatenation scoring schemes.", "labels": [], "entities": []}, {"text": "That the minimal scoring scheme performs worse than the left-right scheme is unsurprising, since the latter is a strict generalization of the former.", "labels": [], "entities": []}, {"text": "It is evident, however, that joint scoring of left and right subspans is not required for strong results-in fact, the left-right scheme which scores child subspans in isolation slightly outperforms the concatenation scheme in all but one case, and is stronger than the deep biaffine scoring function across the board.", "labels": [], "entities": []}, {"text": "Comparing results across the choice of label loss, however, we find that fewer trends are apparent.", "labels": [], "entities": []}, {"text": "The scores obtained by training with a 0-1 loss are all within 0.1 of those obtained using a structured Hamming loss, being slightly higher in four out of eight cases and slightly lower in the other half.", "labels": [], "entities": [{"text": "Hamming", "start_pos": 104, "end_pos": 111, "type": "TASK", "confidence": 0.8560890555381775}]}, {"text": "This leads us to conclude that the more elementary approach is sufficient when selecting atomic labels from a fixed inventory.", "labels": [], "entities": []}, {"text": "We also perform the same set of experiments under the setting where the top-middle-bottom label scoring function described in Section 5.1 is used in place of anatomic label scoring function.", "labels": [], "entities": []}, {"text": "These results are shown in A priori, we might expect that exposing additional structure would allow the model to make better predictions, but on the whole we find that the scores in this set of experiments are worse than those in the previous set.", "labels": [], "entities": [{"text": "A", "start_pos": 27, "end_pos": 28, "type": "METRIC", "confidence": 0.9806864857673645}]}, {"text": "Trends similar to before hold across the different choices of scoring functions, though in this case the minimal setting has scores closer to those of the left-right setting, even exceeding its performance in the case of a chart parser with a 0-1 label loss.", "labels": [], "entities": []}, {"text": "Our final test results are given in   achieve anew state-of-the-art F1 score of 91.79 with our best model.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9849233329296112}]}, {"text": "Interestingly, we observe that our parsers have a noticeably higher gap between precision and recall than do other top parsers, likely owing to the structured label loss which penalizes mismatching nonterminals more heavily than it does a nonterminal and empty label mismatch.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.999213695526123}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.996667206287384}]}, {"text": "In addition, there is little difference between the best top-down model and the best chart model, indicating that global normalization is not required to achieve strong results.", "labels": [], "entities": []}, {"text": "Processing one sentence at a time on a c4.4xlarge Amazon EC2 instance, our best chart and top-down parsers operate at speeds of 20.3 sentences per second and 75.5 sentences per second, respectively, as measured on the test set.", "labels": [], "entities": []}, {"text": "We additionally train parsers on the French Treebank using the same settings from our English experiments, selecting the best model of each type based on development performance.", "labels": [], "entities": [{"text": "French Treebank", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9930756986141205}]}, {"text": "We list our test results along with those of several other recent papers in.", "labels": [], "entities": []}, {"text": "Although we fall short of the scores obtained by, we achieve competitive performance relative to the neural CRF parser of.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Development F1 scores on the Penn Treebank. Each table corresponds to a particular choice  of label loss (either the basic 0-1 loss or the structured Hamming label loss of Section 5.5) and labeling  scheme (either the basic atomic scheme or the top-middle-bottom labeling scheme of Section 5.1). The  columns within each table correspond to different split scoring schemes: basic minimal scoring, the left- right scoring of Section 5.2, the concatenation scoring of Section 5.3, and the deep biaffine scoring of  Section 5.4.", "labels": [], "entities": [{"text": "F1", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.8133347034454346}, {"text": "Penn Treebank", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.9959864914417267}]}, {"text": " Table 2: Comparison of final test F1 scores on  the Penn Treebank. Here we only include scores  from single-model parsers trained without external  parse data.", "labels": [], "entities": [{"text": "F1", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.8794551491737366}, {"text": "Penn Treebank", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9970572590827942}]}, {"text": " Table 3: Comparison of final test F1 scores on the  French Treebank.", "labels": [], "entities": [{"text": "F1", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9195796847343445}, {"text": "French Treebank", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.974244624376297}]}]}