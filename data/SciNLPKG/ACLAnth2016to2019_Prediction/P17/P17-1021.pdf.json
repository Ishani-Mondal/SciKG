{"title": [{"text": "An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8210253715515137}]}], "abstractContent": [{"text": "With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important.", "labels": [], "entities": []}, {"text": "Question answering over knowledge base (KB-QA) is one of the promising approaches to access the substantial knowledge.", "labels": [], "entities": [{"text": "Question answering over knowledge base (KB-QA)", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.805260680615902}]}, {"text": "Meanwhile, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results.", "labels": [], "entities": []}, {"text": "However, previous work did not put more emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers.", "labels": [], "entities": [{"text": "question representation", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.7686267495155334}]}, {"text": "This simple representation strategy is not easy to express the proper information in the question.", "labels": [], "entities": []}, {"text": "Hence, we present an end-to-end neural network model to represent the questions and their corresponding scores dynamically according to the various candidate answer aspects via cross-attention mechanism.", "labels": [], "entities": []}, {"text": "In addition , we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers.", "labels": [], "entities": []}, {"text": "As a result, it could alleviates the out-of-vocabulary (OOV) problem, which helps the cross-attention model to represent the question more precisely.", "labels": [], "entities": []}, {"text": "The experimental results on WebQuestions demonstrate the effectiveness of the proposed approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "As the amount of the knowledge bases (KBs) grows, people are paying more attention to seeking effective methods for accessing these precious intellectual resources.", "labels": [], "entities": []}, {"text": "There are several tailor-made languages designed for querying KBs, such as SPARQL (.", "labels": [], "entities": []}, {"text": "However, to handle such query languages, users are required to not only be familiar with the particular language grammars, but also be aware of the architectures of the KBs.", "labels": [], "entities": []}, {"text": "By contrast, knowledge base-based question answering (KB-QA), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years.", "labels": [], "entities": [{"text": "knowledge base-based question answering (KB-QA)", "start_pos": 13, "end_pos": 60, "type": "TASK", "confidence": 0.6692727889333453}]}, {"text": "Given natural language questions, the goal of KB-QA is to automatically return answers from the KB.", "labels": [], "entities": []}, {"text": "There are two mainstream research directions for this task: semantic parsing-based (SPbased)) and information retrieval-based (IR-based),b) methods.", "labels": [], "entities": [{"text": "semantic parsing-based", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.7133951187133789}]}, {"text": "SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms.", "labels": [], "entities": []}, {"text": "IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers.", "labels": [], "entities": []}, {"text": "Recently, with the progress of deep learning, neural network-based (NN-based) methods have been introduced to the KB-QA task ().", "labels": [], "entities": []}, {"text": "Different from previous methods, NNbased methods represent both of the questions and the answers as semantic vectors.", "labels": [], "entities": []}, {"text": "Then the complex process of KB-QA could be converted into a similarity matching process between an input question and its candidate answers in a semantic space.", "labels": [], "entities": []}, {"text": "The candidates with the highest similarity score will be selected as the final answers.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 32, "end_pos": 48, "type": "METRIC", "confidence": 0.9812653660774231}]}, {"text": "Because they are more adaptive, NN-based methods have attracted more and more attention, and this paper also focuses on using end-to-end neural networks to answer questions over knowledge base.", "labels": [], "entities": []}, {"text": "In NN-based methods, the crucial step is to compute the similarity score between a question and a candidate answer, where the key is to learn their representations.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 56, "end_pos": 72, "type": "METRIC", "confidence": 0.9627068340778351}]}, {"text": "Previous methods put more emphasis on learning representation of the answer end.", "labels": [], "entities": []}, {"text": "For example, consider the importance of the subgraph of the candidate answer.", "labels": [], "entities": []}, {"text": "make use of the context and the type of the answer.", "labels": [], "entities": []}, {"text": "However, the representation of the question end is oligotrophic.", "labels": [], "entities": []}, {"text": "Existing approaches often represent a question into a single vector using simple bag-of-words (BOW) model, whereas the relatedness to the answer end is neglected.", "labels": [], "entities": []}, {"text": "We argue that a question should be represented differently according to the different focuses of various answer aspects . Take the question \"Who is the president of France?\" and one of its candidate answers \"Francois Hollande\" as an example.", "labels": [], "entities": []}, {"text": "When dealing with the answer entity Francois Holland, \"president\" and \"France\" in the question is more focused, and the question representation should bias towards the two words.", "labels": [], "entities": []}, {"text": "While facing the answer type /business/board member, \"Who\" should be the most prominent word.", "labels": [], "entities": []}, {"text": "Meanwhile, some questions may value answer type more than other answer aspects.", "labels": [], "entities": []}, {"text": "While in some other questions, answer relation maybe the most important information we should consider, which is dynamic and flexible corresponding to different questions and answers.", "labels": [], "entities": [{"text": "answer relation", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.8131670951843262}]}, {"text": "Obviously, this is an attention mechanism, which reveals the mutual influences between the representation of questions and the corresponding answer aspects.", "labels": [], "entities": []}, {"text": "We believe that such kind of representation is more expressive.", "labels": [], "entities": []}, {"text": "represents questions using three CNNs with different parameters when dealing with different answer aspects including answer path, answer context and answer type.", "labels": [], "entities": []}, {"text": "The method is very enlightening and achieves the best performance on WebQeustions at that time among the end-to-end approaches.", "labels": [], "entities": []}, {"text": "However, we argue that simply selecting three independent CNNs is mechanical and inflexible.", "labels": [], "entities": []}, {"text": "Thus, we go one step further, and propose a crossattention based neural network to perform KB-QA.", "labels": [], "entities": []}, {"text": "The cross-attention model, which stands for the mutual attention between the question and the answer aspects, contains two parts: the answertowards-question attention part and the questiontowards-answer attention part.", "labels": [], "entities": []}, {"text": "The former help learn flexible and adequate question representation, and the latter help adjust the question-answer weight, getting the final score.", "labels": [], "entities": [{"text": "question representation", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.702757939696312}]}, {"text": "We illustrate in section 3.2 for more details.", "labels": [], "entities": []}, {"text": "In this way, we formulate the cross-attention mechanism to model the question answering procedure.", "labels": [], "entities": [{"text": "question answering procedure", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.831112782160441}]}, {"text": "Note that our proposed model is an entire end-to-end approach which only depends on training data.", "labels": [], "entities": []}, {"text": "Some integrated systems which use extra patterns and resources are not directly comparable to ours.", "labels": [], "entities": []}, {"text": "Our target is to explore a better solution following the end-to-end KB-QA technical path.", "labels": [], "entities": []}, {"text": "Moreover, we notice that the representations of the KB resources (entities and relations) are also limited in previous work.", "labels": [], "entities": []}, {"text": "specifically, they are often learned barely on the QA training data, which results in two limitations.", "labels": [], "entities": [{"text": "QA training data", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.6657171547412872}]}, {"text": "1) The global information of the KB is deficient.", "labels": [], "entities": []}, {"text": "For example, if question-answer pair (q, a) appears in the training data, and the global KB information implies us that a is similar to a 2 , denoted by (a \u223c a ), then (q, a ) is more probable to be right.", "labels": [], "entities": []}, {"text": "However, current QA training mechanism cannot guarantee (a \u223c a ) could be learned.", "labels": [], "entities": []}, {"text": "2) The problem of out-ofvocabulary (OOV) stands out.", "labels": [], "entities": []}, {"text": "Due to the limited coverage of the training data, the OOV problem is common while testing, and many answer entities in testing candidate set have never been seen before.", "labels": [], "entities": [{"text": "OOV", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.7749612927436829}]}, {"text": "The attention of these resources become the same because they shared the same OOV embedding, and this will do harm to the proposed attention model.", "labels": [], "entities": []}, {"text": "To tackle these two problems, we additionally incorporates KB itself as training data for training embeddings besides original questionanswer pairs.", "labels": [], "entities": []}, {"text": "In this way, the global structure of the whole knowledge could be captured, and the OOV problem could be alleviated naturally.", "labels": [], "entities": [{"text": "OOV", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.6300441026687622}]}, {"text": "In summary, the contributions are as follows.", "labels": [], "entities": []}, {"text": "1) We present a novel cross-attention based NN model tailored to KB-QA task, which considers the mutual influence between the representation of questions and the corresponding answer aspects.", "labels": [], "entities": []}, {"text": "2) We leverage the global KB information, aiming at represent the answers more precisely.", "labels": [], "entities": [{"text": "KB information", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.6679586172103882}]}, {"text": "It also al-leviates the OOV problem, which is very helpful to the cross-attention model.", "labels": [], "entities": [{"text": "OOV", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9482911229133606}]}, {"text": "3) The experimental results on the open dataset WebQuestions demonstrate the effectiveness of the proposed approach.", "labels": [], "entities": [{"text": "open dataset WebQuestions", "start_pos": 35, "end_pos": 60, "type": "DATASET", "confidence": 0.7655417323112488}]}], "datasetContent": [{"text": "To evaluate the proposed method, we conduct experiments on WebQuestions ( dataset that includes 3,778 question-answer pairs for training and 2,032 for testing.", "labels": [], "entities": []}, {"text": "The questions are collected from Google Suggest API, and the answers are labeled manually by Amazon MTurk.", "labels": [], "entities": [{"text": "Amazon MTurk", "start_pos": 93, "end_pos": 105, "type": "DATASET", "confidence": 0.8608912527561188}]}, {"text": "All the answers are from Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.9908900856971741}]}, {"text": "We use threequarter of the training data as training set, and the left as validate set.", "labels": [], "entities": []}, {"text": "We use F 1 score as evaluation matric, and the average result is computed by the script provided by.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9622443914413452}]}, {"text": "Note that our proposed approach is an entire end-to-end method, which totally depends on training data.", "labels": [], "entities": []}, {"text": "It is worth noting that achieve much higher F 1 scores than other methods.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9897264838218689}]}, {"text": "Their staged system is able to address more questions with constraints and aggregations.", "labels": [], "entities": []}, {"text": "However, their approach applies numbers of manually designed rules and features, which come from the observations on the training set questions.", "labels": [], "entities": []}, {"text": "These particular manual efforts reduce the adaptability of their approach.", "labels": [], "entities": []}, {"text": "Moreover, there are some integrated systems such as) achieve higher F 1 scores which leverage Wikipedia free text as external knowledge, so their systems are not directly comparable to ours.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9717997312545776}]}], "tableCaptions": [{"text": " Table 1: The evaluation results on WebQuestions.", "labels": [], "entities": [{"text": "WebQuestions", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9462849497795105}]}]}