{"title": [{"text": "Vancouver Welcomes You! Minimalist Location Metonymy Resolution", "labels": [], "entities": [{"text": "Vancouver Welcomes You! Minimalist Location Metonymy", "start_pos": 0, "end_pos": 52, "type": "DATASET", "confidence": 0.829570301941463}]}], "abstractContent": [{"text": "Named entities are frequently used in a metonymic manner.", "labels": [], "entities": []}, {"text": "They serve as references to related entities such as people and organisations.", "labels": [], "entities": []}, {"text": "Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications , such as Named Entity Recognition and Geographical Parsing.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.692313551902771}, {"text": "Geographical Parsing", "start_pos": 149, "end_pos": 169, "type": "TASK", "confidence": 0.7707840204238892}]}, {"text": "Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries , external word lists and other hand-crafted lexical resources.", "labels": [], "entities": [{"text": "metonymy resolution (MR)", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.8617266774177551}]}, {"text": "We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the Se-mEval 2007 task on Metonymy Resolution.", "labels": [], "entities": [{"text": "Metonymy Resolution", "start_pos": 148, "end_pos": 167, "type": "TASK", "confidence": 0.7131720334291458}]}, {"text": "Additionally, we contribute with anew Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.", "labels": [], "entities": [{"text": "Wikipedia-based MR dataset", "start_pos": 38, "end_pos": 64, "type": "DATASET", "confidence": 0.589260071516037}]}], "introductionContent": [{"text": "In everyday language, we come across many types of figurative speech.", "labels": [], "entities": []}, {"text": "These irregular expressions are understood with little difficulty by humans but require special attention in NLP.", "labels": [], "entities": []}, {"text": "One of these is metonymy, a type of common figurative language, which stands for the substitution of the concept, phrase or word being meant with a semantically related one.", "labels": [], "entities": []}, {"text": "For example, in \"Moscow traded gas and aluminium with Beijing.\", both location names were substituted in place of governments.", "labels": [], "entities": []}, {"text": "Named Entity Recognition (NER) taggers have no provision for handling metonymy, meaning that this frequent linguistic phenomenon goes largely undetected within current NLP.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER) taggers", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.817417425768716}]}, {"text": "Classification decisions presently focus on the entity using features such as orthography to infer its word sense, largely ignoring the context, which provides the strongest clue about whether a word is used metonymically.", "labels": [], "entities": []}, {"text": "A common classification approach is choosing the N words to the immediate left and right of the entity or the whole paragraph as input to the model.", "labels": [], "entities": []}, {"text": "However, this \"greedy\" approach also processes input that should in practice be ignored.", "labels": [], "entities": []}, {"text": "Metonymy is problematic for applications such as Geographical Parsing (, GP) and other information extraction tasks in NLP.", "labels": [], "entities": [{"text": "Geographical Parsing (, GP)", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.7984538674354553}, {"text": "information extraction tasks", "start_pos": 87, "end_pos": 115, "type": "TASK", "confidence": 0.7920657396316528}]}, {"text": "In order to accurately identify and ground location entities, for example, we must recognise that metonymic entities constitute false positives and should not be treated the same way as regular locations.", "labels": [], "entities": []}, {"text": "For example, in \"London voted for the change.\", London refers to the concept of \"people\" and should not be classified as a location.", "labels": [], "entities": []}, {"text": "There are many types of metonymy (, however, in this paper, we primarily address metonymic location mentions with reference to GP and NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.5752935409545898}]}, {"text": "Contributions: (1) We investigate how to improve classification tasks by introducing a novel minimalist method called Predicate Window (PreWin), which outperforms common feature selection baselines.", "labels": [], "entities": [{"text": "classification tasks", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.8902422487735748}]}, {"text": "Our final minimalist classifier is comparable to systems which use many external features and tools.", "labels": [], "entities": []}, {"text": "(2) We improve the annotation guidelines in MR and contribute with anew Wikipedia-based MR dataset called ReLocaR to address the training data shortage.", "labels": [], "entities": [{"text": "MR", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9352972507476807}, {"text": "Wikipedia-based MR dataset", "start_pos": 72, "end_pos": 98, "type": "DATASET", "confidence": 0.5776068965593973}, {"text": "ReLocaR", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.47842487692832947}]}, {"text": "(3) We make an annotated subset of the CoNLL 2003 (NER) Shared Task available for extra MR training data, alongside models, tools and other data.", "labels": [], "entities": [{"text": "CoNLL 2003 (NER) Shared Task", "start_pos": 39, "end_pos": 67, "type": "DATASET", "confidence": 0.9436209031513759}, {"text": "MR training", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.8721811473369598}]}], "datasetContent": [{"text": "Our main standard for performance evaluation is the) dataset first introduced in.", "labels": [], "entities": []}, {"text": "Two types of entities were evaluated, organisations and locations, randomly retrieved from the British National Corpus (BNC).", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 95, "end_pos": 124, "type": "DATASET", "confidence": 0.9756092329819998}]}, {"text": "We only use the locations dataset, which comprises a train (925 samples) and a test (908 samples) partition.", "labels": [], "entities": []}, {"text": "For medium evaluation, the classes are literal (geographical territories and political entities), metonymic (place-for-people, place-forproduct, place-for-event, capital-for-government or place-for-organisation) and mixed (metonymic and literal frames invoked simultaneously or unable to distinguish).", "labels": [], "entities": []}, {"text": "The metonymic class further breaks down into two levels of subclasses allowing for fine evaluation.", "labels": [], "entities": [{"text": "fine evaluation", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.6938711404800415}]}, {"text": "The class distribution within SemEval is approx 80% literal, 18% metonymic and 2% mixed.", "labels": [], "entities": []}, {"text": "This seems to be the approximate natural distribution of the classes for location metonymy, which we have also observed while sampling Wikipedia for our new dataset.", "labels": [], "entities": []}, {"text": "SemEval / PreWin 90.6 57.3 SemEval / SOTA 91.6 59.1 ReLocaR / PreWin 84.4 84.8: Per class f-scores -all figures obtained using the Ensemble method, averaged over 10 runs.", "labels": [], "entities": [{"text": "PreWin 84.4 84.8", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.8599788745244344}]}, {"text": "Note the model class bias for SemEval.", "labels": [], "entities": []}, {"text": "shows the SOTA f-scores, our best results for SemEval 2007 and the best f-scores for ReLocaR.", "labels": [], "entities": [{"text": "SemEval 2007", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.6974155604839325}, {"text": "ReLocaR", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.9260081648826599}]}, {"text": "The class imbalance inside SemEval (80% literal, 18% metonymic, 2% mixed) is reflected as a high bias in the final model.", "labels": [], "entities": []}, {"text": "This is not the case with ReLocaR and its 49% literal, 49% metonymic and 2% mixed ratio of 3 classes.", "labels": [], "entities": [{"text": "ReLocaR", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.8643918037414551}]}, {"text": "The model was equally capable of distinguishing between literal and non-literal cases.", "labels": [], "entities": []}], "tableCaptions": []}