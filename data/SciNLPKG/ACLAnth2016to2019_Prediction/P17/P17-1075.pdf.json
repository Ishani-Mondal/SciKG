{"title": [{"text": "Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability", "labels": [], "entities": []}], "abstractContent": [{"text": "Knowing the quality of reading comprehension (RC) datasets is important for the development of natural-language understanding systems.", "labels": [], "entities": []}, {"text": "In this study, two classes of metrics were adopted for evaluating RC datasets: prerequisite skills and readability.", "labels": [], "entities": [{"text": "prerequisite", "start_pos": 79, "end_pos": 91, "type": "METRIC", "confidence": 0.9480865597724915}]}, {"text": "We applied these classes to six existing datasets, including MCTest and SQuAD, and highlighted the characteristics of the datasets according to each metric and the correlation between the two classes.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.9468705058097839}]}, {"text": "Our dataset analysis suggests that the readability of RC datasets does not directly affect the question difficulty and that it is possible to create an RC dataset that is easy to read but difficult to answer.", "labels": [], "entities": []}], "introductionContent": [{"text": "A major goal of natural language processing (NLP) is to develop agents that can understand natural language.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.8210766613483429}]}, {"text": "Such an ability can be tested with a reading comprehension (RC) task that requires the agent to read open-domain documents and answer questions about them.", "labels": [], "entities": []}, {"text": "Constructing systems with RC competence is challenging because RC comprises multiple processes including parsing, understanding cohesion, and inference with linguistic and general knowledge.", "labels": [], "entities": []}, {"text": "Clarifying what a system achieves is important in the development of RC systems.", "labels": [], "entities": []}, {"text": "To achieve robust improvement, systems should be measured according to a variety of metrics beyond simple accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9860174059867859}]}, {"text": "However, a current problem is that most RC datasets are presented only with superficial categories, such as question types (e.g., what, where, and who) and answer types (e.g., numeric, location, and person).", "labels": [], "entities": []}, {"text": "In addition, noted that some questions in datasets may not be suited to the testing of RC systems.", "labels": [], "entities": []}, {"text": "In such ID: SQuAD, United Methodist Church Context: The United Methodist Church (UMC) practices infant and adult baptism.", "labels": [], "entities": [{"text": "United Methodist Church Context", "start_pos": 19, "end_pos": 50, "type": "DATASET", "confidence": 0.8191854506731033}]}, {"text": "Baptized Members are those who have been baptized as an infant or child, but who have not subsequently professed their own faith.", "labels": [], "entities": []}, {"text": "Question: What are members who have been baptized as an infant or child but who have not subsequently professed their own faith?", "labels": [], "entities": []}, {"text": "Answer: Baptized Members ID: MCTest, mc160.dev.8 Context: Sara wanted to play on a baseball team.", "labels": [], "entities": [{"text": "Answer", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9518353939056396}, {"text": "MCTest", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.6619459390640259}]}, {"text": "She had never tried to swing a bat and hit a baseball before.", "labels": [], "entities": []}, {"text": "Her Dad gave her a bat and together they went to the park to practice.", "labels": [], "entities": []}, {"text": "Question: Why was Sara practicing?", "labels": [], "entities": []}, {"text": "Answer: She wanted to play on a team situations, it is difficult to obtain an accurate assessment of the RC system.", "labels": [], "entities": [{"text": "RC system", "start_pos": 105, "end_pos": 114, "type": "DATASET", "confidence": 0.8852664828300476}]}, {"text": "argued that questions that are easy for humans to answer often turnout to be difficult for machines.", "labels": [], "entities": []}, {"text": "For example, consider the two RC questions in.", "labels": [], "entities": []}, {"text": "The first example is from SQuAD (), although the document is taken from a Wikipedia article and was therefore written for adults.", "labels": [], "entities": []}, {"text": "The question is answerable simply by noticing one sentence, without needing to fully understand the content of the text.", "labels": [], "entities": []}, {"text": "On the other hand, consider the second example from MCTest (, which was written for children and is easy to read.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.8950757384300232}]}, {"text": "Here, answering the question involves gathering information from multiple sentences and utilizing a combination of several skills, such as understanding causal relations (Sara wanted...", "labels": [], "entities": []}, {"text": "\u2192 they went to...), coreference resolution (Sara and Her Dad = they), and complementing ellipsis (baseball team = team).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.9371338188648224}]}, {"text": "These two examples show that the readability of the text does not necessarily correlate with the difficulty of answering questions about it.", "labels": [], "entities": []}, {"text": "Furthermore, the accompanying categories of existing RC datasets cannot help with the analysis of this issue.", "labels": [], "entities": [{"text": "RC datasets", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.65537129342556}]}, {"text": "In this study, our goal is to investigate how these two types of difficulty, namely \"answering questions\" and \"reading text,\" are correlated in RC.", "labels": [], "entities": []}, {"text": "Corresponding to each type, we formalize two classes of evaluation metrics, prerequisite skills and readability, and analyze existing RC datasets.", "labels": [], "entities": []}, {"text": "Our intention is to provide the basis of an evaluation methodology of RC systems to help their robust development.", "labels": [], "entities": []}, {"text": "Our two classes of metrics are inspired by the analysis in of human text comprehension in psychology.", "labels": [], "entities": []}, {"text": "They considered two aspects of text comprehension, namely \"strategic/skilled comprehension\" and \"text ease of processing.\"", "labels": [], "entities": [{"text": "ease", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.8663467764854431}]}, {"text": "Our first class defines metrics for \"strategic/skilled comprehension,\" namely the difficulty of comprehending the context when answering questions.", "labels": [], "entities": []}, {"text": "We adopted the set of prerequisite skills that proposed for the finegrained analysis of RC capability.", "labels": [], "entities": []}, {"text": "Their study also presented an important observation of the relation between the difficulty of an RC task and prerequisite skills: the more skills that are required to answer a question, the more difficult is the question.", "labels": [], "entities": []}, {"text": "Based on this observation, in this work, we assume that the number of skills required to answer a question is a reasonable indication of the difficulty of the question.", "labels": [], "entities": []}, {"text": "This is because each skill corresponds to one of the functions of an NLP system, which has to be capable of that functionality.", "labels": [], "entities": []}, {"text": "Our second class defines metrics for \"text ease of processing,\" namely the difficulty of reading the text.", "labels": [], "entities": [{"text": "ease", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.799652099609375}]}, {"text": "We regard it as readability of the text in terms of syntactic and lexical complexity.", "labels": [], "entities": []}, {"text": "From among readability studies in NLP, we adopt a wide range of linguistic features proposed by, which can be used for texts with no available annotations.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "1. We adopt two classes of evaluation metrics to show the qualitative features of RC datasets.", "labels": [], "entities": []}, {"text": "Through analyses of RC datasets, we demonstrate that there is only a weak correlation between the difficulty of questions and the readability of context texts in RC datasets.", "labels": [], "entities": []}, {"text": "2. We revise a previous classification of prerequisite skills for RC.", "labels": [], "entities": [{"text": "RC", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9150336980819702}]}, {"text": "Specifically, skills of knowledge reasoning are organized by using insights of entailment phenomena in NLP and human text comprehension in psychology.", "labels": [], "entities": [{"text": "knowledge reasoning", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7429051399230957}]}, {"text": "3. We annotate six existing RC datasets, compared to the two datasets considered in Sugawara and Aizawa (2016), with our organized metrics being used in the comparison.", "labels": [], "entities": [{"text": "RC datasets", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.6659290194511414}, {"text": "Sugawara and Aizawa (2016)", "start_pos": 84, "end_pos": 110, "type": "TASK", "confidence": 0.6083674629529318}]}, {"text": "We have made the results publicly available 1 and report on the characteristics of the datasets and the differences between them.", "labels": [], "entities": []}, {"text": "We should note that, in this study, RC datasets with different task formulations were annotated with prerequisite skills under the same conditions.", "labels": [], "entities": []}, {"text": "Annotators first saw a context, a question, and its answer.", "labels": [], "entities": []}, {"text": "They selected the sentences required to provide the answer, and then annotated them with appropriate prerequisite skills.", "labels": [], "entities": []}, {"text": "That is, the datasets were annotated from the point of view of whether the context entailed the hypothesis constructed from the pair of the question and answer.", "labels": [], "entities": []}, {"text": "This means that our methodology cannot quantify the systems' competence in searching the context for necessary sentences and answer candidates.", "labels": [], "entities": []}, {"text": "In other words, our methodology can be only used to evaluate the competence of understanding RC questions as contextual entailments.", "labels": [], "entities": []}, {"text": "The remainder of this paper is divided into the following sections.", "labels": [], "entities": []}, {"text": "First, we discuss related work in Section 2.", "labels": [], "entities": []}, {"text": "Next, we specify our two classes of metrics in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, we annotate existing RC datasets with the prerequisite skills.", "labels": [], "entities": [{"text": "RC datasets", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.6598475724458694}]}, {"text": "Section 5 gives the results of our dataset analysis and Section 6 discusses their implications.", "labels": [], "entities": []}, {"text": "Section 7 presents our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present a short history of RC datasets.", "labels": [], "entities": [{"text": "RC datasets", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.7317472398281097}]}, {"text": "To our knowledge, were the first to use NLP methods for RC.", "labels": [], "entities": [{"text": "RC", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9763882160186768}]}, {"text": "Their dataset comprised reading materials for grades 3-6 with simple 5W (wh-) questions.", "labels": [], "entities": []}, {"text": "Subsequent investigations into questions of natural language understanding focused on other formulations, such as question answering) and textual entailment ().", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.6570254961649576}, {"text": "question answering", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.8732174038887024}, {"text": "textual entailment", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.7009503543376923}]}, {"text": "One of the RC tasks of the time was).", "labels": [], "entities": []}, {"text": "The highest accuracy achieved for this task was 59% and the size of the dataset was very limited: there were only 224 gold-standard questions, which is insufficient for machine learning methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9993688464164734}]}, {"text": "This means that an important issue for designing RC datasets is their scalability.", "labels": [], "entities": []}, {"text": "which is an opendomain narrative dataset for gauging comprehension at a child's level.", "labels": [], "entities": []}, {"text": "This dataset was created by crowdsourcing and was based on a scalable methodology.", "labels": [], "entities": []}, {"text": "Since then, additional large-scale datasets have been proposed with the development of machine learning methods in NLP.", "labels": [], "entities": []}, {"text": "For example, the CNN/Daily Mail dataset () and CBTest () have approximately 1.4M and 688K passages, respectively.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 17, "end_pos": 39, "type": "DATASET", "confidence": 0.9133417725563049}, {"text": "CBTest", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.9280152320861816}]}, {"text": "These context texts and questions were automatically curated and generated from large corpora.", "labels": [], "entities": []}, {"text": "However, indicated that approximately 25% of the questions in the CNN/Daily Mail dataset are either unsolvable or nonsensical.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 66, "end_pos": 88, "type": "DATASET", "confidence": 0.941861891746521}]}, {"text": "This dataset-quality issue highlights the demand for more stable and robust sourcing methods.", "labels": [], "entities": [{"text": "sourcing", "start_pos": 76, "end_pos": 84, "type": "TASK", "confidence": 0.9619511961936951}]}, {"text": "Several additional RC datasets were presented in the last half of 2016, involving large documents and sensible queries that were guaranteed by crowdsourcing or other human testing.", "labels": [], "entities": []}, {"text": "They were intended to provide large and high-quality content for machine learning models.", "labels": [], "entities": []}, {"text": "Nonetheless, as shown in the examples of, they were not offered with metrics that could evaluate NLP systems adequately with respect to the difficulty of questions and the surface features of texts.", "labels": [], "entities": []}, {"text": "Following the depiction of text comprehension by, we adopted two classes for the evaluation of RC datasets: prerequisite skills and readability.", "labels": [], "entities": [{"text": "prerequisite", "start_pos": 108, "end_pos": 120, "type": "METRIC", "confidence": 0.9478956460952759}]}, {"text": "For the prerequisite skills class (Section 3.1), we refined RC skills that were proposed by and.", "labels": [], "entities": []}, {"text": "However, a problem in these studies is that their categorization of knowledge reasoning was provisional and with a weak theoretical background.", "labels": [], "entities": [{"text": "categorization of knowledge reasoning", "start_pos": 50, "end_pos": 87, "type": "TASK", "confidence": 0.6504916250705719}]}, {"text": "Therefore, in this study, we reorganized the category of knowledge reasoning in terms of textual entailment in NLP and human text comprehension in psychology.", "labels": [], "entities": [{"text": "knowledge reasoning", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7563206255435944}]}, {"text": "In research on textual entailment, several methodologies have been proposed for the precise analysis of entailment phenomena).", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.6573293954133987}]}, {"text": "In psychology research, as described in Section 2.2, McNamara and Magliano proposed a similar distinction for inferences: bridging versus elaboration.", "labels": [], "entities": []}, {"text": "We utilized these insights in developing a comprehensive but not overly specific classification of knowledge reasoning.", "labels": [], "entities": []}, {"text": "Our prerequisite skills class includes the textbase and situation model.", "labels": [], "entities": []}, {"text": "In our terminology, this means understanding each fact and associating multiple facts in a text, such as the relations of events, characters, or the topic of a story.", "labels": [], "entities": [{"text": "understanding each fact and associating multiple facts in a text, such as the relations of events, characters, or the topic of a story", "start_pos": 31, "end_pos": 165, "type": "Description", "confidence": 0.7531594909154452}]}, {"text": "The skills also involve knowledge reasoning, which is divided into several metrics according to the distinctions of human inferences.", "labels": [], "entities": [{"text": "knowledge reasoning", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7727995216846466}]}, {"text": "This point is discussed by and.", "labels": [], "entities": []}, {"text": "It also accords with the classification of entailment phenomena by and.", "labels": [], "entities": []}, {"text": "Readability metrics (Section 3.2) are quantitative measures used to assess the difficulty of reading, with respect to vocabulary and the complexity of texts.", "labels": [], "entities": []}, {"text": "In this study, they measure the competence in understanding the first basic representation of a text, called the surface code).", "labels": [], "entities": []}, {"text": "We annotated six existing RC datasets with the prerequisite skills.", "labels": [], "entities": [{"text": "RC datasets", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.7340381890535355}]}, {"text": "We explain the annotation procedure in Section 4.1 and the annotated RC datasets in Section 4.2.", "labels": [], "entities": [{"text": "RC datasets", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.7425549477338791}]}, {"text": "As summarized in       the fact that QA4MRE involves technical documents that contain a wide range of knowledge, multiple clauses, and punctuation.", "labels": [], "entities": []}, {"text": "Moreover, the questions are devised by experts.", "labels": [], "entities": []}, {"text": "MCTest achieved a high score for several skills (best for causal relation and meta-knowledge and second-best for coreference resolution and spatiotemporal relation), but a low score for punctuation.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7622857689857483}, {"text": "coreference resolution", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.8980982303619385}]}, {"text": "These scores seem to be because the MCTest dataset consists of narratives.", "labels": [], "entities": [{"text": "MCTest dataset", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.9193284511566162}]}, {"text": "Another dataset that achieved notable scores is Who-did-What.", "labels": [], "entities": []}, {"text": "This dataset achieved the highest score for ellipsis.", "labels": [], "entities": []}, {"text": "This is because the questions of Who-did-What are automatically generated from articles not used as context.", "labels": [], "entities": []}, {"text": "This methodology tends to avoid textual overlap between a question and its context, thereby requiring frequently the skills of ellipsis, bridging, and elaboration.", "labels": [], "entities": []}, {"text": "With regard to nonsense, MS MARCO and Who-did-What received relatively high scores.", "labels": [], "entities": [{"text": "MS MARCO", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.6931672692298889}]}, {"text": "This appears to have been caused by the automated sourcing methods, which may generate a separation between the contents of the context and question (i.e., web segments and a search query in MS MARCO, and a context article and question article in Who-did-What).", "labels": [], "entities": [{"text": "MS MARCO", "start_pos": 191, "end_pos": 199, "type": "DATASET", "confidence": 0.8552618324756622}]}, {"text": "In contrast, NewsQA had no nonsense questions.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.9751591682434082}]}, {"text": "Although this result was affected by our filtering (described in Appendix A), it is important to note that the NewsQA dataset includes annotations of meta-information whether or not a question makes sense (is question bad).", "labels": [], "entities": [{"text": "NewsQA dataset", "start_pos": 111, "end_pos": 125, "type": "DATASET", "confidence": 0.9877766072750092}]}, {"text": "(ii) Number of required prerequisite skills (see): QA4MRE had the highest score.", "labels": [], "entities": [{"text": "QA4MRE", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.5782849192619324}]}, {"text": "On average, each question required 3.25 skills.", "labels": [], "entities": []}, {"text": "There were few questions in QA4MRE that re- quired zero or one skill, whereas such questions were contained more frequently in other datasets.", "labels": [], "entities": [{"text": "QA4MRE", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.9169971942901611}]}, {"text": "also indicates that more than 90% of the MS MARCO questions required fewer than three skills according to the annotation.", "labels": [], "entities": [{"text": "MS MARCO questions", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.724855919679006}]}, {"text": "(iii) Readability metrics for each dataset (see): SQuAD and QA4MRE achieved the highest scores for most metrics.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.7655791640281677}, {"text": "QA4MRE", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.6766366362571716}]}, {"text": "This reflects the fact that Wikipedia articles and technical documents usually require a high-grade level of understanding.", "labels": [], "entities": []}, {"text": "In contrast, MCTest had the lowest scores, with its dataset consisting of narratives for children.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.8310615420341492}]}, {"text": "(iv) Correlation between numbers of required prerequisite skills and readability metrics (see, and): our main interest was in the correlation between prerequisite skills and readability.", "labels": [], "entities": []}, {"text": "To investigate this, we examined the relation between the number of required prerequisite skills and readability metrics.: Pearson's correlation coefficients (r) with the p-values (p) for the readability metrics and number of required prerequisite skills for all questions in the RC datasets.", "labels": [], "entities": [{"text": "RC datasets", "start_pos": 280, "end_pos": 291, "type": "DATASET", "confidence": 0.8081375956535339}]}, {"text": "We used the Flesch-Kincaid grade level () as an intuitive reference for readability.", "labels": [], "entities": [{"text": "Flesch-Kincaid grade level", "start_pos": 12, "end_pos": 38, "type": "METRIC", "confidence": 0.7760956883430481}]}, {"text": "This value represents the typical number of years of education required to understand texts based on counts of syllables, words, and sentences.", "labels": [], "entities": []}, {"text": "show the relation between two values for each dataset and for each question, respectively.", "labels": [], "entities": []}, {"text": "shows the trends of the datasets.", "labels": [], "entities": []}, {"text": "QA4MRE was relatively difficult both to read and to answer, whereas SQuAD was difficult to read but easy to answer.", "labels": [], "entities": [{"text": "QA4MRE", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8221229314804077}]}, {"text": "For further investigation, we selected three datasets (QA4MRE, MCTest, and SQuAD) and plotted all of their questions in.", "labels": [], "entities": [{"text": "QA4MRE", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.7784808278083801}, {"text": "MCTest", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.7230615615844727}]}, {"text": "Three separate domains can be seen.", "labels": [], "entities": []}, {"text": "presents Pearson's correlation coefficients between the number of required prerequisite skills and each readability metric for all questions in the RC datasets.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 9, "end_pos": 30, "type": "METRIC", "confidence": 0.685237874587377}, {"text": "RC datasets", "start_pos": 148, "end_pos": 159, "type": "DATASET", "confidence": 0.7369455695152283}]}, {"text": "Although there are weak correlations, from 0.025 to 0.416, these results demonstrate that there is not necessarily a strong correlation between the two values.", "labels": [], "entities": []}, {"text": "This leads to the following two insights.", "labels": [], "entities": []}, {"text": "First, the readability of RC datasets does not directly affect the difficulty of their questions.", "labels": [], "entities": []}, {"text": "That is, RC datasets that are difficult to read are not necessarily difficult to answer.", "labels": [], "entities": []}, {"text": "Second, it is possible to create difficult questions from the context that are easy to read.", "labels": [], "entities": []}, {"text": "MCTest is a good example.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.954476535320282}]}, {"text": "The context texts in the MCTest dataset are easy to read, but the difficulty of its questions compares to that for the other datasets.", "labels": [], "entities": [{"text": "MCTest dataset", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9476327002048492}]}, {"text": "To summarize our results in terms of each RC dataset, we can make the following observations.", "labels": [], "entities": [{"text": "RC dataset", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.7351382523775101}]}, {"text": "-QA4MRE is difficult both to read and to answer among the datasets analyzed.", "labels": [], "entities": []}, {"text": "This would seem to follow its questions being devised by experts.", "labels": [], "entities": []}, {"text": "-MCTest is a good example of an RC dataset that is easy to read but difficult to answer.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 1, "end_pos": 7, "type": "DATASET", "confidence": 0.902101457118988}]}, {"text": "We presume that this is because the corpus genre (i.e., narrative) reflects the trend in required skills for the questions.", "labels": [], "entities": []}, {"text": "-SQuAD is difficult to read, along with QA4MRE, but relatively easy to answer compared with the other datasets.", "labels": [], "entities": []}, {"text": "-Who-did-What performs well in terms of its query-sourcing method.", "labels": [], "entities": []}, {"text": "Although its questions are created automatically, they are sophisticated in terms of knowledge reasoning.", "labels": [], "entities": []}, {"text": "However, the automated sourcing method must be improved to exclude nonsense questions.", "labels": [], "entities": [{"text": "sourcing", "start_pos": 23, "end_pos": 31, "type": "TASK", "confidence": 0.8998984098434448}]}, {"text": "-MS MARCO is a relatively easy dataset in terms of prerequisite skills.", "labels": [], "entities": [{"text": "MS MARCO", "start_pos": 1, "end_pos": 9, "type": "DATASET", "confidence": 0.6696018725633621}]}, {"text": "However, one problem is that the dataset contained nonsense questions.", "labels": [], "entities": []}, {"text": "-NewsQA is advantageous in that it provides meta-information on the reliability of the questions.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 1, "end_pos": 7, "type": "DATASET", "confidence": 0.9569835662841797}]}, {"text": "Such information enabled us to avoid using nonsense questions, as for the training of machine learning models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Frequencies (%) of prerequisite skills  needed for the RC datasets.", "labels": [], "entities": [{"text": "Frequencies", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9973752498626709}, {"text": "RC datasets", "start_pos": 65, "end_pos": 76, "type": "DATASET", "confidence": 0.756594717502594}]}, {"text": " Table 4: Frequencies (%) of the number of re- quired prerequisite skills for the RC datasets.", "labels": [], "entities": [{"text": "Frequencies", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9984748959541321}, {"text": "RC datasets", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.8129964470863342}]}, {"text": " Table 5: Results of readability metrics for the RC  datasets. F-K is the Flesch-Kincaid grade level  (Kincaid et al., 1975). Words is the average word  count of the context for each question.", "labels": [], "entities": [{"text": "RC  datasets", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.7671749889850616}, {"text": "F-K", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9921173453330994}, {"text": "Flesch-Kincaid grade level", "start_pos": 74, "end_pos": 100, "type": "METRIC", "confidence": 0.9693916241327921}]}, {"text": " Table 6: Pearson's correlation coefficients (r) with  the p-values (p) for the readability metrics and  number of required prerequisite skills for all ques- tions in the RC datasets.", "labels": [], "entities": [{"text": "RC datasets", "start_pos": 171, "end_pos": 182, "type": "DATASET", "confidence": 0.7637800872325897}]}]}