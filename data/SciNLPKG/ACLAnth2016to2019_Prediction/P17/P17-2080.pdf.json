{"title": [{"text": "A Conditional Variational Framework for Dialog Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "Deep latent variable models have been shown to facilitate the response generation for open-domain dialog systems.", "labels": [], "entities": [{"text": "response generation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7422353327274323}]}, {"text": "However , these latent variables are highly ran-domized, leading to uncontrollable generated responses.", "labels": [], "entities": []}, {"text": "In this paper, we propose a framework allowing conditional response generation based on specific attributes.", "labels": [], "entities": [{"text": "conditional response generation", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.6807142098744711}]}, {"text": "These attributes can be either manually assigned or automatically detected.", "labels": [], "entities": []}, {"text": "Moreover , the dialog states for both speakers are modeled separately in order to reflect personal features.", "labels": [], "entities": []}, {"text": "We validate this framework on two different scenarios, where the attribute refers to genericness and sentiment states respectively.", "labels": [], "entities": []}, {"text": "The experiment result testified the potential of our model, where meaningful responses can be generated in accordance with the specified attributes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Seq2seq neural networks, ever since the successful application in machine translation), have demonstrated impressive results on dialog generation and spawned a great deal of variants (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7453409433364868}, {"text": "dialog generation", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.9308224320411682}]}, {"text": "The vanilla seq2seq models suffer from the problem of generating too many generic responses (generic denotes safe, commonplace responses like \"I don't know\").", "labels": [], "entities": []}, {"text": "One major reason is that the element-wise prediction models stochastical variations only at the token level, seducing the system to gain immediate short rewards and neglect the long-term structure.", "labels": [], "entities": []}, {"text": "To * Authors contributed equally.", "labels": [], "entities": []}, {"text": "Correspondence should be sent to H.", "labels": [], "entities": []}, {"text": "Su (suhui15@iscas.ac.cn) and X. Shen (xshen@lsv.uni-saarland.", "labels": [], "entities": []}, {"text": "cope with this problem, () proposed a variational hierarchical encoder-decoder model (VHRED) that brought the idea of variational auto-encoders (VAE) () into dialog generation.", "labels": [], "entities": [{"text": "dialog generation", "start_pos": 158, "end_pos": 175, "type": "TASK", "confidence": 0.8186225593090057}]}, {"text": "For each utterance, VHRED samples a latent variable as a holistic representation so that the generative process will learn to maintain a coherent global sentence structure.", "labels": [], "entities": [{"text": "VHRED", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.7741991877555847}]}, {"text": "However, the latent variable is learned purely in an unsupervised way and can only be explained vaguely as higher level decisions like topic or sentiment.", "labels": [], "entities": []}, {"text": "Though effective in generating utterances with more information content, it lacks the ability of explicitly controlling the generating process.", "labels": [], "entities": []}, {"text": "This paper presents a conditional variational framework for generating specific responses, inspired by the semi-supervised deep generative model ( ).", "labels": [], "entities": []}, {"text": "The principle idea is to generate the next response based on the dialog context, a stochastical latent variable and an external label.", "labels": [], "entities": []}, {"text": "Furthermore, the dialog context for both speakers is modeled separately because they have different talking styles, personality and sentiment.", "labels": [], "entities": []}, {"text": "The whole network structure functions like a conditional VAE (.", "labels": [], "entities": [{"text": "VAE", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9128208160400391}]}, {"text": "We test our framework on two scenarios.", "labels": [], "entities": []}, {"text": "For the first scenario, the label serves as a signal to indicate whether the response is generic or not.", "labels": [], "entities": []}, {"text": "By assigning different values to the label, either generic or non-generic responses can be generated.", "labels": [], "entities": []}, {"text": "For the second scenario, the label represents an imitated sentiment tag.", "labels": [], "entities": []}, {"text": "Before generating the next response, the appropriate sentiment tag is predicted to direct the generating process.", "labels": [], "entities": []}, {"text": "Our framework is expressive and extendable.", "labels": [], "entities": []}, {"text": "The generated responses agree with the predefined labels while maintaining meaningful.", "labels": [], "entities": []}, {"text": "By changing the definition of the label, our framework can be easily applied to other specific areas.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted our experiments on the Ubuntu dialog Corpus (, which contains about 500,000 multi-turn dialogs.", "labels": [], "entities": [{"text": "Ubuntu dialog Corpus", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.9452884991963705}]}, {"text": "The vocabulary was set as the most frequent 20,000 words.", "labels": [], "entities": []}, {"text": "All the letters are transferred to lowercase and the Outof-Vocabulary (OOV) words were preprocessed as <unk> tokens.", "labels": [], "entities": []}, {"text": "Accurate automatic evaluation of dialog generation is difficult (.", "labels": [], "entities": [{"text": "dialog generation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.8116288185119629}]}, {"text": "In our experiment, we conducted three embedding-based evaluations (average, greedy and extrema) () on all our models, which map responses into vector space and compute the cosine similarity.", "labels": [], "entities": []}, {"text": "Though not necessarily accurate, the embedding-based metrics canto a large extent measure the semantic similarity and test the ability of successfully generating a response sharing a similar topic with the golden answer.", "labels": [], "entities": []}, {"text": "The results of a GRU language model (LM), HRED and VHRED were also provided for comparison.", "labels": [], "entities": [{"text": "HRED", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9912583231925964}, {"text": "VHRED", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.7427878975868225}]}, {"text": "For the two scenarios of our framework, we further measured the percentage of generated responses matching the correct labels (accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9992215633392334}]}, {"text": "In (, current popular metrics are shown to be not well correlated with human judgements.", "labels": [], "entities": []}, {"text": "Therefore, we also carried out a human evaluation.", "labels": [], "entities": []}, {"text": "100 examples were randomly sampled from the test dataset.", "labels": [], "entities": []}, {"text": "The generated responses from the models were shuffled and randomly distributed to 5 volunteers 2 . People were requested to give a binary score to the response from 3 aspects, grammaticality, coherence with history context and diversity.", "labels": [], "entities": []}, {"text": "Every response was evaluated 3 times and the result agreed by most people was adopted.", "labels": [], "entities": []}, {"text": "As can be seen from, SPHRED outperforms both HRED and LM overall the three embedding-based metrics.", "labels": [], "entities": [{"text": "SPHRED", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.776479184627533}, {"text": "HRED", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.8159738779067993}]}, {"text": "This implies separating the single-line context RNN into two independent parts can actually lead to a better context representation.", "labels": [], "entities": []}, {"text": "It is worth mentioning the size of context RNN hidden states in SPHRED is only half of that in HRED, but it still behaves better with fewer parameters.", "labels": [], "entities": []}, {"text": "Hence it is reasonable to apply this context information to our framework.", "labels": [], "entities": []}, {"text": "The last 4 rows in display the results of our framework applied in two scenarios mentioned in Section 2.3 and 2.4.", "labels": [], "entities": []}, {"text": "SCENE1-A and SCENE1-B correspond to Scenario 1 with the label fixed as 1 and 0. 90.9% of generated responses in SCENE1-A are generic and 86.9% in SCENE1-B are non-generic according to the manually-built rule, which verified the proper effect of the label.", "labels": [], "entities": []}, {"text": "SCENE2-A and SCENE2-B correspond to rule 1 and 2 in Scenario 2.", "labels": [], "entities": []}, {"text": "Both successfully predict the sentiment with very minor mismatches (0.2% and 0.8%).", "labels": [], "entities": [{"text": "sentiment", "start_pos": 30, "end_pos": 39, "type": "TASK", "confidence": 0.513161301612854}]}, {"text": "The high accuracy further demonstrated SPHRED's capability of maintaining individual context information.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9993682503700256}, {"text": "SPHRED", "start_pos": 39, "end_pos": 45, "type": "TASK", "confidence": 0.8497273921966553}]}, {"text": "We also experimented by substituting the encoder with a normal HRED, the resulting model cannot predict the correct sentiment at all because the context information is highly mingled for both speakers.", "labels": [], "entities": []}, {"text": "The embedding based scores of our framework are still comparable with SPHRED and even better than VHRED.", "labels": [], "entities": [{"text": "SPHRED", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.5481271743774414}, {"text": "VHRED", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.9451082348823547}]}, {"text": "Imposing an external label didn't bring any significant quality decline.: Metric-based Evaluation.", "labels": [], "entities": []}, {"text": "SCENE1-A is set to generate generic responses, so it makes no sense to measure it with embedding-based metrics  We conducted human evaluations on VHRED and our framework).", "labels": [], "entities": [{"text": "VHRED", "start_pos": 146, "end_pos": 151, "type": "DATASET", "confidence": 0.9604455232620239}]}, {"text": "All models share similar scores, except SCENE1-A receiving lower scores with respect to coherence.", "labels": [], "entities": []}, {"text": "This can be explained by the fact that SCENE1-A is trained to generate only generic responses, which limits its power of taking coherence into account.", "labels": [], "entities": []}, {"text": "VHRED and Scenario 2 perform close to each other.", "labels": [], "entities": [{"text": "VHRED", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9677896499633789}]}, {"text": "Scenario 1, due to the effect of the label, receives extreme scores for diversity.", "labels": [], "entities": []}, {"text": "In general, the statistical results of human evaluations on sentence quality are very similar between the VHRED model and our framework.", "labels": [], "entities": [{"text": "VHRED model", "start_pos": 106, "end_pos": 117, "type": "DATASET", "confidence": 0.9240607619285583}]}, {"text": "This agrees with the metric-based results and supports the conclusion drawn in Section 3.3.", "labels": [], "entities": []}, {"text": "Though the sample size is relatively small and human judgements can be inevitably disturbed by subjective factors, we believe these results can shed some light on the understanding of our framework.", "labels": [], "entities": []}, {"text": "A snippet of the generated responses can be: Human Judgements, G refers to Grammaticality and the last four columns is the confusion matrix with respect to coherence and diversity seen in.", "labels": [], "entities": []}, {"text": "Generally speaking, SPHRED better captures the intentions of both speakers, while HRED updates the common context state and the main topic might gradually vanish for the different talking styles of speakers.", "labels": [], "entities": [{"text": "SPHRED", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.5756407976150513}, {"text": "HRED", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9198616147041321}]}, {"text": "SCENE1-A and SCENE1-B are designed to reply to a given context in two different ways.", "labels": [], "entities": []}, {"text": "We can see both responses are reasonable and fit into the right class.", "labels": [], "entities": []}, {"text": "The third and fourth rows are the same context with different appended sentiment tags and rules, both generate a suitable response and append the correct tag at the end.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Metric-based Evaluation. SCENE1-A is  set to generate generic responses, so it makes no  sense to measure it with embedding-based metrics", "labels": [], "entities": []}, {"text": " Table 2: Examples of context-response pairs for the neural network models. eou denotes end-of- utterance and eot denotes end-of-turn token", "labels": [], "entities": []}, {"text": " Table 3: Human Judgements, G refers to Gram- maticality and the last four columns is the confu- sion matrix with respect to coherence and diversity", "labels": [], "entities": [{"text": "Human Judgements", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7464604079723358}]}]}