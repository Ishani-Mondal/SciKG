{"title": [{"text": "AMR-to-text Generation with Synchronous Node Replacement Grammar", "labels": [], "entities": [{"text": "AMR-to-text Generation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8006805181503296}]}], "abstractContent": [{"text": "This paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar.", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.9895611107349396}, {"text": "synchronous node replacement grammar", "start_pos": 70, "end_pos": 106, "type": "TASK", "confidence": 0.743067130446434}]}, {"text": "During training , graph-to-string rules are learned using a heuristic extraction algorithm.", "labels": [], "entities": []}, {"text": "At test time, a graph transducer is applied to collapse input AMRs and generate output sentences.", "labels": [], "entities": []}, {"text": "Evaluated on a standard benchmark , our method gives the state-of-the-art result.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) () is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8476811945438385}]}, {"text": "AMR uses a graph to represent meaning, where nodes (such as \"boy\", \"want-01\") represent concepts, and edges (such as \"ARG0\", \"ARG1\") represent relations between concepts.", "labels": [], "entities": [{"text": "AMR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7280062437057495}]}, {"text": "Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation, question answering (, summarization () and event detection (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8255784213542938}, {"text": "question answering", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.8873565793037415}, {"text": "summarization", "start_pos": 136, "end_pos": 149, "type": "TASK", "confidence": 0.9783220887184143}, {"text": "event detection", "start_pos": 157, "end_pos": 172, "type": "TASK", "confidence": 0.7446470260620117}]}, {"text": "AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations.", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9619927108287811}]}, {"text": "Despite much literature so far on text-to-AMR parsing (, there has been little work on AMR-to-text generation.", "labels": [], "entities": [{"text": "text-to-AMR parsing", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.6535113304853439}, {"text": "AMR-to-text generation", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.9451936185359955}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Rules used for decoding.", "labels": [], "entities": []}]}