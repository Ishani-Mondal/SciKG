{"title": [{"text": "Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation", "labels": [], "entities": [{"text": "Incorporating Word Reordering Knowledge", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7946729063987732}, {"text": "Neural Machine Translation", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.7135511438051859}]}], "abstractContent": [{"text": "This paper proposes three distortion models to explicitly incorporate the word reordering knowledge into attention-based Neural Machine Translation (NMT) for further improving translation performance.", "labels": [], "entities": [{"text": "attention-based Neural Machine Translation (NMT)", "start_pos": 105, "end_pos": 153, "type": "TASK", "confidence": 0.785224437713623}]}, {"text": "Our proposed models enable attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty.", "labels": [], "entities": []}, {"text": "Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve significant translation improvements over a basic attention-based N-MT by large margins.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.7093615829944611}, {"text": "word alignment", "start_pos": 80, "end_pos": 94, "type": "TASK", "confidence": 0.7703876197338104}]}, {"text": "Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 107, "end_pos": 118, "type": "TASK", "confidence": 0.9594066143035889}]}], "introductionContent": [{"text": "Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) () which provides word reordering knowledge to ensure reasonable translation order of source words.", "labels": [], "entities": [{"text": "Word reordering", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7122080475091934}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 67, "end_pos": 104, "type": "TASK", "confidence": 0.8267910778522491}]}, {"text": "It is separately trained and then incorporated into the SMT framework in a pipeline style.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9848986268043518}]}, {"text": "In recent years, end-to-end NMT) has made tremendous progress).", "labels": [], "entities": []}, {"text": "An encoder-decoder framework () with attention mechanism ( is widely used, in which an encoder compresses the source sentence, an attention mechanism evaluates related source words and a decoder generates target words.", "labels": [], "entities": []}, {"text": "The attention mechanism evaluates the distribution of to-be-translated source words in a content-based addressing fashion () which tends to attend to the source words regarding the content relation with current translation status.", "labels": [], "entities": []}, {"text": "Lack of explicit models to exploit the word reordering knowledge may lead to attention faults and generate fluent but inaccurate or inadequate translations.", "labels": [], "entities": []}, {"text": "shows a translation instance and depicts the corresponding word alignment matrix that produced by the attention mechanism.", "labels": [], "entities": []}, {"text": "In this example, even though the word \"zuixin (latest)\" is a common adjective in Chinese and its following word should be translated soon in Chinese to English translation direction, the word \"yiju (evidence)\" does not obtain appropriate attention which leads to the incorrect translation.", "labels": [], "entities": []}, {"text": "src youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de('s) zuixin(latest) yiju(evidence) . ref the report is the latest evidence that supports their arguments . NMT the report supports their perception of the latest . count zuixin yiju {0}: An instance in Chinese-English translation task.", "labels": [], "entities": [{"text": "NMT", "start_pos": 201, "end_pos": 204, "type": "DATASET", "confidence": 0.9055601954460144}]}, {"text": "The row \"count\" represents the frequency of the word collocation in the training corpus.", "labels": [], "entities": []}, {"text": "The collocation \"zuixin yiju\" does not appear in the training data.", "labels": [], "entities": []}, {"text": "To enhance the attention mechanism, implicit word reordering knowledge needs to be incorporated into attention-based NMT.", "labels": [], "entities": []}, {"text": "In this paper, we introduce three distortion models that originated from SMT (, so as to model the word reordering knowledge as the probability distribution of the relative jump distances between the newly translated source word and the to-be-translated source word.", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9572023153305054}]}, {"text": "Our focus is to extend the attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 121, "end_pos": 136, "type": "TASK", "confidence": 0.6543669998645782}]}, {"text": "Our models have three merits: 1.", "labels": [], "entities": []}, {"text": "Our models capture explicit word reordering knowledge to guide the attending process for attention mechanism.", "labels": [], "entities": []}, {"text": "2. Convenient to be incorporated into attention-based NMT.", "labels": [], "entities": []}, {"text": "Our distortion models are differentiable and can be trained in the end-to-end style.", "labels": [], "entities": []}, {"text": "The interpolation approach ensures that the proposed models can coordinately work with the original attention mechanism.", "labels": [], "entities": []}, {"text": "3. Flexible to utilize variant context for computing the word reordering penalty.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.6434554755687714}]}, {"text": "In this paper, we exploit three categories of information as distortion context conditions to compute the word reordering penalty, but variant context information can be utilized due to our model's flexibility.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.6805330663919449}]}, {"text": "We validate our models on the ChineseEnglish translation task and achieve notable improvements: \u2022 On 16K vocabularies, NMT models are usually inferior in comparison with the phrase-based SMT, but our model surpasses phrase-based Moses by average 4.43 BLEU points and outperforms the attention-based NMT baseline system by 5.09 BLEU points.", "labels": [], "entities": [{"text": "ChineseEnglish translation task", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.8378624518712362}, {"text": "SMT", "start_pos": 187, "end_pos": 190, "type": "TASK", "confidence": 0.6653459668159485}, {"text": "BLEU", "start_pos": 251, "end_pos": 255, "type": "METRIC", "confidence": 0.9953619837760925}, {"text": "BLEU", "start_pos": 327, "end_pos": 331, "type": "METRIC", "confidence": 0.9962833523750305}]}, {"text": "\u2022 On 30K vocabularies, the improvements over the phrase-based Moses and the attention-based NMT baseline system are average 6.06 and 1.57 BLEU points respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9961782693862915}]}, {"text": "\u2022 Compared with previous work on identical corpora, we achieve the state-of-theart translation performance on average.", "labels": [], "entities": []}, {"text": "The word alignment quality evaluation shows that our model can effectively improve the word alignment quality that is crucial for improving translation quality.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.817604124546051}, {"text": "word alignment", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.7336126565933228}]}], "datasetContent": [{"text": "We carry the translation task on the ChineseEnglish direction to evaluate the effectiveness of our models.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9824522733688354}, {"text": "ChineseEnglish direction", "start_pos": 37, "end_pos": 61, "type": "DATASET", "confidence": 0.9761146306991577}]}, {"text": "To investigate the word alignment quality, we take the word alignment quality evaluation on the manually aligned corpus.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.7286703288555145}, {"text": "word alignment", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.6955680102109909}]}, {"text": "We also conduct the experiments to observe effects of hyper-parameters and the training strategies.) is calculated to assess the word alignment quality.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 129, "end_pos": 143, "type": "TASK", "confidence": 0.7131712287664413}]}], "tableCaptions": [{"text": " Table 2: BLEU-4 scores (%) on NIST test set 03-06 of Moses (default settings), Groundhog  (default settings), RNNsearch  *  and RNNsearch  *  with distortion models respectively. The val- ues in brackets are increases on RNNsearch  *  , Moses and Groundhog respectively.  \u2021 indicates  statistical significant difference (p<0.01) from RNNsearch  *  and  \u2020 means statistical significant  difference (p<0.05) from RNNsearch  *  .", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9996224641799927}, {"text": "NIST test set 03-06", "start_pos": 31, "end_pos": 50, "type": "DATASET", "confidence": 0.9795826077461243}]}, {"text": " Table 3: Comparison with previous work on identical training corpora. Coverage (Tu et al.,  2016) is a basic RNNsearch model with a coverage model to alleviate the over-translation and  under-translation problems. MEMDEC (Wang et al., 2016) is to improve translation quality  with external memory. NMT IA (Meng et al., 2016) exploits a readable and writable attention  mechanism to keep track of interactive history in decoding. Our work is NMT with H-Distortion  model. The vocabulary sizes of all work are 30K and maximum lengths of sentence differ.", "labels": [], "entities": []}, {"text": " Table 4: BLEU-4 scores (%) and AER scores  on Tsinghua manually aligned Chinese-English  evaluation set. The lower the AER score, the  better the alignment quality.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9996092915534973}, {"text": "AER", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9996961355209351}, {"text": "Tsinghua manually aligned Chinese-English  evaluation set", "start_pos": 47, "end_pos": 104, "type": "DATASET", "confidence": 0.588057150443395}, {"text": "AER", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9989124536514282}]}, {"text": " Table 3. Our work evidently outper- forms previous work on average performance.  Although we restrict the maximum length of  sentence to 50, our model achieves the state-", "labels": [], "entities": []}, {"text": " Table 5: Comparison between pre-training and no pre-training H-Distortion model. The per- formances are consistent.", "labels": [], "entities": []}]}