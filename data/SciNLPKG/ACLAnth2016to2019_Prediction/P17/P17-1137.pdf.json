{"title": [{"text": "Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling", "labels": [], "entities": [{"text": "Neural Language Modeling", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.6257119675477346}]}], "abstractContent": [{"text": "Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types.", "labels": [], "entities": []}, {"text": "Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the \"bursty\" distribution of such words.", "labels": [], "entities": []}, {"text": "In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words.", "labels": [], "entities": []}, {"text": "To validate our model we construct anew open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus; MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.", "labels": [], "entities": [{"text": "Multilingual Wikipedia Corpus; MWC)", "start_pos": 86, "end_pos": 121, "type": "DATASET", "confidence": 0.8259190917015076}]}], "introductionContent": [{"text": "Language modeling is an important problem in natural language processing with many practical applications (translation, speech recognition, spelling autocorrection, etc.).", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7455182671546936}, {"text": "natural language processing", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.6735363006591797}, {"text": "translation", "start_pos": 107, "end_pos": 118, "type": "TASK", "confidence": 0.970573902130127}, {"text": "speech recognition", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.6960403621196747}]}, {"text": "Recent advances in neural networks provide strong representational power to language models with distributed representations and unbounded dependencies based on recurrent networks (RNNs).", "labels": [], "entities": []}, {"text": "However, most language models operate by generating words by sampling from a closed vocabulary which is composed of the most frequent words in a corpus.", "labels": [], "entities": []}, {"text": "Rare tokens are typically replaced by a special token, called the unknown word token, UNK.", "labels": [], "entities": [{"text": "UNK", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.6928169131278992}]}, {"text": "Although fixedvocabulary language models have some important practical applications and are appealing models for study, they fail to capture two empirical facts about the distribution of words in natural languages.", "labels": [], "entities": []}, {"text": "First, vocabularies keep growing as the number of documents in a corpus grows: new words are constantly being created.", "labels": [], "entities": []}, {"text": "Second, rare and newly created words often occur in \"bursts\", i.e., once anew or rare word has been used once in a document, it is often repeated).", "labels": [], "entities": []}, {"text": "The open-vocabulary problem can be solved by dispensing with word-level models in favor of models that predict sentences as sequences of characters.", "labels": [], "entities": []}, {"text": "Character-based models are quite successful at learning what (new) word forms look like (e.g., they learn a language's orthographic conventions that tell us that sustinated is a plausible English word and bzoxqir is not) and, when based on models that learn long-range dependencies such as RNNs, they can also be good models of how words fit together to form sentences.", "labels": [], "entities": []}, {"text": "However, existing character-sequence models have no explicit mechanism for modeling the fact that once a rare word is used, it is likely to be used again.", "labels": [], "entities": []}, {"text": "In this paper, we propose an extension to character-level language models that enables them to reuse previously generated tokens ( \u00a72).", "labels": [], "entities": []}, {"text": "Our starting point is a hierarchical LSTM that has been previously used for modeling sentences (word by word) in a conversation (, except here we model words (character by character) in a sentence.", "labels": [], "entities": []}, {"text": "To this model, we add a caching mechanism similar to recent proposals for caching that have been advocated for closed-vocabulary models (.", "labels": [], "entities": []}, {"text": "As word tokens are generated, they are placed in an LRU cache, and, at each time step the model decides whether to copy a previously generated word from the cache or to generate it from scratch, character by character.", "labels": [], "entities": []}, {"text": "The decision of whether to use the cache or not is a latent variable that is marginalised during learning and inference.", "labels": [], "entities": []}, {"text": "In summary, our model has three properties: it creates new words, it accounts for their burstiness using a cache, and, being based on LSTM s over word representations, it can model long range dependencies.", "labels": [], "entities": []}, {"text": "To evaluate our model, we perform ablation experiments with variants of our model without the cache or hierarchical structure.", "labels": [], "entities": []}, {"text": "In addition to standard English data sets (PTB and WikiText-2), we introduce anew multilingual data set: the Multilingual Wikipedia Corpus (MWC), which is constructed from comparable articles from Wikipedia in 7 typologically diverse languages ( \u00a73) and show the effectiveness of our model in all languages ( \u00a74).", "labels": [], "entities": [{"text": "PTB", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.8947020173072815}, {"text": "Multilingual Wikipedia Corpus (MWC)", "start_pos": 109, "end_pos": 144, "type": "DATASET", "confidence": 0.796490877866745}]}, {"text": "By looking at the posterior probabilities of the generation mechanism (language model vs. cache) on held-out data, we find that the cache is used to generate \"bursty\" word types such as proper names, while numbers and generic content words are generated preferentially from the language model ( \u00a75).", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on a range of datasets, employing preexisting benchmarks for comparison to previous published results, and anew multilingual corpus which specifically tests our model's performance across a range of typological settings.", "labels": [], "entities": []}, {"text": "We now turn to a series of experiments to show the value of our hierarchical character-level cache language model.", "labels": [], "entities": []}, {"text": "For each dataset we trained the model with LSTM units.", "labels": [], "entities": []}, {"text": "To compare our results with a strong baseline, we also train a model without the cache.", "labels": [], "entities": []}, {"text": "Model Configuration For HCLM and HCLM with cache models, We used 600 dimensions for the character embeddings and the LSTMs have 600 hidden units for all the experiments.", "labels": [], "entities": []}, {"text": "This keeps the model complexity to be approximately the same as previous works which used an LSTM with 1000 dimension.", "labels": [], "entities": []}, {"text": "Our baseline LSTM have 1000 dimensions for embeddings and reccurence weights.", "labels": [], "entities": []}, {"text": "For the cache model, we used cache size 100 in every experiment.", "labels": [], "entities": []}, {"text": "All the parameters including character projection parameters are randomly sampled from uniform distribution from \u22120.08 to 0.08.", "labels": [], "entities": []}, {"text": "The initial hidden and memory state of LSTM enc and LSTM ctx are initialized with zero.", "labels": [], "entities": []}, {"text": "Mini-batches of size 25 are used for PTB experiments and 10 for WikiText-2, due to memory limitations.", "labels": [], "entities": [{"text": "PTB experiments", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.8695663809776306}]}, {"text": "The sequences were truncated with 35 words.", "labels": [], "entities": []}, {"text": "Then the words are decomposed to characters and fed into the model.", "labels": [], "entities": []}, {"text": "A Dropout rate of 0.5 was used for all but the recurrent connections.", "labels": [], "entities": [{"text": "Dropout rate", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.9786953330039978}]}, {"text": "Learning The models were trained with the Adam update rule () with a learning rate of 0.002.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 69, "end_pos": 82, "type": "METRIC", "confidence": 0.9717698395252228}]}, {"text": "The maximum norm of the gradients was clipped at 10.", "labels": [], "entities": [{"text": "clipped", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9852301478385925}]}, {"text": "Evaluation We evaluated our models with bitsper-character (bpc) a standard evaluation metric where |c| is the length of the corpus in characters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: PTB Corpus Statistics.", "labels": [], "entities": [{"text": "PTB Corpus Statistics", "start_pos": 10, "end_pos": 31, "type": "DATASET", "confidence": 0.9379929502805074}]}, {"text": " Table 2: WikiText-2 Corpus Statistics.", "labels": [], "entities": [{"text": "WikiText-2 Corpus Statistics", "start_pos": 10, "end_pos": 38, "type": "DATASET", "confidence": 0.9157130320866903}]}, {"text": " Table 3: Summary of MWC Corpus.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.7353611588478088}, {"text": "MWC Corpus", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.7547867894172668}]}, {"text": " Table 4: Results on PTB Corpus (bits-per- character). HCLM augmented with a cache obtains  the best results among models which have approx- imately the same numbers of parameter as single  layer LSTM with 1,000 hidden units.", "labels": [], "entities": [{"text": "PTB Corpus", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.8390269875526428}]}, {"text": " Table 5: Results on WikiText-2 Corpus .", "labels": [], "entities": [{"text": "WikiText-2 Corpus", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.9527520537376404}]}, {"text": " Table 6: Results on MWC Corpus (bits-per-character).", "labels": [], "entities": [{"text": "MWC Corpus", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.9247443377971649}]}, {"text": " Table 7: Word types with the highest/lowest av- erage posterior probability of having been copied  from the cache while generating the test set. The  probability tells whether the model used the cache  given a word and its context. Left: Cache is  used for frequent words (the, of ) and proper nouns  (Lesnar, Gore). Right: Character level generation  is used for basic words and numbers.", "labels": [], "entities": [{"text": "av- erage posterior probability", "start_pos": 45, "end_pos": 76, "type": "METRIC", "confidence": 0.7165397346019745}, {"text": "Character level generation", "start_pos": 325, "end_pos": 351, "type": "TASK", "confidence": 0.6340440809726715}]}]}