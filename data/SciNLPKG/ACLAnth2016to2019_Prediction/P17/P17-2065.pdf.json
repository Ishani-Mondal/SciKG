{"title": [{"text": "Automatic Compositor Attribution in the First Folio of Shakespeare", "labels": [], "entities": [{"text": "Automatic Compositor Attribution in the First Folio of Shakespeare", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.6712688853343328}]}], "abstractContent": [{"text": "Compositor attribution, the clustering of pages in a historical printed document by the individual who set the type, is a bib-liographic task that relies on analysis of orthographic variation and inspection of visual details of the printed page.", "labels": [], "entities": [{"text": "Compositor attribution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7208967506885529}]}, {"text": "In this paper, we introduce a novel unsuper-vised model that jointly describes the tex-tual and visual features needed to distinguish compositors.", "labels": [], "entities": []}, {"text": "Applied to images of Shakespeare's First Folio, our model predicts attributions that agree with the manual judgements of bibliographers with an accuracy of 87%, even on text that is the output of OCR.", "labels": [], "entities": [{"text": "Shakespeare's First Folio", "start_pos": 21, "end_pos": 46, "type": "DATASET", "confidence": 0.6742570176720619}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.998934805393219}, {"text": "OCR", "start_pos": 196, "end_pos": 199, "type": "DATASET", "confidence": 0.9097996950149536}]}], "introductionContent": [{"text": "Within literary studies, the field of bibliography has an unusually long tradition of quantitative analysis.", "labels": [], "entities": []}, {"text": "One particularly relevant area is that of compositor attribution-the clustering of pages in a historical printed document by the individual (the compositor) who set the type.", "labels": [], "entities": [{"text": "compositor attribution-the clustering of pages in a historical printed document", "start_pos": 42, "end_pos": 121, "type": "TASK", "confidence": 0.8335357695817948}]}, {"text": "Like stylometry, a long-standing area of NLP that has largely focused on attributing the authorship of text, the analysis of orthographic patterns is fundamental to compositor attribution.", "labels": [], "entities": []}, {"text": "Additionally, compositor attribution often makes use of visual features, such as whitespace layout, introducing new challenges.", "labels": [], "entities": []}, {"text": "These analyses have traditionally been done by hand, but efforts are painstaking due to the difficulty of manually recording these features.", "labels": [], "entities": []}, {"text": "In this paper, we present an unsupervised model specifically designed for compositor attribution that incorporates both textual and visual sources of evidence traditionally used by bibliographers.", "labels": [], "entities": []}, {"text": "Our model jointly describes the patterns of variation both in orthography and in the whitespace between glyphs, allowing us to cluster pages by discovering patterns of similarity and difference.", "labels": [], "entities": []}, {"text": "When applied to digital scans of historical printed documents, our approach learns orthographic and whitespace preferences of individual compositors and predicts groupings of pages set by the same compositor.", "labels": [], "entities": []}, {"text": "1 This is, to our knowledge, the first attempt to perform compositor attribution automatically.", "labels": [], "entities": []}, {"text": "Prior work has proposed automatic approaches to authorship attributionwhich is typically viewed as the supervised problem of identifying a particular author given samples of their writing.", "labels": [], "entities": [{"text": "authorship attributionwhich", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.7834397852420807}]}, {"text": "In contrast, compositor attribution lacks supervision because compositors are unknown and, in addition, focuses on different linguistic patterns.", "labels": [], "entities": [{"text": "compositor attribution", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7676154375076294}]}, {"text": "We explain spellings of words conditioned on word choice, not the word choice itself.", "labels": [], "entities": []}, {"text": "Figure 2: In our model, a compositor ci is generated for page i from a multinomial prior.", "labels": [], "entities": []}, {"text": "Then, each diplomatic word, dij, is generated conditioned on ci and the corresponding modern word, mij, from a distribution parameterized by weight vector wc.", "labels": [], "entities": []}, {"text": "Finally, each medial comma spacing width (measured in pixels), s ik , is generated conditioned on ci from a distribution parameterized by \u03b8c.", "labels": [], "entities": []}, {"text": "To evaluate our approach, we fit our model to digital scans of Shakespeare's First Folio (1623)-a document with well established manual judgements of compositor attribution.", "labels": [], "entities": [{"text": "Shakespeare's First Folio (1623)-a document", "start_pos": 63, "end_pos": 106, "type": "DATASET", "confidence": 0.8762773010465834}]}, {"text": "We find that even when relying on noisy OCR transcriptions of textual content, our model predicts compositor attributions that agree with manual annotations 87% of the time, outperforming several simpler baselines.", "labels": [], "entities": []}, {"text": "Our approach opens new possibilities for considering patterns across a larger vocabulary of words and at a higher visual resolution than has been possible historically.", "labels": [], "entities": []}, {"text": "Such a tool may enable scalable first-pass analysis in understudied domains as a complement to humanistic studies of composition.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data: To evaluate our model when it has access to perfectly transcribed historical text, we use the Bodleian diplomatic transcription of the First Folio.", "labels": [], "entities": [{"text": "Bodleian diplomatic transcription of the First Folio", "start_pos": 100, "end_pos": 152, "type": "DATASET", "confidence": 0.9219767195837838}]}, {"text": "3 To test whether our approach can also work with untranscribed books, we ran the Ocular OCR system) on the Bodleian facsimile images to create an automatic diplomatic transcription.", "labels": [], "entities": [{"text": "Bodleian facsimile images", "start_pos": 108, "end_pos": 133, "type": "DATASET", "confidence": 0.9898358583450317}, {"text": "diplomatic transcription", "start_pos": 157, "end_pos": 181, "type": "TASK", "confidence": 0.7277183830738068}]}, {"text": "In both cases, we used Ocular's estimates of glyph bounding boxes on the complete First Folio images to extract spacing information.", "labels": [], "entities": [{"text": "First Folio images", "start_pos": 82, "end_pos": 100, "type": "DATASET", "confidence": 0.9427420099576315}]}, {"text": "The modern text was taken from MIT Complete Works of Shakespeare 4 and was aligned with diplomatic transcriptions by running a word-level edit distance calculation.", "labels": [], "entities": [{"text": "MIT Complete Works of Shakespeare 4", "start_pos": 31, "end_pos": 66, "type": "DATASET", "confidence": 0.9350400368372599}]}, {"text": "The extracted substitutions form the model's observed modern-diplomatic word pairs.", "labels": [], "entities": []}, {"text": "Evaluation: To compare the recovered attribution with those proposed by bibliographers, we evaluate against an authoritative attribution compiled by Peter Blayney (1996) which includes the work of various scholars.", "labels": [], "entities": []}, {"text": "We also evaluate our system against an earlier, highly influential model proposed by, which we approximate by reverting certain compositor divisions in Blayney's attribution.", "labels": [], "entities": []}, {"text": "Hinman's attribution posited five compositors, while Blayney's posited eight.", "labels": [], "entities": []}, {"text": "In experiments, we set the model's maximum number of compositors to C = 5 when evaluating on Hinman's attribution, and use C = 8 BASIC model variant: We evaluate a simple baseline model that uses a multinomial parameterization for generating diplomatic words and does not incorporate spacing information.", "labels": [], "entities": []}, {"text": "We use two different options for selection of spelling variants to be considered by the model.", "labels": [], "entities": []}, {"text": "First, we consider only the three words selected by Hinman: do, go and here (referred to as HINMAN).", "labels": [], "entities": [{"text": "HINMAN", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9346508383750916}]}, {"text": "Second, we use a larger, automatically selected, word list (referred to as AUTO).", "labels": [], "entities": [{"text": "AUTO", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.6379683613777161}]}, {"text": "Here, we select all modern words with frequency greater than 70 that are not names and that exhibit sufficient variance in diplomatic spellings (most common diplomatic spelling occurs in less than 80% of aligned tokens).", "labels": [], "entities": []}, {"text": "For our full model, described in the next section, we always use the larger AUTO word list.", "labels": [], "entities": [{"text": "AUTO word list", "start_pos": 76, "end_pos": 90, "type": "DATASET", "confidence": 0.6271945238113403}]}, {"text": "FEAT model variant: We run experiments with several variants of our full model, described in Section 3 (referred to as FEAT since they use a feature-based parameterization of diplomatic word generation.)", "labels": [], "entities": [{"text": "FEAT model", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7674072980880737}, {"text": "FEAT", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.6890902519226074}, {"text": "diplomatic word generation", "start_pos": 175, "end_pos": 201, "type": "TASK", "confidence": 0.6612708369890848}]}, {"text": "We try ablations of WORD and EDIT features, as well as model variants with and without the spacing generation component (referred to as SPACE.)", "labels": [], "entities": []}, {"text": "We refer to the full model that includes both types of features and spacing generation as ALL.", "labels": [], "entities": [{"text": "spacing generation", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.6700358539819717}]}], "tableCaptions": [{"text": " Table 1: The experi- mental results for all se- tups of the model. In the  experiments with BASIC  model, we compare the  short HINMAN word list  with the automatically  filtered AUTO word list.  We show results for sev- eral variants of our full  model, labeled as FEAT,  both with and without  spacing generation. A  random baseline is in- cluded for comparison.", "labels": [], "entities": [{"text": "FEAT", "start_pos": 267, "end_pos": 271, "type": "METRIC", "confidence": 0.9400955438613892}]}]}