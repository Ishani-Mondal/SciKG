{"title": [{"text": "Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "We study asymmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal.", "labels": [], "entities": []}, {"text": "The open-ended dialogue state in this setting poses new challenges for existing dialogue systems.", "labels": [], "entities": []}, {"text": "We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements.", "labels": [], "entities": []}, {"text": "To model both struc-tured knowledge and unstructured language , we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses.", "labels": [], "entities": []}, {"text": "Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current task-oriented dialogue systems ( require a pre-defined dialogue state (e.g., slots such as food type and price range fora restaurant searching task) and a fixed set of dialogue acts (e.g., request, inform).", "labels": [], "entities": []}, {"text": "However, human conversation often requires richer dialogue states and more nuanced, pragmatic dialogue acts.", "labels": [], "entities": []}, {"text": "Recent opendomain chat systems () learn a mapping directly from previous utterances to the next utterance.", "labels": [], "entities": []}, {"text": "While these models capture open-ended aspects of dialogue, the lack of structured dialogue state prevents them from being directly applied to settings that require interfacing with structured knowledge.", "labels": [], "entities": []}, {"text": "In order to bridge the gap between the two types: An example dialogue from the MutualFriends task in which two agents, A and B, each given a private list of a friends, try to identify their mutual friend.", "labels": [], "entities": []}, {"text": "Our objective is to build an agent that can perform the task with a human.", "labels": [], "entities": []}, {"text": "Crosstalk (Section 2.3) is italicized. of systems, we focus on asymmetric collaborative dialogue setting, which is task-oriented but encourages open-ended dialogue acts.", "labels": [], "entities": []}, {"text": "In our setting, two agents, each with a private list of items with attributes, must communicate to identify the unique shared item.", "labels": [], "entities": []}, {"text": "Consider the dialogue in 1, in which two people are trying to find their mutual friend.", "labels": [], "entities": []}, {"text": "By asking \"do you have anyone who went to columbia?\", B is suggesting that she has some Columbia friends, and that they probably work at Google.", "labels": [], "entities": [{"text": "B", "start_pos": 54, "end_pos": 55, "type": "METRIC", "confidence": 0.9681753516197205}]}, {"text": "Such conversational implicature is lost when interpreting the utterance as simply an information request.", "labels": [], "entities": []}, {"text": "In addition, it is hard to define a structured state that captures the diverse semantics in many utterances (e.g., defining \"most of\", \"might be\"; see details in).", "labels": [], "entities": []}, {"text": "To model both structured and open-ended context, we propose the Dynamic Knowledge Graph Network (DynoNet), in which the dialogue state is modeled as a knowledge graph with an embedding for each node (Section 3).", "labels": [], "entities": []}, {"text": "Our model is similar to EntNet) in that node/entity embeddings are updated recurrently given new utterances.", "labels": [], "entities": []}, {"text": "The difference is that we structure entities as a knowledge graph; as the dialogue proceeds, new nodes are added and new context is propagated on the graph.", "labels": [], "entities": []}, {"text": "An attention-based mechanism () over the node embeddings drives generation of new utterances.", "labels": [], "entities": []}, {"text": "Our model's use of knowledge graphs captures the grounding capability of classic task-oriented systems and the graph embedding provides the representational flexibility of neural models.", "labels": [], "entities": []}, {"text": "The naturalness of communication in the symmetric collaborative setting enables large-scale data collection: We were able to crowdsource around 11K human-human dialogues on Amazon Mechanical Turk (AMT) in less than 15 hours.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 173, "end_pos": 201, "type": "DATASET", "confidence": 0.9354905982812246}]}, {"text": "We show that the new dataset calls for more flexible representations beyond fully-structured states (Section 2.2).", "labels": [], "entities": []}, {"text": "In addition to conducting the third-party human evaluation adopted by most work (,c), we also conduct partner evaluation () where AMT workers rate their conversational partners (other workers or our models) based on fluency, correctness, cooperation, and human-likeness.", "labels": [], "entities": []}, {"text": "We compare DynoNet with baseline neural models and a strong rulebased system.", "labels": [], "entities": []}, {"text": "The results show that DynoNet can perform the task with humans efficiently and naturally; it also captures some strategic aspects of human-human dialogues.", "labels": [], "entities": []}, {"text": "The contributions of this work are: (i) anew symmetric collaborative dialogue setting and a large dialogue corpus that pushes the boundaries of existing dialogue systems; (ii) DynoNet, which integrates semantically rich utterances with structured knowledge to represent open-ended dialogue states; (iii) multiple automatic metrics based on bot-bot chat and a comparison of third-party and partner evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We show the basic statistics of our dataset in.", "labels": [], "entities": []}, {"text": "An utterance is defined as a message sent by one of the agents.", "labels": [], "entities": []}, {"text": "The average utterance length is short due to the informality of the chat, however, an agent usually sends multiple utterances in one turn.", "labels": [], "entities": []}, {"text": "Some example dialogues are shown in  We categorize utterances into coarse typesinform, ask, answer, greeting, apology-by pattern matching (Appendix E).", "labels": [], "entities": []}, {"text": "There are 7.4% multitype utterances, and 30.9% utterances contain more than one entity.", "labels": [], "entities": []}, {"text": "In, we show example utterances with rich semantics that cannot be sufficiently represented by traditional slot-values.", "labels": [], "entities": []}, {"text": "Some of the standard ones are also non-trivial due to coreference and logical compositionality.", "labels": [], "entities": []}, {"text": "Our dataset also exhibits some interesting communication phenomena.", "labels": [], "entities": []}, {"text": "Coreference occurs frequently when people check multiple attributes of one item.", "labels": [], "entities": []}, {"text": "Sometimes mentions are dropped, as an utterance simply continues from the partner's utterance.", "labels": [], "entities": []}, {"text": "People occasionally use external knowledge to group items with out-of-schema attributes (e.g., gender based on names, location based on schools).", "labels": [], "entities": []}, {"text": "We summarize these phenomena in.", "labels": [], "entities": []}, {"text": "In addition, we find 30% utterances involve cross-talk where the conversation does not progress linearly (e.g., italic utterances in), a common characteristic of online chat.", "labels": [], "entities": []}, {"text": "One strategic aspect of this task is choosing the order of attributes to mention.", "labels": [], "entities": []}, {"text": "We find that people tend to start from attributes with fewer unique values, e.g., \"all my friends like morning\" given the KB B in, as intuitively it would help exclude items quickly given fewer values to check.", "labels": [], "entities": [{"text": "KB B", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.8922606408596039}]}, {"text": "We provide a more detailed analysis of strategy in Section 4.2 and Appendix F.  We compare our model with a rule-based system and a baseline neural model.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.930114209651947}]}, {"text": "Both automatic and human evaluations are conducted to test the models in terms of fluency, correctness, cooperation, and human-likeness.", "labels": [], "entities": [{"text": "correctness", "start_pos": 91, "end_pos": 102, "type": "METRIC", "confidence": 0.967019259929657}]}, {"text": "The results show that DynoNet is able to converse with humans in a coherent and strategic way.", "labels": [], "entities": []}, {"text": "We test our systems in two interactive settings: bot-bot chat and bot-human chat.", "labels": [], "entities": []}, {"text": "We perform both automatic evaluation and human evaluation.", "labels": [], "entities": []}, {"text": "First, we compute the cross-entropy (`) of a model on test data.", "labels": [], "entities": []}, {"text": "As shown in, DynoNet has the lowest test loss.", "labels": [], "entities": [{"text": "DynoNet", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.8889157176017761}]}, {"text": "Next, we have a model chat with itself on the scenarios from the test set.", "labels": [], "entities": []}, {"text": "We evaluate the chats with respect to language variation, effectiveness, and strategy.", "labels": [], "entities": []}, {"text": "For language variation, we report the average utterance length Lu and the unigram entropy H in.", "labels": [], "entities": [{"text": "utterance length Lu", "start_pos": 46, "end_pos": 65, "type": "METRIC", "confidence": 0.6740141610304514}, {"text": "unigram entropy H", "start_pos": 74, "end_pos": 91, "type": "METRIC", "confidence": 0.7116439342498779}]}, {"text": "Compared to Rule, the neural models tend to generate shorter utterances ().", "labels": [], "entities": []}, {"text": "However, they are more diverse; for example, questions are asked in multiple ways such as \"Do you have ...\", \"Any friends like ...\", \"What about ...\".", "labels": [], "entities": []}, {"text": "At the discourse level, we expect the distribution of a bot's utterance types to match the distribution of human's.", "labels": [], "entities": []}, {"text": "We show percentages of each utterance type in.", "labels": [], "entities": []}, {"text": "For Rule, the decision about which action to take is written in the rules, while StanoNet and DynoNet learned to behave in a more human-like way, frequently informing and asking questions.", "labels": [], "entities": []}, {"text": "To measure effectiveness, we compute the overall success rate (C) and the success rate per turn (C T ) and per selection (C S ).", "labels": [], "entities": []}, {"text": "As shown in, humans are the best at this game, followed by Rule which is comparable to DynoNet.", "labels": [], "entities": [{"text": "Rule", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.7386566400527954}]}, {"text": "Next, we investigate the strategies leading to these results.", "labels": [], "entities": []}, {"text": "An agent needs to decide which entity/attribute to check first to quickly reduce the search space.", "labels": [], "entities": []}, {"text": "We hypothesize that humans tend to first focus on a majority entity and an attribute with fewer unique values (Section 2.3).", "labels": [], "entities": []}, {"text": "For example, in the scenario in  To examine the overall strategy, we show the average number of attributes (#Attr) and entities (#Ent) mentioned during the conversation in Table 4.", "labels": [], "entities": []}, {"text": "Humans and DynoNet strategically focus on a few attributes and entities, whereas Rule needs almost twice entities to achieve similar success rates.", "labels": [], "entities": []}, {"text": "This suggests that the effectiveness of Rule mainly comes from large amounts of unselective information, which is consistent with comments from their human partners.", "labels": [], "entities": [{"text": "Rule", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.9375658631324768}]}, {"text": "We generated 200 new scenarios and put up the bots on AMT using the same chat interface that was used for data collection.", "labels": [], "entities": [{"text": "AMT", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.9372813105583191}]}, {"text": "The bots follow simple turn-taking rules explained in Appendix H. Each AMT worker is randomly paired with Rule, StanoNet, DynoNet, or another human (but the worker doesn't know which), and we make sure that all four types of agents are tested in each scenario at least once.", "labels": [], "entities": []}, {"text": "At the end of each dialogue, humans are asked to rate their partner in terms of fluency, correctness, cooperation, and human-likeness from 1 (very bad) to 5 (very good), along with optional comments.", "labels": [], "entities": [{"text": "correctness", "start_pos": 89, "end_pos": 100, "type": "METRIC", "confidence": 0.9663962125778198}, {"text": "cooperation", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9570004940032959}]}, {"text": "We show the average ratings (with significance tests) in and the histograms in Appendix J. In terms of fluency, the models have similar performance since the utterances are usually short.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.8636341691017151}]}, {"text": "Judgment on correctness is a mere guess since the evaluator cannot seethe partner's KB; we will analyze correctness more meaningfully in the thirdparty evaluation below.", "labels": [], "entities": [{"text": "Judgment", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8846956491470337}, {"text": "KB", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.8566014170646667}]}, {"text": "Noticeably, DynoNet is more cooperative than the other models.", "labels": [], "entities": []}, {"text": "As shown in the example dialogues in, DynoNet cooperates smoothly with the human partner, e.g., replying with relevant information about morning/indoor friends when the partner mentioned that all her friends prefer morning and most like indoor.", "labels": [], "entities": []}, {"text": "StanoNet starts well but doesn't followup on the morning friend, presumably because the morning node is not updated dynamically when mentioned by the partner.", "labels": [], "entities": []}, {"text": "Rule follows the partner poorly.", "labels": [], "entities": []}, {"text": "In the comments, the biggest complaint about Rule was that it was not 'listening' or 'understanding'.", "labels": [], "entities": [{"text": "Rule", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.9169559478759766}]}, {"text": "Overall, DynoNet achieves better partner satisfaction, especially in cooperation.", "labels": [], "entities": []}, {"text": "We also created a third-party evaluation task, where an independent AMT worker is shown a conversation and the KB of one of the agents; she is asked to rate the same aspects of the agent as in the partner evaluation and provide justifications.", "labels": [], "entities": [{"text": "KB", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.8396473526954651}]}, {"text": "Each agent in a dialogue is rated by at least 5 people.", "labels": [], "entities": []}, {"text": "The average ratings and histograms are shown in and Appendix J. For correctness, we see that Rule has the best performance since it always tells the truth, whereas humans can make mistakes due to carelessness and the neural models can generate false information.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9941754937171936}]}, {"text": "For example, in, DynoNet 'lied' when saying that it has a morning friend who likes outdoor.", "labels": [], "entities": []}, {"text": "Surprisingly, there is a discrepancy between the two evaluation modes in terms of cooperation and human-likeness.", "labels": [], "entities": []}, {"text": "Manual analysis of the comments indicates that third-party evaluators focus lesson the dialogue strategy and more on linguistic features, probably because they were not fully engaged in the dialogue.", "labels": [], "entities": []}, {"text": "For example, justification  for cooperation often mentions frequent questions and timely answers, less attention is paid to what is asked about though.", "labels": [], "entities": []}, {"text": "For human-likeness, partner evaluation is largely correlated with coherence (e.g., not repeating or ignoring past information) and task success, whereas third-party evaluators often rely on informality (e.g., usage of colloquia like \"hiya\", capitalization, and abbreviation) or intuition.", "labels": [], "entities": []}, {"text": "Interestingly, third-party evaluators noted most phenomena listed in as indicators of humanbeings, e.g., correcting oneself, making chit-chat other than simply finishing the task.", "labels": [], "entities": []}, {"text": "See example comments in Appendix K.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Statistics of the MutualFriends dataset.", "labels": [], "entities": [{"text": "MutualFriends dataset", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.9883900284767151}]}, {"text": " Table 4: Automatic evaluation on human-human and bot-bot chats on test scenarios. We use \" / # to  indicate that higher / lower values are better; otherwise the objective is to match humans' statistics. Best  results (except Human) are in bold. Neural models generate shorter (lower L u ) but more diverse (higher  H) utterances. Overall, their distributions of utterance types match those of the humans'. (We only show  the most frequent speech acts therefore the numbers do not sum to 1.) Rule is effective in completing  the task (higher C S ), but it is not information-efficient given the large number of attributes (#Attr) and  entities (#Ent) mentioned.", "labels": [], "entities": []}, {"text": " Table 5: Results on human-bot/human chats. Best results (except Human) in each column are in bold.  We report the average ratings of each system. For third-party evaluation, we first take mean of each  question then average the ratings. DynoNet has the best partner satisfaction in terms of fluency (Flnt),  correctness (Crct), cooperation (Coop), human likeness (Human). The superscript of a result indicates that  its advantage over other systems (r: Rule, s: StanoNet, d: DynoNet) is statistically significant with  p < 0.05 given by paired t-tests.", "labels": [], "entities": [{"text": "Flnt", "start_pos": 301, "end_pos": 305, "type": "METRIC", "confidence": 0.9496220350265503}, {"text": "correctness (Crct)", "start_pos": 309, "end_pos": 327, "type": "METRIC", "confidence": 0.9126875102519989}, {"text": "cooperation", "start_pos": 329, "end_pos": 340, "type": "METRIC", "confidence": 0.9548871517181396}]}]}