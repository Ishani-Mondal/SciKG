{"title": [{"text": "Integrating Deep Linguistic Features in Factuality Prediction over Unified Datasets", "labels": [], "entities": [{"text": "Factuality Prediction", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.7558053731918335}]}], "abstractContent": [{"text": "Previous models for the assessment of commitment towards a predicate in a sentence (also known as factuality prediction) were trained and tested against a specific annotated dataset, subsequently limiting the generality of their results.", "labels": [], "entities": [{"text": "assessment of commitment towards a predicate in a sentence", "start_pos": 24, "end_pos": 82, "type": "TASK", "confidence": 0.7527478668424818}, {"text": "factuality prediction)", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.7633026043574015}]}, {"text": "In this work we propose an intuitive method for mapping three previously annotated corpora onto a single factuality scale, thereby enabling models to be tested across these corpora.", "labels": [], "entities": []}, {"text": "In addition, we design a novel model for factuality prediction by first extending a previous rule-based factuality prediction system and applying it over an abstraction of dependency trees, and then using the output of this system in a supervised classifier.", "labels": [], "entities": [{"text": "factuality prediction", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.8417535424232483}]}, {"text": "We show that this model outperforms previous methods on all three datasets.", "labels": [], "entities": []}, {"text": "We make both the unified factu-ality corpus and our new model publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Factuality prediction is the task of determining the level of commitment towards a predicate in a sentence according to a specific source, e.g., the author.", "labels": [], "entities": [{"text": "Factuality prediction is the task of determining the level of commitment towards a predicate in a sentence according to a specific source, e.g., the author", "start_pos": 0, "end_pos": 155, "type": "Description", "confidence": 0.7975809265066076}]}, {"text": "For instance, the author uses linguistic cues to mark the embedded proposition as factual in (1) (cue: surprising), as uncertain in (2) and (3) (cues: risk, might), and as counterfactual (cue: did not manage) or uncertain (cue: will not manage) in (4).", "labels": [], "entities": []}, {"text": "(1) It is not surprising that they work.", "labels": [], "entities": []}, {"text": "(2) She takes the risk to find out the truth.", "labels": [], "entities": []}, {"text": "(3) She might find out the truth.", "labels": [], "entities": []}, {"text": "(4) He did/will not manage to be in time.", "labels": [], "entities": []}, {"text": "Detecting factuality is hard as the linguistic means used to express it closely interact.", "labels": [], "entities": [{"text": "Detecting factuality", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8864068388938904}]}, {"text": "For example, lexical cues, such as the proposition-embedding predicates in (1) and (4) interact with negation (in (1), (4)) and tense (in (4)).", "labels": [], "entities": []}, {"text": "Detecting factuality has many potential applications.", "labels": [], "entities": [{"text": "Detecting factuality", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9054400622844696}]}, {"text": "For instance, in knowledge base population, only propositions marked as factual should be admitted into the knowledge base, while hypothetical or negated ones should be left out.", "labels": [], "entities": []}, {"text": "Similarly, for argumentation analysis and question answering, factuality can play a major role in backing a specific claim or supporting evidence for an answer to a question at hand.", "labels": [], "entities": [{"text": "argumentation analysis", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.9188613295555115}, {"text": "question answering", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.9103098213672638}]}, {"text": "Recent research efforts have approached the factuality task from two complementing directions: automatic prediction and large scale annotation.", "labels": [], "entities": [{"text": "automatic prediction", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.6841686815023422}]}, {"text": "Previous attempts for automatic factuality prediction either took a rule-based, deep syntactic approach () or a machine learning approach over more shallow features (.", "labels": [], "entities": [{"text": "factuality prediction", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.8496083915233612}]}, {"text": "In terms of annotation, each effort was largely carried out independently of the others, picking up different factuality flavors and different annotation scales.", "labels": [], "entities": []}, {"text": "In correlation, the proposed algorithms have targeted a single annotated resource which they aim to recover.", "labels": [], "entities": []}, {"text": "Subsequently, this separation between annotated corpora has prevented a comparison across datasets.", "labels": [], "entities": []}, {"text": "Further, the models are nonportable, inhibiting advancements in one dataset to carryover to any of the other annotations.", "labels": [], "entities": []}, {"text": "Our contribution in this work is twofold.", "labels": [], "entities": []}, {"text": "First, we suggest that the task can benefit from a unified representation.", "labels": [], "entities": []}, {"text": "We exemplify this by mapping the representation of two recent datasets and MEAN-TIME () onto the [\u22123, +3] scale, as annotated by (.", "labels": [], "entities": [{"text": "MEAN-TIME", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9314864873886108}]}, {"text": "This unification allows us to test the generality of mod-els which were previously applicable on a single dataset.", "labels": [], "entities": []}, {"text": "Second, we design anew model for factuality prediction that extends, which employed implicative signatures over dependency trees using a large predicate lexicon.", "labels": [], "entities": [{"text": "factuality prediction", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.8483400344848633}]}, {"text": "We first extend TruthTeller's lexicon by about 40% through a semi-automatic process (following).", "labels": [], "entities": []}, {"text": "We then apply TruthTeller's rules over an abstraction of dependency trees (, which represents predicate-argument structures more consistently, thereby allowing TruthTeller rules to apply on a wider range of syntactic constructions.", "labels": [], "entities": []}, {"text": "Finally, we surpass previous methods by using the output from TruthTeller as deep linguisticallyinformed features in a supervised classifier, thus successfully integrating a rule-based approach in a machine learning framework.", "labels": [], "entities": []}, {"text": "Overall, we hope that our unified representation will enable training and testing on larger, more diverse datasets, and that the good performance of our new model indicates its usability across different flavors of factuality prediction.", "labels": [], "entities": [{"text": "factuality prediction", "start_pos": 215, "end_pos": 236, "type": "TASK", "confidence": 0.828538328409195}]}, {"text": "We make both the unified factuality corpus and the new model publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the experiments we carried out on the three unified datasets (FactBank, MEANTIME, and UW).", "labels": [], "entities": [{"text": "FactBank", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.9068586826324463}, {"text": "MEANTIME", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9368667006492615}, {"text": "UW", "start_pos": 114, "end_pos": 116, "type": "DATASET", "confidence": 0.943691074848175}]}, {"text": "For a fair comparison, we use the same train, development, test split of the datasets for all systems.", "labels": [], "entities": []}, {"text": "We preprocess the data with the spaCy Python library.", "labels": [], "entities": [{"text": "spaCy Python library", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.9198108315467834}]}, {"text": "In all our experiments we compute the metrics used in: (1) Mean Absolute Error 5 (MAE), which computes the absolute fit of the model and (2) Pearson correlation coefficient between automatic predictions and gold labels, especially informative in biased test sets as it assesses how well the model captures the variability in the gold data.", "labels": [], "entities": [{"text": "Mean Absolute Error 5 (MAE)", "start_pos": 59, "end_pos": 86, "type": "METRIC", "confidence": 0.9543716907501221}, {"text": "Pearson correlation coefficient", "start_pos": 141, "end_pos": 172, "type": "METRIC", "confidence": 0.9714810252189636}]}], "tableCaptions": [{"text": " Table 1: Factuality annotation statistics and mappings used in this paper -the number of tokens and  sentences in each corpus, the original factuality value with the corresponding converted value to UW  scale, the type of annotation (discrete or continuous), the annotators' proficiency, and the perspective to  which the annotation refers.  \u2020 This is an abstraction over the original MEANTIME annotation (suggested  by the MEANTIME authors), which is composed of polarity, certainty and temporality.", "labels": [], "entities": [{"text": "UW  scale", "start_pos": 200, "end_pos": 209, "type": "METRIC", "confidence": 0.5747301876544952}, {"text": "MEANTIME authors", "start_pos": 425, "end_pos": 441, "type": "DATASET", "confidence": 0.9007398784160614}, {"text": "certainty", "start_pos": 475, "end_pos": 484, "type": "METRIC", "confidence": 0.9903185367584229}]}, {"text": " Table 2: Performance of the baselines against  our new supervised model (bottom).  \u2020 The perfor- mance of UW features on MEANTIME and Fact- Bank uses a different solver from that in", "labels": [], "entities": []}]}