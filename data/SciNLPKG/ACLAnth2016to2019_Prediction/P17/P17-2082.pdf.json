{"title": [], "abstractContent": [{"text": "One expensive step when defining crowd-sourcing tasks is to define the examples and control questions for instructing the crowd workers.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a self-training strategy for crowd-sourcing.", "labels": [], "entities": []}, {"text": "The main idea is to use an automatic classifier, trained on weakly supervised data, to select examples associated with high confidence.", "labels": [], "entities": []}, {"text": "These are used by our automatic agent to explain the task to crowd workers with a question answering approach.", "labels": [], "entities": [{"text": "question answering", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7065110206604004}]}, {"text": "We compared our relation extraction system trained with data annotated (i) with distant supervision and (ii) by workers instructed with our approach.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.8383155465126038}]}, {"text": "The analysis shows that our method relatively improves the relation extraction system by about 11% in F1.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8882794082164764}, {"text": "F1", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.9957692623138428}]}], "introductionContent": [{"text": "Recently, the Relation Extraction (RE) task has attracted the attention of many researchers due to its wide range of applications such as question answering, text summarization and bio-medical text mining.", "labels": [], "entities": [{"text": "Relation Extraction (RE) task", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.927896241346995}, {"text": "question answering", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.9026458859443665}, {"text": "text summarization", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.7970608472824097}, {"text": "bio-medical text mining", "start_pos": 181, "end_pos": 204, "type": "TASK", "confidence": 0.7211604317029318}]}, {"text": "The aim of this task is to identify the type of relation between two entities in a given text.", "labels": [], "entities": []}, {"text": "Most work on RE has mainly regarded the application of supervised methods, which require costly annotation, especially for large-scale datasets.", "labels": [], "entities": [{"text": "RE", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9909362196922302}]}, {"text": "To overcome the annotation problem, firstly proposed to collect automatic annotation through Distant Supervision (DS).", "labels": [], "entities": [{"text": "Distant Supervision (DS)", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.6907879710197449}]}, {"text": "In the DS setting, the training data for RE is often automatically annotated utilizing an external Knowledge-Base (KB) such as Wikipedia or Freebase (.", "labels": [], "entities": [{"text": "RE", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9656146168708801}]}, {"text": "Although DS has shown to be promising for RE, it also produces many noisy labels in the automatic annotated data, which deteriorate the performance of the system trained on it. showed that by simply adding a small set of high quality labeled instances (i.e., human-annotated training data) to a larger set of instances annotated by DS, makes the overall precision of the system significantly increases.", "labels": [], "entities": [{"text": "RE", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9739891290664673}, {"text": "precision", "start_pos": 354, "end_pos": 363, "type": "METRIC", "confidence": 0.9987610578536987}]}, {"text": "Such level of quality of the labels usually can be obtained at low cost via crowdsourcing.", "labels": [], "entities": []}, {"text": "However, this finding does not hold for more complex tasks, where the annotators 1 need to have some expertise on them.", "labels": [], "entities": []}, {"text": "For instance in RE, several works have shown that only a marginal improvement can be achieved via crowdsourcing the data ().", "labels": [], "entities": [{"text": "RE", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9306187629699707}]}, {"text": "In such papers, the wellknown Gold Standard quality control mechanism was used without annotators being trained.", "labels": [], "entities": [{"text": "Gold Standard quality control", "start_pos": 30, "end_pos": 59, "type": "DATASET", "confidence": 0.8584436625242233}]}, {"text": "Very recently, despite the previous results, showed a larger improvement for the RE task when training crowd workers in an interactive tutorial procedure called \"Gated Instruction\".", "labels": [], "entities": [{"text": "RE task", "start_pos": 81, "end_pos": 88, "type": "TASK", "confidence": 0.8486490249633789}]}, {"text": "This approach, however, requires a set of high-quality labeled data (i.e., the Gold Standard) for providing the instruction and feedback to the crowd workers.", "labels": [], "entities": [{"text": "Gold Standard", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.6416555941104889}]}, {"text": "However, acquiring such data requires a considerable amount of human effort.", "labels": [], "entities": []}, {"text": "In this paper, we propose to alternatively use Silver Standard, i.e., a high-quality automatic annotated data, to train the crowd workers.", "labels": [], "entities": []}, {"text": "Specifically, we introduce a self-training strategy for crowd-sourcing, where the workers are first trained with simpler examples (which we assume to be less noisy) and then gradually presented with more difficult ones.", "labels": [], "entities": []}, {"text": "This is biologically inspired by the common human process of gradual learn- Moreover, we propose an iterative humanmachine co-training framework for the task of RE.", "labels": [], "entities": [{"text": "RE", "start_pos": 161, "end_pos": 163, "type": "TASK", "confidence": 0.9106730222702026}]}, {"text": "The main idea is (i) to automatically select a subset of less-noisy examples applying an automatic classifier, (ii) training the annotators with such subset, and (iii) iterating this process after retraining the classifiers using the annotated data.", "labels": [], "entities": []}, {"text": "That is, the educated crowd workers can provide higher quality annotations, which can be used by the system in the next iteration to improve the quality of its classification.", "labels": [], "entities": []}, {"text": "In other words, this cycle gradually improves both system and human annotators.", "labels": [], "entities": []}, {"text": "This is inline with the studies in human-based computational approaches, which showed that the crowd intelligence can effectively alleviate the drifting problem in auto-annotation systems (.", "labels": [], "entities": []}, {"text": "Our study shows that even without using any gold standard, we can still train workers and their annotations can achieve results comparable with the more costly state-of-the-art methods.", "labels": [], "entities": []}, {"text": "In summary our contributions are the following: \u2022 we introduce a self-training strategy for crowdsourcing; \u2022 we propose an iterative human-machine cotraining framework for the task of RE; and \u2022 we test our approach on a standard benchmark, obtaining a slightly lower performance compared to the state-of-the-art methods based on Gold Standard data.", "labels": [], "entities": [{"text": "RE", "start_pos": 184, "end_pos": 186, "type": "TASK", "confidence": 0.8920305371284485}, {"text": "Gold Standard data", "start_pos": 329, "end_pos": 347, "type": "DATASET", "confidence": 0.9368384877840678}]}, {"text": "This study opens up avenues for exploiting inexpensive crowdsourcing solutions similar to ours to achieve performance gain in NLP tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first introduce the details of the used corpora, then explain the feature extraction and RE pipeline and finally present the experiments and discuss the results in detail.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.6777394562959671}, {"text": "RE pipeline", "start_pos": 109, "end_pos": 120, "type": "METRIC", "confidence": 0.7801920771598816}]}, {"text": "In the first set of experiments, we verified the quality of our Silver Standard set used in our selftraining methods.", "labels": [], "entities": [{"text": "Silver Standard set", "start_pos": 64, "end_pos": 83, "type": "DATASET", "confidence": 0.7332233786582947}]}, {"text": "For this purpose, we trained MultiR on CS I , CS Q and CS A and evaluate them on our test set.", "labels": [], "entities": []}, {"text": "illustrates the results in terms of Precision, Recall and F1 for each partition separately.", "labels": [], "entities": [{"text": "Precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9992545247077942}, {"text": "Recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9970709085464478}, {"text": "F1", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.9987395405769348}]}, {"text": "They suggest that, the extractors trained on CS I and CS Q are significantly better than the extractor trained on the lower part of CS, i.e., CS A , even if the latter is much larger than the other two (80% vs. 10%).", "labels": [], "entities": []}, {"text": "In the next set of experiments, we evaluated the impact of adding a small set of crowdsourced data to a large set of instances annotated by Distant Supervision.", "labels": [], "entities": []}, {"text": "We conducted the RE experiments in this setting, as this allowed us to directly compare with.", "labels": [], "entities": [{"text": "RE", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.7749035358428955}]}, {"text": "Thus, we used CS A annotated by our proposed method along with the noisy annotated DS to train the extractor.", "labels": [], "entities": []}, {"text": "We compared our method with (i) the DS-only baseline and (ii) the state of the art, Gated Instruction (GI) strategy (.", "labels": [], "entities": []}, {"text": "We emphasize that the same set of examples (both DS and CS) are used in this experiment and just replaced the GI annotations with the annotations collected using our proposed framework.", "labels": [], "entities": []}, {"text": "Models DS-only Our Model GI Accuracy 56% 82% 91%: Annotation Accuracy of crowd workers The results are shown in.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9710059762001038}, {"text": "Annotation", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9627395868301392}, {"text": "Accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.6199462413787842}]}, {"text": "Our method improves the DS-only baseline by 7%, 5% and 2% (absolute) in Precision, Recall and F1, respectively.", "labels": [], "entities": [{"text": "DS-only baseline", "start_pos": 24, "end_pos": 40, "type": "METRIC", "confidence": 0.7881164252758026}, {"text": "Precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9887855052947998}, {"text": "Recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9772476553916931}, {"text": "F1", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.9972246885299683}]}, {"text": "This improvement clearly confirms the benefit of our fully automatic approach to crowdsourcing in RE task.", "labels": [], "entities": [{"text": "RE task", "start_pos": 98, "end_pos": 105, "type": "TASK", "confidence": 0.8385054171085358}]}, {"text": "Additionally, our model is just 3% lower than the GI method in terms of F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9989148378372192}]}, {"text": "In both our method and GI, the crowd workers are trained before enrolling in the main task.", "labels": [], "entities": [{"text": "GI", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9330437183380127}]}, {"text": "However, GI trains annotators using Gold Standard data, which involves a higher level of supervision with respect to our method.", "labels": [], "entities": [{"text": "Gold Standard data", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.8449368874231974}]}, {"text": "Thus our self-training method is potentially effective and an inexpensive alternative to GI.", "labels": [], "entities": []}, {"text": "We also analyzed the accuracy of the crowd workers in terms of the quality of their annotations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.999100923538208}]}, {"text": "For this purpose, we randomly selected 100 sentences from CS A and then had them manually annotated by an expert.", "labels": [], "entities": []}, {"text": "We compared the accuracy of the annotations collected with our proposed approach with those provided by DS-only baseline and the GI method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9993919134140015}]}, {"text": "shows the results: the annotations performed by workers trained with our method are just slightly less accurate than the annotations produced by annotators trained with GI.", "labels": [], "entities": [{"text": "accurate", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9613804817199707}, {"text": "GI", "start_pos": 169, "end_pos": 171, "type": "DATASET", "confidence": 0.7656223177909851}]}, {"text": "This outcome is inline with the positive impact of our good quality annotation on the RE performance.", "labels": [], "entities": [{"text": "RE", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.7490552663803101}]}], "tableCaptions": []}