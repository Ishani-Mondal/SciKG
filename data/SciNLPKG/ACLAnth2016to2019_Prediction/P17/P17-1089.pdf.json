{"title": [{"text": "Learning a Neural Semantic Parser from User Feedback", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an approach to rapidly and easily build natural language interfaces to databases for new domains, whose performance improves overtime based on user feedback, and requires minimal intervention.", "labels": [], "entities": []}, {"text": "To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations.", "labels": [], "entities": []}, {"text": "These models are immediately deployed online to solicit feedback from real users to flag incorrect queries.", "labels": [], "entities": []}, {"text": "Finally, the popularity of SQL facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models.", "labels": [], "entities": []}, {"text": "This complete feedback loop, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers.", "labels": [], "entities": []}, {"text": "Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a semantic parser for an online academic database from scratch.", "labels": [], "entities": []}], "introductionContent": [{"text": "Existing semantic parsing approaches for building natural language interfaces to databases (NLIDBs) either use special-purpose intermediate meaning representations that lack the full expressivity of database query languages or require extensive feature engineering, making it difficult to deploy them in new domains.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7877911329269409}]}, {"text": "We present a robust approach to quickly and easily learn and deploy semantic parsers from scratch, whose performance improves overtime based on user feedback, and requires very little expert intervention.", "labels": [], "entities": []}, {"text": "To learn these semantic parsers, we (1) adapt neural sequence models to map utterances directly to SQL thereby bypassing intermediate representations and taking full advantage of SQL's querying capabilities, (2) immediately deploy the model online to solicit questions and user feedback on results to reduce SQL annotation efforts, and (3) use crowd workers from skilled markets to provide SQL annotations that can directly be used for model improvement, in addition to being easier and cheaper to obtain than logical meaning representations.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of the complete approach by successfully learning a semantic parser for an academic domain by simply deploying it online for three days.", "labels": [], "entities": []}, {"text": "This type of interactive learning is related to a number of recent ideas in semantic parsing, in-cluding batch learning of models that directly produce programs (e.g., regular expressions (), learning from paraphrases (often gathered through crowdsourcing (), data augmentation (e.g. based on manually engineered semantic grammars) and learning through direct interaction with users (e.g., where a single user teaches the model new concepts ().", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.7231809198856354}]}, {"text": "However, there are unique advantages to our approach, including showing (1) that non-linguists can write SQL to encode complex, compositional computations (see an example), (2) that external paraphrase resources and the structure of facts from the target database itself can be used for effective data augmentation, and (3) that actual database users can effectively drive the overall learning by simply providing feedback about what the model is currently getting correct.", "labels": [], "entities": []}, {"text": "Our experiments measure the performance of these learning advances, both in batch on existing datasets and through a simple online experiment for the full interactive setting.", "labels": [], "entities": []}, {"text": "For the batch evaluation, we use sentences from the benchmark GeoQuery and ATIS domains, converted to contain SQL meaning representations.", "labels": [], "entities": [{"text": "ATIS domains", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.7802308797836304}]}, {"text": "Our neural learning with data augmentation achieves reasonably high accuracies, despite the extra complexities of mapping directly to SQL.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9870556592941284}]}, {"text": "We also perform simulated interactive learning on this data, showing that with perfect user feedback our full approach could learn high quality parsers with only 55% of the data.", "labels": [], "entities": []}, {"text": "Finally, we do a small scale online experiment fora new domain, academic paper metadata search, demonstrating that actual users can provide useful feedback and our full approach is an effective method for learning a high quality parser that continues to improve overtime as it is used.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our first set of experiments demonstrates that our semantic parsing model has comparable accuracy to previous work, despite the increased difficulty of directly producing SQL.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7125935256481171}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9988870024681091}]}, {"text": "We demonstrate this result by running our model on two benchmark datasets for semantic parsing, GEO880 and ATIS.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.7560272514820099}, {"text": "GEO880", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.9752682447433472}, {"text": "ATIS", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.7444196939468384}]}, {"text": "We follow a standard train/dev/test methodology for our experiments.", "labels": [], "entities": []}, {"text": "The training set is augmented using schema templates and 3 paraphrases per training example, as described in Section 4.", "labels": [], "entities": []}, {"text": "Utterances were anonymized by replacing them with their corresponding types and all words that occur only once were replaced by UNK symbols.", "labels": [], "entities": []}, {"text": "The development set is used for hyperparameter tuning and early stopping.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.8209725022315979}, {"text": "early stopping", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.655808225274086}]}, {"text": "For GEO880, we use cross validation on the training set to tune hyperparameters.", "labels": [], "entities": [{"text": "GEO880", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.9630990624427795}]}, {"text": "We used a minibatch size of 100 and used Adam () with a learning rate of 0.001 for 70 epochs for all our experiments.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 56, "end_pos": 69, "type": "METRIC", "confidence": 0.9694394469261169}]}, {"text": "We used abeam size of 5 for decoding.", "labels": [], "entities": []}, {"text": "We report test set accuracy of our SQL query predictions by executing them on the target database and comparing the result with the true result.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9914052486419678}]}, {"text": "show test accuracies based on denotations for our model on GEO880 and ATIS respectively, compared with previous work.", "labels": [], "entities": [{"text": "GEO880", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.9845734238624573}, {"text": "ATIS", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.8185960650444031}]}, {"text": "To our knowledge, this is the first result on directly parsing to SQL to achieve comparable performance to prior work without using any database-specific feature engineering. and also directly produce SQL queries but on a subset of 700 examples from GEO880.", "labels": [], "entities": [{"text": "GEO880", "start_pos": 250, "end_pos": 256, "type": "DATASET", "confidence": 0.9885724782943726}]}, {"text": "The former only works on semantically tractable utterances where words can be un-  In this section, we learn a semantic parser for an academic domain from scratch by deploying an online system using our interactive learning algorithm (Section 3).", "labels": [], "entities": []}, {"text": "After three train-deploy cycles, the system correctly answered 63.51% of user's questions.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first effort to learn a semantic parser using a live system, and is enabled by our models that can directly parse language to SQL without manual intervention.", "labels": [], "entities": []}, {"text": "In this experiment, using our developed user interface, we use Algorithm 1 to learn a semantic parser from scratch.", "labels": [], "entities": []}, {"text": "The experiment had three stages; in each stage, we recruited 10 new users (computer science graduate students) and asked them to issue at least 10 utterances each to the system and to provide feedback on the results.", "labels": [], "entities": []}, {"text": "We considered results marked as either Correct or Incomplete Result as correct queries for learning.", "labels": [], "entities": [{"text": "Correct", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9708815813064575}]}, {"text": "The remaining incorrect utterances were sent to a crowd worker for annotation and were used to retrain the system for the next stage.", "labels": [], "entities": []}, {"text": "The crowd worker had prior experience in writing SQL queries and was hired from Upwork after completing a short SQL test.", "labels": [], "entities": []}, {"text": "The worker was also given access to the database to be able to execute the queries and ensure that they are correct.", "labels": [], "entities": []}, {"text": "For the first stage, the system was trained using 640 examples generated using templates, that were augmented to 1746 examples using paraphrasing (see Section 4.3).", "labels": [], "entities": []}, {"text": "The complexity of the utterances issued in each of the three phases were comparable, in that, the average length of the correct SQL query for the utterances, and the number of tables required to be queried, were similar.", "labels": [], "entities": []}, {"text": "shows the percent of utterances judged by users as either Correct or Incomplete Result in each stage.", "labels": [], "entities": [{"text": "Correct", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9547017216682434}]}, {"text": "In the first stage, we do not have any labeled examples, and the model is trained using only synthetically generated data from schema templates and paraphrases (see Section 4.3).", "labels": [], "entities": []}, {"text": "Despite the lack of real examples, the system correctly answers 25% of questions.", "labels": [], "entities": []}, {"text": "The system's accuracy increases and annotation effort decreases in each successive stage as additional utterances are contributed and incorrect utterances are labeled.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9992961883544922}]}, {"text": "This result demonstrates that we can successfully build semantic parsers for new domains by using neural models to generate SQL with crowdsourced annotations driven by user feedback.", "labels": [], "entities": []}, {"text": "We analyzed the feedback signals provided by the users in the final stage of the experiment to measure the quality of feedback.", "labels": [], "entities": []}, {"text": "We found that 22.3% of the generated queries did not execute (and hence were incorrect).", "labels": [], "entities": []}, {"text": "6.1% of correctly generated queries were marked wrong by users (see).", "labels": [], "entities": []}, {"text": "This erroneous feedback results in redundant annotation of already correct examples.", "labels": [], "entities": []}, {"text": "The main cause of this erroneous feedback was incomplete data for aggregation queries, where users chose Wrong instead of Incomplete.", "labels": [], "entities": []}, {"text": "6.3% of incorrect queries were erroneously deemed correct by users.", "labels": [], "entities": []}, {"text": "It is important that this fraction below, as these queries become incorrectly-labeled exam-  We release anew semantic parsing dataset for academic database search using the utterances gathered in the user study.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 109, "end_pos": 125, "type": "TASK", "confidence": 0.7785956561565399}]}, {"text": "We augment these labeled utterances with additional utterances labeled by crowd workers.", "labels": [], "entities": []}, {"text": "(Note that these additional utterances were not used in the online experiment).", "labels": [], "entities": []}, {"text": "The final dataset comprises 816 natural language utterances labeled with SQL, divided into a 600/216 train/test split.", "labels": [], "entities": []}, {"text": "We also provide a database on which to execute these queries containing academic papers with their authors, citations, journals, keywords and datasets used.", "labels": [], "entities": []}, {"text": "Table 1 shows statistics of this dataset.", "labels": [], "entities": []}, {"text": "Our parser achieves an accuracy of 67% on this train/test split in the fully supervised setting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9996768236160278}]}, {"text": "In comparison, a nearest neighbor strategy that uses the cosine similarity metric using a TF-IDF representation for the utterances yields an accuracy of 52.75%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9993653893470764}]}, {"text": "We found that 15% of the predicted queries did not execute, predominantly owing to (1) accessing table columns without joining with those tables, and (2) generating incorrect types that could not be deanonymized using the utterance.", "labels": [], "entities": []}, {"text": "The main types of errors in the remaining well-formed queries that produced incorrect results were (1) portions of the utterance (such as 'top' and 'cited by both') were ignored, and (2) some types from the utterance were not transferred to the SQL query.", "labels": [], "entities": []}, {"text": "We conducted additional simulated interactive learning experiments using GEO880 and ATIS to better understand the behavior of our train-deploy feedback loop, the effects of our data augmentation approaches, and the annotation effort required.", "labels": [], "entities": [{"text": "GEO880", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.9834814667701721}, {"text": "ATIS", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.5933912396430969}]}, {"text": "We randomly divide each training set into K batches and present these batches sequentially to our interactive learning algorithm.", "labels": [], "entities": []}, {"text": "Correctness feedback is provided by comparing the result of the predicted query to the gold query, i.e., we assume that users are able to perfectly distinguish correct results from incorrect ones.", "labels": [], "entities": []}, {"text": "shows accuracies on GEO880 and ATIS respectively of each batch when the model is trained on all previous batches.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9925034046173096}, {"text": "GEO880", "start_pos": 20, "end_pos": 26, "type": "DATASET", "confidence": 0.8905485272407532}, {"text": "ATIS", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9933165311813354}]}, {"text": "As in the live experiment, accuracy improves with successive batches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9995918869972229}]}, {"text": "Data augmentation using templates helps in the initial stages of GEO880, but its advantage Batch Size 150 100 50 % Wrong 70.2 60.4 54.3: Percentage of examples that required annotation (i.e., where the model initially made an incorrect prediction) on GEO880 vs. batch size. is reduced as more labeled data is obtained.", "labels": [], "entities": [{"text": "GEO880", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.9732388854026794}, {"text": "Percentage", "start_pos": 137, "end_pos": 147, "type": "METRIC", "confidence": 0.959969699382782}, {"text": "GEO880", "start_pos": 251, "end_pos": 257, "type": "DATASET", "confidence": 0.9698587656021118}]}, {"text": "Templates did not improve accuracy on ATIS, possibly because most ATIS queries involve two entities, i.e., a source city and a destination city, whereas our templates only generate questions with a single entity type.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9984803795814514}, {"text": "ATIS", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.7525586485862732}]}, {"text": "Nevertheless, templates are important in a live system to motivate users to interact with it in early stages.", "labels": [], "entities": []}, {"text": "As observed before, paraphrasing improves performance at all stages.", "labels": [], "entities": []}, {"text": "shows the percent of examples that require annotation using various batch sizes for GEO880.", "labels": [], "entities": [{"text": "GEO880", "start_pos": 84, "end_pos": 90, "type": "DATASET", "confidence": 0.9869112968444824}]}, {"text": "Smaller batch sizes reduce annotation effort, with a batch size of 50 requiring only 54.3% of the examples to be annotated.", "labels": [], "entities": []}, {"text": "This result demonstrates that more frequent deployments of improved models leads to fewer mistakes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Utterance and SQL query statistics for  each dataset. Vocabulary sizes are counted after  entity anonymization.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy of SQL query results on the  Geo880 corpus;  *  use Geo700; convert to logi- cal forms instead of SQL;  \u2020 measure accuracy in  terms of obtaining the correct logical form, other  systems, including ours, use denotations.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9775190353393555}, {"text": "Geo880 corpus", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9376366436481476}, {"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9964672327041626}]}, {"text": " Table 3: Accuracy of SQL query results on ATIS;  convert to logical forms instead of SQL;  \u2020 mea- sure accuracy in terms of obtaining the correct log- ical form, other systems, including ours, use deno- tations.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9751377701759338}, {"text": "ATIS", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8960277438163757}, {"text": "mea- sure", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.8675551017125448}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.7856961488723755}]}, {"text": " Table 4: Addition of paraphrases to the training set  helps performance, but template based data aug- mentation does not significantly help in the fully  supervised setting. Accuracies are reported on the  standard dev set for ATIS and on the training set,  using cross-validation, for Geo880.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 175, "end_pos": 185, "type": "METRIC", "confidence": 0.9939152598381042}, {"text": "ATIS", "start_pos": 228, "end_pos": 232, "type": "DATASET", "confidence": 0.9397686719894409}, {"text": "Geo880", "start_pos": 287, "end_pos": 293, "type": "DATASET", "confidence": 0.9251168370246887}]}]}