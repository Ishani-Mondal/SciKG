{"title": [{"text": "TakeLab at SemEval-2017 Task 5: Linear Aggregation of Word Embeddings for Fine-Grained Sentiment Analysis on Financial News", "labels": [], "entities": [{"text": "Sentiment Analysis on Financial News", "start_pos": 87, "end_pos": 123, "type": "TASK", "confidence": 0.8161887288093567}]}], "abstractContent": [{"text": "This paper describes our system for fine-grained sentiment scoring of news headlines submitted to SemEval 2017 task 5, subtask 2.", "labels": [], "entities": [{"text": "sentiment scoring of news headlines submitted to SemEval 2017 task 5", "start_pos": 49, "end_pos": 117, "type": "TASK", "confidence": 0.8392640731551431}]}, {"text": "Our system uses a feature-light method that consists of a Support Vector Regression (SVR) with various kernels and word embedding vectors as features.", "labels": [], "entities": [{"text": "Support Vector Regression (SVR)", "start_pos": 58, "end_pos": 89, "type": "METRIC", "confidence": 0.770317499836286}]}, {"text": "Our best-performing submission scored 3rd on the task out of 29 teams and 4th out of 45 submissions, with a cosine score of 0.733.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis) is a task of predicting whether the text expresses a positive, negative, or neutral opinion in general or with respect to an entity of interest.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8999406099319458}, {"text": "predicting whether the text expresses a positive, negative, or neutral opinion in general or with respect to an entity of interest", "start_pos": 33, "end_pos": 163, "type": "Description", "confidence": 0.7536348221094712}]}, {"text": "Developing systems capable of performing highly accurate sentiment analysis has attracted considerable attention over the last two decades.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.8963632583618164}]}, {"text": "The topic has been one of the main research areas in recent shared tasks, with main focus on social media texts, which are of particular interest for social studies) and marketing analysis (.", "labels": [], "entities": [{"text": "marketing analysis", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.8332367837429047}]}, {"text": "At the same time, social media texts pose a big challenge for sentiment analysis due to their short, informal and often ungrammatical format.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.9748946130275726}]}, {"text": "This work focuses on the second subtask of SemEval-2017 Task 5, which aims to perform finegrained sentiment analysis of the financial news.", "labels": [], "entities": [{"text": "SemEval-2017 Task", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.8228340744972229}, {"text": "finegrained sentiment analysis of the financial news", "start_pos": 86, "end_pos": 138, "type": "TASK", "confidence": 0.7450065314769745}]}, {"text": "Given that sentiments can affect market dynamics (, sentiment analysis of financial news can be a powerful tool for predicting market reactions.", "labels": [], "entities": [{"text": "sentiment analysis of financial news", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.8993794798851014}, {"text": "predicting market reactions", "start_pos": 116, "end_pos": 143, "type": "TASK", "confidence": 0.876363476117452}]}, {"text": "Similar to social media posts, finance news are short texts, but, unlike social media posts, the text is edited and hence grammatically correct.", "labels": [], "entities": []}, {"text": "On the other hand, news headlines are notorious for the use of a specific language, which is often elliptical and compressed, and thus differs from the language used in the rest of the news story.", "labels": [], "entities": []}, {"text": "Many approaches to sentiment analysis resort to rich, domain-specific, hand-crafted features.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.9683055877685547}]}, {"text": "At the same time, there has been a growing interest in featurelight methods, including kernel-methods () and neural embeddings).", "labels": [], "entities": []}, {"text": "These methods alleviate the need for manual creation of domain-specific features, while maintaining high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9963802695274353}]}, {"text": "Most of the recently published work focuses on sentiment analysis problems that are framed as a classification task, while fine-grained analysis is framed as a regression problem.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.9604993760585785}]}, {"text": "However, most of the high performing classification methods can be easily tuned to perform regression.", "labels": [], "entities": []}, {"text": "In this work we focus on feature-light methods as they do not require complex, time consuming feature engineering.", "labels": [], "entities": []}, {"text": "More specifically, we focus on string kernels () and methods using neural word embeddings ().", "labels": [], "entities": []}, {"text": "Developing domain-specific, rich feature sets would probably make the method highly dependent to the specific problem and would be hardly applicable to similar problems in other domains.", "labels": [], "entities": []}, {"text": "Feature-light methods have no such constrains: they typically offer satisfactory performance across different domains and may therefore be preferred to other domain-specific methods which use handcrafted features.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our task was, given a news headline, to predict the sentiment score fora specific company mentioned in the headline.", "labels": [], "entities": []}, {"text": "The dataset consisted of the name of the company, the text of the news headline and a value denoting the sentiment.", "labels": [], "entities": []}, {"text": "The sentiment was on a scale between \u22121 and 1 (inclusive), where \u22121 corresponds to very negative sentiment, 0 is considered neutral, while 1 stands fora very positive sentiment.", "labels": [], "entities": []}, {"text": "The news headlines were on average 10 words in length and largely composed of abbreviations.", "labels": [], "entities": []}, {"text": "The training set was composed of 1142 news headlines, while the test set contained 491 headlines, i.e., a 70:30 train-test split.", "labels": [], "entities": []}, {"text": "The training set id 5 company Ryanair title EasyJet attracts more passengers in June but still lags Ryanair sentiment 0.259: Sample training data instance and the test set mention 294 and 168 unique companies, respectively.", "labels": [], "entities": [{"text": "Ryanair title EasyJet", "start_pos": 30, "end_pos": 51, "type": "DATASET", "confidence": 0.8461055556933085}, {"text": "Ryanair sentiment", "start_pos": 100, "end_pos": 117, "type": "DATASET", "confidence": 0.7654801905155182}]}, {"text": "The distribution of headlines fora specific company was not uniform, and only 58 companies in the train set were targets of more than 4 news headlines, while \"Barclays\" -the most frequently mentioned one -was the target 67 times.", "labels": [], "entities": [{"text": "Barclays\"", "start_pos": 159, "end_pos": 168, "type": "DATASET", "confidence": 0.9075053930282593}]}, {"text": "In total, 112 companies occur in both the train and test set.", "labels": [], "entities": []}, {"text": "An example of a training data instance is given in.", "labels": [], "entities": []}, {"text": "This particular example also illustrates a possible difficulty regarding the headlines as they might refer to more than one company.", "labels": [], "entities": []}, {"text": "Such examples, however, are pretty rare in the dataset.", "labels": [], "entities": []}, {"text": "As for the class breakdown in the training set, we observe that the number of positively labeled instances is significantly larger than the number of negatively labeled instances (a ratio of 653 : 451 in favor of headlines with positive sentiment, including 38 headlines with a perfectly neutral score of 0.0).", "labels": [], "entities": []}, {"text": "However, the distribution of the target variable has an almost zero mean value of 0.031 and a standard deviation of 0.39.", "labels": [], "entities": []}, {"text": "All things considered, we conclude that the dataset was fairly well-balanced and the dependent variable was not skewed towards either class.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Cosine similarity between ground truth  annotations and model predictions (higher is bet- ter). Subscript displayed with (F)WEM methods  indicate the kernel used to train the model. Model  marked with (  *  ) is the submitted model.", "labels": [], "entities": [{"text": "Cosine similarity", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.8837990164756775}]}]}