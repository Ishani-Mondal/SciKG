{"title": [{"text": "SEF@UHH at SemEval-2017 Task 1: Unsupervised Knowledge-Free Semantic Textual Similarity via Paragraph Vector", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes our unsupervised knowledge-free approach to the SemEval-2017 Task 1 Competition.", "labels": [], "entities": [{"text": "SemEval-2017 Task 1 Competition", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.8160740584135056}]}, {"text": "The proposed method makes use of Paragraph Vector for assessing the semantic similarity between pairs of sentences.", "labels": [], "entities": []}, {"text": "We experimented with various dimensions of the vector and three state-of-the-art similarity metrics.", "labels": [], "entities": []}, {"text": "Given a cross-lingual task, we trained models corresponding to its two languages and combined the models by averaging the similarity scores.", "labels": [], "entities": []}, {"text": "The results of our submitted runs are above the median scores for five out of seven test sets by means of Pear-son Correlation.", "labels": [], "entities": []}, {"text": "Moreover, one of our system runs performed best on the Spanish-English-WMT test set ranking first out of 53 runs submitted in total by all participants .", "labels": [], "entities": [{"text": "Spanish-English-WMT test set", "start_pos": 55, "end_pos": 83, "type": "DATASET", "confidence": 0.8694969415664673}]}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) aims to assess the degree to which two snippets of text are related in meaning to each other.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8138210823138555}]}, {"text": "The SemEval annual competition offers a track on STS ( where submitted STS systems are evaluated in terms of the Pearson correlation between machine assigned semantic similarity scores and human judgments.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 113, "end_pos": 132, "type": "METRIC", "confidence": 0.9406100809574127}]}, {"text": "We participated in both monolingual sub-tracks and cross-lingual sub-tracks.", "labels": [], "entities": []}, {"text": "Given a sentence pair in the same language, the SemEval STS task is to assign a similarity score to it ranging from 0 to 5, with 0 implying that the semantics of the sentences are completely independent and 5 denoting semantic equivalence).", "labels": [], "entities": [{"text": "SemEval STS", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.9226182401180267}]}, {"text": "The crosslingual side of STS is similar to the initial task, but differs in the input sentences which come from two languages.", "labels": [], "entities": [{"text": "STS", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9330159425735474}]}, {"text": "This year's shared task features six sub-tasks: Arabic-Arabic, Arabic-English, Spanish-Spanish, Spanish-English (two test sets), English-English and a surprise task (Turkish-English) for which no annotated data is offered.", "labels": [], "entities": []}, {"text": "For example, for the English monolingual STS track, the pair of sentences below had a score of 3 assigned by human annotators, meaning that the two sentences are roughly equivalent, but some essential information differs or is missing.", "labels": [], "entities": []}, {"text": "Bayes' theorem was named after Rev Thomas Bayes and is a method used in probability theory.", "labels": [], "entities": [{"text": "probability theory", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.9501654207706451}]}, {"text": "As an official theorem, Bayes' theorem is valid in all universal interpretations of probability.", "labels": [], "entities": []}, {"text": "We present an unsupervised, knowledge-free approach that utilizes Paragraph Vector ( to represent sentences by means of continuous distributed vectors.", "labels": [], "entities": []}, {"text": "In addition to experimenting with feature spaces of different dimensionality, we also compare three state-of-the-art similarity metrics (Cosine, Bray-Curtis and Correlation) for calculating the STS scores.", "labels": [], "entities": [{"text": "Cosine", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9164379239082336}, {"text": "Correlation", "start_pos": 161, "end_pos": 172, "type": "METRIC", "confidence": 0.9489795565605164}]}, {"text": "We do not make use of any lexical or semantic resources, nor hand-annotated labeled corpora in addition to the distributed representations trained on non-annotated text.", "labels": [], "entities": []}, {"text": "The approach gives promising results on all sub-tasks, with our submitted systems ranking first out of 53 for one Spanish-English sub-track and above the median scores for five out of seven test sets.", "labels": [], "entities": []}, {"text": "We first shortly summarize related work in STS and describe Paragraph Vector in Section 2.", "labels": [], "entities": [{"text": "STS", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9586628079414368}, {"text": "Paragraph Vector", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.6615362763404846}]}, {"text": "Then we present our method in Section 3 along with the corpora we used in training the Paragraph Vector models.", "labels": [], "entities": []}, {"text": "Section 4 contains an overview of the evaluation and the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The similarity scores are evaluated by computing the Pearson Correlation between them and human judgments for the same sentence pairs.", "labels": [], "entities": [{"text": "Pearson Correlation", "start_pos": 53, "end_pos": 72, "type": "METRIC", "confidence": 0.9480063021183014}]}, {"text": "This section presents our results for all sub-tasks of the 2017 test sets and also for the STS Benchmark 9).", "labels": [], "entities": [{"text": "STS Benchmark 9", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.9080546895662943}]}], "tableCaptions": [{"text": " Table 3: Pearson Correlation results for various parameters", "labels": [], "entities": [{"text": "Pearson Correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8405779004096985}]}, {"text": " Table 4: Results for the submitted runs", "labels": [], "entities": []}]}