{"title": [{"text": "QLUT at SemEval-2017 Task 1: Semantic Textual Similarity Based on Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper reports the details of our submissions in the task 1 of SemEval 2017.", "labels": [], "entities": [{"text": "SemEval 2017", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.7162742614746094}]}, {"text": "This task aims at assessing the semantic textual similarity of two sentences or texts.", "labels": [], "entities": []}, {"text": "We submit three unsupervised systems based on word embeddings.", "labels": [], "entities": []}, {"text": "The differences between these runs are the various preprocessing on evaluation data.", "labels": [], "entities": []}, {"text": "The best performance of these systems on the evaluation of Pearson correlation is 0.6887.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 59, "end_pos": 78, "type": "METRIC", "confidence": 0.7163622379302979}]}, {"text": "Unsurprisingly, results of our runs demonstrate that data preprocessing, such as to-kenization, lemmatization, extraction of content words and removing stop words, is helpful and plays a significant role in improving the performance of models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) has been held in SemEval since 2012, which is a basic task in natural language processing (NLP) field.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8136735608180364}]}, {"text": "It aims at computing the semantic similarity of two short texts or sentences, and the result will be evaluated on a gold standard set, which is made by several official annotators.", "labels": [], "entities": []}, {"text": "In recent years, as an unsupervised method, word embedding () becomes more and more popular in SemEval (.", "labels": [], "entities": []}, {"text": "The paper describes the submission of our systems to STS 2017, which utilize word embedding method.", "labels": [], "entities": []}, {"text": "Different from some teams who have _______________________ * Corresponding author used word embedding described above, what we pay attention to is the point of preprocessing evaluation data.", "labels": [], "entities": []}, {"text": "With this consideration, we process the evaluation data with different method in order to verify whether it works or not.", "labels": [], "entities": []}, {"text": "The framework of our systems is showed in.", "labels": [], "entities": []}, {"text": "Its simple description is as follows: Tokenization: This is to tokenize the two sentences of the system's input.", "labels": [], "entities": []}, {"text": "Though the English sentence is tokenized naturally, the punctuations are not.", "labels": [], "entities": []}, {"text": "For instance, the sentence \"A person is on a baseball team.\" will be tokenized to \"A person is on a baseball team .\".", "labels": [], "entities": []}, {"text": "Extraction of content words: In this process, content words of the tokenized sentence will be extracted.", "labels": [], "entities": [{"text": "Extraction of content words", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.843782901763916}]}, {"text": "For example, the tokenized sentence \"A person is on a baseball team .\" turns into \"person is baseball team\".", "labels": [], "entities": []}, {"text": "In this paper, content words include nouns, verbs, adverbs or adjectives.", "labels": [], "entities": []}, {"text": "Lemmatization: It is known that words in English sentences have a variety of forms.", "labels": [], "entities": []}, {"text": "This operation will lemmatize these words to their basic forms, for example, word \"made\" and \"making\" will be changed to \"make\".", "labels": [], "entities": []}, {"text": "In addition, this process also convert the uppercase to lowercase, for instance, \"Make\" will be changed to \"make\".", "labels": [], "entities": []}, {"text": "Word embeddings: This process utilizes the word2vec toolkit 1 to train on the Wikipedia corpus, then the word embeddings can be obtained.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.9034021198749542}]}, {"text": "Sentence similarity: The similarity of two sentences is computed as the cosine of their sentence embeddings, which can begotten easily (see 2.3).", "labels": [], "entities": []}, {"text": "Normalization: Due to the different range of the results of runs, similarity scores are normalized to meet the official standard.", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 66, "end_pos": 83, "type": "METRIC", "confidence": 0.9763962030410767}]}], "datasetContent": [{"text": "In the task, the official evaluation tool is based on Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 54, "end_pos": 73, "type": "METRIC", "confidence": 0.8592690527439117}]}, {"text": "A system run in each test set is evaluated by its Pearson correlation with the official provided gold standard set.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 50, "end_pos": 69, "type": "METRIC", "confidence": 0.9799537658691406}, {"text": "gold standard set", "start_pos": 97, "end_pos": 114, "type": "DATASET", "confidence": 0.7981725533803304}]}, {"text": "The results in above shows that the system Run2 get the best performance of 0.6433.", "labels": [], "entities": []}, {"text": "Compared with Run1, Run2 achieves a 2.78% improvement, which implies that to lemmatize content words can be helpful.", "labels": [], "entities": [{"text": "Run1", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.9234299063682556}, {"text": "Run2", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.836168646812439}]}, {"text": "The difference of 12.31% between Run1 and Run3 indicates that the extraction of content words can make a larger improvement for the similarity computation of the sentence pairs.", "labels": [], "entities": [{"text": "Run1", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.9149807095527649}, {"text": "Run3", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.8870341181755066}]}, {"text": "In order to further know the effect of lemmatization with Run3, we make the system Run3'.", "labels": [], "entities": [{"text": "Run3", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.9625676870346069}, {"text": "Run3", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.9482712745666504}]}, {"text": "The only difference between them is that in the operation of preprocessing the data, Run3' makes the lemmatization of the sentence pairs in the data, on the contrary, Run3 do not do it.", "labels": [], "entities": []}, {"text": "The contrast of Run3 and Run3' again confirms that lemmatization for computing the similarity of the sentence pairs can be effective.", "labels": [], "entities": [{"text": "Run3", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.9284689426422119}, {"text": "Run3", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.8781036734580994}]}, {"text": "As is shown in, the relative performance of each run is similar with.", "labels": [], "entities": []}, {"text": "Run2-get the best performance of 0.6887, which demonstrate the effectiveness of content words extraction and lemmatization.", "labels": [], "entities": [{"text": "content words extraction", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.6683865388234457}]}, {"text": "Each run in achieves a better performance than that in, which demonstrates that it is necessary to remove stop words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Official evaluation results of our  submitted runs on Track 5.", "labels": [], "entities": []}]}