{"title": [{"text": "Classifying Semantic Clause Types: Modeling Context and Genre Characteristics with Recurrent Neural Networks and Attention", "labels": [], "entities": []}], "abstractContent": [{"text": "Detecting aspectual properties of clauses in the form of situation entity types has been shown to depend on a combination of syntactic-semantic and contextual features.", "labels": [], "entities": [{"text": "Detecting aspectual properties of clauses", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8722206115722656}]}, {"text": "We explore this task in a deep-learning framework, where tuned word representations capture lexical, syntactic and semantic features.", "labels": [], "entities": []}, {"text": "We introduce an attention mechanism that pinpoints relevant context not only for the current instance , but also for the larger context.", "labels": [], "entities": []}, {"text": "Apart from implicitly capturing task relevant features, the advantage of our neu-ral model is that it avoids the need to reproduce linguistic features for other languages and is thus more easily transfer-able.", "labels": [], "entities": []}, {"text": "We present experiments for English and German that achieve competitive performance.", "labels": [], "entities": []}, {"text": "We present a novel take on modeling and exploiting genre information and showcase the adaptation of our system from one language to another.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic clause types, called Situation Entity (SE) types are linguistic characterizations of aspectual properties shown to be useful for argumentation structure analysis), genre characterization , and detection of generic and generalizing sentences . Recent work on automatic identification of SE types relies on feature-based classifiers for English that have been successfully applied to various textual genres (, and also show that a sequence labeling approach that models contextual clause labels yields improved classification performance.", "labels": [], "entities": [{"text": "argumentation structure analysis", "start_pos": 138, "end_pos": 170, "type": "TASK", "confidence": 0.6488234798113505}, {"text": "genre characterization", "start_pos": 173, "end_pos": 195, "type": "TASK", "confidence": 0.7368786036968231}, {"text": "detection of generic and generalizing sentences", "start_pos": 202, "end_pos": 249, "type": "TASK", "confidence": 0.7064338227113088}]}, {"text": "Deep learning provides a powerful framework in which linguistic and semantic regularities can be implicitly captured through word embeddings ().", "labels": [], "entities": []}, {"text": "Patterns in larger text fragments can be encoded and exploited by recurrent (RNNs) or convolutional neural networks (CNNs) which have been successfully used for various sentence-based classification tasks, e.g. sentiment or relation classification ().", "labels": [], "entities": [{"text": "sentence-based classification tasks", "start_pos": 169, "end_pos": 204, "type": "TASK", "confidence": 0.7675859530766805}, {"text": "sentiment or relation classification", "start_pos": 211, "end_pos": 247, "type": "TASK", "confidence": 0.73280169069767}]}, {"text": "We frame the task of classifying clauses with respect to their aspectual properties -i.e., situation entity types -in a recurrent neural network architecture.", "labels": [], "entities": []}, {"text": "We adopt a Gated Recurrent Unit (GRU)-based RNN architecture that is well suited to modeling long sequences (.", "labels": [], "entities": []}, {"text": "This initial model is enhanced with an attention mechanism shown to be beneficial for sentence classification (  and sequence modeling.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.782539576292038}, {"text": "sequence modeling", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.7090141922235489}]}, {"text": "We explore the usefulness of attention in two settings: (i) the individual classification task and (ii) in a setting approximating sequential labeling in which the attention vector provides features that describe the clauses preceding the current instance.", "labels": [], "entities": [{"text": "individual classification task", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.7520918150742849}]}, {"text": "Compared to the strong baseline provided by the feature based system of, we achieve competitive performance and find that attention as well as context representation using predicted or goldstandard labels of the previous N clauses, and text genre information improve our model.", "labels": [], "entities": []}, {"text": "A strong motivation for developing NN-based systems is that they can be transferred with low cost to other languages without major feature engineering or use of hand-crafted linguistic knowledge resources.", "labels": [], "entities": []}, {"text": "Given the highly-engineered feature sets used for SE classification so far, porting such classifiers to other languages is a non-trivial issue.", "labels": [], "entities": [{"text": "SE classification", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.9868099987506866}]}, {"text": "We test the portability of our system by applying it to German.", "labels": [], "entities": []}, {"text": "We present a novel take on modeling and exploiting genre information and test it on the English multi-genre corpus of.", "labels": [], "entities": []}, {"text": "Our aims and contributions are: (i) We study the performance of GRU-based models enhanced with attention for modeling local and non-local characteristics of semantic clause types.", "labels": [], "entities": []}, {"text": "(ii) We compare the effectiveness of the learned attention weights as features fora sequence labeling system to the explicitly defined syntactic-semantic features in.", "labels": [], "entities": []}, {"text": "(iii) We define extensions of our models that integrate external knowledge about genre and show that this can be used to improve classification performance across genres.", "labels": [], "entities": []}, {"text": "(iv) We test the portability of our models to other languages by applying them to a smaller, manually annotated German dataset.", "labels": [], "entities": [{"text": "German dataset", "start_pos": 112, "end_pos": 126, "type": "DATASET", "confidence": 0.803335040807724}]}, {"text": "The performance is comparable to English.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the English dataset, we use the same testtrain split as.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.8442513346672058}]}, {"text": "The German dataset was split into training and testing with a balanced distribution of genres (as is the case for the English dataset).", "labels": [], "entities": [{"text": "German dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.876054972410202}, {"text": "English dataset", "start_pos": 118, "end_pos": 133, "type": "DATASET", "confidence": 0.7779453694820404}]}, {"text": "Both datasets have a 80-20 split between training and testing (20% of training is used for development).", "labels": [], "entities": []}, {"text": "We report results in terms of accuracy and macro-average F1 score on the held-out test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9996508359909058}, {"text": "F1 score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9740735292434692}]}, {"text": "The feature-based system of Palmer07 () simulates context through predicted labels from previous clauses.) report results for their CRF-based SE The Wiki texts were selected by  precisely in order to target GENERIC SENTENCE clauses.", "labels": [], "entities": [{"text": "Palmer07", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.9563105702400208}]}, {"text": "The cross validation splits of the data used by Friedrich et al.   type labeler for different feature sets, with 10-fold cross validation and on a held-out test set.", "labels": [], "entities": []}, {"text": "To test if the context is useful they extend their classifier with a CRF that includes the predicted label of the preceding clause.", "labels": [], "entities": []}, {"text": "In the oracle setting it includes the gold label of the previous clause.", "labels": [], "entities": []}, {"text": "Feature set A consists of standard NLP features including POS tags and Brown clusters.", "labels": [], "entities": []}, {"text": "Feature set B includes more detailed features such as tense, lemma, negation, modality, WordNet sense, WordNet supersense and WordNet hypernym sense.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.8735665678977966}, {"text": "WordNet", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9152635931968689}, {"text": "WordNet hypernym sense", "start_pos": 126, "end_pos": 148, "type": "DATASET", "confidence": 0.7840940554936727}]}, {"text": "We presume that some of the information captured by feature set B, particularly sense and hypernym information, may not be captured in the word embeddings we use in our approach.", "labels": [], "entities": []}, {"text": "Evaluation of our neural systems.", "labels": [], "entities": []}, {"text": "Our local system (cf. Section 4.1) achieves an accuracy of 66.55.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9997338652610779}]}, {"text": "Adding genre information does not help, but adding attention within the local clause yields an improvement of 2.44 percentage points (pp).", "labels": [], "entities": []}, {"text": "Using both attention and genre information leads to a 2.13 pp increase over the model that uses only attention.", "labels": [], "entities": []}, {"text": "Adding context information beyond the local clause -a window of up to three previous clauses -improves the wordbased attention models slightly, but a wider window (four or more clauses) causes a major drop: SE-type classification on English test set, sequence oracle model using gold labels (gLab). inaccuracy.", "labels": [], "entities": []}, {"text": "Using context as predicted labels of previous clauses improves the model slightly (up to 0.38 pp), but adding genre on top of that improves the model by up to 2.62 pp compared to the basic model with attention.", "labels": [], "entities": []}, {"text": "The oracle model (cf), which uses the gold labels of previous clauses, gives an upper bound for the impact of sequence information: 73.40% accuracy for previous 5 gold labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9994057416915894}]}, {"text": "Combined with genre information, the upper bound reaches 73.45% accuracy when using the previous 2 gold labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9990805387496948}]}, {"text": "The best accuracy on the English data (ignoring the oracle) is achieved by the model that uses 2 previous predicted labels plus genre information (71.61%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9993669390678406}, {"text": "English data", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.689134031534195}]}, {"text": "This model outperforms's results when using standard NLP features (feature set A) and their model using feature set B separately.", "labels": [], "entities": []}, {"text": "Our model comes close to Friedrich et al.'s best results obtained by applying their entire set of features, particularly considering that our system only uses generic word embeddings.", "labels": [], "entities": []}, {"text": "We achieve best results when incorporating two previous labels or two previous clauses (cf.).", "labels": [], "entities": []}, {"text": "This is inline with who report that inmost cases performance starts to degrade as the model incorporates more than two previous labels.", "labels": [], "entities": []}, {"text": "A window size of two does not always lead to best performance on the German dataset (cf. Section 7), where the model using predicted labels from the maximum window size (5) performs best.", "labels": [], "entities": [{"text": "German dataset", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.954119473695755}]}, {"text": "When adding genre information, we achieve best results with window size two (cf.).", "labels": [], "entities": []}, {"text": "This inconsistency can possibly be traced back to the fact that we applied the best-performing vari- We achieve 36.24 acc for 4 and 36.17 acc for 5 clauses.", "labels": [], "entities": []}, {"text": "ations of our system developed on English data to our German dataset without further hyperparameter tuning.", "labels": [], "entities": [{"text": "German dataset", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.9134104549884796}]}, {"text": "shows macroaverage F1 scores of our best performing system for the single SE classes.", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.7403398156166077}]}, {"text": "The scores are very similar to the results of.", "labels": [], "entities": []}, {"text": "Scores for GENERALIZING SENTENCE are the lowest as this class is very infrequent in the data set, while scores for the classes STATE, EVENT, and RE-PORT are the highest.", "labels": [], "entities": [{"text": "GENERALIZING", "start_pos": 11, "end_pos": 23, "type": "METRIC", "confidence": 0.64499431848526}, {"text": "SENTENCE", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.39742663502693176}, {"text": "STATE", "start_pos": 127, "end_pos": 132, "type": "METRIC", "confidence": 0.9368764162063599}, {"text": "EVENT", "start_pos": 134, "end_pos": 139, "type": "METRIC", "confidence": 0.9487924575805664}, {"text": "RE-PORT", "start_pos": 145, "end_pos": 152, "type": "METRIC", "confidence": 0.9934413433074951}]}, {"text": "In addition, we explored our system's performance for binary classification): here we classified STATE vs. the remaining classes, EVENT vs. the remaining classes etc.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.6959402561187744}, {"text": "STATE", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9858723282814026}, {"text": "EVENT", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.9933550357818604}]}, {"text": "Binary classification achieves better performance and can be helpful for downstream applications which only need information about specific  SE types, for example for distinguishing generic from non-generic sentences.", "labels": [], "entities": [{"text": "Binary classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9195840656757355}]}, {"text": "Attention is not only an effective mechanism that allows models to focus on specific parts of the input, but it may also enable interesting linguistic insights: (1) the attention to specific words or POS for specific SE types, (2) the overall distribution of attention weights among POS tag labels and SE types, and (3) the position of words with maximum/high attention scores within a clause.", "labels": [], "entities": []}, {"text": "shows example clauses with their attention weights.", "labels": [], "entities": []}, {"text": "In the first clause, a STATE, the model attends most to the nouns \"China\" and \"Japan\".", "labels": [], "entities": [{"text": "STATE", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.7880427837371826}]}, {"text": "In the next clause, a GENERALIZING SENTENCE, the noun \"system\" is assigned the highest attention weight.", "labels": [], "entities": [{"text": "GENERALIZING", "start_pos": 22, "end_pos": 34, "type": "METRIC", "confidence": 0.9164314270019531}, {"text": "SENTENCE", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.5074164271354675}]}, {"text": "The highest weighted word in the GENERIC SENTENCE is the pronoun \"their\", and in REPORT it is the verb \"answered\".", "labels": [], "entities": [{"text": "GENERIC SENTENCE", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.5716100931167603}, {"text": "REPORT", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.6052040457725525}]}, {"text": "visualizes the mean attention score per POS tag for all SE types (gold labels).", "labels": [], "entities": [{"text": "mean attention score", "start_pos": 15, "end_pos": 35, "type": "METRIC", "confidence": 0.7588238517443339}]}, {"text": "Interestingly, attention seems to be especially important for classes that are rare, such as IMPERA- We post-process our data with POS tags using spaCy 11 with the PTB tagset (.", "labels": [], "entities": [{"text": "PTB tagset", "start_pos": 164, "end_pos": 174, "type": "DATASET", "confidence": 0.9472639262676239}]}, {"text": "TIVE or REPORT, each less than 5% of the English dataset.", "labels": [], "entities": [{"text": "TIVE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7008705139160156}, {"text": "REPORT", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9513190984725952}, {"text": "English dataset", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.9555794894695282}]}, {"text": "The heat map indicates that the model especially attends to verbs when classifying the SE type REPORT.", "labels": [], "entities": []}, {"text": "This is not surprising, since REPORT clauses are signaled by verbs of speech.", "labels": [], "entities": [{"text": "REPORT clauses", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.9091363549232483}]}, {"text": "GENERALIZING SENTENCE attend to symbols, mainly punctuation, and genitive markers such as \"'s\".", "labels": [], "entities": [{"text": "GENERALIZING", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.681240439414978}, {"text": "SENTENCE", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.48147648572921753}]}, {"text": "The OTHER class, which includes clauses without an assigned SE type label, attends mostly to interjections.", "labels": [], "entities": []}, {"text": "Indeed, OTHER is frequent in genres with fragmented sentences (emails, blogs), and numerous interjections such as \"wow\" or \"um\".", "labels": [], "entities": [{"text": "OTHER", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.7707507610321045}]}, {"text": "shows the relative positions of words with maximum attention within clauses.", "labels": [], "entities": []}, {"text": "The model mostly attends to words at the end of clauses and almost never to words in the first half of clauses.", "labels": [], "entities": []}, {"text": "This distribution shifts to the left when considering more words with high attention scores instead of only the word with maximum attention -words with 2 nd (3 rd , 4 th , 5 th ) highest attention score can often be found at the beginning of clauses.", "labels": [], "entities": []}, {"text": "The model seems to draw information from abroad range of positions.", "labels": [], "entities": []}, {"text": "We explored the impact of the attention vectors as inputs to a sequence labeling modeleach clause is described through the words with the highest attention weights and these weights, and used in a conditional random field system (CRF++ 12 ).", "labels": [], "entities": []}, {"text": "The best performance was obtained when using the attention vector of the current clause (and no additional context) -61.68% accuracy (47.18% F1 score).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9994722008705139}, {"text": "F1 score", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9828136265277863}]}, {"text": "CRF++ maps the attention information to binary features, and as such cannot take advantage of information captured in the numerical values of the attention weights, or the embeddings of the given words.", "labels": [], "entities": [{"text": "CRF++", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8962209522724152}]}], "tableCaptions": [{"text": " Table 1: Data sets with SE-labeled clauses", "labels": [], "entities": []}, {"text": " Table 2: Reported results of baseline models for  English (accuracy and macro-average F1 score).  CV=10-fold cross validation, test=eval. on test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9993914365768433}, {"text": "F1 score", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.964021772146225}, {"text": "CV", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.9231534004211426}]}, {"text": " Table 3: SE-type classification on English test set,  with context as predicted labels (pLab).", "labels": [], "entities": [{"text": "SE-type classification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8754084408283234}, {"text": "English test set", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.8545657197634379}]}, {"text": " Table 4: SE-type classification on English test set,  sequence oracle model using gold labels (gLab).", "labels": [], "entities": [{"text": "SE-type classification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8007950782775879}]}, {"text": " Table 5: SE-type classification on German test set.", "labels": [], "entities": [{"text": "SE-type classification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6796130836009979}, {"text": "German test set", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9819655219713846}]}, {"text": " Table 6: SE-type classification on German test set,  sequence oracle model .", "labels": [], "entities": [{"text": "SE-type classification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7115707546472549}, {"text": "German test set", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9415762424468994}]}]}