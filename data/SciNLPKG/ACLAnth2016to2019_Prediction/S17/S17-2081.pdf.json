{"title": [{"text": "IKM at SemEval-2017 Task 8: Convolutional Neural Networks for Stance Detection and Rumor Verification", "labels": [], "entities": [{"text": "Stance Detection and Rumor Verification", "start_pos": 62, "end_pos": 101, "type": "TASK", "confidence": 0.7430215179920197}]}], "abstractContent": [{"text": "This paper describes our approach for SemEval-2017 Task 8.", "labels": [], "entities": [{"text": "SemEval-2017 Task 8", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.9220599333445231}]}, {"text": "We aim at detecting the stance of tweets and determining the veracity of the given rumor.", "labels": [], "entities": [{"text": "detecting the stance of tweets", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.8288246512413024}]}, {"text": "We utilize a convolutional neural network for short text categorization using multiple filter sizes.", "labels": [], "entities": [{"text": "short text categorization", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.6945586601893107}]}, {"text": "Our approach beats the baseline classifiers on different event data with good F 1 scores.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9745481411616007}]}, {"text": "The best of our submitted runs achieves rank 1 st among all scores on sub-task B.", "labels": [], "entities": []}], "introductionContent": [{"text": "Rumors in social networks are widely noticed due to the broad success of online social media.", "labels": [], "entities": []}, {"text": "Unconfirmed rumors usually spark discussion before being verified.", "labels": [], "entities": []}, {"text": "These have created cost for society and panic among people.", "labels": [], "entities": []}, {"text": "Rather than relying on human observers to identify trending rumors, it would be helpful to detect them automatically and limit the damage immediately.", "labels": [], "entities": []}, {"text": "However, identifying false rumors early is a hard task without sufficient evidence such as responses, retweet and fact checking sites.", "labels": [], "entities": [{"text": "identifying false rumors early", "start_pos": 9, "end_pos": 39, "type": "TASK", "confidence": 0.8919640183448792}]}, {"text": "Instead of propagation structure, context-level patterns are more obvious and useful for the identification of rumors at this stage -in particular, observing the different patterns of stances amongst participants.", "labels": [], "entities": [{"text": "identification of rumors", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.8806062738100687}]}, {"text": "Recent research has proposed a 4-way classification task to encompass all the different kinds of reactions to rumors.", "labels": [], "entities": []}, {"text": "A schema of classifications including supporting, denying, querying and commenting (SDQC) is applied in SemEval2017 Task 8.", "labels": [], "entities": [{"text": "SemEval2017 Task 8", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.7159280379613241}]}, {"text": "In this paper, we describe a system for stance classification and rumor verification in tweets.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.9319632351398468}, {"text": "rumor verification", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7024378925561905}]}, {"text": "For the first task, we are given tree-structured conversations, where replies are triggered by a source tweet.", "labels": [], "entities": []}, {"text": "We need to categorize the replies into one of the SDQC categories by reply-source pairs.", "labels": [], "entities": []}, {"text": "The second task is about rumor verification.", "labels": [], "entities": [{"text": "rumor verification", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9493923783302307}]}, {"text": "Our system is for the closed variant -which means the veracity of a rumor will have to be predicted solely without external data.", "labels": [], "entities": [{"text": "veracity", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9652717113494873}]}, {"text": "It is a challenging NLP task.", "labels": [], "entities": []}, {"text": "Statements containing sarcasm, irony and metaphor often need personal experience to be able to infer their broader context.", "labels": [], "entities": [{"text": "Statements containing sarcasm, irony and metaphor", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7071639469691685}]}, {"text": "Furthermore, lots of background knowledge is required to do the fact checking.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.9362932443618774}]}, {"text": "In this paper, we develop convolutional neural network models for both tasks.", "labels": [], "entities": []}, {"text": "Our system relies on a supervised classifier, using text features of different word representation methods such as learning word embedding through training and pre-trained word embedding model like GloVe ().", "labels": [], "entities": []}, {"text": "The experiment section presents our results and discusses the performance of our work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments using the rumor datasets annotated for stance (.", "labels": [], "entities": []}, {"text": "The statistics of the datasets are shown in.", "labels": [], "entities": []}, {"text": "For subtask B, conversation threads are not available for the participants and the use of external data is forbidden on the closed variant.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4. We notice that the  comment stance is the easiest to detect, since they  take a large part of the data. The number of query  stances are similar to support and deny, while it  has much better precision and recall because the  features of queries are more obvious. Likewise,  there are some negative words in the deny stance as features. However, it is challenging to extract  features of supporting which results in a poorer  performance.", "labels": [], "entities": [{"text": "precision", "start_pos": 202, "end_pos": 211, "type": "METRIC", "confidence": 0.9993141889572144}, {"text": "recall", "start_pos": 216, "end_pos": 222, "type": "METRIC", "confidence": 0.9986227750778198}]}, {"text": " Table 2: Accuracy and F 1 scores for different  methods across datasets. The upper lines of the re- sults are our baseline.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995313882827759}, {"text": "F 1 scores", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.989333987236023}]}, {"text": " Table 3: results of using different window sizes.", "labels": [], "entities": []}, {"text": " Table 4: Result on test data for subtask A.", "labels": [], "entities": []}, {"text": " Table 5: Rank on test data for subtask B.", "labels": [], "entities": [{"text": "Rank", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9742465019226074}]}]}