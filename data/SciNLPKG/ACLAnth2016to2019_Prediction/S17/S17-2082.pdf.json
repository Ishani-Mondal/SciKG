{"title": [{"text": "NileTMRG at SemEval-2017 Task 8: Determining Rumour and Veracity Support for Rumours on Twitter", "labels": [], "entities": [{"text": "NileTMRG", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8817428946495056}]}], "abstractContent": [{"text": "This paper presents the results and conclusions of our participation in SemEval-2017 task 8: Determining rumour veracity and support for rumours.", "labels": [], "entities": [{"text": "SemEval-2017 task 8", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.8723892569541931}, {"text": "Determining rumour veracity", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.8208275437355042}]}, {"text": "We have participated in 2 subtasks: SDQC (Subtask A) which deals with tracking how tweets orient to the accuracy of a rumourous story, and Ve-racity Prediction (Subtask B) which deals with the goal of predicting the veracity of a given rumour.", "labels": [], "entities": [{"text": "tracking how tweets orient to the accuracy of a rumourous story", "start_pos": 70, "end_pos": 133, "type": "TASK", "confidence": 0.6778417554768649}, {"text": "predicting the veracity of a given rumour", "start_pos": 201, "end_pos": 242, "type": "TASK", "confidence": 0.7990013020379203}]}, {"text": "Our participation was in the closed task variant, in which the prediction is made solely from the tweet itself.", "labels": [], "entities": []}, {"text": "For subtask A, linear support vector classification was applied to a model of bag of words, and the help of a na\u00efve Bayes classifier was used for semantic feature extraction.", "labels": [], "entities": [{"text": "linear support vector classification", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.6071569621562958}, {"text": "semantic feature extraction", "start_pos": 146, "end_pos": 173, "type": "TASK", "confidence": 0.7117792765299479}]}, {"text": "For subtask B, a similar approach was used.", "labels": [], "entities": []}, {"text": "Many features were used during the experimentation process but only a few proved to be useful with the data set provided.", "labels": [], "entities": []}, {"text": "Our system achieved 71% accuracy and ranked 5th among 8 systems for subtask A and achieved 53% accuracy with the lowest RMSE value of 0.672 ranking at the first place among 5 systems for subtask B.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9994531273841858}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9991908669471741}, {"text": "RMSE", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9903918504714966}]}], "introductionContent": [{"text": "Over the past 15 years, and in a gradual manner, social media has started to become a main source of news.", "labels": [], "entities": []}, {"text": "However, social media has also become a ripe ground for rumours, spreading them in a matter of a few minutes.", "labels": [], "entities": []}, {"text": "A rumour is defined as a claim that could be true or false.", "labels": [], "entities": []}, {"text": "False rumours may greatly affect the social, economic and political stability of any society around the world, hence the need for tools to help people, especially journalists, analyze the spread of rumours and their effect on the society as well as determine their veracity.", "labels": [], "entities": []}, {"text": "Twitter is a famous social media platform capable of spreading breaking news, thus most of rumour related research uses Twitter feed as a basis for research.", "labels": [], "entities": []}, {"text": "SemEval (Semantic Evaluation) is an ongoing series of evaluations of computational semantic analysis systems.", "labels": [], "entities": [{"text": "SemEval (Semantic Evaluation)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6720734417438508}]}, {"text": "Task 8 (RumourEval)) is one of 12 tasks presented in.", "labels": [], "entities": []}, {"text": "This paper describes the system that we have used to participate in this task.", "labels": [], "entities": []}, {"text": "The task consists of 2 subtasks: SDQC (Subtask A) which has the objective of tracking how other tweets orient to the accuracy of a rumourous story, and Veracity Prediction (Subtask B) for which has the goal to predict the veracity of a given rumour.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9760704636573792}]}, {"text": "Task B has two variants: an open variant and a closed one.", "labels": [], "entities": []}, {"text": "We have only participated in the closed variant, in which the prediction should be made solely from the tweet itself.", "labels": [], "entities": []}, {"text": "Scientific literature related to rumours on social media has started to emerge over the past 7 years.", "labels": [], "entities": []}, {"text": "It can be categorized into 4 main categories: 1) the detection of the spreading of a rumour, 2) the determination of the veracity of a rumour, 3) the analysis of the rumour propagation through asocial network and 4) speech act analysis of different online replies to the rumour.", "labels": [], "entities": [{"text": "speech act analysis of different online replies to the rumour", "start_pos": 216, "end_pos": 277, "type": "TASK", "confidence": 0.7221946060657501}]}, {"text": "Subtask A belongs to the 4th category, while subtask B belongs to the 2nd category.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: section 2 briefly overviews related work, section 3 provides task description details, section 4 provides a detailed system description covering preprocessing, feature extraction and selection, learning model and evaluation done for both subtasks A and B.", "labels": [], "entities": [{"text": "feature extraction and selection", "start_pos": 207, "end_pos": 239, "type": "TASK", "confidence": 0.8160796016454697}]}, {"text": "In the end a conclusion is given with the future work needed., presented a methodology that enabled them to collect, identify and annotate a big data set of rumours associated with multiple newsworthy events, and analyzed how people orient to and spread rumours in social media.", "labels": [], "entities": []}, {"text": "This data set was used for task 8 of SemEval 2017: RumourEval., addressed the problem of automatic rumour detection in microblogs as well as identifying users that support or deny or question the rumour.", "labels": [], "entities": [{"text": "SemEval 2017: RumourEval.", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.7602259814739227}, {"text": "rumour detection", "start_pos": 99, "end_pos": 115, "type": "TASK", "confidence": 0.6961940824985504}]}, {"text": "They achieved this by exploring the effectiveness of 3 categories of features: content-based, network-based and micro-blog specific memes., addressed the problem of rumour detection via a speech act classifier that detects assertions using different semantic and syntactic features in addition to a clustering algorithm to cluster groups of rumourous tweets talking about the same topic together.,, and addressed the issue of detecting the veracity of rumours using manually selected and annotated rumours on Twitter using linguistic, user, rumour, pragmatic, content, twitter-specific and propagation features and the latter developed a software demonstration that provides a visual user interface to allow the user to examine the analysis., concentrated on linguistic features such as comprehensibility, sentiment and writing style to predict rumour veracity, ignoring all non-linguistic features., also concentrated on linguistic features to detect disinformation by comparing the text to search results using the significant sentences in that text., proposed the first real time rumour debunking algorithm for Twitter while, concentrated on identifying a trending rumour as early as possible without trying to assess its veracity.", "labels": [], "entities": [{"text": "rumour detection", "start_pos": 165, "end_pos": 181, "type": "TASK", "confidence": 0.7804027497768402}]}], "datasetContent": [{"text": "Several scikit-learn classifiers were used during experimentation before deciding on the final model.", "labels": [], "entities": []}, {"text": "For subtask A, the linear support vector machine classifier (Linear SVC) proved to be the most accurate during cross validation, however, logistic regression generalized the best on test data.", "labels": [], "entities": []}, {"text": "During cross-validation the macro-averaged F1 measure was used to evaluate the classifiers and choose the best amongst them, as the distribution of categories was clearly skewed towards comments.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9578281939029694}]}, {"text": "For subtask B, Linear SVC proved to be the best in terms of accuracy and the confidence root mean square error (RMSE).", "labels": [], "entities": [{"text": "Linear SVC", "start_pos": 15, "end_pos": 25, "type": "TASK", "confidence": 0.437349408864975}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.99972003698349}, {"text": "confidence root mean square error (RMSE)", "start_pos": 77, "end_pos": 117, "type": "METRIC", "confidence": 0.9422375932335854}]}, {"text": "shows the features used for each subtask along with its type.", "labels": [], "entities": []}, {"text": "Type 'Content' refers to the features determined from the tweet's text, 'user' refers to the features determined from the user who tweeted and his behavior, 'twitter' refers to twitter specific features used.", "labels": [], "entities": []}, {"text": "compares the accuracy of different classifiers for each subtask.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9993756413459778}]}], "tableCaptions": []}