{"title": [{"text": "BB twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs", "labels": [], "entities": [{"text": "BB", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9678356647491455}, {"text": "Twitter Sentiment Analysis", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.6130315562089285}]}], "abstractContent": [{"text": "In this paper we describe our attempt at producing a state-of-the-art Twitter sentiment classifier using Convolutional Neural Networks (CNNs) and Long Short Term Memory (LSTMs) networks.", "labels": [], "entities": [{"text": "Twitter sentiment classifier", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.7044654289881388}]}, {"text": "Our system leverages a large amount of unlabeled data to pre-train word embeddings.", "labels": [], "entities": []}, {"text": "We then use a subset of the unlabeled data to fine tune the embeddings using distant supervision.", "labels": [], "entities": []}, {"text": "The final CNNs and LSTMs are trained on the SemEval-2017 Twitter dataset where the embeddings are fined tuned again.", "labels": [], "entities": [{"text": "SemEval-2017 Twitter dataset", "start_pos": 44, "end_pos": 72, "type": "DATASET", "confidence": 0.9395883679389954}]}, {"text": "To boost performances we ensemble several CNNs and LSTMs together.", "labels": [], "entities": []}, {"text": "Our approach achieved first rank on all of the five English subtasks amongst 40 teams.", "labels": [], "entities": [{"text": "English subtasks", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.9568383395671844}]}], "introductionContent": [{"text": "Determining the sentiment polarity of tweets has become a landmark homework exercise in natural language processing (NLP) and data science classes.", "labels": [], "entities": [{"text": "Determining the sentiment polarity of tweets", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7846211691697439}]}, {"text": "This is perhaps because the task is easy to understand and it is also easy to get good results with very simple methods (e.g. positive -negative words counting).", "labels": [], "entities": []}, {"text": "The practical applications of this task are wide, from monitoring popular events (e.g. Presidential debates, Oscars, etc.) to extracting trading signals by monitoring tweets about public companies.", "labels": [], "entities": [{"text": "monitoring popular events (e.g. Presidential debates, Oscars, etc.)", "start_pos": 55, "end_pos": 122, "type": "TASK", "confidence": 0.7099177191654841}]}, {"text": "These applications often benefit greatly from the best possible accuracy, which is why the SemEval-2017 Twitter competition promotes research in this area.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9984251260757446}]}, {"text": "The competition is divided into five subtasks which involve standard classification, ordinal classification and distributional estimation.", "labels": [], "entities": []}, {"text": "For a more detailed description see ().", "labels": [], "entities": []}, {"text": "In the last few years, deep learning techniques have significantly out-performed traditional methods in several NLP tasks, and sentiment analysis is no exception to this trend.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.9773802757263184}]}, {"text": "In fact, previous iterations of the SemEval Twitter sentiment analysis competition have already established their power over other approaches (.", "labels": [], "entities": [{"text": "SemEval Twitter sentiment analysis", "start_pos": 36, "end_pos": 70, "type": "TASK", "confidence": 0.9068072736263275}]}, {"text": "Two of the most popular deep learning techniques for sentiment analysis are CNNs and LSTMs.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.9717995524406433}]}, {"text": "Consequently, in an effort to build a state-of-the-art Twitter sentiment classifier, we explore both models and build a system which combines both.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "2 we describe the architecture of the CNN and the LSTM used in our system.", "labels": [], "entities": [{"text": "CNN", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.9227259755134583}]}, {"text": "3 we expand on the three training phases used in our system.", "labels": [], "entities": []}, {"text": "4 we discuss the various tricks that were used to fine tune the system for each individual subtasks.", "labels": [], "entities": []}, {"text": "5 we present the performance of the system and in sec.", "labels": [], "entities": []}, {"text": "6 we outline our main conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Validation results on the historical test sets of subtask A. Bold values represent the best score  for a given test set. The 2013 test set contains 3,813 tweets, the 2014 test set contains 1,853 tweets, the  2015 test set contains 2,392 tweets and the 2016 test set contains 20,632 tweets. Word2vec, fasttext and  glove refer to the choice of algorithm in the unsupervised phase. No class weights means no weights  were used in the cost function to counteract the imbalanced classes. No distant training means that we  used the embeddings from the unsupervised phase without distant training. No fully connected layer  means we removed the fully connected hidden layer from the network. Ensemble model refers to the  ensemble model described in Sec. 3.4. The previous best historical scores were collected from (Nakov  et al., 2016). They do not come from a single system or from a single team; they are the best previous  scores obtained for each test set over the years.", "labels": [], "entities": []}, {"text": " Table 2: Correlation matrix for the most important models. System 1: CNN (word2vec, convolution  size=[3,4,5]), System 2: CNN (fasttext, convolution size=[3,4,5]), System 3: CNN (word2vec, convolu- tion size=[1,2,3]), System 4: CNN (word2vec, convolution size=[5,6,7]), System 5: LSTM (word2vec),  System 6: LSTM (fasttext).", "labels": [], "entities": []}]}