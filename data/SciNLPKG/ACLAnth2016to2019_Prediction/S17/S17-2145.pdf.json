{"title": [{"text": "funSentiment at SemEval-2017 Task 5: Fine-Grained Sentiment Anal- ysis on Financial Microblogs Using Word Vectors Built from StockTwits and Twitter", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the approach we used for SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs.", "labels": [], "entities": [{"text": "SemEval-2017 Task 5", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.9015340805053711}, {"text": "Sentiment Analysis on Financial Microblogs", "start_pos": 80, "end_pos": 122, "type": "TASK", "confidence": 0.7566928386688232}]}, {"text": "We use three types of word embeddings in our algorithm: word embeddings learned from 200 million tweets, sentiment-specific word embeddings learned from 10 million tweets using distance supervision, and word embeddings learned from 20 million StockTwits messages.", "labels": [], "entities": []}, {"text": "In our approach, we also take the left and right context of the target company into consideration when generating polarity prediction features.", "labels": [], "entities": [{"text": "polarity prediction", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.684501439332962}]}, {"text": "All the features generated from different word embeddings and contexts are integrated together to train our algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "Domain specific Sentiment Analysis has received much attention recently.", "labels": [], "entities": [{"text": "Domain specific Sentiment Analysis", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.665837250649929}]}, {"text": "The financial domain is a high-impact use case for Sentiment Analysis because it has been shown that sentiments and opinions can affect market dynamics.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9781732559204102}]}, {"text": "Given the link between sentiment and market dynamics, the analysis of public sentiment becomes a powerful method to predict the market reaction.", "labels": [], "entities": []}, {"text": "One main source of public sentiment is social media, such as In this paper, we describe our approach for SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs ().", "labels": [], "entities": [{"text": "SemEval-2017 Task 5", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.8895612160364786}, {"text": "Sentiment Analysis on Financial Microblogs", "start_pos": 139, "end_pos": 181, "type": "TASK", "confidence": 0.7961011290550232}]}, {"text": "The task is: given a microblog message, predict the sentiment score for each of the companies/stocks mentioned.", "labels": [], "entities": []}, {"text": "Sentiment values needed to be floating point values within the range of -1 (very negative/bearish) to 1 (very positive/ bullish), with 0 designating neutral sentiment.", "labels": [], "entities": []}, {"text": "Our approach uses word embeddings (WETwitter) learned from general tweets, sentiment specific word embeddings (SSWE) learned from distance supervised tweets, and word embeddings learned from StockTwits messages (WE-StockTwits).", "labels": [], "entities": []}, {"text": "Message or sentence level sentiment classification has been studied by many previous works (), but there are few studies on target-dependent, or entity level, sentiment prediction.", "labels": [], "entities": [{"text": "Message or sentence level sentiment classification", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.6308792680501938}, {"text": "entity level, sentiment prediction", "start_pos": 145, "end_pos": 179, "type": "TASK", "confidence": 0.7266874313354492}]}, {"text": "A target entity in a message does not necessarily have the same polarity type as the message, and different entities in the same message may have different polarities.", "labels": [], "entities": []}, {"text": "For example, in the tweet \"iPhone is better than Blackberry\", the two named entities, iPhone and Blackberry, will have different sentiment polarities.", "labels": [], "entities": []}, {"text": "Recent studies have focused on learning features directly from tweet text.", "labels": [], "entities": []}, {"text": "One approach is to generate sentence representations from word embeddings.", "labels": [], "entities": []}, {"text": "Several word embedding generation algorithms have been proposed in previous studies.", "labels": [], "entities": [{"text": "word embedding generation", "start_pos": 8, "end_pos": 33, "type": "TASK", "confidence": 0.7491489251454672}]}, {"text": "Using the general word embeddings directly in sentiment analysis is not effective, since they mainly model a word's semantic context, ignoring the sentiment clues in text.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.9332335889339447}]}, {"text": "Therefore, words with opposite polarity, such as worst and best, are mapped onto vectors embeddings that are close to each other in some dimensions.", "labels": [], "entities": []}, {"text": "propose a sentiment-specific word embedding (SSWE) method for sentiment analysis, by extending the word embedding algorithm.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.9564207494258881}]}, {"text": "SSWE encodes sentiment information in the word embeddings.", "labels": [], "entities": [{"text": "SSWE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8344852328300476}]}, {"text": "Many terms in financial market have different meanings, especially sentiment polarity, from that in other domains or sources, such as general news articles and Twitter.", "labels": [], "entities": []}, {"text": "For example, terms long, short, put and call have special meanings in stock market.", "labels": [], "entities": []}, {"text": "Another example is the term underestimate, which is a negative term in general, but it can suggest an opportunity to buy when used in stock market messages.", "labels": [], "entities": []}, {"text": "Therefore, in this study we also build word embedding specifically from StockTwits messages.", "labels": [], "entities": [{"text": "StockTwits messages", "start_pos": 72, "end_pos": 91, "type": "DATASET", "confidence": 0.9170932769775391}]}, {"text": "The context of an entity will affect its polarity value, and usually an entity has a left context and also aright one, unless it is at the beginning or end of a message.", "labels": [], "entities": []}, {"text": "Both the context information and the interaction between these two contexts are included in the algorithm features of our approach.", "labels": [], "entities": []}, {"text": "In this task, the financial microblogs are from StockTwits and Twitter, so in our approach, we incorporate features generated from WE-Twitter, SSWE and WE-StockTwits to represent these contexts, since they complement each other.", "labels": [], "entities": [{"text": "StockTwits", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.9461026787757874}, {"text": "WE-Twitter", "start_pos": 131, "end_pos": 141, "type": "DATASET", "confidence": 0.9373828172683716}, {"text": "WE-StockTwits", "start_pos": 152, "end_pos": 165, "type": "DATASET", "confidence": 0.8189034461975098}]}], "datasetContent": [{"text": "For this task, the training data are provided by the task organizers.", "labels": [], "entities": []}, {"text": "There are 1,704 tweets and StockTwits messages.", "labels": [], "entities": [{"text": "StockTwits", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.918539822101593}]}, {"text": "We downloaded them from Twitter and StockTwits.", "labels": [], "entities": [{"text": "StockTwits", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.9665498733520508}]}, {"text": "To build our model, we split this data set into three parts: 80% as training data and 20% as development data.", "labels": [], "entities": []}, {"text": "Since the predicted output in this task is areal value, so we use a liner regression algorithm in our approach.", "labels": [], "entities": []}, {"text": "Based on the cosine similarity metric and the evaluation data set, which consists of 800 tweets and StockTwits messages, the score of our approach is 0.7153, and our team is ranked at #6 among the 38 submissions.", "labels": [], "entities": []}], "tableCaptions": []}