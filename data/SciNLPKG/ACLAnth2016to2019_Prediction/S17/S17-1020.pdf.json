{"title": [{"text": "Evaluating Semantic Parsing against a Simple Web-based Question Answering Model", "labels": [], "entities": [{"text": "Evaluating Semantic Parsing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8194490273793539}, {"text": "Question Answering", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.6887786835432053}]}], "abstractContent": [{"text": "Semantic parsing shines at analyzing complex natural language that involves composition and computation over multiple pieces of evidence.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8603499233722687}]}, {"text": "However, datasets for semantic parsing contain many factoid questions that can be answered from a single web document.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7942739725112915}]}, {"text": "In this paper, we propose to evaluate semantic parsing-based question answering models by comparing them to a question answering baseline that queries the web and extracts the answer only from web snippets, without access to the target knowledge-base.", "labels": [], "entities": [{"text": "semantic parsing-based question answering", "start_pos": 38, "end_pos": 79, "type": "TASK", "confidence": 0.8365726321935654}]}, {"text": "We investigate this approach on COMPLEXQUESTIONS, a dataset designed to focus on composi-tional language, and find that our model obtains reasonable performance (\u223c35 F 1 compared to 41 F 1 of state-of-the-art).", "labels": [], "entities": []}, {"text": "We find in our analysis that our model performs well on complex questions involving conjunctions, but struggles on questions that involve relation composition and superlatives.", "labels": [], "entities": [{"text": "relation composition", "start_pos": 138, "end_pos": 158, "type": "TASK", "confidence": 0.7269906997680664}]}], "introductionContent": [{"text": "Question answering (QA) has witnessed a surge of interest in recent years (, as it is one of the prominent tests for natural language understanding.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9565652251243592}, {"text": "natural language understanding", "start_pos": 117, "end_pos": 147, "type": "TASK", "confidence": 0.6738054653008779}]}, {"text": "QA can be coarsely divided into semantic parsingbased QA, where a question is translated into a logical form that is executed against a knowledgebase (, and unstructured QA, where a question is answered directly from some relevant text.", "labels": [], "entities": []}, {"text": "In semantic parsing, background knowledge has already been compiled into a knowledge-base (KB), and thus the challenge is in interpreting the question, which may contain compositional constructions (\"What is the second-highest mountain in Europe?\") or computations (\"What is the difference in population between France and Germany?\").", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.7382305562496185}]}, {"text": "In unstructured QA, the model needs to also interpret the language of a document, and thus most datasets focus on matching the question against the document and extracting the answer from some local context, such as a sentence or a paragraph (.", "labels": [], "entities": []}, {"text": "Since semantic parsing models excel at handling complex linguistic constructions and reasoning over multiple facts, a natural way to examine whether a benchmark indeed requires modeling these properties, is to train an unstructured QA model, and check if it under-performs compared to semantic parsing models.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.7185706794261932}, {"text": "semantic parsing", "start_pos": 285, "end_pos": 301, "type": "TASK", "confidence": 0.7655452489852905}]}, {"text": "If questions can be answered by examining local contexts only, then the use of a knowledge-base is perhaps unnecessary.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, only models that utilize the KB have been evaluated on common semantic parsing benchmarks.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.7350782155990601}]}, {"text": "The goal of this paper is to bridge this evaluation gap.", "labels": [], "entities": []}, {"text": "We develop a simple log-linear model, in the spirit of traditional web-based QA systems (), that answers questions by querying the web and extracting the answer from returned web snippets.", "labels": [], "entities": []}, {"text": "Thus, our evaluation scheme is suitable for semantic parsing benchmarks in which the knowledge required for answering questions is covered by the web (in contrast with virtual assitants for which the knowledge is specific to an application).", "labels": [], "entities": [{"text": "semantic parsing benchmarks", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.7904214759667715}]}, {"text": "We test this model on COMPLEXQUESTIONS (, a dataset designed to require more compositionality compared to earlier datasets, such as WEBQUESTIONS () and SIMPLEQUESTIONS ( . We find that a simple QA model, despite having no access to the target KB, performs reasonably well on this dataset (\u223c35 F 1 compared to the state-of-the-art of 41 F 1 ).", "labels": [], "entities": [{"text": "WEBQUESTIONS", "start_pos": 132, "end_pos": 144, "type": "DATASET", "confidence": 0.5927176475524902}, {"text": "F 1", "start_pos": 293, "end_pos": 296, "type": "METRIC", "confidence": 0.9684002995491028}]}, {"text": "Moreover, for the subset of questions for which the right answer can be found in one of the web snippets, we outperform the semantic parser (51.9 F 1 vs. 48.5 F 1 ).", "labels": [], "entities": [{"text": "F 1", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.9621340334415436}]}, {"text": "We analyze results for different types of compositionality and find that superlatives and relation composition constructions are challenging fora webbased QA system, while conjunctions and events with multiple arguments are easier.", "labels": [], "entities": [{"text": "relation composition constructions", "start_pos": 90, "end_pos": 124, "type": "TASK", "confidence": 0.7956456939379374}]}, {"text": "An important insight is that semantic parsers must overcome the mismatch between natural language and formal language.", "labels": [], "entities": []}, {"text": "Consequently, language that can be easily matched against the web may become challenging to express in logical form.", "labels": [], "entities": []}, {"text": "For example, the word \"wife\" is anatomic binary relation in natural language, but expressed with a complex binary \u03bbx.\u03bby.Spouse(x, y) \u2227 Gender(x, Female) in knowledge-bases.", "labels": [], "entities": []}, {"text": "Thus, some of the complexity of understanding natural language is removed when working with a natural language representation.", "labels": [], "entities": []}, {"text": "To conclude, we propose to evaluate the extent to which semantic parsing-based QA benchmarks require compositionality by comparing semantic parsing models to a baseline that extracts the answer from short web snippets.", "labels": [], "entities": [{"text": "semantic parsing-based QA", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.8039861520131429}]}, {"text": "We obtain reasonable performance on COMPLEXQUESTIONS, and analyze the types of compositionality that are challenging fora web-based QA model.", "labels": [], "entities": [{"text": "COMPLEXQUESTIONS", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.8627539873123169}]}, {"text": "To ensure reproducibility, we release our dataset, which attaches to each example from COMPLEXQUES-TIONS the top-100 retrieved web snippets.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given a training set of triples , where q (i) is a question, R (i) is a web result set, and a (i) is the answer, our goal is to learn a model that produces an answer a fora new questionresult set pair (q, R).", "labels": [], "entities": []}, {"text": "A web result set R consists of K(= 100) web snippets, where each snippet s i  COMPLEXQUESTIONS contains 1,300 training examples and 800 test examples.", "labels": [], "entities": []}, {"text": "We performed 5 random 70/30 splits of the training set for development.", "labels": [], "entities": []}, {"text": "We computed POS tags and named entities with Stanford CoreNLP ( ).", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.9278316497802734}]}, {"text": "We did not employ any co-reference resolution tool in this work.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.7413382828235626}]}, {"text": "If after candidate extraction, we do not find the gold answer in the top-K(=140) candidates, we discard the example, resulting in a training set of 856 examples.", "labels": [], "entities": [{"text": "candidate extraction", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.7434102594852448}]}, {"text": "We compare our model, WEBQA, to STAGG (Yih et al., 2015) and COMPQ (, which are to the best of our knowledge the highest performing semantic parsing models on both COMPLEXQUESTIONS and WEBQUES-TIONS.", "labels": [], "entities": [{"text": "WEBQA", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.81269770860672}, {"text": "semantic parsing", "start_pos": 132, "end_pos": 148, "type": "TASK", "confidence": 0.749211847782135}, {"text": "WEBQUES-TIONS", "start_pos": 185, "end_pos": 198, "type": "DATASET", "confidence": 0.9406774640083313}]}, {"text": "For these systems, we only report test F 1 numbers that are provided in the original papers, as   between the set of answers returned by the system and the set of gold answers, and averages across questions.", "labels": [], "entities": [{"text": "F 1 numbers", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.8615445295969645}]}, {"text": "To allow WEBQA to return a set rather than a single answer, we return the most probable answer a * as well as any answer a such that (\u03c6(q, R, a * ) \u03b8 \u2212 \u03c6(q, R, a) \u03b8) < 0.5.", "labels": [], "entities": [{"text": "WEBQA", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.597927451133728}]}, {"text": "We also compute precision@1 and Mean Reciprocal Rank (MRR) for WEBQA, since we have a ranking over answers.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9987416863441467}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 32, "end_pos": 58, "type": "METRIC", "confidence": 0.9664036333560944}, {"text": "WEBQA", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.7230212688446045}]}, {"text": "To compute metrics we lowercase the gold and predicted spans and perform exact string match.", "labels": [], "entities": [{"text": "exact string match", "start_pos": 73, "end_pos": 91, "type": "METRIC", "confidence": 0.8351032535235087}]}, {"text": "presents the results of our evaluation.", "labels": [], "entities": []}, {"text": "WEBQA obtained 32.6 F 1 (33.5 p@1, 42.4 MRR) compared to 40.9 F 1 of COMPQ.", "labels": [], "entities": [{"text": "WEBQA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9381908178329468}, {"text": "F 1", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.992346853017807}, {"text": "MRR", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9972266554832458}, {"text": "F 1", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9335646331310272}, {"text": "COMPQ", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.9300219416618347}]}, {"text": "Our candidate extraction step finds the correct answer in the top-K candidates in 65.9% of development examples and 62.7% of test examples.", "labels": [], "entities": [{"text": "candidate extraction", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7186135351657867}]}, {"text": "Thus, our test F 1 on examples for which candidate extraction succeeded (WEBQA-SUBSET) is 51.9 (53.4 p@1, 67.5 MRR).", "labels": [], "entities": [{"text": "F 1", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9647039175033569}, {"text": "candidate extraction", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7338677644729614}, {"text": "WEBQA-SUBSET)", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.5641049593687057}, {"text": "MRR", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9966849684715271}]}, {"text": "We were able to indirectly compare WEBQA-SUBSET to COMPQ: graciously provided us with the predictions of COMPQ when it was trained on COMPLEXQUESTIONS, WE-BQUESTIONS, and SIMPLEQUESTIONS.", "labels": [], "entities": [{"text": "WEBQA-SUBSET", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.8680976629257202}, {"text": "WE-BQUESTIONS", "start_pos": 152, "end_pos": 165, "type": "DATASET", "confidence": 0.6990646719932556}]}, {"text": "In this  setup, COMPQ obtained 42.2 F 1 on the test set (compared to 40.9 F 1 , when training on COM-PLEXQUESTIONS only, as we do).", "labels": [], "entities": [{"text": "F 1", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9698642194271088}, {"text": "F 1", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9749908149242401}]}, {"text": "Restricting the predictions to the subset for which candidate extraction succeeded, the F 1 of COMPQ-SUBSET is 48.5, which is 3.4 F 1 points lower than WEBQA-SUBSET, which was trained on less data.", "labels": [], "entities": [{"text": "candidate extraction", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.7145914435386658}, {"text": "F 1", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9971258640289307}, {"text": "COMPQ-SUBSET", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.7860881090164185}, {"text": "F 1", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.9801755547523499}, {"text": "WEBQA-SUBSET", "start_pos": 152, "end_pos": 164, "type": "DATASET", "confidence": 0.838951826095581}]}, {"text": "Not using a KB, results in a considerable disadvantage for WEBQA.", "labels": [], "entities": [{"text": "WEBQA", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.6721304059028625}]}, {"text": "KB entities have normalized descriptions, and the answers have been annotated according to those descriptions.", "labels": [], "entities": []}, {"text": "We, conversely, find answers on the web and often predict a correct answer, but get penalized due to small string differences.", "labels": [], "entities": []}, {"text": "E.g., for \"what is the longest river in China?\" we answer \"yangtze river\", while the gold answer is \"yangtze\".", "labels": [], "entities": []}, {"text": "To quantify this effect we manually annotated all 258 examples in the first random development set split, and determined whether string matching failed, and we actually returned the gold answer.", "labels": [], "entities": [{"text": "string matching", "start_pos": 129, "end_pos": 144, "type": "TASK", "confidence": 0.6984919011592865}]}, {"text": "This improved performance from 53.6 F 1 to 56.6 F 1 (on examples that passed candidate extraction).", "labels": [], "entities": [{"text": "F 1", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.941129744052887}, {"text": "F 1", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9419027268886566}, {"text": "candidate extraction", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.7345622181892395}]}, {"text": "Further normalizing gold and predicted entities, such that \"Hillary Clinton\" and \"Hillary Rodham Clinton\" are unified, improved F 1 to 57.3 F 1 . Extrapolating this to the test set would result in an F 1 of 34.4 (WEBQA-EXTRAPOL in) and 34.9, respectively.", "labels": [], "entities": [{"text": "F 1", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.9949278831481934}, {"text": "F 1", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9547294974327087}, {"text": "F 1", "start_pos": 200, "end_pos": 203, "type": "METRIC", "confidence": 0.9953280091285706}, {"text": "WEBQA-EXTRAPOL", "start_pos": 213, "end_pos": 227, "type": "METRIC", "confidence": 0.7394548058509827}]}, {"text": "Last, to determine the contribution of each feature template, we performed ablation tests and we present the five feature templates that resulted in the largest drop to performance on the development set in.", "labels": [], "entities": []}, {"text": "Note that TF-IDF is by far the most impactful feature, leading to a large drop of 12 points in performance.", "labels": [], "entities": []}, {"text": "This shows the importance of using the redundancy of the web for our QA system.", "labels": [], "entities": []}, {"text": "ally annotated the compositionality type of 100 random examples that passed candidate extraction and 50 random examples that failed candidate extraction.", "labels": [], "entities": []}, {"text": "presents the results of this analysis, as well as the average F 1 obtained for each compositionality type on the 100 examples that passed candidate extraction (note that a question can belong to multilpe compositionality types).", "labels": [], "entities": [{"text": "F 1", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9911361932754517}]}, {"text": "We observe that COMPOSITION and SUPERLATIVE questions are challenging for WE-BQA, while SIMPLE, FILTER, and N-ARY quesitons are easier (recall that a large fraction of the questions in COMPLEXQUESTIONS are N-ARY).", "labels": [], "entities": [{"text": "WE-BQA", "start_pos": 74, "end_pos": 80, "type": "DATASET", "confidence": 0.5073952078819275}, {"text": "FILTER", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9930849075317383}]}, {"text": "Interestingly, WEBQA performs well on CON-JUNCTION questions (\"what film victor garber starred in that rob marshall directed\"), possibly because the correct answer can obtain signal from multiple snippets.", "labels": [], "entities": [{"text": "WEBQA", "start_pos": 15, "end_pos": 20, "type": "DATASET", "confidence": 0.6967552304267883}]}, {"text": "An advantage of finding answers to questions from web documents compared to semantic parsing, is that we do not need to learn the \"language of the KB\".", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.7508679330348969}]}, {"text": "For example, the question \"who is the governor of California 2010\" can be matched directly to web snippets, while in Freebase () the word \"governor\" is expressed by a complex predicate \u03bbx.\u2203z.GoverPos(x, z) \u2227 PosTitle(z, Governor).", "labels": [], "entities": []}, {"text": "This could provide a partial explanation for the reasonable performance of WEBQA.", "labels": [], "entities": [{"text": "WEBQA", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.8058601021766663}]}], "tableCaptions": [{"text": " Table 3: Results on development (average over random splits)  and test set. Middle: results on all examples. Bottom: results  on the subset where candidate extraction succeeded.", "labels": [], "entities": [{"text": "Middle", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9659377336502075}, {"text": "candidate extraction", "start_pos": 147, "end_pos": 167, "type": "TASK", "confidence": 0.7023074179887772}]}, {"text": " Table 4: Feature ablation results. The five features that lead  to largest drop in performance are displayed.", "labels": [], "entities": []}]}