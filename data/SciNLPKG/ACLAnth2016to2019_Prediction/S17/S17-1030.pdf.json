{"title": [{"text": "Ways of Asking and Replying in Duplicate Question Detection", "labels": [], "entities": [{"text": "Asking and Replying in Duplicate Question Detection", "start_pos": 8, "end_pos": 59, "type": "TASK", "confidence": 0.7070683198315757}]}], "abstractContent": [{"text": "This paper presents the results of systematic experimentation on the impact in duplicate question detection of different types of questions across both a number of established approaches and a novel, superior one used to address this language processing task.", "labels": [], "entities": [{"text": "duplicate question detection", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.6766960024833679}]}, {"text": "This study permits to gain a novel insight on the different levels of robustness of the diverse detection methods with respect to different conditions of their application, including the ones that approximate real usage scenarios.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic detection of semantically equivalent questions is a language processing task of the utmost importance given the upsurge of interest in conversational interfaces.", "labels": [], "entities": [{"text": "Automatic detection of semantically equivalent questions", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.8704767127831777}]}, {"text": "It is a key procedure in finding answers to questions.", "labels": [], "entities": [{"text": "finding answers to questions", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.8432075381278992}]}, {"text": "For instance, in a context of customer support via a chat channel, with the help of duplicate question detection, previous interactions between customers and human operators can be explored to provide an increasingly automatic question answering service.", "labels": [], "entities": [{"text": "question detection", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.6971286833286285}, {"text": "question answering", "start_pos": 227, "end_pos": 245, "type": "TASK", "confidence": 0.7212427705526352}]}, {"text": "If anew input question is equivalent to a question already stored, it can be replied automatically with the answer stored with its recorded duplicate.", "labels": [], "entities": []}, {"text": "Though it has been less researched than similar tasks, duplicate question detection (DQD) is attracting an increasing interest.", "labels": [], "entities": [{"text": "duplicate question detection (DQD)", "start_pos": 55, "end_pos": 89, "type": "TASK", "confidence": 0.7182434548934301}]}, {"text": "It can be seen as belonging to a family of semantic text similarity tasks, which have been addressed in SemEval challenges since 2012, and which in the last SemEval2016, for instance, included also tasks like plagiarism detection or degree of similarity between machine translation output and its postedited version, among others.", "labels": [], "entities": [{"text": "plagiarism detection", "start_pos": 209, "end_pos": 229, "type": "TASK", "confidence": 0.7610885798931122}]}, {"text": "Semantic textual similarity assesses the degree to which two textual segments are semantically equivalent to each other, which is typically scored on an ordinal scale ranging from semantic equivalence to complete semantic dissimilarity.", "labels": [], "entities": [{"text": "Semantic textual similarity", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6926183501879374}]}, {"text": "Paraphrase detection can be seen as a special case of semantic textual similarity, where the scale is reduced to its two extremes and the outcome for an input pair is yes/no.", "labels": [], "entities": [{"text": "Paraphrase detection", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9436951577663422}]}, {"text": "DQD, in turn, could be seen as a special case of paraphrase detection that is restricted to interrogative expressions.", "labels": [], "entities": [{"text": "DQD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6901047825813293}, {"text": "paraphrase detection", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.8525470793247223}]}, {"text": "While SemEval2016 had no task on yes/no DQD, it had a \"Question-Question\" graded similarity subtask of Task 1.", "labels": [], "entities": []}, {"text": "The top performing system in this subtask (0.74 Pearson correlation) scored below the best result when all subtasks of Task 1 are considered (0.77), and also below the best scores of many of the other subtasks (e.g. 0.84 in plagiarism detection ().", "labels": [], "entities": [{"text": "Pearson correlation)", "start_pos": 48, "end_pos": 68, "type": "METRIC", "confidence": 0.9591448704401652}, {"text": "plagiarism detection", "start_pos": 224, "end_pos": 244, "type": "TASK", "confidence": 0.7144489288330078}]}, {"text": "While scores obtained for different tasks by systems trained and evaluated over different datasets cannot be compared, those results nonetheless lead one to ponder whether focusing on pairs of interrogatives maybe a task that is harder than paraphrase detection that focuses on pairs of noninterrogatives (e.g. plagiarism pairs), or at least whether it needs different and specific approaches for similar levels of performance to be attained.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 241, "end_pos": 261, "type": "TASK", "confidence": 0.7994935810565948}]}, {"text": "When checking for other research results specifically addressing DQD, pretty competitive results can be found, however, as in.", "labels": [], "entities": []}, {"text": "These authors used a dataset that included a dump from the Meta forum in StackExchange (a source that would be explored also in SemEval2016) and a dump from the AskUbuntu forum, and reported over 92% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 200, "end_pos": 208, "type": "METRIC", "confidence": 0.9968208074569702}]}, {"text": "The pairs in these datasets are made of the textual segments that are submitted by the users of the forums to elicit some feedback from other users that maybe of help, and that will pileup in threads of reactions.", "labels": [], "entities": []}, {"text": "They have two parts, known as \"ti-tle\" and \"body\".", "labels": [], "entities": []}, {"text": "The title tends to be a short segment identifying the issue being addressed, and the body is where that issue is expanded, and can be several paragraphs long.", "labels": [], "entities": []}, {"text": "To avoid amaze of exactly duplicate questions, and thus of duplicate threads, which would hamper the usability of the forums, for the same issue, all duplicates except one are removed, leaving only near duplicates-that are marked as such and cross-linked to each other, and maybe of help in addressing the same topic from a different angle.", "labels": [], "entities": []}, {"text": "The pairs of duplicate segments included in the experimental datasets mentioned above are the titles and bodies of nearly duplicate threads.", "labels": [], "entities": []}, {"text": "The pairs of non-duplicate segments are made of titles and bodies that are not near duplicate.", "labels": [], "entities": []}, {"text": "While these \"real life\" data are important for the development of DQD solutions that support the management of these community forums, their textual segments are quite far from expressions in clean and clear interrogative form.", "labels": [], "entities": []}, {"text": "The short supply of this sort of datasets has been perhaps part of the reason why the DQD has not been more researched.", "labels": [], "entities": [{"text": "DQD", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.8429540395736694}]}, {"text": "This may help to explain also the lack of further studies so far on how the nature of the questions and the data may impact the performance of the systems on this task.", "labels": [], "entities": []}, {"text": "The experiments reported in this paper aim to address this issue and help to advance our understanding of the nature of DQD and to improve its application.", "labels": [], "entities": []}, {"text": "We will resort to previous datasets used in the literature, just mentioned above, but we will seek to explore also anew dataset from Quora, released recently, in January 2017.", "labels": [], "entities": [{"text": "Quora", "start_pos": 133, "end_pos": 138, "type": "DATASET", "confidence": 0.9118269085884094}]}, {"text": "The pairs of segments in this Quora dataset concern any subject and are thus not restricted to any domain.", "labels": [], "entities": [{"text": "Quora dataset", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.9072140157222748}]}, {"text": "The segments are typically one sentence long, clean and clear interrogative expressions.", "labels": [], "entities": []}, {"text": "Their grammatical well-formedness is ensured by the volunteer experts that answer them and that, before writing their replies, can use the editing facility to adjust the wording of the question entered by the user if needed.", "labels": [], "entities": []}, {"text": "This is in clear contrast with the other datasets extracted from community forums.", "labels": [], "entities": []}, {"text": "The forums are organized by specific domains.", "labels": [], "entities": []}, {"text": "The segments maybe several sentences long and are typically offered in a sloppy wording, with non-standard expressions and suboptimal grammaticality.", "labels": [], "entities": []}, {"text": "By resorting only to data of the latter type, confirmed that systems trained (and evaluated) on a smaller dataset that is domain specific can perform substantially better than when they are trained (and evaluated) on a larger dataset from a generic domain.", "labels": [], "entities": []}, {"text": "In this paper, we seek to further advance the understanding of DQD and possible constraints on their development and application.", "labels": [], "entities": []}, {"text": "We assess the level of impact of the length of the segments in the pairs, and study whether there is a difference when systems handle well-edited, generic domain segments, versus domain specific and sloppy ones.", "labels": [], "entities": []}, {"text": "As the datasets with labeled pairs of segments are scarce, to develop a system to anew specific domain lacking a training dataset, the natural way to go is to train it on a generic domain dataset.", "labels": [], "entities": []}, {"text": "We also study the eventual loss of performance in this real usage scenario.", "labels": [], "entities": []}, {"text": "These empirical contrasts may have a different impact in different types of approaches to DQD.", "labels": [], "entities": []}, {"text": "The present study will be undertaken across a range of different techniques, encompassing a rule-based baseline, a classifier-based system and solutions based on neural networks.", "labels": [], "entities": []}, {"text": "To secure comparability of the individual results, the experimental datasets used are organized along common settings.", "labels": [], "entities": []}, {"text": "They have the same volume (30K pairs), the same training vs. testing split rate (80%/20%), and the same class balance (50%/50% of duplicates and non-duplicates).", "labels": [], "entities": [{"text": "testing split rate", "start_pos": 61, "end_pos": 79, "type": "METRIC", "confidence": 0.6086860398451487}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, the datasets used are described. and 5 present the experimental results of a range of different detection techniques, respectively, rulebased, supervised classifiers and neural networks.", "labels": [], "entities": []}, {"text": "In section 6, the results obtained are discussed, and further experiments are reported in Section 7, approximating areal usage scenarios of application.", "labels": [], "entities": []}, {"text": "Sections 8 and 9 present the related work and the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used two datasets, from two sources: 1 (i) from the AskUbuntu online community forum where a query entered by a user (in the form of a title followed by a body) is answered with contributions from any other user (which are piled up in a thread); and (ii) from Quora, an online moderated question answering site where each query introduced by a user, typically in a grammatical inter-  The other dataset used here is similar to one of the datasets used by.", "labels": [], "entities": [{"text": "AskUbuntu online community forum where a query entered by a user (in the form of a title followed by a body) is answered with contributions from any other user (which are piled up in a thread)", "start_pos": 55, "end_pos": 247, "type": "Description", "confidence": 0.8195022478699684}, {"text": "question answering site", "start_pos": 290, "end_pos": 313, "type": "TASK", "confidence": 0.767014354467392}]}, {"text": "It is made of queries from the AskUbuntu forum, 3 which are thus on a specific domain, namely from the IT area, in particular about the Ubuntu operative system.", "labels": [], "entities": [{"text": "AskUbuntu forum", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.9095230400562286}]}, {"text": "We used AskUbuntu dump available, from September 2014, 4 containing 167,765 questions, of which 17,115 were labeled as a duplicate.", "labels": [], "entities": [{"text": "AskUbuntu dump", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.9188709259033203}]}, {"text": "A portion with 30k randomly selected pairs of title+body was extracted, the same size as the portion used by.", "labels": [], "entities": []}, {"text": "This portion is balanced, thus with an identical number of duplicate and non-duplicate pairs.", "labels": [], "entities": []}, {"text": "To support the experiments described below, it was divided into 24k/6k for training/testing, an 80%/20% split.", "labels": [], "entities": []}, {"text": "The textual segments in this dataset contain both the title and the body of the query in the corresponding thread, and this dataset is referred to as AskUbuntuTB, while its counterpart with titles only-obtained by removing the bodies-is referred to as AskUbuntuTO.", "labels": [], "entities": []}, {"text": "To support comparison, a portion with 30k randomly selected pairs was extracted also from the Quora release, with the same duplicate vs. nonduplicate balance and the same training vs. test split rates as for the AskUbuntu dataset.", "labels": [], "entities": [{"text": "Quora release", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.9317466914653778}, {"text": "AskUbuntu dataset", "start_pos": 212, "end_pos": 229, "type": "DATASET", "confidence": 0.9623929560184479}]}, {"text": "The average length of the segments in number of words is 84 in AskUbuntuTB.", "labels": [], "entities": [{"text": "AskUbuntuTB", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.9765706658363342}]}, {"text": "Its counterpart AskUbuntuTO, with titles only, represent a very substantial (10 times) drop to 8 words per segment on average, which is similar to the 10 words per segment in the Quora dataset.", "labels": [], "entities": [{"text": "Quora dataset", "start_pos": 179, "end_pos": 192, "type": "DATASET", "confidence": 0.9650329351425171}]}, {"text": "The vocabularies sizes of AskUbuntuTB, AskUbuntuTO and Quora are 45k, 16k and 24k items, respectively, and their volumes are 5M, 500k and 650k tokens, respectively.", "labels": [], "entities": [{"text": "AskUbuntuTB", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9512297511100769}, {"text": "AskUbuntuTO", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.9389325380325317}]}, {"text": "Concerning the 400k pair Quora release, in turn, it contains 9M tokens and a 125k item vocabulary.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of the 6 systems (columns) over the 3 datasets (lines)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981426000595093}]}]}