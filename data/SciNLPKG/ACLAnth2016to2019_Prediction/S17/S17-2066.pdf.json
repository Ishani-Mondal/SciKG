{"title": [{"text": "TakeLab at SemEval-2017 Task 6: #RankingHumorIn4Pages", "labels": [], "entities": [{"text": "RankingHumorIn4Pages", "start_pos": 33, "end_pos": 53, "type": "DATASET", "confidence": 0.795931875705719}]}], "abstractContent": [{"text": "This paper describes our system for humor ranking in tweets within the SemEval 2017 Task 6: #HashtagWars (6A and 6B).", "labels": [], "entities": [{"text": "SemEval 2017 Task", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.6890161236127218}]}, {"text": "For both subtasks, we use an off-the-shelf gradient boosting model built on a rich set of features, handcrafted to provide the model with the external knowledge needed to better predict the humor in the text.", "labels": [], "entities": []}, {"text": "The features capture various cultural references and specific humor patterns.", "labels": [], "entities": []}, {"text": "Our system ranked 2nd (officially 7th) among 10 submissions on the Subtask A and 2nd among 9 submissions on the Subtask B.", "labels": [], "entities": []}], "introductionContent": [{"text": "While extremely interesting, understanding humor expressed in text is a challenging natural language problem.", "labels": [], "entities": [{"text": "understanding humor expressed in text", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.8497655272483826}]}, {"text": "Besides standard ambiguity of natural language, humor is also highly subjective and lacks an universal definition).", "labels": [], "entities": []}, {"text": "Moreover, humor should almost never betaken at face value, as its understanding often requires a broader context -external knowledge and commonsense.", "labels": [], "entities": []}, {"text": "On top of that, what is funny today might not be funny tomorrow, as humor goes hand in hand with ever-changing trends of popular culture.", "labels": [], "entities": []}, {"text": "Even though there has been some work on humor generation), most work has been concerned with humor detection, a task of classifying whether a given text snippet is humorous.", "labels": [], "entities": [{"text": "humor generation", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.7315660864114761}, {"text": "humor detection", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.7090110927820206}]}, {"text": "However, this research was mostly focused on a simple binary detection of humor.", "labels": [], "entities": [{"text": "binary detection of humor", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.7679104506969452}]}, {"text": "In this paper, we describe a system for ranking humor in tweets, which we participated within the.", "labels": [], "entities": []}, {"text": "It comprised two subtasks, one dealing with predicting which tweet out of two is more humorous, and other with ranking a set of tweets by their humorousness.", "labels": [], "entities": []}, {"text": "Even though these tasks can be both posed and tackled differently, we straightforwardly used the obtained pairwise classifications from the first task in coming up with ranked lists for the second.", "labels": [], "entities": []}, {"text": "Our system uses a standard gradient boosting classifier (GB) based on a rich set of features and collections of external knowledge.", "labels": [], "entities": [{"text": "gradient boosting classifier (GB)", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.6222246438264847}]}, {"text": "We ranked 2nd among 10 submissions (7th officially) at the Subtask 6A, and 2nd among 9 submissions at the Subtask 6B.", "labels": [], "entities": [{"text": "Subtask 6A", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.9022887945175171}, {"text": "Subtask 6B", "start_pos": 106, "end_pos": 116, "type": "DATASET", "confidence": 0.927513062953949}]}], "datasetContent": [{"text": "The subtask 6A was evaluated in terms of accuracy (higher is better), whereas the subtask 6B was evaluated in terms of a metric inspired by edit distance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9993811845779419}]}, {"text": "The metric captures how many moves the tweet must make to fall into a correct bin (lower is better).", "labels": [], "entities": []}, {"text": "This metric was normalized by the maximum possible edit distance.", "labels": [], "entities": []}, {"text": "Both metrics were micro-averaged across hashtags.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: External knowledge collections we used  in the model.", "labels": [], "entities": []}, {"text": " Table 2: Final rankings on the subtask 6A. Our  submissions are bolded.", "labels": [], "entities": []}, {"text": " Table 3: Final rankings on the subtask 6B. Our  submissions are bolded.", "labels": [], "entities": []}]}