{"title": [{"text": "QLUT at SemEval-2017 Task 2: Word Similarity Based on Word Embedding and Knowledge Base", "labels": [], "entities": [{"text": "QLUT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7632670998573303}, {"text": "Word Similarity", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.6680944561958313}]}], "abstractContent": [{"text": "This paper shows the details of our system submissions in the task 2 of SemEval 2017.", "labels": [], "entities": []}, {"text": "We take part in the subtask 1 of this task, which is an English monolingual sub-task.", "labels": [], "entities": []}, {"text": "This task is designed to evaluate the semantic word similarity of two linguistic items.", "labels": [], "entities": []}, {"text": "The results of runs are assessed by standard Pearson and Spearman correlation , contrast with official gold standard set.", "labels": [], "entities": [{"text": "Pearson and Spearman correlation", "start_pos": 45, "end_pos": 77, "type": "METRIC", "confidence": 0.809622198343277}]}, {"text": "The best performance of our runs is 0.781 (Final).", "labels": [], "entities": []}, {"text": "The techniques of our runs mainly make use of the word embeddings and the knowledge-based method.", "labels": [], "entities": []}, {"text": "The results demonstrate that the combined method is effective for the computation of word similarity, while the word embeddings and the knowledge-based technique, respectively , needs more deeply improvement in details.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.6720012426376343}]}], "introductionContent": [{"text": "Semantic word similarity aims at measuring the extent to which two words are similar.", "labels": [], "entities": [{"text": "Semantic word similarity", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7031625111897787}]}, {"text": "Given two words, the runs in this competition should give a score which indicates the similarity between them, and it will be evaluated by the official gold standard set.", "labels": [], "entities": []}, {"text": "This task doesn't offer any annotated corpus and the organizers encourage systems to utilize unlabeled corpus.", "labels": [], "entities": []}, {"text": "With the development of word embeddings technique, more and more attentions are paid to it).", "labels": [], "entities": []}, {"text": "We also adopt the word embeddings ________________________ * Corresponding author method in our runs.", "labels": [], "entities": []}, {"text": "Besides the word embeddings method, another knowledge-based method is proposed by us, which is based on BabelNet (.", "labels": [], "entities": []}, {"text": "Integrating Wikipedia and WordNet, BabelNet is a multilingual encyclopedic and lexicographic knowledge base, which builds an enormous semantic network linking concepts and named entities with the aid of a large semantic relations.", "labels": [], "entities": []}, {"text": "Based on the word embedding method and the knowledge-based method, a combined method is implemented, which achieves the best performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "Test Set: In this task, we submit our runs on the English monolingual word similarity dataset, which includes 500 word pairs.", "labels": [], "entities": [{"text": "English monolingual word similarity dataset", "start_pos": 50, "end_pos": 93, "type": "DATASET", "confidence": 0.5635010302066803}]}, {"text": "These word pairs maybe concepts or named entities, which are tabseparated.", "labels": [], "entities": []}, {"text": "Gold Standard Set: This set is gold standard set, which is annotated by official annotators.", "labels": [], "entities": [{"text": "Gold Standard Set", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9113958676656088}]}, {"text": "Each line in this set is a similarity value according to the test set describe above in rating scale.", "labels": [], "entities": []}, {"text": "4 shows that the two words are very similar, i.e., synonyms; 3 means that the two words are similar, but have slightly different details; 2 represents that the two words are slightly similar, having a topic/domain/function and ideas or related concepts in common; 1 shows that the two words are dissimilar, which only having some small details in common.", "labels": [], "entities": []}, {"text": "0 means that the two words are totally dissimilar.", "labels": [], "entities": []}, {"text": "Run1: This run uses the word embeddings method described in Section 2.2.", "labels": [], "entities": []}, {"text": "Given two words or phrases, it can get the semantic similarity by computing the cosine between their word vectors.", "labels": [], "entities": []}, {"text": "Run2: This run use the combined method described in Section 2.4.", "labels": [], "entities": []}, {"text": "It can leverage the word embeddings method and knowledge-based method.", "labels": [], "entities": []}, {"text": "Runkb: This run use the knowledge-based method which is described in Section 2.3.", "labels": [], "entities": [{"text": "Runkb", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9449636936187744}]}, {"text": "The runs are evaluated according to the measures of standard Pearson and Spearman correlation.", "labels": [], "entities": [{"text": "Pearson and Spearman correlation", "start_pos": 61, "end_pos": 93, "type": "METRIC", "confidence": 0.6100000850856304}]}, {"text": "The final score (see the last column in   last row) is the baseline system which is created by the official of this task.", "labels": [], "entities": []}, {"text": "As we can see in that the system Run2 make a 9.5% (Final) improvement in contrast with the baseline system (NASARI), and achieves the best performance.", "labels": [], "entities": [{"text": "Run2", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.8444732427597046}, {"text": "Final)", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9840415418148041}]}, {"text": "The performance of the system Run1 does not exceed the baseline system.", "labels": [], "entities": []}, {"text": "shows that the system Runkb get its best performance when \uf06d is set to 1.4 (see 2.3).", "labels": [], "entities": [{"text": "Runkb", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.8879109621047974}]}, {"text": "shows that Run2 get its best performance when \uf061 is set to 0.4 instead of 0.6 (see 2.4).", "labels": [], "entities": [{"text": "Run2", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.7878057360649109}]}, {"text": "These results show that the word embeddings method and the knowledge-based method, respectively, are not enough effective while the combined method of them makes the best performance of 0.781 in all our runs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of our runs and baseline.", "labels": [], "entities": []}, {"text": " Table 2: Results of Runkb with various pa- rameters.", "labels": [], "entities": [{"text": "Runkb", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.8737886548042297}]}, {"text": " Table 3: Results of Run2 with various param- eters.", "labels": [], "entities": [{"text": "Run2", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.8966683149337769}]}]}