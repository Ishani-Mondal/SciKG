{"title": [{"text": "QU-BIGIR at SemEval 2017 Task 3: Using Similarity Features for Arabic Community Question Answering Forums", "labels": [], "entities": [{"text": "SemEval 2017 Task", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7898645202318827}, {"text": "Arabic Community Question Answering Forums", "start_pos": 63, "end_pos": 105, "type": "TASK", "confidence": 0.7241729021072387}]}], "abstractContent": [{"text": "In this paper, we describe our QU-BIGIR system for the Arabic subtask D of the Se-mEval 2017 Task 3.", "labels": [], "entities": [{"text": "Arabic subtask D of the Se-mEval 2017 Task 3", "start_pos": 55, "end_pos": 99, "type": "DATASET", "confidence": 0.6312019162707858}]}, {"text": "Our approach builds on our participation in the past version of the same subtask.", "labels": [], "entities": []}, {"text": "This year, our system uses different similarity features that encodes lexical and semantic pairwise similarity of text pairs.", "labels": [], "entities": []}, {"text": "In addition to well-known similarity measures such as cosine similarity, we use other measures based on the summary statistics of word embedding representation fora given text.", "labels": [], "entities": []}, {"text": "To rank a list of candidate question-answer pairs fora given question, we train a linear SVM classifier over our similarity features.", "labels": [], "entities": []}, {"text": "Our best resulting run came second in subtask D with a very competitive performance to the first-ranking system.", "labels": [], "entities": [{"text": "subtask D", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.7293304204940796}]}], "introductionContent": [{"text": "The ubiquitous presence of community question answering (CQA) websites has motivated research on building automatic question answering (QA) systems that can benefit from previously-answered questions to answer newly-posed ones.", "labels": [], "entities": [{"text": "community question answering (CQA)", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.8269258737564087}, {"text": "question answering (QA)", "start_pos": 116, "end_pos": 139, "type": "TASK", "confidence": 0.8494173169136048}]}, {"text": "A core functionality of such systems is their ability to effectively rank previouslysuggested answers with respect to their degree/probability of relevance to a posted question.", "labels": [], "entities": []}, {"text": "Ranking is vital to push away irrelevant and low quality answers, which are commonplace in CQA as they are generally open with no restrictions on who can post or answer questions.", "labels": [], "entities": [{"text": "CQA", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.9368709921836853}]}, {"text": "To this effect, SemEval 2017 Task 3 \"Community Question Answering\" has emphasized the ranking component in the main task of the challenge.", "labels": [], "entities": [{"text": "SemEval 2017 Task 3", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7547889947891235}, {"text": "Community Question Answering", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.6042220691839854}]}, {"text": "shows an example of a question and four of its 30 given candidate question-answer pairs.", "labels": [], "entities": []}, {"text": "Further details about SemEval 2017 Task 3 can be found in (.", "labels": [], "entities": [{"text": "SemEval 2017 Task 3", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8049065172672272}]}, {"text": "In this paper, we describe the system we developed to participate in that task.", "labels": [], "entities": []}, {"text": "The system leverages a supervised learning approach over similarity features.", "labels": [], "entities": []}, {"text": "We utilize two types of similarity features.", "labels": [], "entities": []}, {"text": "First, we employ similarity features based on term representation fora given pairs of text.", "labels": [], "entities": []}, {"text": "Second, we utilize word2vec to build text representation following the same approach as in our last year's submission for the same subtask.", "labels": [], "entities": []}, {"text": "We used similarity features based on that text representation to encode the semantic similarity for pairs of texts.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows; the approach and description of features are in-troduced in section 2; the experimental setup followed in our submitted runs and the results are presented in section 3.", "labels": [], "entities": []}, {"text": "Finally we conclude our study with final remarks in section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present the experimental setup and results of our primary, contrastive-1 and contrastive-2 submissions.", "labels": [], "entities": []}, {"text": "We used the Arabic collection of questions and their potentially related question-answer pairs provided by Task 3 organizers to train our word embedding model.", "labels": [], "entities": []}, {"text": "The Gensim 3 tool was used to generate the word2vec model from training data 4 , setting d = 100.", "labels": [], "entities": []}, {"text": "We used the learned model to compute our features as described in section 2.", "labels": [], "entities": []}, {"text": "Features were generated for the three data setups described in section 2.1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The official scores attained by our primary and contrastive submissions to SemEval 2017 Task 3-SubTask D", "labels": [], "entities": [{"text": "SemEval 2017 Task 3-SubTask D", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.8221768379211426}]}, {"text": " Table 2: The development set MAP scores obtained by our", "labels": [], "entities": [{"text": "MAP", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.5580220222473145}]}]}