{"title": [{"text": "TakeLab at SemEval-2017 Task 4: Recent Deaths and the Power of Nostalgia in Sentiment Analysis in Twitter", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.881273090839386}]}], "abstractContent": [{"text": "This paper describes the system we submitted to SemEval-2017 Task 4 (Sen-timent Analysis in Twitter), specifically subtasks A, B, and D.", "labels": [], "entities": [{"text": "SemEval-2017 Task 4", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7686832149823507}, {"text": "Sen-timent Analysis in Twitter)", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.8177346587181091}]}, {"text": "Our main focus was topic-based message polarity classification on a two-point scale (subtask B).", "labels": [], "entities": [{"text": "topic-based message polarity classification", "start_pos": 19, "end_pos": 62, "type": "TASK", "confidence": 0.7197742089629173}]}, {"text": "The system we submitted uses a Support Vector Machine classifier with rich set of features, ranging from standard to more creative, task-specific features, including a series of rating-based features as well as features that account for sentimental reminiscence of past topics and deceased famous people.", "labels": [], "entities": []}, {"text": "Our system ranked 14th out of 39 submissions in subtask A, 5th out of 24 submissions in subtask B, and 3rd out of 16 submissions in subtask D.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis (), a task of determining polarity of text towards some topic, recently gained a lot of interest, mostly due to its applicability in various fields, such as public relations () and market analysis.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9074482619762421}, {"text": "market analysis", "start_pos": 200, "end_pos": 215, "type": "TASK", "confidence": 0.7774452865123749}]}, {"text": "Following the growing popularity of social networks and an increasing number of user comments that can be found there, sentiment analysis of texts on social networks, such as tweets from Twitter, has been the focus of much research.", "labels": [], "entities": [{"text": "sentiment analysis of texts", "start_pos": 119, "end_pos": 146, "type": "TASK", "confidence": 0.8859450668096542}]}, {"text": "However, determining the sentiment of a tweet is often not an easy task, since the length of the tweet is limited and language is mostly informal, including slang, abbreviations, and hashtags.", "labels": [], "entities": [{"text": "determining the sentiment of a tweet", "start_pos": 9, "end_pos": 45, "type": "TASK", "confidence": 0.9022814631462097}]}, {"text": "Various systems have been proposed for tackling this problem, ranging from simple unsupervised models that use precompiled sentiment lexicons for evaluating polarity of tweets (O') to more complex supervised models that use textual feature representations in combination with machine learning algorithms such as Support Vector Machines (SVM) ( or deep neural networks.", "labels": [], "entities": []}, {"text": "In this paper, we present our system for determining sentiment of tweets, which we submitted, more specifically to the English versions of subtasks A, B, and D.", "labels": [], "entities": []}, {"text": "In subtask A, the goal was to predict the sentiment of a tweet as either positive, neutral, or negative.", "labels": [], "entities": []}, {"text": "Subtask B consisted of predicting the sentiment of a given tweet on a 2-point scale (positive or negative) given a topic.", "labels": [], "entities": [{"text": "predicting the sentiment of a given tweet", "start_pos": 23, "end_pos": 64, "type": "TASK", "confidence": 0.8261411786079407}]}, {"text": "In subtask D, the task was to determine the distribution of positive and negative tweets for each topic in a given set of tweets annotated with topics.", "labels": [], "entities": []}, {"text": "The system we submitted uses an SVM classifier with a linear kernel and a number of features.", "labels": [], "entities": []}, {"text": "We experiment with basic features such as tf-idf and pretrained word embeddings, as well as more task-specific features including sentiment lexicons, ratings-based, \"nostalgia features\", and \"recent deaths\".", "labels": [], "entities": []}, {"text": "Ratings-based features use external data from different online resources to leverage the information such as rating of a movie or an actor mentioned in a tweet.", "labels": [], "entities": []}, {"text": "\"Recent deaths\" features make use of information about recent deaths of notable people, while \"nostalgia feature\" makes use of topic's \"age\" -the rationale being that people usually reminisce about past events in a sentimental and positive way.", "labels": [], "entities": []}, {"text": "Our system ranked 3rd out of 16 teams in subtask D, 5th out of 24 teams in subtask B, and 14th out of 39 teams in subtask A.", "labels": [], "entities": []}], "datasetContent": [{"text": "We started with a number of different classifiers and chose the one that gave the best result on a hold-out test set (2016 test set) () for each subtask.", "labels": [], "entities": [{"text": "hold-out test set (2016 test set)", "start_pos": 99, "end_pos": 132, "type": "DATASET", "confidence": 0.7705998718738556}]}, {"text": "After the official results were published, together with the gold labels for test sets, we additionally performed a simple feature analysis over predefined feature groups to analyze the impact each of these groups has on the final result.", "labels": [], "entities": []}, {"text": "We submitted our solution to three subtasks: A, B, and D.", "labels": [], "entities": []}, {"text": "Subtask A uses macro-averaged recall (\u03c1) overall three classes (positive, neutral, and negative) as an official metric.", "labels": [], "entities": [{"text": "recall (\u03c1)", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9578695595264435}]}, {"text": "For subtask B, macroaveraged recall over positive and negative classes (\u03c1 PN ) is used, while subtask D uses KullbackLeibler Divergence (KLD) as the official measure.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9825131893157959}, {"text": "KullbackLeibler Divergence (KLD)", "start_pos": 109, "end_pos": 141, "type": "METRIC", "confidence": 0.808817446231842}]}], "tableCaptions": [{"text": " Table 1: Results for subtasks A and B on 2016 test  set using the official measures for both subtasks", "labels": [], "entities": [{"text": "2016 test  set", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.817501425743103}]}, {"text": " Table 2: Official results for subtasks B and D (top  7 teams only)", "labels": [], "entities": []}, {"text": " Table 3: Feature analysis results for all subtasks", "labels": [], "entities": []}]}