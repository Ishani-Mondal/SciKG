{"title": [{"text": "SemEval-2017 Task 8: RumourEval: Determining rumour veracity and support for rumours", "labels": [], "entities": []}], "abstractContent": [{"text": "Media is full of false claims.", "labels": [], "entities": []}, {"text": "Even Ox-ford Dictionaries named \"post-truth\" as the word of 2016.", "labels": [], "entities": [{"text": "Ox-ford Dictionaries", "start_pos": 5, "end_pos": 25, "type": "DATASET", "confidence": 0.9359099864959717}]}, {"text": "This makes it more important than ever to build systems that can identify the veracity of a story, and the nature of the discourse around it.", "labels": [], "entities": []}, {"text": "Ru-mourEval is a SemEval shared task that aims to identify and handle rumours and reactions to them, in text.", "labels": [], "entities": [{"text": "SemEval shared task", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.8803164958953857}]}, {"text": "We present an annotation scheme, a large dataset covering multiple topics-each having their own families of claims and replies-and use these to pose two concrete challenges as well as the results achieved by participants on these challenges.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The two subtasks were evaluated as follows.", "labels": [], "entities": []}, {"text": "SDQC stance classification: The evaluation of the SDQC needed careful consideration, as the distribution of the categories is clearly skewed towards comments.", "labels": [], "entities": [{"text": "SDQC stance classification", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6256580253442129}]}, {"text": "Evaluation is through classification accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.9322839379310608}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.8802095055580139}]}, {"text": "Veracity prediction: The evaluation of the predicted veracity, which is either true or false for each instance, was done using macroaveraged accuracy, hence measuring the ratio of instances for which a correct prediction was made.", "labels": [], "entities": [{"text": "Veracity prediction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7142335176467896}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.7826076149940491}]}, {"text": "Additionally, we calculated RMSE \u03c1 for the difference between system and reference confidence incorrect examples and provided the mean of these scores.", "labels": [], "entities": [{"text": "RMSE \u03c1", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.975608229637146}]}, {"text": "Incorrect examples have an RMSE of 1.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9900240898132324}]}, {"text": "This is normalised and combined with the macroaveraged accuracy to give a final score; e.g. acc = (1 \u2212 \u03c1)acc.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.8922837972640991}, {"text": "acc", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9901072978973389}]}, {"text": "The baseline is the most common class.", "labels": [], "entities": []}, {"text": "For: Results for Task A: support/deny/query/comment classification.", "labels": [], "entities": [{"text": "support/deny/query/comment classification", "start_pos": 25, "end_pos": 66, "type": "TASK", "confidence": 0.5511921495199203}]}, {"text": "Task A, we also introduce a baseline excluding the common, low-impact \"comment\" class, considering accuracy over only support, deny and query.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9986028075218201}]}, {"text": "This is included as the SDQ baseline.", "labels": [], "entities": [{"text": "SDQ baseline", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.8927121460437775}]}], "tableCaptions": [{"text": " Table 1: Label distribution of training and test  datasets.", "labels": [], "entities": []}, {"text": " Table 2:  Results for Task A: sup- port/deny/query/comment classification.", "labels": [], "entities": []}, {"text": " Table 3: Results for Task B: Rumour veracity - open variant.", "labels": [], "entities": []}, {"text": " Table 4: Results for Task B: Rumour veracity - closed variant.", "labels": [], "entities": []}]}