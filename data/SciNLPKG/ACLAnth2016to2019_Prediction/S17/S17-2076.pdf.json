{"title": [{"text": "BuzzSaw at SemEval-2017 Task 7: Global vs. Local Context for Interpreting and Locating Homographic English Puns with Sense Embeddings", "labels": [], "entities": [{"text": "Interpreting and Locating Homographic English Puns with Sense Embeddings", "start_pos": 61, "end_pos": 133, "type": "TASK", "confidence": 0.7454084489080641}]}], "abstractContent": [{"text": "This paper describes our system participating in the SemEval-2017 Task 7, for the subtasks of homographic pun location and homographic pun interpretation.", "labels": [], "entities": [{"text": "SemEval-2017 Task 7", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8159281214078268}, {"text": "homographic pun interpretation", "start_pos": 123, "end_pos": 153, "type": "TASK", "confidence": 0.6665912171204885}]}, {"text": "For pun interpretation, we use a knowledge-based Word Sense Disambiguation (WSD) method based on sense embeddings.", "labels": [], "entities": [{"text": "pun interpretation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.870156317949295}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.7104809582233429}]}, {"text": "Pun-based jokes can be divided into two parts, each containing information about the two distinct senses of the pun.", "labels": [], "entities": []}, {"text": "To exploit this structure we split the context that is input to the WSD system into two local contexts and find the best sense for each of them.", "labels": [], "entities": []}, {"text": "We use the output of pun interpretation for pun location.", "labels": [], "entities": [{"text": "pun interpretation", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8264924585819244}, {"text": "pun location", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.7537550926208496}]}, {"text": "As we expect the two meanings of a pun to be very dissimilar , we compute sense embedding cosine distances for each sense-pair and select the word that has the highest distance.", "labels": [], "entities": []}, {"text": "We describe experiments on different methods of splitting the context and compare our method to several baselines.", "labels": [], "entities": []}, {"text": "We find evidence supporting our hypotheses and obtain competitive results for pun interpretation .", "labels": [], "entities": [{"text": "pun interpretation", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.9523819088935852}]}], "introductionContent": [{"text": "A pun is a word used in a context to evoke two or more distinct senses for humorous effect.", "labels": [], "entities": []}, {"text": "For example, in the 1987 movie \"The Running Man\", Arnold Schwarzenegger's character cuts his enemy Buzzsaw in half with a chainsaw, then announces: \"He had to split.\"", "labels": [], "entities": [{"text": "1987 movie \"The Running Man\"", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.6371331342629024}]}, {"text": "The verb split is the pun here, evoking two senses in the context: that of leaving, and that of disintegrating into two parts.", "labels": [], "entities": []}, {"text": "Recognizing and appreciating puns requires sophisticated feats of intelligence currently unique to humans.", "labels": [], "entities": [{"text": "Recognizing and appreciating puns", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6854617819190025}]}, {"text": "A recently proposed set of artificial intelligence tasks) challenges computers to try their hand at it: pun detection (tell whether or not a text contains a pun), pun location (given a text with a pun, tell which word is the pun) and pun interpretation (given a pun in context, tell which senses it evokes).", "labels": [], "entities": [{"text": "pun detection", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.789053350687027}, {"text": "pun interpretation", "start_pos": 234, "end_pos": 252, "type": "TASK", "confidence": 0.7115819603204727}]}, {"text": "Pun interpretation is closely related to the task of Word Sense Disambiguation.", "labels": [], "entities": [{"text": "Pun interpretation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9220313429832458}, {"text": "Word Sense Disambiguation", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.7183345953623453}]}, {"text": "A typical WSD system chooses that sense of a word which fits best in the context the word appears in.", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9732419848442078}]}, {"text": "A pun interpretation system, however, should return not one but two different senses of a word.", "labels": [], "entities": [{"text": "pun interpretation", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.6994754523038864}]}, {"text": "suggest a straightforward extension of the WSD approach to pun interpretation: choose the best scoring sense and secondbest scoring sense for the word in its context.", "labels": [], "entities": [{"text": "WSD", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9111477136611938}, {"text": "pun interpretation", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.8749625980854034}, {"text": "secondbest scoring sense", "start_pos": 113, "end_pos": 137, "type": "METRIC", "confidence": 0.9272930026054382}]}, {"text": "However, this approach does not take into account the specific structure of pun-based jokes.", "labels": [], "entities": []}, {"text": "In most cases, such jokes can be divided into two parts, wherein the first part cues for one sense are concentrated, and in the second part, cues for another sense.", "labels": [], "entities": []}, {"text": "shows examples of such cases.", "labels": [], "entities": []}, {"text": "A pun interpretation system could exploit this two-part structure by splitting the global context of the entire joke into two local contexts and performing WSD separately for each local context, choosing the best sense for each of the two.", "labels": [], "entities": [{"text": "pun interpretation", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.7132275998592377}]}, {"text": "As this process makes each context more informative for the respective sense, we hypothesize that it leads to more accurate pun interpretation than the simple approach which uses the top-scoring two senses according to the global context.", "labels": [], "entities": [{"text": "pun interpretation", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.6577281951904297}]}, {"text": "Additionally, we believe that we can use the output of our pun interpretation system for pun location.", "labels": [], "entities": [{"text": "pun interpretation", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.6820077002048492}, {"text": "pun location", "start_pos": 89, "end_pos": 101, "type": "TASK", "confidence": 0.8665597140789032}]}, {"text": "We hypothesize that the two senses of a pun are typically very dissimilar, as this is important for the joke to be recognizable.", "labels": [], "entities": []}, {"text": "We therefore attempt to locate puns by selecting the polysemous word with the most dissimilar two senses.", "labels": [], "entities": []}, {"text": "The pun is typeset in boldface.", "labels": [], "entities": []}, {"text": "Words that we judge to be cues for one sense are marked with a dashed underline, words and n-grams that we judge to be cues for the other sense are marked with a dotted underline.", "labels": [], "entities": []}, {"text": "Note that the cues for the two senses tend to divide the jokes into two non-overlapping parts.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the sense and lexeme embeddings from . They lie within the same vector space as the pre-trained word embeddings by . This model contains 300-dimensional vectors for 3 million words and phrases from the Google News dataset.", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 209, "end_pos": 228, "type": "DATASET", "confidence": 0.8774368762969971}]}, {"text": "Our 1 http://www.cis.lmu.de/ sascha/AutoExtend/ 2 see https://code.google.com/p/word2vec/ sense inventory is Princeton WordNet 3.1.", "labels": [], "entities": [{"text": "Princeton WordNet 3.1", "start_pos": 109, "end_pos": 130, "type": "DATASET", "confidence": 0.9383753140767416}]}, {"text": "Although a pun can have two or more different part-of-speech tags, our method does not account for this.", "labels": [], "entities": []}, {"text": "Instead, we use the POS that was assigned by the Stanford POS tagger ().", "labels": [], "entities": [{"text": "Stanford POS tagger", "start_pos": 49, "end_pos": 68, "type": "DATASET", "confidence": 0.888811469078064}]}], "tableCaptions": [{"text": " Table 1: Results for pun interpretation on the  shared task test data.", "labels": [], "entities": [{"text": "pun interpretation", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.9098155796527863}]}, {"text": " Table 2: Results for pun location on the shared  task test data.", "labels": [], "entities": [{"text": "pun location", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.8331924080848694}]}, {"text": " Table 3: Results for pun location on the shared  task test data.", "labels": [], "entities": [{"text": "pun location", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.8512018620967865}]}]}