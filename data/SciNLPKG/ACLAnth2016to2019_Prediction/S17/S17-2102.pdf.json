{"title": [{"text": "NNEMBs at SemEval-2017 Task 4: Neural Twitter Sentiment Classification: a Simple Ensemble Method with Different Embeddings", "labels": [], "entities": [{"text": "Neural Twitter Sentiment Classification", "start_pos": 31, "end_pos": 70, "type": "TASK", "confidence": 0.8179000318050385}]}], "abstractContent": [{"text": "Recently, neural twitter sentiment classification has become one of state-of-the-arts, which requires less feature engineering work compared with traditional methods.", "labels": [], "entities": [{"text": "neural twitter sentiment classification", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.7489311471581459}]}, {"text": "In this paper, we propose a simple and effective ensemble method to further boost the performances of neural models.", "labels": [], "entities": []}, {"text": "We collect several word embedding sets which are publicly released (often are learned on different corpus) or constructed by running Skip-gram on released large-scale corpus.", "labels": [], "entities": []}, {"text": "We make an assumption that different word embeddings cover different words and encode different semantic knowledge, thus using them together can improve the generalizations and performances of neural models.", "labels": [], "entities": []}, {"text": "In the SemEval 2017, our method ranks 1st in Accuracy, 5th in AverageR.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.999312162399292}, {"text": "AverageR", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9890444278717041}]}, {"text": "Meanwhile, the additional comparisons demonstrate the superiority of our model over these ones based on only one word embedding set.", "labels": [], "entities": []}, {"text": "We release our code 1 for the method replicabil-ity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Twitter sentiment classification has attracted a lot of attention (, which aims to classify a tweet into three sentiment categories: negative, neutral, and positive.", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6413830816745758}]}, {"text": "Tweet text has several features: written by the informal language, hash-tags and emoticons indicate sentiments, and sometimes is sarcasm, which make decisions of tweet sentiment hard for machines.", "labels": [], "entities": []}, {"text": "With releases of annotated datasets, more researchers prefer to use the 1 https://github.com/zwjyyc/NNEMBs twitter sentiment classification as one testbed to evaluate their proposed models.", "labels": [], "entities": [{"text": "NNEMBs twitter sentiment classification", "start_pos": 100, "end_pos": 139, "type": "DATASET", "confidence": 0.7156766653060913}]}, {"text": "Traditional methods) for twitter sentiment classification use a variety of hand-crafted features including surface-form, semantic and sentiment lexicons.", "labels": [], "entities": [{"text": "twitter sentiment classification", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.9168491363525391}]}, {"text": "The performances of these methods often depend on the quality of feature engineering work, and building a state-ofthe-art system is difficult for novices.", "labels": [], "entities": []}, {"text": "Moreover, these designed features are presented by the onehot representation which cannot capture the semantic relativeness of different features and proposes a problem of feature sparsity.", "labels": [], "entities": []}, {"text": "To address this, induced sentiment-specific low-dimensional, real-valued embedding features for twitter classification, which encode both semantics and sentiments of words.", "labels": [], "entities": [{"text": "twitter classification", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.7090546637773514}]}, {"text": "In the experiments, the embedding features and hand-crafted features obtain similar results, and also they are complementary for each other in the system.", "labels": [], "entities": []}, {"text": "With the developments of neural networks in natural language processing, neural sentiment classification ( has attracted a lot of attention recently and become the state-of-the-arts.", "labels": [], "entities": [{"text": "neural sentiment classification", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.7478895386060079}]}, {"text": "These methods first learn word embeddings from large-scale twitter corpus, then tune neural networks by the tweets which have distant labels, and finally fine-tune the proposed models by the annotated datasets.", "labels": [], "entities": []}, {"text": "Learning word embeddings using in-domain data is an effective way to boost model performances (.", "labels": [], "entities": []}, {"text": "However, collecting large-scale twitter corpus is often time-consuming.", "labels": [], "entities": []}, {"text": "In this paper, we use the different word embedding sets to boost the performances of our neural networks, which only include released different word embeddings sets and the word embedding set derived from the released Yelp large-scale datasets by Skip-gram (.", "labels": [], "entities": [{"text": "Yelp large-scale datasets", "start_pos": 218, "end_pos": 243, "type": "DATASET", "confidence": 0.8677735726038615}]}, {"text": "A simple and effective ensemble method is proposed, which takes different word embedding sets as input to train neural networks and predicts labels of testing tweets by merging all output of neural models.", "labels": [], "entities": []}, {"text": "Our ensemble method show its effectiveness in SemEval 2017, though most of used word embedding sets are not learned from twitter corpus, which can be explained that different embedding sets has different vocabularies and encode different parts of sentiment knowledge.", "labels": [], "entities": []}, {"text": "Moreover, we conduct additional experiments to analyze our model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use 4 embedding sets which are described in.", "labels": [], "entities": []}, {"text": "Meanwhile, we crawl and merge all annotated datasets of previous SemEvals, and split them into training, development, and testing sets with ratio 8:1:1, which are shown in the regularization parameter is set to 1e-5; (2) the dropout rate is set to 0.3, which is applied in the final text representation.", "labels": [], "entities": []}, {"text": "All parameters are learned by Adam optimizer) with the learning rate 0.001.", "labels": [], "entities": []}, {"text": "Note that, all word embedding sets are fixed when training.", "labels": [], "entities": []}, {"text": "All models are tuned by the development set in Training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the embedding sets. R means the embedding set is publicly released and S means  the embedding set is self-contained. GloVec (Mikolov et al., 2013) and Word2Vec (Pennington et al.,  2014) are most popular embedding algorithms. Scale means the size of tokens in corpus, M and B refer  to million and billion respectively. The embedding set word2vecY are trained by Word2Vec with default  settings and Yelp reviews are available at https://www.yelp.com/dataset challenge.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 175, "end_pos": 183, "type": "DATASET", "confidence": 0.9600616693496704}, {"text": "Word2Vec", "start_pos": 387, "end_pos": 395, "type": "DATASET", "confidence": 0.9535311460494995}]}, {"text": " Table 2: Statistics of datasets.", "labels": [], "entities": []}, {"text": " Table 3: Results on datasets of previous SemEval. R * means recall value.", "labels": [], "entities": [{"text": "R", "start_pos": 51, "end_pos": 52, "type": "METRIC", "confidence": 0.9806806445121765}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9987792372703552}]}, {"text": " Table 4: Results on SemEval 2017. The median system is the system of rank 19th among 38 teams.", "labels": [], "entities": [{"text": "SemEval 2017", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.6256203651428223}]}]}