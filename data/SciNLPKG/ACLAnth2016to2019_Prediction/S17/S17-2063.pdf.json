{"title": [{"text": "QUB at SemEval-2017 Task 6: Cascaded Imbalanced Classification for Humor Analysis in Twitter", "labels": [], "entities": [{"text": "Cascaded Imbalanced Classification", "start_pos": 28, "end_pos": 62, "type": "TASK", "confidence": 0.7184335986773173}, {"text": "Humor Analysis", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.9051360487937927}]}], "abstractContent": [{"text": "This paper presents our submission to SemEval-2017 Task 6: #HashtagWars: Learning a Sense of Humor.", "labels": [], "entities": [{"text": "SemEval-2017 Task 6", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8086220026016235}, {"text": "HashtagWars: Learning a Sense of Humor", "start_pos": 60, "end_pos": 98, "type": "TASK", "confidence": 0.6237252524920872}]}, {"text": "There are two subtasks: A.", "labels": [], "entities": []}, {"text": "Pairwise Comparison, and B. Semi-Ranking.", "labels": [], "entities": [{"text": "B.", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9964689016342163}]}, {"text": "Our assumption is that the distribution of humorous and non-humorous texts in real life language is naturally imbalanced.", "labels": [], "entities": []}, {"text": "Using Na\u00efve Bayes Multino-mial with standard text-representation features , we approached Subtask B as a sequence of imbalanced classification problems , and optimized our system per the macro-average recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 201, "end_pos": 207, "type": "METRIC", "confidence": 0.9534146189689636}]}, {"text": "Subtask A was then solved via the Semi-Ranking results.", "labels": [], "entities": []}, {"text": "On the final test, our system was ranked 10 th for Subtask A, and 3 rd for Subtask B.", "labels": [], "entities": []}], "introductionContent": [{"text": "Humor is an essential trait of human intelligence that has not yet been addressed extensively in current AI research 1 . It's certainly one of the most interesting and puzzling research areas in the field of natural language understanding, and developing techniques that enable computers to understand humor inhuman languages deserves research attention (.", "labels": [], "entities": [{"text": "Humor", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.8819605112075806}, {"text": "natural language understanding", "start_pos": 208, "end_pos": 238, "type": "TASK", "confidence": 0.6553223232428232}]}, {"text": "Humor recognition or analysis by computers aims to determine whether a sentence in context expresses a certain degree of humor.", "labels": [], "entities": [{"text": "Humor recognition or analysis", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8746154308319092}]}, {"text": "This can be extremely challenging because no universal definition of humor has been achieved, humor is highly contextual, and there are many different types of humor with different characteristics.", "labels": [], "entities": []}, {"text": "Previous studies (Mihalcea and 1 http://alt.qcri.org/semeval2017/task6/) dealt with the humor recognition task as a binary classification task, which was to categorize a given text as humorous or non-humorous (.", "labels": [], "entities": [{"text": "humor recognition task", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.7758171161015829}]}, {"text": "Textual data consisting of comparable amounts of humorous texts and nonhumorous texts were collected, and a classification model was then built using textual features.", "labels": [], "entities": []}, {"text": "examined cross-domain application of humor detection using Twitter data.", "labels": [], "entities": [{"text": "humor detection", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.8634829521179199}]}, {"text": "used data from a famous TV series, Friends.", "labels": [], "entities": []}, {"text": "Speakers' turns which occurred right before simulated laughter were defined as humorous ones and the other turns as nonhumorous ones.", "labels": [], "entities": []}, {"text": "They also used speakers' acoustic characteristics as features.", "labels": [], "entities": []}, {"text": "Their target was to categorize an utterance in a sitcom, The Big Bang Theory, into those followed by laughter or not.", "labels": [], "entities": [{"text": "categorize an utterance in a sitcom, The Big Bang Theory", "start_pos": 20, "end_pos": 76, "type": "TASK", "confidence": 0.7454170936887915}]}, {"text": "They were the first to use a deep learning algorithm for humor classification.", "labels": [], "entities": [{"text": "humor classification", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.8863823115825653}]}, {"text": "Besides, because genre bias can be problematic, tried to minimize genre differences between humorous and non-humorous texts.", "labels": [], "entities": []}, {"text": "SemEval-2017 Task 6 aims to encourage the development of methods that should take into account the continuous nature of humor, on the one hand, and to characterize the sense of humor of a particular source, on the other.", "labels": [], "entities": [{"text": "SemEval-2017 Task 6", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.758541981379191}]}, {"text": "The dataset was based on humorous responses submitted to a Comedy Central TV show @midnight 2 . There are two subtasks: A.", "labels": [], "entities": [{"text": "Comedy Central TV show @midnight 2", "start_pos": 59, "end_pos": 93, "type": "DATASET", "confidence": 0.9202772634369987}]}, {"text": "Pairwise Comparison, where a successful system should be able to predict among a pair of tweets which is funnier; and B. Semi-Ranking, where, given a file of tweets fora hashtag, systems should produce a ranking of tweets from funniest to least funny.", "labels": [], "entities": [{"text": "B.", "start_pos": 118, "end_pos": 120, "type": "METRIC", "confidence": 0.9615738987922668}]}, {"text": "Since automatic humor analysis is difficult, our goal is only to provide computer assistance to human experts.", "labels": [], "entities": [{"text": "automatic humor analysis", "start_pos": 6, "end_pos": 30, "type": "TASK", "confidence": 0.6505830387274424}]}, {"text": "We approached Subtask B as a sequence of imbalanced classification problems, and optimized our system per the macro-average recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9589559435844421}]}, {"text": "Subtask A was then solved simply via the SemiRanking results of Subtask B.", "labels": [], "entities": []}], "datasetContent": [{"text": "This task is a 3-partite problem that could be solved via the algorithms given in and 2.", "labels": [], "entities": []}, {"text": "Using Java and Na\u00efve Bayes Multinomial (NBM) classification of Weka 3.7 4 (Witten et al, 2011), we did the experiment with the training and trial data as training set.", "labels": [], "entities": [{"text": "Na\u00efve Bayes Multinomial (NBM) classification of Weka 3.7 4", "start_pos": 15, "end_pos": 73, "type": "DATASET", "confidence": 0.7032095979560505}]}, {"text": "As for classification features, our present research simply chose word n-grams with n = 1, 2, and 3.", "labels": [], "entities": []}, {"text": "By optimizing the macro-average recall of an NBM classifier on the training set with all original class labels, 3200 word types were kept before vectorization.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9795774817466736}]}, {"text": "gives some results of apart of the optimizing process.", "labels": [], "entities": []}, {"text": "The star denotes the optimized point.", "labels": [], "entities": []}, {"text": "For tuning the cost matrix and prediction confidence, we used 10-fold cross validation.", "labels": [], "entities": []}, {"text": "The parameter values of the largest macro-average recall and the least standard deviation were returned for training final NBM classifiers on the whole training set and predicting for the final test set.", "labels": [], "entities": [{"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9634696841239929}]}, {"text": "lists the results of tuning cost matrix and prediction confidence.", "labels": [], "entities": []}, {"text": "We first tuned the cost matrices, and the best macro-average recalls are marked with 1 in. gives parts of the cost matrix tuning process nearby the optimization points (denoted with stars).", "labels": [], "entities": []}, {"text": "To make the tuning less expensive, we fixed the cost for false positive as 1, and only tuned the cost for false positive.", "labels": [], "entities": []}, {"text": "Then, with the optimized cost matrices for NBM classifiers, we tuned the confidence for predicting negative items, and the best macro-average recalls are marked with 2 in. gives parts of the prediction confidence tuning process near the optimization points (denoted with stars).", "labels": [], "entities": [{"text": "predicting negative items", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.8804552952448527}]}, {"text": "We finally trained our system on the whole training set with the tuned parameters, and applied this system on the evaluation set.", "labels": [], "entities": []}, {"text": "For Subtask A, our submission is ranked 10 th , with a micro-averaged accuracy of 0.187.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.8678387403488159}]}, {"text": "For Subtask B, our submission is ranked 3 rd , with an edit distance of 0.924.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 55, "end_pos": 68, "type": "METRIC", "confidence": 0.970018059015274}]}], "tableCaptions": [{"text": " Table 3: Tuning results for cost matrix and pre- diction confidence", "labels": [], "entities": [{"text": "Tuning", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9789345860481262}]}]}