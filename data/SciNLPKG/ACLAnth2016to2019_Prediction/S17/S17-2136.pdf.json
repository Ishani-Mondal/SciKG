{"title": [{"text": "UCSC-NLP at SemEval-2017 Task 4: Sense n-grams for Sentiment Analysis in Twitter", "labels": [], "entities": [{"text": "UCSC-NLP", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9493967890739441}, {"text": "Sentiment Analysis", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8905972540378571}]}], "abstractContent": [{"text": "This paper describes the system submitted to SemEval-2017 Task 4-A Sentiment Analysis in Twitter developed by the UCSC-NLP team.", "labels": [], "entities": [{"text": "SemEval-2017 Task 4-A Sentiment Analysis in Twitter", "start_pos": 45, "end_pos": 96, "type": "TASK", "confidence": 0.8903779302324567}]}, {"text": "We studied how relationships between sense n-grams and sentiment polarities can contribute to this task, i.e. co-occurrences of WordNet senses in the tweet, and the polarity.", "labels": [], "entities": []}, {"text": "Furthermore , we evaluated the effect of discarding a large set of features based on char-grams reported in preceding works.", "labels": [], "entities": []}, {"text": "Based on these elements, we developed a SVM system, which exploring SentiWord-Net as a polarity lexicon.", "labels": [], "entities": []}, {"text": "It achieves an F 1 = 0.624 of average.", "labels": [], "entities": [{"text": "F 1", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9967007339000702}]}, {"text": "Among 39 submissions to this task, we ranked 10th.", "labels": [], "entities": []}], "introductionContent": [{"text": "To determine whether a text expresses a POSI-TIVE, NEGATIVE or NEUTRAL opinion has attracted an increasingly attention.", "labels": [], "entities": [{"text": "NEGATIVE", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.6297280788421631}]}, {"text": "In particular, sentiment classification of tweets has immediate applications in areas such as marketing, political, and social analysis ( Different approaches have shown to be very promising for polarity classification of tweets such as Convolutional Neural Networks trained with large amounts of data (.", "labels": [], "entities": [{"text": "sentiment classification of tweets", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.8967534750699997}]}, {"text": "Several authors have studied Machine Learning approaches based on lexicon, surface and semantic features.", "labels": [], "entities": []}, {"text": "The proposal of as well as an improved version of show very competitive scores.", "labels": [], "entities": []}, {"text": "The latter approach was re-implemented by as apart of an ensemble of twitter polarity classifier which is top-ranked in the SemEval 2015 Task 9: Sentiment Analysis in Twitter.", "labels": [], "entities": [{"text": "SemEval 2015 Task 9", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7823082208633423}, {"text": "Sentiment Analysis", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.7861246168613434}]}, {"text": "Our system proposes to enrich the set of features used by.", "labels": [], "entities": []}, {"text": "We describe here only the features more relevant for our experiments, further details in all features could be found in;.", "labels": [], "entities": []}, {"text": "\u2022 Lexicon Based Features (LB) NRC-Emotion, NRC-Sentiment140, NRCHashtag (, BingLiu (Hu and) and MPQA () lexicons have been used to generate features.", "labels": [], "entities": [{"text": "NRC-Sentiment140", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.9403640627861023}, {"text": "NRCHashtag", "start_pos": 61, "end_pos": 71, "type": "DATASET", "confidence": 0.8825837969779968}]}, {"text": "Given a tweet, the following features were computed: \u2022 N-gram Based Features (WG and CG) Each 1 to 4-word n-gram present in the training corpus is associated with a feature which indicates if the tweet includes or not the n-gram.", "labels": [], "entities": []}, {"text": "For characters, all different occurrences of 3 to 5 grams are considered.", "labels": [], "entities": []}, {"text": "Given its definition, the number of generated ngram based features is variable and related with the training corpus.", "labels": [], "entities": []}, {"text": "In experiments with SemEval 807 2017 training data, we got near three million of features of this type that is much largest than the number of tweets.", "labels": [], "entities": [{"text": "SemEval 807 2017 training data", "start_pos": 20, "end_pos": 50, "type": "DATASET", "confidence": 0.7789864718914032}]}, {"text": "\u2022 Cluster Based Features (CB) For each one of the 1000 clusters identified by using Brown algorithm) a feature indicates whether the terms of the tweet belong to them.", "labels": [], "entities": []}, {"text": "studied the effect of removing individual set of features as well a whole group of them.", "labels": [], "entities": []}, {"text": "Empirical results suggest that lexicon and n-gram based features are the most important since removing them causes the greatest drop on the classifier efficacy measured as the macroaverage F-score in the test set.", "labels": [], "entities": [{"text": "F-score", "start_pos": 189, "end_pos": 196, "type": "METRIC", "confidence": 0.5594754219055176}]}, {"text": "In this work, we studied how to reduce the number of generated features by removing some of the n-gram based.", "labels": [], "entities": []}, {"text": "Next sections describe further details of our approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our predictor is based in an ensemble of Support Vector Machines with linear kernel, and C = 0.005 trained with all the features proposed by As, we want to evaluate how removing n-gram and cluster based features affect the results of our models.", "labels": [], "entities": []}, {"text": "show eight base models resulting of removing combinations of features of the types WG, CG and CB; with X indicating the characteristic set included in the model.", "labels": [], "entities": []}, {"text": "show different arrangements of the new features which were combined with the based models fora total of 96 experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Best (B) and worst (W) results in previous  SemEval test data. In parenthesis, the number of  the base model. An * indicates that the WSD was  using Lesk algorithm.", "labels": [], "entities": [{"text": "SemEval test data", "start_pos": 54, "end_pos": 71, "type": "DATASET", "confidence": 0.7826897899309794}]}, {"text": " Table 4: Results in SemEval 2017 test, Precision  (P), Recall (R) and F1.", "labels": [], "entities": [{"text": "SemEval 2017 test", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.7151443958282471}, {"text": "Precision  (P)", "start_pos": 40, "end_pos": 54, "type": "METRIC", "confidence": 0.96052585542202}, {"text": "Recall (R)", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9663393646478653}, {"text": "F1", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.9993359446525574}]}]}