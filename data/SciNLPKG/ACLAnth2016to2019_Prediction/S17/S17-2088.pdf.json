{"title": [{"text": "SemEval-2017 Task 4: Sentiment Analysis in Twitter", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.9540230631828308}]}], "abstractContent": [{"text": "This paper describes the fifth year of the Sentiment Analysis in Twitter task.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter task", "start_pos": 43, "end_pos": 77, "type": "TASK", "confidence": 0.951601767539978}]}, {"text": "SemEval-2017 Task 4 continues with a rerun of the subtasks of SemEval-2016 Task 4, which include identifying the overall sentiment of the tweet, sentiment towards a topic with classification on a two-point and on a five-point ordinal scale, and quantification of the distribution of sentiment towards a topic across a number of tweets: again on a two-point and on a five-point ordinal scale.", "labels": [], "entities": [{"text": "SemEval-2017 Task 4", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.8346042633056641}]}, {"text": "Compared to 2016, we made two changes: (i) we introduced anew language, Arabic, for all subtasks, and (ii) we made available information from the profiles of the Twitter users who posted the target tweets.", "labels": [], "entities": []}, {"text": "The task continues to be very popular, with a total of 48 teams participating this year.", "labels": [], "entities": []}], "introductionContent": [{"text": "The identification of sentiment in text is an important field of study, with social media platforms such as Twitter garnering the interest of researchers in language processing as well as in political and social sciences.", "labels": [], "entities": [{"text": "identification of sentiment in text", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.9107027411460876}, {"text": "language processing", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.7196128368377686}]}, {"text": "The task usually involves detecting whether apiece of text expresses a POSITIVE, a NEGATIVE, or a NEUTRAL sentiment; the sentiment can be general or about a specific topic, e.g., a person, a product, or an event.", "labels": [], "entities": [{"text": "detecting whether apiece of text expresses a POSITIVE", "start_pos": 26, "end_pos": 79, "type": "TASK", "confidence": 0.6186561807990074}]}, {"text": "The Sentiment Analysis in Twitter task has been run yearly at SemEval since 2013), with the 2015 task introducing sentiment towards a topic ( and the 2016 task introducing tweet quantification and five-point ordinal classification).", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter task", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.9319771885871887}]}, {"text": "SemEval is the International Workshop on Semantic Evaluation, formerly SensEval.", "labels": [], "entities": [{"text": "International Workshop on Semantic Evaluation", "start_pos": 15, "end_pos": 60, "type": "TASK", "confidence": 0.5904874324798584}]}, {"text": "It is an ongoing series of evaluations of computational semantic analysis systems, organized under the umbrella of SIGLEX, the Special Interest Group on the Lexicon of the Association for Computational Linguistics.", "labels": [], "entities": []}, {"text": "Other related tasks at SemEval have explored sentiment analysis of product review and their aspects (, sentiment analysis of figurative language on Twitter (, implicit event polarity (, detecting stance in tweets (), out-of-context sentiment intensity of words and phrases (, and emotion detection.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.918147623538971}, {"text": "sentiment analysis of figurative language", "start_pos": 103, "end_pos": 144, "type": "TASK", "confidence": 0.7825991749763489}, {"text": "emotion detection", "start_pos": 280, "end_pos": 297, "type": "TASK", "confidence": 0.7267606258392334}]}, {"text": "Some of these tasks featured languages other than English, such as Arabic (); however, they did not target tweets, nor did they focus on sentiment towards a topic.", "labels": [], "entities": []}, {"text": "This year, we performed a re-run of the subtasks in SemEval-2016 Task 4, which, in addition to the overall sentiment of a tweet, featured classification, ordinal regression, and quantification with respect to a topic.", "labels": [], "entities": []}, {"text": "Furthermore, we introduced anew language, Arabic.", "labels": [], "entities": []}, {"text": "Finally, we made available to the participants demographic information about the users who posted the tweets, which we extracted from the respective public profiles.", "labels": [], "entities": []}, {"text": "Ordinal Classification As last year, SemEval-2017 Task 4 includes sentiment analysis on a fivepoint scale {HIGHLYPOSITIVE, POSITIVE, NEU-TRAL, NEGATIVE, HIGHLYNEGATIVE}, which is inline with product ratings occurring in the corporate world, e.g., Amazon, TripAdvisor, and Yelp.", "labels": [], "entities": [{"text": "SemEval-2017 Task 4", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8195191820462545}, {"text": "POSITIVE", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9496855735778809}, {"text": "NEU-TRAL", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.731637716293335}, {"text": "Yelp", "start_pos": 272, "end_pos": 276, "type": "DATASET", "confidence": 0.9604703783988953}]}, {"text": "In machine learning terms, moving from a categorical two-point scale to an ordered five-point scale means moving from binary to ordinal classification (aka ordinal regression).", "labels": [], "entities": []}, {"text": "Tweet Quantification SemEval-2017 Task 4 includes tweet quantification tasks along with tweet classification tasks, also on 2-point and 5-point scales.", "labels": [], "entities": [{"text": "Tweet Quantification SemEval-2017 Task", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7765009477734566}, {"text": "tweet classification", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.7041284590959549}]}, {"text": "While the tweet classification task is concerned with whether a specific tweet expresses a given sentiment towards a topic, the tweet quantification task looks at estimating the distribution of tweets about a given topic across the different sentiment classes.", "labels": [], "entities": [{"text": "tweet classification", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7760935127735138}, {"text": "tweet quantification", "start_pos": 128, "end_pos": 148, "type": "TASK", "confidence": 0.7540638148784637}]}, {"text": "Most (if not all) tweet sentiment classification studies within political science), economics, social science (, and market research (, study Twitter with an interest in aggregate statistics about sentiment and are not interested in the sentiment expressed in individual tweets.", "labels": [], "entities": [{"text": "tweet sentiment classification", "start_pos": 18, "end_pos": 48, "type": "TASK", "confidence": 0.7159159580866495}]}, {"text": "We should also note that quantification is not a mere byproduct of classification, as it can be addressed using different approaches and it also needs different evaluation measures.", "labels": [], "entities": [{"text": "quantification", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.9497722387313843}]}, {"text": "Analysis in Arabic This year, we added anew language, Arabic, in order to encourage participants to experiment with multilingual and crosslingual approaches for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.9451164305210114}]}, {"text": "Our objective was to expand the Twitter sentiment analysis resources available to the research community, not only for general multilingual sentiment analysis, but also for multilingual sentiment analysis towards a topic, which is still a largely unexplored research direction for many languages and in particular for morphologically complex languages such as Arabic.", "labels": [], "entities": [{"text": "Twitter sentiment analysis", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.7262880206108093}, {"text": "multilingual sentiment analysis", "start_pos": 127, "end_pos": 158, "type": "TASK", "confidence": 0.7006608843803406}, {"text": "multilingual sentiment analysis", "start_pos": 173, "end_pos": 204, "type": "TASK", "confidence": 0.7423587540785471}]}, {"text": "Arabic has become an emergent language for sentiment analysis, especially as more resources and tools for it have recently become available.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.9819239675998688}]}, {"text": "It is also both interesting and challenging due to its rich morphology and abundance of dialectal use in Twitter.", "labels": [], "entities": []}, {"text": "Early Arabic studies focused on sentiment analysis in newswire), but recently there has been a lot more work on social media, especially Twitter, where the challenges of sentiment analysis are compounded by the presence of multiple dialects and orthographical variants, which are frequently used in conjunction with the formal written language.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.9201695919036865}, {"text": "sentiment analysis", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.9113142192363739}]}, {"text": "Some work studied the utility of machine translation for sentiment analysis of Arabic texts, identification of sentiment holders, and sentiment targets.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7710916996002197}, {"text": "sentiment analysis of Arabic texts", "start_pos": 57, "end_pos": 91, "type": "TASK", "confidence": 0.9181547164916992}, {"text": "identification of sentiment holders", "start_pos": 93, "end_pos": 128, "type": "TASK", "confidence": 0.9111389666795731}]}, {"text": "We believe that the development of a standard Arabic Twitter dataset for sentiment, and particularly with respect to topics, will encourage further research in this regard.", "labels": [], "entities": [{"text": "Arabic Twitter dataset", "start_pos": 46, "end_pos": 68, "type": "DATASET", "confidence": 0.6597246627012888}]}, {"text": "User Information Demographic information in Twitter has been studied and analyzed using network analysis and natural language processing (NLP) techniques.", "labels": [], "entities": [{"text": "User Information Demographic information", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7422237396240234}]}, {"text": "Recent work has shown that user information and information from the network can help sentiment analysis in other corpora and in Twitter (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.9493243396282196}]}, {"text": "Thus, this year we encouraged participants to use information from the public profiles of Twitter users such as demographics (e.g., age, location) as well as information from the rest of the social network (e.g., sentiment of the tweets of friends), with the goal of analyzing the impact of this information on improving sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 321, "end_pos": 339, "type": "TASK", "confidence": 0.912859171628952}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents in more detail the five subtasks of SemEval-2017 Task 4.", "labels": [], "entities": [{"text": "SemEval-2017 Task 4", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8169313669204712}]}, {"text": "Section 3 describes the English and the Arabic datasets and how we created them.", "labels": [], "entities": []}, {"text": "Section 4 introduces and motivates the evaluation measures for each subtask.", "labels": [], "entities": []}, {"text": "Section 5 presents the results of the evaluation and discusses the techniques and the tools that the participants used.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes and points to some possible directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our datasets consist of tweets annotated for sentiment on a 2-point, 3-point, and 5-point scales.", "labels": [], "entities": []}, {"text": "We made available to participants all the data from previous years () for the English training sets, and we collected new training data for Arabic, as well as new test sets for both English and Arabic.", "labels": [], "entities": []}, {"text": "The annotation scheme remained the same as last year (, with the key new contribution being to apply the task and instructions to Arabic as well as providing a script to download basic user information.", "labels": [], "entities": []}, {"text": "All annotations were performed on CrowdFlower.", "labels": [], "entities": [{"text": "CrowdFlower", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.9525171518325806}]}, {"text": "Note that we release all our datasets to the research community to be used freely beyond SemEval.", "labels": [], "entities": []}, {"text": "This section describes the evaluation measures for our five subtasks.", "labels": [], "entities": []}, {"text": "Note that for Subtasks B to E, the datasets are each subdivided into a number of topics, and the subtask needs to be carried out independently for each topic.", "labels": [], "entities": []}, {"text": "As a result, each of the evaluation measures will be \"macroaveraged\" across the topics, i.e., we compute the measure individually for each topic, and we then average the results across the topics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Statistics about the English training and testing datasets. The training data is the aggregate of  all data from prior years, while the testing data is new.", "labels": [], "entities": [{"text": "English training and testing datasets", "start_pos": 31, "end_pos": 68, "type": "DATASET", "confidence": 0.5817948997020721}]}, {"text": " Table 5: Statistics about the newly collected Arabic training and testing datasets.", "labels": [], "entities": []}, {"text": " Table 6: Results for Subtask A \"Message Polar- ity Classification\", English. The systems are or- dered by average recall AvgRec (higher is better).  In each column, the rankings according to the cor- responding measure are indicated with a subscript.  Bx indicates a baseline.", "labels": [], "entities": [{"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9933405518531799}, {"text": "AvgRec", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.6751747131347656}]}, {"text": " Table 8: Results for Subtask B \"Tweet classifi- cation according to a two-point scale\", English.  The systems are ordered by average recall AvgRec  (higher is better). Bx indicates a baseline.", "labels": [], "entities": [{"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9966973066329956}, {"text": "AvgRec", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.7432156205177307}]}, {"text": " Table 9: Results for Subtask B \"Tweet classifi- cation according to a two-point scale\", Arabic.  The systems are ordered by average recall AvgRec  (higher is better). Bx indicates a baseline.", "labels": [], "entities": [{"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9970197081565857}, {"text": "AvgRec", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.741316556930542}]}, {"text": " Table 10: Results for Subtask C \"Tweet classifi- cation according to a five-point scale\", English.  The systems are ordered by their M AE M score  (lower is better). Bx indicates a baseline.", "labels": [], "entities": [{"text": "M AE M score", "start_pos": 134, "end_pos": 146, "type": "METRIC", "confidence": 0.8580531924962997}]}, {"text": " Table 11: Results for Subtask C \"Tweet classifi- cation according to a five-point scale\", Arabic.  The systems are ordered by their M AE M score  (lower is better). Bx indicates a baseline.", "labels": [], "entities": [{"text": "M AE M score", "start_pos": 133, "end_pos": 145, "type": "METRIC", "confidence": 0.8695089519023895}]}, {"text": " Table 12: Results for Subtask D \"Tweet quan- tification according to a two-point scale\", En- glish. The systems are ordered by their KLD  score (lower is better). Bx indicates a baseline.", "labels": [], "entities": [{"text": "KLD  score", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.9199841320514679}]}, {"text": " Table 13: Results for Subtask D \"Tweet quan- tification according to a two-point scale\", Ara- bic. The systems are ordered by their KLD score  (lower is better). Bx indicates a baseline.", "labels": [], "entities": [{"text": "KLD score", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9413096010684967}]}, {"text": " Table 14: Results for Subtask E \"Tweet quan- tification according to a five-point scale\", En- glish. The systems are ordered by their EM D  score (lower is better). Bx indicates a baseline.", "labels": [], "entities": [{"text": "EM D  score", "start_pos": 135, "end_pos": 146, "type": "METRIC", "confidence": 0.968022882938385}]}, {"text": " Table 15: Results for Subtask E \"Tweet quan- tification according to a five-point scale\", Ara- bic. The systems are ordered by their EM D score  (lower is better). Bx indicates a baseline.", "labels": [], "entities": [{"text": "EM D score", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.9673843383789062}]}, {"text": " Table 16: Alphabetical list of the participating teams, their affiliation, country, the subtasks they par- ticipated in, and the system description paper that they contributed to SemEval-2017. Teams whose  Affiliation column is typeset on more than one row include researchers from different institutions, which  have collaborated to build a joint system submission. An N/A entry for the Paper column indicates that  the team did not contribute a system description paper. Finally, the last row gives statistics about the total  number of system submissions for each subtask.", "labels": [], "entities": []}]}