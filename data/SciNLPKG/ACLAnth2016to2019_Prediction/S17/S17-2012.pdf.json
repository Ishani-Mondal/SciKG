{"title": [{"text": "CompiLIG at SemEval-2017 Task 1: Cross-Language Plagiarism Detection Methods for Semantic Textual Similarity", "labels": [], "entities": [{"text": "Cross-Language Plagiarism Detection", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.7491090496381124}, {"text": "Semantic Textual Similarity", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.6203498542308807}]}], "abstractContent": [{"text": "We present our submitted systems for Semantic Textual Similarity (STS) Track 4 at SemEval-2017.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) Track 4", "start_pos": 37, "end_pos": 78, "type": "TASK", "confidence": 0.8262333422899246}]}, {"text": "Given a pair of Spanish-English sentences, each system must estimate their semantic similarity by a score between 0 and 5.", "labels": [], "entities": []}, {"text": "In our submission, we use syntax-based, dictionary-based, context-based, and MT-based methods.", "labels": [], "entities": [{"text": "MT-based", "start_pos": 77, "end_pos": 85, "type": "TASK", "confidence": 0.9585205912590027}]}, {"text": "We also combine these methods in unsu-pervised and supervised way.", "labels": [], "entities": []}, {"text": "Our best run ranked 1 st on track 4a with a correlation of 83.02% with human annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "CompiLIG is a collaboration between Compilatio 1 -a company particularly interested in crosslanguage plagiarism detection -and LIG research group on natural language processing (GETALP).", "labels": [], "entities": [{"text": "CompiLIG", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8613964915275574}, {"text": "crosslanguage plagiarism detection", "start_pos": 87, "end_pos": 121, "type": "TASK", "confidence": 0.7486470341682434}]}, {"text": "Cross-language semantic textual similarity detection is an important step for cross-language plagiarism detection, and evaluation campaigns in this new domain are rare.", "labels": [], "entities": [{"text": "Cross-language semantic textual similarity detection", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.6859659671783447}, {"text": "cross-language plagiarism detection", "start_pos": 78, "end_pos": 113, "type": "TASK", "confidence": 0.8163530230522156}]}, {"text": "For the first time, SemEval STS task () was extended with a Spanish-English cross-lingual sub-task in 2016.", "labels": [], "entities": [{"text": "SemEval STS task", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.8696489930152893}]}, {"text": "This year, sub-task was renewed under track 4 (divided in two sub-corpora: track 4a and track 4b).", "labels": [], "entities": []}, {"text": "Given a sentence in Spanish and a sentence in English, the objective is to compute their semantic textual similarity according to a score from 0 1 www.compilatio.net to 5, where 0 means no similarity and 5 means full semantic similarity.", "labels": [], "entities": []}, {"text": "The evaluation metric is a Pearson correlation coefficient between the submitted scores and the gold standard scores from human annotators.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 27, "end_pos": 58, "type": "METRIC", "confidence": 0.9647213220596313}]}, {"text": "Last year, among 26 submissions from 10 teams, the method that achieved the best performance) was a supervised system (SVM regression with RBF kernel) based on word alignment algorithm presented in.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 160, "end_pos": 174, "type": "TASK", "confidence": 0.7294718027114868}]}, {"text": "Our submission in 2017 is based on crosslanguage plagiarism detection methods combined with the best performing STS detection method published in 2016.", "labels": [], "entities": [{"text": "crosslanguage plagiarism detection", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.7217872738838196}, {"text": "STS detection", "start_pos": 112, "end_pos": 125, "type": "TASK", "confidence": 0.8842745125293732}]}, {"text": "CompiLIG team participated to SemEval STS for the first time in 2017.", "labels": [], "entities": [{"text": "SemEval STS", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.6204396784305573}]}, {"text": "The methods proposed are syntax-based, dictionary-based, context-based, and MT-based.", "labels": [], "entities": [{"text": "MT-based", "start_pos": 76, "end_pos": 84, "type": "TASK", "confidence": 0.9336936473846436}]}, {"text": "They show additive value when combined.", "labels": [], "entities": []}, {"text": "The submitted runs consist in (1) our best single unsupervised approach (2) an unsupervised combination of best approaches (3) a fine-tuned combination of best approaches.", "labels": [], "entities": []}, {"text": "The best of our three runs ranked 1 st with a correlation of 83.02% with human annotations on track 4a among all submitted systems (51 submissions from 20 teams for this track).", "labels": [], "entities": []}, {"text": "Correlation results of all participants (including ours) on track 4b were much lower and we try to explain why (and question the validity of track 4b) in the last part of this paper.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9879103899002075}]}], "datasetContent": [{"text": "Dataset, annotation and evaluation systems are presented in SemEval-2017 STS task description paper (.", "labels": [], "entities": [{"text": "SemEval-2017 STS task description paper", "start_pos": 60, "end_pos": 99, "type": "DATASET", "confidence": 0.6353368163108826}]}, {"text": "We can see in that our systems work well on SNLI    To investigate deeper on this issue, we manually annotated 60 random pairs of each sub-corpus (120 annotated pairs among 500).", "labels": [], "entities": []}, {"text": "These annotations provide a second annotator reference.", "labels": [], "entities": []}, {"text": "We can see in that, on SNLI corpus (4a), our methods behave the same way for both annotations (a difference of about 1.3%).", "labels": [], "entities": [{"text": "SNLI corpus (4a", "start_pos": 23, "end_pos": 38, "type": "DATASET", "confidence": 0.9376451373100281}]}, {"text": "However, the difference in correlation is huge between our annotations and SemEval gold standard on the WMT corpus (4b): 30% on average.", "labels": [], "entities": [{"text": "correlation", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9839407801628113}, {"text": "WMT corpus", "start_pos": 104, "end_pos": 114, "type": "DATASET", "confidence": 0.9293491840362549}]}, {"text": "The Pearson correlation between our annotated pairs and the related gold standard is 85.76% for the SNLI corpus and 29.16% for the WMT corpus.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9685059785842896}, {"text": "SNLI corpus", "start_pos": 100, "end_pos": 111, "type": "DATASET", "confidence": 0.9487681984901428}, {"text": "WMT corpus", "start_pos": 131, "end_pos": 141, "type": "DATASET", "confidence": 0.937905341386795}]}, {"text": "These results question the validity of the WMT corpus (4b) for semantic textual similarity detection.", "labels": [], "entities": [{"text": "WMT corpus", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.8280942738056183}, {"text": "semantic textual similarity detection", "start_pos": 63, "end_pos": 100, "type": "TASK", "confidence": 0.7279245033860207}]}], "tableCaptions": [{"text": " Table 1: Results of the methods on SemEval-2016  STS cross-lingual evaluation dataset.", "labels": [], "entities": [{"text": "SemEval-2016  STS cross-lingual evaluation dataset", "start_pos": 36, "end_pos": 86, "type": "DATASET", "confidence": 0.6160551369190216}]}, {"text": " Table 2: Official results of our submitted systems  on SemEval-2017 STS track 4 evaluation dataset.", "labels": [], "entities": [{"text": "SemEval-2017 STS track 4 evaluation dataset", "start_pos": 56, "end_pos": 99, "type": "DATASET", "confidence": 0.7421144445737203}]}, {"text": " Table 3: Results of our submitted systems scored  on our 120 annotated pairs and on the same 120  SemEval annotated pairs.", "labels": [], "entities": []}]}