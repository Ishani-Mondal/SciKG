{"title": [{"text": "Comparing Approaches for Automatic Question Identification", "labels": [], "entities": [{"text": "Automatic Question Identification", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.6126485466957092}]}], "abstractContent": [{"text": "Collecting spontaneous speech corpora that are open-ended, yet topically constrained , is increasingly popular for research in spoken dialogue systems and speaker state, inter alia.", "labels": [], "entities": []}, {"text": "Typically, these corpora are labeled by human annota-tors, either in the labor through crowd-sourcing; however, this is cumbersome and time-consuming for large corpora.", "labels": [], "entities": []}, {"text": "We present four different approaches to automatically tagging a corpus when general topics of the conversations are known.", "labels": [], "entities": []}, {"text": "We develop these approaches on the Columbia X-Cultural Deception corpus and find accuracy that significantly exceeds the base-line.", "labels": [], "entities": [{"text": "Columbia X-Cultural Deception corpus", "start_pos": 35, "end_pos": 71, "type": "DATASET", "confidence": 0.9175773113965988}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.99928879737854}]}, {"text": "Finally, we conduct a cross-corpus evaluation by testing the best performing approach on the Columbia/SRI/Colorado corpus.", "labels": [], "entities": [{"text": "Columbia/SRI/Colorado corpus", "start_pos": 93, "end_pos": 121, "type": "DATASET", "confidence": 0.943443497021993}]}], "introductionContent": [{"text": "Corpora of spontaneous speech are often collected through interviews or by otherwise providing subjects with question prompts.", "labels": [], "entities": []}, {"text": "Such corpora are semi-structured; they are constrained by the prompts used, but the elicited speech is openended in vocabulary and structure.", "labels": [], "entities": []}, {"text": "It is often desirable to segment these corpora into their underlying topics based on the questions asked.", "labels": [], "entities": []}, {"text": "This is typically done manually by annotators in the labor via crowd-sourcing.", "labels": [], "entities": []}, {"text": "However, such annotation is impractical and time-consuming for large corpora.", "labels": [], "entities": []}, {"text": "In this paper we describe a set of experiments aimed at automatically tagging a large corpus with topic labels.", "labels": [], "entities": []}, {"text": "We tag the Columbia XCultural Deception (CXD) corpus, a large-scale (120-hour) corpus of deceptive and non-deceptive dialogues collected using a semi-structured interview paradigm.", "labels": [], "entities": [{"text": "Columbia XCultural Deception (CXD) corpus", "start_pos": 11, "end_pos": 52, "type": "DATASET", "confidence": 0.822807422706059}]}, {"text": "Participants took turns interviewing each other using a fixed set of biographical interview questions , but the questions were asked in individual variants, in any order, and interviewers often asked follow-up questions.", "labels": [], "entities": []}, {"text": "For example, the question, \"Are your parents divorced?\" could be produced as \"Are your mom and dad still together?\"", "labels": [], "entities": []}, {"text": "These questions are semantically similar, but differ lexically, presenting the challenge of topically tagging a corpus based on semantic similarity.", "labels": [], "entities": []}, {"text": "The question, \"Have you ever broken a bone?\" could be followed by another, \"How did you break your bone?\"", "labels": [], "entities": []}, {"text": "This illustrates the challenge of distinguishing between phrases that are lexically similar, but differ semantically.", "labels": [], "entities": []}, {"text": "These two examples highlight problems faced when trying to automatically annotate a corpus for responses to a given set of questions.", "labels": [], "entities": []}, {"text": "With such a large corpus, it is not practical to manually annotate topic boundaries.", "labels": [], "entities": []}, {"text": "So, to compare question responses from multiple subjects, we identify conversational turns in the corpus that correspond to the original interview questions.", "labels": [], "entities": []}, {"text": "We compare four approaches to question identification: (1) a baseline approach that identifies questions using strict string matches, (2) the ROUGE metric which is based on n-gram comparisons, (3) cosine similarity between word embedding representations and (4) cosine similarity between document embeddings.", "labels": [], "entities": [{"text": "question identification", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.7361737191677094}, {"text": "ROUGE metric", "start_pos": 142, "end_pos": 154, "type": "METRIC", "confidence": 0.9665831029415131}]}, {"text": "We include experiments with varying thresholds for approaches (2), (3), and (4) to highlight the trade-off between precision and recall for these approaches.", "labels": [], "entities": [{"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9992883801460266}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9973453879356384}]}, {"text": "Finally, we test our best approach using word embeddings on another corpus, the Columbia/SRI/Colorado (CSC) corpus), collected with a similar interview paradigm but different questions, in order to evaluate the utility of this method in another domain.", "labels": [], "entities": [{"text": "Columbia/SRI/Colorado (CSC) corpus", "start_pos": 80, "end_pos": 114, "type": "DATASET", "confidence": 0.9504871235953437}]}, {"text": "This work draws upon the body of research on short-text semantic similarity (e.g. ().", "labels": [], "entities": [{"text": "short-text semantic similarity", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.6771459877490997}]}, {"text": "It is also related to work on topic segmentation (e.g. ( ), however here we focus on matching conversational turns to a fixed set of possible topics.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7412542402744293}]}, {"text": "While this work is done in support of our ongoing work on deception detection using speech and text-based features, we believe that our approach could be applied to other spontaneous transcribed speech or text corpora which were collected with some constraints on topics.", "labels": [], "entities": [{"text": "deception detection", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.9269596934318542}]}], "datasetContent": [{"text": "To further evaluate our best-performing approach, we applied the word embeddings method to another corpus collected using a similar interview paradigm, the Columbia SRI Colorado (CSC) corpus.", "labels": [], "entities": [{"text": "Columbia SRI Colorado (CSC) corpus", "start_pos": 156, "end_pos": 190, "type": "DATASET", "confidence": 0.9298508848462786}]}, {"text": "To test word embeddings on this corpus, we compiled 31 interviewer sessions that were already hand annotated, giving us a total of 6395 turns.", "labels": [], "entities": []}, {"text": "The (single) interviewer involving in collecting this corpus always began with a list of four standard biographical questions, thus reducing the number of turns that contained an interviewergenerated question to 114.", "labels": [], "entities": []}, {"text": "Following the word embeddings method described above, we obtained an accuracy of 99.8%, precision of 91.2%, recall of 100%, and F1-score of 95.3 on the CSC corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9996163845062256}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9996111989021301}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9995964169502258}, {"text": "F1-score", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9997456669807434}, {"text": "CSC corpus", "start_pos": 152, "end_pos": 162, "type": "DATASET", "confidence": 0.9661782681941986}]}, {"text": "The incorrectly identified questions were largely because the interviewer did not ask all four biographical questions in every session, while the word embeddings approach assumes that all questions were asked and therefore, matches some turn to the original question even though the interviewer did not ask it.", "labels": [], "entities": []}, {"text": "The higher accuracy obtained on the CSC corpus is probably due to the fact that the interviews were all conducted by a single interviewer, so the questions were asked with greater consistency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9992160797119141}, {"text": "CSC corpus", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.8942671418190002}, {"text": "consistency", "start_pos": 180, "end_pos": 191, "type": "METRIC", "confidence": 0.9601315855979919}]}, {"text": "In addition, all subjects were native speakers of Standard American English, while half the participants in the CXD corpus were native speakers of Mandarin Chinese.", "labels": [], "entities": [{"text": "CXD corpus", "start_pos": 112, "end_pos": 122, "type": "DATASET", "confidence": 0.9285494089126587}]}], "tableCaptions": []}