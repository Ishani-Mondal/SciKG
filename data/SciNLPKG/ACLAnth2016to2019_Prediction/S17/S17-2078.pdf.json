{"title": [], "abstractContent": [{"text": "This paper describes our submissions to task 7 in SemEval 2017, i.e., Detection and Interpretation of English Puns.", "labels": [], "entities": [{"text": "Detection and Interpretation of English Puns", "start_pos": 70, "end_pos": 114, "type": "TASK", "confidence": 0.8913910090923309}]}, {"text": "We participated in the first two subtasks, which are to detect and locate English puns respectively.", "labels": [], "entities": [{"text": "detect and locate English puns", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.7614172160625458}]}, {"text": "For subtask 1, we presented a supervised system to determine whether or not a sentence contains a pun using similarity features calculated on sense vectors or cluster center vectors.", "labels": [], "entities": []}, {"text": "For subtask 2, we established an unsupervised system to locate the pun by scoring each word in the sentence and we assumed that the word with the smallest score is the pun.", "labels": [], "entities": []}], "introductionContent": [{"text": "A pun is a form of wordplay in which one signifier (e.g., a word or phrase) suggests two or more meanings by exploiting polysemy, or phonological similarity to another signifier, for an intended humorous or rhetorical effect.", "labels": [], "entities": [{"text": "A pun is a form of wordplay in which one signifier (e.g., a word or phrase) suggests two or more meanings by exploiting polysemy, or phonological similarity to another signifier, for an intended humorous or rhetorical effect", "start_pos": 0, "end_pos": 224, "type": "Description", "confidence": 0.8371483641011375}]}, {"text": "The study of puns can be seen as a respectable research topic in traditional linguistics and the cognitive sciences.) contains three subtasks, i.e., pun detection, pun location, and pun interpretation.", "labels": [], "entities": [{"text": "pun detection", "start_pos": 149, "end_pos": 162, "type": "TASK", "confidence": 0.7169920057058334}, {"text": "pun location", "start_pos": 164, "end_pos": 176, "type": "TASK", "confidence": 0.6473708152770996}, {"text": "pun interpretation", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.7120126932859421}]}, {"text": "And we participated in the first two subtasks.", "labels": [], "entities": []}, {"text": "The detection and location of English puns are to determine whether or not a sentence contains a pun and which word is a pun respectively, which differ from traditional word sense disambiguation (WSD).", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 169, "end_pos": 200, "type": "TASK", "confidence": 0.7832160294055939}]}, {"text": "WSD is to determine an exact meaning of the target word in the given context.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.910982072353363}]}, {"text": "However, WSD algorithms could provide the lexical-semantic understanding for pun detection and location.", "labels": [], "entities": [{"text": "WSD", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.8800064921379089}, {"text": "pun detection", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.8003119826316833}]}, {"text": "And we adopted a knowledge-based WSD algorithm to obtain possible senses 1 for each word in the sentence.", "labels": [], "entities": []}, {"text": "The sense is the gloss provided by WordNet There are two types of puns: some are homographic puns and the others are heterographic puns.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9358180165290833}]}, {"text": "A homographic pun exploits distinct meanings of the same written word, and a heterographic pun exploits distinct meanings of the similar but not exactly the same spoken word.", "labels": [], "entities": []}, {"text": "The organizers provided two test datasets about homographic puns and heterographic puns respectively for each subtask.", "labels": [], "entities": []}, {"text": "Since they did not provide official training datasets, we collected our own positive samples(each sentence contains a pun) from the Pun of the Day website 2 , which conclude 60 homographic puns and 60 heterographic puns.", "labels": [], "entities": [{"text": "Pun of the Day website 2", "start_pos": 132, "end_pos": 156, "type": "DATASET", "confidence": 0.8227591812610626}]}, {"text": "Besides, we also assembled a raw dataset of 120 negative samples(sentences that do not contain puns) from the Internet.", "labels": [], "entities": []}, {"text": "Then, we combined 120 negative samples with 60 homographic puns or 60 heterographic puns into homographic or heterographic training dataset.", "labels": [], "entities": []}, {"text": "Then, we did the same data preprocessing for both training and test datasets.", "labels": [], "entities": []}, {"text": "Firstly, we performed part-of-speech(POS) tagging using Stanford CoreNLP tools).", "labels": [], "entities": [{"text": "part-of-speech(POS) tagging", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.6024450540542603}, {"text": "Stanford CoreNLP", "start_pos": 56, "end_pos": 72, "type": "DATASET", "confidence": 0.7950565814971924}]}, {"text": "Secondly, we removed the stop words in sentences.", "labels": [], "entities": []}, {"text": "The words produced after this series of processing are denoted as target words for each sentence.", "labels": [], "entities": []}, {"text": "Since there are two different puns, we adopted different methods for them to detect and locate English puns.", "labels": [], "entities": [{"text": "detect and locate English puns", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.7538493037223816}]}, {"text": "For homographic puns, we calculated the semantic similarity between sense vectors of each target word in the sentence to obtain a vector representation of a sentence and score each target word in the sentence, and for heterographic puns, we computed the semantic similarity between cluster center vectors of each sentence for the same purpose.", "labels": [], "entities": []}], "datasetContent": [{"text": "Although organizers did not provide the training datasets, we collected our own training datasets.", "labels": [], "entities": []}, {"text": "shows the statistics of the datasets we used in our experiments.: Statistics of datasets in training and test data.", "labels": [], "entities": []}, {"text": "The number in brackets are the percentages of different classes in each dataset.", "labels": [], "entities": []}, {"text": "For both subtask 1 and 2, the three widely-used evaluation measures precision(P), recall(R) and F 1 are adopted.", "labels": [], "entities": [{"text": "precision(P)", "start_pos": 68, "end_pos": 80, "type": "METRIC", "confidence": 0.9589607417583466}, {"text": "recall(R)", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9655386507511139}, {"text": "F 1", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9893875122070312}]}, {"text": "Moreover, for subtask 1, accuracy(Acc) is also included and for subtask 2, coverage(C) is used.", "labels": [], "entities": [{"text": "accuracy(Acc)", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.8846518695354462}, {"text": "coverage(C)", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.938555583357811}]}, {"text": "Coverage is defined as the ratio of sentence for which a location assignment was attempted.", "labels": [], "entities": []}, {"text": "Subtask 1 show the results of different algorithms of subtask 1 on homographic and heterographic training datasets respectively.", "labels": [], "entities": []}, {"text": "The 5-fold cross validation is performed for system development.", "labels": [], "entities": []}, {"text": "From and: Results of subtask 1 on heterographic training dataset. and show the results of our systems and the top-ranked systems provided by organizers for subtask 1 and subtask 2 respectively.", "labels": [], "entities": []}, {"text": "Compared with the top ranked systems, there is much room for improvement in our work.", "labels": [], "entities": []}, {"text": "The reason for the poor performance maybe that the constructing method of sense vectors is simple and straightforward, which neglects the word sequence and the sentence structure of the sense.", "labels": [], "entities": []}, {"text": "We find that detecting puns at the sentence level is more effective than locating puns at the word level, and our systems performed better on heterographic puns.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of subtask 1 on homographic  training dataset.", "labels": [], "entities": []}, {"text": " Table 3: Results of subtask 1 on heterographic  training dataset.", "labels": [], "entities": [{"text": "heterographic  training dataset", "start_pos": 34, "end_pos": 65, "type": "DATASET", "confidence": 0.6002165575822195}]}, {"text": " Table 4: Performance of our systems and the top- ranked(ranked by P) systems. The numbers in the  brackets are the official ranking", "labels": [], "entities": []}, {"text": " Table 5: Performance of our systems and the top- ranked(ranked by P) systems. The numbers in the  brackets are the official ranking", "labels": [], "entities": []}]}