{"title": [{"text": "Deep Active Learning for Dialogue Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose an online, end-to-end, neural generative conversational model for open-domain dialogue.", "labels": [], "entities": [{"text": "neural generative conversational", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.7993310888608297}]}, {"text": "It is trained using a unique combination of offline two-phase supervised learning and online human-in-the-loop active learning.", "labels": [], "entities": []}, {"text": "While most existing research proposes offline supervision or hand-crafted reward functions for online reinforcement, we devise a novel interactive learning mechanism based on hamming-diverse beam search for response generation and one-character user-feedback at each step.", "labels": [], "entities": [{"text": "response generation", "start_pos": 207, "end_pos": 226, "type": "TASK", "confidence": 0.6845385283231735}]}, {"text": "Experiments show that our model inherently promotes the generation of semantically relevant and interesting responses, and can be used to train agents with customized personas, moods and conversational styles.", "labels": [], "entities": []}], "introductionContent": [{"text": "Several recent works propose neural generative conversational agents (CAs) for open-domain and task-oriented dialogue.", "labels": [], "entities": [{"text": "neural generative conversational agents (CAs)", "start_pos": 29, "end_pos": 74, "type": "TASK", "confidence": 0.8497962270464215}]}, {"text": "These models typically use LSTM encoder-decoder architectures (e.g. the sequence-to-sequence (Seq2Seq) framework), which are linguistically robust but can often generate short, dull and inconsistent responses ().", "labels": [], "entities": []}, {"text": "Researchers are now exploring Deep Reinforcement Learning (DRL) to address the hard problems of NLU and NLG in dialogue generation.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7922338247299194}]}, {"text": "In most of the existing works, the reward function is hand-crafted, and is either specific to the task to be completed, or is based on a few desirable developer-defined conversational properties.", "labels": [], "entities": []}, {"text": "In this work we demonstrate how online Deep Active Learning can be integrated with standard neural network based dialogue systems to enhance their open-domain conversational skills.", "labels": [], "entities": []}, {"text": "The architectural backbone of our model is the Seq2Seq framework, which initially undergoes offline supervised learning on two different types of conversational datasets.", "labels": [], "entities": []}, {"text": "We then initiate an online active learning phase to interact with human users for incremental model improvement, where a unique single-character 1 user-feedback mechanism is used as a form of reinforcement at each turn in the dialogue.", "labels": [], "entities": []}, {"text": "The intuition is to rely on this all-encompassing human-centric 'reinforcement' mechanism, instead of defining hand-crafted reward functions that individually try to capture each of the many subtle conversational properties.", "labels": [], "entities": []}, {"text": "This mechanism inherently promotes interesting and relevant responses by relying on the humans' far superior conversational prowess.", "labels": [], "entities": []}, {"text": "posed to incorporate online human feedback into neural conversation models ().", "labels": [], "entities": []}, {"text": "Our work falls in this line of research, and is distinguished from existing approaches in the following key ways.", "labels": [], "entities": []}, {"text": "1. We use online deep active learning as a form of reinforcement in a novel way, which eliminates the need for hand-crafted reward criteria.", "labels": [], "entities": []}, {"text": "We use a diversity-promoting decoding heuristic () to facilitate this process.", "labels": [], "entities": []}, {"text": "2. Unlike existing CAs, our model can be tuned for one-shot learning.", "labels": [], "entities": []}, {"text": "It also eliminates the need to explicitly incorporate coherence, relevance or interestingness in the responses.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model via qualitative comparison with offline SL, as well as quantitative evaluation on four axes: syntactical coherence, relevance to prompts, interestingness and user engagement.", "labels": [], "entities": []}, {"text": "We begin by presenting the experimental results of the quantitative evaluation our CA's conversational abilities when trained via one-phase SL, two-phase SL and online AL (denoted by SL1, SL2 and SL2+oAL respectively).", "labels": [], "entities": [{"text": "oAL", "start_pos": 200, "end_pos": 203, "type": "METRIC", "confidence": 0.9035595655441284}]}, {"text": "We first asked a human trainer to actively train SL2+oAL using 200 prompts of his choice.", "labels": [], "entities": [{"text": "oAL", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.8305754661560059}]}, {"text": "We then created a test set of 100 prompts by randomly choosing 100 of the 200 training prompts and linguistically rephrasing each of them to convey the same semantics.", "labels": [], "entities": []}, {"text": "For instance, the AL training prompts 'How's it going?', 'I hate you' and 'What are your favorite pizza toppings?' were altered to the following test prompts: 'How are you doing?', 'I don't like you!' and 'What do you like on your pizza?'.", "labels": [], "entities": []}, {"text": "Next, we recorded SL1's, SL2's and SL2+oAL's responses to these test prompts.", "labels": [], "entities": [{"text": "oAL", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9217187762260437}]}, {"text": "Finally, we asked five human judges (not including the human trainer) to subjectively evaluate the responses of the three models on the test set.", "labels": [], "entities": []}, {"text": "The evaluation of each response was done on four axes: syntactical coherence, relevance to the prompt, interestingness and user engagement     was asked to assign each response an integer score of 0 (label = bad) or 1 (label = good).", "labels": [], "entities": []}, {"text": "Their averaged scores for the three models, SL1, SL2 and SL2+oAL, are shown in.", "labels": [], "entities": [{"text": "oAL", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9501175880432129}]}, {"text": "We see that SL2+oAL outperforms the other models on three of the four axes by 14-21%.", "labels": [], "entities": [{"text": "oAL", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9013463854789734}]}, {"text": "Next, we asked the human trainer to train SL2+oAL with the same 200 prompts and responses for different values of the initial learning rate for Adam (lr in Algorithm 1).", "labels": [], "entities": [{"text": "oAL", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.901918351650238}]}, {"text": "We then asked the five human judges to subjectively rate each model's syntactical coherence, response relevance, interestingness and user engagement.", "labels": [], "entities": []}, {"text": "Each model's percentage success on the test prompts was recorded on four axes.", "labels": [], "entities": []}, {"text": "The averaged scores are given in.", "labels": [], "entities": []}, {"text": "We see that the response quality drops significantly for higher values of learning rate.", "labels": [], "entities": []}, {"text": "This is due to the instability in the parameters induced by a high learning value associated with new data, causing the model to forget what it learned previously.", "labels": [], "entities": []}, {"text": "Our experiments suggest that a learning rate of 0.005 strikes the right balance between stability and one-shot learning.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.9411090314388275}]}, {"text": "Finally, we asked the human trainer to train SL2+oAL with lr = 0.005 and different number of training interactions.", "labels": [], "entities": [{"text": "oAL", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9502831101417542}]}, {"text": "The results in confirm that the model improves slowly as it continues to converse with humans.", "labels": [], "entities": []}, {"text": "This is an appropriate reflection of how humans learn language: gradually but effectively.", "labels": [], "entities": []}, {"text": "Although the curves seem to plateau after 300 training interactions and suggest that the learning has stopped, this is not the case.", "labels": [], "entities": []}, {"text": "The gradient is small but nonzero, which is an expected behavior of reinforcement learning algorithms in general.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Customized moods. Each SL2+oAL model was trained via 100 interactions.", "labels": [], "entities": [{"text": "oAL", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9418871402740479}]}]}