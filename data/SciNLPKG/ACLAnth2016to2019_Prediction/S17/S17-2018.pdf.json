{"title": [{"text": "OPI-JSA at SemEval-2017 Task 1: Application of Ensemble learning for computing semantic textual similarity", "labels": [], "entities": [{"text": "computing semantic textual similarity", "start_pos": 69, "end_pos": 106, "type": "TASK", "confidence": 0.6486806124448776}]}], "abstractContent": [{"text": "Semantic Textual Similarity (STS) evaluation assesses the degree to which two parts of texts are similar, based on their semantic evaluation.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) evaluation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7971328241484505}]}, {"text": "In this paper, we describe three models submitted to STS SemEval 2017.", "labels": [], "entities": [{"text": "STS SemEval 2017", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.73675008614858}]}, {"text": "Given two English parts of a text, each of proposed methods outputs the assessment of their semantic similarity.", "labels": [], "entities": []}, {"text": "We propose an approach for computing monolingual semantic textual similarity based on an ensemble of three distinct methods.", "labels": [], "entities": [{"text": "computing monolingual semantic textual similarity", "start_pos": 27, "end_pos": 76, "type": "TASK", "confidence": 0.6174545109272003}]}, {"text": "Our model consists of recursive neural network (RNN) text auto-encoders ensemble with supervised a model of vec-torized sentences using reduced part of speech (PoS) weighted word embeddings as well as unsupervised a method based on word coverage (TakeLab).", "labels": [], "entities": []}, {"text": "Additionally , we enrich our model with additional features that allow disambiguation of ensemble methods based on their efficiency.", "labels": [], "entities": []}, {"text": "We have used Multi-Layer Perceptron as an ensemble classifier basing on estimations of trained Gradient Boosting Regressors.", "labels": [], "entities": []}, {"text": "Results of our research proves that using such ensemble leads to a higher accuracy due to a fact that each member-algorithm tends to specialize in particular type of sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9993870258331299}]}, {"text": "Simple model based on PoS weighted Word2Vec word embed-dings seem to improve performance of more complex RNN based auto-encoders in the ensemble.", "labels": [], "entities": []}, {"text": "In the monolingual English-English STS subtask our Ensemble based model achieved mean Pearson correlation of .785 compared with human annotators.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 86, "end_pos": 105, "type": "METRIC", "confidence": 0.9650352597236633}]}], "introductionContent": [{"text": "The objective of a system for evaluating semantic textual similarity, is to produce a value which serves as a rating of semantic similarity between pair of text samples.", "labels": [], "entities": []}, {"text": "Such task certainly could not be regarded as toy problem, the results could be used to solve multiple real-world problems, e.g. plagiarism detection.", "labels": [], "entities": [{"text": "plagiarism detection", "start_pos": 128, "end_pos": 148, "type": "TASK", "confidence": 0.7597341537475586}]}, {"text": "We used described methods in STS task in the).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The official results on the test dataset for  Subtask 5 (english-english).  Method  Pearson score  Run 3: Ensemble  0.7850  Run 2: Skip Thoughts Vectors  0.7342  STS Baseline  0.7278  Run 1: PoS-Word2Vec  0.6796", "labels": [], "entities": [{"text": "Pearson score  Run", "start_pos": 94, "end_pos": 112, "type": "METRIC", "confidence": 0.9394681453704834}, {"text": "STS", "start_pos": 172, "end_pos": 175, "type": "METRIC", "confidence": 0.8846752047538757}]}]}