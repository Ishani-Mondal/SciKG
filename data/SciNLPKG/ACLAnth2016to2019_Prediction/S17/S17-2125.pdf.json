{"title": [{"text": "funSentiment at SemEval-2017 Task 4: Topic-Based Message Senti- ment Classification by Exploiting Word Embeddings, Text Features and Target Contexts", "labels": [], "entities": [{"text": "Topic-Based Message Senti- ment Classification", "start_pos": 37, "end_pos": 83, "type": "TASK", "confidence": 0.6914441585540771}]}], "abstractContent": [{"text": "This paper describes the approach we used for SemEval-2017 Task 4: Sentiment Analysis in Twitter.", "labels": [], "entities": [{"text": "SemEval-2017 Task 4", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.8929333885510763}, {"text": "Sentiment Analysis in Twitter", "start_pos": 67, "end_pos": 96, "type": "TASK", "confidence": 0.8439564555883408}]}, {"text": "Topic-based (target-dependent) sentiment analysis has become attractive and been used in some applications recently, but it is still a challenging research task.", "labels": [], "entities": [{"text": "Topic-based (target-dependent) sentiment analysis", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.6422782987356186}]}, {"text": "In our approach, we take the left and right context of a target into consideration when generating polarity classification features.", "labels": [], "entities": []}, {"text": "We use two types of word embeddings in our classifiers: the general word embeddings learned from 200 million tweets, and sentiment specific word embeddings learned from 10 million tweets using distance supervision.", "labels": [], "entities": []}, {"text": "We also incorporate a text feature model in our algorithm.", "labels": [], "entities": []}, {"text": "This model produces features based on text negation, tf.idf weighting scheme, and a Rocchio text classification method.", "labels": [], "entities": [{"text": "text negation", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.6979333758354187}, {"text": "text classification", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7250031530857086}]}, {"text": "We participated in four subtasks (B, C, D & E for English), all of which are about topic-based message polarity classification.", "labels": [], "entities": [{"text": "topic-based message polarity classification", "start_pos": 83, "end_pos": 126, "type": "TASK", "confidence": 0.6739944666624069}]}, {"text": "Our team is ranked #6 in subtask B, #3 by MAE u and #9 by MAE min sub-task C, #3 using RAE and #6 using KLD in subtask D, and #3 in subtask E.", "labels": [], "entities": [{"text": "RAE", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9660153985023499}]}], "introductionContent": [{"text": "There have been many studies on message or sentence level sentiment classification (;), but there are few studies on target-dependent, or topic-based, sentiment prediction.", "labels": [], "entities": [{"text": "message or sentence level sentiment classification", "start_pos": 32, "end_pos": 82, "type": "TASK", "confidence": 0.663917675614357}, {"text": "topic-based, sentiment prediction", "start_pos": 138, "end_pos": 171, "type": "TASK", "confidence": 0.7124557793140411}]}, {"text": "A target entity in a message does not necessarily have the same polarity type as the message, and different entities in the same message may have different polarities.", "labels": [], "entities": []}, {"text": "For example, in the tweet \"Linux is better than Windows\", the two named entities, Linux and Windows, will have different sentiment polarities.", "labels": [], "entities": []}, {"text": "In this paper, we describe our approach for the subtask B, C, D & E of SemEval-17 Task 4: Sentiment Analysis in Twitter).", "labels": [], "entities": [{"text": "SemEval-17 Task 4", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.86299200852712}, {"text": "Sentiment Analysis in Twitter", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.8066907376050949}]}, {"text": "All the four subtasks are on topic-based message sentiment classification.", "labels": [], "entities": [{"text": "topic-based message sentiment classification", "start_pos": 29, "end_pos": 73, "type": "TASK", "confidence": 0.6983411088585854}]}, {"text": "Task B and C are about topic-based message polarity classification.", "labels": [], "entities": [{"text": "topic-based message polarity classification", "start_pos": 23, "end_pos": 66, "type": "TASK", "confidence": 0.6893950253725052}]}, {"text": "Given a message and a topic, in task B, we classify the message on a two-point scale: positive or negative sentiment towards the topic.", "labels": [], "entities": []}, {"text": "And in task C, we classify the message on a five-point scale: sentiment conveyed by the tweet towards the topic on a five-point scale.", "labels": [], "entities": []}, {"text": "Task D and E are about Tweet quantification.", "labels": [], "entities": [{"text": "Tweet quantification", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.913351833820343}]}, {"text": "Given a set of tweets about a given topic, in task D, we want to estimate the distribution of the tweets across two-point scale -the positive and negative classes, and in task E, we estimate that on a five-point scale -the five classes of a fivepoint scale.", "labels": [], "entities": []}, {"text": "Our approach uses word embeddings (WE) learned from general tweets, sentiment specific word embeddings (SSWE) learned from distance supervised tweets, and a weighted text feature model (WTM).", "labels": [], "entities": []}, {"text": "Learning features directly from tweet text has recently gained lot of attention.", "labels": [], "entities": []}, {"text": "One approach is to generate sentence representations from word embeddings.", "labels": [], "entities": []}, {"text": "Several word embedding generation algorithms have been proposed in previous studies.", "labels": [], "entities": [{"text": "word embedding generation", "start_pos": 8, "end_pos": 33, "type": "TASK", "confidence": 0.7491489251454672}]}, {"text": "Using the general word embeddings directly in sentiment classification is not effective, since they mainly model a word's semantic context, ignoring the sentiment clues in text.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.9347098767757416}]}, {"text": "Therefore, words with opposite polarity, such as worst and best, are mapped onto vectors embeddings that are close to each other in some dimensions.", "labels": [], "entities": []}, {"text": "propose a sentiment-specific word embedding (SSWE) method for sentiment analysis, by extending the word embedding algorithm.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.9564207494258881}]}, {"text": "SSWE encodes sentiment information in the word embeddings.", "labels": [], "entities": [{"text": "SSWE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8344852328300476}]}, {"text": "In our approach, we incorporate WE, SSWE and a weighted text feature model (WTM) together.", "labels": [], "entities": []}, {"text": "The WTM model generates two types of features.", "labels": [], "entities": []}, {"text": "The first type is a negation feature based on the negation words in a tweet.", "labels": [], "entities": []}, {"text": "The second set of features is created by computing the similarity between the tweet and each of the polarity types, using cosine similarity and the tf.idf word weighting scheme.", "labels": [], "entities": []}, {"text": "Each polarity category is represented by a pseudo centroid tweet learned from training data.", "labels": [], "entities": []}, {"text": "This is very similar to the Rocchio text classifier, but here all the similarity values with all the polarity types are used as features, and fed to the classification algorithm.", "labels": [], "entities": []}, {"text": "The rationale behind the second set of features is that the similarity values with the different polarity types will have some correlations, and using all of them as features will provide more information to the classifier.", "labels": [], "entities": []}, {"text": "For example, a positive tweet usually will have a higher similarity value with neutral type than with the negative type.", "labels": [], "entities": [{"text": "similarity", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9475148916244507}]}, {"text": "This will provide an additional signal to the classifier.", "labels": [], "entities": []}, {"text": "The context of an entity will affect its polarity value, and usually an entity has a left context and also aright one, unless it is at the beginning or end of a message.", "labels": [], "entities": []}, {"text": "Both the context information and the interaction between these two contexts are included in the classification features of our approach.", "labels": [], "entities": []}, {"text": "Our approach uses both SSWE and WE to represent these contexts, since WE and SSWE complement each other, and our experiment shows that using both increases the accuracy by more than 6%, compared to using only one of them.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9992595314979553}]}, {"text": "In the following sections, we present the related studies, our methodology and the experiments and results for subtask B, C, D and E.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Evaluation result for subtask B, C, D & E.", "labels": [], "entities": []}]}