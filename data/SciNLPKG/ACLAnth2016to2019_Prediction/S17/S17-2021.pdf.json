{"title": [{"text": "ResSim at SemEval-2017 Task 1: Multilingual Word Representations for Semantic Textual Similarity", "labels": [], "entities": [{"text": "ResSim", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9132044315338135}, {"text": "Multilingual Word Representations", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.6338389913241068}, {"text": "Semantic Textual Similarity", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.6072400510311127}]}], "abstractContent": [{"text": "Shared Task 1 at SemEval-2017 deals with assessing the semantic similarity between sentences, either in the same or in different languages.", "labels": [], "entities": []}, {"text": "In our system submission, we employ multilingual word representations , in which similar words in different languages are close to one another.", "labels": [], "entities": []}, {"text": "Using such representations is advantageous, since the increasing amount of available parallel data allows for the application of such methods to many of the languages in the world.", "labels": [], "entities": []}, {"text": "Hence, semantic similarity can be inferred even for languages for which no annotated data exists.", "labels": [], "entities": []}, {"text": "Our system is trained and evaluated on all language pairs included in the shared task (English, Span-ish, Arabic, and Turkish).", "labels": [], "entities": []}, {"text": "Although development results are promising, our system does not yield high performance on the shared task test sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) is the task of assessing the degree to which two sentences are semantically similar.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8211836715539297}]}, {"text": "Within the SemEval STS shared tasks, this is measured on a scale ranging from 0 (no semantic similarity) to 5 (complete semantic similarity) (.", "labels": [], "entities": [{"text": "SemEval STS shared tasks", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.8413571715354919}]}, {"text": "Monolingual STS is an important task, for instance for evaluation of machine translation (MT) systems, where estimating the semantic similarity between a system's translation and the gold translation can aid both system evaluation and development.", "labels": [], "entities": [{"text": "Monolingual STS", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.824660450220108}, {"text": "evaluation of machine translation (MT)", "start_pos": 55, "end_pos": 93, "type": "TASK", "confidence": 0.7557461474622998}]}, {"text": "The task is already a challenging one in a monolingual setting, e.g., when estimating the similarity between two English sentences.", "labels": [], "entities": [{"text": "estimating the similarity between two English sentences", "start_pos": 75, "end_pos": 130, "type": "TASK", "confidence": 0.7843662926128933}]}, {"text": "In this paper, we tackle the more difficult case of cross-lingual STS, e.g., estimating the similarity between an English and an Arabic sentence.", "labels": [], "entities": [{"text": "cross-lingual STS", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7118955254554749}]}, {"text": "Previous approaches to this problem have focussed on two main approaches.", "labels": [], "entities": []}, {"text": "On the one hand, MT approaches have been attempted (e.g.), which allow for monolingual similarity assessment, but suffer from the fact that involving a fully-fledged MT system severely increases system complexity.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9836408495903015}, {"text": "monolingual similarity assessment", "start_pos": 75, "end_pos": 108, "type": "TASK", "confidence": 0.5723235209782919}]}, {"text": "Applying bilingual word representations, on the other hand, bypasses this issue without inducing such complexity (e.g.).", "labels": [], "entities": []}, {"text": "However, bilingual approaches do not allow for taking advantage of the increasing amount of STS data available for more than one language pair.", "labels": [], "entities": []}, {"text": "Currently, there are several methods available for obtaining high quality multilingual word representations.", "labels": [], "entities": []}, {"text": "It is therefore interesting to investigate whether language can be ignored entirely in an STS system after mapping words to their respective representations.", "labels": [], "entities": []}, {"text": "We investigate the utility of multilingual word representations in a crosslingual STS setting.", "labels": [], "entities": []}, {"text": "We approach this by combining multilingual word representations with a deep neural network, in which all parameters are shared, regardless of language combinations.", "labels": [], "entities": []}, {"text": "The contributions of this paper can be summed as follows: i) we show that multilingual input representations in some cases can be used to train an STS system without access to training data fora given language; ii) we show that access to data from other languages in some cases improves system performance fora given language.", "labels": [], "entities": []}], "datasetContent": [{"text": "We aim to investigate whether using a multilingual input representation and shared weights allow us to ignore languages in STS.", "labels": [], "entities": []}, {"text": "We first train and evaluate single-source trained systems (i.e. on a single language pair), and evaluate this both us-ing the same language pair as target, and on all other target language pairs.", "labels": [], "entities": []}, {"text": "1 Secondly, we investigate the effect of bundling training data together, investigating which language pairings are helpful for each other.", "labels": [], "entities": []}, {"text": "We measure performance between gold similarities and system output using the Pearson correlation measure, as this is standard in the SemEval STS shared tasks.", "labels": [], "entities": [{"text": "Pearson correlation measure", "start_pos": 77, "end_pos": 104, "type": "METRIC", "confidence": 0.9434667030970255}, {"text": "SemEval STS shared tasks", "start_pos": 133, "end_pos": 157, "type": "TASK", "confidence": 0.7352914810180664}]}, {"text": "We first present results on the development sets, and finally the official shared task evaluation results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Training on the tar- get language pair generally yields the highest  results, except for one case. When evaluating  on Arabic/Arabic sentence pairs, training on En- glish/Arabic texts yields comparable, or slightly  better, performance than when training on Ara- bic/Arabic.", "labels": [], "entities": []}, {"text": " Table 2: Single-source training results (Pearson  correlations). Columns indicate training language  pairs, and rows indicate testing language pairs.  Bold numbers indicate best results per row.", "labels": [], "entities": [{"text": "Pearson  correlations", "start_pos": 42, "end_pos": 63, "type": "METRIC", "confidence": 0.8954967260360718}]}, {"text": " Table 3: Training results with one source in ad- dition to in-language data (Pearson correlations).  Columns indicate added training language pairs,  and rows indicate testing language pairs. Bold  numbers indicate best results per row.", "labels": [], "entities": [{"text": "Pearson correlations)", "start_pos": 78, "end_pos": 99, "type": "METRIC", "confidence": 0.9333416819572449}]}, {"text": " Table 4: Ablation results (Pearson correlations).  Columns indicate ablated language pairs, and rows  indicate testing language pairs. The none column  indicates no ablation, i.e., training on all three  monolingual pairs. Bold indicates results when  not training on the language pair evaluated on.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.998427152633667}, {"text": "Pearson correlations", "start_pos": 28, "end_pos": 48, "type": "METRIC", "confidence": 0.9151621758937836}]}, {"text": " Table 5: Results on SemEval-2017 Shared Task Test sets.", "labels": [], "entities": [{"text": "SemEval-2017 Shared Task Test sets", "start_pos": 21, "end_pos": 55, "type": "DATASET", "confidence": 0.79940185546875}]}]}