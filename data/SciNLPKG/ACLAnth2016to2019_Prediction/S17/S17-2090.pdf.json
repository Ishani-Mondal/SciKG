{"title": [{"text": "Findings of the 2016 conference on machine translation", "labels": [], "entities": [{"text": "machine translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8254894614219666}]}], "abstractContent": [{"text": "In this report we summarize the results of the 2017 AMR SemEval shared task.", "labels": [], "entities": [{"text": "AMR SemEval shared task", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.7369480580091476}]}, {"text": "The task consisted of two separate yet related subtasks.", "labels": [], "entities": []}, {"text": "In the parsing sub-task, participants were asked to produce Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs fora set of English sentences in the biomedical domain.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 60, "end_pos": 97, "type": "METRIC", "confidence": 0.8662251929442087}]}, {"text": "In the generation subtask, participants were asked to generate English sentences given AMR graphs in the news/fo-rum domain.", "labels": [], "entities": []}, {"text": "A total of five sites participated in the parsing subtask, and four participated in the generation subtask.", "labels": [], "entities": [{"text": "parsing subtask", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.8978594243526459}]}, {"text": "Along with a description of the task and the par-ticipants' systems, we show various score ablations and some sample outputs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8513260285059611}]}, {"text": "It includes entity identification and typing, PropBank semantic roles), individual entities playing multiple roles, as well as treatments of modality, negation, etc.", "labels": [], "entities": [{"text": "entity identification", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7868285179138184}]}, {"text": "AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj).", "labels": [], "entities": [{"text": "AMR abstracts", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.8135852217674255}]}, {"text": "In 2016 an AMR parsing shared task was held at SemEval.", "labels": [], "entities": [{"text": "AMR parsing shared task", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.8880584239959717}]}, {"text": "Task participants demonstrated several new directions in AMR parsing technology and also validated the strong performance of existing parsers.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9499496221542358}]}, {"text": "We sought, in 2017, to focus AMR parsing performance on the biomedical domain, for which a not insignificant but still relatively small training corpus had been produced.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9770482182502747}]}, {"text": "While sentences from this domain are quite The soldier was not afraid of dying.", "labels": [], "entities": []}, {"text": "The soldier was not afraid to die.", "labels": [], "entities": []}, {"text": "The soldier did not fear death.", "labels": [], "entities": []}, {"text": "formal compared to some of those evaluated in last year's task, they are also very complex, and have many terms unique to the domain.", "labels": [], "entities": []}, {"text": "An example is shown in.", "labels": [], "entities": []}, {"text": "We continue to use) as a metric for AMR parsing, but we perform additional ablative analysis using the approach proposed by.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.9286162257194519}]}, {"text": "Along with parsing into AMR, it is important to encourage improvements in automatic generation of natural language (NL) text from AMR.", "labels": [], "entities": [{"text": "automatic generation of natural language (NL) text from AMR", "start_pos": 74, "end_pos": 133, "type": "TASK", "confidence": 0.7869936742565848}]}, {"text": "Humans favor communication in NL.", "labels": [], "entities": [{"text": "NL", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.8024622201919556}]}, {"text": "An AI that is able to parse text into AMR at a quality level indistinguishable from humans maybe said to understand NL, but without the ability to render its own semantic representations into NL no human will ever be able to appreciate this.", "labels": [], "entities": []}, {"text": "The advent of several systems that generate English text from AMR input () inspired us to conduct a generation-based shared task from AMRs in the news/discussion forum domain.", "labels": [], "entities": []}, {"text": "For the generation subtask, we solicited human judgments of sentence quality.", "labels": [], "entities": []}, {"text": "We followed the precedent established by the Workshop in Machine Translation ( and used the Appraise solicitation system, lightly mod-Interestingly, serpinE2 mRNA and protein were also markedly enhanced inhuman CRC cells exhibiting mutation in <i>KRAS </i>and <i>BRAF</i>.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.8018447160720825}]}, {"text": "(e / enhance-01 :li 2 :ARG1 (a3 / and :op1 (n6 / nucleic-acid :name (n / name :op1 \"mRNA\") :ARG0-of (e2 / encode-01 :ARG1 p)) :op2 (p / protein :name (n2 / name :op1 \"serpinE2\"))) :manner (m / marked) :mod (a2 / also) :location (c / cell :ARG0-of (e3 / exhibit-01 :ARG1 (m2 / mutate-01 :ARG1 (a4 / and :op1 (g / gene :name (n4 / name :op1 \"KRAS\")) :op2 (g2 / gene :name (n5 / name :op1 \"BRAF\"))))) :mod (h / human) :mod (d / disease :name (n3 / name :op1 \"CRC\"))) :manner (i / interesting)): One of the simpler biomedical domain sentences and its AMR.", "labels": [], "entities": []}, {"text": "Note the italics markers in the original sentence are preserved, as they are semantically important to the sentence's understanding.", "labels": [], "entities": []}, {"text": "ified, to gather human rankings, then TrueSkill () to elicit an overall system ranking.", "labels": [], "entities": []}, {"text": "Since the same training data and tools are available to both subtasks (though, in the case of the generation subtask, the utility of the Bio-AMR corpus is unclear), we will describe all the resources for both subtasks in Sections 2 and 3 but then will handle descriptions and ablations for the parsing and generation subtasks separately, in, respectively, Sections 4 and 5.", "labels": [], "entities": [{"text": "Bio-AMR corpus", "start_pos": 137, "end_pos": 151, "type": "DATASET", "confidence": 0.8303328454494476}]}, {"text": "Readers interested in only one of these subtasks should not feel compelled to read the other section.", "labels": [], "entities": []}, {"text": "We will reconvene in Section 6 to conclude and discuss hardware, as we continue the tradition established last year in the awarding of trophies to the declared winners of each subtask.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used Appraise, an opensource system for manual evaluation of machine translation, to conduct a human evaluation of generation quality.", "labels": [], "entities": [{"text": "Appraise", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.8257750868797302}, {"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7345803380012512}]}, {"text": "The system asks human judges to rank randomly selected systems' translations of sentences from the test corpus.", "labels": [], "entities": []}, {"text": "This in turn yields pairwise preference information that can be used to effect an overall system ranking.", "labels": [], "entities": []}, {"text": "For the purposes of this task we needed to adapt the Appraise system to admit nested representations of AMRs, and to be compatible with our IT infrastructure.", "labels": [], "entities": [{"text": "Appraise system", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.7218252122402191}]}, {"text": "A screenshot is shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A summary of data used in this task; split sizes indicate the number of AMRs per sub-corpus.", "labels": [], "entities": []}, {"text": " Table 2: Main parsing results: For Smatch, a mean of ten runs with ten restarts per run is shown;  standard deviation was about 0.0003 per system. For the remaining ablations, a single run was used.", "labels": [], "entities": [{"text": "Main parsing", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.7539441585540771}, {"text": "Smatch", "start_pos": 36, "end_pos": 42, "type": "TASK", "confidence": 0.8337030410766602}, {"text": "standard deviation", "start_pos": 100, "end_pos": 118, "type": "METRIC", "confidence": 0.947791337966919}]}, {"text": " Table 3: Main generation results: The three  manually-derived metrics agree on the systems'  relative rankings.", "labels": [], "entities": []}, {"text": " Table 4: Human judgments of generation results  after self-judgments are removed: The results are  fundamentally the same", "labels": [], "entities": []}]}