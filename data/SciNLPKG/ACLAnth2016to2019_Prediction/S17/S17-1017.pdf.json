{"title": [{"text": "The (Too Many) Problems of Analogical Reasoning with Word Vectors", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper explores the possibilities of analogical reasoning with vector space models.", "labels": [], "entities": [{"text": "analogical reasoning", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.8805505931377411}]}, {"text": "Given two pairs of words with the same relation (e.g. man:woman :: king:queen), it was proposed that the offset between one pair of the corresponding word vectors can be used to identify the unknown member of the other pair (\u2212\u2212\u2192 king \u2212 \u2212\u2212\u2192 man + \u2212 \u2212\u2212\u2212\u2212 \u2192 woman = ? \u2212\u2212\u2212\u2192 queen).", "labels": [], "entities": []}, {"text": "We argue against such \"linguistic regularities\" as a model for linguistic relations in vector space models and as a benchmark, and we show that the vector offset (as well as two other, better-performing methods) suffers from dependence on vector similarity.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper considers the phenomenon of \"vectororiented reasoning\" via linear vector offset in vector space models (VSMs).", "labels": [], "entities": []}, {"text": "Given two pairs of words with the same linguistic relation (woman:man :: king:queen), it has been proposed that the offset between one pair of word vectors can be used to identify the unknown member of a different pair of words via solving proportional analogy problems ( This approach attracted a lot of attention, both as the \"poster child\" of word embeddings, and for its potential practical utility.", "labels": [], "entities": []}, {"text": "Given the vital role that analogical reasoning plays inhuman cognition for discovering new knowledge and understanding new concepts, automated analogical reasoning could become a game-changer in many fields, providing a universal mechanism for detecting linguistic relations and word sense disambiguation (.", "labels": [], "entities": [{"text": "detecting linguistic relations", "start_pos": 244, "end_pos": 274, "type": "TASK", "confidence": 0.7703808347384135}, {"text": "word sense disambiguation", "start_pos": 279, "end_pos": 304, "type": "TASK", "confidence": 0.6644954979419708}]}, {"text": "It is already used in many downstream NLP tasks, such as splitting compounds (, semantic search, cross-language relational search (, to name a few. provided.", "labels": [], "entities": [{"text": "splitting compounds", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.8720034658908844}, {"text": "cross-language relational search", "start_pos": 97, "end_pos": 129, "type": "TASK", "confidence": 0.6806479493776957}]}, {"text": "We have explored several related methods and found that the proposed method performs well for both syntactic and semantic relations.", "labels": [], "entities": []}, {"text": "We note that this measure is qualitatively similar to relational similarity model of, which predicts similarity between members of the word pairs (x b , x d ), (x c , x d ) and dis-similarity for (x a , x d ).", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the vector offset method, we used vectors generated by the RNN toolkit of.", "labels": [], "entities": [{"text": "vector offset", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.7133232057094574}, {"text": "RNN toolkit", "start_pos": 71, "end_pos": 82, "type": "DATASET", "confidence": 0.880869060754776}]}, {"text": "Vectors of dimensionality 80, 320, and 640 were generated, along with a composite of several systems, with total dimensionality 1600.", "labels": [], "entities": []}, {"text": "The systems were trained with 320M words of Broadcast News data as described in, and had an 82k vocabulary.", "labels": [], "entities": [{"text": "Broadcast News data", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.950349231561025}]}, {"text": "shows results for both RNNLM and LSA vectors on the syntactic task.", "labels": [], "entities": []}, {"text": "LSA was trained on the same data as the RNN.", "labels": [], "entities": [{"text": "LSA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8982892036437988}, {"text": "RNN", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.926240861415863}]}, {"text": "We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly.", "labels": [], "entities": []}, {"text": "In we compare the RNN vectors with those based on the methods of Collobert and Weston and, as implemented by) and available online Since different words are present in these datasets,    The idea that linguistic relations are mirrored in neat geometrical relations (as shown in) is also intuitively appealing, and 3CosAdd has become a popular benchmark.", "labels": [], "entities": []}, {"text": "Roughly, the current VSMs score between 40% () and 75% () on the Google test set ().", "labels": [], "entities": [{"text": "Google test set", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.9238417545954386}]}, {"text": "However, in fact performance varies widely for different types of relations (.", "labels": [], "entities": []}, {"text": "One way to explain the current limitations is to attribute them to the imperfections of the current models and/or corpora with which they are built: with this view, in a perfect VSM, any linguistic relation should be recoverable via vector offset.", "labels": [], "entities": []}, {"text": "The alternative to be explored in this paper is that perhaps natural language semantics is more complex than suggested by, and there maybe both theoretical and mathematical issues with analogical reasoning with word vectors and its 3CosAdd implementation.", "labels": [], "entities": []}, {"text": "We present a series of experiments with two popular VSMs (GloVe and Word2Vec) to show that the accuracy of 3CosAdd depends on the proximity of the target vector to its source (i.e. \u2212\u2212\u2212\u2192 queen should be quite similar to \u2212\u2212\u2192 king).", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.9294635057449341}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9993414282798767}]}, {"text": "Since not all linguistic relations can be expected to result in high word vector proximity, the method is limited to those that happen to be so in a given VSM.", "labels": [], "entities": []}, {"text": "Furthermore, its accuracy also varies because the \"linguistic regularities\" are actually not so regular, and should not be expected to be so.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9994713664054871}]}, {"text": "We also compare 3CosAdd to two alternative methods to investigate whether better algorithms can improve on these and other accounts.", "labels": [], "entities": []}, {"text": "2 Background: \"Relational Similarity\" vs \"Word Analogies\" The most fundamental term for what 3CosAdd is supposed to capture is actually not analogy, but rather relational similarity, i.e. the idea that pairs of words may hold similar relations to those between other pairs of words.", "labels": [], "entities": [{"text": "Relational Similarity", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.8761507868766785}]}, {"text": "For example, the relation between cat and feline is similar to the relation between dog and canine.", "labels": [], "entities": []}, {"text": "Notably, this is similarity rather than identity: \"instances of a single relation may still have significant variability in how characteristic they are of that class\" (.", "labels": [], "entities": []}, {"text": "Analogy as it is known in philosophy and logic is something quite different.", "labels": [], "entities": []}, {"text": "The \"classical\" analogical reasoning follows roughly this template: objects X and Y share properties a, b, and c; therefore, they may also share the property d.", "labels": [], "entities": []}, {"text": "For example, both Earth and Mars orbit the Sun, have at least one moon, revolve on axis, and are subject to gravity; therefore, if Earth supports life, so could Mars (.", "labels": [], "entities": []}, {"text": "The NLP move from relational similarity to analogy follows the use of the term by P. Turney, who distinguishes between attributional similarity between two words and relational similarity between two pairs of words.", "labels": [], "entities": []}, {"text": "On this interpretation, two word pairs that have a high degree of relational similarity are analogous).", "labels": [], "entities": []}, {"text": "In terms of practical NLP tasks, introduced the task of solving SAT 1 analogy problems by choosing from several provided options.", "labels": [], "entities": [{"text": "solving SAT 1 analogy problems", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.717678701877594}]}, {"text": "These problems were formulated as proportional analogies, written in the form a : a :: It is this use of the term \"analogy\" that Mikolov et al.", "labels": [], "entities": []}, {"text": "(2013c) followed in proposing the 3CosAdd method.", "labels": [], "entities": []}, {"text": "They formulated the task as selecting a single best fitting vector out of the whole vocabu-1 Scholastic Aptitude Test.", "labels": [], "entities": []}, {"text": "It became known as word analogy task, but in its core it is still basically estimation of relational similarity, and could be formulated as such: given a pair of words a and a , find how they are related and then find word b , such that it has a similar relation with the word b.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.8417485952377319}]}, {"text": "A crucial difference is that the graded, non-binary nature of relational similarity is now not in focus: the goal is to find a single correct answer.", "labels": [], "entities": []}, {"text": "The dataset that came to be known as the Google analogy test set (), included 14 linguistic relations with 19544 questions in total.", "labels": [], "entities": [{"text": "Google analogy test set", "start_pos": 41, "end_pos": 64, "type": "DATASET", "confidence": 0.7876813560724258}]}, {"text": "It has become one of the most popular benchmarks for VSMs.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.8585779666900635}]}, {"text": "This evaluation paradigm assumes that: (1) Words in similar linguistic relations should in principle be recoverable via relational similarity to known word pairs.", "labels": [], "entities": []}, {"text": "(2) 3CosAdd score reflects the extent to which a given VSM encodes linguistic relations.", "labels": [], "entities": [{"text": "VSM encodes linguistic relations", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.7429259270429611}]}, {"text": "(1) became dubious when it was shown that accuracy of 3CosAdd varies widely between categories (, and even the best-performing GloVe model scores under 30% on the more challenging Bigger Analogy Test Set (BATS) ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9994956254959106}, {"text": "Bigger Analogy Test Set (BATS)", "start_pos": 180, "end_pos": 210, "type": "DATASET", "confidence": 0.7967682225363595}]}, {"text": "It appears that not all relations can be identified in this way, with lexical semantic relations such as synonymy and antonymy being particularly difficult.", "labels": [], "entities": []}, {"text": "The assumption of a single best-fitting candidate answer is also being targeted.", "labels": [], "entities": []}, {"text": "(2) was refuted when demonstrated that some relations missed by 3CosAdd could be recovered with a supervised method, and therefore the information was present in the VSM -just not recoverable with 3CosAdd.", "labels": [], "entities": []}, {"text": "Let us consider why both (1) and (2) failed.", "labels": [], "entities": []}, {"text": "3 What Does 3CosAdd Really Do?", "labels": [], "entities": []}, {"text": "The analogy task continues to enjoy immense popularity in the NLP community as the standard evaluation task for VSMs.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 112, "end_pos": 116, "type": "TASK", "confidence": 0.8675360679626465}]}, {"text": "We have already mentioned two problems with the task: the problem of the Google test scores being flattering to the VSMs (, and also 3CosAdd disadvantaging them, because the required semantic information maybe encoded in more complex ways.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.8736250400543213}]}, {"text": "What the present work adds to the discussion is the demonstration of how strongly the accuracy on the analogy task depends on the target vector being relatively close to the source in the vector space model -not only for 3CosAdd, but also 3CosMul and LRCos.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9994906187057495}, {"text": "LRCos", "start_pos": 251, "end_pos": 256, "type": "DATASET", "confidence": 0.8966609239578247}]}, {"text": "This is in fact a fundamental problem that is encountered in many other NLP tasks . That problem brings about the following question: what have we been evaluating with 3CosAdd all this time?", "labels": [], "entities": []}, {"text": "The answer seems to be this: analogy task scores indicate to what extent the semantic space of a given VSM was structured in away that, for each word category, favored the linguistic relation that happened to be picked by the creators of the particular test dataset.", "labels": [], "entities": []}, {"text": "BATS makes this clearer, because it is well balanced across different types of relations.", "labels": [], "entities": [{"text": "BATS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7147765755653381}]}, {"text": "Most models score well on morphological inflections -because morphological forms of the same word are highly distributionally similar and are likely to be close.", "labels": [], "entities": []}, {"text": "But we do not see equal success for synonyms, suffixes, colors and other categories -because it is hard to expect of anyone model to \"guess\" which words should have synonyms as closest neighbors and which words should be close to their antonyms.", "labels": [], "entities": []}, {"text": "As a matter of fact, fora general-purpose VSM we would not want that: every word can participate in hundreds of linguistic relations that we maybe interested in, but we cannot expect them all to be close neighbors.", "labels": [], "entities": []}, {"text": "We would want a VSM whose vector neighborhoods simply reflect whatever distributional properties were observed in a corpus.", "labels": [], "entities": []}, {"text": "The challenge is to find reasoning methods that could reliably identify linguistic relations from vectors at any distance.", "labels": [], "entities": []}, {"text": "Given the irregularities discussed in section 5, these methods would also have to rely on a more linguistically and cognitively realistic model of how meanings are reflected in distributional properties of words.", "labels": [], "entities": []}, {"text": "LRCos made a step in the right direction, as it does not rely on unique and neatly aligned word pairs, but it can only work for relations between coherent word classes.", "labels": [], "entities": [{"text": "LRCos", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.905643880367279}]}, {"text": "That excludes many lexicographic relations like synonyms (car is to automobile as snake is to serpent), frame-semantic or encyclopedic relations (white is to snow as red is to rose).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: 3CosAdd: effect of various a : a vector  pairs with the same b : b pair ( \u2212 \u2212\u2212\u2212 \u2192  marry: \u2212 \u2212\u2212\u2212\u2212\u2212 \u2192  remarry)", "labels": [], "entities": []}]}