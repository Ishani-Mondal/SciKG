{"title": [{"text": "Use convolutional neural network to evaluate Semantic Textual Similarity", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.6844737728436788}]}], "abstractContent": [{"text": "This paper describes our convolutional neural network (CNN) system for the Semantic Textual Similarity (STS) task.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task", "start_pos": 75, "end_pos": 113, "type": "TASK", "confidence": 0.7646648798670087}]}, {"text": "We calculated semantic similarity score between two sentences by comparing their semantic vectors.", "labels": [], "entities": [{"text": "semantic similarity score", "start_pos": 14, "end_pos": 39, "type": "METRIC", "confidence": 0.6062055627504984}]}, {"text": "We generated a semantic vector by max pooling over every dimension of all word vectors in a sentence.", "labels": [], "entities": []}, {"text": "There are two key design tricks used by our system.", "labels": [], "entities": []}, {"text": "One is that we trained a CNN to transfer GloVe word vectors to a more proper form for the STS task before pooling.", "labels": [], "entities": [{"text": "STS task", "start_pos": 90, "end_pos": 98, "type": "TASK", "confidence": 0.8518992960453033}]}, {"text": "Another is that we trained a fully-connected neural network (FCNN) to transfer the difference of two semantic vectors to the probability distribution over similarity scores.", "labels": [], "entities": []}, {"text": "All hyperparame-ters were empirically tuned.", "labels": [], "entities": []}, {"text": "In spite of the simplicity of our neural network system, we achieved a good accuracy and ranked 3rd on primary track of SemEval 2017.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9995512366294861}]}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) is the task of determining the degree of semantic similarity between two sentences.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8141267249981562}]}, {"text": "STS task is a building block of many natural language processing (NLP) applications.", "labels": [], "entities": [{"text": "STS task", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.8230945467948914}]}, {"text": "Therefore, it has received a significant amount of attention in recent years.", "labels": [], "entities": []}, {"text": "STS tasks in SemEval have been held from 2012 to.", "labels": [], "entities": []}, {"text": "Successfully estimating the degree of semantic similarity between two sentences requires a very deep understanding of both sentences.", "labels": [], "entities": []}, {"text": "Well performing STS methods can be applied to many other natural language understanding tasks including paraphrasing, entailment detection, answer selection, hypothesis evidencing, machine translation (MT) evaluation and quality estimation, summarization, question answering (QA) and short answer grading.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.6724070906639099}, {"text": "entailment detection", "start_pos": 118, "end_pos": 138, "type": "TASK", "confidence": 0.78045254945755}, {"text": "answer selection", "start_pos": 140, "end_pos": 156, "type": "TASK", "confidence": 0.8841083347797394}, {"text": "machine translation (MT) evaluation", "start_pos": 181, "end_pos": 216, "type": "TASK", "confidence": 0.8514753580093384}, {"text": "summarization", "start_pos": 241, "end_pos": 254, "type": "TASK", "confidence": 0.9902826547622681}, {"text": "question answering (QA)", "start_pos": 256, "end_pos": 279, "type": "TASK", "confidence": 0.8418945908546448}, {"text": "short answer grading", "start_pos": 284, "end_pos": 304, "type": "TASK", "confidence": 0.5895440777142843}]}, {"text": "Measuring sentence similarity is challenging for two reasons.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7076010555028915}]}, {"text": "One is the variability of linguistic expression and the other is the limited amount of annotated training data.", "labels": [], "entities": []}, {"text": "Therefore, conventional NLP approaches, such as sparse, hand-crafted features are difficult to use.", "labels": [], "entities": []}, {"text": "However, neural network systems ( can alleviate data sparseness with pre-training and distributed representations.", "labels": [], "entities": []}, {"text": "We propose a convolutional neural network system with 5 components: 1) Enhance GloVe word vectors by adding handcrafted features.", "labels": [], "entities": []}, {"text": "2) Transfer the enhanced word vectors to a more proper form by a convolutional neural network.", "labels": [], "entities": []}, {"text": "3) Max pooling over every dimension of all word vectors to generate semantic vector.", "labels": [], "entities": []}, {"text": "4) Generate semantic difference vector by concatenating the element-wise absolute difference and the element-wise multiplication of two semantic vectors.", "labels": [], "entities": []}, {"text": "5) Transfer the semantic difference vector to the probability distribution over similarity scores by fully-connected neural network.", "labels": [], "entities": []}], "datasetContent": [{"text": "We randomly split all dataset files of into ten.", "labels": [], "entities": []}, {"text": "We used the preparation of the data from (.", "labels": [], "entities": []}, {"text": "We used 90% of the pairs in each individual dataset file for training and the other 10% for validation.", "labels": [], "entities": []}, {"text": "We tested our model in the English dataset of.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.8555971384048462}]}, {"text": "Our objective function is the Pearson correlation coefficient computed over each batch.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 30, "end_pos": 61, "type": "METRIC", "confidence": 0.9243844747543335}]}, {"text": "ADAM was used as the gradient descent optimization method.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6748831868171692}, {"text": "gradient descent optimization", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.7672811845938364}]}, {"text": "All parameters are set to the values We also used the same model design to take part in all tracks of SemEval-2017.", "labels": [], "entities": []}, {"text": "One with machine translation (MT) and another without (non-MT).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.6743093430995941}]}, {"text": "In MT run, we translated all the other languages in the test dataset into English by Google Translate and used the English model to evaluate all similarity scores.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9453244805335999}, {"text": "similarity", "start_pos": 145, "end_pos": 155, "type": "METRIC", "confidence": 0.9513680934906006}]}, {"text": "For the monolingual tracks, we also tried non-MT run, which means we trained the models directly from the English, Spanish and Arabic data.", "labels": [], "entities": []}, {"text": "Here, we independently trained another English model for each run.", "labels": [], "entities": []}, {"text": "The difference between English-English performance from MT and non-MT is caused by the random shuffling of data during training.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9585682153701782}]}, {"text": "We also trained another English model with same design to evaluate the STS benchmark dataset ( . We used only the Train part for training and the Dev.", "labels": [], "entities": [{"text": "STS benchmark dataset", "start_pos": 71, "end_pos": 92, "type": "DATASET", "confidence": 0.8506453235944113}]}, {"text": "We also run our system without any hand-crafted features.", "labels": [], "entities": []}, {"text": "The purely sentence representation system also got a good accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9972655773162842}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Our model achieves 4 th place on the STS benchmark 5 .", "labels": [], "entities": [{"text": "STS benchmark", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.6988472938537598}]}], "tableCaptions": [{"text": " Table 2: Pearson correlation coefficient with the  golden standard of 2017 test dataset", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.8967934846878052}, {"text": "2017 test dataset", "start_pos": 71, "end_pos": 88, "type": "DATASET", "confidence": 0.6395583152770996}]}, {"text": " Table 2. Our model achieves 4 th place on the STS  benchmark 5 .", "labels": [], "entities": [{"text": "STS  benchmark", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.7016099244356155}]}]}