{"title": [{"text": "UINSUSKA-TiTech at SemEval-2017 Task 3: Exploiting Word Importance Levels for Similarity Features for CQA", "labels": [], "entities": [{"text": "CQA", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.5429943799972534}]}], "abstractContent": [{"text": "The majority of core techniques to solve many problems in Community Question Answering (CQA) task rely on similarity computation.", "labels": [], "entities": [{"text": "Community Question Answering (CQA) task", "start_pos": 58, "end_pos": 97, "type": "TASK", "confidence": 0.7658727296761104}]}, {"text": "This work focuses on similarity between two sentences (or questions in subtask B) based on word embeddings.", "labels": [], "entities": []}, {"text": "We exploit words importance levels in sentences or questions for similarity features , for classification and ranking with machine learning.", "labels": [], "entities": []}, {"text": "Using only 2 types of similarity metric, our proposed method has shown comparable results with other complex systems.", "labels": [], "entities": []}, {"text": "This method on subtask B 2017 dataset is ranked on position 7 out of 13 participants.", "labels": [], "entities": [{"text": "subtask B 2017 dataset", "start_pos": 15, "end_pos": 37, "type": "DATASET", "confidence": 0.6340255439281464}]}, {"text": "Evaluation on 2016 da-taset is on position 8 of 12, outperforms some complex systems.", "labels": [], "entities": []}, {"text": "Further, this finding is explorable and potential to be used as baseline and extensible for many tasks in CQA and other textual similarity based system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Community Question Answering (CQA) is getting popular for requesting valid information from experienced people.", "labels": [], "entities": [{"text": "Community Question Answering (CQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7515763690074285}]}, {"text": "However, waiting for such favorable answers fora new submitted question, is a boring task for users once querying to online community forums.", "labels": [], "entities": []}, {"text": "IR system can utilize thread in online community forum for question queries.", "labels": [], "entities": [{"text": "IR", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.511039674282074}]}, {"text": "Even so, the appropriate answers are often mixed among snippets of many irrelevant documents, and opening full articles is still required.", "labels": [], "entities": []}, {"text": "A post-processing system is needed in order to obtain the most relevant answers.", "labels": [], "entities": []}, {"text": "CQA tasks want to address this need, to help user get the most favorable answers by improving IR system results.", "labels": [], "entities": [{"text": "IR", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.9656131863594055}]}, {"text": "SemEval CQA Task 3 is designed to gather some possible solutions, in five coherent subtasks.", "labels": [], "entities": [{"text": "SemEval CQA Task 3", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.6536377519369125}]}, {"text": "Since some subtasks are related, we focus only on subtask B, with goal to provide a good basis framework for solving problem in other subtasks.", "labels": [], "entities": []}, {"text": "In of the previous year, word embeddings obtained with a tool such as word2vec () contributed to the best systems for all subtasks.", "labels": [], "entities": []}, {"text": "In addition, machine learning based methods were mostly ranked in the top positions for all subtasks.", "labels": [], "entities": []}, {"text": "The most popular machine learning approach was SVM for classification, regression and ranking, while neural networks, even though widely used, did not win any subtasks ( . Most machine learning approaches rely on several similarity features as the basis.", "labels": [], "entities": []}, {"text": "Various techniques to compute semantic similarity based on word embeddings, were used by,,,, and . Besides, they also used various lexical and semantic similarities including simple match counts on words or n-grams.", "labels": [], "entities": []}, {"text": "Specifically,, also used nouns and n-grams overlaps, distributed word alignments, knowledge graphs, and common frame.", "labels": [], "entities": [{"text": "distributed word alignments", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.6083938876787821}]}, {"text": "Interestingly, used cosine distance between topic pairs, and text distance for SVM learning features, rather than using similarity features.", "labels": [], "entities": []}, {"text": "They also implemented other Boolean and Qatar Living Forum users as task specific features.", "labels": [], "entities": [{"text": "Boolean and Qatar Living Forum users", "start_pos": 28, "end_pos": 64, "type": "DATASET", "confidence": 0.8475164075692495}]}, {"text": "constructed many types of similarity based on text pairs, e.g. n-grams of word lemmas, n-grams of POS tags, parse tree, and LCS for SVM learning features.", "labels": [], "entities": []}, {"text": "Then they stack the classifiers across subtasks to solve substasks B and C in such away that utilizes other subtasks' results.", "labels": [], "entities": []}, {"text": "This task-specific features seem to be the key success for the team to get the relatively best performance on all English subtasks.", "labels": [], "entities": []}, {"text": "In this CQA task, we focus on machine learning approaches with a small number of features.", "labels": [], "entities": []}, {"text": "We attempt to find an effective way to use word embeddings as the basis of our similarity features.", "labels": [], "entities": []}, {"text": "We also make use of the words (lemmas) that are frequent in a thread or small document collection (i.e. the original and the 10 related questions), in the calculation of similarity between sentences.", "labels": [], "entities": []}, {"text": "We create several sets of words with different 'word importance levels', from which we derive similarity features for machine learning methods.", "labels": [], "entities": []}, {"text": "The experiment on this 2017 shared task (subtask B) shows good results with respect to MAP scores.", "labels": [], "entities": [{"text": "MAP", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.879600465297699}]}, {"text": "Our method also surpasses IR baseline and achieved the 7 th position out of 13 teams for the primary submission.", "labels": [], "entities": [{"text": "IR baseline", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.8131665289402008}]}], "datasetContent": [{"text": "We use 2016 Task 3 datasets provided by the organizer 3 , i.e. TRAIN-part1, DEV and TEST.", "labels": [], "entities": [{"text": "2016 Task 3 datasets", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.625780425965786}, {"text": "TRAIN-part1", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.7345592379570007}, {"text": "DEV", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.8247873187065125}, {"text": "TEST", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.46518048644065857}]}, {"text": "We do not use TRAIN-part2 for it is less reliable and contains more noise as informed in the readmefile.", "labels": [], "entities": [{"text": "TRAIN-part2", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.975019633769989}]}, {"text": "We also conduct experiments on TEST-2016 dataset to test our system performance and compare it with the published official scores in  as seen in.", "labels": [], "entities": [{"text": "TEST-2016 dataset", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.9727266728878021}]}], "tableCaptions": [{"text": " Table 2: MAP Scores on DEV", "labels": [], "entities": [{"text": "MAP Scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8102537989616394}, {"text": "DEV", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.8683632016181946}]}, {"text": " Table 3: Final Result on Task B", "labels": [], "entities": []}, {"text": " Table 4: Experiment on SemEval 2016 subtask B", "labels": [], "entities": [{"text": "SemEval 2016 subtask", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.777829090754191}]}]}