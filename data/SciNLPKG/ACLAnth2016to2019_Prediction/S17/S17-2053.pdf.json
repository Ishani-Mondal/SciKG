{"title": [{"text": "KeLP at SemEval-2017 Task 3: Learning Pairwise Patterns in Community Question Answering", "labels": [], "entities": [{"text": "KeLP at SemEval-2017 Task 3", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.711066722869873}, {"text": "Community Question Answering", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.6101483106613159}]}], "abstractContent": [{"text": "This paper describes the KeLP system participating in the SemEval-2017 community Question Answering (cQA) task.", "labels": [], "entities": [{"text": "SemEval-2017 community Question Answering (cQA) task", "start_pos": 58, "end_pos": 110, "type": "TASK", "confidence": 0.8444875478744507}]}, {"text": "The system is a refinement of the kernel-based sentence pair modeling we proposed for the previous year challenge.", "labels": [], "entities": []}, {"text": "It is implemented within the Kernel-based Learning Platform called KeLP, from which we inherit the team's name.", "labels": [], "entities": [{"text": "KeLP", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.799111545085907}]}, {"text": "Our primary submission ranked first in subtask A, and third in subtasks B and C, being the only systems appearing in the top-3 ranking for all the English subtasks.", "labels": [], "entities": []}, {"text": "This shows that the proposed framework, which has minor variations among the three subtasks, is extremely flexible and effective in tackling learning tasks defined on sentence pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the KeLP system participating in the SemEval-2017 cQA challenge ().", "labels": [], "entities": [{"text": "SemEval-2017 cQA challenge", "start_pos": 58, "end_pos": 84, "type": "DATASET", "confidence": 0.665224552154541}]}, {"text": "The task setting for the English part is the same as the previous edition: the corpus is extracted from Qatar Living 1 , a web forum where people pose questions about multiple aspects of their daily life in Qatar, and three subtasks are defined: Subtask A: Given a question q and its first 10 comments c 1 , . .", "labels": [], "entities": [{"text": "Qatar Living 1", "start_pos": 104, "end_pos": 118, "type": "DATASET", "confidence": 0.9039394656817118}]}, {"text": ", c 10 in its question thread, rerank these 10 comments according to their relevance with respect to the question, i.e., the good comments have to be ranked above potential or bad comments.", "labels": [], "entities": []}, {"text": "Subtask B: Given anew question o and the set of the first 10 related questions q 1 , . .", "labels": [], "entities": []}, {"text": ", q 10 (retrieved by a search engine), re-rank the related questions according to their similarity with respect http://www.qatarliving.com/forum too, i.e., the perfect match and relevant questions should be ranked above the irrelevant ones.", "labels": [], "entities": []}, {"text": "Subtask C: Given anew question o, and the set of the first 10 related questions, q 1 , . .", "labels": [], "entities": []}, {"text": ", q 10 , (retrieved by a search engine), each one associated with its first 10 comments, c q 1 , . .", "labels": [], "entities": []}, {"text": ", c q 10 , appearing in its thread, re-rank the 100 comments according to their relevance with respect too, i.e., the good comments are to be ranked above potential or bad comments.", "labels": [], "entities": []}, {"text": "We participated to the previous year edition, where our system () achieved very good results, i.e., first in subtask A, third in B and second in C.", "labels": [], "entities": [{"text": "B", "start_pos": 129, "end_pos": 130, "type": "METRIC", "confidence": 0.9724783301353455}]}, {"text": "For the new year challenge, we therefore decided to reuse the same system applied to anew method for selecting tree structures, ) summarized in Sec.", "labels": [], "entities": []}, {"text": "3. We modeled the three subtasks as binary classification problems: kernel-based classifiers are trained and the classification score is used to sort the instances and produce the final ranking.", "labels": [], "entities": []}, {"text": "We implemented models within the Kernel-based Learning Platform 2 (KeLP) (, which determined the team's name.", "labels": [], "entities": []}, {"text": "Our tests provide two main contributions: (i) we asses the results obtained in (, demonstrating that our kernel-based models for relational learning tasks between two texts (Filice et al., 2015b) are effective for community Question Answering.", "labels": [], "entities": [{"text": "community Question Answering", "start_pos": 214, "end_pos": 242, "type": "TASK", "confidence": 0.6054712335268656}]}, {"text": "(ii) We studied the impact of text selection described in . Our primary submission ranked first in subtask A, and third in subtasks B and C, demonstrating that the proposed method is very accurate and adaptable to different learning problems.", "labels": [], "entities": [{"text": "text selection", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.7902029752731323}]}, {"text": "At the moment, we could not find out if text selection is always useful as our contrastive submission not using it turned out to be much more accurate for Task B. In the reminder, Section 2 introduces the proposed kernel-based system, Section 3 describes the pruning technique to select the relevant parts from the input sentences, while Section 4 reports official results.", "labels": [], "entities": [{"text": "text selection", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.7011213898658752}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on subtask A on the 2016 and  2017 official testsets. IR is the baseline system  based on the search engine results.", "labels": [], "entities": [{"text": "IR", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.8240048289299011}]}, {"text": " Table 2: Results on subtask B on the 2016 and  2017 official test sets. KeLP is our primary sub- mission, while KC1 is the contrastive one. IR is  the baseline system based on the search engine re- sults.", "labels": [], "entities": []}, {"text": " Table 3: Results on subtask C on the 2016 and  2017 official test sets. KeLP is our primary sub- mission, while IR is the baseline system based on  the search engine results.", "labels": [], "entities": [{"text": "KeLP", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.751541793346405}, {"text": "IR", "start_pos": 113, "end_pos": 115, "type": "METRIC", "confidence": 0.8590022325515747}]}]}