{"title": [{"text": "NLM NIH at SemEval-2017 Task 3: from Question Entailment to Question Similarity for Community Question Answering", "labels": [], "entities": [{"text": "NLM NIH at SemEval-2017 Task", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.8234779715538025}, {"text": "Question Entailment", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7148172706365585}, {"text": "Question Similarity", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7487202286720276}, {"text": "Community Question Answering", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.6308690508206686}]}], "abstractContent": [{"text": "This paper describes our participation in SemEval-2017 Task 3 on Community Question Answering (cQA).", "labels": [], "entities": [{"text": "SemEval-2017 Task 3 on Community Question Answering (cQA)", "start_pos": 42, "end_pos": 99, "type": "TASK", "confidence": 0.8563768982887268}]}, {"text": "The Question Similarity subtask (B) aims to rank a set of related questions retrieved by a search engine according to their similarity to the original question.", "labels": [], "entities": [{"text": "Question Similarity subtask (B)", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.6575512091318766}]}, {"text": "We adapted our feature-based system for Recognizing Question Entailment (RQE) to the question similarity task.", "labels": [], "entities": [{"text": "Recognizing Question Entailment (RQE)", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.805494358142217}]}, {"text": "Tested on cQA-B-2016 test data, our RQE system outperformed the best system of the 2016 challenge in all measures with 77.47 MAP and 80.57 Accuracy.", "labels": [], "entities": [{"text": "cQA-B-2016 test data", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.9810235699017843}, {"text": "MAP", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.9975601434707642}, {"text": "Accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9912829399108887}]}, {"text": "On cQA-B-2017 test data, performances of all systems dropped by around 30 points.", "labels": [], "entities": [{"text": "cQA-B-2017 test data", "start_pos": 3, "end_pos": 23, "type": "DATASET", "confidence": 0.9811957677205404}]}, {"text": "Our primary system obtained 44.62 MAP, 67.27 Accuracy and 47.25 F1 score.", "labels": [], "entities": [{"text": "MAP", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9992560744285583}, {"text": "Accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9990943670272827}, {"text": "F1 score", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9805535972118378}]}, {"text": "The cQA-B-2017 best system achieved 47.22 MAP and 42.37 F1 score.", "labels": [], "entities": [{"text": "cQA-B-2017", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8902035355567932}, {"text": "MAP", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9989128112792969}, {"text": "F1 score", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9827187657356262}]}, {"text": "Our system is ranked sixth in terms of MAP and third in terms of F1 out of 13 participating teams.", "labels": [], "entities": [{"text": "MAP", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.850718080997467}, {"text": "F1", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9986603260040283}]}], "introductionContent": [{"text": "SemEval-2017 Task 3 1 on Community Question Answering (cQA) focuses on answering new questions by retrieving related answered questions in community forums (.", "labels": [], "entities": [{"text": "Community Question Answering (cQA)", "start_pos": 25, "end_pos": 59, "type": "TASK", "confidence": 0.6689157585302988}]}, {"text": "This task extends the previous cQA tasks.", "labels": [], "entities": []}, {"text": "This year, five subtasks were proposed: English Question-Comment Similarity (subtask A), English Question-Question Similarity (subtask B), English Question-External Comment Similarity (subtask C), Arabic Answer Re-rank (subtask D) and English Multi-Domain Duplicate Question Detection (subtask E).", "labels": [], "entities": [{"text": "English Question-Comment Similarity", "start_pos": 40, "end_pos": 75, "type": "TASK", "confidence": 0.5490949948628744}, {"text": "English Question-External Comment Similarity", "start_pos": 139, "end_pos": 183, "type": "TASK", "confidence": 0.498804472386837}, {"text": "English Multi-Domain Duplicate Question Detection", "start_pos": 235, "end_pos": 284, "type": "TASK", "confidence": 0.5176636099815368}]}, {"text": "1 http://alt.qcri.org/semeval2017/task3 Subtask B (Question Similarity) aims to re-rank a set of similar questions retrieved by a search engine with respect to the original question, with the idea that the answers to the similar questions should also be answers to the new question.", "labels": [], "entities": []}, {"text": "For a given question, a set often similar questions is provided for re-ranking.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: cQA-B-2017 Official Results (Nakov et al., 2017)", "labels": [], "entities": [{"text": "cQA-B-2017", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.8401297330856323}, {"text": "Official", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.6640345454216003}]}, {"text": " Table 2: Results on cQA-B-2016-Test data. RQE system trained on cQA-B-2016 training and develop- ment datasets (3,169 pairs)", "labels": [], "entities": [{"text": "cQA-B-2016-Test data", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.976490318775177}]}]}