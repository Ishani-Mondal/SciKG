{"title": [{"text": "FCICU at SemEval-2017 Task 1: Sense-Based Language Independent Semantic Textual Similarity Approach", "labels": [], "entities": [{"text": "FCICU", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.808998167514801}, {"text": "Sense-Based Language Independent Semantic Textual Similarity Approach", "start_pos": 30, "end_pos": 99, "type": "TASK", "confidence": 0.5399887093475887}]}], "abstractContent": [{"text": "This paper describes FCICU team systems that participated in SemEval-2017 Semantic Textual Similarity task (Task1) for monolingual and cross-lingual sentence pairs.", "labels": [], "entities": [{"text": "SemEval-2017 Semantic Textual Similarity task (Task1)", "start_pos": 61, "end_pos": 114, "type": "TASK", "confidence": 0.8366335183382034}]}, {"text": "A sense-based language independent textual similarity approach is presented, in which a proposed alignment similarity method coupled with new usage of a semantic network (BabelNet) is used.", "labels": [], "entities": []}, {"text": "Additionally , a previously proposed integration between sense-based and surface-based semantic textual similarity approach is applied together with our proposed approach.", "labels": [], "entities": []}, {"text": "For all the tracks in Task1, Run1 is a string kernel with alignments metric and Run2 is a sense-based alignment similarity method.", "labels": [], "entities": []}, {"text": "The first run is ranked 10th, and the second is ranked 12th in the primary track, with correlation 0.619 and 0.617 respectively .", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) is the task of measuring the similarity between two short texts semantically.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7973850270112356}]}, {"text": "STS is very important because a wide range of Natural Language Processing (NLP) applications rely heavily on such task.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9630460739135742}]}, {"text": "This paper describes our participation in the STS task (Task1) at SemEval 2017 in all the six monolingual and cross-lingual tracks.", "labels": [], "entities": [{"text": "STS task (Task1) at SemEval 2017", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.7209297865629196}]}, {"text": "The STS task seeks to calculate a graded similarity score from 0 to 5 between two sentences according to their meaning, i.e. semantically.", "labels": [], "entities": [{"text": "graded similarity score", "start_pos": 34, "end_pos": 57, "type": "METRIC", "confidence": 0.6705643932024637}]}, {"text": "The monolingual tracks are Arabic, English, and Spanish sentence-pairs (track1, track3, and track5 respectively), while the cross-lingual tracks are Arabic, Spanish, and Turkish sentences paired with English sentences (track2, track4a-4b, and track6 respectively).", "labels": [], "entities": []}, {"text": "An additional Primary track is provided that presents the mean score of the results of all the other tracks.", "labels": [], "entities": [{"text": "mean score", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.926406741142273}]}, {"text": "The similarity between two natural language sentences can be inferred from the quantity/quality of aligned constituents in both sentences.", "labels": [], "entities": []}, {"text": "Such alignments provide valuable information regarding how and to what extent the two sentences are related or semantically similar, where semantically equivalent text pairs are likely to have a successful alignment between their words.", "labels": [], "entities": []}, {"text": "Our proposed sense-based approach employs this aspect to calculate the similarity between sentence-pairs regardless of their language.", "labels": [], "entities": []}, {"text": "This is achieved through a proposed word-sense aligner that relies mainly on anew usage of the semantic network BabelNet.", "labels": [], "entities": []}, {"text": "BabelNet utilization compensates the need of a machine translation module that is most commonly used to transfer crosslingual STS to monolingual.", "labels": [], "entities": []}, {"text": "Besides, the proposed sense-based similarity score is combined with a surface-based similarity score.", "labels": [], "entities": [{"text": "sense-based similarity score", "start_pos": 22, "end_pos": 50, "type": "METRIC", "confidence": 0.6616861522197723}, {"text": "surface-based similarity score", "start_pos": 70, "end_pos": 100, "type": "METRIC", "confidence": 0.6705814997355143}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 explains our main multilingual sense-based aligner.", "labels": [], "entities": []}, {"text": "Section 3 describes our system that participated in all tracks.", "labels": [], "entities": []}, {"text": "Section 4 shows the experiments conducted and analyzes the results achieved.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper and mentions some future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main evaluation measure selected by the task organizers was the Pearson correlation between the system scores and the gold standard scores.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 68, "end_pos": 87, "type": "METRIC", "confidence": 0.9590829610824585}]}, {"text": "presents the official results of our submissions in SemEval2017-Task1 for both Run1 and Run2 in the six tracks as well as the primary track.", "labels": [], "entities": [{"text": "Run1", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.9067980051040649}, {"text": "Run2", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.7896785140037537}]}, {"text": "The best performing score obtained in each track is included as well alongside with the baseline system results announced by the task organizers.", "labels": [], "entities": []}, {"text": "Our best system (Run1) achieved 0.619 correlation and ranked the 10 th run and the 5 th team out of 84 runs and 31 teams respectively.", "labels": [], "entities": [{"text": "correlation", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.9924237728118896}]}, {"text": "Although the performance of the two Runs differs slightly, it is noticeable from the table that Run1 (Kernel) performs better with cross-lingual sentence-pairs, while Run2 (Alignments) performs better with monolingual sentence-pairs.", "labels": [], "entities": []}, {"text": "Hence, relying on aligned tokens only in crosslingual sentences is insufficient.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: System performance on SemEval-2107 da- tasets.", "labels": [], "entities": []}]}