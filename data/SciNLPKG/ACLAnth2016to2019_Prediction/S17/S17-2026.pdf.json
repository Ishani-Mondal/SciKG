{"title": [{"text": "UMDeep at SemEval-2017 Task 1: End-to-End Shared Weight LSTM Model for Semantic Textual Similarity", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a modified shared-LSTM network for the Semantic Textual Similarity (STS) task at SemEval-2017.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task at SemEval-2017", "start_pos": 51, "end_pos": 105, "type": "TASK", "confidence": 0.7944744030634562}]}, {"text": "The network builds on previously explored Siamese network architectures.", "labels": [], "entities": []}, {"text": "We treat max sentence length as an additional hy-perparameter to be tuned (beyond learning rate, regularization, and dropout).", "labels": [], "entities": []}, {"text": "Our results demonstrate that hand-tuning max sentence training length significantly improves final accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9750657677650452}]}, {"text": "After optimizing hyperparameters, we train the network on the multilingual semantic similarity task using pre-translated sentences.", "labels": [], "entities": []}, {"text": "We achieved a correlation of 0.4792 for all the subtasks.", "labels": [], "entities": [{"text": "correlation", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.98409503698349}]}, {"text": "We achieved the fourth highest team correlation for Task 4b, which was our best relative placement.", "labels": [], "entities": [{"text": "correlation", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.5168020725250244}]}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) has been a staple of the SemEval competition and requires systems that automatically identify the semantic relatedness of two sentences.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.819013794263204}]}, {"text": "The resulting system could be used down-stream in many important NLP tasks, such as scoring the output of a machine translation system or finding related document/query pairs in web search.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.6801411807537079}]}, {"text": "The data available for this competition has been updated annually and contains gold-label, humanevaluated scores based on sentence pairs across multiple languages,,,).", "labels": [], "entities": []}, {"text": "The gold label for each sentence pair is in the range, with 0 being the sentences are completely dissimilar to 5 being the sentences are completely equivalent.", "labels": [], "entities": []}, {"text": "(Agirre et al., 2016) The task is not restricted to English or monolingual similarity scoring.", "labels": [], "entities": []}, {"text": "The 2017 SemEval task consists of seven different tracks, each with a different language pair: Arabic-Arabic, Arabic-English, Spanish-Spanish, SpanishEnglish, an additional Spanish-English track, English-English, and English-Turkish.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.8956910073757172}]}, {"text": "We avoid language-specific feature engineering and take a representation learning approach to STS.", "labels": [], "entities": []}, {"text": "This requires constructing directly-comprable sentence representations that can be induced from the limited amounts of annotated STS training data.", "labels": [], "entities": []}, {"text": "We present a modified version of the Siamese Long Short-Term Memory (LSTM) network to solve this problem.", "labels": [], "entities": []}, {"text": "A Siamese network is one in which parameters between layers are shared, and are updated in parallel during the learning phase.", "labels": [], "entities": []}, {"text": "For the semantic relatedness task, this allows two sentences to be encoded into the same space using a single shared recurrent neural network.", "labels": [], "entities": []}, {"text": "The dual-encoding enables the use of end-to-end supervised deep learning, using only the surface forms of the sentences and the gold labels.", "labels": [], "entities": []}, {"text": "We extend the Siamese LSTM in two ways.", "labels": [], "entities": [{"text": "Siamese LSTM", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.6425836980342865}]}, {"text": "First, we consider the semantic relatedness as a classification, rather than a regression problem.", "labels": [], "entities": []}, {"text": "Initially, semantic relatedness appears to be a continuous one-dimensional measure suitable for regression.", "labels": [], "entities": [{"text": "semantic relatedness", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.7171583771705627}]}, {"text": "However, there are many subtleties within the bands of scores, as sentences can differ along more than a single dimension.", "labels": [], "entities": []}, {"text": "Thus, rather than regressing over the label, our model generates a distribution over possible labels.", "labels": [], "entities": []}, {"text": "Second, we use a different concatenative dense layer on top of the dual LSTMs to better model the classification problem, and train using KL-Divergence as the loss function for training.", "labels": [], "entities": [{"text": "classification problem", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.9147873818874359}]}, {"text": "Our results did not achieve the state-of-the-art performance possible with a Siamese LSTM ar-chitecture.", "labels": [], "entities": []}, {"text": "Despite this set-back, we are able to demonstrate the effect of sentence training length on a LSTM.", "labels": [], "entities": []}, {"text": "Additionally, all foreign languages were translated through Google Translate and the same model was used for the seven tracks.", "labels": [], "entities": []}, {"text": "This standardization provides insight into the quality of Google Translate and the negative effect of machine translation on correlation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7097818106412888}]}, {"text": "The following section provides detail on our system and the training process.", "labels": [], "entities": []}, {"text": "As our submission is focused on the use of end-to-end deep learning in semantic relatedness, we do not use hand-crafted features from external data, except for pre-trained word embeddings to speedup training.", "labels": [], "entities": [{"text": "semantic relatedness", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.7451445758342743}]}, {"text": "A visual overview of the shared LSTM model can be seen in.", "labels": [], "entities": []}], "datasetContent": [{"text": "We opted for minimal preprocessing in our final model: merely tokenizing and lower-casing the input.", "labels": [], "entities": []}, {"text": "Lemmatizing the words did not lead to a notable improvement, and hence was omitted.", "labels": [], "entities": []}, {"text": "Additionally, experimenting with targeted Part of Speech exclusion (removing all articles, increasing weight of proper nouns, etc.) did not produce dramatically higher results.", "labels": [], "entities": [{"text": "Part of Speech exclusion", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.9039303064346313}]}, {"text": "Therefore, we decided to let the LSTM learn for itself.", "labels": [], "entities": []}, {"text": "Our final results on the 2017 data are shown in.", "labels": [], "entities": []}, {"text": "Retrospectively, we saw that the 2016 postediting data (65.13% accuracy when 2016 data was held out from training) would have served as a close proxy for 2017 En-En performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9936516284942627}]}, {"text": "Our three submissions to the 2017 Semeval task were trained treating maximum training sentence length as a hyperparameter.", "labels": [], "entities": [{"text": "Semeval task", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.8610304296016693}]}, {"text": "Our results show that this parameter can have a large impact on the final outcome of the model.", "labels": [], "entities": []}, {"text": "The cosine baseline provided by SemEval organizers achieved a 0.72 correlation for the EnglishEnglish sentences, which was roughly 0.07 higher than our best performance on the same dataset.", "labels": [], "entities": []}, {"text": "Although disheartening, a Siamese LSTM model is capable of performing dramatically better with curated training data, whereas the baseline approach cannot be significantly modified.", "labels": [], "entities": []}, {"text": "Our final model used length 300 GloVe embeddings, 100 LSTM cells, 50 neurons in the final dense layer, and 6 output neurons, one for each class.", "labels": [], "entities": []}, {"text": "We used the Kullback-Leibler Divergence of the output distribution and the gold label distribution as the objective function.", "labels": [], "entities": []}, {"text": "We used all available past STS Task 1 datasets and no external data.", "labels": [], "entities": [{"text": "STS Task 1 datasets", "start_pos": 27, "end_pos": 46, "type": "DATASET", "confidence": 0.7546229064464569}]}, {"text": "In order to participate in the non-English tracks, we used Google Translate to translate all the sentence pairs into English.", "labels": [], "entities": []}, {"text": "We then used the model trained on the English-English pairs on the translated-English data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results in the different tracks of SemEval-2017. The lengths refer to the maximum lengths of  the sentences used for training the model.", "labels": [], "entities": []}, {"text": " Table 2: A selection of results showing the successes and failures of our shared-LSTM architecture.  These sentences were selected to show areas in which our system excels or under-performs.", "labels": [], "entities": []}]}