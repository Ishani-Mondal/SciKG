{"title": [{"text": "MI&T Lab at SemEval-2017 task 4: An Integrated Training Method of Word Vector for Sentiment Classification", "labels": [], "entities": [{"text": "SemEval-2017 task", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.5816523134708405}, {"text": "Sentiment Classification", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.9444887936115265}]}], "abstractContent": [{"text": "A CNN method for sentiment classification task in Task 4A of SemEval 2017 is presented.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.9669608175754547}]}, {"text": "To solve the problem of word2vec training word vector slowly, a method of training word vector by integrating word2vec and Convolutional Neu-ral Network (CNN) is proposed.", "labels": [], "entities": []}, {"text": "This training method not only improves the training speed of word2vec, but also makes the word vector more effective for the target task.", "labels": [], "entities": []}, {"text": "Furthermore, the word2vec adopts a full connection between the input layer and the projection layer of the Continuous Bag-of-Words (CBOW) for acquiring the semantic information of the original sentence.", "labels": [], "entities": []}], "introductionContent": [{"text": "The polarity of a Twitter message is classified into positive, negative and neutral in Twitter sentiment analysis.", "labels": [], "entities": []}, {"text": "However, the difficulty of sentiment analysis greatly increases due to the ambiguity and the rhetorical of natural language.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9696607887744904}]}, {"text": "In recent years, the deep learning model has shown great potential in the task of sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.9637371003627777}]}, {"text": "For short text data such as Twitter, Convolutional Neural Network (CNN) model) is the most widely and successfully used, and in the SemEval 2016-task4A competition, the system ranked first also uses CNN model ().", "labels": [], "entities": []}, {"text": "So CNN model is used to complete the task in our system.", "labels": [], "entities": []}, {"text": "The task 4A of SemEval 2017 1 is a polarity classification task which requires participated systems to classify a given Twitter message into positive, negative or neutral (.", "labels": [], "entities": [{"text": "SemEval 2017 1", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.8013106385866801}, {"text": "polarity classification task", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.7923641800880432}]}, {"text": "The system integrates the word2vec and CNN to train the labeled data, generating the word vector of each word in the data.", "labels": [], "entities": []}, {"text": "This method can improve the training speed of word vector.", "labels": [], "entities": [{"text": "word vector", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.738988995552063}]}, {"text": "In order to preserve the more semantic information of the original sentence effectively, the word2vec is fully connected between the input layer and the projection layer of the Continuous Bag-of-Words (CBOW).", "labels": [], "entities": []}], "datasetContent": [{"text": "The datasets of the experiment is provided by SemEval 2017, and the specific datasets used are shown in.", "labels": [], "entities": [{"text": "SemEval 2017", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.8348303735256195}]}, {"text": "The measure metric of the Evaluation is average macro recall ().", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.8013454675674438}]}, {"text": "The formula is as follows: Here, \u03c1 P , \u03c1 U and \u03c1 N denote recall for the positive class, neutral class and negative class.", "labels": [], "entities": [{"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9990598559379578}]}, {"text": "The other two measure metrics are the average macro F 1 and the average macro precision: Here, F P 1 , F U 1 and F N 1 denote F 1 for the positive class, neutral class and negative class; PP , P U and P N denote precision for the positive class, neutral class and negative class.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.8863098621368408}, {"text": "F 1", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.961278647184372}, {"text": "precision", "start_pos": 212, "end_pos": 221, "type": "METRIC", "confidence": 0.997000515460968}]}, {"text": "lists the average macro recall for each model on the development dataset.", "labels": [], "entities": [{"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9422779679298401}]}, {"text": "From the table 3, the effect of word2vec+CNN model is better than SVM, and word2vec + CNN + SVM is the best of the three models, so the best results on the test set are submitted.", "labels": [], "entities": []}, {"text": "System SVM CNN SVM+CNN \u03c1 P UN 0.589 0.601 0.653: The results of different models on the development dataset.", "labels": [], "entities": [{"text": "CNN \u03c1 P UN 0.589 0.601 0.653", "start_pos": 19, "end_pos": 47, "type": "METRIC", "confidence": 0.8878107411520821}]}, {"text": "shows the details of our system's result in comparison with the three top ranked systems' results.", "labels": [], "entities": []}, {"text": "It can be seen from the table that our result's \u03c1 N is not good, but \u03c1 U is better than the top three systems.", "labels": [], "entities": []}, {"text": "The decrease of experimental results is from the quantity bias in training data of different classes.", "labels": [], "entities": []}, {"text": "For deep learning models, a lot of training data are required.", "labels": [], "entities": []}, {"text": "Due to the lack of Twitter texts, word2vec training is not sufficient and do not generate effective words vector representation.", "labels": [], "entities": []}, {"text": "In the future, semi-supervisory mechanisms will be considered to expand the number of training data.", "labels": [], "entities": []}, {"text": "In the future, we can improve the system's performance from following points: (i) to expand the amount of training data; (ii) to improve the type of combination: the results can be combined with multiple CNN systems to predict; (iii) to add more emotional semantic features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Datasets of the experiment.", "labels": [], "entities": []}, {"text": " Table 2: Parameters used in the word2vec and  CNN.", "labels": [], "entities": [{"text": "word2vec", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9262959361076355}, {"text": "CNN", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.6879884004592896}]}, {"text": " Table 4: The details of our result and the three top ranked results.", "labels": [], "entities": []}]}