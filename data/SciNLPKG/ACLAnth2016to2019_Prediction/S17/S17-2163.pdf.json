{"title": [{"text": "Ensemble of nEural Learners for kEyphrase ClassificaTION", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes our approach to the SemEval 2017 Task 10: \"Extracting Keyphrases and Relations from Scientific Publications\", specifically to Subtask (B): \"Classification of identified keyphrases\".", "labels": [], "entities": [{"text": "SemEval 2017 Task 10", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.8717596083879471}, {"text": "Extracting Keyphrases and Relations from Scientific Publications", "start_pos": 64, "end_pos": 128, "type": "TASK", "confidence": 0.8770346896989005}, {"text": "Classification of identified keyphrases", "start_pos": 161, "end_pos": 200, "type": "TASK", "confidence": 0.8746995180845261}]}, {"text": "We explored three different deep learning approaches: a character-level convo-lutional neural network (CNN), a stacked learner with an MLP meta-classifier, and an attention based Bi-LSTM.", "labels": [], "entities": []}, {"text": "From these approaches, we created an ensemble of differently hyper-parameterized systems, achieving a micro-F 1-score of 0.63 on the test data.", "labels": [], "entities": []}, {"text": "Our approach ranks 2nd (score of 1st placed system: 0.64) out of four according to this official score.", "labels": [], "entities": []}, {"text": "However, we erroneously trained 2 out of 3 neural nets (the stacker and the CNN) on only roughly 15% of the full data, namely, the original development set.", "labels": [], "entities": []}, {"text": "When trained on the full data (training+development), our ensemble has a micro-F 1-score of 0.69.", "labels": [], "entities": []}, {"text": "Our code is available from https://github.", "labels": [], "entities": []}, {"text": "com/UKPLab/semeval2017-scienceie.", "labels": [], "entities": [{"text": "UKPLab", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.9843364953994751}]}], "introductionContent": [{"text": "Although scientific experiments are often accompanied by vast amounts of structured data, full-text scientific publications still remain one of the main means for communicating academic knowledge.", "labels": [], "entities": []}, {"text": "Given the dynamic nature of modern research and its ever-accelerating pace, it is crucial to automatically analyze new works in order to have a complete picture of advances in a given field.", "labels": [], "entities": []}, {"text": "Recently, some progress has been made in this direction for the fixed-domain use case . However, creating a universal open-domain system still 1 E.g. BioNLP: http://2016.bionlp-st.org/ remains a challenge due to significant domain differences between articles originating from different fields of research.", "labels": [], "entities": []}, {"text": "The SemEval 2017 Task 10: ScienceIE ( promotes the multi-domain use case, providing source articles from three domains: Computer Science, Material Sciences and Physics.", "labels": [], "entities": [{"text": "SemEval 2017 Task 10", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.6827089637517929}]}, {"text": "The task consists of three subtasks, namely (A) identification of keyphrases, (B) classifying them into broad domain-independent classes and (C) inferring relations between the identified keyphrases.", "labels": [], "entities": []}, {"text": "For example, for the input sentence 'The thermodynamics of copper-zinc alloys (brass) was subject of numerous investigations' the following output would be expected: (A) 1.", "labels": [], "entities": []}, {"text": "The thermodynamics of copper-zinc alloys 2.", "labels": [], "entities": []}, {"text": "brass (B) 1.", "labels": [], "entities": []}, {"text": "Our submission focuses on (B) keyphrase classification given item boundaries.", "labels": [], "entities": [{"text": "keyphrase classification", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.7280595302581787}]}, {"text": "We avoid taskspecific feature engineering, which would potentially render the system domain-dependent.", "labels": [], "entities": []}, {"text": "Instead, we build an ensemble of several deep learning classifiers detailed in \u00a73, whose inputs are word embeddings learned from general domains.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Micro-F1 results in % for our systems.", "labels": [], "entities": []}, {"text": " Table 3: F1 results in % across different classes.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9984332919120789}]}, {"text": " Table 4: Stackers+CNNs+AB-LSTMs confusion matrix.", "labels": [], "entities": [{"text": "AB-LSTMs", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.7997491955757141}]}]}