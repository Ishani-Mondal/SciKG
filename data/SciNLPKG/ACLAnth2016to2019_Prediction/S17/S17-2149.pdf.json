{"title": [{"text": "UW-FinSent at SemEval-2017 Task 5: Sentiment Analysis on Financial News Headlines using Training Dataset Augmentation", "labels": [], "entities": [{"text": "UW-FinSent at SemEval-2017 Task 5", "start_pos": 0, "end_pos": 33, "type": "DATASET", "confidence": 0.7434989094734192}, {"text": "Sentiment Analysis on Financial News Headlines", "start_pos": 35, "end_pos": 81, "type": "TASK", "confidence": 0.8573236664136251}]}], "abstractContent": [{"text": "This paper discusses the approach taken by the UWaterloo team to arrive at a solu", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of this SemEval task is to identify finegrained levels of sentiment polarity in financial news headlines and microblog posts.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.8994181156158447}, {"text": "sentiment polarity in financial news headlines and microblog posts", "start_pos": 67, "end_pos": 133, "type": "TASK", "confidence": 0.7285183270772299}]}, {"text": "Specifically, the task aims at identifying bullish (optimistic) sentiment, expressing the belief that the stock price will increase, and bearish (pessimistic) sentiment, expressing the belief that the stock price will decline.", "labels": [], "entities": [{"text": "identifying bullish (optimistic) sentiment", "start_pos": 31, "end_pos": 73, "type": "TASK", "confidence": 0.7013645172119141}]}, {"text": "The expressed sentiment is quantified as floating point values in the range of -1 (very negative/bearish) to 1 (very positive/bullish), with 0 denoting neutral sentiment.", "labels": [], "entities": []}, {"text": "(. This paper describes our system developed for subtask 2 (News Statements and Headlines).", "labels": [], "entities": []}, {"text": "While developing the system for this subtask, we systematically evaluated a number of alternative solutions for each step in the pipeline.", "labels": [], "entities": []}, {"text": "Specifically, we investigated different document vectorization approaches, such as N-gram models, TF-IDF and paragraph vectors.", "labels": [], "entities": []}, {"text": "A number of regression models were evaluated, namely, Simple Linear Regression, Support Vector Regression and XGBoost Linear Regression.", "labels": [], "entities": [{"text": "Support Vector Regression", "start_pos": 80, "end_pos": 105, "type": "METRIC", "confidence": 0.8518390655517578}]}, {"text": "One of the challenges with performing sentiment analysis in the financial domain is scarcity of training data.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.9370919466018677}]}, {"text": "We explored different approaches to augment the training data provided by the task organizers with training data from other sources in the financial domain, as well as using out-ofdomain sentiment resources.", "labels": [], "entities": []}], "datasetContent": [{"text": "A few different datasets were used to train the models on, in an attempt to identify the best representative training set.", "labels": [], "entities": []}, {"text": "The dataset augmentation strategies used are enumerated below.", "labels": [], "entities": []}, {"text": "For arriving at the baseline scores, an exhaustive set of tests were conducted using each of the document vectorization techniques in combination with the regression techniques described in the previous sections.", "labels": [], "entities": []}, {"text": "Using the automated test-suite included as part of the system, it was concluded that the Doc2Vec model performed best when the number of dimensions (features) of text is around 832 and the learning algorithm completes 40 passes before settling on a vector representation.", "labels": [], "entities": []}, {"text": "It was also concluded, that a combinations of unigrams & bigrams had the best baseline accuracy scores for the training datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9853869676589966}]}, {"text": "The measure of accuracy used was the R 2 score, also called the co-efficient of determination.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9995036125183105}, {"text": "R 2 score", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9717986385027567}]}, {"text": "The R 2 score can be computed using the below formula: where y is the gold set score vector and f is the predicted score vector, and N is the number of test samples.", "labels": [], "entities": [{"text": "R 2 score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.6657174825668335}]}, {"text": "For the two submissions permitted by SemEval, the methods used for the submissions made are described in.", "labels": [], "entities": []}, {"text": "The evaluation was done using the task evaluation metric, the cosine score (. and cosine weight = |P | |G| and G, P are the gold set scores and the predicted scores respectively, for N test samples.", "labels": [], "entities": []}, {"text": "The simplest model implemented, using Unigrams & Bigrams, combined with Simple Linear Regression, was what yielded the best performance by the system, with a cosine similarity score of 0.644.", "labels": [], "entities": [{"text": "cosine similarity score", "start_pos": 158, "end_pos": 181, "type": "METRIC", "confidence": 0.7643518050511678}]}], "tableCaptions": []}