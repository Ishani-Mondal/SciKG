{"title": [{"text": "TwiSe at SemEval-2017 Task 4: Five-point Twitter Sentiment Classification and Quantification", "labels": [], "entities": [{"text": "Five-point Twitter Sentiment Classification", "start_pos": 30, "end_pos": 73, "type": "TASK", "confidence": 0.6069313138723373}, {"text": "Quantification", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.6767893433570862}]}], "abstractContent": [{"text": "The paper describes the participation of the team \"TwiSE\" in the SemEval-2017 challenge.", "labels": [], "entities": [{"text": "SemEval-2017 challenge", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.8358277380466461}]}, {"text": "Specifically, I participated at Task 4 entitled \"Sentiment Analysis in Twitter\" for which I implemented systems for five-point tweet classification (Sub-task C) and five-point tweet quantification (Subtask E) for English tweets.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter\"", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.8751304030418396}]}, {"text": "In the feature extraction steps the systems rely on the vector space model, morpho-syntactic analysis of the tweets and several sentiment lexicons.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7214608937501907}]}, {"text": "The classification step of Subtask C uses a Logistic Regression trained with the one-versus-rest approach.", "labels": [], "entities": [{"text": "Subtask C", "start_pos": 27, "end_pos": 36, "type": "TASK", "confidence": 0.888777881860733}]}, {"text": "Another instance of Logistic Regression combined with the classify-and-count approach is trained for the quantification task of Subtask E. In the official leaderboard the system is ranked 5/15 in Subtask C and 2/12 in Subtask E.", "labels": [], "entities": [{"text": "Logistic Regression", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.8351652324199677}]}], "introductionContent": [{"text": "Microblogging platforms like Twitter have lately become ubiquitous, democratizing the way people publish and access information.", "labels": [], "entities": []}, {"text": "This vast amount of information that reflects the opinions, news or comments of people creates several opportunities for opinion mining.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 121, "end_pos": 135, "type": "TASK", "confidence": 0.7908112704753876}]}, {"text": "Among other platforms, Twitter is particularly popular for research due to its scale, representativeness and ease of access to the data it provides.", "labels": [], "entities": []}, {"text": "Furthermore, to facilitate the study of opinion mining, high quality resources and data challenges are organized.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.8164082765579224}]}, {"text": "The Task 4 of the SemEval-2017 challenges, entitled \"Sentiment Analysis in Twitter\" is among them.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter\"", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.9093450546264649}]}, {"text": "The paper describes the participation of the team Twitter Sentiment (TwiSe) in two of the subtasks of Task 4 of SemEval-2017.", "labels": [], "entities": []}, {"text": "Specifically, I participated in Subtasks C and E.", "labels": [], "entities": []}, {"text": "Both of them assume that sentiment is distributed across a fivepoint scale ranging from VeryNegative to VeryPositive.", "labels": [], "entities": []}, {"text": "Subtask C is a sentiment classification task, where given a tweet the aim is to assign one of the five classes.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.9475306272506714}]}, {"text": "Subtask E is a quantification task, whose aim is given a set of tweets referring to a subject to estimate the prevalence of each of the five classes.", "labels": [], "entities": []}, {"text": "The tasks are described in more detail at (.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 describes the feature extraction steps performed in order to construct the representation of a tweet, which is the same for both subtasks C and E.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7553166747093201}]}, {"text": "Section 3 details the learning approaches used and Section 4 summarizes the achieved performance.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes with pointers for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data shows the data released by the task organizers.", "labels": [], "entities": []}, {"text": "To tune the hyper-parameters of my models, I used a simple validation mechanism: I concatenated the \"Train2016\", \"Development2016\", and \"DevTest2016\" (9,070 tweets totally) to use them as training and I left the \"Test2016\" as validation data.", "labels": [], "entities": []}, {"text": "I acknowledge that using the \"Test2016\" part of the data only for validation purposes maybe limiting in terms of the achieved performance, since these data could have also used to train the system.", "labels": [], "entities": []}, {"text": "I also highlight that by using more elaborate validation strategies like using the subjects of the tweets, one should be able to achieve better results for tuning.", "labels": [], "entities": []}, {"text": "Official Rankings shows the performance the systems achieved.", "labels": [], "entities": []}, {"text": "There are two main observations.", "labels": [], "entities": []}, {"text": "For Subtask C, where TwiSe is ranked 5 th , I note that the system is a slightly improved version of the system of (, ranked first in the Subtask in the 2016 edition.", "labels": [], "entities": []}, {"text": "The only difference is the addition of the extra features from clustering the word embeddings.", "labels": [], "entities": []}, {"text": "This entails that significant progress was made to the task, which is either due to the extra data (\"Test2016\" we only used for validation) or more efficient algorithms.", "labels": [], "entities": []}, {"text": "On the other hand, TwiSe is ranked 2 nd in Subtask E. This, along with the simplicity of the approach used that is based on aggregating the counts of the classification step, entails that there is more work to be done in this direction.", "labels": [], "entities": [{"text": "TwiSe", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.7289601564407349}]}, {"text": "Five-Scale Classification: Error Analysis Analyzing the classification errors, one finds out that the (macro-averaged) mean-absolute-error per sentiment category is distributed as follows: VeryNegative: 0.836, Negative: 0.566, Neutral: 0.584, Positive: 0.771, VeryPositive: 0.443.", "labels": [], "entities": []}, {"text": "The system performed the best in the VeryPositive class (lowest error) and the worst in the VeryNegative class.", "labels": [], "entities": []}, {"text": "Interestingly, the system did not do as well in the Positive class.", "labels": [], "entities": []}, {"text": "To better understand why, plots the distribution of the instances across the five sentiment classes, for the training data we used and the test data.", "labels": [], "entities": []}, {"text": "Notice how the Positive class is the dominant in the training data, while this changes in the test data.", "labels": [], "entities": []}, {"text": "I believe that that the distribution drift, between the training and test data is indicative as of why the system performed poorly in the \"Positive\" class.", "labels": [], "entities": []}, {"text": "Five-Scale Quantification: Error Analysis I repeat, here, the error analysis process for the quantification task.", "labels": [], "entities": [{"text": "Five-Scale Quantification: Error Analysis", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.6574836313724518}, {"text": "quantification task", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8962698876857758}]}, {"text": "The best performance was achieved in the subject \"leonard cohen\", whose EMD was 0.029 while the worst performance in the topics \"maduro\" (EMD=0.709) and \"medicaid\" (EMD=0.660).", "labels": [], "entities": [{"text": "EMD", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9980579018592834}, {"text": "EMD", "start_pos": 165, "end_pos": 168, "type": "METRIC", "confidence": 0.8905985355377197}]}, {"text": "The distribution of sentiment for \"leonard cohen\" is very similar to the distribution of sentiment in the training set, KullbackLeibner divergence of 0.140.", "labels": [], "entities": []}, {"text": "On the other hand the Kullback-Leibner divergence for \"maduro\" and \"medicaid\", which are both skewed towards the negative sentiment, are 1.328 and 0.896 respectively.", "labels": [], "entities": []}, {"text": "Although a more detailed error analysis is required in order to improve the performance of the system, I believe that the distribution drift between the training examples and the examples of a subject plays an important role.", "labels": [], "entities": []}, {"text": "This maybe further enhanced by the fact I used a classify and count approach which does not account for drifts.", "labels": [], "entities": [{"text": "count", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.960480809211731}]}], "tableCaptions": [{"text": " Table 1: Size of the data used for training and de- velopment purposes. We only relied on the Se- mEval 2016 datasets.", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9203571081161499}, {"text": "Se- mEval 2016 datasets", "start_pos": 95, "end_pos": 118, "type": "DATASET", "confidence": 0.867387056350708}]}, {"text": " Table 2: Top-5 systems ranks for Subtask C based  on MAE M and of Subtask E based on EMD.", "labels": [], "entities": [{"text": "MAE", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.6077125072479248}]}]}