{"title": [{"text": "Combination of Neural Similarity Features and Comment Plausibility Features", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a text-ranking system developed by bunji team in SemEval-2017 Task 3: Community Question Answering, Subtask A and C.", "labels": [], "entities": [{"text": "Community Question Answering", "start_pos": 91, "end_pos": 119, "type": "TASK", "confidence": 0.5476104915142059}]}, {"text": "The goal of the task is to re-rank the comments in a question-and-answer forum such that useful comments for answering the question are ranked high.", "labels": [], "entities": []}, {"text": "We proposed a method that combines neural similarity features and hand-crafted comment plausibility features, and we modeled inter-comments relationship using conditional random field.", "labels": [], "entities": []}, {"text": "Our approach obtained the fifth place in the Sub-task A and the second place in the Subtask C.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper explains the participation of the bunji team in SemEval-2017 Task 3 on Community Question Answering (CQA) (, Subtask A and Subtask C. The goal of the task is to re-rank the comments in a question-and-answer forum such that useful comments for answering the question are ranked high.", "labels": [], "entities": [{"text": "SemEval-2017 Task 3 on Community Question Answering (CQA)", "start_pos": 59, "end_pos": 116, "type": "TASK", "confidence": 0.7073410093784332}]}, {"text": "Subtask A is extraction of relevant answers from comments in a question thread.", "labels": [], "entities": [{"text": "extraction of relevant answers from comments in a question thread", "start_pos": 13, "end_pos": 78, "type": "TASK", "confidence": 0.8298582315444947}]}, {"text": "Given a question and its comments, the system must re-rank the comments according to their relevance with respect to the question.", "labels": [], "entities": []}, {"text": "Subtask C is extraction of relevant answers from comments in different question threads.", "labels": [], "entities": [{"text": "extraction of relevant answers from comments in different question threads", "start_pos": 13, "end_pos": 87, "type": "TASK", "confidence": 0.8037823736667633}]}, {"text": "Given a question (the original question), questions that are possibly related to the original question (the relevant questions) and comments to the relevant questions, the system must re-rank the comments according to their relevance with respect to the original question.", "labels": [], "entities": []}, {"text": "Since the task is ranking, the primary metric is mean average precision (MAP).", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 49, "end_pos": 77, "type": "METRIC", "confidence": 0.9152782162030538}]}, {"text": "Our model consists of three elements; use of similarity features, use of comment plausibility features and a supervised scoring method that models inter-comments relationship.", "labels": [], "entities": []}, {"text": "The similarity features are designed to capture the similarities between a question and a comment because a valid answer should be on the same topic as the question.", "labels": [], "entities": []}, {"text": "Similarity features were utilized by many teams in.", "labels": [], "entities": []}, {"text": "In this work, we take a deep learning approach to extract similarity features.", "labels": [], "entities": []}, {"text": "The comment plausibility features are designed to capture characteristics that relevant answers tend to have.", "labels": [], "entities": []}, {"text": "Similar concept was proposed by, who tried to model readability, credibility, sentiment and trollness.", "labels": [], "entities": []}, {"text": "The comment plausibility features were hand-crafted to incorporate human knowledge about CQA.", "labels": [], "entities": [{"text": "CQA", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.8475279808044434}]}, {"text": "In past CQA tasks, some teams incorporated inter-comments relationship.", "labels": [], "entities": []}, {"text": "An example of such relationship is acknowledgement, where a good answer is likely to be followed by acknowledgement of the questioner.", "labels": [], "entities": [{"text": "acknowledgement", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.9659036993980408}]}, {"text": "modeled inter-comments relationship by taking distance to nearest acknowledgement as a feature and using Conditional Random Field (CRF) to model transition probability between relevant and irrelevant comments.", "labels": [], "entities": [{"text": "Conditional Random Field (CRF)", "start_pos": 105, "end_pos": 135, "type": "METRIC", "confidence": 0.8050851275523504}]}, {"text": "In our work, we try to model inter-comments relationship in much simpler way; by concatenating features of adjacent comments and by utilizing CRF for final ranking function.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our Primary submission was CRF with combined features.", "labels": [], "entities": [{"text": "CRF", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.44189390540122986}]}, {"text": "Contrastive 1 was CRF with only the comment plausibility features.", "labels": [], "entities": [{"text": "CRF", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.6704465746879578}]}, {"text": "Contrastive 2 was CRF with only the neural similarity features.", "labels": [], "entities": []}, {"text": "The official results for the 2017 test data are shown in.", "labels": [], "entities": []}, {"text": "The Primary obtained the fifth and the second in Subtask A and C, respectively.", "labels": [], "entities": []}, {"text": "The combined features (Primary) was much better than Contrastive 1 and 2 in Subtask A, as expected.", "labels": [], "entities": []}, {"text": "The large increase of 1.29 MAP score from Contrastive 1 to the Primary implies that the neural features and comment plausibility features were capturing different aspects of the problem.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9561885595321655}]}, {"text": "On the other hand, Contrastive 1 performed poorly in Subtask C. This was partially because the similarity was more important in Subtask C, which contained many unrelated comments.", "labels": [], "entities": []}, {"text": "Thus neural similarity features performed much better than in Subtask A and comment plausibility feature did much worse.", "labels": [], "entities": []}, {"text": "Another reason for Contrastive 1's poor performance may have been due to the over-fitting to development dataset, as implied by large performance drop from 2016 dataset (.", "labels": [], "entities": []}, {"text": "75.73 (Subtask A) and 7 (Subtask C).", "labels": [], "entities": []}, {"text": "From the result, the comment plausibility features seem to work as a blacklist for comments that are unlikely to bean answer.", "labels": [], "entities": []}, {"text": "For example, occurrence of words \"?,\" \"do,\" \"does,\" \"did,\" and \"what\" all contribute to identifying a question which are less likely to be a comment.", "labels": [], "entities": []}, {"text": "Our neural similarity feature performed worse than the previous application of recurrent neural network to Subtask A (MAP scores of 75.7 against our 71.4) and to Subtask C (MAP scores of 47.2 against our 28.0) (Wu and Lan, 2016).", "labels": [], "entities": [{"text": "MAP", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.921674370765686}]}, {"text": "The reason for the inferior performance maybe due to very large vocabulary of CQA, which caused the neural network to fallback to only using commonly appearing words in many cases.", "labels": [], "entities": []}, {"text": "As a supporting observation, attention weight seem to localize on very few commonly appearing words instead of on more meaningful region of text.", "labels": [], "entities": []}, {"text": "Use of sub-word vocabulary can help overcome this problem.", "labels": [], "entities": []}, {"text": "Also, we manually tuned the hyperparamters for neural network.", "labels": [], "entities": []}, {"text": "Random searching for better hyperparameters can improve the overall performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hyperparameters of the neural network", "labels": [], "entities": []}, {"text": " Table 2: Lexicons used in function-of-a-comment  features. \u27e8\u2022\u27e9 denotes a feature that is positive  when any of the words are present in the comment.", "labels": [], "entities": []}, {"text": " Table 4: 2017 official result", "labels": [], "entities": []}, {"text": " Table 5: Comparison of MAP scores in 2016 and  2017 test dataset", "labels": [], "entities": [{"text": "MAP", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.5900554656982422}]}, {"text": " Table 6: The top 8 contributing comment plausi- bility features in Subtask A", "labels": [], "entities": []}, {"text": " Table 7: The top 8 contributing comment plausi- bility features in Subtask C", "labels": [], "entities": []}]}