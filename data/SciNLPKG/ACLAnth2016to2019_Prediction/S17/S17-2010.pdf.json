{"title": [{"text": "HumorHawk at SemEval-2017 Task 6: Mixing Meaning and Sound for Humor Recognition", "labels": [], "entities": [{"text": "Humor Recognition", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.9040918052196503}]}], "abstractContent": [{"text": "This paper describes the winning system for SemEval-2017 Task 6: #HashtagWars: Learning a Sense of Humor.", "labels": [], "entities": [{"text": "SemEval-2017 Task 6", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8737112482388815}, {"text": "HashtagWars: Learning a Sense of Humor", "start_pos": 66, "end_pos": 104, "type": "TASK", "confidence": 0.627652930361884}]}, {"text": "Humor detection has up until now been predominantly addressed using feature-based approaches.", "labels": [], "entities": [{"text": "Humor detection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9871152639389038}]}, {"text": "Our system utilizes recurrent deep learning methods with dense embed-dings to predict humorous tweets from the @midnight show #HashtagWars.", "labels": [], "entities": []}, {"text": "In order to include both meaning and sound in the analysis, GloVe embeddings are combined with a novel phonetic representation to serve as input to an LSTM component.", "labels": [], "entities": []}, {"text": "The output is combined with a character-based CNN model, and an XG-Boost component in an ensemble model which achieved 0.675 accuracy in the official task evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9987785220146179}]}], "introductionContent": [{"text": "Computational approaches to how humour is expressed in language have received relatively limited attention up until very recently.", "labels": [], "entities": []}, {"text": "With few exceptions, they have used feature-based machine learning techniques () drawing on hand-engineered features such as sentence length, the number of nouns, number of adjectives, and tf-idf-based LexRank ().", "labels": [], "entities": [{"text": "LexRank", "start_pos": 202, "end_pos": 209, "type": "DATASET", "confidence": 0.8964009284973145}]}, {"text": "Among the recent proposals, puns have been emphasized as a crucial component of humor expression.", "labels": [], "entities": [{"text": "humor expression", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.7133228033781052}]}, {"text": "Others have proposed that text is perceived as humorous when it deviates in someway from what is expected (.", "labels": [], "entities": []}, {"text": "One of the reasons for such dominant position of the feature-based approaches is the fact that the datasets have been relatively small, rendering deep learning methods ineffective.", "labels": [], "entities": []}, {"text": "Furthermore, existing humour detection datasets tended to treat humor as a classification task in which text has to be labeled as funny or not funny, with nothing in between, which makes the task considerably simpler.", "labels": [], "entities": [{"text": "humour detection", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.6970920562744141}]}, {"text": "In contrast, the #HashtagWars dataset () provided for SemEval-2016 Task 6 assumes that humor can be evaluated on a scale, reflecting the reality that humor is nonbinary and some things maybe seen as funnier than others.", "labels": [], "entities": [{"text": "HashtagWars dataset", "start_pos": 18, "end_pos": 37, "type": "DATASET", "confidence": 0.6282873451709747}]}, {"text": "It is also large in size, making it better suited to the application of deep learning techniques.", "labels": [], "entities": []}, {"text": "SemEval 2017 Task 6 used the tweets posted by the viewers of the Comedy Central's @mid-night show, the #HashtagWars segment.", "labels": [], "entities": []}, {"text": "Our team participated in subtask A, which was as follows: given a pair of tweets supplied fora given hashtag by the viewers, the goal was to identify the tweet that the show judged to be funnier (.", "labels": [], "entities": []}, {"text": "This paper describes the winning submission, and specifically, our systems that took first and second place in the official rankings for the task.", "labels": [], "entities": []}, {"text": "Our goal was to create a model that could represent both meaning and sound, thus covering different aspects of the tweet that might make it funny.", "labels": [], "entities": []}, {"text": "Word embeddings have been used in a variety of applications, but phonetic information can provide new insights into the punchline of humor not present in traditional embeddings.", "labels": [], "entities": []}, {"text": "The pronunciation of a sentence is important to the delivery of a punchline, and can connect sound-alike words.", "labels": [], "entities": []}, {"text": "In our first submission for Subtask A, semantic information for each word is provided to the model in the form of a GloVe embedding.", "labels": [], "entities": []}, {"text": "We then provide the model with a novel phonetic representation of each word, in the form of a learned phonetic embedding taken as an intermediate state from an encoder-decoder character-tophoneme model.", "labels": [], "entities": []}, {"text": "With access to both meaning and sound embeddings, the model learns to read each tweet using a Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) encoder.", "labels": [], "entities": []}, {"text": "The encoded state of each tweet passes into dense layers, where a prediction is made as to which tweet is funnier.", "labels": [], "entities": []}, {"text": "In addition to the embedding model described above, we construct a Convolutional Neural Network (CNN) to process each tweet character by character.", "labels": [], "entities": []}, {"text": "This character-level model was used by, and serves as a baseline.", "labels": [], "entities": []}, {"text": "The output of the CNN feeds into the same final dense layers as the embedding LSTM tweet encoders.", "labels": [], "entities": []}, {"text": "This model achieved 63.7% accuracy in the official task evaluation, placing it second in the official task rankings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.999625563621521}]}, {"text": "To boost prediction performance further, we built an ensemble model over different model configurations.", "labels": [], "entities": []}, {"text": "In addition to the model above, we provided an embedding-LSTM-only model and a character-CNN-only model as input to the ensemble.", "labels": [], "entities": []}, {"text": "Inspired by previous work in NLP, we added an XGBoost feature-based model as input to the ensemble.", "labels": [], "entities": []}, {"text": "This system was our second submission.", "labels": [], "entities": []}, {"text": "The predictions of the ensemble model achieved 67.5% accuracy, placing it first in the official rankings for the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9992916584014893}]}, {"text": "We also report experiments we conducted after the release of the test data, in which a few of the bugs present in the original submissions were addressed, and in which the best model achieves the accuracy of 68.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.9995699524879456}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Sample character-to-phoneme model output.", "labels": [], "entities": [{"text": "Sample character-to-phoneme model", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.8508790930112203}]}, {"text": " Table 2: Model performance (accuracy). Official results reported for joint and ensemble models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9990424513816833}]}]}