{"title": [{"text": "Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM", "labels": [], "entities": [{"text": "SemEval-2017 Task 8", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.5894284844398499}, {"text": "Rumour Stance Classification", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.8887901504834493}]}], "abstractContent": [{"text": "This paper describes team Turing's submission to SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Sub-task A).", "labels": [], "entities": [{"text": "SemEval 2017 RumourEval", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.6745253006617228}, {"text": "Determining rumour veracity", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.8089802662531534}]}, {"text": "Subtask A addresses the challenge of rumour stance classification, which involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing.", "labels": [], "entities": [{"text": "rumour stance classification", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.9050301909446716}]}, {"text": "Stance classification is considered to bean important step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours.", "labels": [], "entities": [{"text": "Stance classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8785733282566071}, {"text": "rumour verification", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.9309539496898651}]}, {"text": "In this work we classify a set of Twit-ter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours.", "labels": [], "entities": [{"text": "Twit-ter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours", "start_pos": 34, "end_pos": 152, "type": "Description", "confidence": 0.7328015969080084}]}, {"text": "We propose a LSTM-based sequential model that, through modelling the conversational structure of tweets, which achieves an accuracy of 0.784 on the RumourEval test set outperforming all other systems in Sub-task A.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.999170184135437}, {"text": "RumourEval test set", "start_pos": 148, "end_pos": 167, "type": "DATASET", "confidence": 0.7786049743493398}]}], "introductionContent": [{"text": "In stance classification one is concerned with determining the attitude of the author of a text towards a target (.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.9450345039367676}]}, {"text": "Targets can range from abstract ideas, to concrete entities and events.", "labels": [], "entities": []}, {"text": "Stance classification is an active research area that has been studied in different domains (.", "labels": [], "entities": [{"text": "Stance classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9538973569869995}]}, {"text": "Here we focus on stance classification of tweets towards the truthfulness of rumours circulating in Twitter conversations in the context of breaking news.", "labels": [], "entities": [{"text": "stance classification of tweets", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.8386387228965759}]}, {"text": "Each conversation is defined by a tweet that initiates the conversation and a set of nested replies to it that form a conversation thread.", "labels": [], "entities": []}, {"text": "The goal is to classify each of the tweets in the conversation thread as either supporting, denying, querying or commenting (SDQC) on the rumour initiated by the source tweet.", "labels": [], "entities": []}, {"text": "Being able to detect stance automatically is very useful in the context of events provoking public resonance and associated rumours, as a first step towards verification of early reports (.", "labels": [], "entities": []}, {"text": "For instance, it has been shown that rumours that are later proven to be false tend to spark significantly larger numbers of denying tweets than rumours that are later confirmed to be true ().", "labels": [], "entities": []}, {"text": "Here we focus on exploiting the conversational structure of social media threads for stance classification and introduce a novel LSTM-based approach to harness conversations.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.9706677794456482}]}], "datasetContent": [{"text": "The dataset provided for this task contains Twitter conversation threads associated with rumours around ten different events in breaking news, including the Paris shootings in Charlie Hebdo, the Ferguson unrest, the crash of a Germanwings plane.", "labels": [], "entities": []}, {"text": "These events include 325 conversation threads consisting of 5568 underlying tweets annotated for stance at the tweet level (breakdown between training, testing and development sets is shown in  Each thread includes a source tweet that initiates a conversation and nested tweets responding to either the source tweet or earlier replies.", "labels": [], "entities": []}, {"text": "The thread can be split into linear branches of tweets, where a branch is defined as a chain of tweets that starts with a leaf tweet including its direct parent tweets, all the way up to the source tweet.", "labels": [], "entities": []}, {"text": "shows an example of a conversation along with its annotations represented as a tree structure with highlighted branches.", "labels": [], "entities": []}, {"text": "The depth of a tweet is the number of its parents starting from the root node.", "labels": [], "entities": []}, {"text": "Branches 1 and 2 in have depth one whereas branch 3 has depth three.", "labels": [], "entities": []}, {"text": "There is a clear class imbalance in favour of commenting tweets (66%) and supporting tweets (18%), whereas the denying (8%) and querying classes (8%) are under-represented (see).", "labels": [], "entities": []}, {"text": "While this imbalance poses a challenge, it is also indicative of the realistic scenario where only a few users question the veracity of a statement.", "labels": [], "entities": []}, {"text": "The dataset is split into training, development and test sets by the task organisers.", "labels": [], "entities": []}, {"text": "We determined the optimal set of hyperparameters via testing the performance of our model on the development set for different parameter combinations.", "labels": [], "entities": []}, {"text": "We used the   Tree of Parzen Estimators (TPE) algorithm 6 to search the parameter space, which is defined as follows: the number of dense ReLU layers varies from one to four; the number of LSTM layers is one or two; the mini-batch size is either 32 or 64; the number of units in the ReLU layer is one of {100, 200, 300, 400, 500}, and in the LSTM layer one of {100, 200, 300}; the strength of the L2 regularisation is one of {0.0, 1e-4, 3e-4, 1e-3} and the number of epochs is selected from {30, 50, 70, 100}.", "labels": [], "entities": []}, {"text": "We performed 100 trials of different parameter combinations optimising for accuracy on the development set in order to choose the best combination.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9992254972457886}]}, {"text": "We fixed hyperparameters to train the model on combined training and development sets and evaluated on the held out test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of threads, branches and tweets  in the training, development and testing sets.", "labels": [], "entities": []}, {"text": " Table 2: Per-class distribution of tweets in the  training, development and testing sets.", "labels": [], "entities": []}, {"text": " Table 3: Results on the development and testing sets. Accuracy and F1 scores: macro-averaged and per  class (S: supporting, D: denying, Q: querying, C: commenting).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9985717535018921}, {"text": "F1", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9992558360099792}]}, {"text": " Table 4: Number of tweets per depth and performance at each of the depths.", "labels": [], "entities": []}, {"text": " Table 3. Together  with the accuracy we show macro-averaged F- score and per-class macro-averaged F-scores as  these metrics account for the class imbalance. The  difference in accuracy between testing and devel- opment set is minimal, however we see significant  difference in Macro-F score due to different class  balance in these sets. Macro-F score could be im- proved if we used it as a metric for optimising  hyper-parameters. The branch-LSTM model pre- dicts commenting, the majority class well, how- ever it is unable to pick out any denying, the most- challenging under-represented class. Most deny-", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.998659610748291}, {"text": "F- score", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9393384059270223}, {"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9984123706817627}]}]}