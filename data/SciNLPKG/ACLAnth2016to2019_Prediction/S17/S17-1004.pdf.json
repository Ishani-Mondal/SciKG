{"title": [{"text": "Detecting Asymmetric Semantic Relations in Context: A Case-Study on Hypernymy Detection", "labels": [], "entities": [{"text": "Detecting Asymmetric Semantic Relations in Context", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8728911280632019}, {"text": "Detection", "start_pos": 78, "end_pos": 87, "type": "TASK", "confidence": 0.6469118595123291}]}], "abstractContent": [{"text": "We introduce WHIC 1 , a challenging testbed for detecting hypernymy, an asym-metric relation between words.", "labels": [], "entities": [{"text": "detecting hypernymy", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8262418806552887}]}, {"text": "While previous work has focused on detecting hy-pernymy between word types, we ground the meaning of words in specific contexts drawn from WordNet examples, and require predictions to be sensitive to changes in contexts.", "labels": [], "entities": []}, {"text": "WHIC lets us analyze complementary properties of two approaches of inducing vector representations of word meaning in context.", "labels": [], "entities": [{"text": "WHIC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9354782104492188}]}, {"text": "We show that such contextualized word representations also improve detection of a wider range of semantic relations in context.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language understanding applications like question answering () and textual entailment () benefit from identifying semantic relations between words beyond synonymy and paraphrasing.", "labels": [], "entities": [{"text": "Language understanding", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.690681591629982}, {"text": "question answering", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.8405179679393768}, {"text": "textual entailment", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7052977681159973}]}, {"text": "For instance, given \"Anand plays chess.\", and the question \"Which game does Anand play?\", successfully answering the question requires knowing that chess is a kind of game, i.e. chess entails game.", "labels": [], "entities": []}, {"text": "Such lexical entailment relations are asymmetric (chess =\u21d2 game, but game =\u21d2 chess), and detecting their direction accurately is a challenge.", "labels": [], "entities": []}, {"text": "While prior work has defined lexical entailment as a relation between word types, we argue that it is better defined between word meanings illustrated by examples of usage in context.", "labels": [], "entities": []}, {"text": "Ignoring context is problematic since entailment might hold between some senses of the words, but not others.", "labels": [], "entities": []}, {"text": "Consider the word game in the following contexts: 1 https://github.com/yogarshi/whic 1.", "labels": [], "entities": []}, {"text": "The championship game was played in NYC.", "labels": [], "entities": []}, {"text": "2. The hunters were interested in the big game.", "labels": [], "entities": []}, {"text": "Given the sentence, Anand is the world chess champion, chess =\u21d2 game in the first context, while chess =\u21d2 game in the second context.", "labels": [], "entities": []}, {"text": "Lexical entailment encompasses several semantic relations, with one important relation being hypernymy (.", "labels": [], "entities": [{"text": "Lexical entailment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8741995692253113}]}, {"text": "In this work, we focus on hypernymy detection in context, and show that existing resources can be leveraged to automatically create test beds for evaluation.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8172126114368439}]}, {"text": "We introduce \"Wordnet Hypernyms in Context\" (WHIC, pronounced which), a large dataset, automatically extracted from WordNet) using examples provided with synsets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 116, "end_pos": 123, "type": "DATASET", "confidence": 0.9451292157173157}]}, {"text": "Crucially, WHIC includes challenging negative examples that assess the ability of models to detect the direction of hypernymy.", "labels": [], "entities": [{"text": "WHIC", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.6053663492202759}]}, {"text": "We use WHIC to determine the effectiveness of existing supervised models for hypernymy detection) applied to representations, not only of word types, but of words in context.", "labels": [], "entities": [{"text": "WHIC", "start_pos": 7, "end_pos": 11, "type": "DATASET", "confidence": 0.8151713609695435}, {"text": "hypernymy detection", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7664510309696198}]}, {"text": "Such contextualized representations are induced in two ways: the first is based on Context2Vec, a BiLSTM model that embeds contexts and words in the same space; the second aims to capture geometric properties of the context in a standard word embedding space built using GloVe ().", "labels": [], "entities": []}, {"text": "We show that the two contextualized representations improve performance over contextagnostic baselines.", "labels": [], "entities": []}, {"text": "The structure of WHIC lets us show that they have complementary properties: Context2Vec-based models have higher recall and tend to identify directionality much better than Glove-based models.", "labels": [], "entities": [{"text": "WHIC", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.8051254153251648}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9992994070053101}]}, {"text": "We also show that the context-aware representations improve performance on identifying a broader range of semantic relations . staff , stick cl = He walked with the help of a wooden staff . Yes c r = The kid had a candied apple on a stick.", "labels": [], "entities": []}, {"text": "staff , body cl = The hospital has an excellent nursing staff . Yes c r = The whole body filed out of the auditorium.", "labels": [], "entities": []}, {"text": "staff , stick cl = The hospital has an excellent nursing staff . No c r = The kid had a candied apple on a stick.", "labels": [], "entities": []}, {"text": "We frame hypernymy detection in context as a binary classification task.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7261459082365036}]}, {"text": "Each example consists of a 4-tuple (w l , w r , cl , c r ), where w land w rare word types, and cl and c rare sentences which illustrate each word usage.", "labels": [], "entities": []}, {"text": "The example is treated as positive if w l =\u21d2 w r , given the meaning of each word exemplified by the contexts, and negative otherwise, as can be seen in.", "labels": [], "entities": []}, {"text": "As mentioned in Section 1, hypernymy is only one specific case of lexical entailment.", "labels": [], "entities": []}, {"text": "The nature of entailment relations captured out-of-context can be broader depending on the test beds considered 2 . These relations can include synonymy, hypernymy, some meronymy relations, and also cause-effect relations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We require a dataset to study hypernymy detection in context to satisfy the following desiderata: (1) the dataset should make it possible to assess the sensitivity of context-aware models to contexts that signal different word senses, and (2) the dataset should help quantify the extent to which models detect the asymmetric direction of hypernymy, rather than symmetric semantic similarity.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.723230704665184}]}, {"text": "Tasks In addition to WHIC, we evaluate our context-aware representations on CONTEXT-PPDB.", "labels": [], "entities": [{"text": "WHIC", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.63886559009552}]}, {"text": "As mentioned in Section 2.3, CONTEXT-PPDB is a dataset for fine-grained lexical inference in context that captures other semantic relations beyond hypernymy.", "labels": [], "entities": []}, {"text": "It has been created using 375 word pairs from a subset of the English Paraphrase Database ().", "labels": [], "entities": [{"text": "English Paraphrase Database", "start_pos": 62, "end_pos": 89, "type": "DATASET", "confidence": 0.8538431922594706}]}, {"text": "These word pairs are semiautomatically labeled with semantic relations outof-context.", "labels": [], "entities": []}, {"text": "augmented them with examples of word usage in context, and re-annotated the word pairs given the extra contextual information.", "labels": [], "entities": []}, {"text": "The final dataset consists of 3750 words/contexts tuples with a corresponding semantic label, one of which is entailment.", "labels": [], "entities": []}, {"text": "All our experiments are with the default train/dev/test splits on both datasets.", "labels": [], "entities": []}, {"text": "Contextualized Word Representations To obtain the Context2Vec representations, we use an existing 600-dimensional model trained on ukWaC).", "labels": [], "entities": [{"text": "Contextualized Word Representations", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5795583625634512}, {"text": "ukWaC", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.9781674742698669}]}, {"text": "We use 600 dimensional GloVe embeddings trained on the same corpus to create w l , w r , Cl , and Cr , and allow fora controlled comparison with Context2Vec.", "labels": [], "entities": []}, {"text": "Context2Vec representations are significantly more expensive to train: indicate that training requires ~30 hours on a Tesla K80 GPU, while the GloVe embeddings can be trained on the exact same amount of data in less than 7 hours on a CPU.", "labels": [], "entities": []}, {"text": "In our first set of experiments, we evaluate the two models described in Section 3 on WHIC under a variety of combinations.", "labels": [], "entities": [{"text": "WHIC", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.884736955165863}]}], "tableCaptions": [{"text": " Table 2: Results on WHIC. a) Word type indi- cates (GloVe or Context2Vec (C2V)) H-Features  extracted from context-agnostic representations.  b) Context aware indicates H-Features extracted  from the context-aware representations described  in Section 3.", "labels": [], "entities": [{"text": "WHIC", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.8525158762931824}]}, {"text": " Table 3: Macro-P/R/F1 and Pairwise accuracy, are intended to capture context-awareness (Section 6.2)  and directionality-discrimination abilities (Section 6.3) of the models, respectively.", "labels": [], "entities": [{"text": "F1", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.9757426381111145}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9349454045295715}]}, {"text": " Table 4: Impact of masks on WHIC measured by Precision (P), Recall (R), F-Measure (F), context sen- sitivity (Macro-F1) and directionality (Pairwise accuracy). Replacing our contextualized representations  by a mean representation of the context, or a contextualized representation based only on the mean, leads  to drops in performance.", "labels": [], "entities": [{"text": "WHIC", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.5672691464424133}, {"text": "Precision (P)", "start_pos": 46, "end_pos": 59, "type": "METRIC", "confidence": 0.9389669001102448}, {"text": "Recall (R)", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9521841108798981}, {"text": "F-Measure (F)", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9527881890535355}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.740100622177124}]}, {"text": " Table 5: Results on CONTEXT-PPDB. Baseline  indicates the previous state of the art result on this  dataset", "labels": [], "entities": [{"text": "CONTEXT-PPDB", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.5693159699440002}, {"text": "Baseline", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9598667621612549}]}, {"text": " Table 6: Performance of the baseline and  augmented model on all semantic relations in  CONTEXT-PPDB measured using per-class F1", "labels": [], "entities": []}]}