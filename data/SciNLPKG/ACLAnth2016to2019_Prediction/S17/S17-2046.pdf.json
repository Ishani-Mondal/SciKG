{"title": [{"text": "MoRS At SemEval-2017 Task 3: Easy To Use SVM In Ranking Tasks", "labels": [], "entities": [{"text": "MoRS At SemEval-2017 Task 3", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.6901923656463623}]}], "abstractContent": [{"text": "This paper describes our system, dubbed MoRS (Modular Ranking System), pronounced 'Morse', which participated in Task 3 of SemEval-2017.", "labels": [], "entities": []}, {"text": "We used MoRS to perform the Community Question Answering Task 3, which consisted on reordering a set of comments according to their usefulness in answering the question in the thread.", "labels": [], "entities": [{"text": "MoRS", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.9007816910743713}, {"text": "Community Question Answering Task", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.6730225160717964}]}, {"text": "This was made fora large collection of questions created by a user community.", "labels": [], "entities": []}, {"text": "As for this challenge we wanted to go back to simple, easy-to-use, and somewhat forgotten technologies that we think, in the hands of non-expert people , could be reused in their own data sets.", "labels": [], "entities": []}, {"text": "Some of our techniques included the annotation of text, the retrieval of meta-data for each comment, POS tagging and Named Entity Recognition, among others.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 101, "end_pos": 112, "type": "TASK", "confidence": 0.7772144079208374}, {"text": "Named Entity Recognition", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.600879559914271}]}, {"text": "These gave place to syntactical analysis and semantic measurements.", "labels": [], "entities": [{"text": "syntactical analysis", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.7500740885734558}]}, {"text": "Finally we show and discuss our results and the context of our approach, which is part of a more comprehensive system in development, named MoQA.", "labels": [], "entities": [{"text": "MoQA", "start_pos": 140, "end_pos": 144, "type": "DATASET", "confidence": 0.9430021047592163}]}], "introductionContent": [{"text": "The main difference between Question Answering (QA) and Community Question Answering (cQA) is, while QA systems rely on a user query in order to search and prepare an answer based on the searching capabilities it already has and its documents, in cQA the query and respective related answers are already provided, being only necessary a reordering by relevance of such answers, or perhaps even a rephrasing of such an answer in order to suit better the query.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.8923292517662048}, {"text": "Community Question Answering (cQA)", "start_pos": 56, "end_pos": 90, "type": "TASK", "confidence": 0.7939318418502808}]}, {"text": "In recent times, QA systems have attracted great interest in the information retrieval community, and also in the cQA.", "labels": [], "entities": [{"text": "cQA", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.9543312788009644}]}, {"text": "A characteristic of cQA, is that a user resorts to the web for answers without a given structured knowledge base.", "labels": [], "entities": []}, {"text": "The arbitrariness of cQA forums, the dependence and waiting time on their results, may slow the gathering of answers in real time.", "labels": [], "entities": []}, {"text": "Also, public forums are dependent of the users input (i.e. answers), which might be rather unstructured, not straight to the point, not related to the question at hand, not well written (i.e. grammatically), lengthy or even incorrect.", "labels": [], "entities": []}, {"text": "Our team shares this exact same interest and continuous development in this area.", "labels": [], "entities": []}, {"text": "Our participation focused on the Community Question Answering (CQA) Task 3 SubTask A of 2017 SemEval edition 1 (, which consisted on reordering a set of comments according to their usefulness in answering the question in the thread.", "labels": [], "entities": [{"text": "Community Question Answering (CQA) Task 3 SubTask", "start_pos": 33, "end_pos": 82, "type": "TASK", "confidence": 0.7900328934192657}, {"text": "SemEval edition 1", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.5242229004700979}]}, {"text": "This was made fora large collection of questions created by a user community, provided by the task organizers.", "labels": [], "entities": []}, {"text": "Succinctly, Subtask A: Question-Comment Similarity, involved ranking \"Good\" comments above the \"PotentiallyUseful\" or \"Bad\" comments, where there was no distinction between them, since their difference was not important for the task's evaluation method.", "labels": [], "entities": []}, {"text": "Finally, the result file was to be a ranked list of the probability of the comments according to their relevance.", "labels": [], "entities": []}, {"text": "We developed MoRS (pronounced \"morse\"), which went back to simple and rather effective technologies, making it available at a later stage to the public, with minimal pre-requisites and ease of (re)use.", "labels": [], "entities": []}, {"text": "MoRS addressed Subtask A of Task 3 by first recognizing relevant terms in each query and also in the respective comments in the thread related to the question being analyzed.", "labels": [], "entities": [{"text": "MoRS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7698128819465637}]}, {"text": "Next, the system builds its features by passing through a submodule which analyzes each question and respective comments.", "labels": [], "entities": []}, {"text": "Then, MoRS compares named entities from the questions and comments and crossreferences them.", "labels": [], "entities": []}, {"text": "It also identifies the comments that shared most concepts with the ones associated to the thread question.", "labels": [], "entities": []}, {"text": "MoRS employed a semantic similarity to measure how close in meaning both comments and questions are even if they do no share the same exact concepts.() Additionally, MoRS used Machine Learning (Pedregosa et al., 2011) techniques to classify if a comment as \"Good\", as explained in the description of the Subtask, and \"Not Good\" according to the comment's relevance.", "labels": [], "entities": [{"text": "MoRS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9198285937309265}]}, {"text": "The paper also describes the main system where MoRS is part of, which is a larger and modular pipeline, MoQA (Modular Question Answering, pronounced \"mocha\") and also presents the result measure values obtained in Subtask A. Despite successful implementation we did not get the desired results due to data set corruption found only after result submission.", "labels": [], "entities": [{"text": "Modular Question Answering", "start_pos": 110, "end_pos": 136, "type": "TASK", "confidence": 0.6799370447794596}]}, {"text": "The following section, Section 3, approaches some works that have inspired our system overall, Section 4 explains its composing sub-modules, in Section 5, we present and discuss our results, and finally in sections 6 and 7 we talk about our conclusions and how we plan to approach the future work and applicability of MoRS.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: MoRS' comparison to last year's task 3  results.", "labels": [], "entities": [{"text": "MoRS", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.483571857213974}]}, {"text": " Table 2: MoRS' results for the development set of  2017.", "labels": [], "entities": []}]}