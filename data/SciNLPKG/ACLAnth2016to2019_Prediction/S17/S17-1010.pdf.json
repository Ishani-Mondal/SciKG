{"title": [{"text": "Semantic Frame Labeling with Target-based Neural Model", "labels": [], "entities": [{"text": "Semantic Frame Labeling", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6902686059474945}]}], "abstractContent": [{"text": "This paper explores the automatic learning of distributed representations of the tar-get's context for semantic frame labeling with target-based neural model.", "labels": [], "entities": [{"text": "semantic frame labeling", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.6943409045537313}]}, {"text": "We constrain the whole sentence as the model's input without feature extraction from the sentence.", "labels": [], "entities": []}, {"text": "This is different from many previous works in which local feature extraction of the targets is widely used.", "labels": [], "entities": []}, {"text": "This constraint makes the task harder, especially with long sentences, but also makes our model easily applicable to a range of resources and other similar tasks.", "labels": [], "entities": []}, {"text": "We evaluate our model on several resources and get the state-of-the-art result on subtask 2 of SemEval 2015 task 15.", "labels": [], "entities": [{"text": "SemEval 2015 task 15", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.7572782784700394}]}, {"text": "Finally, we extend the task to word-sense disambiguation task and we also achieve a strong result in comparison to state-of-the-art work.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.7478557229042053}]}], "introductionContent": [], "datasetContent": [{"text": "We simply divide all the datasets in two types: per-target and non per-target.", "labels": [], "entities": []}, {"text": "Per-target semantic frame resources define a different set of frame labels for each target and we train one model for each target; different targets may share some semantic frame labels in non per-target resources and we train a single model for such resources.", "labels": [], "entities": []}, {"text": "We use the Semlink project ( ) to create our datasets 1 . Semlink aims to link together different lexical resources via a set of mappings.", "labels": [], "entities": []}, {"text": "We use its corpus which annotates FrameNet and Propbank frames for the WSJ section of the Penn Treebank.", "labels": [], "entities": [{"text": "WSJ section", "start_pos": 71, "end_pos": 82, "type": "DATASET", "confidence": 0.914710134267807}, {"text": "Penn Treebank", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.8585084974765778}]}, {"text": "Another resource we use is PDEV 2 which is quite new and has CPA frame annotated examples on British National Corpus.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 93, "end_pos": 116, "type": "DATASET", "confidence": 0.8776490887006124}]}, {"text": "All the original instances are sentence-tokenized and the punctuation was removed.", "labels": [], "entities": []}, {"text": "The details of creating the datasets are as follows: \u2022 FrameNet: Non per-target type.", "labels": [], "entities": []}, {"text": "We get FrameNet annotated instances through Semlink.", "labels": [], "entities": []}, {"text": "If one FrameNet frame label contains more than 300 instances, we divide it proportionately: 70%, 20% and 10%.", "labels": [], "entities": []}, {"text": "Then we respectively accumulate the three parts by each frame label to create the training, test and validation set.", "labels": [], "entities": []}, {"text": "\u2022 PropBank: Per-target type.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 2, "end_pos": 10, "type": "DATASET", "confidence": 0.7059111595153809}]}, {"text": "The creation process is same as FrameNet except that we finally get training, test and validation set for each target and the cutoff is set to 70 instead of 300.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.8239263296127319}]}, {"text": "\u2022 PDEV: Same as PropBank but with the cutoff set to 100 instead of 70.", "labels": [], "entities": [{"text": "PDEV", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.6592676043510437}, {"text": "PropBank", "start_pos": 16, "end_pos": 24, "type": "DATASET", "confidence": 0.9751697182655334}]}, {"text": "Since the performance of our model is almost decided by the training data we empirically choose the cutoff above to keep the instances of each label enough.", "labels": [], "entities": []}, {"text": "Summary statistics of the above datasets are in.", "labels": [], "entities": []}, {"text": "Corpus Pattern Analysis (CPA) is anew technique for identifying the main patterns in which a word is used in text and is currently being used to build the PDEV resource as we mentioned above.", "labels": [], "entities": [{"text": "Corpus Pattern Analysis (CPA)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7962274054686228}, {"text": "PDEV resource", "start_pos": 155, "end_pos": 168, "type": "DATASET", "confidence": 0.8713361918926239}]}, {"text": "It is also a shared task in SemEval-2015 task 15 ().", "labels": [], "entities": [{"text": "SemEval-2015 task 15", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7084708213806152}]}, {"text": "The task is divided into three subtasks: CPA parsing, CPA clustering and CPA lexicography.", "labels": [], "entities": [{"text": "CPA parsing", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.8370509743690491}, {"text": "CPA clustering", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.698582649230957}]}, {"text": "We only introduce the first two related subtasks.", "labels": [], "entities": []}, {"text": "CPA parsing aims at identifying the arguments of the target and tagging predefined semantic meaning on them; CPA clustering clusters the instances to obtain CPA frames based on the result of CPA parsing.", "labels": [], "entities": [{"text": "CPA parsing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6811179965734482}, {"text": "CPA parsing", "start_pos": 191, "end_pos": 202, "type": "TASK", "confidence": 0.6163288950920105}]}, {"text": "However, the first step results seem unpromising which will influence the process of obtaining CPA frames.", "labels": [], "entities": []}, {"text": "Since our model can be applied on sentence-level input without feature extraction we can directly evaluate  Finally, we choose Word Sense Disambiguation (WSD) task to extend our experiment.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 127, "end_pos": 158, "type": "TASK", "confidence": 0.7013684262832006}]}, {"text": "As our benchmark for WSD task, we choose English Lexical Sample WSD tasks of SemEval-2007 task 17 (.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.9107293784618378}]}, {"text": "We use cross-validation on the training set and we observe the model performs better when we update the word vectors which is different from the preceding experimental setup.", "labels": [], "entities": []}, {"text": "The number of hidden units is set to 55.", "labels": [], "entities": []}, {"text": "The rows from 4 to 6 come from.", "labels": [], "entities": []}, {"text": "They integrate word embeddings into IMS (It Makes Sense) system (Zhong and Ng, 2010) which uses support vector machine as its classifier based on some standard WSD features and they get the best result; they use an exponential decay function, also designed to give more importance to close context, to compute the word representation, but their method need manually choose the window size of the target word and one parameter of their exponential decay function.", "labels": [], "entities": []}, {"text": "Both with word vectors only, our model is comparable with the sixth row.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Non per-target examples. Frames are  from FrameNet and the target words are in bold.", "labels": [], "entities": [{"text": "Frames", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.7989286184310913}, {"text": "FrameNet", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9359151124954224}]}, {"text": " Table 2: Summary statistics for the datasets. The  average numbers per target are shown in the paren- theses for per-target resources.", "labels": [], "entities": []}, {"text": " Table 3.  Target-Only gets very high scores on FrameNet  dataset. FrameNet dataset has 55 targets which  has multiple frame labels in the training data and  these targets have 1981 instances in the test da- ta. We get 0.769 F-score on these instances and  0.393 F-score on 64 unseen targets with 77 test  instances. This can be the extreme case that the  main feature for the correct frame is the target it- self. Despite this simple fact, standard LSTM per- forms very badly on FrameNet. The main reason  is that sentences in FrameNet dataset are too long  and standard LSTM can not learn well due to the  large number of irrelevant words that appear in  long sentences. To show this, we select the size  of truncation window for original FrameNet sen- tences and we get the best size of 5 on validation  data with each 2 words surrounding the target. Fi- nally, we get 0.958 F-score on FrameNet test data  which is still lower than TRNN on full sentences.  As for PropBank and PDEV dataset, we train one  model for each target so the final F-score is the av- erage of all targets. However, the number of train- ing instances per target is limited. TRNN will usu- ally not perform well when it tries to learn some frames which consist of many different concept- s and especially when the frame has a few train- ing instances. Considering the sentence 4 of", "labels": [], "entities": [{"text": "FrameNet  dataset", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.9438851177692413}, {"text": "FrameNet dataset", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.9118888974189758}, {"text": "F-score", "start_pos": 225, "end_pos": 232, "type": "METRIC", "confidence": 0.9962956309318542}, {"text": "F-score", "start_pos": 263, "end_pos": 270, "type": "METRIC", "confidence": 0.992095947265625}, {"text": "FrameNet", "start_pos": 480, "end_pos": 488, "type": "DATASET", "confidence": 0.9550382494926453}, {"text": "FrameNet dataset", "start_pos": 528, "end_pos": 544, "type": "DATASET", "confidence": 0.9514463543891907}, {"text": "F-score", "start_pos": 878, "end_pos": 885, "type": "METRIC", "confidence": 0.9882807731628418}, {"text": "FrameNet test data", "start_pos": 889, "end_pos": 907, "type": "DATASET", "confidence": 0.8443489670753479}, {"text": "PropBank", "start_pos": 967, "end_pos": 975, "type": "DATASET", "confidence": 0.9073964357376099}, {"text": "PDEV dataset", "start_pos": 980, "end_pos": 992, "type": "DATASET", "confidence": 0.8890590667724609}, {"text": "F-score", "start_pos": 1043, "end_pos": 1050, "type": "METRIC", "confidence": 0.970416784286499}]}, {"text": " Table 3: Results on several semantic frame  resources. The format of cell value is \"F- score/hidden unit\" for TRNN and LSTM and \"F- score/iteration\" for MaxEnt toolkit.", "labels": [], "entities": [{"text": "F- score/hidden unit", "start_pos": 85, "end_pos": 105, "type": "METRIC", "confidence": 0.9448582132657369}, {"text": "F- score", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9559270143508911}]}, {"text": " Table 5: Results on Microcheck dataset of CPA  clustering.", "labels": [], "entities": [{"text": "Microcheck dataset", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.942655473947525}, {"text": "CPA  clustering", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7567821443080902}]}, {"text": " Table 6: Result on Lexical Sample task of  SemEval-2007 task 17", "labels": [], "entities": [{"text": "Lexical Sample task", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.8201534350713094}, {"text": "SemEval-2007 task", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7880622446537018}]}]}