{"title": [{"text": "XJSA at SemEval-2017 Task 4\uff1aA Deep System for Sentiment Classification in Twitter", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.9326803982257843}]}], "abstractContent": [{"text": "This paper describes the XJSA System submission from XJTU.", "labels": [], "entities": [{"text": "XJSA System submission from XJTU", "start_pos": 25, "end_pos": 57, "type": "DATASET", "confidence": 0.8401175260543823}]}, {"text": "Our system was created for SemEval2017 Task 4-subtask A which is very popular and fundamental.", "labels": [], "entities": [{"text": "SemEval2017 Task 4-subtask", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.8120815555254618}]}, {"text": "The system is based on convolutional neural network and word embedding.", "labels": [], "entities": []}, {"text": "We used two pre-trained word vectors and adopt a dynamic strategy for k-max pooling.", "labels": [], "entities": []}], "introductionContent": [{"text": "Several years ago, the typical approaches to sentiment analysis of tweets were based on classifiers trained using several hand-crafted features, in particular lexicons of words with an assigned polarity value.", "labels": [], "entities": [{"text": "sentiment analysis of tweets", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.918704628944397}]}, {"text": "About since 2014 the deep neural network methods have got state-ofthe-art results in many NLP tasks, especially in sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.9637474119663239}]}, {"text": "The work of Harvard NLP group in have suggested that convolutional neural network and word embedding play important roles in this field.", "labels": [], "entities": [{"text": "Harvard NLP", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.8921317756175995}]}, {"text": "General word embedding has got excellent results.", "labels": [], "entities": []}, {"text": "If we can embed sentiment information in vectors, we will get better results.", "labels": [], "entities": []}, {"text": "There are some open word vectors on the web already such as Word2Vec (),), SSWE ().", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9760019779205322}]}, {"text": "In our system we use Word2Vec and SSWE at the same time.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9763129949569702}]}, {"text": "Deep learning models have achieved excellent results in computer vision and speech recognition in recent years.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7957562208175659}]}, {"text": "In the field of natural language processing, much work with deep learning methods has involved learning word vectors representations for their own task or problem ().The others exploit the open word vectors which was mentioned above.Word vectors is a transformation of the feature of letter,word,sentence and paragraph or even text.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.693505048751831}]}, {"text": "It's a lower dimensional, dense and continuous vectors.", "labels": [], "entities": []}, {"text": "In this vector, the words have similar syntactic are close -in Euclidean or cosine distance in the vector space.", "labels": [], "entities": []}, {"text": "So one can study and compare the syntactic functionality between different words via word vectors.", "labels": [], "entities": []}, {"text": "Convolutional neural network (CNN) utilize layers with convolutional filters that are applied to local features ().", "labels": [], "entities": []}, {"text": "CNN originally invented for computer vision, recently CNN models have achieved remarkably results in many natural language processing problem, such as sentence modeling), semantic parsing (), sentiment classification) and other traditional natural language processing tasks).", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 151, "end_pos": 168, "type": "TASK", "confidence": 0.713532343506813}, {"text": "semantic parsing", "start_pos": 171, "end_pos": 187, "type": "TASK", "confidence": 0.7153192758560181}, {"text": "sentiment classification", "start_pos": 192, "end_pos": 216, "type": "TASK", "confidence": 0.8309225738048553}]}, {"text": "Our system was inspired by the work) and another work ().", "labels": [], "entities": []}, {"text": "In the aspect of CNN, we use a simple 3 layers CNN to automatic extract features.", "labels": [], "entities": [{"text": "CNN", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.8278231024742126}]}, {"text": "In the aspect of pretrained vectors, we use the Word2Vec and SSWE to filter our training set to get a proper input for CNN.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.9877235889434814}, {"text": "CNN", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.8371809124946594}]}, {"text": "The reason that we use the vectors trained by is the 100 billion words of Google News and the vectors are publicly for free.", "labels": [], "entities": [{"text": "Google News", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.7832860052585602}]}, {"text": "We use the SSWE vectors because the vectors was especially trained for sentiment classification by.", "labels": [], "entities": [{"text": "SSWE vectors", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.8123001158237457}, {"text": "sentiment classification", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.9490716755390167}]}, {"text": "SSWE contains sentiment information which is not in word vectors trained by Mikolov.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test our system on the following settings:  The experiments were run on a linux server with an nVIDIA GTX 1080 accelerated GPU.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Official results of our submission  compared to the top one.", "labels": [], "entities": []}]}