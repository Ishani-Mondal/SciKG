{"title": [{"text": "ITNLP-AiKF at SemEval-2017 Task 1: Rich Features Based SVR for Semantic Textual Similarity Computing", "labels": [], "entities": [{"text": "ITNLP-AiKF", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9328242540359497}, {"text": "Semantic Textual Similarity Computing", "start_pos": 63, "end_pos": 100, "type": "TASK", "confidence": 0.7049215734004974}]}], "abstractContent": [{"text": "Semantic Textual Similarity (STS) devotes to measuring the degree of equivalence in the underlying semantic of the sentence pair.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8147112329800924}]}, {"text": "We proposed anew system, ITNLP-AiKF, which applies in the SemEval 2017 Task1 Semantic Textual Similarity track 5 English monolingual pairs.", "labels": [], "entities": [{"text": "ITNLP-AiKF", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.8350343108177185}, {"text": "SemEval 2017 Task1 Semantic Textual Similarity track 5 English monolingual pairs", "start_pos": 58, "end_pos": 138, "type": "TASK", "confidence": 0.7879975275559858}]}, {"text": "In our system, rich features are involved, including On-tology based, word embedding based, Corpus based, Alignment based and Literal based feature.", "labels": [], "entities": []}, {"text": "We leveraged the features to predict sentence pair similarity by a Support Vector Regression (SVR) model.", "labels": [], "entities": []}, {"text": "In the result, a Pearson Correlation of 0.8231 is achieved by our system, which is a competitive result in the contest of this track.", "labels": [], "entities": [{"text": "Pearson Correlation", "start_pos": 17, "end_pos": 36, "type": "METRIC", "confidence": 0.9567077159881592}]}], "introductionContent": [{"text": "Semantic Evaluation (SemEval) contest devotes to pushing the research of semantic analysis, which attracts many participants and promote a lot of groundbreaking achievements in natural language processing (NLP) field.", "labels": [], "entities": [{"text": "Semantic Evaluation (SemEval) contest", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7780472785234451}, {"text": "semantic analysis", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7737501263618469}]}, {"text": "Semantic textual similarity (STS) task works for computing word and text semantics, which has made extensive attraction to the researchers and NLP community since SemEval 2012 (.", "labels": [], "entities": [{"text": "Semantic textual similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7641290724277496}, {"text": "computing word and text semantics", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.7327741980552673}]}, {"text": "In STS 2017, The organizers added monolingual Arabic and Cross-lingual Arabic-English semantic calculation in order to increase the difficulty in the contest.", "labels": [], "entities": [{"text": "Cross-lingual Arabic-English semantic calculation", "start_pos": 57, "end_pos": 106, "type": "TASK", "confidence": 0.6171870678663254}]}, {"text": "The task definition is given two sentences participating systems are asked to predict a continuous similarity score on a scale from 0 to 5 of the sentence pair, with 0 indicating that the semantics of the sentences completely independent and 5 semantic equivalence.", "labels": [], "entities": [{"text": "continuous similarity score", "start_pos": 88, "end_pos": 115, "type": "METRIC", "confidence": 0.6823240717252096}]}, {"text": "The evaluation criterion uses Pearson Correlation Coefficient, which computing the correlation between golden standard scores and semantic system predicted scores.", "labels": [], "entities": []}, {"text": "In our system, in order to predict similarity score of two sentences, we trained a Support Vector Regression (SVR) model with abundant features including Ontology based features, Word Embedding based features, Corpus based features, Alignment based features and Literal based features.", "labels": [], "entities": []}, {"text": "All the English training, trial and evaluation data set released by previous STS tasks in SemEval were used to construct our system.", "labels": [], "entities": []}, {"text": "Our best system achieved 0.8231 Pearson Correlation coefficient in the SemEval 2017 evaluation data set, and the winner achieved 0.8547.", "labels": [], "entities": [{"text": "Pearson Correlation coefficient", "start_pos": 32, "end_pos": 63, "type": "METRIC", "confidence": 0.9544542034467062}, {"text": "SemEval 2017 evaluation data set", "start_pos": 71, "end_pos": 103, "type": "DATASET", "confidence": 0.8566643834114075}]}], "datasetContent": [{"text": "In our system, We build our data set by collecting all off-the-shelf English data sets which released by prior STS evaluations (except the evaluation data set of STS 2016).", "labels": [], "entities": [{"text": "English data sets", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.7907252113024393}, {"text": "evaluation data set of STS 2016)", "start_pos": 139, "end_pos": 171, "type": "DATASET", "confidence": 0.8974584170750209}]}, {"text": "After that, 80% data set are used as train data set and 20% as valid data set.", "labels": [], "entities": []}, {"text": "In our system, we trained SVR model, and the SVR parameters are set as. parameter kernel C gamma epsilon value rbf 0.1 auto 0.0: parameter setting in SVR.", "labels": [], "entities": [{"text": "parameter kernel C gamma epsilon value rbf 0.1 auto 0.0", "start_pos": 72, "end_pos": 127, "type": "METRIC", "confidence": 0.758550974726677}]}, {"text": "Ontology based, Word embedding based, Corpus based, Alignment based and Literal based features are used in SVR model respectively, in order to explore the effect of each kind of features.", "labels": [], "entities": []}, {"text": "We used SemEval 2016 evaluation data set to test the performance of different feature set, and the results of Pearson Correlation coefficients are shown in.", "labels": [], "entities": [{"text": "SemEval 2016 evaluation data set", "start_pos": 8, "end_pos": 40, "type": "DATASET", "confidence": 0.8002115428447724}, {"text": "Pearson Correlation", "start_pos": 110, "end_pos": 129, "type": "METRIC", "confidence": 0.892810583114624}]}, {"text": "The indicates Word2Vec performed better in HDL, Postediting, Plagiarism data set, and WordNet performed better in Ans-Ans, Qus-Qus data set.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.9048349261283875}, {"text": "Plagiarism data set", "start_pos": 61, "end_pos": 80, "type": "DATASET", "confidence": 0.7612448930740356}, {"text": "WordNet", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9326030015945435}, {"text": "Qus-Qus data set", "start_pos": 123, "end_pos": 139, "type": "DATASET", "confidence": 0.7812056640783945}]}, {"text": "The reason maybe that training Word2vec uses all the English corpus of Wikipedia, and it can learn better word vectors.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.9354407787322998}, {"text": "English corpus of Wikipedia", "start_pos": 53, "end_pos": 80, "type": "DATASET", "confidence": 0.7919150143861771}]}, {"text": "WordNet can make full uses of lexical information to match the synonyms between two sentences.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9605253338813782}]}, {"text": "We also used SemEval 2017 evaluation data to test our system, and adding each kind of feature one by one.", "labels": [], "entities": [{"text": "SemEval 2017 evaluation data", "start_pos": 13, "end_pos": 41, "type": "DATASET", "confidence": 0.7467036619782448}]}, {"text": "The result of Pearson Correlation coefficients are shown in.", "labels": [], "entities": [{"text": "Pearson Correlation", "start_pos": 14, "end_pos": 33, "type": "METRIC", "confidence": 0.777644544839859}]}], "tableCaptions": [{"text": " Table 1: The Pearson Correlation on SemEval 2016 evaluation data sets.", "labels": [], "entities": [{"text": "Pearson Correlation on SemEval 2016 evaluation data sets", "start_pos": 14, "end_pos": 70, "type": "DATASET", "confidence": 0.7296313717961311}]}, {"text": " Table 2: parameter setting in SVR.", "labels": [], "entities": []}]}