{"title": [{"text": "Deep Learning Models For Multiword Expression Identification", "labels": [], "entities": [{"text": "Multiword Expression Identification", "start_pos": 25, "end_pos": 60, "type": "TASK", "confidence": 0.8409761786460876}]}], "abstractContent": [{"text": "Multiword expressions (MWEs) are lexical items that can be decomposed into multiple component words, but have properties that are unpredictable with respect to their component words.", "labels": [], "entities": [{"text": "Multiword expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7014237165451049}]}, {"text": "In this paper we propose the first deep learning models for token-level identification of MWEs.", "labels": [], "entities": [{"text": "token-level identification of MWEs", "start_pos": 60, "end_pos": 94, "type": "TASK", "confidence": 0.8559684604406357}]}, {"text": "Specifically, we consider a layered feed-forward network, a recurrent neural network , and convolutional neural networks.", "labels": [], "entities": []}, {"text": "In experimental results we show that con-volutional neural networks are able to out-perform the previous state-of-the-art for MWE identification, with a convolutional neural network with three hidden layers giving the best performance.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.9832013249397278}]}], "introductionContent": [{"text": "Multiword expressions (MWEs) are lexical items that can be decomposed into multiple component words, but have properties that are idiomatic, i.e., marked or unpredictable, with respect to properties of their component words (.", "labels": [], "entities": [{"text": "Multiword expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6899266898632049}]}, {"text": "MWEs include a wide range of phenomena such as noun compounds (e.g., speed limit and monkey business), verb-particle constructions (e.g., cleanup and throw out), and verb-noun idiomatic combinations (e.g., hit the roof and blow the whistle), as well as named entities (e.g., Prime Minister Justin Trudeau) and proverbs (e.g., Two wrongs don't make a right).", "labels": [], "entities": []}, {"text": "One particular challenge for natural language processing (NLP) is MWE identification -i.e., to identify which tokens in running text correspond to MWEs so that they can be analyzed accordingly.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.7859195669492086}, {"text": "MWE identification", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.9770926237106323}]}, {"text": "The challenges posed by MWEs have led to them to be referred to as a \"pain in the neck\" for NLP (); nevertheless, incorporating knowledge of MWEs into NLP applications can lead to improvements in tasks including machine translation, information retrieval (, and opinion mining).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 212, "end_pos": 231, "type": "TASK", "confidence": 0.8333073854446411}, {"text": "information retrieval", "start_pos": 233, "end_pos": 254, "type": "TASK", "confidence": 0.8311105668544769}, {"text": "opinion mining", "start_pos": 262, "end_pos": 276, "type": "TASK", "confidence": 0.79583540558815}]}, {"text": "Recent work on token-level MWE identification has focused on methods that are applicable to the full spectrum of kinds of MWEs (, in contrast to earlier work that tended to focus on specific kinds of MWEs (.", "labels": [], "entities": [{"text": "token-level MWE identification", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.8029136459032694}]}, {"text": "Deep learning is an emerging class of machine learning models that have recently achieved promising results on a range of NLP tasks such as machine translation (), named entity recognition (, natural language generation (, and sentence classification.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.7962817251682281}, {"text": "named entity recognition", "start_pos": 164, "end_pos": 188, "type": "TASK", "confidence": 0.6239451865355173}, {"text": "natural language generation", "start_pos": 192, "end_pos": 219, "type": "TASK", "confidence": 0.6693066358566284}, {"text": "sentence classification", "start_pos": 227, "end_pos": 250, "type": "TASK", "confidence": 0.7644597291946411}]}, {"text": "Such models have, however, not yet been applied to broad-coverage MWE identification.", "labels": [], "entities": [{"text": "broad-coverage MWE identification", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.7424029111862183}]}, {"text": "In this paper we propose the first deep learning models for broad-coverage MWE identification.", "labels": [], "entities": [{"text": "broad-coverage MWE identification", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.749769389629364}]}, {"text": "Specifically, we propose and evaluate a layered feedforward network, a recurrent neural network, and two convolutional neural networks.", "labels": [], "entities": []}, {"text": "We compare these models against the previous state-of-the-art () and several more-traditional supervised machine learning approaches.", "labels": [], "entities": []}, {"text": "We show that the convolutional neural networks outperform the previous state-of-the-art.", "labels": [], "entities": []}, {"text": "This finding is particularly remarkable given the relatively small size of the training data available, and demonstrates that deep learning models are able to learn well from small datasets.", "labels": [], "entities": []}, {"text": "Moreover, we show that our proposed deep learning models are able to generalize more-effectively than previous approaches, based on comparisons between the models' performances on validation and test data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the DiMSUM dataset ( for our experiments, which allows for direct comparison with previous results.", "labels": [], "entities": [{"text": "DiMSUM dataset", "start_pos": 11, "end_pos": 25, "type": "DATASET", "confidence": 0.8726193606853485}]}, {"text": "displays the source corpora from which the dataset was constructed; their domain (i.e., reviews, tweets, or TED talks); the number of sentences, words, MWEs, and gappy (i.e., discontiguous) MWES in each source corpus; and the percentage of tokens belonging to an MWE in each source corpus.", "labels": [], "entities": []}, {"text": "The dataset is split into training and testing sets such that the testing data contains a novel text type, i.e., TED talks.", "labels": [], "entities": []}, {"text": "For parameter tuning purposes, we also require validation data.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.734963595867157}]}, {"text": "We form a validation set from the training data by splitting the training data to create 5 folds, where every fold contained 20% validation data, and the remaining 80% was used for training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics describing the composition of the DiMSUM dataset.", "labels": [], "entities": [{"text": "DiMSUM dataset", "start_pos": 55, "end_pos": 69, "type": "DATASET", "confidence": 0.9008144438266754}]}]}