{"title": [{"text": "NRU-HSE at SemEval-2017 Task 4: Tweet Quantification Using Deep Learning Architecture", "labels": [], "entities": [{"text": "NRU-HSE", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9249011874198914}, {"text": "SemEval-2017 Task", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.5300111472606659}, {"text": "Tweet Quantification", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.9458236396312714}]}], "abstractContent": [{"text": "In many areas, such as social science, politics or market research, people need to deal with dataset shifting overtime.", "labels": [], "entities": []}, {"text": "Distribution drift phenomenon usually appears in the field of sentiment analysis, when proportions of instances are changing overtime.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.9524267315864563}]}, {"text": "In this case, the task is to correctly estimate proportions of each sentiment expressed in the set of documents (quantification task).", "labels": [], "entities": []}, {"text": "Basically, our study was aimed to analyze the effectiveness of a mixture of quantification technique with one of deep learning architecture.", "labels": [], "entities": []}, {"text": "All the techniques are evaluated using the SemEval-2017 Task4 dataset and source code, mentioned in this paper and available online in the Python programming language.", "labels": [], "entities": [{"text": "SemEval-2017 Task4 dataset", "start_pos": 43, "end_pos": 69, "type": "DATASET", "confidence": 0.8023820718129476}]}, {"text": "The results of an application of the quantification techniques are discussed .", "labels": [], "entities": []}], "introductionContent": [{"text": "A traditional classification task is often based on the assumption that data for training a classifier represent test data.", "labels": [], "entities": []}, {"text": "But in many areas, such as customer-relationship management or opinion mining, people need to deal with dataset shift or population drift phenomenon.", "labels": [], "entities": [{"text": "customer-relationship management", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.7902384996414185}, {"text": "opinion mining", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.7779805958271027}]}, {"text": "The simplest type of dataset shift is when training set and test set vary only in the distribution of the classes of the instances aka distribution drift.", "labels": [], "entities": []}, {"text": "If we would like to measure this variation, the task of accurate classification of each item is replaced by the task of providing accurate proportions of instances from each class (quantification).", "labels": [], "entities": []}, {"text": "George Forman suggested defining the 'quantification task' as finding the best estimate for the amount of cases in each class in a test set, using a training set with a substantially different class distribution.", "labels": [], "entities": []}, {"text": "Application of the quantification approach in opinion mining (, networkbehavior analysis, word-sense disambiguation), remote sensing (), quality control (, monitoring supportcall logs) and credit scoring) showed high performance even with a relatively small training set.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.8295941054821014}, {"text": "word-sense disambiguation", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.6864681839942932}]}, {"text": "Although quantification techniques are able to provide accurate sentiment analysis of proportions in situations of distribution drift, the question of an optimal technique for analysis of tweets still raises a lot of questions.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.793358564376831}]}, {"text": "It is worth mentioning that sentiment analysis of tweets presents additional challenges to natural language processing, because of the small amount of text (less than 140 characters in each document), usage of creative spelling (e.g. \"happpyyy\", \"some1 yg bner2 tulus\"), abbreviations (such as \"wth\" or \"lol\"), informal constructions (\"hahahaha yava quiet so !ma I m bored av even home nw\") and hashtags (BREAKING: US GDP growth is back!", "labels": [], "entities": [{"text": "sentiment analysis of tweets", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.9326818734407425}, {"text": "BREAKING", "start_pos": 405, "end_pos": 413, "type": "METRIC", "confidence": 0.9858323335647583}]}, {"text": "#kid-ding), which area type of tagging for Twitter messages.", "labels": [], "entities": []}, {"text": "We participated in D and E subtasks of the tweet sentiment quantification competition SemEval-2017 Task 4.", "labels": [], "entities": [{"text": "tweet sentiment quantification competition SemEval-2017 Task 4", "start_pos": 43, "end_pos": 105, "type": "TASK", "confidence": 0.7981309337275369}]}, {"text": "To solve them we used a quantification method, which showed good accuracy last year () and deep learning architecture mentioned in literature for text classification task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9988596439361572}, {"text": "text classification task", "start_pos": 146, "end_pos": 170, "type": "TASK", "confidence": 0.8727185924847921}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we first look at the notation, then we briefly overview a method to solve the quantification problem.", "labels": [], "entities": []}, {"text": "Section 3 describes a deep learning architec-ture and approach to train our network.", "labels": [], "entities": []}, {"text": "In Section 4 we show an experiment methodology.", "labels": [], "entities": []}, {"text": "Section 5 describes the results of our experiments, while Section 6 concludes the work defining open research issues for further investigation.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes our experimental setup for participation in the SemEval-2017 Task 4 called \"Sentiment Analysis in Twitter\".", "labels": [], "entities": [{"text": "SemEval-2017 Task 4", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.8620215455691019}, {"text": "Sentiment Analysis in Twitter", "start_pos": 99, "end_pos": 128, "type": "TASK", "confidence": 0.9249943196773529}]}, {"text": "Task 4 consists of five subtasks, but we only participated in topic-based message polarity quantification -subtasks D, E according to a two-point scale and five-point scale, respectively.", "labels": [], "entities": [{"text": "topic-based message polarity quantification", "start_pos": 62, "end_pos": 105, "type": "TASK", "confidence": 0.661255732178688}]}, {"text": "Its dataset consists of Twitter messages (aka observations) divided into several topics.", "labels": [], "entities": []}, {"text": "These subtasks are evaluated independently for different topics, and the final result is counted as an average of evaluation measure out of all the topics (.", "labels": [], "entities": []}, {"text": "For the quantification algorithm described in Section 2, we need to build a cost-sensitive classifier in the function build_clf.", "labels": [], "entities": []}, {"text": "The results of five point scale subtask are shown in.", "labels": [], "entities": []}, {"text": "During the development period, we compare our system with last year one on the last year dataset.", "labels": [], "entities": []}, {"text": "New system produced an EMD measure of 0.347 while last year system was slightly better -0.334.", "labels": [], "entities": [{"text": "EMD measure", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.8374470174312592}]}, {"text": "We explain this by the fact that dataset for network fine-tuning was relatively small last year.", "labels": [], "entities": []}, {"text": "This year training dataset is three times bigger, that is why we decide to submit results from the new version of the algorithm.", "labels": [], "entities": []}, {"text": "EMD of our new system on the new dataset is 0.317 while the best system scored 0.245.", "labels": [], "entities": [{"text": "EMD", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.997154951095581}]}, {"text": "The results of two-point scale subtask are shown in.", "labels": [], "entities": []}, {"text": "Our algorithm shows KLD equals to 0.078 while the best system is 0.036.", "labels": [], "entities": [{"text": "KLD", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9365569949150085}]}], "tableCaptions": [{"text": " Table 1: Results of Task 4E.", "labels": [], "entities": []}, {"text": " Table 2: Results of Task 4D.", "labels": [], "entities": []}]}