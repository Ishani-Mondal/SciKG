{"title": [], "abstractContent": [{"text": "We describe an end-to-end pipeline processing approach for SemEval 2017's Task 10 to extract keyphrases and their relations from scientific publications.", "labels": [], "entities": [{"text": "SemEval 2017's Task 10", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7699128031730652}]}, {"text": "We jointly identify and classify keyphrases by mod-eling the subtasks as sequential labeling.", "labels": [], "entities": []}, {"text": "Our system utilizes standard, surface-level features along with the adjacent word features , and performs conditional decoding on whole text to extract keyphrases.", "labels": [], "entities": []}, {"text": "We focus only on the identification and typing of keyphrases (Subtasks A and B, together referred as extraction), but provide an end-to-end system inclusive of keyphrase relation identification (Subtask C) for completeness.", "labels": [], "entities": [{"text": "keyphrase relation identification", "start_pos": 160, "end_pos": 193, "type": "TASK", "confidence": 0.620723158121109}]}, {"text": "Our top performing configuration achieves an F 1 of 0.27 for the end-to-end keyphrase extraction and relation identification scenario on the final test data, and compares on par to other top ranked systems for keyphrase extraction.", "labels": [], "entities": [{"text": "F 1", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9960049092769623}, {"text": "keyphrase extraction", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7445577830076218}, {"text": "relation identification", "start_pos": 101, "end_pos": 124, "type": "TASK", "confidence": 0.767626941204071}, {"text": "keyphrase extraction", "start_pos": 210, "end_pos": 230, "type": "TASK", "confidence": 0.8700012266635895}]}, {"text": "Our system outperforms other techniques that do not employ global decoding and hence do not account for dependencies between keyphrases.", "labels": [], "entities": []}, {"text": "We believe this is crucial for keyphrase classification in the given context of scientific document mining.", "labels": [], "entities": [{"text": "keyphrase classification", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.8421398997306824}, {"text": "scientific document mining", "start_pos": 80, "end_pos": 106, "type": "TASK", "confidence": 0.6377840836842855}]}], "introductionContent": [{"text": "Keyphrases are often used for representing the salient concepts of a document.", "labels": [], "entities": []}, {"text": "In scientific documents, keyphrase extraction is an important prerequisite task that feeds downstream tasks such as summarization, clustering and indexing, among others.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.8534989356994629}, {"text": "summarization", "start_pos": 116, "end_pos": 129, "type": "TASK", "confidence": 0.9794518947601318}]}, {"text": "As such, automatic keyphrase extraction has garnered attention and become a focal point for many researchers.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.7961594462394714}]}, {"text": "Usually, the most common scenario of keyphrase extraction is to identify the keyphrases over the whole scientific document.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.8724986910820007}]}, {"text": "Existing techniques in aforesaid setups use elaborate, hand-crafted features fed for selected candidate keyphrases to machine learning models such as support vector machines and multilayer perceptrons, to learn keyphrases (.", "labels": [], "entities": []}, {"text": "The scope of features vary from simple, surface-level features like character n-grams, token type, and part-of-speech tags -to features drawn from global statistics and lexical semantics, such as TF-IDF, keywordness, relation to document's logical structure.", "labels": [], "entities": []}, {"text": "However, the given task setup is inherently different as it requires to identify all the keyphrases of certain types (or classes -Material, Process and Task) over an excerpt of a scientific document.", "labels": [], "entities": []}, {"text": "As inferred from our primary analysis of the training data, some of the crucial challenges of keyphrase extraction in this particular task setup are: \u2022 Keyphrases occur more densely in the excerpts compared against the standard keyphrase extraction task where systems typically identify 5-25 keyphrases over an entire document; \u2022 Keyphrases overlap significantly.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 94, "end_pos": 114, "type": "TASK", "confidence": 0.8519361913204193}, {"text": "keyphrase extraction task", "start_pos": 228, "end_pos": 253, "type": "TASK", "confidence": 0.7742907305558523}]}, {"text": "For example \"equally sized blocks\" and \"blocks\" both need to be extracted as keyphrases of type Materials; \u2022 Determining the keyphrase type depends on the context.", "labels": [], "entities": []}, {"text": "For example \"oxidation test\" and \"assessment of the corrosion condition\" can potentially be either of Task or Process, depending on the context.", "labels": [], "entities": []}, {"text": "Considering these differences, we believe that sequence labeling based modeling is more suited than the standard, top K keyphrase extraction.", "labels": [], "entities": [{"text": "sequence labeling based modeling", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.7502744346857071}]}, {"text": "Such a model also easily extends to a joint approach for both extraction and classification, which we investigated.", "labels": [], "entities": [{"text": "extraction and classification", "start_pos": 62, "end_pos": 91, "type": "TASK", "confidence": 0.6349439024925232}]}], "datasetContent": [{"text": "We use the CRF++ implementation 1 of CRFs.", "labels": [], "entities": []}, {"text": "CRF++ takes as input a feature template file, describing contextual positions (like previous token, next two tokens) to incorporate component features from (F0 -F18, O).", "labels": [], "entities": [{"text": "CRF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9157242178916931}, {"text": "F0 -F18", "start_pos": 157, "end_pos": 164, "type": "METRIC", "confidence": 0.5028234620889028}]}, {"text": "The template also can direct CRF++ to compute more complex featurebigrams.", "labels": [], "entities": []}, {"text": "Our final model's expanded feature list includes many of the surrounding tokens features 2 . Dataset.", "labels": [], "entities": []}, {"text": "We participate in SemEval-2017 Task 10 on science information extraction () using the dataset consisting of 350 training samples (Train), 50 development samples (Dev) and 100 testing samples (Test).", "labels": [], "entities": [{"text": "SemEval-2017 Task 10", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.7224431236584982}, {"text": "science information extraction", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.6186925371487936}]}, {"text": "Each data sample is an excerpt of a scientific document.", "labels": [], "entities": []}, {"text": "Unlike previous work creating similar benchmark dataset), the dataset excerpt is taken out of a random section of the document.", "labels": [], "entities": []}, {"text": "The excerpt requires its keyphrases to be identified (Subtask A), typed among one of three types: Materials, Process and Task (Subtask B), then finally followed by Synonym-of and Hyponym-of identification among the extracted keyphrases (Subtask C).", "labels": [], "entities": []}, {"text": "Our work focuses specifically only on Subtasks A and B; our effort on Subtask C is minimal.", "labels": [], "entities": []}, {"text": "The designed evaluations test complete end-to-end pipeline of keyphrase identification, classification and relationship identification (Scenario 1); classification and relationship identification, given extracted keyphrases (Scenario 2); and relationship identification, given the extracted and classified keyphrases (Scenario 3).", "labels": [], "entities": [{"text": "keyphrase identification, classification and relationship identification", "start_pos": 62, "end_pos": 134, "type": "TASK", "confidence": 0.6735036330563682}, {"text": "classification and relationship identification", "start_pos": 149, "end_pos": 195, "type": "TASK", "confidence": 0.6467059999704361}, {"text": "relationship identification", "start_pos": 242, "end_pos": 269, "type": "TASK", "confidence": 0.8156959414482117}]}, {"text": "As the most intuitive and challenging scope, we only examine Scenario 1 in depth here.", "labels": [], "entities": []}, {"text": "In Scenario 1 all the tasks are performed over the (noisy) system output of previous tasks whenever applicable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Model performance over different feature ablation, as evaluated on Dev. Best performance is  bolded.", "labels": [], "entities": []}, {"text": " Table 2: Subtask A performance for Joint versus  Unified models, as assessed on Dev. Best perfor- mance is bolded.", "labels": [], "entities": [{"text": "Dev", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.8970682621002197}]}, {"text": " Table 3: Subtask B performance for Joint versus  Individual models, as assessed on Dev. Best per- formance is bolded.", "labels": [], "entities": [{"text": "Dev", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.940388560295105}]}, {"text": " Table 4: Scores for evaluation on Test. Paren- thetical numbers give differences from Dev per- formance.", "labels": [], "entities": [{"text": "Paren- thetical numbers", "start_pos": 41, "end_pos": 64, "type": "METRIC", "confidence": 0.9404473900794983}]}, {"text": " Table 5: Type Count and Percentages.", "labels": [], "entities": [{"text": "Type Count", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8956821262836456}]}, {"text": " Table 6: Official scores on Subtask evaluations.", "labels": [], "entities": []}]}