{"title": [{"text": "SemEval 2017 Task 10: ScienceIE -Extracting Keyphrases and Relations from Scientific Publications", "labels": [], "entities": [{"text": "Extracting Keyphrases and Relations from Scientific Publications", "start_pos": 33, "end_pos": 97, "type": "TASK", "confidence": 0.7996007629803249}]}], "abstractContent": [{"text": "We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials.", "labels": [], "entities": [{"text": "SemEval task of extracting keyphrases and relations between them from scientific documents", "start_pos": 16, "end_pos": 106, "type": "TASK", "confidence": 0.5444572667280833}]}, {"text": "Although this was anew task, we had a total of 26 submissions across 3 evaluation scenarios.", "labels": [], "entities": []}, {"text": "We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 188, "end_pos": 210, "type": "TASK", "confidence": 0.7000045925378799}]}], "introductionContent": [{"text": "Empirical research requires gaining and maintaining an understanding of the body of work in specific area.", "labels": [], "entities": []}, {"text": "For example, typical questions researchers face are which papers describe which tasks and processes, use which materials and how those relate to one another.", "labels": [], "entities": []}, {"text": "While there are review papers for some areas, such information is generally difficult to obtain without reading a large number of publications.", "labels": [], "entities": []}, {"text": "Current efforts to address this gap are search engines such as Google Scholar, 1 Scopus 2 or Semantic Scholar, 3 which mainly focus on navigating author and citations graphs.", "labels": [], "entities": []}, {"text": "The task tackled here is mention-level identification and classification of keyphrases, e.g. Keyphrase Extraction (TASK), as well as extracting semantic relations between keywords, e.g. Keyphrase Extraction HYPONYM-OF Information Extraction.", "labels": [], "entities": [{"text": "mention-level identification and classification of keyphrases", "start_pos": 25, "end_pos": 86, "type": "TASK", "confidence": 0.7000654886166254}, {"text": "Keyphrase Extraction (TASK)", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.7550525546073914}, {"text": "Keyphrase Extraction HYPONYM-OF Information Extraction", "start_pos": 186, "end_pos": 240, "type": "TASK", "confidence": 0.5296055912971497}]}, {"text": "These tasks are related to the tasks of named entity recognition, named entity classification and relation extraction.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6157047351201376}, {"text": "named entity classification", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.6428138514359792}, {"text": "relation extraction", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.8181498944759369}]}, {"text": "However, keyphrases are much more challenging to identify than e.g. person names, since they vary significantly between domains, lack clear signifiers and contexts and can consist of many tokens.", "labels": [], "entities": []}, {"text": "For this purpose, a double-annotated corpus of 500 publications with mention-level annotations was produced, consisting of scientific articles of the Computer Science, Material Sciences and Physics domains.", "labels": [], "entities": []}, {"text": "Extracting keyphrases and relations between them is of great interest to scientific publishers as it helps to recommend articles to readers, highlight missing citations to authors, identify potential reviewers for submissions, and analyse research trends overtime.", "labels": [], "entities": [{"text": "Extracting keyphrases and relations between", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8418645858764648}]}, {"text": "Note that organising keyphrases in terms of synonym and hypernym relations is particularly useful for search scenarios, e.g. a reader may search for articles on information extraction, and through hypernym prediction would also receive articles on named entity recognition or relation extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 161, "end_pos": 183, "type": "TASK", "confidence": 0.728690966963768}, {"text": "hypernym prediction", "start_pos": 197, "end_pos": 216, "type": "TASK", "confidence": 0.7135061472654343}, {"text": "named entity recognition or relation extraction", "start_pos": 248, "end_pos": 295, "type": "TASK", "confidence": 0.633477250734965}]}, {"text": "We expect the outcomes of the task to be relevant to the wider information extraction, knowledge base population and knowledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.7725992500782013}]}, {"text": "Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 166, "end_pos": 188, "type": "TASK", "confidence": 0.7696600556373596}]}, {"text": "Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (, semantic relation extraction (, topic classification of scientific articles), citation context extraction, extracting author and citation graphs) or a combination of those.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 133, "end_pos": 153, "type": "TASK", "confidence": 0.6988738179206848}, {"text": "semantic relation extraction", "start_pos": 157, "end_pos": 185, "type": "TASK", "confidence": 0.6760640343030294}, {"text": "topic classification of scientific articles)", "start_pos": 189, "end_pos": 233, "type": "TASK", "confidence": 0.8415078123410543}, {"text": "citation context extraction", "start_pos": 235, "end_pos": 262, "type": "TASK", "confidence": 0.7365016539891561}]}, {"text": "The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of anew corpus, leading to novel research methods for information extraction from scientific documents.", "labels": [], "entities": [{"text": "information extraction from scientific documents", "start_pos": 180, "end_pos": 228, "type": "TASK", "confidence": 0.8425685942173005}]}, {"text": "What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym relation resources are on type-level, e.g. WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 203, "end_pos": 210, "type": "DATASET", "confidence": 0.9444257616996765}]}, {"text": "Further, we expect that these methods will directly impact industrial solutions to making sense of publications, partly due to the task organisers' collaboration with Elsevier.", "labels": [], "entities": []}], "datasetContent": [{"text": "SemEval 2017 Task 10 offers three different evaluation scenarios: 1) Only plain text is given (Subtasks A, B, C).", "labels": [], "entities": [{"text": "SemEval 2017 Task 10", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.7090655863285065}]}, {"text": "2) Plain text with manually annotated keyphrase boundaries are given (Subtasks B, C).", "labels": [], "entities": []}, {"text": "3) Plain text with manually annotated keyphrases and their types are given (Subtask C).", "labels": [], "entities": []}, {"text": "We refer to the above scenarios as Scenario 1, Scenario 2, and Scenario 3 respectively.", "labels": [], "entities": []}, {"text": "In this scenario teams need to solve all three subtasks A, B, and C; where no annotation information was given.", "labels": [], "entities": []}, {"text": "Some teams participated only in Subtask A, or B; but the overall micro F1 performance across subtasks is considered for the ranking of the teams.", "labels": [], "entities": [{"text": "F1", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.9440938234329224}]}, {"text": "Seventeen teams participated in this scenario.", "labels": [], "entities": []}, {"text": "The F1 scores range from 0.04 to 0.43.", "labels": [], "entities": [{"text": "F1", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9994497895240784}]}, {"text": "Complete results are given in.", "labels": [], "entities": []}, {"text": "Various different types of methods have been applied by different teams with various levels of supervision.", "labels": [], "entities": []}, {"text": "The best three teams TTI COIN, TIAL UW, and s2 end2end have used recurrent neural network (RNN) based approaches to obtain F1 scores of 0.38, 0.42 and 0.43 respectively.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9798630774021149}]}, {"text": "However, TIAL UW, and s2 end2end, by using a conditional random fields (CRF) layer on top of RNNs achieve a higher F1 in Subtask A compared to TTI COIN.", "labels": [], "entities": [{"text": "TIAL UW", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.597676083445549}, {"text": "F1", "start_pos": 115, "end_pos": 117, "type": "METRIC", "confidence": 0.999334990978241}]}, {"text": "The fourth team PKU ICL with an F1 of 0.37 found classification models based on random forest and support vector machines (SVM) useful with carefully engineered feature such as TF-IDF over a very large external corpus, IDF weighted word-embeddings etc, along with an existing taxonomy.", "labels": [], "entities": [{"text": "PKU ICL", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.8362758159637451}, {"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9954943656921387}]}, {"text": "SciX on the other hand used noun phrase chunking and trained an SVM classifier on provided training data to classify phrases, and used a CRF to predict labels of the phrases.", "labels": [], "entities": [{"text": "noun phrase chunking", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.6896400054295858}]}, {"text": "CRF based methods with parts-of-speech (POS) tagging and orthographic features such as presence of symbols and capitalisation have been tried by several teams (NTNU, SZTE-NLP, WING-NUS) and they leading to a reasonable performance (  former is surprising, as keyphrases are with an overwhelming majority noun phrases, the latter not as much, many keyphrases only appear once in the dataset (see).", "labels": [], "entities": [{"text": "NTNU", "start_pos": 160, "end_pos": 164, "type": "DATASET", "confidence": 0.9610671997070312}]}, {"text": "GMBUAP further tried using empirical rules obtained by observing the training data for Subtask A, and a Naive Bayes classifier trained on provided training data for Subtask B. Such simple methods on their own prove not to be accurate enough.", "labels": [], "entities": [{"text": "GMBUAP", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9849978089332581}]}, {"text": "Attempts of such give us additional insight about the hardness of the problem and applicability of simple methods to the task.", "labels": [], "entities": []}, {"text": "In this scenario teams needed to solve sub-tasks B, and C.", "labels": [], "entities": []}, {"text": "Partial annotation was provided to the teams, that is, solution to the Subtask A. Four teams participated in this scenario with F1 cores ranging from 0.43 to 0.64.", "labels": [], "entities": [{"text": "F1", "start_pos": 128, "end_pos": 130, "type": "METRIC", "confidence": 0.9742516875267029}]}, {"text": "Please refer to for complete result.", "labels": [], "entities": []}, {"text": "Except MayoNLP, other three teams participated only in Subtask B. Although ranking is done based on overall performance, but in this scenario After the end of the evaluation period, team UKP/EELECTION discovered those results were based on training on the development set.", "labels": [], "entities": [{"text": "MayoNLP", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9472944736480713}, {"text": "UKP", "start_pos": 187, "end_pos": 190, "type": "DATASET", "confidence": 0.6375681161880493}]}, {"text": "For training on the training set, their results are: 0.69 F1 overall and 0.72 F1 for Subtask B only rankings are consistent in each category.", "labels": [], "entities": [{"text": "F1", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.9939168095588684}, {"text": "F1", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9960015416145325}]}, {"text": "BUAP with the worst F1 score for Subtask B (0.45), is still better than the best team in Scenario 1 s2 end2end for Subtask B (0.44).", "labels": [], "entities": [{"text": "BUAP", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7974603176116943}, {"text": "F1 score", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9869876801967621}]}, {"text": "Partial annotation or accuracy for Subtask A proves to be critical, reinforcing again that identifying keyphrase boundaries is the most difficult part of the shared task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9987379908561707}]}, {"text": "Unlike the Scenario 1, in this case the top two teams used classifiers with lexical features (F1: 0.64) as well as neural networks (F1: 0.63).", "labels": [], "entities": [{"text": "F1", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.99212646484375}, {"text": "F1", "start_pos": 132, "end_pos": 134, "type": "METRIC", "confidence": 0.9472035765647888}]}, {"text": "The first team MayoNLP used SVM with rich feature sets like n-grams, lexical features, orthographic features, whereas the second team UKP/EELECTION used used three different neural network approaches and subsequently combined them via majority voting.", "labels": [], "entities": [{"text": "MayoNLP", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.975963830947876}, {"text": "UKP/EELECTION", "start_pos": 134, "end_pos": 147, "type": "DATASET", "confidence": 0.8328165809313456}]}, {"text": "Both these methods perform quite similarly.", "labels": [], "entities": []}, {"text": "However, a CRF based approach and an SVM with simpler feature sets attempted by the two teams LABDA and BUAP are found to be less effective in this scenario.", "labels": [], "entities": [{"text": "LABDA", "start_pos": 94, "end_pos": 99, "type": "DATASET", "confidence": 0.7804437875747681}, {"text": "BUAP", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.827995777130127}]}, {"text": "MayoNLP applied a simple rule based method for synonym-of relation extraction, and Hearst patterns for hyponym-of relation detection.", "labels": [], "entities": [{"text": "MayoNLP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.971407413482666}, {"text": "synonym-of relation extraction", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.7527098059654236}, {"text": "relation detection", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.7230041325092316}]}, {"text": "The rules for synonym-of detection is based on presence of phrases such as in terms of, equivalently,   which are called etc in the text between two keyphrases.", "labels": [], "entities": [{"text": "synonym-of detection", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.9542917907238007}]}, {"text": "Interestingly, the RNN based approach of s2 end2end in Scenario 1 performs better than MayoNLP without using partial annotation of Subtask A.  In this scenario, teams need to solve only Subtask C. Partial annotations were provided to the teams for Subtask B and C.", "labels": [], "entities": [{"text": "MayoNLP", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.8590807318687439}]}, {"text": "Five teams participated in this scenario, and F1 scores ranged from 0.1 to 0.64.", "labels": [], "entities": [{"text": "F1", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9994078874588013}]}, {"text": "Please refer to for complete result.", "labels": [], "entities": []}, {"text": "Neural network (NN) based models are found to perform better than other methods in this scenario.", "labels": [], "entities": []}, {"text": "The best method by MIT uses a convolutional NN (CNN).", "labels": [], "entities": []}, {"text": "The other method uses two phases of NN and found to be reasonably effective (F1: 0.54).", "labels": [], "entities": [{"text": "F1", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9993067979812622}]}, {"text": "On the other hand, application of supervised classification with five different classifiers (SVM, decision tree, random forest, multinomial naive After the end of the evaluation period, team TTI COIN rel discovered a bug in preprocessing, leading to low results.", "labels": [], "entities": []}, {"text": "Their overall result after having corrected for that error is a Macro F1 of 0.48.", "labels": [], "entities": [{"text": "Macro F1", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.8009984493255615}]}, {"text": "Bayes and k-nearest neighbour) using three different feature selection techniques (chi square, decision tree, and recursive feature elimination) found close accuracy (F1: 0.5) with the top performing ones.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.999107301235199}, {"text": "F1", "start_pos": 167, "end_pos": 169, "type": "METRIC", "confidence": 0.9942618608474731}]}, {"text": "LaBDA also use a CNN based method.", "labels": [], "entities": [{"text": "LaBDA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9543036818504333}]}, {"text": "However, the rule based post-processing and argument ordering strategy applied by MIT seemed to give additional advantage as also observed by them.", "labels": [], "entities": [{"text": "argument ordering", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7728939652442932}]}, {"text": "However most of the teams in this scenario outperform, all teams from other scenarios (who did not have access to partial information for Subtask B, and C) in relation prediction.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.9503829181194305}]}, {"text": "This also asserts the significance of accuracy on Subtask A, and B in order to perform accurately on Subtask C.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.999241828918457}]}], "tableCaptions": [{"text": " Table 1: Characteristics of SemEval 2017 Task 10 dataset, statistics of training sets", "labels": [], "entities": [{"text": "SemEval 2017 Task 10 dataset", "start_pos": 29, "end_pos": 57, "type": "DATASET", "confidence": 0.792582654953003}]}, {"text": " Table 3: F1 scores of teams participating in Scenario 1 and baseline models for Overall, Subtask A,  Subtask B, and Subtask C. Ranking of the teams is based on overall performance measured in Micro F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9989326596260071}, {"text": "Micro F1", "start_pos": 193, "end_pos": 201, "type": "DATASET", "confidence": 0.8841304779052734}]}, {"text": " Table 4: F1 scores of teams participating in Scenario 2 and baseline models for Overall, Subtask B,  and Subtask C. Ranking of the teams is based on overall performance measured in Micro F1. Teams  participating in Scenario 2 received partial annotation with respect to Subtask A.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9992225170135498}]}, {"text": " Table 5: F1 scores of teams participating in Sce- nario 3 and baseline models. Teams participating  in Scenario 3 received partial annotation with re- spect to Subtask A, and Subtask B. Ranking of the  teams is based on overall performance measured  in Micro F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9988138675689697}, {"text": "Micro F1", "start_pos": 254, "end_pos": 262, "type": "DATASET", "confidence": 0.8641293048858643}]}]}