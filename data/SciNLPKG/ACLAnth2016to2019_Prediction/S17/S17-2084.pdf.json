{"title": [{"text": "Mama Edha at SemEval-2017 Task 8: Stance Classification with CNN and Rules", "labels": [], "entities": [{"text": "SemEval-2017 Task 8", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7455901900927225}, {"text": "Stance Classification", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.983757883310318}, {"text": "CNN", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.980855405330658}]}], "abstractContent": [{"text": "For the competition SemEval-2017 we investigated the possibility of performing stance classification (support, deny, query or comment) for messages in Twitter conversation threads related to rumours.", "labels": [], "entities": [{"text": "stance classification (support, deny, query or comment) for messages in Twitter conversation threads related to rumours", "start_pos": 79, "end_pos": 198, "type": "TASK", "confidence": 0.6165120780467988}]}, {"text": "Stance classification is interesting since it can provide a basis for rumour veracity assessment.", "labels": [], "entities": [{"text": "Stance classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9375533163547516}, {"text": "rumour veracity assessment", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.8877489964167277}]}, {"text": "Our ensemble classification approach of combining convolutional neural networks with both automatic rule mining and manually written rules achieved a final accuracy of 74.9% on the compe-tition's test data set for Task 8A.", "labels": [], "entities": [{"text": "ensemble classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.6846087127923965}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9654771685600281}, {"text": "compe-tition's test data set", "start_pos": 181, "end_pos": 209, "type": "DATASET", "confidence": 0.8401751399040223}]}, {"text": "To improve classification we also experimented with data relabeling and using the grammatical structure of the tweet contents for classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.9611348509788513}]}], "introductionContent": [{"text": "The task of determining the veracity of a rumour is sometimes a difficult one, even with the reasoning power of a human being.", "labels": [], "entities": [{"text": "determining the veracity of a rumour", "start_pos": 12, "end_pos": 48, "type": "TASK", "confidence": 0.841734915971756}]}, {"text": "This paper presents an approach to an automatic analysis of discussion elements with respect to rumours.", "labels": [], "entities": [{"text": "automatic analysis of discussion elements", "start_pos": 38, "end_pos": 79, "type": "TASK", "confidence": 0.7072182595729828}]}, {"text": "Discussion structure and analysis can well play apart in a broader effort to assess rumour veracity, and the expectation is that the results presented here is one step of the way towards that end goal.", "labels": [], "entities": []}, {"text": "The research presented in this paper is a submission to SemEval-2017, Task 8 (RumourEval: Determining rumour veracity and support for rumours), Subtask A (SDQC) (.", "labels": [], "entities": []}, {"text": "The objective of this subtask is to classify the relation between a tweet and the rumour it is related to in terms of support, deny, query or comment.", "labels": [], "entities": []}, {"text": "Our approach to this classification task is building three different classifiers and combining the predictions in an ensemble method.", "labels": [], "entities": []}, {"text": "The general idea is that different types of classifiers may learn different concepts and hence complement each other, resulting in a better prediction capability for the joint classifier.", "labels": [], "entities": []}, {"text": "Furthermore we tested the accuracy in applying our ensemble approach to both originally labeled and relabeled data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9994366765022278}]}, {"text": "The remainder of this paper is organized as follows: Section 2 describes the data given in the task and our observations on irregularities in the data labeling.", "labels": [], "entities": []}, {"text": "Section 3 contains a description of the process and used methods employed for the experiments.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 describe the results and discusses the findings.", "labels": [], "entities": []}, {"text": "Finally, we conclude the work in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results on test data set, comparison between classifiers. Subscript R after the classifier name  stands for \"Relabeled\" and subscript G stands for \"Grammatical Representation\". The best result in each  column is marked with bold face.", "labels": [], "entities": [{"text": "Relabeled", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.954624354839325}]}]}