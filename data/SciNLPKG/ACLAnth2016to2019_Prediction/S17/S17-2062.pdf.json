{"title": [{"text": "Talla at SemEval-2017 Task 3: Identifying Similar Questions Through Paraphrase Detection", "labels": [], "entities": [{"text": "Identifying Similar Questions", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.9054242571194967}, {"text": "Paraphrase Detection", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.8466179966926575}]}], "abstractContent": [{"text": "This paper describes our approach to the SemEval-2017 shared task of determining question-question similarity in a community question-answering setting (Task 3B).", "labels": [], "entities": [{"text": "SemEval-2017 shared task", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.8755502502123514}]}, {"text": "We extracted both syntactic and semantic similarity features between candidate questions, performed pairwise-preference learning to optimize for ranking order, and then trained a random forest classifier to predict whether the candidate questions were paraphrases of each other.", "labels": [], "entities": []}, {"text": "This approach achieved a MAP of 45.7% out of max achievable 67.0% on the test set.", "labels": [], "entities": [{"text": "MAP", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9995754361152649}]}], "introductionContent": [{"text": "A large amount of information of interest to users of community forums is stored in semi-structured text, but surfacing that information can be challenging given the variety of ways users can phrase their search queries.", "labels": [], "entities": []}, {"text": "Question-answering is a significant task for both natural language processing (NLP) and information retrieval (IR), as both the actual terms used in the query plus the semantic intent of the query itself need to be accounted for in surfacing relevant potential answers.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.8025210698445638}, {"text": "information retrieval (IR)", "start_pos": 88, "end_pos": 114, "type": "TASK", "confidence": 0.8547213673591614}]}, {"text": "The Community Question Answering (cQA) task of) seeks to address this problem through several related subtasks around effectively determining and ranking the relevance of related stored questions and associated answers.", "labels": [], "entities": [{"text": "Community Question Answering (cQA) task", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.7827560220445905}]}, {"text": "We chose to focus on subtask B: questionquestion similarity.", "labels": [], "entities": [{"text": "questionquestion similarity", "start_pos": 32, "end_pos": 59, "type": "METRIC", "confidence": 0.7054774463176727}]}, {"text": "This problem can be seen as one of paraphrase detection -determine if two questions have the same meaning.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.8896942734718323}]}, {"text": "We reviewed existing performant paraphrase detection methods and selected several to implement and ensemble (; along with the related question IR system rank provided in the dataset.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.7585646510124207}, {"text": "IR system rank", "start_pos": 143, "end_pos": 157, "type": "METRIC", "confidence": 0.8672550121943156}]}, {"text": "As paraphrase detection is a classification problem while subtask B is a ranking problem, we also incorporated pairwisepreference learning to aid in improving the key metric of mean average precision (MAP).", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.9065716862678528}, {"text": "mean average precision (MAP)", "start_pos": 177, "end_pos": 205, "type": "METRIC", "confidence": 0.9286003609498342}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides a detailed description of our system, including the key identified features that were extracted, while Section 3 provides the results from experiments used to evaluate the system.", "labels": [], "entities": []}, {"text": "Section 4 concludes the paper with a summary of the work and directions for future exploration.", "labels": [], "entities": []}], "datasetContent": [{"text": "We combined the provided training and dev datasets as our system training set and used the provided SemEval-2016 test data with gold labels as our test set.", "labels": [], "entities": [{"text": "SemEval-2016 test data", "start_pos": 100, "end_pos": 122, "type": "DATASET", "confidence": 0.7921733061472574}]}, {"text": "No additional external data, other than pre-trained word embeddings, were used.", "labels": [], "entities": []}, {"text": "We evaluated different classifier hyperparameters using 10-fold cross-validation and ultimately chose a random forest classifier with 2000 trees as our final model.", "labels": [], "entities": []}, {"text": "This system achieved fourth place overall (Table 1) on the SemEval-2017 test dataset, and while both contrastive submissions placed higher than the primary, nether was able to achieve a greater MAP than the third place entry.", "labels": [], "entities": [{"text": "SemEval-2017 test dataset", "start_pos": 59, "end_pos": 84, "type": "DATASET", "confidence": 0.8879151741663615}, {"text": "MAP", "start_pos": 194, "end_pos": 197, "type": "METRIC", "confidence": 0.9986284971237183}]}, {"text": "Contrastive1 was identical in feature set to the primary submission, but included the SemEval-2016 test dataset as part of the training data, suggesting that MAP can be improved by increasing the amount of examples used to train the system.", "labels": [], "entities": [{"text": "Contrastive1", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8336237072944641}, {"text": "SemEval-2016 test dataset", "start_pos": 86, "end_pos": 111, "type": "DATASET", "confidence": 0.8649699687957764}, {"text": "MAP", "start_pos": 158, "end_pos": 161, "type": "METRIC", "confidence": 0.58628249168396}]}, {"text": "Contrastive2 did not include the extra data and also omitted the TF-KLD features.", "labels": [], "entities": [{"text": "Contrastive2", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9548322558403015}]}, {"text": "Comparing the effects of ablating the other individual features.", "labels": [], "entities": []}, {"text": "The train + dev dataset we used for general training was more closely aligned with the distribution of class labels in 2016 than in 2017, suggesting a potential i.i.d. data dependence on this approach to produce good results on test data.: Distribution of the PerfectMatch (PM), Relevant (R), and Irrelevant (I) classes within the datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: System performance on the SemEval-2017 test dataset", "labels": [], "entities": [{"text": "SemEval-2017 test dataset", "start_pos": 36, "end_pos": 61, "type": "DATASET", "confidence": 0.7757921616236368}]}, {"text": " Table 3: Distribution of the PerfectMatch (PM),  Relevant (R), and Irrelevant (I) classes within the  datasets.", "labels": [], "entities": []}]}