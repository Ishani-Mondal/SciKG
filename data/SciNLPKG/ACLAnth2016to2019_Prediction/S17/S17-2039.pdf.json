{"title": [{"text": "HHU at SemEval-2017 Task 2: Fast Hash-Based Embeddings for Semantic Word Similarity Assessment", "labels": [], "entities": [{"text": "HHU", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7447748184204102}, {"text": "SemEval-2017 Task", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.6642896234989166}, {"text": "Semantic Word Similarity Assessment", "start_pos": 59, "end_pos": 94, "type": "TASK", "confidence": 0.7650432735681534}]}], "abstractContent": [{"text": "This paper describes the HHU system that participated in Task 2 of SemEval 2017, Multilingual and Cross-lingual Semantic Word Similarity.", "labels": [], "entities": [{"text": "SemEval 2017", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.8673179745674133}, {"text": "Cross-lingual Semantic Word Similarity", "start_pos": 98, "end_pos": 136, "type": "TASK", "confidence": 0.565377488732338}]}, {"text": "We introduce our un-supervised embedding learning technique and describe how it was employed and configured to address the problems of monolingual and multilingual word similarity measurement.", "labels": [], "entities": [{"text": "multilingual word similarity measurement", "start_pos": 151, "end_pos": 191, "type": "TASK", "confidence": 0.6651218980550766}]}, {"text": "This paper reports from empirical evaluations on the benchmark provided by the task's organizers.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of Task 2 of SemEval-2017 is to provide a reliable benchmark for the evaluation of monolingual and multilingual semantic representations).", "labels": [], "entities": []}, {"text": "The proposed evaluation benchmark goes beyond classic semantic relatedness tests by providing both monolingual and cross-lingual data sets that include multiword expressions, domain-specific terms, and named entities for five languages.", "labels": [], "entities": []}, {"text": "To measure 'semantic similarity' between pairs of lexical items, the HHU system uses the algorithm proposed in (, which is based on a derandomization of the 'random positive-only projections' method proposed by.", "labels": [], "entities": [{"text": "HHU", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.8100677728652954}]}, {"text": "Word embedding techniques (i.e., using distributional frequencies to produce word vectors of reduced dimensionality) are one of the most popular approaches to semantic word similarity problems.", "labels": [], "entities": [{"text": "semantic word similarity problems", "start_pos": 159, "end_pos": 192, "type": "TASK", "confidence": 0.791606143116951}]}, {"text": "These methods are often rationalized using Harris' Distributional Hypothesis that words of similar linguistic properties appear with/within a similar set of 'contexts'.", "labels": [], "entities": []}, {"text": "For example, words of related meanings co-occur with similar context words {c 1 , . .", "labels": [], "entities": []}, {"text": "This hypothesis implies that if these context words are grouped randomly into m buckets, e.g. {{c . .", "labels": [], "entities": []}, {"text": "c x } 1 , . .", "labels": [], "entities": []}, {"text": ", {c y , . .", "labels": [], "entities": []}, {"text": "c n } m }, then co-related words still co-occur with similar sets of buckets.", "labels": [], "entities": []}, {"text": "exploit this assumption and propose random positive-only projections for building word vectors directly at a reduced dimensionality m.", "labels": [], "entities": []}, {"text": "In this paper, we propose a derandomization of this method and a hashbased technique for learning word embeddings.", "labels": [], "entities": []}, {"text": "In Section 2, we describe our method.", "labels": [], "entities": []}, {"text": "In Section 3, we report results obtained by applying this method to the shared-task benchmark.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "While our official submissions are limited to English and Farsi, to provide a better understanding of the method's performance, we provide results for all the five languages in the monolingual subtask.", "labels": [], "entities": []}, {"text": "To build models, we use the feature sets described in the previous section.", "labels": [], "entities": []}, {"text": "The remaining hyper-parameter of our method ism (the dimensionality of models); we report results form \u2208 {300, 700, 2000}.", "labels": [], "entities": []}, {"text": "Results obtained using various: Results for vectors of various dimensionality (denoted by dim), and when using PPMI for weighting and Pearson's r for measuring similarity between them.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 134, "end_pos": 145, "type": "METRIC", "confidence": 0.838300625483195}]}, {"text": "H denotes the harmonic mean of rand \u03c1 (i.e., the task's official score).", "labels": [], "entities": [{"text": "rand \u03c1", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.8696891963481903}]}, {"text": "#M is the number of lexical items which have not occurred in our input corpora; for pairs containing these items, we use 0 as a default value for similarity.", "labels": [], "entities": []}, {"text": "Those settings that yield better results than the baseline are marked using \u2191.", "labels": [], "entities": []}, {"text": "combinations of weighting techniques and similarity measure are summarized in to 7.", "labels": [], "entities": [{"text": "similarity measure", "start_pos": 41, "end_pos": 59, "type": "METRIC", "confidence": 0.9779534041881561}]}, {"text": "Disregarding the choice of weighting technique and similarity measure, an increase in m often produces better results, but at the expense of higher computational cost.", "labels": [], "entities": []}, {"text": "In addition, as suggested in Section 2.1, by comparing results between Table 2 to 4 and     only exception is when m is small (e.g., m = 300) and \u03b3 is used to measure similarities.", "labels": [], "entities": []}, {"text": "For small m = 300, this combination of PPMI weighting and \u03b3 gives the best performance; we witness that form = 300, this combination also gives the best results for Camacho-Collados et al.'s data sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for our official submissions.", "labels": [], "entities": []}, {"text": " Table 2: Results for vectors of various dimensionality (denoted by dim), and when using PPMI for  weighting and Pearson's r for measuring similarity between them. H denotes the harmonic mean of r  and \u03c1 (i.e., the task's official score). #M is the number of lexical items which have not occurred in our  input corpora; for pairs containing these items, we use 0 as a default value for similarity. Those settings  that yield better results than the baseline are marked using \u2191.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 113, "end_pos": 124, "type": "METRIC", "confidence": 0.8806147376696268}]}, {"text": " Table 3: Method's performance when using PPMI  for weighting and Goodman and Kruskal's \u03b3 for  a similarity measurement. This combination gives  the best performance for models of small dimen- sionality such as m = 300.", "labels": [], "entities": []}, {"text": " Table 4: Method's performance when using the  combination of PPMI and sim lin .", "labels": [], "entities": []}, {"text": " Table 5: Method's performance when using the  combination of cascaded-PPMI and Pearson's r.", "labels": [], "entities": []}, {"text": " Table 7: Method's performance for the combina- tion of cascaded-PPMI and sim lin : This combina- tion proves to provide the best results for high- dimensional models.", "labels": [], "entities": []}, {"text": " Table 8: Results for EN-FA detest.", "labels": [], "entities": [{"text": "EN-FA detest", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.7353376746177673}]}]}