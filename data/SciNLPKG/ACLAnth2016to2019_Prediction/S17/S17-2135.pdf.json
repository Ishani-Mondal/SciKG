{"title": [{"text": "TSA-INF at SemEval-2017 Task 4: An Ensemble of Deep Learning Architectures Including Lexicon Features for Twitter Sentiment Analysis", "labels": [], "entities": [{"text": "TSA-INF at SemEval-2017 Task 4", "start_pos": 0, "end_pos": 30, "type": "DATASET", "confidence": 0.6828745007514954}, {"text": "Twitter Sentiment Analysis", "start_pos": 106, "end_pos": 132, "type": "TASK", "confidence": 0.611339290936788}]}], "abstractContent": [{"text": "This paper describes the submission of team TSA-INF to SemEval-2017 Task 4 Subtask A. The submitted system is an ensemble of three varying deep learning architectures for sentiment analysis.", "labels": [], "entities": [{"text": "SemEval-2017 Task 4 Subtask", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.6756086647510529}, {"text": "sentiment analysis", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.957492470741272}]}, {"text": "The core of the architecture is a convolutional neural network that performs well on text classification as is.", "labels": [], "entities": [{"text": "text classification", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.8015477955341339}]}, {"text": "The second subsystem is a gated recurrent neural network implementation.", "labels": [], "entities": []}, {"text": "Additionally, the third system integrates opinion lexicons directly into a convolution neural network architecture.", "labels": [], "entities": []}, {"text": "The resulting ensemble of the three archi-tectures achieved atop ten ranking with a macro-averaged recall of 64.3%.", "labels": [], "entities": [{"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9349793791770935}]}, {"text": "Additional results comparing variations of the submitted system are not conclusive enough to determine a best architecture, but serve as a benchmark for further implementations .", "labels": [], "entities": []}], "introductionContent": [{"text": "The SemEval competitions continually offer suitable dataset and resulting benchmarks fora variety of natural language processing tasks.", "labels": [], "entities": []}, {"text": "The SemEval-2017 Task 4 Subtask A addresses the polarity classification task of informal texts.", "labels": [], "entities": [{"text": "polarity classification task", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.8036186695098877}]}, {"text": "Tweets serve as a very accessible sample of the abundant social media content.", "labels": [], "entities": []}, {"text": "Submitted systems must classify tweets into the categories of negative, positive and neutral opinion.", "labels": [], "entities": []}, {"text": "Submitted results are compared over macroaveraged recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9730049967765808}]}, {"text": "In recent benchmarks across this task, deep learning implementations achieved top results (.", "labels": [], "entities": []}, {"text": "We seek to combine three varying deep learning approaches in an ensemble.", "labels": [], "entities": []}, {"text": "Conventional methods seem to become obsolete since convolutional neural networks (CNN) have first shown state-of-the-art results in sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.9440692365169525}]}, {"text": "SemEval has since seen successful results by similar models as well as ensembles of CNNs (.", "labels": [], "entities": []}, {"text": "Long term short term recurrent neural networks (LSTM) have also been applied successfully in ensemble with a CNN (.", "labels": [], "entities": []}, {"text": "As an alternative to LSTMs gated recurrent neural networks (GRNN) have been shown to be competitive in other domains ().", "labels": [], "entities": []}, {"text": "These models are well suited to model sequential data and were successfully applied for sentiment analysis of larger documents).", "labels": [], "entities": [{"text": "sentiment analysis of larger documents", "start_pos": 88, "end_pos": 126, "type": "TASK", "confidence": 0.9227117776870728}]}, {"text": "The core contribution of a recent non deep learning system to win this task () back to back in 2013 and 2014, was the integration of opinion lexicons into a support vector machine system.", "labels": [], "entities": []}, {"text": "Opinion lexicons have since then also been integrated into CNN architectures.", "labels": [], "entities": []}, {"text": "In this work we combine these diverse architectures.", "labels": [], "entities": []}, {"text": "We use a CNN, thoroughly optimized for text classification, as the foundation of our ensemble approach.", "labels": [], "entities": [{"text": "text classification", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7969931662082672}]}, {"text": "We add a lexicon integrated CNN to take advantage of lexicon features.", "labels": [], "entities": []}, {"text": "In order to diversify the approach we also include a GRNN architecture as a sequential model.", "labels": [], "entities": []}, {"text": "The idea is to get better and more robust results with a broader architecture.", "labels": [], "entities": []}, {"text": "Results show that adding the latter two systems does not improve overall results, though the results for the core CNN approach were already good.", "labels": [], "entities": []}, {"text": "Furthermore, results for individual classes do improve, making this a viable option when prioritizing specific classes or evaluation metrics.", "labels": [], "entities": []}, {"text": "With this work we seek to contribute to the growing body of literature that presents comparable and reproducible solutions for this task.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: SemEval data subsets as available to au- thors. Aside from development test (A) and test  (B) split, all sets where combined for a train and  dev split.", "labels": [], "entities": []}, {"text": " Table 2: Per class results of recall precision and F1 on test data (2017-test, Table 1) for CNN and  ensemble architectures, where  *  marks the submission system", "labels": [], "entities": [{"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.995795488357544}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.5467740297317505}, {"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9986068606376648}]}, {"text": " Table 3: Macro-averaged recall \u03c1, negative posi- tive macro-averaged F1 and accuracy on develop- ment test data (2016-test, Table 1) for CNN and  ensemble architectures.", "labels": [], "entities": [{"text": "recall \u03c1", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9670265018939972}, {"text": "F1", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.894344687461853}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9995474219322205}]}, {"text": " Table 4: Macro-averaged recall \u03c1, negative posi- tive macro-averaged F1 and accuracy on test data  (2017-test, Table 1) for CNN and ensemble archi- tectures, where  *  marks the ranked submission.", "labels": [], "entities": [{"text": "recall \u03c1", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.968347430229187}, {"text": "F1", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9167317748069763}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9995266199111938}]}]}