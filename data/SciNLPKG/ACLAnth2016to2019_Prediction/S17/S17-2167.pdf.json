{"title": [{"text": "Know-Center at SemEval-2017 Task 10: Sequence Classification with the CODE Annotator", "labels": [], "entities": [{"text": "Sequence Classification", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.9459556937217712}]}], "abstractContent": [{"text": "This paper describes our participation in SemEval-2017 Task 10, named ScienceIE (Machine Reading for Scientist).", "labels": [], "entities": [{"text": "SemEval-2017 Task 10", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.8907789389292399}]}, {"text": "We competed in Subtask 1 and 2 which consist respectively in identifying all the key phrases in scientific publications and label them with one of the three categories: Task, Process, and Material.", "labels": [], "entities": []}, {"text": "These scientific publications are selected from Computer Science, Material Sciences, and Physics domains.", "labels": [], "entities": []}, {"text": "We followed a supervised approach for both subtasks by using a sequential classifier (CRF-Conditional Random Fields).", "labels": [], "entities": []}, {"text": "For generating our solution we used a web-based application implemented in the EU-funded research project, named CODE.", "labels": [], "entities": [{"text": "CODE", "start_pos": 113, "end_pos": 117, "type": "DATASET", "confidence": 0.9044609665870667}]}, {"text": "Our system achieved an F1 score of 0.39 for the Subtask 1 and 0.28 for the Subtask 2.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9845338761806488}]}], "introductionContent": [{"text": "Information Retrieval (IR) systems for scientific publications face different challenges compared to the standard approaches.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8330097913742065}]}, {"text": "This, mainly is due to the unavailability of the whole text from reviewed papers and the vague specification of the searching information.", "labels": [], "entities": []}, {"text": "The identification and the extraction of the key phrases from such articles can partially overcome the limits described above by allowing search engines to access and use them as text features.", "labels": [], "entities": []}, {"text": "Furthermore, the classification of the key phrases as a Task, a Process, or a Material, can help the researchers to correctly specify the type of information they are seeking.", "labels": [], "entities": []}, {"text": "The Subtasks 1 and 2 of Task 10 () in SemEval-2017 named ScienceIE (Machine Reading for Scientist), tackle the aforementioned problems.", "labels": [], "entities": []}, {"text": "This task consists in identifying (Subtask 1) and labeling (Subtask 2) all the key phrases in scientific publications from Computer Science, Material Science, and Physics.", "labels": [], "entities": []}, {"text": "For training and evaluating this task, it was provided a set of scientific papers together with the annotated key phrases and their associated labels.", "labels": [], "entities": []}, {"text": "The annotations were represented with their start and end offsets in the text.", "labels": [], "entities": []}, {"text": "The labels associated with each annotation can be from one of the three options: Task, Process, and Material.", "labels": [], "entities": []}, {"text": "The example in illustrates the given dataset.", "labels": [], "entities": []}, {"text": "We followed a supervised approach for both subtasks.", "labels": [], "entities": []}, {"text": "More specifically, we trained a sequential classifier CRF -Conditional Random Fields () and fed it with grammatical and text features.", "labels": [], "entities": []}, {"text": "The model built from this classifier represents our solution for identifying and labeling the key phrases.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section we describe our system's details.", "labels": [], "entities": []}, {"text": "In section 3 we show the results of our systems and compare it with the other participants in the challenge.", "labels": [], "entities": []}, {"text": "We end with section 4 summing up the con-961 clusions and foreseeing our future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Official results for the Subtask 1 and 2 of the Task", "labels": [], "entities": []}]}