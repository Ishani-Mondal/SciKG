{"title": [{"text": "Using Semantic Information Space to Evaluate Semantic Textual Similarity", "labels": [], "entities": [{"text": "Evaluate Semantic Textual Similarity", "start_pos": 36, "end_pos": 72, "type": "TASK", "confidence": 0.6108223274350166}]}], "abstractContent": [{"text": "This paper presents three systems for semantic textual similarity (STS) evaluation at SemEval-2017 STS task.", "labels": [], "entities": [{"text": "semantic textual similarity (STS) evaluation", "start_pos": 38, "end_pos": 82, "type": "TASK", "confidence": 0.7590644317013877}, {"text": "SemEval-2017 STS task", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.5637193024158478}]}, {"text": "One is an unsupervised system and the other two are supervised systems which simply employ the unsupervised one.", "labels": [], "entities": []}, {"text": "All our systems mainly depend on the semantic information space (SIS), which is constructed based on the semantic hierarchical taxonomy in WordNet, to compute non-overlapping information content (IC) of sentences.", "labels": [], "entities": []}, {"text": "Our team ranked 2nd among 31 participating teams by the primary score of Pearson correlation coefficient (PCC) mean of 7 tracks and achieved the best performance on Track 1 (AR-AR) dataset.", "labels": [], "entities": [{"text": "Pearson correlation coefficient (PCC) mean", "start_pos": 73, "end_pos": 115, "type": "METRIC", "confidence": 0.9655800717217582}, {"text": "Track 1 (AR-AR) dataset", "start_pos": 165, "end_pos": 188, "type": "DATASET", "confidence": 0.6264986197153727}]}], "introductionContent": [{"text": "Given two snippets of text, semantic textual similarity (STS) measures the degree of equivalence in the underlying semantics.", "labels": [], "entities": [{"text": "semantic textual similarity (STS)", "start_pos": 28, "end_pos": 61, "type": "METRIC", "confidence": 0.6964773188034693}]}, {"text": "STS is a basic but important issue with multitude of application areas in natural language processing (NLP) such as example based machine translation (EBMT), machine translation evaluation, information retrieval (IR), question answering (QA), text summarization and soon.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9692670702934265}, {"text": "example based machine translation (EBMT)", "start_pos": 116, "end_pos": 156, "type": "TASK", "confidence": 0.7878159284591675}, {"text": "machine translation evaluation", "start_pos": 158, "end_pos": 188, "type": "TASK", "confidence": 0.8743405938148499}, {"text": "information retrieval (IR)", "start_pos": 190, "end_pos": 216, "type": "TASK", "confidence": 0.8407688796520233}, {"text": "question answering (QA)", "start_pos": 218, "end_pos": 241, "type": "TASK", "confidence": 0.8575441122055054}, {"text": "text summarization", "start_pos": 243, "end_pos": 261, "type": "TASK", "confidence": 0.8070498704910278}]}, {"text": "The SemEval STS task has become the most famous activity for STS evaluation in recent years and the STS shared task has been held annually since 2012 (, as part of the SemEval/*SEM family of workshops.", "labels": [], "entities": [{"text": "SemEval STS task", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7129080096880595}, {"text": "STS evaluation", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.9211420714855194}]}, {"text": "The organizers have setup publicly available datasets of sentence pairs with similarity scores from human annotators, which are up to more than 16,000 * Corresponding author sentence pairs for training and evaluation, and attracted a large number of teams with a variety of systems to participate the competitions.", "labels": [], "entities": []}, {"text": "Generally, STS systems could be divided into two categories: One kind is unsupervised systems (, some of which are appeared fora longtime when there wasn't enough training data; The other kind is supervised systems) applying machine learning algorithms, including deep learning, after adequate training data has been constructed.", "labels": [], "entities": []}, {"text": "Each kind of methods has its advantages and application areas.", "labels": [], "entities": []}, {"text": "In this paper, we present three systems, one unsupervised system and two supervised systems which simply make use of the unsupervised one.", "labels": [], "entities": []}], "datasetContent": [{"text": "To apply non-overlapping IC of sentences in STS evaluation, we construct the semantic information space (SIS), which employs the super-subordinate (is-a) relation from the hierarchical taxonomy of WordNet (.", "labels": [], "entities": [{"text": "STS evaluation", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.9518963992595673}]}, {"text": "The space size of a concept is the information content of the concept.", "labels": [], "entities": []}, {"text": "SIS is not a traditional orthogonality multidimensional space, while it is the space with inclusion relation among concepts.", "labels": [], "entities": []}, {"text": "Sentences in SIS are represented as areal physical space instead of a point in vector space.", "labels": [], "entities": [{"text": "SIS", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.903355598449707}]}, {"text": "We have the intuitions about similarity: The similarity between A and B is related to their commonality and differences, the more commonality and the less differences they have, the more similar they are; The maximum similarity is reached when A and B are identical, no matter how much commonality they share.", "labels": [], "entities": [{"text": "similarity", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.947952389717102}, {"text": "similarity", "start_pos": 217, "end_pos": 227, "type": "METRIC", "confidence": 0.9583865404129028}]}, {"text": "The principle of Jaccard coefficient is accordance with the intuitions about similarity and we define the similarity of two sentences S a and Sb based on it: The quantity of the intersection of the information provided by the two sentences can be obtained through that of the union of them: So the remaining problem is how to compute the quantity of the union of non-overlapping information of sentences.", "labels": [], "entities": []}, {"text": "We calculate it by employing the inclusion-exclusion principle from combinatorics for the total IC of sentence s a and the same way is used for sentence s band both sentences: For the IC of n-concepts intersection in Equation (4), we use the following equation 1 : 1 For the sake of high computational complexity introduced by Equation (4), we simplify the calculation of common IC of n-concepts and use the approximate formula in Equation (6).", "labels": [], "entities": []}, {"text": "The accurate formula of common IC is: Algorithm 1: getInExT otalIC(S ) Input: S : {c i |i = 1, 2, . .", "labels": [], "entities": []}, {"text": ", n; n = |S |} Output: tIC: Total IC of input S 1 if S = \u2205 then 2 return 0 3 Initialize: tIC \u2190 0 4 for i = 1; i \u2264 n; i + + do foreach comb in C(n, i)-combinations do cIC \u2190 commonIC (comb) where, subsum (c 1 , \u00b7 \u00b7 \u00b7 , c n ) is the set of concepts that subsume all the concepts of c 1 , \u00b7 \u00b7 \u00b7 , c n in SIS.", "labels": [], "entities": []}, {"text": "Algorithm 1 is according to Equation (4) and (6), here C (n, i) is the number of combinations of i-concepts from n-concepts, commonIC(comb) is calculated through Equation (6).", "labels": [], "entities": [{"text": "commonIC(comb)", "start_pos": 125, "end_pos": 139, "type": "METRIC", "confidence": 0.9265045523643494}]}, {"text": "For more details about this section, please seethe paper (Wu and Huang, 2016) for reference.", "labels": [], "entities": []}, {"text": "Total GS: Test sets at SemEval 2017 STS task.", "labels": [], "entities": [{"text": "GS", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.9108206629753113}, {"text": "SemEval 2017 STS task", "start_pos": 23, "end_pos": 44, "type": "DATASET", "confidence": 0.6312138140201569}]}, {"text": "corpora and train the linear regression (LR) model.", "labels": [], "entities": []}, {"text": "There two features: One is the outputs of SIS, the other is from a modified version of basic sentence embedding which is the simply combination of word embeddings.", "labels": [], "entities": []}, {"text": "The word embedding vectors are generated from word2vec () over the 5th edition of the Gigaword (LDC2011T07)).", "labels": [], "entities": [{"text": "Gigaword (LDC2011T07", "start_pos": 86, "end_pos": 106, "type": "DATASET", "confidence": 0.8633869489034017}]}, {"text": "We also preprocess the Gigaword data with tokenizer.perl and truecase.perl.", "labels": [], "entities": [{"text": "Gigaword data", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.9254641830921173}]}, {"text": "We modify this basic sentence embedding by importing domain IDF information.", "labels": [], "entities": []}, {"text": "The domain IDFs of words could be obtained from the current test dataset by deeming each sentence as a document.", "labels": [], "entities": []}, {"text": "We did not directly use the domain IDFs d as the weight of a word embedding.", "labels": [], "entities": []}, {"text": "On previous datasets, we found d 0.8 as its weight performed nearly the best.", "labels": [], "entities": []}, {"text": "The evaluation metric is the Pearson productmoment correlation coefficient (PCC) between semantic similarity scores of machine assigned and human judgements.", "labels": [], "entities": [{"text": "Pearson productmoment correlation coefficient (PCC)", "start_pos": 29, "end_pos": 80, "type": "METRIC", "confidence": 0.8906395179884774}]}, {"text": "PCC is used for each individual test set, and the primary evaluation is measured by weighted mean of PCC on all datasets.", "labels": [], "entities": [{"text": "PCC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6329881548881531}]}, {"text": "Performances of our three runs on each of SemEval 2017 STS test set are shown in.", "labels": [], "entities": [{"text": "SemEval 2017 STS test set", "start_pos": 42, "end_pos": 67, "type": "DATASET", "confidence": 0.814034229516983}]}, {"text": "Bold numbers represents the best scores from any our system on each test set, including the primary scores.", "labels": [], "entities": []}, {"text": "Cosine Baseline utilizes basic sentence embedding method for monolingual similarity (Track 1, 3 and 5) provided officially by STS organizers; Best system denotes all the scores are from the state-of-the-art system; All Systems Best means the best scores from all the systems participated in each track, regardless of whether they come from the same system; Differences indicates the differences between the best scores from our three systems and All Single Best in each track, primary difference is between our best system and state-of-the-art system.", "labels": [], "entities": []}, {"text": "Team Rankings show the rankings of our best scores from that of other teams.", "labels": [], "entities": []}, {"text": "Team Rankings of Primary could be the most important ranking for participants who submitted scores for all tracks.", "labels": [], "entities": []}, {"text": "Our team ranked 2nd for the primary score and achieved the best performance in Track 1 (ArabicArabic).", "labels": [], "entities": [{"text": "ArabicArabic)", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.9225169122219086}]}, {"text": "Track 1 is the only track that totally employed new languages which has no references from the past (cross-lingual evaluation contains English sentences).", "labels": [], "entities": []}, {"text": "The very failing performance is in Track 4b.", "labels": [], "entities": []}, {"text": "We guess the reasons could be the followings and further research is needed on this issue: 1) Our methods, especially for unsupervised SIS, ignore some important information as the embedding methods and are currently not suit for complicated post-editing sentences.", "labels": [], "entities": []}, {"text": "We tested basic sentence embedding method in isolation which could achieve the score of more than 0.16,  much better than our IC based systems of Run 1 (0.0758) and Run 2 (0.0584),which are without embedding modules.", "labels": [], "entities": []}, {"text": "2) The translation quantity for long sentences by machine translation maybe not good enough as that for short sentences.", "labels": [], "entities": []}, {"text": "The translation results may lose some information in the original sentences for SIS and introduce more noise.", "labels": [], "entities": [{"text": "SIS", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.8862922191619873}]}], "tableCaptions": [{"text": " Table 1: Test sets at SemEval 2017 STS task.", "labels": [], "entities": [{"text": "SemEval 2017 STS task", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.6980856209993362}]}, {"text": " Table 2: Performances on SemEval 2017 STS evaluation datasets.", "labels": [], "entities": [{"text": "SemEval 2017 STS evaluation datasets", "start_pos": 26, "end_pos": 62, "type": "DATASET", "confidence": 0.7480722188949585}]}]}