{"title": [{"text": "Distributed Prediction of Relations for Entities: The Easy, The Difficult, and The Impossible", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings are supposed to provide easy access to semantic relations such as \"male of\" (man-woman).", "labels": [], "entities": []}, {"text": "While this claim has been investigated for concepts, little is known about the distributional behavior of relations of (Named) Entities.", "labels": [], "entities": []}, {"text": "We describe two word embedding-based models that predict values for relational attributes of entities, and analyse them.", "labels": [], "entities": []}, {"text": "The task is challenging, with major performance differences between relations.", "labels": [], "entities": []}, {"text": "Contrary to many NLP tasks, high difficulty fora relation does not result from low frequency, but from (a) one-to-many mappings; and (b) lack of context patterns expressing the relation that are easy to pickup byword embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "A central claim about distributed models of word meaning (e.g.,) is that word embedding space provides easy access to semantic relations.", "labels": [], "entities": []}, {"text": "E.g.,s space was shown to encode the \"male-female relation\" linearly, as a vector The accessibility of semantic relations was subsequently examined in more detail. and reported successful modeling of lexical relations such as hypernymy and synonymy.", "labels": [], "entities": []}, {"text": "considered a broader range of relationships,with mixed results.", "labels": [], "entities": []}, {"text": "developed an improved, nonlinear relation extraction method.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7051818370819092}]}, {"text": "These studies were conducted primarily on concepts and their semantic relations, like hypernym(politician) = person.", "labels": [], "entities": []}, {"text": "Meanwhile, entities and the relations they partake in are much less well understood.", "labels": [], "entities": []}, {"text": "Entities are instances of concepts, i.e., they refer to specific individual objects in the real world, for example, Donald Trump is an instance of the concept politician.", "labels": [], "entities": []}, {"text": "Consequently, entities are generally associated with a rich set of numeric and relational attributes (for politician instances: size, office, etc.).", "labels": [], "entities": []}, {"text": "In contrast to concepts, the values of these attributes tend to be discrete: while the size of politician is best described by a probability distribution, the size of Donald Trump is 1.88m.", "labels": [], "entities": []}, {"text": "Since distributional representations are notoriously bad at handling discrete knowledge (, this raises the question of how well such models can capture entity-related knowledge.", "labels": [], "entities": []}, {"text": "In our previous work, we analysed distributional prediction of numeric attributes of entities, found a large variance in quality among attributes, and identified factors determining prediction difficulty.", "labels": [], "entities": [{"text": "distributional prediction of numeric attributes of entities", "start_pos": 34, "end_pos": 93, "type": "TASK", "confidence": 0.8223144837788173}]}, {"text": "A corresponding analysis for relational (categorial) attributes of entities is still missing, even though entities are highly relevant for NLP.", "labels": [], "entities": []}, {"text": "This is evident from the highly active area of knowledge base completion (KBC), the task of extending incomplete entity information in knowledge bases such as Yago or Wikidata (e.g.,.", "labels": [], "entities": [{"text": "knowledge base completion (KBC)", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.818759967883428}]}, {"text": "In this paper, we assess to what extent relational attributes of entities are easily accessible from word embedding space.", "labels": [], "entities": []}, {"text": "To this end, we define two models that predict, given a target entity (Star Wars) and a relation (director), a distributed representation for the relatum (George Lucas).", "labels": [], "entities": []}, {"text": "We carryout a detailed per-relation analyses of their performance on seven major FreeBase domains and identify what makes a relation difficult by correlating performance with properties of the relations.", "labels": [], "entities": []}, {"text": "We find that, contrary to many other NLP tasks, relations are not difficult if they are infrequent or sparse, but instead (a) if they relate one target to multiple relata; (b) if they do not give rise to linguistic patterns that can be picked up by bag-of-words distributional models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extract relation data from FreeBase.", "labels": [], "entities": [{"text": "FreeBase", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.9668759703636169}]}, {"text": "We follow our earlier work, but go beyond its limitation to two domains (country, citytown).", "labels": [], "entities": []}, {"text": "We experiment with seven major FreeBase domains: animal, book, citytown, country, employer, organization, people.", "labels": [], "entities": []}, {"text": "We limit the number of datapoints of very large relation types to 3000 with random sampling for efficiency reasons.", "labels": [], "entities": []}, {"text": "We only remove relation types with fewer than 3 datapoints.", "labels": [], "entities": []}, {"text": "This results in a quite challenging dataset that demonstrates the generalizability of our models and is roughly comparable, in variety and size, to the FB15K dataset ().", "labels": [], "entities": [{"text": "variety", "start_pos": 127, "end_pos": 134, "type": "METRIC", "confidence": 0.978154718875885}, {"text": "FB15K dataset", "start_pos": 152, "end_pos": 165, "type": "DATASET", "confidence": 0.9856101274490356}]}, {"text": "The distributed representations for all entities come from the 1000-dimensional \"Google News\" skip-gram model () for FreeBase entities 2 trained on a 100G token news corpus.", "labels": [], "entities": []}, {"text": "We only retain relation datapoints where both target and relatum are covered in the Google News vectors.", "labels": [], "entities": [{"text": "Google News vectors", "start_pos": 84, "end_pos": 103, "type": "DATASET", "confidence": 0.9190147320429484}]}, {"text": "shows the numbers of relations and unique objects (target plus relata).", "labels": [], "entities": []}, {"text": "We split all domains into training, validation, and test sets (60%-20%-20%).", "labels": [], "entities": []}, {"text": "The split applies to each relation type: in test, we face no unseen relation types, but unseen datapoints for each relation.", "labels": [], "entities": []}, {"text": "The NonLinM model uses an L 2 norm constraint of s=3.", "labels": [], "entities": []}, {"text": "We adopt the best AdaDelta parameters from Zeiler (2012), viz.", "labels": [], "entities": []}, {"text": "\u03c1 = 0.95 and = 10 \u22126 . We optimize the negative sampling weight \u03b1 (cf. Eq.", "labels": [], "entities": []}, {"text": "3) byline search with a step size of 0.1 on the largest domain, country, and find 0.6 to be the optimal value, which we reuse for all domains.", "labels": [], "entities": []}, {"text": "Due to the varying dimensionality m of the relation vector per domain, we set the size of the hidden layer to k = 2n + m/10 (n is the dimensionality of the word embeddings, cf..", "labels": [], "entities": []}, {"text": "We train all models fora maximum of 1000 epochs with early stopping.", "labels": [], "entities": []}, {"text": "Models that predict vectors in a continuous vector space, like ours, cannot expect to predict the output vector precisely.", "labels": [], "entities": []}, {"text": "Thus, we apply nearest neighbor mapping using the set of all unique targets and relata in each domain (cf. Table 1) to identify the correct relatum name.", "labels": [], "entities": []}, {"text": "We then perform an Information Retrieval-style ranking evaluation: We compute the rank of the correct relatum r, given the target t and the relation \u03c1, in the test set T and aggregate these ranks to compute the mean reciprocal rank (MRR): where rank is the nearest neighbor rank of the relatum vector r given the prediction of the model The dataset are available at: http://www.ims.unistuttgart.de/data/RelationPrediction.html for the input t, \u03c1.", "labels": [], "entities": [{"text": "Information Retrieval-style ranking", "start_pos": 19, "end_pos": 54, "type": "TASK", "confidence": 0.8217882116635641}, {"text": "mean reciprocal rank (MRR)", "start_pos": 211, "end_pos": 237, "type": "METRIC", "confidence": 0.7812847991784414}]}, {"text": "We report results at the relation level as well as macro-and micro-averaged MRR for the complete dataset.", "labels": [], "entities": [{"text": "MRR", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.4341343641281128}]}, {"text": "Frequency Baseline (BL).", "labels": [], "entities": [{"text": "Frequency Baseline (BL)", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.8281862497329712}]}, {"text": "Our baseline model ignores the target.", "labels": [], "entities": []}, {"text": "For each relation, it predicts the frequency-ordered list of all training set relata.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test set statistics and results. #R: relations; #Ts+Ra: unique targets and relata; BL/LM/NLM:  Baseline, linear and nonlinear model (macro-average MRR); %Rx: percent of relations with MRR x;  \u03c1: Spearman correlation; #In: instances; #RpT: relata per target", "labels": [], "entities": [{"text": "BL", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9684343338012695}, {"text": "RpT", "start_pos": 244, "end_pos": 247, "type": "METRIC", "confidence": 0.9560302495956421}]}, {"text": " Table 3: The three most easy and most difficult  relations for the country domain", "labels": [], "entities": []}]}