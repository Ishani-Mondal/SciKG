{"title": [{"text": "The NTNU System at SemEval-2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications Using Multiple Conditional Random Fields", "labels": [], "entities": [{"text": "NTNU System at SemEval-2017 Task 10", "start_pos": 4, "end_pos": 39, "type": "DATASET", "confidence": 0.8226079742113749}, {"text": "Extracting Keyphrases and Relations from Scientific Publications", "start_pos": 41, "end_pos": 105, "type": "TASK", "confidence": 0.8868040187018258}]}], "abstractContent": [{"text": "This study describes the design of the NTNU system for the ScienceIE task at the SemEval 2017 workshop.", "labels": [], "entities": [{"text": "ScienceIE task at the SemEval 2017 workshop", "start_pos": 59, "end_pos": 102, "type": "DATASET", "confidence": 0.7735290442194257}]}, {"text": "We use self-defined feature templates and multiple conditional random fields with extracted features to identify keyphrases along with categorized labels and their relations from scientific publications.", "labels": [], "entities": []}, {"text": "A total of 16 teams participated in evaluation scenario 1 (subtasks A, B, and C), with only 7 teams competing in all sub-tasks.", "labels": [], "entities": []}, {"text": "Our best micro-averaging F1 across the three subtasks is 0.23, ranking in the middle among all 16 submissions.", "labels": [], "entities": [{"text": "F1", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.8560575246810913}]}], "introductionContent": [{"text": "Keyphrases are usually regarded as phrases that capture the main topics mentioned in a given text.", "labels": [], "entities": []}, {"text": "Automatically extracting keyphrases and determining their relations from scientific articles has various applications, such as recommending articles to readers, matching reviewers to submissions, facilitating the exploration of huge document collections, and soon.", "labels": [], "entities": []}, {"text": "An adapted nominal group chunker and a supervised ranking method based on support vector machines have previously been used to extract keyphrase candidates).", "labels": [], "entities": []}, {"text": "The conditional random field based keyphrase extraction method has been presented (.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.6955620348453522}]}, {"text": "A na\u00efve approach has been proposed to investigate characteristics of keyphrases with section information from well-structured scientific articles.", "labels": [], "entities": []}, {"text": "Features broadly used for the supervised approaches in scientific articles have been assessed in the compilation of a comprehensive feature list.", "labels": [], "entities": []}, {"text": "Maximal sequences and page ranking have been combined to discover latent keyphrases within scientific articles.", "labels": [], "entities": []}, {"text": "Noun phrases containing multiple modifiers have been extracted from earth science publications and generalized by matching tree patterns to the syntax trees of the sources texts (.", "labels": [], "entities": []}, {"text": "Keyphrase boundary classification has been regarded as a multi-task learning problem using deep recurrent neural network.", "labels": [], "entities": [{"text": "Keyphrase boundary classification", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8735882639884949}]}, {"text": "The ScienceIE task seeks solutions to automatically identify keyphrases within scientific publications, label them, and determine their relationships.", "labels": [], "entities": []}, {"text": "Specifically, the ScienceIE task contains three subtasks: (A) Identification of keyphrases: to identify all the keyphrases within a given scientific publication; (B) Classification of identified keyphrases: to label each keyphrase as Process, Task, or Material; (C) Extraction of relationships between two identified keyphrases: to label keyphrases as Hyponym-of or Synonymof.", "labels": [], "entities": []}, {"text": "The ScienceIE task presents three evaluation scenarios.", "labels": [], "entities": [{"text": "ScienceIE task", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.848406195640564}]}, {"text": "In Scenario 1, only plain text is given for subtasks A, B, and C; in Scenario 2, plain text with manually annotated keyphrase boundaries are given for subtasks B and C; and in Scenario 3, plain text with manually annotated keyphrases and their types are given for subtask C.", "labels": [], "entities": []}, {"text": "System output is matched against a gold standard to measure system performance.", "labels": [], "entities": []}, {"text": "The micro-averaging precision, recall, and F1 across the subtask(s) are used in the task.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.8428632020950317}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9997162222862244}, {"text": "F1", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.999737560749054}]}, {"text": "Each participating team can submit at most three results and the best result for each evaluation scenario is taken as the performance of the participating team.", "labels": [], "entities": []}, {"text": "This article describes the NTNU (National Taiwan Normal University) system for the ScienceIE task at the SemEval 2017 workshop.", "labels": [], "entities": [{"text": "NTNU (National Taiwan Normal University) system", "start_pos": 27, "end_pos": 74, "type": "DATASET", "confidence": 0.9036145806312561}, {"text": "SemEval 2017 workshop", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.5883843004703522}]}, {"text": "Our solution uses multiple conditional random fields at the sentence level.", "labels": [], "entities": []}, {"text": "Each sentence is parsed to obtain features, including words, lemmas, part-ofspeech tags, and syntactic phrases.", "labels": [], "entities": []}, {"text": "CRFs are then trained to learn sequential patterns using the datasets provided by task organizers.", "labels": [], "entities": []}, {"text": "We participated in the evaluation scenario 1 with three subtasks.", "labels": [], "entities": []}, {"text": "Our best micro-averaging F1 of 0.23 ranked in the middle of all 16 submissions.", "labels": [], "entities": [{"text": "F1", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9073988199234009}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the details of the NTNU system for the ScienceIE task.", "labels": [], "entities": [{"text": "NTNU system", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.8582218885421753}]}, {"text": "Section 3 presents the evaluation results and performance comparisons.", "labels": [], "entities": []}, {"text": "Section 4 discusses some findings.", "labels": [], "entities": []}, {"text": "Conclusions are finally drawn in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Our results for each type.", "labels": [], "entities": []}, {"text": " Table 4: Our results for each subtask.", "labels": [], "entities": []}]}