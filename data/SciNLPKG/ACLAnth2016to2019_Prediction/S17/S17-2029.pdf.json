{"title": [{"text": "PurdueNLP at SemEval-2017 Task 1: Predicting Semantic Textual Similarity with Paraphrase and Event Embeddings", "labels": [], "entities": [{"text": "PurdueNLP", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9535890221595764}, {"text": "Predicting Semantic Textual Similarity", "start_pos": 34, "end_pos": 72, "type": "TASK", "confidence": 0.8762850612401962}]}], "abstractContent": [{"text": "This paper describes our proposed solution for SemEval 2017 Task 1: Semantic Textual Similarity (Daniel Cer and Spe-cia, 2017).", "labels": [], "entities": [{"text": "SemEval 2017 Task 1", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.9159572720527649}]}, {"text": "The task aims at measuring the degree of equivalence between sentences given in English.", "labels": [], "entities": []}, {"text": "Performance is evaluated by computing Pearson Correlation scores between the predicted scores and human judgements.", "labels": [], "entities": [{"text": "Pearson Correlation scores", "start_pos": 38, "end_pos": 64, "type": "METRIC", "confidence": 0.936625341574351}]}, {"text": "Our proposed system consists of two subsystems and one regression model for predicting STS scores.", "labels": [], "entities": [{"text": "predicting STS", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.8251278400421143}]}, {"text": "The two subsystems are designed to learn Paraphrase and Event Embeddings that can take the consideration of paraphrasing characteristics and sentence structures into our system.", "labels": [], "entities": []}, {"text": "The regression model associates these embeddings to make the final predictions.", "labels": [], "entities": []}, {"text": "The experimental result shows that our system acquires 0.8 of Pearson Correlation Scores in this task.", "labels": [], "entities": [{"text": "Pearson Correlation Scores", "start_pos": 62, "end_pos": 88, "type": "METRIC", "confidence": 0.9357127547264099}]}], "introductionContent": [{"text": "The SemEval Semantic Textual Similarity (STS) task) is to assess the degree of similarity between two given sentences and assign a score on a scale from 0 to 5.", "labels": [], "entities": [{"text": "SemEval Semantic Textual Similarity (STS) task", "start_pos": 4, "end_pos": 50, "type": "TASK", "confidence": 0.8689652532339096}]}, {"text": "A score of 0 indicates that the two sentences are completely dissimilar, while a score of 5 indicates that the sentences have the same meaning.", "labels": [], "entities": []}, {"text": "Predicting the similarity between pieces of texts finds utility in many NLP tasks such as question-answering, and plagiarism detection.", "labels": [], "entities": [{"text": "Predicting the similarity between pieces of texts", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7779117226600647}, {"text": "plagiarism detection", "start_pos": 114, "end_pos": 134, "type": "TASK", "confidence": 0.7816941738128662}]}, {"text": "In this paper, we proposed a system to facilitate STS task.", "labels": [], "entities": [{"text": "STS task", "start_pos": 50, "end_pos": 58, "type": "TASK", "confidence": 0.876488208770752}]}, {"text": "Our system includes training two types of embeddings-Paraphrase Embeddings (PE) and Event Embeddings (EE)-as features to assess STS.", "labels": [], "entities": [{"text": "STS", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.8726624250411987}]}, {"text": "For the first type of embeddings, PE, we exploit two crucial properties for measuring sentence similarity: paraphrasing characteristics and sentence structures.", "labels": [], "entities": []}, {"text": "The paraphrasing characteristics help identifying if two sentences share the same meaning.", "labels": [], "entities": []}, {"text": "Our system incorporates it using an unsupervised learning step over the Paraphrase Database (PPDB;), which is inspired by.", "labels": [], "entities": []}, {"text": "The sentence structure, on the other hand, can detect structural differences, which reflect different aspects of the similarity between the input sentences.", "labels": [], "entities": []}, {"text": "Our system employs a Convolutional Neural Network (CNN) to strengthen the embedding by including the sentence structure into our representation.", "labels": [], "entities": []}, {"text": "The second type of embeddings, EE, conveys the distributional semantics of events in a narrative setting, associating a vector with each event.", "labels": [], "entities": []}, {"text": "In the last part of our system, we build a regression model that associates the two distributed representations and predicts the similarity scores.", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 129, "end_pos": 146, "type": "METRIC", "confidence": 0.9569588899612427}]}], "datasetContent": [{"text": "We train PE using two datasets, PPDB 2.0 (Pavlick et al., 2015) and SemEval STS data.", "labels": [], "entities": [{"text": "PE", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9617882966995239}, {"text": "SemEval STS data", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.6315724849700928}]}, {"text": "These are used in the first (Section 2.1) and second steps (Section 2.2) respectively.", "labels": [], "entities": []}, {"text": "We used the New York Times (NYT) section of the Gigaword corpus (Parker et al., 2011) for training EE and our baselines.", "labels": [], "entities": [{"text": "New York Times (NYT) section of the Gigaword corpus", "start_pos": 12, "end_pos": 63, "type": "DATASET", "confidence": 0.8026353716850281}, {"text": "EE", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.8606717586517334}]}, {"text": "The SemEval STS data is also used in training the final regression model.", "labels": [], "entities": [{"text": "SemEval STS data", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.6659531792004904}]}, {"text": "The data splits are as follows: SemEval STS2012-2015 was used as the training set, STS2016 data was used as the development set, and STS2017 was used as the test set.", "labels": [], "entities": [{"text": "SemEval STS2012-2015", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.7896997332572937}, {"text": "STS2016 data", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.8279258012771606}]}, {"text": "After the development stage was finished, the training and development sets were both used to train a final model with the best hyperparameters.", "labels": [], "entities": []}, {"text": "To update the parameters, Mini-batch Stochastic Gradient Descent is used for optimizing the parameters and Adagrad) is used to update the learning rate while training.", "labels": [], "entities": []}, {"text": "The batch size is set to 100 and the number of epochs is set to 10.", "labels": [], "entities": []}, {"text": "L2-regularization is included in all the objective functions and the \u03bb is tuned over {1e\u22125, 1e\u22126, 1e\u22127, 1e\u22128}.", "labels": [], "entities": []}, {"text": "Both PE and EE's dimensions are set to 300.", "labels": [], "entities": [{"text": "PE", "start_pos": 5, "end_pos": 7, "type": "METRIC", "confidence": 0.7572122812271118}, {"text": "EE", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.9087716937065125}]}, {"text": "The first baseline we compare with is the Word2Vec Skip-Gram (W2V;) model, one of the most popular universal word embeddings.", "labels": [], "entities": [{"text": "Word2Vec Skip-Gram (W2V", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.8990144431591034}]}, {"text": "It was trained over the same corpus as EE (NYT section of Gigaword).", "labels": [], "entities": [{"text": "EE", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.631679892539978}, {"text": "NYT section of Gigaword)", "start_pos": 43, "end_pos": 67, "type": "DATASET", "confidence": 0.8736266374588013}]}, {"text": "The second baseline (paragram-small) and third baseline (paragram-XXL) are the best performing word embeddings for STS tasks shown in,a.", "labels": [], "entities": []}, {"text": "In order to represent the input sentences with the word embeddings, we average the word embeddings based on the words in the input sentences.", "labels": [], "entities": []}, {"text": "This approach has been shown to be effective in, lists the Pearson Correlation Score of SemEval 2017 STS tasks.", "labels": [], "entities": [{"text": "Pearson Correlation Score", "start_pos": 59, "end_pos": 84, "type": "METRIC", "confidence": 0.8950921297073364}, {"text": "SemEval 2017 STS tasks", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.7918049991130829}]}, {"text": "We can see that the general embedding models, (W2V and EE), do not perform well as their general purpose representation does not fit the textual similarity task.", "labels": [], "entities": []}, {"text": "On the other hand, paragram-small and paragram-XXL which were trained with the textual-similarityrelated data (PPDB and STS data) perform reasonably well.", "labels": [], "entities": []}, {"text": "The PE model, which takes paragram-XXL as the initial embeddings and tunes all the parameters on a CNN, gets higher score in both development and test sets.", "labels": [], "entities": [{"text": "PE", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.6286795139312744}]}, {"text": "The performance further increases as we introduce EE to be parts of input representations (PE+EE), while the W2V does not provide such improvement (PE+W2V).", "labels": [], "entities": []}, {"text": "PE is specifically designed for identifying paraphrasing characteristics and sentence structures, which we believe are the keys to STS task, resulting in the strongest feature set in our system.", "labels": [], "entities": []}, {"text": "We do not expect that using EE alone will give high performance, since considerable amounts of information are filtered out during event chain extraction.", "labels": [], "entities": [{"text": "event chain extraction", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.6117891371250153}]}, {"text": "In addition, EE does not use any STS-related data during training.", "labels": [], "entities": [{"text": "EE", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.7642674446105957}]}, {"text": "However, it is still helpful for capturing high-level event semantics, which can be a complement to our PE.", "labels": [], "entities": []}, {"text": "The official result of PE+EE is also included in.", "labels": [], "entities": [{"text": "PE+EE", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.5541586577892303}]}, {"text": "Our best results improve on it, by fine tuning the model's hyperparameters.", "labels": [], "entities": []}, {"text": "In addition, the best performing system of SemEval STS2017 acquires the score of 0.8547, outperforming our model.", "labels": [], "entities": [{"text": "SemEval STS2017", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.6748945116996765}]}, {"text": "However, it is not clear that what external resources or hand-crafted features were used in their work.", "labels": [], "entities": []}, {"text": "Our system, nevertheless, can accommodate additional resources and features.", "labels": [], "entities": []}, {"text": "We believe that our results can be further improved by including such information and we will look into it in the future.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson Correlation Scores for the mod- els we tested, where the Train data is STS2012- 2015, Dev. data is STS2016, Test data is STS2017.  The best scores of our model are in bold fonts.", "labels": [], "entities": [{"text": "Pearson Correlation Scores", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.810810407002767}, {"text": "Train data", "start_pos": 75, "end_pos": 85, "type": "DATASET", "confidence": 0.9027495980262756}, {"text": "STS2016", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.8190112709999084}, {"text": "STS2017", "start_pos": 139, "end_pos": 146, "type": "DATASET", "confidence": 0.9379397034645081}]}]}