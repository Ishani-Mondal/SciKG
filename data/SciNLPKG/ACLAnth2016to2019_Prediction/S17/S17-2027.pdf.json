{"title": [{"text": "MITRE at SemEval-2017 Task 1: Simple Semantic Similarity", "labels": [], "entities": [{"text": "Similarity", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.5518200397491455}]}], "abstractContent": [{"text": "This paper describes MITRE's participation in the Semantic Textual Similarity task (SemEval-2017 Task 1), which evaluated machine learning approaches to the identification of similar meaning among text snippets in English, Arabic, Spanish, and Turkish.", "labels": [], "entities": [{"text": "Semantic Textual Similarity task", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.7331158220767975}, {"text": "identification of similar meaning among text snippets in English", "start_pos": 157, "end_pos": 221, "type": "TASK", "confidence": 0.8412819107373556}]}, {"text": "We detail the techniques we explored, ranging from simple bag-of-ngrams classifiers to neural architectures with varied attention and alignment mechanisms.", "labels": [], "entities": []}, {"text": "Linear regression is used to tie the systems together into an ensemble submitted for evaluation.", "labels": [], "entities": []}, {"text": "The resulting system is capable of matching human similarity ratings of image captions with correlations of 0.73 to 0.83 in monolingual settings and 0.68 to 0.78 in cross-lingual conditions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) measures the degree to which two snippets of text convey the same meaning.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7822369535764059}]}, {"text": "Cross-lingual STS measures the same for sentence pairs written in two different languages.", "labels": [], "entities": [{"text": "Cross-lingual STS", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6198257952928543}]}, {"text": "Automatic identification of semantically similar text has practical applications in domains such as evaluation of machine translation outputs, discovery of parallel sentences in comparable corpora, essay grading, and news summarization.", "labels": [], "entities": [{"text": "Automatic identification of semantically similar text", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.7997681200504303}, {"text": "evaluation of machine translation outputs", "start_pos": 100, "end_pos": 141, "type": "TASK", "confidence": 0.7011305809020996}, {"text": "discovery of parallel sentences in comparable corpora", "start_pos": 143, "end_pos": 196, "type": "TASK", "confidence": 0.8042811410767692}, {"text": "news summarization", "start_pos": 217, "end_pos": 235, "type": "TASK", "confidence": 0.7284121513366699}]}, {"text": "It serves as an easily explained assay for systems modeling semantics.", "labels": [], "entities": [{"text": "systems modeling semantics", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.8317714730898539}]}, {"text": "SemEval-2017 marked the sixth consecutive year of a shared task measuring progress in STS.", "labels": [], "entities": [{"text": "STS", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9034719467163086}]}, {"text": "Current machine learning approaches to measuring semantic similarity vary widely.", "labels": [], "entities": [{"text": "measuring semantic similarity", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.6874412894248962}]}, {"text": "One design decision for STS systems is whether to explicitly align words between paired sentences.", "labels": [], "entities": []}, {"text": "demonstrate that sentence embeddings without explicit alignment or attention can often provide reasonable performance on STS tasks.", "labels": [], "entities": []}, {"text": "Related work in textual entailment offers evidence that neural models with soft alignment outperform embeddings-only approaches;.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7706445753574371}]}, {"text": "However these results were obtained on a dataset multiple orders of magnitude larger than existing STS datasets.", "labels": [], "entities": [{"text": "STS datasets", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.8511085212230682}]}, {"text": "In absence of large datasets, word alignments similar to those used in statistical machine translation have proven to be useful (.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.7506951093673706}, {"text": "statistical machine translation", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.6456132431825002}]}, {"text": "In this effort we explored diverse methods for aligning words in pairs of candidate sentences: translation-inspired hard word alignments as well as soft alignments learned by deep neural networks with attention.", "labels": [], "entities": [{"text": "translation-inspired hard word alignments", "start_pos": 95, "end_pos": 136, "type": "TASK", "confidence": 0.8013427257537842}]}, {"text": "We also examined a variety of approaches for comparing aligned words, ranging from bag-of-ngrams features leveraging handengineered lexical databases, to recurrent and convolutional neural networks operating over distributed representations.", "labels": [], "entities": []}, {"text": "Although an ideal crosslingual STS system might operate directly on input sentences in their original language, we used machine translation to convert all the inputs into English.", "labels": [], "entities": [{"text": "crosslingual STS", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.6208880841732025}]}, {"text": "The paucity of in-domain training data and the simplicity of the image caption genre made the translation approach reasonable.", "labels": [], "entities": [{"text": "image caption genre", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7847772936026255}, {"text": "translation", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.9689000844955444}]}, {"text": "Our contribution builds on approaches developed for English STS but points away forward for progress on knowledge-lean, fully-supervised methods for semantic comparison across different languages.", "labels": [], "entities": [{"text": "English STS", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.7014431655406952}, {"text": "semantic comparison", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.7877911031246185}]}], "datasetContent": [{"text": "Semantic Textual Similarity was a shared task organized within.", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7702057957649231}]}, {"text": "The task organizers released 1,750 sentence pairs of evaluation data organized into six tracks: Arabic, Spanish, and English monolingual, as well as Arabic-English, Spanish-English, and Turkish-English cross-lingual.", "labels": [], "entities": []}, {"text": "Most of this evaluation data was sourced from the Stanford Natural Language Inference corpus (.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference corpus", "start_pos": 50, "end_pos": 92, "type": "DATASET", "confidence": 0.8152756333351135}]}, {"text": "The sentences are English-language image captions, grouped into pairs and human-annotated on a scale of 0 to 5 for semantic similarity.", "labels": [], "entities": []}, {"text": "In the monolingual English task, the average sentence length was 8.7 words, and the average rating was 2.3 (e.g. The woman had brown hair. and The woman has gray hair.)", "labels": [], "entities": []}, {"text": "There was a roughly balanced distribution of highly rated pairs (e.g. A woman is bungee jumping. and A girl is bungee jumping.) and poorly rated pairs (e.g. The yard has a dog. and The dog is running after another dog.)", "labels": [], "entities": []}, {"text": "Annotated sentence pairs were manually translated from English into other languages to create additional tracks.", "labels": [], "entities": []}, {"text": "For each pair, task participants predicted a similarity score.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 45, "end_pos": 61, "type": "METRIC", "confidence": 0.97556072473526}]}, {"text": "Systems were evaluated by Pearson correlation with the human ratings.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 26, "end_pos": 45, "type": "METRIC", "confidence": 0.9573004245758057}]}, {"text": "We used as training data a selection of English monolingual sentence pairs released during prior SemEval STS evaluations.", "labels": [], "entities": [{"text": "SemEval STS evaluations", "start_pos": 97, "end_pos": 120, "type": "TASK", "confidence": 0.9311987161636353}]}, {"text": "Specifically, we trained on 6,898 pairs of news and caption genre data from the 2012-2014 and 2016 evaluations.", "labels": [], "entities": [{"text": "news and caption genre", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.6570533066987991}]}, {"text": "We used an additional 400 and 350 captions from the 2015 evaluation as development and tuning sets, respectively.", "labels": [], "entities": []}, {"text": "We did not use out-of-genre data (e.g. dictionary definitions, Europarl, web forums, student essays) or the newly-released multilingual 2017 training data.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.961578369140625}]}, {"text": "The dev set was used to select hyperparameters for individual components, while the tuning set was used to select the hyperparameters for the final ensemble.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson correlations on official test set. Corrected ensemble effects in bold.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9185168445110321}, {"text": "official test set", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.8190531929334005}]}, {"text": " Table 2: Factored and ablated system components  evaluated on our dev set and the official test set.", "labels": [], "entities": [{"text": "official test set", "start_pos": 83, "end_pos": 100, "type": "DATASET", "confidence": 0.7569500803947449}]}]}