{"title": [{"text": "IITP at SemEval-2017 Task 8 : A Supervised Approach for Rumour Evaluation", "labels": [], "entities": [{"text": "IITP", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.7976995706558228}, {"text": "SemEval-2017 Task 8", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.4907270669937134}, {"text": "Rumour Evaluation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.9407930076122284}]}], "abstractContent": [{"text": "This paper describes our system participation in the SemEval-2017 Task 8 'Ru-mourEval: Determining rumour veracity and support for rumours'.", "labels": [], "entities": [{"text": "SemEval-2017 Task 8", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8746866583824158}]}, {"text": "The objective of this task was to predict the stance and veracity of the underlying rumour.", "labels": [], "entities": [{"text": "veracity", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9638009071350098}]}, {"text": "We propose a supervised classification approach employing several lexical, content and twitter specific features for learning.", "labels": [], "entities": []}, {"text": "Evaluation shows promising results for both the problems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Twitter along with Facebook is widely used social networking site which generates tons of authentic and unauthentic information.", "labels": [], "entities": []}, {"text": "The purpose of twitter varies from people to people.", "labels": [], "entities": []}, {"text": "Twitter has been greatly used as a communication channel and also as an information source (.", "labels": [], "entities": []}, {"text": "However, Twitter like any other social media platform does not always poses authentic information.", "labels": [], "entities": []}, {"text": "It also brings a negative by-product called rumour).", "labels": [], "entities": []}, {"text": "Rumours are the statement which cannot be verified for its correctness.", "labels": [], "entities": []}, {"text": "These rumours may confuse people with the unverified information and drive them in poor decision making.", "labels": [], "entities": [{"text": "decision making", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.7473333775997162}]}, {"text": "In many organizations(political, administration etc.), detection and support for rumour invites great interest from the concerned authorities.", "labels": [], "entities": []}, {"text": "Recently, researchers across the globe have started addressing the challenges related to rumours.", "labels": [], "entities": []}, {"text": "A time sequence classification technique has been proposed for detecting the stance against a rumor .  used sequence of label transitions in treestructured conversations for classifying stance.", "labels": [], "entities": [{"text": "time sequence classification", "start_pos": 2, "end_pos": 30, "type": "TASK", "confidence": 0.6275227467219034}]}, {"text": "A study on speech act classifier for veracity prediction is proposed in.", "labels": [], "entities": [{"text": "speech act classifier", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.6291089355945587}, {"text": "veracity prediction", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8926410675048828}]}, {"text": "One of the earlier work reported on rumour detection and classification had used twitter specific and content based features for the prediction.", "labels": [], "entities": [{"text": "rumour detection and classification", "start_pos": 36, "end_pos": 71, "type": "TASK", "confidence": 0.8740415871143341}]}, {"text": "In this paper we present our proposed system submitted as part of the SemEval-2017 shared task on \"RumourEval: Determining rumour veracity and support for rumours\".", "labels": [], "entities": [{"text": "SemEval-2017 shared task", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.7328298489252726}]}, {"text": "Our system is supervised in nature and uses a diverse set of features (c.f. Section 2.3) for training.", "labels": [], "entities": []}, {"text": "The task involves Twitter conversation thread where for every source tweet a number of direct and nested reply tweets are present.", "labels": [], "entities": []}, {"text": "An example thread is depicted in Table 1.", "labels": [], "entities": []}, {"text": "The task defines two separate sub-problems: A) Support, Deny, Query & Comment (SDQC) classification and B) veracity prediction.", "labels": [], "entities": [{"text": "Support", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9809577465057373}, {"text": "Query & Comment (SDQC) classification", "start_pos": 62, "end_pos": 99, "type": "TASK", "confidence": 0.5907173029014042}, {"text": "veracity prediction", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7031803727149963}]}, {"text": "The first subtask checks the stance of any tweet(source or reply) w.r.t. the underlying rumour.", "labels": [], "entities": []}, {"text": "Reply tweet can be director nested.", "labels": [], "entities": []}, {"text": "Second subtask predicts the veracity of a rumour i.e. true (rumour), false (not rumour) or unverified (its veracity cannot be verified).", "labels": [], "entities": [{"text": "veracity", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9676844477653503}]}, {"text": "Further, there were two variants of the veracity task: closed and open variants.", "labels": [], "entities": []}, {"text": "In closed variant, the veracity prediction has to be made solely from the tweet text only.", "labels": [], "entities": [{"text": "veracity", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9849623441696167}]}, {"text": "In addition usage of extra data (Wikipedia article, news article etc.) was allowed for the open variant.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 presents a brief description of the proposed approach.", "labels": [], "entities": []}, {"text": "Experimental results and discussion is furnished in Section 3.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "The training dataset consists of 272 source tweets for which 3966 replies tweet are present.", "labels": [], "entities": []}, {"text": "For tuning the system, validation set contains 256 replies across 25 source tweets.", "labels": [], "entities": []}, {"text": "Each source and reply tweet has one of the four label for stance detection namely, support, deny, query and comment.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.8772582113742828}]}, {"text": "For veracity prediction, each of the source tweets belongs to one of the three classes i.e. true, false and unverified.", "labels": [], "entities": [{"text": "veracity prediction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9282365441322327}]}, {"text": "The gold standard test dataset has 28 source and 1021 reply tweets.", "labels": [], "entities": [{"text": "gold standard test dataset", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.7282145619392395}]}, {"text": "A detailed statistics is depicted in.", "labels": [], "entities": []}, {"text": "We use scikit learn machine learning package 5 for the implementation.", "labels": [], "entities": []}, {"text": "As defined by shared task, we use classification accuracy and micro-average accuracy as evaluation metrics for SDQC and veracity prediction respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.6006728410720825}, {"text": "micro-average accuracy", "start_pos": 62, "end_pos": 84, "type": "METRIC", "confidence": 0.74985271692276}, {"text": "veracity prediction", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7059451639652252}]}, {"text": "For subtask A, we try various feature combinations to train a SVM classifier.", "labels": [], "entities": []}, {"text": "28.57% 0.807: Evaluation results on test set.", "labels": [], "entities": []}, {"text": "Further, we perform error analysis on the results.", "labels": [], "entities": [{"text": "error", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9644472599029541}]}, {"text": "Confusion matrix for SDQC classification is depicted in.", "labels": [], "entities": [{"text": "SDQC classification", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8687006533145905}]}, {"text": "We observe that most of the classes were confused with the comment class.", "labels": [], "entities": []}, {"text": "The possible reason could be the presence of relatively high number instances for the comment 'class'.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Distribution of source and reply tweets with their labels in the dataset", "labels": [], "entities": []}, {"text": " Table 3: SDQC: Accuracy on Development Set", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9891379475593567}]}, {"text": " Table 4: Veracity: Accuracy on Development Set", "labels": [], "entities": [{"text": "Veracity", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.9338487386703491}, {"text": "Accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.965212881565094}]}, {"text": " Table 5: Evaluation results on test set.", "labels": [], "entities": []}, {"text": " Table 6. We observe that most of  the classes were confused with the comment class.  The possible reason could be the presence of rel- atively high number instances for the comment  'class'. Similarly,", "labels": [], "entities": []}, {"text": " Table 7: Veracity (Open): Confusion Matrix on  test set", "labels": [], "entities": [{"text": "Veracity", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.7105117440223694}]}]}