{"title": [{"text": "SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity between Questions for Community Question Answering", "labels": [], "entities": [{"text": "Soft-Cosine Semantic Similarity between Questions for Community Question Answering", "start_pos": 31, "end_pos": 113, "type": "TASK", "confidence": 0.5942473974492815}]}], "abstractContent": [{"text": "This paper describes the SimBow system submitted at SemEval2017-Task3, for the question-question similarity subtask B.", "labels": [], "entities": []}, {"text": "The proposed approach is a supervised combination of different unsupervised tex-tual similarities.", "labels": [], "entities": []}, {"text": "These textual similarities rely on the introduction of a relation matrix in the classical cosine similarity between bag-of-words, so as to get a soft-cosine that takes into account relations between words.", "labels": [], "entities": []}, {"text": "According to the type of relation matrix embedded in the soft-cosine, semantic or lexical relations can be considered.", "labels": [], "entities": []}, {"text": "Our system ranked first among the official submissions of subtask B.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social networks enable people to post questions, and to interact with other people to obtain relevant answers.", "labels": [], "entities": []}, {"text": "The popularity of forums show that they are able to propose reliable answers.", "labels": [], "entities": []}, {"text": "Due to this tremendous popularity, forums are growing fast, and the first reflex for an internet user is to check with his favorite search engine if a similar question has already been posted.Community Question Answering at SemEval focuses on this task, with 3 different subtasks.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 202, "end_pos": 220, "type": "TASK", "confidence": 0.7240381985902786}]}, {"text": "SubtaskA (resp. subtaskC) aims at re-ranking the comments of one original question (resp.", "labels": [], "entities": []}, {"text": "the comments of a set of 10 related questions), regarding the relevancy to the original questions.", "labels": [], "entities": []}, {"text": "SubtaskB aims at re-ranking 10 related questions proposed by a search engine, regarding the relevancy to the original question.", "labels": [], "entities": []}, {"text": "Subtasks A and C are question-answering tasks.", "labels": [], "entities": []}, {"text": "SubtaskB can be viewed as a pure semantic textual similarity task applied on community questions, with noisy user-generated texts, making it different from SemEval-Task1 (, which focuses on semantic similarity between short well-formed sentences.", "labels": [], "entities": []}, {"text": "In this paper, we only focus on subtaskB, with the purpose of developing semantic textual similarity measures for such noisy texts.", "labels": [], "entities": []}, {"text": "Questionquestion similarity appeared in SemEval2016 ( , and is pursued in SemEval2017 (.", "labels": [], "entities": []}, {"text": "The approaches explored last year were mostly supervised fusion of different similarity measures, some being unsupervised, others supervised.", "labels": [], "entities": []}, {"text": "Among the unsupervised measures, many were based on overlap count between components (from n-grams of words or characters to knowledge-based components such as named entities, frame representations, knowledge graphs, e.g. ()...).", "labels": [], "entities": []}, {"text": "Much attention was also paid for the use of word embeddings (e.g. (), with question-level averaged vectors used directly with a cosine similarity or as input of a neural classifier.", "labels": [], "entities": []}, {"text": "Finally, fusion was often performed with SVMs ( Our motivation in this work was slightly different: we considered that forum data were too noisy to get reliable outputs from linguistic analysis and we wanted to focus on core textual semantic similarity.", "labels": [], "entities": []}, {"text": "Hence, we avoided using any metadata analysis (such as user profile...) to get results that could easily generalize to other similarity tasks.Thus, we explore unsupervised similarity measures, with no external resources, hardly any linguistic processing (except a list of stopwords), relying only on the availability of sufficient unannotated corpora representative of the data.", "labels": [], "entities": []}, {"text": "And we fuse them in a robust and simple supervised framework (logistic regression).", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: in section 2, the core unsupervised similarity measure is presented, the submitted systems are described in section 3, and section 4 presents results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present detailed evaluations of Task3/subtaskB.", "labels": [], "entities": []}, {"text": "Given anew question (aka original question), the task consists in reranking the 10 questions (aka related questions) proposed by a search engine.", "labels": [], "entities": []}, {"text": "A precise description of the corpus and metrics can be found in Task3 description paper (.", "labels": [], "entities": [{"text": "Task3 description paper", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.9058193365732828}]}, {"text": "Results are presented with the MAP evaluation measure, on 3 corpora: dev (50 original questions \u00d7 10 related questions), test2016 (70 original questions \u00d7 10 related questions) and test2017 (88 original questions \u00d7 10 related questions).", "labels": [], "entities": []}, {"text": "It is worth noticing that the MAP scorer used in this campaign is sensitive to the amount of original questions which don't have any relevant related questions in the gold labels.", "labels": [], "entities": [{"text": "MAP scorer", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.8756728172302246}]}, {"text": "In fact, these questions always account fora precision of 0 in the MAP scoring.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9991563558578491}, {"text": "MAP scoring", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.6810181736946106}]}, {"text": "Hence, an Oracle evaluation, giving a score of 1 to all related questions labeled as \"true\", and a score of 0 to all related questions labeled as \"false\" in the gold labels, doesn't provide a 100% MAP but an Oracle MAP which corresponds to the proportion of original questions that have at least 1 relevant related question.", "labels": [], "entities": [{"text": "MAP", "start_pos": 197, "end_pos": 200, "type": "METRIC", "confidence": 0.9663270711898804}]}, {"text": "Hence the upper bound of MAP performances is 86.00% for dev, 88.57% for test2016, and only 67.05% for test2017 (29 original questions without any relevant related question out of 88).", "labels": [], "entities": [{"text": "MAP", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.6146277189254761}]}, {"text": "Another difference between test2016 and test2017 is the average number of \"true\" labels for questions that have at least one relevant associated question (3.7 for test2016 and 2.7 for test2017).", "labels": [], "entities": []}, {"text": "On the overall test2017 is more difficult for the Task.", "labels": [], "entities": []}, {"text": "presents the MAP results obtained for different unsupervised textual similarities.", "labels": [], "entities": []}, {"text": "Here, the focus is made on unsupervised textual similarity measure, and we only present results for the subject+body configuration for both the original and related questions.", "labels": [], "entities": []}, {"text": "Performances of the Information Retrieval system (IR), and of the best system submitted at SemEval2016 As a baseline, we use the baseline token cos defined in SemEval2015-Task2 (, for semantic textual similarity between sentences.", "labels": [], "entities": [{"text": "Information Retrieval system (IR)", "start_pos": 20, "end_pos": 53, "type": "TASK", "confidence": 0.8228039642175039}]}, {"text": "It is a simple cosine similarity between bag-of-tokens binary vectors (a token is a nonwhite-space sequence between 2 white spaces, and weights are 1 or 0).", "labels": [], "entities": []}, {"text": "Performances of baseline pp cos, which is also a cosine of binary vectors but obtained after the pre-processing step show the importance of suitable pre-processing.", "labels": [], "entities": []}, {"text": "baseline pp cos tfidf show the influence of appropriate term weighting oversimple binary coefficients.", "labels": [], "entities": []}, {"text": "Next results reveal significant improvements when introducing a relation matrix Min the soft-cosine metric (cos M ).", "labels": [], "entities": []}, {"text": "When M contains semantic relations, a significant difference is observed on dev, between relations estimated on a general corpus WP (Wikipedia, 2.7 Bwords) and on a specialized corpus QL (Qatar Living, 100 Mwords).", "labels": [], "entities": [{"text": "Qatar Living", "start_pos": 188, "end_pos": 200, "type": "DATASET", "confidence": 0.8912620544433594}]}, {"text": "The difference is much lower for test2016, and even negative for test2017.", "labels": [], "entities": []}, {"text": "On the contrary, the Levenshtein-based M matrix performs best on test2017, whereas its gain is only marginal for dev and test2016.", "labels": [], "entities": []}, {"text": "In all cases, introducing a carefully chosen relation matrix Min the cosine-based similarity measure improves performances.", "labels": [], "entities": []}, {"text": "Finally, the cosine between TF-IDF weighted average word2vec is less effective on dev and test2016, but performs well on test2017.", "labels": [], "entities": []}, {"text": "First, fora given unsupervised textual similarity measure, all the possible combinations of paired texts are evaluated, and we give the result of the subset which gives the best performance on average on dev, test2016, and test2017.", "labels": [], "entities": []}, {"text": "Interestingly, it is the same combination of paired texts which performs best for the 3 textual similarity measure: similarity between subject+body for both questions and subject+body for the original question and comments for the related question.", "labels": [], "entities": []}, {"text": "This last pairing performs poorly alone but is interesting in combination with the first one.", "labels": [], "entities": []}, {"text": "Then we report the results of the submitted systems to the official evaluation.", "labels": [], "entities": []}, {"text": "As can be seen in  we re-trained the systems excluding rrk from features.", "labels": [], "entities": []}, {"text": "Actually, if rrk was helpful for both dev and test2016 corpora, we can see that removing rrk provides better results on test2017, yielding a maximum MAP score of 48.38.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9773270487785339}]}, {"text": "This performance is obtained with the following set of similarities: cos M rel between subject+body, cos M lev between subject + body and subject of the original question and body of the relative question, and wavg \u2212 w2v between subject + body and between subject + body of the original question and comments of the relative question.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: MAP results for unsupervised textual  similarity measures", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7629781365394592}]}, {"text": " Table 2: MAP results for supervised combination  of textual similarity measures", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6169057488441467}]}]}