{"title": [{"text": "SemEval-2017 Task 3: Community Question Answering", "labels": [], "entities": [{"text": "SemEval-2017 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8816357254981995}, {"text": "Community Question Answering", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.6443198720614115}]}], "abstractContent": [{"text": "We describe SemEval2017 Task 3 on Community Question Answering.", "labels": [], "entities": [{"text": "SemEval2017 Task 3", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8706513047218323}, {"text": "Community Question Answering", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.5892986257870992}]}, {"text": "This year, we reran the four subtasks from SemEval-2016: (A) Question-Comment Similarity, (B) Question-Question Similarity, (C) Question-External Comment Similarity, and (D) Rerank the correct answers fora new question in Arabic, providing all the data from 2015 and 2016 for training, and fresh data for testing.", "labels": [], "entities": []}, {"text": "Additionally, we added anew subtask E in order to enable experimentation with Multi-domain Question Duplicate Detection in a larger-scale scenario, using StackExchange subforums.", "labels": [], "entities": [{"text": "Multi-domain Question Duplicate Detection", "start_pos": 78, "end_pos": 119, "type": "TASK", "confidence": 0.7466078698635101}]}, {"text": "A total of 23 teams participated in the task, and submitted a total of 85 runs (36 primary and 49 contrastive) for sub-tasks AD.", "labels": [], "entities": []}, {"text": "Unfortunately, no teams participated in subtask E.", "labels": [], "entities": []}, {"text": "A variety of approaches and features were used by the participating systems to address the different subtasks.", "labels": [], "entities": []}, {"text": "The best systems achieved an official score (MAP) of 88.43, 47.22, 15.46, and 61.16 in subtasks A, B, C, and D, respectively.", "labels": [], "entities": [{"text": "official score (MAP)", "start_pos": 29, "end_pos": 49, "type": "METRIC", "confidence": 0.9549351096153259}]}, {"text": "These scores are better than the base-lines, especially for subtasks A-C.", "labels": [], "entities": []}], "introductionContent": [{"text": "Community Question Answering (CQA) on web forums such as Stack Overflow and Qatar Living, 2 is gaining popularity, thanks to the flexibility of forums to provide information to a user.", "labels": [], "entities": [{"text": "Community Question Answering (CQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7519453217585882}, {"text": "Qatar Living, 2", "start_pos": 76, "end_pos": 91, "type": "DATASET", "confidence": 0.9636797606945038}]}, {"text": "Forums are moderated only indirectly via the community, rather open, and subject to few restrictions, if any, on who can post and answer a question, or what questions can be asked.", "labels": [], "entities": []}, {"text": "On the positive side, a user can freely ask any question and can expect a variety of answers.", "labels": [], "entities": []}, {"text": "On the negative side, it takes efforts to go through the provided answers of varying quality and to make sense of them.", "labels": [], "entities": []}, {"text": "It is not unusual fora popular question to have hundreds of answers, and it is very time-consuming fora user to inspect them all.", "labels": [], "entities": []}, {"text": "Hence, users can benefit from automated tools to help them navigate these forums, including support for finding similar existing questions to anew question, and for identifying good answers, e.g., by retrieving similar questions that already provide an answer to the new question.", "labels": [], "entities": []}, {"text": "Given the important role that natural language processing (NLP) plays for CQA, we have organized a challenge series to promote related research for the past three years.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.7783695161342621}]}, {"text": "We have provided datasets, annotated data and we have developed robust evaluation procedures in order to establish a common ground for comparing and evaluating different approaches to CQA.", "labels": [], "entities": []}, {"text": "In greater detail, in SemEval-2015 Task 3 \"Answer Selection in Community Question Answering\"), we mainly targeted conventional Question Answering (QA) tasks, i.e., answer selection.", "labels": [], "entities": [{"text": "SemEval-2015 Task 3", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7817573547363281}, {"text": "Answer Selection in Community Question Answering", "start_pos": 43, "end_pos": 91, "type": "TASK", "confidence": 0.7092186510562897}, {"text": "Question Answering (QA) tasks", "start_pos": 127, "end_pos": 156, "type": "TASK", "confidence": 0.8498808095852534}, {"text": "answer selection", "start_pos": 164, "end_pos": 180, "type": "TASK", "confidence": 0.9421016871929169}]}, {"text": "In contrast, in), we targeted a fuller spectrum of CQA-specific tasks, moving closer to the real application needs, 4 particularly in Subtask C, which was defined as follows: \"given (i) anew question and (ii) a large collection of question-comment threads created by a user community, rank the comments that are most useful for answering the new question\".", "labels": [], "entities": []}, {"text": "A test question is new with respect to the forum, but can be related to one or more questions that have been previously asked in the forum.", "labels": [], "entities": []}, {"text": "The best answers can come from different question-comment threads.", "labels": [], "entities": []}, {"text": "The threads are independent of each other, the lists of comments are chronologically sorted, and there is meta information, e.g., date of posting, who is the user who asked/answered the question, category the question was asked in, etc.", "labels": [], "entities": []}, {"text": "The comments in a thread are intended to answer the question initiating that thread, but since this is a resource created by a community of casual users, there is a lot of noise and irrelevant material, in addition to the complications of informal language use, typos, and grammatical mistakes.", "labels": [], "entities": []}, {"text": "Questions in the collection can also be related in different ways, although there is in general no explicit representation of this structure.", "labels": [], "entities": []}, {"text": "In addition to Subtask C, we designed subtasks A and B to give participants the tools to create a CQA system to solve subtask C.", "labels": [], "entities": []}, {"text": "Specifically, Subtask A (Question-Comment Similarity) is defined as follows: \"given a question from a questioncomment thread, rank the comments according to their relevance (similarity) with respect to the question.\"", "labels": [], "entities": [{"text": "Question-Comment Similarity)", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.7213539282480875}]}, {"text": "Subtask B (Question-Question Similarity) is defined as follows: \"given anew question, rerank all similar questions retrieved by a search engine, assuming that the answers to the similar questions should also answer the new question.\"", "labels": [], "entities": [{"text": "Question-Question Similarity)", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.7662694851557413}]}, {"text": "The relationship between subtasks A, B, and C is illustrated in.", "labels": [], "entities": []}, {"text": "In the figure, q stands for the new question, q is an existing related question, and c is a comment within the thread of question q . The edge qc relates to the main CQA task (subtask C), i.e., deciding whether a comment fora potentially related question is a good answer to the original question.", "labels": [], "entities": []}, {"text": "This relation captures the relevance of c for q.", "labels": [], "entities": []}, {"text": "The edge qq represents the similarity between the original and the related questions (subtask B).", "labels": [], "entities": []}, {"text": "This relation captures the relatedness of q and q . Finally, the edge q c represents the decision of whether c is a good answer for the question from its thread, q (subtask A).", "labels": [], "entities": []}, {"text": "This relation captures the appropriateness of c for q . In this particular example, q and q are indeed related, and c is a good answer for both q and q.", "labels": [], "entities": []}, {"text": "The participants were free to approach Subtask C with or without solving Subtasks A and B, and participation in the main subtask and/or the two subtasks was optional.", "labels": [], "entities": []}, {"text": "We had three objectives for the first two editions of our task: (i) to focus on semantic-based solutions beyond simple \"bag-of-words\" representations and \"word matching\" techniques; (ii) to study new NLP challenges arising in the CQA scenario, e.g., relations between the comments in a thread, relations between different threads, and question-to-question similarity; and (iii) to facilitate the participation of non-IR/QA experts.", "labels": [], "entities": [{"text": "word matching\"", "start_pos": 155, "end_pos": 169, "type": "TASK", "confidence": 0.7993747591972351}]}], "datasetContent": [{"text": "The official evaluation measure we used to rank the participating systems is Mean Average Precision (\"MAP\"), calculated over the top-10 comments as ranked by a participating system.", "labels": [], "entities": [{"text": "Mean Average Precision (\"MAP\")", "start_pos": 77, "end_pos": 107, "type": "METRIC", "confidence": 0.9722599188486735}]}, {"text": "We further report the results for two unofficial ranking measures, which we also calculated over the top-10 results only: Mean Reciprocal Rank (\"MRR\") and Average Recall (\"AvgRec\").", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (\"MRR\")", "start_pos": 122, "end_pos": 150, "type": "METRIC", "confidence": 0.9536597629388174}, {"text": "Average Recall (\"AvgRec\")", "start_pos": 155, "end_pos": 180, "type": "METRIC", "confidence": 0.9424578070640564}]}, {"text": "Additionally, we report the results for four standard classification measures, which we calculate over the full list of results: Precision, Recall and F 1 (with respect to the Good/Relevant class), and Accuracy.", "labels": [], "entities": [{"text": "Precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9991845488548279}, {"text": "Recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9962060451507568}, {"text": "F 1", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.98762246966362}, {"text": "Accuracy", "start_pos": 202, "end_pos": 210, "type": "METRIC", "confidence": 0.9989905953407288}]}, {"text": "We released a specialized scorer that calculates and returns all the above-mentioned scores.", "labels": [], "entities": []}, {"text": "In CQA archives, the majority of new questions do not have a duplicate in the archive.", "labels": [], "entities": [{"text": "CQA archives", "start_pos": 3, "end_pos": 15, "type": "DATASET", "confidence": 0.9639280438423157}]}, {"text": "We maintained this characteristic in the training, in the development, and in the test data, to stay as close to areal world setting as possible.", "labels": [], "entities": []}, {"text": "This means that for most query questions, the correct result is an empty list.", "labels": [], "entities": []}, {"text": "This has two consequences: (1) a system that always returns an empty list is a challenging baseline to beat, and (2) standard IR evaluation metrics like MAP, which is used in the other subtasks, cannot be used, because they breakdown when the result list is empty or there are no relevant documents fora given query.", "labels": [], "entities": [{"text": "MAP", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.880943775177002}]}, {"text": "To solve this problem we used a modified version of MAP, as proposed by.", "labels": [], "entities": [{"text": "MAP", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.5188906788825989}]}, {"text": "To make sure standard IR evaluation metrics do not breakdown on empty result list queries, add a nominal terminal document to the end of the ranking returned by a system, to indicate where the number of relevant documents ended.", "labels": [], "entities": [{"text": "IR evaluation", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9080018997192383}]}, {"text": "This terminal document has a corresponding gain value of: The result of this adjustment is that queries without relevant documents in the index, receive a MAP score of 1.0 for an empty result ranking.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9809668362140656}]}, {"text": "This is desired, because in such cases, the empty ranking is the correct result.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics about the English CQA-QL dataset. Note that the Potentially Useful class was merged  with Bad at test time for SemEval-2016 Task 3, and was eliminated altogether at SemEval-2017 task 3.", "labels": [], "entities": [{"text": "English CQA-QL dataset", "start_pos": 31, "end_pos": 53, "type": "DATASET", "confidence": 0.8359212279319763}, {"text": "SemEval-2016 Task 3", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.6465628941853842}]}, {"text": " Table 2: Statistics about the CQA-MD corpus.", "labels": [], "entities": [{"text": "CQA-MD corpus", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.9511736333370209}]}, {"text": " Table 3: Statistics on the data for Subtask E.  Shown is the number of query questions; for each  of them, 50 candidate questions were provided.", "labels": [], "entities": [{"text": "Subtask E.", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.864141047000885}]}, {"text": " Table 5: Subtask A, English (Question-Comment Similarity): results for all submissions. The first  column shows the rank of the primary runs with respect to the official MAP score. The second column  contains the team's name and its submission type (primary vs. contrastive). The following columns show  the results for the primary, and then for other, unofficial evaluation measures. The subindices show the  rank of the primary runs with respect to the evaluation measure in the respective column. All results are  presented as percentages. The system marked with a was a late submission.", "labels": [], "entities": [{"text": "Question-Comment Similarity)", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.7506901125113169}]}, {"text": " Table 6: Subtask B, English (Question-Question Similarity): results for all submissions. The first  column shows the rank of the primary runs with respect to the official MAP score. The second column  contains the team's name and its submission type (primary vs. contrastive). The following columns show  the results for the primary, and then for other, unofficial evaluation measures. The subindices show the  rank of the primary runs with respect to the evaluation measure in the respective column. All results are  presented as percentages.", "labels": [], "entities": [{"text": "Question-Question Similarity)", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.7663199007511139}, {"text": "MAP score", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.5720692425966263}]}, {"text": " Table 7: Subtask C, English (Question-External Comment Similarity): results for all submissions.  The first column shows the rank of the primary runs with respect to the official MAP score. The sec- ond column contains the team's name and its submission type (primary vs. contrastive). The follow- ing columns show the results for the primary, and then for other, unofficial evaluation measures. The  subindices show the rank of the primary runs with respect to the evaluation measure in the respective  column. All results are presented as percentages. The system marked with a was a late submission.", "labels": [], "entities": [{"text": "Question-External Comment Similarity)", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.731644295156002}]}, {"text": " Table 8: Subtask D, Arabic (Reranking the correct answers for a new question): results for all  submissions. The first column shows the rank of the primary runs with respect to the official MAP score.  The second column contains the team's name and its submission type (primary vs. contrastive). The  following columns show the results for the primary, and then for other, unofficial evaluation measures.  The subindices show the rank of the primary runs with respect to the evaluation measure in the respective  column. All results are presented as percentages.", "labels": [], "entities": []}, {"text": " Table 9: Subtask E, English (Multi-Domain Duplicate Detection): Baseline results on the test dataset.  The empty result baseline has an empty result list for all queries. The IR baselines are the results of  applying BM25 with perfect truncation. All results are presented as percentages.", "labels": [], "entities": [{"text": "BM25", "start_pos": 218, "end_pos": 222, "type": "DATASET", "confidence": 0.6699084639549255}]}]}