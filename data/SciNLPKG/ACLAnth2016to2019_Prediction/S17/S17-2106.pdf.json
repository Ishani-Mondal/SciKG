{"title": [{"text": "Experiments for Target oriented Sentiment Analysis in Twitter", "labels": [], "entities": [{"text": "Target oriented Sentiment Analysis", "start_pos": 16, "end_pos": 50, "type": "TASK", "confidence": 0.6063363552093506}]}], "abstractContent": [{"text": "This paper describes the system we have used for participating in Subtasks A (Mes-sage Polarity Classification) and B (Topic-Based Message Polarity Classification according to a two-point scale) of SemEval-2017 Task 4 Sentiment Analysis in Twit-ter.", "labels": [], "entities": [{"text": "SemEval-2017 Task 4 Sentiment Analysis", "start_pos": 198, "end_pos": 236, "type": "TASK", "confidence": 0.8430432438850403}]}, {"text": "We used several features with a sentiment lexicon and NLP techniques, Maximum Entropy as a classifier for our system .", "labels": [], "entities": []}], "introductionContent": [{"text": "Text data has been growing dramatically.", "labels": [], "entities": []}, {"text": "We have demands to process and mine from Social networks and online platforms.", "labels": [], "entities": []}, {"text": "Opinions in usergenerated content, are valuable for market and trend analysis.", "labels": [], "entities": []}, {"text": "Processing of sentiment analysis helps us to automatically distinguish from these written opinions.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.8539827466011047}]}, {"text": "This paper describes a participation in SemEval-2017 Task 4 with the ej-sa-2017 system.", "labels": [], "entities": [{"text": "SemEval-2017 Task 4", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8824992775917053}]}, {"text": "We have participated in SemEval-2017 Task 4 on Sentiment Analysis in Twitter, subtasks A (Message Polarity Classification), B (TopicBased Message Polarity Classification)(.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.856469988822937}]}, {"text": "Subtask A is to classify message polarity from given a message that is of positive, negative, or neutral sentiment.", "labels": [], "entities": []}, {"text": "Subtask B is to classify positive or negative sentiment of a tweet towards that topic on a two-point scale.", "labels": [], "entities": []}, {"text": "We utilized a supervised machine learning classifier, having bag-of-word (BoW), lemmas, bigrams of adjective, punctuation based features, and lexicon-based features.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: In Section 2, we present some related work in features and approaches with a lexicon.", "labels": [], "entities": []}, {"text": "In Section 3, this section describes the algorithm and feature representation used to detect sentiment of text.", "labels": [], "entities": []}, {"text": "In Section 4, the experimental results are introduced.", "labels": [], "entities": []}, {"text": "Finally, the conclusions as well as further work are described in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "All: Trainset for our system in Task 4-B.", "labels": [], "entities": []}, {"text": "The classification results are presented in.", "labels": [], "entities": []}, {"text": "In Subtask A, 37 submissions evaluated, the best F1-score value was 0.685, while our result F1-score was 0.539.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9996583461761475}, {"text": "F1-score", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9987695813179016}]}, {"text": "There are 24 submissions in Subtask B, the best F1-score was 0.89 and our F1-score was 0.486.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9993670582771301}, {"text": "F1-score", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9990309476852417}]}], "tableCaptions": [{"text": " Table 1: Trainset for our system in Task 4-A.", "labels": [], "entities": []}, {"text": " Table 2: Trainset for our system in Task 4-B.", "labels": [], "entities": []}, {"text": " Table  3. In Subtask A, 37 submissions evaluated, the  best F1-score value was 0.685, while our result  F1-score was 0.539. There are 24 submissions in  Subtask B, the best F1-score was 0.89 and our F1- score was 0.486.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9992910623550415}, {"text": "F1-score", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9978145360946655}, {"text": "F1-score", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9963850975036621}, {"text": "F1- score", "start_pos": 200, "end_pos": 209, "type": "METRIC", "confidence": 0.995800773302714}]}, {"text": " Table 3: Results achieved by our system", "labels": [], "entities": []}]}