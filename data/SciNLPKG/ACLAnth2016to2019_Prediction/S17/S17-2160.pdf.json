{"title": [{"text": "The Meaning Factory at SemEval-2017 Task 9: Producing AMRs with Neural Semantic Parsing", "labels": [], "entities": [{"text": "SemEval-2017 Task 9", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.8392305572827657}, {"text": "AMRs", "start_pos": 54, "end_pos": 58, "type": "TASK", "confidence": 0.6596772074699402}]}], "abstractContent": [{"text": "We evaluate a semantic parser based on a character-based sequence-to-sequence model in the context of the SemEval-2017 shared task on semantic parsing for AMRs.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 134, "end_pos": 150, "type": "TASK", "confidence": 0.6782173961400986}]}, {"text": "With data augmentation, super characters, and POS-tagging we gain major improvements in performance compared to a baseline character-level model.", "labels": [], "entities": []}, {"text": "Although we improve on previous character-based neural semantic parsing models, the overall accuracy is still lower than a state-of-the-art AMR parser.", "labels": [], "entities": [{"text": "character-based neural semantic parsing", "start_pos": 32, "end_pos": 71, "type": "TASK", "confidence": 0.6658726781606674}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9995765089988708}, {"text": "AMR parser", "start_pos": 140, "end_pos": 150, "type": "TASK", "confidence": 0.7010215520858765}]}, {"text": "An ensemble combining our neural semantic parser with an existing, traditional parser, yields a small gain in performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditional open-domain semantic parsers often use statistical syntactic parsers to derive syntactic structure on which to build a meaning representation.", "labels": [], "entities": []}, {"text": "Recently there have been interesting attempts to view semantic parsing as a translation task, mapping a source language (here: English) to a target language (a logical form of some kind).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7211200445890427}]}, {"text": "used sequence-tosequence and sequence-to-tree neural translation models to produce logical forms from sentences, while and used a similar method to produce AMRs.", "labels": [], "entities": [{"text": "sequence-to-tree neural translation", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.678189734617869}]}, {"text": "From a purely engineering point of view, these are interesting attempts as complex models of the semantic parsing process can be avoided.", "labels": [], "entities": [{"text": "semantic parsing process", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.7815477252006531}]}, {"text": "Yet little is known about the performance and fine-tuning of such parsers, and whether they can reach performance of traditional semantic parsers, or whether they could contribute to performance in an ensemble setting.", "labels": [], "entities": []}, {"text": "In the context of SemEval-2017 Task 9 we aim to shed more light on these questions.", "labels": [], "entities": [{"text": "SemEval-2017 Task 9", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.8074400226275126}]}, {"text": "In particular we participated in Subtask 1, Parsing Biomedical Data, and work with parallel English-AMR training data comprising extracts of scientific articles about cancer pathway discovery.", "labels": [], "entities": [{"text": "cancer pathway discovery", "start_pos": 167, "end_pos": 191, "type": "TASK", "confidence": 0.6301043728987376}]}, {"text": "More specifically, our objectives are (1) try to reproduce the results of, who used character-level models for neural semantic parsing; (2) improve on their results by employing several novel techniques; and (3) combine the resulting neural semantic parser with an existing off-the-shelf AMR parser to reach state-of-the-art results.", "labels": [], "entities": [{"text": "neural semantic parsing", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.666597823301951}, {"text": "AMR parser", "start_pos": 288, "end_pos": 298, "type": "TASK", "confidence": 0.661356970667839}]}], "datasetContent": [{"text": "Our training set consists of the second LDC AMR release (LDC2016E25) containing 39,620 AMRs, as well as the training set of the bio AMR corpus that contains 5,452 AMRs.", "labels": [], "entities": [{"text": "LDC AMR release (LDC2016E25)", "start_pos": 40, "end_pos": 68, "type": "DATASET", "confidence": 0.7262605180342993}, {"text": "bio AMR corpus", "start_pos": 128, "end_pos": 142, "type": "DATASET", "confidence": 0.6575503051280975}]}, {"text": "As development and test set we use the designated development and test partition of the bio AMR corpus, both containing 500 AMRs.", "labels": [], "entities": [{"text": "bio AMR corpus", "start_pos": 88, "end_pos": 102, "type": "DATASET", "confidence": 0.651902049779892}]}, {"text": "HTML-tags are removed from the sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Model specifics for the seq2seq model.", "labels": [], "entities": []}, {"text": " Table 2: Results of the different seq2seq models  on the test set of the biomedical data.", "labels": [], "entities": []}, {"text": " Table 4: Official results on the evaluation set for  both the ensemble and the seq-to-seq neural se- mantic parser.", "labels": [], "entities": []}]}