{"title": [{"text": "L2F/INESC-ID at SemEval-2017 Tasks 1 and 2: Lexical and semantic features in word and textual similarity", "labels": [], "entities": [{"text": "word and textual similarity", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.5686588510870934}]}], "abstractContent": [{"text": "This paper describes our approach to the SemEval-2017 \"Semantic Textual Similar-ity\" and \"Multilingual Word Similarity\" tasks.", "labels": [], "entities": [{"text": "SemEval-2017 \"Semantic Textual Similar-ity\"", "start_pos": 41, "end_pos": 84, "type": "TASK", "confidence": 0.7511317332585653}, {"text": "Multilingual Word Similarity\" tasks", "start_pos": 90, "end_pos": 125, "type": "TASK", "confidence": 0.7324898540973663}]}, {"text": "In the former, we test our approach in both English and Spanish, and use a linguistically-rich set of features.", "labels": [], "entities": []}, {"text": "These move from lexical to semantic features.", "labels": [], "entities": []}, {"text": "In particular, we try to take advantage of the recent Abstract Meaning Representation and SMATCH measure.", "labels": [], "entities": [{"text": "Abstract Meaning Representation", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.7065961956977844}, {"text": "SMATCH", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.8231434226036072}]}, {"text": "Although without state of the art results, we introduce semantic structures in textual similarity and analyze their impact.", "labels": [], "entities": []}, {"text": "Regarding word similarity , we target the English language and combine WordNet information with Word Embeddings.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7376293241977692}]}, {"text": "Without matching the best systems, our approach proved to be simple and effective.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we present two systems that competed in SemEval-2017 tasks \"Semantic Textual Similarity\" and \"Multilingual Word Similarity\", using supervised and unsupervised techniques, respectively.", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.6237074136734009}, {"text": "Multilingual Word Similarity", "start_pos": 108, "end_pos": 136, "type": "TASK", "confidence": 0.5850234230359396}]}, {"text": "For the first task we used lexical features, as well as a semantic feature, based in the Abstract Meaning Representation (AMR) and in the SMATCH measure.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 89, "end_pos": 126, "type": "METRIC", "confidence": 0.8908153374989828}, {"text": "SMATCH measure", "start_pos": 138, "end_pos": 152, "type": "DATASET", "confidence": 0.6099579483270645}]}, {"text": "AMR is a semantic formalism, structured as a graph (.", "labels": [], "entities": []}, {"text": "SMATCH is a metric for comparison of AMRs.", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.48008203506469727}]}, {"text": "To the best of our knowledge, these were not yet applied to Semantic Textual Similarity.", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.6934730807940165}]}, {"text": "In this paper we focus on the contribution of the SMATCH score as a semantic feature for Semantic Textual Similarity, relative to a model based on lexical clues only.", "labels": [], "entities": [{"text": "SMATCH score", "start_pos": 50, "end_pos": 62, "type": "METRIC", "confidence": 0.6728664189577103}, {"text": "Semantic Textual Similarity", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.7887853185335795}]}, {"text": "For word similarity, we test semantic equivalence functions based on WordNet and Word Embeddings (.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7821989953517914}, {"text": "WordNet", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9568334817886353}]}, {"text": "Experiments are performed on test data provided in the SemEval-2017 tasks, and yielded competitive results, although outperformed by other approaches in the official ranking.", "labels": [], "entities": [{"text": "SemEval-2017 tasks", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.6456573009490967}]}, {"text": "The document is organized as follows: in Section 2 we briefly discuss some related work; in Sections 3 and 4, we describe our systems regarding the \"Semantic Textual Similarity\" and \"Multilingual Word Similarity\" tasks, respectively.", "labels": [], "entities": [{"text": "Multilingual Word Similarity\" tasks", "start_pos": 183, "end_pos": 218, "type": "TASK", "confidence": 0.7065709054470062}]}, {"text": "In Section 5 we present the main conclusions and point to future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied all metrics to the train, test and trial examples of the SICK corpus () and train and test examples from previous Semantic Textual Similarity in SemEval, as compiled by.", "labels": [], "entities": [{"text": "SICK corpus", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.7826838791370392}]}, {"text": "Thus, our training dataset is comprised of 24623 vectors (with 9841 from SICK) assigned to a continuous value ranging from 0 to 5.", "labels": [], "entities": [{"text": "SICK", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.7317085862159729}]}, {"text": "Each vector contains our 159 feature values for the similarity among the sentences in an example pair.", "labels": [], "entities": []}, {"text": "We standardized the features by removing the mean and scaling to unit variance and norm.", "labels": [], "entities": []}, {"text": "Then, machine learning algorithms were applied to the feature sets to train a model of our Semantic Textual Similarity representations.", "labels": [], "entities": []}, {"text": "Namely, we employed ensemble learning by gradient boosting with decision trees, and feedforward neural networks (NN) with 1 and 2 fully connected hidden layers.", "labels": [], "entities": []}, {"text": "SMATCH is not available for Spanish, therefore this feature was left out when evaluating Spanish pairs (es-es).", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5700570940971375}]}, {"text": "For English pairs (en-en), the scenarios include: a) only lexical features, or b) an ensemble with lexical features and the SMATCH score (without differentiation).", "labels": [], "entities": [{"text": "SMATCH score", "start_pos": 124, "end_pos": 136, "type": "METRIC", "confidence": 0.8950647413730621}]}, {"text": "Gradient boosting was applied with the default configuration provided in scikit-learn).", "labels": [], "entities": []}, {"text": "NN were configured with single and multiple hidden layers, both with a rectifier as activation function.", "labels": [], "entities": []}, {"text": "The first layer combines the 159 input features (or 158 when not using SMATCH) into 270 neurons, which are either combined into a second layer with 100 neurons, or to the output layer (with 1 neuron).", "labels": [], "entities": []}, {"text": "Finally, we employed the mean square error cost function and the ADAM optimizer (, and fit a model in 100 epochs and batches of 5.", "labels": [], "entities": [{"text": "mean square error cost function", "start_pos": 25, "end_pos": 56, "type": "METRIC", "confidence": 0.7213561654090881}]}, {"text": "Our experiments were run with Tensorflow 0.11 (, with NN implementations from the Keras framework . Gradient boosting implementation is from scikit-learn.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson scores on monolingual evalu- ation, in descending order of performance on the  English track.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9090008735656738}, {"text": "English track", "start_pos": 97, "end_pos": 110, "type": "DATASET", "confidence": 0.9066505134105682}]}, {"text": " Table 2: Predictions for pairs A, B and C where SMATCH excels, grouped by pair.", "labels": [], "entities": [{"text": "Predictions", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9327191114425659}]}, {"text": " Table 3: Results of our functions in some instances of the trial set.", "labels": [], "entities": []}, {"text": " Table 4: Results for the runs submitted for Task 2.1 -English.", "labels": [], "entities": []}]}