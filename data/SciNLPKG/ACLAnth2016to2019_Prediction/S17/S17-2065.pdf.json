{"title": [{"text": "DataStories at SemEval-2017 Task 6: Siamese LSTM with Attention for Humorous Text Comparison", "labels": [], "entities": [{"text": "Humorous Text Comparison", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.7406434416770935}]}], "abstractContent": [{"text": "In this paper we present a deep-learning system that competed at SemEval-2017 Task 6 \"#HashtagWars: Learning a Sense of Humor\".", "labels": [], "entities": [{"text": "SemEval-2017 Task 6", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7416504621505737}, {"text": "HashtagWars: Learning a Sense of Humor", "start_pos": 87, "end_pos": 125, "type": "TASK", "confidence": 0.6037435787064689}]}, {"text": "We participated in Subtask A, in which the goal was, given two Twitter messages, to identify which one is funnier.", "labels": [], "entities": []}, {"text": "We propose a Siamese architecture with bidirectional Long Short-Term Memory (LSTM) networks, augmented with an attention mechanism.", "labels": [], "entities": []}, {"text": "Our system works on the token-level, leveraging word em-beddings trained on a big collection of un-labeled Twitter messages.", "labels": [], "entities": []}, {"text": "We ranked 2 nd in 7 teams.", "labels": [], "entities": []}, {"text": "A post-completion improvement of our model, achieves state-of-the-art results on #HashtagWars dataset.", "labels": [], "entities": [{"text": "HashtagWars dataset", "start_pos": 82, "end_pos": 101, "type": "DATASET", "confidence": 0.7808094918727875}]}], "introductionContent": [{"text": "Computational humor) is an area in computational linguistics and natural language understanding.", "labels": [], "entities": [{"text": "Computational humor)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8163846731185913}, {"text": "natural language understanding", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.6564883987108866}]}, {"text": "Most computational humor tasks focus on the problem of humor detection.", "labels": [], "entities": [{"text": "humor detection", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.757542759180069}]}, {"text": "However) explores the subjective nature of humor, using a dataset of Twitter messages posted in the context of the TV show \"@midnight\".", "labels": [], "entities": []}, {"text": "At each episode during the segment \"Hashtag Wars\", a topic in the form of a hashtag is given and viewers of the show post funny tweets including that hashtag.", "labels": [], "entities": [{"text": "Hashtag Wars\"", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.7303720712661743}]}, {"text": "In the next episode, the show selects the ten funniest tweets and a final winning tweet.", "labels": [], "entities": []}, {"text": "In the past, computational humor tasks have been approached using hand-crafted features;.", "labels": [], "entities": [{"text": "computational humor tasks", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.7819597919782003}]}, {"text": "However, these approaches require a laborious feature-engineering process, which usually leads to missing or redundant features, especially in the case of humor, which is hard to define and consequently hard to model.", "labels": [], "entities": []}, {"text": "Recently, approaches using neural networks, that perform feature-learning, have shown great results outperforming the traditional methods.", "labels": [], "entities": []}, {"text": "In this paper, we present a deep-learning system that we developed for subtask A -\"Pairwise Comparison\".", "labels": [], "entities": []}, {"text": "The goal of the task is, given two tweets about the same topic, to identify which one is funnier.", "labels": [], "entities": []}, {"text": "The labels are applied using the show's relative ranking.", "labels": [], "entities": []}, {"text": "This is a very challenging task, because humor is subjective and the machine learning system must develop a sense of humor similar to that of the show, in order to perform well.", "labels": [], "entities": []}, {"text": "We employ a Siamese neural network, which generates a dense vector representation for each tweet and then uses those representations as features for classification.", "labels": [], "entities": []}, {"text": "For modeling the Twitter messages we use Long Short-Term Memory (LSTM) networks augmented with a contextaware attention mechanism ().", "labels": [], "entities": []}, {"text": "Furthermore, we perform thorough text preprocessing that enables our neural network to learn better features.", "labels": [], "entities": [{"text": "text preprocessing", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7276512980461121}]}, {"text": "Finally, our approach does not rely on any hand-crafted features.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Dataset Statistics for Subtask A.", "labels": [], "entities": []}, {"text": " Table 2: The Results of our submitted and fixed  models, evaluated on the official Semeval test set.  The updated model would have ranked 1 st .", "labels": [], "entities": [{"text": "official Semeval test set", "start_pos": 75, "end_pos": 100, "type": "DATASET", "confidence": 0.7126512974500656}]}, {"text": " Table 3: Comparison on #HastagWars dataset.", "labels": [], "entities": [{"text": "HastagWars dataset", "start_pos": 25, "end_pos": 43, "type": "DATASET", "confidence": 0.9442707896232605}]}]}