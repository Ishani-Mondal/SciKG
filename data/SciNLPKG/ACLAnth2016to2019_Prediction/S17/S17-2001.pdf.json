{"title": [{"text": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation", "labels": [], "entities": [{"text": "Cross-lingual Focused Evaluation", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.6051103075345358}]}], "abstractContent": [{"text": "Semantic Textual Similarity (STS) measures the meaning similarity of sentences.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7453098247448603}]}, {"text": "Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8638705253601074}, {"text": "summarization", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9831236600875854}, {"text": "question answering (QA)", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.7912686884403228}, {"text": "short answer grading", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.5776014029979706}]}, {"text": "The STS shared task is avenue for assessing the current state-of-the-art.", "labels": [], "entities": []}, {"text": "The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data.", "labels": [], "entities": [{"text": "MT quality estimation (MTQE)", "start_pos": 91, "end_pos": 119, "type": "TASK", "confidence": 0.8773214717706045}]}, {"text": "The task obtained strong participation from 31 teams, with 17 participating in all language tracks.", "labels": [], "entities": []}, {"text": "We summarize performance and review a selection of well performing methods.", "labels": [], "entities": []}, {"text": "Analysis highlights common errors, providing insight into the limitations of existing models.", "labels": [], "entities": []}, {"text": "To support ongoing work on semantic representations, the STS Benchmark is introduced as anew shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).", "labels": [], "entities": [{"text": "STS Benchmark", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.8856730163097382}]}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) assesses the degree to which two sentences are semantically equivalent to each other.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8122130334377289}]}, {"text": "The STS task is motivated by the observation that accurately modeling the meaning similarity of sentences is a foundational language understanding problem relevant to numerous applications including: machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems.", "labels": [], "entities": [{"text": "STS task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.8763901889324188}, {"text": "machine translation (MT)", "start_pos": 200, "end_pos": 224, "type": "TASK", "confidence": 0.8200159788131713}, {"text": "summarization, generation", "start_pos": 226, "end_pos": 251, "type": "TASK", "confidence": 0.6561020414034525}, {"text": "question answering (QA)", "start_pos": 253, "end_pos": 276, "type": "TASK", "confidence": 0.7974720358848572}, {"text": "short answer grading", "start_pos": 278, "end_pos": 298, "type": "TASK", "confidence": 0.608885665734609}]}, {"text": "STS enables the evaluation of techniques from a diverse set of domains against a shared interpretable performance criteria.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9662420153617859}]}, {"text": "Semantic inference tasks related to STS include textual entailment (, semantic relatedness () and paraphrase detection ().", "labels": [], "entities": [{"text": "STS", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9621862173080444}, {"text": "paraphrase detection", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.8331963121891022}]}, {"text": "STS differs from both textual entailment and paraphrase detection in that it captures gradations of meaning overlap rather than making binary classifications of particular relationships.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9396517872810364}, {"text": "paraphrase detection", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.927923709154129}]}, {"text": "While semantic relatedness expresses a graded semantic relationship as well, it is non-specific about the nature of the relationship with contradictory material still being a candidate fora high score (e.g., \"night\" and \"day\" are highly related but not particularly similar).", "labels": [], "entities": []}, {"text": "To encourage and support research in this area, the STS shared task has been held annually since 2012, providing avenue for evaluation of state-ofthe-art algorithms and models.", "labels": [], "entities": []}, {"text": "During this time, diverse similarity methods and data sets have been explored.", "labels": [], "entities": []}, {"text": "Early methods focused on lexical semantics, surface form matching and basic syntactic similarity).", "labels": [], "entities": [{"text": "surface form matching", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.6333807706832886}]}, {"text": "During subsequent evaluations, strong new similarity signals emerged, such as's alignment based method.", "labels": [], "entities": []}, {"text": "More recently, deep learning became competitive with top performing feature engineered systems ( ).", "labels": [], "entities": []}, {"text": "The best performance tends to be obtained by ensembling feature engineered and deep learning models ().", "labels": [], "entities": []}, {"text": "Significant research effort has focused on STS over English sentence pairs.", "labels": [], "entities": [{"text": "STS", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9853495955467224}]}, {"text": "2 English STS is a 1 i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet), FrameNet (),), web discussion fora, plagiarism, MT post-editing and Q&A data sets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.9354846477508545}, {"text": "MT post-editing", "start_pos": 172, "end_pos": 187, "type": "TASK", "confidence": 0.8751784265041351}]}, {"text": "Data sets are summarized on: http://ixa2.si.ehu.es/stswiki.", "labels": [], "entities": []}, {"text": "The 2012 and 2013 STS tasks were English only.", "labels": [], "entities": [{"text": "2012 and 2013 STS tasks", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7155519604682923}]}, {"text": "The 2014 and 2015 task included a Spanish track and 2016 had a 1 well-studied problem, with state-of-the-art systems often achieving 70 to 80% correlation with human judgment.", "labels": [], "entities": []}, {"text": "To promote progress in other languages, the 2017 task emphasizes performance on Arabic and Spanish as well as cross-lingual pairings of English with material in Arabic, Spanish and Turkish.", "labels": [], "entities": []}, {"text": "The primary evaluation criteria combines performance on all of the different language conditions except English-Turkish, which was run as a surprise language track.", "labels": [], "entities": []}, {"text": "Even with this departure from prior years, the task attracted 31 teams producing 84 submissions.", "labels": [], "entities": []}, {"text": "STS shared task data sets have been used extensively for research on sentence level similarity and semantic representations (i.a.,;;;;; ;;;;; ;).", "labels": [], "entities": [{"text": "STS shared task data sets", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.7058105349540711}]}, {"text": "To encourage the use of a common evaluation set for assessing new methods, we present the STS Benchmark, a publicly available selection of data from English STS shared tasks).", "labels": [], "entities": [{"text": "STS Benchmark", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.9218697547912598}]}], "datasetContent": [{"text": "The Stanford Natural Language Inference (SNLI) corpus) is the primary evaluation data source with the exception that one of the pilot track on cross-lingual Spanish-English STS.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) corpus", "start_pos": 4, "end_pos": 53, "type": "DATASET", "confidence": 0.570391334593296}, {"text": "cross-lingual Spanish-English STS", "start_pos": 143, "end_pos": 176, "type": "TASK", "confidence": 0.6310484409332275}]}, {"text": "The English tracks attracted the most participation and have the largest use of the evaluation data in ongoing research.", "labels": [], "entities": []}, {"text": "Evaluation data from SNLI tend to have sentences that are slightly shorter than those from prior years of the STS shared task, while the track 4b MT qual-   Similarity scores for our pairings of the SNLI sentences are slightly lower than recent shared task years and much lower than early years.", "labels": [], "entities": [{"text": "track 4b MT qual-   Similarity", "start_pos": 137, "end_pos": 167, "type": "METRIC", "confidence": 0.6534116218487421}]}, {"text": "The change is attributed to differences in data selection and filtering.", "labels": [], "entities": []}, {"text": "The average 2017 similarity score is 2.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 17, "end_pos": 33, "type": "METRIC", "confidence": 0.9700185060501099}]}, {"text": "This section reports participant evaluation results for the SemEval-2017 STS shared task.", "labels": [], "entities": [{"text": "SemEval-2017 STS shared task", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.854892686009407}]}], "tableCaptions": [{"text": " Table 5: Spanish-English training data.", "labels": [], "entities": [{"text": "Spanish-English training data", "start_pos": 10, "end_pos": 39, "type": "DATASET", "confidence": 0.5835397839546204}]}, {"text": " Table 6: Arabic training data.", "labels": [], "entities": [{"text": "Arabic training data", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.6844245791435242}]}, {"text": " Table 8: Arabic-English parallel data.", "labels": [], "entities": []}, {"text": " Table 10: STS 2017 rankings ordered by average correlation across tracks 1-5. Performance is reported  by convention as Pearson's r \u00d7 100. For tracks 1-6, the top ranking result is marked with a \u2022 symbol  and results in bold have no statistically significant difference with the best result on a track, p > 0.05  Williams' t-test", "labels": [], "entities": [{"text": "Pearson's r \u00d7 100", "start_pos": 121, "end_pos": 138, "type": "METRIC", "confidence": 0.9167457342147827}, {"text": "Williams' t-test", "start_pos": 314, "end_pos": 330, "type": "DATASET", "confidence": 0.8375941216945648}]}, {"text": " Table 11: STS Benchmark annotated examples  by genres (rows) and by train, dev. test splits  (columns).", "labels": [], "entities": [{"text": "STS Benchmark", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.9108410775661469}]}, {"text": " Table 12: Difficult English sentence pairs (Track 5) and scores assigned by top performing systems. 19", "labels": [], "entities": []}, {"text": " Table 13: STS Benchmark detailed break-down by  files and years.", "labels": [], "entities": [{"text": "STS Benchmark", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.9162191450595856}]}, {"text": " Table 14: STS Benchmark. Pearson's r \u00d7 100 results for select participants and baseline models.", "labels": [], "entities": [{"text": "STS Benchmark", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.8795914351940155}]}]}