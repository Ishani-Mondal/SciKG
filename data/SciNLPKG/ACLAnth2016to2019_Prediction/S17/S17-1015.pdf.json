{"title": [{"text": "A Mixture Model for Learning Multi-Sense Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings are now a standard technique for inducing meaning representations for words.", "labels": [], "entities": []}, {"text": "For getting good representations , it is important to take into account different senses of a word.", "labels": [], "entities": []}, {"text": "In this paper, we propose a mixture model for learning multi-sense word embeddings.", "labels": [], "entities": []}, {"text": "Our model generalizes the previous works in that it allows to induce different weights of different senses of a word.", "labels": [], "entities": []}, {"text": "The experimental results show that our model outper-forms previous models on standard evaluation tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.9697039127349854}, {"text": "topic models", "start_pos": 89, "end_pos": 101, "type": "TASK", "confidence": 0.8228711485862732}, {"text": "script learning", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.8150419294834137}, {"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.805827409029007}, {"text": "sequence labeling", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.6605056673288345}]}, {"text": "A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (.", "labels": [], "entities": []}, {"text": "However, usually word embedding models do not take into account lexical ambiguity.", "labels": [], "entities": []}, {"text": "For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution.", "labels": [], "entities": []}, {"text": "Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding., and proposed methods to cluster the contexts of each word and then using cluster centroids as vector representations for word senses.,, and extended Word2Vec models ( to learn a vector representation for each sense of a word., and performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. and presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets.,, and directly opt the Word2Vec Skipgram model) for learning the embeddings of words and topics on a topicassigned corpus.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 390, "end_pos": 410, "type": "TASK", "confidence": 0.7694507837295532}, {"text": "Word2Vec", "start_pos": 506, "end_pos": 514, "type": "DATASET", "confidence": 0.9591859579086304}]}, {"text": "One issue in these previous works is that they assign the same weight to every sense of a word.", "labels": [], "entities": []}, {"text": "The central assumption of our work is that each sense of a word given a context, should correspond to a mixture of weights reflecting different association degrees of the word with multiple senses in the context.", "labels": [], "entities": []}, {"text": "The mixture weights will help to model word meaning better.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew model for learning Multi-Sense Word Embeddings (MSWE).", "labels": [], "entities": []}, {"text": "Our MSWE model learns vector representations of a word based on a mixture of its sense representations.", "labels": [], "entities": []}, {"text": "The key difference between MSWE and other models is that we induce the weights of senses while jointly learning the word and sense embeddings.", "labels": [], "entities": []}, {"text": "Specifically, we train a topic model ( to obtain the topic-to-word and document-to-topic probability distributions which are then used to infer the weights of topics.", "labels": [], "entities": []}, {"text": "We use these weights to define a compositional vector representation for each target word to predict its context words.", "labels": [], "entities": []}, {"text": "MSWE thus is different from the topic-based models (, in which we do not use the topic assignments when jointly learning vector representations of words and topics.", "labels": [], "entities": [{"text": "MSWE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8585836887359619}]}, {"text": "Here we not only learn vectors based on the most suitable topic of a word given its context, but we also take into consideration all possible meanings of the word.", "labels": [], "entities": []}, {"text": "The main contributions of our study are: (i) We introduce a mixture model for learning word and sense embeddings (MSWE) by inducing mixture weights of word senses.", "labels": [], "entities": [{"text": "learning word and sense embeddings (MSWE)", "start_pos": 78, "end_pos": 119, "type": "TASK", "confidence": 0.6069211736321449}]}, {"text": "(ii) We show that MSWE performs better than the baseline Word2Vec Skipgram and other embedding models on the word analogy task () and the word similarity task.", "labels": [], "entities": [{"text": "Word2Vec Skipgram", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.9715498089790344}, {"text": "word analogy task", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.8253675699234009}, {"text": "word similarity task", "start_pos": 138, "end_pos": 158, "type": "TASK", "confidence": 0.7898896733919779}]}], "datasetContent": [{"text": "We evaluate MSWE on two different tasks: word similarity and word analogy.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.7060654014348984}, {"text": "word analogy", "start_pos": 61, "end_pos": 73, "type": "TASK", "confidence": 0.7839145362377167}]}, {"text": "We also provide experimental results obtained by the baseline Word2Vec Skip-gram model and other previous works.", "labels": [], "entities": [{"text": "Word2Vec Skip-gram model", "start_pos": 62, "end_pos": 86, "type": "DATASET", "confidence": 0.9647151430447897}]}, {"text": "Note that not all previous results are mentioned in this paper for comparison because the training corpora used inmost previous research work are much larger than ours (.", "labels": [], "entities": []}, {"text": "Also there are differences in the pre-processing steps that could affect the results.", "labels": [], "entities": []}, {"text": "We could also improve obtained results by using a larger training corpus, but this is not central point of our paper.", "labels": [], "entities": []}, {"text": "The objective of our paper is that the embeddings of topic and word can be combined into a single mixture model, leading to good improvements as established empirically.", "labels": [], "entities": []}, {"text": "Following and, we use the Wesbury Lab Wikipedia corpus containing over 2M articles with about 990M words for training.", "labels": [], "entities": [{"text": "Wesbury Lab Wikipedia corpus", "start_pos": 26, "end_pos": 54, "type": "DATASET", "confidence": 0.9922211021184921}]}, {"text": "In the preprocessing step, texts are lowercased and tokenized, numbers are mapped to 0, and punctuation marks are removed.", "labels": [], "entities": []}, {"text": "We extract a vocabulary of 200,000 most frequent word tokens from the pre-processed corpus.", "labels": [], "entities": []}, {"text": "Words not occurring in the vocabulary are mapped to a special token UNK, in which we use the embedding of UNK for unknown words in the benchmark datasets.", "labels": [], "entities": []}, {"text": "We firstly use a small subset extracted from the WS353 dataset () to tune the hyper-parameters of the baseline Word2Vec Skip-gram model for the word similarity task (see Section 3.2 for the task definition).", "labels": [], "entities": [{"text": "WS353 dataset", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.9861696064472198}, {"text": "Word2Vec Skip-gram model", "start_pos": 111, "end_pos": 135, "type": "DATASET", "confidence": 0.9324107567469279}, {"text": "word similarity task", "start_pos": 144, "end_pos": 164, "type": "TASK", "confidence": 0.7739957372347513}]}, {"text": "We then directly use the tuned hyper-parameters for our MSWE variants.", "labels": [], "entities": [{"text": "MSWE", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.8494327664375305}]}, {"text": "Vector size is also a hyperparameter.", "labels": [], "entities": []}, {"text": "While some approaches use a higher number of dimensions to obtain better results, we fix the vector size to be 300 as used by the baseline fora fair comparison.", "labels": [], "entities": []}, {"text": "The vanilla Latent Dirichlet Allocation (LDA) topic model ( is not scalable to a very large corpus, so we explore faster online topic models developed for large corpora.", "labels": [], "entities": [{"text": "vanilla Latent Dirichlet Allocation (LDA) topic", "start_pos": 4, "end_pos": 51, "type": "TASK", "confidence": 0.5655889175832272}]}, {"text": "We train the online LDA topic model) on the training corpus, and use the output of this topic model to compute the mixture weights as in Equation 1.", "labels": [], "entities": []}, {"text": "We also use the same WS353 subset to tune the numbers of topics T \u2208 {50, 100, 200, 300, 400}.", "labels": [], "entities": [{"text": "WS353 subset", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.874447375535965}]}, {"text": "We find that the most suitable numbers are T = 50 and T = 200 then used for all our experiments.", "labels": [], "entities": [{"text": "T", "start_pos": 43, "end_pos": 44, "type": "METRIC", "confidence": 0.9907296299934387}, {"text": "T", "start_pos": 54, "end_pos": 55, "type": "METRIC", "confidence": 0.9817150831222534}]}, {"text": "Here we learn 300-dimensional embeddings with the fixed context size k = 5 (in Equation 2) and K = 10 (in Equation 3) as used by the baseline.", "labels": [], "entities": []}, {"text": "During training, we randomly initialize model parameters (i.e. word and topic embeddings) and then learn them by using SGD with the initial learning rate of 0.01.", "labels": [], "entities": []}, {"text": "We use default parameters in gensim", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The benchmark datasets. WS353:  WordSimilarity-353. RW: Rare-Words. SIMLEX:  SimLex-999. SCWS: Stanford's Contextual Word  Similarities. MEN: The MEN Test Collection.  Each dataset contains similarity scores of human  judgments for pairs of words.", "labels": [], "entities": [{"text": "WS353", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.7834392786026001}, {"text": "WordSimilarity-353", "start_pos": 42, "end_pos": 60, "type": "DATASET", "confidence": 0.9063735008239746}, {"text": "MEN Test Collection", "start_pos": 156, "end_pos": 175, "type": "DATASET", "confidence": 0.6773175001144409}]}, {"text": " Table 2: Spearman's rank correlation (\u03c1 \u00d7 100) for  the word similarity task when using GlobalSim.  Subscripts 50 and 200 denote the online LDA  topic model trained with T = 50 and T = 200  topics, respectively. denotes that our best score  is significantly higher than the score of the base- line (with p < 0.05, online toolkit from http:  //www.philippsinger.info/?p=347). Scores in bold  and underline are the best and second best scores.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.8149302005767822}]}, {"text": " Table 4: Accuracies for the word analogy task. All  our results are significantly higher than the result  of Word2Vec Skip-gram (with two-tail p < 0.001  using McNemar's test).", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9706096053123474}, {"text": "word analogy task", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.8498295148213705}, {"text": "Word2Vec Skip-gram", "start_pos": 110, "end_pos": 128, "type": "DATASET", "confidence": 0.9294934868812561}, {"text": "McNemar's test", "start_pos": 161, "end_pos": 175, "type": "DATASET", "confidence": 0.8575872381528219}]}]}