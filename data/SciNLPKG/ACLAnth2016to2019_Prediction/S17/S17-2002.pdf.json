{"title": [{"text": "SemEval-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity", "labels": [], "entities": [{"text": "Cross-lingual Semantic Word Similarity", "start_pos": 38, "end_pos": 76, "type": "TASK", "confidence": 0.5242595821619034}]}], "abstractContent": [{"text": "This paper introduces anew task on Multilingual and Cross-lingual Semantic Word Similarity which measures the semantic similarity of word pairs within and across five languages: English, Farsi, German, Italian and Spanish.", "labels": [], "entities": [{"text": "Cross-lingual Semantic Word Similarity", "start_pos": 52, "end_pos": 90, "type": "TASK", "confidence": 0.5328111797571182}]}, {"text": "High quality datasets were manually curated for the five languages with high inter-annotator agreements (consistently in the 0.9 ballpark).", "labels": [], "entities": []}, {"text": "These were used for semi-automatic construction often cross-lingual datasets.", "labels": [], "entities": []}, {"text": "17 teams participated in the task, submitting 24 systems in subtask 1 and 14 systems in subtask 2.", "labels": [], "entities": []}, {"text": "Results show that systems that combine statistical knowledge from text corpora, in the form of word embeddings, and external knowledge from lexical resources are best performers in both sub-tasks.", "labels": [], "entities": []}, {"text": "More information can be found on the task website: http://alt.qcri.", "labels": [], "entities": []}, {"text": "org/semeval2017/task2/ .", "labels": [], "entities": []}], "introductionContent": [{"text": "Measuring the extent to which two words are semantically similar is one of the most popular research fields in lexical semantics, with a wide range of Natural Language Processing (NLP) applications.", "labels": [], "entities": []}, {"text": "Examples include Word Sense Disambiguation (), Information Retrieval (), Machine Translation (, Lexical Substitution), Question Answering (), Text Summarization, and Ontology Alignment ).", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.7062325477600098}, {"text": "Information Retrieval", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7922406494617462}, {"text": "Machine Translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.8481549024581909}, {"text": "Question Answering", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.8235975503921509}, {"text": "Text Summarization", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.8098919987678528}, {"text": "Ontology Alignment", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.7575428485870361}]}, {"text": "Moreover, word similarity is generally accepted as the most direct in-vitro evaluation framework for Authors marked with * contributed equally.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6394772082567215}]}, {"text": "word representation, a research field that has recently received massive research attention mainly as a result of the advancements in the use of neural networks for learning dense low-dimensional semantic representations, often referred to as word embeddings ().", "labels": [], "entities": [{"text": "word representation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8100217580795288}]}, {"text": "Almost any application in NLP that deals with semantics can benefit from efficient semantic representation of words.", "labels": [], "entities": []}, {"text": "However, research in semantic representation has in the main focused on the English language only.", "labels": [], "entities": [{"text": "semantic representation", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.8602542877197266}]}, {"text": "This is partly due to the limited availability of word similarity benchmarks in languages other than English.", "labels": [], "entities": []}, {"text": "Given the central role of similarity datasets in lexical semantics, and given the importance of moving beyond the barriers of the English language and developing languageindependent and multilingual techniques, we felt that this was an appropriate time to conduct a task that provides a reliable framework for evaluating multilingual and cross-lingual semantic representation and similarity techniques.", "labels": [], "entities": []}, {"text": "The task has two related subtasks: multilingual semantic similarity (Section 1.1), which focuses on representation learning for individual languages, and crosslingual semantic similarity (Section 1.2), which provides a benchmark for multilingual research that learns unified representations for multiple languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "As for monolingual datasets, we opted fora size of 500 word pairs in order to provide a large enough set to allow reliable evaluation and comparison of the systems.", "labels": [], "entities": []}, {"text": "The following procedure was used for the construction of multilingual datasets:: The set of thirty-four domains.", "labels": [], "entities": []}, {"text": "wide range of domains (Section 2.1.1), (2) through translation of these pairs, we obtained word pairs for the other four languages (Section 2.1.2) and, (3) all word pairs of each dataset were manually scored by multiple annotators (Section 2.1.3).", "labels": [], "entities": []}, {"text": "The dataset creation started with the selection of 500 English words.", "labels": [], "entities": []}, {"text": "One of the main objectives of the task was to provide an evaluation framework that contains named entities and multiword expressions and covers a wide range of domains.", "labels": [], "entities": []}, {"text": "To achieve this, we considered the 34 different domains available in BabelDomains 1, which in the main correspond to the domains of the Wikipedia featured articles page 2 . shows the list of all the 34 domains used for the creation of the datasets.", "labels": [], "entities": []}, {"text": "From each domain, 12 words were sampled in such away as to have at least one multiword expression and two named entities.", "labels": [], "entities": []}, {"text": "In order to include words that may not belong to any of the pre-defined domains, we added 92 extra words whose domain was not decided beforehand.", "labels": [], "entities": []}, {"text": "We also tried to sample these seed words in such away as to have a balanced set across occurrence frequency.", "labels": [], "entities": []}, {"text": "Of the 500 English seed words, 84 (17%) and 83 were, respectively, named entities and multiwords.", "labels": [], "entities": []}, {"text": "For the annotation of the datasets, we adopted the five-point Likert scale of the SemEval-2014 task on Cross-Level Semantic 4 Very similar The two words are synonyms (e.g., midday-noon or motherboard-mainboard).", "labels": [], "entities": [{"text": "Likert scale", "start_pos": 62, "end_pos": 74, "type": "METRIC", "confidence": 0.9549654126167297}]}, {"text": "The remaining four multilingual datasets (i.e., Farsi, German, Italian, and Spanish) were constructed by translating words in the English dataset to the target language.", "labels": [], "entities": []}, {"text": "We had two goals in mind while selecting translation as the construction strategy of these datasets (as opposed to independent word samplings per language): (1) to have comparable datasets across languages in terms of domain coverage, multiword and named entity distribution and to enable an automatic construction of cross-lingual datasets (see Section 2.2).", "labels": [], "entities": []}, {"text": "Each English word pair was translated by two independent annotators.", "labels": [], "entities": []}, {"text": "In the case of disagreement, a third annotator was asked to pick the preferred translation.", "labels": [], "entities": []}, {"text": "While translating, the annotators were shown the word pair along with their initial similarity score, which was provided to help them in selecting the correct translation for the intended meanings of the words.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 84, "end_pos": 100, "type": "METRIC", "confidence": 0.8715460300445557}]}, {"text": "The cross-lingual datasets were automatically created on the basis of the translations obtained with the method described in Section 2.", "labels": [], "entities": []}, {"text": "two languages (e.g., mind-brain in English and mente-cerebro in Spanish), the approach creates two cross-lingual pairs between the two languages (mind-cerebro and brain-mente in the example).", "labels": [], "entities": []}, {"text": "The similarity scores for the constructed crosslingual pairs are computed as the average of the corresponding language-specific scores in the monolingual datasets.", "labels": [], "entities": []}, {"text": "In order to avoid semantic shifts between languages interfering in the process, these pairs are only created if the difference between the corresponding language-specific scores is lower than 1.0.", "labels": [], "entities": []}, {"text": "The full details of the algorithm can be found in Camacho-.", "labels": [], "entities": [{"text": "Camacho-.", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.9528390765190125}]}, {"text": "The approach has been validated by human judges and shown to achieve agreements of around 0.90 with human judges, which is similar to inter-annotator agreements reported in Section 2.1.3.", "labels": [], "entities": []}, {"text": "See for some sample pairs in all monolingual and cross-lingual datasets.", "labels": [], "entities": []}, {"text": "shows the final number of pairs for each language pair.", "labels": [], "entities": []}, {"text": "We carried out the evaluation on the datasets described in the previous section.", "labels": [], "entities": []}, {"text": "The experimental setting is described in Section 3.1 and the results are presented in Section 3.2.", "labels": [], "entities": []}, {"text": "Participating systems were evaluated according to standard Pearson and Spearman correlation mea-sures on all word similarity datasets, with the final official score being calculated as the harmonic mean of Pearson and Spearman correlations).", "labels": [], "entities": []}, {"text": "Systems were allowed to participate in either multilingual word similarity, crosslingual word similarity, or both.", "labels": [], "entities": [{"text": "crosslingual word similarity", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.5718980630238851}]}, {"text": "Each participating system was allowed to submit a maximum of two runs.", "labels": [], "entities": []}, {"text": "For the multilingual word similarity subtask, some systems were multilingual (applicable to different languages), whereas others were monolingual (only applicable to a single language).", "labels": [], "entities": []}, {"text": "While monolingual approaches were evaluated in their respective languages, multilingual and languageindependent approaches were additionally given a global ranking provided that they tested their systems on at least four languages.", "labels": [], "entities": []}, {"text": "The final score of a system was calculated as the average harmonic mean of Pearson and Spearman correlations of the four languages on which it performed best.", "labels": [], "entities": []}, {"text": "Likewise, the participating systems of the crosslingual semantic similarity subtask were allowed to provide a score fora single cross-lingual dataset, but must have provided results for at least six cross-lingual word similarity datasets in order to be considered for the final ranking.", "labels": [], "entities": []}, {"text": "For each system, the global score was computed as the average harmonic mean of Pearson and Spearman correlation on the six cross-lingual datasets on which it provided the best performance.", "labels": [], "entities": [{"text": "Pearson and Spearman correlation", "start_pos": 79, "end_pos": 111, "type": "METRIC", "confidence": 0.8053129017353058}]}], "tableCaptions": [{"text": " Table 3: Average pairwise Pearson correlation among annotators for the five monolingual datasets.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 27, "end_pos": 46, "type": "METRIC", "confidence": 0.8674346804618835}]}, {"text": " Table 4: Example pairs and their ratings (EN: En- glish, DE: German, ES: Spanish, IT: Italian, FA:  Farsi).", "labels": [], "entities": [{"text": "FA:  Farsi", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.8877679308255514}]}, {"text": " Table 6: Pearson (r), Spearman (\u03c1) and official (Final) results of participating systems on the five  monolingual word similarity datasets (subtask 1).", "labels": [], "entities": [{"text": "Pearson (r)", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9473316520452499}, {"text": "Spearman (\u03c1)", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.9708025604486465}]}, {"text": " Table 7: Global results of participating systems  on subtask 1 (multilingual word similarity).", "labels": [], "entities": []}, {"text": " Table 8: Pearson (r), Spearman (\u03c1) and the official (Final) results of participating systems on the ten  cross-lingual word similarity datasets (subtask 2).", "labels": [], "entities": [{"text": "Pearson (r)", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9384071677923203}, {"text": "Spearman (\u03c1)", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.9573245793581009}]}, {"text": " Table 9: Global results of participating systems in  subtask 2 (cross-lingual word similarity).", "labels": [], "entities": []}]}