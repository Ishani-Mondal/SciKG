{"title": [{"text": "Embedded Semantic Lexicon Induction with Joint Global and Local Optimization", "labels": [], "entities": []}], "abstractContent": [{"text": "Creating annotated frame lexicons such as PropBank and FrameNet is expensive and labor intensive.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9481415152549744}]}, {"text": "We present a method to induce an embedded frame lexicon in an minimally supervised fashion using nothing more than unlabeled predicate-argument word pairs.", "labels": [], "entities": []}, {"text": "We hypothesize that aggregating such pair se-lectional preferences across training leads us to a global understanding that captures predicate-argument frame structure.", "labels": [], "entities": []}, {"text": "Our approach revolves around a novel integration between a predictive embedding model and an Indian Buffet Process posterior regularizer.", "labels": [], "entities": []}, {"text": "We show, through our experimental evaluation, that we outper-form baselines on two tasks and can learn an embedded frame lexicon that is able to capture some interesting generalities in relation to hand-crafted semantic frames.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic lexicons such as) and FrameNet () contain information about predicate-argument frame structure.", "labels": [], "entities": []}, {"text": "These frames capture knowledge about the affinity of predicates for certain types of arguments, their number and their semantic nature, regardless of syntactic realization.", "labels": [], "entities": []}, {"text": "For example, PropBank specifies frames in the following manner: These frames provide semantic information such as the fact that \"eat\" is transitive, while \"give\" is ditransitive, or that the beneficiary of one action is a \"patient\", while the other is a \"recipient\".", "labels": [], "entities": []}, {"text": "This structural knowledge is crucial fora number of NLP applications.", "labels": [], "entities": []}, {"text": "Information about frames has been successfully used to drive and improve diverse tasks such as information extraction (, semantic parsing ( and question answering), among others.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.8096267580986023}, {"text": "semantic parsing", "start_pos": 121, "end_pos": 137, "type": "TASK", "confidence": 0.7350451350212097}, {"text": "question answering)", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.8509098887443542}]}, {"text": "However, building these frame lexicons is very expensive and time consuming.", "labels": [], "entities": []}, {"text": "Thus, it remains difficult to port applications from resource-rich languages or domains to data impoverished ones.", "labels": [], "entities": []}, {"text": "The NLP community has tackled this issue along two different lines of unsupervised work.", "labels": [], "entities": []}, {"text": "At the local token level, researchers have attempted to model frame structure by the selectional preference of predicates for certain arguments.", "labels": [], "entities": []}, {"text": "For example, on this problem a good model might assign a high probability to the word \"pasta\" occurring as an argument of the word \"eat\".", "labels": [], "entities": []}, {"text": "Contrastingly, at the global type level, work has focussed on inducing frames by clustering predicates and arguments in a joint framework.", "labels": [], "entities": []}, {"text": "In this case, one is interested in associating predicates such as \"eat\", \"consume\", \"devour\", with a joint clustering of arguments such as \"pasta\", \"chicken\", \"burger\".", "labels": [], "entities": []}, {"text": "While these methods have been useful for several problems, they also have shortcomings.", "labels": [], "entities": []}, {"text": "Selectional preference modelling only captures local predicate-argument affinities, but does not aggregate these associations to arrive at a structural understanding of frames.", "labels": [], "entities": []}, {"text": "Meanwhile, frame induction performs clustering at a global level.", "labels": [], "entities": [{"text": "frame induction", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.8737399280071259}]}, {"text": "But most approaches tend to be algorithmic methods (or some extension thereof) that focus on semantic role labelling.", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.6612656017144521}]}, {"text": "Their lack of portable features or model parameters unfortunately means they cannot be used to solve other applications or problems that require lexicon-level information -such as information extraction or machine translation.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 180, "end_pos": 202, "type": "TASK", "confidence": 0.7849194407463074}, {"text": "machine translation", "start_pos": 206, "end_pos": 225, "type": "TASK", "confidence": 0.7204980999231339}]}, {"text": "Another limitation is that they always depend on high-level linguistic annotation, such as syntactic dependencies, which may not exist in resource-poor settings.", "labels": [], "entities": []}, {"text": "Thus, in this paper we propose to combine the two approaches to induce a frame semantic lexicon in a minimally supervised fashion with nothing more than unlabeled predicate-argument word pairs.", "labels": [], "entities": []}, {"text": "Additionally, we will learn an embedded lexicon that jointly produces embeddings for predicates, arguments and an automatically induced collection of latent slots.", "labels": [], "entities": []}, {"text": "The embeddings provide flexibility for usage in downstream applications, where predicate-argument affinities can be computed at will.", "labels": [], "entities": []}, {"text": "To jointly capture the local and global streams of knowledge we propose a novel integration between a predictive embedding model and the posterior of an Indian Buffet Process.", "labels": [], "entities": []}, {"text": "The embedding model maximizes the predictive accuracy of predicate-argument selectional preference at the local token level, while the posterior of the Indian Buffet process induces an optimal set of latent slots at the global type level that capture the regularities in the learned predicate embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9793720841407776}]}, {"text": "We evaluate our approach and show that our models are able to outperform baselines on both the local and global level of frame knowledge.", "labels": [], "entities": []}, {"text": "At the local level we score higher than a standard predictive embedding model on selectional preference, while at the global level we outperform a syntactic baseline on lexicon overlap with PropBank.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 190, "end_pos": 198, "type": "DATASET", "confidence": 0.9691286683082581}]}, {"text": "Finally, our analysis on the induced latent slots yields insight into some interesting generalities that we are able to capture from unlabeled predicate-argument pairs.", "labels": [], "entities": []}], "datasetContent": [{"text": "In what follows, we detail experimental results on two quantitative evaluation tasks: at the local and global levels of predicate-argument structure.", "labels": [], "entities": []}, {"text": "In particular we evaluate on pseudo disambiguation of selectional preference, and semantic frame lexicon overlap.", "labels": [], "entities": []}, {"text": "We also qualitatively inspect the learned latent relations against handannotated roles.", "labels": [], "entities": []}, {"text": "We first specify the implementational details.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on pseudo disambiguation of se- lectional preference. Numbers are in % accuracy  of distinguishing true arguments from false ones.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9995660185813904}]}, {"text": " Table 2: Results on the lexicon overlap task. Our models outperform the syntactic baseline on all the  metrics.", "labels": [], "entities": []}, {"text": " Table 3: Examples for several predicates with mappings of latent slots to the majority class of the closest  argument vector in the shared embedded space.", "labels": [], "entities": []}]}