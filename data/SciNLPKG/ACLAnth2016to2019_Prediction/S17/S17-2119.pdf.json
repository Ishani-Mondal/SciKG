{"title": [{"text": "YNUDLG at SemEval-2017 Task 4: A GRU-SVM Model for Sentiment Classification and Quantification in Twitter", "labels": [], "entities": [{"text": "YNUDLG", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.6398144364356995}, {"text": "Sentiment Classification and Quantification", "start_pos": 51, "end_pos": 94, "type": "TASK", "confidence": 0.8831522166728973}]}], "abstractContent": [{"text": "Sentiment analysis is one of the central issues in Natural Language Processing and has become more and more important in many fields.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.964192658662796}, {"text": "Natural Language Processing", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.6216095189253489}]}, {"text": "Typical sentiment analysis classifies the sentiment of sentences into several discrete classes (e.g.,positive or negative).", "labels": [], "entities": [{"text": "sentiment analysis classifies the sentiment of sentences", "start_pos": 8, "end_pos": 64, "type": "TASK", "confidence": 0.9141885467938015}]}, {"text": "In this paper we describe our deep learning system(combining GRU and SVM) to solve both two-, three-and five-tweet polarity classifications.", "labels": [], "entities": []}, {"text": "We first trained a gated recurrent neural network using pre-trained word embeddings, then we extracted features from GRU layer and input these features into support vector machine to fulfill both the classification and quantification subtasks.", "labels": [], "entities": []}, {"text": "The proposed approach achieved 37th, 19th, and 14rd places in subtasks A, B, and C, respectively.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis (SA) is afield of knowledge which deals with the analysis of people's opinions, sentiments, evaluations, appraisals, attitudes and emotions towards particular entities (.", "labels": [], "entities": [{"text": "Sentiment analysis (SA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9320172429084778}, {"text": "analysis of people's opinions, sentiments, evaluations", "start_pos": 68, "end_pos": 122, "type": "TASK", "confidence": 0.6803974641693963}]}, {"text": "Typical approaches to sentiment analysis is to classify the sentiment of a sentence into several discrete classes such as positive and negative polarities, or six basic emotions: anger, happiness, fear, sadness, disgust and surprise.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.9379622340202332}]}, {"text": "SA is widely considered to be one of the most popular and challenging, competitive and the hot research area in computational linguistics.", "labels": [], "entities": [{"text": "SA", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9473398327827454}, {"text": "computational linguistics", "start_pos": 112, "end_pos": 137, "type": "TASK", "confidence": 0.7318839132785797}]}, {"text": "There are many ways to tackle the sentiment classification problems, such as random forest, support vector machine (SVM), Bayes classifier.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.9212207198143005}]}, {"text": "In addition, there are many challenges, such as analysis of noise texts (e.g. oral language) in natural language processing tasks, despite numerous notable advances in recently years (e.g.,).", "labels": [], "entities": [{"text": "analysis of noise texts (e.g. oral language)", "start_pos": 48, "end_pos": 92, "type": "TASK", "confidence": 0.7569108539157443}, {"text": "natural language processing tasks", "start_pos": 96, "end_pos": 129, "type": "TASK", "confidence": 0.7026290595531464}]}, {"text": "Based on this, our way is to extract features with Gated Recurrent Unit (GRU) and classify sentences by SVM using these features.", "labels": [], "entities": []}, {"text": "Task 4 subtask A is to classify a tweet's sentiment as positive, negative, or neutral.", "labels": [], "entities": []}, {"text": "Subtask B (Tweet classification according to a two-point scale) requires classifying a tweet's sentiment towards the given topic.", "labels": [], "entities": [{"text": "Tweet classification", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.8476456105709076}]}, {"text": "Similar to B, subtask C is a five-point scale (.", "labels": [], "entities": []}, {"text": "Unlike typical classification approaches, ordinal classification can assign different ratings (e.g., very negative, negative, neutral, positive and very positive) according to the sentiment strength.", "labels": [], "entities": [{"text": "ordinal classification", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7089439928531647}]}, {"text": "This paper presents a system that combine GRU and SVM to process subtasks A, B and C.", "labels": [], "entities": []}, {"text": "Our system uses a GRU neural network with word embeddings) that are slightly fine-tuned) on each training set.", "labels": [], "entities": []}, {"text": "The word embeddings were obtained by training GloVe (Jeffrey Pennington et al,.", "labels": [], "entities": []}, {"text": "2014) on 2 billion tweets that we crawled for this purpose.", "labels": [], "entities": []}, {"text": "These word vectors are then used to build sentence vectors through a recurrent convolutional neural network.", "labels": [], "entities": []}, {"text": "The proposed gated recurrent neural network consists of the GRU layer and SVM classifier.", "labels": [], "entities": []}, {"text": "The choice based on the following two reasons: (1) it is more computational efficient than Convolution Neural Network (CNN) models); (2) unlike CNN, it also can extract long semantic patterns without tuning the parameter when training the model.", "labels": [], "entities": []}, {"text": "Our system architecture is composed of a word embedding layer, dropout layer, GRU layer, a hyperbolic relu layer, SVM classifier and softmax layer.", "labels": [], "entities": []}, {"text": "By capturing features from GRU layer, we obtain training and test data features, and integrate them with given labels as inputs, so SVM classifier can train the parameters.", "labels": [], "entities": []}], "datasetContent": [{"text": "All our experiments have been developed using Keras deep learning library with Theano backend, and with CUDA enabled.", "labels": [], "entities": [{"text": "Keras deep learning library", "start_pos": 46, "end_pos": 73, "type": "DATASET", "confidence": 0.7979349941015244}]}, {"text": "And all our experiments were performed on a computer with Intel Core(TM) i3 @3.4GHz 16GB of RAM and GeForce GTX 1060 GPU.", "labels": [], "entities": []}, {"text": "The hyper-parameters of the network are chosen based on the performance on the dev-test data.", "labels": [], "entities": []}, {"text": "We firstly carryout our system: put data into GRU model, as we known GRU can also train and test dataset at the same time we adjust some important parameters to make our GRU model to be the newest.", "labels": [], "entities": []}, {"text": "Then we define a Theano function, and input para-meter is the GRU network portal, output is the GRU network dense layer before softmax layer.", "labels": [], "entities": []}, {"text": "Using the Theano function, once we throw new data, we can get features that meeting our requirements.", "labels": [], "entities": []}, {"text": "Last we throw these features into SVM so we can get classification results as we hope.", "labels": [], "entities": [{"text": "SVM", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.8768468499183655}]}, {"text": "After experiment we know our system performance well on two classification question, and poor on five classification.", "labels": [], "entities": []}, {"text": "Some factors may cause this: small training data and in five-classification dataset many twitters belong to negative, neutral and positive; our deep model GRU maybe not actually called deep due to number of layers and our manually tuning.: Results for Subtask C \"Tweet classification according to a five-point scale\", English.", "labels": [], "entities": [{"text": "Tweet classification", "start_pos": 263, "end_pos": 283, "type": "TASK", "confidence": 0.8911441266536713}]}, {"text": "The systems are ordered by their MAEMscore (lower is better).", "labels": [], "entities": [{"text": "MAEMscore", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9975804090499878}]}], "tableCaptions": [{"text": " Table 2: Overview of datasets and number of tweets we d- owloaded. The data was divided into training, development  and testing sets", "labels": [], "entities": []}, {"text": " Table 4: Results for Subtask B \"Tweet classification a- ccording to a two-point scale\", English. The systems are  ordered by their", "labels": [], "entities": [{"text": "Tweet classification", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.8363178372383118}]}, {"text": " Table 5: Results for Subtask C \"Tweet classification accor- ding to a five-point scale\", English. The systems are ordered  by their MAEMscore (lower is better).", "labels": [], "entities": [{"text": "MAEMscore", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9954272508621216}]}]}