{"title": [{"text": "IIT-UHH at SemEval-2017 Task 3: Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification", "labels": [], "entities": [{"text": "IIT-UHH at SemEval-2017 Task", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.7663524895906448}, {"text": "Community Question Answering", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.5939308404922485}, {"text": "Implicit Dialogue Identification", "start_pos": 97, "end_pos": 129, "type": "TASK", "confidence": 0.7724151810010275}]}], "abstractContent": [{"text": "In this paper we present the system for Answer Selection and Ranking in Community Question Answering, which we build as part of our participation in SemEval-2017 Task 3.", "labels": [], "entities": [{"text": "Answer Selection", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.9468548893928528}, {"text": "Community Question Answering", "start_pos": 72, "end_pos": 100, "type": "TASK", "confidence": 0.6108623445034027}, {"text": "SemEval-2017 Task 3", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.760339617729187}]}, {"text": "We develop a Support Vector Machine (SVM) based system that makes use of textual, domain-specific, word-embedding and topic-modeling features.", "labels": [], "entities": []}, {"text": "In addition, we propose a novel method for dialogue chain identification in comment threads.", "labels": [], "entities": [{"text": "dialogue chain identification", "start_pos": 43, "end_pos": 72, "type": "TASK", "confidence": 0.6617324650287628}]}, {"text": "Our primary submission won subtask C, outperforming other systems in all the primary evaluation met-rics.", "labels": [], "entities": []}, {"text": "We performed well in other English subtasks, ranking third in subtask A and eighth in subtask B.", "labels": [], "entities": [{"text": "A", "start_pos": 70, "end_pos": 71, "type": "METRIC", "confidence": 0.784416913986206}]}, {"text": "We also developed open source toolkits for all the three En-glish subtasks by the name cQARank 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents the system built for participation in the SemEval-2017 Shared Task 3 on Community Question Answering (CQA).", "labels": [], "entities": [{"text": "SemEval-2017 Shared Task 3 on Community Question Answering (CQA)", "start_pos": 62, "end_pos": 126, "type": "TASK", "confidence": 0.8105193268169056}]}, {"text": "The task aims to classify and rank a candidate text c in relevance to a target text t.", "labels": [], "entities": []}, {"text": "Based on the nature of the candidate and target texts, the main task is subdivided into three subtasks in which the teams are expected to solve the problem of Question-Comment similarity, Question-Question similarity and Question-External Comment similarity (.", "labels": [], "entities": []}, {"text": "In this work, we propose a rich feature-based system for solving these problems.", "labels": [], "entities": []}, {"text": "We create an architecture which integrates textual, semantic and domain-specific features to achieve good results in the proposed task.", "labels": [], "entities": []}, {"text": "Due to the extremely noisy nature of the social forum data, we also develop a 1 https://github.com/TitasNandi/cQARank customized preprocessing pipeline, rather than using the standard tools.", "labels": [], "entities": []}, {"text": "We use Support Vector Machine (SVM) for classification, and its confidence score for ranking.", "labels": [], "entities": [{"text": "classification", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.9614376425743103}, {"text": "confidence score", "start_pos": 64, "end_pos": 80, "type": "METRIC", "confidence": 0.9536121487617493}]}, {"text": "We initially define a generic set of features to develop a robust system for all three subtasks, then include additional features based on the nature of the subtasks.", "labels": [], "entities": []}, {"text": "To adapt the system to subtasks B and C, we include features extracted from the scores of the other subtasks, propagating meaningful information essential in an incremental setting.", "labels": [], "entities": []}, {"text": "We propose a novel method for identification of dialogue groups in the comment thread by constructing a user interaction graph and also incorporate features from this graph in our system.", "labels": [], "entities": []}, {"text": "Our algorithm outputs mutually disjoint groups of users who are involved in conversation with each other in the comment thread.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 describes the related work.", "labels": [], "entities": []}, {"text": "Sections 3, 4, and 5 elucidate the system architecture, features used and algorithms developed.", "labels": [], "entities": []}, {"text": "Section 6 provides experimentation details and reports the official results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extensively experimented with a lot of feature engineering.", "labels": [], "entities": []}, {"text": "Notable features that were discarded in the feature ablation process are: \u2022 Statistical Paraphrasing: We found the top 10 semantically related words corresponding to every word in the comment, based on word vectors and did an n-gram matching (n = 1,2,3) on the extended wordlist.", "labels": [], "entities": []}, {"text": "\u2022 Doc2Vec: We also used Doc2Vec () to generate sentence vectors directly, but these degraded the results.", "labels": [], "entities": []}, {"text": "Our primary submission for subtasks A and B uses SVM with an RBF kernel for classification as this yielded the best results on the dev set.", "labels": [], "entities": []}, {"text": "We also achieved similar results with the linear and L2-regularized logistic regression classifiers and we use these for our contrastive submissions.", "labels": [], "entities": []}, {"text": "All the submissions comprised of same number of features.", "labels": [], "entities": []}, {"text": "For subtask C, we oversample the training data using the SMOTE () technique in the ImbalancedLearn 3 toolkit, due to the highly skewed distribution of labels.", "labels": [], "entities": [{"text": "SMOTE", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.6519656181335449}]}, {"text": "We use regular SMOTE for our primary and SMOTE SVM for our first contrastive submission.", "labels": [], "entities": [{"text": "SMOTE SVM", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.6070496141910553}]}, {"text": "For the sec-ond contrastive submission, we integrate the feature sets of subtasks A and B directly in the feature set of subtask C.", "labels": [], "entities": [{"text": "contrastive submission", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.6197418570518494}]}, {"text": "The feature ablation results on the development set and the results of different runs on the test set are presented in.", "labels": [], "entities": []}, {"text": "It reports the system performance on all evaluation metrics including Mean Average Precision (MAP), Average Recall (AvgRec), Mean Reciprocal Rank (MRR), Precision (P), Recall (R), F1-score (F1) and Accuracy.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 70, "end_pos": 98, "type": "METRIC", "confidence": 0.9598841269810995}, {"text": "Average Recall (AvgRec)", "start_pos": 100, "end_pos": 123, "type": "METRIC", "confidence": 0.9573601841926574}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 125, "end_pos": 151, "type": "METRIC", "confidence": 0.952059934536616}, {"text": "Precision (P)", "start_pos": 153, "end_pos": 166, "type": "METRIC", "confidence": 0.8757543563842773}, {"text": "Recall (R)", "start_pos": 168, "end_pos": 178, "type": "METRIC", "confidence": 0.956001341342926}, {"text": "F1-score (F1)", "start_pos": 180, "end_pos": 193, "type": "METRIC", "confidence": 0.8859934061765671}, {"text": "Accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9986355900764465}]}], "tableCaptions": [{"text": " Table 1: Feature Ablation Results on Development Set and Runs on Test Set", "labels": [], "entities": [{"text": "Ablation", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.5659365653991699}]}]}