{"title": [{"text": "SemEval-2017 Task 12: Clinical TempEval", "labels": [], "entities": [{"text": "SemEval-2017 Task 12", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.884161134560903}, {"text": "TempEval", "start_pos": 31, "end_pos": 39, "type": "TASK", "confidence": 0.393171101808548}]}], "abstractContent": [{"text": "Clinical TempEval 2017 aimed to answer the question: how well do systems trained on annotated timelines for one medical condition (colon cancer) perform in predicting timelines on another medical condition (brain cancer)?", "labels": [], "entities": []}, {"text": "Nine sub-tasks were included , covering problems in time expression identification, event expression identification and temporal relation identification.", "labels": [], "entities": [{"text": "time expression identification", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.6469499965508779}, {"text": "event expression identification", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.6657947897911072}, {"text": "temporal relation identification", "start_pos": 120, "end_pos": 152, "type": "TASK", "confidence": 0.6239589750766754}]}, {"text": "Participant systems were evaluated on clinical and pathology notes from Mayo Clinic cancer patients, annotated with an extension of TimeML for the clinical domain.", "labels": [], "entities": [{"text": "TimeML", "start_pos": 132, "end_pos": 138, "type": "DATASET", "confidence": 0.9244931936264038}]}, {"text": "11 teams participated in the tasks, with the best systems achieving F1 scores above 0.55 for time expressions, above 0.70 for event expressions, and above 0.30 for temporal relations.", "labels": [], "entities": [{"text": "F1", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9995675683021545}]}, {"text": "Most tasks observed about a 20 point drop over Clinical TempEval 2016, where systems were trained and evaluated on the same domain (colon cancer).", "labels": [], "entities": [{"text": "Clinical TempEval", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.5124901980161667}]}], "introductionContent": [{"text": "The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (.", "labels": [], "entities": [{"text": "temporal information extraction", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.6133246521155039}]}, {"text": "In recent years the community has moved toward testing such information extraction systems on clinical data, to address a common need of doctors and clinical researchers to search over timelines of clinical events like symptoms, diseases, and procedures.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.7772683501243591}]}, {"text": "In the Clinical TempEval shared tasks (, participant systems have competed to identify critical components of the timeline of a clinical text: time expressions, event expressions, and temporal relations.", "labels": [], "entities": [{"text": "Clinical TempEval shared tasks", "start_pos": 7, "end_pos": 37, "type": "TASK", "confidence": 0.6008979231119156}]}, {"text": "For example, shows the annotations that a system is expected to produce when given the text: April 23, 2014: The patient did not have any postoperative bleeding so we'll resume chemotherapy with a larger bolus on Friday even if there is slight nausea.", "labels": [], "entities": []}, {"text": "Clinical TempEval 2017 introduced anew aspect to this problem: domain adaptation.", "labels": [], "entities": [{"text": "Clinical TempEval 2017", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.7545326352119446}, {"text": "domain adaptation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7739704251289368}]}, {"text": "Whereas in Clinical TempEval 2015 and 2016, systems were both trained and tested on notes from colon cancer patients, in 2017, systems were trained on colon cancer patients, but tested on brain cancer patients.", "labels": [], "entities": []}, {"text": "The diseases, symptoms, procedures, etc.", "labels": [], "entities": []}, {"text": "vary widely across these two patient populations, and the doctors treating these different kinds of cancer make a variety of different linguistic choices when discussing such patients.", "labels": [], "entities": []}, {"text": "As a result, systems that participated in Clinical TempEval 2017 were faced with a much more challenging task than systems from 2015 or 2016.", "labels": [], "entities": [{"text": "Clinical TempEval 2017", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.5565500954786936}]}], "datasetContent": [{"text": "All of the tasks were evaluated using the standard metrics of precision (P ), recall (R) and F 1 : where S is the set of items predicted by the system and H is the set of items annotated by the humans.", "labels": [], "entities": [{"text": "precision (P )", "start_pos": 62, "end_pos": 76, "type": "METRIC", "confidence": 0.9369404464960098}, {"text": "recall (R)", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9601085335016251}, {"text": "F 1", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9929302334785461}]}, {"text": "Applying these metrics only requires a definition of what is considered an \"item\" for each task.", "labels": [], "entities": []}, {"text": "\u2022 For evaluating the spans of event expressions or time expressions, items were tuples of (begin, end) character offsets.", "labels": [], "entities": []}, {"text": "Thus, systems only received credit for identifying events and times with exactly the same character offsets as the manually annotated ones.", "labels": [], "entities": []}, {"text": "\u2022 For evaluating the attributes of event expressions or time expressions -Class, Contextual Modality, Degree, Polarity and Type -items were tuples of (begin, end, value) where begin and end are character offsets and value is the value that was given to the relevant attribute.", "labels": [], "entities": [{"text": "Degree", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9654521346092224}]}, {"text": "Thus, systems only received credit for an event (or time) attribute if they both found an event (or time) with the correct character offsets and then assigned the correct value for that attribute.", "labels": [], "entities": []}, {"text": "\u2022 For relations between events and the document creation time, items were tuples of (begin, end, value), just as if it were an event attribute.", "labels": [], "entities": []}, {"text": "Thus, systems only received credit if they found a correct event and assigned the correct relation (BEFORE, OVERLAP, BEFORE-OVERLAP, or AFTER) between that event and the document creation time.", "labels": [], "entities": [{"text": "BEFORE", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9930324554443359}, {"text": "OVERLAP", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.7465189695358276}, {"text": "BEFORE-OVERLAP", "start_pos": 117, "end_pos": 131, "type": "METRIC", "confidence": 0.9895680546760559}, {"text": "AFTER)", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9763699769973755}]}, {"text": "\u2022 For narrative container relations, items were tuples of ((begin 1 , end 1 ), (begin 2 , end 2 )), where the begins and ends corresponded to the character offsets of the events or times participating in the relation.", "labels": [], "entities": []}, {"text": "Thus, systems only received credit fora narrative container relation if they found both events/times and correctly assigned a CON-TAINS relation between them.", "labels": [], "entities": []}, {"text": "For narrative container relations, the P and R definitions were modified to take into account temporal closure, where additional relations are deterministically inferred from other relations (e.g., A CON-TAINS B and B CONTAINS C, so A CONTAINS C): Similar measures were used in prior work (UzZaman and Allen, 2011) and TempEval 2013, following the intuition that precision should measure the fraction of systempredicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of human-annotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure).", "labels": [], "entities": [{"text": "precision", "start_pos": 363, "end_pos": 372, "type": "METRIC", "confidence": 0.9987444877624512}, {"text": "recall", "start_pos": 579, "end_pos": 585, "type": "METRIC", "confidence": 0.9957946538925171}]}], "tableCaptions": [{"text": " Table 1: Number of documents, event expressions, time expressions and narrative container relations in  Train, Dev, and Test portions of the THYME data. All colon cancer data was released as part of Clinical  TempEval 2015 and 2016. The Train-10 column is the data from the first 10 patients of the brain cancer  Train data, which was the only additional training data released in Clinical TempEval 2017.", "labels": [], "entities": [{"text": "THYME data", "start_pos": 142, "end_pos": 152, "type": "DATASET", "confidence": 0.9167658090591431}]}, {"text": " Table 2: System performance and annotator agree- ment on TIMEX3 tasks: identifying the time ex- pression's span (character offsets) and class (DATE,  TIME, DURATION, QUANTIFIER, PREPOSTEXP  or SET).", "labels": [], "entities": [{"text": "agree- ment", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.8882116278012594}, {"text": "SET", "start_pos": 194, "end_pos": 197, "type": "METRIC", "confidence": 0.9370594024658203}]}, {"text": " Table 3: System performance and annotator agreement on EVENT tasks: identifying the event expression's  span (character offsets), contextual modality (ACTUAL, HYPOTHETICAL, HEDGED or GENERIC), degree  (MOST, LITTLE or N/A), polarity (POS or NEG) and type (ASPECTUAL, EVIDENTIAL or N/A).", "labels": [], "entities": []}, {"text": " Table 4: System performance and annotator agree- ment on temporal relation tasks: identifying rela- tions between events and the document creation  time (DOCTIMEREL), and identifying narrative  container relations (CONTAINS).", "labels": [], "entities": []}]}