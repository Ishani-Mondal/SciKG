{"title": [{"text": "Predicting Sentiment Polarity and Intensity with Financial Word Embeddings", "labels": [], "entities": [{"text": "Predicting Sentiment Polarity", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8933784564336141}]}], "abstractContent": [{"text": "This paper presents the approach developed at the Faculty of Engineering of University of Porto, to participate in SemEval 2017, Task 5: Fine-grained Sentiment Analysis on Financial Microblogs and News.", "labels": [], "entities": [{"text": "SemEval 2017", "start_pos": 115, "end_pos": 127, "type": "TASK", "confidence": 0.9000085890293121}, {"text": "Sentiment Analysis on Financial Microblogs and News", "start_pos": 150, "end_pos": 201, "type": "TASK", "confidence": 0.8220823066575187}]}, {"text": "The task consisted in predicting areal continuous variable from-1.0 to +1.0 representing the polarity and intensity of sentiment concerning companies/stocks mentioned in short texts.", "labels": [], "entities": []}, {"text": "We modeled the task as a regression analysis problem and combined traditional techniques such as pre-processing short texts, bag-of-words representations and lexical-based features with enhanced financial specific bag-of-embeddings.", "labels": [], "entities": []}, {"text": "We used an external collection of tweets and news headlines mentioning companies/stocks from S&P 500 to create financial word embeddings which are able to capture domain-specific syntactic and semantic similarities.", "labels": [], "entities": [{"text": "S&P 500", "start_pos": 93, "end_pos": 100, "type": "DATASET", "confidence": 0.7351593151688576}]}, {"text": "The resulting approach obtained a cosine similarity score of 0.69 in sub-task 5.1-Microblogs and 0.68 in sub-task 5.2-News Headlines.", "labels": [], "entities": [{"text": "cosine similarity score", "start_pos": 34, "end_pos": 57, "type": "METRIC", "confidence": 0.7339394489924113}]}], "introductionContent": [{"text": "Sentiment Analysis on financial texts has received increased attention in recent years (.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.962595671415329}]}, {"text": "Neverthless, there are some challenges yet to overcome).", "labels": [], "entities": []}, {"text": "Financial texts, such as microblogs or newswire, usually contain highly technical and specific vocabulary or jargon, making the develop of specific lexical and machine learning approaches necessary.", "labels": [], "entities": []}, {"text": "Most of the research in Sentiment Analysis in the financial domain has focused in analyzing subjective text, labeled with explicitly expressed sentiment.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9654182493686676}]}, {"text": "However, it is also common to express financial sentiment in an implicit way.", "labels": [], "entities": []}, {"text": "Business news stories often refer to events that might indicate a positive or negative impact, such as in the news title \"company X will cut 1000 jobs\".", "labels": [], "entities": []}, {"text": "Economic indicators, such as unemployment and future state modifiers such as drop or increase can also provide clues on the implicit sentiment).", "labels": [], "entities": []}, {"text": "Contrary to explicit expressions (subjective utterances), factual text types often contain objective statements that convey a desirable or undesirable fact (.", "labels": [], "entities": []}, {"text": "Recent work proposes to consider all types of implicit sentiment expressions (Van de.", "labels": [], "entities": []}, {"text": "The authors created a fine grained sentiment annotation procedure to identify polar expressions (implicit and explicit expressions of positive and negative sentiment).", "labels": [], "entities": []}, {"text": "A target (company of interest) is identified in each polar expression to identify the sentiment expressions that are relevant.", "labels": [], "entities": []}, {"text": "The annotation procedure also collected information about the polarity and the intensity of the sentiment expressed towards the target.", "labels": [], "entities": []}, {"text": "However, there is still no automatic approach, either lexical-based or machine learning based, that tries to model this annotation scheme.", "labels": [], "entities": []}, {"text": "In this work, we propose to tackle the aforementioned problem by taking advantage of unsupervised learning of word embeddings in financial tweets and financial news headlines to construct a domain-specific syntactic and semantic representation of words.", "labels": [], "entities": []}, {"text": "We combine bag-ofembeddings with traditional approaches, such as pre-processing techniques, bag-of-words and financial lexical-based features to train a regressor for sentiment polarity and intensity.", "labels": [], "entities": []}, {"text": "We study how different regression algorithms perform using all features in two different sub-tasks at SemEval-2017 Task 5: microblogs and news headlines mentioning companies/stocks.", "labels": [], "entities": [{"text": "SemEval-2017 Task 5", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7428480188051859}]}, {"text": "Moreover, we compare Sub-task Company Text Span Sentiment Score 5.1 -Microblogs JPMorgan \"its time to sell banks\" -0.763 5.2 -Headlines Glencore \"Glencore's annual results beat forecasts\" +0.900: Training set examples for both sub-tasks.", "labels": [], "entities": [{"text": "Microblogs", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.8149639964103699}, {"text": "JPMorgan", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.501782238483429}]}, {"text": "how different combinations of features perform in both sub-tasks.", "labels": [], "entities": []}, {"text": "The system source code and word embeddings developed for the competition are publicly available.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We start by describing SemEval-2017 Task 5 and how we created financial-specific word embeddings.", "labels": [], "entities": []}, {"text": "In Section 4 we present the implementation details of the system created for the competition followed by the experimental setup.", "labels": [], "entities": []}, {"text": "We then present the experimental results and analysis, ending with the conclusions of this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to avoid overfitting we created a validation set from the original training datasets provided by the organizers.", "labels": [], "entities": []}, {"text": "We used a 80%-20% split and sampled the validation set using the same distribution as the original training set.", "labels": [], "entities": []}, {"text": "We sorted the examples in the training set by the target variable values and skipped every 5 examples.", "labels": [], "entities": []}, {"text": "Results are evaluated using Cosine similarity ( and Mean Average Error (MAE).", "labels": [], "entities": [{"text": "Cosine similarity", "start_pos": 28, "end_pos": 45, "type": "METRIC", "confidence": 0.8576995730400085}, {"text": "Mean Average Error (MAE)", "start_pos": 52, "end_pos": 76, "type": "METRIC", "confidence": 0.9702145854632059}]}, {"text": "The former gives more importance to differences in the polarity of the predicted sentiment while the latter is concerned with how well the system predicts the intensity of the sentiment.", "labels": [], "entities": []}, {"text": "We opted to model both sub-tasks as single regression problems.", "labels": [], "entities": []}, {"text": "Three different regressors were applied: Random Forests (RF), Support Vector Machines (SVM) and MultiLayer Perceptron (MLP).", "labels": [], "entities": []}, {"text": "Parameter tuning was carried using 10 fold cross validation on the training sets.", "labels": [], "entities": [{"text": "Parameter", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8660449385643005}]}], "tableCaptions": [{"text": " Table 2: Microblog results with all features on val- idation and test sets.", "labels": [], "entities": []}, {"text": " Table 3: Features performance breakdown on test  set using RF.", "labels": [], "entities": []}, {"text": " Table 4: News Headlines results with all features  on validation and test sets.", "labels": [], "entities": []}, {"text": " Table 5: Features performance breakdown on test  set using MLP.", "labels": [], "entities": []}]}