{"title": [{"text": "DT Team at SemEval-2017 Task 1: Semantic Similarity Using Alignments, Sentence-Level Embeddings and Gaussian Mixture Model Output", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe our system (DT Team) submitted at SemEval-2017 Task 1, Semantic Textual Similarity (STS) challenge for English (Track 5).", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) challenge", "start_pos": 67, "end_pos": 110, "type": "TASK", "confidence": 0.7362549347536904}]}, {"text": "We developed three different models with various features including similarity scores calculated using word and chunk alignments, word/sentence em-beddings, and Gaussian Mixture Model (GMM).", "labels": [], "entities": []}, {"text": "The correlation between our sys-tem's output and the human judgments were up to 0.8536, which is more than 10% above baseline, and almost as good as the best performing system which was at 0.8547 correlation (the difference is just about 0.1%).", "labels": [], "entities": []}, {"text": "Also, our system produced leading results when evaluated with a separate STS benchmark dataset.", "labels": [], "entities": [{"text": "STS benchmark dataset", "start_pos": 73, "end_pos": 94, "type": "DATASET", "confidence": 0.8880701263745626}]}, {"text": "The word alignment and sentence embeddings based features were found to be very effective.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7731189131736755}]}], "introductionContent": [{"text": "Measuring the Semantic Textual Similarity (STS) is to quantify the semantic equivalence between given pair of texts (.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 14, "end_pos": 47, "type": "TASK", "confidence": 0.7580287208159765}]}, {"text": "For example, a similarity score of 0 means that the texts are not similar at all while a score of 5 means that they have same meaning.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 15, "end_pos": 31, "type": "METRIC", "confidence": 0.9604642689228058}]}, {"text": "In this paper, we describe our system DT Team and the three different runs that we submitted to this year's SemEval shared task on STS English track (Track 5;).", "labels": [], "entities": [{"text": "SemEval shared task", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.8581956426302592}, {"text": "STS English track", "start_pos": 131, "end_pos": 148, "type": "DATASET", "confidence": 0.7109949390093485}]}, {"text": "We applied Support Vector Regression (SVR), Linear Regression (LR) and Gradient Boosting Regressor (GBR) with various features (see \u00a7 3.4) in order to predict the semantic similarity of texts in a given pair.", "labels": [], "entities": [{"text": "Linear Regression (LR)", "start_pos": 44, "end_pos": 66, "type": "METRIC", "confidence": 0.746078622341156}, {"text": "Gradient Boosting Regressor (GBR)", "start_pos": 71, "end_pos": 104, "type": "METRIC", "confidence": 0.9316178063551585}]}, {"text": "We also report the results of our models when evaluated with a separate STS benchmark dataset created recently by the STS task organizers.", "labels": [], "entities": [{"text": "STS benchmark dataset", "start_pos": 72, "end_pos": 93, "type": "DATASET", "confidence": 0.8678163687388102}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Summary of training data.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.6039949655532837}]}, {"text": " Table 2: Results of our submitted runs on test data  (1 st is the best result among the participants).", "labels": [], "entities": []}]}