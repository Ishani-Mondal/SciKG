{"title": [{"text": "Duluth at SemEval-2017 Task 6: Language Models in Humor Detection", "labels": [], "entities": [{"text": "Humor Detection", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.8827334940433502}]}], "abstractContent": [{"text": "This paper describes the Duluth system that participated in SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor.", "labels": [], "entities": [{"text": "SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor", "start_pos": 60, "end_pos": 119, "type": "TASK", "confidence": 0.7839235934344205}]}, {"text": "The system participated in Subtasks A and B using N-gram language models, ranking highly in the task evaluation.", "labels": [], "entities": []}, {"text": "This paper discusses the results of our system in the development and evaluation stages and from two post-evaluation runs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Humor is an expression of human uniqueness and intelligence and has drawn attention in diverse areas such as linguistics, psychology, philosophy and computer science.", "labels": [], "entities": [{"text": "Humor", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.9529955387115479}]}, {"text": "Computational humor draws from all of these fields and is a relatively new area of study.", "labels": [], "entities": [{"text": "Computational humor", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8595741987228394}]}, {"text": "There is some history of systems that are able to generate humor (e.g.,,).", "labels": [], "entities": []}, {"text": "However, humor detection remains a less explored and challenging problem (e.g.,), (Zhang and Liu, 2014), (,).", "labels": [], "entities": [{"text": "humor detection", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.889091819524765}]}, {"text": "SemEval-2017 Task 6 () also focuses on humor detection by asking participants to develop systems that learn a sense of humor from the Comedy Central TV show, @midnight with Chris Hardwick.", "labels": [], "entities": [{"text": "humor detection", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7739578485488892}, {"text": "Comedy Central TV show, @midnight with Chris Hardwick", "start_pos": 134, "end_pos": 187, "type": "DATASET", "confidence": 0.8811145067214966}]}, {"text": "Our system ranks tweets according to how funny they are by training N-gram language models on two different corpora.", "labels": [], "entities": []}, {"text": "One consisting of funny tweets provided by the task organizers, and the other on a freely available research corpus of news data.", "labels": [], "entities": []}, {"text": "The funny tweet data is made up of tweets that are intended to be humorous responses to a hashtag given by host Chris Hardwick during the program.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present the results from our development stage, the evaluation stage, and two post-evaluation results).", "labels": [], "entities": []}, {"text": "Since we implemented both bigram and trigam language models during the development stage but only results from trigram language models were submitted to the task, we evaluated bigram language models in the post-evaluation stage.", "labels": [], "entities": []}, {"text": "Note that the accuracy and distance measurements listed in are defined by the task organizers (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9995293617248535}]}, {"text": "shows results from the development stage.", "labels": [], "entities": []}, {"text": "These results show that for the tweet data the best setting is to keep the # and @, omit sentence boundaries, be case sensitive, and ignore tokenization.", "labels": [], "entities": []}, {"text": "While using these settings the trigram language model performed better on Subtask B (.887) and the bigram language model performed better on Subtask A (.548).", "labels": [], "entities": []}, {"text": "We decided to rely on trigram language models for the task evaluation since the advantage of bigrams on Subtask A was very slight (.548 versus .543).", "labels": [], "entities": []}, {"text": "For the news data, we found that the best setting was to perform tokenization, omit sentence boundaries, and to be case sensitive.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 65, "end_pos": 77, "type": "TASK", "confidence": 0.9758140444755554}]}, {"text": "Given that trigrams performed most effectively in the development stage, we decided to use those during the evaluation.", "labels": [], "entities": []}, {"text": "shows the results of our system during the task evaluation.", "labels": [], "entities": []}, {"text": "We submitted two runs, one with a trigram language model trained on the tweet data, and another with a trigram language model trained on the news data.", "labels": [], "entities": []}, {"text": "In addition, after the evaluation was concluded we also decided to run the bigram language models as well.", "labels": [], "entities": []}, {"text": "Contrary to what we observed in the development data, the bigram language model actually performed somewhat better than the trigram language model.", "labels": [], "entities": []}, {"text": "In addition, and also contrary to what we observed with the development data, the news data proved generally more effective in the post-evaluation runs than the tweet data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Scored tweets according to the trigram LM. The log probability scores computed based on the  trigram LM are shown in the third column.", "labels": [], "entities": []}, {"text": " Table 2: Development results based on trial dir data. The settings we chose to train LMs are in bold.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation results (bold) and post-evaluation results based on evaluation dir data. The trigram  LM trained on the news data ranked 4th place on Subtask A and 1st place on Subtask B.", "labels": [], "entities": []}]}