{"title": [{"text": "Sheffield at SemEval-2017 Task 9: Transition-based language generation from AMR", "labels": [], "entities": [{"text": "Transition-based language generation", "start_pos": 34, "end_pos": 70, "type": "TASK", "confidence": 0.6440934240818024}, {"text": "AMR", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.48716533184051514}]}], "abstractContent": [{"text": "This paper describes the submission by the University of Sheffield to the SemEval 2017 Abstract Meaning Representation Parsing and Generation task (SemEval 2017 Task 9, Subtask 2).", "labels": [], "entities": [{"text": "SemEval 2017 Abstract Meaning Representation Parsing and Generation task", "start_pos": 74, "end_pos": 146, "type": "TASK", "confidence": 0.8751437597804599}, {"text": "SemEval 2017 Task 9", "start_pos": 148, "end_pos": 167, "type": "TASK", "confidence": 0.5387743934988976}]}, {"text": "We cast language generation from AMR as a sequence of actions (e.g., insert/remove/rename edges and nodes) that progressively transform the AMR graph into a dependency parse tree.", "labels": [], "entities": []}, {"text": "This transition-based approach relies on the fact that an AMR graph can be considered structurally similar to a dependency tree, with a focus on content rather than function words.", "labels": [], "entities": []}, {"text": "An added benefit to this approach is the greater amount of data we can take advantage of to train the parse-to-text linearizer.", "labels": [], "entities": [{"text": "parse-to-text linearizer", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.9458645582199097}]}, {"text": "Our submitted run on the test data achieved a BLEU score of 3.32 and a Trueskill score of-2.204 on automatic and human evaluation respectively.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.986282616853714}, {"text": "Trueskill score", "start_pos": 71, "end_pos": 86, "type": "METRIC", "confidence": 0.9806933999061584}, {"text": "automatic", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9527960419654846}]}], "introductionContent": [{"text": "Abstract meaning representation (AMR) is a formalism representing the meaning of a sentence (or multiple sentences) as a directed, acyclic graph, where each node represents a concept, and each edge represents a relation between concepts (.", "labels": [], "entities": [{"text": "Abstract meaning representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8242531965176264}]}, {"text": "Natural language generation (NLG) from AMRs introduces challenges, as AMR abstracts away from syntactic structure, function words, or inflections. were the first work to perform NLG from AMR; they used a weighted combination of a tree-to-string transducer and a language model to transform the AMR graph into English.", "labels": [], "entities": [{"text": "Natural language generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7634341369072596}]}, {"text": "Later work by proposed segmenting the AMR graph into fragments and generating subphrases from them, using a set of subgraph-to-string rules.", "labels": [], "entities": []}, {"text": "They then cast the problem of ordering these subphrases as a travelling salesman problem.", "labels": [], "entities": []}, {"text": "suggested linearizing the AMR graph using a maximum entropy classifier.", "labels": [], "entities": [{"text": "AMR graph", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.7305029332637787}]}, {"text": "The linearization is then used as input to a phrase-based machine translation system, to produce the final sentence.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.6132670640945435}]}, {"text": "Our submission to SemEval task 9 on AMR-toEnglish Generation is based on inverting previous work on transition-based parsers, which was in turn based on the previous work of.", "labels": [], "entities": [{"text": "SemEval task 9", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.8919230103492737}, {"text": "AMR-toEnglish Generation", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.5740067511796951}]}, {"text": "Beyond inverting the transition from AMR graph to dependency tree, our system also separates the transition in three passes.", "labels": [], "entities": []}, {"text": "Briefly, during the first pass we convert the AMR concepts into content words, during the second pass the structure of the tree is modified (e.g. by inserting, deleting, and moving nodes and edges), while in the third pass missing function words are inserted, and existing words realized in their final form.", "labels": [], "entities": []}, {"text": "To form a natural language sentence, the dependency tree needs only to be linearized; we note that this is not part of the transition, but should be considered a separate post-processing step.", "labels": [], "entities": []}, {"text": "We train a separate classifier for each pass, to learn which action should betaken at each time-step.", "labels": [], "entities": []}, {"text": "2 System description 2.1 Pre-processing During pre-processing the graph structure of the AMR is converted to a tree by identifying each node n with multiple incoming edges in the graph.", "labels": [], "entities": []}, {"text": "Each additional incoming edge is redirected to a duplicate node n (as shown in the transition between stage a and b in).", "labels": [], "entities": []}, {"text": "These duplicate nodes are inserted as leaves in the structure, and maintain no edges to the n's children.", "labels": [], "entities": []}, {"text": "The system randomly determines which of the incoming edges will remain connected with n, and lets the transition system remove duplicate nodes, or move any of n's descendants as required.", "labels": [], "entities": []}, {"text": "During training, we employ the SpaCy dependency parser to construct the dependency tree of the training sentence and obtain part-of-speech tags; the dataset's sentences are already split into tokens.", "labels": [], "entities": []}, {"text": "Heuristics are used to normalize all date occurrences and numeric expressions in both the sentence and dependency tree, to help our system handle temporal and numerical AMR concepts and structures.", "labels": [], "entities": [{"text": "AMR concepts and structures", "start_pos": 169, "end_pos": 196, "type": "TASK", "confidence": 0.8809123039245605}]}, {"text": "Additionally, we construct a simplified version of the dependency tree where articles, auxiliary words, and punctuation, are removed.", "labels": [], "entities": []}, {"text": "This simplified tree is useful for the first and second phases of the transition where the focus is on content words.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Ablation results on the testing set.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9965699911117554}]}]}