{"title": [{"text": "Semantic Frames and Visual Scenes: Learning Semantic Role Inventories from Image and Video Descriptions", "labels": [], "entities": []}], "abstractContent": [{"text": "Frame-semantic parsing and semantic role labelling, that aim to automatically assign semantic roles to arguments of verbs in a sentence, have recently become an active strand of research in NLP.", "labels": [], "entities": [{"text": "Frame-semantic parsing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8634198904037476}, {"text": "semantic role labelling", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.6844127376874288}]}, {"text": "However, to date these methods have relied on a pre-defined inventory of semantic roles.", "labels": [], "entities": []}, {"text": "In this paper, we present a method to automatically learn argument role inventories for verbs from large corpora of text, images and videos.", "labels": [], "entities": []}, {"text": "We evaluate the method against manually constructed role inventories in FrameNet and show that the visual model outperforms the language-only model and operates with a high precision.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.8978129029273987}, {"text": "precision", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.974668025970459}]}], "introductionContent": [{"text": "The theory of frame semantics postulates that our interpretation of word meanings is not limited to isolated concepts, but rather instantiates complex knowledge structures about events and their participants, known as semantic frames.", "labels": [], "entities": []}, {"text": "For instance, the COMMERCIAL TRANS-ACTION frame includes elements such as a seller, a buyer, goods and money which can be mapped to higher-level semantic roles such as agent, patient, instrument etc.", "labels": [], "entities": []}, {"text": "The verbs linked to this frame are buy, sell, pay, cost and charge, each evoking different aspects of the frame.", "labels": [], "entities": []}, {"text": "This theory has been implemented in a lexicalsemantic resource called FrameNet (.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.8538776636123657}]}, {"text": "Each semantic frame is encoded in FrameNet as a list of lexical units that evoke this frame (typically verbs) and the roles that their semantic arguments may take given the scenario represented by the frame.", "labels": [], "entities": []}, {"text": "FrameNet has inspired a direction in NLP research known as semantic role labelling () and frame-semantic parsing (), whose goal is to assign semantic roles to arguments of the verbs in a sentence.", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.6313063899676005}, {"text": "frame-semantic parsing", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.6829421520233154}]}, {"text": "However, these works point out the coverage limitations of the hand-constructed FrameNet database, suggesting that a data-driven frame acquisition method is needed to enable the integration of frame semantics into real-world NLP applications.", "labels": [], "entities": [{"text": "FrameNet database", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.8844066262245178}, {"text": "frame acquisition", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.7139163762331009}]}, {"text": "In this paper, we propose such a method, experimenting with semantic frame induction from linguistic and visual data.", "labels": [], "entities": [{"text": "semantic frame induction", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.6887659629185995}]}, {"text": "Our system first performs clustering of verb arguments to identify their possible semantic roles and then computes the level of association between a given argument role and the verb, thus deriving the structure of the semantic frame in which the verb participates.", "labels": [], "entities": []}, {"text": "Frame semantics emphasizes the relation between our lexical semantic knowledge and our experience in the world, suggesting that semantic frames are not merely a linguistic construct but also a result of our sensory-motor and perceptual experience.", "labels": [], "entities": [{"text": "Frame semantics", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7622480690479279}]}, {"text": "However, frame semantic approaches in NLP typically rely on textual data.", "labels": [], "entities": []}, {"text": "Our method, in contrast, induces semantic frames from both a text corpus and a corpus of tagged images and videos.", "labels": [], "entities": []}, {"text": "We evaluate the method against handconstructed frames in FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9209288358688354}]}, {"text": "Our results show that the visual model outperforms the languageonly model and achieves a high precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9989032745361328}]}, {"text": "This frame induction method can be used to complement existing FrameNets or to construct anew resource of automatically mined semantic frames, free from manual annotation bias.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extracted linguistic features for our model from the British National Corpus (BNC).", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 56, "end_pos": 85, "type": "DATASET", "confidence": 0.9754734834035238}]}, {"text": "We parsed the corpus using the RASP parser () and extracted subject-verb and verb-object relations from its dependency output.", "labels": [], "entities": [{"text": "RASP parser", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.5549237132072449}]}, {"text": "These relations were then used as features for clustering to obtain arguments classes, which we then used as proxies for frame elements, i.e. argument roles.", "labels": [], "entities": []}, {"text": "Webscope Flickr-100M dataset) to extract visual relations between verbs and their arguments.", "labels": [], "entities": [{"text": "Webscope Flickr-100M dataset", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.8812037507692972}]}, {"text": "Flickr-100M contains 99.3 million images and 0.7 million videos with natural language tags for scenes, objects and actions annotated by users.", "labels": [], "entities": [{"text": "Flickr-100M", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9458165764808655}]}, {"text": "We first stem the tags and remove words that are absent in WordNet (e.g. named entities and misspellings).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9551229476928711}]}, {"text": "We then identify their part of speech based on their visual context using the method of and extract verb-noun cooccurrences.", "labels": [], "entities": []}, {"text": "We evaluate the effectiveness of visual information for our task by comparing the model based on vision and language (VIS) to a baseline model using language alone (LING).", "labels": [], "entities": []}, {"text": "In the LING system, the predicate-argument association scores are computed based on verb-argument co-occurrence information extracted from verbsubject, verb-direct object and verb-indirect object relations in the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 213, "end_pos": 216, "type": "DATASET", "confidence": 0.9054298400878906}]}, {"text": "In case of the indirect object relations, the accompanying prepostions were discarded and the noun counts were aggregated.", "labels": [], "entities": []}, {"text": "0.180 defeat fall death tragedy loss collapse decline disaster destruction fate 0.141 girl other woman child person people 0.128 suicide killing offence murder breach crime ...", "labels": [], "entities": [{"text": "defeat fall death tragedy loss collapse decline disaster destruction fate 0.141 girl other woman child person people 0.128 suicide killing offence murder breach crime", "start_pos": 6, "end_pos": 172, "type": "TASK", "confidence": 0.5763229466974735}]}, {"text": "0.113 handle weapon horn knife blade stick sword ankle waist neck wrist 0.095 victim bull teenager prisoner hero gang enemy rider offender youth killer thief driver defender hell 0.086 recession disappointment shock pain frustration embarrassment guilt sensation depression wound 0.030 sister daughter parent relative lover cousin friend wife mother husband brother father 0.020 motive self origin meaning cause secret truth ...", "labels": [], "entities": [{"text": "handle weapon horn knife blade stick sword ankle waist neck wrist 0.095 victim bull teenager prisoner hero gang enemy rider offender youth killer thief driver defender hell 0.086 recession disappointment shock pain frustration embarrassment guilt sensation depression wound", "start_pos": 6, "end_pos": 279, "type": "TASK", "confidence": 0.7185299757279848}]}, {"text": "0.018 official officer inspector journalist detective constable police policeman reporter: System output for kill Evaluation setup.", "labels": [], "entities": [{"text": "official officer inspector journalist detective constable police policeman reporter", "start_pos": 6, "end_pos": 89, "type": "TASK", "confidence": 0.5008602837721506}]}, {"text": "In order to investigate the role of visual information for different types of verbs, we selected 25 concrete verbs (e.g. cut, throw, swim) and 25 abstract verbs (e.g. trust, prepare, cheat), according to the MRC concreteness database.", "labels": [], "entities": [{"text": "MRC concreteness database", "start_pos": 208, "end_pos": 233, "type": "DATASET", "confidence": 0.891667107741038}]}, {"text": "The verb was considered concrete if its concreteness score was 400 and abstract if it was < 400.", "labels": [], "entities": []}, {"text": "We extracted the 10 highest-ranked verb-argument class pairings produced by the system for each verb.", "labels": [], "entities": []}, {"text": "Each pairing was then evaluated against the argument roles listed for this verb in FrameNet via manual comparison.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.9218683242797852}]}, {"text": "This resulted in a dataset of 500 verbargument pairings for VIS and 500 for LING.", "labels": [], "entities": [{"text": "VIS", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.44826826453208923}]}, {"text": "The pairing was considered correct if the argument cluster corresponded to the semantic type of the role listed in FrameNet and contained nouns listed in the linguistic examples (if these were provided in FrameNet).", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.940777063369751}, {"text": "FrameNet", "start_pos": 205, "end_pos": 213, "type": "DATASET", "confidence": 0.923812747001648}]}, {"text": "We have evaluated the system performance in terms of precision at top 10 argument classes and recall of the Core Frame Elements (FEs) among the top 10 argument classes.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9992857575416565}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9995631575584412}]}], "tableCaptions": []}