{"title": [{"text": "ECNU at SemEval-2017 Task 1: Leverage Kernel-based Traditional NLP features and Neural Networks to Build a Universal Model for Multilingual and Cross-lingual Semantic Textual Similarity", "labels": [], "entities": []}], "abstractContent": [{"text": "To model semantic similarity for multilingual and cross-lingual sentence pairs, we first translate foreign languages into En-glish, and then build an efficient mono-lingual English system with multiple NLP features.", "labels": [], "entities": []}, {"text": "Our system is further supported by deep learning models and our best run achieves the mean Pearson correlation 73.16% in primary track.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 91, "end_pos": 110, "type": "METRIC", "confidence": 0.8883053362369537}]}], "introductionContent": [{"text": "Sentence semantic similarity is the building block of natural language understanding.", "labels": [], "entities": [{"text": "Sentence semantic similarity", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9293148120244344}, {"text": "natural language understanding", "start_pos": 54, "end_pos": 84, "type": "TASK", "confidence": 0.6601954102516174}]}, {"text": "Previous Semantic Textual Similarity (STS) tasks in SemEval focused on judging sentence pairs in English and achieved great success.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 9, "end_pos": 42, "type": "TASK", "confidence": 0.7974187533060709}]}, {"text": "In SemEval-2017 STS shared task concentrates on the evaluation of sentence semantic similarity in multilingual and cross-lingual (.", "labels": [], "entities": [{"text": "SemEval-2017 STS", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.389900803565979}, {"text": "evaluation of sentence semantic similarity", "start_pos": 52, "end_pos": 94, "type": "TASK", "confidence": 0.6646937370300293}]}, {"text": "There are two challenges in modeling multilingual and crosslingual sentence similarity.", "labels": [], "entities": []}, {"text": "On the one hand, this task requires human linguistic expertise to design specific features due to the different characteristics of languages.", "labels": [], "entities": []}, {"text": "On the other hand, lack of enough training data fora particular language would lead to a poor performance.", "labels": [], "entities": []}, {"text": "The SemEval-2017 STS shared task assesses the ability of participant systems to estimate the degree of semantic similarity between monolingual and cross-lingual sentences in Arabic, English and Spanish, which is organized into a set of six secondary sub-tracks (Track 1 to Track 6) and a single combined primary track (Primary Track) achieved by submitting results for all of the secondary sub-tracks.", "labels": [], "entities": [{"text": "SemEval-2017 STS shared task", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.7070780247449875}]}, {"text": "Specifically, track 1, 3 and 5 are to determine STS scores for monolingual sentence pairs in Arabic, Spain and English, respectively.", "labels": [], "entities": [{"text": "STS scores", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9584170281887054}]}, {"text": "Track 2, 4, and 6 involve estimating STS scores for cross-lingual sentence pairs from the combination of two particular languages, i.e., Arabic-English, Spanish-English and surprise language (here is Turkish)-English cross-lingual pairs.", "labels": [], "entities": [{"text": "STS scores", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9277745485305786}]}, {"text": "Given two sentences, a continuous valued similarity score on a scale from 0 to 5 is returned, with 0 indicating that the semantics of the sentences are completely independent and 5 signifying semantic equivalence.", "labels": [], "entities": [{"text": "continuous valued similarity score", "start_pos": 23, "end_pos": 57, "type": "METRIC", "confidence": 0.638983428478241}]}, {"text": "The system is assessed by computing the Pearson correlation between system returned semantic similarity scores and human judgements.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 40, "end_pos": 59, "type": "METRIC", "confidence": 0.9699293673038483}]}, {"text": "To address this task, we first translate all sentences into English through the state-of-the-art machine translation (MT) system, i.e., Google Translator 1 . Then we adopt a combination method to build a universal model to estimate semantic similarity, which consists of traditional natural language processing (NLP) methods and deep learning methods.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.846788513660431}]}, {"text": "For traditional NLP methods, we design multiple effective NLP features to depict the semantic matching degree and then supervised machine learning-based regressors are trained to make prediction.", "labels": [], "entities": []}, {"text": "For neural networks methods, we first obtain distributed representations for each sentence in sentence pairs and then feed these representations into end-to-end neural networks to output similarity scores.", "labels": [], "entities": []}, {"text": "Finally, the scores returned by the regressors with traditional NLP methods and by the neural network models are equally averaged to get a final score to estimate semantic similarity.", "labels": [], "entities": []}, {"text": "shows the overall architecture of our system, which consists of the following three modules: Traditional NLP Module is to extracts two kinds of NLP features.", "labels": [], "entities": []}, {"text": "The sentence pair matching features are to directly calculate the similarity of two sentences from several aspects and the single sentence features are to first represent each sentence in NLP method and then to adopt kernelbased method to calculate the similarity of two sentences.", "labels": [], "entities": []}, {"text": "All these NLP-based similarity scores act as features to build regressors to make prediction.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets: SemEval-2017 provided 7 tracks in monolingual and cross-lingual language pairs.", "labels": [], "entities": []}, {"text": "We first translate all sentences into English via Google Translator and then we build a universal model on only English pairs.", "labels": [], "entities": []}, {"text": "The training set we used is all the monolingual English dataset from SemEval STS task) consisting of 13, 592 sentence pairs.", "labels": [], "entities": [{"text": "SemEval STS task", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.5745342373847961}]}, {"text": "For each track, we grant the training datasets provided by SemEval-2017 as development set.", "labels": [], "entities": []}, {"text": "lists the statistics of the development and the test data for each track in SemEval-2017.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Feature comparison on English STS 2016, the last three are top three systems in STS 2016", "labels": [], "entities": [{"text": "English STS 2016", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.9138050278027853}]}, {"text": " Table 5: Algorithms comparison on English STS 2016 datasets", "labels": [], "entities": [{"text": "English STS 2016 datasets", "start_pos": 35, "end_pos": 60, "type": "DATASET", "confidence": 0.9552512317895889}]}, {"text": " Table 6: Pearson correlations on Cross-lingual  STS 2016, the last row is the top system in 2016.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8661107420921326}, {"text": "Cross-lingual  STS", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.6600236892700195}]}, {"text": " Table 7: Pearson correlations on Spanish-English  WMT. MT(es) 3 is calculated using their translated  Spanish-Spanish form. We did not perform cross  validation in deep learning models and did not en- semble them due to time constraint.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9195698499679565}, {"text": "WMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.6932050585746765}]}]}