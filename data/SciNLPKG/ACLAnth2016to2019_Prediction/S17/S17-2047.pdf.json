{"title": [{"text": "EICA Team at SemEval-2017 Task 3: Semantic and Metadata-based Features for Community Question Answering", "labels": [], "entities": [{"text": "Community Question Answering", "start_pos": 75, "end_pos": 103, "type": "TASK", "confidence": 0.6008129318555196}]}], "abstractContent": [{"text": "We describe our system for participating in SemEval-2017 Task 3 on Community Question Answering.", "labels": [], "entities": [{"text": "SemEval-2017 Task 3 on Community Question Answering", "start_pos": 44, "end_pos": 95, "type": "TASK", "confidence": 0.7896503635815212}]}, {"text": "Our approach relies on combining a rich set of various types of features: semantic and metadata.", "labels": [], "entities": []}, {"text": "The most important types turned out to be the metadata feature and the semantic vectors trained on QatarLiving data.", "labels": [], "entities": [{"text": "QatarLiving data", "start_pos": 99, "end_pos": 115, "type": "DATASET", "confidence": 0.9536228775978088}]}, {"text": "In the main Subtask C, our primary submission was ranked fourth, with a MAP of 13.48 and accuracy of 97.08.", "labels": [], "entities": [{"text": "MAP", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9996222257614136}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9998123049736023}]}, {"text": "In Subtask A, our primary submission get into the top 50%.", "labels": [], "entities": []}], "introductionContent": [{"text": "SemEval-2017 Task 3 on Community Question Answering () aims to solve a real-life application problem.", "labels": [], "entities": [{"text": "Community Question Answering", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.625355045000712}]}, {"text": "The main subtask C (Question-External Comment Similarity) asks to find an answer in the forum that is appropriate as a response to a newly posted question.", "labels": [], "entities": [{"text": "Question-External Comment Similarity)", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.6684083864092827}]}, {"text": "This is achieved by retrieving similar questions and ranking their answers with respect to the new question.", "labels": [], "entities": []}, {"text": "Three additional supporting subtasks are defined: Subtask A (Question-Comment Similarity): Given a question from a question-comment thread, rank the comments within the thread based on their relevance with respect to the question.", "labels": [], "entities": [{"text": "Question-Comment Similarity)", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.7080471714337667}]}, {"text": "The comments in a question-comment thread are annotated as Good, PotentiallyUseful and Bad.", "labels": [], "entities": []}, {"text": "A good ranking is the one that ranks all Good comments above PotentiallyUseful and Bad ones.", "labels": [], "entities": []}, {"text": "Subtask B (Question-Question Similarity): Given anew question, re-rank the similar questions retrieved by a search engine with respect to that question.", "labels": [], "entities": [{"text": "Question-Question Similarity)", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.760586162408193}]}, {"text": "The potentially relevant questions are annotated as PerfectMatch, Relevant and Irrelevant with respect to the original question.", "labels": [], "entities": [{"text": "Relevant", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9629769921302795}, {"text": "Irrelevant", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9477560520172119}]}, {"text": "A good ranking is the one that the PerfectMatch and the Relevant questions are both ranked above the Irrelevant ones.", "labels": [], "entities": []}, {"text": "Subtask C (Question-External Comment Similarity): Given anew question and the set of the first 10 related questions (retrieved by a search engine), each associated with its first 10 comments appearing in its thread.", "labels": [], "entities": [{"text": "Question-External Comment Similarity)", "start_pos": 11, "end_pos": 48, "type": "TASK", "confidence": 0.7295638173818588}]}, {"text": "Re-rank the 100 comments (10 questions \u00d7 10 comments) according to their relevance with respect to the original question.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents the evaluation of the SemEval-2017 Task 3 on CQA (.", "labels": [], "entities": [{"text": "SemEval-2017 Task", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.8488967716693878}, {"text": "CQA", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.8620710372924805}]}, {"text": "Note that for our system EICA we did not use data from SemEval-2015 CQA.", "labels": [], "entities": [{"text": "EICA", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.9260045886039734}, {"text": "SemEval-2015 CQA", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.8350383341312408}]}, {"text": "The best result of each partition and subtask is highlighted.", "labels": [], "entities": []}, {"text": "Our percentage comparisons all use absolute values.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Results of Subtask A: English Question-Comment Similarity(test set for 2016).", "labels": [], "entities": [{"text": "English Question-Comment Similarity", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.6087869803110758}]}, {"text": " Table 2  Results of Subtask A: English Question-Comment Similarity(test set for 2017).", "labels": [], "entities": [{"text": "English Question-Comment Similarity", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.6159438292185465}]}, {"text": " Table 3  Results of Subtask B: English Question-Question Similarity(test set for 2016).", "labels": [], "entities": [{"text": "English Question-Question Similarity", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.5843969583511353}]}, {"text": " Table 4  Results of Subtask B: English Question-Question Similarity(test set for 2017).", "labels": [], "entities": [{"text": "English Question-Question Similarity", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.5868506332238516}]}, {"text": " Table 5  Results of Subtask C: English Question-External Comment Similarity(test set for  2016).", "labels": [], "entities": [{"text": "English Question-External Comment Similarity", "start_pos": 32, "end_pos": 76, "type": "TASK", "confidence": 0.5766894668340683}]}, {"text": " Table 6  Results of Subtask C: English Question-External Comment Similarity(test set for  2017).", "labels": [], "entities": [{"text": "English Question-External Comment Similarity", "start_pos": 32, "end_pos": 76, "type": "TASK", "confidence": 0.5875543057918549}]}]}