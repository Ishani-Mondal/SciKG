{"title": [], "abstractContent": [{"text": "This paper reports team IITPB's participation in the SemEval 2017 Task 5 on 'Fine-grained sentiment analysis on financial microblogs and news'.", "labels": [], "entities": [{"text": "SemEval 2017 Task 5", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8804288059473038}, {"text": "Fine-grained sentiment analysis", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.5934300124645233}]}, {"text": "We developed 2 systems for the two tracks.", "labels": [], "entities": []}, {"text": "One system is based on an ensemble of Support Vector Classifier and Logistic Regression.", "labels": [], "entities": []}, {"text": "This system relis on Distributional Thesaurus (DT), word embeddings and lexicon features to predict a continuous sentiment value between-1 and +1.", "labels": [], "entities": []}, {"text": "The other system is based on Support Vector Regression using word embeddings, lexicon features , and PMI scores as features.", "labels": [], "entities": [{"text": "Support Vector Regression", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.5232532421747843}]}, {"text": "Our systems are ranked 5 thin track 1 and 8 thin track 2.", "labels": [], "entities": []}], "introductionContent": [{"text": "We are living in a world where stock market directly affects the economic system of a country.", "labels": [], "entities": []}, {"text": "Therefore, a reliable and prompt delivery of information plays an important role in the financial market.", "labels": [], "entities": []}, {"text": "Up until the last decade printed/television news were the major source of stock marketrelated information.", "labels": [], "entities": []}, {"text": "However, with the introduction of micro-blogging websites (e.g. Twitter etc.) the trend has been shifted.", "labels": [], "entities": []}, {"text": "The rise of Twitter and StockTwits has given the people and organizations an opportunity to vent out their feelings and views.", "labels": [], "entities": [{"text": "StockTwits", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.9351658821105957}]}, {"text": "This information can be used by an individual or an organization to make an informed prediction related to any company or stock (.", "labels": [], "entities": []}, {"text": "This opens anew avenue for sentiment analysis in the financial domain of microblogs and news.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9746513366699219}]}, {"text": "News headlines area short piece of text describing the nature of an article.", "labels": [], "entities": []}, {"text": "Due to space constraints, headlines normally follow a compact writing style, known as headlinese, which limits the usage of articles, the verb form of to be, conjunctions etc.", "labels": [], "entities": []}, {"text": "Similarly, social media platforms text is prone to noise.", "labels": [], "entities": []}, {"text": "There is a very high possibility of the data lacking a proper structure, grammar and appropriate punctuations.", "labels": [], "entities": []}, {"text": "These inconsistencies make it challenging to solve any NLP problems including sentiment analysis).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.9580380022525787}]}, {"text": "Moreover, each tweet can have reference to multiple company names (or stock symbols) and the expressed sentiment can be different towards different companies.", "labels": [], "entities": []}, {"text": "Hence, there is a need to perform fine-grained sentiment analysis wherein, generally, a context is used to decide the relevant portion of a tweet fora particular company.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7529091835021973}]}, {"text": "Another inherent challenge with the microblog and news data is the use of short languages, hashtag, emoticons and embedded URL.", "labels": [], "entities": []}, {"text": "Special attention should be given to these as they can provide some important hidden information.", "labels": [], "entities": []}, {"text": "Example -#bullish-Market and #increasingProfit can reflect positive sentiment.", "labels": [], "entities": [{"text": "increasingProfit", "start_pos": 30, "end_pos": 46, "type": "METRIC", "confidence": 0.6054744720458984}]}, {"text": "These are some of the major challenges associated with fine-grained sentiment analysis of microblogging and news data.", "labels": [], "entities": [{"text": "fine-grained sentiment analysis", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.6763958732287089}]}, {"text": "The SemEval-2017 task 5 (Fine-Grained Sentiment Analysis on Financial Microblogs and News) has two tracks ().", "labels": [], "entities": [{"text": "Fine-Grained Sentiment Analysis on Financial Microblogs and News)", "start_pos": 25, "end_pos": 90, "type": "TASK", "confidence": 0.7801814443535275}]}, {"text": "For both the tracks, the overall aim was to assign a sentiment score to a cashtag/company over a continuous range of -1 (very negative/bearish) to 1 (very positive/bullish).", "labels": [], "entities": []}, {"text": "First track involves finding a sentiment score towards a given 'cashtag' (stock symbol preceded by a $, e.g. $AAPL for Apple Inc.) in microblog messages while the second track involves finding a sentiment score towards a given company name in the news headlines.Instances in track 1 datasets also contain 'span'.", "labels": [], "entities": [{"text": "AAPL", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9583650827407837}]}, {"text": "It is the section of a tweet from where sentiment score should be derived.", "labels": [], "entities": []}, {"text": "We participated and submitted our system for both the tracks.", "labels": [], "entities": []}, {"text": "A total of 27 and 29 teams participated in track 1 and track 2 respectively.", "labels": [], "entities": []}, {"text": "Our system ranked 5 thin the first track with a cosine similarity of 0.725.", "labels": [], "entities": []}, {"text": "In the second track, our system scored cosine similarity of 0.695 and ranked 8 th overall.", "labels": [], "entities": [{"text": "similarity", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.5011927485466003}]}, {"text": "The rest of the paper is organized as follows: Section 2 briefly describes the proposed systems.", "labels": [], "entities": []}, {"text": "Description of the feature set is given in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 is devoted to experimental result and error analysis.", "labels": [], "entities": []}, {"text": "Lastly, we conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The training datasets contains 1700 and 1142 instances of microblog messages and news headlines respectively.", "labels": [], "entities": []}, {"text": "Test data comprises of 800 and 491 resp. of such instances for the two tracks.", "labels": [], "entities": []}, {"text": "We use 20% of the training dataset as validation set.", "labels": [], "entities": []}, {"text": "We used python based machine learning package scikit-learn 4 for the implementation.", "labels": [], "entities": []}, {"text": "As classification algorithm, we used Logistic Regression (LR), Support Vector Machine (SVM) and Support Vector Regression (SVR).", "labels": [], "entities": []}, {"text": "As discussed earlier, each instance of the dataset need a score over a continuous range of -1 to +1.", "labels": [], "entities": []}, {"text": "Since SVM predicts discrete class labels, as post-processing we use the probability of predicted class as the score.", "labels": [], "entities": [{"text": "SVM predicts discrete class labels", "start_pos": 6, "end_pos": 40, "type": "TASK", "confidence": 0.8010249257087707}]}, {"text": "During validation phase we observed that models trained on SVM work better than that of SVR for the microblog datasets.", "labels": [], "entities": [{"text": "SVM", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.934195339679718}, {"text": "microblog datasets", "start_pos": 100, "end_pos": 118, "type": "DATASET", "confidence": 0.7751914262771606}]}, {"text": "In contrast, SVR works better than SVM in news headline datasets.", "labels": [], "entities": []}, {"text": "The hyperparameters of the SVM were C = 30 and \u03b3 = 0.01, for SVR we used C = 10 and \u03b3 = 0.01 and for LR we set C = 6.", "labels": [], "entities": [{"text": "SVR", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.8939486145973206}]}, {"text": "Cosine similarity of various combinations of the feature set is listed in and 4 for microblogs and news headlines validation set respectively.", "labels": [], "entities": [{"text": "Cosine similarity", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.9106579422950745}]}, {"text": "For fine tuning of hyper-parameters, we did an exhaustive grid search evaluated through ten-fold cross-validation on the training set.", "labels": [], "entities": []}, {"text": "As a result, we observed that the word embedding along with lexicon based features produce the 4 http://scikit-learn.org best cosine similarity for both the datasets.", "labels": [], "entities": []}, {"text": "Further, we observed the output of different classifier are contrasting in nature, therefore we merge the outputs of different classifiers using averaging and harmonic mean.", "labels": [], "entities": []}, {"text": "We found that harmonic mean of LR and SVM produces better cosine similarity score than other combinations for microblogs messages.", "labels": [], "entities": [{"text": "cosine similarity score", "start_pos": 58, "end_pos": 81, "type": "METRIC", "confidence": 0.7323544919490814}]}, {"text": "However, for news headline performance did not improve on the ensemble, so we choose the best feature combination to train an SVR.", "labels": [], "entities": [{"text": "SVR", "start_pos": 126, "end_pos": 129, "type": "DATASET", "confidence": 0.9241377115249634}]}, {"text": "shows the results for harmonic mean of SVM and LR cosine similarities in microblogs datasets.", "labels": [], "entities": []}, {"text": "Cosine similarity Track 1: Microblogs 0.725 Track 2: News headlines 0.695: Cosine similarity on test dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Microblog messages: Cosine similarity  on validation set.", "labels": [], "entities": []}, {"text": " Table 4: News headline: Cosine similarity on val- idation set.", "labels": [], "entities": [{"text": "News headline", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.8574113845825195}, {"text": "Cosine similarity", "start_pos": 25, "end_pos": 42, "type": "METRIC", "confidence": 0.8537132143974304}]}, {"text": " Table 5: Microblog messages: Ensemble of SVM  & LR on validation set.", "labels": [], "entities": []}]}