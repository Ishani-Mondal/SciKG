{"title": [{"text": "RiTUAL-UH at SemEval-2017 Task 5: Sentiment Analysis on Financial Data Using Neural Networks", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.9727708697319031}]}], "abstractContent": [{"text": "In this paper, we present our systems for \"SemEval-2017 Task-5 on Fine-Grained Sentiment Analysis on Financial Mi-croblogs and News\".", "labels": [], "entities": [{"text": "SemEval-2017 Task-5 on Fine-Grained Sentiment Analysis on Financial Mi-croblogs and News", "start_pos": 43, "end_pos": 131, "type": "TASK", "confidence": 0.7472158372402191}]}, {"text": "In our system, we combined hand-engineered lexical, sentiment , and metadata features with the representations learned from Convolutional Neural Networks (CNN) and Bidirectional Gated Recurrent Unit (Bi-GRU) having Attention model applied on top.", "labels": [], "entities": []}, {"text": "With this architecture we obtained weighted cosine similarity scores of 0.72 and 0.74 for subtask-1 and subtask-2, respectively.", "labels": [], "entities": []}, {"text": "Using the official scoring system, our system ranked in the second place for subtask-2 and in the eighth place for the subtask-1.", "labels": [], "entities": []}, {"text": "However, it ranked first in both subtasks when evaluated with an alternate scoring metric 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Predicting sentiments of financial data has a wide range of applications.", "labels": [], "entities": [{"text": "Predicting sentiments of financial", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8871871829032898}]}, {"text": "The most important application is being able to predict the ups and downs of the share market as the changes in sentiments and opinions can change the market dynamics (.", "labels": [], "entities": []}, {"text": "Stock market related information is typically found in newspapers ( and people discuss them in social media platforms like Twitter and StockTwits.", "labels": [], "entities": [{"text": "Stock market related information", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5885151624679565}, {"text": "StockTwits", "start_pos": 135, "end_pos": 145, "type": "DATASET", "confidence": 0.9505115747451782}]}, {"text": "Positive news has the capability to boost the market by increasing optimism among people (Van de.", "labels": [], "entities": []}, {"text": "SemEval-2017 Task 5 on 'Fine-Grained Sentiment Analysis on Financial Microblogs and News' aims at analyzing the polarity of public sentiments from financial data found in newspapers and social media.", "labels": [], "entities": [{"text": "Fine-Grained Sentiment Analysis on Financial Microblogs and News'", "start_pos": 24, "end_pos": 89, "type": "TASK", "confidence": 0.7532339245080948}]}, {"text": "In this paper, we describe our systems, which exploit automatically learned representations using deep learning architecture based methods along with hand-engineered features in order to predict the sentiment polarity of financial data.", "labels": [], "entities": []}], "datasetContent": [{"text": "As the trial dataset too small compared to the training data, we merged it with the training data and ran 10-fold cross-validation to evaluate different models.", "labels": [], "entities": []}, {"text": "We tuned the hyper-parameters during the training phase through grid-search cross validation method for System 1.", "labels": [], "entities": []}, {"text": "We also experimented with different architectures to build System 1 in this phase.", "labels": [], "entities": []}, {"text": "We evaluated our models using the official scoring system that measures the weighted cosine similarity, similar to the scorer used in.", "labels": [], "entities": [{"text": "weighted cosine similarity", "start_pos": 76, "end_pos": 102, "type": "METRIC", "confidence": 0.6002121468385061}]}, {"text": "As the predictions are continuous values between -1 and +1, cosine similarity measures the degree of alignment between the true values and the predictions.", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 60, "end_pos": 77, "type": "METRIC", "confidence": 0.8416417241096497}]}, {"text": "The final weighted score is computed by multiplying the cosine similarity with the ratio of predicted values against the number of test cases.", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 56, "end_pos": 73, "type": "METRIC", "confidence": 0.7576902508735657}]}, {"text": "As no official baseline scores were provided, we did a baseline experiment using a simple linear regression model with the hand-engineered features to compare our models.", "labels": [], "entities": []}, {"text": "presents the weighted cosine scores achieved by the models we experimented with.", "labels": [], "entities": []}, {"text": "System-1 achieved weighted cosine scores of 0.70 and 0.67 for subtask-1 and subtask-2, respectively.", "labels": [], "entities": []}, {"text": "Among the neural network models, Bi-GRU performed better than the others.", "labels": [], "entities": [{"text": "Bi-GRU", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9398038983345032}]}, {"text": "It achieved 0.68 for both of the subtasks.", "labels": [], "entities": []}, {"text": "The combination of the three neural network based model (System-2) performed better than the individual models.", "labels": [], "entities": []}, {"text": "The neural network models were trained for 10 epochs.", "labels": [], "entities": []}, {"text": "We observed issue of overfitting when we increased the number of epochs beyond this.", "labels": [], "entities": []}, {"text": "From the results for subtask-1 we can see that all the other models performed better than CNN.", "labels": [], "entities": [{"text": "CNN", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.8178573250770569}]}, {"text": "It indicates that other features captured by Bi-GRU and hand-engineering process were more informative than the local information captured by   CNN.", "labels": [], "entities": []}, {"text": "We can understand the strength of the handcrafted features also by observing the performance of SVR.", "labels": [], "entities": [{"text": "SVR", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.7416538596153259}]}, {"text": "Although it did not perform as expected on the test data of subtask-1, it showed good performance on the validation set.", "labels": [], "entities": []}, {"text": "We submitted predictions by System-1 (sub-1) and System-2 (sub-2) for subtask-1 as they were the best models.", "labels": [], "entities": []}, {"text": "Due to comparatively better performance of System-2 in subtask-2, we submitted predictions from two different models with this system but varied the number of epochs from 10 (sub-1) to 20 (sub-2).", "labels": [], "entities": []}, {"text": "For subtask-1, sub-1 and sub-2 was ranked eleventh and eighth, respectively.", "labels": [], "entities": []}, {"text": "For subtask-2, both of the submissions achieved almost similar scores and ranked second.", "labels": [], "entities": []}, {"text": "Submitted systems were evaluated simultaneously with an alternate scoring system that measures cosine similarity by grouping instances based on the related company.", "labels": [], "entities": []}, {"text": "Our systems ranked the first for both subtasks when evaluated with this scorer.", "labels": [], "entities": []}, {"text": "shows that our system worked well when there are more plain texts (MB2, HL2).", "labels": [], "entities": []}, {"text": "But it struggles when the text contains more statistics (e.g. $RIG -13% $EK -10%) than plain texts or mix of words with strong positive and negative sentiments (worst, strong, weird, wilt, falls, exit).", "labels": [], "entities": [{"text": "RIG", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9894769191741943}]}, {"text": "If we look at MB1, it is very difficult to determine the sentiment polarity from the message.", "labels": [], "entities": [{"text": "MB1", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.8949913382530212}]}, {"text": "The message starts with 'Worst performers today', which indicates a negative polarity.", "labels": [], "entities": []}, {"text": "Rest of the message contains statistics for different companies.", "labels": [], "entities": []}, {"text": "Among them four indicate a drop in prices and only one indicates arise in the stock price.", "labels": [], "entities": []}, {"text": "It is noticeable that, although the message started with negative impression, it ended with a positive impression by saying 'best stock: $WTS +15%'.", "labels": [], "entities": []}, {"text": "As this is the only possible reason for the highly positive true sentiment polarity score of 0.857 for this message, we get a hint that our systems might need to put more attention on how a message ends.", "labels": [], "entities": []}, {"text": "MB3 starts with the phrase 'Weird day' followed by a positive and a negative news about stock prices of two companies and our model predicted 0.588 as the polarity score where gold score is -0.649.", "labels": [], "entities": [{"text": "MB3", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9618349671363831}, {"text": "gold score", "start_pos": 176, "end_pos": 186, "type": "METRIC", "confidence": 0.9523616433143616}]}, {"text": "We tried to find out the possible reason behind our prediction by simply looking at the distribution of the words in this message.", "labels": [], "entities": []}, {"text": "In the training data, the word 'weird' appeared only once.", "labels": [], "entities": []}, {"text": "68% of the 241 messages that contain 'day' are positive in the training data.", "labels": [], "entities": []}, {"text": "Out of 270 messages that contain 'up', 201 messages (75%) are positives.", "labels": [], "entities": []}, {"text": "We found 118 messages that contain 'down' and 80 (68%) of them are negative.", "labels": [], "entities": []}, {"text": "So, we can see that if a message contains 'up' and 'down', chance of predicting it as positive is higher.", "labels": [], "entities": []}, {"text": "Related cashtag $GPRO was found in three messages and $amba appeared only in one message.", "labels": [], "entities": []}, {"text": "Our model predicted a positive sentiment for HL1 although it contains a clear indication of a negative polarity by the word 'wilt'.", "labels": [], "entities": [{"text": "HL1", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.702329158782959}]}, {"text": "To find out the reason we observed that, 'wilt' did not appear in the headlines training data at all.", "labels": [], "entities": [{"text": "headlines training data", "start_pos": 70, "end_pos": 93, "type": "DATASET", "confidence": 0.8218922813733419}]}, {"text": "Its polarity is -0.087 in the scale of -1 to +1 according to the SenticNet database we used.", "labels": [], "entities": []}, {"text": "So we can say that, our model needs to handle this type of trigger word that can control the polarity itself.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of 10-fold cross-validation on training and", "labels": [], "entities": []}, {"text": " Table 3: Weighted cosine scores with ranks achieved by the", "labels": [], "entities": []}, {"text": " Table 4: Sample texts from both subtasks with anno-", "labels": [], "entities": []}]}