{"title": [], "abstractContent": [{"text": "This paper describes our system for detection and interpretation of English puns.", "labels": [], "entities": [{"text": "detection and interpretation of English puns", "start_pos": 36, "end_pos": 80, "type": "TASK", "confidence": 0.7933287421862284}]}, {"text": "We participated in 2 subtasks related to homographic puns achieve comparable results for these tasks.", "labels": [], "entities": []}, {"text": "Through the paper we provide detailed description of the approach , as well as the results obtained in the task.", "labels": [], "entities": []}, {"text": "Our models achieved an F1-score of 77.65% for Subtask 1 and 52.15% for Sub-task 2.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9996597766876221}]}], "introductionContent": [{"text": "The pun, also called paronomasia, is a form of wordplay that suggests two or more meanings, by exploiting multiple meanings of words, or of similar-sounding words, for an intended humorous or rhetorical effect.", "labels": [], "entities": []}, {"text": "A pun is also a special form of ambiguity (mostly lexical) that is consciously used to create statements with ambiguous-distinctmeanings.", "labels": [], "entities": []}, {"text": "These ambiguities can arise from the intentional use of homophonic, homographic, metonymic, or figurative language.", "labels": [], "entities": []}, {"text": "A homographic pun exploits distinct meanings of the same written word, and a homophonic pun exploits distinct meanings of the same spoken word.", "labels": [], "entities": []}, {"text": "\u2022 \"I used to be a banker but I lost interest\", \u2022 \"Tires are fixed fora flat rate\" \u2022 \"Getting rid of your boat for another could cause a whole raft of problems\" In the first example, the word interest is the pun denoting interest as willingness and also as a fixed charge for borrowing money.", "labels": [], "entities": []}, {"text": "In the second example, the word flat is the pun denoting flat as in a flat tyre and flat as in flat rate.", "labels": [], "entities": []}, {"text": "The third example, the word raft is the pun denoting raft as a batch and raft as a type of boat.", "labels": [], "entities": []}, {"text": "In the present work, we focus on homographic puns.", "labels": [], "entities": [{"text": "homographic puns", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7957897186279297}]}, {"text": "We present methods to a) identify a pun sentence, b) identify the pun word given a pun sentence and c) interpret the different word senses of the pun word.", "labels": [], "entities": []}, {"text": "The details of the shared task are available at 2 Subtask 1 -Pun detection In this task, for each sentence, the system must decide whether it contains a pun or not.", "labels": [], "entities": [{"text": "Subtask 1 -Pun detection", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.506298765540123}]}, {"text": "While this task is mostly unsupervised, we cast this problem as a supervised learning classification problem.", "labels": [], "entities": [{"text": "supervised learning classification", "start_pos": 66, "end_pos": 100, "type": "TASK", "confidence": 0.7035626173019409}]}, {"text": "We have randomly selected part of the dataset and manually annotated them as a pun sentence or a non-pun.", "labels": [], "entities": []}, {"text": "We decided to leverage on models that try to model sequences of word vectors.", "labels": [], "entities": []}, {"text": "We can view the each sentence as a sequence where we only have one label at the end.", "labels": [], "entities": []}, {"text": "This many-to-one mapping lends itself nicely to Recurrent Neural Network.", "labels": [], "entities": []}, {"text": "Due to this reason, we used a recurrent neural network to train the classifier and generate the model.", "labels": [], "entities": []}, {"text": "Using this model we classify the remaining dataset.", "labels": [], "entities": []}, {"text": "We used two settings for the train and test split.", "labels": [], "entities": []}, {"text": "In setting 1, we used 70% of the dataset for training and 30% of the data for testing.", "labels": [], "entities": []}, {"text": "In Setting 2, we used 30% of the dataset for training and the remaining 70% of the data for testing.", "labels": [], "entities": []}, {"text": "Recent research has shown that deep learning methods can minimize the reliance on feature engineering by automatically extracting meaningful features from raw text.", "labels": [], "entities": []}, {"text": "Thus, we propose to use distributed word embeddings which capture lexical and semantic features as input features to our neural network model.", "labels": [], "entities": []}, {"text": "Distributed word embeddings map words in a language to high dimensional real-valued vectors in order to capture hidden semantic and syntactic properties of words.", "labels": [], "entities": []}, {"text": "These embeddings are typically learned from large unlabeled text corpora.", "labels": [], "entities": []}, {"text": "In our work, we use the pre-trained 50 dimensional GloVe embeddings () which were trained on about 6B words from the twitter using the Continuous Bag of Words architecture.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance of the pun classifier using Bidirectional RNN in 2 settings for Subtask 1", "labels": [], "entities": [{"text": "pun classifier", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7086226046085358}]}, {"text": " Table 2: All word similarity pairs for the example  pun 3", "labels": [], "entities": []}, {"text": " Table 3: Performance of the pun word identifica- tion using MAXSIM for Subtask 2", "labels": [], "entities": []}]}