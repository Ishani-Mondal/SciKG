{"title": [{"text": "UWaterloo at SemEval-2017 Task 7: Locating the Pun Using Syntactic Characteristics and Corpus-based Metrics", "labels": [], "entities": [{"text": "UWaterloo at SemEval-2017 Task 7", "start_pos": 0, "end_pos": 32, "type": "DATASET", "confidence": 0.8001242756843567}]}], "abstractContent": [{"text": "The paper presents a system for locating a pun word.", "labels": [], "entities": [{"text": "locating a pun word", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.9068253934383392}]}, {"text": "The developed method calculates a score for each word in a pun, using a number of components, including its Inverse Document Frequency (IDF), Normalized Pointwise Mutual Information (NPMI) with other words in the pun text, its position in the text, part-of-speech and some syntactic features.", "labels": [], "entities": [{"text": "Inverse Document Frequency (IDF)", "start_pos": 108, "end_pos": 140, "type": "METRIC", "confidence": 0.834174687663714}, {"text": "Normalized Pointwise Mutual Information (NPMI)", "start_pos": 142, "end_pos": 188, "type": "METRIC", "confidence": 0.6339839271136692}]}, {"text": "The method achieved the best performance in the Heterographic category and the second best in the Homographic.", "labels": [], "entities": []}, {"text": "Further analysis showed that IDF is the most useful characteristic , whereas the count of words with which the given word has high NPMI has a negative effect on performance.", "labels": [], "entities": [{"text": "IDF", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9994456171989441}, {"text": "NPMI", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9767892956733704}]}], "introductionContent": [{"text": "The pun is defined as \"A joke exploiting the different possible meanings of a word or the fact that there are words which soundalike but have different meanings\".", "labels": [], "entities": []}, {"text": "When a pun is a spoken utterance, two types of puns are commonly distinguished: homophonic puns, which exploit different meanings of the same word, and heterophonic puns, in which one or more words have similar but not identical pronunciations to some other word or phrase that is alluded to in the pun.", "labels": [], "entities": []}, {"text": "The SemEval Task 7 () focused on the identification of puns as written texts, rather than spoken utterances, and hence distinguished between homographic and heterographic puns.", "labels": [], "entities": [{"text": "SemEval Task 7", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.870227575302124}, {"text": "identification of puns as written texts", "start_pos": 37, "end_pos": 76, "type": "TASK", "confidence": 0.8419349988301595}]}, {"text": "We participated in Subtask 2: Pun location, which required participating systems to identify which word is the pun.", "labels": [], "entities": [{"text": "Subtask 2: Pun location", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.8428902745246887}]}, {"text": "Only the cases which contain exactly one pun word were given to the participants in each of the two categories: homographic and heterographic puns.", "labels": [], "entities": []}, {"text": "Our approach to identifying the pun word is to rank words in the pun text by a score calculated as the sum of values of eleven features.", "labels": [], "entities": []}, {"text": "The feature values are calculated using a combination of corpus statistics and rule-based methods.", "labels": [], "entities": []}, {"text": "The word with the highest score is considered to be the pun word.", "labels": [], "entities": []}, {"text": "The method is described in detail in Section 2.", "labels": [], "entities": []}, {"text": "In developing the word ranking method, we were guided by a number of intuitions, outlined below.", "labels": [], "entities": [{"text": "word ranking", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.7667614817619324}]}, {"text": "The punchline in a pun or a joke is almost always close to the end, since it is at the end that the reader is expected to uncover the second hidden (non-obvious) meaning of the pun.", "labels": [], "entities": []}, {"text": "This intuition is consistent with Ruskin's Script-based Semantic Theory of humour.", "labels": [], "entities": []}, {"text": "The system therefore only assigns scores to words located in the second half of the pun text.", "labels": [], "entities": []}, {"text": "What makes a homographic pun humorous is the simultaneous perception by a reader of two conflicting meanings of the same pun word.", "labels": [], "entities": [{"text": "homographic pun humorous", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.8269281983375549}]}, {"text": "The pun author can achieve this by using words that are associated with (or evoke) different senses of the pun word.", "labels": [], "entities": []}, {"text": "For example in \"Why don't programmers like nature?", "labels": [], "entities": []}, {"text": "It has too many bugs\" The word \"programmers\" is associated with one sense of \"bugs\", but the word \"nature\" is associated with another sense.", "labels": [], "entities": []}, {"text": "We operationalize this intuition by calculating Normalized Pointwise Mutual Information (NPMI) between pairs of words to find words that are semantically associated with each other.", "labels": [], "entities": []}, {"text": "Heterographic puns often contain one or more words that are associated with either the pun word itself or its similarly sounding word.", "labels": [], "entities": []}, {"text": "In the case of \"What did the grape say when it got stepped on?", "labels": [], "entities": []}, {"text": "Nothing -but it let out a little whine.\"", "labels": [], "entities": []}, {"text": "The pun word \"whine\" has a similarly sounding word \"wine\", which is associated with the preceding word \"grape\".", "labels": [], "entities": []}, {"text": "To operationalize this intuition, we used a dictionary of similarly sounding words.", "labels": [], "entities": []}, {"text": "If fora given word in the pun text there exists a similarly sounding word (or words), we calculate NPMI between it and each other word in the text.", "labels": [], "entities": [{"text": "NPMI", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9516412019729614}]}, {"text": "We also calculate NPMI between the original word as it appears in the pun and each other word.", "labels": [], "entities": [{"text": "NPMI", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9507520794868469}]}, {"text": "We hypothesize that if a similarly sounding word is more strongly associated (i.e. has higher NPMI) with other words in the text, compared to the original word, it is likely to be the pun word, and receives an additional weight.", "labels": [], "entities": [{"text": "NPMI", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9875116348266602}]}, {"text": "The pun word has to standout from the rest of the text and attract the reader's attention, as it is the realization of the joke's punchline.", "labels": [], "entities": []}, {"text": "One possible reason why it stands out is because it is a more rare word compared to the surrounding words.", "labels": [], "entities": []}, {"text": "Inverse Document Frequency (IDF) is a measure of how rare the word is in a corpus.", "labels": [], "entities": [{"text": "Inverse Document Frequency (IDF)", "start_pos": 0, "end_pos": 32, "type": "METRIC", "confidence": 0.7631445427735647}]}, {"text": "The less frequent the word is in a corpus, the higher is its IDF.", "labels": [], "entities": [{"text": "IDF", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9984645843505859}]}, {"text": "We hypothesize that a word, which has the highest IDF in the second half of the text is more likely to be the pun word than words with lower IDFs.", "labels": [], "entities": [{"text": "IDF", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.997894823551178}, {"text": "IDFs", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9540917277336121}]}, {"text": "We thus assign an additional weight to such a word.", "labels": [], "entities": []}, {"text": "Furthermore, only nouns, adjectives, adverbs and verbs are assigned scores by our system.", "labels": [], "entities": []}, {"text": "Sometimes, a pun word is a made up word, e.g. \"velcrows\" in \"There is a special species of bird that is really good at holding stuff together.", "labels": [], "entities": []}, {"text": "They are called velcrows.\"", "labels": [], "entities": []}, {"text": "We assign an additional weight to words that have zero frequency in a large corpus.", "labels": [], "entities": []}, {"text": "A number of intuitions were guided by the syntactic structure of the text.", "labels": [], "entities": []}, {"text": "Thus, we hypothesize that if the pun text consists of two sentences, the pun word is located in the second sentence, as it is most likely to contain the punchline.", "labels": [], "entities": []}, {"text": "Therefore, all words in the second sentence receive an additional weight.", "labels": [], "entities": []}, {"text": "Ina similar vein, if the text contains a comma or the words \"then\" or \"but\", all words following them receive additional weights.", "labels": [], "entities": []}, {"text": "These clues can signal a pause, a shift in the narrative or a juxtaposition, which all precede the punchline.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Components of the score calculated for every content word x in the text of the pun.", "labels": [], "entities": [{"text": "Components", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9561607837677002}]}, {"text": " Table 3: Post-submission results with added/removed features (Heterographic puns)", "labels": [], "entities": []}]}