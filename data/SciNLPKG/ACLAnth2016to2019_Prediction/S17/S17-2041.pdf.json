{"title": [{"text": "SEW-EMBED at SemEval-2017 Task 2: Language-Independent Concept Representations from a Semantically Enriched Wikipedia", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes SEW-EMBED, our language-independent approach to multilingual and cross-lingual semantic word similarity as part of the SemEval-2017 Task 2.", "labels": [], "entities": [{"text": "cross-lingual semantic word similarity", "start_pos": 86, "end_pos": 124, "type": "TASK", "confidence": 0.5954446494579315}, {"text": "SemEval-2017 Task 2", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.840427041053772}]}, {"text": "We leverage the Wikipedia-based concept representations developed by Raganato et al.", "labels": [], "entities": []}, {"text": "(2016), and propose an embedded augmentation of their explicit high-dimensional vectors, which we obtain by plugging in an arbitrary word (or sense) embedding representation, and computing a weighted average in the continuous vector space.", "labels": [], "entities": []}, {"text": "We evaluate SEW-EMBED with two different off-the-shelf embedding representations, and report their performances across all monolin-gual and cross-lingual benchmarks available for the task.", "labels": [], "entities": []}, {"text": "Despite its simplicity , especially compared with supervised or overly tuned approaches, SEW-EMBED achieves competitive results in the cross-lingual setting (3rd best result in the global ranking of subtask 2, score 0.56).", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic similarity is a well established research area of Natural Language Processing, concerned with measuring the extent to which two linguistic items are similar ().", "labels": [], "entities": [{"text": "Semantic similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8375160992145538}]}, {"text": "In particular, word similarity is nowadays a widely used evaluation benchmark for word and sense representations.", "labels": [], "entities": [{"text": "word and sense representations", "start_pos": 82, "end_pos": 112, "type": "TASK", "confidence": 0.5875572264194489}]}, {"text": "While many classical approaches to word similarity have been limited to the English language (), a growing interest for multilingual and cross-lingual models is emerging) and it is accompanied by the development of multilingual benchmarks.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7662872672080994}]}, {"text": "In this respect Wikipedia, as one of the most popular semi-structured resources in the field (), provides a convenient bridge to multilinguality, with several million inter-language links among articles refferring to the same concept or entity.", "labels": [], "entities": []}, {"text": "In fact, a number of successful approaches to semantic similarity make explicit use of Wikipedia, from ESA ( to NASARI.", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7720058560371399}, {"text": "ESA", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.9156041741371155}, {"text": "NASARI", "start_pos": 112, "end_pos": 118, "type": "DATASET", "confidence": 0.9124505519866943}]}, {"text": "Others, like SENSEMBED (, report state-of-the-art results when trained on an automatically disambiguated version of a Wikipedia dump.", "labels": [], "entities": [{"text": "SENSEMBED", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.6905852556228638}]}, {"text": "Regardless of whether Wikipedia is seen as a multilingual semantic network of concepts and entities or as a sense-annotated corpus, hyperlinks (inter-page links) constitute its key structural property: in light of this, addressed the sparsity problem of original hyperlinks and developed SEW 1 , a semantically enriched Wikipedia where the overall number of linked mentions has been more than tripled by solely exploiting the structure of Wikipedia itself and the wide-coverage sense inventory of BabelNet ( . In addition to building the corpus, the authors used SEW's sense annotations to construct vector representations of concepts and entities from the BabelNet sense inventory, and tested them on multiple semantic similarity tasks.", "labels": [], "entities": []}, {"text": "Being defined at the concept level, SEW's representations are inherently multilingual: however, they consist of high- dimensional sparse vectors, not immediately comparable with existing approaches, especially those based on word embeddings, and less flexible to use within downstream applications.", "labels": [], "entities": []}, {"text": "In this paper we propose SEW-EMBED, an embedded augmentation of SEW's original representations in which sparse vectors, defined in the high-dimensional space of Wikipedia pages, are mapped to continuous vector representations via a weighted average of embedded vectors from an arbitrary, pre-specified word (or sense) representation.", "labels": [], "entities": []}, {"text": "Regardless of the particular representation used, the resulting vectors are still defined at the concept level, and hence immediately expendable in a multilingual and cross-lingual setting.", "labels": [], "entities": []}, {"text": "We describe and evaluate SEW-EMBED with two off-the-shelf embedded representations: the popular word embeddings of) and the embedded concept representations of NASARI . We report and discuss the results obtained by both versions on all monolingual and crosslingual benchmarks available for the task (Camacho, and include a comparison with the original explicit representations of.", "labels": [], "entities": [{"text": "NASARI", "start_pos": 160, "end_pos": 166, "type": "DATASET", "confidence": 0.9723238348960876}]}], "datasetContent": [{"text": "In this section we report and discuss the performance of SEW-EMBED on the monolingual and cross-lingual benchmark of the Semeval 2017 Task 2.", "labels": [], "entities": [{"text": "SEW-EMBED", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.6135311722755432}, {"text": "Semeval 2017 Task 2", "start_pos": 121, "end_pos": 140, "type": "DATASET", "confidence": 0.6819302216172218}]}, {"text": "We consider two versions of SEW-EMBED: one based on the pre-trained word embeddings of Word2Vec (, SEW-EMBED: Results on the multilingual word similarity benchmarks (subtask 1) of Semeval 2017 task 2, in terms of Pearson correlation (r), Spearman correlation (\u03c1), and the harmonic mean of rand \u03c1.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.9734253883361816}, {"text": "Pearson correlation (r)", "start_pos": 213, "end_pos": 236, "type": "METRIC", "confidence": 0.9675055623054505}, {"text": "Spearman correlation (\u03c1)", "start_pos": 238, "end_pos": 262, "type": "METRIC", "confidence": 0.860750812292099}]}, {"text": "0 correspond to the results of SEW-EMBED reported in the task description paper.", "labels": [], "entities": [{"text": "SEW-EMBED", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.45192453265190125}]}, {"text": "We additionally include the results obtained by the original explicit representations based on SEW (cf. Section 3.1) and by the NASARI baseline, and use them as comparison systems across Sections 4.1 and 4.2. 6 shows the overall performance on multilingual word similarity for each monolingual dataset.", "labels": [], "entities": [{"text": "SEW", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.9344002604484558}, {"text": "NASARI baseline", "start_pos": 128, "end_pos": 143, "type": "DATASET", "confidence": 0.9634168148040771}]}], "tableCaptions": [{"text": " Table 1: Results on the multilingual word similarity benchmarks (subtask 1) of Semeval 2017 task 2, in  terms of Pearson correlation (r), Spearman correlation (\u03c1), and the harmonic mean of r and \u03c1.", "labels": [], "entities": [{"text": "Pearson correlation (r)", "start_pos": 114, "end_pos": 137, "type": "METRIC", "confidence": 0.9647797584533692}, {"text": "Spearman correlation (\u03c1)", "start_pos": 139, "end_pos": 163, "type": "METRIC", "confidence": 0.9120912909507751}]}, {"text": " Table 2: Results on the cross-lingual word similarity benchmarks (subtask 2) of Semeval 2017 task 2, in  terms of Pearson correlation (r), Spearman correlation (\u03c1), and the harmonic mean of r and \u03c1.", "labels": [], "entities": [{"text": "Pearson correlation (r)", "start_pos": 115, "end_pos": 138, "type": "METRIC", "confidence": 0.9658180594444274}, {"text": "Spearman correlation (\u03c1)", "start_pos": 140, "end_pos": 164, "type": "METRIC", "confidence": 0.9050955533981323}]}]}