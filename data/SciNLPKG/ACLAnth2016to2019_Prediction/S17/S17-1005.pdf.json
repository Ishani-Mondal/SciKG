{"title": [{"text": "Domain-Specific New Words Detection in Chinese", "labels": [], "entities": [{"text": "Domain-Specific New Words Detection", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5374118611216545}]}], "abstractContent": [{"text": "With the explosive growth of Internet, more and more domain-specific environments appear, such as forums, blogs, MOOCs and etc.", "labels": [], "entities": []}, {"text": "Domain-specific words appear in these areas and always play a critical role in the domain-specific NLP tasks.", "labels": [], "entities": []}, {"text": "This paper aims at extracting Chi-nese domain-specific new words automatically.", "labels": [], "entities": [{"text": "extracting Chi-nese domain-specific new words", "start_pos": 19, "end_pos": 64, "type": "TASK", "confidence": 0.7809834361076355}]}, {"text": "The extraction of domain-specific new words has two parts including both new words in this domain and the especially important words.", "labels": [], "entities": []}, {"text": "In this work, we propose a joint statistical model to perform these two works simultaneously.", "labels": [], "entities": []}, {"text": "Compared to traditional new words detection models, our model doesn't need handcraft features which are labor intensive.", "labels": [], "entities": [{"text": "words detection", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.7195027470588684}]}, {"text": "Experimental results demonstrate that our joint model achieves a better performance compared with the state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Accompanying with the development of Internet, many new specific domains appear, such as forums, blogs, Massive Open Online Courses (MOOCs) and etc.", "labels": [], "entities": []}, {"text": "There are always a group of important words in these domains, which are known as domain-specific words.", "labels": [], "entities": []}, {"text": "Domainspecific words include two types as shown in Table 1.", "labels": [], "entities": []}, {"text": "The first ones are rare and unambiguous words which will seldom appear in other domains such as \"\u6808\u9876\"(stack top) and \"\u4e8c\u53c9\u6811\"(binary tree).", "labels": [], "entities": []}, {"text": "These words may cause word segmentation problems.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.732568770647049}]}, {"text": "For example, if we do not recognize \"\u6808\u9876\"(stack top) as a word, the segmentation \"\u6808 \u9876 \u8fd0\u7b97\u7b26 \u662f \u4e58\u53f7\"(the operator at stack top is multiplication sign) will be like \"\u6808 \u9876\u8fd0 \u7b97\u7b26 \u662f \u4e58\u53f7\".", "labels": [], "entities": []}, {"text": "In this case, \"\u6808\u9876\" means \"stack top\": Examples of domain-specific word in data structure domain and \"\u8fd0\u7b97\u7b26\" means \"operator\", but in the segmentation result, \"\u9876\u8fd0\" is segmented into a word in mistake and will bring lots of problems to the further applications.", "labels": [], "entities": []}, {"text": "The other type is common and ambiguous words which have specific new meanings in this domain, such as \"\u590d\u6742\u5ea6\"(complexity) and \"\u904d \u5386\"(iterate).", "labels": [], "entities": []}, {"text": "These words often play important roles in domain-specific tasks.", "labels": [], "entities": []}, {"text": "For example\uff0cin MOOCs which are typical domain-specific environments, there is an Automated Navigation Suggestion(ANS)() task which suggests a time point for users when they want to review the front contents of the video.", "labels": [], "entities": [{"text": "MOOCs", "start_pos": 15, "end_pos": 20, "type": "TASK", "confidence": 0.9293153285980225}, {"text": "Automated Navigation Suggestion(ANS)() task", "start_pos": 81, "end_pos": 124, "type": "TASK", "confidence": 0.7620637246540615}]}, {"text": "With the help of the recognition of this type of words, we can easily give higher weights to those domainspecific contents.", "labels": [], "entities": []}, {"text": "After extracting these two type of words, we can also use them for creating ontologies, term lists, and in the Semantic Web Area for finding novel entities(.", "labels": [], "entities": []}, {"text": "Besides, in MOOCs area it will also benefit Certification Prediction(CP)) (which predicts whether a user will get a course certification or not), Course Recommendation(CR) and soon by providing textual knowledge.", "labels": [], "entities": [{"text": "MOOCs", "start_pos": 12, "end_pos": 17, "type": "TASK", "confidence": 0.8643832802772522}, {"text": "Course Recommendation(CR)", "start_pos": 146, "end_pos": 171, "type": "METRIC", "confidence": 0.6939547061920166}]}, {"text": "Researchers have made great efforts to extract domain-specific words.", "labels": [], "entities": []}, {"text": "Traditional new word detection methods usually employ statistical methods according to the pattern that new words ap-pear constantly.", "labels": [], "entities": [{"text": "word detection", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.708235502243042}]}, {"text": "Such methods like Pointwise Mutual Information), Enhanced Mutual Information( , and Multi-word Expression Distance(.", "labels": [], "entities": []}, {"text": "These methods focus on extracting the first type of domain-specific words and conduct postprocessing to discover the second type of words.", "labels": [], "entities": []}, {"text": "Deng et al. proposed a statistical model TopWords( to extract the first type of words, it can imply some of these statistical measures into the model itself.", "labels": [], "entities": []}, {"text": "Besides, it designs a feature called relative frequency to extract the second type of domain-specific words.", "labels": [], "entities": []}, {"text": "TopWords is based on a Word Dictionary Model(WDM)) in which a sentence is sampled from a word dictionary.", "labels": [], "entities": []}, {"text": "To extract the second type of words, it needs to train its model on a common background corpus which is expensive and time-consuming.", "labels": [], "entities": []}, {"text": "To address these issues, we propose a Domain TopWords model by assuming that a sentence is sampled from two word dictionaries, one for common words and the other for domain-specific words.", "labels": [], "entities": []}, {"text": "Besides, we propose a flexible domain score function to take the external information into consideration, such as word frequencies in common background corpus.", "labels": [], "entities": []}, {"text": "Therefore, the proposed model can extract these two types of words jointly.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are summarized as follows: \u2022 We propose a novel Domain TopWords model that can extract both two types of domain-specific words jointly.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate the effectiveness of our model.", "labels": [], "entities": []}, {"text": "\u2022 Our model achieves a comparable performance even with much less information comparing to the origin TopWords model.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: the related work will be introduced in section 2.", "labels": [], "entities": []}, {"text": "Our model will be introduced in section 3, including model definition and the algorithm details.", "labels": [], "entities": [{"text": "model definition", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.6970419734716415}]}, {"text": "Then we will present the experiments in section 4.", "labels": [], "entities": []}, {"text": "Finally, the work is summarized in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first perform an experiment to compare our method to several baselines.", "labels": [], "entities": []}, {"text": "And  then we perform parameter analysis to demonstrate how the parameters will affect our model.", "labels": [], "entities": []}, {"text": "At last, we conduct some case studies to analysis these methods in details.", "labels": [], "entities": []}, {"text": "The output of our method is a ranked list, so we use mean average precision (MAP) as one of our 1 http://github.com/dreamszl/dtopwords evaluation metrics.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 53, "end_pos": 81, "type": "METRIC", "confidence": 0.9411310454209646}]}, {"text": "The MAP value is computed as follows: where the P (k) is the precision of the top k words, rel(k) is a indicator function which return 1 when word at rank k is a domain-specific word and 0 otherwise.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9968337416648865}]}, {"text": "K is the length of the result list.", "labels": [], "entities": [{"text": "length", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9692466855049133}]}, {"text": "When we get a list whose elements are all domainspecific words, the M AP (K) will be 1.", "labels": [], "entities": [{"text": "M AP (K)", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9239460825920105}]}, {"text": "We will also display the precision-recall curves of our results.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 25, "end_pos": 41, "type": "METRIC", "confidence": 0.9991263747215271}]}, {"text": "We compare different settings of our method with two baselines.", "labels": [], "entities": []}, {"text": "The first baseline is pattern-based unsupervised new word detection method, which is proposed by.", "labels": [], "entities": [{"text": "word detection", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.7454367130994797}]}, {"text": "The following statistical features are taken into consideration: left pattern entropy (LPE), normalized multiword expression distance (NMED), enhanced mutual information (EMI).", "labels": [], "entities": [{"text": "left pattern entropy (LPE)", "start_pos": 65, "end_pos": 91, "type": "METRIC", "confidence": 0.6742749859889349}, {"text": "normalized multiword expression distance (NMED)", "start_pos": 93, "end_pos": 140, "type": "METRIC", "confidence": 0.7243504524230957}, {"text": "enhanced mutual information (EMI)", "start_pos": 142, "end_pos": 175, "type": "METRIC", "confidence": 0.7064010699590048}]}, {"text": "We implement both character based and word-based version, and the wordbased version outperforms character based version.", "labels": [], "entities": []}, {"text": "We use the optimal parameter setting in Huang's method, which is the LPE+NMED setting in their paper.", "labels": [], "entities": [{"text": "LPE+NMED setting", "start_pos": 69, "end_pos": 85, "type": "METRIC", "confidence": 0.8031041622161865}]}, {"text": "And we use annotated words to extract the candidate patterns which is a pretty good treatment for this method.", "labels": [], "entities": []}, {"text": "The second baseline is origin TopWords method which has been mentioned in above section.", "labels": [], "entities": []}, {"text": "We first run the TopWords method in the domainspecific corpus, and then use a function to rerank the word dictionary \u03b8.", "labels": [], "entities": []}, {"text": "We use two functions to rerank the dictionary.", "labels": [], "entities": []}, {"text": "The first one is the background frequency function and we denote this version as TopWords+Fre.", "labels": [], "entities": [{"text": "Fre", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.8843374252319336}]}, {"text": "The second one is the standard relative frequency method, we use the dictionary \u03b8 B of TopWords method run in background \u6362\u800c\u8a00\u4e4b(in other words) \u81f3\u5c11(at least) \u6362\u800c\u8a00\u4e4b(in other words) \u5177\u4f53\u6765\u8bf4(specifically speaking) \u5bf9\u9f50\u4f4d\u7f6e(alignment position) \u5b57\u7b26(character) \u540c\u5b66\u4eec\u597d(hello students) \u987a\u5e8f\u6027(succession) \u62ec\u53f7(brackets) \u6211\u4eec(we) \u8bf8\u5982\u6b64\u7c7b(and so on): PR-Curves of our methods and two baselines corpus to rerank \u03b8.", "labels": [], "entities": []}, {"text": "We denote this version as TopWords+RF.", "labels": [], "entities": [{"text": "RF", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9343206286430359}]}], "tableCaptions": [{"text": " Table 2: Discovering new words in data structure domain (MAP)", "labels": [], "entities": [{"text": "Discovering new words in data structure domain (MAP)", "start_pos": 10, "end_pos": 62, "type": "TASK", "confidence": 0.8195554673671722}]}, {"text": " Table 5: MAP of top 100 results'performance  with different \u03b1 and \u03d5, under the D- TopWords+Fre model.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9855816960334778}]}]}