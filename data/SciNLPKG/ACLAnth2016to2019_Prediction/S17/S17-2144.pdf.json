{"title": [{"text": "NLG301 at SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News", "labels": [], "entities": [{"text": "NLG301", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9027482867240906}, {"text": "Fine-Grained Sentiment Analysis", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.6407192846139272}, {"text": "Financial Microblogs and News", "start_pos": 66, "end_pos": 95, "type": "DATASET", "confidence": 0.6066868081688881}]}], "abstractContent": [{"text": "Short length, multi-targets, target relationship , monetary expressions, and outside reference are characteristics of financial tweets.", "labels": [], "entities": []}, {"text": "This paper proposes methods to extract target spans from a tweet and its referencing web page.", "labels": [], "entities": []}, {"text": "Total 15 publicly available sentiment dictionaries and one sentiment dictionary constructed from training set, containing sentiment scores in binary or real numbers, are used to compute the sentiment scores of text spans.", "labels": [], "entities": []}, {"text": "Moreover, the correlation coefficients of the price return between any two stocks are learned with the price data from Bloomberg.", "labels": [], "entities": []}, {"text": "They are used to capture the relationships between the interesting target and other stocks mentioned in a tweet.", "labels": [], "entities": []}, {"text": "The best result of our method in both sub-task are 56.68% and 55.43%, evaluated by evaluation method 2.", "labels": [], "entities": []}], "introductionContent": [{"text": "Nowadays the discussion of finance on social media such as twitter reveals the feeling of a market in some degrees.", "labels": [], "entities": []}, {"text": "Market sentiment analysis becomes an important financial technology in trading strategies.", "labels": [], "entities": [{"text": "Market sentiment analysis", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8017615079879761}]}, {"text": "Financial tweet sentiment classification differs from traditional sentiment classification in several ways.", "labels": [], "entities": [{"text": "Financial tweet sentiment classification", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6513969972729683}, {"text": "sentiment classification", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.7490932643413544}]}, {"text": "Firstly, the sentiment degree is areal number rather than discrete numbers such as 1 (positive), 0 (neutral), and -1 (negative).", "labels": [], "entities": []}, {"text": "Secondly, a financial tweet usually concerns multiple targets.", "labels": [], "entities": []}, {"text": "In the tweet, \"Oil To Break Out: Adding Chevron https://t.co/IrZkAVxjiE $AXP $CLGRF $CSCO $ERX $IBM $MCD $SSRI $VLO $WMT $XOM $CVX\", all companies denoted by the cashtag $ticker-symbol share only one description.", "labels": [], "entities": []}, {"text": "The activity, oil to breakout, is a good news for energy companies, but maybe bad for shipping companies.", "labels": [], "entities": []}, {"text": "Domain knowledge about the targets is necessary for suitable interpretation in this case.", "labels": [], "entities": []}, {"text": "Thirdly, numbers in the financial tweets are quite important.", "labels": [], "entities": []}, {"text": "In the tweet, \"MarketWatch: RT wmwitkowski: Guess who sold off about $800 million in $MDLZ after losing about $1 billion on $VRX???https://t.co/SHiJutyenv\", large number means more negative.", "labels": [], "entities": []}, {"text": "In contrast, in the tweet (named T1 hereafter), \"$AAPL now up 2.2% w/div since my original call, while $SPY up only 0.6% even w/ this Fri's div.", "labels": [], "entities": [{"text": "AAPL", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.99497389793396}]}, {"text": "Still holding\", the larger the number is, the more positive score the target will get.", "labels": [], "entities": []}, {"text": "The activity related to numbers determines the polarity and its degree.", "labels": [], "entities": []}, {"text": "Fourthly, sentiment scores depend on the activity of the companies, and their relationships, e.g., the adversarial relation versus the cooperate relation.", "labels": [], "entities": []}, {"text": "In the tweet, \"Report: Apple signs up for Google's cloud, uses much less of Amazon's $AAPL $GOOG $GOOGL $AMZN $DROPB https://t.co/zN3KDGYvGT\", $AAPL $GOOGL $AMZN and $DROPB are assigned sentiment scores 0.15, 0.443, -0.38 and -0.213, respectively, by human annotators because Amazon and Dropbox are two competitors of Google in the cloud market.", "labels": [], "entities": []}, {"text": "This paper explores various types of features selected from the text span related to the interesting targets for fine-grained financial tweet sentiment classification.", "labels": [], "entities": [{"text": "fine-grained financial tweet sentiment classification", "start_pos": 113, "end_pos": 166, "type": "TASK", "confidence": 0.7228477597236633}]}, {"text": "Both human and machine labelled text spans are used and compared.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 surveys the related work.", "labels": [], "entities": []}, {"text": "Section 3 presents the identification of text spans and extraction of features from them.", "labels": [], "entities": [{"text": "identification of text spans", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.8630427718162537}]}, {"text": "Section 4 shows and discusses the experimental results.", "labels": [], "entities": []}, {"text": "Section 5 concludes the remarks.", "labels": [], "entities": []}, {"text": "This paper is different from the above coarsegrained approaches.", "labels": [], "entities": []}, {"text": "Multi-targets in a short text is one of the major issues to be tackled.", "labels": [], "entities": []}, {"text": "We will find the sentiment of an interesting target in a tweet", "labels": [], "entities": []}], "datasetContent": [{"text": "In coarse-grained classification, we classify the sentiment of a given target mentioned in a tweet into positive, negative, or neutral.", "labels": [], "entities": []}, {"text": "The SVM model with the proposed 21 features are used.", "labels": [], "entities": []}, {"text": "In fine-grained classification, we predict the sentiment score of the given target in real number.", "labels": [], "entities": []}, {"text": "The SVR is adopt for this task.", "labels": [], "entities": [{"text": "SVR", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.6108795404434204}]}, {"text": "Both model followed the default parameters used in python Scikit-learn (Pedregosa et al., 2011) Accuracy and cosine similarity are used to measure the performance of the coarse-grained and the fine-grained tasks, respectively.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9987720847129822}, {"text": "cosine similarity", "start_pos": 109, "end_pos": 126, "type": "METRIC", "confidence": 0.8507543206214905}]}, {"text": "The ground truth is represented as a vector of targets' sentiment scores in cosine measurement.", "labels": [], "entities": []}, {"text": "Using manual spans achieves the ideal performance because the critical text span for the target is known beforehand.", "labels": [], "entities": []}, {"text": "Using EP spans is better than using the complete tweet, but worse than using manual spans.", "labels": [], "entities": []}, {"text": "The performance of the ED-S Span approach does not meet our original expectation due to the noise results in dependency parsing in tweets, which are usually incomplete sentences.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.719204843044281}]}, {"text": "The ensemble of the first five methods show in show the best accuracy in the 4-fold validation for both coarse-grained and fine-grained case.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9993131160736084}]}, {"text": "Although ED-T method provides the span whose average length is the closest to manual span, it gets the worse accuracy.", "labels": [], "entities": [{"text": "ED-T", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.7991909384727478}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.998403012752533}]}, {"text": "It's worth to leave no stone unturned.", "labels": [], "entities": []}, {"text": "We leave this part in the future works.", "labels": [], "entities": []}, {"text": "Due to the limited amount of submission, we submit two test results: manual span and the ensemble.", "labels": [], "entities": []}, {"text": "The final result for the SemEval-2017 Task 5 by cosine similarity are 35.66% and 38.28%, and by evaluation method 2 4 are 55.34% and 56.68%, as the test of 4-fold validation that ensemble result got the best accuracy.", "labels": [], "entities": [{"text": "SemEval-2017 Task", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.8144799172878265}, {"text": "accuracy", "start_pos": 208, "end_pos": 216, "type": "METRIC", "confidence": 0.9980965256690979}]}, {"text": "The same dictionaries and SVR model are used to subtask 2, using the news headline data.", "labels": [], "entities": [{"text": "news headline data", "start_pos": 69, "end_pos": 87, "type": "DATASET", "confidence": 0.7583660085995992}]}, {"text": "The best result is 55.43% using evaluation method 2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Total 15 sentiment dictionaries  of two forms, real value and binary, are consulted.  In addition to the publicly available dictionaries,  we also construct a sentiment dictionary from the  training set automatically.", "labels": [], "entities": []}]}