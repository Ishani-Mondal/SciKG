{"title": [{"text": "NTNU-2 at SemEval-2017 Task 10: Identifying Synonym and Hyponym Relations among Keyphrases in Scientific Documents", "labels": [], "entities": [{"text": "NTNU-2", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8621490001678467}, {"text": "Identifying Synonym and Hyponym Relations among Keyphrases in Scientific Documents", "start_pos": 32, "end_pos": 114, "type": "TASK", "confidence": 0.8096829235553742}]}], "abstractContent": [{"text": "This paper presents our relation extraction system for subtask C of SemEval-2017 Task 10: ScienceIE.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8137701451778412}, {"text": "SemEval-2017 Task 10: ScienceIE", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.49814821481704713}]}, {"text": "Assuming that the keyphrases are already annotated in the input data, our work explores a wide range of linguistic features, applies various feature selection techniques, optimizes the hyper parameters and class weights and experiments with different problem formulations (single classification model vs individual classifiers for each keyphrase type, single-step classifier vs pipeline clas-sifier for hyponym relations).", "labels": [], "entities": []}, {"text": "Performance of five popular classification algorithms are evaluated for each problem formulation along with feature selection.", "labels": [], "entities": []}, {"text": "The best setting achieved an F 1 score of 71.0% for synonym and 30.0% for hyponym relation on the test data.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9929145574569702}]}], "introductionContent": [], "datasetContent": [{"text": "Preprocessing Input text is linguistically analyzed with the Stanford CoreNLP library), which includes sentence boundary detection, tokenization, lemmatization, partof-speech (POS) tagging and dependency parsing.", "labels": [], "entities": [{"text": "Stanford CoreNLP library", "start_pos": 61, "end_pos": 85, "type": "DATASET", "confidence": 0.8891903360684713}, {"text": "sentence boundary detection", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.6715942819913229}, {"text": "partof-speech (POS) tagging", "start_pos": 161, "end_pos": 188, "type": "TASK", "confidence": 0.6028522729873658}, {"text": "dependency parsing", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.7527587711811066}]}, {"text": "Feature Extraction Features are extracted for every possible keyphrase pair within a sentence.", "labels": [], "entities": []}, {"text": "The feature extraction process dependents heavily on contextual information and dependency structures, specifically, the shortest dependency path between two keyphrase heads and the dependency subtree connecting two keyphrases as described in ().", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7254483550786972}]}, {"text": "The major feature categories are:  Feature Selection Methods As shown, the keyhrase length () and the in-between context length (\u03bb) can be arbitrarily large.", "labels": [], "entities": [{"text": "in-between context length (\u03bb)", "start_pos": 102, "end_pos": 131, "type": "METRIC", "confidence": 0.7777067720890045}]}, {"text": "As a result, the feature extraction process generates a large number of features, many of which are unlikely to provide any useful information.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7782191336154938}]}, {"text": "Therefore we investigated three different feature selection techniques, as shown in the bottom half of.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7579794526100159}]}, {"text": "Among these feature selection techniques, \u03c7 2 -based feature selection (X2) gave the best result.", "labels": [], "entities": []}, {"text": "Parameter Optimization through CV The training instances were extracted from 350 training files, indexed by training filename, followed by preprocessing and feature extraction as described above.", "labels": [], "entities": []}, {"text": "The class weights, parameters for five classifiers and k (the top-k feature for \u03c7 2 -based feature selection) were optimized for the three different experimental setups (System 1-3) descibed below using five fold cross validation with grid search, where training instances from the same training file are always in the same fold.", "labels": [], "entities": []}, {"text": "Our implementation relied on classifiers, feature selection methods and CV grid search from Scikitlearn . System-1 We ran CV experiments to optimize settings for the separate relation prediction tasks: synonym process (SP), synonym task (ST), synonym material (SM), hyponym process (HP), hyponym task (HT) and hyponym material (HM).", "labels": [], "entities": []}, {"text": "For each task, we optimized the hyper-parameters of five classifiers as shown in.", "labels": [], "entities": []}, {"text": "The performance of the best classifier was then evaluated on the development dataset.", "labels": [], "entities": []}, {"text": "For the hyponym relation, we optimized on the micro-average score over the forward and backward relation.", "labels": [], "entities": []}, {"text": "System-2 System-2 consists of a combination of one synonym classifier and one hyponym classifier.", "labels": [], "entities": []}, {"text": "System-3 Hyponym relations and their directions were predicted by separate classifiers connected in a pipeline.", "labels": [], "entities": []}, {"text": "Parameters were therefore optimized for relation and direction prediction separately.", "labels": [], "entities": [{"text": "relation and direction prediction", "start_pos": 40, "end_pos": 73, "type": "TASK", "confidence": 0.7348449155688286}]}, {"text": "The synonym predictions of System-3 result from the combination of the synonym classifier of 1-4 and 2 where any keyphrase pair predicted by either classifier 1-4 or classifier 2 is considered as synonym.", "labels": [], "entities": []}, {"text": "shows the result of System 1-3 on development data, while shows performance on test data.", "labels": [], "entities": []}, {"text": "According to, the combined performance of individual classifiers (of System-1) for synonym (SM-SP-ST) and hyponym (HM-HP-HT) is 77% and 29%, which is slightly lower then the corresponding performance of system-2.", "labels": [], "entities": []}, {"text": "This is consistent with performance on the test data.On the other-hand, the pipeline of System-3 shows a lower score than System-1 and System-2 for the hyponym relation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Keyphrase related statistics on data sets", "labels": [], "entities": []}, {"text": " Table 2: Relation related statistics on data sets", "labels": [], "entities": [{"text": "Relation", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.9391152858734131}]}, {"text": " Table 4: Result of individual classifiers where  hyponym relations are considered as three class  problem with micro average of positive classes", "labels": [], "entities": [{"text": "Result", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9612101912498474}]}, {"text": " Table 5: Result of synonym and hyponym relation  of System 1-3 on test data", "labels": [], "entities": []}]}