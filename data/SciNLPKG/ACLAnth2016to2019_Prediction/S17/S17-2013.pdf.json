{"title": [{"text": "UdL at SemEval-2017 Task 1: Semantic Textual Similarity Estimation of English Sentence Pairs Using Regression Model over Pairwise Features", "labels": [], "entities": [{"text": "Semantic Textual Similarity Estimation of English Sentence Pairs", "start_pos": 28, "end_pos": 92, "type": "TASK", "confidence": 0.7716540694236755}]}], "abstractContent": [{"text": "This paper describes the model UdL we proposed to solve the semantic textual similarity task of SemEval 2017 workshop.", "labels": [], "entities": [{"text": "semantic textual similarity task of SemEval 2017 workshop", "start_pos": 60, "end_pos": 117, "type": "TASK", "confidence": 0.682998564094305}]}, {"text": "The track we participated in was estimating the semantics relatedness of a given set of sentence pairs in English.", "labels": [], "entities": [{"text": "estimating the semantics relatedness of a given set of sentence pairs", "start_pos": 33, "end_pos": 102, "type": "TASK", "confidence": 0.7349205586043271}]}, {"text": "The best run out of three submitted runs of our model achieved a Pearson correlation score of 0.8004 compared to a hidden human annotation of 250 pairs.", "labels": [], "entities": [{"text": "Pearson correlation score", "start_pos": 65, "end_pos": 90, "type": "METRIC", "confidence": 0.9204955498377482}]}, {"text": "We used random forest ensemble learning to map an expandable set of extracted pair-wise features into a semantic similarity estimated value bounded between 0 and 5.", "labels": [], "entities": []}, {"text": "Most of these features were calculated using word embedding vectors similarity to align Part of Speech (PoS) and Name Entities (NE) tagged tokens of each sentence pair.", "labels": [], "entities": []}, {"text": "Among other pairwise features, we experimented a classical tf-idf weighted Bag of Words (BoW) vector model but with character-based range of n-grams instead of words.", "labels": [], "entities": []}, {"text": "This sentence vector BoW-based feature gave a relatively high importance value percentage in the feature importances analysis of the ensemble learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) is a shared task that have been running every year by SemEval workshop since 2012.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8074250022570292}]}, {"text": "Each year, the participating teams are encouraged to utilize the previous years data sets as a training set for their models.", "labels": [], "entities": []}, {"text": "The teams are then ranked by their test score on a hidden human annotated pairs of sentences.", "labels": [], "entities": []}, {"text": "After the end of the competition, the organizers publish the gold standards and ask the teams of the coming year task to use it as a training set and soon.", "labels": [], "entities": []}, {"text": "The description of STS2017 task is reported in.", "labels": [], "entities": [{"text": "STS2017 task", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.7618787884712219}]}, {"text": "In STS2017 , the primary task consisted in 6 tracks covering both monolingual and cross-lingual sentence pairs for the languages Spanish, English, Arabic, and Turkish.", "labels": [], "entities": []}, {"text": "Our team, UdL, only participated in the English monolingual track (Track 5).", "labels": [], "entities": [{"text": "UdL", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.9409682750701904}, {"text": "English monolingual track", "start_pos": 40, "end_pos": 65, "type": "DATASET", "confidence": 0.8192301193873087}]}, {"text": "The data consist in thousands of pairs of sentences from various resources like (Twitter news, image captions, news headline, questions, answers, paraphrasing, post-editing...).", "labels": [], "entities": []}, {"text": "For each pair, a human annotated score (from 0 to 5) is assigned and indicates the semantic similarity values of the two sentences.", "labels": [], "entities": []}, {"text": "The challenge is then to estimate the semantic similarity of 250 sentence pairs with hidden similarity values.", "labels": [], "entities": []}, {"text": "The quality of the proposed models would then be evaluated by the Pearson correlation between the estimated and the human annotated hidden values.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 66, "end_pos": 85, "type": "METRIC", "confidence": 0.9804459810256958}]}, {"text": "In section 2, we link to some related work to this problem.", "labels": [], "entities": []}, {"text": "The data preparation method followed by a full description of the model pipeline and its implementation are then presented in sections 3, 4, and 5.", "labels": [], "entities": []}, {"text": "Results of the model selection experiments and the final task results are shown in section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Pairwise features set.", "labels": [], "entities": []}, {"text": " Table 3: Regression estimator selection based on experimental evaluation score over a few data sets.", "labels": [], "entities": [{"text": "Regression", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9024929404258728}]}, {"text": " Table 4: Evaluation 2-decimal-rounded score on some testsets. DF: domain feature, AA:answer-answer,  AS:answers students, H16:headlines 2016, QQ:question-question, BH:bigger data set size where hash- tags are filtered", "labels": [], "entities": [{"text": "AA", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9854239821434021}, {"text": "BH", "start_pos": 165, "end_pos": 167, "type": "METRIC", "confidence": 0.9912388920783997}]}]}