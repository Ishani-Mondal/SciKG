{"title": [{"text": "TakeLab-QA at SemEval-2017 Task 3: Classification Experiments for Answer Retrieval in Community QA", "labels": [], "entities": [{"text": "Classification Experiments", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.8835605680942535}, {"text": "Answer Retrieval in Community QA", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.820287561416626}]}], "abstractContent": [{"text": "In this paper we present the TakeLab-QA entry to SemEval 2017 task 3, which is a question-comment re-ranking problem.", "labels": [], "entities": [{"text": "SemEval 2017 task 3", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7420253455638885}]}, {"text": "We present a classification based approach, including two supervised learning models-Support Vector Machines (SVM) and Con-volutional Neural Networks (CNN).", "labels": [], "entities": []}, {"text": "We use features based on different semantic similarity models (e.g., Latent Dirichlet Allocation), as well as features based on several types of pre-trained word embed-dings.", "labels": [], "entities": []}, {"text": "Moreover, we also use some hand-crafted task-specific features.", "labels": [], "entities": []}, {"text": "For training, our system uses no external labeled data apart from that provided by the organizers.", "labels": [], "entities": []}, {"text": "Our primary submission achieves a MAP-score of 81.14 and F1-score of 66.99-ranking us 10th on the SemEval 2017 task 3, subtask A.", "labels": [], "entities": [{"text": "MAP-score", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9993415474891663}, {"text": "F1-score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9998114705085754}, {"text": "SemEval 2017 task 3", "start_pos": 98, "end_pos": 117, "type": "DATASET", "confidence": 0.6257405281066895}]}], "introductionContent": [{"text": "The ever-growing Community Question Answering (CQA) on-line services are gaining popularity at an increasing rate.", "labels": [], "entities": [{"text": "Community Question Answering (CQA) on-line", "start_pos": 17, "end_pos": 59, "type": "TASK", "confidence": 0.7507980934211186}]}, {"text": "However, there are some problems inherent to question-answer collections created by on-line communities.", "labels": [], "entities": [{"text": "question-answer collections", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.8188552558422089}]}, {"text": "A major issue is the sheer volume of CQA collections, which makes finding an answer to a user question infeasible without some kind of an automated retrieval system.", "labels": [], "entities": [{"text": "CQA collections", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.8229433596134186}]}, {"text": "Consequently, information retrieval on CQA collections has gained increased focus in the research community, giving rise to several shared tasks on SemEval (.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.7704944312572479}, {"text": "CQA collections", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.6695961207151413}]}, {"text": "From a natural language processing perspective, this is a difficult task due to high variance in the quality of questions and answers in CQA collections (.", "labels": [], "entities": []}, {"text": "The cause of this is the self-moderated nature of CQA sites, which implies that there are few restrictions on who is allowed to answer a question.", "labels": [], "entities": []}, {"text": "In this paper we describe our entries for the SemEval 2017 Question-Comment Similarity subtask (.", "labels": [], "entities": [{"text": "SemEval 2017 Question-Comment Similarity subtask", "start_pos": 46, "end_pos": 94, "type": "TASK", "confidence": 0.7944580912590027}]}, {"text": "Given a question q and a comment list C, the task is to rank the comments in C according to their relevance with respect to q.", "labels": [], "entities": []}, {"text": "Datasets for this task were extracted from Qatar Living, a web forum where people pose questions about various aspects of their daily life in Qatar.", "labels": [], "entities": [{"text": "Qatar Living", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.935175210237503}]}, {"text": "Following, we framed the task as a binary classification problem.", "labels": [], "entities": []}, {"text": "We experimented with two classification approaches -Support Vector Machines (SVM) and Convolutional Neural Networks (CNN)).", "labels": [], "entities": []}, {"text": "Most of the features we use follow the work of  and.", "labels": [], "entities": []}, {"text": "Moreover, we use embedding-based) features for both models.", "labels": [], "entities": []}, {"text": "The CNN model with the full feature set has proven to be the most successful, ranking 10th in the competition with a MAP-score of 81.14 and an F1-score of 66.99.", "labels": [], "entities": [{"text": "MAP-score", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.999271810054779}, {"text": "F1-score", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9997808337211609}]}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief overview of the data set.", "labels": [], "entities": []}, {"text": "A detailed description of the our models is given in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 outlines our experiments and results.", "labels": [], "entities": []}, {"text": "Finally, we present our conclusions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset we used was provided by the sharedtask organizers.", "labels": [], "entities": []}, {"text": "Incoming user queries are denoted as original questions.", "labels": [], "entities": []}, {"text": "For each original question, we are provided with 10 annotated threads.", "labels": [], "entities": []}, {"text": "Each thread consists of a related question  10 comments posted as answers to the related question.", "labels": [], "entities": []}, {"text": "Subtask A, the main focus of this work, is concerned with correctly ranking the 10 comments with respect to a related question (henceforth: question).", "labels": [], "entities": []}, {"text": "Every question-comment pair contains a classification label Good, Bad, or PotentiallyUseful.", "labels": [], "entities": []}, {"text": "In our experiments, labels Bad and PotentiallyUseful are both considered non-relevant and the label Good is considered relevant.", "labels": [], "entities": []}, {"text": "shows the class distribution of question-comment pairs in the shared task dataset.", "labels": [], "entities": []}, {"text": "There is a slight bias towards the non-relevant class, but no mechanisms were implemented to address this.", "labels": [], "entities": []}, {"text": "The official split allocates 86% of the data to the train set and 14% to the development set.", "labels": [], "entities": []}, {"text": "For further information on the collection we refer to ().", "labels": [], "entities": []}, {"text": "Participants were allowed to make three submissions and mark them as primary, contrastive1, or contrastive2.", "labels": [], "entities": []}, {"text": "All submissions were evaluated but only the primary was considered for the competition system ranking.", "labels": [], "entities": []}, {"text": "For our contrastive1 run we submitted an SVM classifier trained on features from the embeddingbased and other groups.", "labels": [], "entities": []}, {"text": "We denote this model as SVM-EO.", "labels": [], "entities": []}, {"text": "In our contrastive2 run we submitted a completely different combination: the CNN classifier with the SEMILAR features as additional input to the hidden layer.", "labels": [], "entities": [{"text": "SEMILAR", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.9548594355583191}]}, {"text": "We refer to this variant of the model as CNN-S.", "labels": [], "entities": [{"text": "CNN-S", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.9382734894752502}]}, {"text": "The last combination we considered was the CNN classifier with all the other features provided as additional input to the hidden layer.", "labels": [], "entities": [{"text": "CNN classifier", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.5843605399131775}]}, {"text": "We refer to this submission as CNN-EOS.", "labels": [], "entities": [{"text": "CNN-EOS", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9788145422935486}]}, {"text": "This model has access to both the feature representations generated by the convolutional layers, as well as to all other features.", "labels": [], "entities": []}, {"text": "Thus, expectedly, it performed best on the development set, and we decided to submit it as our primary run.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Class distribution statistics.", "labels": [], "entities": [{"text": "Class distribution", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8560720086097717}]}, {"text": " Table 2: Submission results summary.", "labels": [], "entities": [{"text": "Submission", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9673700332641602}]}]}