{"title": [{"text": "INF-UFRGS at SemEval-2017 Task 5: A Supervised Identification of Sentiment Score in Tweets and Headlines", "labels": [], "entities": [{"text": "INF-UFRGS at SemEval-2017 Task 5", "start_pos": 0, "end_pos": 32, "type": "DATASET", "confidence": 0.8975311160087586}]}], "abstractContent": [{"text": "This paper describes a supervised solution for detecting the polarity scores of tweets or headline news in the financial domain, submitted to the SemEval 2017 Fine-Grained Sentiment Analysis on Financial Microblogs and News Task.", "labels": [], "entities": [{"text": "SemEval 2017 Fine-Grained Sentiment Analysis on Financial Microblogs", "start_pos": 146, "end_pos": 214, "type": "TASK", "confidence": 0.638133566826582}]}, {"text": "The premise is that it is possible to understand market reaction over a company stock by measuring the positive/negative sentiment contained in the financial tweets and news headlines, where polarity is measured in a continuous scale ranging from-1.0 (very bearish) to 1.0 (very bullish).", "labels": [], "entities": []}, {"text": "Our system receives as input the textual content of tweets or news headlines, together with their ids, stock cashtag or name of target company, and the polarity score gold standard for the training dataset.", "labels": [], "entities": []}, {"text": "Our solution retrieves features from these text instances using n-gram, hashtags, sentiment score calculated by a external APIs and others features to train a regression model capable to detect continuous score of these sentiments with precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 236, "end_pos": 245, "type": "METRIC", "confidence": 0.9926727414131165}]}], "introductionContent": [{"text": "Sentiment analysis involves the automatic identification of opinions, feelings, evaluations, attitudes expressed by people in the written language.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9488690495491028}, {"text": "identification of opinions, feelings, evaluations", "start_pos": 42, "end_pos": 91, "type": "TASK", "confidence": 0.735838064125606}]}, {"text": "A popular line of work in this field is opinion mining.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.9134984910488129}]}, {"text": "Growing attention has been dedicated to sentiment analysis in the financial domain, given its links to market dynamics.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.960351437330246}]}, {"text": "The challenges are to detect how sentiment is expressed in documents in this domain, and how it can translate to a reaction over a company stock, ranging from bullish to bearish.", "labels": [], "entities": []}, {"text": "This problem is addressed as part of SemEval-2017 (International Workshop on Semantic Evaluation 2017), Task 5 1 . The task was defined as follows: \"given a text instance (microblog message in Track 1, news statement or headline in Track 2), predict the sentiment score for each of the companies/stocks mentioned.", "labels": [], "entities": [{"text": "SemEval-2017 (International Workshop on Semantic Evaluation 2017)", "start_pos": 37, "end_pos": 102, "type": "TASK", "confidence": 0.7938936352729797}]}, {"text": "Sentiment values need to be floating point values in the range of -1 (very negative/bearish) to 1 (very positive/bullish), with 0 designating neutral sentiment.\"", "labels": [], "entities": []}, {"text": "The task was divided into two subtasks, according to the type of document (i.e. tweets and financial headlines) and sentiment target, and this paper describes our solution for both problems.", "labels": [], "entities": []}, {"text": "We addressed these sub-tasks by building a supervised model to do regression of sentiment value in the documents based solely on their textual content.", "labels": [], "entities": []}, {"text": "The target of the sentiment in Task 5-1 is the company stocks for which two sets of annotated tweets were supplied: a training corpus with 1700 annotated tweets and a test corpus with 800 unannotated tweets for task evaluation purpose.", "labels": [], "entities": []}, {"text": "Two sets of news headlines were made available as part of Task 5-2, where the target of opinion is a company.", "labels": [], "entities": []}, {"text": "The training set was composed of 1142 annotated instances, and the test corpus has 491 unannotated instances for task evaluation.", "labels": [], "entities": []}, {"text": "Details of Task 5 can be found at (.", "labels": [], "entities": []}, {"text": "The regression of sentiment in a text can be complex, because the sentiment can be related in different levels and complexities to the document or just with an aspect or even with a comparison between entities.", "labels": [], "entities": []}, {"text": "Our strategy was to address the regression as an opinion mining problem.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.7610621452331543}]}, {"text": "In addition, sentiment score detection faces challenges common to sentiment analysis in general, such as use of vocabulary and slang specific of the stock market, orthography errors, sarcasm, etc.", "labels": [], "entities": [{"text": "sentiment score detection", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.9175624450047811}, {"text": "sentiment analysis", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.8726974725723267}]}, {"text": "Our method extracts a set of features from financial texts and associate this data with annotated sentiment score provided by each task to train a prediction model specific to sentiment found in tweets and another for sentiment found in headlines.", "labels": [], "entities": []}, {"text": "To explain the details of our solution the remaining of the paper describes the obtained results, the proposed solution and the experiments developed in the next sections respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "We made experiments as the basis for our proposed solutions.", "labels": [], "entities": []}, {"text": "The experiments for Tasks 5-1 and 5-2 are described in subsections 4.1 and 4.2, respectively.", "labels": [], "entities": []}, {"text": "In the both experiments we use different baselines.", "labels": [], "entities": []}, {"text": "For each subtask we add some features and test the improvements in cosine similarity measurements.", "labels": [], "entities": []}, {"text": "Based on the models built with improvement results reported in the experiment of each subtask (using 70% of instances for training the model and 30% of them to test the cosine similarity) we evaluate the test instances provided for each subtask.", "labels": [], "entities": []}, {"text": "The results of our experiments are reported in.", "labels": [], "entities": []}, {"text": "To evaluate the performance of the proposed system, we adopted as baseline a simple model trained over n-grams (with n =).", "labels": [], "entities": []}, {"text": "As an improvement, we kept the same n-gram textual features that appeared least twice, and at most in 95% of tweets instances.", "labels": [], "entities": []}, {"text": "Then we added the \"hashashtag' feature and the Alchemy score.", "labels": [], "entities": [{"text": "Alchemy score", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.811433881521225}]}, {"text": "These results are reported in as Final, as it corresponds to the solution submitted to Task 5-1.", "labels": [], "entities": []}, {"text": "We further improved this model (labeled Intermediate in) using the previous features, and in addition, all external stock features mentioned in Section 3.2.", "labels": [], "entities": []}, {"text": "The only exception was the feature variation delta in tweet date.", "labels": [], "entities": []}, {"text": "Despite the better result, this model was not submitted to the task, because the features added were not trustworthy in the test data due to the reasons explained in Section 3.2.", "labels": [], "entities": []}, {"text": "To evaluate the performance of the proposed system, we compare it to a baseline trained over ngrams with n = [1, 2, 3, 4] and keeping only its features that are present at least two instances of headlines.", "labels": [], "entities": []}, {"text": "Using the same algorithm we add the feature of sentiment polarity and score of Alchemy API.", "labels": [], "entities": [{"text": "sentiment polarity", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.6970767825841904}]}, {"text": "Results are reported in table 4.: Improvements gained after the changes in the initial baseline of models in the metric of cosine similarity", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Confusion Matrix -Microblog", "labels": [], "entities": [{"text": "Microblog", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.48211824893951416}]}, {"text": " Table 2: Confusion Matrix -News Headline", "labels": [], "entities": [{"text": "News Headline", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.8934313058853149}]}, {"text": " Table 3: Polarity Distribution in the Training  Datasets", "labels": [], "entities": [{"text": "Training  Datasets", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.8531699478626251}]}, {"text": " Table 4: Improvements gained after the changes  in the initial baseline of models in the metric of  cosine similarity", "labels": [], "entities": []}]}