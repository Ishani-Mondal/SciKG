{"title": [{"text": "STS-UHH at SemEval-2017 Task 1: Scoring Semantic Textual Similarity Using Supervised and Unsupervised Ensemble", "labels": [], "entities": [{"text": "Scoring Semantic Textual Similarity", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.8595769256353378}]}], "abstractContent": [{"text": "This paper reports the STS-UHH participation in the SemEval 2017 shared Task 1 of Semantic Textual Similarity (STS).", "labels": [], "entities": [{"text": "SemEval 2017 shared Task 1 of Semantic Textual Similarity (STS)", "start_pos": 52, "end_pos": 115, "type": "TASK", "confidence": 0.8044762561718622}]}, {"text": "Overall, we submitted 3 runs covering monolingual and cross-lingual STS tracks.", "labels": [], "entities": []}, {"text": "Our participation involves two approaches: unsupervised approach, which estimates a word alignment-based similarity score, and supervised approach, which combines dependency graph similarity and coverage features with lexical similarity measures using regression methods.", "labels": [], "entities": []}, {"text": "We also present away on ensem-bling both models.", "labels": [], "entities": []}, {"text": "Out of 84 submitted runs, our team best multilingual run has been ranked 12 thin overall performance with correlation of 0.61, 7 th among 31 participating teams.", "labels": [], "entities": [{"text": "correlation", "start_pos": 106, "end_pos": 117, "type": "METRIC", "confidence": 0.9777784943580627}]}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) measures the degree of semantic equivalence between a pair of sentences.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7513194928566614}]}, {"text": "Accurate estimation of semantic similarity would benefit many Natural Language Processing (NLP) applications such as textual entailment, information retrieval, paraphrase identification and plagiarism detection (.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7362193763256073}, {"text": "information retrieval", "start_pos": 137, "end_pos": 158, "type": "TASK", "confidence": 0.7882942855358124}, {"text": "paraphrase identification", "start_pos": 160, "end_pos": 185, "type": "TASK", "confidence": 0.8804673850536346}, {"text": "plagiarism detection", "start_pos": 190, "end_pos": 210, "type": "TASK", "confidence": 0.7788155972957611}]}, {"text": "In an attempt to support the research efforts in STS, the SemEval STS shared Task () offers an opportunity for developing creative new sentence-level semantic similarity approaches and to evaluate them on benchmark datasets.", "labels": [], "entities": [{"text": "SemEval STS shared Task", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.7335240095853806}]}, {"text": "Given a pair of sentences, the task is to provide a similarity score on a scale of 0..5 according to the extent to which the two sentences are considered semantically similar, with 0 indicating that the semantics of the sentences are * *These authors contributed equally to this work completely unrelated and 5 signifying semantic equivalence.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 52, "end_pos": 68, "type": "METRIC", "confidence": 0.9583170115947723}]}, {"text": "Final performance is measured by computing the Pearson's correlation (\u03c1) between machine-assigned semantic similarity scores and gold standard scores provided by human annotators.", "labels": [], "entities": [{"text": "Pearson's correlation (\u03c1)", "start_pos": 47, "end_pos": 72, "type": "METRIC", "confidence": 0.9749923547108968}]}, {"text": "Since last year, the STS task have been extended to involve additional subtasks for crosslingual STS.", "labels": [], "entities": [{"text": "STS task", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.7621296942234039}, {"text": "crosslingual STS", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.7432391047477722}]}, {"text": "Similar to the monolingual STS task, the cross-lingual task requires the semantic similarity measurement for two snippets of text that are written in different languages.", "labels": [], "entities": [{"text": "monolingual STS task", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7021946708361307}]}, {"text": "In contrast to last year's edition, the task is organized into 6 sub-tracks and a primary track, which is the average of all of the secondary sub-tracks results.", "labels": [], "entities": []}, {"text": "Secondary sub-tracks involve scoring similarity for monolingual sentence pairs in one language (Arabic, English, Spanish), and cross-lingual sentence pairs from the combination of two different languages (Arabic-English, Spanish-English, Turkish-English).", "labels": [], "entities": []}, {"text": "Our paper proposes both supervised and unsupervised systems to automatically scoring semantic similarity between monolingual and cross-lingual short sentences.", "labels": [], "entities": [{"text": "scoring semantic similarity between monolingual and cross-lingual short sentences", "start_pos": 77, "end_pos": 158, "type": "TASK", "confidence": 0.82551509141922}]}, {"text": "The two systems are then combined with an average ensemble to strengthen the similarity scoring performance.", "labels": [], "entities": [{"text": "similarity scoring", "start_pos": 77, "end_pos": 95, "type": "METRIC", "confidence": 0.9356876611709595}]}, {"text": "Out of 84 submissions, our system is placed 12 th with an overall primary score of 0.61.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report our results in.", "labels": [], "entities": []}, {"text": "Overall we submitted 3 runs: Run1 uses the unsupervised approach discussed earlier in Sec.", "labels": [], "entities": []}, {"text": "3.2, Run2 uses a supervised MLP neural network trained as described in Sec.", "labels": [], "entities": []}, {"text": "3.3, and Run3 uses the ensemble average system described in Sec.", "labels": [], "entities": [{"text": "Run3", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.9259595274925232}]}, {"text": "Due to time constraints and technical issues, only evaluation for English monolingual track was given.", "labels": [], "entities": []}, {"text": "Additionally, we were notable to compute the topic modeling and expansion features.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.6828331798315048}]}, {"text": "We included the missing features later after the task deadline.", "labels": [], "entities": []}, {"text": "Final ensemble results are given under Ens.*.", "labels": [], "entities": [{"text": "Ens.", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9717177152633667}]}, {"text": "According to the results, we can make following observations: \u2022 Our results significantly outperform the baseline provided by the task organizers for monolingual tracks by a large margin.", "labels": [], "entities": []}, {"text": "\u2022 The ensemble outperforms the individual ensemble members.", "labels": [], "entities": []}, {"text": "\u2022 Results obtained in monolingual, especially English, are markedly higher than in crosslingual tracks.", "labels": [], "entities": []}, {"text": "This might be due to noise introduced by the automatic translation.", "labels": [], "entities": []}, {"text": "\u2022 Results of track 4b appears to be significantly worse compared to other tracks results.", "labels": [], "entities": []}, {"text": "In addition to the machine translation accuracy challenge, the difficulty of this track lies in providing longer sentences with less informative surface overlap between the sentences compared to other tracks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7912560403347015}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.8693253397941589}]}], "tableCaptions": [{"text": " Table 1: Results obtained in terms of Pearson correlation over three runs for all the six sub-tracks in  comparison with the baseline and the top obtained correlation in each track. The primary score represents  the weighted mean correlation. Ens.* represents the results after adding the expansion and topic modeling  features.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 39, "end_pos": 58, "type": "METRIC", "confidence": 0.9417821168899536}, {"text": "weighted mean correlation", "start_pos": 217, "end_pos": 242, "type": "METRIC", "confidence": 0.6756414373715719}]}]}