{"title": [{"text": "DFKI-DKT at SemEval-2017 Task 8: Rumour Detection and Classification using Cascading Heuristics", "labels": [], "entities": [{"text": "DFKI-DKT at SemEval-2017 Task 8", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.7850638628005981}, {"text": "Rumour Detection and Classification", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.836373895406723}]}], "abstractContent": [{"text": "We describe our submissions for SemEval-2017 Task 8, Determining Rumour Veracity and Support for Rumours.", "labels": [], "entities": [{"text": "SemEval-2017 Task 8", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.8664843837420145}, {"text": "Determining Rumour Veracity", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.7913130720456442}]}, {"text": "The Digital Curation Technologies (DKT) (Rehm and Sasaki, 2016, 2015) team at the German Research Center for Artificial Intelligence (DFKI) participated in two subtasks: Subtask A (determining the stance of a message) and Subtask B (determining veracity of a message, closed variant).", "labels": [], "entities": []}, {"text": "In both cases, our implementation consisted of a Multivariate Logistic Regression (Maximum Entropy) classifier coupled with handwritten patterns and rules (heuristics) applied in a post-process cascading fashion.", "labels": [], "entities": []}, {"text": "We provide a detailed analysis of the system performance and report on variants of our systems that were not part of the official submission.", "labels": [], "entities": []}], "introductionContent": [{"text": "In today's digital age, the social, political and economic relevance of online media and online content is becoming more and more relevant.", "labels": [], "entities": []}, {"text": "Accordingly, the task of analysing and determining the veracity of online content is receiving a growing amount of attention by the NLP community.", "labels": [], "entities": []}, {"text": "The ability to detect whether apiece of news is fake or not, and to do so automatically, is a very timely language technology application (.", "labels": [], "entities": [{"text": "detect whether apiece of news is fake", "start_pos": 15, "end_pos": 52, "type": "TASK", "confidence": 0.7511207972254071}]}, {"text": "Through these shared tasks, we intend to address which linguistic and contextual features characterise a rumour.", "labels": [], "entities": []}, {"text": "SemEval2017 Task 8 () provided all participants with a dataset consisting of tweets in response to breaking news stories.", "labels": [], "entities": [{"text": "SemEval2017 Task 8", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.6062081158161163}]}, {"text": "It contains conversations responding to rumourous tweets.", "labels": [], "entities": []}, {"text": "These tweets have been annotated for support, deny, query or comment (SDQC).", "labels": [], "entities": []}, {"text": "The competition consisted of two subtasks: \u2022 Subtask A: Determining whether response tweets support, deny, query or comment (SDQC) on rumours (source tweet) \u2022 Subtask B: Given a tweet, determine whether the statement is true or false (i. e., a rumour).", "labels": [], "entities": []}, {"text": "This subtask featured two variants: closed (determining veracity from the tweet alone) and open (determining veracity from additional context).", "labels": [], "entities": []}, {"text": "We participated in the closed task.", "labels": [], "entities": []}, {"text": "Our approach to both subtasks involved extracting relevant features from the provided data and training a classifier followed by a set of heuristics implemented in a cascading decision tree style).", "labels": [], "entities": []}, {"text": "These rules, applied as a postprocess, help induce a better mapping from classification results to rumour categorisation and veracity detection because they take into account specific features characterising a particular class.", "labels": [], "entities": [{"text": "veracity detection", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.7974562048912048}]}, {"text": "In this paper we seek to answer two questions using Rumour Detection and Classification as a case-study: \u2022 Which features comprise the set of postprocess rules?", "labels": [], "entities": [{"text": "Rumour Detection and Classification", "start_pos": 52, "end_pos": 87, "type": "TASK", "confidence": 0.9149245470762253}]}, {"text": "\u2022 What is the optimal technique to implement these heuristics (cascading order)?", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a bird's eye overview of our systems submitted for evaluation.", "labels": [], "entities": []}, {"text": "Section 3 describes the various rumour detection and classification models as well as experimental setups (not part of the official submission).", "labels": [], "entities": [{"text": "rumour detection and classification", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.8628962337970734}]}, {"text": "Section 4 displays the results and analyses them.", "labels": [], "entities": []}, {"text": "Section 5 contains a discussion of the task in general followed by an explanation of some design decisions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the details of the features used in our models as well as the different experimental settings.", "labels": [], "entities": []}, {"text": "The features used in the classification algorithms consisted of a vector of the words (twitter text).", "labels": [], "entities": []}, {"text": "When we attempted to incorporate some of the features described above in the classification algorithm, the performance deteriorated.", "labels": [], "entities": []}, {"text": "This led us to implement a post-process heuristic module and subject the results of the classification to a second: Evaluation scores of submitted system (first row) as well as other runs of our system.", "labels": [], "entities": []}, {"text": "shows the results of our experiments.", "labels": [], "entities": []}, {"text": "We submitted the MaxEnt results.", "labels": [], "entities": [{"text": "MaxEnt", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.8983817100524902}]}, {"text": "However, the ensemble method (combination of all three models) shows a much better performance.", "labels": [], "entities": []}, {"text": "demonstrates the number of correct categories we classified accurately (blue bar).", "labels": [], "entities": []}, {"text": "Our systems performed best at predicting the \"comment\" and \"query\" in subtask A and \"false\" in subtask B.", "labels": [], "entities": []}, {"text": "The poor performance on \"support\" in subtask A and \"true\" in subtask B can be attributed to our post-process framework, i.e. our rules are not sufficiently discriminative.", "labels": [], "entities": []}, {"text": "A work-around is to label all tweets as \"support\" and then implement the if-then rules.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Percentage of tweets in the four categories of training data containing a specific feature.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation scores of submitted system  (first row) as well as other runs of our system.", "labels": [], "entities": []}]}