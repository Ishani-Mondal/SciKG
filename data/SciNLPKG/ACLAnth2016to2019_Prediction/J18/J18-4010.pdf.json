{"title": [{"text": "We Usually Don't Like Going to the Dentist: Using Common Sense to Detect Irony on Twitter", "labels": [], "entities": [{"text": "Common Sense to Detect Irony", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.5818954885005951}]}], "abstractContent": [{"text": "Although commonsense and connotative knowledge come naturally to most people, computers still struggle to perform well on tasks for which such extratextual information is required.", "labels": [], "entities": []}, {"text": "Automatic approaches to sentiment analysis and irony detection have revealed that the lack of such world knowledge undermines classification performance.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9782271981239319}, {"text": "irony detection", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.9454146325588226}]}, {"text": "In this article, we therefore address the challenge of modeling implicit or prototypical sentiment in the framework of automatic irony detection.", "labels": [], "entities": [{"text": "automatic irony detection", "start_pos": 119, "end_pos": 144, "type": "TASK", "confidence": 0.6724611520767212}]}, {"text": "Starting from manually annotated connoted situation phrases (e.g., \"flight delays,\" \"sitting the whole day at the doctor's office\"), we defined the implicit sentiment held towards such situations automatically by using both a lexico-semantic knowledge base and a data-driven method.", "labels": [], "entities": []}, {"text": "We further investigate how such implicit sentiment information affects irony detection by assessing a state-of-the-art irony classifier before and after it is informed with implicit sentiment information.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.9561602771282196}]}], "introductionContent": [{"text": "With the advent of the Web 2.0, information sharing has acquired anew dimension.", "labels": [], "entities": [{"text": "information sharing", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.8167034685611725}]}, {"text": "Gifted with the ability to contribute actively to Web content, people constantly share their ideas and opinions using services like Facebook, Twitter, and WhatsApp.", "labels": [], "entities": [{"text": "WhatsApp", "start_pos": 155, "end_pos": 163, "type": "DATASET", "confidence": 0.8897348642349243}]}, {"text": "Similarly to face-to-face interactions, Web users strive for efficient communication, limiting the amount of conversation to what is necessarily required to understand the message and leave obvious things unstated.", "labels": [], "entities": []}, {"text": "As such, the utterance \"He lacks social responsibility\" will be perceived as negative, because it is obvious for most people that social responsibility is a positive human quality.", "labels": [], "entities": []}, {"text": "These obvious things are part of common sense: knowledge that people have of the world they live in, and that serves as a basis to form judgments and ideas.", "labels": [], "entities": []}, {"text": "Although this commonsense knowledge often refers to the obvious factual things people normally know about the world they live in, it can also refer to the affective information associated with these real-world events, actions, or objects.", "labels": [], "entities": []}, {"text": "This connotative knowledge, or typical sentiment related to real-world concepts, comes naturally to most people, but is far from trivial for computers.", "labels": [], "entities": []}, {"text": "Perhaps the most salient example of this are sentiment analysis systems, which show good performance on explicit sentiment expressions (e.g., \"brilliant\" in Example (1) (e.g., Van Hee et al., but struggle with text fragments that involve implicit sentiment (e.g., \"shuts down at 40%\" in Example (2).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.9400931894779205}]}, {"text": "(1) 3000th tweet dedicated to Andy Carroll and West Ham, brilliant start to the season!", "labels": [], "entities": [{"text": "West Ham", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.8981783390045166}]}, {"text": "(2) Since last update iPhone 6 battery shuts down at 40%.", "labels": [], "entities": []}, {"text": "Such implicit sentiment or connotative knowledge (i.e., the feeling a concept generally invokes fora person or a group of people) is also referred to as prototypical sentiment).", "labels": [], "entities": []}, {"text": "Like in Example (2), prototypical sentiment expressions are devoid of subjective words and rely on commonsense shared by the speaker and receiver in an interaction.", "labels": [], "entities": []}, {"text": "To be able to grasp such implied sentiment, sentiment analysis systems require additional knowledge that provides insight into the world we live in and affective information associated with natural language concepts.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9186441004276276}]}, {"text": "Although modeling implicit sentiment is still in its infancy (), such linking of concepts or situations to implicit sentiment will open new perspectives in natural language processing (NLP) applications, not only for sentiment analysis tasks, but also for any type of tasks that involves semantic text processing, such as automatic irony detection and the detection of cyberharassment ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 217, "end_pos": 235, "type": "TASK", "confidence": 0.8454367220401764}, {"text": "automatic irony detection", "start_pos": 322, "end_pos": 347, "type": "TASK", "confidence": 0.6407789587974548}]}, {"text": "Although automatic systems perform well in deriving particular information from a given text (i.e., the meaning of a word in context, emotions expressed by the author of a text), they often struggle to perform tasks where extratextual information is necessary to grasp the meaning of an utterance.", "labels": [], "entities": []}, {"text": "As mentioned earlier, one typical application where the lack of such implicit information becomes apparent is automatic irony detection.", "labels": [], "entities": [{"text": "automatic irony detection", "start_pos": 110, "end_pos": 135, "type": "TASK", "confidence": 0.657760888338089}]}, {"text": "Irony is traditionally defined as a rhetorical device where an evaluative utterance expresses the opposite of what is actually intended (e.g.,.", "labels": [], "entities": []}, {"text": "This implies the expression of a positive evaluation when a negative one is intended, or vice versa.", "labels": [], "entities": []}, {"text": "By doing so, speakers often rely on un-uttered knowledge such as mutually shared information or world knowledge.", "labels": [], "entities": []}, {"text": "An example of such implied (subjective) information is contained in the ironic utterance \"Cannot wait to go to the dentist tomorrow!\".", "labels": [], "entities": []}, {"text": "The apparent positive polarity expressed by \"cannot wait\" is contrasted with the negative sentiment implied by a visit to the dentist.", "labels": [], "entities": []}, {"text": "The latter can be considered a prototypical unpleasant activity; that is, even without explicit sentiment words, users understand its negative connotation.", "labels": [], "entities": []}, {"text": "Observing this contrast enables its listener to infer that the utterance is meant ironically.", "labels": [], "entities": []}, {"text": "The human brain excels at understanding such implicit meanings, as people learn from experiences and feel certain emotions based on appraisals) and inferences from related experiences.", "labels": [], "entities": []}, {"text": "Computers, by contrast, lack such world knowledge and can only rely on what they have learned from specific data.", "labels": [], "entities": []}, {"text": "In fact, when modeling subjectivity and sentiments in text, commonly used approaches include lexicon-based and statistical or machine learning methods ().", "labels": [], "entities": []}, {"text": "The majority of these approaches focus on identifying explicit sentiment clues in text: Lexicon-based approaches make use of sentiment dictionaries (e.g., and machine learning sentiment classifiers generally exploit features that represent explicit sentiment clues, like bags-of-words, punctuation marks, flooded characters, and so forth.", "labels": [], "entities": []}, {"text": "Much more challenging is detecting implicit sentiment, or identifying non-evaluative words that evoke a particular sentiment.", "labels": [], "entities": [{"text": "detecting implicit sentiment", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.862342913945516}]}, {"text": "In this article, we confront the challenge of automatically recognizing implicit sentiment in tweets and we explore whether such implicit sentiment information benefits automatic irony detection.", "labels": [], "entities": [{"text": "automatic irony detection", "start_pos": 169, "end_pos": 194, "type": "TASK", "confidence": 0.6741855442523956}]}, {"text": "Several studies have underlined the importance of implicit sentiment for irony detection (e.g.,; here, we present, to our knowledge, the first attempt to model implicit sentiment by using both a lexicosemantic knowledge base and a data-driven approach based on real-time tweets.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.9298231303691864}]}, {"text": "Moreover, manual annotations of our irony data set allowed us to evaluate the approach using gold-standard connoted situations such as going to the dentist.", "labels": [], "entities": [{"text": "irony data set", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.7536914944648743}]}, {"text": "Next, the validity of our approach to model implicit sentiment is assessed by evaluating the performance of a state-of-the-art irony detection system before and after informing it with implicit sentiment information.", "labels": [], "entities": [{"text": "model implicit sentiment", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6488040784994761}, {"text": "irony detection", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.7304959893226624}]}, {"text": "The remainder of this article is structured as follows: Section 2 presents an overview of related research on irony detection and implicit sentiment modeling, and Sections 3, 4, and 5 zoom in on the different experimental set-ups.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 110, "end_pos": 125, "type": "TASK", "confidence": 0.9533069729804993}, {"text": "implicit sentiment modeling", "start_pos": 130, "end_pos": 157, "type": "TASK", "confidence": 0.6630579630533854}]}, {"text": "In Section 3, we present a state-of-the-art irony detection system and in Section 4 we investigate the feasibility to model implicit sentiment in an automatic way.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.8417267799377441}]}, {"text": "Exploring whether implicit sentiment information benefits irony detection is the focus of Section 5.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.9556173086166382}]}, {"text": "Section 6 concludes and suggests some directions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the experiments, we made use of an SVM, as implemented in the LIBSVM library (Chang and Lin 2011).", "labels": [], "entities": [{"text": "LIBSVM library", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.8912762403488159}]}, {"text": "We chose an SVM as the classification algorithm because it has been successfully implemented with large feature sets and because its performance for similar tasks has been recognized (e.g.,.", "labels": [], "entities": []}, {"text": "We performed binary SVM classification using the default radial basis function (i.e., RBF or Gaussian) kernel, the performance of which equals that of a linear kernel if it is properly tuned (.", "labels": [], "entities": [{"text": "SVM classification", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.9474610388278961}]}, {"text": "Preliminary experiments on our data set showed even better results using RBF.", "labels": [], "entities": [{"text": "RBF", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.4537044167518616}]}, {"text": "Given the importance of parameter optimization to obtain good SVM models (Chang and Lin 2011), optimal C-and \u03b3-values were defined for each experiment, exploiting a different feature group or feature group combination.", "labels": [], "entities": []}, {"text": "For this purpose, a cross-validated grid search was performed across the complete training data.", "labels": [], "entities": []}, {"text": "During the parametrization, \u03b3 was varied between 2 \u221215 and 2 3 (stepping by factor 4), and C was varied between 2 -5 and 2 15 (stepping by factor 4).", "labels": [], "entities": []}, {"text": "The optimal parameter settings were used to build a model for each feature set-up using all the training data, which was evaluated on the held-out test set.", "labels": [], "entities": []}, {"text": "As the evaluation metrics, we report accuracy (the number of correct predictions divided by the total number of predictions), precision (the proportion of the data points the model says was relevant actually were relevant), recall (the proportion of relevant instances that were retrieved), and F 1 score (the harmonic mean of precision and recall), indicating how well the classifier detects irony.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9988985061645508}, {"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9993771910667419}, {"text": "recall", "start_pos": 224, "end_pos": 230, "type": "METRIC", "confidence": 0.9991844296455383}, {"text": "F 1 score", "start_pos": 295, "end_pos": 304, "type": "METRIC", "confidence": 0.9909791946411133}, {"text": "precision", "start_pos": 327, "end_pos": 336, "type": "METRIC", "confidence": 0.9967801570892334}, {"text": "recall", "start_pos": 341, "end_pos": 347, "type": "METRIC", "confidence": 0.9852322340011597}]}, {"text": "While the latter represents an average of the F 1 scores per class when used in multi-label or multi-class classification, in binary classification or detection tasks like the present, it is calculated on the positive (i.e., ironic) instances only.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9802778760592142}, {"text": "binary classification or detection tasks", "start_pos": 126, "end_pos": 166, "type": "TASK", "confidence": 0.7062193274497985}]}, {"text": "It is important to note that the experimental results we present in the following sections are calculated on the held-out test set.", "labels": [], "entities": []}, {"text": "In-between results obtained through cross-validation on the development set are not included because of space constraints.", "labels": [], "entities": []}, {"text": "Three straightforward baselines were implemented against which the performance of our irony detection model can be compared: a random class baseline and two n-gram baselines.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.909969687461853}]}, {"text": "The random class baseline randomly assigns a class label (i.e., ironic or not ironic) to each instance.", "labels": [], "entities": []}, {"text": "Next, we calculated the performance of two classifiers, one based on token unigram (w1g) and bigram (w2g) features, and a second one using character trigram (ch3g) and fourgrams (ch4g) features.", "labels": [], "entities": []}, {"text": "Hyperparameter optimization is crucial to the good functioning of the algorithm; hence it was also applied in the baseline experiments, except for random class.", "labels": [], "entities": []}, {"text": "displays the baseline scores on the held-out test set.", "labels": [], "entities": []}, {"text": "Although the random class baseline clearly benefits from the balanced class distribution, we find that the n-gram classifiers already present strong baselines for the task.", "labels": [], "entities": []}, {"text": "In fact, earlier studies showed that n-gram features have proven to work well for this task, despite their simplicity and universal character (e.g., Liebrecht, Kunneman, and van den Bosch 2013; Reyes, Rosso, and Veale 2013).", "labels": [], "entities": []}, {"text": "Having established the baselines, we tested the importance of the individual feature groups.", "labels": [], "entities": []}, {"text": "For this purpose, four models were built on the basis of lexical, syntactic, sentiment, and semantic features.", "labels": [], "entities": []}, {"text": "displays the scores of the individual feature groups on the held-out test set.", "labels": [], "entities": []}, {"text": "To facilitate comparison, the baseline scores are included in gray.", "labels": [], "entities": []}, {"text": "The best results per column are indicated in bold.", "labels": [], "entities": []}, {"text": "confirms the strong baseline that present n-gram features, given that none of the feature groups outperforms the character n-gram baseline in terms of F 1 score.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.9811383485794067}]}, {"text": "In terms of recall, syntactic features score better.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.999146580696106}]}, {"text": "Character n-gram features outperforming the lexical feature group (which contains a fair number of other lexical clues in addition to character n-grams) seems to indicate that the former work better for irony detection.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 203, "end_pos": 218, "type": "TASK", "confidence": 0.9419496655464172}]}, {"text": "This seems counterintuitive, however, because the lexical feature group includes information that has proven its usefulness for irony detection in related research (e.g., punctuation, flooding).", "labels": [], "entities": [{"text": "irony detection", "start_pos": 128, "end_pos": 143, "type": "TASK", "confidence": 0.9148261547088623}]}, {"text": "An explanation would be that the strength of a number of  individual features in the lexical feature group (potentially the most informative ones) is undermined by the feature abundance in the group.", "labels": [], "entities": []}, {"text": "Lexical features are, however, the only ones that outperform the token n-gram baseline.", "labels": [], "entities": []}, {"text": "Although this would suggest that lexical features are more informative for irony detection than the other feature groups, it is noteworthy that all other feature groups (i.e., syntactic, sentiment, and semantic) contain much less features, and that these features are not directly derived from the training data, as opposed to bag-of-words features.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.9083459973335266}]}, {"text": "Recall being less than 50% for the sentiment lexicon features shows that, when using merely explicit sentiment clues, about half of the ironic tweets are missed by the classifier.", "labels": [], "entities": []}, {"text": "This observation is inline with the findings of, who report irony detection scores between F 1 = 14% and F 1 = 47% when using merely sentiment lexicons.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.7760984599590302}, {"text": "F 1", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9853671491146088}, {"text": "F 1", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9805605113506317}]}, {"text": "Based on the results, we conclude that overall, lexical features perform best for the task (F 1 = 67%).", "labels": [], "entities": [{"text": "F 1", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9922867119312286}]}, {"text": "However, the best recall (69%) is obtained using syntactic features.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9995831847190857}]}, {"text": "A qualitative analysis of the classifiers' output indeed revealed that lexical features are not the holy grail to irony detection, and that each feature group has its own strength, by identifying a specific type or realization of irony.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 114, "end_pos": 129, "type": "TASK", "confidence": 0.7878556549549103}]}, {"text": "We observed, for instance, that lexical features are strongly predictive of irony (especially ironic by clash) in short tweets and tweets containing exaggerations (e.g., character repetition, see Example (6)), while sentiment features often capture ironic by clash instances that are very subjective or expressive (Example).", "labels": [], "entities": [{"text": "character repetition", "start_pos": 170, "end_pos": 190, "type": "TASK", "confidence": 0.6912446767091751}]}, {"text": "Syntactic features seem to work well for predicting irony in long tweets and tweets containing other verbal irony (Example).", "labels": [], "entities": [{"text": "predicting irony", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.8886390924453735}]}, {"text": "Finally, semantic features contribute most to detecting situational irony (Example (9)).", "labels": [], "entities": [{"text": "detecting situational irony", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.8849716385205587}]}, {"text": "(6) Loooovvveeee when my phone gets wiped -.- 3.8.3 Feature Group Combinations.", "labels": [], "entities": []}, {"text": "The previous paragraphs showed that although only lexical features outperform the word n-gram baseline, semantic, syntactic, and (to a lesser extent) sentiment features show to be good indicators of irony as well.", "labels": [], "entities": []}, {"text": "This is why we investigate in this section the potential of combining the aforementioned feature groups.", "labels": [], "entities": []}, {"text": "presents the results of a binary irony classifier exploiting a combination of feature groups obtained on the held-out test set.", "labels": [], "entities": []}, {"text": "The best individual feature group (i.e., lexical) and the character n-gram baselines are also included for the purpose of comparison.", "labels": [], "entities": []}, {"text": "From the results in, we can deduce that combining feature types improves classification performance, given that more than half of the combinations present an improvement over the character n-gram baseline and lexical features alone.", "labels": [], "entities": []}, {"text": "In particular, combining lexical with semantic and syntactic features seems to work well for irony detection, yielding atop F 1 score of 70.11%.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.9645620882511139}, {"text": "F 1 score", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9888888001441956}]}, {"text": "In, we compare the results of our best SVM-classifier (i.e., exploiting lexical + semantic + syntactic features) with that of state-of-the-art irony detection approaches.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 143, "end_pos": 158, "type": "TASK", "confidence": 0.8154534101486206}]}, {"text": "All scores are obtained on the test set created by, which originally consisted of 3,000 manually annotated tweets, 690 (or 23%) of which were sarcastic.", "labels": [], "entities": []}, {"text": "However, the reported scores only apply to a subset of the corpus, because of the perishability of Twitter data (i.e., only tweet IDs could be provided to download the actual content of the tweets).", "labels": [], "entities": []}, {"text": "808 presents precision, recall, and F 1 scores (except for Far\u00edas, Patti, and Rosso, who only report F 1 scores) obtained by three state-of-the-art irony detection systems on the same data set.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9994820952415466}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9973141551017761}, {"text": "F 1 scores", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9886234402656555}, {"text": "F 1 scores", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.8917383750279745}, {"text": "irony detection", "start_pos": 148, "end_pos": 163, "type": "TASK", "confidence": 0.7930358350276947}]}, {"text": "Our approach outperforms that of and in terms of F 1 score, but not that of.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9895230531692505}]}, {"text": "However, it is important to note that the results in the table should be interpreted carefully.", "labels": [], "entities": []}, {"text": "Our approach reports macro-averaged scores to assign each class equal weight in the evaluation (thus giving equal weight to the minority positivei.e., ironic class) and Joshi, Sharma, and Bhattacharyya (2015) report that they applied weighted averaging.", "labels": [], "entities": []}, {"text": "It is not clear, however, whether and which averaging method was applied by.", "labels": [], "entities": []}, {"text": "As scores may vary according to how they are averaged (e.g., if micro-averaged, more weight is given to the negative class), such information is required to allow fora fair comparison.", "labels": [], "entities": []}, {"text": "In the annotations section (see Section 3.2), we found that most ironic tweets in our corpus (i.e., 72%) show a contrast between a positive and a negative polarity expression.", "labels": [], "entities": []}, {"text": "In the following paragraphs, we aim to verify whether this category is also the most likely to be recognized automatically, as compared to other irony types.", "labels": [], "entities": []}, {"text": "To verify the validity of our assumption, we analyzed the classification output for the different irony types in our corpus.", "labels": [], "entities": []}, {"text": "visualizes the accuracy of the bestperforming classifier (i.e., lexical + semantic + syntactic features) for each irony type and the different types of non-ironic tweets (i.e., hashtag vs. background corpus).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9993830919265747}]}, {"text": "The bar chart seems to confirm our intuition that the system performs best on detecting ironic tweets that are realized by means of a polarity contrast (78% accuracy), followed by instances describing situational irony.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9817311763763428}]}, {"text": "On the other hand, detecting other type of irony appears much more challenging (45%).", "labels": [], "entities": [{"text": "detecting other type of irony", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.8019206762313843}]}, {"text": "A closer look at other verbal irony reveals that the instances are often ambiguous and realized in diverse ways, as shown in Examples (10) and (11).", "labels": [], "entities": []}, {"text": "It is important to recall that, prior to classification, the hashtags #irony, #sarcasm, and #not were removed from the tweets.", "labels": [], "entities": []}, {"text": "Classification errors on the ironic by clash category include tweets where the irony results from a polarity contrast which cannot be identified using sentiment lexicon features alone.", "labels": [], "entities": []}, {"text": "We see two possible explanations for this.", "labels": [], "entities": []}, {"text": "First, we observed that in the majority (77%) of the misclassified tweets, the only clue fora polarity contrast was an irony-related hashtag (i.e., \"#not\"), which was removed from the data prior to training.", "labels": [], "entities": []}, {"text": "In fact, as illustrated by Example (12), without such a meta-hashtag, it is very difficult to know whether the instance is ironic.", "labels": [], "entities": []}, {"text": "(12) Thanks dad for your support!", "labels": [], "entities": []}, {"text": "#not Second, tweets that do not require a meta-hashtag to perceive a polarity contrast, but that were nevertheless missed by the classifier (23%), included an evaluation as part of a hashtag (e.g., \"#thisonlygetsbetter\") or an implicit evaluation (Example).", "labels": [], "entities": []}, {"text": "As explained in the Introduction, understanding such implicit sentiment requires connotative (world) knowledge.", "labels": [], "entities": []}, {"text": "(13) Spending the majority of my day in and out of the doctor has been awesome.", "labels": [], "entities": []}, {"text": "#sarcasm Whereas the polarity opposition in Example (12) would be impossible-even for humans-to recognize without hashtag information, the polarity contrast in Example is likely to be identified, on the condition that the system could access commonsense or connotative knowledge.", "labels": [], "entities": []}, {"text": "As such, it would ideally recognize phrases like \"spending (...) day in and out of the doctor\" as related to negative sentiment, and find the contrast with the positive expression \"awesome.\"", "labels": [], "entities": []}, {"text": "In the next section, we therefore take a closer look at the implicit sentiment expressions (or targets) that were annotated in the irony corpus and take the first steps to detect such implicit or prototypical sentiment automatically.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Inter-annotator agreement (Fleiss' Kappa) obtained in two annotation rounds.", "labels": [], "entities": [{"text": "Fleiss' Kappa)", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.7376107970873514}]}, {"text": " Table 2  Experimental corpus statistics: Number of instances per annotation category plus non-ironic  tweets from a background corpus.", "labels": [], "entities": []}, {"text": " Table 3  Feature statistics per feature group.", "labels": [], "entities": []}, {"text": " Table 4  Classification results of the baselines (obtained on the test set).", "labels": [], "entities": []}, {"text": " Table 5  Irony detection results (obtained on the held-out test set) using individual feature groups.", "labels": [], "entities": [{"text": "Irony detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8596376776695251}]}, {"text": " Table 6  Irony detection results (obtained on the held-out test set) using combined feature groups.", "labels": [], "entities": [{"text": "Irony detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8444519937038422}]}, {"text": " Table 7  Comparison of our approach to three state-of-the-art irony detection methods. The results are  obtained on Riloff et al.'s (2013) irony data set. The best results per column are in bold.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.8127160966396332}, {"text": "Riloff et al.'s (2013) irony data set", "start_pos": 117, "end_pos": 154, "type": "DATASET", "confidence": 0.7200982787392356}]}, {"text": " Table 9  Automatically assigned implicit sentiment using SenticNet 4.", "labels": [], "entities": []}, {"text": " Table 12  Sentiment analysis accuracy based on a Twitter crawl using content word targets.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.9568131864070892}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9701650738716125}]}, {"text": " Table 14  Sentiment analysis accuracy based on a Twitter crawl using dependency heads as queries.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.9623346030712128}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9621075391769409}]}, {"text": " Table 16  Sentiment analysis accuracy based on a Twitter crawl using dependency heads as queries.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.9610557854175568}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9621357917785645}]}, {"text": " Table 17  Performance of the clash-based system for irony detection using gold-standard and automatic  implicit sentiment information.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.9782683551311493}]}, {"text": " Table 19  Performance of the hybrid approach to detecting irony using automatic and gold-standard  implicit sentiment information.", "labels": [], "entities": [{"text": "detecting irony", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.9215585291385651}]}]}