{"title": [{"text": "On the Derivational Entropy of Left-to-Right Probabilistic Finite-State Automata and Hidden Markov Models under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license", "labels": [], "entities": [{"text": "Derivational Entropy of Left-to-Right Probabilistic Finite-State Automata", "start_pos": 7, "end_pos": 80, "type": "TASK", "confidence": 0.7503360680171421}]}], "abstractContent": [{"text": "Probabilistic finite-state automata area formalism that is widely used in many problems of automatic speech recognition and natural language processing.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 91, "end_pos": 119, "type": "TASK", "confidence": 0.6129055420557658}, {"text": "natural language processing", "start_pos": 124, "end_pos": 151, "type": "TASK", "confidence": 0.6323688328266144}]}, {"text": "Probabilistic finite-state automata are closely related to other finite-state models as weighted finite-state automata, word lattices, and hidden Markov models.", "labels": [], "entities": []}, {"text": "Therefore, they share many similar properties and problems.", "labels": [], "entities": []}, {"text": "Entropy measures of finite-state models have been investigated in the past in order to study the information capacity of these models.", "labels": [], "entities": []}, {"text": "The derivational entropy quantifies the uncertainty that the model has about the probability distribution it represents.", "labels": [], "entities": []}, {"text": "The derivational entropy in a finite-state automaton is computed from the probability that is accumulated in all of its individual state sequences.", "labels": [], "entities": []}, {"text": "The computation of the entropy from a weighted finite-state automaton requires a normalized model.", "labels": [], "entities": []}, {"text": "This article studies an efficient computation of the derivational entropy of left-to-right probabilistic finite-state automata, and it introduces an efficient algorithm for normalizing weighted finite-state automata.", "labels": [], "entities": []}, {"text": "The efficient computation of the derivational entropy is also extended to continuous hidden Markov models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic Finite-State Automata (PFA) and hidden Markov models (HMM) are well-known formalisms that have been widely used in automatic speech recognition, machine translation, natural language processing, and, more recently, in handwritten text recognition.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 129, "end_pos": 157, "type": "TASK", "confidence": 0.651240756114324}, {"text": "machine translation", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.7771669626235962}, {"text": "handwritten text recognition", "start_pos": 232, "end_pos": 260, "type": "TASK", "confidence": 0.6364036599795023}]}, {"text": "PFA and HMM can be considered special cases of weighted finite-state automata (WFA).", "labels": [], "entities": []}, {"text": "PFA and HMM were extensively researched in and interesting probabilistic properties were demonstrated.", "labels": [], "entities": []}, {"text": "In formal language theory, automata are considered as string acceptors, but PFA maybe considered as generative processes (see Section 2.2 in Vidal et al.).", "labels": [], "entities": []}, {"text": "We have followed the point of view of in this article about this issue.", "labels": [], "entities": []}, {"text": "PFA are closely related to word lattices (WL)), which are currently a fundamental tool for many applications because WL convey most of the hypotheses produced by a decoder.", "labels": [], "entities": []}, {"text": "WL have also been used for parsing, for computing confidence measures in speech recognition (, machine translation, and handwritten text recognition) for interactive transcription and term detection.", "labels": [], "entities": [{"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9773136973381042}, {"text": "speech recognition", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7244937121868134}, {"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7395727634429932}, {"text": "handwritten text recognition", "start_pos": 120, "end_pos": 148, "type": "TASK", "confidence": 0.639949252208074}, {"text": "interactive transcription", "start_pos": 154, "end_pos": 179, "type": "TASK", "confidence": 0.6565493792295456}, {"text": "term detection", "start_pos": 184, "end_pos": 198, "type": "TASK", "confidence": 0.8552712798118591}]}, {"text": "All of these applications require the WL to be correctly defined, and therefore, knowing the stochastic properties of WL becomes very important.", "labels": [], "entities": []}, {"text": "This article deals with entropy-based measures that are computed for PFA and HMM.", "labels": [], "entities": []}, {"text": "Entropy measures give account of their expressiveness, and, therefore, the computation of measures of this type is a fundamental problem related to these models.", "labels": [], "entities": []}, {"text": "Entropy measures are fundamental to studying the uncertainty that a model has about the distribution it represents.", "labels": [], "entities": []}, {"text": "The concepts of sequence entropy (also known as sentential entropy) and derivational entropy for formal grammars were introduced in.", "labels": [], "entities": []}, {"text": "Given a finite-state automaton A, the sequence entropy 1 of the model is defined as: 2 H(A) = \u2212 w\u2208L(A) p A (w) log p A (w) where L(A) is the set of all word sequences generated by A. Note that this expression requires that the condition w\u2208L(A) p A (w) = 1.0 must be fulfilled.", "labels": [], "entities": []}, {"text": "The concept of sequence entropy of the model is different from the concept of the entropy given an observation sequence w, which is defined in Hernando, Crespi, and: p A (\u03b8|w) log p A (\u03b8|w) 1 Although the concrete formal notation used in this article will be introduced in Section 2, in this introduction we assume that the reader is familiar with some concepts related to finite-state automata theory.", "labels": [], "entities": []}, {"text": "2 Throughout this article, we assume that 0 log 0 = 0.", "labels": [], "entities": []}, {"text": "In addition, logarithm to base 2 is used in this article.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}