{"title": [{"text": "A Dependency Perspective on RST Discourse Parsing and Evaluation under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "labels": [], "entities": [{"text": "RST Discourse Parsing and Evaluation", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.9064267039299011}]}], "abstractContent": [{"text": "Computational text-level discourse analysis mostly happens within Rhetorical Structure Theory (RST), whose structures have classically been presented as constituency trees, and relies on data from the RST Discourse Treebank (RST-DT); as a result, the RST discourse parsing community has largely borrowed from the syntactic constituency parsing community.", "labels": [], "entities": [{"text": "Computational text-level discourse analysis", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.6420719102025032}, {"text": "Rhetorical Structure Theory (RST)", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.8120752076307932}, {"text": "RST Discourse Treebank (RST-DT)", "start_pos": 201, "end_pos": 232, "type": "DATASET", "confidence": 0.8220786352952322}, {"text": "RST discourse parsing", "start_pos": 251, "end_pos": 272, "type": "TASK", "confidence": 0.8837921023368835}, {"text": "syntactic constituency parsing", "start_pos": 313, "end_pos": 343, "type": "TASK", "confidence": 0.7303733030954996}]}, {"text": "The standard evaluation procedure for RST discourse parsers is thus a simplified variant of PARSEVAL, and most RST discourse parsers use techniques that originated in syntactic constituency parsing.", "labels": [], "entities": [{"text": "RST discourse parsers", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.9324464599291483}, {"text": "RST discourse parsers", "start_pos": 111, "end_pos": 132, "type": "TASK", "confidence": 0.914081335067749}, {"text": "syntactic constituency parsing", "start_pos": 167, "end_pos": 197, "type": "TASK", "confidence": 0.6483593285083771}]}, {"text": "In this article, we isolate a number of conceptual and computational problems with the constituency hypothesis.", "labels": [], "entities": []}, {"text": "We then examine the consequences, for the implementation and evaluation of RST discourse parsers, of adopting a dependency perspective on RST structures, a view advocated so far only by a few approaches to discourse parsing.", "labels": [], "entities": [{"text": "RST discourse parsers", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.9298510750134786}, {"text": "RST structures", "start_pos": 138, "end_pos": 152, "type": "TASK", "confidence": 0.9102067947387695}, {"text": "discourse parsing", "start_pos": 206, "end_pos": 223, "type": "TASK", "confidence": 0.7554970979690552}]}, {"text": "While doing that, we show the importance of the notion of headedness of RST structures.", "labels": [], "entities": [{"text": "RST structures", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.9172267913818359}]}, {"text": "We analyze RST discourse parsing as dependency parsing by adapting to RST a recent proposal in syntactic parsing that relies on head-ordered dependency trees, a representation isomorphic to headed constituency trees.", "labels": [], "entities": [{"text": "RST discourse parsing", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.9230111042658488}, {"text": "dependency parsing", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7876788973808289}, {"text": "syntactic parsing", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7405300736427307}]}, {"text": "We show how to convert the original trees from the RST corpus, RST-DT, and their binarized versions used by all existing RST parsers to head-ordered dependency trees.", "labels": [], "entities": [{"text": "RST corpus", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.7378455400466919}]}, {"text": "We also propose away to convert existing simple dependency parser output to constituent Submission Volume 44, Number 2 trees.", "labels": [], "entities": []}, {"text": "This allows us to evaluate and to compare approaches from both constituent-based and dependency-based perspectives in a unified framework, using constituency and dependency metrics.", "labels": [], "entities": []}, {"text": "We thus propose an evaluation framework to compare extant approaches easily and uniformly , something the RST parsing community has lacked up to now.", "labels": [], "entities": [{"text": "RST parsing", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.9550079703330994}]}, {"text": "We can also compare parsers' predictions to each other across frameworks.", "labels": [], "entities": []}, {"text": "This allows us to characterize families of parsing strategies across the different frameworks, in particular with respect to the notion of headedness.", "labels": [], "entities": []}, {"text": "Our experiments provide evidence for the conceptual similarities between dependency parsers and shift-reduce constituency parsers, and confirm that dependency parsing constitutes a viable approach to RST discourse parsing.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7810525894165039}, {"text": "dependency parsing", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.8080027103424072}, {"text": "RST discourse parsing", "start_pos": 200, "end_pos": 221, "type": "TASK", "confidence": 0.9433258970578512}]}], "introductionContent": [{"text": "Discourse analysis takes various forms in the NLP literature, from mostly local relations between propositions in the Penn Discourse Treebank () to structures covering a whole document, for instance, constituent structures (trees) in Rhetorical Structure Theory (RST) (, or graphs between discourse elements in Segmented Discourse Representation Theory (SDRT) or.", "labels": [], "entities": [{"text": "Discourse analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8122381269931793}, {"text": "Penn Discourse Treebank", "start_pos": 118, "end_pos": 141, "type": "DATASET", "confidence": 0.9739880760510763}, {"text": "Rhetorical Structure Theory (RST)", "start_pos": 234, "end_pos": 267, "type": "TASK", "confidence": 0.7552704215049744}, {"text": "Segmented Discourse Representation Theory (SDRT)", "start_pos": 311, "end_pos": 359, "type": "TASK", "confidence": 0.8007676260811942}]}, {"text": "In this article, we reevaluate recent efforts to predict full discourse structures at the document level within a constituency-based approach to discourse analysis.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.7122207581996918}]}, {"text": "We argue that the problem is more naturally formulated as the prediction of a dependency structure, on which simpler parsing strategies can be competitively applied.", "labels": [], "entities": []}, {"text": "We show how to evaluate both families of discourse parsers in a constituent-based and a dependency-based framework.", "labels": [], "entities": []}, {"text": "Discourse aspects are becoming more and more present in various NLP tasks.", "labels": [], "entities": []}, {"text": "Text-level structures are useful as a representation of the coherence of a text and its topical organization, with applications to summarization), sentiment analysis, or document classification (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 131, "end_pos": 144, "type": "TASK", "confidence": 0.98866206407547}, {"text": "sentiment analysis", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.9697559177875519}, {"text": "document classification", "start_pos": 170, "end_pos": 193, "type": "TASK", "confidence": 0.7481122612953186}]}, {"text": "Most empirical approaches to this problem focus on predicting structures following RST and are based on the corresponding RST-Discourse Treebank corpus (RST-DT), a large annotated resource of texts for discourse analysis in.", "labels": [], "entities": [{"text": "predicting structures following RST", "start_pos": 51, "end_pos": 86, "type": "TASK", "confidence": 0.6868789494037628}, {"text": "RST-Discourse Treebank corpus (RST-DT)", "start_pos": 122, "end_pos": 160, "type": "DATASET", "confidence": 0.8593187828858694}]}, {"text": "On the other hand, there is a need to examine in depth some of the representational choices made within the RST framework.", "labels": [], "entities": [{"text": "RST", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.9393307566642761}]}, {"text": "Many discourse parsers for RST have made simplifying assumptions with respect to the linguistic annotations for practical purposes, and these choices affect the generality of the models and their evaluation.", "labels": [], "entities": [{"text": "RST", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9697256684303284}]}, {"text": "We thus focus on the issue of predicting a structure fora text, comparing different representations over the RST-DT.", "labels": [], "entities": []}, {"text": "We will analyze the impact of the practical choices one makes while doing discourse parsing, while taking into account specificities of the discourse annotations provided in the corpus.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7358853220939636}]}, {"text": "RST is a constituency-based theory: Discourse units combine with discourse relations to form recursively larger units up to a global document unit.", "labels": [], "entities": [{"text": "RST", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8941517472267151}]}, {"text": "The linguistic descriptions and pictorial representations of these structures implicate a similarity to the constituents of phrase structure grammars.", "labels": [], "entities": []}, {"text": "However, this similarity is more apparent than real; discourse structure even in its RST format is a relational structure, formalized as a set of \"discourse constituents\" together with a set of relations, instances of which hold between the constituents.", "labels": [], "entities": []}, {"text": "All of this suggests a different approach to discourse parsing-namely, \"dependency parsing,\" which has gained some currency in the discourse parsing community ().", "labels": [], "entities": [{"text": "discourse parsing-namely", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.6969285756349564}, {"text": "dependency parsing", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7890972197055817}, {"text": "discourse parsing", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.7255343496799469}]}, {"text": "Discourse dependency parsing is analogous to syntactic dependency analysis, where units are directly related to each other without intermediate upper-level structures, to yield a directed tree structure over the discourse units of a text.", "labels": [], "entities": [{"text": "Discourse dependency parsing", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7846428751945496}, {"text": "syntactic dependency analysis", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.6906927029291788}]}, {"text": "Simple dependency structures are simplified representations of more complex SDRT graph structures (), or a simplified representation of RST structures ( ), with some applications to specific tasks ().", "labels": [], "entities": []}, {"text": "As we will show, this engenders some information loss.", "labels": [], "entities": [{"text": "information loss", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.8394849300384521}]}, {"text": "Once one takes the order of attachment into account, however, dependency representations do not in principle imply a loss of information, as established in the syntactic domain by, using the notion of headordered dependency structures.", "labels": [], "entities": []}, {"text": "Applying the head-ordered idea to discourse provides some advantages over the use of constituent structures, as head-ordered structures do not assume a higher abstraction level, a potentially contentious issue in discourse analysis ().", "labels": [], "entities": []}, {"text": "They also lend themselves to different computational models, as is also the casein syntactic parsing.", "labels": [], "entities": [{"text": "casein syntactic parsing", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.6222608387470245}]}, {"text": "We also relate this to how some work already makes use of a related notion of head constituent to inform discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.7042092233896255}]}, {"text": "Dependency structures are also more general, and allow for the different structures advocated by the various aforementioned theories: tree structures (projective or not) or graph structures.", "labels": [], "entities": []}, {"text": "They can thus provide a common representation framework between the different existing corpora.", "labels": [], "entities": []}, {"text": "Dependency parsing also comes with different evaluation measures from those used in constituent-based parsing.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8356248736381531}, {"text": "constituent-based parsing", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.6736641228199005}]}, {"text": "We will discuss in detail the relationships between evaluation measures of these two approaches.", "labels": [], "entities": []}, {"text": "Our discussion parallels those that emerged in the syntactic community about evaluations on constituents or grammatical relations, even though discourse raises specific questions, which should benefit the discourse parsing community.", "labels": [], "entities": []}, {"text": "Our article is organized as follows: In Section 2 we present the traditional view of RST structures as constituency trees and point out specificities of the RST-DT treebank.", "labels": [], "entities": [{"text": "RST structures", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.924112468957901}, {"text": "RST-DT treebank", "start_pos": 157, "end_pos": 172, "type": "DATASET", "confidence": 0.7830012738704681}]}, {"text": "Then in Section 3 we discuss the duality of discourse structures between constituencyand dependency-based representations, and present translation procedures from one to the other.", "labels": [], "entities": []}, {"text": "In Section 4, we present an extensive evaluation of existing RST discourse parsers on the test section of the RST-DT, using both constituency and dependency metrics, thus providing a uniform framework for comparing extant approaches to discourse parsing.", "labels": [], "entities": [{"text": "RST discourse parsers", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.8923628330230713}, {"text": "discourse parsing", "start_pos": 236, "end_pos": 253, "type": "TASK", "confidence": 0.7179916501045227}]}, {"text": "We also analyze empirically how diverse parsing strategies are related to each other.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present here the different evaluation measures used in the discourse parsing literature, focusing on constituent-based approaches.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7315646111965179}]}, {"text": "We will see that even within that 204 framework there are variants, which can lead to potential confusions when comparing different approaches.", "labels": [], "entities": []}, {"text": "We will consider dependency-based evaluation in Sections 3.5 and 3.6, and see how they differ in the kind of information they measure.", "labels": [], "entities": []}, {"text": "The differences partially reflect similar discussions within the syntactic community, where dependency structures have been advocated in part because of issues with constituentbased evaluation.", "labels": [], "entities": []}, {"text": "Constituency-based approaches naturally lend themselves to evaluation procedures that originated in the syntactic parsing literature.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7536352872848511}]}, {"text": "The most common evaluation procedure is, which combines precision and recall on (typed) constituents, plus a Crossing Parentheses score, which is the ratio of the number of predicted constituents that cross a gold standard constituent with respect to the total number of predicted constituents.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9994018077850342}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9986814856529236}]}, {"text": "Early work on RST parsing (Marcu 2000) introduced a modified version of Parseval to accommodate RST c-trees encoded as outlined above.", "labels": [], "entities": [{"text": "RST parsing", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9672296941280365}, {"text": "Marcu 2000)", "start_pos": 27, "end_pos": 38, "type": "DATASET", "confidence": 0.7536642154057821}]}, {"text": "This early work addressed the two tasks of segmenting elementary discourse units and parsing.", "labels": [], "entities": [{"text": "segmenting elementary discourse units", "start_pos": 43, "end_pos": 80, "type": "TASK", "confidence": 0.8720189481973648}]}, {"text": "As almost all existing work assumes gold segmentation and focuses on the parsing task itself, and because the trees are binarized, there area few simplifications to consider.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 73, "end_pos": 85, "type": "TASK", "confidence": 0.9021611213684082}]}, {"text": "First, precision and recall are equal if admissible structures are binary trees, and thus existing work only provides accuracy scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.999472439289093}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9994698166847229}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9986122846603394}]}, {"text": "Note that this is not necessarily the case for n-ary structures, since the number of predicted and reference spans can then be different.", "labels": [], "entities": []}, {"text": "On the contrary, a binary tree spanning n EDUs is always made of n \u2212 1 binary constituents, resulting from n \u2212 1 attachment decisions, thus containing a total of n + (n \u2212 1) = 2n \u2212 1 \"spans.\"", "labels": [], "entities": []}, {"text": "Counting crossing brackets is not necessary because this number is directly correlated to the accuracy on the spans, whereas in syntactic parsing this measure is necessary to mitigate the effect of missing spans (empty categories, for instance) on pure precision and recall.", "labels": [], "entities": [{"text": "Counting crossing brackets", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8513501683870951}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9991455078125}, {"text": "syntactic parsing", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.7059599757194519}, {"text": "precision", "start_pos": 253, "end_pos": 262, "type": "METRIC", "confidence": 0.9984330534934998}, {"text": "recall", "start_pos": 267, "end_pos": 273, "type": "METRIC", "confidence": 0.9956954717636108}]}, {"text": "RST parsing studies usually consider the unlabeled spans (S) as the basic building blocks to evaluate, irrespective of the discourse relation that links two discourse units to form a constituent.", "labels": [], "entities": [{"text": "RST parsing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9778823256492615}]}, {"text": "The discursive function of correctly identified spans is then evaluated, either on the sole label of the incoming relation (R) or on the assignments of nuclearities to the arguments (N) or both (F).", "labels": [], "entities": []}, {"text": "We note also that all of these evaluations provide a kind of edit distance over RST c-trees, and are hence true metrics (as is any linear combination of them).", "labels": [], "entities": []}, {"text": "Nuclearity in RST is a type of information that can provide a partial notion of directionality, as shows, for the case of mononuclear relations.", "labels": [], "entities": [{"text": "RST", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9172941446304321}]}, {"text": "For multinuclear relations there is no directionality, as the two arguments are of \"equal importance.\"", "labels": [], "entities": []}, {"text": "Constituency-based RST parsers should distinguish three types of nuclearity in their binary attachment decisions: \"NS\" if the left subtree is the nucleus of a mononuclear relation and the right subtree is the satellite, \"SN\" if the linear ordering of the satellite and nucleus is inverted, and \"NN\" for multinuclear relations.", "labels": [], "entities": [{"text": "RST parsers", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.8643553256988525}]}, {"text": "shows an example of a scoring of a constituency tree against a reference, with all four different scores (S), (N), (R), (F).", "labels": [], "entities": []}, {"text": "A consequence of assuming the gold segmentation for evaluation of existing discourse parsing methods is that accuracy on the spans should be restricted to non-EDUs.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7457930445671082}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9985989928245544}]}, {"text": "Otherwise, the span measures will be overestimated, since then basic spans (EDUs) are necessarily correct, and only (n \u2212 1) spans corresponding to binary relations on top of the EDUs are predicted; for instance, a predicted RST tree whose structure is entirely different from the reference would still have a full unlabeled span (S) score of n/(2n \u2212 1).", "labels": [], "entities": [{"text": "span (S) score", "start_pos": 324, "end_pos": 338, "type": "METRIC", "confidence": 0.6840140521526337}]}, {"text": "Excluding the EDU spans from the evaluation is not the general practice in the literature, however, because as we have (e) Scores  Having defined precisely the correspondences between constituent and dependency structures, we can now discuss how dependency structures should be evaluated.", "labels": [], "entities": [{"text": "EDU", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.8463786840438843}]}, {"text": "In syntactic parsing evaluation, dependency structures are compared straightforwardly with respect to their edges, either labeled or unlabeled.", "labels": [], "entities": [{"text": "syntactic parsing evaluation", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.7990667025248209}]}, {"text": "Unlabeled attachment score (UAS) is the proportion of correct (oriented) edges in a system prediction with respect to edges in the gold dependency structure.", "labels": [], "entities": [{"text": "Unlabeled attachment score (UAS)", "start_pos": 0, "end_pos": 32, "type": "METRIC", "confidence": 0.8193057825167974}]}, {"text": "Labeled attachment score (LAS) is the proportion of correct oriented edges with the correct relation label in the gold.", "labels": [], "entities": [{"text": "Labeled attachment score (LAS)", "start_pos": 0, "end_pos": 30, "type": "METRIC", "confidence": 0.7653709004322687}]}, {"text": "These evaluations are also types of edit distances and provide true metrics over dependency trees.", "labels": [], "entities": []}, {"text": "These two measures can be directly applied to discourse representations in the form of a labeled dependency structure between EDUs.", "labels": [], "entities": []}, {"text": "When the structure is constrained to be a tree spanning n given discourse units, the number of dependencies is necessarily n \u2212 1 both in the gold and the predicted tree structure.", "labels": [], "entities": []}, {"text": "This is not necessarily the casein discourse theories which allow for non-tree graphs, such as SDRT in the corpus used in, where precision and recall are used.", "labels": [], "entities": [{"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9986490607261658}, {"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9964892864227295}]}, {"text": "In the case of headed RST c-trees, we have seen that there are several options to produce a comparable dependency structure, though the notion of headedness is not part of the original conception of an RST constituent tree.", "labels": [], "entities": []}, {"text": "We have shown the equivalence with head-ordered dependency trees, and it is thus necessary to account for the rank of attachment of dependents to their head.", "labels": [], "entities": []}, {"text": "We do not know of any work considering this kind of structure in the discourse dependency parsing literature, as predicted dependencies approximate more complex structures: in the case of RST parsing ( ), human annotations are converted to simple dependency structures, and then compared to predicted structures with UAS and LAS, without considering attachment ranks.", "labels": [], "entities": [{"text": "discourse dependency parsing", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.6569813092549642}, {"text": "RST parsing", "start_pos": 188, "end_pos": 199, "type": "TASK", "confidence": 0.962439090013504}]}, {"text": "They also give constituent scores based on a list of correspondences between simple dependency trees and simple constituent trees, but they do not explain their conversion further, neither how they resolve the inherent ambiguity of simple dependency trees.", "labels": [], "entities": []}, {"text": "Some researchers argue that dependency metrics, even on the simple version without attachment order, area better reflection of the decisions made during the annotation process than constituency metrics, where a unique attachment decision counts several times.", "labels": [], "entities": []}, {"text": "Some attachment rankings are not semantically meaningful but are rather an artifact of the annotation process (Iruskieta, da Cunha, and Taboada 2015, page 276), especially in the case of binary trees (van der Vliet and Redeker 2011, page. 8).", "labels": [], "entities": []}, {"text": "Indeed, the \"qualitative metric\" proposed by Iruskieta, da Cunha, and Taboada (2015) seems very similar to the standard dependency metrics (UAS/LAS).", "labels": [], "entities": []}, {"text": "If we consider again, for instance, we can see that the decision to include \u03c0 3 in the scope of the explanation between \u03c0 1 and \u03c0 2 in (c) is really an attachment decision about the pair (\u03c0 2 , \u03c0 3 ), which makes the whole subtree false under constituency metrics with respect to the tree in (b), while dependency metrics will evaluate rightly the explanation between \u03c0 1 and \u03c0 2 . The binarization of trees compounds the problem by forcing artificial constructions for relations with arity of more than 2, which can be seen later in for the List relation, where the two options (left or right binarization) are semantically equivalent and a matter of convention, but can generate errors in the constituent evaluations, while a simple dependency representation would make them truly equivalent.", "labels": [], "entities": []}, {"text": "This cannot be solved by post-processing these subtrees without losing potential List substructures (something that would be lost on simple dependency trees, too).", "labels": [], "entities": []}, {"text": "Note that sublists could be represented with a head-ordered dependency tree, thus keeping the binary relation framework that is assumed by all parsing approaches.", "labels": [], "entities": []}, {"text": "All of this militates in favor of using dependency metrics, as a better reflection of the annotation decisions, and a more cautious approach to the semantics of discourse relations.", "labels": [], "entities": []}, {"text": "In this section, we turn to the empirical study of RST parsing, using both the dependency and the constituent viewpoint in a unified framework.", "labels": [], "entities": [{"text": "RST parsing", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.9805254638195038}]}, {"text": "This is made possible by the explicit conversion between viewpoints, and the awareness of the specificities of theoretical models and their practical representations.", "labels": [], "entities": []}, {"text": "We first replicated the de facto standard evaluation protocol from the literature and use as reference a right-heavy binarized version of the c-trees from the RST-DT.", "labels": [], "entities": [{"text": "RST-DT", "start_pos": 159, "end_pos": 165, "type": "DATASET", "confidence": 0.8829934000968933}]}, {"text": "In, we report the F 1 scores of the standard constituency metrics in RST-Parseval: unlabeled spans (S), spans labeled with Nuclearity (N), Relation (R), or both (F), following the abbreviations in.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9746724963188171}, {"text": "Relation (R)", "start_pos": 139, "end_pos": 151, "type": "METRIC", "confidence": 0.9263103753328323}]}, {"text": "Additionally, we report the scores for the same metrics where each span is augmented with its head EDU, denoted S+H, N+H, R+H, and F+H.", "labels": [], "entities": [{"text": "EDU", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.650649905204773}]}, {"text": "The overall level of performance on RST-Parseval is quite homogeneous among the three groups of parsers, even though two parsers stand out: FH14 gCRF on unlabeled spans (S) and spans labeled with nuclearity (N), JE14 DPLP on spans labeled with relation (R), and fully labeled spans (F).", "labels": [], "entities": [{"text": "FH14 gCRF", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9023205041885376}, {"text": "JE14", "start_pos": 212, "end_pos": 216, "type": "DATASET", "confidence": 0.49355918169021606}, {"text": "DPLP", "start_pos": 217, "end_pos": 221, "type": "METRIC", "confidence": 0.6016026139259338}]}, {"text": "Another point is that the dependency parsers (HHN16 MST, dep-chain, dep-tree) are competitive on constituency metrics with constituency parsers.", "labels": [], "entities": [{"text": "HHN16 MST", "start_pos": 46, "end_pos": 55, "type": "DATASET", "confidence": 0.9166677296161652}]}, {"text": "This is slightly surprising because these dependency parsers are trained toward a different objective (attachments between head EDUs), and we RST-Parseval F 1 scores against right-heavy binarized reference trees.", "labels": [], "entities": [{"text": "RST-Parseval F 1", "start_pos": 142, "end_pos": 158, "type": "METRIC", "confidence": 0.8873911499977112}]}, {"text": "S = Span; N = (Span and Nuclearity); R = (Span and Relation); F = (Span and Relation and Nuclearity).", "labels": [], "entities": [{"text": "Relation", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.7634603977203369}, {"text": "F", "start_pos": 62, "end_pos": 63, "type": "METRIC", "confidence": 0.9762064218521118}]}, {"text": "An asterisk (*) signals scores on predictions we reproduced using code and material made available by the authors; a double asterisk (**) signals scores on predictions provided by the author with an improved, unpublished version of the parser posterior to the original study; +h marks the use of our heuristic component to determine the nuclearity and order of attachment.", "labels": [], "entities": []}, {"text": "The best score in each group is underlined; the best score overall is in bold.", "labels": [], "entities": []}, {"text": "only used a heuristic component to predict the order of attachment between modifiers of the same head.", "labels": [], "entities": []}, {"text": "The four columns on the right in show the scores obtained with the same evaluation procedure, but where each span is augmented with the index of its head EDU.", "labels": [], "entities": [{"text": "EDU", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.8142920732498169}]}, {"text": "Although the relative performance of parsers is globally stable inside each group of parsers, the addition of the head EDU takes a slightly higher toll on the first group of parsers compared with the second and third groups.", "labels": [], "entities": [{"text": "EDU", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.7281611561775208}]}, {"text": "We also looked at constituency and dependency metrics against the original, nonbinarized c-trees from the RST-DT.", "labels": [], "entities": [{"text": "RST-DT", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.8501696586608887}]}, {"text": "provides the scores using Marcu's encoding and evaluation procedure, the scores on dependency metrics.", "labels": [], "entities": []}, {"text": "The relative performance of parsers is quite similar to that observed against right-heavy binarized c-trees.", "labels": [], "entities": []}, {"text": "The scores of greedy and chart-based parsers are stable compared to, while the shift-reduce and dependency parsers obtain lower scores.", "labels": [], "entities": []}, {"text": "The performance of parsers is more homogeneous overall.", "labels": [], "entities": []}, {"text": "The d-trees produced by the tree transformation appear to be harder for parsers than those produced by the chain transformation.", "labels": [], "entities": []}, {"text": "Note  that our chain and tree parser obtain similar scores, despite the fact that the former is trained on a different transformation of the reference trees.", "labels": [], "entities": []}, {"text": "This suggests that the higher level of accuracy attainable by the chain parser that trains on shorter dependencies compensates for the mismatch between the distributions of attachments during the training and testing stages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9990772008895874}]}], "tableCaptions": [{"text": " Table 1  RST-Parseval F 1 scores against right-heavy binarized reference trees. S = Span; N = (Span and  Nuclearity); R = (Span and Relation); F = (Span and Relation and Nuclearity). An asterisk (*)  signals scores on predictions we reproduced using code and material made available by the  authors; a double asterisk (**) signals scores on predictions provided by the author with an  improved, unpublished version of the parser posterior to the original study; +h marks the use of  our heuristic component to determine the nuclearity and order of attachment. The best score in  each group is underlined; the best score overall is in bold.", "labels": [], "entities": []}, {"text": " Table 2  Corrected constituency evaluation on labeled attachment decisions.", "labels": [], "entities": []}, {"text": " Table 3  Dependency evaluation. UAS = unlabeled dependencies; LAS-N = dependencies labeled with  nuclearity alone; LAS-R = dependencies labeled with relation alone; LAS-F = dependencies  labeled with relation and nuclearity.", "labels": [], "entities": [{"text": "UAS", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9869716763496399}, {"text": "LAS-N", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9506961107254028}]}, {"text": " Table 4  RST-Parseval metrics scores against the original (non-binarized) trees.", "labels": [], "entities": [{"text": "RST-Parseval", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.7523002028465271}]}, {"text": " Table 5  Dependency evaluation on non-binarized trees.", "labels": [], "entities": []}, {"text": " Table 6  Pairwise similarity on parser predictions: RST-Parseval S on labeled attachment decisions.  LLC16 HHN16 H SHV15 D JCN15 1 FH14 gC BPS16 BCS17 m BCS17 c JE14 HHN16 M dep-tr dep-ch gold", "labels": [], "entities": [{"text": "LLC16 HHN16 H SHV15 D JCN15 1 FH14 gC BPS16 BCS17 m BCS17 c JE14 HHN16 M dep-tr dep-ch", "start_pos": 102, "end_pos": 188, "type": "METRIC", "confidence": 0.6908855830368242}]}, {"text": " Table 7  Pairwise similarity on parser predictions: dependency metric UAS.  LLC16 HHN16 H SHV15 D JCN15 1 FH14 gC BPS16 BCS17 m BCS17 c JE14 HHN16 M dep-tr dep-ch gold", "labels": [], "entities": [{"text": "UAS.  LLC16 HHN16 H SHV15 D JCN15 1 FH14 gC BPS16 BCS17", "start_pos": 71, "end_pos": 126, "type": "METRIC", "confidence": 0.7335333526134491}]}]}