{"title": [{"text": "A Structured Review of the Validity of BLEU", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9579840302467346}]}], "abstractContent": [{"text": "The BLEU metric has been widely used in NLP for over 15 years to evaluate NLP systems, especially in machine translation and natural language generation.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9651094377040863}, {"text": "machine translation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7859779298305511}, {"text": "natural language generation", "start_pos": 125, "end_pos": 152, "type": "TASK", "confidence": 0.6420792539914449}]}, {"text": "I present a structured review of the evidence on whether BLEU is a valid evaluation technique-in other words, whether BLEU scores correlate with real-world utility and user-satisfaction of NLP systems; this review covers 284 correlations reported in 34 papers.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9896215200424194}, {"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9947299957275391}]}, {"text": "Overall, the evidence supports using BLEU for diagnostic evaluation of MT systems (which is what it was originally proposed for), but does not support using BLEU outside of MT, for evaluation of individual texts, or for scientific hypothesis testing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9965624213218689}, {"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.940759003162384}, {"text": "BLEU", "start_pos": 157, "end_pos": 161, "type": "METRIC", "confidence": 0.9838677644729614}, {"text": "MT", "start_pos": 173, "end_pos": 175, "type": "TASK", "confidence": 0.889411211013794}]}], "introductionContent": [{"text": "BLEU () is a metric that is widely used to evaluate Natural Language Processing (NLP) systems which produce language, especially machine translation (MT) and Natural Language Generation (NLG) systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.98846834897995}, {"text": "machine translation (MT) and Natural Language Generation (NLG)", "start_pos": 129, "end_pos": 191, "type": "TASK", "confidence": 0.80158864458402}]}, {"text": "Because BLEU itself just computes word-based overlap with a gold-standard reference text, its use as an evaluation metric depends on an assumption that it correlates with and predicts the real-world utility of these systems, measured either extrinsically (e.g., by task performance) or by user satisfaction.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9640880227088928}]}, {"text": "From this perspective, it is similar to surrogate endpoints in clinical medicine, such as evaluating an AIDS medication by its impact on viral load rather than by explicitly assessing whether it leads to longer or higher-quality life.", "labels": [], "entities": []}, {"text": "Hence the usage of BLEU to evaluate NLP systems is only sensible in the presence of validation studies which show that BLEU scores correlate with direct evaluations of the utility of NLP systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9962922930717468}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9981847405433655}]}, {"text": "In rough terms, a validation study involves evaluating a number of NLP systems (or individual output texts) using both a metric such as BLEU and a gold standard human evaluation, and then calculating how well the metric correlates with the gold-standard human evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9947589039802551}]}, {"text": "Many such studies have been published, and in this paper I present a structured review of these studies.", "labels": [], "entities": []}, {"text": "Structured reviews are literature reviews that are designed to be objective, repeatable, and comprehensive.", "labels": [], "entities": []}, {"text": "In other words, whereas the author of a normal literature review uses their knowledge of the field to identify key papers and qualitatively summarize their findings, the author of a structured review identifies relevant papers via objective search criteria and extracts from each paper key metric validity pitfalls evaluation", "labels": [], "entities": []}], "datasetContent": [{"text": "Surrogate endpoints such as BLEU are useful if they can reliably predict an outcome that is of real-world importance or is the core of a scientific hypothesis we wish to test.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9975385665893555}]}, {"text": "Ina medical context, validation studies are expected to correlate the surrogate endpoint against direct measurements of the outcome of interest.", "labels": [], "entities": []}, {"text": "In NLP, human evaluations can be based on human ratings or rankings (intrinsic) or on measurement of an outcome such as task performance (extrinsic); they can also be carried out in laboratory or real-world contexts.", "labels": [], "entities": []}, {"text": "The strongest and most meaningful evaluation is a real-world outcome-based evaluation, where a system is operationally deployed and we measure its impact on real-world outcomes.", "labels": [], "entities": []}, {"text": "From this perspective, it is striking that few of the surveyed papers looked at task/outcome measures.", "labels": [], "entities": []}, {"text": "Indeed, only one paper correlated system-level BLEU scores with task outcomes, and all such correlations in that paper were Low or Negative.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9524314999580383}]}, {"text": "None of the surveyed papers used real-world human evaluations; that is, they all used human evaluations performed in an artificial context (usually by paid individuals, crowdsourced workers, or the researchers themselves), rather than looking at the impact of systems on real-world users.", "labels": [], "entities": []}, {"text": "The most common way to measure real-world effectiveness in computing is with A/B testing, where different real-world users of a service are given access to different systems.", "labels": [], "entities": [{"text": "A/B testing", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.8657160103321075}]}, {"text": "A/B testing is most often used to measure user satisfaction, but it could also be used to measure extrinsic outcomes such as post-edit time in an MT context.", "labels": [], "entities": [{"text": "A/B testing", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.8198600262403488}, {"text": "MT context", "start_pos": 146, "end_pos": 156, "type": "TASK", "confidence": 0.886872798204422}]}, {"text": "I suspect that many commercial providers of online NLP services have carried out a considerable amount of A/B testing.", "labels": [], "entities": [{"text": "A/B", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.8718371987342834}]}, {"text": "I realize that the results of such tests are commercially confidential, but if it were possible for such providers to publish correlations between their A/B tests and BLEU, that would be very helpful in assessing the validity of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9871215224266052}, {"text": "validity", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.9496824741363525}, {"text": "BLEU", "start_pos": 229, "end_pos": 233, "type": "METRIC", "confidence": 0.8857830166816711}]}, {"text": "I am not suggesting that academic researchers evaluate systems using task/ outcome-based real-world A/B testing-this is clearly not feasible.", "labels": [], "entities": []}, {"text": "What I am saying is that the results of real-world A/B testing could be used to determine contexts in which BLEU reliably had good correlation with real-world effectiveness.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9946112632751465}]}, {"text": "Researchers could then confidently use BLEU as a surrogate endpoint in these contexts.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9925058484077454}]}], "tableCaptions": [{"text": " Table 1  Correlation of BLEU with ranking-based human evaluation reported in WMT events, for  German-English and English-German MT in a news domain.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9987603425979614}, {"text": "WMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.5959649085998535}]}]}