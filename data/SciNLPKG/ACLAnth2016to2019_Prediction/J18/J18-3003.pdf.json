{"title": [{"text": "Native Language Identification With Classifier Stacking and Ensembles", "labels": [], "entities": [{"text": "Native Language Identification", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.580058773358663}]}], "abstractContent": [{"text": "Ensemble methods using multiple classifiers have proven to be among the most successful approaches for the task of Native Language Identification (NLI), achieving the current state of the art.", "labels": [], "entities": [{"text": "Native Language Identification (NLI)", "start_pos": 115, "end_pos": 151, "type": "TASK", "confidence": 0.8278220991293589}]}, {"text": "However, a systematic examination of ensemble methods for NLI has yet to be conducted.", "labels": [], "entities": []}, {"text": "Additionally, deeper ensemble architectures such as classifier stacking have not been closely evaluated.", "labels": [], "entities": [{"text": "classifier stacking", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8681665062904358}]}, {"text": "We present a set of experiments using three ensemble-based models, testing each with multiple configurations and algorithms.", "labels": [], "entities": []}, {"text": "This includes a rigorous application of meta-classification models for NLI, achieving state-of-the-art results on several large data sets, evaluated in both intra-corpus and cross-corpus modes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Native Language Identification (NLI) is the task of identifying a writer's native language (L1) based only on their writings in a second language (the L2).", "labels": [], "entities": [{"text": "Native Language Identification (NLI)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7796178509791692}]}, {"text": "NLI works by identifying language use patterns that are common to groups of speakers of the same native language.", "labels": [], "entities": []}, {"text": "This process is underpinned by the presupposition that an author's L1 disposes them towards certain language production patterns in their L2, as influenced by their mother tongue.", "labels": [], "entities": []}, {"text": "This relates to cross-linguistic influence, a key topic in the field of Second Language Acquisition (SLA), which analyzes transfer effects from the L1 on later learned languages.", "labels": [], "entities": [{"text": "Second Language Acquisition (SLA)", "start_pos": 72, "end_pos": 105, "type": "TASK", "confidence": 0.8268290360768636}]}, {"text": "It has been noted in the linguistics literature since the 1950s that speakers of particular languages have characteristic production patterns when writing in a second language.", "labels": [], "entities": []}, {"text": "This language transfer phenomenon has been investigated independently in various fields from different perspectives, including qualitative research in SLA and recently via predictive models in NLP (.", "labels": [], "entities": [{"text": "language transfer", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.7090695202350616}]}, {"text": "Recently, this has motivated studies in NLI, a subtype of text classification where the goal is to determine the native language of an author, using texts that they have written in a second language).", "labels": [], "entities": [{"text": "text classification", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7361680716276169}]}, {"text": "The motivations for NLI are manifold.", "labels": [], "entities": []}, {"text": "Such techniques can help SLA researchers identify important L1-specific learning and teaching issues.", "labels": [], "entities": [{"text": "SLA", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9572792053222656}]}, {"text": "In turn, the identification of such issues can enable researchers to develop pedagogical material that takes into consideration a learner's L1 and addresses them.", "labels": [], "entities": []}, {"text": "It can also be applied in a forensic context, for example, to glean information about the discriminant L1 cues in an anonymous text.", "labels": [], "entities": []}, {"text": "NLI is most commonly framed as a multiclass supervised classification task.", "labels": [], "entities": [{"text": "multiclass supervised classification task", "start_pos": 33, "end_pos": 74, "type": "TASK", "confidence": 0.6823316216468811}]}, {"text": "Researchers have experimented with a range of machine learning algorithms, with support vector machines (SVMs) having found the most success.", "labels": [], "entities": []}, {"text": "However, some of the most successful approaches have made use of classifier ensemble methods to further improve performance on this task.", "labels": [], "entities": []}, {"text": "This is a trend that has become apparent in recent work on this task, as we will outline in Section 2.", "labels": [], "entities": []}, {"text": "In fact, all recent state-of-the-art systems have relied on some form of multiple classifier system.", "labels": [], "entities": []}, {"text": "However, a thorough examination of ensemble methods for NLI-one empirically comparing different architectures and algorithms-has yet to be conducted.", "labels": [], "entities": []}, {"text": "Additionally, more sophisticated ensemble architectures, such as classifier stacking, have not been closely evaluated.", "labels": [], "entities": [{"text": "classifier stacking", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8727885484695435}]}, {"text": "This meta-classification approach is an advanced method that has proven to be effective in many classification tasks; its systematic application could improve the state of the art in NLI.", "labels": [], "entities": []}, {"text": "This has links to the idea of adding layers to increase power in neural networkbased deep learning, which has come to bean important approach in NLP over the last few years (Manning 2015); note that \"Overwhelming empirical evidence as well as intuition indicates that having depth in the neural network is indeed important.\"", "labels": [], "entities": []}, {"text": "Deep neural networks can in fact be seen as layered classifiers, and ensemble methods as an alternative way of adding power via additional layers.", "labels": [], "entities": []}, {"text": "In this article we look just at ensemble methods: Deep learning has not yet produced state-of-the-art results on related tasks (), and our goal is to understand what it is that has made ensemble methods to date in NLI so successful.", "labels": [], "entities": []}, {"text": "The primary focus of the present work is to address this gap by presenting a comprehensive and rigorous examination of how ensemble methods can be applied for NLI.", "labels": [], "entities": []}, {"text": "We aim to examine several different ensemble and meta-classification architectures, each of which can utilize different configurations and algorithms.", "labels": [], "entities": []}, {"text": "Furthermore, previous ensemble methods have not been tested on different data sets, making the ability of these models to generalize for NLI unclear.", "labels": [], "entities": []}, {"text": "Ideally, the same method should be tested across multiple corpora to assess its validity.", "labels": [], "entities": []}, {"text": "To this end, we also apply our methods to multiple data sets, in both English and other languages, to evaluate their generalization capacity.", "labels": [], "entities": []}, {"text": "In addition, following the observation of that patterns utilized by machine learners can be corpus-specific and influenced by topic, even with apparently topic-neutral features-something that could also be true in our multi-level classifier context-we carryout cross-corpus experiments along the lines of, , and Ionescu,.", "labels": [], "entities": []}, {"text": "To summarize, the principal aims of the present study are the following: 1.", "labels": [], "entities": []}, {"text": "Apply several advanced ensemble combination methods to NLI and evaluate their performance against previously used ensemble methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this study we use a supervised multiclass classification approach.", "labels": [], "entities": [{"text": "multiclass classification", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.6940064877271652}]}, {"text": "The learner texts are organized into classes according to the author's L1 and these documents are used for training and testing in our experiments.", "labels": [], "entities": []}, {"text": "In this section we describe our experimental methodology, including evaluation and the algorithms we use.", "labels": [], "entities": []}, {"text": "Our classification features will be described in Section 5; and the three classification models we create using these algorithms and features are then described in Section 6.", "labels": [], "entities": []}, {"text": "In the same manner as many previous NLI studies and also the NLI 2013 shared task, we report our results as classification accuracy under k-fold cross-validation, with k = 10.", "labels": [], "entities": [{"text": "NLI 2013 shared task", "start_pos": 61, "end_pos": 81, "type": "DATASET", "confidence": 0.8530787229537964}, {"text": "classification", "start_pos": 108, "end_pos": 122, "type": "TASK", "confidence": 0.9403213858604431}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.91948401927948}]}, {"text": "In recent years this has become a de facto standard for reporting NLI results.", "labels": [], "entities": [{"text": "reporting NLI", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.7173943817615509}]}, {"text": "For creating our folds, we used stratified cross-validation, which aims to ensure that the proportion of classes within each partition is equal).", "labels": [], "entities": []}, {"text": "For TOEFL11 we also test on the standard test set, which we call TOEFL11-TEST.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9289535880088806}, {"text": "standard test set", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.7700757384300232}, {"text": "TOEFL11-TEST", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.9534803032875061}]}, {"text": "We use three baselines to provide lower bounds for accuracy: r Random-uniform random over classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9990543723106384}]}, {"text": "r Majority Class (where this differs from Random).", "labels": [], "entities": []}, {"text": "r Single Vector-a linear SVM (Section 4.3) with a single feature vector consisting of all features used in the data set, as a direct comparator for using an ensemble vs. not using one (as per Tetreault et al.).", "labels": [], "entities": []}, {"text": "We use additional baselines depending on the corpus, where comparable results by other systems are available.", "labels": [], "entities": []}, {"text": "Oracles, which we describe in Section 6.1.8, will also be used to estimate a potential upper bound for accuracy.", "labels": [], "entities": [{"text": "Section 6.1.8", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.8626419305801392}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9968147873878479}]}, {"text": "We use them to assess how close our models are to achieving optimal performance.", "labels": [], "entities": []}, {"text": "Additionally, we also compare our results against those reported in previous work.", "labels": [], "entities": []}, {"text": "These were described earlier in Section 2.", "labels": [], "entities": []}, {"text": "We divide our experiments into three parts: comprehensive experiments on the English TOEFL11 data (Section 7.1); comparative experiments using the EFCAMDAT data set, in both intra-corpus and cross-corpus modes (Section 7.2); and experiments on our non-English L2 corpora (Section 7.3).", "labels": [], "entities": [{"text": "English TOEFL11 data", "start_pos": 77, "end_pos": 97, "type": "DATASET", "confidence": 0.7570577065149943}, {"text": "EFCAMDAT data set", "start_pos": 147, "end_pos": 164, "type": "DATASET", "confidence": 0.9666886528333029}]}, {"text": "Details of system configurations and parameter settings are available in Appendix A.  As additional baselines for the TOEFL11 experiments, beyond the ones common to all experiments described in Section 4.1, we also compare our ensemble results against the winning system from the 2013 NLI shared task (Jarvis, Bestgen, and Pepper 2013) and two systems by and, which presented state-of-the-art results following the task.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 118, "end_pos": 125, "type": "DATASET", "confidence": 0.7418268322944641}, {"text": "NLI shared task (Jarvis, Bestgen, and Pepper 2013)", "start_pos": 285, "end_pos": 335, "type": "DATASET", "confidence": 0.6178735196590424}]}, {"text": "They were all previously described in Section 2.", "labels": [], "entities": []}, {"text": "As described in Section 6, we create our ensembles from a set of models where each is trained on a different feature type.", "labels": [], "entities": []}, {"text": "We first test our individual classifiers that form this first layer of our three models; this can inform us about their individual performance and the single best feature type.", "labels": [], "entities": []}, {"text": "This is done by training the models on the combined TOEFL11-TRAIN and TOEFL11-DEV data, which we refer to as TOEFL11-TRAINDEV, and testing against the TOEFL11-TEST set.", "labels": [], "entities": [{"text": "TOEFL11-TRAIN", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.898633599281311}, {"text": "TOEFL11-DEV data", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.8740330934524536}, {"text": "TOEFL11-TRAINDEV", "start_pos": 109, "end_pos": 125, "type": "DATASET", "confidence": 0.8822628259658813}, {"text": "TOEFL11-TEST set", "start_pos": 151, "end_pos": 167, "type": "DATASET", "confidence": 0.9176352620124817}]}, {"text": "As noted in Section 4.3, we considered linear SVMs and logistic regression for these base classifiers.", "labels": [], "entities": []}, {"text": "presents the results of these.", "labels": [], "entities": []}, {"text": "SVMs are clearly the best in this evaluation; there are only two instances (for POS bigrams, with PTB or RASP tags) where they are beaten by the logistic regression classifier with L2 regularization, and only by small amounts (1%).", "labels": [], "entities": []}, {"text": "We therefore use linear SVMs as our base classifiers in the following experimental work.", "labels": [], "entities": []}, {"text": "These SVM results are shown graphically in.", "labels": [], "entities": []}, {"text": "We observe that there is a range of performance, and some features achieve similar accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9978572726249695}]}, {"text": "We also note that the best single-model performance is approximately 78%.", "labels": [], "entities": []}, {"text": "Having shown that our base classifiers achieve good results on their own, we now apply our three ensemble models.", "labels": [], "entities": []}, {"text": "We begin by applying the six ensemble combination methods discussed in Section 6.1 as part of our first model.", "labels": [], "entities": []}, {"text": "We do this using both cross-validation within TOEFL11-TRAINDEV and by training our models on TOEFL11-TRAINDEV and testing on TOEFL11-TEST.", "labels": [], "entities": [{"text": "TOEFL11-TRAINDEV", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.968755304813385}, {"text": "TOEFL11-TRAINDEV", "start_pos": 93, "end_pos": 109, "type": "DATASET", "confidence": 0.9556683301925659}, {"text": "TOEFL11-TEST", "start_pos": 125, "end_pos": 137, "type": "DATASET", "confidence": 0.9781447052955627}]}, {"text": "The results for all fusion methods are shown in.", "labels": [], "entities": []}, {"text": "The mean probability combiner which uses continuous values for each class (Section 6.1.3) achieves the best performance for both of our test sets.", "labels": [], "entities": [{"text": "mean probability combiner", "start_pos": 4, "end_pos": 29, "type": "METRIC", "confidence": 0.7295820911725363}]}, {"text": "This is followed by the plurality vote and median probability combiners, both of which have similar performance.", "labels": [], "entities": [{"text": "median probability combiners", "start_pos": 43, "end_pos": 71, "type": "METRIC", "confidence": 0.8219163417816162}]}, {"text": "Although plurality voting achieves good results, the Borda Count method performs worse.", "labels": [], "entities": [{"text": "Borda Count", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9569362998008728}]}, {"text": "In our cross-validation experiments, some 2.7% of the instances resulted in voting ties that were broken arbitrarily.", "labels": [], "entities": []}, {"text": "This randomness leads to some variance in the results for voting-based fusion methods; running the combiner on the same input can produce slightly different results.", "labels": [], "entities": []}, {"text": "Results from the highest confidence and product rule combiners are the poorest among the set, and by a substantial margin.", "labels": [], "entities": []}, {"text": "We hypothesize that this is because they are both highly sensitive to outputs from all classifiers.", "labels": [], "entities": []}, {"text": "A single outlier or poor prediction can adversely affect the results.", "labels": [], "entities": []}, {"text": "They should generally be used in circumstances where the base classifiers are known to be extremely accurate, which is not the case here.", "labels": [], "entities": []}, {"text": "Accordingly, we do not experiment with these any further.", "labels": [], "entities": []}, {"text": "These results from this first model comport with previous research reporting that ensembles outperform single-vector approaches (see Section 2); our best ensemble result is some 5% higher than our best feature, and 4-6% higher than the single vector baseline, the direct comparator for using the same features without an ensemble.", "labels": [], "entities": []}, {"text": "We next apply our meta-classifier (Section 6.2) to both the discrete and continuous outputs generated by the base classifiers.", "labels": [], "entities": []}, {"text": "Although the base classifiers remain the same, we train a meta-classifier, using each of the machine learning algorithms we listed earlier in Section 4.3.", "labels": [], "entities": []}, {"text": "This results in 15 meta-classification models.", "labels": [], "entities": []}, {"text": "Each model is tested using both discrete and continuous input, using both cross-validation and the TOEFL11-TEST set.", "labels": [], "entities": [{"text": "TOEFL11-TEST set", "start_pos": 99, "end_pos": 115, "type": "DATASET", "confidence": 0.6515173316001892}]}, {"text": "The results for all of these experiments are shown in.", "labels": [], "entities": []}, {"text": "Broadly, we observe two important trends here: The meta-classification results are substantially better than the ensemble combination methods from, and the metaclassifiers trained on continuous output perform better than their discrete label counterparts.", "labels": [], "entities": []}, {"text": "This last pattern is not all that surprising because we already observed that the probability-based ensemble combiners outperformed the voting-based combiners.", "labels": [], "entities": []}, {"text": "Using continuous values associated with each label provides the meta-learner with more information than a single label, likely helping it make better decisions.", "labels": [], "entities": []}, {"text": "Although most of our algorithms perform well, the LDA meta-classifier yields the best results across both input types and test conditions.", "labels": [], "entities": []}, {"text": "These results, 85.2% under Results from our 15 meta-classifiers applied to TOEFL11, using both discrete and continuous outputs from the base classifiers.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.9146682024002075}]}, {"text": "The best result in each column is in bold.", "labels": [], "entities": []}, {"text": "The results can be compared with the baselines in. cross-validation and 86.8% on TOEFL11-TEST, are already higher than the current state of the art on this data and well exceed the baselines listed in.", "labels": [], "entities": [{"text": "TOEFL11-TEST", "start_pos": 81, "end_pos": 93, "type": "DATASET", "confidence": 0.8420038223266602}]}, {"text": "It is also important to note that the same classifier achieves the best performance across all four testing conditions.", "labels": [], "entities": []}, {"text": "The linear and RBF SVMs also achieve competitive results here.", "labels": [], "entities": [{"text": "RBF SVMs", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.6423832178115845}]}, {"text": "Instance-based k-NN and Nearest Centroid classifiers also do well.", "labels": [], "entities": []}, {"text": "On the other hand, decision trees, QDA, and the Perceptron algorithm have the poorest performance across both discrete and continuous inputs.", "labels": [], "entities": []}, {"text": "The second set of our experiments focuses on investigating the extent to which our findings so far generalize to another corpus.", "labels": [], "entities": []}, {"text": "The result patterns observed on TOEFL11 have been stable across the training and test set, but we now apply them to another large data set, both corpus-internally and in a cross-corpus setting.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.8261957764625549}]}, {"text": "We present results for the top performing models tested in Section 7.1: four ensemble combiners, four bagging-based meta-classifiers, and four ensembles of metaclassifiers.", "labels": [], "entities": []}, {"text": "The results for the models trained and tested on EFCAMDAT are in those trained on TOEFL11 and tested on EFCAMDAT in; and those trained on TOEFL11 and tested on EFCAMDAT in.", "labels": [], "entities": [{"text": "EFCAMDAT", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.9243519902229309}, {"text": "TOEFL11", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.9602682590484619}, {"text": "EFCAMDAT", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.9242587089538574}, {"text": "TOEFL11", "start_pos": 138, "end_pos": 145, "type": "DATASET", "confidence": 0.9683069586753845}, {"text": "EFCAMDAT", "start_pos": 160, "end_pos": 168, "type": "DATASET", "confidence": 0.9416307806968689}]}, {"text": "The baselines (except Single Vector) and oracle scores in all three tables are reproduced from .   The primary point to note is that all results, both intra-corpus and cross-corpus, follow the same pattern as the results on TOEFL11 in Section 7.1.", "labels": [], "entities": [{"text": "TOEFL11 in Section 7.1", "start_pos": 224, "end_pos": 246, "type": "DATASET", "confidence": 0.8692725151777267}]}, {"text": "The Current Best Result baseline (Malmasi and Dras 2015) was an ensemble using mean probability combination, which is reproduced in the Ensembles row; the other ensemble models produce slightly lower scores, as in TOEFL11.", "labels": [], "entities": [{"text": "Current Best Result baseline (Malmasi and Dras 2015)", "start_pos": 4, "end_pos": 56, "type": "DATASET", "confidence": 0.7936259895563126}, {"text": "TOEFL11", "start_pos": 214, "end_pos": 221, "type": "DATASET", "confidence": 0.9263857007026672}]}, {"text": "Also for TOEFL11, all ensembles are better than the Single Vector baseline (although only marginally in the case of plurality voting).", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.8596499562263489}]}, {"text": "The meta-classifier again provides a strong improvement over the plain ensembles, smaller in the intra-corpus results (2.3%, comparing the best ensemble against the best meta-classifier in) and more substantial in the cross-corpus case (3.9% for and a much larger 9.8% for, where this is in the context of lower absolute accuracies.", "labels": [], "entities": []}, {"text": "In all three cases, the ensemble of meta-classifiers yields additional improvement over the single meta-classifier model.", "labels": [], "entities": []}, {"text": "This gap is a smaller one than seen between plain ensembles and single meta-classifiers, but is still a consistent one that leads to our best results, which as with TOEFL11 occur with LDA-based meta-classification.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.727979838848114}]}, {"text": "Again, the largest improvements are seen in the cross-corpus case: The ultimate difference between the Best Current Result baseline and our best result in the intra-corpus case is 2.8%, compared with 4.0% and 10.2%.", "labels": [], "entities": []}, {"text": "The final set of our experiments looks at another kind of generalizability.", "labels": [], "entities": []}, {"text": "The result patterns observed on TOEFL11 have also held in a cross-corpus analysis with EFCAMDAT; we now look at non-English L2s.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.8821606636047363}, {"text": "EFCAMDAT", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.9457126259803772}]}, {"text": "The experiments in this section are conducted on the Chinese and Norwegian data sets, described in Section 3.", "labels": [], "entities": [{"text": "Norwegian data sets", "start_pos": 65, "end_pos": 84, "type": "DATASET", "confidence": 0.8695839444796244}]}, {"text": "As these data sets do not have a predefined test set like TOEFL11, these experiments were performed using stratified cross-validation, as discussed in Section 4.1.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.8793595433235168}]}, {"text": "Previous experiments on these corpora have also been conducted using cross-validation only.", "labels": [], "entities": []}, {"text": "We again utilize the top performing models tested in Section 7.1: four ensemble combiners, four bagging-based meta-classifiers, and four ensembles of meta-classifiers.", "labels": [], "entities": []}, {"text": "These selected models, and their results, are listed in along with baselines for both raw and generated data sets for each language.", "labels": [], "entities": []}, {"text": "Only a generated data set Current Best baseline is available for each language.", "labels": [], "entities": [{"text": "Current Best baseline", "start_pos": 26, "end_pos": 47, "type": "DATASET", "confidence": 0.7267689406871796}]}, {"text": "29 do carryout experiments on raw documents from the Norwegian ASK corpus, and reach accuracies up to 68%.", "labels": [], "entities": [{"text": "Norwegian ASK corpus", "start_pos": 53, "end_pos": 73, "type": "DATASET", "confidence": 0.9409694075584412}, {"text": "accuracies", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9976537823677063}]}, {"text": "However, their set-up is rather different, following Pepper (2012).", "labels": [], "entities": [{"text": "Pepper (2012)", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9232668578624725}]}, {"text": "They work with seven L1s, and results are over varying subsets of five L1s.", "labels": [], "entities": []}, {"text": "Their string kernel approach improves by around 15% over the LDA baseline of, which is greater than the improvement we see for our best ensemble model over our SVM baseline, but this is not directly comparable to the numbers in.", "labels": [], "entities": [{"text": "LDA baseline", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.7929563224315643}]}, {"text": "Immediately apparent is that the magnitude of results differs between raw and generated data sets, by around 20%: Prediction is easier on the generated set.", "labels": [], "entities": [{"text": "Prediction", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.9329831004142761}]}, {"text": "This maybe connected to greater length consistency (so that all documents are of reasonable length) outweighing the extra difficulty of topic and proficiency clues being removed by the homogenization process.", "labels": [], "entities": [{"text": "consistency", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.6426672339439392}]}, {"text": "Abstracting away from that and focusing on the effect of ensembles, the patterns are the same across both L2s and raw and generated data sets.", "labels": [], "entities": []}, {"text": "The oracle values are quite high for the generated data sets at over 90%, similar to TOEFL11 (which was listed in).", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.8273021578788757}]}, {"text": "As with all other results in these tables, the oracles for the raw data sets are much lower.", "labels": [], "entities": []}, {"text": "The ensemble model does well, beating the previously reported best result for Chinese and coming close for Norwegian (generated data set).", "labels": [], "entities": [{"text": "Norwegian (generated data set", "start_pos": 107, "end_pos": 136, "type": "DATASET", "confidence": 0.8556285858154297}]}, {"text": "Just like our previous experiments, the mean probability combiner yields the best performance.", "labels": [], "entities": [{"text": "mean probability combiner", "start_pos": 40, "end_pos": 65, "type": "METRIC", "confidence": 0.8308786551157633}]}, {"text": "A little differently from the English L2 data sets, not all ensembles perform better than the Single Vector baseline for Norwegian.", "labels": [], "entities": [{"text": "English L2 data sets", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.7213353291153908}]}, {"text": "The meta-classifier model achieves anew state of the art for both (generated) data sets, just as it did for TOEFL11 and EFCAMDAT.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.8670759797096252}]}, {"text": "Also consistent with the previous experiment, LDA achieves the best results for all data sets.", "labels": [], "entities": [{"text": "LDA", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.6129981875419617}]}, {"text": "Finally, the ensemble of meta-classifiers yields additional improvement over the single meta-classifier model, achieving our best results.", "labels": [], "entities": []}, {"text": "All meta-classifier models and all ensembles of metaclassifiers perform better than the Single Vector baseline.", "labels": [], "entities": []}, {"text": "In terms of the generated data sets, we achieve best accuracies of 76.5% on the Chinese data and 81.8% on the Norwegian data, both substantial improvements over previous work.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9948692321777344}, {"text": "Chinese data", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.7294418662786484}, {"text": "Norwegian data", "start_pos": 110, "end_pos": 124, "type": "DATASET", "confidence": 0.9417336285114288}]}, {"text": "These results show that these classification models are applicable to data sets over other L2s besides English.", "labels": [], "entities": []}, {"text": "The results followed the same pattern across all data sets, with LDA-based meta-classification yielding top results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  The 10 L1 classes included in the Norwegian NLI data set and the numbers of raw documents  and documents we generated for each class.", "labels": [], "entities": [{"text": "Norwegian NLI data set", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.9686394035816193}]}, {"text": " Table 2. In addition to these we also generate 250 control texts written by native", "labels": [], "entities": []}, {"text": " Table 3  The 11 L1 classes included in the Chinese NLI data set, and the numbers of raw documents and  documents we generated for each class.", "labels": [], "entities": [{"text": "Chinese NLI data set", "start_pos": 44, "end_pos": 64, "type": "DATASET", "confidence": 0.9421909004449844}]}, {"text": " Table 6  Comparing as potential base classifiers SVMs and Logistic Regression with both L1 and L2  regularization. The classifiers were trained on TOEFL11-TRAINDEV data and the accuracy in the  table is evaluated on the TOEFL11-TEST set.", "labels": [], "entities": [{"text": "TOEFL11-TRAINDEV data", "start_pos": 148, "end_pos": 169, "type": "DATASET", "confidence": 0.9418197274208069}, {"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9992042183876038}, {"text": "TOEFL11-TEST set", "start_pos": 221, "end_pos": 237, "type": "DATASET", "confidence": 0.9306343197822571}]}, {"text": " Table 7  Comparing different ensemble classifiers against our baselines and oracles. The CV column lists  cross-validation within the TOEFL11-TRAINDEV data and the Test column is the TOEFL11-TEST  set. Best ensemble result per column represented in bold.", "labels": [], "entities": [{"text": "TOEFL11-TRAINDEV data", "start_pos": 135, "end_pos": 156, "type": "DATASET", "confidence": 0.9466888010501862}, {"text": "TOEFL11-TEST  set", "start_pos": 184, "end_pos": 201, "type": "DATASET", "confidence": 0.9101643860340118}]}, {"text": " Table 8  Results from our 15 meta-classifiers applied to TOEFL11, using both discrete and continuous  outputs from the base classifiers. The best result in each column is in bold. The results can be  compared with the baselines in", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.9305788278579712}]}, {"text": " Table 7. It is also important  to note that the same classifier achieves the best performance across all four testing  conditions.  The linear and RBF SVMs also achieve competitive results here. Instance-based  k-NN and Nearest Centroid classifiers also do well. On the other hand, decision trees,  QDA, and the Perceptron algorithm have the poorest performance across both discrete  and continuous inputs.", "labels": [], "entities": [{"text": "RBF", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.860274076461792}]}, {"text": " Table 9  Results for our ensembles of meta-classifiers applied to TOEFL11. Best result per column in bold,  best result per row grouping is underlined.", "labels": [], "entities": [{"text": "TOEFL11", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.8880810737609863}]}, {"text": " Table 10  Per-class performance breakdown of our top system's results on the TOEFL11-TEST set. Best  result per column in bold. Best and worst performances per column are also highlighted in  green/yellow.", "labels": [], "entities": [{"text": "TOEFL11-TEST set", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.9586370289325714}]}, {"text": " Table 12  Results for our three models on the EFCAMDAT data set, using continuous classifier outputs.  Best result per column in bold, best result per row grouping is underlined.", "labels": [], "entities": [{"text": "EFCAMDAT data set", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.9839788675308228}]}, {"text": " Table 13  Results for our three models trained on the TOEFL11 data set and tested on EFCAMDAT, using  continuous classifier outputs. Best result per column in bold, best result per row grouping is  underlined.", "labels": [], "entities": [{"text": "TOEFL11 data set", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.9835760593414307}, {"text": "EFCAMDAT", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.9141517877578735}]}, {"text": " Table 14  Results for our three models trained on the EFCAMDAT data set and tested on TOEFL11, using  continuous classifier outputs. Best result per column in bold, best result per row grouping is  underlined.", "labels": [], "entities": [{"text": "EFCAMDAT data set", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.9810493389765421}, {"text": "TOEFL11", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.953010082244873}]}, {"text": " Table 15  Results for our three models on the Chinese data sets (one consisting of raw documents and the  other of generated documents), using continuous classifier outputs. Best result per column in  bold, best result per row grouping is underlined.", "labels": [], "entities": [{"text": "Chinese data sets", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.8896777033805847}]}, {"text": " Table 16  Results for our three models on the Norwegian data sets (one consisting of raw documents and  the other of generated documents), using continuous classifier outputs. Best result per column in  bold, best result per row grouping is underlined.", "labels": [], "entities": [{"text": "Norwegian data sets", "start_pos": 47, "end_pos": 66, "type": "DATASET", "confidence": 0.9652479290962219}]}]}