{"title": [], "abstractContent": [{"text": "Orthographic similarities across languages provide a strong signal for unsupervised probabilistic transduction (decipherment) for closely related language pairs.", "labels": [], "entities": []}, {"text": "The existing decipherment models , however, are not well suited for exploiting these orthographic similarities.", "labels": [], "entities": []}, {"text": "We propose a log-linear model with latent variables that incorporates orthographic similarity features.", "labels": [], "entities": []}, {"text": "Maximum likelihood training is computationally expensive for the proposed log-linear model.", "labels": [], "entities": []}, {"text": "To address this challenge, we perform approximate inference via Markov chain Monte Carlo sampling and contrastive divergence.", "labels": [], "entities": [{"text": "approximate inference", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.8240056931972504}]}, {"text": "Our results show that the proposed log-linear model with contrastive divergence outperforms the existing generative decipherment models by exploiting the ortho-graphic features.", "labels": [], "entities": []}, {"text": "The model both scales to large vocabularies and preserves accuracy in low-and no-resource contexts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9989320635795593}]}], "introductionContent": [{"text": "Word-level translation models are typically learned by applying statistical word alignment algorithms on large-scale bilingual parallel corpora ().", "labels": [], "entities": [{"text": "Word-level translation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7016153782606125}, {"text": "word alignment", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.7342540919780731}]}, {"text": "Building a parallel corpus, however, is expensive and time-consuming.", "labels": [], "entities": []}, {"text": "As a result, parallel data are limited or even unavailable for many language pairs.", "labels": [], "entities": []}, {"text": "In the absence of a sufficient amount of parallel data, the accuracy of standard word alignment algorithms drops significantly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9995461106300354}, {"text": "word alignment", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.752270370721817}]}, {"text": "This is also true of supervised neural methods: Even with hundreds of thousands of parallel training sentences, neural methods only achieve modest results (.", "labels": [], "entities": []}, {"text": "Low-and no-resource languages generally do not have parallel corpora, and even their monolingual corpora tend to be small.", "labels": [], "entities": []}, {"text": "However, these monolingual corpora can often be downloaded from the Internet, and are much easier to obtain or produce than parallel corpora.", "labels": [], "entities": []}, {"text": "Leveraging useful information from monolingual corpora can be extremely helpful for learning translation models for low-and no-resource language pairs.", "labels": [], "entities": []}, {"text": "Decipherment algorithms (so-called because of the assumption that one language is a cipher for the other) aim to exploit such monolingual corpora in order to learn translation model parameters, when parallel data are limited or unavailable (.", "labels": [], "entities": []}, {"text": "The key intuition is that similar words and n-grams tend to have similar distributional properties across languages.", "labels": [], "entities": []}, {"text": "For example, if a bigram appears frequently in the monolingual source corpus, its translation is likely to appear frequently in the monolingual target corpus, and vice versa.", "labels": [], "entities": []}, {"text": "This is especially true when the corpora share similar topics and context.", "labels": [], "entities": []}, {"text": "Furthermore, for many such language pairs, we observe similar monotonic word ordering-that is, the translation of a bigram is often the same as the concatenation of the translations of individual unigrams (consider the shared use of postnominal adjectives in the French maison bleu and Spanish casa azul).", "labels": [], "entities": []}, {"text": "Although this certainly is not always true, we assume that it is common enough to provide a useful signal.", "labels": [], "entities": []}, {"text": "The goal of decipherment algorithms is to leverage such statistical similarities across languages, and effectively learn word-level translation probabilities from monolingual data.", "labels": [], "entities": []}, {"text": "Existing decipherment methods are predominantly based on probabilistic generative models).", "labels": [], "entities": []}, {"text": "These models primarily focus on the statistical similarities between the n-gram frequencies in the source and the target language, and rely on the expectation maximization (EM) algorithm or its faster approximations.", "labels": [], "entities": []}, {"text": "However, there can be many other types of statistical and linguistic similarities across languages beyond n-gram frequencies (similarities in spelling, word-length distribution, syntactic structure, etc.).", "labels": [], "entities": []}, {"text": "Unfortunately, existing generative models do not allow incorporating such a wide range of linguistically motivated features.", "labels": [], "entities": []}, {"text": "Previous research has shown the effectiveness of incorporating linguistically motivated features for many different unsupervised learning tasks, such as unsupervised part-of-speech induction), word alignment (, and grammar induction).", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 166, "end_pos": 190, "type": "TASK", "confidence": 0.7085416465997696}, {"text": "word alignment", "start_pos": 193, "end_pos": 207, "type": "TASK", "confidence": 0.822110652923584}, {"text": "grammar induction", "start_pos": 215, "end_pos": 232, "type": "TASK", "confidence": 0.7360329478979111}]}, {"text": "Many pairs of related languages share vocabulary or grammatical structure due to borrowing or inheritance: the English aquatic and Spanish agua share the Latin root aqua, and the English beige was borrowed from French.", "labels": [], "entities": []}, {"text": "As a result, orthographic features provide crucial information for determining word-level translations for closely related language pairs.", "labels": [], "entities": [{"text": "word-level translations", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.668153390288353}]}, {"text": "Church (1993) leveraged orthographic similarity for character alignment.", "labels": [], "entities": [{"text": "character alignment", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8513283431529999}]}, {"text": "Haghighi, proposed a generative model for inducing a bilingual lexicon from monolingual text by exploiting orthographic and contextual similarities among the words in two different languages.", "labels": [], "entities": []}, {"text": "The model proposed by Haghighi et al. learns a one-to-one mapping between the words in two languages by analyzing type-level features only, while ignoring the token-level n-gram frequencies.", "labels": [], "entities": []}, {"text": "We propose a decipherment model that unifies the type-level feature-based approach of Haghighi et al. with token-level EM-based approaches such as and.", "labels": [], "entities": []}, {"text": "In addition to orthographic similarity, we also often observe similarity in the distribution of word lengths across different languages.", "labels": [], "entities": []}, {"text": "Linguists have long noted the relationship between word frequency and length, so the tendency of words and their translations to have similar frequencies) may apply to length as well.", "labels": [], "entities": []}, {"text": "Our feature-rich log-linear model can easily incorporate such length-based similarity features.", "labels": [], "entities": []}, {"text": "One of the key challenges with the proposed latent variable log-linear model is the high computational complexity of training, as it requires normalizing globally via summing overall possible observations and latent variables.", "labels": [], "entities": []}, {"text": "As a result, an exact implementation is impractical even for the moderate vocabulary size of most lowresource languages.", "labels": [], "entities": []}, {"text": "To address this challenge, we perform approximate inference using Markov chain Monte Carlo (MCMC) sampling for scalable training of the log-linear decipherment models.", "labels": [], "entities": []}, {"text": "We present a series of increasingly scalable approximations, each most suitable fora different amount of available data.", "labels": [], "entities": []}, {"text": "They are applicable in contexts ranging from no-resource languages (such as \"lost\" languages, a context considered by Snyder, Barzilay, and Knight) to languages with a modest amount of data that is still insufficient for state-of-the-art unsupervised methods based on word embeddings.", "labels": [], "entities": []}, {"text": "The main contributions of this article are as follows.", "labels": [], "entities": []}, {"text": "r We propose a feature-based decipherment model for low-and no-resource languages that combines both type-level orthographic features and token-level distributional similarities.", "labels": [], "entities": []}, {"text": "Our proposed model outperforms the existing EM-based decipherment models.", "labels": [], "entities": []}, {"text": "r We extend the contrastive divergence method to sample entire sentences, rather than bigram pairs, allowing more context to be used in reconstructing latent translations.", "labels": [], "entities": []}, {"text": "r Finally, we extend the model to exploit parallel as well as monolingual data, for situations in which limited amounts of parallel data maybe available.", "labels": [], "entities": []}, {"text": "The remainder of the article is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce the general problem formulation for monolingual decipherment, and present our notations.", "labels": [], "entities": []}, {"text": "We discuss the background literature on different decipherment models for machine translation in Section 3.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.79842808842659}]}, {"text": "Section 4 describes the proposed feature-based decipherment model.", "labels": [], "entities": []}, {"text": "A detailed discussion of MCMC sampling-based approximations follows in Section 5.", "labels": [], "entities": []}, {"text": "We extend the fully monolingual model to exploit parallel data in Section 6.", "labels": [], "entities": []}, {"text": "Our orthographic features are described in Section 7.", "labels": [], "entities": []}, {"text": "Finally, we present our detailed results in Section 8 and conclude with our findings and discuss our future work in Section 9.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the accuracy of decipherment by the percentage of source words that are mapped to the correct target translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9993641972541809}]}, {"text": "We find the maximum-probability mapping for all source words; precision could be increased at the expense of recall by imposing some threshold, below which no mapping would be made fora given source word.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9995362758636475}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9980986714363098}]}, {"text": "The correct translation for each source word was determined automatically using the Google Translation API.", "labels": [], "entities": [{"text": "Google Translation API", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.7017826835314432}]}, {"text": "Although the Google Translation API did a fair job of translating the French and Spanish words to English, it returned only a single target translation.", "labels": [], "entities": []}, {"text": "We noticed occasional cases where the decipherment algorithm retrieved a correct translation, but it did not get credit because of not matching the translation from the API.", "labels": [], "entities": []}, {"text": "Additionally, we performed Viterbi decoding on the sentences in a small held-out test corpus from the OPUS data set, and compared the BLEU scores with the previously published results on the same test set as and.", "labels": [], "entities": [{"text": "OPUS data set", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9849704106648763}, {"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9991177916526794}]}, {"text": "Our training set, however, was different: Their data were parallel, so we split the data set into two disjoint sections, one for each language.", "labels": [], "entities": []}, {"text": "This reduced our model's performance (as expected), but we still achieve a higher BLEU score than the baselines.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.986990749835968}]}], "tableCaptions": [{"text": " Table 3  Statistics on the data sets used in our experiments.", "labels": [], "entities": []}, {"text": " Table 4  The running time per iteration and accuracy of decipherment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9996147155761719}]}, {"text": " Table 5  The effect on accuracy of incorporating parallel data for our model (first three columns) and  IBM Model 1 and Model 4 (on Hansard-1000 parallel data).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.99935382604599}, {"text": "Hansard-1000 parallel data", "start_pos": 133, "end_pos": 159, "type": "DATASET", "confidence": 0.8580067356427511}]}, {"text": " Table 6  Comparison of MT performance on the OPUS data set.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9887596368789673}, {"text": "OPUS data set", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.9736228783925375}]}]}