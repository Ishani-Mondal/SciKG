{"title": [], "abstractContent": [{"text": "Computational models for sarcasm detection have often relied on the content of utterances in isolation.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.9430851638317108}]}, {"text": "However, the speaker's sarcastic intent is not always apparent without additional context.", "labels": [], "entities": []}, {"text": "Focusing on social media discussions, we investigate three issues: (1) does modeling conversation context help in sarcasm detection?", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.9678294062614441}]}, {"text": "(2) can we identify what part of conversation context triggered the sarcastic reply? and (3) given a sarcastic post that contains multiple sentences, can we identify the specific sentence that is sarcastic?", "labels": [], "entities": []}, {"text": "To address the first issue, we investigate several types of Long Short-Term Memory (LSTM) networks that can model both the conversation context and the current turn.", "labels": [], "entities": []}, {"text": "We show that LSTM networks with sentence-level attention on context and current turn, as well as the conditional LSTM network, outperform the LSTM model that reads only the current turn.", "labels": [], "entities": []}, {"text": "As conversation context, we consider the prior turn, the succeeding turn, or both.", "labels": [], "entities": []}, {"text": "Our computational models are tested on two types of social media platforms: Twitter and discussion forums.", "labels": [], "entities": []}, {"text": "We discuss several differences between these data sets, ranging from their size to the nature of the gold-label annotations.", "labels": [], "entities": []}, {"text": "To address the latter two issues, we present a qualitative analysis of the attention weights produced by the LSTM models (with attention) and discuss the results compared with human performance on the two tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social media has stimulated the production of user-generated content that contains figurative language use such as sarcasm and irony.", "labels": [], "entities": []}, {"text": "Recognizing sarcasm and verbal irony is critical for understanding people's actual sentiments and beliefs.", "labels": [], "entities": []}, {"text": "For instance, the utterance \"I love waiting at the doctor's office for hours . .", "labels": [], "entities": []}, {"text": "\" is ironic, expressing a negative sentiment toward the situation of \"waiting for hours at the doctor's office,\" even if the speaker uses positive sentiment words such as \"love.\"", "labels": [], "entities": []}, {"text": "Verbal irony and sarcasm area type of interactional phenomenon with specific perlocutionary effects on the hearer, such as to break their pattern of expectation.", "labels": [], "entities": [{"text": "Verbal irony", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.6884447038173676}]}, {"text": "For the current report, we do not make a clear distinction between sarcasm and verbal irony.", "labels": [], "entities": []}, {"text": "Most computational models for sarcasm detection have considered utterances in isolation;).", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.9549269676208496}]}, {"text": "In many instances, however, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation ().", "labels": [], "entities": []}, {"text": "Thus, to detect the speaker's sarcastic intent, it is necessary (even if maybe not sufficient) to consider their utterance(s) in the larger conversation context.", "labels": [], "entities": []}, {"text": "Consider the Twitter conversation example in.", "labels": [], "entities": []}, {"text": "Without the context of userA's statement, the sarcastic intent of userB's response might not be detected.", "labels": [], "entities": []}, {"text": "In this article, we investigate the role of conversation context for the detection of sarcasm in social media discussions (Twitter conversations and discussion forums).", "labels": [], "entities": [{"text": "detection of sarcasm in social media discussions", "start_pos": 73, "end_pos": 121, "type": "TASK", "confidence": 0.8214914969035557}]}, {"text": "The unit of analysis (i.e., what we label as sarcastic or not sarcastic) is a message/turn in asocial media conversation (i.e., a tweet in Twitter or a post/comment in discussion forums).", "labels": [], "entities": []}, {"text": "We call this unit current turn (C TURN).", "labels": [], "entities": [{"text": "C TURN)", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.7403523127237955}]}, {"text": "The conversation context that we consider is the prior turn (P TURN), and, when available, also the succeeding turn (S TURN), which is the reply to the current turn.", "labels": [], "entities": [{"text": "P TURN)", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.8403269449869791}, {"text": "succeeding turn (S TURN)", "start_pos": 100, "end_pos": 124, "type": "METRIC", "confidence": 0.6858291923999786}]}, {"text": "shows some examples of sarcastic messages (C TURNs), together with their respective prior turns (P TURN) taken from Twitter and two discussion forum corpora: the Internet Argument Corpus (IAC v2 ) () and Reddit.", "labels": [], "entities": [{"text": "P TURN)", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9502589106559753}, {"text": "Internet Argument Corpus (IAC v2 )", "start_pos": 162, "end_pos": 196, "type": "DATASET", "confidence": 0.7553671385560717}]}, {"text": "shows examples from the IAC v2 corpus of sarcastic messages (C TURNs; userB's post) and the conversation context given by the prior turn (P TURN; userA's post) as well as the succeeding turn (S TURN; userC's post).", "labels": [], "entities": [{"text": "IAC v2 corpus", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.8795471986134847}]}, {"text": "We address three specific questions: 1.", "labels": [], "entities": []}, {"text": "Does modeling of conversation context help in sarcasm detection?", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.9702282249927521}]}], "datasetContent": [{"text": "To answer the first research question-does modeling of conversation context help in sarcasm detection-we consider two binary classification tasks.", "labels": [], "entities": [{"text": "sarcasm detection-we", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.986006498336792}]}, {"text": "We refer to sarcastic instances as Sand non-sarcastic instances as NS.", "labels": [], "entities": []}, {"text": "The first task is to predict whether the current turn (C TURN abbreviated as ct) is sarcastic or not, considering it in isolation-S ct vs. NS ct task.", "labels": [], "entities": []}, {"text": "The second task is to predict whether the current turn is sarcastic or not, considering both the current turn and its conversation context given by the prior turn (P TURN, abbreviated as pt), succeeding turn (S TURN, abbreviated as st), or both-S ct+context vs. NS ct+context task, where context is pt, st, or pt+st.", "labels": [], "entities": []}, {"text": "For all the corpora introduced in Section 3-IAC v2 , IAC + v2 , Reddit, and Twitter-we conduct S ct vs. NS ct and S ct+pt vs. NS ct+pt classification tasks.", "labels": [], "entities": [{"text": "IAC + v2", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.8528518279393514}, {"text": "NS ct+pt classification tasks", "start_pos": 126, "end_pos": 155, "type": "TASK", "confidence": 0.6581156601508459}]}, {"text": "For IAC + v2 we also perform experiments considering the succeeding turn st as conversation context (i.e., S ct+st vs. NS ct+st and S ct+pt+st vs. NS ct+pt+st ).", "labels": [], "entities": [{"text": "IAC + v2", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.5634644031524658}]}, {"text": "We experiment with two types of computational models: (1) support vector machines (SVM) (Cortes and Vapnik 1995; Chang and Lin 2011) with linguistically motivated discrete features (used as one baseline; disc bl ) and with tf-idf representations of the n-grams (used as another baseline; tf-idf bl ), and (2) approaches using distributed representations.", "labels": [], "entities": []}, {"text": "For the latter, we use the LSTM networks, which have been shown to be successful in various NLP tasks, such as constituency parsing (), language modeling (Zaremba, Sutskever, and Vinyals 2014), machine translation (Sutskever, Vinyals, and Le 2014), and textual entailment ().", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 111, "end_pos": 131, "type": "TASK", "confidence": 0.8409991264343262}, {"text": "language modeling", "start_pos": 136, "end_pos": 153, "type": "TASK", "confidence": 0.7667511403560638}, {"text": "machine translation", "start_pos": 194, "end_pos": 213, "type": "TASK", "confidence": 0.7987011671066284}, {"text": "textual entailment", "start_pos": 253, "end_pos": 271, "type": "TASK", "confidence": 0.7361241579055786}]}, {"text": "We present these models in the next sections.", "labels": [], "entities": []}, {"text": "We designed an Amazon Mechanical Turk task (for brevity, MTurk) as follows: given a pair of a sarcastic current turn (C TURN) and its prior turn (P TURN), we ask Turkers to identify one or more sentences in P TURN that they think triggered the sarcastic reply.", "labels": [], "entities": []}, {"text": "Turkers could select one or more sentences from the conversation context P TURN, including the entire turn.", "labels": [], "entities": [{"text": "TURN", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9332246780395508}]}, {"text": "We selected all sarcastic examples from the IAC v2 test set where the prior turn contain between three and seven sentences, because longer turns might be a more complex task for the Turkers.", "labels": [], "entities": [{"text": "IAC v2 test set", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.8456474542617798}]}, {"text": "This selection resulted in 85 pairs.", "labels": [], "entities": []}, {"text": "We provided several definitions of sarcasm to capture all characteristics.", "labels": [], "entities": []}, {"text": "The first definition is inspired by the Standard Pragmatic Model, which identifies verbal irony or sarcasm as a speech or form of writing that means the opposite of what it seems to say.", "labels": [], "entities": []}, {"text": "In another definition, taken from, we mentioned that sarcasm often is used with the intention to mock or insult someone or to be funny.", "labels": [], "entities": []}, {"text": "We provided a couple of examples of sarcasm from the IAC v2 data set to show how to successfully complete the task (See Appendix A for the instructions given the the Turkers).", "labels": [], "entities": [{"text": "IAC v2 data set", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.9275728464126587}]}, {"text": "Each HIT contains only one pair of C TURN and P TURN and five Turkers were allowed to attempt each HIT.", "labels": [], "entities": []}, {"text": "Turkers with reasonable quality (i.e., more than 95% of acceptance rate with experience of over 8,000 HITs) were selected and paid $0.07 per task.", "labels": [], "entities": []}, {"text": "Because Turkers were asked to select one or multiple sentences from the prior turn, standard interannotator agreement (IAA) metrics are not applicable.", "labels": [], "entities": []}, {"text": "Instead, we look at two aspects to understand the user annotations.", "labels": [], "entities": []}, {"text": "First, we look at the distribution of the triggers (i.e., sentences that trigger the sarcastic reply) selected by the five annotators).", "labels": [], "entities": []}, {"text": "It can be seen that in 3% of instances all five annotators selected the exact same trigger(s), while in 58% of instances 3 or 4 different selections were made per posts.", "labels": [], "entities": []}, {"text": "Second, we looked at the distribution of the number of sentences in the P TURN that were selected as triggers by Turkers.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 113, "end_pos": 120, "type": "DATASET", "confidence": 0.8356940150260925}]}, {"text": "We notice that 43% of the time three sentences were selected.", "labels": [], "entities": []}, {"text": "The second study is an extension of the first study.", "labels": [], "entities": []}, {"text": "Given a pair of a sarcastic turn C TURN and its prior turn P TURN, we ask the Turkers to perform two subtasks.", "labels": [], "entities": []}, {"text": "First, they were asked to identify \"only one\" sentence from C TURN that expresses the speaker's sarcastic intent.", "labels": [], "entities": [{"text": "TURN", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.6331686973571777}]}, {"text": "Next, based on the selected sarcastic sentence, they were asked to identify one or more sentences in P TURN that may trigger that sarcastic sentence (similar to the Crowdsourcing Experiment 1).", "labels": [], "entities": []}, {"text": "We selected examples both from the IAC v2 corpus (60 pairs) as well as the Reddit corpus (100 pairs).", "labels": [], "entities": [{"text": "IAC v2 corpus", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.8735940853754679}, {"text": "Reddit corpus", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.9605317115783691}]}, {"text": "Each of the P TURN and C TURN contains three to seven sentences (note that the examples from the IAC v2 corpus area subset of the ones used in the previous experiment).", "labels": [], "entities": [{"text": "IAC v2 corpus area subset", "start_pos": 97, "end_pos": 122, "type": "DATASET", "confidence": 0.8538004517555237}]}, {"text": "We replicate the same design as the previous MTurk (i.e., we included definitions of sarcasm, provided examples of the task, used only one pair of C TURN and P TURN per HIT, required the same qualification for the Turkers, and paid the same payment of $0.07 per HIT; see Appendix A for the instructions given to Turkers).", "labels": [], "entities": []}, {"text": "Each HIT was done by five Turkers (a total of 160 HITs).", "labels": [], "entities": [{"text": "HIT", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.8693825602531433}, {"text": "Turkers", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.7088946104049683}]}, {"text": "To measure the IAA between the Turkers for the first subtask (i.e., identifying a particular sentence from C TURN that expresses the speaker's sarcastic intent) we used Krippendorf's \u03b1 (Krippendorff 2012).", "labels": [], "entities": [{"text": "IAA", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9981531500816345}]}, {"text": "We measure IAA on nominal data (i.e., each sentence is treated as a separate category).", "labels": [], "entities": [{"text": "IAA", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9424945712089539}]}, {"text": "Because the number of sentences (i.e., categories) can vary between three and seven, we report separate \u03b1 scores based on the number of sentences.", "labels": [], "entities": [{"text": "\u03b1 scores", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9519680142402649}]}, {"text": "For C TURN that contains three, four, five, or more than five sentences, the \u03b1 scores are 0.66, 0.71, 0.65, 0.72, respectively.", "labels": [], "entities": []}, {"text": "The \u03b1 scores are modest and illustrate (a) identifying sarcastic sentences from a discussion forum post is a hard task and (b) it is plausible that the current turn (C TURN) contains multiple sarcastic sentences.", "labels": [], "entities": [{"text": "identifying sarcastic sentences from a discussion forum post", "start_pos": 43, "end_pos": 103, "type": "TASK", "confidence": 0.7761870250105858}, {"text": "TURN", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.6826790571212769}]}, {"text": "For the second subtask, we carried a similar analysis as for Experiment 1, and results are shown in both for the IAC v2 and Reddit data.", "labels": [], "entities": [{"text": "IAC v2 and Reddit data", "start_pos": 113, "end_pos": 135, "type": "DATASET", "confidence": 0.7975048542022705}]}, {"text": "Identify what triggers a sarcastic reply.", "labels": [], "entities": []}, {"text": "Sarcasm is a speech or form of writing that means the opposite of what it seems to say.", "labels": [], "entities": []}, {"text": "Sarcasm is usually intended to mock or insult someone or to be funny.", "labels": [], "entities": [{"text": "Sarcasm is", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9553464353084564}]}, {"text": "People participating in social media platforms, such as discussion forums, are often sarcastic.", "labels": [], "entities": []}, {"text": "In this experiment, a pair of posts (previous post and sarcastic reply) from an online discussion forum is presented to you.", "labels": [], "entities": []}, {"text": "The sarcastic reply is a response to the previous post.", "labels": [], "entities": []}, {"text": "However, given that these posts may contain more than one sentence, often sarcasm in the sarcastic reply is triggered by only one or just a few of the sentences from the previous post.", "labels": [], "entities": []}, {"text": "Your task will be to identify the sentence/sentences from the previous post that triggers the sarcasm in the sarcastic reply.", "labels": [], "entities": []}, {"text": "Consider the following pair of posts (sentence numbers are in \"()\").", "labels": [], "entities": []}, {"text": "\u2022 DESCRIPTION OF THE TASK.", "labels": [], "entities": [{"text": "DESCRIPTION OF THE TASK", "start_pos": 2, "end_pos": 25, "type": "METRIC", "confidence": 0.7382953017950058}]}, {"text": "Given such a pair of online posts, your task is to identify the sentences from the previous post that trigger sarcasm in the sarcastic reply.", "labels": [], "entities": []}, {"text": "You only need to select the sentence numbers from the previous post (do not retype the sentences).", "labels": [], "entities": []}, {"text": "At the same time, UserB is sarcastic on the previous post from UserA and the sarcasm is triggered by sentence 2 (\"Caribbean, poverty and crime was near nil . .", "labels": [], "entities": [{"text": "UserA", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.924156665802002}]}, {"text": "\") and sentence 3 (\"and everyone was self-sufficient . .", "labels": [], "entities": []}, {"text": "\") and not the other sentences in the post.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4  Data sets description (number of instances in Train/Dev/Test).", "labels": [], "entities": []}, {"text": " Table 5  Average words/post and sentences/post from the three corpora.", "labels": [], "entities": []}, {"text": " Table 6  Experimental results for the discussion forum data set (IAC v2 ) (bold are best scores).", "labels": [], "entities": [{"text": "discussion forum data set (IAC v2 )", "start_pos": 39, "end_pos": 74, "type": "DATASET", "confidence": 0.7082815393805504}]}, {"text": " Table 7  Experimental results for Twitter data set (bold are best scores).", "labels": [], "entities": [{"text": "Twitter data set", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.9481353561083475}]}, {"text": " Table 8  Experimental results for Reddit data set (bold are best scores).", "labels": [], "entities": [{"text": "Reddit data set", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.955646832784017}]}, {"text": " Table 9  Experimental results for training on the Reddit data set and testing on IAC v2 using the best LSTM  models (sentence-level attention).", "labels": [], "entities": [{"text": "Reddit data set", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.9449705878893534}]}, {"text": " Table 10  Experimental results for the Reddit data set under the unbalanced setting.", "labels": [], "entities": [{"text": "Reddit data set", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.9486624002456665}]}]}