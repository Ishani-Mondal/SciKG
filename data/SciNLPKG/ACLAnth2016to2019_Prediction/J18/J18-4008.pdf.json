{"title": [{"text": "A Joint Model of Conversational Discourse and Latent Topics on Microblogs under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "labels": [], "entities": []}], "abstractContent": [{"text": "Conventional topic models are ineffective for topic extraction from microblog messages, because the data sparseness exhibited in short messages lacking structure and contexts results in poor message-level word co-occurrence patterns.", "labels": [], "entities": [{"text": "topic extraction from microblog messages", "start_pos": 46, "end_pos": 86, "type": "TASK", "confidence": 0.859285545349121}]}, {"text": "To address this issue, we organize microblog messages as conversation trees based on their reposting and replying relations, and propose an unsupervised model that jointly learns word distributions to represent: (1) different roles of conversational discourse, and (2) various latent topics in reflecting content information.", "labels": [], "entities": []}, {"text": "By explicitly distinguishing the probabilities of messages with varying discourse roles in containing topical words, our model is able to discover clusters of discourse words that are indicative of topical content.", "labels": [], "entities": []}, {"text": "In an automatic evaluation on large-scale microblog corpora, our joint model yields topics with better coherence scores than competitive topic models from previous studies.", "labels": [], "entities": []}, {"text": "* Jing Li is the corresponding author.", "labels": [], "entities": []}, {"text": "This work was partially conducted when Jing Li was Qualitative analysis on model outputs indicates that our model induces meaningful representations for both discourse and topics.", "labels": [], "entities": [{"text": "Qualitative", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.8976469039916992}]}, {"text": "We further present an empirical study on microblog summarization based on the outputs of our joint model.", "labels": [], "entities": [{"text": "microblog summarization", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7416300177574158}]}, {"text": "The results show that the jointly modeled discourse and topic representations can effectively indicate summary-worthy content in microblog conversations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the past two decades, the Internet has been revolutionizing the way we communicate.", "labels": [], "entities": []}, {"text": "Microblogging, asocial networking channel over the Internet, further accelerates communication and information exchange.", "labels": [], "entities": [{"text": "information exchange", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.7636596858501434}]}, {"text": "Popular microblog platforms, such as Twitter 1 and Sina Weibo, have become important outlets for individuals to share information and voice opinions, which further benefit downstream applications such as instant detection of breaking events (), real-time and ad hoc search of microblog messages (, public opinions and user behavior understanding on societal issues, and so forth.", "labels": [], "entities": [{"text": "Sina Weibo", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.8434215188026428}, {"text": "instant detection of breaking events", "start_pos": 204, "end_pos": 240, "type": "TASK", "confidence": 0.7953154861927032}]}, {"text": "However, the explosive growth of microblog data far outpaces human beings' speed of reading and understanding.", "labels": [], "entities": []}, {"text": "As a consequence, there is a pressing need for effective natural language processing (NLP) systems that can automatically identify gist information, and make sense of the unmanageable amount of user-generated social media content.", "labels": [], "entities": []}, {"text": "As one of the important and fundamental text analytic approaches, topic models extract key components embedded in microblog content by clustering words that describe similar semantic meanings to form latent \"topics.\"", "labels": [], "entities": []}, {"text": "The derived intermediate topic representations have proven beneficial to many NLP applications for social media, such as summarization (Harabagiu and Hickl 2011), classification), and recommendation on microblogs ().", "labels": [], "entities": [{"text": "classification", "start_pos": 163, "end_pos": 177, "type": "TASK", "confidence": 0.9676014184951782}]}, {"text": "Conventionally, probabilistic topic models (e.g., probabilistic latent semantic analysis and latent Dirichlet allocation ]) have achieved huge success over the past decade, owing to their fully unsupervised manner and ease of extension.", "labels": [], "entities": [{"text": "probabilistic latent semantic analysis", "start_pos": 50, "end_pos": 88, "type": "TASK", "confidence": 0.6320711597800255}]}, {"text": "The semantic structure discovered by these topic models have facilitated the progress of many research fields, for example, information retrieval (Boyd-Graber, Hu, and Mimno 2017), data mining (), and NLP ().", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 124, "end_pos": 145, "type": "TASK", "confidence": 0.8429827094078064}, {"text": "data mining", "start_pos": 181, "end_pos": 192, "type": "TASK", "confidence": 0.864793449640274}]}, {"text": "Nevertheless, ascribing to their reliance on document-level word co-occurrence patterns, the progress is still limited to formal conventional documents such as news reports and scientific articles (Rosen-).", "labels": [], "entities": []}, {"text": "The aforementioned models work poorly when directly applied to short and colloquial texts (e.g., microblog posts) owing to severe sparsity exhibited in such text genre (.", "labels": [], "entities": []}, {"text": "Previous research has proposed several methods to deal with the sparsity issue in short texts.", "labels": [], "entities": []}, {"text": "One common approach is to aggregate short messages into long pseudodocuments.", "labels": [], "entities": []}, {"text": "Many studies heuristically aggregate messages based on authorship), shared words (), or hashtags).", "labels": [], "entities": []}, {"text": "propose self-aggregation-based topic modeling (SATM) that aggregates texts jointly with topic inference.", "labels": [], "entities": [{"text": "self-aggregation-based topic modeling (SATM)", "start_pos": 8, "end_pos": 52, "type": "TASK", "confidence": 0.7626316994428635}]}, {"text": "Another popular solution is to take into account word relations to alleviate document-level word sparseness.", "labels": [], "entities": []}, {"text": "Biterm topic model (BTM) directly models the generation of word-pair co-occurrence patterns in each individual message ().", "labels": [], "entities": []}, {"text": "More recently, word embeddings trained by large-scale external data are leveraged to capture word relations and improve topic models on short texts.", "labels": [], "entities": []}, {"text": "To date, most efforts focus on content in messages, but ignore the rich discourse structure embedded in ubiquitous user interactions on microblog platforms.", "labels": [], "entities": []}, {"text": "On microblogs, which were originally built for user communication and interaction, conversations are freely formed on issues of interests by reposting messages and replying to others.", "labels": [], "entities": []}, {"text": "When joining a conversation, users generally post topically related content, which naturally provide effective contextual information for topic discovery.", "labels": [], "entities": [{"text": "topic discovery", "start_pos": 138, "end_pos": 153, "type": "TASK", "confidence": 0.813374936580658}]}, {"text": "have shown that simply aggregating messages based on conversations can significantly boost the performance of conventional topic models and outperform models exploiting hashtag-based and user-based aggregations.", "labels": [], "entities": []}, {"text": "Another important issue ignored inmost previous studies is the effective separation of topical words from non-topic ones ().", "labels": [], "entities": []}, {"text": "In microblog content, owing to its colloquial nature, non-topic words such as sentimental (e.g., \"great\" and \"ToT\"), functional (e.g., \"doubt\" and \"why\"), and other non-topic words (e.g., \"oh\" and \"oops\") are common and usually mixed with topical words.", "labels": [], "entities": []}, {"text": "The occurrence of non-topic words may distract the models from recognizing topical content, which thus leads to the failure to produce coherent and meaningful topics.", "labels": [], "entities": []}, {"text": "In this article, we propose a novel model that examines the entire context of a conversation and jointly explores word distributions representing varying types of topical content and discourse roles such as agreement, question-asking, argument, and other dialogue acts.", "labels": [], "entities": []}, {"text": "Though Ritter, Cherry, and Dolan (2010) separate discourse, topic, and other words for modeling conversations, their model focuses on dialogue act modeling and only yields one distribution for topical content.", "labels": [], "entities": [{"text": "dialogue act modeling", "start_pos": 134, "end_pos": 155, "type": "TASK", "confidence": 0.6751696268717448}]}, {"text": "Therefore, their model is unable to distinguish varying latent topics reflecting message content underlying the corpus.", "labels": [], "entities": []}, {"text": "leverage conversational discourse structure to detect topical words from microblog posts, which explicitly explores the probabilities of different discourse roles that contain topical words.", "labels": [], "entities": []}, {"text": "However, depend on a pre-trained discourse tagger and acquire a time-consuming and expensive manual annotation process for annotating conversational discourse roles on microblog messages, which does not scale for large data sets.", "labels": [], "entities": []}, {"text": "To exploit discourse structure of microblog conversations, we link microblog posts using reposting and replying relations to build conversation trees.", "labels": [], "entities": []}, {"text": "Particularly, the root of a conversation tree refers to the original post and its edges represent the reposting or replying relations.", "labels": [], "entities": []}, {"text": "To illustrate the interplay between topic and discourse, displays a snippet of Twitter conversation about \"Trump administration's immigration ban.\"", "labels": [], "entities": [{"text": "Trump administration's immigration ban", "start_pos": 107, "end_pos": 145, "type": "TASK", "confidence": 0.45766273736953733}]}, {"text": "From the conversation, we can observe two major components: (1) discourse,", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents an experiment on the coherence of topics yielded by our joint model of conversational discourse and latent topics.", "labels": [], "entities": []}, {"text": "We conduct an empirical study on the outputs of our joint model on microblog conversation summarization, whose data preparation and set-up processes are presented in this section.", "labels": [], "entities": [{"text": "microblog conversation summarization", "start_pos": 67, "end_pos": 103, "type": "TASK", "confidence": 0.6672031084696451}]}, {"text": "Our experiments are conducted on a large-scale corpus containing ten large conversation trees collected from Sina Weibo, which is released by our prior work () and constructed following the settings described in.", "labels": [], "entities": []}, {"text": "The conversation trees discuss hot events taking place during 2 January to 28 July 2014, and are crawled using the PKUVIS toolkit ().", "labels": [], "entities": [{"text": "PKUVIS toolkit", "start_pos": 115, "end_pos": 129, "type": "DATASET", "confidence": 0.9064303040504456}]}, {"text": "The detailed descriptions of the ten conversation trees are shown in.", "labels": [], "entities": []}, {"text": "As can be observed, more than 12K messages on average and covers discussions about social issues, breaking news, jokes, celebrity scandals, love, and fashion, which matches the official list of typical categories for microblog posts released by Sina Weibo.", "labels": [], "entities": [{"text": "Sina Weibo", "start_pos": 245, "end_pos": 255, "type": "DATASET", "confidence": 0.8203304708003998}]}, {"text": "For each conversation tree, three experienced editors are invited to write summaries.", "labels": [], "entities": []}, {"text": "Based on the manual summaries written by them, we conduct ROUGE evaluation, shown in Section 5.2.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9757971167564392}]}, {"text": "Though compared with many other tasks in the NLP and information retrieval community, the corpus looks relatively small.", "labels": [], "entities": [{"text": "information retrieval community", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.8200020988782247}]}, {"text": "However, to the best of our knowledge, it is currently the only publicly available data set for conversation summarization.", "labels": [], "entities": [{"text": "conversation summarization", "start_pos": 96, "end_pos": 122, "type": "TASK", "confidence": 0.815768837928772}]}, {"text": "It is difficult and time-consuming for human editors to write summaries for conversation trees because of the substantial number of nodes and complex structure involved (); in fact, it would be impossible for human editors to reconstruct conversation trees in a reasonable amount of time.", "labels": [], "entities": []}, {"text": "In the evaluation for each tree, we compute the average ROUGE F1 score between the model-generated summary and the three human-generated summaries.", "labels": [], "entities": [{"text": "ROUGE F1 score", "start_pos": 56, "end_pos": 70, "type": "METRIC", "confidence": 0.8675510287284851}]}, {"text": "Here we describe how summaries are produced given the outputs of topics models.", "labels": [], "entities": []}, {"text": "For each conversation tree c, given the latent topics produced by topic models, we use a content word distribution \u03b3 c to describe its core focus and topic.", "labels": [], "entities": []}, {"text": "Equation (1) shows the formula to compute \u03b3 c . We further plugin \u03b3 c to the criterion proposed by.", "labels": [], "entities": []}, {"text": "The goal is to extract L messages to form a summary set E * c that closely matches \u03b3 c . In our joint model, salient content of tree c is captured without including background noise (modeled with \u03c6 B ) or discourse indicative words (modeled with \u03b4 D d ).", "labels": [], "entities": []}, {"text": "Following, conversation summarization is cast into the following Integer Programming problem: where U(E c ) denotes the empirical unigram distribution of the candidate summary set E c and KL(P||Q) is the Kullback-Lieber (KL) divergence defined as w P(w) log P(w) Q(w) . 26 In implementation, as globally optimizing Equation is exponential in the total number of messages in a conversation, which is a non-deterministic polynomial-time (NP) problem, we use the greedy approximation adopted in for local optimization.", "labels": [], "entities": [{"text": "conversation summarization", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.6397745460271835}]}, {"text": "Specifically, messages are greedily added to a summary so long as they minimize the KL-divergence in the current step.", "labels": [], "entities": []}, {"text": "We consider baselines that rank and select messages by (1) LENGTH; (2) POPULARITY (# of reposts and replies); (3) USER influence (# of authors' followers); and (4) message-message text similarities using LEXRANK).", "labels": [], "entities": [{"text": "LENGTH", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9985583424568176}, {"text": "POPULARITY", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9975404739379883}, {"text": "USER influence", "start_pos": 114, "end_pos": 128, "type": "METRIC", "confidence": 0.9236043393611908}, {"text": "LEXRANK", "start_pos": 204, "end_pos": 211, "type": "METRIC", "confidence": 0.9179621338844299}]}, {"text": "We also consider two state-of-the-art summarizers in comparison:  Preprocessing.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 66, "end_pos": 79, "type": "METRIC", "confidence": 0.837213397026062}]}, {"text": "For baselines and the two state-of-the-art summarizers, we filter out non-Chinese characters in a preprocessing step following their common settings.", "labels": [], "entities": []}, {"text": "For summarization systems based on our topic model variants (i.e., TOPIC ONLY, TOPIC+DISC, and TOPIC+DISC+REL), the hyper-parameters and preprocessing steps are kept the same as in Section 4.1.", "labels": [], "entities": [{"text": "TOPIC", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.5307219624519348}, {"text": "ONLY", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.5005248188972473}, {"text": "REL", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.5935987830162048}]}, {"text": "To further evaluate the generated summaries, we conduct human evaluations on informativeness (Info), conciseness (Conc), and readability (Read) of the extracted summaries.", "labels": [], "entities": [{"text": "readability (Read)", "start_pos": 125, "end_pos": 143, "type": "METRIC", "confidence": 0.6675037294626236}]}, {"text": "Two native Chinese speakers are invited to read the output summaries and subjectively rate on a 1-5 Likert scale and in 0.5 units, where a higher rating indicates better quality.", "labels": [], "entities": [{"text": "Likert scale", "start_pos": 100, "end_pos": 112, "type": "METRIC", "confidence": 0.9574160575866699}]}, {"text": "Their overall inter-rater agreement achieves Krippendorff's \u03b1 of 0.73, which indicates reliable results).", "labels": [], "entities": [{"text": "Krippendorff's \u03b1", "start_pos": 45, "end_pos": 61, "type": "METRIC", "confidence": 0.7243123551209768}]}, {"text": "shows the average ratings by the two raters and over ten conversation trees.", "labels": [], "entities": []}, {"text": "As can be seen, despite of the closing results produced by supervised and wellperformed unsupervised systems in automatic ROUGE evaluation (shown in Section 5.2), when the outputs are judged by humans, supervised systems CHANG ET AL.", "labels": [], "entities": [{"text": "CHANG ET AL", "start_pos": 221, "end_pos": 232, "type": "METRIC", "confidence": 0.8900845448176066}]}], "tableCaptions": [{"text": " Table 1  Statistics of our five data sets on Twitter and Sina Weibo for evaluating topic coherence.", "labels": [], "entities": [{"text": "Sina Weibo", "start_pos": 58, "end_pos": 68, "type": "DATASET", "confidence": 0.9004825949668884}]}, {"text": " Table 3  CV coherence scores for topics produced by various models on Twitter. Higher is better. K50 = 50  topics; K100 = 100 topics; N = the number of top words ranked by topic-word probabilities.  Higher scores indicate better coherence. Best result for each setting is in bold.", "labels": [], "entities": []}, {"text": " Table 6  Description of the ten conversation trees for summarization. Each line describes the statistic  information of one conversation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.9638685584068298}]}, {"text": " Table 6. As can be observed, more than 12K  messages on average and covers discussions about social issues, breaking news, jokes,  celebrity scandals, love, and fashion, which matches the official list of typical categories  for microblog posts released by Sina Weibo.", "labels": [], "entities": [{"text": "Sina Weibo", "start_pos": 258, "end_pos": 268, "type": "DATASET", "confidence": 0.8085079193115234}]}, {"text": " Table 7  Average ROUGE score for model-produced summaries against the three human-generated  references. Len = count of Chinese characters in the extracted summary; Prec, Rec, and  F1 = average precision, recall, and F1 ROUGE measures over the ten conversation trees (%", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.9237176775932312}, {"text": "Len", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9657561182975769}, {"text": "Prec", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9771337509155273}, {"text": "Rec", "start_pos": 172, "end_pos": 175, "type": "METRIC", "confidence": 0.6904952526092529}, {"text": "F1", "start_pos": 182, "end_pos": 184, "type": "METRIC", "confidence": 0.9914719462394714}, {"text": "precision", "start_pos": 195, "end_pos": 204, "type": "METRIC", "confidence": 0.9990563988685608}, {"text": "recall", "start_pos": 206, "end_pos": 212, "type": "METRIC", "confidence": 0.9992731213569641}, {"text": "F1", "start_pos": 218, "end_pos": 220, "type": "METRIC", "confidence": 0.999845027923584}, {"text": "ROUGE", "start_pos": 221, "end_pos": 226, "type": "METRIC", "confidence": 0.7428911328315735}]}]}