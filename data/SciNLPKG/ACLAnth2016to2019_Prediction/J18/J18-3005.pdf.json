{"title": [{"text": "Using Semantics for Granularities of Tokenization under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license", "labels": [], "entities": []}], "abstractContent": [{"text": "Depending on downstream applications, it is advisable to extend the notion of tokenization from low-level character-based token boundary detection to identification of meaningful and useful language units.", "labels": [], "entities": [{"text": "token boundary detection", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.6238447626431783}]}, {"text": "This entails both identifying units composed of several single words that form a multiword expression (MWE), as well as splitting single-word compounds into their meaningful parts.", "labels": [], "entities": []}, {"text": "In this article, we introduce unsupervised and knowledge-free methods for these two tasks.", "labels": [], "entities": []}, {"text": "The main novelty of our research is based on the fact that methods are primarily based on distributional similarity, of which we use two flavors: a sparse count-based and a dense neural-based distributional semantic model.", "labels": [], "entities": []}, {"text": "First, we introduce DRUID, which is a method for detecting MWEs.", "labels": [], "entities": [{"text": "DRUID", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9082400798797607}, {"text": "detecting MWEs", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.7745041847229004}]}, {"text": "The evaluation on MWE-annotated data sets in two languages and newly extracted evaluation data sets for 32 languages shows that DRUID compares favorably over previous methods not utilizing distributional information.", "labels": [], "entities": [{"text": "MWE-annotated data sets", "start_pos": 18, "end_pos": 41, "type": "DATASET", "confidence": 0.8860759735107422}, {"text": "DRUID", "start_pos": 128, "end_pos": 133, "type": "METRIC", "confidence": 0.5357290506362915}]}, {"text": "Second, we present SECOS, an algorithm for decompounding close compounds.", "labels": [], "entities": [{"text": "SECOS", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.5559453368186951}]}, {"text": "In an evaluation of four dedicated decompounding data sets across four languages and on data sets extracted from Wiktionary for 14 languages, we demonstrate the superiority of our approach over unsupervised baselines, sometimes even matching the performance of previous language-specific and supervised methods.", "labels": [], "entities": []}, {"text": "Ina final experiment, we show how both decompounding and MWE information can be used in information retrieval.", "labels": [], "entities": [{"text": "MWE information", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.7769772708415985}, {"text": "information retrieval", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.8251346349716187}]}, {"text": "Here, we obtain the best results when combining word information with MWEs and the compound parts in a bag-of-words retrieval setup.", "labels": [], "entities": []}, {"text": "Overall, our methodology paves the way to automatic detection of lexical units beyond standard tokenization techniques without language-specific preprocessing steps such as POS tagging.", "labels": [], "entities": [{"text": "automatic detection of lexical units", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.773189252614975}, {"text": "POS tagging", "start_pos": 173, "end_pos": 184, "type": "TASK", "confidence": 0.7395407855510712}]}], "introductionContent": [{"text": "If we take Ron Kaplan's motivation for tokenization seriously that the \"stream of characters in a natural language text must be broken up into distinct meaningful units\") to enable natural language processing beyond the character level, then tokenization is more than the low-level preprocessing task of treating interpunctuation, hyphenation, and enclitics.", "labels": [], "entities": []}, {"text": "Rather, tokenization also should aspire to produce meaningful units, or, as define it, tokens should be linguistically significant and methodologically useful.", "labels": [], "entities": []}, {"text": "In practice, however, tokenizers are rather not concerned with meaning or significance-placed right at the beginning of any NLP pipeline and usually implemented in a rule-based fashion, they are merely workhorses to enable higher levels of processing, which includes a reasonable split of the input into word tokens and some normalization to cater to the sensitivity of subsequent processing components.", "labels": [], "entities": []}, {"text": "Although it is clear that the methodological utility of a specific tokenization depends on the overall task, it seems much more practical to fix the tokenization in the beginning of the text ingestion process and handle task-specific adjustments later.", "labels": [], "entities": []}, {"text": "The work presented in this article operationalizes lexical semantics in order to identify meaningful units.", "labels": [], "entities": [{"text": "operationalizes lexical semantics", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.8146171371142069}]}, {"text": "Assuming that low-level processing has already been performed, we devise a method that can identify multiword units, namely, word n-grams that have a non-compositional meaning, as well as a method that can split close compound words into their parts.", "labels": [], "entities": []}, {"text": "Both methods are primarily based on distributional semantics: By operationalizing language unit similarity in various ways, we are able to inform the tokenization process with semantic information, enabling us to yield meaningful units, which are shown to be linguistically valid and methodologically useful in a series of suitable evaluations.", "labels": [], "entities": []}, {"text": "Both methods do not make use of languagespecific processing, thus could be applied directly after low-level tokenization without assuming the existence of, for example, apart of speech tagger.", "labels": [], "entities": [{"text": "speech tagger", "start_pos": 178, "end_pos": 191, "type": "TASK", "confidence": 0.6747311949729919}]}, {"text": "Depending on the task, the low-level \"standard\" tokenization can be too finegrained, as from a semiotic perspective multiword expressions (MWEs) refer to a single concept.", "labels": [], "entities": []}, {"text": "On the flipside, tokenization can be too coarse-grained, as close compound words are detected as single words, whereas they are formed by the concatenation of at least two stems and can be considered as MWEs without white spaces.", "labels": [], "entities": []}, {"text": "In this article, we will describe two different approaches to represent (nominal) concepts in a similar fashion.", "labels": [], "entities": []}, {"text": "This results in an extended tokenization, similar to the work by.", "labels": [], "entities": []}, {"text": "However, they extend their tokenization solely by bracketing phrases and MWEs and do not split text in more fine-grained units.", "labels": [], "entities": [{"text": "bracketing phrases and MWEs", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6690983474254608}]}, {"text": "differentiates between low-level and high-level tokenization.", "labels": [], "entities": []}, {"text": "Whereas high-level tokenization concentrates on the identification of MWEs and phrases, low-level tokenization mostly splits words that are connected by apostrophes or hyphens.", "labels": [], "entities": [{"text": "identification of MWEs and phrases", "start_pos": 52, "end_pos": 86, "type": "TASK", "confidence": 0.8586346745491028}]}, {"text": "Our notion of coarsegrained tokenization is similar to high-level tokenization.", "labels": [], "entities": []}, {"text": "However, the fine-grained tokenization goes one step beyond low-level tokenization, as we split close compound words.", "labels": [], "entities": []}, {"text": "First, we describe a method for detecting MWEs.", "labels": [], "entities": [{"text": "detecting MWEs", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.8587222397327423}]}, {"text": "For defining MWEs, we follow the definition by) that claims that MWEs are \"idiosyncratic interpretations that crossword boundaries (or spaces).\"", "labels": [], "entities": []}, {"text": "Furthermore, MWEs are made up of compounds, phrases, or sentences.", "labels": [], "entities": []}, {"text": "The detection of named entities (e.g., names, locations, companies, or concepts) is often considered as a task of its own, which aims at identifying a subset of MWEs and is relevant for information extraction (e.g., relation extraction or event extraction), but also for information retrieval or automatic speech recognition systems.", "labels": [], "entities": [{"text": "detection of named entities (e.g., names, locations, companies, or concepts)", "start_pos": 4, "end_pos": 80, "type": "TASK", "confidence": 0.7801495231688023}, {"text": "information extraction", "start_pos": 186, "end_pos": 208, "type": "TASK", "confidence": 0.7183607220649719}, {"text": "relation extraction or event extraction", "start_pos": 216, "end_pos": 255, "type": "TASK", "confidence": 0.6650381505489349}, {"text": "information retrieval", "start_pos": 271, "end_pos": 292, "type": "TASK", "confidence": 0.7787633538246155}, {"text": "automatic speech recognition", "start_pos": 296, "end_pos": 324, "type": "TASK", "confidence": 0.6419393320878347}]}, {"text": "As a second contribution, we present a method for splitting close compounds.", "labels": [], "entities": []}, {"text": "Examples for such close compounds include, for example, dishcloth (English), pancake (English), Hefeweizen (German for wheat beer), bijenzwerm (Dutch for swarm of bees) or hiilikuitu (Finnish for carbon fibre).", "labels": [], "entities": []}, {"text": "Similar to MWEs, compounds are created by combining existing words, although in close compounds the stems are not separated by white space.", "labels": [], "entities": []}, {"text": "Detecting the single stems, called decompounding, showed impact in several natural language processing (NLP) applications like automatic speech recognitions (Adda-Decker and Adda 2000), machine translation (, or information retrieval (IR) (Monz and de and is perceived as a crucial component for the processing of languages that are productive with respect to this phenomenon.", "labels": [], "entities": [{"text": "speech recognitions", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.752494752407074}, {"text": "machine translation", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.7627894282341003}, {"text": "information retrieval (IR)", "start_pos": 212, "end_pos": 238, "type": "TASK", "confidence": 0.7576002478599548}]}, {"text": "For both the detection of MWEs and the decompounding of words, most existing approaches rely either on supervised methods or use language-dependent part-ofspeech (POS) information.", "labels": [], "entities": [{"text": "detection of MWEs", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.8336022893587748}]}, {"text": "In this work, we present two knowledge-free and unsupervised (and therefore language-independent) methods that rely on information gained by distributional semantic models that are computed using large unannotated corpora, namely, word2vec () and JoBimText (.", "labels": [], "entities": []}, {"text": "First, we describe these methods and highlight how their information can help for both tokenization tasks.", "labels": [], "entities": [{"text": "tokenization tasks", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.9177308976650238}]}, {"text": "Then, we present results for the identification of MWEs and afterwards show the performance of the method for decompounding.", "labels": [], "entities": [{"text": "identification of MWEs", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.7506515781084696}]}, {"text": "For both tasks, we first show the performance using manually annotated gold data before we present evaluations for multiple languages using automatically extracted data sets from Wikipedia and Wiktionary.", "labels": [], "entities": []}, {"text": "Lastly, we demonstrate how both flavors of such an extended tokenization can be used in an IR setting.", "labels": [], "entities": [{"text": "IR setting", "start_pos": 91, "end_pos": 101, "type": "TASK", "confidence": 0.8944065868854523}]}, {"text": "The article is partly based on previous work () that has been substantially extended by adding experiments for several languages and showing the advantage of combining the methods in an information retrieval evaluation.", "labels": [], "entities": [{"text": "information retrieval evaluation", "start_pos": 186, "end_pos": 218, "type": "TASK", "confidence": 0.812539279460907}]}, {"text": "The article is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the distributional semantic models that are used to compute similarities between lexical units, which are the main source of information for both fine-and coarse-grained tokenization.", "labels": [], "entities": []}, {"text": "Then, we describe how multiword expressions can be detected and evaluate our methodology.", "labels": [], "entities": []}, {"text": "In Section 4, we describe the workings and the evaluation for compound splitting.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.8207003474235535}]}, {"text": "How to use both methods for information retrieval is shown in Section 5.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.8384877443313599}]}, {"text": "In Section 6, we present the related work.", "labels": [], "entities": []}, {"text": "Afterwards, we highlight the main findings in the conclusion in Section 7 and give an overview of future work in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the method, we examine two experimental settings: first, we compute all measures on a small corpus that has been annotated for MWEs, which serves as the gold standard.", "labels": [], "entities": []}, {"text": "In the second setting, we compute the measures on a larger in-domain corpus.", "labels": [], "entities": []}, {"text": "The evaluation is again performed for the same candidate terms as given by the gold standard.", "labels": [], "entities": []}, {"text": "Results for the top k ranked entries are reported using the precision at k: with xi equal to 1 if the ith ranked candidate is annotated as MWE and 0 otherwise.", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9978099465370178}, {"text": "MWE", "start_pos": 139, "end_pos": 142, "type": "DATASET", "confidence": 0.6861011385917664}]}, {"text": "For an overall performance we use the average precision (AP) as defined by Thater, Dinu, and Pinkal: with T mwe being the set of positive MWEs.", "labels": [], "entities": [{"text": "average precision (AP)", "start_pos": 38, "end_pos": 60, "type": "METRIC", "confidence": 0.8252574563026428}]}, {"text": "When facing tied scores, we mix false and true candidates randomly following.", "labels": [], "entities": []}, {"text": "In order to demonstrate the performance of DRUID for several languages, we perform an evaluation on 32 languages.", "labels": [], "entities": []}, {"text": "For this experiment, we compute similarities on their respective Wikipedias.", "labels": [], "entities": []}, {"text": "The evaluation is performed by extracting the 1,000 highest ranked words using DRUID.", "labels": [], "entities": [{"text": "DRUID", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.8488889932632446}]}, {"text": "In order to determine whether a word sequence is a MWE, we use Wiktionary as \"gold\" standard and test whether it occurs as word entry.", "labels": [], "entities": [{"text": "Wiktionary", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.9131163358688354}]}, {"text": "Using this information, we compute the AP for these 1,000 ranked words.", "labels": [], "entities": [{"text": "AP", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9985241293907166}]}, {"text": "We present the results for this experiment in columns 2 to 5 in.", "labels": [], "entities": []}, {"text": "The t-test with stopword filtering mostly performs similar to the frequency baseline and improves from an average score of 0.07 to 0.08.", "labels": [], "entities": []}, {"text": "We observe that in comparison to two baselines, frequency (freq.) and the t-test with stopword filtering, the DRUID method yields the best scores for 6 out of the 32 languages.", "labels": [], "entities": [{"text": "frequency (freq.)", "start_pos": 48, "end_pos": 65, "type": "METRIC", "confidence": 0.8538891524076462}, {"text": "DRUID", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.7267941832542419}]}, {"text": "However, if we multiply the logarithmic frequency by the DRUID measure, we gain the best performance for 30 languages.", "labels": [], "entities": [{"text": "DRUID", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9418136477470398}]}, {"text": "In general, numerical scores are low-for example, for Arabic, Slovene, or Italian, we obtain APs below 0.10.", "labels": [], "entities": [{"text": "APs", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9977027773857117}]}, {"text": "The highest scores are achieved for Swedish (0.33), German (0.36), Turkish (0.36), French (0.44), and English (0.70).", "labels": [], "entities": []}, {"text": "Analyzing the results, we observe that many \"false\" MWEs are multiword units that are in fact multiword units, which are just not covered in the respective language's Wiktionary.", "labels": [], "entities": []}, {"text": "Furthermore, we detect that these word sequences often are titles of Wikipedia articles.", "labels": [], "entities": []}, {"text": "The absence of word lemmatization causes further decline, as words in Wiktionary are recorded in lemmatized form.", "labels": [], "entities": []}, {"text": "To alleviate this influence, we extend our evaluation and check the occurrence of word sequences both in Wiktionary and Wikipedia.", "labels": [], "entities": []}, {"text": "Using the Wikipedia API also normalizes query terms and, thus, we obtain a better word sequence coverage.", "labels": [], "entities": []}, {"text": "This is confirmed by much higher results, as shown in columns 6 through 9 in.", "labels": [], "entities": []}, {"text": "Using the frequency combination with DRUID, we even gain higher APs for languages, which attained worse scores in the previous setting (e.g., Arabic [0.62], Slovene, and Italian).", "labels": [], "entities": [{"text": "DRUID", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.6287586092948914}, {"text": "APs", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9984071850776672}]}, {"text": "Except for Estonian and Polish, using the logarithmic frequency weighting performs best for all languages.", "labels": [], "entities": []}, {"text": "For these two languages, using the sole DRUID measure performs best.", "labels": [], "entities": [{"text": "DRUID", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.8936979174613953}]}, {"text": "The best performance is obtained for English (0.87), Turkish (0.66), French (0.66), German (0.62), and Portuguese (0.64).", "labels": [], "entities": []}, {"text": "Based on these multilingual experiments, we have demonstrated that DRUID not only performs well for English and French, but also for other languages, showing that its elements, uniqueness and incompleteness, are language-independent principles for multiword characterization.", "labels": [], "entities": [{"text": "multiword characterization", "start_pos": 248, "end_pos": 274, "type": "TASK", "confidence": 0.8294514715671539}]}, {"text": "For the computation of our method, we use similarities computed on various languages.", "labels": [], "entities": []}, {"text": "First, we compute the DTs using JoBimText using the left and the right neighboring word as context representation.", "labels": [], "entities": []}, {"text": "In addition, we extract a DT from the CBOW method from word2vec () using 500 dimensions, as described in Section 2.", "labels": [], "entities": []}, {"text": "We compute the similarities for German based on 70M sentences and for Finnish on 4M sentences that are provided via the Leipzig Corpora Collection corpus ().", "labels": [], "entities": [{"text": "Leipzig Corpora Collection corpus", "start_pos": 120, "end_pos": 153, "type": "DATASET", "confidence": 0.9672024846076965}]}, {"text": "For the generation of the Dutch similarities, we use the Dutch web corpus, which is composed of 259 million sentences.", "labels": [], "entities": [{"text": "Dutch web corpus", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.9744193355242411}]}, {"text": "Similarities for Afrikaans are computed using the Taalkommissie corpus (3M sentences) (Taalkommissie 2011) and we use 150GB of texts for Russian.", "labels": [], "entities": [{"text": "Taalkommissie corpus", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.856226921081543}]}, {"text": "The evaluation for various languages based on the automatically extracted data set is performed on similarities computed on text from the respective Wikipedias.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the algorithms using a splitwise precision and recall measure that is inspired by the measures introduced by.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9046825170516968}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9952391386032104}]}, {"text": "Our evaluation is based on the splits of the compounds and is defined as shown: 15 We set = 0.01.", "labels": [], "entities": []}, {"text": "In the range of = [0.0001, 1] we observe marginally higher scores using smaller values.", "labels": [], "entities": []}, {"text": "16 Although our method mostly does not assume language knowledge, we uppercase the first letter of each w i , when we apply our method on German nouns.", "labels": [], "entities": []}, {"text": "17 Available at: http://webcorpora.org/.", "labels": [], "entities": []}, {"text": "18 The sentences are extracted from: http://lib.rus.ec.", "labels": [], "entities": []}, {"text": "As unsupervised baselines we use the semantic analogy-based splitter (SAS) from ( and the split ranking by, called KK.", "labels": [], "entities": []}, {"text": "In this section, we first show results for manually extracted data sets and then demonstrate the multilingual capabilities of our method using a data set that was automatically extracted from Wiktionary.", "labels": [], "entities": []}, {"text": "We compare our results to previously available methods, which will be discussed in Section 6.2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Amounts and percentages of MWEs contained in WordNet 3.1 for different POS.", "labels": [], "entities": [{"text": "WordNet 3.1", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.9447241127490997}]}, {"text": " Table 2  The ten most similar entries for the term red blood cell (left) and red blood (right). Here, seven of  ten terms are single words in both lists.", "labels": [], "entities": []}, {"text": " Table 5  Number of MWE candidates after filtering for the expected POS tag. Additionally, the table  shows the distribution over n-grams with n \u2208 {1, 2, 3, 4}.", "labels": [], "entities": []}, {"text": " Table 6  Results for P@100, P@500, and the average precision (AP) for various ranking measures. The  gold standard is extracted using the GENIA corpus. This corpus is also used for computing the  measures.", "labels": [], "entities": [{"text": "average precision (AP)", "start_pos": 44, "end_pos": 66, "type": "METRIC", "confidence": 0.8375721096992492}, {"text": "GENIA corpus", "start_pos": 139, "end_pos": 151, "type": "DATASET", "confidence": 0.9822732210159302}]}, {"text": " Table 7  Results for MWE detection on the French SPMRL corpus. Both the generation of the gold  standard and the computations of the measures have been performed on this corpus.", "labels": [], "entities": [{"text": "MWE detection", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9885298907756805}, {"text": "French SPMRL corpus", "start_pos": 43, "end_pos": 62, "type": "DATASET", "confidence": 0.9292667110761007}]}, {"text": " Table 8  Results of n-gram ranking on the medical data. Whereas the gold standard is extracted from the  GENIA data set, the ranking measures as well as the frequency threshold for selecting the gold  candidates are computed using the Medline corpus.", "labels": [], "entities": [{"text": "GENIA data set", "start_pos": 106, "end_pos": 120, "type": "DATASET", "confidence": 0.9873319665590922}, {"text": "Medline corpus", "start_pos": 236, "end_pos": 250, "type": "DATASET", "confidence": 0.9925742149353027}]}, {"text": " Table 9  Results for ranking n-grams, according to their multiwordness, based on the French ERC.  The candidates are extracted based on the smaller SPMRL corpus.", "labels": [], "entities": [{"text": "French ERC", "start_pos": 86, "end_pos": 96, "type": "DATASET", "confidence": 0.9464015662670135}, {"text": "SPMRL corpus", "start_pos": 149, "end_pos": 161, "type": "DATASET", "confidence": 0.9117127060890198}]}, {"text": " Table 10  MWE ranking results based on different methods without using any linguistic preprocessing.", "labels": [], "entities": [{"text": "MWE", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9658793210983276}]}, {"text": " Table 11  AP for the frequency baseline, t-test, and DRUID evaluated against Wiktionary and a combination  of Wiktionary and Wikipedia, including word normalization.", "labels": [], "entities": [{"text": "AP", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.972655713558197}, {"text": "DRUID", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.992989182472229}, {"text": "word normalization", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.7522478401660919}]}, {"text": " Table 12  Top ranked candidates from the GENIA data set using our ranking method (left) and the  competitive method (right). Each term is marked if it is an MWE (1) or not (0).", "labels": [], "entities": [{"text": "GENIA data set", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.9837135076522827}]}, {"text": " Table 13  Top ranked candidates from the SPMRL data set for the best DRUID method (left) and the best  competitive method (right). Each term is marked if it is an MWE (1) or not (0).", "labels": [], "entities": [{"text": "SPMRL data set", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.8247248629728953}]}, {"text": " Table 14  Top ranked terms for the Medline corpus, which are not marked as MWEs. The rank is denoted  to the left of each term and all terms, which can be found within a lexicon, are marked in bold.", "labels": [], "entities": [{"text": "Medline corpus", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.9651941657066345}]}, {"text": " Table 18  Results based on manually created data sets for German, Dutch, Afrikaans, and Finnish.  We mark the best results in bold font and use an asterisk (*) to show if a method performs  significantly better than the baseline methods and use two asterisks (**) when a single method  outperforms all others significantly.", "labels": [], "entities": []}, {"text": " Table 20  Number of compounds that have been split incorrectly with respect to the gold data. We report  numbers of how many of these compounds have fewer split points (under-split), too many split  points (over-split), or the correct number but wrong split points (wrongly-split). In addition,  we show the total number of missed, wrong, and correct splits for these compounds.", "labels": [], "entities": []}]}