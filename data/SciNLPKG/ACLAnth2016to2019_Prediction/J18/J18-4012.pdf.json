{"title": [{"text": "Modeling Speech Acts in Asynchronous Conversations: A Neural-CRF Approach", "labels": [], "entities": [{"text": "Modeling Speech Acts in Asynchronous Conversations", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8435882031917572}]}], "abstractContent": [{"text": "Participants in an asynchronous conversation (e.g., forum, e-mail) interact with each other at different times, performing certain communicative acts, called speech acts (e.g., question, request).", "labels": [], "entities": []}, {"text": "In this article, we propose a hybrid approach to speech act recognition in asynchronous conversations.", "labels": [], "entities": [{"text": "speech act recognition", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.6671410302321116}]}, {"text": "Our approach works in two main steps: along short-term memory recurrent neu-ral network (LSTM-RNN) first encodes each sentence separately into a task-specific distributed representation, and this is then used in a conditional random field (CRF) model to capture the conversational dependencies between sentences.", "labels": [], "entities": []}, {"text": "The LSTM-RNN model uses pretrained word embeddings learned from a large conversational corpus and is trained to classify sentences into speech act types.", "labels": [], "entities": []}, {"text": "The CRF model can consider arbitrary graph structures to model conversational dependencies in an asynchronous conversation.", "labels": [], "entities": []}, {"text": "In addition, to mitigate the problem of limited annotated data in the asynchronous domains, we adapt the LSTM-RNN model to learn from synchronous conversations (e.g., meetings), using domain adversarial training of neural networks.", "labels": [], "entities": []}, {"text": "Empirical evaluation shows the effectiveness of our approach over existing ones: (i) LSTM-RNNs provide better task-specific representations, (ii) conversational word embeddings benefit the LSTM-RNNs more than the off-the-shelf ones, (iii) adversarial training gives better domain-invariant representations, and (iv) the global CRF model improves over local models.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In this section, we present our experimental settings, results, and analysis.", "labels": [], "entities": []}, {"text": "We start with an outline of the experiments.", "labels": [], "entities": []}, {"text": "Our main objective is to evaluate our speech act recognizer on asynchronous conversations.", "labels": [], "entities": [{"text": "speech act recognizer", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.6620208223660787}]}, {"text": "For this, we evaluate our models on the forum and e-mail data sets introduced earlier in Section 4.1: (i) our newly created QC3 data set, (ii) the TripAdvisor (TA) data set from Jeong, Lin, and, and (iii) the BC3 e-mail corpus from.", "labels": [], "entities": [{"text": "QC3 data set", "start_pos": 124, "end_pos": 136, "type": "DATASET", "confidence": 0.9752558668454488}, {"text": "TripAdvisor (TA) data set", "start_pos": 147, "end_pos": 172, "type": "DATASET", "confidence": 0.6806027640899023}, {"text": "BC3 e-mail corpus", "start_pos": 209, "end_pos": 226, "type": "DATASET", "confidence": 0.9213246703147888}]}, {"text": "In addition, we validate our sentence encoding approach on the MRDA meeting corpus.", "labels": [], "entities": [{"text": "sentence encoding", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7731655240058899}, {"text": "MRDA meeting corpus", "start_pos": 63, "end_pos": 82, "type": "DATASET", "confidence": 0.8785485227902731}]}, {"text": "Because of the noisy and informal nature of conversational texts, we performed a series of preprocessing steps before using it for training or testing.", "labels": [], "entities": []}, {"text": "We normalize all characters to their lowercased forms, truncate elongations to two characters, and spell out every digit and URL.", "labels": [], "entities": []}, {"text": "We further tokenized the texts using the CMU TweetNLP tool ().", "labels": [], "entities": [{"text": "CMU TweetNLP", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.8257632553577423}]}, {"text": "For performance comparison, we use both accuracy and macro-averaged F 1 score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9997603297233582}, {"text": "F 1 score", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9399148027102152}]}, {"text": "Accuracy gives the overall performance of a classifier but could be biased toward the most populated classes, whereas macro-averaged F 1 weights every class equally, and is not influenced by class imbalance.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9844709634780884}]}, {"text": "Statistical significance tests are done using an approximate randomization test based on the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9986761212348938}]}, {"text": "We used SIGF V.2) with 10,000 iterations.", "labels": [], "entities": []}, {"text": "In the following, we first demonstrate the effectiveness of our LSTM-RNN model for learning task-specific sentence encoding by training it on the task in three different settings: (i) training on in-domain data only, (ii) training on a simple concatenation of synchronous and asynchronous data, and (iii) training it with adversarial training for domain adaptation.", "labels": [], "entities": [{"text": "learning task-specific sentence encoding", "start_pos": 83, "end_pos": 123, "type": "TASK", "confidence": 0.7000218480825424}, {"text": "domain adaptation", "start_pos": 347, "end_pos": 364, "type": "TASK", "confidence": 0.717502161860466}]}, {"text": "We also compare the effectiveness of different embedding types in these three training settings.", "labels": [], "entities": []}, {"text": "The best task-specific embeddings are then extracted and fed into the CRF models to learn inter-sentence dependencies.", "labels": [], "entities": []}, {"text": "In Section 5.3, we compare how our CRF models with different conversational graph structure perform.", "labels": [], "entities": []}, {"text": "gives an outline of our experimental roadmap.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Dialog act tags and their relative frequencies in the BC3 and TripAdvisor (TA) corpora.", "labels": [], "entities": [{"text": "TripAdvisor (TA) corpora", "start_pos": 72, "end_pos": 96, "type": "DATASET", "confidence": 0.7138382852077484}]}, {"text": " Table 3  Statistics about TripAdvisor (TA), BC3, and QC3 corpora.", "labels": [], "entities": []}, {"text": " Table 4  Distribution of speech acts (in percentage) in our corpora.", "labels": [], "entities": []}, {"text": " Table 5  Cohen's \u03ba agreement for different speech acts in QC3.", "labels": [], "entities": [{"text": "QC3", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.9526682496070862}]}, {"text": " Table 6  Data sets and their statistics used for training the conversational word embeddings.", "labels": [], "entities": []}, {"text": " Table 8  Number of sentences in train, development, and test sets for different data sets.", "labels": [], "entities": []}, {"text": " Table 14  Data sets for CONV-LEVEL (conversation-level) setting to train, validate, and test our CRF  models. Numbers in parentheses indicate the number of sentences.", "labels": [], "entities": []}, {"text": " Table 15  Results of CRFs on CONV-LEVEL data set. Best results are boldfaced. Accuracies significantly  higher than adapted B-LSTM conv-glove are marked with *.", "labels": [], "entities": [{"text": "CONV-LEVEL data set", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.9796585837999979}, {"text": "Accuracies", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9626398086547852}]}]}