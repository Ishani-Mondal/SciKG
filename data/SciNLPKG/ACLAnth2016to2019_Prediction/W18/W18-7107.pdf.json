{"title": [{"text": "Work Smart -Reducing Effort in Short-Answer Grading", "labels": [], "entities": [{"text": "Short-Answer Grading", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.7132232785224915}]}], "abstractContent": [{"text": "In language (and content) instruction, free-text questions are important instruments for gauging student ability.", "labels": [], "entities": []}, {"text": "Grading is often done manually, so that frequent testing means high teacher workloads.", "labels": [], "entities": []}, {"text": "We propose anew strategy for supporting manual graders: We carefully analyse the performance of automated graders individually and as a grader ensemble and present a procedure to guide manual effort and to estimate the size of the remaining grading error.", "labels": [], "entities": []}, {"text": "We evaluate our approach on a range of data sets to demonstrate its robustness.", "labels": [], "entities": []}], "introductionContent": [{"text": "Using computers in teaching has opened up new possibilities for learning independent of time or location while receiving individual feedback through frequent testing.", "labels": [], "entities": []}, {"text": "For this, automated evaluation of student answers, supported most easily by closed question formats like multiple choice, is key.", "labels": [], "entities": []}, {"text": "This means that tests usually do not contain open question types like short answer questions, although these are didactically valuable because they provide insight into students' reasoning.", "labels": [], "entities": []}, {"text": "There is a substantial body of research addressing automated short-answer grading (SAG, see for an overview).", "labels": [], "entities": []}, {"text": "However, the resulting tools are not widely used to produce completely automated student feedback.", "labels": [], "entities": []}, {"text": "Instead, automated methods to reduce manual grading workload have been proposed (which can also be used to reduce annotation workload for training data in general).", "labels": [], "entities": []}, {"text": "The use of clustering for label propagation ( and of Active) has been investigated.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7861408293247223}]}, {"text": "In this paper, we describe anew strategy to reduce human graders' workloads.", "labels": [], "entities": []}, {"text": "We pre-grade student answers with automated methods that have been carefully analysed to reveal their strengths and weaknesses with regard to the target categories.", "labels": [], "entities": []}, {"text": "Combining several automated graders into an ensemble additionally yields insight into the reliability of individual machine grades.", "labels": [], "entities": []}, {"text": "Human grading effort can now be focused on reviewing those answers that were most likely not graded correctly.", "labels": [], "entities": []}, {"text": "Effectively, we harness two basic insights of machine learning: Learners perform best on frequently-attested classes (and consequently, under-represented classes require more human attention), and ensembles of learners outperform any given single model (and consequently, automated decisions with high agreement across learners are likely reliable).", "labels": [], "entities": []}, {"text": "Our strategy allows a sizeable reduction of human effort (by at least 40% and up to 93%), while grading accuracy remains at or even improves beyond purely human grading.Since not every student answer is reviewed by a teacher, our approach does not support individual teacher comments on each answer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9773041605949402}]}, {"text": "It is useful in situations where overall performance is being determined by accumulating the grades for individual answers, for example placement tests or recurring text comprehension tests.", "labels": [], "entities": []}, {"text": "Our paper is structured as follows: We first give an overview over related work in manual grader support for SAG (Section 2).", "labels": [], "entities": [{"text": "SAG", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.7172927856445312}]}, {"text": "We then describe our method and our seven data sets, the machine learning algorithms and features, as well as the evaluation measures in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, we analyse human grading performance in terms of Precision, Recall and Inter-Annotator Agreement to establish a point of comparison.", "labels": [], "entities": [{"text": "Precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9980500936508179}, {"text": "Recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9648449420928955}, {"text": "Inter-Annotator Agreement", "start_pos": 85, "end_pos": 110, "type": "METRIC", "confidence": 0.9041893184185028}]}, {"text": "We then investigate the strengths and weaknesses of an automated grader compared to the human gold standard.", "labels": [], "entities": []}, {"text": "In Section 6, we assess how much grading effort can be saved and how much grading error remains when we use reliability estimates that are based on the Inter-Annotator Agreement of machine grades only.", "labels": [], "entities": []}, {"text": "We summarise our conclusions and point out future work in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments target ad-hoc tests such as weekly quizzes or end-of-term exams, where question re-use is limited.", "labels": [], "entities": []}, {"text": "This sets us apart from approaches that use a corpus of sample answers to prepare grading models fora standardised question pool.", "labels": [], "entities": []}, {"text": "Rather, it restricts us to an unseen-question setting in which no training answers are available for any of the questions in the test set.", "labels": [], "entities": []}, {"text": "We use various data sets (Section 3.1) to train machine learners (Section 3.2) and evaluate their performance using Precision/Recall and Inter-Annotator Agreement (IAA) (Section 3.3).", "labels": [], "entities": [{"text": "Precision/Recall and Inter-Annotator Agreement (IAA)", "start_pos": 116, "end_pos": 168, "type": "METRIC", "confidence": 0.690793106953303}]}, {"text": "We report weighted Precision (P) and Recall (R) -on the whole corpus in Section 4, for comparison to human performance; and per predicted category (see Section 5), fora more detailed performance analysis.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 19, "end_pos": 32, "type": "METRIC", "confidence": 0.9434774369001389}, {"text": "Recall (R) -", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.9735397458076477}]}, {"text": "P and R indicate how reliable a learner's category predictions are and how well they overlap with the actual incidence of that category.", "labels": [], "entities": []}, {"text": "Note that weighted overall Recall corresponds to overall Accuracy.", "labels": [], "entities": [{"text": "Recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9560946226119995}, {"text": "Accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9775955677032471}]}, {"text": "Overall weighted Recall Rec is computed as in Equation 1.", "labels": [], "entities": [{"text": "Recall Rec", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.8054312765598297}, {"text": "Equation", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9070096015930176}]}, {"text": "The advantage of Inter-annotator Agreement (IAA) measures such as, which is more general than) is that they take into account chance agreement by considering the study-specific distribution of annotation categories.", "labels": [], "entities": []}, {"text": "Fleiss' \u03ba allows us to compute agreement for individual answers as well as on the question level.", "labels": [], "entities": []}, {"text": "\u03ba estimates the annotation reliability in cases where two or more annotators (human or machine) are present.", "labels": [], "entities": []}, {"text": "It is computed as where 1 \u2212 \u00af P e denotes the agreement predicted by chance and \u00af P \u2212 \u00af P e denotes the agreement actually attained.", "labels": [], "entities": []}, {"text": "\u00af P and \u00af P e are calculated as: For each answer that receives a grade, we can calculate the individual agreement P i as where m is the number of annotators, n ij is the number of annotators that chose a category j for token i, k the number of categories and N the number of tokens.", "labels": [], "entities": []}, {"text": "We follow Yannakoudakis and Cummins (2015) and do not report correlation measures like Pearson's rand Spearman's \u03c1, as they are not appropriate for data with many ties (such as grading data sets with their fixed range of categories).", "labels": [], "entities": [{"text": "correlation", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9661821722984314}, {"text": "Pearson's rand Spearman's \u03c1", "start_pos": 87, "end_pos": 114, "type": "METRIC", "confidence": 0.5182459155718485}]}, {"text": "Furthermore, r is sensitive to outliers, while \u03c1 inherently measures the ability of a system to rank answers appropriately, as opposed to predicting the correct category.", "labels": [], "entities": []}, {"text": "As such, correlation measures do not support our goal of determining how reliable itemwise machine predictions are.", "labels": [], "entities": [{"text": "correlation", "start_pos": 9, "end_pos": 20, "type": "METRIC", "confidence": 0.9488070607185364}]}, {"text": "Our first experiment investigates human-human performance and compares it to the reliability of automated grading.", "labels": [], "entities": []}, {"text": "We compute human P, Rand \u03ba for the data sets where informative double manual annotations are available (ASAP, CREE, CSSAG and Mohler).", "labels": [], "entities": [{"text": "Rand \u03ba", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9379307329654694}, {"text": "ASAP", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.6156893372535706}, {"text": "CREE", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.7108864188194275}]}, {"text": "For the human data, we report the performance of the best single annotator against the gold labels and show machine P and R for comparison.", "labels": [], "entities": []}, {"text": "We begin with the easiest setting: binary correct-incorrect classification for all corpora (where correct means > 50% of the max score).", "labels": [], "entities": []}, {"text": "Human P/R results (H P and HR in) are in the eighties (up to 94 for ASAP) throughout.", "labels": [], "entities": [{"text": "P/R", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.6796330213546753}, {"text": "H P and HR in)", "start_pos": 19, "end_pos": 33, "type": "METRIC", "confidence": 0.7128421266873678}, {"text": "ASAP", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.3994855582714081}]}, {"text": "For the Mohler data, we show two performance numbers: For this data set, the gold standard is created by averaging the two single annotations; therefore, every annotator's grades are highly correlated with the gold.", "labels": [], "entities": [{"text": "Mohler data", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.7235955893993378}, {"text": "gold standard", "start_pos": 77, "end_pos": 90, "type": "METRIC", "confidence": 0.814566969871521}]}, {"text": "This leads to artificially inflated P and R values (shown in brackets in).", "labels": [], "entities": []}, {"text": "For the other grades, the gold standard was created independently (CSSAG) or one annotator's grades are marked as the gold standard (CREE and ASAP), so that the other's grades are independent of gold.", "labels": [], "entities": [{"text": "CREE", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.8204250335693359}, {"text": "ASAP", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9332192540168762}]}, {"text": "Treating the Mohler data in this way (using annotator \"me\" as gold annotation) results in performance in the low eighties.", "labels": [], "entities": [{"text": "Mohler data", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.8219714462757111}]}, {"text": "We refer to this evaluation method als Mohler strict evaluation below.", "labels": [], "entities": []}, {"text": "Our results show that human annotation with up to 16.5% of error (Mohler strict evaluation, 14.2% for CREE) has been accepted in the past for low-volume testing.", "labels": [], "entities": [{"text": "error", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9155077338218689}, {"text": "CREE", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.8580841422080994}]}, {"text": "(Assuming error to be 1-Accuracy, that is 1-R, since weighted R equals Accuracy).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9988362193107605}]}, {"text": "For high-volume testing (ASAP), we see much lower rates at 6% error.", "labels": [], "entities": [{"text": "ASAP)", "start_pos": 25, "end_pos": 30, "type": "TASK", "confidence": 0.8347969949245453}, {"text": "error", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.963908314704895}]}, {"text": "Human \u03ba values vary widely across corpora and range from 0.41 (Mohler) to 0.82 (ASAP).", "labels": [], "entities": [{"text": "ASAP", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.6152710318565369}]}, {"text": "The higher \u03ba, the better the human annotators agreed on the grades, producing clearly defined categories and clean training data.", "labels": [], "entities": []}, {"text": "We find low \u03bas for corpora collected as a byproduct of low-volume, ad hoc testing with many different questions and different grade categories.", "labels": [], "entities": []}, {"text": "ASAP, collected in a high-volume testing setting, is the opposite, since the reliability of multi-annotator grading is a priority when single-annotator grading is impossible due to testing volume.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.908852219581604}, {"text": "reliability", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.9876376390457153}]}, {"text": "Consequently, there is a small number of grade categories and clear scoring rubrics exist for each question (and graders were likely carefully trained to apply them consistently).", "labels": [], "entities": []}, {"text": "We present machine P and R for the RF learner, our best individual machine grader.", "labels": [], "entities": []}, {"text": "It outperforms literature results from Pad\u00f3 (2016) (who used the same features).", "labels": [], "entities": []}, {"text": "The SVM and DT learners show similar result patterns as RF, but perform an average of 3 (SVM) and 4 (DT) percentage points worse.", "labels": [], "entities": []}, {"text": "Machine results are worse than human results except for the CREE corpus and also outperform the strict Mohler human-human P and R values (see above).", "labels": [], "entities": [{"text": "CREE corpus", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.8706892430782318}]}, {"text": "Note that both these corpora are strongly skewed towards one class (87% and 72% of the items, respectively).", "labels": [], "entities": []}, {"text": "In CSSAG (as well as in SEB and Beetle), the class balance moves to 60-40 and learner performance is noticeably worse.", "labels": [], "entities": [{"text": "CSSAG", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.8187838196754456}, {"text": "Beetle", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.8076925277709961}]}, {"text": "In fact, human P/R results for CSSAG are the strongest among the low-volume corpora, but  the machine results are the lowest of all four corpora.", "labels": [], "entities": [{"text": "P/R", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.8235984444618225}]}, {"text": "This maybe caused by the low reliability of the human annotations (evidenced by low \u03ba).", "labels": [], "entities": [{"text": "reliability", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.9960319399833679}]}, {"text": "The strong skew of the Mohler data (49% of data points are annotated with the highest of 11 categories) probably masks a similar effect for that corpus.", "labels": [], "entities": [{"text": "Mohler data", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.8167988359928131}]}, {"text": "Experiment 1 has presented human annotation standards and the performance of a vanilla automated grading model.", "labels": [], "entities": []}, {"text": "While the automated grader clearly has room for improvement, our next analyses show that even unreliable machine predictions can considerably reduce human grading effort.", "labels": [], "entities": []}, {"text": "Our goal is to focus the human grading effort on those answers where it is most needed.", "labels": [], "entities": []}, {"text": "We accept the consequence that not every student answer will be reviewed by a human grader and that some errors will remain in the final grades.", "labels": [], "entities": []}, {"text": "Therefore, the approach is most suitable for testing situations where the grades for individual answers are combined into an overall grade.", "labels": [], "entities": []}, {"text": "This accumulated grade is more robust towards some remaining error.", "labels": [], "entities": []}, {"text": "Note that the notion of \"most needed human attention\" depends on the testing context.", "labels": [], "entities": []}, {"text": "In formative feedback situations, it is more acceptable to receive approximate grades than in high-stakes testing, since no decisive consequences depend on formative feedback.", "labels": [], "entities": []}, {"text": "We will further discuss these issues below, where we strive to present the tradeoff between grading accuracy and grading effort in order to allow users to find the ideal balance for their situation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9377321004867554}]}, {"text": "In this Section, we take a first step and discuss how to identify reliable machine grades based only on the RF grader's strengths and weaknesses.", "labels": [], "entities": []}, {"text": "In Section 6, we will move onto comparing automated predictions from several learners for improved reliability estimates.: Weighted P and R per category (binary classification) and majority class (CREG is balanced by design).", "labels": [], "entities": []}, {"text": "shows category-wise P and R for binary classification.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.6745303720235825}]}, {"text": "As can be expected, the majority class is predicted more reliably and with fewer errors in all cases.", "labels": [], "entities": []}, {"text": "As CREG is balanced by design, there is no such frequency effect.", "labels": [], "entities": []}, {"text": "For the highly imbalanced data sets, R drops steeply in the minority category (between 25%-points for ASAP -71% incorrect -and 83%-points for Mohler -87% correct) as the machine grader over-generalises to the majority category.", "labels": [], "entities": [{"text": "R", "start_pos": 37, "end_pos": 38, "type": "METRIC", "confidence": 0.9977074861526489}, {"text": "ASAP", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.5490190982818604}]}, {"text": "We now switch from using a single automated grader to combining three automated graders (RF, DT and SVM).", "labels": [], "entities": []}, {"text": "This approach allows us to generate multiple machine annotations and use them for reliability estimates.", "labels": [], "entities": []}, {"text": "We use \u03ba to analyse the automated graders' reliability down to the singleitem level and to generate fine-grained reviewing recommendations for manual graders.", "labels": [], "entities": []}, {"text": "We assume that a machine grade is more reliable if more of the graders in our ensemble predict it (and therefore agree better, such that \u03ba is high).", "labels": [], "entities": []}, {"text": "With three learners, the item-wise predictions can be in full agreement (FullA, \u03ba = 1), partial agreement (PartA, \u03ba = 0.83) or no agreement (NA, \u03ba = 0).", "labels": [], "entities": []}, {"text": "shows automated grader agreement and disagreement for the binary (and, where applicable) multi-class case.", "labels": [], "entities": [{"text": "agreement", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.7844336628913879}]}, {"text": "The number of target categories is given in brackets.", "labels": [], "entities": []}, {"text": "This figure demonstrates that our assumption of greater agreement = greater reliability is generally justified: Compare the proportion of true and false predictions for full agreement and partial agreement.", "labels": [], "entities": [{"text": "reliability", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.991917073726654}]}, {"text": "There are vastly fewer cases of FullAF (full agreement, false; yellow) than FullAT (full agreement, true; dark green), while cases of PartAF (partial agreement, false; orange) and PartAT (light green) are closer to balance.", "labels": [], "entities": [{"text": "FullAF", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.7826434373855591}, {"text": "FullAT", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.8603610396385193}, {"text": "PartAF", "start_pos": 134, "end_pos": 140, "type": "DATASET", "confidence": 0.8831963539123535}, {"text": "PartAT", "start_pos": 180, "end_pos": 186, "type": "DATASET", "confidence": 0.8840057253837585}]}, {"text": "Cases of FullAF (full agreement, false prediction; yellow) are generally around 10%, with a maximum of 19% for CSSAG (9).", "labels": [], "entities": [{"text": "FullAF", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.7882037162780762}, {"text": "CSSAG", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.8729411363601685}]}, {"text": "This is similar to human standards: Human annotators may also agree on a category that does not match the gold standard.", "labels": [], "entities": []}, {"text": "In our CSSAG subset, this occurred for 10% of annotations, as well.", "labels": [], "entities": [{"text": "CSSAG subset", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.8623548448085785}]}, {"text": "Given this picture, our recommendation is to manually review answers in the order of the amount of learner disagreement on the category, beginning with NA.", "labels": [], "entities": []}, {"text": "The first lines of show that in the binary case, reviewing only NA cases of course means no manual work (three graders have to agree at least partially on two labels, so there is always a clear majority for one label), at error levels between 11% (Mohler (2)) and 30% (SEB).", "labels": [], "entities": [{"text": "SEB", "start_pos": 269, "end_pos": 272, "type": "METRIC", "confidence": 0.9553078413009644}]}, {"text": "For four out of the seven corpora, error levels would already be close to human agreement error (at 1 \u2212 Accuracy =15% -recall Section 4).", "labels": [], "entities": [{"text": "error", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9917755126953125}, {"text": "human agreement error", "start_pos": 74, "end_pos": 95, "type": "METRIC", "confidence": 0.5897999902566274}, {"text": "Accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9906847476959229}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9217077493667603}]}, {"text": "This picture is close to single-grader performance in Section 5.1.", "labels": [], "entities": [{"text": "Section 5.1", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.8455670177936554}]}, {"text": "In the more complex multi-class case, this strategy reduces the manual grading effort to between 6% (CSSAG (9)) and 9%) of all items in the multi-class case.", "labels": [], "entities": []}, {"text": "Assuming that the    hand-assigned categories are always correct, remaining error would then range between 28% (ASAP (5)) and 45% (CSSAG).", "labels": [], "entities": [{"text": "error", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.9941977262496948}, {"text": "ASAP", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.5310415625572205}, {"text": "CSSAG", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.7703128457069397}]}, {"text": "More human effort is clearly needed to further reduce error inmost grading situations, even though grader workload has been greatly reduced over the singlegrader case (where 25-50% of predictions had to be reviewed) and remaining error also drops for ASAP and Mohler compared to using a single grader.", "labels": [], "entities": [{"text": "error", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9647934436798096}, {"text": "error", "start_pos": 230, "end_pos": 235, "type": "METRIC", "confidence": 0.9767988920211792}, {"text": "ASAP", "start_pos": 251, "end_pos": 255, "type": "DATASET", "confidence": 0.6705079078674316}]}, {"text": "Finding Errors implies that grades predicted in partial agreement are unreliable between 32% (CREE, CREG) and 43% (CSSAG (2), Mohler (2)) for the binary case and (at best) half of the time for the multiclass case.", "labels": [], "entities": [{"text": "CREG", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.7427842020988464}]}, {"text": "For comparison, grades predicted in full agreement are unreliable between 4 and 25% in the binary case and between 14 and 38% in the multiclass case.", "labels": [], "entities": []}, {"text": "Focusing on PartA predictions is therefore an efficient use of human effort.", "labels": [], "entities": [{"text": "PartA predictions", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.716239720582962}]}, {"text": "We can zoom in further on likely errors by concentrating on the categories that are most likely affected because the machine graders perform weakly on them.", "labels": [], "entities": []}, {"text": "For ASAP (5), machine grading performance is known to be worst for classes 0.33, 0.66 and 1 (see RF performance in).", "labels": [], "entities": [{"text": "ASAP", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.7990743517875671}]}, {"text": "60% of the erroneous PA predictions are in fact for those classes.", "labels": [], "entities": [{"text": "PA predictions", "start_pos": 21, "end_pos": 35, "type": "METRIC", "confidence": 0.9294483363628387}]}, {"text": "Reviewing all PartA cases for these categories, which makeup 16% of the total data set, and additionally checking all items where the machine graders disagree (7%) results in a reduction of manual grading effort of 77% of the items, while holding remaining error at 18%.", "labels": [], "entities": [{"text": "error", "start_pos": 257, "end_pos": 262, "type": "METRIC", "confidence": 0.8362110257148743}]}, {"text": "Remaining error can be further reduced to 11% by revising the PartA predictions for all classes instead of just the weakest classes.", "labels": [], "entities": [{"text": "Remaining error", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.7933453023433685}]}, {"text": "Humans still review only 39% of all answers in this case (corresponding to 61% of effort saved).", "labels": [], "entities": []}, {"text": "shows the remaining manual effort and error for all data sets for the PartA/weak strategy (revise cases of NA and those PartA categories that the RF grader is known to perform weakly on) as well as for the PartA/all strategy.", "labels": [], "entities": []}, {"text": "For the binary corpora, the predictions for the minority class are reviewed for the PartA/weak strategy.", "labels": [], "entities": []}, {"text": "Clearly, the same patterns hold across all data sets: For binary classification, just using the ensemble predictions reaches error at human levels for CREE and Mohler (recall, however, that these corpora are strongly biased towards the majority class).", "labels": [], "entities": [{"text": "binary classification", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.6744309961795807}]}, {"text": "When reviewing all PartA predictions, five out of seven corpora show remaining error levels below the observed human error level of 15%, and for four of these five, the error is even below 10% at a maximum of 28% of items reviewed.", "labels": [], "entities": [{"text": "human error level", "start_pos": 111, "end_pos": 128, "type": "METRIC", "confidence": 0.8425432443618774}]}, {"text": "In the harder multiclass case, two of the three corpora show human-level remaining error, but some more reviewing effort is needed (up to 60% of items, or 50% for CSSAG at 20% remaining error).", "labels": [], "entities": [{"text": "remaining error", "start_pos": 73, "end_pos": 88, "type": "METRIC", "confidence": 0.8256230354309082}]}, {"text": "This mirrors the complexity of the task, but is still a sizeable reduction.", "labels": [], "entities": []}, {"text": "Relaxing the Evaluation A second measure that helps save human effort is reconsidering the gravity of machine errors in the multiclass case: In repeated formative testing, a difference between actual and predicted grade of one grade step out of five (or even eleven) may not be of much consequence for the student.", "labels": [], "entities": []}, {"text": "To model this relaxed evaluation, we use the definitions from above for FullA, PartA and NA predictions.", "labels": [], "entities": [{"text": "FullA", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.8625580072402954}]}, {"text": "However, we now count a prediction as correct if it is within one grade step of the gold category.", "labels": [], "entities": []}, {"text": "We also apply the relaxed prediction matching to the reviewing recommendations: We now only review NA cases and those PartA cases where the predictions differ by more than one grade step (the majority prediction is accepted for the other PartA cases).", "labels": [], "entities": []}, {"text": "This relaxation is very relevant: 75% of PartA predictions for the ASAP (5) data and 72% for the Mohler (11) data differ within one grade step.", "labels": [], "entities": [{"text": "ASAP (5) data", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.7963449239730835}, {"text": "Mohler (11) data", "start_pos": 97, "end_pos": 113, "type": "DATASET", "confidence": 0.7312076449394226}]}, {"text": "For CSSAG (9), however, only four out of more than 800 PartA predictions are within one grade step of another.", "labels": [], "entities": []}, {"text": "This pattern of results can be explained by a tendency of the Decision Tree (DT) learner to predict the extreme categories.", "labels": [], "entities": []}, {"text": "Since the Mohler and ASAP data are biased towards those categories, all the learners show this pattern and predictions match closely.", "labels": [], "entities": [{"text": "ASAP data", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.6480706483125687}]}, {"text": "CSSAG has a bias towards the middle category as well as the top that SVM and RF reflect better than DT.", "labels": [], "entities": [{"text": "CSSAG", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8937404751777649}, {"text": "RF", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9432074427604675}]}, {"text": "Therefore, they may cast votes that differ strongly from the DT vote.", "labels": [], "entities": [{"text": "DT vote", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.7966189384460449}]}, {"text": "The results of relaxed evaluation and review are very encouraging for practical application: For ASAP, error drops to 7% when reviewing just 25% of the data.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 97, "end_pos": 101, "type": "TASK", "confidence": 0.899324357509613}, {"text": "error", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9989149570465088}]}, {"text": "This is at the human level observed for ASAP.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.9735653400421143}]}, {"text": "Previously, reviewing all PartA predictions in strict evaluation, 11% of error remained and 38% of data were reviewed.", "labels": [], "entities": [{"text": "PartA", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.7354732155799866}, {"text": "error", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.9824506640434265}]}, {"text": "For Mohler, 5% error remain after reviewing 38% of data (from 15% of error while reviewing 59% of data).", "labels": [], "entities": [{"text": "error", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9935842156410217}]}, {"text": "For CSSAG, there is of course no change.", "labels": [], "entities": [{"text": "CSSAG", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.7374871969223022}]}, {"text": "This pattern of results makes the approach very promising for formative assessment, where testing is frequent (causing high grader workload) and the individual test result can still be informative even if it is approximate.", "labels": [], "entities": [{"text": "formative assessment", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.9494084417819977}]}], "tableCaptions": [{"text": " Table 1: Corpus sizes and characteristics", "labels": [], "entities": []}, {"text": " Table 2: Weighted Precision (P) and Recall (R): Human-gold (H) and machine-gold (Random Forest, RF) perfor- mance for binary classification. Human-human Fleiss' \u03ba. n.a.: Single human annotation only.", "labels": [], "entities": [{"text": "Weighted Precision (P)", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8733530640602112}, {"text": "Recall (R)", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9454367011785507}, {"text": "Random Forest, RF", "start_pos": 82, "end_pos": 99, "type": "DATASET", "confidence": 0.8734768331050873}, {"text": "Fleiss'", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.9261224865913391}]}, {"text": " Table 3: Weighted P and R per category (binary classi- fication) and majority class (CREG is balanced by de- sign). RF classifier.", "labels": [], "entities": []}, {"text": " Table 4: CSSAG: Human-gold (H) and machine-gold (RF) P and R, human-human \u03ba values. -: No prediction  made.", "labels": [], "entities": []}, {"text": " Table 5: ASAP: Human-gold (H) and machine-gold  (RF) P and R. Human-human \u03ba values.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.47024106979370117}]}, {"text": " Table 6: Mohler: Human-gold (H) and machine-gold (RF) P and R. Human-human \u03ba values. n.a.: No prediction  made.", "labels": [], "entities": []}, {"text": " Table 7: Remaining effort (in % of items) and remaining error for all corpora following different review strategies.  NA only: Review no-agreement answers; PartA/weak: Revise PartA predictions of classes with weak classifier  performance; PartA/all: Revise all PartA predictions. Bold: Error at or below observed human agreement. -:  CREG has no majority class.", "labels": [], "entities": []}]}