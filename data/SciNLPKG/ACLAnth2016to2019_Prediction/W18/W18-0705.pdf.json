{"title": [{"text": "Integrating Predictions from Neural-Network Relation Classifiers into Coreference and Bridging Resolution", "labels": [], "entities": [{"text": "Coreference", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.9562225937843323}, {"text": "Bridging Resolution", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7007288932800293}]}], "abstractContent": [{"text": "Cases of coreference and bridging resolution often require knowledge about semantic relations between anaphors and antecedents.", "labels": [], "entities": [{"text": "coreference", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.969344437122345}, {"text": "bridging resolution", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.8442303836345673}]}, {"text": "We suggest state-of-the-art neural-network classi-fiers trained on relation benchmarks to predict and integrate likelihoods for relations.", "labels": [], "entities": []}, {"text": "Two experiments with representations differing in noise and complexity improve our bridging but not our coreference resolver.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 104, "end_pos": 124, "type": "TASK", "confidence": 0.8343510329723358}]}], "introductionContent": [{"text": "Noun phrase (NP) coreference resolution is the task of determining which noun phrases in a text or dialogue refer to the same discourse entities.", "labels": [], "entities": [{"text": "Noun phrase (NP) coreference resolution", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6635827847889492}]}, {"text": "The most difficult cases in NP coreference are those which require semantic knowledge to infer the relation between the anaphor and the antecedent, as in Example (1) where we need to know that Malaria is a disease.", "labels": [], "entities": [{"text": "NP coreference", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.9574886858463287}]}], "datasetContent": [{"text": "Data We based our experiments on the benchmark dataset for coreference resolution, the OntoNotes corpus).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.9689674079418182}, {"text": "OntoNotes corpus", "start_pos": 87, "end_pos": 103, "type": "DATASET", "confidence": 0.8663917779922485}]}, {"text": "For bridging, we used the ISNotes corpus, a small subset of OntoNotes annotated with information status.", "labels": [], "entities": [{"text": "ISNotes corpus", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.9117970764636993}]}, {"text": "In order to obtain candidate pairs for semantic relation prediction, we considered all heads of noun phrases in the OntoNotes corpus) and combined them with preceding heads of noun phrases in the same document.", "labels": [], "entities": [{"text": "semantic relation prediction", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.8176741003990173}, {"text": "OntoNotes corpus", "start_pos": 116, "end_pos": 132, "type": "DATASET", "confidence": 0.9315737187862396}]}, {"text": "Due to the different corpus sizes, the generally higher frequency of coreferent anaphors and the transitivity of the coreference relation, we obtained many more coreference pairs (65,113 unique pairs) than bridging pairs (633 in total, including 608 unique pairs).", "labels": [], "entities": []}, {"text": "Bridging resolver As there is no publicly available bridging resolver, we re-implemented the rule-based approach by.", "labels": [], "entities": [{"text": "Bridging", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9126690030097961}]}, {"text": "It contains eight rules which all propose anaphorantecedent pairs, independently of the other rules.", "labels": [], "entities": []}, {"text": "The rules are applied in order of their precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.998915433883667}]}, {"text": "Apart from information on the connectivity of two nouns, which is derived from counting how often two nouns appear in a noun 1 preposition noun 2 pattern in a large corpus, the tool does not contain information about general relations.", "labels": [], "entities": []}, {"text": "Coreference resolver We used the IMS HotCoref resolver) as a coreference resolver, because it allows an easy integration of new features.", "labels": [], "entities": [{"text": "Coreference resolver", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7857126891613007}, {"text": "IMS HotCoref resolver", "start_pos": 33, "end_pos": 54, "type": "DATASET", "confidence": 0.9191951950391134}, {"text": "coreference resolver", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.8897806406021118}]}, {"text": "While its performance is slightly worse than the state-of-the-art neural coreference resolver, the neural resolver relies on very few basic features and word embeddings, which already implicitly contain semantic relations.", "labels": [], "entities": []}, {"text": "Evaluation metrics For coreference resolution, we report the performance as CoNLL score, version 8.01).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.9749261140823364}, {"text": "CoNLL score", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.5291775465011597}]}, {"text": "For bridging resolution, we report performance in precision, recall and F1.", "labels": [], "entities": [{"text": "bridging resolution", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9121324121952057}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9997803568840027}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9996432065963745}, {"text": "F1", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9996762275695801}]}, {"text": "For bridging evaluation, we take coreference chains into account during the evaluation, i.e. the predicted antecedent is considered correct if it is in the same coreference chain as the gold antecedent.", "labels": [], "entities": []}, {"text": "We applied train-developmenttest splits, used the training and development set for optimisation, and report performance on the test set.", "labels": [], "entities": []}, {"text": "The first experiment had a few major shortcomings.", "labels": [], "entities": []}, {"text": "First, we did not have lemmatised vectors, and as a result, singular and plural forms of the same lemma had different values.", "labels": [], "entities": []}, {"text": "Sometimes, this led to the wrong analysis, cf. Example (8), where the singular and plural versions of novel make different predictions, and where a lemmatised version would have preferred the correct antecedent: Second, many proper nouns were assigned zero values, as they were not covered by our vector representations.", "labels": [], "entities": []}, {"text": "These pairs thus could not be used in the new rule.", "labels": [], "entities": []}, {"text": "Third, the relations in the benchmark dataset BLESS do not completely match our hypotheses.", "labels": [], "entities": [{"text": "benchmark dataset", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.6759891510009766}, {"text": "BLESS", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.8615570664405823}]}, {"text": "We thus designed a second experiment to overcome these shortcomings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. It can be seen that the model is skewed  towards the majority class (random), whereas in  particular the hypernym relation seems to be dif- ficult. Here we observed many false decision be- tween coord/hyper.", "labels": [], "entities": []}, {"text": " Table 1: Results of the intrinsic evaluation on BLESS  (without lexical overlap).", "labels": [], "entities": [{"text": "BLESS", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9625043869018555}]}, {"text": " Table 2: Average cosine similarities and relation clas- sifier probabilities for coreferent and bridging pairs in  comparison to other pairs of nouns, experiment 1.", "labels": [], "entities": []}, {"text": " Table 3: Correct and wrong bridging pairs which are found by the additional semantic rule, with and without  additional cosine threshold constraint (> 0.2).", "labels": [], "entities": []}, {"text": " Table 4: Effect of the cosine threshold constraint, for  the relation meronymy.", "labels": [], "entities": []}, {"text": " Table 5. In- terestingly, the performances with respect to the  individual relations differ strongly from the first  experiment. In this second experiment, with bal- anced relations, meronym and antonym are well- detected whereas random performs inferior.", "labels": [], "entities": []}, {"text": " Table 6: Average relation classifier probabilities and  cosine similarities for coreferent and bridging pairs in  comparison to other pairs of nouns, experiment 2.", "labels": [], "entities": []}]}