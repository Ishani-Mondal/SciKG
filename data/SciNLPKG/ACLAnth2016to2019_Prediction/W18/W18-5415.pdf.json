{"title": [{"text": "Interpretable Neural Architectures for Attributing an Ad's Performance to its Writing Style", "labels": [], "entities": []}], "abstractContent": [{"text": "How much does \"free shipping!\" help an ad-vertisement's ability to persuade?", "labels": [], "entities": []}, {"text": "This paper presents two methods for performance attri-bution: finding the degree to which an outcome can be attributed to parts of a text while controlling for potential confounders 1.", "labels": [], "entities": []}, {"text": "Both algorithms are based on interpreting the behaviors and parameters of trained neural networks.", "labels": [], "entities": []}, {"text": "One method uses a CNN to encode the text, an adversarial objective function to control for confounders, and projects its weights onto its activations to interpret the importance of each phrase towards each output class.", "labels": [], "entities": []}, {"text": "The other method leverages residualization to control for confounds and performs interpretation by aggregating over learned word vectors.", "labels": [], "entities": []}, {"text": "We demonstrate these algorithms' efficacy on 118,000 internet search advertisements and outcomes, finding language indicative of high and low click through rate (CTR) regardless of who the ad is by or what it is for.", "labels": [], "entities": [{"text": "click through rate (CTR)", "start_pos": 142, "end_pos": 166, "type": "METRIC", "confidence": 0.8749598264694214}]}, {"text": "Our results suggest the proposed algorithms are high performance and data efficient, able to glean actionable insights from fewer than 10,000 data points.", "labels": [], "entities": []}, {"text": "We find that quick, easy, and authoritative language is associated with success, while lackluster embellishment is related to failure.", "labels": [], "entities": []}, {"text": "These findings agree with the advertising industry's emperical wisdom, automatically revealing insights which previously required manual A/B testing to discover.", "labels": [], "entities": [{"text": "A/B testing", "start_pos": 137, "end_pos": 148, "type": "METRIC", "confidence": 0.847072035074234}]}], "introductionContent": [{"text": "A text's style can affect our cognitive responses and attitudes, thereby influencing behavior).", "labels": [], "entities": []}, {"text": "The predictive relationship between language and behavior has been well studied in applications of NLP to tasks like linking text to sales figures ( and voter preference.", "labels": [], "entities": []}, {"text": "In this paper, we are interested in interpreting rather than predicting the relationship between language and behavior.", "labels": [], "entities": []}, {"text": "We focus on a specific instance: the relationship between the way a search advertisement is written and internet user behavior as measured by click through rate (CTR).", "labels": [], "entities": [{"text": "click through rate (CTR)", "start_pos": 142, "end_pos": 166, "type": "METRIC", "confidence": 0.847308506568273}]}, {"text": "In this study CTR is the ratio of clicks to impressions over a 90-day period, i.e. the probability of a click, given the person saw the ad.", "labels": [], "entities": [{"text": "CTR", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.964593231678009}]}, {"text": "Our goal is to develop a method for performance attribution in textual advertisements: identifying lexical features (words, phrases, etc.) to which we can attribute the success (or failure) of a search ad, regardless of who created the advertisement or what it is selling.", "labels": [], "entities": []}, {"text": "Identifying linguistic features that are associated with various outcomes is a common activity among machine learning scientists and practitioners.", "labels": [], "entities": [{"text": "Identifying linguistic", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.877161294221878}]}, {"text": "Indeed, it is essential for developing transparent and interpretable machine learning NLP models.", "labels": [], "entities": []}, {"text": "However, the various forms of regression and association quantifiers like mutual information or log-odds ratio that are the de-facto standard for feature weighting and text attribution all have known drawbacks, largely related to problems of multicollinearity.", "labels": [], "entities": [{"text": "text attribution", "start_pos": 168, "end_pos": 184, "type": "TASK", "confidence": 0.7216090112924576}]}, {"text": "Furthermore, these prior methods of text attribution critically fail to disentangle the explanatory power of the text from that of confounding information which could also explain the outcome.", "labels": [], "entities": [{"text": "text attribution", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7211821526288986}]}, {"text": "For example, in movie reviews, the actors who star in a film are the most powerful predictors of box office success (.", "labels": [], "entities": []}, {"text": "However, these are words that the film's marketers can't change.", "labels": [], "entities": []}, {"text": "Likewise, the name of a well-known brand in an ad for shoes might boost its effectiveness, but if we attribute the ad's success to the brand terms, we are actually crediting the power of the brand, not necessarily an actionable writing strategy ().", "labels": [], "entities": []}, {"text": "There is an emerging line of work on text understanding for confound-controlled settings, but these methods are usually concerned with making causal inferences using text.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7594423592090607}]}, {"text": "They are limited to word-features and can only tell you whether a word is discriminative.", "labels": [], "entities": []}, {"text": "Attribution involves the more fine-grained problem of identifying discriminative subsequences of the text and being able to explain which level of the outcome these subsequences support.", "labels": [], "entities": [{"text": "Attribution", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9701412916183472}]}, {"text": "We present a pair of new algorithms for solving this problem.", "labels": [], "entities": []}, {"text": "Based on the Adversarial and Residualizing models of, these algorithms first train a machine learning model and then analyze the trained parameters on strategically chosen inputs to infer the most important features for each output class.", "labels": [], "entities": []}, {"text": "Our first algorithm encodes the text with a convolutional neural network (CNN) and proceeds to predict the outcome and adversarially predict the confounders.", "labels": [], "entities": []}, {"text": "We select attributional n-grams by projecting back the weights of the output layer onto the encoder's convolutional feature maps.", "labels": [], "entities": []}, {"text": "Our second algorithm uses a bag-of-words text representation and is trained to learn the part of the text's effect that the confounds cannot explain.", "labels": [], "entities": []}, {"text": "We get n-grams from this method by tracing back the contribution of each feature towards each outcome class.", "labels": [], "entities": []}, {"text": "We demonstrate these algorithms' efficacy by conducting attribution studies on high-and lowperforming search advertisements across three domains: real estate, job listings, and apparel.", "labels": [], "entities": []}, {"text": "We find the proposed algorithms lend importance to words that are more predictive and less confoundrelated than a variety of strong baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "We demonstrate the efficacy of the proposed algorithms on a dataset of internet advertisements.", "labels": [], "entities": []}, {"text": "In this setting our (T , Y , C) data triples consist of \u2022 T : the header text of sponsored search results in an internet search engine.", "labels": [], "entities": []}, {"text": "\u2022 Y : a binary categorical variable which indicates whether the corresponding advertisement was high-performing or lowperforming.", "labels": [], "entities": []}, {"text": "\u2022 C: a categorical variable which indicates the brand of the ad.", "labels": [], "entities": []}, {"text": "We use customer id and the hostname of the landing page the ad points to as a proxy for this.", "labels": [], "entities": []}, {"text": "We collect advertisements across three domains: apparel (16,000 advertisements), job listings (70,000), and real estate (32,000).", "labels": [], "entities": []}, {"text": "See section A for more details on these data.", "labels": [], "entities": []}, {"text": "We selected pairs of ads where both had the same landing page and targeting, but where one ad was in the 97.5 th CTR percentile (high-performing) and its counterpart was in the 2.5 th percentile (low-performing).", "labels": [], "entities": []}, {"text": "This implies that any performance differences maybe attributed to differences in their text.", "labels": [], "entities": []}, {"text": "We tokenized these data with Moses () and joined word-tokens into n-grams of size 1, 2, 3, and 4 for the n-gram portion of the study.", "labels": [], "entities": []}, {"text": "We implemented nonlinear models with the Tensorflow framework () and optimized using Adam () with a learning rate of 0.001.", "labels": [], "entities": []}, {"text": "We implemented linear models with the scikit learn package).", "labels": [], "entities": []}, {"text": "We evaluate each algorithm by selecting lexicons of size |L i | = 50.", "labels": [], "entities": []}, {"text": "We optimized the hyperparameters of all algorithms for each dataset.", "labels": [], "entities": []}, {"text": "Complete hyperparameter specifications are provided in the online supplementary materials; for the proposed DR and CA algorithms we set |e| to 8, 32, and 32 for the apparel, job listing, and real estate data, respectively.", "labels": [], "entities": []}, {"text": "Along with the Convolutional Adversarial Selector (CA) and Directed Residualization Selector (DR) of Section 3, we compare the following methods: Regression (R), Residualized Regressions (RR), Regression with Confound features (RC), and the Adversarial Selection (AS) algorithm of (, which selects words that are important fora confound-controlled prediction task by considering the attentional scores of an adversarially-trained RNN.", "labels": [], "entities": [{"text": "Residualized Regressions (RR)", "start_pos": 162, "end_pos": 191, "type": "METRIC", "confidence": 0.8151866793632507}]}, {"text": "We begin by investigating whether the proposed methods successfully discovered features that are simultaneously indicative of each CTR status and untangled from the confounding effects of brand.: Comparative performance over apparel advertisements.", "labels": [], "entities": []}, {"text": "I and I are inflated by an order of magnitude for readability.", "labels": [], "entities": []}, {"text": "On the apparel data, we find that the proposed algorithms select words that are often both the most influential on CTR (highest I) and are also the most strongly associated with their target outcome classes (highest \u00af lo).", "labels": [], "entities": [{"text": "CTR (highest I)", "start_pos": 115, "end_pos": 130, "type": "METRIC", "confidence": 0.7590443670749665}]}, {"text": "It is not surprising that the Adversarial Selector of (Pryzant et al., 2018) (AS) had low \u00af lo because the method is only capable of identifying discriminative features while controlling for confounds.", "labels": [], "entities": []}, {"text": "AS was also inconsistent in its ability to select words that are predictive of CTR while being unrelated to brand.", "labels": [], "entities": [{"text": "AS", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.5925146341323853}]}, {"text": "This maybe due to the instability of adversarial learning ( or the complex nonlinear relationship between the model's attention scores and final predictions.", "labels": [], "entities": []}, {"text": "On the job advertisements, the proposed DR algorithm performed the best, selecting words that were both more influential on CTR and more strongly associated with its target than  any other algorithm.", "labels": [], "entities": []}, {"text": "In general, I values were an order of magnitude larger for n-grams than unigrams, indicating that for job postings on the internet, phrases are more important than the individual words they are composed of.", "labels": [], "entities": []}, {"text": "This suggests job seekers may read advertisements more closely than internet shoppers, who are known to \"skim\" content and are thus more attuned to individual keywords).", "labels": [], "entities": []}, {"text": "For real estate, indicates that except for the case of weak unigrams, the proposed DR and CA algorithms can perform best.", "labels": [], "entities": []}, {"text": "In many cases, the regression-based approaches successfully selected words that are strongly related to each target outcome class ( \u00af lo was relatively high), but failed to choose words whose explanatory power exceeds that of the confounds (I was relatively low).", "labels": [], "entities": []}, {"text": "For a plain regression (R) this makes sense; there is no mechanism to control for confounders.", "labels": [], "entities": []}, {"text": "For the other regression-based approaches, this maybe due to the multicolinearity of confounders and text which is described in) as a fundamental weakness of these attribution algorithms.", "labels": [], "entities": []}, {"text": "Again, n-grams performed drastically better than unigrams, implying that phraseology may matter more than vocabulary to prospective home-  owners.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparative performance over apparel adver- tisements. I and I are inflated by an order of magni- tude for readability.", "labels": [], "entities": []}, {"text": " Table 2: Comparative performance over job postings.  I and I are inflated by an order of magnitude for the  unigram results only.", "labels": [], "entities": []}, {"text": " Table 3: Comparative performance over real estate ad- vertisements. I and I are inflated by an order of mag- nitude for the unigram results only.", "labels": [], "entities": []}]}