{"title": [{"text": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "labels": [], "entities": [{"text": "GLUE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.737927258014679}, {"text": "Natural Language Understanding", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.6971379717191061}]}], "abstractContent": [{"text": "Human ability to understand language is general , flexible, and robust.", "labels": [], "entities": []}, {"text": "In contrast, most NLU models above the word level are designed fora specific task and struggle with out-of-domain data.", "labels": [], "entities": []}, {"text": "If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains.", "labels": [], "entities": []}, {"text": "To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an on-line platform for evaluating and comparing models.", "labels": [], "entities": []}, {"text": "For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set.", "labels": [], "entities": []}, {"text": "GLUE thus favors models that can represent linguistic knowledge in away that facilitates sample-efficient learning and effective knowledge-transfer across tasks.", "labels": [], "entities": [{"text": "GLUE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9347240328788757}]}, {"text": "While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly.", "labels": [], "entities": []}, {"text": "We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique , as well as state-of-the-art sentence representation models.", "labels": [], "entities": []}, {"text": "The best models still achieve fairly low absolute scores.", "labels": [], "entities": [{"text": "absolute", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9808782339096069}]}, {"text": "Analysis with our diagnostic dataset yields similarly weak performance overall phenomena tested, with some exceptions.", "labels": [], "entities": []}, {"text": "The GLUE benchmark GLUE consists of nine English sentence understanding tasks covering abroad range of domains, data quantities, and difficulties.", "labels": [], "entities": [{"text": "GLUE benchmark GLUE", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.7547787527243296}, {"text": "English sentence understanding", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.6190329690774282}]}, {"text": "As the goal of GLUE is to spur development of generalizable NLU systems, we design the benchmark such that good performance should re", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Task descriptions and statistics. Bold de- notes tasks for which there is privately-held test  data. All tasks are binary classification, except  STS-B (regression) and MNLI (three classes).", "labels": [], "entities": [{"text": "MNLI", "start_pos": 179, "end_pos": 183, "type": "DATASET", "confidence": 0.723724901676178}]}, {"text": " Table 2: Baseline performance on the GLUE tasks. For MNLI, we report accuracy on the matched and  mismatched test sets. For MRPC and QQP, we report accuracy and F1. For STS-B, we report Pearson and  Spearman correlation. For CoLA, we report Matthews correlation (Matthews, 1975). For all other tasks  we report accuracy. All values are scaled by 100. A similar table is presented on the online platform.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.8246675133705139}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.995850682258606}, {"text": "MRPC", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.7393062710762024}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9987938404083252}, {"text": "F1", "start_pos": 162, "end_pos": 164, "type": "METRIC", "confidence": 0.9981197714805603}, {"text": "Pearson and  Spearman correlation", "start_pos": 187, "end_pos": 220, "type": "METRIC", "confidence": 0.8622371107339859}, {"text": "Matthews correlation", "start_pos": 242, "end_pos": 262, "type": "METRIC", "confidence": 0.8202355802059174}, {"text": "accuracy", "start_pos": 312, "end_pos": 320, "type": "METRIC", "confidence": 0.9989616870880127}]}]}