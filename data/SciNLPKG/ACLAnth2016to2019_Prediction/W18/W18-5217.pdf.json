{"title": [{"text": "Cross-Lingual Argumentative Relation Identification: from English to Portuguese", "labels": [], "entities": [{"text": "Cross-Lingual Argumentative Relation Identification", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.7127589955925941}]}], "abstractContent": [{"text": "Argument mining aims to detect and identify argument structures from textual resources.", "labels": [], "entities": [{"text": "Argument mining", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8388673365116119}]}, {"text": "In this paper, we aim to address the task of argumentative relation identification, a subtask of argument mining, for which several approaches have been recently proposed in a monolingual setting.", "labels": [], "entities": [{"text": "argumentative relation identification", "start_pos": 45, "end_pos": 82, "type": "TASK", "confidence": 0.6679004927476248}, {"text": "argument mining", "start_pos": 97, "end_pos": 112, "type": "TASK", "confidence": 0.733930304646492}]}, {"text": "To overcome the lack of annotated resources in less-resourced languages , we present the first attempt to address this subtask in a cross-lingual setting.", "labels": [], "entities": []}, {"text": "We compare two standard strategies for cross-language learning, namely: projection and direct-transfer.", "labels": [], "entities": []}, {"text": "Experimental results show that by using unsupervised language adaptation the proposed approaches perform at a competitive level when compared with fully-supervised in-language learning settings.", "labels": [], "entities": []}], "introductionContent": [{"text": "The aim of argument mining (AM) is the automatic detection and identification of argumentative structures contained within natural language text.", "labels": [], "entities": [{"text": "argument mining (AM)", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.8751699805259705}, {"text": "automatic detection and identification of argumentative structures contained within natural language text", "start_pos": 39, "end_pos": 144, "type": "TASK", "confidence": 0.8120964616537094}]}, {"text": "In general, arguments are justifiable positions where pieces of evidence (premises) are offered in support of a conclusion.", "labels": [], "entities": []}, {"text": "Most existing approaches to AM build upon supervised machine learning (ML) methods that learn to identify argumentative content from manually annotated examples.", "labels": [], "entities": [{"text": "AM", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9791441559791565}]}, {"text": "Building a corpus with reliably annotated arguments is a challenging and time-consuming task, due to its complexity ().", "labels": [], "entities": []}, {"text": "Consequently, training data for AM is scarce, in particular for less-resourced languages.", "labels": [], "entities": [{"text": "AM", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9869541525840759}]}, {"text": "To overcome the lack of annotated resources for AM in lessresourced languages, we explore cross-language learning approaches.", "labels": [], "entities": [{"text": "AM", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9710620641708374}]}, {"text": "The aim of cross-language learning is to develop ML techniques that exploit annotated resources in a source language to solve tasks in a target language.", "labels": [], "entities": [{"text": "ML", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9799181818962097}]}, {"text": "propose the first attempt to address the identification of argumentative components in a cross-language learning setting.", "labels": [], "entities": []}, {"text": "In this paper, we aim to employ existing state-of-theart cross-language learning techniques to address the task of argumentative relation identification, leveraging knowledge extracted from annotated corpora in English to address the task in a lessresourced language, such as Portuguese.", "labels": [], "entities": [{"text": "argumentative relation identification", "start_pos": 115, "end_pos": 152, "type": "TASK", "confidence": 0.6270494759082794}]}, {"text": "As it maybe costly to produce small amounts of training data in many different languages, we employ unsupervised language adaptation techniques, which do not require labeled data in the target language.", "labels": [], "entities": [{"text": "language adaptation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7475134134292603}]}, {"text": "The aim of argumentative relation identification, the last subtask of the AM process (, is to classify each argumentative discourse unit (ADU) pair as argumentatively related or not.", "labels": [], "entities": [{"text": "argumentative relation identification", "start_pos": 11, "end_pos": 48, "type": "TASK", "confidence": 0.6845148801803589}]}, {"text": "We assume that the subtask of text segmentation in ADUs is already solved (although no ADU classification is assumed).", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7041708528995514}, {"text": "ADU classification", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.6748203337192535}]}, {"text": "The task is formulated as a binary classification problem: given a tuple ADU s , ADU t , we aim to classify the relation from ADU s to ADU t as \"support\" (where ADU s plays the role of premise and ADU t plays the role of conclusion), or \"none\" (unrelated ADUs).", "labels": [], "entities": []}, {"text": "This is a consistent way of formulating the problem (i.e. the premise on the left and conclusion on the right side of the tuple), which is an important requirement for the learning process as the relation we aim to capture is a directional relation (i.e. ADU s supports/refutes ADU t and not on the way around).", "labels": [], "entities": []}, {"text": "We hypothesize that good semantic representations of text, capturing argumentative relations between ADUs, can be independent of the text language.", "labels": [], "entities": []}, {"text": "By capturing the semantics of such relations in a higher-level representation (through sentence encoding and aggregation techniques) that is agnostic of the input language, we believe that transfer learning) is feasible and, consequently, encouraging results can be obtained for less-resourced languages.", "labels": [], "entities": []}, {"text": "For that, we propose employing cross-language learning techniques, such as projection () and direct transfer).", "labels": [], "entities": []}, {"text": "We show promising results following the approach presented in this paper, by obtaining performance scores in an unsupervised cross-language setting that are competitive (and in some settings better) than fully-supervised in-language ML approaches.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first approach to consider the task of argumentative relation identification in a cross-lingual setting.", "labels": [], "entities": [{"text": "argumentative relation identification", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.7355096538861593}]}], "datasetContent": [{"text": "As shown in, the distribution of labels is skewed towards the \"none\" class.", "labels": [], "entities": []}, {"text": "In the presence of unbalanced datasets, ML algorithms tend to favor predictions of the majority class.", "labels": [], "entities": [{"text": "ML", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9790144562721252}]}, {"text": "Aiming to improve the results for the \"support\" label (minority class), we explore two widely used techniques to deal with unbalanced datasets in ML problems: random undersampling and cost-sensitive learning.", "labels": [], "entities": [{"text": "ML problems", "start_pos": 146, "end_pos": 157, "type": "TASK", "confidence": 0.93590047955513}]}, {"text": "Random undersampling consists of randomly removing examples from the majority class until a predefined number of examples, determined to obtain a balanced dataset in the end of process.", "labels": [], "entities": []}, {"text": "In cost-sensitive learning, each class is assigned a weight that works as a penalty cost.", "labels": [], "entities": []}, {"text": "Higher/lower costs are used for examples of the minority/majority class, respectively.", "labels": [], "entities": []}, {"text": "The ML model is then trained to minimize the total cost, which will become more sensitive to misclassification of examples in the minority class.", "labels": [], "entities": [{"text": "ML", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.7983386516571045}]}, {"text": "To determine the weight matrix for each class we follow the heuristic proposed by.", "labels": [], "entities": []}, {"text": "For all the experiments presented in the following sections, these techniques are only applied to the training set (random undersampling) or during the training phase (cost-sensitive learning).", "labels": [], "entities": []}, {"text": "In order to validate the main hypothesis proposed in this paper -that the proposed models can capture argumentative relations between ADUs at a semantic-level that is transferable across languages -we have run a set of in-language and cross-language experiments.", "labels": [], "entities": []}, {"text": "Our cross-language experiments use 80% of ADU pairs originally available in L S as training data and the remaining 20% as test data.", "labels": [], "entities": []}, {"text": "In order to tune the parameters of the model, we sample 10% of the training set as the validation data.", "labels": [], "entities": []}, {"text": "All splits of the datasets are made at the documentlevel (i.e., ADU pairs belonging to document Dare not spread in different partitions) and keeping the original distribution of labels in each partition (stratified splitting).", "labels": [], "entities": []}, {"text": "Then, the models are evaluated on the full dataset in L T without retraining (unsupervised language adaptation).", "labels": [], "entities": []}, {"text": "In-language experiments aim to establish baseline scores fora supervised ML system that can make use of annotated resources in L T . We perform 5-fold cross-validation for in-language experiments.", "labels": [], "entities": []}, {"text": "Final scores correspond to the sum of the confusion matrix from the test set predictions in each fold.", "labels": [], "entities": []}, {"text": "Following this procedure, we obtain final evaluation metrics for the full dataset in L T that are directly comparable with the scores reported on the full dataset for L T in cross-language experiments, as the evaluation scores are obtained from exactly the same data in both settings.", "labels": [], "entities": []}, {"text": "Cross-validation splits are also at the document-level and keep the original label distribution.", "labels": [], "entities": []}, {"text": "Since reporting single performance scores is insufficient to compare non-deterministic learning approaches, we report average scores of 10 runs with different random seeds.", "labels": [], "entities": []}, {"text": "Due to the unbalanced nature of the datasets, evaluation metrics reported in the experiments are average macro F1-scores overall 10 runs.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.8309363126754761}]}, {"text": "All models are trained using the Adam optimizer, using the default parameters suggested in the original paper, and cross-entropy loss function.", "labels": [], "entities": []}, {"text": "The activation function used in all the layers was ReLU ().", "labels": [], "entities": [{"text": "ReLU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9672045111656189}]}, {"text": "To find the best model in each run, we stop training once the accuracy on the validation set does not improve for 5 epochs (early-stop criterion) or 50 epochs are completed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9994877576828003}]}, {"text": "The batch size used in the experiments was set to 32 learning instances.", "labels": [], "entities": []}, {"text": "The dimension of the LSTM cell, used by some of the models, was set to 96 after hyperparameter tunning (we tried with 32, 64, 96 and 128).", "labels": [], "entities": []}, {"text": "Finally, to accelerate training, we set the maximum length for all ADUs to 50 tokens 7 . summarizes in-language results obtained for the Argumentative Essays corpus, which contains essays written in English.", "labels": [], "entities": [{"text": "Argumentative Essays corpus", "start_pos": 137, "end_pos": 164, "type": "DATASET", "confidence": 0.6153208315372467}]}], "tableCaptions": [{"text": " Table 2: Annotated examples extracted from the Argumentative Essays (EN)", "labels": [], "entities": [{"text": "Argumentative Essays (EN)", "start_pos": 48, "end_pos": 73, "type": "DATASET", "confidence": 0.8522878408432006}]}, {"text": " Table 3: In-Language Scores -Arg. Essays (EN). Bold  numbers indicate the highest score in the column.", "labels": [], "entities": []}, {"text": " Table 4: In and Cross-Language scores on the Portuguese (PT) corpus. Bold numbers indicate the highest score in  the column. = equal or above in-language scores. All metrics correspond to F1-scores.", "labels": [], "entities": [{"text": "Portuguese (PT) corpus", "start_pos": 46, "end_pos": 68, "type": "DATASET", "confidence": 0.6700527608394623}, {"text": "F1-scores", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9964385032653809}]}]}