{"title": [{"text": "IRISA at SMM4H 2018: Neural Network and Bagging for Tweet Classification", "labels": [], "entities": [{"text": "IRISA at SMM4H 2018", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.8443042486906052}, {"text": "Bagging for Tweet Classification", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.8711892813444138}]}], "abstractContent": [{"text": "This paper describes the systems developed by IRISA to participate to the four tasks of the SMM4H 2018 challenge.", "labels": [], "entities": [{"text": "IRISA", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.8239234089851379}, {"text": "SMM4H 2018 challenge", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.9217374126116434}]}, {"text": "For these tweet classification tasks, we adopt a common approach based on recurrent neural networks (BiLSTM).", "labels": [], "entities": [{"text": "tweet classification tasks", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.8459334174791971}]}, {"text": "Our main contributions are the use of certain features, the use of Bagging in order to deal with unbalanced datasets, and on the automatic selection of difficult examples.", "labels": [], "entities": [{"text": "Bagging", "start_pos": 67, "end_pos": 74, "type": "TASK", "confidence": 0.8021467328071594}]}, {"text": "These techniques allow us to reach 91.4, 46.5, 47.8, 85.0 as F1-scores for Tasks 1 to 4.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9995008707046509}]}], "introductionContent": [{"text": "IRISA has participated in the four tasks of the SMM4H challenge ().", "labels": [], "entities": [{"text": "IRISA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8761656284332275}, {"text": "SMM4H challenge", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.9222240149974823}]}, {"text": "Yet, we have focused on Task 2 and 3, which are the most challenging ones, in particular because they have unbalanced data.", "labels": [], "entities": []}, {"text": "Moreover, for Task 2, the three classes have very fuzzy boundaries, which makes some tweets difficult to classify even for humans.", "labels": [], "entities": []}, {"text": "Our main contribution is to rely on Bagging (Bootstrap Aggregating) in order to deal with this problem of unbalanced data.", "labels": [], "entities": [{"text": "Bagging", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.7485401630401611}]}], "datasetContent": [{"text": "Every manually annotated dataset may contain annotation errors or uncertain annotation due to the difficulty of the task.", "labels": [], "entities": []}, {"text": "In order to improve the classification performance of our system, we have tried to cleanup the training data by removing potential errors.", "labels": [], "entities": []}, {"text": "We have considered that the tweets to be removed are those incorrectly classified although it was part of the training data used to train the model.", "labels": [], "entities": []}, {"text": "More precisely, we proceed as follows: a model is trained on the complete training dataset; this model is then applied to predict the class of every example of this training dataset; the misclassified tweets are finally removed from the data; the cleaned dataset is then used to train the final model.", "labels": [], "entities": []}, {"text": "We have removed 234, 183 and 250 examples respectively for Tasks 1, 3 and 4.", "labels": [], "entities": []}, {"text": "For the Task 2, we have not observed improvement while removing difficult examples.", "labels": [], "entities": []}, {"text": "For each experiment the data is split into train set (80%) and dev set (20%).", "labels": [], "entities": []}, {"text": "Evaluation is performed with a 5-fold cross validation, except when using bagging techniques.", "labels": [], "entities": []}, {"text": "For the experiments with bagging, we train 20 models (with more models we do not get any improvement).", "labels": [], "entities": []}, {"text": "The description of all the submitted runs and the obtained results on the training data are given in.", "labels": [], "entities": []}, {"text": "The official results are given in.", "labels": [], "entities": []}, {"text": "The use of bagging techniques enables us to improve from 1.9 to 3.9 points the performance of our systems for Task 2 and Task 3.", "labels": [], "entities": []}, {"text": "The automatic cleaning of the datasets is also a reason fora light improvement for Task 1 and Task 4.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Description of the submitted runs and results  obtained on the training dataset.", "labels": [], "entities": []}, {"text": " Table 2. The use  of bagging techniques enables us to improve from  1.9 to 3.9 points the performance of our systems  for Task 2 and Task 3. The automatic cleaning of  the datasets is also a reason for a light improve- ment for Task 1 and Task 4.", "labels": [], "entities": []}]}