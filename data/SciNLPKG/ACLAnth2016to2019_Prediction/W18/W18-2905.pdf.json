{"title": [{"text": "Subcharacter Information in Japanese Embeddings: When Is It Worth It?", "labels": [], "entities": []}], "abstractContent": [{"text": "Languages with logographic writing systems present a difficulty for traditional character-level models.", "labels": [], "entities": []}, {"text": "Leveraging the subcharacter information was recently shown to be beneficial fora number of intrinsic and extrinsic tasks in Chinese.", "labels": [], "entities": []}, {"text": "We examine whether the same strategies could be applied for Japanese, and contribute anew analogy dataset for this language.", "labels": [], "entities": []}], "introductionContent": [{"text": "No matter how big a corpus is, there will always be rare and out-of-vocabulary (OOV) words, and they pose a problem for the widely used word embedding models such as word2vec.", "labels": [], "entities": []}, {"text": "A growing body of work on subword and character-level representations addresses this limitation in composing the representations for OOV words out of their parts.", "labels": [], "entities": []}, {"text": "However, logographic writing systems consist of thousands of characters, varying in frequency in different domains.", "labels": [], "entities": []}, {"text": "Fortunately, many Chinese characters (called kanji in Japanese) contain semantically meaningful components.", "labels": [], "entities": []}, {"text": "For example, \u6728 (a standalone kanji for the word tree) also occurs as a component in \u685c (sakura) and \u6749 (Japanese cypress).", "labels": [], "entities": []}, {"text": "We investigate the effect of explicit inclusion of kanjis and kanji components in the word embedding space on word similarity and word analogy tasks, as well as sentiment polarity classification.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 110, "end_pos": 125, "type": "TASK", "confidence": 0.7019010335206985}, {"text": "word analogy", "start_pos": 130, "end_pos": 142, "type": "TASK", "confidence": 0.747748076915741}, {"text": "sentiment polarity classification", "start_pos": 161, "end_pos": 194, "type": "TASK", "confidence": 0.832414468129476}]}, {"text": "We show that the positive results reported for Chinese carryover to Japanese only partially, that the gains are not stable, and in many cases character ngrams perform better than character-level models.", "labels": [], "entities": []}, {"text": "We also contribute anew large dataset for word analogies, the first one for this relatively lowresourced language, and a tokenizer-friendly version of its only similarity dataset.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present jBATS 9 , anew analogy dataset for Japanese that is comparable to BATS (, currently the largest analogy dataset for English.", "labels": [], "entities": [{"text": "BATS", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.8918952345848083}]}, {"text": "Like BATS, jBATS covers 40 linguistic relations which are listed in.", "labels": [], "entities": []}, {"text": "There are 4 types of relations: inflectional and derivational morphology, and encyclopedic and lexicographic semantics.", "labels": [], "entities": []}, {"text": "Each type has 10 categories, with 50 word pairs per category (except for E03 which has 47 pairs, since there are only 47 prefectures).", "labels": [], "entities": []}, {"text": "This enables generation of 97,712 analogy questions.", "labels": [], "entities": []}, {"text": "The inflectional morphology set is based on the traditional Japanese grammar which lists 7 different forms of godan, shimoichidan and kamiichidan verbs, as well as 5 forms of i-adjectives.", "labels": [], "entities": []}, {"text": "Including the past tense form, there allelize training.", "labels": [], "entities": []}, {"text": "8 http://vecto.space/data/embeddings/ja 9 http://vecto.space/projects/jBATS are 8 and 6 forms for verbs and adjectives respectively.", "labels": [], "entities": []}, {"text": "All categories were adjusted to the MeCab tokenization.", "labels": [], "entities": [{"text": "MeCab tokenization", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.8891212046146393}]}, {"text": "After excluding redundant or rare forms there were 5 distinctive forms for verbs and 3 for adjectives, which were paired to form 7 verb and 3 adjective categories.", "labels": [], "entities": []}, {"text": "The derivational morphology set includes 9 highly productive affixes which are usually represented by a single kanji character, and a set of pairs of transitive and intransitive verbs which are formed with several infix patterns.", "labels": [], "entities": []}, {"text": "The encyclopedic and lexicographic semantics sections were designed similarly to BATS), but adjusted for Japanese.", "labels": [], "entities": [{"text": "BATS", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.6582111120223999}]}, {"text": "For example, UK counties were replaced with Japanese prefectures.", "labels": [], "entities": []}, {"text": "The E09 animal-young category of BATS would be rendered with a prefix in Japanese, and was replaced with plain: honorific word pairs, a concept highly relevant for the Japanese culture.", "labels": [], "entities": [{"text": "BATS", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.4943172335624695}]}, {"text": "All tokens were chosen based on their frequencies in BCCWJ 10, the Balanced Corpus of Contemporary Written Japanese, and the Mainichi newspaper corpus described in Section 3.", "labels": [], "entities": [{"text": "BCCWJ 10", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.9210337996482849}, {"text": "Balanced Corpus of Contemporary Written Japanese", "start_pos": 67, "end_pos": 115, "type": "DATASET", "confidence": 0.8658923109372457}, {"text": "Mainichi newspaper corpus", "start_pos": 125, "end_pos": 150, "type": "DATASET", "confidence": 0.9841002225875854}]}, {"text": "We aimed to choose relatively frequent and not genre-specific words.", "labels": [], "entities": []}, {"text": "For broader categories (adjectives and verbs) we balanced between BCCWJ and Mainichi corpora, choosing items of mean frequencies between 3,000 and 100,000 whenever possible.", "labels": [], "entities": [{"text": "BCCWJ", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.8743703961372375}, {"text": "Mainichi corpora", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.8929840922355652}]}], "tableCaptions": [{"text": " Table 2: The size of the original and modified  Japanese similarity datasets (in word pairs)", "labels": [], "entities": [{"text": "Japanese similarity datasets", "start_pos": 49, "end_pos": 77, "type": "DATASET", "confidence": 0.5801547467708588}]}, {"text": " Table 3: Spearman's correlation with human similarity judgements. Boldface indicates the highest result  on a given corpus (separately for in-vocabulary and OOV conditions). Shaded numbers indicate the  highest result among the three Skip-Gram models.", "labels": [], "entities": [{"text": "human similarity judgements", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.6217314104239146}]}, {"text": " Table 4: Word analogy task accuracy (LRCos).  Boldface indicates the highest result for a corpus,  and the shaded numbers indicate the highest result  among three Skip-Gram models.", "labels": [], "entities": [{"text": "Word analogy task accuracy (LRCos)", "start_pos": 10, "end_pos": 44, "type": "METRIC", "confidence": 0.7806472991194043}]}, {"text": " Table 6: Sentiment analysis accuracy", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9792708456516266}]}]}