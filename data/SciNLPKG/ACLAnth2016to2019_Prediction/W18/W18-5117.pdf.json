{"title": [{"text": "Cross-Domain Detection of Abusive Language Online", "labels": [], "entities": [{"text": "Cross-Domain Detection of Abusive Language", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.880863606929779}]}], "abstractContent": [{"text": "We investigate to what extent the models trained to detect general abusive language generalize between different datasets labeled with different abusive language types.", "labels": [], "entities": [{"text": "general abusive language generalize", "start_pos": 59, "end_pos": 94, "type": "TASK", "confidence": 0.6268275231122971}]}, {"text": "To this end, we compare the cross-domain performance of simple classification models on nine different datasets, finding that the models fail to generalize to out-domain datasets and that having at least some in-domain data is important.", "labels": [], "entities": []}, {"text": "We also show that using the frustratingly simple domain adaptation (Daume III, 2007) inmost cases improves the results over in-domain training, especially when used to augment a smaller dataset with a larger one.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abusive language online ( is an increasing problem in modern society.", "labels": [], "entities": [{"text": "Abusive language online", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8824125528335571}]}, {"text": "Although abusive language is undoubtedly not anew phenomenon inhuman communication, the rise of the internet has made it concerningly prevalent.", "labels": [], "entities": []}, {"text": "The main reason behind this is the cloak of relative anonymity offered when commenting online, which lowers the inhibitions of individuals prone to abusive language and removes some of the social mechanisms present in real life that serve to protect potential victims.", "labels": [], "entities": []}, {"text": "Moreover, this type of psychological violence can occur at anytime and regardless of the physical distance between the persons involved.", "labels": [], "entities": []}, {"text": "While abusive language online can probably never be weeded out entirely, its effect can certainly be lessened by locating abusive posts and removing them before they cause too much harm.", "labels": [], "entities": []}, {"text": "Training supervised machine learning models to recognize abusive texts and alert human moderators can make this process much more efficient.", "labels": [], "entities": []}, {"text": "However, retaining humans in the loop is crucial, since blindly relying on model predictions would in effect turn every false positive prediction into infringement of free speech.", "labels": [], "entities": []}, {"text": "This would defeat the initial purpose of using machine learning models to facilitate a free and civilized online discussions.", "labels": [], "entities": []}, {"text": "Detecting abusive language online is a subject of much ongoing research in the NLP community.", "labels": [], "entities": [{"text": "Detecting abusive language online", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.9206720143556595}]}, {"text": "Different studies have zeroed in on different types of abusive language (e.g., aggressive language, toxic language, hate speech) and have yielded a number of different datasets collected from various domains (e.g., news, Twitter, Wikipedia).", "labels": [], "entities": []}, {"text": "However, from a practical perspective -if one simply wishes to build a classifier for detecting general abusive language in a given domain -the question arises as to which of these datasets to use for training.", "labels": [], "entities": []}, {"text": "More generally, the question is to what extent abusive language detection transfers across domains, and how much, if anything, can be gained from a simple domain adaptation technique that combines the source and the target domain.", "labels": [], "entities": [{"text": "language detection", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7391177713871002}]}, {"text": "This paper investigates the question to what extent abusive language detection can benefit from combining training sets and sharing information between them through domain adaptation techniques.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.6384583413600922}]}, {"text": "First, we compare the cross-domain performance of simple classification models on nine different English datasets of abusive language.", "labels": [], "entities": []}, {"text": "Second, we explore whether the framework of frustratingly simple domain adaptation (FEDA) can be applied to improve classifier performance, in particular for smaller data sets.", "labels": [], "entities": [{"text": "domain adaptation (FEDA)", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.6471304833889008}]}, {"text": "In addition, we show how a simple post-hoc feature analysis can reveal which features are specific to a certain domain and which are shared between two domains.", "labels": [], "entities": []}, {"text": "We make our code and links to the used datasets available online.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our study we use nine publicly available datasets in English; summarizes their main characteristics.", "labels": [], "entities": []}, {"text": "For reasons of efficiency and comparability, we use a fixed split on each of the datasets into a train, development, and test portions.", "labels": [], "entities": []}, {"text": "We respected the official splits where they were provided.", "labels": [], "entities": []}, {"text": "As we are interested in detecting the presence of general abusive language, rather than in discerning among its many subtypes, we binarize the labels on all datasets into positive (abusive language) and negative (not abusive language).", "labels": [], "entities": []}, {"text": "We do this by labeling all classes typeset in bold in as positive and all other classes as negative.", "labels": [], "entities": []}, {"text": "There are two exceptions to this rule.", "labels": [], "entities": []}, {"text": "First, on the Kol dataset, we consider as positive those examples for which at least one annotator gave a rating higher than 1.", "labels": [], "entities": [{"text": "Kol dataset", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.9287064075469971}]}, {"text": "Second, on the Kaggle dataset, which uses a multilabeling scheme, we consider as positive all instances annotated with at least one of the six harmful labels, and as negative all instances without any labels.", "labels": [], "entities": [{"text": "Kaggle dataset", "start_pos": 15, "end_pos": 29, "type": "DATASET", "confidence": 0.9800639152526855}]}, {"text": "We perform only the very basic preprocessing by lowercasing all words and lemmatizing them using NTLK.", "labels": [], "entities": []}, {"text": "While these modifications to original datasets make a comparison to previous work difficult, they allow a direct comparison across the datasets and a straightforward application of FEDA.", "labels": [], "entities": [{"text": "FEDA", "start_pos": 181, "end_pos": 185, "type": "DATASET", "confidence": 0.5434404611587524}]}, {"text": "We investigate the potential of applying domain adaptation to augment the original domain with the information from a different domain.", "labels": [], "entities": []}, {"text": "To this end, we employ the FEDA framework, which works by copying features several times to account for different domains, allowing the model to learn domain-dependent weights for each feature.", "labels": [], "entities": [{"text": "FEDA", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.561013400554657}]}, {"text": "Let the dataset from the original domain be denoted as O and the data set from an augmentation domain as A.", "labels": [], "entities": []}, {"text": "We generate a joint train set as a union of train sets of O and A by keeping three copies of each feature: (1) a general copy, which is unaltered for instances from both domains, (2) an O-specific copy, which is set to 0 for all instances not from O, and (3) an A-specific copy, which is set to 0 for all instances not from A.", "labels": [], "entities": []}, {"text": "In the same: FEDA domain adaptation results.", "labels": [], "entities": [{"text": "FEDA domain adaptation", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.6763266126314799}]}, {"text": "Rows correspond to original datasets and columns to augmentation datasets.", "labels": [], "entities": []}, {"text": "The best performance for each original dataset (row) is shown in bold.", "labels": [], "entities": []}, {"text": "\"*\" indicates statistical significance at significance level \u03b1 = 0.05 against the \"None\" column, which is equivalent to the diagonal of way we generate joint development and test sets.", "labels": [], "entities": [{"text": "statistical significance", "start_pos": 14, "end_pos": 38, "type": "METRIC", "confidence": 0.8520942032337189}, {"text": "significance level \u03b1", "start_pos": 42, "end_pos": 62, "type": "METRIC", "confidence": 0.89921106894811}, {"text": "None\" column", "start_pos": 83, "end_pos": 95, "type": "METRIC", "confidence": 0.952302078406016}]}, {"text": "The intuition behind why this effectively leads to domain adaptation is that it allows the underlying machine learning model to differentiate features (words) that are generally useful from those that are useful in only one of the domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7525222599506378}]}, {"text": "Consequently, it can better learn the similarities and differences of the domains and how to exploit them to maximize performance.", "labels": [], "entities": []}, {"text": "For example, a word such as moron is almost universally abusive in all domains and would generalize well.", "labels": [], "entities": []}, {"text": "On the other hand, a word like fruit is almost always completely non-abusive except in specific domains where it might denote a derogatory slang fora homosexual person.", "labels": [], "entities": []}, {"text": "As before, the SVM is trained on the joint training set, with model selection on the joint development set.", "labels": [], "entities": [{"text": "SVM", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.7923153042793274}]}, {"text": "The model is then trained using optimal hyperparameters on the union of joint training and joint development set and applied to the joint test set.", "labels": [], "entities": []}, {"text": "Note that the joint test set contains test instances from both O and A.", "labels": [], "entities": []}, {"text": "We evaluate the model only on the test instances from O, as the goal is to determine whether augmentation with A improves performance on the dataset from the original domain O.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Nine abusive language datasets: the source, the number of instances in the train, development, and test  set, positive instance base rate (BR), and label sets. We mapped the boldface labels to the positive label.", "labels": [], "entities": [{"text": "positive instance base rate (BR)", "start_pos": 120, "end_pos": 152, "type": "METRIC", "confidence": 0.8024344784872872}]}, {"text": " Table 2: Results of cross-domain model performance. Rows are the training datasets and columns are the test  datasets. The best performance for each test set (column) is shown in bold. \"*\" indicates statistical significance at  significance level \u03b1 = 0.05 with respect to the diagonal cell.", "labels": [], "entities": []}, {"text": " Table 3: FEDA domain adaptation results. Rows correspond to original datasets and columns to augmentation  datasets. The best performance for each original dataset (row) is shown in bold. \"*\" indicates statistical significance  at significance level \u03b1 = 0.05 against the \"None\" column, which is equivalent to the diagonal of", "labels": [], "entities": [{"text": "FEDA domain adaptation", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.652386744817098}]}]}