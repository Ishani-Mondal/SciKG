{"title": [{"text": "LIUM-CVC Submissions for WMT18 Multimodal Translation Task", "labels": [], "entities": [{"text": "WMT18 Multimodal Translation", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.6269090374310812}]}], "abstractContent": [{"text": "This paper describes the multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT18 Shared Task on Multimodal Translation.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.6018433074156443}, {"text": "CVC", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.802297830581665}, {"text": "WMT18 Shared Task on Multimodal Translation", "start_pos": 101, "end_pos": 144, "type": "TASK", "confidence": 0.6032955894867579}]}, {"text": "This year we propose several modifications to our previous multimodal attention architecture in order to better integrate convolutional features and refine them using encoder-side information.", "labels": [], "entities": []}, {"text": "Our final constrained submissions ranked first for English\u2192French and second for English\u2192German language pairs among the constrained submissions according to the automatic evaluation metric METEOR.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.549386739730835}]}], "introductionContent": [{"text": "In this paper, we present the neural machine translation (NMT) and multimodal NMT (MMT) systems developed by LIUM and CVC for the third edition of the shared task.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.7956716219584147}, {"text": "CVC", "start_pos": 118, "end_pos": 121, "type": "DATASET", "confidence": 0.8198664784431458}]}, {"text": "Several lines of work have been conducted since the introduction of the shared task on MMT in 2016 ( . The majority of last year submissions including ours) were based on the integration of global visual features into various parts of the NMT architecture ( . Apart from these, hierarchical multimodal attention (Helcl and Libovick\u00b4yLibovick\u00b4y, 2017) and multi-task learning were also explored by the participants.", "labels": [], "entities": []}, {"text": "This year we decided to revisit the multimodal attention) since our previous observations about qualitative analysis of the visual attention was not satisfying.", "labels": [], "entities": []}, {"text": "In order to improve the multimodal attention both qualitatively and quantitatively, we experiment with several refinements to it: first, we try to use different input image sizes prior to feature extraction and second we normalize the final convolutional feature maps to assess its impact on the final MMT performance.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 188, "end_pos": 206, "type": "TASK", "confidence": 0.6994116008281708}, {"text": "MMT", "start_pos": 302, "end_pos": 305, "type": "TASK", "confidence": 0.9775940179824829}]}, {"text": "In terms of architecture, we propose to refine the visual features by learning an encoderguided early spatial attention.", "labels": [], "entities": []}, {"text": "In overall, we find that normalizing feature maps is crucial for the multimodal attention to obtain a comparable performance to monomodal NMT while the impact of the input image size remains unclear.", "labels": [], "entities": []}, {"text": "Finally, with the help of the refined attention, we obtain modest improvements in terms of BLEU () and METEOR.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.999535322189331}, {"text": "METEOR", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.993415355682373}]}, {"text": "The paper is organized as follows: data preprocessing, model details and training hyperparameters are detailed respectively in section 2 and section 3.", "labels": [], "entities": []}, {"text": "The results based on automatic evaluation metrics are reported in section 4.", "labels": [], "entities": []}, {"text": "Finally the paper ends with a conclusion in section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Impact of L 2 normalization on the perfor- mance of multimodal attention.", "labels": [], "entities": []}, {"text": " Table 2: Impact of input image width on the perfor- mance of multimodal attention variants.", "labels": [], "entities": []}, {"text": " Table 3: EN\u2192DE results: Filtered attention is statistically different than the NMT (p \u2264 0.02).", "labels": [], "entities": [{"text": "DE", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.8674409985542297}]}, {"text": " Table 5: Official test2018 results ( \u2020: Unconstrained,  : Constrained.)", "labels": [], "entities": [{"text": "Official test2018", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.7493965327739716}, {"text": "Constrained", "start_pos": 59, "end_pos": 70, "type": "METRIC", "confidence": 0.961909830570221}]}]}