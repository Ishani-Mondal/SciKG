{"title": [{"text": "Computationally efficient discrimination between language varieties with large feature vectors and regularized classifiers", "labels": [], "entities": []}], "abstractContent": [{"text": "The present contribution revolves around efficient approaches to language classification which have been field-tested in the Vardial evaluation campaign.", "labels": [], "entities": [{"text": "language classification", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.7286728471517563}, {"text": "Vardial evaluation campaign", "start_pos": 125, "end_pos": 152, "type": "DATASET", "confidence": 0.8998586734135946}]}, {"text": "The methods used in several language identification tasks comprising different language types are presented and their results are discussed, giving insights on real-world application of regularization, linear classifiers and corresponding linguistic features.", "labels": [], "entities": [{"text": "language identification tasks", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.8022787769635519}]}, {"text": "The use of a specially adapted Ridge classifier proved useful in 2 tasks out of 3.", "labels": [], "entities": [{"text": "Ridge classifier", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.6511641889810562}]}, {"text": "The overall approach (XAC) has slightly outperformed most of the other systems on the DFS task (Dutch and Flemish) and on the ILI task (Indo-Aryan languages), while its comparative performance was poorer in on the GDI task (Swiss German dialects).", "labels": [], "entities": []}], "introductionContent": [{"text": "Language identification is the task of predicting the language(s) that a given document is written in.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6920889616012573}]}, {"text": "It can be seen as a text categorization task in which documents are assigned to pre-existing categories.", "labels": [], "entities": []}, {"text": "This research field has found renewed interest in the 1990s due to advances in statistical approaches and it has been active ever since, particularly since the methods developed have also been deemed relevant for text categorization, native language identification, authorship attribution, text-based geolocation, and dialectal studies.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 213, "end_pos": 232, "type": "TASK", "confidence": 0.7603479027748108}, {"text": "native language identification", "start_pos": 234, "end_pos": 264, "type": "TASK", "confidence": 0.6285703082879385}, {"text": "authorship attribution", "start_pos": 266, "end_pos": 288, "type": "TASK", "confidence": 0.7243764251470566}]}, {"text": "As of 2014 and the first Discriminating between Similar Languages (DSL) shared task, a unified dataset ( ) comprising news texts of closely-related language varieties has been used to test and benchmark systems.", "labels": [], "entities": [{"text": "Discriminating between Similar Languages (DSL) shared task", "start_pos": 25, "end_pos": 83, "type": "TASK", "confidence": 0.7012292544047037}]}, {"text": "The instances to be classified are quite short and may even be difficult to distinguish for human annotators, thus adding to the difficulty and the interest of the task.", "labels": [], "entities": []}, {"text": "An analysis of recent developments can be found in, in the reports on previous shared tasks as well as in a recently published survey on language and dialect identification.", "labels": [], "entities": [{"text": "language and dialect identification", "start_pos": 137, "end_pos": 172, "type": "TASK", "confidence": 0.644457072019577}]}, {"text": "In previous editions the shared tasks organized at VarDial included dialects of Arabic, German, and the DSL shared task which featured similar languages and language varieties ().", "labels": [], "entities": [{"text": "VarDial", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.924910306930542}]}, {"text": "Other related shared tasks are the MGB challenge on Arabic ( , the PAN lab on author profiling which included dialects and language varieties (, and the TweetLID shared task which included similar languages (.", "labels": [], "entities": [{"text": "MGB challenge", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.7118300199508667}]}, {"text": "As in previous tasks, the number of instances is limited in size, the training and test sets are on the order of magnitude of thousands or tens of thousands of instances, the latter being fairly small in size, with at most a sentence each time.", "labels": [], "entities": []}, {"text": "As a consequence, a classifier constructed on small training sets maybe biased and have a large variance, as the classifier parameters (coefficients) are poorly estimated.", "labels": [], "entities": []}, {"text": "It is also unstable, as small changes in the training set may cause large changes in the classifier.", "labels": [], "entities": []}, {"text": "A key component of winning systems is their capacity to make correct predictions on unseen data based on the trained model.", "labels": [], "entities": []}, {"text": "In this context, it has been shown that more conventional statistical methods can very well be more accurate than latest machine learning approaches, resulting in a paradox common to a fair number of applications: \"Knowing that a certain sophisticated method is not as accurate as a much simpler one is upsetting from a scientific point of view as the former requires a great deal of academic expertise and ample computer time to be applied.\")", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: in section 2 the preprocessing and feature extraction steps are presented, the classifiers follow in section 3, and three systems are then evaluated and discussed in section 4.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.670156717300415}]}], "datasetContent": [{"text": "The results for the DFS task are summarized in.", "labels": [], "entities": [{"text": "DFS task", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.8067699074745178}]}, {"text": "The chosen n-gram window was 2 to 6 characters, the feature extraction and classification processes have been conducted as described above.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7138510644435883}]}, {"text": "The classification methods used yield relatively low improvements with respect to the Bayesline, which shows this is a challenging task.", "labels": [], "entities": [{"text": "Bayesline", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.858761727809906}]}, {"text": "The multinomial Naive Bayes (run 1) indeed outperformed the SGD classifier (run 2) although the latter had been optimized during training by using parameter tuning.", "labels": [], "entities": []}, {"text": "The Ridge classifier was significantly more robust during training (higher cross-validation scores) and effectively reached a slightly higher score in the test run.", "labels": [], "entities": []}, {"text": "This submission has been ranked 3rd out of 7.", "labels": [], "entities": []}, {"text": "The confusion matrix shown in depicts a rather balanced mix of errors.", "labels": [], "entities": []}, {"text": "The same method has been tested with the same data preprocessing in the identification of Swiss German varieties (GDI shared task), using only the training data from this year's edition.", "labels": [], "entities": [{"text": "identification of Swiss German varieties", "start_pos": 72, "end_pos": 112, "type": "TASK", "confidence": 0.6649350047111511}]}, {"text": "Contrarily to the other tasks, a larger N-Gram span has been selected (1 to 6) as this seemed to carry a little more useful information.", "labels": [], "entities": []}, {"text": "The classifiers have been optimized during training in a similar way.", "labels": [], "entities": []}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "This task has seen the worst result compared to the other teams, with the best submission ranked 3rd out of 4.", "labels": [], "entities": []}, {"text": "This result is inline with the fact that the Naive Bayes classifier outperformed more complex methods, which shows they could not gain more fine-grained information or generalize better on the test data.", "labels": [], "entities": []}, {"text": "As in the last edition of the shared task, the classification of the Lucerne variant is more problematic than the other ones as shown in, however the Ridge classifier is more robust so that the gap is closing.", "labels": [], "entities": []}, {"text": "The F1 score is significantly higher than in the previous competition -0.634 against 0.606.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9730941653251648}]}, {"text": "The improvements can be explained by potentially cleaner data, focus on F1 instead of accuracy during model selection and parameter tuning, whereas the comparatively low score can be explained by the nature of the occurrences to discriminate: they are much shorter and less regular than in the other tasks, orthographic normalization is a potential problem and the instances seem to be closer to speech corpora.", "labels": [], "entities": [{"text": "F1", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9994170665740967}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.998625636100769}, {"text": "parameter tuning", "start_pos": 122, "end_pos": 138, "type": "TASK", "confidence": 0.7145285755395889}, {"text": "orthographic normalization", "start_pos": 307, "end_pos": 333, "type": "TASK", "confidence": 0.7050394713878632}]}, {"text": "Additionally, they had already been tokenized, so that the tokenization during   Last, the Indo-Aryan varieties have seen the Naive Bayes classifier lead to a significantly lower score than the other methods, which highlights their comparatively good performance and also explains why the best submission (Ridge classifier) has been ranked 2nd out of 6 in the competition, which demonstrates empirically that the processing chain described here also works well for other alphabets.", "labels": [], "entities": []}, {"text": "The results are summarized in, the finer discrimination using regularized classifiers can be explained by the morphological characteristics of Indo-Aryan languages and the subsequent necessity to better assess the statistical significance of the extracted n-grams.", "labels": [], "entities": []}, {"text": "The confusion matrix in highlights the difficulty of the AWA variety, which has the worst recall whereas the BRA variety seems to be easier to discriminate from the rest.", "labels": [], "entities": [{"text": "AWA variety", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.7832031548023224}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9991781115531921}, {"text": "BRA", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.7371461987495422}]}], "tableCaptions": [{"text": " Table 1: Results for DFS task, all classifiers used the same data preparation pipeline", "labels": [], "entities": [{"text": "DFS task", "start_pos": 22, "end_pos": 30, "type": "TASK", "confidence": 0.6222764849662781}]}, {"text": " Table 2: Results for GDI task, all classifiers used the same data preparation pipeline", "labels": [], "entities": []}, {"text": " Table 3: Results for the ILI task, all classifiers used the same data preparation pipeline", "labels": [], "entities": [{"text": "ILI task", "start_pos": 26, "end_pos": 34, "type": "TASK", "confidence": 0.689229428768158}]}]}