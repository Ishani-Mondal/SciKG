{"title": [{"text": "Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation", "labels": [], "entities": [{"text": "Texar", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9771535396575928}, {"text": "Text Generation", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.8161228597164154}]}], "abstractContent": [{"text": "We introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks.", "labels": [], "entities": [{"text": "Texar", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.9148910045623779}, {"text": "text generation tasks", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.816421627998352}]}, {"text": "Different from many existing toolkits that are specialized for specific applications (e.g., neural machine translation), Texar is designed to be highly flexible and versatile.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.6883501609166464}, {"text": "Texar", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.8537899255752563}]}, {"text": "This is achieved by abstracting the common patterns underlying the diverse tasks and methodolo-gies, creating a library of highly reusable modules and functionalities, and enabling arbitrary model architectures and various algorithmic paradigms.", "labels": [], "entities": []}, {"text": "The features make Texar particularly suitable for technique sharing and generalization across different text generation applications.", "labels": [], "entities": [{"text": "Texar", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.7057136297225952}, {"text": "technique sharing", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.72706738114357}]}, {"text": "The toolkit emphasizes heavily on extensibil-ity and modularized system design, so that components can be freely plugged in or swapped out.", "labels": [], "entities": []}, {"text": "We conduct extensive experiments and case studies to demonstrate the use and advantage of the toolkit.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text generation spans abroad set of natural language processing tasks that aim at generating natural language from input data or machine representations.", "labels": [], "entities": [{"text": "Text generation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7479915618896484}]}, {"text": "Such tasks include machine translation (), dialog systems (), text summarization (, article writing), text paraphrasing and manipulation (), image captioning (, and more.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.8199138641357422}, {"text": "text summarization", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7809242904186249}, {"text": "article writing)", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.7833319008350372}, {"text": "text paraphrasing and manipulation", "start_pos": 102, "end_pos": 136, "type": "TASK", "confidence": 0.8285560607910156}, {"text": "image captioning", "start_pos": 141, "end_pos": 157, "type": "TASK", "confidence": 0.7501344382762909}]}, {"text": "Recent years have seen rapid progress of this active area in both academia and industry, especially with the adoption of modern deep learning approaches in many of the tasks.", "labels": [], "entities": []}, {"text": "On the other hand, considerable research efforts are still needed to improve relevant techniques and enable real-world practical applications.", "labels": [], "entities": []}, {"text": "The variety of text generation tasks share many common properties and goals, e.g., to generate well-formed, grammatical and readable text, and to realize in the generation the desired information inferred from inputs.", "labels": [], "entities": [{"text": "text generation", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.7359850108623505}]}, {"text": "To this end, a few key models and algorithms are increasingly widely-used to empower the different applications, such as neural encoder-decoders (), attentions (), memory networks (, adversarial methods (, reinforcement learning (, as well as some optimization techniques, data preprocessing and result post-processing procedures, evaluations, etc.", "labels": [], "entities": []}, {"text": "It is therefore highly desirable to have an opensource platform that unifies the development of the diverse yet closely-related applications, backed with clean and consistent implementations of the core algorithms.", "labels": [], "entities": []}, {"text": "Such a unified platform enables reuse of common components and functionalities, standardizes design, implementation, and experimentation, fosters reproducible research, and importantly, encourages technique sharing among different text generation tasks, so that an algorithmic advance originally developed fora specific task can quickly be evaluated and potentially generalized to many other tasks.", "labels": [], "entities": []}, {"text": "Though a few remarkable open-source toolkits have been developed, they have been largely designed for one or few specific tasks, especially neural machine translation () and dialog related algorithms.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 140, "end_pos": 166, "type": "TASK", "confidence": 0.661699910958608}]}, {"text": "This pa-per introduces Texar, a general-purpose text generation toolkit that aims to support most of the popular applications in the field, by providing researchers and practitioners a unified and flexible framework for building their models.", "labels": [], "entities": [{"text": "Texar", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.8618519306182861}, {"text": "text generation toolkit", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.747092584768931}]}, {"text": "Texar is built upon TensorFlow 1 , a popular deep learning platform.", "labels": [], "entities": [{"text": "Texar", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9644514918327332}]}, {"text": "Texar emphasizes on three key properties, namely, versatility, modularity, and extensibility.", "labels": [], "entities": [{"text": "Texar", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.955184280872345}]}, {"text": "\u2022 Versatility: Texar contains a wide range of features and functionalities for 1) arbitrary model architectures as a combination of encoders, decoders, discriminators, memories, and many other modules; and 2) different modeling and learning paradigms such as sequence-to-sequence, probabilistic models, adversarial methods, and reinforcement learning.", "labels": [], "entities": [{"text": "Texar", "start_pos": 15, "end_pos": 20, "type": "DATASET", "confidence": 0.9245964288711548}]}, {"text": "Based on these, both workhorse and cutting-edge solutions to the broad spectrum of text generation tasks are either already included or can be easily constructed.", "labels": [], "entities": [{"text": "text generation tasks", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.8331718444824219}]}, {"text": "\u2022 Modularity: Texar is designed to be highly modularized, by decoupling solutions to diverse tasks into a set of highly reusable modules.", "labels": [], "entities": [{"text": "Texar", "start_pos": 14, "end_pos": 19, "type": "DATASET", "confidence": 0.8383809328079224}]}, {"text": "Users can construct their model at a high conceptual level just like assembling LEGO bricks.", "labels": [], "entities": []}, {"text": "It is convenient to plugin or swap out modules, configure rich options of each module, or even switch between distinct modeling paradigms.", "labels": [], "entities": []}, {"text": "For example, switching between maximum likelihood learning and reinforcement learning involves only minimal code changes.", "labels": [], "entities": []}, {"text": "Modularity makes Texar useful for fast prototyping, parameter tuning, and model experimentation.", "labels": [], "entities": [{"text": "Texar", "start_pos": 17, "end_pos": 22, "type": "DATASET", "confidence": 0.8937761187553406}, {"text": "parameter tuning", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7970822751522064}]}, {"text": "\u2022 Extensibility: The toolkit provides interfaces of multiple functionality levels, ranging from simple Python-like configuration files to full library APIs.", "labels": [], "entities": []}, {"text": "Users of different needs and expertise are free to choose different interfaces for appropriate programmability and internal accessibility.", "labels": [], "entities": []}, {"text": "The library APIs are fully compatible with the native TensorFlow interfaces, which allows a seamless integration of usercustomized modules, and enables the toolkit to take advantage of the vibrant open-source community by easily importing any external components as needed.", "labels": [], "entities": []}, {"text": "Furthermore, Texar puts much emphasis on well-structured high-quality code of uniform design patterns and consistent styles, along with clean documentations and rich tutorial examples.", "labels": [], "entities": [{"text": "Texar", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.9598694443702698}]}, {"text": "In the following, we provide details of the toolkit structure and design.", "labels": [], "entities": []}, {"text": "To demonstrate the use of the toolkit and its advantages, we perform extensive experiments and cases studies, including generalizing the state-of-the-art machine translation model to multiple text generation tasks, investigating different algorithms for language modeling, and implementing composite neural architectures beyond conventional encoder-decoder for text style transfer.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 154, "end_pos": 173, "type": "TASK", "confidence": 0.7271523773670197}, {"text": "language modeling", "start_pos": 254, "end_pos": 271, "type": "TASK", "confidence": 0.7137512266635895}, {"text": "text style transfer", "start_pos": 361, "end_pos": 380, "type": "TASK", "confidence": 0.7068409323692322}]}, {"text": "All are easily realized with the versatile toolkit.", "labels": [], "entities": []}, {"text": "Texar is under Apache license 2.0, and will be released very soon.", "labels": [], "entities": [{"text": "Texar", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9820830821990967}]}, {"text": "Please checkout http: //www.cs.cmu.edu/ \u02dc zhitingh for the release progress.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform extensive experiments to demonstrate the use and advantage of Texar.", "labels": [], "entities": [{"text": "Texar", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.9895988702774048}]}, {"text": "In particular, we conduct case studies on technique sharing that is uniquely supported by our toolkit: (1) We deploy the state-of-the-art machine translation model on other tasks to study its generality, and obtain improved performance over previous methods; (2) We apply various model paradigms on the task of language modeling to compare the different methods.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.7199409753084183}, {"text": "language modeling", "start_pos": 311, "end_pos": 328, "type": "TASK", "confidence": 0.7422463297843933}]}, {"text": "Besides, to further demonstrate the versatility of Texar, we show a case study on the newlyemerging task of text style transfer, which typically involves composite neural architectures beyond the conventional encoder-decoder..", "labels": [], "entities": [{"text": "Texar", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.928199827671051}, {"text": "text style transfer", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.6816895107428232}]}, {"text": "Test set perplexity (PPL) and sentence-level negative log likelihood (NLL) are evaluated (The lower the better).", "labels": [], "entities": [{"text": "sentence-level negative log likelihood (NLL)", "start_pos": 30, "end_pos": 74, "type": "METRIC", "confidence": 0.7848699603761945}]}], "tableCaptions": [{"text": " Table 1: Comparison of Transformer decoder and LSTM RNN decoder on VAE language model- ing (", "labels": [], "entities": [{"text": "VAE language model- ing", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.6702484548091888}]}, {"text": " Table 2: Comparison of Transformer decoder and  GRU RNN decoder on conversation generation  within the HERD model (Bowman et al., 2015).  The Switchboard data (Zhao et al., 2017) is used.", "labels": [], "entities": [{"text": "conversation generation", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.7233607470989227}, {"text": "Switchboard data", "start_pos": 143, "end_pos": 159, "type": "DATASET", "confidence": 0.7310848236083984}]}, {"text": " Table 3: Comparison of the three models on  the task of language modeling, using the PTB  dataset (", "labels": [], "entities": [{"text": "language modeling", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7549344897270203}, {"text": "PTB  dataset", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.9762703478336334}]}, {"text": " Table 4: Text style transfer on the Yelp data (Shen  et al., 2017). The first row is the original open- source implementation by the author (Shen et al.,  2017). The subsequent two rows are Texar imple- mentations of the two work.", "labels": [], "entities": [{"text": "Text style transfer", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.6204949915409088}, {"text": "Yelp data", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.9629875719547272}]}]}