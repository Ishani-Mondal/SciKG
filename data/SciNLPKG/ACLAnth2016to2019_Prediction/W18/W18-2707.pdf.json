{"title": [{"text": "Enhancement of Encoder and Attention Using Target Monolingual Corpora in Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.6791494886080424}]}], "abstractContent": [{"text": "A large-scale parallel corpus is required to train encoder-decoder neural machine translation.", "labels": [], "entities": [{"text": "encoder-decoder neural machine translation", "start_pos": 51, "end_pos": 93, "type": "TASK", "confidence": 0.604508139193058}]}, {"text": "The method of using synthetic parallel texts, in which target monolingual corpora are automatically translated into source sentences, is effective in improving the decoder, but is unreliable for enhancing the encoder.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method that enhances the encoder and attention using target monolingual corpora by generating multiple source sentences via sampling.", "labels": [], "entities": []}, {"text": "By using multiple source sentences, diversity close to that of humans is achieved.", "labels": [], "entities": []}, {"text": "Our experimental results show that the translation quality is improved by increasing the number of synthetic source sentences for each given target sentence, and quality close to that using a manually created parallel corpus was achieved.", "labels": [], "entities": [{"text": "translation", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.9562731385231018}]}], "introductionContent": [{"text": "In recent years, neural machine translation (NMT) based on encoder-decoder models) has become the mainstream approach for machine translation.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 17, "end_pos": 49, "type": "TASK", "confidence": 0.8173780043919882}, {"text": "machine translation", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.797031432390213}]}, {"text": "In this method, the encoder converts an input sentence into numerical vectors called \"states,\" and the decoder generates a translation on the basis of these states.", "labels": [], "entities": []}, {"text": "Although the encoder-decoder models can generate high-quality translations, they require large amounts of parallel texts for training.", "labels": [], "entities": []}, {"text": "On the other hand, monolingual corpora are readily available in large quantities.", "labels": [], "entities": []}, {"text": "proposed a method using synthetic parallel texts, in which target monolingual corpora are translated back into the source language ().", "labels": [], "entities": []}, {"text": "The advantage of this method is that the decoder is accurately trained because the target side of the synthetic parallel texts consists of manually created (correct) sentences.", "labels": [], "entities": []}, {"text": "Consequently, this method provides steady improvements.", "labels": [], "entities": []}, {"text": "However, this approach may not contribute to the improvement of the encoder because the source side of the synthetic parallel texts are automatically generated.", "labels": [], "entities": []}, {"text": "In this paper, we extend the method proposed by to enhance the encoder and attention using target monolingual corpora.", "labels": [], "entities": []}, {"text": "Our proposed method generates multiple source sentences by sampling when each target sentence is translated back.", "labels": [], "entities": []}, {"text": "By using multiple source sentences, we aim to achieve the following.", "labels": [], "entities": []}, {"text": "\u2022 To average errors in individual synthetic sentences and reduce their harmful effects.", "labels": [], "entities": []}, {"text": "\u2022 To ensure diversity as human translations.", "labels": [], "entities": []}, {"text": "This is a countermeasure against machinetranslated sentences that have less variety.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides an overview of related work that uses monolingual corpora in NMT.", "labels": [], "entities": []}, {"text": "Section 3 describes the proposed method, and Section 4 evaluates the proposed method through experiments.", "labels": [], "entities": []}, {"text": "In addition, Section 5 proposes the application of our method as a self-training approach.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Corpora The corpus sizes used here are shown in.", "labels": [], "entities": []}, {"text": "We used the global communication plan corpus (the GCP corpus, (Imamura and Sumita, 2018)), which is an in-house parallel corpus of daily life conversations and consists of Japanese (Ja), English (En), and Chinese (Zh).", "labels": [], "entities": [{"text": "GCP corpus", "start_pos": 50, "end_pos": 60, "type": "DATASET", "confidence": 0.8349202573299408}]}, {"text": "The experiments were performed on Englishto-Japanese and Chinese-to-Japanese translation tasks.", "labels": [], "entities": [{"text": "Chinese-to-Japanese translation tasks", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.7216802835464478}]}, {"text": "We randomly selected 400K sentences for the base parallel corpus, and the remaining (1.55M sentences) were used as the Japanese monolingual corpus.", "labels": [], "entities": [{"text": "Japanese monolingual corpus", "start_pos": 119, "end_pos": 146, "type": "DATASET", "confidence": 0.6774530410766602}]}, {"text": "The reason for dividing the parallel corpus into two corpora is to measure the upper-bound of quality improvement by using existing parallel texts on the same domain as the manual backtranslation.", "labels": [], "entities": []}, {"text": "We also used the Balanced Corpus of Contemporary Written Japanese (BCCWJ) 5 as a monolingual corpus from a different domain.", "labels": [], "entities": [{"text": "Balanced Corpus of Contemporary Written Japanese (BCCWJ) 5", "start_pos": 17, "end_pos": 75, "type": "DATASET", "confidence": 0.8347563475370408}]}, {"text": "We used approximately 4.8M sentences, each of which contains less than 1024 characters.", "labels": [], "entities": []}, {"text": "We assume practical situations in which the domains of parallel and monolingual corpora are not identical.", "labels": [], "entities": []}, {"text": "All sentences were segmented into words using an in-house word segmenter.", "labels": [], "entities": []}, {"text": "The words were further segmented into 16K sub-words based on the byte-pair encoding rules (Sennrich et al., 2016b) acquired from the base parallel corpus for each language independently.", "labels": [], "entities": []}, {"text": "Translation System The translation system used in this study was OpenNMT (.", "labels": [], "entities": [{"text": "OpenNMT", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9119469523429871}]}, {"text": "We modified it to accept Sections 3.1 and 3.2.", "labels": [], "entities": []}, {"text": "The encoder was comprised of a two-layer Bi-LSTM (500 + 500 units), the decoder included a two-layer LSTM (1,000 units), and the stochastic gradient descent was used for optimization.", "labels": [], "entities": [{"text": "Bi-LSTM", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9266412258148193}]}, {"text": "The learning rate for the base parallel corpus was 1.0 for the first 14 epochs, followed by the annealing of 6 epochs while decreasing the learning rate by half.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9709118008613586}]}, {"text": "The mini-batch size was 64.", "labels": [], "entities": []}, {"text": "At the translation stage, we generated 10-best translations and selected the best among them on the basis of the length reranking (.", "labels": [], "entities": [{"text": "translation", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.9743578433990479}]}, {"text": "Equation 3 was used as the score function for the reranking.", "labels": [], "entities": [{"text": "Equation 3", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9686139822006226}]}, {"text": "By correcting the translation length, the translation quality can be compared without the effect of the brevity penalty of the BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9984586238861084}]}, {"text": "The back-translator was comprised of the same system.", "labels": [], "entities": []}, {"text": "We generated 10 synthetic source sentences per target sentence using the method described in Section 3.1, and filtered them to create synthetic parallel sentences.", "labels": [], "entities": []}, {"text": "Competing Methods In this paper, we consider the casein which only the base parallel corpus is used as the baseline, and the casein which the manual back-translation of the GCP corpus is added as the upper-bound of the translation quality.", "labels": [], "entities": [{"text": "GCP corpus", "start_pos": 173, "end_pos": 183, "type": "DATASET", "confidence": 0.8513685166835785}]}, {"text": "Thereafter, we compare the following methods and settings: \u2022 Various numbers of synthetic source sentences fora given target sentence \u2022 The methods for generating synthetic source sentences: sampling vs. n-best generation \u2022 The three filtering methods described in Section 3.3 Evaluation BLEU () was used for the evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 288, "end_pos": 292, "type": "METRIC", "confidence": 0.9724459052085876}]}, {"text": "The multeval tool (Clark et al., 2011) 6 was used for statistical testing at a significance level of 5% (p < 0.05).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The BLEU scores of the BCCWJ and  GCP corpora according to the number of synthetic  source sentences (En-Ja, the random filtering).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9991217255592346}, {"text": "BCCWJ", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.9120008945465088}]}, {"text": " Table 4: The BLEU scores and the edit distances  of synthetic source sentences based on 10 syn- thetic sentences and manual back-translation for  the same 1,000 target sentences in the GCP cor- pus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9986095428466797}, {"text": "GCP cor- pus", "start_pos": 186, "end_pos": 198, "type": "DATASET", "confidence": 0.9253111928701401}]}, {"text": " Table 5: The effect of self-training (En-Ja transla- tion)", "labels": [], "entities": []}]}