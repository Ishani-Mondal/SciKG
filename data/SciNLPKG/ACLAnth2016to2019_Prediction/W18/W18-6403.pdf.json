{"title": [{"text": "Findings of the WMT 2018 Biomedical Translation Shared Task: Evaluation on Medline test sets", "labels": [], "entities": [{"text": "WMT 2018 Biomedical Translation Shared Task", "start_pos": 16, "end_pos": 59, "type": "TASK", "confidence": 0.7698988715807596}, {"text": "Medline test", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.9776931703090668}]}], "abstractContent": [{"text": "Machine translation enables the automatic translation of textual documents between languages and can facilitate access to information only available in a given language for non-speakers of this language, e.g. research results presented in scientific publications.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7550801634788513}, {"text": "automatic translation of textual documents between languages", "start_pos": 32, "end_pos": 92, "type": "TASK", "confidence": 0.8119734185082572}]}, {"text": "In this paper , we provide an overview of the Biomed-ical Translation shared task in the Workshop on Machine Translation (WMT) 2018, which specifically examined the performance of machine translation systems for biomedical texts.", "labels": [], "entities": [{"text": "Biomed-ical Translation shared task in the Workshop on Machine Translation (WMT) 2018", "start_pos": 46, "end_pos": 131, "type": "TASK", "confidence": 0.7717321621520179}, {"text": "machine translation", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.7468007802963257}]}, {"text": "This year, we provided test sets of scientific publications from two sources (EDP and Med-line) and for six language pairs (English with each of Chinese, French, German, Portuguese, Romanian and Spanish).", "labels": [], "entities": [{"text": "EDP", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.9507262110710144}, {"text": "Med-line", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.8430262207984924}]}, {"text": "We describe the development of the various test sets, the submissions that we received and the evaluations that we carried out.", "labels": [], "entities": []}, {"text": "We obtained a total of 39 runs from six teams and some of this year's BLEU scores were somewhat higher that last year's, especially for teams that made use of biomedical resources or state-of-the-art MT algorithms (e.g. Transformer).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9993738532066345}, {"text": "MT", "start_pos": 200, "end_pos": 202, "type": "TASK", "confidence": 0.9644209742546082}]}, {"text": "Finally, our manual evaluation scored automatic translations higher than the reference translations for German and Spanish.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic translation of documents from one language to another facilitates broader information access for resources only available in a particular language.", "labels": [], "entities": []}, {"text": "Even in the scientific literature, in which most important articles are published only in English, an increasing number of researchers support citing articles published in other languages for the sake of not missing important research or to avoid carrying out duplicate experiments (.", "labels": [], "entities": []}, {"text": "Recent discussions on this topic in the journal Nature have appealed for translation of the best Chinese papers ( and the development of automatic tools for the automatic translation of publications.", "labels": [], "entities": [{"text": "translation", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.9742616415023804}]}, {"text": "Therefore, biomedicine is a domain for which suitable parallel corpora, official evaluation test sets and machine translation (MT) systems are in high demand.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.8410503387451171}]}, {"text": "There is active development of parallel corpora in this domain (see the recent survey in).", "labels": [], "entities": []}, {"text": "In this year alone, three new corpora have been published in a single conference: a compilation of full texts from the Scielo database for English, Portuguese, and Spanish ( , medical documents and glossaries for Spanish/English () and a biomedical corpus for Romanian.", "labels": [], "entities": []}, {"text": "However, in spite of the growing number of parallel corpora and the many open source tools for MT (e.g.,), OpenNMT ( and Marian), there is still no ready-to-use tool for automatic translation of biomedical publications for any language pair.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9099619388580322}, {"text": "OpenNMT", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.9197620749473572}]}, {"text": "With the aim of fostering advances in this field, we organized the third edition of the Biomedical Translation Task in the Conference for Machine Translation (WMT).", "labels": [], "entities": [{"text": "Biomedical Translation Task in the Conference for Machine Translation (WMT)", "start_pos": 88, "end_pos": 163, "type": "TASK", "confidence": 0.7874281133214632}]}, {"text": "It builds on the two previous editions) by offering test sets from Medline for six language pairs and from EDP for one language pair, as detailed below: \u2022 Chinese-English (zh/en); Eng.-Chinese (en/zh) \u2022 French-English (fr/en); Eng.-French (en/fr) \u2022 German-English (de/en); Eng.-German (en/de) \u2022 Portuguese-English (pt/en); Eng.-Port.", "labels": [], "entities": [{"text": "Medline", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9802767038345337}, {"text": "EDP", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.9532052278518677}]}, {"text": "(en/pt) \u2022 English-Romanian (en/ro) \u2022 Spanish-English (es/en); Eng.-Span.", "labels": [], "entities": []}, {"text": "(en/es) Most test sets were derived from scientific abstracts from Medline which were available in both languages.", "labels": [], "entities": [{"text": "Medline", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9382883906364441}]}, {"text": "Except for Romanian, we addressed translation in both directions for all language pairs.", "labels": [], "entities": []}, {"text": "This was not possible for Romanian due to the low number (less than 50) of parallel abstracts which are available in Medline.", "labels": [], "entities": [{"text": "Medline", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.9717528820037842}]}, {"text": "For the first time, we have an Asian language, specifically Chinese.", "labels": [], "entities": []}, {"text": "In this paper, we describe details of the challenge.", "labels": [], "entities": []}, {"text": "Section 2 presents the construction and quality analysis of the test sets, followed by the details on the six participating teams in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 presents the results for both automatic and manual evaluation that we carried out, as well as some additional evaluations which are new this year.", "labels": [], "entities": []}, {"text": "Finally, we provide a comprehensive discussion of the results and quality of the translations in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "After compiling the Medline test sets, we manually checked the totality of the abstracts to assess the quality of the automatic alignment (cf. results shown in).", "labels": [], "entities": [{"text": "Medline test sets", "start_pos": 20, "end_pos": 37, "type": "DATASET", "confidence": 0.9573868115743002}]}, {"text": "We utilized a modified version of the Quality Checking task of our installation of the Appraise tool and one native speaker of each non-English language carried out the validation (cf.).", "labels": [], "entities": [{"text": "Appraise", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.569429874420166}]}, {"text": "The only exception were the Chinese abstracts which were manually checked without the use of the Appraise tool.", "labels": [], "entities": [{"text": "Appraise", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.6969121098518372}]}, {"text": "For each language pair, we checked the totality of the abstracts for both translation directions, e.g., en/de and de/en, which was later randomly Test sets de/en fr/en pt/en es/en en/de en/fr en/pt en/es en/ro en/zh zh/en: Overview of the test sets.", "labels": [], "entities": []}, {"text": "We present the number of documents and sentences in each test set.", "labels": [], "entities": []}, {"text": "The number of sentences might be different for the two languages in a test set.", "labels": [], "entities": []}, {"text": "split into two test sets.", "labels": [], "entities": []}, {"text": "The only exception was the Romanian test set.", "labels": [], "entities": [{"text": "Romanian test set", "start_pos": 27, "end_pos": 44, "type": "DATASET", "confidence": 0.9780967036883036}]}, {"text": "Due to its small size, we only built one test set for one translation direction (en/ro).", "labels": [], "entities": []}, {"text": "The number of completely unaligned sentences was rather uniform across the various language pairs and usually less than 5%, with the exception of Spanish (more than 8%) and German (more that 15%).", "labels": [], "entities": []}, {"text": "All other partial alignments (Overlap, Source>Target and Target>Source) had a contribution of less than 10%.", "labels": [], "entities": []}, {"text": "For three languages, at least 80% of the sentences were correctly aligned, while for Spanish and German only 70% and 65% of the sentences were correctly aligned.", "labels": [], "entities": [{"text": "correctly", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.992525577545166}]}, {"text": "The lower quality of these two test sets could certainly affect the calculation of the BLEU score and we will address this problem later in Section 4.2.3.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9820823669433594}]}, {"text": "During the manual validation, we detected problems in the parallel abstracts.", "labels": [], "entities": []}, {"text": "For instance, four abstracts) in the German test set had to be excluded because they were wrongly tagged in Medline as being in German, while they were written in Italian.", "labels": [], "entities": [{"text": "German test set", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9178251624107361}, {"text": "Medline", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.962556779384613}]}, {"text": "The same occurred in the French test sets, were two abstracts (PMIDs 23396711 and 24883131) were also in Italian.", "labels": [], "entities": [{"text": "French test sets", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.9659565885861715}, {"text": "PMIDs", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9975271821022034}]}, {"text": "This suggests that the use of automatic language identification tools could be useful to validate the language metadata retrieved from MEDLINE.", "labels": [], "entities": [{"text": "language identification", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.7240650504827499}, {"text": "MEDLINE", "start_pos": 135, "end_pos": 142, "type": "DATASET", "confidence": 0.841456949710846}]}, {"text": "In this section we describe the various submissions that we obtained and present the results that these achieved based on both automatic and manual valuation.", "labels": [], "entities": []}, {"text": "Here we provide the results for the automatic evaluation and rank the systems regarding the resulting scores.", "labels": [], "entities": []}, {"text": "We computed BLEU scores at the sentence level using the script mteval-v14.pl from the Moses distribution.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9699298739433289}]}, {"text": "For all test sets and language pairs, we compare the submissions (automatic translations) to the respective reference one.", "labels": [], "entities": []}, {"text": "The BLEU scores for the EDP test set are presented in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9986205101013184}, {"text": "EDP test set", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.922897458076477}]}, {"text": "Given that we received only two submissions from a single team, we could not perform comparison between teams.", "labels": [], "entities": []}, {"text": "We ranked the two submissions as follows: \u2022 en/fr: Hunter (run 1) < Hunter (run 2).", "labels": [], "entities": []}, {"text": "Run2 obtained a slightly higher score than run1, however, reasons for this improvement are unknown.", "labels": [], "entities": [{"text": "Run2", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9110705256462097}]}, {"text": "This year, we calculated BLEU scores based on the totality of the sentences (including the ones with incorrect alignments) as well as based only   on the sentences which were perfectly aligned (cf. Section 2).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9992009997367859}]}, {"text": "BLEU scores for the Medline test set are presented in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.974420428276062}, {"text": "Medline test set", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.993865450223287}]}, {"text": "For some language pairs, i.e., de/en, en/de, en/fr and fr/en, we could not compare results between various teams since we received submissions only from one team.", "labels": [], "entities": []}, {"text": "Moreover, we only received one submission from one team for de/en.", "labels": [], "entities": [{"text": "de/en", "start_pos": 60, "end_pos": 65, "type": "TASK", "confidence": 0.7676917314529419}]}, {"text": "Therefore, no further comparison was possible for this language pair.", "labels": [], "entities": []}, {"text": "In the following we provide a short summary of the results with regard to the method or resources that have been used.", "labels": [], "entities": []}, {"text": "The run based on the Transformer architecture from the LMU team obtained a reasonable BLEU score.", "labels": [], "entities": [{"text": "LMU team", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.9494607150554657}, {"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9995007514953613}]}, {"text": "However, we could not compare this to any other submission.", "labels": [], "entities": []}, {"text": "There was little difference in the BLEU score between the two first submissions, both based on the Transformer architecture, but both did seem to be superior to the third run based on the encoder-decoder model.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9790114462375641}]}, {"text": "The best results for en/es were obtained by the UFRGS team when using the Moses system (run2) instead of neural MT (run1), as expected by the team.", "labels": [], "entities": [{"text": "UFRGS", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.9444395899772644}]}, {"text": "However, the difference between both submissions is not significant.", "labels": [], "entities": []}, {"text": "We observed no significant difference between the three submissions from the UHH-DS team.", "labels": [], "entities": [{"text": "UHH-DS team", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.963098555803299}]}, {"text": "However, all of them yield much lower BLEU scores than the submissions by the UFRGS team.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9996058344841003}, {"text": "UFRGS team", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.9508315026760101}]}, {"text": "The submissions from the Hunter team obtained very similar scores for the Medline test sets.", "labels": [], "entities": [{"text": "Medline test sets", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.9930753509203593}]}, {"text": "Details on each run is unclear but these differences seem to have brought significant improvement on the scores only on the EDP test set (cf.).", "labels": [], "entities": [{"text": "EDP test set", "start_pos": 124, "end_pos": 136, "type": "DATASET", "confidence": 0.9298837780952454}]}, {"text": "Both submissions from the UFRGS team obtained the highest BLEU scores, which again and similar to the results obtained for en/es, did not confirm the superiority of neural MT.", "labels": [], "entities": [{"text": "UFRGS team", "start_pos": 26, "end_pos": 36, "type": "DATASET", "confidence": 0.9416761994361877}, {"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9995771050453186}, {"text": "MT", "start_pos": 172, "end_pos": 174, "type": "TASK", "confidence": 0.8526911735534668}]}, {"text": "The three runs from the UHH-DS team were closer to the ones from the UFRGS team (in comparison to the ones for en/es), but still rather inferior.", "labels": [], "entities": [{"text": "UHH-DS team", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.9698415100574493}, {"text": "UFRGS team", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.9622230231761932}]}, {"text": "This time, run1 (under-sampling based on the English side) did perform a little better than the other two runs, specially regarding run2 (undersampling based on the non-English side).", "labels": [], "entities": []}, {"text": "The run2 of the FOKUS team, which consisted on an ensemble of various NMT systems and used heuristics for selecting the best translation, obtained the highest BLEU score.", "labels": [], "entities": [{"text": "FOKUS team", "start_pos": 16, "end_pos": 26, "type": "DATASET", "confidence": 0.8909662961959839}, {"text": "BLEU score", "start_pos": 159, "end_pos": 169, "type": "METRIC", "confidence": 0.9775142073631287}]}, {"text": "The two submissions from UHH-DS reached BLEU scores which were slightly below results from the FOKUS team.", "labels": [], "entities": [{"text": "UHH-DS", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.9386022090911865}, {"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9998014569282532}, {"text": "FOKUS team", "start_pos": 95, "end_pos": 105, "type": "DATASET", "confidence": 0.7255201041698456}]}, {"text": "We observed no significant difference between the three runs.", "labels": [], "entities": []}, {"text": "However, similar to en/pt, under-sampling based on the English side (run1) seems to perform slightly better than undersampling based on both sides (run3).", "labels": [], "entities": []}, {"text": "Once again the statistical MT system (Moses) from the UFRGS team obtained a slightly higher score than their neural MT system (Open-NMT).", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.8978453874588013}, {"text": "UFRGS team", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.9661934971809387}]}, {"text": "Indeed, the BLEU scores obtained by run2 of the team was the highest one among all submissions to the shared task for all language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.999138355255127}]}, {"text": "The following two best scores belonged to the Transformer-based MT systems from the TGF TALP UPC team.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.5877406001091003}, {"text": "TGF TALP UPC team", "start_pos": 84, "end_pos": 101, "type": "DATASET", "confidence": 0.8233017325401306}]}, {"text": "Even though a more recent and currently state-of-the-art method (Transformer) was used by team TGF TALP UPC, the better results obtained by UFRGS were probably due to the larger training collection that they used.", "labels": [], "entities": [{"text": "TGF TALP UPC", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.6738444964090983}, {"text": "UFRGS", "start_pos": 140, "end_pos": 145, "type": "DATASET", "confidence": 0.8600645661354065}]}, {"text": "The model trained on various sources obtained a slightly better score.", "labels": [], "entities": []}, {"text": "The three runs from the UHH-DS team were rather inferior than the ones from the two other teams.", "labels": [], "entities": [{"text": "UHH-DS team", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.9563717246055603}]}, {"text": "A significant difference was only observed for run2 (under-sampling based on the non-English side) which achieved lower BLEU scores for this language pair.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9994168281555176}]}, {"text": "The only submissions for fr/en belonged to the Transformer-based MT systems from the TGF TALP UPC team.", "labels": [], "entities": [{"text": "TGF TALP UPC team", "start_pos": 85, "end_pos": 102, "type": "DATASET", "confidence": 0.8820831328630447}]}, {"text": "This time, the improvement of the multi-source model over the single model was very significant.", "labels": [], "entities": []}, {"text": "However, the highest score was rather low in comparison to the other submissions of the team.", "labels": [], "entities": []}, {"text": "There was no difference in the two submissions from the UFRGS team, both obtained the highest BLEU scores.", "labels": [], "entities": [{"text": "UFRGS team", "start_pos": 56, "end_pos": 66, "type": "DATASET", "confidence": 0.9668438136577606}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9995081424713135}]}, {"text": "The three runs from the UHH-DS team obtained the second best results but we observed no significant difference between the three of them.", "labels": [], "entities": [{"text": "UHH-DS team", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.950819581747055}]}, {"text": "Finally, the two lowest scores were obtained by the Transformer-based MT systems from the TGF TALP UPC team.", "labels": [], "entities": [{"text": "Transformer-based MT", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.6004714071750641}, {"text": "TGF TALP UPC team", "start_pos": 90, "end_pos": 107, "type": "DATASET", "confidence": 0.7815325558185577}]}, {"text": "Similar to results for es/en and fr/en, the system trained on various sources obtained a little improvement over the single models.", "labels": [], "entities": []}, {"text": "Also similar to the es/en results, the higher performance from UFRGS was probably due to the use of more resources for training the MT systems.", "labels": [], "entities": [{"text": "UFRGS", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.9291189312934875}, {"text": "MT", "start_pos": 132, "end_pos": 134, "type": "TASK", "confidence": 0.9747902154922485}]}, {"text": "This year, we also calculated additional BLEU scores when considering only the sentences whose alignments we manually classified as being correct.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9995055198669434}]}, {"text": "Correctly aligned means that sentences in both languages contained exactly the same information and neither of the sentences contained more information than the other (cf. Section 2).", "labels": [], "entities": []}, {"text": "Results for this subset of the test set are presented in.", "labels": [], "entities": []}, {"text": "For most teams, improvements were significant, ranging from two to four BLEU points, but was up to one point for en2ro.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9987779259681702}, {"text": "en2ro", "start_pos": 113, "end_pos": 118, "type": "DATASET", "confidence": 0.8784278035163879}]}, {"text": "The overall order of the results mostly remained the same.", "labels": [], "entities": [{"text": "order", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.9524549841880798}]}, {"text": "In the particular case of the Romanian language, there were fairly recent changes in the ortho-  graphic recommendation with respect to diacritics notation 6 : comma-below s , and S , should be used instead of cedilla-below s \u00b8 and S \u00b8 ; in the same way, the comma-below t , and T , should be used instead of the cedilla-below t \u00b8 and T \u00b8 , according to a 2003 communicate from the \"Iorgu Iordan\" Institute of Linguistics of the Romanian Academy.", "labels": [], "entities": []}, {"text": "While the two comma-below and cedilla-below variants of those letters are hardly distinguishable to a human reader, they have different unicode codes and thus replacing one with another in a word makes it a completely different word, for an automated method.", "labels": [], "entities": []}, {"text": "Having the \"wrong\" word affects all n-grams containing that word for the BLEU scoring.", "labels": [], "entities": [{"text": "BLEU scoring", "start_pos": 73, "end_pos": 85, "type": "METRIC", "confidence": 0.9608132243156433}]}, {"text": "In order to achieve more quality in the translation assessment, we normalized all diacritics both in gold standard and in the submissions for Romanian.", "labels": [], "entities": [{"text": "translation assessment", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.8817517161369324}]}, {"text": "Results for the Medline en/ro test set are shown in, based on all sentences (en/ro) and only based on correctly aligned sentences (en/ro-OK).", "labels": [], "entities": [{"text": "Medline en/ro test set", "start_pos": 16, "end_pos": 38, "type": "DATASET", "confidence": 0.871701697508494}]}, {"text": "To this end, we wrote and used a simple sed-based script which brings the Romanian diacritics to the latest standard 7 .  We performed manual evaluation of the primary runs (as identified by the participants) for each team and each language pair.", "labels": [], "entities": []}, {"text": "The primary runs are compared to the reference translation and to each other, if more than one submission (from distinct teams) is available for the language pair.", "labels": [], "entities": []}, {"text": "We   computed pairwise combinations of translations either between two automated systems, or one automated system and the reference translation.", "labels": [], "entities": []}, {"text": "The human validators were native speakers of the languages and were either members of the participating teams or colleagues from the research community.", "labels": [], "entities": []}, {"text": "The validation task was carried out using the 3-way ranking task in our installation of the Appraise tool.", "labels": [], "entities": [{"text": "validation", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.979228138923645}, {"text": "Appraise tool", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.7001888900995255}]}, {"text": "For each pairwise comparison, we checked a total of 100 randomlychosen sentence pairs.", "labels": [], "entities": []}, {"text": "The validation consisted of reading the two sentences (A and B), i.e., translations from two systems or from the reference, and choosing one of the options below: \u2022 A<B: when the quality of translation B was higher than A.", "labels": [], "entities": []}, {"text": "\u2022 A=B: when both translations had similar quality.", "labels": [], "entities": [{"text": "A=B", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.7059952219327291}]}, {"text": "\u2022 A>B: when the quality of translation A was higher than B.", "labels": [], "entities": []}, {"text": "\u2022 Flag error: when the translation did not seem to be derived from the same input sentence.", "labels": [], "entities": [{"text": "Flag error", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.7992443740367889}]}, {"text": "This is usually related to errors in corpus alignment.", "labels": [], "entities": [{"text": "corpus alignment", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.6943069845438004}]}, {"text": "We present the results for the manual evaluation of the Medline test sets in.", "labels": [], "entities": [{"text": "Medline test sets", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.9819592038790385}]}, {"text": "The reason are misaligned sentences in the German reference.", "labels": [], "entities": [{"text": "German reference", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.8886047303676605}]}, {"text": "Automatic German translations on the other hand are most often correct in translation and alignment of content.", "labels": [], "entities": [{"text": "translation", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.9310588836669922}, {"text": "alignment of content", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.8100458184878031}]}, {"text": "Indeed, the quality of the German dataset was one of the lowest (cf. Section 2.1).", "labels": [], "entities": [{"text": "German dataset", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.9569328129291534}]}, {"text": "We present the results for the manual evaluation of the EDP test sets in.", "labels": [], "entities": [{"text": "EDP test sets", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9157392779986063}]}, {"text": "Based on the number of times that the submission was validated as being better than the reference translation, we ranked the two translations as follow: \u2022 en/fr: Hunter < reference.", "labels": [], "entities": []}, {"text": "Similar to previous years, we did not notice any difference while ranking the teams for most language pairs regarding the automatic and manual evaluation of the translations.", "labels": [], "entities": []}, {"text": "This year, the only significant difference we noticed was for the English translations, more specifically for es/en and pt/en pairs.", "labels": [], "entities": []}, {"text": "For es/en, the ranking order changed between the teams UFRGS and TGF TALP UPC.", "labels": [], "entities": [{"text": "UFRGS", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.9793480634689331}, {"text": "TGF TALP UPC", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.7667524615923563}]}, {"text": "While the runs from the UFRGS teams achieved a higher BLEU score (43.31 vs. 40.49), our evaluators found the translations from the TGF TALP UPC team to be considerable better (79 vs. 7).", "labels": [], "entities": [{"text": "UFRGS", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.9523956775665283}, {"text": "BLEU score", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9857204556465149}, {"text": "TGF TALP UPC team", "start_pos": 131, "end_pos": 148, "type": "DATASET", "confidence": 0.9270559698343277}]}, {"text": "As for pt/en, the ranking of the teams changed from TGF TAP UPC < UHH-DS < UFRGS (automatic evaluation: 42.55 < 44.27 < 46.01) to UHH-DS < UFRGS < TGF TAP UPC (manual evaluation: 55 vs. 21 to UFRGS, 58 vs. 24 to UHH-DS).", "labels": [], "entities": [{"text": "TGF TAP UPC", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.6092041730880737}, {"text": "UHH-DS", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.8744674324989319}, {"text": "UFRGS", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.5502858757972717}, {"text": "UHH-DS", "start_pos": 130, "end_pos": 136, "type": "DATASET", "confidence": 0.9290899038314819}, {"text": "TGF TAP UPC", "start_pos": 147, "end_pos": 158, "type": "DATASET", "confidence": 0.8215054273605347}, {"text": "UFRGS", "start_pos": 192, "end_pos": 197, "type": "DATASET", "confidence": 0.9500859379768372}, {"text": "UHH-DS", "start_pos": 212, "end_pos": 218, "type": "DATASET", "confidence": 0.948634922504425}]}, {"text": "While no difference in ranking was observed between teams UHH-DS and UFRGS, in comparison to the automatic evaluation, team TGF TAP UPC moved from being the last ranked in the automatic evaluation to the best ranked one on the manual evaluation.", "labels": [], "entities": [{"text": "UHH-DS", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.9698956608772278}, {"text": "UFRGS", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.9189008474349976}, {"text": "TGF TAP UPC", "start_pos": 124, "end_pos": 135, "type": "DATASET", "confidence": 0.5488728284835815}]}, {"text": "We can only hypothesize that the better BLEU scores that the UFRGS team obtained were probably due to better translation of particular concepts or due to using the same terms as in the reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9988003969192505}, {"text": "UFRGS", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.9036087989807129}]}, {"text": "However, the TGF TAP UPC team could obtain higher quality of the manual translations using their Transformer architecture.", "labels": [], "entities": [{"text": "TGF TAP UPC", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.8603784640630087}]}, {"text": "The better performance of the TGF TAP UPC team could also have been due to the test set being included in the their training corpus, i.e. overlaps between Medline and the Scielo databases.", "labels": [], "entities": [{"text": "TGF TAP UPC team", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.8091462850570679}, {"text": "Medline", "start_pos": 155, "end_pos": 162, "type": "DATASET", "confidence": 0.9464588761329651}, {"text": "Scielo databases", "start_pos": 171, "end_pos": 187, "type": "DATASET", "confidence": 0.7398025393486023}]}, {"text": "While both teams trained on the Scielo corpus,   the UFRGS team reported that they tried to remove potential overlaps between Medline and Scielo ( . Overlaps of Medline and Scielo do not explain the lower BLEU scores obtained by the TGF TAP UPC team.", "labels": [], "entities": [{"text": "UFRGS", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.9544614553451538}, {"text": "BLEU", "start_pos": 205, "end_pos": 209, "type": "METRIC", "confidence": 0.9991241097450256}, {"text": "TGF TAP UPC team", "start_pos": 233, "end_pos": 249, "type": "DATASET", "confidence": 0.8865341395139694}]}], "tableCaptions": [{"text": " Table 1: Overview of the test sets. We present the number of documents and sentences in each test set. The number  of sentences might be different for the two languages in a test set.", "labels": [], "entities": []}, {"text": " Table 4: Overview of submissions for each language pair and test set: [M]edline and [E]DP. The number next to  the letter indicates the number of runs that the team submitted for the corresponding test set (if larger than one).", "labels": [], "entities": [{"text": "M", "start_pos": 72, "end_pos": 73, "type": "METRIC", "confidence": 0.9529207348823547}]}, {"text": " Table 5: BLEU scores for the EDP en/fr dataset. * in- dicates the primary run as informed by the participants.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992283582687378}, {"text": "EDP en/fr dataset", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.8589393258094787}]}, {"text": " Table 6: Results for the Medline dataset. * indicates the primary run as informed by the participants.", "labels": [], "entities": [{"text": "Medline dataset", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.9879327416419983}]}, {"text": " Table 7: Results for the Medline dataset using OK aligned sentences. * indicates the primary run as informed by  the participants.", "labels": [], "entities": [{"text": "Medline dataset", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.9842043220996857}]}, {"text": " Table 9: Results for the manual validation for the Medline test sets. Values are absolute numbers (not percentages).  They might not sum up to 100 due to the skipped sentences. Two evaluators (both participants) carried out the  validation of the Romanian dataset and results from both of them are shown (separated by a slash).", "labels": [], "entities": [{"text": "Medline test sets", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.9797672033309937}, {"text": "Romanian dataset", "start_pos": 248, "end_pos": 264, "type": "DATASET", "confidence": 0.8836607038974762}]}, {"text": " Table 10: Results for the manual validation for the EDP test sets. Values are absolute numbers (not percentages).  They might not sum up to 100 due to the skipped sentences.", "labels": [], "entities": [{"text": "EDP test sets", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9112376968065897}]}]}