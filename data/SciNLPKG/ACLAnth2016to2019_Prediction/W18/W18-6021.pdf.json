{"title": [{"text": "Approximate Dynamic Oracle for Dependency Parsing with Reinforcement Learning", "labels": [], "entities": [{"text": "Approximate", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9822400212287903}, {"text": "Dependency Parsing", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7169249951839447}]}], "abstractContent": [{"text": "We present a general approach with reinforcement learning (RL) to approximate dynamic oracles for transition systems where exact dynamic oracles are difficult to derive.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.6717408895492554}]}, {"text": "We treat oracle parsing as a reinforcement learning problem, design the reward function inspired by the classical dynamic oracle, and use Deep Q-Learning (DQN) techniques to train the oracle with gold trees as features.", "labels": [], "entities": [{"text": "oracle parsing", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.7431673109531403}]}, {"text": "The combination of a priori knowledge and data-driven methods enables an efficient dynamic oracle, which improves the parser performance over static oracles in several transition systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Greedy transition-based dependency parsers trained with static oracles are very efficient but suffer from the error propagation problem.", "labels": [], "entities": []}, {"text": "Goldberg and Nivre laid the foundation of dynamic oracles to train the parser with imitation learning methods to alleviate the problem.", "labels": [], "entities": []}, {"text": "However, efficient dynamic oracles have mostly been designed for arc-decomposable transition systems which are usually projective.", "labels": [], "entities": []}, {"text": "designed a non-projective dynamic oracle but runs in O(n 8 ).", "labels": [], "entities": []}, {"text": "proposed an efficient dynamic oracle for the non-projective Covington system), but the system itself has quadratic worst-case complexity.", "labels": [], "entities": []}, {"text": "Instead of designing the oracles, applied the imitation learning approach) by rolling outwith the parser to estimate the cost of each action.", "labels": [], "entities": []}, {"text": "took the reinforcement learning approach () by directly optimizing the parser towards the reward (i.e., the correct arcs) instead of the the correct action, thus no oracle is required.", "labels": [], "entities": []}, {"text": "Both approaches circumvent the difficulty in designing the oracle cost function by using the parser to (1) explore the cost of each action, and (2) explore erroneous states to alleviate error propagation.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 186, "end_pos": 203, "type": "TASK", "confidence": 0.7560557126998901}]}, {"text": "However, letting the parser explore for both purposes is inefficient and difficult to converge.", "labels": [], "entities": []}, {"text": "For this reason, we propose to separate the two types of exploration: (1) the oracle explores the action space to learn the action cost with reinforcement learning, and (2) the parser explores the state space to learn from the oracle with imitation learning.", "labels": [], "entities": []}, {"text": "The objective of the oracle is to prevent further structure errors given a potentially erroneous state.", "labels": [], "entities": []}, {"text": "We design the reward function to approximately reflect the number of unreachable gold arcs caused by the action, and let the model learn the actual cost from data.", "labels": [], "entities": []}, {"text": "We use DQN ( with several extensions to train an Approximate Dynamic Oracle (ADO), which uses the gold tree as features and estimates the cost of each action in terms of potential attachment errors.", "labels": [], "entities": []}, {"text": "We then use the oracle to train a parser with imitation learning methods following.", "labels": [], "entities": []}, {"text": "A major difference between our ADO and the search-based or RL-based parser is that our oracle uses the gold tree as features in contrast to the lexical features of the parser, which results in a much simpler model solving a much simpler task.", "labels": [], "entities": []}, {"text": "Furthermore, we only need to train one oracle for all treebanks, which is much more efficient.", "labels": [], "entities": []}, {"text": "We experiment with several transition systems, and show that training the parser with ADO performs better than training with static oracles inmost cases, and on a par with the exact dynamic oracle if available.", "labels": [], "entities": []}, {"text": "We also conduct an analysis of the oracle's robustness against error propagation for further investigation and improvement.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7057657092809677}]}, {"text": "Our work provides an initial attempt to combine the advantages of reinforcement learning and imitation learning for structured prediction in the case of dependency parsing.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.6455536633729935}, {"text": "dependency parsing", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.7818074524402618}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: LAS on the selected test sets, where green cells", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9849143624305725}]}, {"text": " Table 3: LAS on the 55 test sets, where green cells mark ADO outperforming the static oracle and red cells for the opposite.", "labels": [], "entities": [{"text": "LAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9935506582260132}, {"text": "ADO", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9731429219245911}]}]}