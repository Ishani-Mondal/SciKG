{"title": [{"text": "Quality Estimation with Force-Decoded Attention and Cross-lingual Embeddings", "labels": [], "entities": [{"text": "Quality Estimation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6524508446455002}]}], "abstractContent": [{"text": "This paper describes the submissions of the team from the University of Tartu for the sentence-level Quality Estimation shared task of WMT18.", "labels": [], "entities": [{"text": "sentence-level Quality Estimation shared task of WMT18", "start_pos": 86, "end_pos": 140, "type": "TASK", "confidence": 0.6619985699653625}]}, {"text": "The proposed models use features based on attention weights of a neural machine translation system and cross-lingual phrase embeddings as input features of a regression model.", "labels": [], "entities": []}, {"text": "Two of the proposed models require only a neural machine translation system with an attention mechanism with no additional resources.", "labels": [], "entities": []}, {"text": "Results show that combining neural networks and baseline features leads to significant improvements over the baseline features alone.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the last several years the quality of machine translation has grown significantly.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7823828458786011}]}, {"text": "However even today most machine translation systems produce a lot of unreliable translations, with translation quality varying greatly between different input and output segments.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7478117942810059}]}, {"text": "To estimate the quality of these translations several methods have been proposed.", "labels": [], "entities": []}, {"text": "In this article we propose an approach to quality estimation that is based on a regression model with different sets of features stemming from the internal parameters of a neural machine translation (NMT) system.", "labels": [], "entities": [{"text": "quality estimation", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.6528929322957993}]}, {"text": "We investigate how different input features of the regression model affect the correlation between the automatic quality estimation score and human assessment.", "labels": [], "entities": []}, {"text": "We show that our models work for any translation output, without access to the translation system that produced the translations in question.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main goal of our experiments is to predict the normalized edit distance (HTER) ().", "labels": [], "entities": [{"text": "normalized edit distance (HTER)", "start_pos": 51, "end_pos": 82, "type": "METRIC", "confidence": 0.78935806453228}]}, {"text": "To estimate the quality of prediction we used the Pearson correlation coefficient.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 50, "end_pos": 81, "type": "METRIC", "confidence": 0.9204055666923523}]}, {"text": "As a regression model we used Random Forest (Ho, 1995) with a grid search algorithm for the optimization of parameters.", "labels": [], "entities": [{"text": "Random Forest (Ho, 1995)", "start_pos": 30, "end_pos": 54, "type": "DATASET", "confidence": 0.8586097274507795}]}, {"text": "To Our chosen implementation of word and phrase embeddings was FastText () with a continuous bag-of-words (CBOW) model and the number of dimensions for embeddings was set to 300.", "labels": [], "entities": []}, {"text": "MUSE () was used for extracting cross-lingual embeddings, with default parameters.", "labels": [], "entities": [{"text": "MUSE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5855271220207214}]}, {"text": "A simple beam search was implemented for finding the quality estimation BLEU2VEC score, with beam size 3.", "labels": [], "entities": [{"text": "quality estimation BLEU2VEC score", "start_pos": 53, "end_pos": 86, "type": "METRIC", "confidence": 0.8045016825199127}]}, {"text": "Initial tests showed that models with features based on cross-lingual embeddings only gave a close-to-zero Pearson correlation score, therefore these were not included as standalone features into the final experiments.", "labels": [], "entities": [{"text": "Pearson correlation score", "start_pos": 107, "end_pos": 132, "type": "METRIC", "confidence": 0.9638960758845011}]}, {"text": "A combination of cross-lingual embeddings (words, phrases, BPE) demonstrated a little bit better results but they were still lower than results obtained by using a model based on the attention weights.", "labels": [], "entities": [{"text": "BPE", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.918531060218811}]}, {"text": "The model with QuEst features was used as a baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of sentences for each language pair and each machine translation system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7067832350730896}]}, {"text": " Table 2: The Pearson correlation coefficients for the dev and test sets for all language pairs.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 14, "end_pos": 33, "type": "METRIC", "confidence": 0.9145530760288239}]}]}