{"title": [{"text": "Fine-grained evaluation of German-English Machine Translation based on a Test Suite", "labels": [], "entities": [{"text": "German-English Machine Translation", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.5275539457798004}]}], "abstractContent": [{"text": "We present an analysis of 16 state-of-the-art MT systems on German-English based on a linguistically-motivated test suite.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9797128438949585}]}, {"text": "The test suite has been devised manually by a team of language professionals in order to cover abroad variety of linguistic phenomena that MT often fails to translate properly.", "labels": [], "entities": [{"text": "MT", "start_pos": 139, "end_pos": 141, "type": "TASK", "confidence": 0.9493868947029114}]}, {"text": "It contains 5,000 test sentences covering 106 linguistic phenomena in 14 categories, with an increased focus on verb tenses, aspects and moods.", "labels": [], "entities": []}, {"text": "The MT outputs are evaluated in a semi-automatic way through regular expressions that focus only on the part of the sentence that is relevant to each phenomenon.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.8940158188343048}]}, {"text": "Through our analysis, we are able to compare systems based on their performance on these categories.", "labels": [], "entities": []}, {"text": "Additionally, we reveal strengths and weaknesses of particular systems and we identify grammatical phenomena where the overall performance of MT is relatively low.", "labels": [], "entities": [{"text": "MT", "start_pos": 142, "end_pos": 144, "type": "TASK", "confidence": 0.9911244511604309}]}], "introductionContent": [{"text": "The evaluation of Machine Translation (MT) has mostly relied on methods that produce a numerical judgment on the correctness of a test set.", "labels": [], "entities": [{"text": "evaluation of Machine Translation (MT)", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.8687591126986912}]}, {"text": "These methods are either based on the human perception of the correctness of the MT output, or on automatic metrics that compare the MT output with the reference translation ().", "labels": [], "entities": [{"text": "MT output", "start_pos": 81, "end_pos": 90, "type": "TASK", "confidence": 0.8489575982093811}]}, {"text": "In both cases, the evaluation is performed on a testset containing articles or small documents that are assumed to be a random representative sample of texts in this domain.", "labels": [], "entities": []}, {"text": "Moreover, this kind of evaluation aims at producing average scores that express a generic sense of correctness for the entire test set and compare the performance of several MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 174, "end_pos": 176, "type": "TASK", "confidence": 0.9732255935668945}]}, {"text": "Although this approach has been proven valuable for the MT development and the assessment of new methods and configurations, it has been suggested that a more fine-grained evaluation, associated with linguistic phenomena, may lead in a better understanding of the errors, but also of the efforts required to improve the systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9969654679298401}]}, {"text": "This is done through the use of test suites, which are carefully devised corpora, whose test sentences include the phenomena that need to be tested.", "labels": [], "entities": []}, {"text": "In this paper we present the fine-grained evaluation results of 16 state-of-the-art MT systems on German-English, based on a test suite focusing on 106 German grammatical phenomena with a focus on verb-related phenomena.", "labels": [], "entities": [{"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9790459871292114}]}], "datasetContent": [{"text": "The evaluation of the MT outputs was performed with TQ-AutoTest (), a tool that organizes the test items in a database, allowing the application of the regular expressions on new MT outputs.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 22, "end_pos": 32, "type": "TASK", "confidence": 0.8805599212646484}, {"text": "MT outputs", "start_pos": 179, "end_pos": 189, "type": "TASK", "confidence": 0.9072020947933197}]}, {"text": "For the purpose of this study, we have compared the 16 systems submitted to the test suite task of the EMNLP2018 Conference of Machine Translation (WMT18) for German\u2192English.", "labels": [], "entities": [{"text": "EMNLP2018 Conference of Machine Translation (WMT18)", "start_pos": 103, "end_pos": 154, "type": "TASK", "confidence": 0.8215629756450653}]}, {"text": "At the time that this paper is written, the creators of 11 of these systems have made their development characteristics available, 10 of them stating that they follow a NMT method and one of them a method combining phrase-based SMT and NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 228, "end_pos": 231, "type": "TASK", "confidence": 0.7531057596206665}]}, {"text": "After the application of the existing regular expressions to the outputs of these 16 systems, there was a considerable amount of warnings (i.e. uncertain judgments) that varied between 10% and 45% per system.", "labels": [], "entities": []}, {"text": "A manual inspection of the outputs was consequently performed, stage f) by a linguist, who invested approximately 80 hours of manual annotation.", "labels": [], "entities": []}, {"text": "A small-scale manual inspection of the automatically assigned pass and fail labels indicated that the percentage of the erroneously assigned labels is negligible.", "labels": [], "entities": []}, {"text": "The manual inspection therefore focused on warnings and reduced their amount to less than 10% warnings per system 2 . In particular, 32.1% of the original system outputs ended in warnings, after the application of the regular expressions, whereas the manual inspection and the refining of the regular expressions additionally validated 14,000 of these system outputs, i.e. 15.7% of the original test suite.", "labels": [], "entities": []}, {"text": "In order to analyze the results with respect to the existence of warnings, we performed two different types of analysis: 1.", "labels": [], "entities": []}, {"text": "Remove all sentences from the overall comparison that have even one warning for one system and the translation accuracy on the remaining segments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.970952033996582}]}, {"text": "The unsupervised systems are completely excluded from this analysis in order to keep the sample big enough.", "labels": [], "entities": []}, {"text": "This way, all systems are compared on the same set of segments.", "labels": [], "entities": []}, {"text": "2. Remove the sentences with warnings per system and calculate the translation accuracy on the remaining segments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9626982808113098}]}, {"text": "The unsupervised systems can be included in this analysis.", "labels": [], "entities": []}, {"text": "In this way, the systems are not compared on the same set of segments, but more segments can be included altogether.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: System accuracy (%) on each error category based on Analysis 1, having removed all test sentences whose evaluation remained uncertain, even for", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9486795663833618}]}, {"text": " Table 4: System accuracy (%) on linguistic phenomena related to verb tenses", "labels": [], "entities": [{"text": "System", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9687601923942566}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9001752734184265}]}, {"text": " Table 5: System accuracy (%) on linguistic phenomena related to verb types", "labels": [], "entities": [{"text": "System", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9652342796325684}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9192810654640198}]}]}