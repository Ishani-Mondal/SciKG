{"title": [{"text": "Hierarchical Convolutional Attention Networks for Text Classification", "labels": [], "entities": [{"text": "Hierarchical Convolutional Attention", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.645029197136561}, {"text": "Text Classification", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7798733413219452}]}], "abstractContent": [{"text": "Recent work in machine translation has demonstrated that self-attention mechanisms can be used in place of recurrent neural networks to increase training speed without sacrificing model accuracy.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.78655806183815}, {"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.9630676507949829}]}, {"text": "We propose combining this approach with the benefits of convolutional filters and a hierarchical structure to create a document classification model that is both highly accurate and fast to train-we name our method Hierarchical Convolutional Attention Networks.", "labels": [], "entities": [{"text": "document classification", "start_pos": 119, "end_pos": 142, "type": "TASK", "confidence": 0.6974340677261353}]}, {"text": "We demonstrate the effectiveness of this architecture by surpassing the accuracy of the current state-of-the-art on several classification tasks while being twice as fast to train.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9995182752609253}]}], "introductionContent": [{"text": "Text classification is an important research area in natural language processing (NLP).", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8358409106731415}, {"text": "natural language processing (NLP)", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.8137994905312856}]}, {"text": "Traditional text classification approaches utilize features generated from vector space models such as bag-ofwords or term frequency-inverse document frequency (TF-IDF)).", "labels": [], "entities": [{"text": "text classification", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7126642465591431}]}, {"text": "More recently, deep learning approaches have been shown to outperform traditional approaches based on vector space models ().", "labels": [], "entities": []}, {"text": "These newer deep learning approaches typically rely on architectures based off convolutional neural networks (CNNs) or recurrent neural networks (RNNs) (.", "labels": [], "entities": []}, {"text": "RNNs, which are designed to learn patterns over sequential data, have been successfully applied towards various NLP tasks ().", "labels": [], "entities": []}, {"text": "In NLP, RNNs typically process one word at a time and learn features based on complex sequences of words.", "labels": [], "entities": []}, {"text": "While RNNs are capable of capturing linguistic patterns useful for NLP tasks, especially overlong segments of text, they can be slow to train compared to other deep learning architectures -in order to calculate the gradients associated with any given word in a sequence, an RNN must backpropogate through all previous words in that sequence, resulting in backpropogation functions far more complex than those in feedforward or convolutional architectures.", "labels": [], "entities": []}, {"text": "CNNs, traditionally used for computer vision, have also been applied to NLP tasks with notable success (.", "labels": [], "entities": []}, {"text": "Unlike RNNs, which learn patterns across an entire sequence of text, CNNs use a sliding window that examines only a few words/characters at a time.", "labels": [], "entities": []}, {"text": "Thus, CNNs learn features based on the most salient combinations of X words/characters where X is determined by the window size used; unlike RNNs, CNNs are less capable of capturing linguistic features across long distances.", "labels": [], "entities": []}, {"text": "Despite this shortcoming, CNNs can often be as effective as RNNs in many basic NLP tasks (.", "labels": [], "entities": []}, {"text": "Furthermore, CNNs are generally faster to train than RNNs.", "labels": [], "entities": [{"text": "CNNs", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.9529735445976257}]}, {"text": "In this paper, we introduce Hierarchical Convolutional Attention Networks (HCANs), an architecture based off self-attention that can capture linguistic relationships overlong sequences like RNNs while still being fast to train like CNNs.", "labels": [], "entities": []}, {"text": "HCANs can achieve accuracy that surpasses the current state-of-the-art on several classification tasks while being twice as fast to train.", "labels": [], "entities": [{"text": "HCANs", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8444363474845886}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.999372661113739}]}], "datasetContent": [{"text": "We evaluate the performance of the HCAN on four classification tasks using three datasets.", "labels": [], "entities": [{"text": "HCAN", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.7740267515182495}]}, {"text": "The Yelp reviews dataset 1 consists of over 4.7 million Yelp reviews of various businesses collected over 12 metropolitan areas.", "labels": [], "entities": [{"text": "Yelp reviews dataset 1", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8970661014318466}]}, {"text": "For our task, we use only reviews from 2016 (approximately 1 million reviews) and try to predict the rating 1-5.", "labels": [], "entities": []}, {"text": "The Amazon reviews dataset consists of 83.68 million Amazon product reviews from different product categories spanning May 1996 to July 2014.", "labels": [], "entities": [{"text": "Amazon reviews dataset", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8627234101295471}]}, {"text": "For For our experiments, we use Pubmed abstracts associated with 8 common medical subject heading (MeSH) labels: metabolism, physiology, genetics, chemistry, pathology, surgery, psychology, and diagnosis.", "labels": [], "entities": []}, {"text": "We only use abstracts that are associated with a single label, yielding a final selection of 800,000 abstracts, 100,000 for each label.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset Descriptions  Dataset  Classes Documents Vocabulary Task Description  Yelp Reviews 2016  5 1,033,037  72,880  Sentiment Analysis  Amazon Reviews Sentiment  5 500,000  67,802  Sentiment Analysis  Amazon Reviews Category  10 500,000  67,802  Topic Classification  Pubmed  8 800,000  182,167  Topic Classification", "labels": [], "entities": [{"text": "Dataset Descriptions  Dataset  Classes Documents Vocabulary Task Description  Yelp Reviews 2016  5 1,033,037  72,880  Sentiment Analysis  Amazon Reviews Sentiment  5 500,000  67,802  Sentiment Analysis  Amazon Reviews Category  10 500,000  67,802  Topic Classification  Pubmed  8 800,000  182,167  Topic Classification", "start_pos": 10, "end_pos": 328, "type": "DATASET", "confidence": 0.8191213105854235}]}]}