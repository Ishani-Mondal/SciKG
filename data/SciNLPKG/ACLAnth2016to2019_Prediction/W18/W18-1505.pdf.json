{"title": [], "abstractContent": [{"text": "We present a general framework of analyzing existing story corpora to generate controllable and creative new stories.", "labels": [], "entities": []}, {"text": "The proposed framework needs little manual annotation to achieve controllable story generation.", "labels": [], "entities": [{"text": "controllable story generation", "start_pos": 65, "end_pos": 94, "type": "TASK", "confidence": 0.705895741780599}]}, {"text": "It creates anew interface for humans to interact with computers to generate personalized stories.", "labels": [], "entities": []}, {"text": "We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence 1 (Egidi and Gerrig, 2009) and storyline.", "labels": [], "entities": []}, {"text": "Experiments show that our methods successfully achieve the control and enhance the coherence of stories through introducing storylines. with additional control factors, the generation model gets lower per-plexity, and yields more coherent stories that are faithful to the control factors according to human evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Storytelling is an important task in natural language generation, which plays a crucial role in the generation of various types of texts, such as novels, movies, and news articles.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.7106249332427979}]}, {"text": "Automatic story generation efforts started as early as the 1970s with the TALE-SPIN system.", "labels": [], "entities": [{"text": "Automatic story generation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.616784910360972}, {"text": "TALE-SPIN", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.6884059309959412}]}, {"text": "Early attempts in this field relied on symbolic planning;, casebased reasoning), or generalizing knowledge from existing stories to assemble new ones.", "labels": [], "entities": []}, {"text": "In recent years, deep learning models are used to capture higher level structure in stories.", "labels": [], "entities": []}, {"text": "use skip-thought vectors () to encode sentences, and a Long Short-Term Memory (LSTM) network  erate stories.", "labels": [], "entities": []}, {"text": "train a recurrent encoder-decoder neural network () to predict the next event in the story.", "labels": [], "entities": []}, {"text": "Despite significant progress in automatic story generation, there has been less emphasis on controllability: having a system takes human inputs and composes stories accordingly.", "labels": [], "entities": [{"text": "story generation", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7042539715766907}]}, {"text": "With the recent successes on controllable generation of images, dialog responses (, poems (, and different styles of text (.", "labels": [], "entities": []}, {"text": "people would want to control a story generation system to produce interesting and personalized stories.", "labels": [], "entities": [{"text": "story generation", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7056570798158646}]}, {"text": "This paper emphasizes the controllability aspect.", "labels": [], "entities": []}, {"text": "We propose a completely data-driven approach towards controllable story generation by analyzing the existing story corpora.", "labels": [], "entities": [{"text": "controllable story generation", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.7419828573862711}]}, {"text": "First, an analyzer extracts control factors from existing stories, and then a generator learns to generate stories according to the control factors.", "labels": [], "entities": []}, {"text": "This creates an excellent interface for humans to interact: the generator can take human-supplied control factors to generate stories that reflect a user's intent.", "labels": [], "entities": []}, {"text": "gives the overview (upper) and an example (lower) of the framework.", "labels": [], "entities": []}, {"text": "The instantiations of the analyzer and the generator are flexible and can be easily applied to different scenarios.", "labels": [], "entities": []}, {"text": "We explore two control factors: (1) ending valence (happy or sad ending) and (2) storyline keywords.", "labels": [], "entities": [{"text": "ending valence", "start_pos": 36, "end_pos": 50, "type": "METRIC", "confidence": 0.8846863508224487}]}, {"text": "We use supervised classifiers and rule-based keyword extractors for analysis, and conditional RNNs for generation.", "labels": [], "entities": []}, {"text": "The contributions of the paper are two-fold: 1.", "labels": [], "entities": []}, {"text": "We propose a general framework enabling interactive story generation by analyzing existing story corpora.", "labels": [], "entities": [{"text": "interactive story generation", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.7454062700271606}]}, {"text": "2. We apply the framework to control story ending valence and storyline, and show that with these additional control factors, our models generate stories that are both more coherent and more faithful to human inputs.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on the ROCstories dataset (, which consists of 98,162 five-line stories for training, and 1871 stories each for the development and test sets.", "labels": [], "entities": [{"text": "ROCstories dataset", "start_pos": 30, "end_pos": 48, "type": "DATASET", "confidence": 0.9571351408958435}]}, {"text": "We treat the first four sentences of each story as the body and the last sentence as the ending.", "labels": [], "entities": []}, {"text": "We build analyzers to annotate the ending valence and the storyline for every story, and train the two controlled generators with 98,162 annotated stories.", "labels": [], "entities": []}, {"text": "We compare the controlled generation under our proposed framework with the uncontrolled generation.", "labels": [], "entities": []}, {"text": "We design the experiments to answer the following research questions: 1.", "labels": [], "entities": []}, {"text": "How does the controlled generation framework affect the generation quantitatively?", "labels": [], "entities": []}, {"text": "2. Does the proposed framework enables controls to the stories while maintaining the coherence of the stories?", "labels": [], "entities": []}, {"text": "To answer the former question, we design automatic evaluations that measure the perplexity of the models given appropriate and inappropriate controls.", "labels": [], "entities": []}, {"text": "For the latter question, we design human evaluations to compare the generated stories from controlled and uncontrolled versions in terms of the document-level coherence and the faithfulness to the control factors.", "labels": [], "entities": []}, {"text": "The advantages of the automatic evaluation is that it can be conducted at scale and gives panoramic views of the systems.", "labels": [], "entities": []}, {"text": "We compute the perplexities of different models on the ROCstories development dataset.", "labels": [], "entities": [{"text": "ROCstories development dataset", "start_pos": 55, "end_pos": 85, "type": "DATASET", "confidence": 0.9148238301277161}]}, {"text": "shows the results for the storyline experiments.", "labels": [], "entities": []}, {"text": "With the additional storyline information, it is easier for the generation model to guess what will happen next in a story, thus yield lower perplexities.", "labels": [], "entities": []}, {"text": "We conduct the same experiments for ending valence controlled generation and observe the same.", "labels": [], "entities": [{"text": "valence controlled generation", "start_pos": 43, "end_pos": 72, "type": "TASK", "confidence": 0.6652831931908926}]}, {"text": "However, since ending valence is only one bit of information, the perplexity difference is only 0.8.", "labels": [], "entities": []}, {"text": "We conduct a human evaluation with 1000 story groups for each setting.", "labels": [], "entities": []}, {"text": "Each group consists of stories from: (1) the uncontrolled LSTM generation model, (2) controlled generation with our framework, and (3) a contrastive method which retrieves and re-ranks existing sentences in the training data.", "labels": [], "entities": [{"text": "LSTM generation", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.8333287537097931}]}, {"text": "Users are asked to rate the three stories on a 1-5 scale with respect to faithfulness (whether stories reflect the control factor), and coherence.", "labels": [], "entities": []}, {"text": "All the evaluations are conducted on Amazon Mechanic Turk.", "labels": [], "entities": [{"text": "Amazon Mechanic Turk", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.982038656870524}]}, {"text": "We compute the average score and percentage win of each method.", "labels": [], "entities": []}, {"text": "Ending Valence For the ending valence control, we supply each system with the first 4 sentences from ROCStories test set and an ending valence randomly assigned by a human.", "labels": [], "entities": [{"text": "ROCStories test set", "start_pos": 101, "end_pos": 120, "type": "DATASET", "confidence": 0.8842981457710266}]}, {"text": "The systems generate endings . We only let the systems generate happyEnding or sadEnding stories, with the ratio around 1:1.", "labels": [], "entities": []}, {"text": "Faithfulness is defined as whether the generated stories reflect the given ending valence.", "labels": [], "entities": []}, {"text": "The contrastive method retrieves existing happy or sad endings from the training data instead of generating new sentences.", "labels": [], "entities": []}, {"text": "Specifically, we gather all the stories that are annotated with happyEndings from the 3980 annotated stories in one set, and all the sadEndings in another set.", "labels": [], "entities": []}, {"text": "When the given ending valence is happyEnding, the sys-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Annotation agreement for labeling story ending", "labels": [], "entities": [{"text": "labeling story", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.9281037449836731}]}, {"text": " Table 2: Perplexities on the ROCstories development data.", "labels": [], "entities": [{"text": "ROCstories development data", "start_pos": 30, "end_pos": 57, "type": "DATASET", "confidence": 0.821005125840505}]}, {"text": " Table 3: Human evaluation for the ending valence (left) and storyline (right) controlled generation. Scores range in", "labels": [], "entities": []}]}