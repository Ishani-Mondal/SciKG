{"title": [{"text": "Modeling Student Response Times: Towards Efficient One-on-one Tutoring Dialogues", "labels": [], "entities": [{"text": "Modeling Student Response", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8819958964983622}]}], "abstractContent": [{"text": "In this paper we investigate the task of modeling how long it would take a student to respond to a tutor question during a tutoring dialogue.", "labels": [], "entities": []}, {"text": "Solving such a task has applications in educational settings such as intelligent tutoring systems, as well as in platforms that help busy human tutors to keep students engaged.", "labels": [], "entities": []}, {"text": "Knowing how long it would normally take a student to respond to different types of questions could help tutors optimize their own time while answering multiple dialogues concurrently, as well as deciding when to prompt a student again.", "labels": [], "entities": []}, {"text": "We study this problem using data from a service that offers tutor support for math, chemistry and physics through an instant messaging platform.", "labels": [], "entities": []}, {"text": "We create a dataset of 240K questions.", "labels": [], "entities": []}, {"text": "We explore several strong baselines for this task and compare them with human performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "One-on-one tutoring is often considered the goldstandard of educational interventions.", "labels": [], "entities": []}, {"text": "Past work suggests that this form of personalized instruction can increase student performance by two standard deviation units.", "labels": [], "entities": []}, {"text": "Chatbots, intelligent tutoring systems (ITS), and remote tutoring are often seen as away of providing this form of personalized instruction at an economical scale).", "labels": [], "entities": []}, {"text": "However, their key limitation is that they are unable to identify when students have disengaged or are struggling with a task.", "labels": [], "entities": []}, {"text": "Tutors and ITS need to calibrate how frequently and often they message their students.", "labels": [], "entities": []}, {"text": "Prompting students too frequently could result in students feeling frustrated and disrupted, while prompting too slowly could result in students becoming disengaged or simply not learning as fast they could have with more prompting.", "labels": [], "entities": []}, {"text": "This task is further complicated by the fact that interactions between students and a digital platform involve tasks of varying complexity and duration (such as performing a calculation, explaining a definition, or answering yes or no).", "labels": [], "entities": []}, {"text": "We propose predicting response latency of a tutor's question, as an indirect measure of a student's engagement) and question complexity ().", "labels": [], "entities": []}, {"text": "The domain that we work with is tutoring session transcripts from an on-demand tutoring company, in which students take photos of their math, chemistry, and physics problems with their mobile or tablet devices.", "labels": [], "entities": []}, {"text": "These images are then sent to a tutor in a remote location.", "labels": [], "entities": []}, {"text": "Tutors and students then communicate back and forth over text messages until the student is able to solve the problem.", "labels": [], "entities": []}, {"text": "Specifically, the task that we focus on is: given a question from the tutor, predict whether it can be responded immediately or it is a question that requires more thought (see).", "labels": [], "entities": []}, {"text": "We formulate this task as a binary classification problem (short/long) whose inputs are the tutor's question and several dialogue contextual features.", "labels": [], "entities": []}, {"text": "In this paper we make the following contributions: \u2022 We define the task of modeling student response latency in order to make one-on-one tutoring dialogue more efficient.", "labels": [], "entities": []}, {"text": "\u2022 We have partnered with an Educational Technology company called Yup to produce one of the largest educational dialogue corpora to date.", "labels": [], "entities": []}, {"text": "This initial dataset including over 18K tutoring sessions spanning 7K hours of textbased dialogue, including over 240K questions from tutors.", "labels": [], "entities": []}, {"text": "\u2022 We explore several strong classifiers for this task 1 whose performance is statistically significant better than expert human performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our approach in testing the above hypotheses posed in Section 2 is setting forth experimental augmentations to the baselines introduced above, and evaluating the weighted F 1 scores across all classes in order to assess performance.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 171, "end_pos": 181, "type": "METRIC", "confidence": 0.9681016008059183}]}, {"text": "In other words, we add a feature as a time and evaluate the F 1 score, as reported in.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9910047650337219}]}, {"text": "In this section, each experiment corresponds to the hypothesis with the same number.", "labels": [], "entities": []}, {"text": "For most of our experiments (excluding the RNN), we use both logistic regression and SVM on a bag-of-words model concatenated with respective additional feature(s) (e.g. question word length, question duration, etc.).", "labels": [], "entities": []}, {"text": "For all experiments, we conduct a randomized hyper-parameter search for each model and pick the model that performs the best on the dev set.", "labels": [], "entities": []}, {"text": "Adding question length as a feature improves performance, validating H.1.", "labels": [], "entities": [{"text": "H.1", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.6342068314552307}]}, {"text": "Furthermore, longer questions (which usually involve a lot of technical information in the form of formulae/equations, or are an aggregation of repeated questions) tend to result in higher response times.", "labels": [], "entities": []}, {"text": "This is contrary to results seen in (), but inline with those seen in.", "labels": [], "entities": []}, {"text": "These results potentially indicate behavioral differences between the two domains -instant messaging (, and tutorial dialogue in virtual environments; the latter also being the domain of our current work.", "labels": [], "entities": []}, {"text": "We notice similar trends while analyzing question duration as a feature.", "labels": [], "entities": []}, {"text": "Using question duration along with bag-of-words features helped boost model performance, verifying H.2.", "labels": [], "entities": [{"text": "H.2", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.7431346774101257}]}, {"text": "Intuitively, this feature seems to bean indicative measure of question complexity, and longer duration questions result in higher response times.", "labels": [], "entities": []}, {"text": "H.3 is a mixed bag.", "labels": [], "entities": [{"text": "H.3", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9424540400505066}]}, {"text": "We start off by adding different spans of previous dialogue turn text.", "labels": [], "entities": []}, {"text": "This helps improve performance on the dev set but does not add anything over the baseline when evaluated on the test set, suggesting that these features do not generalize well across conversations.", "labels": [], "entities": []}, {"text": "On the contrary, adding previous dialogue times helps improve model performance in both the dev and test sets.", "labels": [], "entities": []}, {"text": "In both settings, we find the best results while using 5 turns of previous dialogue.", "labels": [], "entities": []}, {"text": "Word entrainment seems to have no effect on model performance.", "labels": [], "entities": [{"text": "Word entrainment", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.655176654458046}]}, {"text": "There are no significant differences based on the set of words used to measure entrainment (function words or 25/50/100 most frequent words), as well as the metric of lexical distance (Jensen-Shannon divergence/cosine similarity).", "labels": [], "entities": []}, {"text": "Therefore, we cannot confirm the validity of H.4 in our setting.", "labels": [], "entities": [{"text": "validity", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9731078147888184}]}, {"text": "A similar narrative is observed with sentiment (H.5).", "labels": [], "entities": []}, {"text": "We note that sentiment analysis is less accurate when sentences get longer, and this might be one of the causes for the relative ineffectiveness of sentiment as a feature.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.9005353450775146}]}, {"text": "Another possible interpretation is that this text is not aligned well with traditional definitions of sentiment.", "labels": [], "entities": []}, {"text": "Many terms in mathematics are neutral but are classified with negative sentiment on a high valence.", "labels": [], "entities": []}, {"text": "In future work we plan to explore the use of sentiment analysis on student generated text rather than on tutor questions.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.9348015785217285}]}, {"text": "The results of using deep learning models (RNN) are promising (H.6).", "labels": [], "entities": []}, {"text": "The RNN achieves a performance which is statistically significant better than the baseline with the same feature: only the question text.", "labels": [], "entities": []}, {"text": "A probable reason is that the baseline uses unigrams, hence it loses the order among the words of the question while the RNN model might benefit from this information.", "labels": [], "entities": []}, {"text": "It must be noted that we have not performed extensive hyperparameter tuning, performance might be further improved with more hyperparameter tuning.", "labels": [], "entities": []}], "tableCaptions": []}