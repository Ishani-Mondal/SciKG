{"title": [], "abstractContent": [], "introductionContent": [{"text": "Neural attention-based sequence-to-sequence models (seq2seq)) have proven to be accurate and robust for many sequence prediction tasks.", "labels": [], "entities": [{"text": "sequence prediction tasks", "start_pos": 109, "end_pos": 134, "type": "TASK", "confidence": 0.780231773853302}]}, {"text": "They have become the standard approach for automatic translation of text, at the cost of increased model complexity and uncertainty.", "labels": [], "entities": [{"text": "automatic translation of text", "start_pos": 43, "end_pos": 72, "type": "TASK", "confidence": 0.8234277963638306}]}, {"text": "End-to-end trained neural models act as a black box, which makes it difficult to examine model decisions and attribute errors to a specific part of a model.", "labels": [], "entities": []}, {"text": "The highly connected and high-dimensional internal representations pose a challenge for analysis and visualization tools.", "labels": [], "entities": []}, {"text": "The development of methods to understand seq2seq predictions is crucial for systems in production settings, as mistakes involving language are often very apparent to human readers.", "labels": [], "entities": []}, {"text": "For instance, a widely publicized incident resulted from a translation system mistakenly translating \"good morning\" into \"attack them\" leading to a wrongful arrest.", "labels": [], "entities": []}, {"text": "In this work, we present the visual analysis tool SEQ2SEQ-VIS that allows interaction and \"what if\"-style exploration of trained seq2seq models through each stage of the translation process.", "labels": [], "entities": []}, {"text": "The aim is to identify which patterns have been learned, to detect errors within a model, and to understand the model through counterfactual scenarios.", "labels": [], "entities": []}, {"text": "In order to investigate the origin of an error within a seq2seq model, we separate errors within each translation stage into the following categories: (1) representation errors, in which an encoder or decoder misrepresent a word within a given context (2) alignment errors, in which the attention focuses on the wrong word, and (3) decoding errors, in which the prediction assigns a wrong probability distribution over words, or the beam search fails to include the correct solution.", "labels": [], "entities": []}, {"text": "We define three steps within an analysis that aim to understand the prediction process, understand how an output relates to training data, and examine causal relationships between inputs and outputs.", "labels": [], "entities": []}, {"text": "Examine Model Outputs: SEQ2SEQ-VIS shows a separate visual representation for the output of each stage of the seq2seq pipeline.", "labels": [], "entities": []}, {"text": "Connect Outputs to Samples: SEQ2SEQ-VIS connects the encoder and decoder of a seq2seq model to relevant training examples by showing a neighborhood of examples with the most similar internal states.", "labels": [], "entities": []}, {"text": "Test Alternative Decisions: SEQ2SEQ-VIS enables \"what if\" explorations and causal relationship testing by manipulation of inputs, attention, and outputs.", "labels": [], "entities": []}, {"text": "The full system is shown in.", "labels": [], "entities": []}, {"text": "It combines visualizations for the external components with internal representations from specific examples and nearest-neighbor lookups over a corpus of precomputed examples.", "labels": [], "entities": []}, {"text": "The entire system integrates with OpenNMT (, one of the largest open source seq2seq libraries.", "labels": [], "entities": [{"text": "OpenNMT", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9559730291366577}]}], "datasetContent": [], "tableCaptions": []}