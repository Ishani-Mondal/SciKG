{"title": [{"text": "The LMU Munich Unsupervised Machine Translation Systems", "labels": [], "entities": [{"text": "LMU", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9208624958992004}, {"text": "Unsupervised Machine Translation", "start_pos": 15, "end_pos": 47, "type": "TASK", "confidence": 0.6489379902680715}]}], "abstractContent": [{"text": "We describe LMU Munich's unsuper-vised machine translation systems for English\u2194German translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.6802113503217697}, {"text": "English\u2194German translation", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.6530800983309746}]}, {"text": "These systems were used to participate in the WMT18 news translation shared task and more specifically, for the unsupervised learning sub-track.", "labels": [], "entities": [{"text": "WMT18 news translation shared task", "start_pos": 46, "end_pos": 80, "type": "TASK", "confidence": 0.8033655405044555}]}, {"text": "The systems are trained on English and German monolingual data only and exploit and combine previously proposed techniques such as using word-byword translated data based on bilingual word embeddings, denoising and on-the-fly backtranslation.", "labels": [], "entities": []}], "introductionContent": [{"text": "The LMU Munich's Center for Information and Language Processing participated in the WMT 2018 news translation shared task for English\u2194German translation.", "labels": [], "entities": [{"text": "WMT 2018 news translation shared task", "start_pos": 84, "end_pos": 121, "type": "TASK", "confidence": 0.7526421149571737}, {"text": "English\u2194German translation", "start_pos": 126, "end_pos": 152, "type": "TASK", "confidence": 0.5987662747502327}]}, {"text": "Specifically, we participated in the unsupervised learning task which focuses on training MT models without access to any parallel data.", "labels": [], "entities": [{"text": "MT", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.9555544257164001}]}, {"text": "The team has a strong track record at previous WMT shared tasks) working on SMT systems) and proposed atop scoring linguistically informed neural machine translation system) based on human evaluation at WMT17.", "labels": [], "entities": [{"text": "WMT shared tasks", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.5813083648681641}, {"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9865380525588989}, {"text": "linguistically informed neural machine translation", "start_pos": 115, "end_pos": 165, "type": "TASK", "confidence": 0.6456518232822418}, {"text": "WMT17", "start_pos": 203, "end_pos": 208, "type": "DATASET", "confidence": 0.968726634979248}]}, {"text": "Neural machine translation (NMT) is state-ofthe-art in automatic translation.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7853182752927145}, {"text": "automatic translation", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.7190201282501221}]}, {"text": "Attention-based neural sequence-to-sequence models () have been established as the basis for most recent work in MT and furthermore, have been used to obtain best scoring systems at WMT in recent years (.", "labels": [], "entities": [{"text": "MT", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.9936873316764832}, {"text": "WMT", "start_pos": 182, "end_pos": 185, "type": "TASK", "confidence": 0.49601033329963684}]}, {"text": "Previous work and the best scoring systems at WMT also showed that NMT can be scaled to millions of sentence pairs and even achieve human parity (.", "labels": [], "entities": [{"text": "WMT", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.912820041179657}]}, {"text": "However, this comes under the caveat that we have access to a large amount of human-translated parallel data.", "labels": [], "entities": []}, {"text": "showed that NMT models cannot be properly trained under low resource conditions and are still behind phrase-based models.", "labels": [], "entities": []}, {"text": "In extremely low resource scenarios, NMT fails completely which is a big obstacle if we want to enable automatic translation over a variety of languages.", "labels": [], "entities": []}, {"text": "This motivates the unsupervised learning task at WMT this year.", "labels": [], "entities": [{"text": "WMT", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8913610577583313}]}, {"text": "The task is run for three language pairs, but we only focus on English\u2194German translation.", "labels": [], "entities": [{"text": "English\u2194German translation", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.6227162256836891}]}, {"text": "Although this language pair has an abundance of parallel data, we are constrained to only using monolingual data provided for the WMT18 news translation task, excluding Europarl and News Commentary because of content overlap.", "labels": [], "entities": [{"text": "WMT18 news translation task", "start_pos": 130, "end_pos": 157, "type": "TASK", "confidence": 0.8560622930526733}, {"text": "Europarl", "start_pos": 169, "end_pos": 177, "type": "DATASET", "confidence": 0.9731109142303467}]}, {"text": "The systems we use for our submissions are based on the recently proposed techniques for unsupervised machine translation by several studies (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7346240282058716}]}, {"text": "The phrase-based unsupervised system uses bilingual word embeddings (BWEs) to create an initial phrase table and also utilizes a target-side ngram language model.", "labels": [], "entities": []}, {"text": "The backbone of the unsupervised NMT methods is denoising and onthe-fly backtranslation which enable a standard NMT architecture to be trained by only leveraging monolingual data.", "labels": [], "entities": []}, {"text": "The model for our submission is mostly based on the work of.", "labels": [], "entities": []}, {"text": "Additionally, we explore how word-byword translated data based on BWEs can be utilized to improve the initial training and experiment with different ways of producing these translations.", "labels": [], "entities": [{"text": "BWEs", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.8315017819404602}]}, {"text": "We also show that disabling denoising in the last stages of learning can provide for further improvements.", "labels": [], "entities": []}, {"text": "We refer the reader to for our supervised systems for news and biomedical translation.", "labels": [], "entities": [{"text": "biomedical translation", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.7442349493503571}]}, {"text": "The remainder of the paper outlines the methods we used for generating BWEs, training a phrasebased and neural unsupervised machine translations systems.", "labels": [], "entities": []}, {"text": "Moreover, it presents the obtained results as well as translation examples showcasing some of the strong and weak points of the NMT system.", "labels": [], "entities": []}], "datasetContent": [{"text": "The models in this work are trained on German and English NewsCrawl articles from 2007 to 2017.", "labels": [], "entities": [{"text": "German and English NewsCrawl articles from 2007", "start_pos": 39, "end_pos": 86, "type": "DATASET", "confidence": 0.6967224052974156}]}, {"text": "Since the total size of this data is very large, we randomly sampled 4M sentences for each language.", "labels": [], "entities": []}, {"text": "Moreover, we study if there is any noticeable effect if we only utilize more recent data.", "labels": [], "entities": []}, {"text": "As a result, we sampled 4M samples from NewsCrawl 2017 and report results with this dataset as well.", "labels": [], "entities": [{"text": "4M samples from NewsCrawl 2017", "start_pos": 24, "end_pos": 54, "type": "DATASET", "confidence": 0.8313838720321656}]}, {"text": "The datasets are tokenized and truecased with the standard scripts from the Moses toolkit (.", "labels": [], "entities": []}, {"text": "When training the truecase models, we actually use all of the available NewsCrawl data, rather than our subsample.", "labels": [], "entities": [{"text": "NewsCrawl data", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.9702019989490509}]}, {"text": "We also use BPE splitting.", "labels": [], "entities": [{"text": "BPE splitting", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.688659206032753}]}, {"text": "The BPE segmentation is computed jointly on all the NewsCrawl data available for both languages.", "labels": [], "entities": [{"text": "BPE segmentation", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7837066948413849}, {"text": "NewsCrawl data", "start_pos": 52, "end_pos": 66, "type": "DATASET", "confidence": 0.9703249931335449}]}, {"text": "Then, all sentences with more than 50 tokens are discarded.", "labels": [], "entities": []}, {"text": "The NewsCrawl data is also used to train the BPE-level embeddings.", "labels": [], "entities": [{"text": "NewsCrawl data", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9797429442405701}, {"text": "BPE-level", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.7759889960289001}]}, {"text": "We implement our neural system on top of the code made available by.", "labels": [], "entities": []}, {"text": "The model is an attention-based encoder-decoder NMT with 2-layer GRU encoder and decoder.", "labels": [], "entities": []}, {"text": "The number of hidden units is 600.", "labels": [], "entities": []}, {"text": "We set the learning rate to 0.0002 and dropout in the encoder and decoder to 0.3.", "labels": [], "entities": []}, {"text": "We checkpoint the model each 10K updates.", "labels": [], "entities": []}, {"text": "The batch size is 32.", "labels": [], "entities": []}, {"text": "We present our word-by-word translation baseline results in.", "labels": [], "entities": []}, {"text": "Using bigrams on the English side helped for de-en but not for en-de.", "labels": [], "entities": [{"text": "English side", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.9481171667575836}]}, {"text": "By analyzing translations we can conclude that 1) German compound words are correctly translated to multiple words in many cases and 2) the drop of en-de direction is caused by incorrectly translating bigrams, that are non-compounds on the target side, to one token units.", "labels": [], "entities": []}, {"text": "On the other hand, using orthographic information gave significant improvements in both directions.", "labels": [], "entities": []}, {"text": "The technique alone provided for improved translation of named entities without the use of a costly NER system.", "labels": [], "entities": [{"text": "translation of named entities", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.8986687213182449}]}, {"text": "We got our best results by combining bigrams and orthographic similarity for German\u2192English.", "labels": [], "entities": []}, {"text": "Comparing the results with the unsupervised and lightly supervised mapping it can be seen that the two systems are on par in performance, the former results higher BLEU points in case of de-en but lower for en-de.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.999470055103302}]}, {"text": "Our conjecture is that the multiple translations of the source words in the used lexicon helped tackle the morphological richness of the German language on the target side while it was not helpful otherwise.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Baseline results (BLEU) with word-by-word trans-", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9900526404380798}]}, {"text": " Table 2: BLEU scores with the unsupervised systems on", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998204231262207}]}]}