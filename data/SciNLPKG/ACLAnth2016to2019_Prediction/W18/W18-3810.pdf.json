{"title": [], "abstractContent": [{"text": "Most extractive summarization techniques operate by ranking all the source sentences and then select the top-ranked sentences as the summary.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 5, "end_pos": 29, "type": "TASK", "confidence": 0.6698917150497437}]}, {"text": "Such methods are known to produce good summaries , especially when applied to news articles and scientific texts.", "labels": [], "entities": [{"text": "summaries", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.9769533276557922}]}, {"text": "However, they do not fare so well when applied to texts such as fictional narratives, which do not have a single central or recurrent theme.", "labels": [], "entities": []}, {"text": "This is because usually the information or plot of the story is spread across several sentences.", "labels": [], "entities": []}, {"text": "In this paper, we discuss a different summarization technique called Telegraphic Summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.9896369576454163}, {"text": "Telegraphic Summarization", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.8251164853572845}]}, {"text": "Here, we do not select whole sentences, rather pick short segments of text spread across sentences, as the summary.", "labels": [], "entities": []}, {"text": "We have tailored a set of guidelines to create such summaries and, using the same, annotate a gold corpus of 200 English short stories.", "labels": [], "entities": []}], "introductionContent": [{"text": "The purpose of summarization is to capture all the useful information from the source text in as few words as possible.", "labels": [], "entities": [{"text": "summarization", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9880470037460327}]}, {"text": "Extractive summarization involves identifying parts of the text which are important.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9503997564315796}]}, {"text": "Such summaries are usually generated by ranking all the source sentences, according to some heuristic or metric, and then selecting the top sentences as the summary.", "labels": [], "entities": []}, {"text": "Most extractive summarization systems have been developed for domains such as newswire articles (), encyclopedic and scientific texts ().", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 5, "end_pos": 29, "type": "TASK", "confidence": 0.7530190050601959}]}, {"text": "They work well in such domains because these texts revolve around a central theme and the information is often enforced by reiteration across several sentences.", "labels": [], "entities": []}, {"text": "However, fictional narratives do not talk about a single topic.", "labels": [], "entities": []}, {"text": "They describe a sequence of events and often contain dialogue.", "labels": [], "entities": []}, {"text": "Information is not repeated and each sentence contributes to developing the plot further.", "labels": [], "entities": []}, {"text": "Hence, selecting a subset of such sentences does not accurately capture the story.", "labels": [], "entities": []}, {"text": "In this paper, we focus on telegraphic summarization.", "labels": [], "entities": [{"text": "telegraphic summarization", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.9344918131828308}]}, {"text": "Telegraphic summary does not contain whole sentences, instead, shorter segments are selected across sentences, and reads like a telegram.", "labels": [], "entities": [{"text": "Telegraphic summary", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.5859850347042084}]}, {"text": "We have described a set of guidelines to create such summaries.", "labels": [], "entities": []}, {"text": "These guidelines were used to annotate a gold corpus of 200 English short stories.", "labels": [], "entities": [{"text": "gold corpus of 200 English short stories", "start_pos": 41, "end_pos": 81, "type": "DATASET", "confidence": 0.8246109400476728}]}, {"text": "The dataset can be very useful to gain an insight into story structures.", "labels": [], "entities": []}, {"text": "No such corpus already exists, to the best of our knowledge.", "labels": [], "entities": []}, {"text": "Formally, the paper makes the following contributions: 1.", "labels": [], "entities": []}, {"text": "Discusses drawbacks of applying traditional extractive summarization methods to narrative texts.", "labels": [], "entities": []}, {"text": "2. Describes a set of guidelines to generate telegraphic summaries.", "labels": [], "entities": [{"text": "telegraphic summaries", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7097395062446594}]}, {"text": "3. Provides a gold corpus of 200 short stories and their telegraphic summaries annotated using these guidelines.", "labels": [], "entities": [{"text": "telegraphic summaries annotated", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.7988690733909607}]}, {"text": "4. Provides abstractive summaries for 50 stories and a set of 45 multiple-choice questions (MCQs) for evaluation purposes.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses existing and related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the data collection and summarization process.", "labels": [], "entities": [{"text": "data collection", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.6857549995183945}, {"text": "summarization", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9180689454078674}]}, {"text": "Section 4 discusses the analysis performed on the dataset.", "labels": [], "entities": []}, {"text": "Section 5 discusses conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The length of the stories varies from 300 to 1100 words, with the average length being 650.", "labels": [], "entities": [{"text": "length", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9567658305168152}]}, {"text": "The average summarization factor (length of summary/length of the story) is 0.37 and 0.36 for telegraphic and abstractive summaries respectively.", "labels": [], "entities": []}, {"text": "From we can see that the summarization factor tends to be high for very short stories since nearly every word is important.", "labels": [], "entities": [{"text": "summarization", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.7539750933647156}]}, {"text": "It reduces gradually as the length of the story increases because longer stories tend to be more descriptive and contain extraneous information not relevant to the central plot.", "labels": [], "entities": []}, {"text": "We used the Alpha metric proposed in to measure the inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Alpha was computed based on 20 cross-annotated stories and found to be 0.73.", "labels": [], "entities": [{"text": "Alpha", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9864704012870789}]}, {"text": "We generated 50 summaries using popular online extractive summarization tools, Smmry 2 and Resoomer 3 , which generate summaries by ranking and selecting the top sentences (after the selection step the sentences are re-arranged according to the source order).", "labels": [], "entities": []}, {"text": "These summaries have the same summarization factor as the corresponding telegraphic summaries.", "labels": [], "entities": []}, {"text": "ROUGE-N score) is a popular metric used to evaluate summaries produced by a system.", "labels": [], "entities": [{"text": "ROUGE-N score", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9731139838695526}]}, {"text": "ROUGE-N recall is computed as shown in Eq.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9798484444618225}, {"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.7295069694519043}, {"text": "Eq", "start_pos": 39, "end_pos": 41, "type": "DATASET", "confidence": 0.9128254055976868}]}, {"text": "1, where N stands for the length of the n-gram, gram n and Count match (gram n ) is the maximum number of n-grams co-occurring in a candidate summary and a set of reference summaries.", "labels": [], "entities": [{"text": "Count match (gram n )", "start_pos": 59, "end_pos": 80, "type": "METRIC", "confidence": 0.8532120188077291}]}, {"text": "ROGUE-N precision can be calculated by replacing the denominator in Eq.", "labels": [], "entities": [{"text": "ROGUE-N precision", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.764815628528595}]}, {"text": "1 by the total number of n-grams present in all the reference summaries instead of system summaries.", "labels": [], "entities": []}, {"text": "The F1 score is defined as the Harmonic Mean of precision and recall.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9890441000461578}, {"text": "Harmonic Mean", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.9588148891925812}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9980772733688354}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9978814721107483}]}, {"text": "In our case, there was only one reference (the abstractive summary) and one summary from each system -Telegraphic, Smmry, and Resoomer.", "labels": [], "entities": []}, {"text": "We report the average F1 score, after removing stop words and stemming, for N = 1,2,3,4 in The higher F1 score for N = 1,2,3 telegraphic summaries indicates that they are more adequate in terms of capturing relevant content.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9846189916133881}, {"text": "F1 score", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9854967296123505}]}, {"text": "Since telegraphic summaries select shorter segments of text, they can retain more information while maintaining the same summarization factor as their sentence-level counterparts.", "labels": [], "entities": [{"text": "telegraphic summaries", "start_pos": 6, "end_pos": 27, "type": "TASK", "confidence": 0.7196458876132965}]}, {"text": "The ROUGE-4 score for Smmry and Resoomer summaries is slightly higher because they select entire source sentences and are therefore likely to have more 4-gram overlaps.", "labels": [], "entities": [{"text": "ROUGE-4 score", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9840731024742126}, {"text": "Smmry and Resoomer summaries", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.6080213338136673}]}, {"text": "High-order n-gram ROUGE measures try to judge fluency to some degree but since ROUGE is based only on the content overlap, it can only measure adequacy and not coherence.", "labels": [], "entities": []}, {"text": "In order to gain an insight into the coherence of the summaries, we made a set of 45 MCQs from 15 stories in the dataset.", "labels": [], "entities": []}, {"text": "Questions were not set by the same annotator who generated the corresponding telegraphic summary.", "labels": [], "entities": []}, {"text": "The test was administered to three groups of two participants each.", "labels": [], "entities": []}, {"text": "Each group was allotted the summaries produced by a single system -Telegraphic, Smmry or Resoomer.", "labels": [], "entities": [{"text": "Resoomer", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.8903862833976746}]}, {"text": "An example set of questions for the story 'The Little Match Girl' is shown in.", "labels": [], "entities": [{"text": "The Little Match Girl'", "start_pos": 43, "end_pos": 65, "type": "DATASET", "confidence": 0.8417322158813476}]}, {"text": "The 'id' field refers to the unique id we assign to each story in the dataset.", "labels": [], "entities": []}, {"text": "For each story, we made a set of 3 multiple-choice questions with 3 options each.", "labels": [], "entities": []}, {"text": "The answers refer to the index of the correct option in the list.", "labels": [], "entities": []}, {"text": "Apart from the given options, the participants were allowed to choose option 4, \"Can't say\", if they could not answer a question based on the summary.", "labels": [], "entities": []}, {"text": "Average scores are reported in  Higher scores on the questionnaire indicate that the telegraphic summaries were more coherent and allowed the reader to understand the story better.", "labels": [], "entities": []}, {"text": "Participants who read the Smmry and Resoomer sum-maries were unable to understand the story and chose \"Can't say\" as the answer for nearly a third of the questions.", "labels": [], "entities": [{"text": "Smmry and Resoomer sum-maries", "start_pos": 26, "end_pos": 55, "type": "DATASET", "confidence": 0.7575212270021439}]}], "tableCaptions": [{"text": " Table 1: ROUGE-N F1 score", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9950110912322998}, {"text": "F1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.6207900047302246}]}]}