{"title": [{"text": "Findings of the E2E NLG Challenge", "labels": [], "entities": [{"text": "E2E NLG Challenge", "start_pos": 16, "end_pos": 33, "type": "DATASET", "confidence": 0.8895565470059713}]}], "abstractContent": [{"text": "This paper summarises the experimental setup and results of the first shared task on end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems.", "labels": [], "entities": [{"text": "end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems", "start_pos": 85, "end_pos": 162, "type": "TASK", "confidence": 0.7695017286709377}]}, {"text": "Recent end-to-end generation systems are promising since they reduce the need for data annotation.", "labels": [], "entities": []}, {"text": "However, they are currently limited to small, delexi-calised datasets.", "labels": [], "entities": []}, {"text": "The E2E NLG shared task aims to assess whether these novel approaches can generate better-quality output by learning from a dataset containing higher lexical richness, syntactic complexity and diverse discourse phenomena.", "labels": [], "entities": [{"text": "E2E NLG shared task", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8559791892766953}]}, {"text": "We compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures-with the majority implementing sequence-to-sequence models (seq2seq)-as well as systems based on grammatical rules and templates.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper summarises the first shared task on end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems (SDSs).", "labels": [], "entities": [{"text": "natural language generation (NLG) in spoken dialogue systems (SDSs)", "start_pos": 64, "end_pos": 131, "type": "TASK", "confidence": 0.7787494819897872}]}, {"text": "Shared tasks have become an established way of pushing research boundaries in the field of natural language processing, with NLG benchmarking tasks running since 2007.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.6563748816649119}]}, {"text": "This task is novel in that it poses new challenges for recent end-to-end, data-driven NLG systems for SDSs which jointly learn sentence planning and surface realisation and do not require costly semantic alignment between meaning representations (MRs) and the corresponding natural language reference texts, e.g..", "labels": [], "entities": [{"text": "semantic alignment between meaning representations (MRs)", "start_pos": 195, "end_pos": 251, "type": "TASK", "confidence": 0.6664108298718929}]}, {"text": "So far, end-to-end approaches to NLG are limited to small, delexicalised datasets, e.g. BAGEL (, SF Hotels/Restaurants (), or RoboCup (, whereas the E2E shared task is based on anew crowdsourced dataset of 50k instances in the restaurant domain, which is about 10 times larger and also more complex than previous datasets.", "labels": [], "entities": [{"text": "NLG", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9363925457000732}, {"text": "BAGEL", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.6426944732666016}]}, {"text": "For the shared challenge, we received 62 system submissions by 17 institutions from 11 countries, with about 1/3 of these submissions coming from industry.", "labels": [], "entities": []}, {"text": "We assess the submitted systems by comparing them to a challenging baseline using automatic as well as human evaluation.", "labels": [], "entities": []}, {"text": "We consider this level of participation an unexpected success, which underlines the timeliness of this task.", "labels": [], "entities": []}, {"text": "While there are previous studies comparing a limited number of end-to-end NLG approaches (, this is the first research to evaluate novel end-to-end generation at scale and using human assessment.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to maximise the chances for data-driven end-to-end systems to produce high quality output, we aim to provide training data in high quality and large quantity.", "labels": [], "entities": []}, {"text": "To collect data in large enough quantity, we use crowdsourcing with automatic   However, the human evaluation study provides a different picture.", "labels": [], "entities": []}, {"text": "Rank-based Magnitude Estimation (RankME) () was used for evaluation, where crowd workers compared outputs of 5 systems for the same MR and assigned scores on a continuous scale.", "labels": [], "entities": [{"text": "Rank-based Magnitude Estimation (RankME)", "start_pos": 0, "end_pos": 40, "type": "METRIC", "confidence": 0.7247068583965302}]}, {"text": "We evaluated output naturalness and overall quality in separate tasks; for naturalness evaluation, the source MR was not shown to workers.", "labels": [], "entities": []}, {"text": "We collected 4,239 5-way rankings for naturalness and 2,979 for quality, comparing 9.5 systems per MR on average.", "labels": [], "entities": []}, {"text": "The final evaluation results were produced using the TrueSkill algorithm (, with partial ordering into significance clusters computed using bootstrap resampling ().", "labels": [], "entities": []}, {"text": "For both criteria, this resulted in 5 clusters of systems with significantly different performance and showed a clear winner: SHEFF2 for naturalness and SLUG for quality.", "labels": [], "entities": [{"text": "SHEFF2", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9805921316146851}, {"text": "SLUG", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9949454665184021}]}, {"text": "The 2nd clusters are quite large for both criteria -they contain 13 and 11 systems, respectively, and both include the baseline TGEN system.", "labels": [], "entities": []}, {"text": "The results indicate that seq2seq systems dominate in terms of naturalness of their outputs, while most systems of other architectures score lower.", "labels": [], "entities": []}, {"text": "The bottom cluster is filled with template-based systems.", "labels": [], "entities": []}, {"text": "The results for quality are, however, more mixed in terms of architectures, with none of them clearly prevailing.", "labels": [], "entities": []}, {"text": "Here, seq2seq systems with reranking based on checking output correctness score high while seq2seq systems with no such mechanism occupy the bottom two clusters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Total number of MRs and human refer- ences in the E2E data sections.", "labels": [], "entities": [{"text": "MRs", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.5543420314788818}, {"text": "E2E data sections", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.982637345790863}]}, {"text": " Table 3: A list of primary systems in the E2E NLG challenge, with word-overlap metric scores.", "labels": [], "entities": [{"text": "E2E NLG challenge", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.7180428306261698}]}, {"text": " Table 4: TrueSkill measurements of quality (left) and naturalness (right).", "labels": [], "entities": []}]}