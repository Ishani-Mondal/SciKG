{"title": [{"text": "Part of Speech Tagging in Luyia: A Bantu Macrolanguage", "labels": [], "entities": [{"text": "Speech Tagging", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.7924575805664062}]}], "abstractContent": [{"text": "Luyia is a macrolanguage in central Kenya.", "labels": [], "entities": []}, {"text": "The Luyia languages, like other Bantu languages, have a complex morphological system.", "labels": [], "entities": []}, {"text": "This system can be leveraged to aid in part of speech tagging.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.7600160837173462}]}, {"text": "Bag-of-characters taggers trained on a source Luyia language can be applied directly to another Luyia language with some degree of success.", "labels": [], "entities": [{"text": "Bag-of-characters taggers", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.858348935842514}]}, {"text": "In addition, mixing data from the target language with data from the source language does produce more accurate predictive models compared to models trained on just the target language data when the training set size is small.", "labels": [], "entities": []}, {"text": "However, for both of these tagging tasks, models involving the more distantly related language, Tiriki, are better at predicting part of speech tags for Wanga data.", "labels": [], "entities": [{"text": "Wanga data", "start_pos": 153, "end_pos": 163, "type": "DATASET", "confidence": 0.9101490676403046}]}, {"text": "The models incorporating Bukusu data are not as successful despite the closer relationship between Bukusu and Wanga.", "labels": [], "entities": [{"text": "Bukusu data", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.837794154882431}, {"text": "Wanga", "start_pos": 110, "end_pos": 115, "type": "DATASET", "confidence": 0.8942581415176392}]}, {"text": "Overlapping vocabulary between the Wanga and Tiriki corpora as well as a bias towards open class words help Tiriki outperform Bukusu.", "labels": [], "entities": [{"text": "Bukusu", "start_pos": 126, "end_pos": 132, "type": "DATASET", "confidence": 0.8845942616462708}]}], "introductionContent": [{"text": "Luyia is a macrolanguage comprised of over 20 individual languages that form a dialect continuum.", "labels": [], "entities": []}, {"text": "These languages have a very high degree of cognates.", "labels": [], "entities": []}, {"text": "This provides a unique opportunity to examine the performance of Natural Language Processing tools on a cluster of very closely related Bantu languages.", "labels": [], "entities": []}, {"text": "The structure of this paper is as follows: first, I provide some background on the Luyia languages, then, I evaluate the relative performance of different languages on two types of predictive part of speech tagging tasks.", "labels": [], "entities": [{"text": "predictive part of speech tagging tasks", "start_pos": 181, "end_pos": 220, "type": "TASK", "confidence": 0.6595163941383362}]}, {"text": "The first task involves direct training of an SVM part of speech tagging model on one language and then evaluation of this model on another language.", "labels": [], "entities": [{"text": "SVM part of speech tagging", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.7117525219917298}]}, {"text": "The second task involves augmenting an SVM part of speech tagging model by training on a mixture of data from the target language and another variety of Luyia.", "labels": [], "entities": [{"text": "SVM part of speech tagging", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.6472370088100433}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Cognate percentages using 200-word lists", "labels": [], "entities": []}, {"text": " Table 2: Total corpus size for the Luyia corpora", "labels": [], "entities": [{"text": "Luyia corpora", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.8735445737838745}]}, {"text": " Table 3: Number of words and sentences for various training dataset sizes", "labels": [], "entities": []}, {"text": " Table 4: Accuracy of training on one language", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9938938021659851}]}, {"text": " Table 5: Accuracy of language mixture models", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9910814762115479}]}, {"text": " Table 6: Out of Vocabulary accuracy for source-Wanga", "labels": [], "entities": [{"text": "Out", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9663876295089722}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9594777822494507}, {"text": "source-Wanga", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.5591722130775452}]}, {"text": " Table 7: Out of Vocabulary accuracy for language mixture tagger", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9264693260192871}, {"text": "language mixture tagger", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7469281752904257}]}, {"text": " Table 8: Percentage of source corpus within specified distance of any word in Wanga corpus", "labels": [], "entities": [{"text": "Wanga corpus", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.9028250575065613}]}]}