{"title": [{"text": "Effective Parallel Corpus Mining using Bilingual Sentence Embeddings", "labels": [], "entities": [{"text": "Parallel Corpus Mining", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6074373126029968}]}], "abstractContent": [{"text": "This paper presents an effective approach for parallel corpus mining using bilingual sentence embeddings.", "labels": [], "entities": [{"text": "parallel corpus mining", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.6759258508682251}]}, {"text": "Our embedding models are trained to produce similar representations exclusively for bilingual sentence pairs that are translations of each other.", "labels": [], "entities": []}, {"text": "This is achieved using a novel training method that introduces hard negatives consisting of sentences that are not translations but have some degree of semantic similarity.", "labels": [], "entities": []}, {"text": "The quality of the resulting embeddings are evaluated on parallel corpus reconstruction and by assessing machine translation systems trained on gold vs. mined sentence pairs.", "labels": [], "entities": [{"text": "parallel corpus reconstruction", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.628414124250412}]}, {"text": "We find that the sentence em-beddings can be used to reconstruct the United Nations Parallel Corpus (Ziemski et al., 2016) at the sentence-level with a precision of 48.9% for en-fr and 54.9% for en-es.", "labels": [], "entities": [{"text": "United Nations Parallel Corpus", "start_pos": 69, "end_pos": 99, "type": "DATASET", "confidence": 0.903016060590744}, {"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9914188385009766}]}, {"text": "When adapted to document-level matching, we achieve a parallel document matching accuracy that is comparable to the significantly more computa-tionally intensive approach of Uszkoreit et al.", "labels": [], "entities": [{"text": "document-level matching", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.7590359747409821}, {"text": "document matching", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.624674990773201}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.8674885034561157}]}, {"text": "Using reconstructed parallel data, we are able to train NMT models that perform nearly as well as models trained on the original data (within 1-2 BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9960731267929077}]}], "introductionContent": [{"text": "Volumes of quality parallel training data are critical to neural machine translation (NMT) systems.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 58, "end_pos": 90, "type": "TASK", "confidence": 0.8314278324445089}]}, {"text": "While large distributed systems have proven useful for mining parallel documents (), these approaches are computationally intensive and rely on heavily engineered subsystems.", "labels": [], "entities": []}, {"text": "Recent work has approached the problem by training lightweight end-to-end models based on word and sentencelevel embeddings.", "labels": [], "entities": []}, {"text": "We propose a novel method for training bilingual sentence embeddings that proves useful for * equal contribution \u2020 Work done during an internship at Google AI.", "labels": [], "entities": []}, {"text": "sentence-level mining of parallel data.", "labels": [], "entities": [{"text": "sentence-level mining of parallel data", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7934398472309112}]}, {"text": "Sentences are encoded using Deep Averaging Networks (DANs) (), a simple bag of n-grams architecture that has been shown to provide surprisingly competitive performance on a number of tasks including sentence classification (, conversation input-response prediction ( , and email response prediction (.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 199, "end_pos": 222, "type": "TASK", "confidence": 0.7851749956607819}, {"text": "conversation input-response prediction", "start_pos": 226, "end_pos": 264, "type": "TASK", "confidence": 0.6784683366616567}, {"text": "email response prediction", "start_pos": 273, "end_pos": 298, "type": "TASK", "confidence": 0.6944029033184052}]}, {"text": "Separate encoders are used for each language with candidate source and target sentences being paired based on the dot-product of their embedded representations.", "labels": [], "entities": []}, {"text": "Training maximizes the dot-product score of sentence pairs that are translations of each other at the expense of sampled negatives.", "labels": [], "entities": []}, {"text": "We contrast using random negatives with carefully selected hard negatives that challenge the model to distinguish between true translation pairs versus non-translation pairs that exhibit some degree of semantic similarity.", "labels": [], "entities": []}, {"text": "The efficiency of the sentence encoders and the use of a dot-product operation to score candidate sentence pairs is well suited for parallel corpus mining.", "labels": [], "entities": [{"text": "parallel corpus mining", "start_pos": 132, "end_pos": 154, "type": "TASK", "confidence": 0.6903307239214579}]}, {"text": "Efficient encoders reduce the amount of computational resources required to obtain sentence embeddings fora large collection of unpaired sentences.", "labels": [], "entities": []}, {"text": "Once the sentence embeddings are available, efficient nearest neighbour search ( can be used to identify candidate translation pairs.", "labels": [], "entities": []}, {"text": "The language pairs English-French (en-fr) and English-Spanish (en-es) are used in our experiments.", "labels": [], "entities": []}, {"text": "Our results show that introducing hard negative sentence pairs, which are semantically similar but that are not translations of each other, systematically outperforms using randomly selected negatives.", "labels": [], "entities": []}, {"text": "Our method can be used to reconstruct the United Nations Parallel Corpus ( at the sentence-level with a level of precision of 48.9% P@1 for en-fr and 54.9% P@1 for en-es.", "labels": [], "entities": [{"text": "United Nations Parallel Corpus", "start_pos": 42, "end_pos": 72, "type": "DATASET", "confidence": 0.9243302643299103}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.998315691947937}]}, {"text": "When we adapt our method to document-level pairings we achieve a matching accuracy that is comparable to that of the much heavier weight and more computationally intensive approach of.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9687387347221375}]}, {"text": "Training an NMT model using the reconstructed corpus results in models that perform nearly as well as those trained on the original parallel corpus (within 1-2 BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9942724704742432}]}, {"text": "Finally, our method has a modest degree of correlation with the pair quality scores provided by Zipporah (.", "labels": [], "entities": []}, {"text": "However, our method has higher agreement with human judgments, and our approach to filter the ParaCrawl corpus results in NMT systems with higher BLEU scores.", "labels": [], "entities": [{"text": "ParaCrawl corpus", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.9189407229423523}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9989700317382812}]}], "datasetContent": [{"text": "We train our proposed model on two language pairs: English-French (en-fr) and English-Spanish (en-es).", "labels": [], "entities": []}, {"text": "First, we evaluate the performance on the translation candidate ranking task, comparing the dual-encoder architectures with random negative sampling versus using hard negatives.", "labels": [], "entities": [{"text": "translation candidate ranking task", "start_pos": 42, "end_pos": 76, "type": "TASK", "confidence": 0.9136936813592911}]}, {"text": "Then, we present results for document-level matching using's method as a strong baseline.", "labels": [], "entities": [{"text": "document-level matching", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7165515124797821}]}, {"text": "We explore training NMT systems using our method to both filter and re-construct parallel corpora.", "labels": [], "entities": []}, {"text": "Finally, we assess the level or agreement between our method and human judgments.", "labels": [], "entities": []}, {"text": "Model configuration and hyperparameters for our sentence embedding models are set mostly based on defaults taken from prior work with very minimal tuning on the held-out dev set.", "labels": [], "entities": []}, {"text": "For each language, we build a vocabulary consisting of 200 thousands unigram and 200 thousands bi-gram tokens.", "labels": [], "entities": []}, {"text": "All inputs are tokenized and normalized be-5 https://paracrawl.eu 169 fore being fed to the model.", "labels": [], "entities": []}, {"text": "We employ an SGD optimizer with a batch size of 128.", "labels": [], "entities": []}, {"text": "The learning rate is set to 0.01 with a learning decay of 0.96 every 5 million steps.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.8381966948509216}, {"text": "learning decay", "start_pos": 40, "end_pos": 54, "type": "METRIC", "confidence": 0.8977249562740326}]}, {"text": "We train for 50 million steps.", "labels": [], "entities": []}, {"text": "For each encoder layer, we employ a four-layer DNN model which contains 320, 320, 500 and 500 hidden units for each layer respectively.", "labels": [], "entities": []}, {"text": "We apply a ReLU activation in the first three layers and no activation in the final layer.", "labels": [], "entities": [{"text": "ReLU activation", "start_pos": 11, "end_pos": 26, "type": "METRIC", "confidence": 0.8974032700061798}]}, {"text": "We enable residual connections between layers with a skip level of 1.", "labels": [], "entities": []}, {"text": "There is no parameter sharing between the source and target encoder layers.", "labels": [], "entities": []}, {"text": "The size of the unigram and bi-gram embeddings is set to 320 and the embeddings are updated during the training process.", "labels": [], "entities": []}, {"text": "The sentence embedding size is set to 512 for both source and target languages.", "labels": [], "entities": []}, {"text": "The calibrated confidence score is trained jointly with the translation candidate ranking task but with a stop gradient that prevents the confidence task from modifying the bilingual sentence encoders.", "labels": [], "entities": []}, {"text": "The tasks are trained in a multitask framework with multiple workers, where 90% of the workers optimize the translation candidate ranking task and the remaining 10% optimize the confidence task.", "labels": [], "entities": []}, {"text": "We use the same configuration for confidence as when training the translation candidate ranking task.", "labels": [], "entities": [{"text": "translation candidate ranking task", "start_pos": 66, "end_pos": 100, "type": "TASK", "confidence": 0.8808335214853287}]}, {"text": "Both use the same batch size 128, meaning there is 1 positive and 127 negative candidates selected for each pass over an example.", "labels": [], "entities": []}, {"text": "We apply a dropout of 0.4 before feeding the feature vector [u, u 2 ] into the hidden layers that calculate scale and bias.", "labels": [], "entities": [{"text": "scale", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.9884157180786133}]}, {"text": "As a proof of concept on using our mined translation pairs as training data, we train translation models with original versus mined parallel sentence pairs from UN corpus, and with filtered ParaCrawl data using Zipporah score versus using our model's confidence score.", "labels": [], "entities": [{"text": "UN corpus", "start_pos": 161, "end_pos": 170, "type": "DATASET", "confidence": 0.8217903971672058}]}, {"text": "We evaluate on wmt13 () and wmt14) testing sets for en-es and enfr, respectively, with performance assessed using BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9984084963798523}]}, {"text": "The translation models are based on Transformer architecture (, and make use of a model dimension of 512 and a hidden dimension of 2048, with 6 layers and 8 attention heads.", "labels": [], "entities": []}, {"text": "The models use the Adam optimizer with the training schedule described in.", "labels": [], "entities": []}, {"text": "For each language pair, sentence pairs are segmented using a shared 32,000 wordpiece vocabulary (.", "labels": [], "entities": []}, {"text": "Sentence pairs are then batched together by approximate sequence length with variable batch sizes based on sequence length.", "labels": [], "entities": []}, {"text": "The average batch size per step is 120 pairs per batch.", "labels": [], "entities": []}, {"text": "We train each model until convergence (approximately 120K steps).: Accuracy of document matching on UN corpus.", "labels": [], "entities": [{"text": "document matching", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.6972794532775879}, {"text": "UN corpus", "start_pos": 100, "end_pos": 109, "type": "DATASET", "confidence": 0.8310905396938324}]}], "tableCaptions": [{"text": " Table 2: Precision at N (P@N) results on the evaluation set for models built using the random negatives  and (M ) hard negatives. Models attempt to select the true translation target for a source sentence against  10M randomly selected targets.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.978870153427124}]}, {"text": " Table 3: Precision at N (P@N) of target sentence retrieval on the UN corpus. Models attempt to select the  true translation target for a source sentence from the entire corpus (11.3 million aligned sentence pairs.)", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.988933801651001}, {"text": "UN corpus", "start_pos": 67, "end_pos": 76, "type": "DATASET", "confidence": 0.9333491921424866}]}, {"text": " Table 4: Accuracy of document matching on UN  corpus.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9680482149124146}, {"text": "document matching", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.6803508847951889}, {"text": "UN  corpus", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.8478327989578247}]}, {"text": " Table 5: BLEU scores on WMT testing sets of the  NMT models trained on original UN pairs (Ora- cle) and on two versions of mined UN corpora.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993756413459778}, {"text": "WMT testing sets", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.7442750136057535}]}, {"text": " Table 6: BLEU scores on WMT testing sets of the  NMT models trained on different data: 1) WMT  training sets, 2) filtered ParaCrawl data, and 3)  combined data of WMT and filtered ParaCrawl.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988381266593933}, {"text": "WMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9381105303764343}, {"text": "ParaCrawl data", "start_pos": 123, "end_pos": 137, "type": "DATASET", "confidence": 0.9028066098690033}, {"text": "ParaCrawl", "start_pos": 181, "end_pos": 190, "type": "DATASET", "confidence": 0.9139333963394165}]}]}