{"title": [{"text": "Interpretable Textual Neuron Representations for NLP", "labels": [], "entities": [{"text": "Interpretable Textual Neuron Representations", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.6505720838904381}]}], "abstractContent": [{"text": "Input optimization methods, such as Google Deep Dream, create interpretable representations of neurons for computer vision DNNs.", "labels": [], "entities": []}, {"text": "We propose and evaluate ways of transferring this technology to NLP.", "labels": [], "entities": []}, {"text": "Our results suggest that gradient ascent with a gumbel soft-max layer produces n-gram representations that outperform naive corpus search in terms of target neuron activation.", "labels": [], "entities": []}, {"text": "The representations highlight differences in syntax awareness between the language and visual models of the Imaginet architecture.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep Neural Networks (DNNs) have led to advances in Natural Language Processing, but they are hard to interpret.", "labels": [], "entities": []}, {"text": "This is partly due to the fact that their smallest components, i.e., neurons, lack interpretable representations.", "labels": [], "entities": []}, {"text": "For computer vision problems, propose to use gradient ascent to find an input image that maximizes the activation of a neuron of interest.", "labels": [], "entities": []}, {"text": "Using these image representations, one can for instance show that lower level neurons in vision CNNs specialize in patterns such as stripes (.", "labels": [], "entities": []}, {"text": "Applying gradient ascent input optimization to NLP is not straightforward, as discrete symbols are not open to continuous manipulation.", "labels": [], "entities": [{"text": "gradient ascent input optimization", "start_pos": 9, "end_pos": 43, "type": "TASK", "confidence": 0.729160487651825}]}, {"text": "A common alternative approach is to search existing corpora for optimal documents or n-grams (e.g.,,).", "labels": [], "entities": []}, {"text": "As this strategy only covers the space of existing inputs, we assume that it may lead to incorrect assumptions.", "labels": [], "entities": []}, {"text": "For instance, the representation of a given neuron may suggest that syntax was learned, when in reality this is due to alack of ungrammatical inputs in the corpus.", "labels": [], "entities": []}, {"text": "In the following, we propose and test methods for gradient ascent input optimization in NLP.", "labels": [], "entities": [{"text": "gradient ascent input optimization", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.692567303776741}]}, {"text": "Our quantitative assessment suggests that one method, which is based on the gumbel softmax trick, produces inputs that are more highly activating than corpus search.", "labels": [], "entities": []}, {"text": "By applying this method to the Imaginet architecture, we confirm that a language model pays attention to syntax to some degree, while a visual model looks for key content words and ignores function words.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the input optimization methods by the activation that they achieve in 160 randomly chosen neurons of the language and visual model projection layers.", "labels": [], "entities": []}, {"text": "For embedding optimization, representations are derived by finding the nearest realword neighbor of the optimized embeddings in the 1 https://zenodo.org/record/804392/files/data.tgz embedding space.", "labels": [], "entities": [{"text": "embedding optimization", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.9512462913990021}]}, {"text": "For word optimization, we take the argmax over the vocabulary dimension of X.", "labels": [], "entities": [{"text": "word optimization", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8153564631938934}, {"text": "argmax", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9903028607368469}]}, {"text": "We find that while representations from embedding, logit and softmax optimization are not competitive, the gumbel softmax trick outperforms the corpus search strategy in terms of target neuron activation.", "labels": [], "entities": []}, {"text": "shows optimal 5-grams for some neurons.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of optimal 5-grams via corpus  search and via gradient ascent with gumbel softmax.  Spelling errors stem from the Imaginet dictionary.", "labels": [], "entities": []}]}