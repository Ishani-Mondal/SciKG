{"title": [{"text": "Unsupervised Random Walk Sentence Embeddings: A Strong but Simple Baseline", "labels": [], "entities": []}], "abstractContent": [{"text": "Using a random walk model of text generation , Arora et al.", "labels": [], "entities": [{"text": "text generation", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7933522462844849}]}, {"text": "(2017) proposed a strong baseline for computing sentence embeddings: take a weighted average of word embeddings and modify with SVD.", "labels": [], "entities": []}, {"text": "This simple method even outperforms far more complex approaches such as LSTMs on textual similarity tasks.", "labels": [], "entities": []}, {"text": "In this paper, we first show that word vector length has a confounding effect on the probability of a sentence being generated in Arora et al.'s model.", "labels": [], "entities": []}, {"text": "We propose a random walk model that is robust to this confound, where the probability of word generation is inversely related to the angular distance between the word and sentence embeddings.", "labels": [], "entities": [{"text": "word generation", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.7190235108137131}]}, {"text": "Our approach beats Arora et al.'s by up to 44.4% on textual similarity tasks and is competitive with state-of-the-art methods.", "labels": [], "entities": []}, {"text": "Unlike Arora et al.'s method, ours requires no hy-perparameter tuning, which means it can be used when there is no labelled data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed representations of words, better known as word embeddings, have become fixtures of current methods in natural language processing.", "labels": [], "entities": []}, {"text": "Word embeddings can be generated in a number of ways () by capturing the semantics of a word using the contexts it appears in.", "labels": [], "entities": []}, {"text": "Recent work has tried to extend that intuition to sequences of words, using methods ranging from a weighted average of word embeddings to convolutional, recursive, and recurrent neural networks ().", "labels": [], "entities": []}, {"text": "Still, found that these sophisticated architectures are often outperformed, particularly in transfer learning settings, by sentence embeddings generated as a simple average of tuned word embeddings.", "labels": [], "entities": []}, {"text": "provided a more powerful approach: compute the sentence embeddings as weighted averages of word embeddings, then subtract from each one the vector projection on their first principal component.", "labels": [], "entities": []}, {"text": "The weighting scheme, smoothed inverse frequency (SIF), is derived from a random walk model where words in a sentence s are produced by the random walk of a latent discourse vector c s . A word unrelated to c scan be produced by chance or if it is part of frequent discourse such as stopwords.", "labels": [], "entities": [{"text": "smoothed inverse frequency (SIF)", "start_pos": 22, "end_pos": 54, "type": "METRIC", "confidence": 0.7116463929414749}]}, {"text": "This approach evens outperforms more complex models such as LSTMs on textual similarity tasks.", "labels": [], "entities": []}, {"text": "argued that the simplicity and effectiveness of their method make it a tough-to-beat baseline for sentence embeddings.", "labels": [], "entities": []}, {"text": "Though they call their approach unsupervised, others have noted that it is actually 'weakly supervised', since it requires hyperparameter tuning.", "labels": [], "entities": []}, {"text": "In this paper, we first propose a class of worst-case scenarios for random walk model.", "labels": [], "entities": []}, {"text": "Specifically, given some sentence g that is dominated by words with zero similarity, and some sentence h that is dominated by identical words, we show that their approach can return two discourse vectors cg and ch such that p(g|c g ) \u21e1 p(h|c h ), provided that the word vectors for g have a sufficiently greater length than those for h.", "labels": [], "entities": []}, {"text": "In other words, word vector length has a confounding effect on the probability of a sentence being generated, and this effect can be strong enough to yield completely unintuitive results.", "labels": [], "entities": []}, {"text": "This problem is not endemic to these scenarios, though they are the most illustrative of it; because of the underlying log-linear word production model, Arora et al.'s model is fundamentally sensitive to word vector length.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are three-fold.", "labels": [], "entities": []}, {"text": "First, we propose a random walk model that is robust to distortion by vector length, where the probability of a word vector being generated by a discourse vector is inversely related to the angular distance between them.", "labels": [], "entities": []}, {"text": "Second, we derive a weighting scheme from this model and compute a MAP estimate for the sentence embedding as follows: normalize the word vectors, take a weighted average of them, and then subtract from each weighted average vector the projection on their first m principal components.", "labels": [], "entities": [{"text": "MAP", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9259435534477234}]}, {"text": "We call the weighting scheme derived from our random walk model unsupervised smoothed inverse frequency (uSIF).", "labels": [], "entities": [{"text": "smoothed inverse frequency (uSIF)", "start_pos": 77, "end_pos": 110, "type": "METRIC", "confidence": 0.8271274516979853}]}, {"text": "It is similar to SIF () in practice, but requires no hyperparameter tuning at all -it is completely unsupervised, allowing it to be used when there is no labelled data.", "labels": [], "entities": []}, {"text": "Lastly, we show that our approach outperforms Arora et al.'s by up to 44.4% on textual similarity tasks, and is even competitive with state-of-the-art methods.", "labels": [], "entities": []}, {"text": "Given the simplicity, effectiveness, and unsupervised nature of our method, we suggest it be used as a baseline for computing sentence embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "For a fair comparison with Arora et al., we use the unigram probability distribution used by them, based on the enwiki dataset (Wikipedia, 3B words).", "labels": [], "entities": [{"text": "enwiki dataset", "start_pos": 112, "end_pos": 126, "type": "DATASET", "confidence": 0.8860933184623718}]}, {"text": "Our preprocessing of the sentences is limited to tokenization.", "labels": [], "entities": []}, {"text": "We try our method with three types of word vectors: GloVe vectors (), PARAGRAM-SL999 (PSL) vectors (, tuned on the SimLex999 dataset, and ParaNMT-50 vectors, tuned on 51M EnglishEnglish sentence pairs translated from EnglishCzech sentence pairs.", "labels": [], "entities": [{"text": "PARAGRAM-SL999", "start_pos": 70, "end_pos": 84, "type": "METRIC", "confidence": 0.9689583778381348}, {"text": "SimLex999 dataset", "start_pos": 115, "end_pos": 132, "type": "DATASET", "confidence": 0.8647725284099579}]}, {"text": "The value of n in is E s2S |s| \u21e1 11 and was estimated using sentences from all corpora.", "labels": [], "entities": []}, {"text": "The value of a in (9) is then 1.2 \u21e5 10 \ud97b\udf593 . Our results are denoted as X+UP, where X 2 {'GloVe', 'PSL', 'ParaNMT'}, U denotes uSIF-weighting, and P denotes piecewise common component removal.", "labels": [], "entities": [{"text": "UP", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.8289690613746643}]}], "tableCaptions": [{"text": " Table 1: Average results (Pearson's r \u21e5 100) on  textual similarity tasks. The highest score in each  column is in bold. \"Glove+UP\" is the application  of uSIF-weighting (U) and piecewise common  component removal (P) to GloVe word vectors;  \"PSL+UP' to PSL word vectors; \"ParaNMT+UP\",  to ParaNMT word vectors.", "labels": [], "entities": []}, {"text": " Table 3: Results on the SST, SICK-R, and SICK-E  tasks. The best score for each task is bolded.  \u2020  indicates our implementation.", "labels": [], "entities": [{"text": "SST", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9459323883056641}]}]}