{"title": [{"text": "Findings of the WMT 2018 Shared Task on Quality Estimation", "labels": [], "entities": [{"text": "WMT 2018 Shared Task on Quality Estimation", "start_pos": 16, "end_pos": 58, "type": "TASK", "confidence": 0.5511887754712786}]}], "abstractContent": [{"text": "We report the results of the WMT18 shared task on Quality Estimation, i.e. the task of predicting the quality of the output of machine translation systems at various granular-ity levels: word, phrase, sentence and document.", "labels": [], "entities": [{"text": "WMT18 shared task", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.5447073578834534}, {"text": "Quality Estimation", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.6301119476556778}]}, {"text": "This year we include four language pairs, three text domains, and translations produced by both statistical and neural machine translation systems.", "labels": [], "entities": [{"text": "statistical and neural machine translation", "start_pos": 96, "end_pos": 138, "type": "TASK", "confidence": 0.7230173647403717}]}, {"text": "Participating teams from ten institutions submitted a variety of systems to different task variants and language pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "This shared task builds on its previous six editions to further examine automatic methods for estimating the quality of machine translation (MT) output at run-time, without the use of reference translations.", "labels": [], "entities": [{"text": "estimating the quality of machine translation (MT) output", "start_pos": 94, "end_pos": 151, "type": "TASK", "confidence": 0.7276785045862197}]}, {"text": "It includes the (sub)tasks of wordlevel, phrase-level, sentence-level and documentlevel estimation.", "labels": [], "entities": [{"text": "documentlevel estimation", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.6632291674613953}]}, {"text": "In addition to advancing the state of the art at all prediction levels, our goals include: \u2022 To study the performance of quality estimation approaches on the output of neural MT systems.", "labels": [], "entities": []}, {"text": "We do so by providing datasets for two language pairs where source segments were translated by both statistical phrasebased and neural MT systems.", "labels": [], "entities": []}, {"text": "\u2022 To study the predictability of missing words in the MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9420081377029419}]}, {"text": "To do so, for the first time we provide data annotated for such errors at training time.", "labels": [], "entities": []}, {"text": "\u2022 To study the predictability of source words that lead to errors in the MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9719831347465515}]}, {"text": "To do so, for the first time we provide source segments annotated for such errors at the word level.", "labels": [], "entities": []}, {"text": "\u2022 To study the effectiveness of manually assigned labels for phrases.", "labels": [], "entities": []}, {"text": "For that we provide a dataset where each phrase was annotated by human translators.", "labels": [], "entities": []}, {"text": "\u2022 To investigate the utility of detailed information logged during post-editing.", "labels": [], "entities": []}, {"text": "We do so by providing post-editing time, keystrokes, as well as post-editor ID.", "labels": [], "entities": []}, {"text": "\u2022 To study quality prediction for documents from errors annotated at word-level with added severity judgements.", "labels": [], "entities": []}, {"text": "This is done using anew corpus manually annotated with a fine-grained error taxonomy, from which document-level scores are derived.", "labels": [], "entities": []}, {"text": "This year's shared task provides new training and test datasets for all tasks, and allows participants to explore any additional data and resources deemed relevant.", "labels": [], "entities": []}, {"text": "Tasks make use of large datasets produced either from post-editions or annotations by professional translators, or from direct human annotations.", "labels": [], "entities": []}, {"text": "The following text domains are available for different languages and tasks: information technology (IT), life sciences, and product title and descriptions on sports and outdoor activities.", "labels": [], "entities": []}, {"text": "In-house statistical and neural MT systems were built to produce translations for the two first domains, while an online system was used for the third domain.", "labels": [], "entities": []}, {"text": "Five datasets and language pairs are used for different tasks (Section 4): English-German (Tasks 1, 2) and English-Czech (Tasks 1, 2) on the IT domain, English-Latvian (Tasks 1, 2) and German-English (Tasks 1, 2, 3), both on the life sciences domain, English-French (Task 4) with product titles and descriptions within the sports and outdoor activities domain.", "labels": [], "entities": []}, {"text": "Participants are provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features, and perform model learning (Section 2).", "labels": [], "entities": []}, {"text": "Participants (Section 3) could submit up to two systems for each task and language pair.", "labels": [], "entities": []}, {"text": "A discussion on the main goals and findings from this year's task is given in Section 9.", "labels": [], "entities": []}], "datasetContent": [{"text": "This year we further expand the datasets used in WMT17 by adding: more instances (see), more languages (four language pairs), more MT architectures (neural and statistical MT), and different types of annotation (manual and extracted from manual post-editing).", "labels": [], "entities": [{"text": "WMT17", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.7722707986831665}]}, {"text": "In addition, new data was collected and provided for Task 4, on a fifth language pair and third text domain.", "labels": [], "entities": []}, {"text": "Words in MT  Words in MT  and ** late submissions that were not considered for the official ranking of participating systems.", "labels": [], "entities": [{"text": "MT  Words in MT", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.43209341168403625}]}, {"text": "Labels We used the phrase segmentation produced by the SMT decoder which generated the translations for the dataset.", "labels": [], "entities": [{"text": "phrase segmentation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7277020215988159}, {"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.8555803894996643}]}, {"text": "The phrases were annotated for errors using four classes: 'OK', 'BAD' -the phrase contain one or more errors, 'BAD word order' -the phrase is in an incorrect position in the sentence, and 'BAD omission' -a word is missing before/after a phrase.", "labels": [], "entities": [{"text": "OK", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9124315977096558}, {"text": "BAD", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9730150699615479}, {"text": "BAD word order", "start_pos": 111, "end_pos": 125, "type": "METRIC", "confidence": 0.8653379082679749}, {"text": "BAD", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.9419099688529968}]}, {"text": "This task in further subdivided in two subtasks: word-level prediction (Task3a), and phrase-level prediction (Task3b).", "labels": [], "entities": [{"text": "word-level prediction", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.6771333515644073}, {"text": "phrase-level prediction", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.7222107797861099}]}, {"text": "The data for Task3a propagates the annotation of each phrase to its words, and thus uses wordlevel segmentation for both source and machinetranslated sentences, such that the task can be addressed as a word-level prediction task.", "labels": [], "entities": [{"text": "word-level prediction task", "start_pos": 202, "end_pos": 228, "type": "TASK", "confidence": 0.72488072514534}]}, {"text": "In other words, all tokens in the target sentence are labelled according to the label of the phrase they belong to.", "labels": [], "entities": []}, {"text": "Therefore, if the phrase is annotated as either 'OK', 'BAD' or 'BAD word order', all tokens (and gap tokens) within that phrase are labelled as either 'OK', 'BAD' or 'BAD word order'.", "labels": [], "entities": [{"text": "BAD", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9409645795822144}, {"text": "BAD word order", "start_pos": 64, "end_pos": 78, "type": "METRIC", "confidence": 0.8304394483566284}, {"text": "BAD", "start_pos": 158, "end_pos": 161, "type": "METRIC", "confidence": 0.8978469371795654}]}, {"text": "To annotate omission errors, a gap token is inserted after each token and at the start of the sentence.", "labels": [], "entities": []}, {"text": "The data for Task3b has phrase-level segmentation with the labels assigned by the human annotator to each phrase.", "labels": [], "entities": [{"text": "phrase-level segmentation", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.7440332174301147}]}, {"text": "A gap token is inserted after each phrase and at the start of the sentence.", "labels": [], "entities": []}, {"text": "The gap is labelled as follows: 'OK' or 'BAD omission', where the latter indicates that one or more words are missing.", "labels": [], "entities": [{"text": "OK", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9693602323532104}, {"text": "BAD omission'", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.9367026487986246}]}, {"text": "Evaluation Similarly to Task 2, our primary metric for predictions at word-level (Task3a) is the multiplication of the F 1 scores of the OK and BAD classes, F 1 -Mult, while for predictions at phrase-level (Task3b), our primary metric is the phrase-level version of F 1 -Mult.", "labels": [], "entities": [{"text": "BAD", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.7660541534423828}, {"text": "F 1 -Mult", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.839822068810463}, {"text": "F 1 -Mult", "start_pos": 266, "end_pos": 275, "type": "METRIC", "confidence": 0.8653025031089783}]}, {"text": "The same metrics were applied to gap and source token labels for both sub-tasks, along with F 1 scores for individual classes for completeness.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9822338223457336}]}, {"text": "We also report F 1 score for BAD word order labels on the target tokens for Task3b.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9878263274828593}, {"text": "BAD word order labels", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.6860226541757584}]}, {"text": "We computed statistical significance of the results using randomised test with Bonferroni correction, as in Task 2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of the datasets used for Tasks 1 and 2: Total number of (source) sentences and words (after  tokenisation) for training, development and test for each language pair and MT system type.", "labels": [], "entities": [{"text": "MT system type", "start_pos": 190, "end_pos": 204, "type": "TASK", "confidence": 0.8833585182825724}]}, {"text": " Table 3: Statistics of the data used for Task 3. Num- ber of sentences, phrases, words and BAD labels for  training, development and test.", "labels": [], "entities": [{"text": "Num- ber", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9393728772799174}, {"text": "BAD", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.5819197297096252}]}, {"text": " Table 4: Statistics of the data used for Task 4. Number  of documents, sentences and (target) words for train- ing, development and test.", "labels": [], "entities": []}, {"text": " Table 5: Official results of the WMT18 Quality Estimation Task 1 for the English-German dataset. The winning  submission is indicated by a \u2022. Baseline systems are highlighted in grey, and ** indicates late submissions that  were not considered for the official ranking of participating systems.", "labels": [], "entities": [{"text": "WMT18 Quality Estimation Task", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.6155429556965828}, {"text": "English-German dataset", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.7768861949443817}]}, {"text": " Table 6: Official results of the WMT18 Quality Estimation Task 1 for the German-English dataset. The winning  submission is indicated by a \u2022. Baseline systems are highlighted in grey, and ** indicates late submissions that  were not considered for the official ranking of participating systems.", "labels": [], "entities": [{"text": "WMT18 Quality Estimation Task", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.610503576695919}, {"text": "German-English dataset", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.8980079889297485}]}, {"text": " Table 7: Official results of the WMT18 Quality Estimation Task 1 for the English-Latvian dataset. The winning  submission is indicated by a \u2022. Baseline systems are highlighted in grey, and ** indicates late submissions that  were not considered for the official ranking of participating systems.", "labels": [], "entities": [{"text": "WMT18 Quality Estimation Task", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.6819948479533195}, {"text": "English-Latvian dataset", "start_pos": 74, "end_pos": 97, "type": "DATASET", "confidence": 0.7085886001586914}]}, {"text": " Table 8: Official results of the WMT18 Quality Estimation Task 1 for the English-Czech dataset. The winning  submission is indicated by a \u2022. Baseline systems are highlighted in grey, and ** indicates late submissions that  were not considered for the official ranking of participating systems.", "labels": [], "entities": [{"text": "WMT18 Quality Estimation Task", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.6298905238509178}, {"text": "English-Czech dataset", "start_pos": 74, "end_pos": 95, "type": "DATASET", "confidence": 0.786181628704071}]}, {"text": " Table 13: Official results of the WMT18 Quality Estimation Task 3a (word-level) for the", "labels": [], "entities": [{"text": "WMT18 Quality Estimation Task", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6759055703878403}]}, {"text": " Table 14: Official results of the WMT18 Quality Estimation Task 3b (phrase-level) for the", "labels": [], "entities": [{"text": "WMT18 Quality Estimation Task", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6793730109930038}]}, {"text": " Table 15. Due to the different nu- meric range, only the Pearson correlation scores  are comparable to those of Task1. Comparing with  the results for Task 1, it can be observed that the  baseline system already obtains very high correla- tion. The neural model SHEF-PT-indomain out- performs the baseline by a modest margin, com- pared to the results obtained in Task 1.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 58, "end_pos": 77, "type": "METRIC", "confidence": 0.964422881603241}]}, {"text": " Table 15: Official results of the WMT18 Quality Es- timation Task 4 for the English-French dataset. The  winning submission is indicated by a \u2022. Baseline sys- tem is in grey, and ** indicates late submissions that  were not considered for the official ranking of partici- pating systems.", "labels": [], "entities": [{"text": "WMT18 Quality Es- timation Task", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.5079251974821091}, {"text": "English-French dataset", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.7212745100259781}]}]}