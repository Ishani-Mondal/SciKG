{"title": [{"text": "Distractor Generation for Multiple Choice Questions Using Learning to Rank", "labels": [], "entities": [{"text": "Distractor Generation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9400667548179626}]}], "abstractContent": [{"text": "We investigate how machine learning models , specifically ranking models, can be used to select useful distractors for multiple choice questions.", "labels": [], "entities": []}, {"text": "Our proposed models can learn to select distractors that resemble those in actual exam questions, which is different from most existing unsupervised ontology-based and similarity-based methods.", "labels": [], "entities": []}, {"text": "We empirically study feature-based and neural net (NN) based ranking models with experiments on the recently released SciQ dataset and our MCQL dataset.", "labels": [], "entities": [{"text": "SciQ dataset", "start_pos": 118, "end_pos": 130, "type": "DATASET", "confidence": 0.9045084118843079}, {"text": "MCQL dataset", "start_pos": 139, "end_pos": 151, "type": "DATASET", "confidence": 0.9670928716659546}]}, {"text": "Experimental results show that feature-based ensemble learning methods (random forest and LambdaMART) outper-form both the NN-based method and unsuper-vised baselines.", "labels": [], "entities": []}, {"text": "These two datasets can also be used as benchmarks for distractor generation.", "labels": [], "entities": [{"text": "distractor generation", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.8847224116325378}]}], "introductionContent": [{"text": "Multiple choice questions (MCQs) are widely used as an assessment of students' knowledge and skills.", "labels": [], "entities": [{"text": "Multiple choice questions (MCQs)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5731834520896276}]}, {"text": "A MCQ consists of three elements: (i) stem, the question sentence; (ii) key, the correct answer; (iii) distractors, alternative answers used to distract students from the correct answer.", "labels": [], "entities": []}, {"text": "Among all methods for creating good MCQs, finding reasonable distractors is crucial and usually the most time-consuming.", "labels": [], "entities": []}, {"text": "We here investigate automatic distractor generation (DG), i.e., generating distractors given the stem and the key to the question.", "labels": [], "entities": [{"text": "automatic distractor generation (DG)", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.7594339996576309}]}, {"text": "We focus on the case where distractors are not limited to single words and can be phrases and sentences.", "labels": [], "entities": []}, {"text": "Rather than generate trivial wrong answers, the goal of DG is to generate plausible false answers -good distractors.", "labels": [], "entities": []}, {"text": "Specifically, a \"good\" distractor should beat least semantically related to the key, grammatically correct given the stem, and consistent with the semantic context of the stem.", "labels": [], "entities": []}, {"text": "Taking these criterion into consideration, most existing methods for DG are based on various similarity measures.", "labels": [], "entities": [{"text": "DG", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9864294528961182}]}, {"text": "These include WordNet-based metrics, embedding-based similarities, n-gram co-occurrence likelihood, phonetic and morphological similarities (, structural similarities in an ontology, a thesaurus (), context similarity (, context-sensitive inference (, and syntactic similarity ().", "labels": [], "entities": []}, {"text": "Then distractors are selected from a candidate distractor set based on a weighted combination of similarities, where the weights are determined by heuristics.", "labels": [], "entities": []}, {"text": "In contrast to the above-mentioned similaritybased methods, we apply learning-based ranking models to select distractors that resemble those in actual exam MCQs.", "labels": [], "entities": []}, {"text": "Specifically, we propose two types of models for DG: feature-based and NNbased models.", "labels": [], "entities": []}, {"text": "Our models are able to take existing heuristics as features and learn from these questions a function beyond a simple linear combination.", "labels": [], "entities": []}, {"text": "Learning to generate distractors has been previously explored in a few studies.", "labels": [], "entities": []}, {"text": "Given a blanked question, use a discriminative model to predict distractors and apply generative adversarial nets.", "labels": [], "entities": []}, {"text": "They view DG as a multi-class classification problem and use answers as output labels while we use them as input.", "labels": [], "entities": []}, {"text": "Other related work () uses a random forest.", "labels": [], "entities": []}, {"text": "However, with the reported binary classification metrics, the quality of the top generated distractors is not quantitatively evaluated.", "labels": [], "entities": []}, {"text": "Here we conduct a more comprehensive study on various learning models and devise ranking evaluation metrics for DG.", "labels": [], "entities": []}, {"text": "Machine learning of a robust model usually requires large-scale training data.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, there is no benchmark dataset for DG, which makes it difficult to directly compare methods.", "labels": [], "entities": [{"text": "DG", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9310051798820496}]}, {"text": "Prior methods were evaluated on different question sets collected from textbooks (), Wikipedia (, ESL corpuses (, etc.", "labels": [], "entities": [{"text": "ESL corpuses", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.8115622401237488}]}, {"text": "We propose to evaluate DG methods with two datasets: the recently released SciQ dataset () (13.7K MCQs) and the MCQL dataset (7.1K MCQs) that we made.", "labels": [], "entities": [{"text": "SciQ dataset", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.8556559383869171}, {"text": "MCQL dataset", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.9238258302211761}]}, {"text": "These two datasets can be used as benchmarks for training and testing DG models.", "labels": [], "entities": []}, {"text": "Our experimental results show that feature-based ensemble learning methods (random forest and LambdaMART) outperform both the NN-based method and unsupervised baselines for DG.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed DG models on the following two datasets: (i) SciQ an approximate ratio of 10:1:1.", "labels": [], "entities": []}, {"text": "We convert the dataset to lowercase, filter out the distractors such as \"all of them\", \"none of them\", \"both A and B\", and keep questions with at least one distractor.", "labels": [], "entities": []}, {"text": "We use all the keys and distractors in the dataset as candidate distractor set D.", "labels": [], "entities": []}, {"text": "We use Logistic Regression (LR) as the first stage ranker.", "labels": [], "entities": [{"text": "Logistic Regression (LR)", "start_pos": 7, "end_pos": 31, "type": "METRIC", "confidence": 0.7860921084880829}]}, {"text": "As for the second stage, we compare LR, Random Forest (RF), LambdaMART (LM), and the proposed NN-based model (NN).", "labels": [], "entities": []}, {"text": "Specifically, we set C to 1 for LR, use 500 trees for RF, and 500 rounds of boosting for LM.", "labels": [], "entities": [{"text": "LR", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9406027793884277}, {"text": "RF", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.8953334093093872}]}, {"text": "For first stage training, the number of negative samples is set to be equal to the number of distractors, which is 3 for most questions.", "labels": [], "entities": []}, {"text": "And we sample 100 negative samples for second stage training.", "labels": [], "entities": []}, {"text": "More details can be found in the supplementary material.", "labels": [], "entities": []}, {"text": "In addition, we also study the following unsupervised baselines that measure similarities between the key and distractors: (i) pointwise mutual information (PMI) based on co-occurrences; (ii) edit distance (ED), which measures the spelling similarity; and (iii) GloVe embedding similarity (Emb Sim).", "labels": [], "entities": [{"text": "edit distance (ED)", "start_pos": 192, "end_pos": 210, "type": "METRIC", "confidence": 0.8880227446556092}, {"text": "GloVe embedding similarity", "start_pos": 262, "end_pos": 288, "type": "METRIC", "confidence": 0.8361582557360331}]}, {"text": "For evaluation, we report top recall (R@10), precision (P@1, P@3), mean average precision (MAP@10), normalized discounted cumulative gain (NDCG@10), and mean reciprocal rank (MRR).", "labels": [], "entities": [{"text": "recall (R@10)", "start_pos": 30, "end_pos": 43, "type": "METRIC", "confidence": 0.8691936930020651}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9991979002952576}, {"text": "mean average precision (MAP@10)", "start_pos": 67, "end_pos": 98, "type": "METRIC", "confidence": 0.9269604757428169}, {"text": "normalized discounted cumulative gain (NDCG@10)", "start_pos": 100, "end_pos": 147, "type": "METRIC", "confidence": 0.8245493570963541}, {"text": "mean reciprocal rank (MRR)", "start_pos": 153, "end_pos": 179, "type": "METRIC", "confidence": 0.904987762371699}]}, {"text": "First Stage Ranker The main goal of the first stage ranker is to reduce the candidate size for the later stage while achieving a relatively high recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9987332224845886}]}, {"text": "shows the Recall@K for the first stage ranker on the two datasets.", "labels": [], "entities": [{"text": "Recall@K", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9268229802449545}]}, {"text": "Validation set is used for choosing top K predictions for later stage training.", "labels": [], "entities": []}, {"text": "We empirically set K to 2000 for SciQ and 2500 for MCQL to get a recall of about 90%.", "labels": [], "entities": [{"text": "MCQL", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.9029761552810669}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9993981122970581}]}, {"text": "Feature Analysis We conduct a feature analysis to have more insights on the proposed feature set.", "labels": [], "entities": [{"text": "Feature Analysis", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6721317917108536}]}, {"text": "Feature importance is calculated by \"mean decrease impurity\" using RF.", "labels": [], "entities": []}, {"text": "It is defined as the total decrease in node impurity, weighted by the probability of reaching that node, averaged overall trees of the ensemble.", "labels": [], "entities": []}, {"text": "lists the top 10 important features for SciQ and MCQL datasets.", "labels": [], "entities": [{"text": "MCQL datasets", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.9130816161632538}]}, {"text": "We find that: (i) the embedding similarity between a and dis the most important feature, which shows embeddings are effective at capturing semantic relations between a and d.", "labels": [], "entities": []}, {"text": "(ii) String similarities such as Token Sim, ED, and Suffix are more important in MCQL than those in SciQ.", "labels": [], "entities": []}, {"text": "This is consistent with the observation that ED has relatively good performance as seen in.", "labels": [], "entities": [{"text": "ED", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.6504532694816589}]}, {"text": "(iii) The set of top 10 features is the same for SciQ and MCQL, regardless of order.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Ranking results (%) for DG.", "labels": [], "entities": [{"text": "DG", "start_pos": 34, "end_pos": 36, "type": "DATASET", "confidence": 0.5514712929725647}]}]}