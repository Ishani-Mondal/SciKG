{"title": [{"text": "Rule induction for global explanation of trained models", "labels": [], "entities": [{"text": "Rule induction", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8531216382980347}, {"text": "global explanation", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7725149095058441}]}], "abstractContent": [{"text": "Understanding the behavior of a trained network and finding explanations for its outputs is important for improving the network's performance and generalization ability, and for ensuring trust in automated systems.", "labels": [], "entities": []}, {"text": "Several approaches have previously been proposed to identify and visualize the most important features by analyzing a trained network.", "labels": [], "entities": []}, {"text": "However , the relations between different features and classes are lost inmost cases.", "labels": [], "entities": []}, {"text": "We propose a technique to induce sets of if-then-else rules that capture these relations to globally explain the predictions of a network.", "labels": [], "entities": []}, {"text": "We first calculate the importance of the features in the trained network.", "labels": [], "entities": []}, {"text": "We then weigh the original inputs with these feature importance scores, simplify the transformed input space, and finally fit a rule induction model to explain the model predictions.", "labels": [], "entities": []}, {"text": "We find that the output rule-sets can explain the predictions of a neu-ral network trained for 4-class text classification from the 20 newsgroups dataset to a macro-averaged F-score of 0.80.", "labels": [], "entities": [{"text": "4-class text classification", "start_pos": 95, "end_pos": 122, "type": "TASK", "confidence": 0.6452731589476267}, {"text": "F-score", "start_pos": 174, "end_pos": 181, "type": "METRIC", "confidence": 0.9744884371757507}]}, {"text": "We make the code available at https://github.com/ clips/interpret_with_rules.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep, non-linear neural networks are notorious for being black boxes, because the basis of a network's decision is unknown.", "labels": [], "entities": []}, {"text": "Although sometimes we only care about better performance, understanding a trained model is important in many cases.", "labels": [], "entities": []}, {"text": "For example, when a statistical system is used to take decisions regarding a patient's health, it is critical to know the underlying reasons.", "labels": [], "entities": []}, {"text": "have previously discussed a rulebased system that had associated the history of asthma in patients suffering from pneumonia with a lower risk of death due to it.", "labels": [], "entities": []}, {"text": "Despite being counterintuitive, it was a predictive pattern in the data because the patients with asthma were admitted directly to the ICU and received more intensive care, which resulted in better outcomes.", "labels": [], "entities": []}, {"text": "Model interpretability is also useful to understand the biases in the data that influence its decision.", "labels": [], "entities": [{"text": "Model interpretability", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6456455886363983}]}, {"text": "For example, explaining a trained model and its outputs can bring attention towards a potentially unfair outcome when a loan or a job opportunity is denied to an individual due to any societal bias present in the training data . Another less discussed aspect of model interpretability is its utility for analyzing a model's strengths and weaknesses.", "labels": [], "entities": []}, {"text": "This understanding can assist with improving the model's performance and generalization ability.", "labels": [], "entities": []}, {"text": "Model interpretability techniques can either have a global or a local scope.", "labels": [], "entities": [{"text": "Model interpretability", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6588228344917297}]}, {"text": "A global explanation refers to the explanation of a complete model, as opposed to local explanations of individual predictions.", "labels": [], "entities": []}, {"text": "Several existing model-agnostic interpretability techniques provide a list of important features as explanations.", "labels": [], "entities": []}, {"text": "In such a list, the information about the interaction between different features and their correspondence to the class is lost.", "labels": [], "entities": []}, {"text": "We propose a technique to understand the relations between the input features and the class labels that a trained supervised neural network captures.", "labels": [], "entities": []}, {"text": "It is therefore a mechanism for global interpretability.", "labels": [], "entities": [{"text": "global interpretability", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.662346214056015}]}, {"text": "We first weigh the input features with their importance in a trained network.", "labels": [], "entities": []}, {"text": "We then select the best features according to the training set, and simplify them to discrete features that represent either a positive, a negative, or no correlation between a high feature value and a class label.", "labels": [], "entities": []}, {"text": "We perform this step to limit the complexity of the output rules and make them easily understandable by humans.", "labels": [], "entities": []}, {"text": "We use this smaller, transformed input space to induce rules that best explain the model's predictions.", "labels": [], "entities": []}, {"text": "We evaluate the technique on a simple text categorization problem to clearly illustrate its operation and results.", "labels": [], "entities": []}, {"text": "We find that the output rules have a macro-averaged F-score 0.80 when explaining the predictions of a feedforward neural network trained to classify a subset of documents from the 20 newsgroups dataset 3 into those about either 'Medicine', 'Space', 'Cryptography', or 'Electronics'.", "labels": [], "entities": [{"text": "F-score", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.9816457033157349}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Precision (P), Recall (R), and F-score (F) when explaining neural network predictions using the  induced rules with features selected using mutual information (MI) and sensitivity analysis (SA).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.945419430732727}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9620107263326645}, {"text": "F-score (F)", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9742636680603027}, {"text": "sensitivity analysis (SA)", "start_pos": 178, "end_pos": 203, "type": "METRIC", "confidence": 0.9383847832679748}]}]}