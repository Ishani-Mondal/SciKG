{"title": [{"text": "How Robust Are Character-Based Word Embeddings in Tagging and MT Against Wrod Scramlbing or Randdm Nouse? Stalin Varanasi", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9169427752494812}]}], "abstractContent": [{"text": "This paper investigates the robustness of NLP against perturbed word forms.", "labels": [], "entities": []}, {"text": "While neural approaches can achieve (almost) human-like accuracy for certain tasks and conditions, they often are sensitive to small changes in the input such as non-canonical input (e.g., typos).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9851341247558594}]}, {"text": "Yet both stability and robustness are desired properties in applications involving user-generated content, and all the more so as humans easily cope with such noisy or adversary conditions.", "labels": [], "entities": []}, {"text": "In this paper, we study the impact of noisy input.", "labels": [], "entities": []}, {"text": "We consider different noise distributions (different density and different types) and mismatched noise distributions for training and testing.", "labels": [], "entities": []}, {"text": "Moreover, we empirically evaluate the robustness of different models (convolutional neu-ral networks, recurrent neural networks, non-neural models), different basic units (characters, byte pair encoding units, and words), and different NLP tasks (morphological tagging, machine translation).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 270, "end_pos": 289, "type": "TASK", "confidence": 0.7626878321170807}]}], "introductionContent": [{"text": "In this paper, we study the effect of non-normalized text on natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.7617531816164652}]}, {"text": "Non-normalized text includes non-canonical word forms, noisy word forms, and word forms with \"small\" perturbations, such as informal spellings, typos, scrambled words.", "labels": [], "entities": []}, {"text": "Compared to normalized text, the variability of non-normalized text is much greater and aggravates the problem of data sparsity.", "labels": [], "entities": []}, {"text": "Non-normalized text dominates in many real world applications.", "labels": [], "entities": []}, {"text": "Similar to humans, ideally NLP should perform reliably and robustly also under suboptimal or even adversarial conditions, without a significant degradation in performance.", "labels": [], "entities": []}, {"text": "Web-based content and social media area rich source for noisy and informal text.", "labels": [], "entities": []}, {"text": "Noise can also be introduced in a downstream NLP application where errors are propagated from one module to the next.", "labels": [], "entities": []}, {"text": "For example, speech translation where the machine translation (MT) module needs to be robust against errors introduced by the automatic speech recognition (ASR) module.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.8397975564002991}, {"text": "machine translation (MT)", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.8256080567836761}, {"text": "automatic speech recognition (ASR)", "start_pos": 126, "end_pos": 160, "type": "TASK", "confidence": 0.7889985342820486}]}, {"text": "Moreover, NLP should not be vulnerable to adversarial input examples.", "labels": [], "entities": []}, {"text": "While all these examples do not pose areal challenge to an experienced human reader, even \"small\" perturbations from the canonical form can make a state-of-the-art NLP system fail.", "labels": [], "entities": []}, {"text": "To illustrate the typical behavior of state-of-the-art NLP on normalized and nonnormalized text, we discuss an example in the context of neural MT (NMT).", "labels": [], "entities": []}, {"text": "Different research groups have shown that NMT can generate natural and fluent translations (, achieving human-like performance in certain settings ( By contrast, an experienced human reader can still understand and correctly translate the noisy sentence and compensate for some information loss (including real word errors such as \"no\" vs. \"on\", but rather not \"10.74\" vs. \"1.074\"), with little additional effort and often not even noticing \"small\" perturbations.", "labels": [], "entities": []}, {"text": "One might argue that a good translation should in fact translate corrupted language into corrupted language.", "labels": [], "entities": []}, {"text": "Here, we rather adopt the position that the objective is to preserve the intended content and meaning of a sentence regardless of noise.", "labels": [], "entities": []}, {"text": "It should be noted that neural networks with sufficient capacity, in particular recurrent neural networks, are universal function approximators).", "labels": [], "entities": []}, {"text": "Hence, the performance degradation on non-normalized text is not so much a question whether the model can capture the variability but rather how to train a robust model.", "labels": [], "entities": []}, {"text": "In particular, it can be expected that training on noisy data will make NLP more robust, as it was successfully demonstrated for other application domains including vision () and speech recognition (.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.8155260384082794}]}, {"text": "In this paper, we empirically evaluate the robustness of different models (convolutional neural networks, recurrent neural networks, non-neural models), different basic units (characters, byte pair encoding units), and different NLP tasks (morphological tagging, NMT).", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 240, "end_pos": 261, "type": "TASK", "confidence": 0.7300050854682922}]}, {"text": "Due to easy availability and to have more control on the experimental setup with respect to error type and error density, we use synthetic data generated from existing clean corpora by perturbing the word forms.", "labels": [], "entities": []}, {"text": "The perturbations include character flips and swaps of neighboring characters to imitate typos, and word scrambling.", "labels": [], "entities": [{"text": "character flips and swaps of neighboring characters", "start_pos": 26, "end_pos": 77, "type": "TASK", "confidence": 0.8290360782827649}]}, {"text": "The contributions of this paper are the following.", "labels": [], "entities": []}, {"text": "Our experiments confirm that (i) noisy input substantially degrades the output of models trained on clean data.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the noise types and Section 4 briefly summarizes the modeling approaches used in this paper.", "labels": [], "entities": []}, {"text": "Experimental results are shown and discussed in Section 5.", "labels": [], "entities": []}, {"text": "The paper is concluded in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we empirically evaluate the robustness against perturbed word forms (Section 3) for the two common NLP tasks morphological tagging and machine translation.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 126, "end_pos": 147, "type": "TASK", "confidence": 0.7231585085391998}, {"text": "machine translation", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.8024713695049286}]}], "tableCaptions": [{"text": " Table 2: BLEU on newstest2016-deen for clean and noisy NMT and different test noise types", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988370537757874}, {"text": "newstest2016-deen", "start_pos": 18, "end_pos": 35, "type": "DATASET", "confidence": 0.965238094329834}]}]}