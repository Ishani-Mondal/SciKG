{"title": [], "abstractContent": [], "introductionContent": [{"text": "A glut of recent research shows that language models capture linguistic structure.", "labels": [], "entities": []}, {"text": "found that LSTM-based language models may encode syntactic information sufficient to favor verbs which match the number of their subject nouns.", "labels": [], "entities": []}, {"text": "suggested that the high performance of LSTMs may depend on the linguistic structure of the input data, as performance on several artificial tasks was higher with natural language data than with artificial sequential data.", "labels": [], "entities": []}, {"text": "Such work answers the question of whether a model represents linguistic structure.", "labels": [], "entities": []}, {"text": "But how and when are these structures acquired?", "labels": [], "entities": []}, {"text": "Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned overtime.", "labels": [], "entities": []}, {"text": "In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early and global topic information learned continuously.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 122, "end_pos": 136, "type": "TASK", "confidence": 0.7102566659450531}]}], "datasetContent": [{"text": "All experiments are conducted on 1.6GB of English Wikipedia (70/10/20 train/dev/test split) with a 2-layer LSTM language model featuring tied weights in the softmax and embedding layers.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.8470968306064606}]}], "tableCaptions": []}