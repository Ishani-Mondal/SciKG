{"title": [{"text": "Identification of Parallel Sentences in Comparable Monolingual Corpora from Different Registers", "labels": [], "entities": [{"text": "Identification of Parallel Sentences", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8752064108848572}]}], "abstractContent": [{"text": "Parallel aligned sentences provide useful information for different NLP applications.", "labels": [], "entities": []}, {"text": "Yet, this kind of data is seldom available, especially for languages other than English.", "labels": [], "entities": []}, {"text": "We propose to exploit comparable corpora in French which are distinguished by their registers (spe-cialized and simplified versions) to detect and align parallel sentences.", "labels": [], "entities": []}, {"text": "These corpora are related to the biomedical area.", "labels": [], "entities": []}, {"text": "Our purpose is to state whether a given pair of specialized and simplified sentences is to be aligned or not.", "labels": [], "entities": []}, {"text": "Manually created reference data show 0.76 inter-annotator agreement.", "labels": [], "entities": []}, {"text": "We exploit a set of features and several automatic classi-fiers.", "labels": [], "entities": []}, {"text": "The automatic alignment reaches up to 0.93 Precision, Recall and F-measure.", "labels": [], "entities": [{"text": "automatic", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9592287540435791}, {"text": "alignment", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.7960744500160217}, {"text": "Precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9981873631477356}, {"text": "Recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9981772899627686}, {"text": "F-measure", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9981895089149475}]}, {"text": "In order to better evaluate the method, it is applied to data in English from the SemEval STS competitions.", "labels": [], "entities": [{"text": "SemEval STS", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.7653210163116455}]}, {"text": "The same features and models are applied in monolingual and cross-lingual contexts , in which they show up to 0.90 and 0.73 F-measure, respectively.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9959859251976013}]}], "introductionContent": [{"text": "The purpose of text simplification is to provide simplified versions of texts, in order to remove or replace difficult words or information.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7271221578121185}]}, {"text": "Simplification can be concerned with different linguistic aspects, such as lexicon, syntax, semantics, pragmatics and even document structure.", "labels": [], "entities": []}, {"text": "Simplification can address needs of people or NLP applications ().", "labels": [], "entities": []}, {"text": "In the first case, simplified documents are typically created for children), people with low literacy or foreigners (, people with mental or neurodegenerative disorders, or laypeople who face specialized documents).", "labels": [], "entities": []}, {"text": "In the second case, the purpose of simplification is to transform documents in order to make them easier to process within other NLP tasks, such as syntactic analysis, semantic annotation (, summarization, machine translation (), indexing), or information retrieval and extraction).", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.719233900308609}, {"text": "summarization", "start_pos": 191, "end_pos": 204, "type": "TASK", "confidence": 0.9828923940658569}, {"text": "machine translation", "start_pos": 206, "end_pos": 225, "type": "TASK", "confidence": 0.6929086893796921}, {"text": "information retrieval and extraction", "start_pos": 244, "end_pos": 280, "type": "TASK", "confidence": 0.8396786451339722}]}, {"text": "Hence, parallel sentences, which align difficult and simple information, provide crucial indicators for the text simplification.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.6922561228275299}]}, {"text": "Indeed such pairs of sentences contain cues on transformations which are suitable for the simplification, such as lexical substitutes and syntactic modifications.", "labels": [], "entities": []}, {"text": "Yet, this kind of resources is seldom available, especially in languages other than English.", "labels": [], "entities": []}, {"text": "The purpose of our work is to detect and align parallel sentences from comparable monolingual corpora, that are differentiated by their registers.", "labels": [], "entities": []}, {"text": "Besides, comparable corpora are easier to obtain.", "labels": [], "entities": []}, {"text": "More precisely, we work with texts written for specialists and their simplified versions.", "labels": [], "entities": []}, {"text": "We work with corpora in French.", "labels": [], "entities": []}], "datasetContent": [{"text": "The training of the system is performed on two thirds of the sentence pairs, and the testis performed on the remaining third.", "labels": [], "entities": []}, {"text": "Several classifiers and several combinations of features are tested.", "labels": [], "entities": []}, {"text": "Classical evaluation measures are computed: Precision, Recall, F-measure, Mean Square Errors, and True Positives.", "labels": [], "entities": [{"text": "Precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9984626770019531}, {"text": "Recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9946698546409607}, {"text": "F-measure", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9899173378944397}, {"text": "Mean Square Errors", "start_pos": 74, "end_pos": 92, "type": "METRIC", "confidence": 0.9442832668622335}]}, {"text": "Our baseline is the combination of length measures with the common words (features 1, 2, 4 and 5).", "labels": [], "entities": []}, {"text": "These features are indeed traditionnally exploited in the existing work.", "labels": [], "entities": []}, {"text": "We also evaluate the system on data in English that were released for STS competitions : we use 750 sentence pairs from SemEval 2012, 1,500 sentence pairs from SemEval 2013, 3,750 sentence pairs from SemEval 2014.", "labels": [], "entities": []}, {"text": "Each pair of sentences is associated with the similarity score.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 46, "end_pos": 62, "type": "METRIC", "confidence": 0.9810313880443573}]}, {"text": "We apply our system to these data in two ways: (1) the system is trained and tested on the STS dataset, and (2) the system is trained on our dataset in French and tested on the STS dataset in English.", "labels": [], "entities": [{"text": "STS dataset", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.9440542161464691}, {"text": "STS dataset", "start_pos": 177, "end_pos": 188, "type": "DATASET", "confidence": 0.9359533786773682}]}, {"text": "We assume indeed that the features used and even the models generated can be transposed to data in other languages.", "labels": [], "entities": []}, {"text": "For the experiments with the English data, we use the same evaluation measures (Precision, Recall, F-measure, Mean Square Errors, and True Positives).", "labels": [], "entities": [{"text": "English data", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.7454176247119904}, {"text": "Precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9968602657318115}, {"text": "Recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9839674830436707}, {"text": "F-measure", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.984774112701416}, {"text": "Mean Square Errors", "start_pos": 110, "end_pos": 128, "type": "METRIC", "confidence": 0.935113787651062}]}, {"text": "The set of stopwords in English contains 150 entities.", "labels": [], "entities": []}, {"text": "In, we present the results obtained on French data using the whole set of features (but without the tf*idf similarity scores) on test set, and non-lemmatized texts.", "labels": [], "entities": [{"text": "French data", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.8244151771068573}]}, {"text": "The results are indicated in terms of Recall R, Precision P , F-measure F , Mean Square Errors M SE and True positives T P (out of the 221 positive sentence pairs in the test set).", "labels": [], "entities": [{"text": "Recall R", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9662289321422577}, {"text": "Precision P", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9716023206710815}, {"text": "F-measure F", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9509193897247314}, {"text": "Mean Square Errors M SE", "start_pos": 76, "end_pos": 99, "type": "METRIC", "confidence": 0.9527711510658264}, {"text": "True positives T P", "start_pos": 104, "end_pos": 122, "type": "METRIC", "confidence": 0.9427291601896286}]}, {"text": "We can see that all the classifiers are competitive with F-measure above 0.80.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9918907880783081}]}, {"text": "Overall, several classifiers (LDA, QDA, LogReg, LinSVM) provide stable results, for which we indicate the evaluation scores obtained in one iteration.", "labels": [], "entities": []}, {"text": "Other classifiers (Perceptron, MLP, SGD) provide fluctuating results, and we indicate then the average scores obtained after 20 iterations.", "labels": [], "entities": []}, {"text": "Another positive observation is that Precision and Recall values are well balanced.", "labels": [], "entities": [{"text": "Precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9978518486022949}, {"text": "Recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.983163058757782}]}, {"text": "Logistic regression seems to be the best classifier for this task, with Precision, Recall and F-measure at 0.93.", "labels": [], "entities": [{"text": "Precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9981054067611694}, {"text": "Recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9781140089035034}, {"text": "F-measure", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9972963929176331}]}, {"text": "This classifier is used for the experiments described in the next sections.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Size of the three source corpora. (column headers : number of documents, total of occurrences (specialized  and simple), total of unique words (specialized and simple))", "labels": [], "entities": []}, {"text": " Table 2: Size of the reference data with consensual alignment of sentences. (number of sentence pairs and word  occurrences for each subset)", "labels": [], "entities": []}, {"text": " Table 3: Alignment results obtained with different clas- sifiers on French data, test set, whole featureset without  tf*idf similarity scores, and non-lemmatized text.", "labels": [], "entities": [{"text": "French data", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.8509560823440552}]}, {"text": " Table 4: Alignment results obtained with various fea- turesets, logistic regression, non-lemmatized text.", "labels": [], "entities": []}, {"text": " Table 5: Alignment results obtained on the STS data in  English, test set, whole featureset, logistic regression,  non-lemmatized text and training on the STS data.", "labels": [], "entities": [{"text": "STS data", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.8137626647949219}, {"text": "STS data", "start_pos": 156, "end_pos": 164, "type": "DATASET", "confidence": 0.8638768494129181}]}, {"text": " Table 6: Alignment results obtained on the STS data in  English, test set, whole featureset, Logistic regression,  non-lemmatized text and training on the French data.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8243332505226135}, {"text": "STS data", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.7481311857700348}, {"text": "French data", "start_pos": 156, "end_pos": 167, "type": "DATASET", "confidence": 0.8516856133937836}]}]}