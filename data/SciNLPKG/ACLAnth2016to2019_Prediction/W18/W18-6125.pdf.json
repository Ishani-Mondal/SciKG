{"title": [{"text": "Low-resource named entity recognition via multi-source projection: Not quite there yet?", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7435457706451416}]}], "abstractContent": [{"text": "Projecting linguistic annotations through word alignments is one of the most prevalent approaches to cross-lingual transfer learning.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.6692725718021393}, {"text": "cross-lingual transfer learning", "start_pos": 101, "end_pos": 132, "type": "TASK", "confidence": 0.8529882431030273}]}, {"text": "Conventional wisdom suggests that annotation projection \"just works\" regardless of the task at hand.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.8025776445865631}]}, {"text": "We carefully consider multi-source projection for named entity recognition.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6363571584224701}]}, {"text": "Our experiment with 17 languages shows that to detect named entities in true low-resource languages , annotation projection may not be the right way to move forward.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.7190506607294083}]}, {"text": "On a more positive note, we also uncover the conditions that do favor named entity projection from multiple sources.", "labels": [], "entities": [{"text": "named entity projection", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.7191073894500732}]}, {"text": "We argue these are infeasible under noisy low-resource constraints.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "shows the NERannotated datasets we used.", "labels": [], "entities": [{"text": "NERannotated datasets", "start_pos": 10, "end_pos": 31, "type": "DATASET", "confidence": 0.9498347043991089}]}, {"text": "These datasets adhere to various differing standards of NE encoding.", "labels": [], "entities": [{"text": "NE encoding", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.9207005500793457}]}, {"text": "Ina non-trivial effort, we semi-automatically normalize the data into 3-class CoNLL IO encoding, as the common denominator for the widely heterogeneous datasets.", "labels": [], "entities": []}, {"text": "We thus detect names of locations (LOC), organizations (ORG), and persons (PER).", "labels": [], "entities": []}, {"text": "Languages with more than 5k monolingual training sentences serve as sources and development languages for parameter tuning, while the remainder pose as low-resource targets; see.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.7141695767641068}]}, {"text": "For languages that have multiple datasets, we concatenate the data.", "labels": [], "entities": []}, {"text": "We end up with typologically diverse sets of sources and targets.", "labels": [], "entities": []}, {"text": "We use the predefined train-dev-test splits if available; if not, we split the data at 70-10-20%.", "labels": [], "entities": []}, {"text": "We contrast two sources of parallel data: Europarl () and Watchtower).", "labels": [], "entities": [{"text": "Europarl", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.973484992980957}, {"text": "Watchtower", "start_pos": 58, "end_pos": 68, "type": "DATASET", "confidence": 0.9731566309928894}]}, {"text": "The former covers only 21 resource-rich languages but with 400k-2M parallel sentences for each language pair, while the latter currently spans over 300 languages, but with only 10-100k sentences per pair.", "labels": [], "entities": []}, {"text": "Europarl comes with near-perfect sentence alignment and tokenization, and we align its words using IBM2 ().", "labels": [], "entities": [{"text": "Europarl", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9546087384223938}, {"text": "sentence alignment", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7566043138504028}]}, {"text": "For Watchtower we inherit the original noisy preprocessing: simple whitespace tokenization, automatic sentence alignment, and IBM1 word alignments by Agi\u00b4cAgi\u00b4c et al. as they show that IBM1 in particular helps debias for low-resource languages.", "labels": [], "entities": [{"text": "Watchtower", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9510898590087891}, {"text": "sentence alignment", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.730035588145256}, {"text": "IBM1 word alignments", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.4946763018767039}]}, {"text": "We implement a bi-LSTM NE tagger inspired by and.", "labels": [], "entities": []}, {"text": "We tune it on English development data at two bi-LSTM layers (d = 300), a final dense layer (d = 4), 10 training epochs with SGD, and regular and recurrent dropout at p = 0.5.", "labels": [], "entities": []}, {"text": "We use pretrained fastText embeddings (.", "labels": [], "entities": []}, {"text": "Currently fastText supports 294 languages and is superior to random initialization in our tagger.", "labels": [], "entities": []}, {"text": "Other than through fastText, we don't make explicit use of sub-word embeddings.", "labels": [], "entities": []}, {"text": "Our monolingual F 1 score on English is 86.35 under the more standard IOB2 encoding.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9215365449587504}, {"text": "IOB2 encoding", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.8768169581890106}]}, {"text": "We do not aim to produce a state-of-the-art model, but to contrast the scores for various annotation projection parameters.", "labels": [], "entities": []}, {"text": "We use our tagger both to annotate the source sides of parallel corpora, and to train projected target language NER models.", "labels": [], "entities": []}, {"text": "All reported NE tagging results are means over 4 runs.", "labels": [], "entities": [{"text": "NE tagging", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.9447052478790283}]}], "tableCaptions": []}