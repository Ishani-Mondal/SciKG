{"title": [{"text": "NILC at CWI 2018: Exploring Feature Engineering and Feature Learning", "labels": [], "entities": [{"text": "NILC at CWI 2018", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.8520409464836121}, {"text": "Feature Learning", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7819980382919312}]}], "abstractContent": [{"text": "This paper describes the results of NILC team at CWI 2018.", "labels": [], "entities": [{"text": "NILC team at CWI 2018", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.897502887248993}]}, {"text": "We developed solutions following three approaches: (i) a feature engineering method using lexical, n-gram and psycholin-guistic features, (ii) a shallow neural network method using only word embeddings, and (iii) a Long Short-Term Memory (LSTM) language model, which is pre-trained on a large text corpus to produce a contextualized word vector.", "labels": [], "entities": []}, {"text": "The feature engineering method obtained our best results for the classification task and the LSTM model achieved the best results for the probabilistic classification task.", "labels": [], "entities": [{"text": "classification task", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.9039222896099091}, {"text": "probabilistic classification task", "start_pos": 138, "end_pos": 171, "type": "TASK", "confidence": 0.7364394863446554}]}, {"text": "Our results show that deep neural networks are able to perform as well as traditional machine learning methods using manually engineered features for the task of complex word identification in English.", "labels": [], "entities": [{"text": "word identification", "start_pos": 170, "end_pos": 189, "type": "TASK", "confidence": 0.7155313789844513}]}], "introductionContent": [{"text": "Research efforts on text simplification have mostly focused on either lexical or syntactic simplification.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.8132963180541992}]}, {"text": "Lexical simplification involves replacing specific words in order to reduce lexical complexity.", "labels": [], "entities": [{"text": "Lexical simplification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9058748781681061}]}, {"text": "Lexical simplification is an open problem, as identifying and simplifying complex words in a given context is not straightforward.", "labels": [], "entities": [{"text": "Lexical simplification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9161383211612701}]}, {"text": "Although very intuitive, this is a challenging task since the substitutions must preserve both the original meaning and the grammaticality of the sentence being simplified.", "labels": [], "entities": []}, {"text": "Complex word identification is part of the usual lexical simplification pipeline (, which is illustrated in.", "labels": [], "entities": [{"text": "Complex word identification", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7083221177260081}]}, {"text": "For the challenge, we focused on the English monolingual CWI track.", "labels": [], "entities": [{"text": "English monolingual CWI track", "start_pos": 37, "end_pos": 66, "type": "DATASET", "confidence": 0.5110840126872063}]}, {"text": "We implemented three * The opinions expressed in this article are those of the authors and do not necessarily reflect the official policy or position of the Ita\u00fa-Unibanco.", "labels": [], "entities": [{"text": "Ita\u00fa-Unibanco", "start_pos": 157, "end_pos": 170, "type": "DATASET", "confidence": 0.9098031520843506}]}, {"text": "approaches using machine learning: the first one uses feature engineering; the second one takes the average embedding of target words as input to a neural network; and the third approach models the context of the target words using an LSTM).", "labels": [], "entities": []}, {"text": "Our code is publicly available at github 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "Train  In this work, we used two extra corpora to train language models, one of these to train a neural language model: \u2022 BookCorpus dataset: which has 11,038 free books written by yet unpublished authors (); \u2022 One Billion Word dataset: which is the largest public benchmark for language modeling ().", "labels": [], "entities": [{"text": "BookCorpus dataset", "start_pos": 122, "end_pos": 140, "type": "DATASET", "confidence": 0.9720009863376617}]}], "tableCaptions": [{"text": " Table 1: CWI 2018 english dataset distribution.", "labels": [], "entities": [{"text": "CWI 2018 english dataset distribution", "start_pos": 10, "end_pos": 47, "type": "DATASET", "confidence": 0.9631394386291504}]}, {"text": " Table 2: F1 (macro) for English monolingual classification task.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9988516569137573}, {"text": "English monolingual classification task", "start_pos": 25, "end_pos": 64, "type": "TASK", "confidence": 0.6472440809011459}]}, {"text": " Table 3: MAE for English monolingual probabilistic classification task.", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9788845777511597}, {"text": "English monolingual probabilistic classification task", "start_pos": 18, "end_pos": 71, "type": "TASK", "confidence": 0.6153213798999786}]}]}