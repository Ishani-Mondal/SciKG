{"title": [{"text": "How much should you ask? On the question structure in QA systems", "labels": [], "entities": [{"text": "question structure", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7014686167240143}]}], "abstractContent": [{"text": "Datasets that boosted state-of-the-art solutions for Question Answering (QA) systems prove that it is possible to ask questions in natural language manner.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.8526637077331543}]}, {"text": "However, users are still used to query-like systems where they type in keywords to search for answer.", "labels": [], "entities": []}, {"text": "In this study we validate which parts of questions are essential for obtaining valid answer.", "labels": [], "entities": []}, {"text": "In order to conclude that, we take advantage of LIME-a framework that explains prediction by local approximation.", "labels": [], "entities": [{"text": "LIME-a", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9469143152236938}]}, {"text": "We find that grammar and natural language is disregarded by QA.", "labels": [], "entities": []}, {"text": "State-of-the-art model can answer properly even if 'asked' only with a few words with high coefficients calculated with LIME.", "labels": [], "entities": [{"text": "LIME", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9741161465644836}]}, {"text": "According to our knowledge, it is the first time that QA model is being explained by LIME.", "labels": [], "entities": [{"text": "LIME", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.7919332981109619}]}], "introductionContent": [{"text": "Release of SQuAD () dataset boosted development of state-of-the-art solutions in Question Answering (QA) systems.", "labels": [], "entities": [{"text": "SQuAD () dataset", "start_pos": 11, "end_pos": 27, "type": "DATASET", "confidence": 0.6682173510392507}, {"text": "Question Answering (QA)", "start_pos": 81, "end_pos": 104, "type": "TASK", "confidence": 0.8571671843528748}]}, {"text": "Questions asked in natural way give opportunity for human-computer interaction.", "labels": [], "entities": []}, {"text": "However, in real life scenario, users are used to 'querying' rather than 'asking'.", "labels": [], "entities": []}, {"text": "This assumption inspired us to investigate whether QA systems trained on SQuAD dataset could be used by people who prefer to write faster and more intuitive queries.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.7993897497653961}]}, {"text": "Our experiments indicate that indeed, QA system returns true answer once we type in just selected keywords without keeping the sentence structure.", "labels": [], "entities": []}, {"text": "We conclude that QA systems have a very limited understanding of natural language.", "labels": [], "entities": []}, {"text": "They rather learn to distinguish specific words.", "labels": [], "entities": []}, {"text": "This indifference to semanticsaltering edits is called overstability.", "labels": [], "entities": []}, {"text": "Research on this issue was also recently *Both authors contributed equally.", "labels": [], "entities": []}, {"text": "conducted by who compute importance of words by application of Integrated Gradients.", "labels": [], "entities": []}, {"text": "Inspired by LIME () we perturb questions and score newly created examples with context held constant.", "labels": [], "entities": []}, {"text": "We prove that we can remove up to over 90% of words in question and still get the right answer.", "labels": [], "entities": []}, {"text": "The contribution of our study is the following: 1.", "labels": [], "entities": []}, {"text": "We use LIME in QA model for determining which parts of question are substantial for obtaining right answer.", "labels": [], "entities": [{"text": "LIME", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9851374626159668}]}, {"text": "We obtain valid results although QA systems are not natural candidates for explanation with LIME, since they do not solve a regular classification problem.", "labels": [], "entities": [{"text": "LIME", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.8113192915916443}]}, {"text": "2. We show that QA models disregard grammar and syntax and thus can give the right answer once queried with most important keywords, which are the words with high coefficients returned by LIME.", "labels": [], "entities": []}, {"text": "Our findings can serve as a starting point for development of QA models that are immune to adversarial examples and as a result -generalize better.", "labels": [], "entities": []}, {"text": "We use QA system developed by.", "labels": [], "entities": []}, {"text": "We pick this model for its good performance combined with simplicity and popularity of the algorithm, which in its basic form builds the core of many other QA models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to query QA system with most important words indicated by coefficients estimated with LIME, we design a two-step algorithm.", "labels": [], "entities": [{"text": "LIME", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9851537942886353}]}, {"text": "First, we adjust the logic of LIME to our problem, which is perturbing questions while holding the context unchanged.", "labels": [], "entities": [{"text": "LIME", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.7313297986984253}]}, {"text": "We treat each word in context as a separate class and words in question serve as features.", "labels": [], "entities": []}, {"text": "This way, we run LIME in a multiclass setting, with a varied number of\u00b4classesof\u00b4 of\u00b4classes\"for each run.", "labels": [], "entities": []}, {"text": "We inspect 800 examples, analyzing questions for which the QA system predicted right answers.", "labels": [], "entities": []}, {"text": "presents example of algorithm performance.", "labels": [], "entities": []}, {"text": "In this particular case we observe that by leaving only one word type we still get the right answer.", "labels": [], "entities": []}, {"text": "As shown in this word has the highest LIME coefficient.", "labels": [], "entities": [{"text": "LIME coefficient", "start_pos": 38, "end_pos": 54, "type": "METRIC", "confidence": 0.9835423529148102}]}, {"text": "This is quite surprising as one-word question does not convey sufficient information about what we want to ask.", "labels": [], "entities": []}, {"text": "Figure 2 shows distribution of percentages of removed words from question that do not disturb the answer.", "labels": [], "entities": []}, {"text": "It is left-skewed indicating that a large proportion of question can be removed.", "labels": [], "entities": []}, {"text": "We observe that root questions consist mostly of wh-words and nouns, as displayed in table 2.", "labels": [], "entities": []}, {"text": "This behavior can be partly traced down to the characteristics of SQuAD dataset.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.8481877148151398}]}, {"text": "Due to the shortness and focus on just single topic in contexts, the network needs just a single keyword to infer the likely remainder of the question.", "labels": [], "entities": []}, {"text": "For example, if we query \"type\"in a text about rocks in Grand Canyon, it is almost guaranteed that context mentions just single \"type\"which refers to the rock itself.", "labels": [], "entities": []}, {"text": "Moreover, we observe that there are questions which start off with wrong answer, but when we remove one or more words they start to consistently give valid answers.", "labels": [], "entities": []}, {"text": "Based on this, we can hypothesize that although performance of QA systems does not depend on grammar, there are still Word/PoS/Phrase % occurences wh-word + 0 or more words (any) 51 % 1 word (any) 32 % 1 noun 18 % who 16 % wh-word + 1 word (any) 13 % what 12 % 7 and more words 10 % some underlying dependencies between words.", "labels": [], "entities": []}], "tableCaptions": []}