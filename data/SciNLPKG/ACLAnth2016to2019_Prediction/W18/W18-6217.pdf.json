{"title": [{"text": "Aspect Based Sentiment Analysis into the Wild", "labels": [], "entities": [{"text": "Aspect Based Sentiment Analysis", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8685296028852463}]}], "abstractContent": [{"text": "In this paper, we test state-of-the-art Aspect Based Sentiment Analysis (ABSA) systems trained on a widely used dataset on actual data.", "labels": [], "entities": [{"text": "Aspect Based Sentiment Analysis (ABSA)", "start_pos": 40, "end_pos": 78, "type": "TASK", "confidence": 0.7636231524603707}]}, {"text": "We created anew manually annotated dataset of user generated data from the same domain as the training dataset, but from other sources and analyse the differences between the new and the standard ABSA dataset.", "labels": [], "entities": [{"text": "ABSA dataset", "start_pos": 196, "end_pos": 208, "type": "DATASET", "confidence": 0.9032599329948425}]}, {"text": "We then analyse the results in performance of different versions of the same system on both datasets.", "labels": [], "entities": []}, {"text": "We also propose light adaptation methods to increase system robustness.", "labels": [], "entities": [{"text": "light adaptation", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7264302670955658}]}], "introductionContent": [{"text": "The aim of Aspect Based Sentiment Analysis (ABSA) is to detect fine-grained opinions expressed about different aspects of a given entity, on user-generated comments.", "labels": [], "entities": [{"text": "Aspect Based Sentiment Analysis (ABSA)", "start_pos": 11, "end_pos": 49, "type": "TASK", "confidence": 0.7600367707865578}]}, {"text": "Aspects are attributes of an entity, e.g. the screen of a cellphone, the service fora restaurant, or the picture quality of a camera, and can be described by an ontology associated to the entity.", "labels": [], "entities": []}, {"text": "ABSA includes therefore to identify aspects of an entity, and the sentiment expressed by the writer of the comment about different aspects.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.6400783658027649}]}, {"text": "For example, from a sentence extracted from a review about a museum, an ABSA system could extract the following information: This museum hosts remarkable collections, however, prices are quite high and the attendants are not always friendly.", "labels": [], "entities": []}, {"text": "\"collections\": aspect=museum#collection, polarity=positive; \"prices\": aspect=museum#price, polarity=negative; \"attendants\": aspect=museum#service, polarity=negative; ABSA receives now a specific interest from the scientific community, especially with the SemEval dedicated challenges, (),,, that provided a framework to design and evaluate ABSA * Both authors contributed equally.", "labels": [], "entities": []}, {"text": "systems, for different domains, initially on English but for 8 languages in the 2016 (last) edition.", "labels": [], "entities": []}, {"text": "Besides SemEval, other challenges focussing on the task have been also launched recently, for example TASS, dedicated to Spanish, (),,, or GermanEval, dedicated to German ABSA, (.", "labels": [], "entities": [{"text": "TASS", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9178210496902466}, {"text": "GermanEval", "start_pos": 139, "end_pos": 149, "type": "DATASET", "confidence": 0.935698926448822}]}, {"text": "Following this particular interest, the technology performing ABSA becomes more and more mature, however, experiments and evaluation are restricted to a small number of academic datasets, in relatively favorable settings.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 62, "end_pos": 66, "type": "TASK", "confidence": 0.9829989075660706}]}, {"text": "The goal of this paper is to test a state-of-the-art ABSA system on actual data, to evaluate the performance loss in real-world application conditions, and to experiment potential solutions to it.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.96661376953125}]}, {"text": "To achieve this goal, we've created anew ABSA annotated dataset, developed on Foursquare data.", "labels": [], "entities": [{"text": "ABSA annotated dataset", "start_pos": 41, "end_pos": 63, "type": "DATASET", "confidence": 0.76478111743927}, {"text": "Foursquare data", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.9822089374065399}]}, {"text": "We also performed evaluation of the full ABSA processing chain (as opposed to sub-tasks evaluation which is traditionally performed).", "labels": [], "entities": [{"text": "ABSA processing", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.8019116222858429}]}, {"text": "We also propose a weakly supervised method for aspect-based lexical acquisition designed to improve the robustness of our initial system.", "labels": [], "entities": [{"text": "aspect-based lexical acquisition", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.6434409519036611}]}], "datasetContent": [{"text": "Usually, ABSA systems are tested on the same dataset as they are developped on.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.944384753704071}]}, {"text": "One of the widely used ABSA datasets was released in Semeval2016 challenge (, in particular the dataset for restaurant domain.", "labels": [], "entities": [{"text": "ABSA datasets", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.7268422693014145}, {"text": "Semeval2016 challenge", "start_pos": 53, "end_pos": 74, "type": "DATASET", "confidence": 0.7745988368988037}]}, {"text": "It is based on the dataset of () who extracted restaurant reviews from City Search New York over year 2006.", "labels": [], "entities": [{"text": "City Search New York over year 2006", "start_pos": 71, "end_pos": 106, "type": "DATASET", "confidence": 0.8842356630734035}]}, {"text": "Since then, the notion of the user review has evolved.", "labels": [], "entities": []}, {"text": "Many factors may impact the linguistic structure of a review, e.g. the support it was written on (computer vs. smartphone), the age of the user, the location (US vs. UK English), the user mother tongue (native vs. non-native speakers), etc.", "labels": [], "entities": []}, {"text": "How would a system trained on Semeval2016 dataset perform on anew data coming from different sources?", "labels": [], "entities": [{"text": "Semeval2016 dataset", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.7630925178527832}]}, {"text": "In order to assess ABSA real-world performances, we manually annotated a completely new dataset from Foursquare 1 comments.", "labels": [], "entities": [{"text": "ABSA real-world", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.8131643235683441}, {"text": "Foursquare 1 comments", "start_pos": 101, "end_pos": 122, "type": "DATASET", "confidence": 0.937662680943807}]}, {"text": "We have access to about 215K user reviews of restaurants allover the world in English 2 . The reviews were written during the period between 2009 to 2018.", "labels": [], "entities": []}, {"text": "From these reviews, we randomly selected 585 samples, which contain 1006 sentences and annotate these sentences with the SemEval2016 annotation guidelines for the restaurant domain.", "labels": [], "entities": []}, {"text": "The annotations have been performed by a single annotator, expert linguist with a very good knowledge of the SemEval2016 annotation guidelines, using BRAT, (.", "labels": [], "entities": [{"text": "BRAT", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9623157978057861}]}, {"text": "Each sentence contains annotations about: 1.", "labels": [], "entities": []}, {"text": "Opinion Target Expression (OTE), i.e. the linguistic expression (term) used in the text to refer to the reviewed entity, annotated as \"NULL\" if the aspect is implicit; 2.", "labels": [], "entities": [{"text": "Opinion Target Expression (OTE)", "start_pos": 0, "end_pos": 31, "type": "METRIC", "confidence": 0.789204070965449}]}, {"text": "Aspect Categories, i.e. the semantic categories of the opinionated aspects, which are part of a predefined ontology.", "labels": [], "entities": []}, {"text": "<text>Their sake list was extensive, but we were looking for Purple Haze, which wasn't listed but made for us upon request!</text> <Opinions> <Opinion target=\"sake list\" category=\"DRINKS#STYLE_OPTIONS\" polarity=\"positive\"/> <Opinion target=\"NULL\" category=\"SERVICE#GENERAL\" polarity=\"positive\"/> </Opinions> gives some statistics about the Foursquare and Semeval2016 datasets.", "labels": [], "entities": [{"text": "Foursquare and Semeval2016 datasets", "start_pos": 340, "end_pos": 375, "type": "DATASET", "confidence": 0.8750692903995514}]}, {"text": "One may notice, that in average, Foursquare reviews are shorter and therefore contain less aspects per sentence.", "labels": [], "entities": [{"text": "Foursquare reviews", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.8161129355430603}]}, {"text": "We believe this is due to the generalisation of smart-phones (and other mobile devices) usage over the world in the last decade, which influenced the way users write.", "labels": [], "entities": []}, {"text": "We release the Foursquare dataset to the community in order to better assess robustness of ABSA systems 3 . Thus, we evaluate separately the OTE detection, aspect detection and finally, we evaluate the polarity of opinion detection on the ground truth of phase A.", "labels": [], "entities": [{"text": "Foursquare dataset", "start_pos": 15, "end_pos": 33, "type": "DATASET", "confidence": 0.9404903054237366}, {"text": "OTE detection", "start_pos": 141, "end_pos": 154, "type": "TASK", "confidence": 0.7250824868679047}, {"text": "aspect detection", "start_pos": 156, "end_pos": 172, "type": "TASK", "confidence": 0.783260852098465}, {"text": "opinion detection", "start_pos": 214, "end_pos": 231, "type": "TASK", "confidence": 0.7498555779457092}]}, {"text": "The advantage of this evaluation procedure is of course to assess the quality of the systems on each of the different subtasks involved in the full ABSA system.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 148, "end_pos": 152, "type": "TASK", "confidence": 0.8006529808044434}]}, {"text": "However, these measures do not reflect the overall results such systems would obtain on the full chain of annotations starting from raw data, in end-to-end application settings.", "labels": [], "entities": []}, {"text": "Therefore, we also propose to evaluate the results obtained with the complete annotation chain, i.e. computing F1-measure on the triplets <OTE, Aspect, Polarity>.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9979434609413147}, {"text": "OTE", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.9461990594863892}]}, {"text": "In addition, we compute the F1-measure on the pairs <Aspect, Polarity> at sentence level.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9984508752822876}]}, {"text": "This last measure can be useful to assess ABSA general Aspect-Polarity performance since many ABSA applications may not require the OTE step.", "labels": [], "entities": [{"text": "ABSA general Aspect-Polarity", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.6780045032501221}, {"text": "OTE", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.8664431571960449}]}, {"text": "In what follows, we refer to these measures as slot1,3 and slot1,2,3 to make connection with the challenge tasks.", "labels": [], "entities": []}, {"text": "We've performed following series of experiments (summarized in table 3): 1.", "labels": [], "entities": []}, {"text": "f lex: foursquare lexicon extending existing lexicon (for systems using lexicons); 2.", "labels": [], "entities": []}, {"text": "f emb: all baselines with foursquare embeddings replacing generic embeddings (GoogleNews-based) 3.", "labels": [], "entities": []}, {"text": "flex +f emb: combination of the previous two.", "labels": [], "entities": [{"text": "flex +f emb", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.8002600520849228}]}, {"text": "We observe light improvements for baseline-1 which are especially due to lexicon enrichment experiments.", "labels": [], "entities": []}, {"text": "We think that Foursquare embeddings didn't bring expected improvements for baseline-1 (embeddings are used only for OTE/s2 task, which in it's turn impacts s1 task), mostly because these embeddings are much smaller and we lose some non domain-specific knowledge when they replace GoogleNews embeddings.", "labels": [], "entities": []}, {"text": "The impact of embedding is opposite for baseline-2 experiments.", "labels": [], "entities": []}, {"text": "Foursquare pretrained embeddings bring important gains on Foursquare dataset thus moving baseline-2 system above baseline-1 for s1 evaluation.", "labels": [], "entities": [{"text": "Foursquare dataset", "start_pos": 58, "end_pos": 76, "type": "DATASET", "confidence": 0.9364193379878998}]}, {"text": "It also improves (although less) system performance on Semeval dataset.", "labels": [], "entities": [{"text": "Semeval dataset", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.8826326727867126}]}, {"text": "Automatically acquired lexicon on baseline-2 systems seems to be very low.", "labels": [], "entities": []}, {"text": "We plan to explore other ways to integrate this knowledge into deep learning framework.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset statistics: Semeval 2016 test set and  Foursquare dataset. #Rev: number of reviews, #S:  number of sentences, #W/S : number of words per  sentence, #A/S: number of <OTE, Aspect Category,  Polarity> tuples per sentence", "labels": [], "entities": [{"text": "Semeval 2016 test set", "start_pos": 30, "end_pos": 51, "type": "DATASET", "confidence": 0.8476492762565613}, {"text": "Foursquare dataset", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.9657978713512421}, {"text": "Rev", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9841660261154175}]}, {"text": " Table 2: Performance of various baseline systems.  s1: Aspect Category detection (F1), s2: Opinion  Target Expression (F1), s3: Sentiment Polarity (Ac- curacy). s1,3: Aspect,Polarity (F1), s1,2,3: As- pect,OTE,Polarity (F1).", "labels": [], "entities": [{"text": "Aspect Category detection", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.5920010109742483}, {"text": "Opinion  Target Expression (F1)", "start_pos": 92, "end_pos": 123, "type": "METRIC", "confidence": 0.807598481575648}, {"text": "Sentiment Polarity (Ac- curacy)", "start_pos": 129, "end_pos": 160, "type": "METRIC", "confidence": 0.7779020411627633}]}, {"text": " Table 3: Experimental results with foursquare embeddings and automatically acquired lexicon", "labels": [], "entities": []}]}