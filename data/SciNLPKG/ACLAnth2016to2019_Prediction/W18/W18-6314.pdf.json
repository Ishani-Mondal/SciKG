{"title": [{"text": "Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection", "labels": [], "entities": [{"text": "Denoising Neural Machine Translation Training", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.700973516702652}]}], "abstractContent": [{"text": "Measuring domain relevance of data and identifying or selecting well-fit domain data for machine translation (MT) is a well-studied topic, but denoising is not yet.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.8370036721229553}]}, {"text": "Denoising is concerned with a different type of data quality and tries to reduce the negative impact of data noise on MT training, in particular, neu-ral MT (NMT) training.", "labels": [], "entities": [{"text": "MT training", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.9226275980472565}, {"text": "MT (NMT) training", "start_pos": 154, "end_pos": 171, "type": "TASK", "confidence": 0.7705036401748657}]}, {"text": "This paper generalizes methods for measuring and selecting data for domain MT and applies them to denoising NMT training.", "labels": [], "entities": [{"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.7378981709480286}, {"text": "NMT training", "start_pos": 108, "end_pos": 120, "type": "TASK", "confidence": 0.8382967114448547}]}, {"text": "The proposed approach uses trusted data and a denoising curriculum realized by online data selection.", "labels": [], "entities": []}, {"text": "Intrinsic and extrinsic evaluations of the approach show its significant effectiveness for NMT to train on data with severe noise.", "labels": [], "entities": []}], "introductionContent": [{"text": "Data noise is an understudied topic in the machine translation (MT) field.", "labels": [], "entities": [{"text": "Data noise", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.8119034469127655}, {"text": "machine translation (MT)", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.8701844692230225}]}, {"text": "Recent research has found that data noise has a bigger impact on neural machine translation (NMT) than on statistical machine translation, but learning what data quality (or noise) means in NMT and how to make NMT training robust to data noise remains an open research question.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.8317968448003134}, {"text": "statistical machine translation", "start_pos": 106, "end_pos": 137, "type": "TASK", "confidence": 0.6395770311355591}]}, {"text": "On the other hand, a rich body of MT data research focuses on domain data relevance and selection for domain adaptation purpose.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9903680086135864}, {"text": "domain adaptation", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.706198126077652}]}, {"text": "As a result, effective and successful methods have been published and shown to work for both SMT and NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9941151142120361}]}, {"text": "For example,) introduce a metric for measuring the data relevance to a domain by using n-gram language models (LM).", "labels": [], "entities": []}, {"text": "(van der) employ a neuralnetwork version of it and propose a graduallyrefining strategy to dynamically schedule data during NMT training.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 124, "end_pos": 136, "type": "TASK", "confidence": 0.8874515891075134}]}, {"text": "In these methods, a large amount of in-domain data are used to help measure data domain relevance.", "labels": [], "entities": []}, {"text": "Data noise is a different quality that has been shown to affect NMT performance in particular.", "labels": [], "entities": [{"text": "NMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9641972780227661}]}, {"text": "In MT, the use of web crawl, automatic methods for parallel data mining, sentence alignment provide us with parallel data of variable quality from many points of view: sentence breaking, poor sentence alignments, translations, domain adequacy, tokenization and so forth.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.972990095615387}, {"text": "sentence alignment", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7666801512241364}, {"text": "sentence breaking", "start_pos": 168, "end_pos": 185, "type": "TASK", "confidence": 0.73899245262146}, {"text": "sentence alignments", "start_pos": 192, "end_pos": 211, "type": "TASK", "confidence": 0.7721572816371918}]}, {"text": "To deal with such data noise, a commonly used practice is (static) data filtering with simple heuristics or classification.", "labels": [], "entities": [{"text": "static) data filtering", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7285684943199158}]}, {"text": "The NMT community increasingly realizes that this type of quality matters for general NMT translation accuracy.", "labels": [], "entities": [{"text": "NMT translation accuracy", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.7214864293734232}]}, {"text": "For example,) studies the types of data noise and their impact on NMT; WMT 2018 introduces a Parallel Corpus Filtering task on noisy webcrawled data.", "labels": [], "entities": [{"text": "WMT", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.7868483066558838}, {"text": "Parallel Corpus Filtering task", "start_pos": 93, "end_pos": 123, "type": "TASK", "confidence": 0.6685368418693542}]}, {"text": "Unfortunately, the ingredients that made domain data selection methods successful have not been studied in the NMT denoising context.", "labels": [], "entities": [{"text": "NMT denoising", "start_pos": 111, "end_pos": 124, "type": "TASK", "confidence": 0.7089005708694458}]}, {"text": "Specifically, \u2022 How to measure noise?", "labels": [], "entities": []}, {"text": "\u2022 How does noise dynamically interact with the training progress?", "labels": [], "entities": []}, {"text": "\u2022 How to denoise the model training with a small, trusted parallel dataset?", "labels": [], "entities": []}, {"text": "In the denoising scenario, the trusted data would be the counterpart of in-domain monolingual data of domain data selection.", "labels": [], "entities": []}, {"text": "Trusted data can be human translations, a small amount of which can be easily available as a development set or validation set from a normal MT setup.", "labels": [], "entities": [{"text": "MT", "start_pos": 141, "end_pos": 143, "type": "TASK", "confidence": 0.95467209815979}]}, {"text": "We use the example in to illustrate the challenges in the NMT denoising problem, as well as the issue of directly applying existing domain methods as is for this purpose.", "labels": [], "entities": [{"text": "NMT denoising problem", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.8819723129272461}]}, {"text": "Both sentences in the example appear to be relevant to travel conversations, but the sentence pair is \"noisy\" in that, zh gongche zhan zai nali?", "labels": [], "entities": []}, {"text": "zh-gloss bus stop is where?", "labels": [], "entities": []}, {"text": "en Where is the bus stop?", "labels": [], "entities": []}, {"text": "apart of the English sentence does not align to anything on the Chinese side, yet the pair contains some translation and the sentences are fluent.", "labels": [], "entities": []}, {"text": "An LM-based domain-data selection method would generally treat it as a suitable domain example for building a travel NMT model and may not consider this noise.", "labels": [], "entities": [{"text": "LM-based domain-data selection", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.5952579180399576}]}, {"text": "A simple data filtering method based on length or a bilingual dictionary can easily filter it, but, intuitively, the example may still be useful for training the NMT model, especially in a data-scarce scenario -the Chinese sentence and the first half of the English sentence are still a translation pair.", "labels": [], "entities": []}, {"text": "This suggests the subtlety in identifying noisy data for MT -It is not a simple binary problem: Some training samples maybe partially useful to training a model, and their usefulness may also change as training progresses.", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9902170896530151}]}, {"text": "An NMT model alone maybe incapable of identifying noise.", "labels": [], "entities": [{"text": "identifying noise", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.894434005022049}]}, {"text": "Under a conditional seq2seq NMT model that translates Chinese into English, a word, e.g., 81, in the extra English fragment may receive a low probability (or a high loss), but that could as well mean that is hard but still correct translation.", "labels": [], "entities": []}, {"text": "Here is then where the trusted data can play a role -It can help produce a (slightly) better model for the first model to compare against to be able to distinguish informative hard examples from harmful noisy ones.", "labels": [], "entities": []}, {"text": "In this paper, we propose an approach to denoising online NMT training.", "labels": [], "entities": [{"text": "denoising online NMT training", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.7284316569566727}]}, {"text": "It uses a small amount of trusted data to help models measure noise in a sentence pair.", "labels": [], "entities": []}, {"text": "The noise is defined based on comparison between a pair of a noisy NMT model and another, slightly denoised NMT model, inspired by the contrastive in-domain LM vs out-of-domain LM idea.", "labels": [], "entities": []}, {"text": "It employs online data selection to sort sentence pairs by noise level so that the model is trained on gradually noise-reduced data batches.", "labels": [], "entities": []}, {"text": "We show that language model based domain data selection method as is does notwork well whereas the proposed approach is quite effective in denoising NMT training.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 149, "end_pos": 161, "type": "TASK", "confidence": 0.8118425607681274}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Scales for human rating sentence pairs. Percentage ranges refer to the amount of words well translated  across sentences in a pair.", "labels": [], "entities": []}, {"text": " Table 4: BLEU scores of Denoising experiments with  en/fr Paracrawl data and WMT data. \"Incr-denoise  P1\" refers to applying the incremental denoising on the  noisy baseline P1 with method in Section 5.1. Similarly  for \"incr-denoise W1\". Under paired bootstrapped test  at p < 0.05, P3 is significantly better than P2, P3 than  P1, P2 than P1, on all test sets. W3 is significantly bet- ter than W1 on n2014.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985194802284241}, {"text": "WMT data", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.7262356877326965}]}, {"text": " Table 5: Online denoising: NMT trained on data sorted  according to noisiness level. P3 is trained on noisier to  cleaner data order. Reversely, P4 is trained on cleaner  to noisier data order.", "labels": [], "entities": [{"text": "Online denoising", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7109552323818207}]}, {"text": " Table 6: Nested datasets: Data order is important for  denoising. S 80% \u2283 S 40% \u2283 S 20% with stricter/smaller  set less noisy.", "labels": [], "entities": []}, {"text": " Table 7: LM method does not denoise, but NMT  method (proposed) does; and a denoised model has im- proved general translation accuracy. P1+NMT 20%:  fine-tune P1 with top 20% selection by NMT method.  P1+NNLM 20%: fine-tune P1 with top 20% selection  by NNLM method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.8940902948379517}]}]}