{"title": [{"text": "Ensemble Sequence Level Training for Multimodal MT: OSU-Baidu WMT18 Multimodal Machine Translation System Report", "labels": [], "entities": [{"text": "Multimodal MT", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.5614302605390549}, {"text": "OSU-Baidu WMT18 Multimodal Machine Translation", "start_pos": 52, "end_pos": 98, "type": "TASK", "confidence": 0.8014684557914734}]}], "abstractContent": [{"text": "This paper describes multimodal machine translation systems developed jointly by Ore-gon State University and Baidu Research for WMT 2018 Shared Task on multimodal translation.", "labels": [], "entities": [{"text": "multimodal machine translation", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.6218465765317281}, {"text": "Baidu Research", "start_pos": 110, "end_pos": 124, "type": "DATASET", "confidence": 0.9143272638320923}, {"text": "WMT 2018 Shared Task", "start_pos": 129, "end_pos": 149, "type": "TASK", "confidence": 0.5751995295286179}, {"text": "multimodal translation", "start_pos": 153, "end_pos": 175, "type": "TASK", "confidence": 0.6555006951093674}]}, {"text": "In this paper, we introduce a simple approach to incorporate image information by feeding image features to the decoder side.", "labels": [], "entities": []}, {"text": "We also explore different sequence level training methods including scheduled sampling and reinforcement learning which lead to substantial improvements.", "labels": [], "entities": []}, {"text": "Our systems ensemble several models using different architectures and training methods and achieve the best performance for three subtasks: En-De and En-Cs in task 1 and (En+De+Fr)-Cs task 1B.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, neural text generation has attracted much attention due to its impressive generation accuracy and wide applicability.", "labels": [], "entities": [{"text": "neural text generation", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.6347131431102753}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.98973548412323}]}, {"text": "In addition to demonstrating compelling results for machine translation), by simple adaptation, similar models have also proven to be successful for summarization (), image or video captioning () and multimodal machine translation (, which aims to translate the caption from one language to another with the help of the corresponding image.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7964938282966614}, {"text": "summarization", "start_pos": 149, "end_pos": 162, "type": "TASK", "confidence": 0.9898073077201843}, {"text": "image or video captioning", "start_pos": 167, "end_pos": 192, "type": "TASK", "confidence": 0.6024662181735039}, {"text": "multimodal machine translation", "start_pos": 200, "end_pos": 230, "type": "TASK", "confidence": 0.6103774905204773}]}, {"text": "However, the conventional neural text generation models suffer from two major drawbacks.", "labels": [], "entities": [{"text": "text generation", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.7687793374061584}]}, {"text": "First, they are typically trained by predicting the next word given the previous ground-truth word.", "labels": [], "entities": []}, {"text": "But attest time, the models recurrently feed their own predictions into it.", "labels": [], "entities": []}, {"text": "This \"exposure bias\" () leads to error accumulation * Equal contribution \u2020 Contributions made while at Baidu Research during generation attest time.", "labels": [], "entities": [{"text": "Baidu Research", "start_pos": 103, "end_pos": 117, "type": "DATASET", "confidence": 0.8754900395870209}]}, {"text": "Second, the models are optimized by maximizing the probability of the next ground-truth words which is different from the desired non-differentiable evaluation metrics, e.g. BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 174, "end_pos": 178, "type": "METRIC", "confidence": 0.9754805564880371}]}, {"text": "Several approaches have been proposed to tackle the previous problems.", "labels": [], "entities": []}, {"text": "propose scheduled sampling to alleviate \"exposure bias\" by feeding back the model's own predictions with a slowly increasing probability during training.", "labels": [], "entities": []}, {"text": "Furthermore, reinforcement learning () is proven to be helpful to directly optimize the evaluation metrics in neural text generation models training.", "labels": [], "entities": []}, {"text": "successfully use the REINFORCE algorithm to directly optimize the evaluation metric over multiple text generation tasks.;  achieve state-of-the-art on image captioning using REINFORCE with baseline to reduce training variance.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 151, "end_pos": 167, "type": "TASK", "confidence": 0.7495166063308716}]}, {"text": "Moreover, many existing works show that neural text generation models can benefit from model ensembling by simply averaging the outputs of different models.", "labels": [], "entities": [{"text": "neural text generation", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7477707266807556}]}, {"text": "claim that it is essential to introduce diverse models into the ensemble.", "labels": [], "entities": []}, {"text": "To this end, we ensemble models with various architectures and training methods.", "labels": [], "entities": []}, {"text": "This paper describes our participation in the WMT 2018 multimodal tasks.", "labels": [], "entities": [{"text": "WMT 2018 multimodal tasks", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.7263914495706558}]}, {"text": "Our submitted systems include a series of models which only consider text information, as well as multimodal models which also include image information to initialize the decoders.", "labels": [], "entities": []}, {"text": "We train these models using scheduled sampling and reinforcement learning.", "labels": [], "entities": []}, {"text": "The final outputs are decoded by ensembling those models.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first multimodal machine translation system that achieves the state-of-the-art using sequence level learning methods.", "labels": [], "entities": [{"text": "multimodal machine translation", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.6148365437984467}]}], "datasetContent": [{"text": "We perform experiments using which are provided by the WMT organization.", "labels": [], "entities": [{"text": "WMT organization", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.7864098846912384}]}, {"text": "Task 1 (Multimodal Machine Translation) consists of translating an image with an English caption into German, French and Czech.", "labels": [], "entities": [{"text": "Multimodal Machine Translation)", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.7269142866134644}]}, {"text": "Task 1b (Multisource Multimodal Machine Translation) involves translating parallel English, German and French sentences with accompanying image into Czech.", "labels": [], "entities": [{"text": "Multisource Multimodal Machine Translation)", "start_pos": 9, "end_pos": 52, "type": "TASK", "confidence": 0.700340086221695}]}, {"text": "As shown in, both tasks have 2900 training and 1014 validation examples.", "labels": [], "entities": []}, {"text": "For preprocessing, we convert all of the sentences to lowercase, normalize the punctuation, and tokenize.", "labels": [], "entities": []}, {"text": "We employ byte-pair encoding (BPE) () on the whole training data including the four languages and reduce the source and target language vocabulary sizes to 20k in total.", "labels": [], "entities": [{"text": "byte-pair encoding (BPE)", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.6138946413993835}]}], "tableCaptions": [{"text": " Table 1: Statistics of Flickr30K Dataset", "labels": [], "entities": [{"text": "Flickr30K Dataset", "start_pos": 24, "end_pos": 41, "type": "DATASET", "confidence": 0.938104510307312}]}, {"text": " Table 2: BLEU scores of different approaches on  the validation set. Details of the ensemble models  are described in", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987002611160278}]}, {"text": " Table 3: Number of different models used for en- sembling.", "labels": [], "entities": []}, {"text": " Table 4: En-De results on test set. 17 systems in  total. (Only including constrained models).", "labels": [], "entities": []}, {"text": " Table 5: En-Fr results on test set. 14 systems in  total. (Only including constrained models).", "labels": [], "entities": []}, {"text": " Table 6: En-Cs results on test set. 8 systems in  total. (Only including constrained models).", "labels": [], "entities": []}, {"text": " Table 7: BLEU scores on validation set for task 1B", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993220567703247}]}, {"text": " Table 8: Task 1B multi-source translation results  on test set. 6 systems in total.", "labels": [], "entities": [{"text": "multi-source translation", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.5099050253629684}]}, {"text": " Table 9: Rank of our models.  \u2020 represents the total  number of models.  \u2021 represents the total number  of teams.", "labels": [], "entities": []}]}