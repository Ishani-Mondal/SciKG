{"title": [], "abstractContent": [{"text": "The task of linearization is to find a grammatical order given a set of words.", "labels": [], "entities": []}, {"text": "Traditional models use statistical methods.", "labels": [], "entities": []}, {"text": "Syntactic linearization systems, which generate a sentence along with its syntactic tree, have shown state-of-the-art performance.", "labels": [], "entities": []}, {"text": "Recent work shows that a multi-layer LSTM language model outperforms competitive statistical syntactic lineariza-tion systems without using syntax.", "labels": [], "entities": []}, {"text": "In this paper, we study neural syntactic lineariza-tion, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Linearization is the task of finding the grammatical order fora given set of words.", "labels": [], "entities": []}, {"text": "Syntactic linearization systems generate output sentences along with their syntactic trees.", "labels": [], "entities": []}, {"text": "Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (), where no syntactic information is available during decoding, full tree linearization (), where full tree information is available, and partial tree linearization, where partial syntactic information is given as input.", "labels": [], "entities": []}, {"text": "Linearization has been adapted to tasks such as machine translation ( , and is potentially helpful for many NLG applications, such as cooking recipe generation (, dialogue response generation (, and question generation ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7937340438365936}, {"text": "cooking recipe generation", "start_pos": 134, "end_pos": 159, "type": "TASK", "confidence": 0.7114583651224772}, {"text": "dialogue response generation", "start_pos": 163, "end_pos": 191, "type": "TASK", "confidence": 0.7852282921473185}, {"text": "question generation", "start_pos": 199, "end_pos": 218, "type": "TASK", "confidence": 0.8251705169677734}]}, {"text": "Previous work ( has shown that jointly predicting the syntactic tree and the surface string gives better results by allowing syntactic information to guide statistical linearization.", "labels": [], "entities": []}, {"text": "On the other hand, most such methods employ statistical models with discriminative features.", "labels": [], "entities": []}, {"text": "Recently, report new state-of-the-art results by leveraging a neural language model without using syntactic information.", "labels": [], "entities": []}, {"text": "In their experiments, the neural language model, which is less sparse and captures long-range dependencies, outperforms previous discrete syntactic systems.", "labels": [], "entities": []}, {"text": "A research question that naturally arises from this result is whether syntactic information is helpful fora neural linearization system.", "labels": [], "entities": []}, {"text": "We empirically answer this question by comparing a neural transition-based syntactic linearizer with the neural language model of.", "labels": [], "entities": []}, {"text": "Following , our linearizer works incrementally given a set of words, using a stack to store partially built dependency trees, and a set to maintain unordered incoming words.", "labels": [], "entities": []}, {"text": "At each step, it either shifts a word onto the stack, or reduces the top two partial trees on the stack.", "labels": [], "entities": []}, {"text": "We leverage a feed forward neural network, which takes stack features as input and predicts the next action (such as SHIFT, LEFTARC and RIGHTARC).", "labels": [], "entities": [{"text": "LEFTARC", "start_pos": 124, "end_pos": 131, "type": "METRIC", "confidence": 0.9802438020706177}, {"text": "RIGHTARC", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.8425830602645874}]}, {"text": "Hence our method can be regarded as an extension of the parser of, adding word ordering functionalities.", "labels": [], "entities": []}, {"text": "In addition, we investigate two methods for integrating neural language models: interpolating the log probabilities of both models and integrating the neural language model as a feature.", "labels": [], "entities": []}, {"text": "On standard benchmarks, our syntactic linearizer gives results that are higher than the LSTM language model of by 7 BLEU points () using greedy search, and the gap can go up to 11 BLEU points by integrating the LSTM language model as features.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9978629946708679}, {"text": "BLEU", "start_pos": 180, "end_pos": 184, "type": "METRIC", "confidence": 0.9951021671295166}]}, {"text": "The integrated system also outperforms the LSTM language model by 1 BLEU point using beam search, which shows that syntactic information is useful fora neural linearization system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9988605976104736}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Main results and decoding times.", "labels": [], "entities": []}, {"text": " Table 4: Parsing accuracy settings, the F1 scores  are measured on the training set.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.8187493085861206}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9726629257202148}, {"text": "F1", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9989808201789856}]}, {"text": " Table 6: Results of various parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9739078283309937}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.6054837703704834}]}, {"text": " Table 7: Top similar actions for shift actions", "labels": [], "entities": []}]}