{"title": [{"text": "Variational Cross-domain Natural Language Generation for Spoken Dialogue Systems", "labels": [], "entities": [{"text": "Variational Cross-domain Natural Language Generation", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.6552576899528504}]}], "abstractContent": [{"text": "Cross-domain natural language generation (NLG) is still a difficult task within spoken dialogue modelling.", "labels": [], "entities": [{"text": "Cross-domain natural language generation (NLG)", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7759647199085781}, {"text": "spoken dialogue modelling", "start_pos": 80, "end_pos": 105, "type": "TASK", "confidence": 0.713870902856191}]}, {"text": "Given a semantic representation provided by the dialogue manager , the language generator should generate sentences that convey desired information.", "labels": [], "entities": []}, {"text": "Traditional template-based generators can produce sentences with all necessary information, but these sentences are not sufficiently diverse.", "labels": [], "entities": []}, {"text": "With RNN-based models, the diversity of the generated sentences can be high, however, in the process some information is lost.", "labels": [], "entities": []}, {"text": "In this work, we improve an RNN-based generator by considering latent information at the sentence level during generation using the conditional variational autoencoder architecture.", "labels": [], "entities": []}, {"text": "We demonstrate that our model outperforms the original RNN-based generator , while yielding highly diverse sentences.", "labels": [], "entities": []}, {"text": "In addition, our model performs better when the training data is limited.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conventional spoken dialogue systems (SDS) require a substantial amount of hand-crafted rules to achieve good interaction with users.", "labels": [], "entities": []}, {"text": "The large amount of required engineering limits the scalability of these systems to settings with new or multiple domains.", "labels": [], "entities": []}, {"text": "Recently, statistical approaches have been studied that allow natural, efficient and more diverse interaction with users without depending on pre-defined rules ().", "labels": [], "entities": []}, {"text": "Natural language generation (NLG) is an essential component of an SDS.", "labels": [], "entities": [{"text": "Natural language generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7634535729885101}, {"text": "SDS", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9691748023033142}]}, {"text": "Given a semantic representation (SR) consisting of a dialogue act and a set of slot-value pairs, the generator should produce natural language containing the desired information.", "labels": [], "entities": []}, {"text": "Traditionally NLG was based on templates, which produce grammatically-correct sentences that contain all desired information.", "labels": [], "entities": []}, {"text": "However, the lack of variation of these sentences made these systems seem tedious and monotonic.", "labels": [], "entities": []}, {"text": "Trainable generators) can generate several sentences for the same SR, but the dependence on pre-defined operations limits their potential.", "labels": [], "entities": []}, {"text": "Corpus-based approaches) learn to generate natural language directly from data without pre-defined rules.", "labels": [], "entities": []}, {"text": "However, they usually require alignment between the sentence and the SR.", "labels": [], "entities": []}, {"text": "Recently, proposed an RNN-based approach, which outperformed previous methods on several metrics.", "labels": [], "entities": []}, {"text": "However, the generated sentences often did not include all desired attributes.", "labels": [], "entities": []}, {"text": "The variational autoencoder) enabled for the first time the generation of complicated, high-dimensional data such as images.", "labels": [], "entities": []}, {"text": "The conditional variational autoencoder (CVAE) (, firstly proposed for image generation, has a similar structure to the VAE with an additional dependency on a condition.", "labels": [], "entities": [{"text": "image generation", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.7813188135623932}]}, {"text": "Recently, the CVAE has been applied to dialogue systems) using the previous dialogue turns as the condition.", "labels": [], "entities": []}, {"text": "However, their output was not required to contain specific information.", "labels": [], "entities": []}, {"text": "In this paper, we improve RNN-based generators by adapting the CVAE to the difficult task of cross-domain NLG.", "labels": [], "entities": [{"text": "RNN-based generators", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.8258593380451202}]}, {"text": "Due to the additional latent information encoded by the CVAE, our model outperformed the SCLSTM at conveying all information.", "labels": [], "entities": []}, {"text": "Furthermore, our model reaches better results when the training data is limited.", "labels": [], "entities": []}], "datasetContent": [{"text": "The proposed model is used for an SDS that provides information about restaurants, hotels, televisions and laptops.", "labels": [], "entities": []}, {"text": "It is trained on a dataset, which consists of sentences with corresponding semantic representations.", "labels": [], "entities": []}, {"text": "shows statistics about the corpus which was split into a training, validation and testing set according to a 3:1:1 split.", "labels": [], "entities": []}, {"text": "The dataset contains 14 different system dialogue acts.", "labels": [], "entities": []}, {"text": "The television and laptop domains are much more complex than other domains.", "labels": [], "entities": []}, {"text": "There are around 7k and 13k different SRs possible for the TV and the laptop domain respectively.", "labels": [], "entities": [{"text": "SRs", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9424019455909729}]}, {"text": "For the restaurant and hotel domains only 248 and 164 unique SRs are possible.", "labels": [], "entities": [{"text": "SRs", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9447843432426453}]}, {"text": "This imbalance makes the NLG task more difficult.", "labels": [], "entities": [{"text": "NLG task", "start_pos": 25, "end_pos": 33, "type": "TASK", "confidence": 0.7850959599018097}]}, {"text": "The generators were implemented using the PyTorch Library (.", "labels": [], "entities": [{"text": "PyTorch Library", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.9306404888629913}]}, {"text": "The size of decoder SCLSTM and thus of the latent variable was set to 128.", "labels": [], "entities": []}, {"text": "KL-annealing was used, with the weight of the KL-loss reaching 1 after 5k minibatch updates.", "labels": [], "entities": [{"text": "KL-annealing", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.6637469530105591}]}, {"text": "The slot error rate (ERR), used in, is the metric that measures the model's ability to convey the desired information.", "labels": [], "entities": [{"text": "slot error rate (ERR)", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.9279906650384268}]}, {"text": "ERR is defined as: (p + q)/N , where N is the number of slots in the SR, p and q are the number of missing and redundant slots in the generated sentence.", "labels": [], "entities": []}, {"text": "The BLEU-4 metric and perplexity (PPL) are also reported.", "labels": [], "entities": [{"text": "BLEU-4 metric", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9546953439712524}, {"text": "perplexity (PPL)", "start_pos": 22, "end_pos": 38, "type": "METRIC", "confidence": 0.9252921640872955}]}, {"text": "The baseline SCLSTM is optimized, which has shown to outperform template-based methods and trainable generators).", "labels": [], "entities": []}, {"text": "NLG often uses the over-generation and reranking paradigm).", "labels": [], "entities": [{"text": "NLG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9300684332847595}]}, {"text": "The SCVAE can generate multiple sentences by sampling multiple z, while the SCLSTM has to sample different words from the output distribution.In our experiments ten sentences are generated per SR.", "labels": [], "entities": []}, {"text": "in the appendix shows one SR in each domain with five illustrative sentences generated by our model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The statistics of the cross-domain dataset", "labels": [], "entities": []}, {"text": " Table 2: Comparison between SCVAE and  SCLSTM. Both are trained with full dataset and  tested on individual domains", "labels": [], "entities": []}, {"text": " Table 3: Comparison between SCVAE and  SCLSTM in K-shot learning", "labels": [], "entities": []}]}