{"title": [{"text": "Underspecified Universal Dependency Structures as Inputs for Multilingual Surface Realisation", "labels": [], "entities": [{"text": "Multilingual Surface Realisation", "start_pos": 61, "end_pos": 93, "type": "TASK", "confidence": 0.645376076300939}]}], "abstractContent": [], "introductionContent": [{"text": "There has long been an assumption in Natural Language Generation (NLG) that surface realisation can be treated as an independent subtask for which stand-alone, plug-and-play tools can, and should, be created.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.8021276891231537}, {"text": "surface realisation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.738932341337204}]}, {"text": "Early surface realisers such as KPML ( and FUF/Surge were ambitious, independent surface realisation tools for English with wide grammatical coverage.", "labels": [], "entities": [{"text": "FUF/Surge", "start_pos": 43, "end_pos": 52, "type": "DATASET", "confidence": 0.8743355671564738}]}, {"text": "However, the question of how the NLG components addressing the stage before surface realisation were supposed to put together inputs of the level of grammatical sophistication required by such tools was never quite resolved.", "labels": [], "entities": []}, {"text": "The success of SimpleNLG () which had much reduced grammatical coverage, but accepted radically simpler inputs demonstrated the importance of this issue.", "labels": [], "entities": []}, {"text": "The recently completed first Multilingual Surface Realisation Task (SR'18) ( used for the first time inputs derived from the Universal Dependencies (UDs)), a framework which was devised with the aim of facilitating cross-linguistically consistent grammatical annotation, and which has grown into a large-scale community effort involving more than 200 contributors, who have created over 100 treebanks in over 70 languages between them.", "labels": [], "entities": [{"text": "Multilingual Surface Realisation Task (SR'18)", "start_pos": 29, "end_pos": 74, "type": "TASK", "confidence": 0.7777475076062339}, {"text": "cross-linguistically consistent grammatical annotation", "start_pos": 215, "end_pos": 269, "type": "TASK", "confidence": 0.6136103942990303}]}, {"text": "UDs provide a more general and potentially flexible input representation for surface realisation (SR).", "labels": [], "entities": [{"text": "surface realisation (SR)", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.8519228339195252}]}, {"text": "However, their use for NLG has not so far been demonstrated.", "labels": [], "entities": [{"text": "NLG", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.8103241920471191}]}, {"text": "In this paper, we present the UD datasets used in the Shallow and Deep Tracks in SR'18, describe the precise conversion processes that were applied to them, and provide an assessment of their quality.", "labels": [], "entities": [{"text": "UD datasets", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.7191343009471893}, {"text": "Shallow and Deep Tracks in SR'18", "start_pos": 54, "end_pos": 86, "type": "DATASET", "confidence": 0.8759359916051229}]}, {"text": "Furthermore, we examine (a) the SR task in general, (b) the motivation for, and likely usefulness of, the derivation of NLG inputs from annotations in resources developed for Natural Language Understanding (NLU), (c) whether the resulting inputs supply enough information of the right kind for the final stage in the NLG process, and more tentatively, (d) what role SR is likely to play in the future in the NLG context.", "labels": [], "entities": [{"text": "SR task", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9020160138607025}, {"text": "Natural Language Understanding (NLU)", "start_pos": 175, "end_pos": 211, "type": "TASK", "confidence": 0.718295951684316}]}, {"text": "Section 2 presents related work; Section 3 describes the datasets used in the two SR'18 tracks, and Section 4 provides a more procedural account of how the datasets were generated.", "labels": [], "entities": [{"text": "SR'18 tracks", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.9134387373924255}]}, {"text": "Section 5 assesses the quality of the obtained representations, while Section 6 discusses their suitability for SR and NLG more generally.", "labels": [], "entities": [{"text": "SR", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.9496200084686279}, {"text": "NLG", "start_pos": 119, "end_pos": 122, "type": "DATASET", "confidence": 0.7669169902801514}]}, {"text": "Some conclusions are presented in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following on from the more declarative description of Shallow and Deep inputs in the previous section, this section describes how those input were automatically created from the CoNLL'17 data.", "labels": [], "entities": [{"text": "CoNLL'17 data", "start_pos": 178, "end_pos": 191, "type": "DATASET", "confidence": 0.953654021024704}]}, {"text": "Since the processing applied to the Shallow inputs consists only in removing information and is very straightforward.", "labels": [], "entities": []}, {"text": "It does not call for an evaluation.", "labels": [], "entities": []}, {"text": "For the Deep Track, however, the changes are much more complex and the quality of the conversion needs to be assessed.", "labels": [], "entities": [{"text": "Deep Track", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.908647209405899}]}, {"text": "We evaluated the quality of the Deep inputs as follows.", "labels": [], "entities": []}, {"text": "One of the authors manually annotated about 900 deep tokens (\u224875 sentences) in each language (English, French and Spanish), by post-editing the automatically converted structures correcting any mistakes.", "labels": [], "entities": []}, {"text": "Since the same person post-edited all three datasets, the resulting goldstandard is consistent across the languages, even though it does not allow for calculating interannotator agreement.", "labels": [], "entities": [{"text": "goldstandard", "start_pos": 68, "end_pos": 80, "type": "METRIC", "confidence": 0.9736209511756897}]}, {"text": "Note that the annotation remains quite open with respect to some phenomena, for which several annotations are considered correct.", "labels": [], "entities": []}, {"text": "For instance, AM relations are left underspecified when it is not clear what argument slot is concerned (e.g., appositions, parentheticals, verbal/nominal adverbials, etc.); some argumental relations are ambiguous and left as such: N\u2192 ADJ is sometimes A1INV, and the adjective is sometimes an argument of the noun; numbers (e.g., ten thousand people) and hours are left as they are in the original annotations.", "labels": [], "entities": [{"text": "A1INV", "start_pos": 252, "end_pos": 257, "type": "METRIC", "confidence": 0.9763033390045166}]}, {"text": "Once post-edited, the reference structures are compared to the ones produced by the automatic mapping from UD structures, using the LAS evaluation method of, specifically designed to handle the comparison between non-isomorphic trees.", "labels": [], "entities": []}, {"text": "Since part of the mapping consists of adding and/or removing nodes, it often happens that the gold-standard and predicted structures end up with a different number of nodes, which makes evaluation scripts based on a strict node-to-node comparison unusable.", "labels": [], "entities": []}, {"text": "shows the results of the evaluation.", "labels": [], "entities": []}, {"text": "Quality is not the same across languages: while English structures obtain an LAS of 79.83, French is more than 6 points lower, and Spanish more than 12.", "labels": [], "entities": [{"text": "LAS", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9982544779777527}]}, {"text": "Since, as mentioned in the previous subsection, the mapping grammars are largely language-independent, and since roughly the same efforts have been dedicated to each language, it is likely that the LAS numbers reflect the quality of the original UD annotation.", "labels": [], "entities": []}, {"text": "Note that during the evaluation, POS and lemma-: Evaluation of the quality of the output structures (Labeled Attachment Scores -LAS).", "labels": [], "entities": [{"text": "POS", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.995122492313385}, {"text": "Labeled Attachment Scores -LAS)", "start_pos": 101, "end_pos": 132, "type": "METRIC", "confidence": 0.75786392390728}]}, {"text": "tisation 4 errors are not corrected, but structural errors due to original tagging/lemmatising errors are counted.", "labels": [], "entities": []}, {"text": "In other words, what is being evaluated is how correct the outputs are in terms of dependencies and labeling, rather than how well the transduction grammars perform.", "labels": [], "entities": []}, {"text": "An error analysis showed that most dependency errors come from the AM relation, which is usually A1, A2, A1INV or A2INV in the reference structures.", "labels": [], "entities": [{"text": "A1", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.9552088975906372}, {"text": "A1INV", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.8180155754089355}, {"text": "A2INV", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.8436603546142578}]}, {"text": "The systematic replacement of AM by one of these four labels always results in a drop of the LAS score.", "labels": [], "entities": [{"text": "AM", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9876656532287598}, {"text": "LAS score", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9848612844944}]}, {"text": "That is, in order to improve the quality of the structures, an improvement of the UD structures or a more fine-grained processing (which would imply a large number of rules and the use of detailed lexicons) would be needed.", "labels": [], "entities": []}, {"text": "The mapping grammars were released together with the SR'18 datasets; 5 they can process about 39 sentences per second on an average laptop.", "labels": [], "entities": [{"text": "SR'18 datasets", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9791729152202606}]}, {"text": "The resulting mapping tool allows for automatically annotating large amounts of data.", "labels": [], "entities": []}, {"text": "The tool has recently been used to convert about 600,000 English sentences that had been automatically parsed with an off-the-shelf UD parser.", "labels": [], "entities": []}, {"text": "This tool is also currently being tested as part of conceptual relation extraction pipelines in the framework of several EU projects (see Acknowledgements).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.6748898178339005}]}], "tableCaptions": [{"text": " Table 2: SR'18 dataset sizes for training, development and test sets.", "labels": [], "entities": [{"text": "SR'18 dataset", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.798461377620697}]}]}