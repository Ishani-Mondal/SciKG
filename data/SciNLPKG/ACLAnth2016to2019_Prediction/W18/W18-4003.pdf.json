{"title": [{"text": "Automatically Linking Lexical Resources with Word Sense Embedding Models", "labels": [], "entities": [{"text": "Automatically Linking Lexical Resources", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6710141450166702}]}], "abstractContent": [{"text": "Automatically learnt word sense embeddings are developed as an attempt to refine the capabilities of coarse word embeddings.", "labels": [], "entities": []}, {"text": "The word sense representations obtained this way are, however, sensitive to underlying corpora and parameterizations, and they might be difficult to relate to word senses as formally defined by linguists.", "labels": [], "entities": []}, {"text": "We propose to tackle this problem by devising a mechanism to establish links between word sense embeddings and lexical resources created by experts.", "labels": [], "entities": []}, {"text": "We evaluate the applicability of these links in a task to retrieve instances of Swedish word senses not present in the lexicon.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings have boosted performance in many Natural Language Processing applications in recent years.", "labels": [], "entities": []}, {"text": "By providing an effective way of representing the meaning of words, embeddings facilitate computations in models and pipelines that need to analyze semantic aspects of language.", "labels": [], "entities": []}, {"text": "Based on their success, an effort has been concentrated in improving embedding models, from devising more computationally effective models to extending them to cover other semantic units beyond words, such as multi-word expressions ( or word senses ().", "labels": [], "entities": []}, {"text": "Being able to represent word senses solves the problem of conflating several meanings of one polysemic word into a single embedding (.", "labels": [], "entities": []}, {"text": "Furthermore, having complete and accurate word sense representations brings embedding models closer to a range of existing, expert-curated resources such as lexica.", "labels": [], "entities": []}, {"text": "Bridging the gap between these two worlds arguably opens a road to new methods that could benefit well-established, widely used resources.", "labels": [], "entities": []}, {"text": "This is the focus of the work we present in this article.", "labels": [], "entities": []}, {"text": "We propose an automatic way of creating a mapping between entries in a lexicon and word sense representations learned from a corpus.", "labels": [], "entities": []}, {"text": "By having an identification between a manual inventory of word senses and word sense embeddings, the lexicon obtains capabilities by which its entries can be manipulated as mathematical objects (vectors), while the vector space model receives support from a linguistic resource.", "labels": [], "entities": []}, {"text": "Furthermore, by analyzing the disagreements between the lexicon and the embedding model, we can acquire insight into the shortcomings of their respective coverage.", "labels": [], "entities": []}, {"text": "For instance, unlinked lexicon entries evidence those instances that the vector model is unable to learn, while unlinked word sense embeddings may suggest new usages of words found in the corpora.", "labels": [], "entities": []}, {"text": "Being able to locate these cases opens the way towards improving lexica and embedding models.", "labels": [], "entities": []}, {"text": "Automatic discovery of novel senses has been shown as a feasible and productive endeavor ().", "labels": [], "entities": []}, {"text": "In our evaluation we provide some insight into these situations: we use a mapping to calculate the probability that a word in a sentence from a corpus is an instance of an unlisted sense (i.e., not present in the lexicon.)", "labels": [], "entities": []}, {"text": "This mapping process, and some of its potential applications, are explained in detail in the following sections.", "labels": [], "entities": []}, {"text": "Section 2 contains a description of the mapping mechanism Section 3 goes onto evaluate the performance of this mapping on finding instances of unlisted senses.", "labels": [], "entities": []}, {"text": "We present our conclusions in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation, we annotated a benchmark dataset.", "labels": [], "entities": []}, {"text": "We selected five target lemmas for which we knew that the corpus contains occurrences of word senses that are unlisted in the lexicon.", "labels": [], "entities": []}, {"text": "In addition, we selected four target lemmas that do not strictly have new word senses, but that tend to be confused with tokenization artifacts, named entities, or foreign words.", "labels": [], "entities": []}, {"text": "shows the selected lemma, an overview of the senses that are listed in the lexicon, as well as the most important unlisted senses.", "labels": [], "entities": []}, {"text": "For each of these nine target lemmas, we selected 1,000 occurrences randomly from the corpus.", "labels": [], "entities": []}, {"text": "Two annotators went through the selected occurrences and determined which of them are instances of the senses present in the lexicon, and which of them are unlisted senses.", "labels": [], "entities": []}, {"text": "A small number of occurrences were discarded because they were difficult for the annotators to understand.: Selected target lemmas.", "labels": [], "entities": []}, {"text": "Given a mapping between lexicographic word senses and automatically discovered senses in a corpus, sentences from the benchmark dataset can be scored by their probability of containing an out-of-lexicon instance.", "labels": [], "entities": []}, {"text": "A score is calculated using the linking probability between lexicographic sense y j and vector model sense xi , P (y j |x i ), and the probability of the AdaGram sense xi in the context of the sentence ctx, P (x i |ctx): thus obtaining the probability of a particular lexicographic sense y j given context ctx.", "labels": [], "entities": []}, {"text": "The sum of all p(y j |ctx), j \u2208 [1, T ], yields the probability that an instance contains one of the listed word senses.", "labels": [], "entities": []}, {"text": "Our score, then, is calculated as the inverse probability: The human annotations of the sentences are interpreted as the gold standard, and the scores as our model's classification output that can be used to rank the sentences from most to least probably containing an outof-lexicon instance of a target word.", "labels": [], "entities": []}, {"text": "We evaluate this classification using the Area Under the Receiver Operating Characteristic Curve (AUC).", "labels": [], "entities": [{"text": "Receiver Operating Characteristic Curve (AUC)", "start_pos": 57, "end_pos": 102, "type": "METRIC", "confidence": 0.5405012198856899}]}, {"text": "For reference, recall that the expected AUC of a random ranking is 0.5.", "labels": [], "entities": [{"text": "AUC", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9968298077583313}]}, {"text": "The potential of the automated mapping between a lexicon and an embedding model to help retrieve instances of word senses unlisted in the lexicon gives us a certain measure of the quality of this mapping.", "labels": [], "entities": []}, {"text": "The recovery of instances of unlisted senses can only succeed if the mapping has successfully identified listed lexicon senses in the embedding model, leaving unlisted ones unlinked.", "labels": [], "entities": []}, {"text": "On the other hand, failure to recover instances of unlisted senses can expose weaknesses in the automated mapping.", "labels": [], "entities": []}, {"text": "(See Section 3.4 for an analysis of unsuccessful cases.)", "labels": [], "entities": []}], "tableCaptions": []}