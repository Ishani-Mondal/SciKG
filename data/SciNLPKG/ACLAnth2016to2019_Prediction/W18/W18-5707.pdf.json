{"title": [{"text": "Curriculum Learning Based on Reward Sparseness for Deep Reinforcement Learning of Task Completion Dialogue Management", "labels": [], "entities": [{"text": "Task Completion Dialogue Management", "start_pos": 82, "end_pos": 117, "type": "TASK", "confidence": 0.6946020200848579}]}], "abstractContent": [{"text": "Learning from sparse and delayed reward is a central issue in reinforcement learning.", "labels": [], "entities": [{"text": "reinforcement learning", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.828445553779602}]}, {"text": "In this paper, to tackle reward sparseness problem of task oriented dialogue management, we propose a curriculum based approach on the number of slots of user goals.", "labels": [], "entities": [{"text": "task oriented dialogue management", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.6823142841458321}]}, {"text": "This curriculum makes it possible to learn dialogue management for sets of user goals with large number of slots.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8105873763561249}]}, {"text": "We also propose a dialogue policy based on progressive neural networks whose modules with parameters are appended with previous parameters fixed as the curriculum proceeds, and this policy improves performances over the one with single set of parameters .", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning in environments that give agents sparse and delayed reward is still a central research issue in reinforcement learning, while there are remarkable successes of deep reinforcement learning methods.", "labels": [], "entities": [{"text": "reinforcement learning", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.9146715104579926}]}, {"text": "The problem on sparse and delayed reward appears in reinforcement learning for task oriented dialogue agents.", "labels": [], "entities": []}, {"text": "Contrary to single turn interactions such as chit-chat or question answering, task oriented dialogue agents often are required to retrieve information from external knowledge bases and to learn the way how the agent reasons with progression of dialogue tasks over multiple dialog turns.", "labels": [], "entities": [{"text": "question answering", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.789814680814743}]}, {"text": "This long term process, however, makes it difficult for Markov Decision Process to identify the part of an action sequence that affects progress of dialogue tasks over multiple turns.", "labels": [], "entities": []}, {"text": "Thus, typical agents must decide from a positive reward, which is obtained from successful task completion, only at the last turn.", "labels": [], "entities": []}, {"text": "It is inevitable for practical scalability to use sparse reward functions, because designing complicated and dense reward criteria over multiple turns involves domain knowledge and human annotators to evaluate dialogue history of large size.", "labels": [], "entities": []}, {"text": "In particular, our aim is to train dialogue policy agents that cannot obtain positive rewards until the last turn.", "labels": [], "entities": []}, {"text": "While general and scalable frameworks of task completion dialogue management have been proposed recently, these frameworks still have had reward sparseness problem.", "labels": [], "entities": [{"text": "task completion dialogue management", "start_pos": 41, "end_pos": 76, "type": "TASK", "confidence": 0.6843609288334846}]}, {"text": "proposed a general neural dialogue framework which has scalability and features to solve information retrieval tasks (TC-Bot), which extended a previous work on information retrieval dialogue system (called KB-Info-Bot) to access external knowledge base ().", "labels": [], "entities": [{"text": "information retrieval tasks", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.7799652814865112}]}, {"text": "While they firstly proposed a robust end-to-end modularized neural dialogue system with separated and independently trainable modules, which are natural language understanding, dialogue management, and natural language generation, difficulty in reinforcement learning with sparse rewards still remains for their learning method with deep Q-networks.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 145, "end_pos": 175, "type": "TASK", "confidence": 0.6654080053170522}, {"text": "dialogue management", "start_pos": 177, "end_pos": 196, "type": "TASK", "confidence": 0.8238136768341064}, {"text": "natural language generation", "start_pos": 202, "end_pos": 229, "type": "TASK", "confidence": 0.6428607205549876}]}, {"text": "In this paper, we propose curriculum learning based on reward sparseness of user goals, and agents using progressive neural networks () to improve the curriculum learning.", "labels": [], "entities": []}, {"text": "First, our curriculum learning makes it possible to learn sets of user goals with large number of slots for which TCBot failed to learn.", "labels": [], "entities": []}, {"text": "As the simulation epoch increases, the minimum number of slots that user goals contain increases.(See an overview in For example, the minimum number of slots is two for the first 200 simulation epochs, and is four for the next 200 ones, and agents are finally trained with user goals that contains at least 10 slots.", "labels": [], "entities": []}, {"text": "In other words, the more simulation epoch proceeds, the more sparse reward is obtained from environments.", "labels": [], "entities": []}, {"text": "There are two practical advantages of this curriculum: (1) our curriculum is domain free and (2) curriculum data preparation is easy because our curriculum only depends on the number of slots of user goals.", "labels": [], "entities": [{"text": "curriculum data preparation", "start_pos": 97, "end_pos": 124, "type": "TASK", "confidence": 0.6235396564006805}]}, {"text": "Second, the proposed application of progressive neural networks improves knowledge transfer from models trained for easier curriculum data to models trained for harder one.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7118810713291168}]}, {"text": "Progressive neural networks have multiple columns with weight parameters.", "labels": [], "entities": []}, {"text": "At first a progressive neural network has single column to be trained, then another column is appended with new parameter set.", "labels": [], "entities": []}, {"text": "All parameters of previous columns are frozen when appended column is training, and the appended columns can exploit information from frozen columns.", "labels": [], "entities": []}, {"text": "Our aim is to apply this progressive freezing mechanism to exploit information of the parameters that are trained with easier user goals of our curriculum, when the latest appended column is in training with harder user goals.", "labels": [], "entities": []}, {"text": "This progressive exploitation is expected to overcome the difficulty in the setting that agents start reinforcement learning with the hardest user goals.", "labels": [], "entities": []}], "datasetContent": [{"text": "Reinforcement Learning Environments and Data Set The curriculum data of user goals for the experiments was created from the movie searching data set used in ( . The same reinforcement learning environment and user simulator in ( ) was used for the experiments.", "labels": [], "entities": []}, {"text": "User Simulator In our experiments, user simulators try to let dialogue agents fill slots which users have informed.", "labels": [], "entities": [{"text": "User Simulator", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6861764937639236}]}, {"text": "The simulators also inform values of slots which users have requested as constraints to retrieve values from a database.", "labels": [], "entities": []}, {"text": "If the simulators have a slot type which they have not informed yet, they also inform its value.", "labels": [], "entities": []}, {"text": "The simulators inform the value I don't care if agents have requested values of a slot type which is not contained in inform slots of user goals.", "labels": [], "entities": []}, {"text": "For example, the simulators with the user goal showed in Table 4 send the message I don't care, when agents have requested the value of slot type price, because price slot type is not contained in inform slots in.", "labels": [], "entities": []}, {"text": "Setup of Experiments RMSprop was used as the optimizer.", "labels": [], "entities": []}, {"text": "The hyper parameters of the optimizer were set to the following values: the learning rate, the decay rate and the momentum were, 0.001, 0.999, and 0.1, respectively.", "labels": [], "entities": []}, {"text": "With the way similar to ( , the error control model that has two kinds of errors: slot level and intent level was used.", "labels": [], "entities": [{"text": "slot level", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9671521782875061}]}, {"text": "In the experiment, slot level and intent level correspond to the case where the slot name is correctly recognized but the slot value is wrong and the case where a dialogue act itself is wrongfully recognized, respectively.", "labels": [], "entities": [{"text": "slot level", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9589524567127228}]}, {"text": "For each simulation epoch there are 100 episodes of dialogue between users and agents.", "labels": [], "entities": []}, {"text": "In each episode of dialogue, a user can send an utterance at most a half of T max times and an agent can perform in the same way.", "labels": [], "entities": []}, {"text": "Results The success rate (moving average with window of size 7) of each simulation epoch is shown in.", "labels": [], "entities": []}, {"text": "This figure shows that progressive neural network can make learning faster, noting that agents were trained for the set D in for simulation epoch more than 600.", "labels": [], "entities": []}, {"text": "The simulation in which agents are trained with only the user goal set D for all simulation epochs was also executed.", "labels": [], "entities": []}, {"text": "The success rates for this hardest simulation were 0.0 for all simulation epochs and for all of six error settings (omitted in).", "labels": [], "entities": []}, {"text": "In particular, for the success rates in 3b and 3c, pro- gressive neural networks improve the performance for all simulation epoch ranges in, while the success rates for progressive networks drop at switching epochs(200, 400, 600).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The minimum number of slots for each  user goal set. The three labels inf, req, all respec- tively correspond to the number of inform slots,  request slots, and all slots.", "labels": [], "entities": []}]}