{"title": [{"text": "Geocoding Without Geotags: A Text-based Approach for reddit", "labels": [], "entities": [{"text": "reddit", "start_pos": 53, "end_pos": 59, "type": "TASK", "confidence": 0.5500758290290833}]}], "abstractContent": [{"text": "In this paper, we introduce the first geolocation inference approach for reddit, asocial media platform where user pseudonymity has thus far made supervised demographic inference difficult to implement and validate.", "labels": [], "entities": []}, {"text": "In particular , we design a text-based heuristic schema to generate ground truth location labels for red-dit users in the absence of explicitly geotagged data.", "labels": [], "entities": []}, {"text": "After evaluating the accuracy of our labeling procedure, we train and test several ge-olocation inference models across our reddit data set and three benchmark Twitter geoloca-tion data sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9981628060340881}, {"text": "reddit data set", "start_pos": 124, "end_pos": 139, "type": "DATASET", "confidence": 0.8210789362589518}, {"text": "Twitter geoloca-tion data sets", "start_pos": 160, "end_pos": 190, "type": "DATASET", "confidence": 0.7655127197504044}]}, {"text": "Ultimately, we show that ge-olocation models trained and applied on the same domain substantially outperform models attempting to transfer training data across domains , even more soon reddit where platform-specific interest-group metadata can be used to improve inferences.", "labels": [], "entities": []}], "introductionContent": [{"text": "The rise of social media over the past decade has brought with it the capability to positively influence and deeply understand demographic groups at a scale unachievable within a controlled lab environment.", "labels": [], "entities": []}, {"text": "For example, despite only having access to sparse demographic metadata, social media researchers have successfully engineered systems to promote targeted responses to public health issues ( and to characterize complex human behaviors.", "labels": [], "entities": []}, {"text": "However, recent studies have demonstrated that social media data sets often contain strong population biases, especially those which are filtered down to users who have opted to share sensitive attributes such as name, age, and location.", "labels": [], "entities": []}, {"text": "These existing biases are likely to be compounded by new data privacy legislation that will require more thorough informed consent processes (.", "labels": [], "entities": []}, {"text": "While some social platforms have previously approached the challenge of balancing data access and privacy by offering users the ability to share and explicitly control public access to sensitive attributes, others have opted not to collect sensitive attribute data altogether.", "labels": [], "entities": []}, {"text": "The social news website reddit is perhaps the largest platform in the latter group; as of January 2018, it was the 5th most visited website in the United States and 6th most visited website globally.", "labels": [], "entities": []}, {"text": "Unlike real-name social media platforms such as Facebook, reddit operates as a pseudonymous website, with the only requirement for participation being a screen name.", "labels": [], "entities": []}, {"text": "Fortunately, there has been significant progress made using statistical models to infer user demographics based on text and other features when self-attribution data is sparse ().", "labels": [], "entities": []}, {"text": "On reddit in particular, has used self-attributed \"flair\" as labels for training a text-based gender inference model.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, user geolocation inference has not yet been attempted on reddit, where a complete lack of location-based features (e.g. geotags, profiles with a location field) has made it difficult to train and validate a supervised model.", "labels": [], "entities": [{"text": "user geolocation inference", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.6397951940695444}]}, {"text": "The ability to geolocate users on reddit has substantial implications for conversation mining and high-level property modeling, especially since pseudonymity tends to encourage disinhibition.", "labels": [], "entities": [{"text": "conversation mining", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7969175279140472}, {"text": "high-level property modeling", "start_pos": 98, "end_pos": 126, "type": "TASK", "confidence": 0.7388827999432882}]}, {"text": "For instance, geolocation could be used to segment users discussing a movie trailer into US and international audiences to estimate a film's global appeal or to inform advertising strategy within different global markets.", "labels": [], "entities": []}, {"text": "Alternatively, user geolocation maybe used in conjunction with sentiment analysis of political discussions to predict future voting outcomes.", "labels": [], "entities": [{"text": "sentiment analysis of political discussions", "start_pos": 63, "end_pos": 106, "type": "TASK", "confidence": 0.8176920056343079}]}, {"text": "Moving toward these goals, we introduce a textbased heuristic schema to generate ground truth location labels for reddit users in the absence of explicitly geotagged data.", "labels": [], "entities": []}, {"text": "After evaluating the accuracy of our labeling procedure, we train and test several geolocation inference models across our reddit data set and three benchmark Twitter geolocation data sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.998132050037384}, {"text": "reddit data set", "start_pos": 123, "end_pos": 138, "type": "DATASET", "confidence": 0.8329446315765381}, {"text": "Twitter geolocation data sets", "start_pos": 159, "end_pos": 188, "type": "DATASET", "confidence": 0.743161603808403}]}, {"text": "Ultimately, we show that geolocation models trained and applied on the same domain substantially outperform models attempting to transfer training data across domains, even more soon reddit where platform-specific interestgroup metadata can be used to improve inferences.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first examine geolocation inference performance within the domain of our reddit data set.", "labels": [], "entities": [{"text": "geolocation inference", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.8464683592319489}, {"text": "reddit data set", "start_pos": 76, "end_pos": 91, "type": "DATASET", "confidence": 0.7839116354783376}]}, {"text": "We query all comment data for users within our set of geolocation labels using a publicly available corpus of reddit comments made between December).", "labels": [], "entities": []}, {"text": "For each user, we keep a maximum of 1000 comments posted up to one-month after they commented in the set of submissions used for labeling; this datebased filtering is done to mitigate the effect of users moving after posting in the seed set of submissions.", "labels": [], "entities": []}, {"text": "Additionally, to ensure our model is not overtly biased by toponym mentions, we remove comments that were used as apart of the labeling procedure.", "labels": [], "entities": []}, {"text": "We separate our reddit data set into two versions-US (restricted to users from the contiguous United States) and GLOBAL (no location restrictions).", "labels": [], "entities": [{"text": "reddit data set", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.7909398277600607}, {"text": "GLOBAL", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9087881445884705}]}, {"text": "We require that users within the United States have a minimum city-level resolution, while users outside the United States have been labeled with at least a state-level resolution.", "labels": [], "entities": []}, {"text": "We quantify model performance using three standard metrics from the user geolocation literature: Average Error Distance (AED), Median Error Distance (MED), and Accuracy at 100 miles (Acc@100).", "labels": [], "entities": [{"text": "Average Error Distance (AED)", "start_pos": 97, "end_pos": 125, "type": "METRIC", "confidence": 0.9676757653554281}, {"text": "Median Error Distance (MED)", "start_pos": 127, "end_pos": 154, "type": "METRIC", "confidence": 0.90831591685613}, {"text": "Accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9994121789932251}, {"text": "Acc", "start_pos": 183, "end_pos": 186, "type": "METRIC", "confidence": 0.9890734553337097}]}, {"text": "AED and MED are simply the arithmetic mean and median of error between predicted coordinates and true coordinates, respectively.", "labels": [], "entities": [{"text": "AED", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9772292375564575}, {"text": "MED", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9711361527442932}, {"text": "arithmetic mean and median of error", "start_pos": 27, "end_pos": 62, "type": "METRIC", "confidence": 0.7694922586282095}]}, {"text": "Acc@100 is the percentage of users whose predicted location is less than 100 miles from their true location.", "labels": [], "entities": [{"text": "Acc", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.985486626625061}]}, {"text": "While within-domain experiments suggest that reddit-specific metadata offers substantial predictive value, we wish to compare the highest degree of performance achieved within-domain to performance achieved using models trained outside the reddit domain.", "labels": [], "entities": []}, {"text": "To do so, we use three benchmark Twitter geolocation data sets-GEOTEXT, TWITTER-US (, and TWITTER-WORLD (.", "labels": [], "entities": [{"text": "TWITTER-US", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.7190845012664795}]}, {"text": "All three data sets were created by monitoring Twitter's streaming API over a discrete period of time and caching comments for users who enabled geotagging features on the Twitter platform.", "labels": [], "entities": []}, {"text": "Due to Twitter's terms of service, TWITTER-US and TWITTER-WORLD must be compiled from scratch using tweet IDs.", "labels": [], "entities": []}, {"text": "Unfortunately, several users within the original data sets have since either deleted their accounts or restricted access to their tweet history.", "labels": [], "entities": []}, {"text": "As such, we were notable to perfectly recreate the original data sets.", "labels": [], "entities": []}, {"text": "Ultimately, our compilation of TWITTER-US contains 246k out of the original 440k users, while TWITTER-WORLD contains 888k out of the original 1.4M users.", "labels": [], "entities": [{"text": "TWITTER-US", "start_pos": 31, "end_pos": 41, "type": "DATASET", "confidence": 0.8237351179122925}, {"text": "TWITTER-WORLD", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.8671117424964905}]}, {"text": "To understand the impact that domain transfer has on geolocation inference performance, we setup a systematic model comparison.", "labels": [], "entities": []}, {"text": "First, we run 5-fold cross-validation within each of the Twitter data sets using both word and timestamp features.", "labels": [], "entities": [{"text": "Twitter data sets", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.9033963481585184}]}, {"text": "For the TWITTER-WORLD data set, we run two independent cross-validation procedures, evaluating on the subset of US users alone and also on the entire data set.", "labels": [], "entities": [{"text": "TWITTER-WORLD data set", "start_pos": 8, "end_pos": 30, "type": "DATASET", "confidence": 0.894666850566864}]}, {"text": "Then, we train models for each of our data sets using all available data and apply them to the data sets not used for training.", "labels": [], "entities": []}, {"text": "When evaluating performance on a data set that only contains US users, we train the corresponding model only using US user data.", "labels": [], "entities": []}, {"text": "All model hyper-parameters (e.g. feature set sizes, regularization, etc.) were chosen to optimize within-data-set performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The figure (left) shows the geographic distribution of labeled users. Users outside the Americas are  generally more likely to self-identify using a state-level resolution and above. The table (right) compares relative reddit traffic  estimated using the proprietary Alexa (2018) panel to the distribution within our labeled data set.", "labels": [], "entities": [{"text": "Alexa (2018) panel", "start_pos": 277, "end_pos": 295, "type": "DATASET", "confidence": 0.8947624206542969}]}, {"text": " Table 3: Summary statistics for the domain-transfer experiment. The best results from our cross-validation procedure are", "labels": [], "entities": []}]}