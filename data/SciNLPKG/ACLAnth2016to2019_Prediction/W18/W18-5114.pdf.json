{"title": [{"text": "Boosting Text Classification Performance on Sexist Tweets by Text Augmentation and Text Generation Using a Combination of Knowledge Graphs", "labels": [], "entities": [{"text": "Boosting Text Classification", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6517422497272491}, {"text": "Text Augmentation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7202470749616623}, {"text": "Text Generation", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.7064803391695023}]}], "abstractContent": [{"text": "Text classification models have been heavily utilized fora slew of interesting natural language processing problems.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7589406967163086}]}, {"text": "Like any other machine learning model, these classifiers are very dependent on the size and quality of the training dataset.", "labels": [], "entities": []}, {"text": "Insufficient and imbalanced datasets will lead to poor performance.", "labels": [], "entities": []}, {"text": "An interesting solution to poor datasets is to take advantage of the world knowledge in the form of knowledge graphs to improve our training data.", "labels": [], "entities": []}, {"text": "In this paper, we use ConceptNet and Wikidata to improve sexist tweet classification by two methods (1) text augmentation and (2) text generation.", "labels": [], "entities": [{"text": "sexist tweet classification", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.65413765112559}, {"text": "text generation", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.7341810166835785}]}, {"text": "In our text generation approach, we generate new tweets by replacing words using data acquired from ConceptNet relations in order to increase the size of our training set, this method is very helpful with frustratingly small datasets, preserves the label and increases diversity.", "labels": [], "entities": [{"text": "text generation", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.7147715091705322}]}, {"text": "In our text augmentation approach, the number of tweets remains the same but their words are augmented (concatenation) with words extracted from their ConceptNet relations and their description extracted from Wikidata.", "labels": [], "entities": []}, {"text": "In our text augmentation approach, the number of tweets in each class remains the same but the range of each tweet increases.", "labels": [], "entities": []}, {"text": "Our experiments show that our approach improves sexist tweet classification significantly in our entire machine learning models.", "labels": [], "entities": [{"text": "sexist tweet classification", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.63407963514328}]}, {"text": "Our approach can be readily applied to any other small dataset size like hate speech or abusive language and text classification problem using any machine learning model.", "labels": [], "entities": [{"text": "text classification problem", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.7773750225702921}]}], "introductionContent": [{"text": "When it comes to machine learning algorithms, the dataset plays a pivotal role in the usability of those models.", "labels": [], "entities": []}, {"text": "There are many problems where datasets are imbalanced, data is rare or data is hard to collect, hard to label or the overlap between the classes is high.", "labels": [], "entities": []}, {"text": "One of the methods which handles these shortcomings in text classification is text generation.", "labels": [], "entities": [{"text": "text classification", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7756209671497345}, {"text": "text generation", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.8105954527854919}]}, {"text": "Text generation has been used widely for machine translation, summarization and dialogue generation and.", "labels": [], "entities": [{"text": "Text generation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7694423198699951}, {"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8356135189533234}, {"text": "summarization", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.98698890209198}, {"text": "dialogue generation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8509919941425323}]}, {"text": "In addition, sentences contain different keywords and concepts.", "labels": [], "entities": []}, {"text": "One way of understanding these concepts and getting more information about them is by using linked data and knowledge graphs.", "labels": [], "entities": []}, {"text": "The popularity of the internet and advancements in linked data research has led to the development of internetscale public domain knowledge graphs such as FreeBase, DBPedia, ConceptNet and Wikidata.", "labels": [], "entities": [{"text": "FreeBase", "start_pos": 155, "end_pos": 163, "type": "DATASET", "confidence": 0.9516566395759583}]}, {"text": "Knowledge in popular knowledge graphs is usually mined from available online resources such as Wikipedia using natural language understanding techniques or harvested by crowd sourcing or a combination of both.", "labels": [], "entities": []}, {"text": "Knowledge graphs are used to represent concepts and their relationships in a computer understandable format.", "labels": [], "entities": []}, {"text": "They have a wide range of application in the text analysis domain such as question answering query expansion (Yao,), recommendation engines (Voorhees, and many more.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.7427411079406738}, {"text": "question answering query expansion", "start_pos": 74, "end_pos": 108, "type": "TASK", "confidence": 0.8693011403083801}]}, {"text": "ConceptNet is a commonsense knowledge graph that represents approximately 21 million everyday concepts and their relationships using one of the 36 existing relationships such as IsA (e.g. jack IsA first name), UsedFor (e.g. car UsedFor driving ) or PartOf (wheel PartOf car) (Dzmitry.", "labels": [], "entities": []}, {"text": "Each fact in ConceptNet has a weight value which shows the degree of truthiness.", "labels": [], "entities": []}, {"text": "Higher values show more confidence.", "labels": [], "entities": [{"text": "confidence", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9829258918762207}]}, {"text": "In other words, it shows the closeness of the concpts to each other.", "labels": [], "entities": []}, {"text": "Wikidata is a wiki project that is used to crowd source structured data which is consumable both by humans and machines.", "labels": [], "entities": [{"text": "Wikidata", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8617593050003052}]}, {"text": "Wikidata contains 4400 types of relationships between more than 45 million concepts.", "labels": [], "entities": []}, {"text": "ConceptNet and Wikidata are far from perfectly consistent and complete.", "labels": [], "entities": [{"text": "Wikidata", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.878874659538269}]}, {"text": "Therefore, we use both of these knowledge graphs in our approach for better coverage of word knowledge with more consistency.", "labels": [], "entities": []}, {"text": "An interesting source of information in Wikidata is concept descriptions.", "labels": [], "entities": []}, {"text": "We use these descriptions for augmenting tweets.", "labels": [], "entities": []}, {"text": "For the text generation task, we replace words in each tweet by words that they are connected to in ConceptNet using some of its 19 relations such as IsA, RelatedTo, HasA, HasProperty, etc.", "labels": [], "entities": [{"text": "text generation", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.7411121279001236}]}, {"text": "Another approach for improving the classification is text augmentation, adding more information or enriching the text semantically for the purpose of achieving better classification results.", "labels": [], "entities": [{"text": "text augmentation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7630163729190826}]}, {"text": "Text augmentation has been widely used in bioinformatics, image processing, computer vision, video and audio processing (Bj\u00f6rn and (X.).", "labels": [], "entities": [{"text": "Text augmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7312311828136444}]}, {"text": "Even though the most prevalent applications of text augmentation are in the fields of vision or audio, we believe that introducing simple but effective ideas can be useful for text classification tasks.", "labels": [], "entities": [{"text": "text augmentation", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7720305025577545}, {"text": "text classification tasks", "start_pos": 176, "end_pos": 201, "type": "TASK", "confidence": 0.8612280289332072}]}, {"text": "In addition, they can help in reducing the scarcity of the data, avoiding overfitting due to lack of data and increasing the generality power of the algorithm.", "labels": [], "entities": []}, {"text": "Our contribution in this paper is using ConceptNet, Wikidata and a combination of both for text generation and augmentation in order to improve sexist tweet classification.", "labels": [], "entities": [{"text": "text generation", "start_pos": 91, "end_pos": 106, "type": "TASK", "confidence": 0.7573502063751221}, {"text": "tweet classification", "start_pos": 151, "end_pos": 171, "type": "TASK", "confidence": 0.6681616604328156}]}, {"text": "Even though we have used our approach for sexist tweet classification, it can be readily applied to other text classification problems using any of the existing text classification models.", "labels": [], "entities": [{"text": "sexist tweet classification", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6632753908634186}, {"text": "text classification", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.768047034740448}, {"text": "text classification", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.7270386070013046}]}, {"text": "It can also be beneficial for hateful speech and abusive dataset where the data is scarce.", "labels": [], "entities": []}, {"text": "The rest of the paper is as follows: In the next section, we will discuss the prior work on sexism, text generation and text augmentation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.8158312439918518}, {"text": "text augmentation", "start_pos": 120, "end_pos": 137, "type": "TASK", "confidence": 0.7925268113613129}]}, {"text": "Then, in the experiment part, we will go through the dataset, text preprocessing, classification algorithms, and the detailed method of text generation and text augmentation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 136, "end_pos": 151, "type": "TASK", "confidence": 0.7433919459581375}, {"text": "text augmentation", "start_pos": 156, "end_pos": 173, "type": "TASK", "confidence": 0.7404508888721466}]}, {"text": "In the results, we will show the result of text generation and text augmentation and finally the conclusion.", "labels": [], "entities": [{"text": "text generation", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7647985219955444}, {"text": "text augmentation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7110245078802109}]}], "datasetContent": [{"text": "Z. were the first who collected hateful tweets and categorized them into sexism, racism or neither.", "labels": [], "entities": []}, {"text": "Inspired by the study of McIntosh, Waseem categorized the tweets into being sexist or racist if they have any of the proposed 18 observations in the tweets such as the usage of any kind of slur to showing sexism racism, criticizing minorities and so forth.) solely focused on sexist tweets and proposed two categories of hostile and benevolent sexism.", "labels": [], "entities": []}, {"text": "However, these categories were very general and simply ignored other types of sexism happening in social media.", "labels": [], "entities": []}, {"text": "In one step further, Sharifirad S. and Matwin S 2018( S sharifirad and S Matwin, 2018), proposed complimentary categories of sexist language inspired from social science work.", "labels": [], "entities": []}, {"text": "They categorized sexist tweets into the categories of (1) indirect harassment, (2) information threat, (3) sexual harassment and (4) physical harassment.", "labels": [], "entities": []}, {"text": "shows the distribution of the dataset along with the sample of tweets in each category mentioned in Table2.", "labels": [], "entities": []}, {"text": "We made bigrams using the python NLTK package and changed them into vectors using word2vec.", "labels": [], "entities": []}, {"text": "For word2vec, we used the genism library trained using CBOW and concatenated the vectors of length 300 to get a vector for the each tweet.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.8927974700927734}]}, {"text": "We used multi-class Na\u00efve Bayes in Scikit learn python, multiclass LSTM and multiclass CNN using Keras for the choice of classifiers.", "labels": [], "entities": []}, {"text": "We divided the dataset into 70% train and 30% set.", "labels": [], "entities": []}, {"text": "For each tweet, we made the labels in the form of one-hot encoding of length four and we used the same labels for all the classification process.", "labels": [], "entities": []}, {"text": "We applied a CNN-based approach to automatically learn and classify sentences into one of the four categories.", "labels": [], "entities": []}, {"text": "During the evaluation, a grid search was applied to get the optimal number of filters and filter sizes.", "labels": [], "entities": []}, {"text": "Also, we tried with multiple configurations of convolutional layers of 2, 4 and 6.", "labels": [], "entities": []}, {"text": "The best performance consisted of two convolutional layers of each followed by a max pooling layer.", "labels": [], "entities": []}, {"text": "Convolutional of size 256 with filter size 5 applied for all the convolutional layers.", "labels": [], "entities": []}, {"text": "A dropout rate of 0.5 was implemented to avoid over fitting.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.9724626541137695}]}, {"text": "A fully connected layer with a length of 128 was followed by a second dropout function.", "labels": [], "entities": []}, {"text": "This was followed by a dense layer with a size of 4 to represent the number of classification classes using the Softmax function.", "labels": [], "entities": []}, {"text": "Our implementation was similar to the model presented in (Bj\u00f6rn).", "labels": [], "entities": []}, {"text": "We trained a simple LSTM model including one hidden layer containing 256 nodes and rectifier activation on the hidden layer, sigmoid activation on the output layer ADAM gradient descent, and a negative likelihood loss function.", "labels": [], "entities": []}, {"text": "We created 300 epochs and batch sizes of 5.", "labels": [], "entities": []}, {"text": "shows the results of the text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.6706845164299011}]}, {"text": "Our first classification experiment was over the original dataset with three classes, since in the original dataset, the second class, indirect harassment, had only 6 tweets and in comparison to the other classes, it didn't have enough tweets; thus we removed this class and performed our classification algorithm on the rest of three classes.", "labels": [], "entities": []}, {"text": "Our second classification approach, verb replacement (VR), was based on the four balanced classes each having about 996 tweets, coming from the first text generation method, all word replacement (AWR).", "labels": [], "entities": [{"text": "verb replacement (VR)", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.8030952036380767}, {"text": "all word replacement (AWR)", "start_pos": 174, "end_pos": 200, "type": "TASK", "confidence": 0.6578342169523239}]}, {"text": "The third classification experiment, noun replacement (NR), was on the four balanced datasets coming from the second method of text generation, each class having about one thousand data points and the last experiment coming from the third approach for text generation; each class having the same number of tweets.", "labels": [], "entities": [{"text": "noun replacement (NR)", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.8819165825843811}, {"text": "text generation", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.7167431861162186}, {"text": "text generation", "start_pos": 252, "end_pos": 267, "type": "TASK", "confidence": 0.7574020028114319}]}, {"text": "We used five classification algorithms, the one-versus-all algorithm as the baseline, naive Bayes and SVM as more traditional classification algorithms and then two artificial neural network approaches, Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN).", "labels": [], "entities": []}, {"text": "On the original dataset, the highest accuracy, we achieved was 75% using CNN (for more results please see).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9993938207626343}, {"text": "CNN", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.9456599354743958}]}, {"text": "We believe this poor performance was due to poor coverage of our dataset and the imbalanced nature of the dataset.", "labels": [], "entities": []}, {"text": "We aimed to alleviate these issues using ConceptNet.", "labels": [], "entities": []}, {"text": "Experimenting on the second dataset, which was the generated data with all word replacement (AWR), showed a considerably higher performance in comparison to the original dataset.", "labels": [], "entities": [{"text": "all word replacement (AWR)", "start_pos": 71, "end_pos": 97, "type": "METRIC", "confidence": 0.6805367271105448}]}, {"text": "In this dataset, all four classes are balanced.", "labels": [], "entities": []}, {"text": "LSTM and CNN both have the same high performance on the classes.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9627090692520142}, {"text": "CNN", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.9509628415107727}]}, {"text": "The second performance relates to the SVM and the last one relates to the one-versus-all classification algorithm.", "labels": [], "entities": []}, {"text": "In the third generated dataset, noun replacement, the highest performance relates to the LSTM and the second highest relates to the CNN with a very small margin.", "labels": [], "entities": [{"text": "noun replacement", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.85582035779953}, {"text": "LSTM", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.7436941266059875}, {"text": "CNN", "start_pos": 132, "end_pos": 135, "type": "DATASET", "confidence": 0.8456408381462097}]}, {"text": "The highest performance is related to the LSTM for the third method of generated data, followed by CNN and the SVM and Naive Bayes.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.6999013423919678}, {"text": "CNN", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.9311971068382263}, {"text": "SVM", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.9118265509605408}]}, {"text": "We ran different text generated methods to know the best way to increase the number of tweets in each class and to balance it.", "labels": [], "entities": []}, {"text": "It seems all word replacement (AWR) of the sentence elements with specific relations from ConceptNet in combination with neural network yields the highest performance boost.", "labels": [], "entities": [{"text": "word replacement (AWR)", "start_pos": 13, "end_pos": 35, "type": "METRIC", "confidence": 0.6313959836959839}]}, {"text": "As mentioned in table 5.", "labels": [], "entities": []}, {"text": "All the generated methods have better performance in comparison to the raw data.", "labels": [], "entities": []}, {"text": "VR has better performance in comparison to the NR.", "labels": [], "entities": [{"text": "VR", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8290396928787231}]}, {"text": "The best performance for the text generation method is AWR (all Word Replacement) using ConceptNet as the generation method and LSTM and CNN as the classification method.", "labels": [], "entities": [{"text": "text generation", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7494118511676788}, {"text": "AWR", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9205045104026794}]}, {"text": "In addition, all the augmentation methods have better performance in comparison to the original data.", "labels": [], "entities": []}, {"text": "The method in which we augment the concepts from Wikidata and ConceptNet along with the description from Wikidata has better performance in comparison to the augmentation with ConceptNet.", "labels": [], "entities": []}, {"text": "However, the performance is not as good as the text generation method.", "labels": [], "entities": [{"text": "text generation", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7816865146160126}]}], "tableCaptions": [{"text": " Table 2: Sample of each category  category  sample  Indirect  harassment  -'act like a woman think like a  man'  -'conservative and intelligent  women did not take the day off'  -'everybody knows that every girl  should only want to marry a sane  man as good as her sane father  nobody can top a girl'  -'i am so sick amp tired of this  attitude oh wow youre smart for a  girl'  Physical  harassment", "labels": [], "entities": [{"text": "Physical  harassment", "start_pos": 380, "end_pos": 400, "type": "TASK", "confidence": 0.690682977437973}]}, {"text": " Table 1: The detail information of the sexist  data distribution.  Category  Number of tweets  Indirect harassment(#1)  260  Information threat(#2)  6  Sexual harassment(#3)  417  Physical harassment(#4)  123", "labels": [], "entities": []}, {"text": " Table 4: Classification results on the original  and on the generated texts.  OVR SVM Naive  Baye  s", "labels": [], "entities": [{"text": "OVR SVM Naive  Baye  s", "start_pos": 79, "end_pos": 101, "type": "DATASET", "confidence": 0.9207972168922425}]}]}