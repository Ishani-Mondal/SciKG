{"title": [{"text": "Improving Neural Network Performance by Injecting Background Knowledge: Detecting Code-switching and Borrowing in Algerian texts", "labels": [], "entities": [{"text": "Improving Neural Network Performance", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8102142512798309}]}], "abstractContent": [{"text": "We explore the effect of injecting background knowledge to different deep neural network (DNN) configurations in order to mitigate the problem of the scarcity of annotated data when applying these models on datasets of low-resourced languages.", "labels": [], "entities": []}, {"text": "The background knowledge is encoded in the form of lexicons and pre-trained sub-word embeddings.", "labels": [], "entities": []}, {"text": "The DNN models are evaluated on the task of detecting code-switching and borrowing points in non-standardised user-generated Algerian texts.", "labels": [], "entities": []}, {"text": "Overall results show that DNNs benefit from adding background knowledge.", "labels": [], "entities": []}, {"text": "However, the gain varies between models and categories.", "labels": [], "entities": []}, {"text": "The proposed DNN archi-tectures are generic and could be applied to other low-resourced languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent success of DNNs in various natural language processing (NLP) tasks has attracted attention from the research community attempting to extend their application to new tasks.", "labels": [], "entities": []}, {"text": "Nevertheless, the large amount of labelled data required to train DNNs limits their application to new tasks and new languages because it is hard to find large labelled corpora for these domains.", "labels": [], "entities": []}, {"text": "The issue is even more severe for low-resourced languages.", "labels": [], "entities": []}, {"text": "Another serious problem with most current NLP approaches and systems is that they are trained on well-edited standardised monolingual corpora, such as the Wall Street Journal, Wikipedia, etc.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 155, "end_pos": 174, "type": "DATASET", "confidence": 0.9136634866396586}]}, {"text": "This could be explained by the fact that fora longtime NLP has been influenced by the dominant descriptive linguistic theories affected by the standard language ideology which assumes that natural languages are uniform and monolingual.", "labels": [], "entities": []}, {"text": "However, standardisation is not universal), meaning that not all languages are standardised.", "labels": [], "entities": []}, {"text": "Therefore, lexical, structural and phonological variation is, for instance, the norm in natural language and not an exception, meaning that well-edited texts do not really reflect the natural usage of natural languages, but only represent formal languages.", "labels": [], "entities": []}, {"text": "The discrepancy between the assumed uniformity of language both in linguistic theory and NLP and their variable nature is accentuated by new technologies, such as social media platforms and messaging services.", "labels": [], "entities": []}, {"text": "These new communication platforms have facilitated the proliferation of writing in non-standardised languages on the web, such as colloquial Arabic or what is commonly referred to as dialectal Arabic.", "labels": [], "entities": []}, {"text": "This is because in interactive scenarios people usually use spokenlike (colloquial) language or, in multilingual societies where people have access to several linguistic codes at the same time, a mixture of languages/language varieties.", "labels": [], "entities": []}, {"text": "Consequently, this new kind of written data has created a serious problem regarding the usability of the existing NLP tools and approaches as they fail to properly process it, even in the case of well-resourced languages.", "labels": [], "entities": []}, {"text": "The contribution of the paper is to explore how to mitigate the problems (i) of the scarcity of annotated data when using DNNs with low-resourced languages, and to what extent can we take advantage of the limited available resources, and (ii) to provide NLP approaches and tools that would be able to deal with non-standardised texts and language-mixing.", "labels": [], "entities": []}, {"text": "In particular, for (i) we investigate what are the optimal ways of injecting available background knowledge to different configurations of DNNs in order to improve their performance.", "labels": [], "entities": []}, {"text": "For (ii) we take the case of the language used in Algeria as it poses serious challenges for the available NLP approaches and tools.", "labels": [], "entities": []}, {"text": "It is a low-resourced multilingual colloquial language.", "labels": [], "entities": []}, {"text": "We chose the task of a word-level language identification which is a first step towards processing such texts.", "labels": [], "entities": [{"text": "word-level language identification", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.6314448118209839}]}, {"text": "The task focuses on detecting code-switching and borrowing points in a text which represents the same utterance.", "labels": [], "entities": []}, {"text": "Knowing what parts of text belong to what language variety allows to perform better qualitative and quantitative analysis of such texts with other tools.", "labels": [], "entities": []}, {"text": "The paper is organised as follows: in Section 2 we briefly describe the complex linguistic situation in Algeria as a result of a language contact.", "labels": [], "entities": []}, {"text": "The section aims to explain the linguistic challenges of processing such texts and motivates our choices based on established sociolinguistic theories.", "labels": [], "entities": []}, {"text": "In Section 3 we present our available linguistic resources and different DNN configurations.", "labels": [], "entities": []}, {"text": "In Section 4 we describe our experimental setup and analyse the results.", "labels": [], "entities": []}, {"text": "Finally, in Section 5 we compare our contribution to previous related work.", "labels": [], "entities": []}], "datasetContent": [{"text": "All models and configurations are evaluated under the same conditions using 10-fold cross-validation on the labelled dataset.", "labels": [], "entities": []}, {"text": "As a baseline we take an existing system (Adouane and Dobnik, 2017), a classification-based system which uses a chain of additional back-off strategies which involve lexicons, linguistic rules, and finally the selection of the most frequent category.", "labels": [], "entities": []}, {"text": "We refer to this system as the baseline.", "labels": [], "entities": []}, {"text": "First, we train the RNN and CNN models only on the labelled data (supervised learning) without any background knowledge.", "labels": [], "entities": []}, {"text": "We also examine the effect of the FastText-based and the Lexiconbased models separately to quantify the contribution of each.", "labels": [], "entities": []}, {"text": "Then we combine both models to optimise their performance.", "labels": [], "entities": []}, {"text": "Second, in order to take advantage of all available linguistic resources, we add to each of the RNN and the CNN models background knowledge in the form of (i) lexicon embeddings; (ii) fasttext embeddings; (iii) a combination of both lexicon and fasttext embeddings; and (iv) bootstrap the unlabelled dataset with the baseline system and train the best performing DNN model on it to investigate whether bootstrapping improves its performance.", "labels": [], "entities": []}, {"text": "All results are reported as the average performance of the 10-fold cross-validation for each model at epoch 100 using the parameters mentioned earlier.", "labels": [], "entities": []}, {"text": "For short, we use FastText to refer to the FastText-based model and fasttext to refer to the fasttext embeddings, Lexicon to refer to the Lexicon-based model and lexicon to refer to the lexicon embeddings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 gives information  about the datasets where texts refer to social media  texts with an average length of 19 words, words  refer to linguistic words excluding other tokens  (digits, punctuation, emoticons), and types refer  to unique words.", "labels": [], "entities": []}, {"text": " Table 2: Statistics about the lexicons.", "labels": [], "entities": []}, {"text": " Table 3: Average error rate of the models without  background knowledge.", "labels": [], "entities": [{"text": "Average error rate", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.8742348750432333}]}, {"text": " Table 4: Average error rate of the models with  background knowledge.", "labels": [], "entities": [{"text": "Average error rate", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.8700407147407532}]}]}