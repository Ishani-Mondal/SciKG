{"title": [{"text": "Addressing Low-Resource Scenarios with Character-aware Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Most modern approaches to computing word embeddings assume the availability of text corpora with billions of words.", "labels": [], "entities": []}, {"text": "In this paper, we explore a setup where only corpora with millions of words are available, and many words in any new text are out of vocabulary.", "labels": [], "entities": []}, {"text": "This setup is both of practical interest -modeling the situation for specific domains and low-resource languages-and of psycholinguistic interest, since it corresponds much more closely to the actual experiences and challenges of human language learning and use.", "labels": [], "entities": []}, {"text": "We evaluate skip-gram word em-beddings and two types of character-based embeddings on word relatedness prediction.", "labels": [], "entities": [{"text": "word relatedness prediction", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.7574251890182495}]}, {"text": "On large corpora, performance of both model types is equal for frequent words, but character awareness already helps for infrequent words.", "labels": [], "entities": []}, {"text": "Consistently, on small corpora, the character-based models perform overall better than skip-grams.", "labels": [], "entities": []}, {"text": "The concatenation of different embed-dings performs best on small corpora and robustly on large corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "State-of-the-art word embedding models are routinely trained on very large corpora.", "labels": [], "entities": []}, {"text": "For example, train word2vec on a corpus of 6 billion tokens, and report the best GloVe results on 42 billion tokens.", "labels": [], "entities": []}, {"text": "From a language technology perspective, it is perfectly reasonable to use large corpora where available.", "labels": [], "entities": []}, {"text": "However, even with large corpora, embeddings struggle to accurately model the meaning of infrequent words (.", "labels": [], "entities": []}, {"text": "Moreover, for the vast majority of languages, substantially less data is available.", "labels": [], "entities": []}, {"text": "For example, there are only 4 languages with Wikipedias larger than 1 billion words, 1 and 25 languages with more than 100 mil-1 https://en.wikipedia.org/wiki/List_ of_Wikipedias#Detailed_list (as of lion words.", "labels": [], "entities": []}, {"text": "Similarly, specialized domains even in very high-resource languages are bound to have much less data available.", "labels": [], "entities": []}, {"text": "From a psycholinguistic point of view, current models miss the crucial ability of the human language faculty to generalize from little data.", "labels": [], "entities": []}, {"text": "By seventh grade, students have only heard about 50 million spoken words, and read about 3.8 million tokens of text, acquiring a vocabulary of 40,000-100,000 words).", "labels": [], "entities": []}, {"text": "This also means that any new text likely contains outof-vocabulary words which students interpret by generalizing from existing knowledge -an ability that plain word embedding models lack.", "labels": [], "entities": []}, {"text": "There are some studies that have focused on modeling infrequent and unseen words by capturing information at the subword and character levels.", "labels": [], "entities": []}, {"text": "break words into morphemes, and use recursive neural networks to compose word meanings from morpheme meanings.", "labels": [], "entities": []}, {"text": "Similarly, represent words as bags of character n-grams, allowing morphology to inform word embeddings without requiring morphological analysis.", "labels": [], "entities": []}, {"text": "However, both models are still typically applied to large corpora of training data, with the smallest English corpora used comprising about 1 billion tokens.", "labels": [], "entities": []}, {"text": "Our study investigates how embedding models fare when applied to much smaller corpora, containing only millions of words.", "labels": [], "entities": []}, {"text": "Few studies, except, have considered this setup in detail.", "labels": [], "entities": []}, {"text": "We evaluate one word-based and two character-based embedding models on word relatedness tasks for English and German.", "labels": [], "entities": []}, {"text": "We find that that the character-based models mimics human learning more closely, with both better results on small datasets and better performance on rare words.", "labels": [], "entities": []}, {"text": "At the same time, a fused representation that takes both word and character level into account yields the best results for small corpora.: Hyperparameters (dim. = dimensionality).", "labels": [], "entities": []}, {"text": "For WL and FT, software defaults were used for all hyperparameters unless otherwise specified.", "labels": [], "entities": [{"text": "FT", "start_pos": 11, "end_pos": 13, "type": "DATASET", "confidence": 0.7754988074302673}]}], "datasetContent": [{"text": "We evaluate our models for two languages, English and German.", "labels": [], "entities": []}, {"text": "These are clearly not low-resource languages, but the availability of corpora and evaluation datasets makes them suitable for experiments.", "labels": [], "entities": []}, {"text": "All models were trained on the standard Wikipedia corpora for English and German preprocessed by Al-.", "labels": [], "entities": []}, {"text": "In addition, we sampled two (sub)corpora with 10 and 100 million characters to evaluate the models' effectiveness on limited training data.", "labels": [], "entities": []}, {"text": "To generate a subcorpus of a particular size, articles were sampled uniformly at random (without replacement) from that language's Wikipedia until the target size was reached.", "labels": [], "entities": []}, {"text": "presents all corpora and subcorpora used, and their sizes.", "labels": [], "entities": []}, {"text": "Our 100M corpora account for around 1% (for English) and 3% (for German) of the full Wikipedia corpora, and 10M corpora for .1% and .3%, respectively.", "labels": [], "entities": []}, {"text": "We picked these sizes because they cover the \"typical\" Wikipedia sizes for low-resource languages.", "labels": [], "entities": []}, {"text": "We evaluate our models on a standard task in lexical semantics, predicting human word relatedness ratings.", "labels": [], "entities": [{"text": "predicting human word relatedness ratings", "start_pos": 64, "end_pos": 105, "type": "TASK", "confidence": 0.78396075963974}]}, {"text": "Compared to relation prediction, this task has the advantage of  being graded instead of categorical).", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.9681766331195831}]}, {"text": "We use two relatedness datasets in both languages.", "labels": [], "entities": []}, {"text": "For comparison to prior work, and fora rough comparison across languages, we utilize the WordSim353 benchmark (WS353) () for English and the German version of the Multilingual WordSim353 benchmark (WS353-de) for German ().", "labels": [], "entities": [{"text": "WordSim353 benchmark (WS353", "start_pos": 89, "end_pos": 116, "type": "DATASET", "confidence": 0.8903816640377045}]}, {"text": "As we are specifically interested in modeling rare words, we also use the Stanford Rare Word Dataset (RW,) for English.", "labels": [], "entities": [{"text": "Stanford Rare Word Dataset (RW", "start_pos": 74, "end_pos": 104, "type": "DATASET", "confidence": 0.8595923284689585}]}, {"text": "It was designed with these goals in mind.", "labels": [], "entities": []}, {"text": "While no parallel to this benchmark exists for German, the GUR350 benchmark ( shows similar properties: As shows, the words in GUR350 are less frequent and longer than in WS353-de.", "labels": [], "entities": [{"text": "GUR350 benchmark", "start_pos": 59, "end_pos": 75, "type": "DATASET", "confidence": 0.9153889417648315}, {"text": "GUR350", "start_pos": 127, "end_pos": 133, "type": "DATASET", "confidence": 0.8766536712646484}, {"text": "WS353-de", "start_pos": 171, "end_pos": 179, "type": "DATASET", "confidence": 0.8884813189506531}]}, {"text": "Thus, many more words are out of vocabulary in GUR350 even in the full corpus.", "labels": [], "entities": [{"text": "GUR350", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.8826879262924194}]}, {"text": "show the results for the three different training corpus sizes.", "labels": [], "entities": []}, {"text": "We report results for all items, just in-vocabulary items, and just out-of-vocabulary items (i.e., one or both elements of the word pair unseen in training).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hyperparameters (dim. = dimensionality). For  WL and FT, software defaults were used for all hyper- parameters unless otherwise specified.", "labels": [], "entities": [{"text": "FT", "start_pos": 63, "end_pos": 65, "type": "DATASET", "confidence": 0.8157480359077454}]}, {"text": " Table 2: Statistics for training corpora.", "labels": [], "entities": []}, {"text": " Table 3: Statistics for evaluation benchmarks (com- puted on full-size corpora). Frequencies averaged geo- metrically (on lemmas with non-zero frequency); mor- phemes/word averaged algebraically.", "labels": [], "entities": []}, {"text": " Table 4: Spearman correlations of embedding simi- larity and human judgments (full training sets: en 9G,  de 3.5G). Top: full benchmarks; middle: in-vocabulary  items; bottom: out-of-vocabulary items. Best model  for each benchmark and training bolded.", "labels": [], "entities": []}, {"text": " Table 5: Spearman correlations of embedding similar- ity and human judgments (100M training sets).", "labels": [], "entities": []}, {"text": " Table 6: Spearman correlations of embedding similar- ity and human judgments (10M training sets).", "labels": [], "entities": []}]}