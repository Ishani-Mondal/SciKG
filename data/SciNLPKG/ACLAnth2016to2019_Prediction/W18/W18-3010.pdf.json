{"title": [{"text": "Connecting Supervised and Unsupervised Sentence Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Representing sentences as numerical vectors while capturing their semantic context is an important and useful intermediate step in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 131, "end_pos": 158, "type": "TASK", "confidence": 0.6489900549252828}]}, {"text": "Representations that are both general and discriminative can serve as a tool for tackling various NLP tasks.", "labels": [], "entities": []}, {"text": "While common sentence representation methods are unsupervised in nature, recently , an approach for learning universal sentence representation in a supervised setting was presented in (Conneau et al., 2017).", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.7455199360847473}, {"text": "learning universal sentence representation", "start_pos": 100, "end_pos": 142, "type": "TASK", "confidence": 0.6287642419338226}]}, {"text": "We argue that although promising results were obtained, an improvement can be reached by adding various un-supervised constraints that are motivated by auto-encoders and by language models.", "labels": [], "entities": []}, {"text": "We show that by adding such constraints , superior sentence embeddings can be achieved.", "labels": [], "entities": []}, {"text": "We compare our method with the original implementation and show improvements in several tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings are considered one of the key building blocks in natural language processing and are widely used for various applications).", "labels": [], "entities": []}, {"text": "While word representations has been successfully used, representing the more complicated and nuanced nature of the next element in the hierarchy -a full sentence -is still considered a challenge.", "labels": [], "entities": []}, {"text": "Once trained, universal sentence representations can be used as an out-of-the-box tool for solving various NLP and computer vision problems.", "labels": [], "entities": []}, {"text": "Even though their importance is unquestionable, it seems that current results are still far from satisfactory.", "labels": [], "entities": []}, {"text": "More concretely, given a set of sentences {s i } n i=1 , sentence embedding methods are designed to map them to some feature space F along with a distance metric M such that given two sentences s i and s j that have similar semantic meaning, their distance M(s i , s j ) would be small.", "labels": [], "entities": []}, {"text": "The challenge is learning a mapping T : {s i } n i=1 \u2192 F that manages to capture the semantics of each s i . While sentence embedding are not always used in similarity probing, we find this formulation useful as the similarity assumption is implicitly made when training classifiers on top of the embeddings in downstream tasks.", "labels": [], "entities": [{"text": "similarity probing", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.7155465930700302}]}, {"text": "Sentences embedding methods were mostly trained in an unsupervised setting.", "labels": [], "entities": [{"text": "Sentences embedding", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8771197199821472}]}, {"text": "In () the ParagraphVector model was proposed which is trained to predict words in the document.", "labels": [], "entities": []}, {"text": "SkipThought ( vectors rely on the continuity of text to train an encoder-decoder model that tries to reconstruct the surrounding sentences of a given passage.", "labels": [], "entities": [{"text": "SkipThought", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9019321799278259}]}, {"text": "In Sequential Denoising Autoencoders (SDAE) ( high-dimensional input data is corrupted according to some noise function, and the model is trained to recover the original data from the corrupted version.", "labels": [], "entities": [{"text": "Sequential Denoising Autoencoders (SDAE)", "start_pos": 3, "end_pos": 43, "type": "TASK", "confidence": 0.6300920794407526}]}, {"text": "FastSent ( learns to predicts a Bag-Of-Word (BOW) representation of adjacent sentences given a BOW representation of some sentence.", "labels": [], "entities": []}, {"text": "In () a Hybrid Gaussian Laplacian density function is fitted to the sentence to derive Fisher Vectors.", "labels": [], "entities": []}, {"text": "While previous methods train sentence embeddings in an unsupervised manner, a recent work ( argued that better representations can be achieved via supervised training on a general sentence inference dataset sentence embedding methods and compare them on various benchmarks.", "labels": [], "entities": []}, {"text": "The SNLI dataset is composed of 570K pairs of sentences with a label depicting the relationship between them, which can be either 'neutral', 'contradiction' or 'entailment'.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7386159598827362}]}, {"text": "The authors show that by leveraging the dataset, state-of-the-art representations can be obtained which are universal and general enough for solving various NLP tasks.", "labels": [], "entities": []}, {"text": "A different, unsupervised, task in NLP is estimating the probability of word sequences.", "labels": [], "entities": []}, {"text": "A family of algorithms for this task titled word language models seek to model the problem as estimating the probability of a word, given the previous words in the text.", "labels": [], "entities": []}, {"text": "In () neural networks were employed and () was among the first methods to use recurrent neural networks (RNN) for modeling the problem, where the probability of the a word is estimated based on the previous words fed to the RNN.", "labels": [], "entities": []}, {"text": "A variant of RNN -Long Short Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) -were used in (.", "labels": [], "entities": []}, {"text": "Following that, () proposed a dropout augmented LSTM.", "labels": [], "entities": []}, {"text": "We note that there exists a connection between those two problems and try to model it more explicitly.", "labels": [], "entities": []}, {"text": "Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -Pe-", "labels": [], "entities": []}], "datasetContent": [{"text": "Following ( we have tested our approach on a wide array of classification tasks, including sentiment analysis, SST -Socher et al.), question-type (TREC -), product reviews (CR -Hu and Liu), subjectivity/objectivity (SUBJ -Pang and) and opinion polarity).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.9304257035255432}, {"text": "TREC -)", "start_pos": 147, "end_pos": 154, "type": "METRIC", "confidence": 0.8773481845855713}]}, {"text": "We also tested our approach on semantic textual similarity (STS 14 -), paraphrase detection (MRPC -Dolan et al.), entailment and semantic relatedness tasks (SICK-R and SICK-E -), though those tasks are more close in nature to the task of the SNLI dataset which the model was trained on.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.8510583937168121}, {"text": "SNLI dataset", "start_pos": 242, "end_pos": 254, "type": "DATASET", "confidence": 0.8228747546672821}]}, {"text": "In our experiments we have set \u03bb from eq.", "labels": [], "entities": []}, {"text": "(1) and eq.", "labels": [], "entities": []}, {"text": "(2) to be 1 and \u03bb 1 , \u03bb 2 from eq.", "labels": [], "entities": []}, {"text": "(3) and eq.", "labels": [], "entities": []}, {"text": "(4) to be 0.5.", "labels": [], "entities": []}, {"text": "All other hyper-parameters and implementation details were left unchanged to provide a fair comparison to the baseline method of ().", "labels": [], "entities": []}, {"text": "Our results are summarized in table 1.", "labels": [], "entities": []}, {"text": "We compared out method against the baseline BiL-STM implementation of ( and included FastSent ( and SkipThought vectors () as a reference.", "labels": [], "entities": []}, {"text": "As evident from table 1 in almost all the tasks evaluated, adding the proposed regularization terms improves performance.", "labels": [], "entities": []}, {"text": "This serve to show that in a supervised learning setting, additional information on the input sequence can be leveraged and injected to the model by adding simple unsupervised loss criteria.", "labels": [], "entities": []}], "tableCaptions": []}