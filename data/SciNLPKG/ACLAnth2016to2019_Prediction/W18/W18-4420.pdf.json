{"title": [{"text": "Delete or not Delete? Semi-Automatic Comment Moderation for the Newsroom", "labels": [], "entities": [{"text": "Newsroom", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.7447908520698547}]}], "abstractContent": [{"text": "Comment sections of online news providers have enabled millions to share and discuss their opinions on news topics.", "labels": [], "entities": []}, {"text": "Today, moderators ensure respectful and informative discussions by deleting not only insults, defamation, and hate speech, but also unverifiable facts.", "labels": [], "entities": []}, {"text": "This process has to be transparent and comprehensive in order to keep the community engaged.", "labels": [], "entities": []}, {"text": "Further, news providers have to make sure to not give the impression of censorship or dissemination of fake news.", "labels": [], "entities": []}, {"text": "Yet manual moderation is very expensive and becomes more and more unfeasible with the increasing amount of comments.", "labels": [], "entities": []}, {"text": "Hence, we propose a semi-automatic, holistic approach, which includes comment features but also their context, such as information about users and articles.", "labels": [], "entities": []}, {"text": "For evaluation, we present experiments on a novel corpus of 3 million news comments annotated by a team of professional moderators.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Our dataset consists of all comments published at a large German online news provider between January 1st, 2016 and March 31st, 2017.", "labels": [], "entities": [{"text": "German online news provider between January 1st", "start_pos": 58, "end_pos": 105, "type": "DATASET", "confidence": 0.7727255863802773}]}, {"text": "In total, there are about 3 million comments by 60k users associated with 26k articles out of which 100k are marked as inappropriate.", "labels": [], "entities": []}, {"text": "Each comment is annotated with several moderation flags, which have been manually curated by professional moderators working at the news provider.", "labels": [], "entities": []}, {"text": "Based on these flags, we create the binary ground truth for each comment, representing whether the comment is inappropriate or not.", "labels": [], "entities": []}, {"text": "visualizes that the amount of inappropriate comments varies overtime, especially due to special or unforeseen high-impact events.", "labels": [], "entities": []}, {"text": "The share of inappropriate comments varies between roughly 2% and 10%.", "labels": [], "entities": []}, {"text": "We indicate events possibly causing the temporary changes with labels in the figure.", "labels": [], "entities": []}, {"text": "For example, terror attacks typically result in emotional and controversial debates, which are prone to include provocative inappropriate remarks.", "labels": [], "entities": []}, {"text": "It is worth noting, though, that the general increase in comments following those events might change the way moderators work.", "labels": [], "entities": []}, {"text": "Under stress, moderators might choose: The share of inappropriate comments (black) aggregated with a 4-day centered moving average stands out at the date of specific news events.", "labels": [], "entities": []}, {"text": "The trend of the total number of comments is shown downscaled for comparison (light gray).", "labels": [], "entities": []}, {"text": "to follow stricter moderation policies.", "labels": [], "entities": []}, {"text": "This decision -wittingly or unwittingly -might result in a higher share of flagged comments, even though they do not seem to be objectively worse than similar comments at a different point in time.", "labels": [], "entities": []}, {"text": "also shows that events that gain the most attention are related to social, political or security issues.", "labels": [], "entities": []}, {"text": "This circumstance is also mirrored in the news categories with the highest share of inappropriate comments: the categories related to society and politics have a share of 4.1% and 3.4%, respectively, while also containing the majority of all comments (70%).", "labels": [], "entities": []}, {"text": "In contrast, the share in all other categories combined is only 2.1%, being lowest in the business category (1.7%).", "labels": [], "entities": []}, {"text": "Furthermore, posts are not distributed uniformly among the users: only about 6% of the users posted more than 200 comments, the vast majority of roughly 48% of users have posted only once or twice.", "labels": [], "entities": []}, {"text": "As to expect in a real-world dataset that was not meant for academic research purposes, it is far from lab conditions.", "labels": [], "entities": []}, {"text": "For example, surprisingly, a single user posted 11,082 comments.", "labels": [], "entities": []}, {"text": "120 of these comments were flagged as inappropriate.", "labels": [], "entities": []}, {"text": "In addition to the platform's guidelines as described in Section 1, we observe that links to foreign language content have been removed by moderators.", "labels": [], "entities": []}, {"text": "As not all users can be expected to understand foreign languages, such content hinders them in joining the discussion.", "labels": [], "entities": []}, {"text": "Furthermore, moderators remove duplicate comments from the news site, which they do by flagging a duplicate just in the same way as they would do for insults or hate speech.", "labels": [], "entities": []}, {"text": "Duplicate detection and hate speech detection are quite different tasks that ask for different approaches.", "labels": [], "entities": [{"text": "Duplicate detection", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9467521607875824}, {"text": "hate speech detection", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.8792101542154948}]}, {"text": "For this reason, we filter exact duplicates in a preprocessing step and resolve data inconsistencies with data cleansing techniques.", "labels": [], "entities": []}, {"text": "We aim to detect abusive and disruptive comments that have been moderated because of insults, discrimination, and defamation, but also unverifiable suspicions, which do not rely on plausible arguments or credible sources.", "labels": [], "entities": []}, {"text": "These comments hinder a respectful discussion directly.", "labels": [], "entities": []}, {"text": "We do not focus on comments flagged because of copyright infringements, web links to inappropriate content, or personally identifiable information.", "labels": [], "entities": []}, {"text": "We split our dataset time-wise into training and test set in order to train only on past data and evaluate on future data.", "labels": [], "entities": []}, {"text": "Stratified sampling is employed to overcome effects of the imbalanced class distribution.", "labels": [], "entities": []}, {"text": "The cutoff timestamp is chosen such that 10,000 inappropriate comments remain in the test set.", "labels": [], "entities": []}, {"text": "Randomly sampled 10,000 appropriate comments from the remaining comments after the cutoff timestamp are added to the test set.", "labels": [], "entities": []}, {"text": "Thereby, a balanced class distribution is obtained in the test set.", "labels": [], "entities": []}, {"text": "All comments posted before that timestamp serve as training data.", "labels": [], "entities": []}, {"text": "The regression model outputs probabilities of a comment being inappropriate, but not a binary label.", "labels": [], "entities": []}, {"text": "In order to be able to compare with our ground truth labels, we map these probabilities to binary labels.", "labels": [], "entities": []}, {"text": "To this end, all comments with a predicted probability above a given threshold are marked as inappropriate and all comments with a probability below that threshold are marked as appropriate.", "labels": [], "entities": []}, {"text": "With regard to use cases other than our real-world example, there could be scenarios with two thresholds: one for almost certainly appropriate comments and one for almost certainly inappropriate ones.", "labels": [], "entities": []}, {"text": "Only comments between those two thresholds are left for manual assessment.", "labels": [], "entities": []}, {"text": "A disadvantage is that the decision to delete a comment could still be made completely automatic if the model is certain enough, which is unthinkable in our scenario.", "labels": [], "entities": []}, {"text": "summarizes the results of our experiments.", "labels": [], "entities": []}, {"text": "As can be seen in the table, we choose the threshold in away that at least 75% of the inappropriate comments are correctly classified, which corresponds to a recall of 75%.", "labels": [], "entities": [{"text": "recall", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.99932861328125}]}, {"text": "The reasoning for this is that it is acceptable to present more comments than necessary to the moderators, but it is important that clearly inappropriate comments do not slip through.", "labels": [], "entities": []}, {"text": "On the one hand, if the threshold is set fora higher recall, then the precision decreases until moderators do not profit from machine support but have to check almost all comments.", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9987578392028809}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9995774626731873}]}, {"text": "On the other hand, if the threshold is set fora lower recall, more and more actually inappropriate comments are falsely classified as appropriate.", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9986305236816406}]}, {"text": "As a consequence, inappropriate comments are inadvertently published without intervention.", "labels": [], "entities": []}], "tableCaptions": []}