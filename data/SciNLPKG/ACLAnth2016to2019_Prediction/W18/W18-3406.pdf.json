{"title": [{"text": "Multi-Task Active Learning for Neural Semantic Role Labeling on Low Resource Conversational Corpus", "labels": [], "entities": [{"text": "Neural Semantic Role Labeling", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.6414981558918953}]}], "abstractContent": [{"text": "Most Semantic Role Labeling (SRL) approaches are supervised methods which require a significant amount of annotated corpus, and the annotation requires linguistic expertise.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 5, "end_pos": 33, "type": "TASK", "confidence": 0.8430788318316141}]}, {"text": "In this paper, we propose a Multi-Task Active Learning framework for Semantic Role Labeling with Entity Recognition (ER) as the auxiliary task to alleviate the need for extensive data and use additional information from ER to help SRL.", "labels": [], "entities": [{"text": "Semantic Role Labeling with Entity Recognition (ER)", "start_pos": 69, "end_pos": 120, "type": "TASK", "confidence": 0.6944731871287028}, {"text": "SRL", "start_pos": 231, "end_pos": 234, "type": "TASK", "confidence": 0.9890369772911072}]}, {"text": "We evaluate our approach on Indonesian conversational dataset.", "labels": [], "entities": [{"text": "Indonesian conversational dataset", "start_pos": 28, "end_pos": 61, "type": "DATASET", "confidence": 0.6827018857002258}]}, {"text": "Our experiments show that multi-task active learning can outperform single-task active learning method and standard multi-task learning.", "labels": [], "entities": []}, {"text": "According to our results, active learning is more efficient by using 12% less of training data compared to passive learning in both single-task and multi-task setting.", "labels": [], "entities": []}, {"text": "We also introduce anew dataset for SRL in Indonesian conversational domain to encourage further research in this area 1 .", "labels": [], "entities": [{"text": "SRL", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9910661578178406}]}], "introductionContent": [{"text": "Semantic Role Labeling (SRL) extracts predicateargument structures from sentences ().", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL) extracts predicateargument structures", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.8402892020013597}]}, {"text": "It tries to recover information beyond syntax.", "labels": [], "entities": []}, {"text": "In particular, information that can answer the question about who did what to whom, when, why and soon.", "labels": [], "entities": []}, {"text": "There have been many proposed SRL techniques, and the high performing models are mostly supervised).", "labels": [], "entities": [{"text": "SRL", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9939471483230591}]}, {"text": "As they are supervised methods, request to research@kata.ai the models are trained on a relatively large annotated corpus.", "labels": [], "entities": []}, {"text": "Building such corpus is expensive as it is laborious, time-consuming, and usually requires expertise in linguistics.", "labels": [], "entities": []}, {"text": "For example, PropBank annotation guideline by is around 90 pages so it can be a steep learning curve even for annotators with a linguistic background.", "labels": [], "entities": [{"text": "PropBank annotation guideline", "start_pos": 13, "end_pos": 42, "type": "DATASET", "confidence": 0.7823103666305542}]}, {"text": "This difficulty makes reproducibility hard for creating annotated data especially in low resource language or different domain of data.", "labels": [], "entities": []}, {"text": "Several approaches have been proposed to reduce the effort of annotation.", "labels": [], "entities": []}, {"text": "introduced a Question Answering-driven approach by casting a predicate as a question and its thematic role as an answer in the system.", "labels": [], "entities": [{"text": "Question Answering-driven", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.8272274434566498}]}, {"text": "used active learning using semantic embedding.", "labels": [], "entities": []}, {"text": "utilized Annotation Projection with hybrid crowd-sourcing to route between hard instances for linguistic experts and easy instances for non-expert crowds.", "labels": [], "entities": []}, {"text": "Active Learning is the most common method to reduce annotation by using a model to minimize the amount of data to be annotated while maximizing its performance.", "labels": [], "entities": []}, {"text": "In this paper, we propose to combine active learning with multi-task learning applied to Semantic Role Labeling by using a related linguistic task as an auxiliary task in an end-to-end role labeling.", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.7913723190625509}]}, {"text": "Our motivation to use a multi-task method is in the same spirit as) where they employed related syntactic tasks to improve SRL in low-resource languages as multi-task learning.", "labels": [], "entities": [{"text": "SRL", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.989439070224762}]}, {"text": "Instead, we used Entity Recognition (ER) as the auxiliary task because we think ER is semantically related with SRL in some ways.", "labels": [], "entities": [{"text": "Entity Recognition (ER)", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.7829541206359864}]}, {"text": "For example, given a sentence: Andy gives a book to John, in SRL context, Andy and John are labeled as AGENT and PATIENT or BENEFACTOR respectively, but in ER context, they are labeled as PERSON.", "labels": [], "entities": [{"text": "AGENT", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9971269965171814}, {"text": "PATIENT", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.971187174320221}, {"text": "BENEFACTOR", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.9792180061340332}, {"text": "PERSON", "start_pos": 188, "end_pos": 194, "type": "METRIC", "confidence": 0.9675890207290649}]}, {"text": "Hence, although the labels are different, we hypothesize that there is some useful information from ER that can be leveraged to improve overall SRL performance.", "labels": [], "entities": [{"text": "SRL", "start_pos": 144, "end_pos": 147, "type": "TASK", "confidence": 0.9865707755088806}]}, {"text": "Our contribution in this paper consists of two parts.", "labels": [], "entities": []}, {"text": "First, we propose to train multi-task active learning with Semantic Role Labeling as the primary task and Entity Recognition as the auxiliary task.", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7024529576301575}, {"text": "Entity Recognition", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7128084897994995}]}, {"text": "Second, we introduce anew dataset and annotation tags for Semantic Role Labeling from conversational chat logs between a bot and human users.", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.8208158214886984}]}, {"text": "While many of the previous work studied SRL on large scale English datasets in news domain, our research aims to explore SRL in Indonesian conversational language, which is still underresourced.", "labels": [], "entities": [{"text": "SRL", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9913143515586853}, {"text": "SRL", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.9918490648269653}]}], "datasetContent": [{"text": "This research presents the dataset of human users conversation with virtual friends bot 2 . The annotated messages are user inquiries or responses to the bot.", "labels": [], "entities": []}, {"text": "Private information in the original data such as name, email, and address will be anonymized.", "labels": [], "entities": []}, {"text": "Three annotators with a linguistic background performed the annotation process.", "labels": [], "entities": []}, {"text": "In this work, we used a set of semantic roles adapted for informal, conversational language.", "labels": [], "entities": []}, {"text": "shows some examples of the semantic roles.", "labels": [], "entities": []}, {"text": "The dataset consists of 6057 unique sentences which contain predicates.", "labels": [], "entities": []}, {"text": "The semantic roles used area subset of PropBank ().", "labels": [], "entities": [{"text": "PropBank", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.953627347946167}]}, {"text": "Also, we added anew role, GREET.", "labels": [], "entities": [{"text": "GREET", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.8553702235221863}]}, {"text": "In our collected data, Indonesian people tend to call the name of the person they are talking to.", "labels": [], "entities": []}, {"text": "Because such case frequently co-occurs with another role, we felt the need to differentiate this previously mentioned entity as anew role.", "labels": [], "entities": []}, {"text": "For example, in the following sentence: Hi Andy!", "labels": [], "entities": []}, {"text": "I brought you a present can help refers \"you\" role as PATIENT to \"Andi\" role as GREET instead of left unassigned.", "labels": [], "entities": [{"text": "PATIENT", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9894980192184448}, {"text": "GREET", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.9987214207649231}]}, {"text": "In our second task, which is Entity Recognition (ER), we annotated the same sentence after the SRL annotation.", "labels": [], "entities": [{"text": "Entity Recognition (ER)", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.8208876073360443}]}, {"text": "We used common labels such as PERSON, LOCATION, ORGANIZATION, and MISC as our entity tags.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9123102426528931}, {"text": "LOCATION", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.905112087726593}, {"text": "ORGANIZATION", "start_pos": 48, "end_pos": 60, "type": "METRIC", "confidence": 0.9874875545501709}, {"text": "MISC", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.8463441133499146}]}, {"text": "Different from Named Entity Recognition (NER), ER also tag nominal objects such as \"I\", \"you\" and referential locations like \"di sana (over there)\".", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.8221845825513204}]}, {"text": "While this tagging might raise a question whether there are overlapping tags with SRL, we argue that entity labels are less ambiguous compared to role arguments which are dependent on the predicate.", "labels": [], "entities": []}, {"text": "An example of this case can be seen in, where both of I and you are tagged as PERSON whereas the roles are varied.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9885886907577515}]}, {"text": "In this task, we used semiautomatic annotation tools using brat ().", "labels": [], "entities": []}, {"text": "These annotation were checked and fixed by four people and one linguistic expert.", "labels": [], "entities": []}, {"text": "The purpose of the experiment is to understand whether multi-task learning and active learning help to improve SRL model performance compared to the baseline model (SRL with no AL scenario).", "labels": [], "entities": [{"text": "SRL model", "start_pos": 111, "end_pos": 120, "type": "TASK", "confidence": 0.8633812963962555}]}, {"text": "In this section, we focus on several experiment scenarios: single-task SRL, single-task SRL with AL, MTL, and MTL with AL.", "labels": [], "entities": []}, {"text": "Model Architecture Our model architecture consists of word embedding, character 5-gram encoder using CNN and predicate embedding as inputs, with 50, 50, and 100 dimension respectively.", "labels": [], "entities": []}, {"text": "These inputs are concatenated into a 200-dimensional vector which then fed into two-layer Highway LSTM with 300 hidden units.", "labels": [], "entities": []}, {"text": "Initialization The word embedding were initialized with unsupervised pre-trained values obtained from training word2vec () on the dataset.", "labels": [], "entities": []}, {"text": "Word tokens were lowercased, while characters were not.", "labels": [], "entities": []}, {"text": "Training Configurations For training configurations, we trained for 10 epochs using AdaDelta) with \u03c1 = 0.95 and = 1.e\u22126.", "labels": [], "entities": []}, {"text": "We also employed early stopping with patience set to 3.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.6817967593669891}, {"text": "patience", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9933295249938965}]}, {"text": "We split our data using 80% training, 10% validation, and 10% test for the fully supervised scenario.", "labels": [], "entities": []}, {"text": "For the active learning scenario, we further split the training data into labeled and unlabeled data.", "labels": [], "entities": []}, {"text": "We used two kinds of split, 50:50 and 85:15.", "labels": [], "entities": []}, {"text": "For the 50:50 scenario, we queried 100 sentences for each epoch.", "labels": [], "entities": []}, {"text": "For the 85:15 scenario, we used a smaller query of 10 sentences in an epoch to keep the number of queries less than the number of available fully supervised training data in 10 epochs.", "labels": [], "entities": []}, {"text": "This number of queried sentences was obtained by tuning on the validation set.", "labels": [], "entities": []}, {"text": "As for the AL query method, in the single-task SRL, we used random and uncertainty sampling query.", "labels": [], "entities": [{"text": "SRL", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.8060113787651062}]}, {"text": "SRL with 100% training data and SRL with random query serve as baseline strategies.", "labels": [], "entities": []}, {"text": "In the MTL SRL, we employed random task and ranking.", "labels": [], "entities": [{"text": "MTL SRL", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.7203533053398132}]}], "tableCaptions": [{"text": " Table 2: Experiment results, Scenario Active  means the query strategy used to sort instance in- formativeness, RandTask = Random Task Selec- tion, Data scenario are initial percentage of labeled  data, 50% means the 50:50 split, 85% means 85:15  split, and 100% means use all training data. P  (Precision), R (Recall), F1 (F1 Score)", "labels": [], "entities": [{"text": "Recall", "start_pos": 312, "end_pos": 318, "type": "METRIC", "confidence": 0.8370788097381592}, {"text": "F1", "start_pos": 321, "end_pos": 323, "type": "METRIC", "confidence": 0.9918215870857239}, {"text": "F1 Score", "start_pos": 325, "end_pos": 333, "type": "METRIC", "confidence": 0.9407204389572144}]}, {"text": " Table 3: Detailed scores of Multi-Task Active  Learning performance with 85% initial data. P  (Precision), R (Recall), F1 (F1 Score)", "labels": [], "entities": [{"text": "Recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.8063713908195496}, {"text": "F1", "start_pos": 120, "end_pos": 122, "type": "METRIC", "confidence": 0.98665851354599}, {"text": "F1 Score", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9702273011207581}]}]}