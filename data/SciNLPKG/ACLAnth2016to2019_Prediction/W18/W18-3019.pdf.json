{"title": [{"text": "Characters or Morphemes: How to Represent Words?", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we investigate the effects of using subword information in representation learning.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.9275803565979004}]}, {"text": "We argue that using syntactic subword units effects the quality of the word representations positively.", "labels": [], "entities": []}, {"text": "We introduce a morpheme-based model and compare it against to word-based, character-based, and character n-gram level models.", "labels": [], "entities": []}, {"text": "Our model takes a list of candidate seg-mentations of a word and learns the representation of the word based on different segmentations that are weighted by an attention mechanism.", "labels": [], "entities": []}, {"text": "We performed experiments on Turkish as a morphologically rich language and English with a comparably poorer morphology.", "labels": [], "entities": []}, {"text": "The results show that morpheme-based models are better at learning word representations of morphologically complex languages compared to character-based and character n-gram level models since the morphemes help to incorporate more syntactic knowledge in learning, that makes morpheme-based models better at syntactic tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "The distributional hypothesis of has been used to motivate work on vector space models to learn word representations.", "labels": [], "entities": []}, {"text": "Deep learning models learn another kind of vector space model for building word representations, which shows superior performance in representing words.", "labels": [], "entities": []}, {"text": "Although deep neural networks have been very successful in representing words via such vectors, those models have not been very successful at estimating the representations of rare words since they do not appear often enough to allow us to collect reliable statistics about their context.", "labels": [], "entities": []}, {"text": "Morphologically complex words are also rare by definition.", "labels": [], "entities": []}, {"text": "state that a word like unbelievableness does not exist in the first 17 million words of Wikipedia.", "labels": [], "entities": []}, {"text": "Some methods have been proposed to deal with the sparsity issue in learning word representations.", "labels": [], "entities": [{"text": "learning word representations", "start_pos": 67, "end_pos": 96, "type": "TASK", "confidence": 0.6302037636439005}]}, {"text": "One approach is to utilize the subword information such as characters, character n-grams, or morphemes rather than learning distinct word representations without considering the inner structure of words.", "labels": [], "entities": []}, {"text": "Character-based models usually learn better word representations compared to word-based models since they capture the regularities inside the words so that it mitigates the sparsity in representation learning.", "labels": [], "entities": []}, {"text": "However, those models learn the representations through the characters that do not correspond to a syntactic or semantic unit.", "labels": [], "entities": []}, {"text": "In Turkish, two words can have similar word representations under a character-based model just because of their common suffixes.", "labels": [], "entities": []}, {"text": "For example, character-based models such as () generate similar word representations for words that have common character n-grams such as kitaplardan (from the books) and kasaplardan (from the butchers) (where lar and dan are suffixes, kitap and kasap are the roots) although the two words are semantically not related at all.", "labels": [], "entities": []}, {"text": "Another problem we observed for the characterbased models is that such models estimate distant representations for words that are semantically related but involve different forms of the same morpheme so called allomorphs.", "labels": [], "entities": []}, {"text": "This is one of the consequences of vowel harmony in some languages like Turkish.", "labels": [], "entities": [{"text": "vowel harmony", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.6992005705833435}]}, {"text": "We observed this through several semantic similarity tasks performed on semantically similar but orthographically different words by using the word representations obtained from character n-gram level models such as fasttext (.", "labels": [], "entities": []}, {"text": "For example, Turkish words such as mavililerinki (of the ones with the blue color) and sar\u0131l\u0131lar\u0131nki (of the ones with the yellow color) with allomorphs li and l\u0131; ler and lar; in and \u0131n are asserted to be distant from each other in regard to their word representations under a character n-gram level model such as fasttext (, although the two words are semantically similar and both referring to colors.", "labels": [], "entities": []}, {"text": "In this paper, we argue that learning word representations through morphemes rather than characters lead to more accurate word vectors especially in morphologically complex languages.", "labels": [], "entities": []}, {"text": "Such character-based models are strongly affected by the orthographic commonness of words, that governs orthographically similar words to have similar word representations.", "labels": [], "entities": []}, {"text": "We introduce a model to learn morpheme and word representations especially for morphologically very complex words without using an external supervised morphological segmentation system.", "labels": [], "entities": []}, {"text": "Instead, we use an unsupervised segmentation model to initialize our model with a list of candidate morphological segmentations of each word in the training data.", "labels": [], "entities": []}, {"text": "We do not provide a single segmentation per word like others), but instead we provide a list of potential segmentations of each word.", "labels": [], "entities": []}, {"text": "Therefore, our model relaxes the requirement of an external segmentation system in morpheme-based representation learning.", "labels": [], "entities": []}, {"text": "To our knowledge, this will be the first attempt in co-learning of morpheme representations and word representations in an unsupervised framework without assuming a single morphological segmentation per word.", "labels": [], "entities": []}, {"text": "Our model is mostly similar to that of and since we also aim to learn morpheme and word representations.", "labels": [], "entities": []}, {"text": "Our model is akin to that of from the training perspective since they infer the out-of-vocabulary word embeddings from pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "Here, we also try to mimic the word2vec () embeddings (i.e. that are the expected outputs of the model) to learn the rare word representations with a complex morphology.", "labels": [], "entities": []}, {"text": "Our model shows some architectural similarities to that of.", "labels": [], "entities": []}, {"text": "Both models use the attention mechanism to up-weight the correct morphological segmentation of a word.", "labels": [], "entities": []}, {"text": "However, their model is character-based and our model is morpheme-based where different segmentations of each word contribute to the resulting vector.", "labels": [], "entities": []}, {"text": "It should be noted that our main concern is to investigate what character-based models cannot learn that the morpheme-based models learn.", "labels": [], "entities": []}, {"text": "As for the experimental setting, we have chosen Turkish language that has a complex morphology and severe allomorphy.", "labels": [], "entities": []}, {"text": "The results show that a morpheme-based model is better at estimating word representations of morphologically complex words (with at least 2-3 suffixes) compared to other word-based and character-based models.", "labels": [], "entities": []}, {"text": "We present experimental results on Turkish as an agglutinative language and English as a morphologically poor language.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed several experiments to assess the quality of our morpheme and word embeddings.", "labels": [], "entities": []}, {"text": "We did experiments on Turkish as a highly agglutinative language with a very complex morphology and English with a comparably poor morphology.", "labels": [], "entities": []}, {"text": "In all experiments, morpheme vectors have a dimension of d morph = 75, while the forward and backward LSTMs have a dimension of d LST M = 300.", "labels": [], "entities": []}, {"text": "Since the output of the Bi-LSTMs is the concatenation of the forward and backward LSTMs, the Bi-LSTM output has a dimensionality of d biLST M = 600.", "labels": [], "entities": []}, {"text": "The output of the Bi-LSTMs is reduced to half after feeding the output through a feed-forward layer that results with a word vector dimension of d word = 300.", "labels": [], "entities": []}, {"text": "Our model is implemented in Keras, and publicly available 3 . For the pre-trained word vectors, we used the word vectors of dimension 300 that were obtained by training word2vec ().", "labels": [], "entities": []}, {"text": "For Turkish, we trained word2vec on Boun corpus () that contains 361 million word tokens.", "labels": [], "entities": [{"text": "Boun corpus", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.9676048457622528}]}, {"text": "For English, we used the Google's pre-trained word2vec model 4 that was trained on 100 billion words with a vocabulary size of 3M.", "labels": [], "entities": []}, {"text": "For training of our model, we used the most frequent 200K words from the pre-trained vocabularies to filter out the noise for both languages.", "labels": [], "entities": []}, {"text": "In order to compare the quality of our embeddings against the embeddings obtained from character n-gram level model fasttext (, we used the pre-trained word vectors trained on Wikipedia ( and we used the Google's pre-trained word vectors . In order to compare our model with the character-based model by: The comparison of the Spearman correlation between the human judgments and the word similarities obtained by computing the cosine similarity between the learned word embeddings for English and Turkish.", "labels": [], "entities": []}, {"text": "Only for testing reasons, we used PC-KIMMO ( for English and the two-level Turkish morphology for Turkish in order to segment test sets to obtain the actual morphemes for generating word representations from the morpheme vectors that are learned in a fully unsupervised setting.", "labels": [], "entities": []}, {"text": "Unsupervised segmentation system also could be used for the evaluation step, but we wanted to minimize the effect of incorrect segmentations to be able to evaluate the embeddings properly.", "labels": [], "entities": []}, {"text": "Yet, we discuss the effect of the supervised vs unsupervised segmentations in Section 5.5.", "labels": [], "entities": []}, {"text": "We did only intrinsic evaluation with a set of experiments that assess the quality of the word and morpheme representations.", "labels": [], "entities": []}, {"text": "In order to evaluate the quality of the word vectors, we did experiments on a list of word pairs.", "labels": [], "entities": []}, {"text": "We computed the cosine similarity between the learned vectors of each word pair and compared the similarity scores against to human judgments.", "labels": [], "entities": []}, {"text": "We used the Set 2 in WordSim353 dataset () for the semantic similarity experiments that already involves the human judgment scores from 1 to 10 for 200 English word pairs.", "labels": [], "entities": [{"text": "WordSim353 dataset", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.9826788306236267}]}, {"text": "Since there is no available word-pair list for Turkish, we prepared WordSimTr 7 that involves 138 word pairs and asked 15 human annotators to judge how similar two words are on a fixed scale from 1 to 10 where 1 shows a poor semantic similarity between the two words.", "labels": [], "entities": []}, {"text": "Our Turkish word pair list involves two groups of words.", "labels": [], "entities": []}, {"text": "The first group involves 81 semantically similar words that have at least two suffixes (possibly allomorphs).", "labels": [], "entities": []}, {"text": "An example pair is televizyonlarda (on the televi-  We performed experiments for the analogy task in order to test whether the suffixes make a linear numerical change on the word vectors in the embedding space.", "labels": [], "entities": []}, {"text": "The analogy experiments are usually performed fora triple of words such that A is to B so C is to ?, where A-B+C is expected to be equal to the questioned word.", "labels": [], "entities": []}, {"text": "The analogy can be semantical such as cat is to meow, so dog is to bark, or syntactic such as go is to gone, so have is to had.", "labels": [], "entities": []}, {"text": "Here, we tested only the syntactic analogy on a list of word tuples since our focus is especially morphologically complex languages.", "labels": [], "entities": []}, {"text": "For English, we used the syntactic relations section provided in the Google analogy dataset () that involves 10675 questions.", "labels": [], "entities": [{"text": "Google analogy dataset", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.8320879936218262}]}, {"text": "Since there is no analogy dataset for Turkish, we prepared a Turkish analogy set SynAnalogyTr 8 with 206 syntactic questions that involves inflected word forms.", "labels": [], "entities": []}, {"text": "The syntactic word tuples are judged by 40 human annotators in a scale from 1 to 10, where 1 shows a weak word analogy.", "labels": [], "entities": []}, {"text": "Most words involve more than one suffix to test the morphological regularity in the analogy task.", "labels": [], "entities": []}, {"text": "The results are given in for English and Turkish.", "labels": [], "entities": []}, {"text": "The results show that our model outperforms both) and fasttext ( on both Turkish and English languages.", "labels": [], "entities": []}, {"text": "Additionally, some examples to analogy results are given in and the nearest neighbors of the Turkish word kitap-lar-dan-m\u0131s\u00b8(m\u0131s\u00b8m\u0131s\u00b8(it was from the books) are given in.", "labels": [], "entities": []}, {"text": "In addition to the evaluation of the word vectors, we also evaluated the morpheme vectors that are the input embeddings to the neural network to be estimated during training.", "labels": [], "entities": []}, {"text": "In order to evaluate how well our morpheme vectors represent the morphemes, we used the allomorphs.", "labels": [], "entities": []}, {"text": "Allomorphs can be considered as true synonyms as they convey the same meaning with each other but with a different orthography.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The comparison of the Spearman corre- lation between the human judgments and the word  similarities obtained by computing the cosine sim- ilarity between the learned word embeddings for  English and Turkish.", "labels": [], "entities": []}, {"text": " Table 4: The comparison of the Spearman corre- lation between the human judgments and the word  similarities obtained by computing the cosine sim- ilarity between the learned word embeddings for  English on Text8 corpus.", "labels": [], "entities": [{"text": "Text8 corpus", "start_pos": 208, "end_pos": 220, "type": "DATASET", "confidence": 0.9788243472576141}]}, {"text": " Table 9: Example Turkish analogy questions and the cosine similarities between the expected words and  the learned word representations obtained from morph2vec, word2vec and fasttext.", "labels": [], "entities": []}]}