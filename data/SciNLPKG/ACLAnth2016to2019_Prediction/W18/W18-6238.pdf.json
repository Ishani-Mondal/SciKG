{"title": [{"text": "Fast Approach to Build an Automatic Sentiment Annotator for Legal Domain using Transfer Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "This study proposes a novel way of identifying the sentiment of the phrases used in the legal domain.", "labels": [], "entities": [{"text": "identifying the sentiment of the phrases used in the legal domain", "start_pos": 35, "end_pos": 100, "type": "TASK", "confidence": 0.6814259453253313}]}, {"text": "The added complexity of the language used in law, and the inability of the existing systems to accurately predict the sentiments of words in law are the main motivations behind this study.", "labels": [], "entities": []}, {"text": "This is a transfer learning approach which can be used for other domain adaptation tasks as well.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8928500413894653}]}, {"text": "The proposed methodology achieves an improvement of over 6% compared to the source model's accuracy in the legal domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9982354640960693}]}], "introductionContent": [{"text": "As described by, sentiment analysis or sentiment classification is a recent methodology that aligns with information retrieval and computational linguistics which is focused on the opinion towards something which is represented by a certain text.", "labels": [], "entities": [{"text": "sentiment analysis or sentiment classification", "start_pos": 17, "end_pos": 63, "type": "TASK", "confidence": 0.8427425503730774}]}, {"text": "In many recent studies involving NLP in various domains, it is common to reuse the seminal RNTN (Recursive Neural Tensor Network) model) trained on movie reviews for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.9689984321594238}]}, {"text": "However, the trained model has bias towards the movie review domain.", "labels": [], "entities": [{"text": "movie review domain", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.60221795241038}]}, {"text": "We propose a novel methodology to perform transfer learning on the RNTN model mentioned in and build a target model.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.9380902051925659}]}, {"text": "Given that this is a transfer learning approach, the manually annotated data on movie reviews is used as the initial source model, rather than creating anew comparable manually annotated dataset for the legal domain.", "labels": [], "entities": []}, {"text": "In the proposed approach, the sentiment of a given phrase is classified into one of the two classes; negative and non-negative.", "labels": [], "entities": []}, {"text": "This classification criterion is selected following the fact that the major use case aligns with classifying terms and entities supporting/referring to either plaintiff or defendant.", "labels": [], "entities": []}, {"text": "Therefore, the proposed methodology is focused on identifying the statements with negative sentiment as much as possible.", "labels": [], "entities": []}, {"text": "This kind of sentiment classification is vital to identify the stakeholder-bias in legal case statements.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.7639590799808502}]}, {"text": "Similarly, sentiment analysis in legal text can become useful in automating the identification of arguments, the supporting/opposing party fora given argument and counter arguments.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.9351789057254791}]}, {"text": "For the testing purposes, we created a manually annotated target domain test dataset such that the phrases belong to one of the two classes: negative or non-negative.", "labels": [], "entities": []}, {"text": "The target system shows a recall of 0.7014 for identifying phrases with negative sentiment in the legal domain.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9995540976524353}]}, {"text": "Furthermore, the overall accuracy of the system is above 76% in classifying sentiments fora given phrase correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9996969699859619}]}, {"text": "If this result is compared with the results of source RNTN model), it is a 6% improvement inaccuracy.", "labels": [], "entities": []}, {"text": "The approach proposed in this study can be tried on other domain adaptation tasks related to sentiment classification as well.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.9436669945716858}]}], "datasetContent": [{"text": "The proposed approach in this paper is based on transfer learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.9299942255020142}]}, {"text": "Therefore, we needed to create a golden standard for identifying sentiments of phrases and sentences in the legal domain in order to evaluate the model.", "labels": [], "entities": [{"text": "identifying sentiments of phrases and sentences", "start_pos": 53, "end_pos": 100, "type": "TASK", "confidence": 0.7777162392934164}]}, {"text": "The phrases and sentences for the test data set are randomly picked from legal case transcripts based on the United States Supreme Court.", "labels": [], "entities": []}, {"text": "During the selection process, we have selected an equal amount of phrases for both classes according to the Socher Model.", "labels": [], "entities": []}, {"text": "Each of these phrases and sentences is annotated by three human annotators.", "labels": [], "entities": []}, {"text": "Since the classification process is binary, we pick the sentiment class for each test subject based on the maximum number of votes.", "labels": [], "entities": []}, {"text": "In the end, we prepare the test data set containing nearly 1500 annotations to use in the evaluation process.", "labels": [], "entities": []}, {"text": "In the experiment, we compare the sentiment class picked by human judges and the modified RNTN model.", "labels": [], "entities": [{"text": "RNTN", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.6327325701713562}]}, {"text": "As the baseline model, we use the source RNTN model ( Socher Model) to check the impact caused by the proposed transfer learning approach.", "labels": [], "entities": []}, {"text": "The acquired results from the baseline model is shown in and results from the target model is shown in.", "labels": [], "entities": []}, {"text": "According to, there is a 10% improvement in identifying phrases with negative sentiment.", "labels": [], "entities": []}, {"text": "The reason is that there area lot of unknown words which are in the legal domain but not in movie reviews corpus.", "labels": [], "entities": []}, {"text": "In addition, we have introduced new criteria based on a threshold for the score of negative class to improve the recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9990112781524658}]}, {"text": "Due to that reason, the precision in identifying phrases with a negative sentiment is 0.8441.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9995984435081482}]}, {"text": "But if we compare with the precision of the baseline model (Socher Model) for negative sentiment class is 0.7962 which is a lower value.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9986783862113953}]}, {"text": "Since the test dataset is not skewed a lot towards one class, it is fair to consider the accuracy of the system in predicting the sentiment for any given phrase.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9993078708648682}]}, {"text": "The baseline model shows the accuracy of 70.17% while the target model shows 76.80%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9997677206993103}]}, {"text": "The improvement inaccuracy is above 6%.", "labels": [], "entities": [{"text": "inaccuracy", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9179772138595581}]}, {"text": "The observed results in and show that there is a 6% improvement of the sentiment with respect to the baseline model.", "labels": [], "entities": []}, {"text": "There area few reasons behind the results.", "labels": [], "entities": []}, {"text": "As we randomly selected phrases from the legal case transcripts corpus, only 45% of the phrases actually contained the words where we had substituted the vector regarding sentiment.", "labels": [], "entities": [{"text": "legal case transcripts corpus", "start_pos": 41, "end_pos": 70, "type": "DATASET", "confidence": 0.66271011531353}]}, {"text": "Therefore, the output for 55% of the phrases from the baseline model and the target model was the same.", "labels": [], "entities": []}, {"text": "If we compare the output provided by the baseline model and the target model, output of 9.5% of the total phrases are different to each other.", "labels": [], "entities": []}, {"text": "Therefore the difference between the two models is based on that 9.5% of the total phrases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Confusion Matrix for Results from the Base- line Model", "labels": [], "entities": []}, {"text": " Table 4: Confusion Matrix for Results from the Im- proved Model", "labels": [], "entities": [{"text": "Im- proved Model", "start_pos": 48, "end_pos": 64, "type": "DATASET", "confidence": 0.8398429751396179}]}]}