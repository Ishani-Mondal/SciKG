{"title": [{"text": "Probing sentence embeddings for structure-dependent tense", "labels": [], "entities": []}], "abstractContent": [], "introductionContent": [{"text": "Learning universal sentence representations which accurately model sentential semantic content is a current goal of natural language processing research (.", "labels": [], "entities": []}, {"text": "A prominent and successful approach is to train recurrent neural networks (RNNs) to encode sentences into fixed length vectors ().", "labels": [], "entities": []}, {"text": "Many core linguistic phenomena that one would like to model in universal sentence representations depend on syntactic structure.", "labels": [], "entities": []}, {"text": "Despite the fact that RNNs do not have explicit syntactic structural representations, there is some evidence that RNNs can approximate such structure-dependent phenomena under certain conditions, in addition to their widespread success in practical tasks.", "labels": [], "entities": []}, {"text": "In this work, we assess RNNs' ability to learn the structure-dependent phenomenon of main clause tense.", "labels": [], "entities": []}, {"text": "To test whether sentence representations derived from RNNs capture main clause tense, we attempt to predict the tense from the representation.", "labels": [], "entities": []}, {"text": "This approach is called probing, and was introduced by and subsequently used by and others.", "labels": [], "entities": [{"text": "probing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9552293419837952}]}, {"text": "probed English sentence representations from various RNN architectures for main clause tense and concluded that these architectures, along with a bag-of-vectors (BoV) baseline, capture tense very well (84-91% accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.9964026212692261}]}, {"text": "However, this result was based on a test set in which the tense category (i.e. pastor present) to be predicted was the most common tense category in the sentence for 95.2% of sentences.", "labels": [], "entities": []}, {"text": "The high performance of the BoV model on this test set is not entirely surprising, given that showed a wide variety of word embedding models capture tense at the word level very well.", "labels": [], "entities": []}, {"text": "The high performance of the RNN models is not strong evidence that they are sensitive to the structure-dependence of main clause tense.", "labels": [], "entities": []}, {"text": "As suggested by, these models maybe learning a flawed heuristic that only works in grammatically simple examples.", "labels": [], "entities": []}, {"text": "Our goal is to determine whether RNNs learn to perform structure-dependent computation or whether they merely learn practical heuristics.", "labels": [], "entities": []}, {"text": "To do this, we extend the experimental setup of, which has a two step nature.", "labels": [], "entities": []}, {"text": "First, we train autoencoders for English, Spanish, French and Italian where both the encoder and decoder are either Simple Recurrent Networks or Long Short-Term Memory networks).", "labels": [], "entities": []}, {"text": "Second, we use the trained encoder to obtain sentence representations and probe those representations for main clause tense.", "labels": [], "entities": []}, {"text": "We investigate whether probing performance is affected by eight potential distractors, one of which is other words in the sentence with tense categories that differ from the tense of the main clause (e.g. we know who won).", "labels": [], "entities": []}, {"text": "To the extent that the representations are insensitive to structure-dependence, we expect to see probing performance negatively affected by distractors.", "labels": [], "entities": []}, {"text": "We compare the RNNs to three BoV baseline models.", "labels": [], "entities": [{"text": "RNNs", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.8669103980064392}]}, {"text": "In this extended abstract, we report on our work in progress.", "labels": [], "entities": []}, {"text": "We have completed data collection and preprocessing, designed our experiments and obtained complete results from our BoV baselines.", "labels": [], "entities": [{"text": "BoV baselines", "start_pos": 117, "end_pos": 130, "type": "DATASET", "confidence": 0.9107239246368408}]}], "datasetContent": [{"text": "In line with, we trained word embeddings on the Wikipedia data using skipgram, with hierarchical softmax and a window size of five, for five epochs.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.9594367742538452}]}, {"text": "We trained 50 sets of embeddings per language, with dimension sizes from 20 to 1000 in steps of 20.", "labels": [], "entities": []}, {"text": "Our three BoV baselines consist of combining these word embeddings by summing, averaging and using Smooth Inverse Frequency (.", "labels": [], "entities": []}, {"text": "Here, we report results from summing, which in contrast to related experiments (, consistently and significantly outperforms the other two baselines.", "labels": [], "entities": [{"text": "summing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9544094204902649}]}, {"text": "For the probing task, we use L1-regularized logistic regression with ten-fold cross validation.", "labels": [], "entities": []}], "tableCaptions": []}