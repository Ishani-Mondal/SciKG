{"title": [{"text": "Do Language Models Understand Anything? On the Ability of LSTMs to Understand Negative Polarity Items", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we attempt to link the inner workings of a neural language model to linguistic theory, focusing on a complex phenomenon well discussed informal linguistics: (negative) polarity items.", "labels": [], "entities": [{"text": "linguistic theory", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7324515283107758}]}, {"text": "We briefly discuss the leading hypotheses about the licensing contexts that allow negative polarity items and evaluate to what extent a neural language model has the ability to correctly process a subset of such constructions.", "labels": [], "entities": []}, {"text": "We show that the model finds a relation between the licensing context and the negative polarity item and appears to be aware of the scope of this context, which we extract from a parse tree of the sentence.", "labels": [], "entities": []}, {"text": "With this research, we hope to pave the way for other studies linking formal linguistics to deep learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the past decade, we have seen a surge in the development of neural language models (LMs).", "labels": [], "entities": []}, {"text": "As they are more capable of detecting long distance dependencies than traditional n-gram models, they serve as a stronger model for natural language.", "labels": [], "entities": []}, {"text": "However, it is unclear what kind of properties of language these models encode.", "labels": [], "entities": []}, {"text": "This does not only hinder further progress in the development of new models, but also prevents us from using models as explanatory models and relating them to formal linguistic knowledge of natural language, an aspect we are particularly interested in in the current paper.", "labels": [], "entities": []}, {"text": "Recently, there has been an increasing interest in investigating what kind of linguistic information is represented by neural models, (see, e.g.,, with a strong focus on their syntactic abilities.", "labels": [], "entities": []}, {"text": "In particular, ( used the ability of neural LMs to detect noun-verb congruence pairs as a proxy for their awareness of syntactic structure, yielding promising results.", "labels": [], "entities": []}, {"text": "In this paper, we followup on this research by studying a phenomenon that has received much attention by linguists and for which the model requires -besides knowledge of syntactic structure -also a semantic understanding of the sentence: negative polarity items.", "labels": [], "entities": []}, {"text": "In short, NPIs area class of words that bear the special feature that they need to be licensed by a specific licensing context (LC) (a more elaborate linguistic account of NPIs can be found in the next section).", "labels": [], "entities": []}, {"text": "A common example of an NPI and LC in English are any and not, respectively: The sentence He didn't buy any books is correct, whereas He did buy any books is not.", "labels": [], "entities": []}, {"text": "To properly process an NPI construction, a language model must be able to detect a relationship between a licensing context and an NPI.", "labels": [], "entities": [{"text": "NPI construction", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7646039724349976}]}, {"text": "Following;, we devise several tasks to assess whether neural LMs (focusing in particular on LSTMs) can handle NPI constructions, and obtain initial positive results.", "labels": [], "entities": [{"text": "NPI constructions", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.7777017951011658}]}, {"text": "Additionally, we use diagnostic classifiers () to increase our insight in how NPIs are processed by neural LMs, where we look in particular at their understanding of the scope of an LCs, an aspect which is also relevant for many other natural language related phenomena.", "labels": [], "entities": []}, {"text": "We obtain positive results focusing on a subset of NPIs that is easily extractable from a parsed corpus but also argue that a more extensive investigation is needed to get a complete view on how NPIs -whose distribution is highly diverse -are processed by neural LMs.", "labels": [], "entities": []}, {"text": "With this research and the methods presented in this paper, we hope to pave the way for other studies linking neural language models to linguistic theory.", "labels": [], "entities": [{"text": "linguistic theory", "start_pos": 136, "end_pos": 153, "type": "TASK", "confidence": 0.718705952167511}]}, {"text": "In the next section, we will first briefly discuss NPIs from a linguistic perspective.", "labels": [], "entities": []}, {"text": "Then, in Sec-tion 3, we provide the setup of our experiments and describe how we extracted NPI sentences from a parsed corpus.", "labels": [], "entities": []}, {"text": "In Section 4 we describe the setup and results of an experiment in which we compare the grammaticality of NPI sentences with and without a licensing context, using the probabilities assigned by the LM.", "labels": [], "entities": []}, {"text": "Our second experiment is outlined in Section 5, in which we describe a method for scope detection on the basis of the intermediate sentence embeddings.", "labels": [], "entities": [{"text": "scope detection", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.9333040118217468}]}, {"text": "We conclude our findings in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experimental setup consists of 2 phases: first we extract the relevant sentences and NPI constructions from a corpus, and then, after passing the sentences through an LM, we apply several diagnostic tasks to them.", "labels": [], "entities": []}], "tableCaptions": []}