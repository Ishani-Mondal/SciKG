{"title": [{"text": "Self-Attention: A Better Building Block for Sentiment Analysis Neural Network Classifiers", "labels": [], "entities": [{"text": "Sentiment Analysis Neural Network Classifiers", "start_pos": 44, "end_pos": 89, "type": "TASK", "confidence": 0.9214068531990052}]}], "abstractContent": [{"text": "Sentiment Analysis has seen much progress in the past two decades.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9608134031295776}]}, {"text": "For the past few years, neural network approaches, primarily RNNs and CNNs, have been the most successful for this task.", "labels": [], "entities": []}, {"text": "Recently, anew category of neural networks, self-attention networks (SANs), have been created which utilizes the attention mechanism as the basic building block.", "labels": [], "entities": []}, {"text": "Self-attention networks have been shown to be effective for sequence model-ing tasks, while having no recurrence or convo-lutions.", "labels": [], "entities": []}, {"text": "In this work we explore the effectiveness of the SANs for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.9687103033065796}]}, {"text": "We demonstrate that SANs are superior in performance to their RNN and CNN counterparts by comparing their classification accuracy on six datasets as well as their model characteristics such as training speed and memory consumption.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.8071141242980957}]}, {"text": "Finally, we explore the effects of various SAN modifications such as multi-head attention as well as two methods of incorporating sequence position information into SANs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis, also know as opinion mining, deals with determining the opinion classification of apiece of text.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9276978373527527}, {"text": "opinion mining", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.8177507519721985}, {"text": "determining the opinion classification of apiece of text", "start_pos": 60, "end_pos": 116, "type": "TASK", "confidence": 0.7798869125545025}]}, {"text": "Most commonly the classification is whether the writer of apiece of text is expressing a position or negative attitude towards a product or a topic of interest.", "labels": [], "entities": []}, {"text": "Having more than two sentiment classes is called fine-grained sentiment analysis with the extra classes representing intensities of positive/negative sentiment (e.g. very-positive) and/or the neutral class.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7666889727115631}]}, {"text": "This field has seen much growth for the past two decades, with many applications and multiple classifiers proposed.", "labels": [], "entities": []}, {"text": "Sentiment analysis has been applied in areas such as social media, movie reviews, commerce, and healthcare [].", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9582536518573761}]}, {"text": "In the past few years, neural network approaches have consistently advanced the state-of-the-art technologies for sentiment analysis and other natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.9700567722320557}]}, {"text": "For sentiment analysis, the neural network approaches typically use pre-trained word embeddings such as word2vec or GloVe for input, which get processed by the model to create a sentence representation that is finally used fora softmax classification output layer.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9787644147872925}, {"text": "softmax classification output layer", "start_pos": 228, "end_pos": 263, "type": "TASK", "confidence": 0.8004648238420486}]}, {"text": "The main neural network architectures that have been applied for sentiment analysis are recurrent neural networks(RNNs) and convolutional neural networks (CNNs), with RNNs being more popular of the two.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.9594728946685791}]}, {"text": "For RNNs, typically gated cell variants such as long short-term memory (LSTM), Bi-Directional LSTM (BiLSTM), or gated recurrent unit (GRU)  are used.", "labels": [], "entities": []}, {"text": "Most recently, Vaswani et al. introduced the first fully-attentional architecture, called Transformer, which utilizes only the self-attention mechanism and demonstrated its effectiveness on neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 190, "end_pos": 222, "type": "TASK", "confidence": 0.8199370205402374}]}, {"text": "The Transformer model achieved state-ofthe-art performance on multiple machine translation datasets, without having recurrence or convolution components.", "labels": [], "entities": []}, {"text": "Since then, self-attention networks have been successfully applied to a variety of tasks, including: image classification, generative adversarial networks , automatic speech recognition, text summarization [ , semantic role labeling, as well as natural language inference and sentiment analysis.", "labels": [], "entities": [{"text": "image classification", "start_pos": 101, "end_pos": 121, "type": "TASK", "confidence": 0.7696033418178558}, {"text": "generative adversarial networks", "start_pos": 123, "end_pos": 154, "type": "TASK", "confidence": 0.9007374246915182}, {"text": "automatic speech recognition", "start_pos": 157, "end_pos": 185, "type": "TASK", "confidence": 0.6147936483224233}, {"text": "text summarization", "start_pos": 187, "end_pos": 205, "type": "TASK", "confidence": 0.731334924697876}, {"text": "semantic role labeling", "start_pos": 210, "end_pos": 232, "type": "TASK", "confidence": 0.6454899807771047}, {"text": "sentiment analysis", "start_pos": 276, "end_pos": 294, "type": "TASK", "confidence": 0.9549642205238342}]}, {"text": "In this paper we demonstrate that self-attention is a better building block compared to recurrence or convolutions for sentiment analysis classifiers.", "labels": [], "entities": [{"text": "sentiment analysis classifiers", "start_pos": 119, "end_pos": 149, "type": "TASK", "confidence": 0.930567721525828}]}, {"text": "We extend the work of by exploring the behaviour of various self-attention architectures on six different datasets and making direct comparisons to their work.", "labels": [], "entities": []}, {"text": "We set our baselines to be their results for LSTM, BiLSTM, and CNN models, and used the same code for dataset preprocessing, word embedding imports, and batch construction.", "labels": [], "entities": [{"text": "word embedding imports", "start_pos": 125, "end_pos": 147, "type": "TASK", "confidence": 0.7301190694173177}, {"text": "batch construction", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.791313886642456}]}, {"text": "Finally, we explore the effectiveness of SAN architecture variations such as different techniques of incorporating positional information into the network, using multi-head attention, and stacking self-attention layers.", "labels": [], "entities": []}, {"text": "Our results suggest that relative position representations is superior to positional encodings, as well as highlight the efficiency of the stacking self-attention layers.", "labels": [], "entities": []}, {"text": "Source code is publicly available 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "To reduce implementation deviations from previous work, we use the codebase from and only replace the model and training process.", "labels": [], "entities": []}, {"text": "We re-use the code for batch preprocessing and batch construction for all datasets, accuracy evaluation, as well as use the same word embeddings 2 . All neural network models use crossentropy for the training loss.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9962170720100403}]}, {"text": "All experiments and benchmarks were run using a single GTX 1080 Ti with an i7 5820k @ 3.3Ghz and 32Gb of RAM.", "labels": [], "entities": [{"text": "RAM", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9840739965438843}]}, {"text": "For model implementations: LSTM, BiLSTM, and CNN baselines are implemented in Keras 2.0.8 with Tensorflow 1.7 backend using cuDNN 5.1.5 and CUDA 9.1.", "labels": [], "entities": [{"text": "cuDNN 5.1.5", "start_pos": 124, "end_pos": 135, "type": "DATASET", "confidence": 0.9209326505661011}]}, {"text": "All self-attention models are implemented in Tensorflow 1.7 and use the same CUDA libraries.", "labels": [], "entities": []}, {"text": "In order to determine if certain neural network building blocks are superior, we test on six datasets from with different properties.", "labels": [], "entities": []}, {"text": "The summary for dataset properties is in grouped, and all positive classes are grouped.", "labels": [], "entities": []}, {"text": "The datasets are pre-processed to only contain sentencelevel labels, and none of the models reported in this work utilize the phrase-level labels that are also provided.", "labels": [], "entities": []}, {"text": "The OpeNER dataset [y Montse Cuadros y Se\u00e1n Gaines y German is a dataset of hotel reviews with four sentiment classes: very negative, negative, positive, and very positive.", "labels": [], "entities": [{"text": "OpeNER dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.932697206735611}]}, {"text": "This is the smallest dataset with the lowest average sentence length.", "labels": [], "entities": []}, {"text": "The SenTube datasets] consist of YouTube comments with two sentiment classes: positive and negative.", "labels": [], "entities": [{"text": "SenTube datasets", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8552962243556976}]}, {"text": "These datasets contain the longest average sentence length as well as the longest maximum sentence length of all the datasets.", "labels": [], "entities": []}, {"text": "The SemEval Twitter dataset (SemEval) consists of tweets with three classes: positive, negative, and neutral.", "labels": [], "entities": [{"text": "SemEval Twitter dataset (SemEval)", "start_pos": 4, "end_pos": 37, "type": "DATASET", "confidence": 0.8074218183755875}]}], "tableCaptions": [{"text": " Table 1: Modified Table 2 from [Barnes et al., 2017]. Dataset statistics, embedding coverage of dataset  vocabularies, as well as splits for Train, Dev (Development), and Test sets. The 'Wiki' embeddings are  the 50, 100, 200, and 600 dimension used for experiments.", "labels": [], "entities": []}, {"text": " Table 3: Neural networks architecture characteristics. A comparison of number of learnable parameters,  GPU VRAM usage (in megabytes) during training, as well as training and inference times (in seconds).", "labels": [], "entities": []}]}