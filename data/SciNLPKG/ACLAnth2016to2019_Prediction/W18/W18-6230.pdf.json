{"title": [{"text": "DataSEARCH at IEST 2018: Multiple Word Embedding based Models for Implicit Emotion Classification of Tweets with Deep Learning", "labels": [], "entities": [{"text": "Implicit Emotion Classification of Tweets", "start_pos": 66, "end_pos": 107, "type": "TASK", "confidence": 0.7463081359863282}]}], "abstractContent": [{"text": "This paper describes an approach to solve implicit emotion classification with the use of pre-trained word embedding models to train multiple neural networks.", "labels": [], "entities": [{"text": "implicit emotion classification", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.7465625603993734}]}, {"text": "The system described in this paper is composed of a sequential combination of Long Short-Term Memory and Convolutional Neural Network for feature extraction and Feedforward Neural Network for classification.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.7079837024211884}]}, {"text": "In this paper, we successfully show that features extracted using multiple pre-trained embeddings can be used to improve the overall performance of the system with Emoji being one of the significant features.", "labels": [], "entities": []}, {"text": "The evaluations show that our approach outperforms the baseline system by more than 8% without using any external corpus or lexicon.", "labels": [], "entities": []}, {"text": "This approach is ranked 8 thin Implicit Emotion Shared Task (IEST) at WASSA-2018.", "labels": [], "entities": [{"text": "WASSA-2018", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.7901698350906372}]}], "introductionContent": [{"text": "Emotion classification is a major area of interest within the field of Sentiment Analysis (SA).", "labels": [], "entities": [{"text": "Emotion classification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9368075430393219}, {"text": "Sentiment Analysis (SA)", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.8788350224494934}]}, {"text": "Social media is a great source of emotional content since people are willing to publish their views on them.", "labels": [], "entities": []}, {"text": "Twitter is one such platform which enables users to publish micro-blogs otherwise known as Tweets.", "labels": [], "entities": []}, {"text": "Although, the tweets are limited by the number of characters, when viewed as a group it can be very significant.", "labels": [], "entities": []}, {"text": "Every day, on average, around 500 million tweets are tweeted on Twitter.", "labels": [], "entities": []}, {"text": "This has attracted much interest from both academia and industries to study about opinions in tweets.", "labels": [], "entities": []}, {"text": "Tweets can generally be considered to contain textual content.", "labels": [], "entities": []}, {"text": "However, tweet text is usually informal containing much casual forms and emoji, thus bringing challenges in research.", "labels": [], "entities": []}, {"text": "Implicit emotions play a major challenge in emotion identification process in tweets.", "labels": [], "entities": [{"text": "emotion identification process", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.8001059492429098}]}, {"text": "This is due to the informal nature of the tweet and lack of methods to properly model such sentences.", "labels": [], "entities": []}, {"text": "Here the term \"implicit emotion\" can be defined as the emotion conveyed in the text without stating the words denoting the emotion directly.", "labels": [], "entities": []}, {"text": "There is an effect of implicit emotions on opinion analysis tasks such as emotion identification and emotional intensity prediction.", "labels": [], "entities": [{"text": "opinion analysis", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7285740971565247}, {"text": "emotion identification", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.7125168591737747}, {"text": "emotional intensity prediction", "start_pos": 101, "end_pos": 131, "type": "TASK", "confidence": 0.6754575967788696}]}, {"text": "However, techniques for modeling implicit emotions in tweets lack the sufficient performance.", "labels": [], "entities": []}, {"text": "Therefore, this study makes a major contribution to research by exploring methods for properly modeling a tweet.", "labels": [], "entities": []}, {"text": "Implicit Emotion Shared Task (IEST) ( hosted by WASSA-2018 1 poses a similar task of finding the emotion expressed in a tweet out of six basic emotions without the use of the word denoting the emotion.", "labels": [], "entities": [{"text": "Implicit Emotion Shared Task (IEST)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7351404471056802}]}, {"text": "This paper presents our approach to solve the above problem.", "labels": [], "entities": []}, {"text": "We were ranked 8 thin the competition related to this task.", "labels": [], "entities": []}, {"text": "Artificial Neural Networks (ANN) has shown to perform better than conventional machine learning algorithms and has been used in variety of Natural Language Processing tasks (.", "labels": [], "entities": []}, {"text": "One of the primary objectives of using neural networks is to model the non-linear relationships in data, which is observed in textual content frequently.", "labels": [], "entities": []}, {"text": "Up to now, a number of studies confirmed the effectiveness of neural networks as feature extractors rather than the final classifier for opinion mining.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 137, "end_pos": 151, "type": "TASK", "confidence": 0.8219589591026306}]}, {"text": "A variety of neural network classifiers has been applied to similar tasks such as emotion identification, polarity classification, and other text classification tasks.", "labels": [], "entities": [{"text": "emotion identification", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7985678911209106}, {"text": "polarity classification", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.7568037509918213}, {"text": "text classification tasks", "start_pos": 141, "end_pos": 166, "type": "TASK", "confidence": 0.7816625932852427}]}, {"text": "Feedforward Neural Networks (FNN), Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM)) networks are commonly used in recent related work.", "labels": [], "entities": []}, {"text": "Furthermore, researchers have studied much complex forms of Neural Networks by combining CNN and LSTM in different ways.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 will provide a brief description on the dataset, Section 3 describes the system architecture, Section 4 reports the results and analysis of our system, finally we conclude our work in Section 5 along with a discussion on further improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset is labeled based on the emotion word present in the tweet before replacing that emotion word in the text with a placeholder.", "labels": [], "entities": []}, {"text": "The dataset is labeled for six basic emotions: Anger, Sad, Joy, Fear, Disgust and Surprise.", "labels": [], "entities": [{"text": "Surprise", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.89042729139328}]}, {"text": "The complete details of the dataset can be found in the task description paper ().", "labels": [], "entities": []}, {"text": "The first set of analyses examined the impact of LSTM-CNN models trained with different word embedding models.", "labels": [], "entities": []}, {"text": "The results of the LSTM-CNN analysis are set out in.", "labels": [], "entities": [{"text": "LSTM-CNN analysis", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.8155319392681122}]}, {"text": "The train set evaluation is performed by training model on training dataset evaluating on trial set.", "labels": [], "entities": []}, {"text": "Test set training data comprised of both training data and trial data.", "labels": [], "entities": [{"text": "Test set training data", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.7149315252900124}]}, {"text": "It is apparent from this that the model has performed similarly for both trial dataset and test dataset, achieving similar/ better F 1 scores and variations from one feature to another.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 131, "end_pos": 141, "type": "METRIC", "confidence": 0.9840783675511678}]}, {"text": "We observe the best performance of the system when using Word2vec trained on twitter.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9449917674064636}]}, {"text": "This could be due to the fact that it contains in-domain vocabulary.", "labels": [], "entities": []}, {"text": "What stands out in the table is the improvement of results of M GW 2V with inclusion of Emoji2Vec.", "labels": [], "entities": [{"text": "M GW 2V", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.7801944414774576}, {"text": "Emoji2Vec", "start_pos": 88, "end_pos": 97, "type": "DATASET", "confidence": 0.941692590713501}]}, {"text": "It can thus be suggested that Emoji provide a substantial support to finding emotion in implicit context.", "labels": [], "entities": []}, {"text": "Furthermore, we observe that MW T F performs better than MW ST F and can be suggested that sub-word information provided by the embedding is not important in crating the model.", "labels": [], "entities": []}, {"text": "Another noteworthy observation is that all the models indicated in outperforms the baseline model in both trial and test cases, thus proving the effectiveness of the proposed model itself for implicit emotion prediction task.", "labels": [], "entities": [{"text": "implicit emotion prediction", "start_pos": 192, "end_pos": 219, "type": "TASK", "confidence": 0.683127204577128}]}, {"text": "In the next part of the analysis we used FNN trained using features extracted from LSTM-CNN models.", "labels": [], "entities": []}, {"text": "provides the evaluation results of these models on the test set.", "labels": [], "entities": []}, {"text": "'+ +' is used to represent vector concatenation operation and f (M ) denotes a function that extracts the learned features form model M from the last dense layer in the neural network fora given input text.", "labels": [], "entities": []}, {"text": "The evaluations are performed using the three best performing LSTM-CNN models: MT W 2V , MW T F and M E2V . We have omitted M GW 2V for this analysis since the word vector used to train M GW 2V is already contained in M E2V . Results from can be compared with the results in which shows that the performance (precision, recall and F 1 ) of models in the latter has improved than the individual model variants.", "labels": [], "entities": [{"text": "precision", "start_pos": 309, "end_pos": 318, "type": "METRIC", "confidence": 0.999531626701355}, {"text": "recall", "start_pos": 320, "end_pos": 326, "type": "METRIC", "confidence": 0.9992284774780273}, {"text": "F 1 )", "start_pos": 331, "end_pos": 336, "type": "METRIC", "confidence": 0.9912277857462565}]}, {"text": "Closer inspection of the shows that the best models are obtained when features from MT W 2V and M E2V are used together.", "labels": [], "entities": [{"text": "MT W 2V", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.846218486626943}]}, {"text": "The overall best performance is obtained when features from MT W 2V , M E2V and MW T F are concatenated together.", "labels": [], "entities": [{"text": "MT W 2V", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.8008003433545431}]}], "tableCaptions": [{"text": " Table 4: Evaluation of LSTM-CNN for different word embeddings", "labels": [], "entities": []}, {"text": " Table 5: Results of FNN for different feature combina- tions", "labels": [], "entities": [{"text": "FNN", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.4198737144470215}]}]}