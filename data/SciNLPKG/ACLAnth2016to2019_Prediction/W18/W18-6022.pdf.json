{"title": [], "abstractContent": [{"text": "This paper presents the Coptic Universal Dependency Treebank, the first dependency tree-bank within the Egyptian subfamily of the Afro-Asiatic languages.", "labels": [], "entities": [{"text": "Coptic Universal Dependency Treebank", "start_pos": 24, "end_pos": 60, "type": "DATASET", "confidence": 0.503577210009098}]}, {"text": "We discuss the composition of the corpus, challenges in adapting the UD annotation scheme to existing conventions for annotating Coptic, and evaluate inter-annotator agreement on UD annotation for the language.", "labels": [], "entities": []}, {"text": "Some specific constructions are taken as a starting point for discussing several more general UD annotation guidelines, in particular for appositions, ambiguous pas-sivization, incorporation and object-doubling.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Coptic language represents the last phase of the Ancient Egyptian phylum of the Afro-Asiatic language family, forming part of the longest continuously documented human language on Earth.", "labels": [], "entities": []}, {"text": "Despite its high value for historical, comparative and typological linguistics, as well as its cultural importance as the heritage language of Copts in Egypt and in the diaspora, digital resources for the study of Coptic have only recently become available, while syntactically annotated data did not exist until the beginning of the present project.", "labels": [], "entities": [{"text": "comparative and typological linguistics", "start_pos": 39, "end_pos": 78, "type": "TASK", "confidence": 0.6653066799044609}]}, {"text": "This paper presents the first treebank of Coptic, constructed within the UD framework and currently encompassing over 20,000 tokens.", "labels": [], "entities": [{"text": "UD framework", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.9395425021648407}]}, {"text": "In this section we give a brief overview of some pertinent facts of Coptic grammar, before moving onto describing how these are encoded in our corpus.", "labels": [], "entities": [{"text": "Coptic grammar", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.7866694629192352}]}, {"text": "Unlike earlier forms of Ancient Egyptian, which were written in hieroglyphs or hieratic script throughout the first three millennia BCE, Coptic was written starting in the early first millenium CE using a variant of the Greek alphabet, with several added letters for Egyptian sounds absent from Greek.", "labels": [], "entities": []}, {"text": "shows the script, which was originally written without spaces (the Greek loanword #uxh 'psyche' is visible at the top left).", "labels": [], "entities": []}, {"text": "Manuscript damage, also shown in the figure, represents a frequent challenge to annotation efforts (see Section 7).", "labels": [], "entities": []}, {"text": "Modern conventions separate Coptic text into multi-word units known as bound groups using spaces, based on the presence of one stressed lexical item in each group.", "labels": [], "entities": []}, {"text": "This leads to multiple units being spelled together which would normally receive separate tokens and part of speech tags in annotated corpora.", "labels": [], "entities": []}, {"text": "Similarly to languages such as Arabic, Amharic, or Hebrew, simple examples include noun phrases or prepositional phrases spelled together, as in (1), or clitic possessors spelled together with nouns, as in (2).", "labels": [], "entities": []}, {"text": "(1) /m:p:ran hm-p-ran 'in-the-name' rnt=k rnt=k 'name-your (SG.M)' However, Coptic fusional morphology can be much more complex than in Semitic languages, for several reasons.", "labels": [], "entities": [{"text": "Coptic fusional morphology", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.7774344285329183}]}, {"text": "Developing from a morphologically rich synthetic language through an analytic phase in Late Egyptian, Coptic has fusional morphology and is usually seen as an agglutinative or even polysynthetic language.", "labels": [], "entities": []}, {"text": "Similarly to inflection in Hausa, auxiliaries and clitics attach to verbs as in (3), and unlike in Semitic languages, compounds are spelled together and do not allow intervening articles.", "labels": [], "entities": []}, {"text": "The language also exhibits frequent verb-object incorporation, complicating word segmentation for tokenization (see Grossman 2014), as in the complex verb shown in (4).", "labels": [], "entities": [{"text": "verb-object incorporation", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.8187670111656189}, {"text": "word segmentation", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7186122238636017}]}, {"text": "Such complex verbs can be embedded in word formation processes, leading to nominalizations such as (5).", "labels": [], "entities": [{"text": "word formation", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.7151175737380981}]}, {"text": "Finally, some auxiliaries, such as the optative in (6) may either fuse with and even circumfix adjacent pronouns as in, or in some cases exhibit 'zero' forms for pronouns, as in (8).", "labels": [], "entities": []}, {"text": "ere:cwtm ero=f ere-s\u00af otm ero=f OPT+2.SG.F-hear to-him.3.SG.M 'may you hear him' (SG.F subj, fused) Representing these discontinuous and null phenomena within the UD framework is difficult in the first instance because of their intrinsic complexity (for example, UD prohibits null pronoun nodes, even in enhanced dependencies), but is further complicated by the use of existing standards in Coptic tokenization and tagging, which we present next.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate the application of the UD annotation scheme to Coptic by conducting an inter-annotator agreement experiment using three pairs of annotators.", "labels": [], "entities": []}, {"text": "We report label scores (LS) using Cohen's Kappa and % unlabeled attachment score (UAS) with and without punctuation.", "labels": [], "entities": [{"text": "label scores (LS)", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.8384963512420655}, {"text": "% unlabeled attachment score (UAS)", "start_pos": 52, "end_pos": 86, "type": "METRIC", "confidence": 0.7888020915644509}]}, {"text": "The annotators include two pairs of BA students with three semesters of Coptic but no experience with corpus annotation or dependencies, and a third pair consisting of one MA student with two semesters of Coptic but substantial experience annotating English (and some Coptic) dependencies, and one professor proficient in Coptic and dependency annotation (these are also the co-authors of the present paper, and will be referred to as the 'Expert' group below).: Agreement Scores.", "labels": [], "entities": []}, {"text": "'no punctuation' denotes scores with punctuation removed from evaluation two experiments: a pre-adjudication round and a post-adjudication round.", "labels": [], "entities": []}, {"text": "In pre-adjudication, annotators only read the online UD Coptic guidelines without any prior annotation experience.", "labels": [], "entities": [{"text": "UD Coptic guidelines", "start_pos": 53, "end_pos": 73, "type": "DATASET", "confidence": 0.811905562877655}]}, {"text": "Afterwards, student annotators discussed points of disagreement with the professor and adjudicated their sentences, before proceeding to the postadjudication round, in which we expected annotators to fare better.", "labels": [], "entities": []}, {"text": "Annotators had unlimited time to complete the task and the text in all rounds was a portion of the Martyrdom of St. Victor, which was presented together with a standard literary translation.", "labels": [], "entities": [{"text": "Martyrdom of St. Victor", "start_pos": 99, "end_pos": 122, "type": "DATASET", "confidence": 0.8812233358621597}]}, {"text": "As an annotation interface, we used the Arborator (Gerdes, 2013).", "labels": [], "entities": [{"text": "Arborator (Gerdes, 2013)", "start_pos": 40, "end_pos": 64, "type": "DATASET", "confidence": 0.780229518810908}]}, {"text": "compares the results of the three pairs of annotators.", "labels": [], "entities": []}, {"text": "All results are divided into two sections: with and without punctuation.", "labels": [], "entities": []}, {"text": "11 Results are further separated into pre-adjudication and postadjudication for the two undergraduate groups.", "labels": [], "entities": []}, {"text": "As shown, the expert annotator scores and the student annotator scores after post-adjudication exhibit relatively high levels of agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9947195053100586}]}, {"text": "Within the label score (LS) category, expert annotators scored k = 0.92 without punctuation and 0.93 with punctuation, both of which can be considered very good agreement.", "labels": [], "entities": [{"text": "label score (LS)", "start_pos": 11, "end_pos": 27, "type": "METRIC", "confidence": 0.8124183118343353}]}, {"text": "Post-adjudication, group B produced a label score (LS) of 0.81, while group A scored 0.88.", "labels": [], "entities": [{"text": "label score (LS)", "start_pos": 38, "end_pos": 54, "type": "METRIC", "confidence": 0.9840625762939453}]}, {"text": "Both of these scores can Scores that include punctuation are based on punctuation attachment to the root, but Udapi () is used to automatically attach punctuation according to UD guidelines for the final adjudicated gold version.", "labels": [], "entities": [{"text": "Udapi", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.8695210814476013}, {"text": "UD", "start_pos": 176, "end_pos": 178, "type": "DATASET", "confidence": 0.7344626188278198}]}, {"text": "be interpreted as strong agreement, and noticeably higher than scores between 0.75-0.79, which were achieved solely by reading the guidelines and without previous annotation experience.", "labels": [], "entities": [{"text": "agreement", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9819279909133911}]}, {"text": "Unlabeled attachment scores (UAS) also shows good results.", "labels": [], "entities": [{"text": "Unlabeled attachment scores (UAS)", "start_pos": 0, "end_pos": 33, "type": "METRIC", "confidence": 0.778795932730039}]}, {"text": "Expert annotators achieve 95.8% without punctuation and 96.0% with, and the student groups have reasonable post-adjudication agreement scores as high as 86.5% and 87.7%, respectively.", "labels": [], "entities": []}, {"text": "We observed notable improvements from pre-adjudication to post-adjudication from the student groups.", "labels": [], "entities": []}, {"text": "This shows that annotation accuracy on this task can improve after experience and discussing common annotation errors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.954017162322998}]}, {"text": "The fact that annotators are non-native speakers with limited experience with the language likely affects the inter-annotator agreement results and makes this a challenging task relative to evaluations in other languages, such as English.", "labels": [], "entities": []}, {"text": "report an agreement experiment on English dependencies with a UAS score of 97.16% and an LS score of 96.3%, conducted on section 23 of the Wall Street Journal corpus.", "labels": [], "entities": [{"text": "UAS score", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.8459373116493225}, {"text": "LS score", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9865705370903015}, {"text": "Wall Street Journal corpus", "start_pos": 139, "end_pos": 165, "type": "DATASET", "confidence": 0.9696764349937439}]}, {"text": "Although the labeled score is evaluated as % agreement rather than kappa, these results likely outperform our scores.", "labels": [], "entities": [{"text": "agreement", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.904121458530426}]}, {"text": "However in a more challenging task of annotating English tweets, report a UAS score of 88.8% and LS score of 84.3%, showing that quality can vary substantially across text types.", "labels": [], "entities": [{"text": "UAS score", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.8663244545459747}, {"text": "LS score", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9929223656654358}]}, {"text": "12 report results from a dependency annotation experiment on Ancient Greek with an attachment score of 87.4% and a label score of 85.3%.", "labels": [], "entities": [{"text": "attachment score", "start_pos": 83, "end_pos": 99, "type": "METRIC", "confidence": 0.9792317152023315}, {"text": "label score", "start_pos": 115, "end_pos": 126, "type": "METRIC", "confidence": 0.9691923558712006}]}, {"text": "While this experiment wasn't within the UD framework, it offers comparable agreement scores with respect to non-native speaker annotation.", "labels": [], "entities": [{"text": "UD framework", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.7713851928710938}]}, {"text": "The scores presented in their study are close to the attachment scores from our undergraduate student annotator pairs, though admittedly Coptic and Greek are typologically very distant.", "labels": [], "entities": []}, {"text": "Scores from other African languages are scarce, but report a kappa score of 0.488 for agreement on UD relations for the morphologically rich language Amharic.", "labels": [], "entities": [{"text": "kappa score", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9543996155261993}, {"text": "UD relations", "start_pos": 99, "end_pos": 111, "type": "TASK", "confidence": 0.8786743879318237}]}, {"text": "This score is interpreted as moderate agreement and is substantially lower than our label scores.", "labels": [], "entities": [{"text": "agreement", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9905795454978943}]}, {"text": "We conducted an error analysis to find common areas of disagreement.", "labels": [], "entities": []}, {"text": "While some errors can be attributed to simple, non-systematic mistakes, many high frequency errors are the result of complicated constructions or alternative interpretations of the text, which is at times not trivial to translate.", "labels": [], "entities": []}, {"text": "The majority of disagreements for the expert annotators pertained to coordination scope (which is often ambiguous in the translation); confusion over labeling objects (obj) and obliques (obl), often due to annotating more closely to the source language or the available translation's interpretation; and whether an item has an (obl) relation to a verb or an (nmod) relation to its dependent noun in constructions that are close to light-verb constructions, but not entirely lexicalized.", "labels": [], "entities": []}, {"text": "Coordination proved challenging for longer ambiguous sentences where, as non-native speakers, we relied on our own interpretation of the text for parsing.", "labels": [], "entities": []}, {"text": "Confusion over labeling items as obj and obl can also be attributed to similar syntactic environments where objects and obliques are both mediated by the preposition n: n-'of'.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Texts and genres in UD Coptic.", "labels": [], "entities": []}, {"text": " Table 4: Agreement Scores. 'no punctuation' denotes scores with punctuation removed from evaluation", "labels": [], "entities": []}]}