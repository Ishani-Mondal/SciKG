{"title": [{"text": "Replicated Siamese LSTM in Ticketing System for Similarity Learning and Retrieval in Asymmetric Texts", "labels": [], "entities": [{"text": "Similarity Learning and Retrieval in Asymmetric Texts", "start_pos": 48, "end_pos": 101, "type": "TASK", "confidence": 0.8306368929999215}]}], "abstractContent": [{"text": "The goal of our industrial ticketing system is to retrieve a relevant solution for an input query, by matching with historical tickets stored in knowledge base.", "labels": [], "entities": []}, {"text": "A query is comprised of subject and description, while a historical ticket consists of subject, description and solution.", "labels": [], "entities": []}, {"text": "To retrieve a relevant solution, we use textual similarity paradigm to learn similarity in the query and historical tickets.", "labels": [], "entities": []}, {"text": "The task is challenging due to significant term mismatch in the query and ticket pairs of asymmetric lengths, where subject is a short text but description and solution are multi-sentence texts.", "labels": [], "entities": []}, {"text": "We present a novel Replicated Siamese LSTM model to learn similarity in asymmetric text pairs, that gives 22% and 7% gain (Accuracy@10) for retrieval task, respectively over unsuper-vised and supervised baselines.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9990094900131226}]}, {"text": "We also show that the topic and distributed semantic features for short and long texts improved both similarity learning and retrieval.", "labels": [], "entities": [{"text": "similarity learning", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.8104658722877502}]}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) is the task to find out if the text pairs mean the same thing.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8023781478404999}]}, {"text": "The important tasks in Natural Language Processing (NLP), such as Information Retrieval (IR) and text understanding maybe improved by modeling the underlying semantic similarity between texts.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.8483689069747925}, {"text": "text understanding", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.8185974061489105}]}, {"text": "With recent progress in deep learning, the STS task has gained success using LSTM and) based architectures; however, these approaches model the underlying semantic similarity between example pairs, each with a single sentence or phrase with term overlaps.", "labels": [], "entities": [{"text": "STS task", "start_pos": 43, "end_pos": 51, "type": "TASK", "confidence": 0.9075786173343658}]}, {"text": "In the domain of question retrieval), users retrieve historical questions which precisely match their questions (single sentence) semantically equivalent or relevant.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7564326226711273}]}, {"text": "However, we investigate similarity learning between texts of asymmetric lengths, such as short (phrase) Vs longer (paragraph/documents) with significant term mismatch.", "labels": [], "entities": []}, {"text": "The application of textual understanding in retrieval becomes more challenging when the relevant document-sized retrievals are stylistically distinct with the input short texts.", "labels": [], "entities": []}, {"text": "Learning a similarity metric has gained much research interest, however due to limited availability of labeled data and complex structures in variable length sentences, the STS task becomes a hard problem.", "labels": [], "entities": [{"text": "STS task", "start_pos": 173, "end_pos": 181, "type": "TASK", "confidence": 0.8881630003452301}]}, {"text": "The performance of IR system is sub-optimal due to significant term mismatch in similar texts, limited annotated data and complex structures in variable length sentences.", "labels": [], "entities": [{"text": "IR", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9691064953804016}]}, {"text": "We address the challenges in a real-world industrial application.", "labels": [], "entities": []}, {"text": "Our ticketing system) consists of a query and historical tickets.", "labels": [], "entities": []}, {"text": "A query (reporting issue, q) has 2 components: subject (SUB) and description (DESC), while a historical ticket (t) stored in the knowledge base (KB) has 3 components: SUB, DESC and solution (SOL).", "labels": [], "entities": []}, {"text": "A SUB is a short text, but DESC and SOL consist of multiple sentences.", "labels": [], "entities": [{"text": "DESC", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.5724605321884155}]}, {"text": "shows that SUB \u0080 q and SUB \u0080 tare semantically similar and few terms in SUB \u0080 q overlap with DESC \u0080 t.", "labels": [], "entities": [{"text": "DESC", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.7060666084289551}]}, {"text": "However, the expected SOL \u0080 t is distinct from both SUB and DESC \u0080 q.", "labels": [], "entities": [{"text": "SOL \u0080 t", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.8896071910858154}, {"text": "DESC \u0080 q", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.5923809508482615}]}, {"text": "The goal is to retrieve an optimal action (i.e. SOL from t) for the input q.", "labels": [], "entities": []}, {"text": "To improve retrieval for an input q, we adapt the Siamese LSTM) for similarity learning in asymmetric text pairs, using the available information in q and t.", "labels": [], "entities": [{"text": "Siamese LSTM)", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.8026515046755472}, {"text": "similarity learning", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7561640739440918}]}, {"text": "For instance, QUERY pqq SUB: GT Trip -Low Frequency Pulsations DESC: GT Tripped due to a sudden increase in Low Frequency Pulsations.", "labels": [], "entities": [{"text": "QUERY pqq SUB", "start_pos": 14, "end_pos": 27, "type": "METRIC", "confidence": 0.8846858342488607}, {"text": "GT Tripped", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.8714507222175598}]}, {"text": "The machine has been restarted and is now operating normally.", "labels": [], "entities": []}, {"text": "Alarm received was: GT XXX Low Frequency Pulsation.", "labels": [], "entities": [{"text": "GT XXX Low Frequency", "start_pos": 20, "end_pos": 40, "type": "METRIC", "confidence": 0.6575657427310944}, {"text": "Pulsation", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.502639651298523}]}, {"text": "HISTORICAL TICKET ptq SUB: Narrow Frequency Pulsations DESC: Low and Narrow frequency pulsations were detected.", "labels": [], "entities": [{"text": "HISTORICAL", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9315102100372314}, {"text": "TICKET ptq SUB", "start_pos": 11, "end_pos": 25, "type": "METRIC", "confidence": 0.8152129650115967}, {"text": "Narrow Frequency Pulsations DESC", "start_pos": 27, "end_pos": 59, "type": "METRIC", "confidence": 0.48213735967874527}]}, {"text": "The peak value for the Low Frequency Pulsations is ## mbar.", "labels": [], "entities": []}, {"text": "SOL: XXXX combustion support is currently working on the issue.", "labels": [], "entities": [{"text": "SOL: XXXX combustion", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.8325334638357162}]}, {"text": "The action is that the machine should not run until resolved.", "labels": [], "entities": []}, {"text": "we compute multi-level similarity between (SUB \u0080 q, SUB \u0080 t) and (DESC \u0080 q, DESC \u0080 t).", "labels": [], "entities": []}, {"text": "However, observe in that the cross-level similarities such as between (SUB \u0080 q, DESC \u0080 t), (DESC \u0080 q, SUB \u0080 t) or (SUB \u0080 q, SOL \u0080 t), etc.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows: (1) Propose a novel architecture (Replicated Siamese LSTM) for similarity learning in asymmetric texts via multi-and-cross-level semantics (2) Investigate distributed and neural topic semantics for similarity learning via multiple channels (3) Demonstrate again of 22% and 7% in Accuracy@10 for retrieval, respectively over unsupervised and supervised baselines in the industrial application of a ticketing system.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 327, "end_pos": 335, "type": "METRIC", "confidence": 0.9986444115638733}]}], "datasetContent": [{"text": "We evaluate the proposed method on our industrial data for textual similarity learning and retrieval tasks in the ticketing system.", "labels": [], "entities": [{"text": "textual similarity learning", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.7027742266654968}]}, {"text": "Compute Similarity using topic vector (T) pairs of a query (X1) and historical ticket (X2) components E (X1-X2) Compute Similarity using embedding vector (E) pairs of a query (X1) and historical ticket (X2) components  Our industrial dataset consist of queries and historical tickets.", "labels": [], "entities": []}, {"text": "As shown in, a query consists of subject and description texts, while a historical ticket in knowledge base (KB) consists of subject, description and solution texts.", "labels": [], "entities": []}, {"text": "The goal of the ITS is to automatically recommend an optimal action i.e. solution for an input query, retrieved from the existing KB.", "labels": [], "entities": []}, {"text": "There are T 949 historical tickets in the KB, out of which 421 pairs are labeled with their relatedness score.", "labels": [], "entities": [{"text": "KB", "start_pos": 42, "end_pos": 44, "type": "DATASET", "confidence": 0.4866061806678772}]}, {"text": "We randomly split the labeled pairs by 80-20% for train (P tr ) and development (P dev ).", "labels": [], "entities": []}, {"text": "The relatedness labels are: YES (similar that provides correct solution), REL (does not provide correct solution, but close to a solution) and NO (not related, not relevant and provides no correct solution).", "labels": [], "entities": [{"text": "YES", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9934594631195068}, {"text": "REL", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9963765740394592}, {"text": "NO", "start_pos": 143, "end_pos": 145, "type": "METRIC", "confidence": 0.9967193007469177}]}, {"text": "We convert the labels into numerical scores  We establish baseline for similarity and retrieval by the following two unsupervised approaches: (1) Topic Semantics T: As discussed in section 2.2, we use DocNADE topic model to learn document representation.", "labels": [], "entities": []}, {"text": "To train, we take 50 held-out samples from the historical tickets T.", "labels": [], "entities": []}, {"text": "We compute perplexity on 100 topics for each ticket component from the held-out set, comparing LDA and DocNADE models trained individually with SUB+DESC (M 1) and SUB+DESC+SOL texts 2 (M 2).", "labels": [], "entities": []}, {"text": "shows that DocNADE outperforms LDA.", "labels": [], "entities": [{"text": "LDA", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.697216272354126}]}, {"text": "Next, we need to determine which DocNADE model (M 1 or M 2) is less perplexed to the queries.", "labels": [], "entities": []}, {"text": "Therefore, we use M 1 and M 2 to evaluate DESC1 and SUB1+DESC1 components of the two sets of queries: (1) Q L is the set of queries from labeled (421) pairs and (2) Q U is the end-user set.", "labels": [], "entities": [{"text": "DESC1", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.7398415803909302}]}, {"text": "shows that M 2 performs better than M 1 for both the sets of queries with DESC1 or SUB1+DESC1 texts.", "labels": [], "entities": [{"text": "DESC1", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.8719972968101501}]}, {"text": "We choose M 2 version of the DocNADE to setup baseline for the similarity learning and retrieval in unsupervised fashion.", "labels": [], "entities": [{"text": "DocNADE", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.9434890151023865}, {"text": "similarity learning", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8186743557453156}]}, {"text": "To compute a similarity score for the given query q and historical ticket t where (q, t)\u0080 P dev , we first compute a latent topic vector (T) each for q and t using DocNADE (M 2) and then apply the similarity metric g (eq 1).", "labels": [], "entities": [{"text": "latent topic vector (T)", "start_pos": 117, "end_pos": 140, "type": "METRIC", "confidence": 0.7563276141881943}, {"text": "DocNADE", "start_pos": 164, "end_pos": 171, "type": "DATASET", "confidence": 0.8999898433685303}]}, {"text": "To evaluate retrieval for q, we retrieve the top 10 similar tickets, ranked by the similarity scores on their topic vectors.", "labels": [], "entities": []}, {"text": "(#No) shows the performance of DocNADE for similarity and retrieval tasks.", "labels": [], "entities": []}, {"text": "Observe that #9 achieves the best MSE (3.502) and Acc@10 (0.40) out of, suggesting that the topic vectors of query (SUB1+DESC1) and historical ticket (SUB2+DESC2+SOL2) are the key in recommending a relevant SOL2.", "labels": [], "entities": [{"text": "MSE", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.6627466082572937}, {"text": "Acc@10 (0.40)", "start_pos": 50, "end_pos": 63, "type": "METRIC", "confidence": 0.9448813001314799}]}, {"text": "See the performance of DocNADE for all labeled pairs i.e. queries and historical tickets (P tr P dev ) in the.", "labels": [], "entities": []}, {"text": "(2) Distributional Semantics E: Beyond topic models, we establish baseline using the SumEMB method (section 2.1), where an embedding vector E is computed following the topic semantics approach.", "labels": [], "entities": []}, {"text": "The experiments #11-14 show that the SumEMB results in lower performance for both the tasks, suggesting a need of a supervised paradigm in order to learn similarities in asymmetric texts.", "labels": [], "entities": [{"text": "SumEMB", "start_pos": 37, "end_pos": 43, "type": "TASK", "confidence": 0.8084553480148315}]}, {"text": "Also, the comparison with DocNADE indicates that the topic features are important in the retrieval of tickets.", "labels": [], "entities": [{"text": "DocNADE", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9345846176147461}]}, {"text": "For semantic relatedness scoring, we train the Replicated Siamese, using backpropagation-through-time under the Mean Squared Error (MSE) loss function (after rescaling the training-set relatedness labels to lie \u0080).", "labels": [], "entities": [{"text": "semantic relatedness scoring", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.7954534490903219}, {"text": "Mean Squared Error (MSE) loss function", "start_pos": 112, "end_pos": 150, "type": "METRIC", "confidence": 0.9184466451406479}]}, {"text": "After training, we apply an additional non-parametric regression step to obtain bettercalibrated predictions \u0080, same as.", "labels": [], "entities": []}, {"text": "We then evaluate the trained model for IR task, where we retrieve the top 10 similar results (SUB2+DESC2+SOL2), ranked by their similarity scores, for each query (SUB1+DESC1) in the development set and compute MAP@K, MRR@K and Acc@K, where K=1, 5, and 10.", "labels": [], "entities": [{"text": "IR task", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9153522849082947}, {"text": "Acc", "start_pos": 227, "end_pos": 230, "type": "METRIC", "confidence": 0.9854848980903625}]}, {"text": "We use 300-dimensional pre-trained word2vec 3 embeddings for input words, however, to generalize beyond the limited vocabulary in word2vec due to industrial domain data with technical vocabulary, we also employ char-BLSTM () to generate additional embeddings (=50 dimension 4 ).The resulting dimension for word embeddings is 350.", "labels": [], "entities": []}, {"text": "We use 50-dimensional hidden vector, ht , memory cells, ct and Adadelta) with dropout and gradient clipping () for optimization.", "labels": [], "entities": []}, {"text": "The topics vector (T) size is 100.", "labels": [], "entities": [{"text": "topics vector (T) size", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.8864565988381704}]}, {"text": "We use python NLTK toolkit 5 for sentence tokenization.", "labels": [], "entities": [{"text": "sentence tokenization", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.6726191937923431}]}, {"text": "See for the hyperparameters in Replicated Siamese LSTM for experiment #No:22.", "labels": [], "entities": []}, {"text": "shows the similarity and retrieval scores for unsupervised and supervised baseline methods.", "labels": [], "entities": [{"text": "similarity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9843643307685852}]}, {"text": "The #9, #18 and #20 show that the supervised approach performs better than unsupervised topic models.", "labels": [], "entities": []}, {"text": "#17 and #19 suggest that the multi-level Siamese improves (Acc@10: 0.51 vs. 0.53) both STS and IR.", "labels": [], "entities": [{"text": "Acc", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9993859529495239}, {"text": "STS", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9987680315971375}, {"text": "IR", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.8424338698387146}]}, {"text": "Comparing #18 and #20, the cross-level Siamese shows performance gains (Acc@10: 0.55 vs. 0.57).", "labels": [], "entities": [{"text": "Acc", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9991993308067322}]}, {"text": "Finally, #21 and #22 demonstrates improved similarity (MSE: 2.354 vs. 2.052) and retrieval (Acc@10: 0.58 vs. 0.62) due to weighted multi-channel (h, E and T ) inputs.", "labels": [], "entities": [{"text": "similarity", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9942627549171448}, {"text": "MSE", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.5522091388702393}, {"text": "retrieval", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9774250984191895}, {"text": "Acc", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9844838976860046}]}, {"text": "We use the trained similarity model to retrieve the top 10 similar tickets from KB for each end-user query Q U , and compute the number of correct similar and relevant tickets.", "labels": [], "entities": []}, {"text": "For ticket ID q6, Middle), GT XXX Low Frequency Pulsation", "labels": [], "entities": [{"text": "GT XXX Low Frequency Pulsation", "start_pos": 27, "end_pos": 57, "type": "METRIC", "confidence": 0.6098639249801636}]}], "tableCaptions": [{"text": " Table 6: DocNADE (M 2) performance for the queries Q L \u0080 pP tr P dev q in the labeled pairs in unsu-", "labels": [], "entities": []}]}