{"title": [{"text": "Deep Models for Arabic Dialect Identification on Benchmarked Data", "labels": [], "entities": [{"text": "Arabic Dialect Identification", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.6784639855225881}]}], "abstractContent": [{"text": "The Arabic Online Commentary (AOC) (Zaidan and Callison-Burch, 2011) is a large-scale repository of Arabic dialects with manual labels for 4 varieties of the language.", "labels": [], "entities": [{"text": "Arabic Online Commentary (AOC) (Zaidan and Callison-Burch, 2011)", "start_pos": 4, "end_pos": 68, "type": "TASK", "confidence": 0.5514278434790097}]}, {"text": "Existing dialect identification models exploiting the dataset pre-date the recent boost deep learning brought to NLP and hence the data are not benchmarked for use with deep learning, nor is it clear how much neural networks can help tease the categories in the data apart.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.7654708027839661}]}, {"text": "We treat these two limitations: We (1) benchmark the data, and (2) empirically test 6 different deep learning methods on the task, comparing peformance to several classical machine learning models under different conditions (i.e., both binary and multi-way classification).", "labels": [], "entities": []}, {"text": "Our experimental results show that variants of (attention-based) bidirectional recurrent neural networks achieve best accuracy (acc) on the task, significantly outperforming all competitive baselines.", "labels": [], "entities": [{"text": "accuracy (acc)", "start_pos": 118, "end_pos": 132, "type": "METRIC", "confidence": 0.9298498034477234}]}, {"text": "On blind test data, our models reach 87.65% acc on the binary task (MSA vs. dialects), 87.4% acc on the 3-way dialect task (Egyptian vs. Gulf vs. Levantine), and 82.45% acc on the 4-way variants task (MSA vs. Egyptian vs. Gulf vs. Levantine).", "labels": [], "entities": []}, {"text": "We release our benchmark for future work on the dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dialect identification is a special type of language identification where the goal is to distinguish closely related languages.", "labels": [], "entities": [{"text": "Dialect identification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8272898495197296}, {"text": "language identification", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.762855052947998}]}, {"text": "Explosion of communication technologies and the accompanying pervasive use of social media strongly motivates need for technologies like language, and dialect, identification.", "labels": [], "entities": []}, {"text": "These technologies are useful for applications ranging from monitoring health and well-being (, to real-time disaster operation management (, and analysis of human mobility).", "labels": [], "entities": [{"text": "real-time disaster operation management", "start_pos": 99, "end_pos": 138, "type": "TASK", "confidence": 0.615624189376831}, {"text": "analysis of human mobility", "start_pos": 146, "end_pos": 172, "type": "TASK", "confidence": 0.765808179974556}]}, {"text": "Language identification is also an enabling technology that can help automatically filter foreign text in some tasks, acquire multilingual data (e.g., from the web) (, including to enhance tasks like machine translation (.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7098781615495682}, {"text": "machine translation", "start_pos": 200, "end_pos": 219, "type": "TASK", "confidence": 0.8095606565475464}]}, {"text": "In this paper our focus is on Arabic, a term that refers to a wide collection of varieties.", "labels": [], "entities": []}, {"text": "These varieties are the result of the interweave between the native languages of the Middle East and North Africa and Arabic itself.", "labels": [], "entities": []}, {"text": "Modern Standard Arabic (MSA), the modern variety of the language used in pan-Arab news outlets like AlJazeera and in educational circles in the Arab world, differs phonetically, phonologically, lexically, and syntactically from the varieties spoken in everyday communication by native speakers of the language ().", "labels": [], "entities": []}, {"text": "These 'everyday' varieties constitute the dialects of Arabic.", "labels": [], "entities": []}, {"text": "Examples of these are Egyptian (EGY), Gulf (GLF), Levantine (LEV), and Moroccan (MOR).", "labels": [], "entities": []}, {"text": "In addition to MSA and dialects, Classical Arabic also exists and is the variety of historical literary texts and religious discourse.", "labels": [], "entities": []}, {"text": "Language varieties, including those of Arabic, can be categorized based on shared linguistic features.", "labels": [], "entities": []}, {"text": "For Arabic, one classical categorization is based on geographical locations.", "labels": [], "entities": []}, {"text": "For example, in addition to MSA,, provides 5 main categories, as shown in.", "labels": [], "entities": [{"text": "MSA", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.8245099782943726}]}, {"text": "For along time, Arabic dialects remained mostly spoken.", "labels": [], "entities": []}, {"text": "Dialects started to find their way in written form with the spread of social media, thus affording an opportunity for researchers to use these data for NLP.", "labels": [], "entities": []}, {"text": "This motivated to create a large-scale repository of Arabic texts, the Arabic Online Commentary (AOC).", "labels": [], "entities": [{"text": "Arabic Online Commentary (AOC)", "start_pos": 71, "end_pos": 101, "type": "TASK", "confidence": 0.5397833635409673}]}, {"text": "The resource is composed of \u223c 3M MSA and dialectal comments on a number of Arabic news sites.", "labels": [], "entities": []}, {"text": "A portion of the data (> 108K comments) is manually annotated via crowdsourcing.", "labels": [], "entities": []}, {"text": "The dataset was exploited for dialect identification in and later in.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.7788005173206329}]}, {"text": "These works, however, pre-date the current boom in NLP where deep neural networks enable better learning (given sufficiently large training data).", "labels": [], "entities": []}, {"text": "use n-fold cross validation in their work, thus making it costly to adopt the same data split procedure to develop deep learning models.", "labels": [], "entities": []}, {"text": "This is the case since deep models can take long times to train and optimize.", "labels": [], "entities": []}, {"text": "For this reason, it is desirable to benchmark the AOC dataset for deep learning research.", "labels": [], "entities": [{"text": "AOC dataset", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.9130421280860901}]}, {"text": "We also ask the empirical question: To what extent can we tease apart the Arabic varieties in AOC using neural networks.", "labels": [], "entities": [{"text": "AOC", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.920284628868103}]}, {"text": "Especially given (a) the morphological richness of Arabic and (b) the inter-relatedness (e.g., lexical overlap) between Arabic varieties, it is not clear how accurately these varieties can be automatically categorized (using deep learning methods).", "labels": [], "entities": []}, {"text": "To answer these important questions, we investigate the utility of several traditional machine learning classifiers and 6 different deep learning models on the task.", "labels": [], "entities": []}, {"text": "Our deep models are based on both recurrent neural networks and convolutional neural networks, as well as combinations (and variations) of these.", "labels": [], "entities": []}, {"text": "Overall, we offer the following contributions: (1) We benchmark the AOC dataset, especially for deep learning work, (2) we perform extensive experiments based on deep neural networks for identifying the 4 Arabic varieties in AOC under various classification conditions, allowing us to perform well on the task, and (3) we carryout an analysis to uncover how the varieties in the data relate to one another based on shared lexica.", "labels": [], "entities": [{"text": "AOC dataset", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.9692626893520355}]}, {"text": "The rest of the paper is organized as follows: In Section 2 we review related work, in Section 3 we briefly describe the AOC dataset.", "labels": [], "entities": [{"text": "AOC dataset", "start_pos": 121, "end_pos": 132, "type": "DATASET", "confidence": 0.8940910995006561}]}, {"text": "In Section 4 we describe our models, Section 5 is Go, go, our team; you've our passionate support.", "labels": [], "entities": []}, {"text": "Frankly, I'm speechless.", "labels": [], "entities": []}, {"text": "Neither ignorance nor any other reason justify this action.", "labels": [], "entities": [{"text": "ignorance", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9545282125473022}]}], "datasetContent": [{"text": "As we mentioned earlier, our work is based on the AOC dataset.", "labels": [], "entities": [{"text": "AOC dataset", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.9859095513820648}]}, {"text": "AOC is composed of 3M MSA and dialectal comments, of which 108, 173 comments are labeled via crowdsourcing.", "labels": [], "entities": [{"text": "AOC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8615256547927856}]}, {"text": "For our experiments, we randomly shuffle the dataset and split it into 80% training (Train), 10% validation (Dev), and 10% test (Test).", "labels": [], "entities": []}, {"text": "shows the distribution of the data across the different splits.", "labels": [], "entities": []}, {"text": "We were interested in identifying how the 4 varieties relate to one another in terms of their shared vocabulary, and so we performed an analysis on the training split (Train) as shown in the heat map in.", "labels": [], "entities": []}, {"text": "The Figure presents the percentages of shared vocabulary between the different varieties after normalizing for the number of data points in each class.", "labels": [], "entities": []}, {"text": "As the Figure shows, both the GLF and LEV dialects are lexically closer to (i.e., share more vocabulary with) MSA than EGY is (does).", "labels": [], "entities": [{"text": "GLF", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.8178026080131531}]}, {"text": "This finding is aligned with the intuition of native speakers of Arabic that EGY diverges more from MSA than the GLF and LEV varieties.", "labels": [], "entities": []}, {"text": "This empirical finding lends some credibility to this intuition.", "labels": [], "entities": []}, {"text": "We perform 3 different classification tasks: (A) binary classification, where we tease apart the MSA and the dialectal data, (B) 3-way dialects, where we attempt to distinguish between EGY, GLF, and LEV; and (C) 4-way variants (i.e., MSA vs. EGY vs. GLF vs. LEV).", "labels": [], "entities": [{"text": "binary classification", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.7124655544757843}]}, {"text": "Since our goal in this work is to explore how several popular traditional settings and deep learning model architectures fare on the dialect identification task, we use classifiers with pre-defined hyper-parameters inspired by previous works as described in Section 4.", "labels": [], "entities": [{"text": "dialect identification task", "start_pos": 133, "end_pos": 160, "type": "TASK", "confidence": 0.8424876928329468}]}, {"text": "As we mention in Section 3, we split the data into 80% Train, 10% Dev, and 10% Test.", "labels": [], "entities": [{"text": "Dev", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9635404348373413}, {"text": "Test", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.945235550403595}]}, {"text": "While we train on Train and report results on both the Dev and Test sets in the current work, our goal is to invest on hyper-parameter tuning based on the development set in the future.", "labels": [], "entities": []}, {"text": "Benchmarking the data is thus helpful as it facilitates comparisons in future works.", "labels": [], "entities": []}, {"text": "We have two settings for the traditional classifiers: (1) presence vs. absence (0 vs. 1) vectors based on combinations of unigrams, bigrams, and trigrams; and (2) term-frequency inverse-document-frequency (TF-IDF) vectors based on combinations of unigrams, bigrams, and trigrams.", "labels": [], "entities": []}, {"text": "We use scikit-learn's (Pedregosa et al., 2011) implementation of these classifiers.", "labels": [], "entities": []}, {"text": "All our deep models are trained for 10 epochs using the RMSprop optimizer.", "labels": [], "entities": []}, {"text": "The model's weights Ware initialized from a normal distribution W \u223c N with a small standard deviation of \u03c3 = 0.05.", "labels": [], "entities": []}, {"text": "Our models are trained using the Keras (Chollet and others, 2015) library with a Tensorflow ( backend.", "labels": [], "entities": [{"text": "Keras (Chollet and others, 2015) library", "start_pos": 33, "end_pos": 73, "type": "DATASET", "confidence": 0.7575432923105028}]}, {"text": "We train each of our 6 deep learning classifiers across 3 different settings pertaining the way we initialize the embeddings for the input layer in each network.", "labels": [], "entities": []}, {"text": "The three embedding settings are: 1.", "labels": [], "entities": []}, {"text": "Random embeddings: Where we initialize the input layer randomly.", "labels": [], "entities": []}, {"text": "2. AOC-based embeddings: We make use of the \u223c 3M unlabeled comments in AOC by training a \"continuous bag of words\" (CBOW) ( shows our results inaccuracy across the three classification tasks (i.e., binary, 3-way, and 4-way), as described in Section 5.", "labels": [], "entities": [{"text": "AOC", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.9000464081764221}]}, {"text": "Our baseline in each task is the majority class in the respective Train set.", "labels": [], "entities": []}, {"text": "As shows, among traditional models, the Naive Bayes classifier achieves the best performance across all three tasks both on Dev and Test data.", "labels": [], "entities": []}, {"text": "As a sole exception, SVMs outperforms Naive Bayes on the Test set for the 4-way classification task.", "labels": [], "entities": [{"text": "Test set", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.925527960062027}, {"text": "4-way classification task", "start_pos": 74, "end_pos": 99, "type": "TASK", "confidence": 0.7019967635472616}]}, {"text": "As best accuracy, traditional classifiers yield 84.53 (binary), 87.81 (3-way), and 78.61 (4-way) on the Test splits.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9993537068367004}, {"text": "Test splits", "start_pos": 104, "end_pos": 115, "type": "DATASET", "confidence": 0.941772073507309}]}], "tableCaptions": [{"text": " Table 2: Distribution of classes in our AOC Train split", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9737102389335632}, {"text": "AOC Train", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.9015896916389465}]}, {"text": " Table 3: Experimental results, in accuracy, on our Dev and Test AOC splits", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9995315074920654}, {"text": "Dev and Test AOC", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.6218401342630386}]}]}