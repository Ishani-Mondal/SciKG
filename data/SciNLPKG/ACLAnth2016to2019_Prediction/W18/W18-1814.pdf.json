{"title": [{"text": "A Dataset and Reranking Method for Multimodal MT of User-Generated Image Captions", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.8108747005462646}, {"text": "User-Generated Image Captions", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.6149967014789581}]}], "abstractContent": [{"text": "We present a dataset and method for improving the translation of noisy image captions that were created by users of Wikimedia Commons.", "labels": [], "entities": [{"text": "translation of noisy image captions", "start_pos": 50, "end_pos": 85, "type": "TASK", "confidence": 0.8443790435791015}]}, {"text": "The dataset is multilingual but non-parallel, and is several orders of magnitude larger than existing parallel data for multimodal machine translation.", "labels": [], "entities": [{"text": "multimodal machine translation", "start_pos": 120, "end_pos": 150, "type": "TASK", "confidence": 0.6291609009106954}]}, {"text": "Our retrieval-based method pivots on similar images and uses the associated captions in the target language to rerank translation outputs.", "labels": [], "entities": []}, {"text": "This method only requires small amounts of parallel captions to find the optimal ensemble of retrieval features based on textual and visual similarity.", "labels": [], "entities": []}, {"text": "Furthermore, our method is compatible with any machine translation system, and allows to quickly integrate new data without the need of retraining the translation system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.742074191570282}]}, {"text": "Tests on three different datasets showed that size and diversity of the data is crucial for the performance of our method.", "labels": [], "entities": []}, {"text": "On the introduced dataset we observe consistent improvements of up to 5 BLEU points and 3 points in Character F-score over strong neural MT baselines for three different language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9991621971130371}, {"text": "F-score", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.4831653833389282}]}], "introductionContent": [{"text": "Image caption translation is the task of translating a caption associated with an image into another language.", "labels": [], "entities": [{"text": "Image caption translation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8184101680914561}, {"text": "translating a caption associated with an image", "start_pos": 41, "end_pos": 87, "type": "TASK", "confidence": 0.7487296462059021}]}, {"text": "What differentiates this task from purely text-based machine translation is the incorporation of image information into the translation process.", "labels": [], "entities": [{"text": "text-based machine translation", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.7456969022750854}]}, {"text": "Images associated with text usually add anew modality of information.", "labels": [], "entities": []}, {"text": "Such information helps to ground the meaning of the corresponding text and is thus especially useful in a translation task.", "labels": [], "entities": [{"text": "translation", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.979242205619812}]}, {"text": "Interest in this task has surged since the first instantiation of the shared task on multimodal machine translation where a dataset of 30,000 German translations of crowdsourced English captions was presented).", "labels": [], "entities": [{"text": "multimodal machine translation", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.6244247357050577}]}, {"text": "However, this dataset has limitations: The captions were created by human annotators that were guided to produce \"conceptual\" descriptions that identify the objects depicted in the image (.", "labels": [], "entities": []}, {"text": "This leads to relatively short captions amounting to a comparatively easy translation task with little room for improvement by incorporating visual information.", "labels": [], "entities": [{"text": "translation", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.9710251092910767}]}, {"text": "This is confirmed by recent results showing that improvements over a text-only MT baseline are inconsistent and hard to achieve.", "labels": [], "entities": [{"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.8416193127632141}]}, {"text": "While caption translation in previous work has been conducted solely on clean, manually labeled captions based on MS COCO (),, or its multilingual variant, the goal of our work is to lift multimodal caption translation to a more realistic setup.", "labels": [], "entities": [{"text": "caption translation", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.9682033658027649}, {"text": "MS COCO", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.7876540124416351}, {"text": "multimodal caption translation", "start_pos": 188, "end_pos": 218, "type": "TASK", "confidence": 0.6942747235298157}]}, {"text": "For this purpose, we extracted a dataset of 4M \"cap-tions in the wild\" as they appear in the user-generated Wikimedia Commons database.", "labels": [], "entities": [{"text": "Wikimedia Commons database", "start_pos": 108, "end_pos": 134, "type": "DATASET", "confidence": 0.8227835893630981}]}, {"text": "This new dataset is very different to previous image-caption data, as it contains highly diverse types of user-generated texts associated with images.", "labels": [], "entities": []}, {"text": "The English captions in this dataset are around 34 tokens long, compared to 11 and 14 for MS COCO and Multi30k, respectively.", "labels": [], "entities": [{"text": "MS COCO", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.8584607243537903}]}, {"text": "Caption translation of Wikimedia Commons data thus contrasts to previous image-caption translation tasks.", "labels": [], "entities": [{"text": "Caption translation of Wikimedia Commons", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.9023464322090149}, {"text": "image-caption translation", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.7238993495702744}]}, {"text": "However, we find the new dataset to provide a lot of room for improvement by incorporating visual information into the translation process.", "labels": [], "entities": []}, {"text": "The dataset is described in Section 4.", "labels": [], "entities": []}, {"text": "Since the dataset only contains very small subsets of parallel captions (which we use for tuning and testing), the proper way to integrate visual information is to leverage monolingual image-caption pairs.", "labels": [], "entities": []}, {"text": "presented an approach based on a crosslingual reranking framework where monolingual captions in the target language are used to rerank translation hypotheses given a source caption and the corresponding image.", "labels": [], "entities": []}, {"text": "In order to retrieve captions for reranking, they pivot on target language image-captions pairs in two ways: A list of monolingual captions is obtained by a joint textual and visual similarity model by comparison between the hypotheses and the captions in the target language.", "labels": [], "entities": []}, {"text": "To calculate the visual similarity component of their joint model, they use rich image feature representations from a convolutional neural network.", "labels": [], "entities": []}, {"text": "Our approach is an extension of, who rely on manually tuned hyperparameters, to a pairwise ranking approach to learn an optimal ensemble of different rerankers.", "labels": [], "entities": []}, {"text": "We also implement separate textual and visual similarity components to incorporate them as distinct features into our reranking model.", "labels": [], "entities": []}, {"text": "Furthermore, we investigate a stronger text-only baseline that is based on neural MT (NMT).", "labels": [], "entities": []}, {"text": "Our translation and reranking methods are described in Section 3.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.980749249458313}]}, {"text": "We present an evaluation on caption data from Wikimedia Commons.", "labels": [], "entities": [{"text": "caption", "start_pos": 28, "end_pos": 35, "type": "TASK", "confidence": 0.956316351890564}]}, {"text": "We find gains of 5 BLEU points () and 3 points in Character F-score by reranking over strong NMT baselines across three different language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9992935657501221}, {"text": "F-score", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.49433633685112}]}, {"text": "In order to discern the contribution of our new learning method, we compare our approach to the only other monolingual reranking approach that we are aware of, namely.", "labels": [], "entities": []}, {"text": "On the MS COCO data, we observe gains by neural MT over phrase-based MT, and small but consistent gains by reranking.", "labels": [], "entities": [{"text": "MS COCO data", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.9140429099400839}]}, {"text": "We also evaluate our approach on the Multi30k () dataset that was used for the WMT17 Multimodal Shared Task 2 (.", "labels": [], "entities": [{"text": "Multi30k () dataset", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.7632943590482076}, {"text": "WMT17 Multimodal Shared Task 2", "start_pos": 79, "end_pos": 109, "type": "DATASET", "confidence": 0.7477840065956116}]}, {"text": "Due to the limited size of data available for retrieval we found no significant improvements over the NMT baseline here.", "labels": [], "entities": [{"text": "NMT baseline", "start_pos": 102, "end_pos": 114, "type": "DATASET", "confidence": 0.8536894917488098}]}, {"text": "Our experiments indicate a strong dependency of our approach's performance on the type and size of retrieval data.", "labels": [], "entities": []}, {"text": "The experiments are described in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparison between Multi30k, MS COCO and Wikimedia Commons data.", "labels": [], "entities": [{"text": "Multi30k", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9491878747940063}, {"text": "MS COCO", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.8762490451335907}, {"text": "Wikimedia Commons data", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.7687131762504578}]}, {"text": " Table 4: BLEU and Character F-scores on German-English Multi30k test data from the WMT17  Multimodal Task 2 (Elliott et al., 2017). Due to the limited data available for retrieval our  approach did now show improvements over the nematus baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992105960845947}, {"text": "German-English Multi30k test data", "start_pos": 41, "end_pos": 74, "type": "DATASET", "confidence": 0.8222667872905731}, {"text": "WMT17  Multimodal Task 2", "start_pos": 84, "end_pos": 108, "type": "DATASET", "confidence": 0.7713353782892227}]}, {"text": " Table 5: BLEU and Character F-scores (ChF) on de-en MS COCO test data from Hitschler  et al.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988781809806824}, {"text": "Character F-scores (ChF)", "start_pos": 19, "end_pos": 43, "type": "METRIC", "confidence": 0.875121808052063}, {"text": "MS COCO test data from Hitschler  et al", "start_pos": 53, "end_pos": 92, "type": "DATASET", "confidence": 0.8088821209967136}]}, {"text": " Table 6: BLEU and Character F-scores (ChF) on Wikimedia test data. Best scoring systems for  each language pair were selected on the dev set and are printed in bold. For BLEU, the preced- ing  \u2020 indicates a significant improvement (p < 0.005) over the baseline system as reported by  MultEval's randomization test", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9938669800758362}, {"text": "Character F-scores (ChF)", "start_pos": 19, "end_pos": 43, "type": "METRIC", "confidence": 0.7166602611541748}, {"text": "Wikimedia test data", "start_pos": 47, "end_pos": 66, "type": "DATASET", "confidence": 0.828393300374349}, {"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9124864339828491}, {"text": "preced- ing  \u2020", "start_pos": 181, "end_pos": 195, "type": "METRIC", "confidence": 0.865448534488678}, {"text": "MultEval's randomization test", "start_pos": 285, "end_pos": 314, "type": "DATASET", "confidence": 0.8923499286174774}]}, {"text": " Table 8: Influence of hypothesis lists length n on Character F-score for the reranking experiment  on MS COCO dev data where we applied the new reranker on the original cdec's hypothesis  lists. Numbers in bold indicate highest score above the baseline (59.64) within a column.", "labels": [], "entities": [{"text": "MS COCO dev data", "start_pos": 103, "end_pos": 119, "type": "DATASET", "confidence": 0.9194903820753098}]}, {"text": " Table 9: Hypothesis lists length n and Character F-score on Multi30k dev data. No combination  of rerankers was able to surpass the baseline (49.35) on this dataset. Note that increasing the  hypothesis lists length always leads to degradation, because the system is unable to identify  better translations in the list.", "labels": [], "entities": [{"text": "Character F-score", "start_pos": 40, "end_pos": 57, "type": "METRIC", "confidence": 0.846626341342926}, {"text": "Multi30k dev data", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.8956896861394247}]}, {"text": " Table 10: Hypothesis lists length n and Character F-score on MS COCO dev data. Numbers in  bold indicate highest score above the baseline (64.76) within a column.", "labels": [], "entities": [{"text": "length", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9835051894187927}, {"text": "Character F-score", "start_pos": 41, "end_pos": 58, "type": "METRIC", "confidence": 0.8903807699680328}, {"text": "MS COCO dev data", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.9200281798839569}]}]}