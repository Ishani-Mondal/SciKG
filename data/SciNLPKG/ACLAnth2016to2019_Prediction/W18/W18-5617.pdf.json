{"title": [{"text": "Syntax-based Transfer Learning for the Task of Biomedical Relation Extraction", "labels": [], "entities": [{"text": "Syntax-based Transfer Learning", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7716788053512573}, {"text": "Biomedical Relation Extraction", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.8413260579109192}]}], "abstractContent": [{"text": "Transfer learning (TL) proposes to enhance machine learning performance on a problem, by reusing labeled data originally designed fora related problem.", "labels": [], "entities": [{"text": "Transfer learning (TL)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9538449168205261}]}, {"text": "In particular, domain adaptation consists, fora specific task, in reusing training data developed for the same task but a distinct domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7357113659381866}]}, {"text": "This is particularly relevant to the applications of deep learning in Natural Language Processing, because those usually require large annotated corpora that may not exist for the targeted domain, but exist for side domains.", "labels": [], "entities": []}, {"text": "In this paper, we experiment with TL for the task of Relation Extraction (RE) from biomedical texts, using the TreeLSTM model.", "labels": [], "entities": [{"text": "TL", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.7197425961494446}, {"text": "task of Relation Extraction (RE) from biomedical texts", "start_pos": 45, "end_pos": 99, "type": "TASK", "confidence": 0.7989642918109894}]}, {"text": "We empirically show the impact of TreeLSTM alone and with domain adaptation by obtaining better performances than the state of the art on two biomedical RE tasks and equal performances for two others, for which few annotated data are available.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7231538593769073}]}, {"text": "Furthermore, we propose an analysis of the role that syntactic features may play in TL for RE.", "labels": [], "entities": [{"text": "TL", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9622087478637695}, {"text": "RE", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.7891517877578735}]}], "introductionContent": [{"text": "A bottleneck problem for training deep learningbased architecture on text is the availability of large enough annotated training corpora.", "labels": [], "entities": []}, {"text": "This is especially an issue in highly specialized domains such as those of biomedicine.", "labels": [], "entities": []}, {"text": "TL approaches address this problem by leveraging existing labeled data originally designed for related tasks or domains (.", "labels": [], "entities": []}, {"text": "However, adaptation between dissimilar domains may lead to negative transfer, i.e. transfer that decreases the performance for the target domain.", "labels": [], "entities": []}, {"text": "In this article, we apply a TL strategy using the TreeLSTM model for the task of biomedical Relation Extraction (RE).", "labels": [], "entities": [{"text": "TL", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9498623609542847}, {"text": "biomedical Relation Extraction (RE)", "start_pos": 81, "end_pos": 116, "type": "TASK", "confidence": 0.8550736606121063}]}, {"text": "We propose an analysis of the syntactic features of source and target domain corpora to provide elements of interpretation for the improvements we obtained.", "labels": [], "entities": []}, {"text": "Relation Extraction (RE) aims at identifying in raw and unstructured text all the instances of a predefined set of relations between identified entities.", "labels": [], "entities": [{"text": "Relation Extraction (RE)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9242430090904236}]}, {"text": "A relationship takes the form of an edge between two or more named entities as illustrated in.", "labels": [], "entities": []}, {"text": "We are considering here binary RE that can be seen as a classification task by computing a score for each possible relation type, given a sentence and two identified entities.", "labels": [], "entities": []}, {"text": "Deep learning methods have demonstrated good ability for RE (), but one of their drawbacks is that, in order to obtain reasonable performances, they generally require a large amount of training data, i.e., text corpora where entities and relationships between them are annotated.", "labels": [], "entities": [{"text": "RE", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.8485409617424011}]}, {"text": "The assembly of this kind of domain-and task-specific corpora, such as those of interest in biomedicine, is time consuming and expensive because it involves complex entities (e.g., genomic variations, complex phenotypes), complex relationships (which maybe hypothetical, contextualized, negated, n-ary) and requires trained annotators.", "labels": [], "entities": []}, {"text": "This explains why only few and relatively small (i.e., few hundreds of sentences) corpora are available for some biomedical RE tasks, making these resources particularly valuable.", "labels": [], "entities": [{"text": "RE tasks", "start_pos": 124, "end_pos": 132, "type": "TASK", "confidence": 0.8011559247970581}]}, {"text": "Distinct approaches, such as TL or distant supervision) have been particularly explored to overcome this limit.", "labels": [], "entities": [{"text": "TL", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.7986327409744263}]}, {"text": "With the latter approach, existing relationships available in knowledge-or data-bases are used to enrich the training set, with-out considering more labeled corpora . Domain adaptation is a type of TL that allows taking advantage of data annotated fora source domain to improve the performances in a related target domain (.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 167, "end_pos": 184, "type": "TASK", "confidence": 0.7423989176750183}]}, {"text": "However, even if the source and target domain share the same language (i.e., English), thus a common syntax, TL between domains may lead to negative transfer since specific source domains may use specific vocabularies as well as specific formulations that are inadequate to the target domain.", "labels": [], "entities": [{"text": "TL", "start_pos": 109, "end_pos": 111, "type": "TASK", "confidence": 0.9679844975471497}]}, {"text": "Hence, we need to better understand and characterize what makes a source corpus potentially helpful, or harmful, with regard to a RE task.", "labels": [], "entities": [{"text": "RE task", "start_pos": 130, "end_pos": 137, "type": "TASK", "confidence": 0.9307493269443512}]}, {"text": "The contribution of this paper is twofold.", "labels": [], "entities": []}, {"text": "First, we show that, compared to a baseline Convolutional Neural Network (CNN)-based model, a syntax-based model (i.e., the TreeLSTM model) can better benefit from a TL strategy, even with very dissimilar additional source data.", "labels": [], "entities": []}, {"text": "We conduct our experiments with two biomedical RE tasks and relatively small associated corpora, SNPPhenA and EU-ADR (van as target corpora and three larger RE corpora,, ADE-EXT (), reACE) as source corpora.", "labels": [], "entities": [{"text": "reACE", "start_pos": 182, "end_pos": 187, "type": "METRIC", "confidence": 0.8198436498641968}]}, {"text": "Second, we propose a syntax-based analysis, using both quantitative criteria and qualitative observations, to better understand the role of syntactic features in the TL behavior.", "labels": [], "entities": [{"text": "TL behavior", "start_pos": 166, "end_pos": 177, "type": "TASK", "confidence": 0.9342772364616394}]}], "datasetContent": [{"text": "We explore how RE tasks that focus on a type of relationship associated with scarce resources may take advantage from larger corpora developed for distinct domains.", "labels": [], "entities": [{"text": "RE tasks", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.9255478978157043}]}, {"text": "To this purpose, we selected (i) two small target biomedical corpora and (ii) three larger source corpora.", "labels": [], "entities": []}, {"text": "All are publicly available and detailed in the following section.", "labels": [], "entities": []}, {"text": "Our models were trained by minimizing the loglikelihood over the training data.", "labels": [], "entities": []}, {"text": "All parameters (weights, biases and embeddings) were iteratively updated via backpropagation for the MCCNN and backpropagation Through Structure for the TreeL-STM.", "labels": [], "entities": [{"text": "MCCNN", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.921047031879425}]}, {"text": "Hyper-parameters were tuned using a 10-fold cross-validation by selecting the values leading to the best averaged performance, and fixed for the remaining experiments.", "labels": [], "entities": []}, {"text": "Word embeddings were pre-trained o\u00f1 3.4 million PubMed abstracts (corresponding to all those published between Jan. 1, 2014 and Dec. 31, 2016) using the method described in.", "labels": [], "entities": []}, {"text": "Following Kim (2014) both channels were initialized with pre-trained word embeddings, but gradients were backpropagated only through one of the channels.", "labels": [], "entities": []}, {"text": "Hyperparameters were fixed to d w = 100, d e = 10, d h = 100 for each of the 2 channels, d s = 2 \u00d7 d h = 200.", "labels": [], "entities": []}, {"text": "We used two kernels of size 3 and 5 respectively.", "labels": [], "entities": []}, {"text": "We applied a dropout regularization after the embedding layers () with a dropout probability fixed to 0.25.: Results of our TL strategy in terms of precision (P), recall (R) and f-measure (F).", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 148, "end_pos": 161, "type": "METRIC", "confidence": 0.9484143108129501}, {"text": "recall (R)", "start_pos": 163, "end_pos": 173, "type": "METRIC", "confidence": 0.9352490454912186}, {"text": "f-measure (F)", "start_pos": 178, "end_pos": 191, "type": "METRIC", "confidence": 0.9289501458406448}]}, {"text": "\u03c3 F is the standard deviation of the f-measure.", "labels": [], "entities": [{"text": "F", "start_pos": 2, "end_pos": 3, "type": "METRIC", "confidence": 0.8005878329277039}]}, {"text": "The + in the column Train corpus indicates that we trained our model using the target corpus plus one additional source corpus.", "labels": [], "entities": []}, {"text": "Dependency trees were derived from parsing trees obtained using the Charniak-Johnson parser trained on GENIA and PubMed data.", "labels": [], "entities": [{"text": "GENIA", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.9678126573562622}, {"text": "PubMed data", "start_pos": 113, "end_pos": 124, "type": "DATASET", "confidence": 0.8978231251239777}]}, {"text": "Hyper-parameters were fixed to d w = 100, d e = 10, d h = 200 and d s = 200.", "labels": [], "entities": []}, {"text": "We applied a dropout regularization after every TreeLSTM unit and after the embedding layers.", "labels": [], "entities": []}, {"text": "The dropout probability was fixed to 0.25.", "labels": [], "entities": [{"text": "dropout probability", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9043125212192535}]}, {"text": "All the parameters are initialized randomly except the word embeddings.", "labels": [], "entities": []}, {"text": "We evaluated performances in terms of precision (P), recall (R) and f-measure (F).", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 38, "end_pos": 51, "type": "METRIC", "confidence": 0.9362180382013321}, {"text": "recall (R)", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.951423779129982}, {"text": "f-measure (F)", "start_pos": 68, "end_pos": 81, "type": "METRIC", "confidence": 0.9458518922328949}]}, {"text": "For multilabel classifications, we report the macro-average performance . For SNPPhenA, we performed a cross-validation using 10% of the corpus for the validation and the provided test corpus for testing (which is about 30% the size of the training cor- The macro-average metric is less impacted by classes with few test instances (and thus a high variance).", "labels": [], "entities": []}, {"text": "For this reason, it is more representative of the performance of our model.", "labels": [], "entities": []}, {"text": "Because no test corpus is provided with EU-ADR, we performed a 10-fold cross-validation using 10% of the corpus for the validation and 10% for the test of our models.", "labels": [], "entities": [{"text": "EU-ADR", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.9201321005821228}]}, {"text": "In this subsection, we present our TL strategy and its results.", "labels": [], "entities": [{"text": "TL", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9806344509124756}]}, {"text": "Following a standard practice in deep learning, the transfer learning is done by training models in parallel while using shared representations, as illustrated by).", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.9076361954212189}]}, {"text": "In other terms, for each experiment, the same network, initialized with random weights, is used for each corpus (i.e., same embedding layer and TreeLSTM weights), except for the scorer, which is adapted to each corpus as the number and types of relationships may change.", "labels": [], "entities": []}, {"text": "During the training phase, using a standard stochastic gradient descent procedure, we randomly pick training sentences from the mixed corpus (i.e., target + one source training corpora).: Performance comparison between the state of the art ( and this work in terms of precision (P), recall (R) and F-measure (F).", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 268, "end_pos": 281, "type": "METRIC", "confidence": 0.9412928968667984}, {"text": "recall (R)", "start_pos": 283, "end_pos": 293, "type": "METRIC", "confidence": 0.9501826465129852}, {"text": "F-measure (F)", "start_pos": 298, "end_pos": 311, "type": "METRIC", "confidence": 0.9607493281364441}]}, {"text": "Results reported for this work are ensembles of the 5 best models obtained.", "labels": [], "entities": []}, {"text": "This training procedure is done, starting from different random initialization for each fold of our cross-validation.", "labels": [], "entities": []}, {"text": "presents the results of the TL study.", "labels": [], "entities": [{"text": "TL", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.8333407640457153}]}, {"text": "Each results is an average of 100 experiment (10 experiments for each fold starting from different random initialization).", "labels": [], "entities": []}, {"text": "We observed that for the TreeLSTM model, additional source corpora consistently improved the performances.", "labels": [], "entities": []}, {"text": "More interestingly, this phenomenon occurs even for corpora of distinct types of entities such as the combination of SNPPhenA and SemEval 2013 DDI and, to a lesser extend, with the corpus that is outside of the biomedical domain, reACE.", "labels": [], "entities": [{"text": "SemEval 2013 DDI", "start_pos": 130, "end_pos": 146, "type": "DATASET", "confidence": 0.7484372655550638}]}, {"text": "We note that the pre-trained embeddings were obtained using biomedical sources.", "labels": [], "entities": []}, {"text": "This may affect the TL performance with reACE that is not of the biomedical domain.", "labels": [], "entities": [{"text": "TL", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.8208425641059875}]}, {"text": "Also, we did not observed any benefit of the TL strategy for the MCCNN model, which performances decrease slightly in comparison with the baseline experiments.", "labels": [], "entities": [{"text": "MCCNN model", "start_pos": 65, "end_pos": 76, "type": "DATASET", "confidence": 0.8252744972705841}]}, {"text": "presents a comparison of performances obtained with our approach versus two state-ofthe-art systems applied to the RE tasks associated respectively with SNPPhenA (Bokharaeian et al., 2017) and EU-ADR ().", "labels": [], "entities": [{"text": "RE tasks", "start_pos": 115, "end_pos": 123, "type": "TASK", "confidence": 0.8060587644577026}, {"text": "EU-ADR", "start_pos": 193, "end_pos": 199, "type": "DATASET", "confidence": 0.900144636631012}]}], "tableCaptions": [{"text": " Table 1: Main characteristics of our target and source corpora. Two corpora are divided into subcorpora. The sizes  of the training and test corpora are reported in term of number of sentences (sent.) and annotated relationships  (rel.). EU-ADR, ADR-EXT and reACE have no proper test corpus.", "labels": [], "entities": [{"text": "EU-ADR", "start_pos": 239, "end_pos": 245, "type": "DATASET", "confidence": 0.6415433883666992}, {"text": "reACE", "start_pos": 259, "end_pos": 264, "type": "METRIC", "confidence": 0.7823174595832825}]}, {"text": " Table 2: Results of our TL strategy in terms of precision (P), recall (R) and f-measure (F). \u03c3 F is the standard  deviation of the f-measure. The + in the column Train corpus indicates that we trained our model using the target  corpus plus one additional source corpus.", "labels": [], "entities": [{"text": "TL", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9798226952552795}, {"text": "precision (P)", "start_pos": 49, "end_pos": 62, "type": "METRIC", "confidence": 0.9207195490598679}, {"text": "recall (R)", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9448915719985962}, {"text": "f-measure (F)", "start_pos": 79, "end_pos": 92, "type": "METRIC", "confidence": 0.8222856223583221}, {"text": "Train corpus", "start_pos": 163, "end_pos": 175, "type": "DATASET", "confidence": 0.8001229763031006}]}, {"text": " Table 4: Cosine similarity score between target and  source corpora for the three different pattern distribu- tions. POS is part of speech pattern and DT is depen- dency type pattern.", "labels": [], "entities": [{"text": "Cosine similarity score", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8163873553276062}]}, {"text": " Table 5: Dictionary coverage. Percentage of words  from the target copora present in the source corpora.", "labels": [], "entities": []}]}