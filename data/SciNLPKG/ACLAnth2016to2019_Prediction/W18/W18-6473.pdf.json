{"title": [{"text": "STACC, OOV Density and N-gram Saturation: Vicomtech's Participation in the WMT 2018 Shared Task on Parallel Corpus Filtering", "labels": [], "entities": [{"text": "STACC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.4681963324546814}, {"text": "WMT 2018 Shared Task", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.5658637955784798}, {"text": "Parallel Corpus Filtering", "start_pos": 99, "end_pos": 124, "type": "TASK", "confidence": 0.6337941388289133}]}], "abstractContent": [{"text": "We describe Vicomtech's participation in the WMT 2018 Shared Task on parallel corpus filtering.", "labels": [], "entities": [{"text": "WMT 2018 Shared Task on parallel corpus filtering", "start_pos": 45, "end_pos": 94, "type": "TASK", "confidence": 0.5229986980557442}]}, {"text": "We aimed to evaluate a simple approach to the task, which can efficiently process large volumes of data and can be easily deployed for new datasets in different language pairs and domains.", "labels": [], "entities": []}, {"text": "We based our approach on STACC, an efficient and portable method for parallel sentence identification in comparable corpora.", "labels": [], "entities": [{"text": "parallel sentence identification", "start_pos": 69, "end_pos": 101, "type": "TASK", "confidence": 0.6307865083217621}]}, {"text": "To address the specifics of the corpus filtering task, which features significant volumes of noisy data, the core method was expanded with a penalty based on the amount of unknown words in sentence pairs.", "labels": [], "entities": [{"text": "corpus filtering task", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.7902506788571676}]}, {"text": "Additionally, we experimented with a complementary data saturation method based on source sentence n-grams, with the goal of demoting parallel sentence pairs that do not contribute significant amounts of yet unob-served n-grams.", "labels": [], "entities": []}, {"text": "Our approach requires no prior training and is highly efficient on the type of large datasets featured in the corpus filtering task.", "labels": [], "entities": [{"text": "corpus filtering task", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.7928842802842458}]}, {"text": "We achieved competitive results with this simple and portable method, ranking in the top half among competing systems overall.", "labels": [], "entities": []}], "introductionContent": [{"text": "Data-driven approaches to Machine Translation (MT) have been the dominant paradigm in the last two decades, with the development of Statistical Machine Translation (SMT) (, and, more recently, of Neural Machine Translation (NMT) ().", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8747571885585785}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 132, "end_pos": 169, "type": "TASK", "confidence": 0.7912814915180206}, {"text": "Neural Machine Translation (NMT)", "start_pos": 196, "end_pos": 228, "type": "TASK", "confidence": 0.8312901059786478}]}, {"text": "These approaches require large volumes of parallel sentences to properly model translation in a given language pair.", "labels": [], "entities": []}, {"text": "However, large quality parallel corpora based on human translations are scarce across language pairs, and there is a strong need to build clean corpora from different sources.", "labels": [], "entities": []}, {"text": "The World Wide Web is a rich source of multilingual data, from which parallel corpora can be automatically created under appropriate conditions of use (.", "labels": [], "entities": []}, {"text": "However, corpora created via crawling, with automated document and sentence alignment, tend to exhibit significant volumes of noisy data, which can be detrimental to the training of MT systems.", "labels": [], "entities": [{"text": "document and sentence alignment", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6550814583897591}, {"text": "MT", "start_pos": 182, "end_pos": 184, "type": "TASK", "confidence": 0.9864051342010498}]}, {"text": "The task of cleaning noisy data from parallel corpora has been tackled by various researchers over the years.", "labels": [], "entities": []}, {"text": "In (), noise removal is performed via a maximum entropy model trained on observations of clean and noisy data.", "labels": [], "entities": [{"text": "noise removal", "start_pos": 7, "end_pos": 20, "type": "TASK", "confidence": 0.7356027513742447}]}, {"text": "include sentence alignment scores in BiTextor, a tool that performs the complete chain of corpus creation from web data, to filter dubious sentence pairs.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.748934805393219}]}, {"text": "In (), two approaches are evaluated, based on length and on lexical translation likelihood, showing statistically significant improvements in translation quality using the filtered corpus.", "labels": [], "entities": []}, {"text": "An unsupervised filtering method based on outlier detection is proposed in), who also report improvements in translation quality from their filtered corpus.", "labels": [], "entities": [{"text": "outlier detection", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7326227128505707}]}, {"text": "In (, the approach to data filtering is based on graph-based random walks, with improvements observed for ChineEnglish machine translation.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.8071869611740112}, {"text": "ChineEnglish machine translation", "start_pos": 106, "end_pos": 138, "type": "TASK", "confidence": 0.7500543196996053}]}, {"text": "Recently, introduced Zipporah, a fast data selection system for noisy parallel corpora, which is shown to result in improved SMT system quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9958347082138062}]}, {"text": "The WMT 2018 task on parallel corpus filtering offers the possibility to compare different approaches to the task, evaluating their impact on both SMT and NMT systems on several test sets in different domains.", "labels": [], "entities": [{"text": "WMT 2018 task on parallel corpus filtering", "start_pos": 4, "end_pos": 46, "type": "TASK", "confidence": 0.7439042500087193}, {"text": "SMT", "start_pos": 147, "end_pos": 150, "type": "TASK", "confidence": 0.9743442535400391}]}, {"text": "Our participation in the task aimed to evaluate a simple and portable approach, based on the efficient STACC system for parallel sentence extraction from comparable corpora.", "labels": [], "entities": [{"text": "parallel sentence extraction", "start_pos": 120, "end_pos": 148, "type": "TASK", "confidence": 0.661071757475535}]}, {"text": "We extended the original approach with a simple method based on the number of unknown words, to tackle the significant amounts of noise featured in the corpus filtering task.", "labels": [], "entities": [{"text": "corpus filtering task", "start_pos": 152, "end_pos": 173, "type": "TASK", "confidence": 0.7840660015741984}]}, {"text": "Additionally, we experimented with a simple approach to data redundancy, based on ngram saturation.", "labels": [], "entities": [{"text": "data redundancy", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7081519812345505}]}, {"text": "Our contribution centred on providing a sound method that can be easily deployed, does not require prior training, and can efficiently process large volumes of data.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our approach implies only minimal deployment settings.", "labels": [], "entities": []}, {"text": "We ran STACC with the following two hyper-parameters: minimal prefix length was set to 4 and k-best translation lists limited to 5 candidates.", "labels": [], "entities": []}, {"text": "For the STACC.OOV.NGSAT variant, the n-gram order was set to 3.", "labels": [], "entities": [{"text": "STACC.OOV.NGSAT", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.5150021314620972}]}, {"text": "For the lexical translation tables needed by the STACC algorithm, we trained IBM2 models with the FASTALIGN toolkit (), on corpora made available for the WMT 2018 news translation task.", "labels": [], "entities": [{"text": "FASTALIGN", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.8610562682151794}, {"text": "WMT 2018 news translation task", "start_pos": 154, "end_pos": 184, "type": "TASK", "confidence": 0.741724944114685}]}, {"text": "The corpora thus included Europarl v7, Common Crawl, NewsCommentary, and the Rapid corpus of EU press releases.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.9810523986816406}, {"text": "Common Crawl", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9299852252006531}, {"text": "NewsCommentary", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9308585524559021}, {"text": "Rapid corpus of EU press releases", "start_pos": 77, "end_pos": 110, "type": "DATASET", "confidence": 0.9189166824022929}]}, {"text": "The Paracrawl corpus was excluded from the training data in order to extract reliable lexical translation tables from less noisy bilingual corpora.", "labels": [], "entities": [{"text": "Paracrawl corpus", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9410819411277771}]}, {"text": "After duplicates removal, the training corpus amounted to 5, 623, 721 parallel sentences.", "labels": [], "entities": []}, {"text": "The corpus was processed on an in-house server, using 64 threads.", "labels": [], "entities": []}, {"text": "The total processing time for the 104 million sentence pairs of the corpus was around 57 minutes with the STACC.OOV variant, consuming a maximum of 11.3GB of RAM.", "labels": [], "entities": []}, {"text": "With the STACC.OOV.NGSAT variant, processing time was approximately 5 times slower, with an order of magnitude larger consumption of RAM, mainly due to our online trie computation.", "labels": [], "entities": [{"text": "RAM", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9949976205825806}]}, {"text": "Given our stated objectives of evaluating a simple and portable method for the task, our preliminary experiments were all based on variants of the STACC approach, evaluated on the development set provided by the organisers.", "labels": [], "entities": []}, {"text": "We no-tably experimented with the variant in (, where the STACC score is computed via frequency-based lexical weighting that favours content words, and the variant in (, which features a scoring penalty that promotes named-entity matching.", "labels": [], "entities": [{"text": "STACC score", "start_pos": 58, "end_pos": 69, "type": "METRIC", "confidence": 0.9115466177463531}]}, {"text": "Although the differences were minor, the original STACC approach performed better overall and was thus selected as the core of the metric for our final submissions.", "labels": [], "entities": [{"text": "STACC", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.38006141781806946}]}], "tableCaptions": [{"text": " Table 1: Results on the WMT 2018 test sets", "labels": [], "entities": [{"text": "WMT 2018 test sets", "start_pos": 25, "end_pos": 43, "type": "DATASET", "confidence": 0.8444886654615402}]}, {"text": " Table 2: Scoring differences on core statistics", "labels": [], "entities": [{"text": "Scoring", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9658159613609314}]}]}