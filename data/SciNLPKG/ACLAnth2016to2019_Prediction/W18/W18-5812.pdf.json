{"title": [{"text": "Phonetic Vector Representations for Sound Sequence Alignment", "labels": [], "entities": [{"text": "Phonetic Vector Representations", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7250550985336304}, {"text": "Sound Sequence Alignment", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7712110380331675}]}], "abstractContent": [{"text": "This study explores a number of data-driven vector representations of the IPA-encoded sound segments for the purpose of sound sequence alignment.", "labels": [], "entities": [{"text": "sound sequence alignment", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.6175106565157572}]}, {"text": "We test the alternative representations based on the alignment accuracy in the context of computational historical linguistics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.844855546951294}]}, {"text": "We show that the data-driven methods consistently do better than linguistically-motivated articulatory-acoustic features.", "labels": [], "entities": []}, {"text": "The similarity scores obtained using the data-driven representations in a monolingual context, however, performs worse than the state-of-the-art distance (or similarity) scoring methods proposed in earlier studies of computational historical linguistics.", "labels": [], "entities": []}, {"text": "We also show that adapting representations to the task at hand improves the results, yielding alignment accuracy comparable to the state of the art methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9465209245681763}]}], "introductionContent": [{"text": "Most studies in computational linguistics or natural language processing treat the phonetic segments as categorical units, which prevents analyzing or exploiting the similarities or differences between these units.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.6876384417215983}]}, {"text": "Alignment of sound sequences, a crucial step in a number of different fields of inquiry, is one of the tasks that suffers if the segments are treated as distinct symbols with no notion of similarity.", "labels": [], "entities": [{"text": "Alignment of sound sequences", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8985781967639923}]}, {"text": "As a result, alignment algorithms commonly employed in practice (e.g.,) use a scoring function based on similarity of the individual units.", "labels": [], "entities": []}, {"text": "The tasks that require or benefit from aligning sequences are prevalent in computational linguistics, as well as relatively unrelated fields such as bioinformatics.", "labels": [], "entities": []}, {"text": "In this study, we focus on aligning phonetically transcribed parallel word lists in the context of computational historical linguistics, where alignment of sound sequences is interesting either on its own (demonstrating differences between language varieties) or as a necessary step in a larger application, for example, for inferring the cognacy of these words or finding synchronic or diachronic sound correspondences.", "labels": [], "entities": []}, {"text": "The use of similarities between the sound segments has been common in computational studies of historical linguistics.", "labels": [], "entities": []}, {"text": "These studies rely on scoring functions most of which are based on the linguistic knowledge about the sound changes that typically occur across languages.", "labels": [], "entities": []}, {"text": "Another trend shared by all of the earlier studies is the use of a reduced alphabet for representing the sound segments.", "labels": [], "entities": []}, {"text": "Even though the standard way to encode sound sequences is the International Phonetic Alphabet (IPA), using a smaller set of symbols, such as ASJP (, seem to help creating scoring functions that are more useful for historical linguistics.", "labels": [], "entities": [{"text": "International Phonetic Alphabet (IPA)", "start_pos": 62, "end_pos": 99, "type": "DATASET", "confidence": 0.8317481676737467}]}, {"text": "In the present study, we explore a number of methods that learn vector representations for IPA tokens from multi-lingual word-lists, either using the words in a monolingual context or making use of the fact that words represent the same concept in different languages.", "labels": [], "entities": []}, {"text": "We use a standard similarity metric over vectors (cosine similarity) for determining the similarities between the segments, and, in turn, use these similarities for aligning IPAtranscribed sequences.", "labels": [], "entities": []}, {"text": "Besides providing a more principled method for measuring distances, compared to only distance information, vector representations are more useful for further analysis, and may yield better results in other computational tasks relying on supervised or unsupervised machine learning techniques.", "labels": [], "entities": []}, {"text": "Vector representations for phonetic, phonological or orthographic units have been used successfully in earlier research, e.g., for word segmentation, transfer learning of named entity recognition) and morphological inflection (.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.779761552810669}, {"text": "transfer learning of named entity recognition", "start_pos": 150, "end_pos": 195, "type": "TASK", "confidence": 0.8555739323298136}]}, {"text": "We compare our methods to a one-hot-encoding baseline (which is equivalent to symbolic representations), linguistically-motivated vectors, and alignments produced using state-of-the-art scoring methods.", "labels": [], "entities": []}, {"text": "We compare the alignment performance of these methods on a manually-annotated goldstandard corpus, using the same alignment algorithm and the same training data where applicable.", "labels": [], "entities": [{"text": "goldstandard corpus", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.9002431035041809}]}], "datasetContent": [{"text": "Obtaining vector representations with the phon2vec and neural network methods involves settings the models' hyperparameters and training on a data set of IPA sequences (or pairs thereof).", "labels": [], "entities": []}, {"text": "We tokenize the input sequences using an open source Python package developed during this study.", "labels": [], "entities": []}, {"text": "The phon2vec and NN embeddings are trained on the set of all tokenised transcriptions in the training set.", "labels": [], "entities": []}, {"text": "For training the RNN, we need cognates, pairs of words in different languages that share a common root.", "labels": [], "entities": []}, {"text": "As our training set does not include cognacy information, the RNN embeddings are trained on the set of tokenised transcriptions of the word pairs constituting probable cognates -pairs in which the words belong to different languages, are linked to the same concept, and have normalised Levenshtein distance lower than 0.5.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 286, "end_pos": 306, "type": "METRIC", "confidence": 0.6121912151575089}]}, {"text": "We have also experimented with thresholds of 0.4 and 0.6, but setting the cutoff at 0.5 yields better-performing embeddings.", "labels": [], "entities": []}, {"text": "For each method, we run the respective model with the Cartesian product of common values for each hyperparameter, practically performing a random search of the hyperparameter space.", "labels": [], "entities": []}, {"text": "The values we have experimented with, as well as the bestperforming combinations thereof, are summarized in the Appendix.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.8038217425346375}]}, {"text": "Note that the models are optimized for the respective prediction task they perform, not for good alignment performance.", "labels": [], "entities": []}, {"text": "The implementation is realized in the Python programming language, and makes use of a number of libraries, including NumPy (), SciPy (), scikit-learn (Pedregosa et al., 2011), Gensim, and).", "labels": [], "entities": []}, {"text": "The source code used for the experiments reported here is publicly available.", "labels": [], "entities": []}, {"text": "In order to quantify the methods' performance, we employ an intuitive evaluation scheme similar to the one used by: if, fora given word pair, m is the number of alternative gold-standard alignments and n is the number of correctly predicted alignments, the score for that pair would be n m . In the common word pair case of a single gold-standard alignment and a single predicted alignment, the latter would yield 1 point if it is correct and 0 points otherwise; partially correct alignment do not yield points.", "labels": [], "entities": []}, {"text": "The percentage scores are obtained by dividing the points by the total number of pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Scores, as percentage of total alignment pairs. Global scores does not include Covington. PMI method  does not handle tonal languages, and its global score is based on the non-tonal language groups.", "labels": [], "entities": []}]}