{"title": [{"text": "Supervised and Unsupervised Minimalist Quality Estimators: Vicomtech's Participation in the WMT 2018 Quality Estimation Task", "labels": [], "entities": [{"text": "Minimalist Quality Estimators", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.7074244817097982}, {"text": "WMT 2018 Quality Estimation", "start_pos": 92, "end_pos": 119, "type": "TASK", "confidence": 0.6890362501144409}]}], "abstractContent": [{"text": "We describe Vicomtech's participation in the WMT 2018 shared task on quality estimation, for which we submitted minimalist quality es-timators.", "labels": [], "entities": [{"text": "WMT 2018 shared task on quality estimation", "start_pos": 45, "end_pos": 87, "type": "TASK", "confidence": 0.49773524914469036}]}, {"text": "The core of our approach is based on two simple features: lexical translation overlaps and language model cross-entropy scores.", "labels": [], "entities": []}, {"text": "These features are exploited in two system variants: uMQE is an unsupervised system, where the final quality score is obtained by averaging individual feature scores; sMQE is a supervised variant, where the final score is estimated by a Support Vector Regressor trained on the available annotated datasets.", "labels": [], "entities": []}, {"text": "The main goal of our minimalist approach to quality estimation is to provide reliable estimators that require minimal deployment effort, few resources , and, in the case of uMQE, do not depend on costly data annotation or post-editing.", "labels": [], "entities": []}, {"text": "Our approach was applied to all language pairs in sentence quality estimation, obtaining competitive results across the board.", "labels": [], "entities": [{"text": "sentence quality estimation", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6667796870072683}]}], "introductionContent": [{"text": "Quality Estimation (QE) refers to the task of estimating the quality of machine translation output without access to reference translations (), which are not always available fora given domain or language pair, and are costly to produce.", "labels": [], "entities": [{"text": "Quality Estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.754867947101593}]}, {"text": "Typical approaches are based on supervised machine learning models using a large array of features, as exemplified by the standard QUEST baseline (, whose base version employs 17 features that include n-gram language model perplexity scores, lexical translation probabilities, number of source tokens and average number of translations per source word, among others.", "labels": [], "entities": [{"text": "QUEST baseline", "start_pos": 131, "end_pos": 145, "type": "DATASET", "confidence": 0.8667748868465424}]}, {"text": "In recent years, QE models based on neural network approaches have significantly improved the state of the art, as shown for instance by the results obtained in the shared tasks.", "labels": [], "entities": []}, {"text": "Despite recent progress, the vast number of potential domains and language pairs is a challenging aspect fora practical use of quality estimation systems.", "labels": [], "entities": []}, {"text": "First, most approaches to QE rely on annotated data, typically based on human postediting, which are costly to produce.", "labels": [], "entities": [{"text": "QE", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9842464923858643}]}, {"text": "Additionally, the best performing approaches based on neural networks (e.g.,) require large volumes of parallel training corpora, a resource which is only available fora small number of language pairs nowadays.", "labels": [], "entities": []}, {"text": "To tackle these challenges, we designed a minimalist approach to quality estimation, to which we will refer as MQE, based on two features: a lexical translation overlap measure to model translation accuracy 1 and a measure based on cross-entropy scores according to a target language model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.8143436908721924}]}, {"text": "No external tools or large computational resources are needed in this approach, which can be used in the two variants described below.", "labels": [], "entities": []}, {"text": "uMQE is an unsupervised variant, where the final quality score is obtained by averaging individual feature scores.", "labels": [], "entities": [{"text": "uMQE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9320147633552551}]}, {"text": "The system was designed to provide reliable estimators in the numerous use cases where no training data are available to train supervised QE models.", "labels": [], "entities": []}, {"text": "To our knowledge, little attention has been paid to this type of approaches, with two main published approaches: Moreau and Vogel (2012) estimate the quality of machinetranslated output against external sets of n-grams and evaluate several variants of n-gram similarity, whereas proposes an unsupervised method based on the arithmetic combination of scores provided by language models and IBM1 models, trained on morphemes as well as part-ofspeech tags.", "labels": [], "entities": []}, {"text": "On the WMT 2012 datasets, neither approach performed better than the QUEST baseline.", "labels": [], "entities": [{"text": "WMT 2012 datasets", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.9616563518842062}, {"text": "QUEST baseline", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.6785064935684204}]}, {"text": "In this paper, we show that our own unsupervised approach can outperform the supervised baselines, without the use of additional resources such as part-of-speech taggers or morphological analysers.", "labels": [], "entities": []}, {"text": "sMQE is a supervised variant, where the final score is estimated by a Support Vector Regressor trained on the available machine translation output annotated with HTER scores.", "labels": [], "entities": []}, {"text": "The goal of this approach is to enable a fast deployment of supervised quality estimators that outperform other supervised approaches with more complex setups, such as the QUEST baselines with 17 features, while using minimal resources.", "labels": [], "entities": [{"text": "QUEST baselines", "start_pos": 172, "end_pos": 187, "type": "DATASET", "confidence": 0.861638069152832}]}, {"text": "Contrary to uMQE, for which only rank correlation is meaningful, the supervised variant can be evaluated on both ranking and scoring tasks.", "labels": [], "entities": [{"text": "uMQE", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.8609119653701782}]}, {"text": "The paper is organised as follows: Section 2 describes the core MQE approach and the computation of the supervised and unsupervised variants; Section 3 describes the experimental setup for the WMT 2018 shared task on sentence quality estimation; Section 4 presents our results on the test sets in all four language pairs and domains; finally, Section 5 draws conclusions from this work.", "labels": [], "entities": [{"text": "WMT 2018 shared task on sentence quality estimation", "start_pos": 193, "end_pos": 244, "type": "TASK", "confidence": 0.660457793623209}]}], "datasetContent": [{"text": "We submitted results from our two system variants in all language pairs for sentence-level QE, using the same models for both variants in each case.", "labels": [], "entities": [{"text": "QE", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.7313054203987122}]}, {"text": "To train the IBM2 and language models, we selected corpora available for the WMT shared tasks for each specific domain and language pair.", "labels": [], "entities": [{"text": "WMT shared tasks", "start_pos": 77, "end_pos": 93, "type": "TASK", "confidence": 0.5888170599937439}]}, {"text": "For English-German, in the IT domain, we used the training data from the WMT 2016 IT translation task, the WMT 2017 QE task and the WMT 2018 PE task; given the low amounts of data in each individual corpus, we also merged the data from the technical manuals of OpenOffice and KDE4 available in the OPUS repository task.", "labels": [], "entities": [{"text": "WMT 2016 IT translation task", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.8155598163604736}, {"text": "WMT 2017 QE task", "start_pos": 107, "end_pos": 123, "type": "DATASET", "confidence": 0.9013069719076157}, {"text": "WMT 2018 PE task", "start_pos": 132, "end_pos": 148, "type": "DATASET", "confidence": 0.8069049417972565}, {"text": "OPUS repository task", "start_pos": 298, "end_pos": 318, "type": "DATASET", "confidence": 0.864624043305715}]}, {"text": "For English-Latvian, also in the biomedical domain, we used the available EMEA corpus along with the training data from the WMT 2018 QE task and the additional data provided for this language pair in this year's QE task.", "labels": [], "entities": [{"text": "EMEA corpus", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.9151810705661774}, {"text": "WMT 2018 QE task", "start_pos": 124, "end_pos": 140, "type": "DATASET", "confidence": 0.711816817522049}]}, {"text": "Finally, for English-Czech in the IT domain, we used the train-techdoc section of the CzEng17 dataset available for the WMT 2018 translation task, along with the QE training data and the additional data provided for the WMT 2018 QE task.", "labels": [], "entities": [{"text": "CzEng17 dataset", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.9428897202014923}, {"text": "WMT 2018 translation task", "start_pos": 120, "end_pos": 145, "type": "TASK", "confidence": 0.7473975270986557}, {"text": "QE training data", "start_pos": 162, "end_pos": 178, "type": "DATASET", "confidence": 0.7168413400650024}, {"text": "WMT 2018 QE task", "start_pos": 220, "end_pos": 236, "type": "DATASET", "confidence": 0.8141317218542099}]}, {"text": "Sentences were tokenised and truecased with the scripts available in the Moses toolkit (, with truecasing models trained on the data described above.", "labels": [], "entities": []}, {"text": "For English-Czech, we experimented with BPE segmentation) to overcome data sparseness issues, training BPE models with a maximum of 30.000 merge operations and segmenting all corpora accordingly for this language pair.", "labels": [], "entities": [{"text": "BPE segmentation", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.7891369462013245}]}, {"text": "All IBM2 models were trained with the FASTAL-IGN toolkit, and all language models are of order 5 trained with the KENLM toolkit (Heafield, 2011) on the target language data.", "labels": [], "entities": [{"text": "FASTAL-IGN toolkit", "start_pos": 38, "end_pos": 56, "type": "DATASET", "confidence": 0.8470853269100189}, {"text": "KENLM toolkit (Heafield, 2011)", "start_pos": 114, "end_pos": 144, "type": "DATASET", "confidence": 0.8284855484962463}]}, {"text": "For the accuracy metric, minimal prefix length was set to 4 and k-best translation lists limited to 4 candidates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9988293051719666}]}], "tableCaptions": [{"text": " Table 1: Results on the WMT 2018 test sets", "labels": [], "entities": [{"text": "WMT 2018 test sets", "start_pos": 25, "end_pos": 43, "type": "DATASET", "confidence": 0.8444886356592178}]}]}