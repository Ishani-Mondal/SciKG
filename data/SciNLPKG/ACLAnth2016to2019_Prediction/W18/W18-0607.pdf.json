{"title": [{"text": "Hierarchical neural model with attention mechanisms for the classification of social media text related to mental health", "labels": [], "entities": [{"text": "classification of social media text", "start_pos": 60, "end_pos": 95, "type": "TASK", "confidence": 0.8416082978248596}]}], "abstractContent": [{"text": "Mental health problems represent a major public health challenge.", "labels": [], "entities": []}, {"text": "Automated analysis of text related to mental health is aimed to help medical decision-making, public health policies and to improve healthcare.", "labels": [], "entities": []}, {"text": "Such analysis may involve text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8097177445888519}]}, {"text": "Traditionally , automated classification has been performed mainly using machine learning methods involving costly feature engineering.", "labels": [], "entities": [{"text": "automated classification", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.683968722820282}]}, {"text": "Recently , the performance of those methods has been dramatically improved by neural methods.", "labels": [], "entities": []}, {"text": "However, mainly Convolutional neural networks (CNNs) have been explored.", "labels": [], "entities": [{"text": "Convolutional neural networks (CNNs)", "start_pos": 16, "end_pos": 52, "type": "TASK", "confidence": 0.6138473798831304}]}, {"text": "In this paper, we apply a hierarchical Recurrent neu-ral network (RNN) architecture with an attention mechanism on social media data related to mental health.", "labels": [], "entities": []}, {"text": "We show that this architecture improves overall classification results as compared to previously reported results on the same data.", "labels": [], "entities": []}, {"text": "Benefitting from the attention mechanism, it can also efficiently select text elements crucial for classification decisions, which can also be used for in-depth analysis.", "labels": [], "entities": [{"text": "classification decisions", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.9416157007217407}]}], "introductionContent": [{"text": "Mental health problems represent a major public health challenge worldwide, and the accumulation of big data offers the opportunity for improving healthcare processes, interventions, and public health policies.", "labels": [], "entities": []}, {"text": "Recent advances in data science, machine learning and Natural Language Processing (NLP) hold great promise in providing technical solutions for the analysis of large sets of clinically relevant information in Psychiatry (.", "labels": [], "entities": []}, {"text": "This includes not only routinely collected data such as Electronic Health Records (EHRs), but also patient-generated text or speech.", "labels": [], "entities": []}, {"text": "Patientgenerated content has been made available by social media, mainly in the form of tweets or forum posts.", "labels": [], "entities": []}, {"text": "As opposed to e.g. documentation produced by healthcare professionals, social media data captures thoughts, feelings and discourse in people's own voice, and these types of data sources are becoming very important for monitoring a number of public health issues including mental health problems such as drug abuse, alcohol, and depression.", "labels": [], "entities": []}, {"text": "In this work, we address the problem of automatically classifying social media posts related to mental health derived from Reddit.", "labels": [], "entities": [{"text": "classifying social media posts related to mental health", "start_pos": 54, "end_pos": 109, "type": "TASK", "confidence": 0.8084631264209747}]}, {"text": "Convolutional neural networks (CNNs) applied to this task have shown good performance in previous studies (.", "labels": [], "entities": []}, {"text": "However, the performance of recurrent neural networks (RNNs) for the same task remains understudied.", "labels": [], "entities": []}, {"text": "RNNs can be particularly beneficial in this case as they are able to model the sequential structure of text.", "labels": [], "entities": []}, {"text": "We also attempt to explore the contribution of attention mechanisms to establishing a certain hierarchy in the sequences.", "labels": [], "entities": []}, {"text": "To be more precise, we apply a hierarchical RNN architecture as described in to the classification of social media posts related to mental health problems, and seek to answer the following main questions: (a) Is a sequence-based model more beneficial than a CNN model for the accurate classification of social media posts?", "labels": [], "entities": [{"text": "classification of social media posts related to mental health problems", "start_pos": 84, "end_pos": 154, "type": "TASK", "confidence": 0.8409997940063476}, {"text": "accurate classification of social media posts", "start_pos": 276, "end_pos": 321, "type": "TASK", "confidence": 0.7017011592785517}]}, {"text": "(b) Which parts of posts are more important for the classification of a post into its mental health topic as defined by the attention mechanism?", "labels": [], "entities": []}, {"text": "Our main contribution in this work is twofold: (1) an attempt to apply an RNN architecture to the text classification task of determining which mental health problem a post is about, which, to our knowledge, is the first attempt of its kind.", "labels": [], "entities": [{"text": "text classification task of determining which mental health problem a post is about", "start_pos": 98, "end_pos": 181, "type": "TASK", "confidence": 0.7790603156273181}]}, {"text": "We show that the ability of RNNs to take the sequence of events reflected in the post content can be beneficial for the classification of health-related social media text; (2) we also study the results of applying an attention mechanism to pinpoint the parts of a text that are contributing more to classification decisions.", "labels": [], "entities": [{"text": "classification of health-related social media text", "start_pos": 120, "end_pos": 170, "type": "TASK", "confidence": 0.8551916380723318}]}, {"text": "Those results can be useful for an in-depth analysis, to filter out irrelevant content, and to reduce the computational costs for real-life applications.", "labels": [], "entities": []}, {"text": "We provide a few examples, and discuss future directions in this area.", "labels": [], "entities": []}], "datasetContent": [{"text": "We study the performance of the hierarchical architecture on the task of classifying posts from social media related to mental health.", "labels": [], "entities": [{"text": "classifying posts from social media related to mental health", "start_pos": 73, "end_pos": 133, "type": "TASK", "confidence": 0.7523987094561259}]}, {"text": "We compare our results for HIERRNN with the attention mechanism (RNN-att) to two other configurations, where we a) take a maximum of vectors (RNN-max) or b) an average of vectors (RNN-av) at both word and sentence levels.", "labels": [], "entities": []}, {"text": "We also compare our results to a baseline result reported by fora CNN-based architecture (CNN).", "labels": [], "entities": [{"text": "CNN-based architecture (CNN)", "start_pos": 66, "end_pos": 94, "type": "DATASET", "confidence": 0.7252366662025451}]}, {"text": "This architecture is a rather simple architecture with 5 layers: an embedding layer, a convolution layer (a filter window of 5), a max-pooling layer, a fully-connected layer and an output sigmoid layer.", "labels": [], "entities": []}, {"text": "The results are directly comparable as performed for the same data split.", "labels": [], "entities": []}, {"text": "In terms of evaluation metrics we use the standard set of precision (PR), recall (RC) and Fmeasure (FM).", "labels": [], "entities": [{"text": "precision (PR)", "start_pos": 58, "end_pos": 72, "type": "METRIC", "confidence": 0.954224094748497}, {"text": "recall (RC)", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.968917965888977}, {"text": "Fmeasure (FM)", "start_pos": 90, "end_pos": 103, "type": "METRIC", "confidence": 0.9660537093877792}]}, {"text": "In addition, we manually review a random sample of the results from the attention mechanism, and provide a few paraphrased examples ().", "labels": [], "entities": []}, {"text": "Thus, we believe that considering the sequential characteristic of text, as done by RNN models, can be beneficial for analyzing posts related to mental health.", "labels": [], "entities": []}, {"text": "We should also note the improvement due to the attention mechanism as compared to the maximum and averaging strategies (on average 2.5 FM).", "labels": [], "entities": [{"text": "FM", "start_pos": 135, "end_pos": 137, "type": "METRIC", "confidence": 0.9576253890991211}]}, {"text": "Those results are consistent with the results presented by for other types of texts (e.g., reviews) and other types of labels (e.g., ratings).", "labels": [], "entities": []}, {"text": "As for per class performance, RNN-att improves this performance by 6 FM on average.", "labels": [], "entities": [{"text": "RNN-att", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.6287992000579834}, {"text": "FM", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.9806060194969177}]}, {"text": "The improvement in precision is twice as low as the improvement in recall (6% relative change in PR vs. 12% in RC).", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9997312426567078}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9996875524520874}, {"text": "PR", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.8936479687690735}]}, {"text": "This difference is particularly remarkable for more rare classes.", "labels": [], "entities": []}, {"text": "We tend to attribute this to intrinsic properties of RNNs (see).", "labels": [], "entities": []}, {"text": "A relatively high performance improvement of 8 FM is observed for the 8 classes of posts (BPD, bipolar, schizophrenia, selfharm, addiction, cripplingalcoholism, Opiates, autism), which are underrepresented (on average represent 4% of all the test set posts) and with a relatively low document length (9 sentences on average vs. 11 sentences for all the classes).", "labels": [], "entities": [{"text": "FM", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.5071876049041748}]}, {"text": "Except for intrinsic properties of RNNs, our modeling approximation (we limit the document size to 17 sentences to avoid optimization issues) could also contribute to this improvement.", "labels": [], "entities": []}, {"text": "As can be seen from the confusion matrix in the intrinsic overlap of post content across the themes can be misleading for classification: e.g., and again, as shown by, a lot of Opiates posts are misclassified as cripplingalcoholism and vice versa.", "labels": [], "entities": []}, {"text": "However, HI-ERRNN is in general more precise and reveals less confusion between classes: e.g., the amount of confusion for schizophrenia with depression has reduced twice as compared to CNN.", "labels": [], "entities": []}, {"text": "One of the advantages of the attention mechanism is that its weights can be visualized and interpreted by humans (which is not always the case with neural network layers).", "labels": [], "entities": []}, {"text": "In this work, we focus on the analysis of sentence-level attention weights.", "labels": [], "entities": [{"text": "analysis of sentence-level attention weights", "start_pos": 30, "end_pos": 74, "type": "TASK", "confidence": 0.6564122796058655}]}, {"text": "This information can be especially helpful for reducing the quantity of analyzed post sentences to create less costly classification solu- tions.", "labels": [], "entities": []}, {"text": "provides results of our analysis of attention weights distributions.", "labels": [], "entities": []}, {"text": "For this analysis we filtered out one-sentence documents.", "labels": [], "entities": []}, {"text": "We study how often an absolute sentence position receives a maximum or a minimum weight from the total amount of cases this position is present across documents (a document is long enough).", "labels": [], "entities": []}, {"text": "We report top three maximum and minimum positions.", "labels": [], "entities": []}, {"text": "We also report average entropy values for the distributions per sentence.", "labels": [], "entities": []}, {"text": "We also report similar statistics fora selection of classes in.", "labels": [], "entities": []}, {"text": "Our analysis shows that RNN-att is able to distinguish a certain semantic importance pattern: the most attention is paid to the first, then to the second and finally last sentences.", "labels": [], "entities": []}, {"text": "The least attention is systematically paid to a sentence after a peak attention at the beginning (4th sentence), to a sentence in the middle (7th position) and to a sentence before the end (14th position).", "labels": [], "entities": []}, {"text": "At the same time, attention weights are quite equally spread between peak positions (average entropy of 1.93).", "labels": [], "entities": []}, {"text": "The entropy values tend to increase for the classes that are better represented and for which posts are on average longer (e.g., depression, suicidewatch).", "labels": [], "entities": []}, {"text": "Relevant information is not concentrated in those longer documents and several sentences are likely to be equally important.", "labels": [], "entities": []}, {"text": "provides some examples of attention distributions for documents of different lengths and belonging to different classes.", "labels": [], "entities": []}, {"text": "So that, fora longer document from suicidewatch the most relevance is given to the first 2 sentences containing words like \"rejection\" and \"depression\", whereas a neutral sentence \"I met this girl.\" receives a low     weight.", "labels": [], "entities": []}, {"text": "For a short document of 2 sentences from cripplingalcoholism 3/4 of the weight is concentrated on the 1st sentence.", "labels": [], "entities": []}, {"text": "This sentence is especially relevant to the topic and contains keywords such as \"beer\" and \"sober\".", "labels": [], "entities": []}, {"text": "Note that, for instance, fora schizophrenia post (a class for which performance was significantly improved by 10 FM as compared to CNN) the elaboration of the topic of auditory hallucinations in the first two sentences might have been taken into account by RNNs.", "labels": [], "entities": []}, {"text": "However, RNNs usually require more computational power to be trained than other neural architectures.", "labels": [], "entities": []}, {"text": "We believe that such information on attention distributions can be particularly useful for the creation of low-resource models, which could operate with filtered data (e.g., only two first sentences of a post).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Multiclass classification evaluation results (we indicate the percentage of posts belonging to a class in the  sample; \u00af l doc refers to average document length in sentences; \u00af l sent -average sentence length in tokens; FM refers  to F-measure; PR -to precision; RC -to recall; )", "labels": [], "entities": [{"text": "Multiclass classification evaluation", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.8526520133018494}, {"text": "F-measure", "start_pos": 244, "end_pos": 253, "type": "METRIC", "confidence": 0.971710741519928}, {"text": "PR -to precision; RC -to", "start_pos": 255, "end_pos": 279, "type": "METRIC", "confidence": 0.7711409665644169}, {"text": "recall", "start_pos": 280, "end_pos": 286, "type": "METRIC", "confidence": 0.5443477034568787}]}, {"text": " Table 3: Absolute sentence positions that receive the most and the least attention. We provide top three positions  with the percentage of their occurrences that received maximum or minimum attention. E.g.: 2nd sentence receives  the most attention in 13% of the cases a post contains a 2nd sentence. H refers to entropy", "labels": [], "entities": []}, {"text": " Table 4: Absolute sentence positions that receive the most and the least attention: selection of classes. We provide  top three positions with the percentage of their occurrences that received maximum or minimum attention. E.g.:  for Opiates the 17th sentence receives maximum attention in 4% of the cases a post contains a 17th sentence. H  refers to entropy.", "labels": [], "entities": []}, {"text": " Table 5: Paraphrased examples of attention weights distributions over post sentences. Medication names have  been replaced with [medication]", "labels": [], "entities": []}]}