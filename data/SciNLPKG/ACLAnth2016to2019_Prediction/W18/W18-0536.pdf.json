{"title": [{"text": "The Effect of Adding Authorship Knowledge in Automated Text Scoring", "labels": [], "entities": [{"text": "Automated Text Scoring", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.598296602567037}]}], "abstractContent": [{"text": "Some language exams have multiple writing tasks.", "labels": [], "entities": []}, {"text": "When a learner writes multiple texts in a language exam, it is not surprising that the quality of these texts tends to be similar, and the existing automated text scoring (ATS) systems do not explicitly model this similarity.", "labels": [], "entities": []}, {"text": "In this paper, we suggest that it could be useful to include the other texts written by this learner in the same exam as extra references in an ATS system.", "labels": [], "entities": []}, {"text": "We propose various approaches of fusing information from multiple tasks and pass this authorship knowledge into our ATS model on six different datasets.", "labels": [], "entities": []}, {"text": "We show that this can positively affect the model performance inmost cases.", "labels": [], "entities": []}], "introductionContent": [{"text": "The existence of various English exam products provides a useful and fairway for language learners to measure their English skills accurately.", "labels": [], "entities": []}, {"text": "It also offers a well-accepted standard to help schools and companies to quantitatively judge whether their non-native English applicants meet the compulsory language requirements they setup.", "labels": [], "entities": []}, {"text": "Many learners have taken different English exams to get the qualifications required by different organisations.", "labels": [], "entities": []}, {"text": "For example, more than two million International English Language Testing System (IELTS) exam sessions have been taken in 2012-2013 1 , and more than 30 million people have taken the Test of English as a Foreign Language (TOEFL) exam 2 . English exams like IELTS and TOEFL have free-text writing tasks to evaluate a learner's writing ability.", "labels": [], "entities": [{"text": "International English Language Testing System (IELTS) exam", "start_pos": 35, "end_pos": 93, "type": "DATASET", "confidence": 0.7544328371683756}, {"text": "TOEFL", "start_pos": 267, "end_pos": 272, "type": "DATASET", "confidence": 0.856468141078949}]}, {"text": "For a writing task, each learner needs to write a text to answer the prompt in the task.", "labels": [], "entities": []}, {"text": "Appropriately assessing the quality of free-text writings requires highly proficient human examiners, and the lack of professional and qualified examiners makes it hard for learners to get accurate feedback on the quality of their writings in a timely fashion.", "labels": [], "entities": []}, {"text": "Consequently, it is hoped that an ATS system can possibly act as a kind of examiner to mitigate this problem, which offers an assistance to both learners and educators.", "labels": [], "entities": []}, {"text": "The goal of ATS is to improve consistency and reduce human resource overheads.", "labels": [], "entities": [{"text": "ATS", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.914110541343689}, {"text": "consistency", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.911963939666748}]}, {"text": "ATS usually utilises machine learning techniques to build a model to learn the underlying relationship between texts and scores.", "labels": [], "entities": [{"text": "ATS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7368577718734741}]}, {"text": "ATS is often used as the second marker in highstakes exams, the only marker in practice and tutoring software products.", "labels": [], "entities": [{"text": "ATS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8934342861175537}]}], "datasetContent": [{"text": "In this paper, we require a dataset that includes more than one text written by each learner, where each text is scored with an individual-level score.", "labels": [], "entities": []}, {"text": "We finally get six datasets in total for our experiments.", "labels": [], "entities": []}, {"text": "Each dataset is a set of texts collected from areal exam, and each exam is targeted atone or more Common European Framework of Reference for Languages (CEFR) 3 levels in English.", "labels": [], "entities": [{"text": "Common European Framework of Reference for Languages (CEFR)", "start_pos": 98, "end_pos": 157, "type": "DATASET", "confidence": 0.592448678612709}]}, {"text": "There are six CEFR levels in total: A1, A2, B1, B2, C1 and C2 arranged from lowest to highest.", "labels": [], "entities": [{"text": "CEFR", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.7490546107292175}]}, {"text": "In each dataset, each script consists of the answers to two tasks.", "labels": [], "entities": []}, {"text": "The answers to both tasks were scored on the same grading scale.", "labels": [], "entities": []}, {"text": "Each script was written on the same day so we can safely assume no dramatic variation in the writing skill for each learner.", "labels": [], "entities": []}, {"text": "The FCE dataset discussed in Section 2 was collected from the FCE exam.", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9238022565841675}, {"text": "FCE exam", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9431574940681458}]}, {"text": "The other five datasets were provided by Cambridge Assessment collected from different years.", "labels": [], "entities": [{"text": "Cambridge Assessment collected", "start_pos": 41, "end_pos": 71, "type": "DATASET", "confidence": 0.9809501965840658}]}, {"text": "We need to choose the score for each text for an ATS model to learn.", "labels": [], "entities": []}, {"text": "As the original score for each text in the FCE is not reported on a numerical scale, Cambridge Assessment helped us convert the grades to integers between 0 and 20.", "labels": [], "entities": [{"text": "FCE", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.8623887300491333}, {"text": "Cambridge Assessment", "start_pos": 85, "end_pos": 105, "type": "DATASET", "confidence": 0.9668162763118744}]}, {"text": "This mapping is available in.", "labels": [], "entities": []}, {"text": "All the texts from the B2-U, B2-S, C1-U and C1-S datasets are evaluated in terms of four aspects: content, communicative achievement, language quality and organisation.", "labels": [], "entities": []}, {"text": "Each aspect is scored as an integer in the range 0-5.", "labels": [], "entities": []}, {"text": "We add the scores of these four aspects of a text together to obtain a total score in the range 0-20, and we use this total score as the score for For the other five datasets, the name of each dataset encodes its target CEFR level learners with whether it is unshuffled or shuffled.", "labels": [], "entities": []}, {"text": "B2-U means that it aims at B2 level learners and is unshuffled.", "labels": [], "entities": [{"text": "B2-U", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7246158123016357}]}, {"text": "MEAN and STD describe the mean and standard deviation of the scores.", "labels": [], "entities": [{"text": "MEAN", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6476988196372986}, {"text": "STD", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.5284120440483093}]}, {"text": "All the datasets have two writing tasks, and for each writing task, each learner is required to write an answer to one prompt.", "labels": [], "entities": []}, {"text": "# prompts describes how many prompts exist in each dataset.", "labels": [], "entities": []}, {"text": "this text for our study.", "labels": [], "entities": []}, {"text": "In contrast, AL-U is marked on a scale of 0-9 at 0.5 mark intervals, where each text also receives a score for each of four aspects including task achievement, coherence, word usage and grammar.", "labels": [], "entities": []}, {"text": "The total score is aggregated from the scores on all four aspects by Cambridge Assessment, and it is still normalised to 0-9 at 0.5 mark intervals.", "labels": [], "entities": [{"text": "Cambridge Assessment", "start_pos": 69, "end_pos": 89, "type": "DATASET", "confidence": 0.9640015661716461}]}, {"text": "In this case, we directly use the existing total score as the individual score fora text in AL-U for our study.", "labels": [], "entities": []}, {"text": "Original We summarise the six datasets in.", "labels": [], "entities": []}, {"text": "The difference between the shuffled and unshuffled datasets in is how texts are presented to human judges to score.", "labels": [], "entities": []}, {"text": "For the four unshuffled datasets, each human judge marks the first and second text written by a learner in sequence, so the score of the second text might be affected by the first marked text.", "labels": [], "entities": []}, {"text": "In comparison, the texts in B2-S and C1-S are shuffled and randomly displayed to human judges.", "labels": [], "entities": []}, {"text": "Hence, this removes any grading bias due to knowing the authorship.", "labels": [], "entities": []}, {"text": "Due to transcription errors, we only kept scripts which do not contain any invalid individual score.", "labels": [], "entities": []}, {"text": "After we cleaned the text scores, each dataset was then split into training, development and test sets.", "labels": [], "entities": []}, {"text": "The total number of scripts in each dataset, and the number of scripts in the training, development and test sets are summarised in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The details of the six datasets. FCE is the dataset released by Yannakoudakis et al..", "labels": [], "entities": [{"text": "FCE", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.9548546075820923}]}, {"text": " Table 4: The comparison of the previous work and  our baseline models on the FCE test set.", "labels": [], "entities": [{"text": "FCE test set", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.9838164250055949}]}, {"text": " Table 5: p-value for each approach estimated by  the Wilcoxon signed-rank test across all the six  datasets. The value bigger than 0.05 is in bold", "labels": [], "entities": []}, {"text": " Table 6: The results of different setups on the test sets. The best setup per dataset is in bold.  GREEN means improvement and RED means degradation over BASE. The optimal interpolation hyper- parameters for each fusion approach are reported as \u03b1/\u03b2. + means significantly better (p < 0.05) than  BASE using the permutation randomisation test (Yeh, 2000) with 2,000 samples. No metric is found  significantly worse than BASE.", "labels": [], "entities": [{"text": "GREEN", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.9992421865463257}, {"text": "RED", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.9987107515335083}, {"text": "BASE", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9703634977340698}, {"text": "BASE", "start_pos": 297, "end_pos": 301, "type": "TASK", "confidence": 0.4098525047302246}, {"text": "BASE", "start_pos": 420, "end_pos": 424, "type": "METRIC", "confidence": 0.9457697868347168}]}]}