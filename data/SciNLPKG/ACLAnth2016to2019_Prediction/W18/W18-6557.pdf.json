{"title": [{"text": "E2E NLG Challenge: Neural Models vs. Templates", "labels": [], "entities": [{"text": "E2E NLG Challenge", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9326502680778503}]}], "abstractContent": [{"text": "E2E NLG Challenge is a shared task on generating restaurant descriptions from sets of key-value pairs.", "labels": [], "entities": [{"text": "E2E NLG Challenge", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9217417438824972}]}, {"text": "This paper describes the results of our participation in the challenge.", "labels": [], "entities": []}, {"text": "We develop a simple, yet effective neural encoder-decoder model 1 which produces fluent restaurant descriptions and outper-forms a strong baseline.", "labels": [], "entities": []}, {"text": "We further analyze the data provided by the organizers and conclude that the task can also be approached with a template-based model developed in just a few hours.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language Generation (NLG) is the task of generating natural language utterances from structured data representations.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7690668851137161}]}, {"text": "The E2E NLG Challenge 2 is a shared task which focuses on end-toend data-driven NLG methods.", "labels": [], "entities": [{"text": "E2E NLG Challenge 2", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.839219942688942}]}, {"text": "These approaches attract a lot of attention, because they perform joint learning of textual structure and surface realization patterns from non-aligned data, which allows fora significant reduction of the amount of human annotation effort needed for NLG corpus creation.", "labels": [], "entities": [{"text": "NLG corpus creation", "start_pos": 250, "end_pos": 269, "type": "TASK", "confidence": 0.7833437522252401}]}, {"text": "The contribution of our submission to the challenge can be summarized as follows: (1) we show how exploiting data properties allows us to design more accurate neural architectures; (2) we develop a simple template-based system which achieves performance comparable to neural approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results show that Model-D outperforms  TGen as measured by all five metrics, albeit the performance variance is quite large.", "labels": [], "entities": []}, {"text": "Model-T clearly scores below both TGen and Model-D. This is expected, since Model-T is not data-driven, and hence the texts it generates might be different from the reference outputs.", "labels": [], "entities": []}, {"text": "Previous studies have shown that widely used automatic metrics (including the ones used in our competition) lack strong correlation with human judgments).", "labels": [], "entities": []}, {"text": "We decided to examine the predictions made by the compared systems on one hundred randomly sampled input instances, focusing on generic errors, which make sense to lookout for in many NLG scenarios.", "labels": [], "entities": []}, {"text": "shows the error types and the number of mistakes found in each of the prediction files.", "labels": [], "entities": []}, {"text": "The error types should be self-explanatory (sample predictions are given in Appendix A.2).", "labels": [], "entities": [{"text": "Appendix", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.7324873208999634}]}, {"text": "As far as the (subjective) manual analysis goes, Model-T outputs descriptions with the best linguistic quality.", "labels": [], "entities": []}, {"text": "shows that the predictions of the template-based system contain no errors -this is because we incorporated our notion of grammaticality into the templates' definition, which allowed Model-T to avoid the errors found in predictions of the other two approaches.", "labels": [], "entities": []}, {"text": "The majority of errors made by Model-D are either wrong verbalizations of the input MR values or punctuation mistakes.", "labels": [], "entities": []}, {"text": "The latter ones are limited to the cases of missing a comma between clauses or not finishing a sentence with a full stop.", "labels": [], "entities": []}, {"text": "An easy solution to this problem is adding a postprocessing step which fixes punctuation mistakes before outputting the text.", "labels": [], "entities": []}, {"text": "Crucially, Model-D often drops or modifies some MR attribute values.", "labels": [], "entities": []}, {"text": "According to the organizers, 40% of the data by design contain either additional or omitted information on the output side (): crowd workers were allowed to not lexicalize attribute values which they deemed unimportant.", "labels": [], "entities": []}, {"text": "We decided to examine the training data and find out if the discrepancies of Model-D were learned from the data.", "labels": [], "entities": []}, {"text": "For the final submission we have chosen Model-T's predictions -despite lower metric scores, they contained most grammatical outputs and kept all input information in the generated text.", "labels": [], "entities": []}, {"text": "The results of the final evaluation on the test data are presented in.", "labels": [], "entities": []}, {"text": "They were produced by the TrueSkill algorithm (), which performs pairwise system comparisons and clusters them into groups.", "labels": [], "entities": []}, {"text": "For completeness, we include the highest reported scores among all the participants (rightmost column).", "labels": [], "entities": [{"text": "completeness", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.8771658539772034}]}, {"text": "Note, however, that the numerical scores are not directly interpretable, but the relative ranking of a system in terms of its range and cluster is important -systems within one cluster are considered tied.", "labels": [], "entities": []}, {"text": "Model-T was assigned to the second best cluster both in terms of quality and naturalness, despite the much lower metric scores.", "labels": [], "entities": []}, {"text": "Retrospectively, this justifies our decision to choose Model-T instead of Model-D for the final submission.", "labels": [], "entities": []}, {"text": "The E2E NLG Challenge focuses on end-to-end data-driven NLG methods, which is why systems like Model-T might not exactly fit into the task setup.", "labels": [], "entities": []}, {"text": "Nevertheless, we view such a system as a necessary candidate for comparison, since the E2E NLG Challenge data was designed to learn models that produce \"more natural, varied and less template-like system utterances\" ().", "labels": [], "entities": [{"text": "E2E NLG Challenge data", "start_pos": 87, "end_pos": 109, "type": "DATASET", "confidence": 0.9555066227912903}]}], "tableCaptions": [{"text": " Table 1: Input representation of the running exam- ple using positional ids.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results according to automatic  metrics (development set).", "labels": [], "entities": []}, {"text": " Table 3: Common errors made by the compared  models (100 randomly sampled development in- stances).", "labels": [], "entities": []}, {"text": " Table 4: Data annotation discrepancies (100 randomly sampled training instances).", "labels": [], "entities": []}, {"text": " Table 5: Final evaluation results on the test set. Hu- man evaluation results have the following format:  score/(range)/cluster.", "labels": [], "entities": []}]}