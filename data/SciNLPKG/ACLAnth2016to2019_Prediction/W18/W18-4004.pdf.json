{"title": [{"text": "Transferred Embeddings for Igbo Similarity, Analogy and Diacritic Restoration Tasks", "labels": [], "entities": [{"text": "Diacritic Restoration Tasks", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.7797409693400065}]}], "abstractContent": [{"text": "Existing NLP models are mostly trained with data from well-resourced languages.", "labels": [], "entities": []}, {"text": "Most minority languages face the challenge of lack of resources-data and technologies-for NLP research.", "labels": [], "entities": []}, {"text": "Building these resources from scratch for each minority language will be very expensive, time-consuming and amount largely to unnecessarily re-inventing the wheel.", "labels": [], "entities": []}, {"text": "In this paper, we applied transfer learning techniques to create Igbo word embeddings from a variety of existing English trained embeddings.", "labels": [], "entities": []}, {"text": "Transfer learning methods were also used to build standard datasets for Igbo word similarity and analogy tasks for intrinsic evaluation of embeddings.", "labels": [], "entities": [{"text": "Igbo word similarity", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.5963512361049652}]}, {"text": "These projected embeddings were also applied to the diacritic restoration task.", "labels": [], "entities": [{"text": "diacritic restoration", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.8593203723430634}]}, {"text": "Our results indicate that the projected models not only outperform the trained ones on the semantic based tasks of analogy, word-similarity and odd-word identifying, but they also achieve enhanced performance on the diacritic restoration with learned diacritic embeddings.", "labels": [], "entities": [{"text": "odd-word identifying", "start_pos": 144, "end_pos": 164, "type": "TASK", "confidence": 0.7419235706329346}]}, {"text": "1 Background Most NLP systems are modelled with English data.", "labels": [], "entities": []}, {"text": "One major challenge to adapting these systems for low resource languages is lack of good quality data.", "labels": [], "entities": []}, {"text": "Such languages often rely on poor quality web-crawled data.", "labels": [], "entities": []}, {"text": "In our case the target language is Igbo, a language spoken by over 30 million indigenes who live mainly in the southeastern part of Nigeria but also in different parts of the world.", "labels": [], "entities": []}, {"text": "Inspite of the relatively large number of speakers, Igbo is critically low-resourced in terms of NLP research (Onyenwe et al., 2018).", "labels": [], "entities": []}, {"text": "Recent efforts to develop resources for Igbo include the design of Igbo POS tagset (Onyenwe et al., 2014), and the tagset refinement (Onyenwe et al., 2015) as well as the development of Igbo POS-tagger (Onyenwe, 2017).", "labels": [], "entities": []}, {"text": "Works are also ongoing with its automatic diacritic restoration and lexical disambiguation (Ezeani et al., 2016) (Ezeani et al., 2017) and morphological segmentation (Enemouh et al., 2017).", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 139, "end_pos": 165, "type": "TASK", "confidence": 0.7438752949237823}]}, {"text": "1.1 Embedding Models Word embeddings are generic semantic representations from corpus.", "labels": [], "entities": []}, {"text": "It enhances the concept of distri-butional hypothesis (Harris, 1954) and count-based distributional vectors (Baroni and Lenci, 2010) and provides an alternative to the one task, one model approach.", "labels": [], "entities": []}, {"text": "Their application areas span most NLP tasks and other fields such as biomedical, psychiatry, psychology, philology, cognitive science and social science (Altszyler et al., 2016).", "labels": [], "entities": []}, {"text": "There are many approaches to training embedding models, however predictive (Mikolov et al., 2013a) and count-based (Pennington et al., 2014) models are very commonly used.", "labels": [], "entities": []}, {"text": "Ideally, a model trained in one language should capture similar semantic distribution in other languages.", "labels": [], "entities": []}, {"text": "Since the large amount of data required to train such a model are not often available for low resource languages, transfer learning techniques could be used to project learned knowledge from one language to another.", "labels": [], "entities": []}, {"text": "This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": []}, {"text": "Licence details: http:// creativecommons.org/licenses/by/4.0/.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Our experimental data consists of a collection of Igbo texts from the Igbo Bible and the translation of the Universal Declaration of Human Rights, two short novels: an Igbo version of Eze Goes to School and another Igbo novel Mmadu . Ka A Na Ari . a. The pipeline has three stages.", "labels": [], "entities": [{"text": "translation of the Universal Declaration of Human Rights", "start_pos": 89, "end_pos": 145, "type": "TASK", "confidence": 0.7343304231762886}]}, {"text": "It starts with building the embedding models using training or projection methods (section 2.1).", "labels": [], "entities": []}, {"text": "The next stage enhances the diacritic words with the embeddings of the its co-aligned English words (section 3.4.2).", "labels": [], "entities": []}, {"text": "Lastly, the diacritic restoration is implemented as laid out in section 3.4.3.", "labels": [], "entities": []}, {"text": "In this experiment, we used only the Igbo-English parallel bible corpora, available from the Jehova Witness website 1 , for the word alignment and projection of embedding models.", "labels": [], "entities": [{"text": "Jehova Witness website 1", "start_pos": 93, "end_pos": 117, "type": "DATASET", "confidence": 0.9233685880899429}, {"text": "word alignment", "start_pos": 128, "end_pos": 142, "type": "TASK", "confidence": 0.7559776902198792}]}, {"text": "The parallel data consist of 32,416 aligned lines of text.", "labels": [], "entities": []}, {"text": "Additional data from the novels (3179 lines) and official documents (90 lines) makeup the rest of the 35,685 lines of text with token sizes of 962,747 (without punctuations) and vocabulary length 16,586 we used.", "labels": [], "entities": []}, {"text": "Although only 34% (328,591) of all tokens have diacritics, 54.8% (9,090) of vocabulary words are diacritic marked.", "labels": [], "entities": []}, {"text": "There are 795 ambiguous wordkeys.", "labels": [], "entities": []}, {"text": "A wordkey is a word stripped of its diacritics if it has any.", "labels": [], "entities": []}, {"text": "Wordkeys could have multiple diacritic variants, one of which could be the same as the wordkey itself.", "labels": [], "entities": [{"text": "Wordkeys", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8766815662384033}]}, {"text": "Over 97% of the ambiguous wordkeys have 2 or 3 variants.", "labels": [], "entities": []}, {"text": "We evaluate the models on their performances on the following NLP tasks: odd-words, analogy and word similarity and diacritic restoration.", "labels": [], "entities": [{"text": "analogy and word similarity", "start_pos": 84, "end_pos": 111, "type": "TASK", "confidence": 0.5777351260185242}, {"text": "diacritic restoration", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.7345010936260223}]}, {"text": "As there are no standard datasets for these tasks in Igbo, we had auto-generate them from our data or transfer existing ones from English.", "labels": [], "entities": []}, {"text": "Igbo native speakers were used to refine and validate instances of the dataset or methods used.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Igbo and English models: vocabulary, vector and training data sizes", "labels": [], "entities": []}, {"text": " Table 5: Trained and Project Embeddings on odd-word prediction", "labels": [], "entities": [{"text": "odd-word prediction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7022744864225388}]}]}