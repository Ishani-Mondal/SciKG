{"title": [], "abstractContent": [{"text": "A challenging task for word embeddings is to capture the emergent meaning or polarity of a combination of individual words.", "labels": [], "entities": []}, {"text": "For example, existing approaches in word embeddings will assign high probabilities to the words \"Penguin\" and \"Fly\" if they frequently co-occur, but it fails to capture the fact that they occur in an opposite sense-Penguins do not fly.", "labels": [], "entities": [{"text": "word embeddings", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7245883047580719}]}, {"text": "We hypothesize that humans do not associate a single polarity or sentiment to each word.", "labels": [], "entities": []}, {"text": "The word contributes to the overall polarity of a combination of words depending upon which other words it is combined with.", "labels": [], "entities": []}, {"text": "This is analogous to the behavior of microscopic particles which exist in all possible states at the same time and interfere with each other to give rise to new states depending upon their relative phases.", "labels": [], "entities": []}, {"text": "We make use of the Hilbert Space representation of such particles in Quantum Mechanics where we subscribe a relative phase to each word, which is a complex number, and investigate two such quantum inspired models to derive the meaning of a combination of words.", "labels": [], "entities": []}, {"text": "The proposed models 1 achieve better performances than state-of-the-art non-quantum models on the binary sentence classification task.", "labels": [], "entities": [{"text": "binary sentence classification task", "start_pos": 98, "end_pos": 133, "type": "TASK", "confidence": 0.6973297446966171}]}], "introductionContent": [{"text": "Word embeddings () are the current state of art techniques to form semantic representations of words based on their contexts.", "labels": [], "entities": []}, {"text": "They have been successfully used in various downstream tasks such as text classification, text generation, etc.", "labels": [], "entities": [{"text": "text classification", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.8462353944778442}, {"text": "text generation", "start_pos": 90, "end_pos": 105, "type": "TASK", "confidence": 0.8229012191295624}]}, {"text": "Building on word embeddings, various unsupervised () and supervised () models for sentence embeddings have been proposed.", "labels": [], "entities": []}, {"text": "The general idea behind word embeddings is to use word co-occurrence as the basis of semantic relationship between words.", "labels": [], "entities": []}, {"text": "This naturally brings about the difficulty for word embedding approaches in capturing the emergent meaning of a combination of words, such as a phrase or a sentence.", "labels": [], "entities": []}, {"text": "For example, the phrase \"ivory tower\" can hardly be modeled as a semantic combination of \"ivory\" and \"tower\".", "labels": [], "entities": []}, {"text": "Or, the high frequency of occurrence of the words \"Penguin\" and \"Fly\" fails to suggest that they are negative correlated.", "labels": [], "entities": []}, {"text": "In the field of information retrieval (IR), various models based on the mathematical framework of Quantum Theory have been applied to capture and represent dependencies between words (, inspired by the pioneering work of Van Rijsbergen.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.8996469140052795}]}, {"text": "models a segment of text as a quantum mixed state, represented by a positive semi-definite matrix called density matrix in a Hilbert Space, whose non-diagonal entries entail word relations in a quantum manner(Quantum Interference).", "labels": [], "entities": []}, {"text": "The resulting Quantum Language Model (QLM) outperforms various classical models on ad-hoc retrieval tasks.", "labels": [], "entities": []}, {"text": "captures Unconditional Pure Dependence (UPD) () between words in a quantum way by demonstrating the equivalence relation between UPD and Quantum Entan-glement (QE) and providing away to incorporate UPD information into QLM, leading to improved performance over the original QLM.", "labels": [], "entities": []}, {"text": "develops a well-performing question answering (QA) system by extracting various features and learning to compare the density matrices between a question and an answer.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.8503072261810303}]}, {"text": "The successful application of quantum-inspired models onto IR tasks () to some extent demonstrates the non-classical nature of word dependency relations.", "labels": [], "entities": [{"text": "IR tasks", "start_pos": 59, "end_pos": 67, "type": "TASK", "confidence": 0.9010413587093353}]}, {"text": "However, all these models simplify the space of interest to be space of real vectors Rn , with the representation of a word or a text segment being a real-valued vector or matrix, largely due to the lack of proper textual features corresponding to the imaginary part.", "labels": [], "entities": []}, {"text": "Since quantum phenomena cannot be faithfully expressed without complex numbers, these models are theoretically limited.", "labels": [], "entities": []}, {"text": "Ina recent work, presents a theoretical quantum framework for modeling a collection of documents called QWeb, in which a concept is represented as a state in a Hilbert Space, and concept combination is represented as a superposition of the concept states.", "labels": [], "entities": []}, {"text": "Under this framework, the complex phases of each concept have a natural correspondence to the extent of interference between concepts.", "labels": [], "entities": []}, {"text": "However, the framework has not given rise to any applicable models onto IR or NLP tasks to the authors' knowledge.", "labels": [], "entities": []}, {"text": "Inspired by the potential of quantum-inspired models to represent word relations, we seek to build quantum models to represent words and word combinations, and explore the use of complex numbers in the modeling process.", "labels": [], "entities": []}, {"text": "Our model is built on top of two hypothesis: I) A word is a linear combination of latent concepts with complex weights.", "labels": [], "entities": []}, {"text": "II) A combination of words is viewed as a complex combination of word states, either a superposition state or a mixed state.", "labels": [], "entities": []}, {"text": "The first hypothesis agrees with QWeb, but here we concretize a concept in QWeb to be a word.", "labels": [], "entities": [{"text": "QWeb", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.9193770885467529}]}, {"text": "The second hypothesis is an extension of both QWeb and the work by, because QWeb restricts a combination of concepts to be a superposition state while the work by assumes that a sentence is a complex mixture of word projectors.", "labels": [], "entities": []}, {"text": "This study sets foot in sentence-level analysis, and treats a sentence as a combination of words.", "labels": [], "entities": [{"text": "sentence-level analysis", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.691734567284584}]}, {"text": "We intend to model a word as a quantum state containing two parts: amplitudes and complex phases, and expect to capture the lowlevel word co-occurrence information by the amplitudes, while using the phases to represent the emergent meaning or polarity when a word is combined with other words.", "labels": [], "entities": []}, {"text": "We investigate on two models to represent the combination of words, either as a superposition of word states or as a mixture of word projectors.", "labels": [], "entities": []}, {"text": "The effectiveness of the two models are evaluated on 5 benchmarking binary sentence classification datasets, and the results show that the mixture model outperforms state-of-the-art word embedding approaches.", "labels": [], "entities": []}, {"text": "The motivation behind this paper stems from an analogy with Quantum Physics.", "labels": [], "entities": []}, {"text": "Consider the phrase \"Penguins fly\".", "labels": [], "entities": []}, {"text": "If we model it along the lines of the famous double slit experiment in Quantum Physics, the two slits corresponds to human interpretation of words \"Penguins\" and \"Fly\" (Verb sense of Fly).", "labels": [], "entities": []}, {"text": "When only one slit is open at a time, the waves corresponding to the individual word will go through the slit and register onto the screen.", "labels": [], "entities": []}, {"text": "The screen is made of a set of polarity detectors judging opinion or sentiment polarities.", "labels": [], "entities": [{"text": "sentiment polarities", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.6752089709043503}]}, {"text": "In.a the human mind sees the word 'Penguins' alone and detects it as a neutral word with a very high probability.", "labels": [], "entities": []}, {"text": "This is analogous to the double slit experiment with one slit open.", "labels": [], "entities": []}, {"text": "The same is the case for the word 'Fly' considered in isolation.", "labels": [], "entities": []}, {"text": "By classical logic, when the two words are taken together as a phrase 'Penguins fly', the human mind should assign a high probability of it being neutral again.", "labels": [], "entities": []}, {"text": "However, we know that it is a false statement.c).", "labels": [], "entities": []}, {"text": "Different from classical representation, this study hypothesizes that the combination of words can be viewed as a superposition or complex mixture of quantum entities which gives rise to anew state.", "labels": [], "entities": []}, {"text": "In this way, the emerging meaning or polarity of a combination of words will manifest in the interference between words, and be captured inherently in the density matrix representation.", "labels": [], "entities": []}, {"text": "For example, two or more words having a neutral sense individually may combine to give a negative sense, just like the casein the analogy given above.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments are conducted on five benchmarking datasets for binary text classification: Customer Review dataset (CR) (), Opinion polarity dataset (MPQA) (), Sentence Subjectivity dataset (SUBJ)), Movie Review dataset (MR), and Stanford Sentiment Treebank (SST) dataset 2 . The statistics for the datasets are shown in.", "labels": [], "entities": [{"text": "binary text classification", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.6939980785051981}, {"text": "Movie Review dataset (MR)", "start_pos": 200, "end_pos": 225, "type": "DATASET", "confidence": 0.7771740307410558}, {"text": "Stanford Sentiment Treebank (SST) dataset", "start_pos": 231, "end_pos": 272, "type": "DATASET", "confidence": 0.8462808515344348}]}, {"text": "In this paper, we compare the classification accuracy of our proposed Complex Embedding Superposition (CE-Sup) network and Complex Embedding Mixture (CE-Mix) network with three existing unsupervised representation training models, Unigram-TFIDF and fastText Bag-of-Words (BOW), as well as two existing supervised representation training models, namely CaptionRep BOW () and DictRep BOW ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9076617360115051}]}, {"text": "We directly take the performances of these systems on the 5 datasets from existing works.", "labels": [], "entities": []}, {"text": "Since the performances for CaptionRep and DictRep are not available on SST, we use the performance of another model called ParagraphPhrase (.", "labels": [], "entities": []}, {"text": "For a fair comparison, we also implement an end-to-end supervised real embedding network (Real-Embed), where each word is mapped to a real-valued vector in the embedding layer, based on which the sentence representation is obtained by averaging the embedding vectors for all words in the sentence, and a fully connected layer maps the sentence vector to the classification label.", "labels": [], "entities": []}, {"text": "CE-mixture, CESuperposition and Real-Embed are trained and tested in a completely identical process.", "labels": [], "entities": [{"text": "CE-mixture", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.673042356967926}, {"text": "CESuperposition", "start_pos": 12, "end_pos": 27, "type": "METRIC", "confidence": 0.5602073073387146}]}, {"text": "For the construction of training, validation and test data, they are readily available for SST dataset, and for the other four datasets we randomly split the whole data into 8:1:1 for training, validation and test data respectively.", "labels": [], "entities": [{"text": "SST dataset", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.8801346719264984}]}, {"text": "The embedding dimension is set to be 100.", "labels": [], "entities": []}, {"text": "We use batch training with batch size being 32 for SST and 16 for the other datasets.", "labels": [], "entities": [{"text": "SST", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9699753522872925}]}, {"text": "We adopt Adam as the optimizer and use the default parameters for Adam in Keras . The experiments are implemented in Keras and Tensorflow 4 under Python 3.6.4.", "labels": [], "entities": []}, {"text": "The experiment is run on a desktop with NVidia Quadro M4000 and 16GB RAM.", "labels": [], "entities": [{"text": "NVidia Quadro M4000", "start_pos": 40, "end_pos": 59, "type": "DATASET", "confidence": 0.8672281901041666}]}], "tableCaptions": [{"text": " Table 2: Experimental Results in percentage(%). The best performed value for each dataset is in bold.", "labels": [], "entities": []}]}