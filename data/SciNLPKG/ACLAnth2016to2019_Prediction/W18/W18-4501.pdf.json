{"title": [{"text": "Learning Diachronic Analogies to Analyze Concept Change", "labels": [], "entities": [{"text": "Analyze Concept Change", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.6406855980555216}]}], "abstractContent": [{"text": "We propose to study the evolution of concepts by learning to complete diachronic analogies between lists of terms which relate to the same concept at different points in time.", "labels": [], "entities": []}, {"text": "We present a number of models based on operations on word embedddings that correspond to different assumptions about the characteristics of diachronic analogies and change in concept vocabularies.", "labels": [], "entities": []}, {"text": "These are tested in a quantitative evaluation for nine different concepts on a corpus of Dutch newspapers from the 1950s and 1980s.", "labels": [], "entities": []}, {"text": "We show that a model which treats the concept terms as analogous and learns weights to compensate for diachronic changes (weighted linear combination) is able to more accurately predict the missing term than a learned transformation and two baselines for most of the evaluated concepts.", "labels": [], "entities": []}, {"text": "We also find that all models tend to be coherent in relation to the represented concept, but less discriminative in regard to other concepts.", "labels": [], "entities": []}, {"text": "Additionally , we evaluate the effect of aligning the time-specific embedding spaces using orthogonal Procrustes, finding varying effects on performance, depending on the model, concept and evaluation metric.", "labels": [], "entities": []}, {"text": "For the weighted linear combination, however, results improve with alignment in a majority of cases.", "labels": [], "entities": [{"text": "alignment", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9859886169433594}]}, {"text": "All related code is released publicly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research on the evolution of concepts is a long-standing topic within philosophy, history and linguistics.", "labels": [], "entities": []}, {"text": "However, recent work on the computational analysis of semantic change based on word embeddings has surprisingly little to offer in this regard.", "labels": [], "entities": [{"text": "computational analysis of semantic change", "start_pos": 28, "end_pos": 69, "type": "TASK", "confidence": 0.7325921654701233}]}, {"text": "Most work focuses on the meaning of individual words.", "labels": [], "entities": []}, {"text": "In comparison, there are only few contributions which analyze concepts and the changing vocabularies which are used to express them (.", "labels": [], "entities": []}, {"text": "The goal of this paper is to provide insights into how distributional semantics, in particular word embeddings, can be used to analyze concept change.", "labels": [], "entities": []}, {"text": "We propose to model concept change in terms of analogies between concept vocabularies at different points in time.", "labels": [], "entities": []}, {"text": "This extends well-established synchronic models of analogy based on word embeddings () to the diachronic case.", "labels": [], "entities": []}, {"text": "We build on the underlying parallelogram model of analogy, cf.), assuming that analogies of the type of \"a is to b as c is to d\" can be described by linear relationships between distributional representations of the four words.", "labels": [], "entities": []}, {"text": "While parallelogram relationships can be found in other vector representations as well (), embeddings derived with skip-gram can be considered a robust baseline for analogy tasks (.", "labels": [], "entities": []}, {"text": "We detail our approach (Section 3) and propose a number of simple models to learn diachronic analogies (Section 4) which are evaluated quantitatively (Section 6) on a corpus of historical Dutch newspapers (Section 5).", "labels": [], "entities": []}, {"text": "We report on two related experiments which are motivated by the intuition that diachronic analogies should be coherent in regard to the represented concept and discriminative in regard to the vocabulary of other concepts.", "labels": [], "entities": []}, {"text": "All related code is released publicly 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "Despite relying on their data, we cannot compare our results directly to , due to the differences in approach and evaluation period.", "labels": [], "entities": []}, {"text": "Therefore, we evaluate the models presented in Section 4 intrinsically in two separate experiments: The first experiment tests how well the different systems predict the missing vector b t 1 . Subsequently, we report on an experiment that evaluates how well these vectors can be mapped back onto the vocabulary to receive human-readable lists of terms.", "labels": [], "entities": []}, {"text": "These experiments are designed to assess as to what extent the diachronic transformations applied result in semantically coherent as well as discriminative concepts at t 1 . Results will be discussed and compared with and without alignment (cf. Section 4.3).", "labels": [], "entities": []}, {"text": "Note that drawing conclusions from this comparison is difficult, because the alignment inevitably changes the vocabulary and thereby the composition of the dataset, so that we effectively compare across two different datasets.", "labels": [], "entities": []}, {"text": "However, given the decisive stance of on alignment for diachronic embedding spaces, it seems important to evaluate its effects on the task.", "labels": [], "entities": []}, {"text": "We train and evaluate all four models separately for each concept listed in.", "labels": [], "entities": []}, {"text": "As some concepts only have few examples, all experiments are run in a k-fold cross-validation setting.", "labels": [], "entities": []}, {"text": "We use k = 5 and report scores averaged overall folds along with their standard deviation (in plots indicated by error bars).", "labels": [], "entities": []}, {"text": "To evaluate the prediction of the missing concept term, we use the cosine similarity between the predicted and the label vector and average overall examples.", "labels": [], "entities": []}, {"text": "For all concepts and models as well as irrespective of alignment, the standard deviation is very low, indicating stable performance across folds.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 70, "end_pos": 88, "type": "METRIC", "confidence": 0.9026395082473755}]}, {"text": "The ADD baseline generally has a lower score than the NO baseline except for koeien and boekje with alignment.", "labels": [], "entities": [{"text": "ADD baseline", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.6452960669994354}, {"text": "NO baseline", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.698659747838974}]}, {"text": "Apparently a naive search in the neighborhood of the core concept term for the later period is better than assuming a simple linear relationship between the terms across time periods.", "labels": [], "entities": []}, {"text": "Alignment improves the scores for the ADD baseline, but we see hardly any differences for NO.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8154579997062683}, {"text": "ADD baseline", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.6462912857532501}, {"text": "NO", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.8772343397140503}]}, {"text": "As the NO baseline ignores the aligned space t 0 , the small differences in performance might be caused by the differences in the dataset due to the altered vocabulary.", "labels": [], "entities": [{"text": "NO baseline", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.7508659064769745}]}, {"text": "The results for TRANS are strongly influenced by alignment.", "labels": [], "entities": [{"text": "TRANS", "start_pos": 16, "end_pos": 21, "type": "TASK", "confidence": 0.6155730485916138}]}, {"text": "With non-aligned spaces, the model is worse than the NO baseline for all concepts.", "labels": [], "entities": [{"text": "NO", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9483227729797363}]}, {"text": "With aligned spaces, the score of the TRANS model is dramatically higher than without, sometimes more than twice as high (boekje, utrecht), and consistently beats both baselines.", "labels": [], "entities": [{"text": "TRANS", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.5289646983146667}]}, {"text": "Apparently it is only beneficial to assume that the same transformation is applicable to both vector spaces across time when their dimensions are aligned, so that the transformation will have a similar effect.", "labels": [], "entities": []}, {"text": "WEIGHTS is the best performing model, sometimes with a notable margin.", "labels": [], "entities": [{"text": "WEIGHTS", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8966081142425537}]}, {"text": "When using alignment, we see hardly any improvements for WEIGHTS.", "labels": [], "entities": [{"text": "WEIGHTS", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.8505862951278687}]}, {"text": "Probably, in the non-aligned case the weights manage  to compensate for the missing alignment to a high degree, so that the alignment does not add much.", "labels": [], "entities": []}, {"text": "Comparing these results to ADD, it seems to be beneficial to include the t 0 embeddings for predicting the missing vector at t 1 only if weights are learned to account for the differences between the two vector spaces.", "labels": [], "entities": []}, {"text": "Comparing to the simple TRANS model, the performance without alignment is clearly superior, but with alignment WEIGHTS is only slightly better.", "labels": [], "entities": [{"text": "WEIGHTS", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9580234885215759}]}, {"text": "In the following, we will report on experiments that evaluate how well the predicted vectors can be used to retrieve meaningful word lists that help to study concept change.", "labels": [], "entities": []}, {"text": "To perform the mapping from an embedding to a term, the system takes a predicted vector and performs an n-nearest neighbor search overall word embeddings of the t 1 vector space using cosine similarity as proximity metric.", "labels": [], "entities": []}, {"text": "In the reported experiments, n = 10 is used.", "labels": [], "entities": []}, {"text": "Note that this evaluation setting is comparatively hard for the proposed models, as the vocabulary retrieval performance is not only determined by the quality of the prediction, but also by the other vectors in the t 1 space outside the set of vectors that are part of the ground truth dataset.", "labels": [], "entities": []}, {"text": "As these vectors were not seen by the model during training, performance inevitably is influenced to an arbitrary degree by the specific concepts and embedding spaces used.", "labels": [], "entities": []}, {"text": "We evaluate the vocabulary retrieval performance in terms of the mean reciprocal rank (MRR)) of the label terms in the ranked lists of vocabulary terms.", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR))", "start_pos": 65, "end_pos": 92, "type": "METRIC", "confidence": 0.8424721360206604}]}, {"text": "For a set of analogies A and the rank of the label term in the list of vocabulary terms for the i-th analogy rank i , the MRR is defined as When for any analogy the label term is not in the list of returned vocabulary terms (i.e., rank i is not defined), the reciprocal rank 1 rank i is set to 0.", "labels": [], "entities": [{"text": "MRR", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9207106828689575}]}, {"text": "Hence, this experiment aims at assessing whether the diachronic transformations applied yield a semantic space at t 1 that effectively discriminates between  different concepts such that terms included in the vocabulary of a concept should be consistently ranked higher than confounders from the vocabulary of other concepts, thus resulting in higher MRR scores.", "labels": [], "entities": [{"text": "MRR", "start_pos": 351, "end_pos": 354, "type": "METRIC", "confidence": 0.8716233968734741}]}, {"text": "shows MRR results per concept for each of the models and baselines using aligned and nonaligned vector spaces computed for the same dataset and 5-fold cross-validation setting as in Section 6.1.", "labels": [], "entities": [{"text": "MRR", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.6154584288597107}]}, {"text": "Overall, compared to Experiment 1, the results are much less uniform and stable.", "labels": [], "entities": []}, {"text": "The ADD baseline performs worse or comparable to the NO baseline.", "labels": [], "entities": [{"text": "ADD baseline", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.6778601408004761}, {"text": "NO baseline", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.7537819743156433}]}, {"text": "An exception is koeien with alignment.", "labels": [], "entities": []}, {"text": "These overall results for ADD are expected, since NO has higher COS scores than ADD for almost all concepts, independent of alignment.", "labels": [], "entities": [{"text": "ADD", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.948715090751648}, {"text": "COS", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9947366118431091}]}, {"text": "Alignment has no clear effect on MRR scores for ADD.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8989664912223816}, {"text": "MRR scores", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.8715275526046753}]}, {"text": "For some concepts (e.g., boekje) the baseline performs worse than non-aligned, for others (e.g., koeien) it performs better.", "labels": [], "entities": []}, {"text": "Surprisingly, without alignment TRANS often yields more relevant predictions than the two baselines, even though its COS scores are mostly lower (cf.).", "labels": [], "entities": [{"text": "TRANS", "start_pos": 32, "end_pos": 37, "type": "TASK", "confidence": 0.9169026017189026}, {"text": "COS", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9651180505752563}]}, {"text": "With aligned embedding spaces, the MRR performance of TRANS shows very variable behavior.", "labels": [], "entities": [{"text": "MRR", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9743862152099609}, {"text": "TRANS", "start_pos": 54, "end_pos": 59, "type": "TASK", "confidence": 0.7777616381645203}]}, {"text": "For some concepts, we see small improvements with alignment, in the case of atoombommen it is even very large.", "labels": [], "entities": [{"text": "alignment", "start_pos": 50, "end_pos": 59, "type": "TASK", "confidence": 0.7881924510002136}]}, {"text": "For other concepts, we see a drop in performance, sometimes very sharp as for beethoven or violen.", "labels": [], "entities": []}, {"text": "This is a puzzling result, since we see notable improvements in the cosine similarity score for TRANS when the vector spaces are aligned.", "labels": [], "entities": [{"text": "cosine similarity score", "start_pos": 68, "end_pos": 91, "type": "METRIC", "confidence": 0.7952719330787659}]}, {"text": "For most concepts, WEIGHTS performs best and often does so with a large margin -with or without alignment.", "labels": [], "entities": [{"text": "WEIGHTS", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.40691810846328735}]}, {"text": "Exceptions are utrecht and boekje with a score below the baselines.", "labels": [], "entities": []}, {"text": "Interestingly, while applying rotational alignment only leads to negligible improvements in COS for WEIGHTS, the MRR score is always higher with alignment than without, although for some concepts (e.g., kleurling) the standard deviation increases exceptionally.", "labels": [], "entities": [{"text": "COS", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9965195655822754}, {"text": "WEIGHTS", "start_pos": 100, "end_pos": 107, "type": "DATASET", "confidence": 0.6255766749382019}, {"text": "MRR score", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9834838211536407}]}], "tableCaptions": []}