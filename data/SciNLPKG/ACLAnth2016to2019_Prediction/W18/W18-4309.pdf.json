{"title": [{"text": "On Training Classifiers for Linking Event Templates", "labels": [], "entities": [{"text": "Linking Event Templates", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.9267539381980896}]}], "abstractContent": [{"text": "The paper reports on exploring various machine learning techniques and a range of textual and meta-data features to train classifiers for linking related event templates automatically extracted from online news.", "labels": [], "entities": [{"text": "linking related event templates automatically extracted from online news", "start_pos": 138, "end_pos": 210, "type": "TASK", "confidence": 0.6996927162011465}]}, {"text": "With the best model using textual features only we achieved 94.7% (92.9%) F1 score on GOLD (SILVER) dataset.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9786732494831085}, {"text": "GOLD (SILVER) dataset", "start_pos": 86, "end_pos": 107, "type": "DATASET", "confidence": 0.6193074882030487}]}, {"text": "These figures were further improved to 98.6% (GOLD) and 97% (SILVER) F1 score by adding meta-data features, mainly thanks to the strong discriminatory power of automatically extracted geographical information related to events.", "labels": [], "entities": [{"text": "GOLD", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.8502629399299622}, {"text": "SILVER) F1 score", "start_pos": 61, "end_pos": 77, "type": "METRIC", "confidence": 0.8900507092475891}]}], "introductionContent": [{"text": "With the rapid proliferation of large digital archives of textual information on what happens in the world, a need has raised recently to apply effective techniques that go beyond the classification and retrieval of text documents in response to profiled queries.", "labels": [], "entities": [{"text": "classification and retrieval of text documents", "start_pos": 184, "end_pos": 230, "type": "TASK", "confidence": 0.7636758089065552}]}, {"text": "Systems already exists that automatically distill structured information on events from free texts, e.g. with the goal of monitoring disease outbreaks (, crisis situations () and other security-related events from online news.", "labels": [], "entities": []}, {"text": "Classical event extraction engines typically extract knowledge by locally matching predefined event templates in text documents, by filling template slots with detected entities.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7857810258865356}]}, {"text": "However, when not coupled with modules for event co-reference detection, these systems tend to suffer of the event duplication problem, consisting of extracting several mentions referring to the same occurring event.", "labels": [], "entities": [{"text": "event co-reference detection", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6475685238838196}]}, {"text": "That makes their output misleading for both real-time situation monitoring and long-term data aggregation and analysis.", "labels": [], "entities": []}, {"text": "While event co-reference is a semantically well-defined relationship (, capturing some additional kinds of relationships, although more fuzzy, that link together events, maybe crucial in order to reduce the information overload of the user of an event extraction engine.", "labels": [], "entities": [{"text": "event extraction engine", "start_pos": 246, "end_pos": 269, "type": "TASK", "confidence": 0.797200342019399}]}, {"text": "Imagine a scenario where, given a large set of news reports about a major Terrorist Attack event, an event extraction engine returns a number of event templates like the ones shown in.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.7111333757638931}]}, {"text": "As it can be noticed from Title and Text of the source articles, while templates a. and b. describe the same main fact (the attack itself), c. provides updates on some police operations following it, d. tells about some public reactions to the event, while e. is about an official claiming of the attack by one terrorist organization.", "labels": [], "entities": []}, {"text": "Recognizing a. and b. as duplicate reporting of the same event would help mitigating the information redundancy in the system.", "labels": [], "entities": []}, {"text": "At the same time, while c., d. and e. should be regarded as semantically distinct events from a., extracting them as independent templates would result in a loss of information preventing a data user to obtain a complete picture of the ongoing situation.", "labels": [], "entities": []}, {"text": "On the contrary, we envision an user-centered process, where an analyst is fed with a target event template and is allowed to explore on demand additional event templates, by calling an on-the-fly computation of related events in order to update the information from the original record.", "labels": [], "entities": []}, {"text": "In this context, we explore the possibility to merge a number of distinct event-event relationships) into a more general, user-centered definition of event linking, and experiment on training statistical classifiers for automatically detecting those links based on textual and non-textual content of event templates.", "labels": [], "entities": []}, {"text": "The motivation behind our work is fourfold.", "labels": [], "entities": []}, {"text": "Firstly, we are interested in elaboration of techniques for linking event information in existing event datasets, such as the one presented in, in order to improve their usability by the analysts.", "labels": [], "entities": []}, {"text": "Therefore we have exploited this corpus to carryout the presented work.", "labels": [], "entities": []}, {"text": "Secondly, as the event extraction engine underlying the corpus is multilingual, we focus on exploring linguistically-lightweight event similarity metrics.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7320455312728882}]}, {"text": "Thirdly, we are interested in exploring how inclusion of automatically extracted event metadata (e.g., location) impacts the performance of the trained event linking models.", "labels": [], "entities": []}, {"text": "Finally, due to scarcity of publicly available resources for carrying out research on event linking our intention was to contribute to the provision of such a resource, focusing particularly on creating a dataset that resembles a real-world scenario of event data for analysts, who are primarily interested in having access to all relevant event information rather than being provided with fine-grained labeling of event relations (e.g., temporal and causal).", "labels": [], "entities": [{"text": "event linking", "start_pos": 86, "end_pos": 99, "type": "TASK", "confidence": 0.7312841266393661}]}, {"text": "Event linking has been modeled as the task of matching monolingual clusters of news articles, describing the same event, across languages.", "labels": [], "entities": [{"text": "Event linking", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7728790938854218}]}, {"text": "For example) use a number of techniques, including Canonical Correlation Analysis, exploiting comparable corpora such as Wikipedia.", "labels": [], "entities": [{"text": "Canonical Correlation Analysis", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.7854142983754476}]}, {"text": "Work similar to ours was performed in the context of the event co-reference resolution task, that consists of clustering of event mentions that refer to the same event ().", "labels": [], "entities": [{"text": "event co-reference resolution task", "start_pos": 57, "end_pos": 91, "type": "TASK", "confidence": 0.6984472051262856}]}, {"text": "We diverge from both task formulations in that our underlying representation of events is richer than local event mentions, including meta-data and text slots from clusters of articles.", "labels": [], "entities": []}, {"text": "proposes a task of linking tweets with news articles to enable other NLP tools to better understand Twitter feeds.", "labels": [], "entities": []}, {"text": "Related work to event linking was also reported in.", "labels": [], "entities": [{"text": "event linking", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.808075875043869}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives an overview of the event linking task.", "labels": [], "entities": [{"text": "event linking task", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.8265818357467651}]}, {"text": "The event similarity metrics explored are introduced in Section 3.", "labels": [], "entities": []}, {"text": "Subsequently, the experiments set-up and evaluation results are presented in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we end up with conclusions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We built 2 corpora consisting of event template pairs taken from the event dataset described in) and labeled as either related or unrelated.", "labels": [], "entities": []}, {"text": "First, we attempted to create balanced groups of event templates, where initial groups were built by extracting events (not less than 5) around keys consisting of a category, location (country) and a timeslot (e.g. time window of +-2 days) in 2017.", "labels": [], "entities": []}, {"text": "Each of such initial groups G was subsequently amended with a set of max.", "labels": [], "entities": []}, {"text": "|G|/6 most 'similar' events from the same time window and another set of max.", "labels": [], "entities": []}, {"text": "|G|/6 most 'similar' events from 2017, but outside of the original time window.", "labels": [], "entities": []}, {"text": "The events were selected through computing cosine similarity with the centroid template in G 8 . Finally, G was amended by adding |G|/3 of randomly selected events (disjoint from the previous groups) from the same time window, regardless of location, category and similarity.", "labels": [], "entities": []}, {"text": "For each resulting group, all event pairs were computed, which were then labeled by 4 annotators, who were asked to consider only textual and meta-data information in the templates.", "labels": [], "entities": []}, {"text": "The average pairwise \u03ba score for inter-annotator agreement on a sample of around 13.4K event pairs was over 0.85.", "labels": [], "entities": []}, {"text": "Questionable cases were typically due to event granularity issues.", "labels": [], "entities": []}, {"text": "For example, the two events in were arguably perpetrated by the same armed group as part of a same armed conflict in the same day and larger area.", "labels": [], "entities": []}, {"text": "Whether the two killing incidents should be considered as different consequences of the same larger armed conflict event and thus be considered as related, or should they be considered as distinct events sharing a large number of slot values is an open question.: GOLD/SILVER dataset statistics.", "labels": [], "entities": [{"text": "GOLD/SILVER dataset", "start_pos": 264, "end_pos": 283, "type": "DATASET", "confidence": 0.5809642225503922}]}, {"text": "The first 2 columns provide number of related (unrelated) event pairs, the others provide % of events falling into: crisis-violence (CRI-VIO), civic-political action (CIV-POL), man-made disasters (MM-DIS), natural disasters (NAT-DIS) and military actions (MIL).", "labels": [], "entities": []}, {"text": "We used pairs with at least 2 non-conflicting judgments to build a GOLD dataset, whereas SILVER dataset was created on top of it through adding event pairs annotated only by one annotator.", "labels": [], "entities": [{"text": "GOLD dataset", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.8286851644515991}, {"text": "SILVER dataset", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.6859441101551056}]}, {"text": "Detailed statistics are provided in.", "labels": [], "entities": []}, {"text": "Experiments were carried out using five different ML models, namely: SVM, Stochastic gradient descent classifier (regularized linear model learned), Decision Tree, Random Forest and AdaBoost classifier.", "labels": [], "entities": []}, {"text": "All models were implemented using).", "labels": [], "entities": []}, {"text": "Hyper-parameters of each model were tuned using grid search.", "labels": [], "entities": []}, {"text": "Each model was trained using full set of event similarity metrics as features and on a subset of features obtained using feature selection SelectFromModel with base estimator being Random Forest.", "labels": [], "entities": []}, {"text": "All models consistently exhibited better performance when using all features vis-a-vis subset of features obtained through feature selection.", "labels": [], "entities": []}, {"text": "All models were trained on the same train-test split (80:20) and 5-fold cross-validation was performed.", "labels": [], "entities": []}, {"text": "Noteworthy, in case of 'missing' features, i.e., whenever event metric could not be computed (e.g., due to missing elements such as named entities or numerical expressions to be compared), we set the respective values to the mean in the corresponding feature distribution assuming that lack of elements to compare should be scored higher than \"zero\" overlap (e.g., different named entities in both texts).", "labels": [], "entities": []}, {"text": "Finally, we carried out the evaluation on both datasets described in 4.1 in two set-ups, one with textbased features only, and second one with both textual and meta-data features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: GOLD/SILVER dataset statistics. The first 2 columns provide number of related (unrelated)  event pairs, the others provide % of events falling into: crisis-violence (CRI-VIO), civic-political action  (CIV-POL), man-made disasters (MM-DIS), natural disasters (NAT-DIS) and military actions (MIL).", "labels": [], "entities": [{"text": "GOLD/SILVER dataset", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.6852535456418991}]}, {"text": " Table 2: Performance on the GOLD (top) and SILVER (bottom) dataset (F1 scores).", "labels": [], "entities": [{"text": "GOLD", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.6915711164474487}, {"text": "SILVER", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9780002236366272}, {"text": "F1 scores", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9705460965633392}]}]}