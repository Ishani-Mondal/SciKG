{"title": [{"text": "Multi-glance Reading Model for Text Understanding", "labels": [], "entities": [{"text": "Text Understanding", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.8306180238723755}]}], "abstractContent": [{"text": "In recent years, a variety of recurrent neu-ral networks have been proposed, e.g LST-M, however, existing models only read the text once, it cannot describe the situation of repeated reading in reading comprehension.", "labels": [], "entities": []}, {"text": "In fact, when reading or analyzing a text, we may read the text several times rather than once if we couldn't well understand it.", "labels": [], "entities": []}, {"text": "So, how to model this kind of the reading behavior?", "labels": [], "entities": []}, {"text": "To address the issue, we propose a multi-glance mechanism (MG-M) for modeling the habit of reading behavior.", "labels": [], "entities": []}, {"text": "In the proposed framework, the actual reading process can be fully simulated , and then the obtained information can be consistent with the task.", "labels": [], "entities": []}, {"text": "Based on the multi-glance mechanism, we design two types of recurrent neural network models for repeated reading: Glance Cell Model (GCM) and Glance Gate Model (GGM).", "labels": [], "entities": []}, {"text": "Visualization analysis of the GCM and the GGM demonstrates the effectiveness of multi-glance mechanisms.", "labels": [], "entities": [{"text": "GCM", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.9073889255523682}, {"text": "GGM", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.9440913796424866}]}, {"text": "Experiments results on the large-scale datasets show that the proposed methods can achieve better performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text understanding is one of the fundamental tasks in Natural Language Processing areas.", "labels": [], "entities": [{"text": "Text understanding", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8787200152873993}]}, {"text": "These years we have seen significant progress in applying neural networks to text analysis applications.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.8604727983474731}]}, {"text": "Recurrent neural network is widely used because of its effective capability of capturing the sequential information.", "labels": [], "entities": []}, {"text": "Long short-term memory (LST-M)) and gated recurrent neural network () have achieved state-of-the-art performance in many areas, such as sentiment analysis, document classification ( ) and neural machine translation.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.9719475209712982}, {"text": "document classification", "start_pos": 156, "end_pos": 179, "type": "TASK", "confidence": 0.8027979731559753}, {"text": "neural machine translation", "start_pos": 188, "end_pos": 214, "type": "TASK", "confidence": 0.63612961769104}]}, {"text": "Besides the success achieved by these basic recurrent neural models, there are also a lot of interesting research works conducted in text analysis.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 133, "end_pos": 146, "type": "TASK", "confidence": 0.8578238189220428}]}, {"text": "Depending on the parsing tree structures, tree-LSTM () and recursive neural network() are proposed.", "labels": [], "entities": []}, {"text": "Bidirectional recurrent neural networks can get the backward features.", "labels": [], "entities": []}, {"text": "In order to align the hidden states, attention mechanism is widely used in language processing.", "labels": [], "entities": []}, {"text": "One of the common characteristics of these existing models is to model only single reading processing and generate a sequence of hidden states ht , as a function of the previous hidden states h t\u22121 and the current input (.", "labels": [], "entities": []}, {"text": "However, the fact is that when we read a text only once, we may merely know the general idea of it, especially when the text is long and obscure.", "labels": [], "entities": []}, {"text": "More often than not, we know that fast repeated reading is more effective than slow careful reading, so, for the obscure text, our primary schoolteacher always teaches us to read several times to get the theme of the text.", "labels": [], "entities": []}, {"text": "In addition, this kind of rereading can help us find some of the details that are ignored when we first glance.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel multi-glance mechanism to model our reading habit: when reading a text, first we will glance through it to get the general meaning and then based on the information we obtained, we will read the text again in order to find some important contents.", "labels": [], "entities": []}, {"text": "Based on the multi-glance mechanism we proposed (  Glance Cell Model (GCM) and Glance Gate Model (GGM).", "labels": [], "entities": [{"text": "Glance Gate Model (GGM)", "start_pos": 79, "end_pos": 102, "type": "METRIC", "confidence": 0.6951716542243958}]}, {"text": "GCM has a special cell to memorize the first impression information obtained after finishing the first reading.", "labels": [], "entities": [{"text": "GCM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9387208819389343}]}, {"text": "GGM has a special gate to control current input and output in order to filter words that are not important.", "labels": [], "entities": [{"text": "GGM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9446228742599487}]}, {"text": "The main contributions of this work are summarized as follows: \u2022 We propose a novel multi-glance mechanism which models the habit of reading.", "labels": [], "entities": []}, {"text": "Comparing to traditional sequential models, our proposed models can better simulate people's reading process and better understand the content.", "labels": [], "entities": []}, {"text": "\u2022 Based on multi-glance mechanism, we propose GCM which can takes the first impression information into consideration.", "labels": [], "entities": []}, {"text": "Glance cell model has a special cell to memorize the global impression information we obtain and add it into the current calculation.", "labels": [], "entities": []}, {"text": "\u2022 Based on multi-glance mechanism, we propose GGM which adopts a extra gate to ignore the less important words and focus on details in the contents.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we conduct experiments on different datasets to evaluate the performance of multiglance mechanism.", "labels": [], "entities": []}, {"text": "We also visualize the glance layers in both glance models.", "labels": [], "entities": []}, {"text": "We evaluate the effectiveness of our glance models on four different datasets .  The datasets are split into training, validation and test sets with the proportion of 8:1:1.", "labels": [], "entities": []}, {"text": "We use the Stanford CoreNLP for tokenization and sentence splitting.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 11, "end_pos": 27, "type": "DATASET", "confidence": 0.9319799244403839}, {"text": "tokenization", "start_pos": 32, "end_pos": 44, "type": "TASK", "confidence": 0.9805068969726562}, {"text": "sentence splitting", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7437420189380646}]}, {"text": "For training, we pre-train the word vector and set the dimension to be 200 with SkipGram (.", "labels": [], "entities": [{"text": "SkipGram", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.6788125038146973}]}, {"text": "In our glance models, the dimensions of hidden states and cells states are set to 200 and the hidden states and cells states initialized randomly.", "labels": [], "entities": []}, {"text": "We adopt AdaDelta to train our models , select the best configuration based on the validation set, and evaluate the performance on the test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistical information of IMDB, Ama- zon, Yelp 2013, Yelp 2014 datasets", "labels": [], "entities": [{"text": "IMDB", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.7175534963607788}, {"text": "Ama- zon, Yelp 2013, Yelp 2014 datasets", "start_pos": 43, "end_pos": 82, "type": "DATASET", "confidence": 0.8336897850036621}]}, {"text": " Table 2: Text analysis results on IMDB, Yelp2014, yelp2013 and Amazon datasets. Evaluation metrics  is Accuracy in percentage (higher the better). The best performance in each group is in bold.", "labels": [], "entities": [{"text": "IMDB", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9541845917701721}, {"text": "Yelp2014", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.9046339988708496}, {"text": "yelp2013", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8861294388771057}, {"text": "Amazon datasets", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.8496662974357605}, {"text": "Accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9986028075218201}]}]}