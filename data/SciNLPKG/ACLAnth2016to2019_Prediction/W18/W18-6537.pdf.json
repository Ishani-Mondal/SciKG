{"title": [{"text": "Evaluation methodologies in Automatic Question Generation 2013-2018", "labels": [], "entities": [{"text": "Automatic Question Generation 2013-2018", "start_pos": 28, "end_pos": 67, "type": "TASK", "confidence": 0.7326881811022758}]}], "abstractContent": [{"text": "In the last few years Automatic Question Generation (AQG) has attracted increasing interest.", "labels": [], "entities": [{"text": "Automatic Question Generation (AQG)", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.7832211305697759}]}, {"text": "In this paper we survey the evaluation methodologies used in AQG.", "labels": [], "entities": []}, {"text": "Based on a sample of 37 papers, our research shows that the systems' development has not been accompanied by similar developments in the methodologies used for the systems' evaluation.", "labels": [], "entities": []}, {"text": "Indeed, in the papers we examine here, we find a wide variety of both intrinsic and extrinsic evaluation methodologies.", "labels": [], "entities": []}, {"text": "Such diverse evaluation practices make it difficult to reliably compare the quality of different generation systems.", "labels": [], "entities": []}, {"text": "Our study suggests that, given the rapidly increasing level of research in the area, a common framework is urgently needed to compare the performance of AQG systems and NLG systems more generally.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluation is a critical phase for the development of Natural Language Generation (NLG) systems.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.8133360048135122}]}, {"text": "It helps to improve performance by highlighting weaknesses, and to identify new tasks to which generation systems can be applied.", "labels": [], "entities": []}, {"text": "Given that generation systems and evaluation methodologies should be developed hand in hand, a systematic study of evaluation methodologies for NLG should take a central role in the effort of building machines which are able to reach human-like levels of linguistic communication.", "labels": [], "entities": []}, {"text": "Such a study should investigate the current evaluation practices used in various areas of NLG in order to see their weaknesses and suggest directions to improve them.", "labels": [], "entities": []}, {"text": "The aim of this paper is to analyze the evaluation methodologies used in Automatic Question Generation (AQG) as a representative subtask of NLG.", "labels": [], "entities": [{"text": "Automatic Question Generation (AQG)", "start_pos": 73, "end_pos": 108, "type": "TASK", "confidence": 0.7254747599363327}]}, {"text": "To the best of our knowledge, since the introduction of the Question Generation Shared Task Evaluation Challenge (QG-STEC) (, no attempts have been made to introduce a common framework for evaluation in AQG.", "labels": [], "entities": [{"text": "Question Generation Shared Task Evaluation Challenge (QG-STEC)", "start_pos": 60, "end_pos": 122, "type": "TASK", "confidence": 0.7173425389660729}]}, {"text": "To approach this task, we examined the papers in the ACL anthology with a publication date between the years.", "labels": [], "entities": [{"text": "ACL anthology", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.964055061340332}]}, {"text": "shows the distribution of the papers involved in the current study across this period.", "labels": [], "entities": []}, {"text": "The ACL anthology website represents a resource of inestimable value 1 for this work.", "labels": [], "entities": [{"text": "ACL anthology website", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.9472716848055521}]}, {"text": "Year of publication # papers 2018 (Jan-June) 7 (so far)  We used the single term question generation as the search term with the search engine provided in the ACL Anthology website.", "labels": [], "entities": [{"text": "single term question generation", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.629639744758606}, {"text": "ACL Anthology website", "start_pos": 159, "end_pos": 180, "type": "DATASET", "confidence": 0.9419949452082316}]}, {"text": "From the papers that were returned by this query, we focussed only on those papers that were about question generation systems.", "labels": [], "entities": [{"text": "question generation systems", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.8167080879211426}]}, {"text": "This gave us 37 papers to analyze, of which 36 were published in conference proceedings and 1 was published in a journal.", "labels": [], "entities": []}, {"text": "The number of papers by year is given in and illustrated in. indicates the rapid increase in publications in this area in recent years.", "labels": [], "entities": []}, {"text": "Note that this study of the literature was carried out in June 2018, and so several major conferences in this area (including ACL, INLG, EMNLP and COLING) had not taken place.", "labels": [], "entities": [{"text": "ACL", "start_pos": 126, "end_pos": 129, "type": "DATASET", "confidence": 0.8860160112380981}, {"text": "INLG", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.5391567349433899}, {"text": "EMNLP", "start_pos": 137, "end_pos": 142, "type": "DATASET", "confidence": 0.6890522837638855}, {"text": "COLING", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.582484781742096}]}, {"text": "Before looking more closely at the publications involved, let us introduce the AQG tasks studied in these papers.", "labels": [], "entities": [{"text": "AQG", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.5979021787643433}]}, {"text": "AQG is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation ().", "labels": [], "entities": []}, {"text": "The above definition, adopted by the AQG community, leaves room for researchers to decide what kind of questions and input work with.", "labels": [], "entities": [{"text": "AQG", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.8439900875091553}]}, {"text": "Following Piwek and Boyer (2012) a particular AQG task can be characterized by three aspects: the input, the 2 A complete list of papers used in this study, as well as useful information to reproduce the results presented in the present paper, can be found at the following link: https://bit.ly/2IuPJIa output, and finally the relationship between the input and the output.", "labels": [], "entities": []}, {"text": "The 37 papers we analyzed can be divided into the following three categories: 1.", "labels": [], "entities": []}, {"text": "Input: text; Output: text; Relation: the output question is answered by the input text or the output question asks a clarification question about the input text.", "labels": [], "entities": []}, {"text": "2. Input: knowledge base structured data (for example triples subject, object, subject/object relation); Output: text; Relation: the output question is answered by the information structure in the input.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present the findings of our analysis.", "labels": [], "entities": []}, {"text": "We focus our analysis on two dimensions: intrinsic evaluation methodology and extrinsic evaluation methodology.", "labels": [], "entities": []}, {"text": "Intrinsic evaluation methods measure the performance of a system by evaluating the system's output \"in its own right, either against a reference corpus or by eliciting human judgements of quality\").", "labels": [], "entities": []}, {"text": "For example, this could involve measuring the output's grammaticality and fluency.", "labels": [], "entities": []}, {"text": "The prevailing intrin-sic methods are human evaluation and automatic evaluation.", "labels": [], "entities": [{"text": "human evaluation", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7207863330841064}]}, {"text": "In order to assess the quality of a generated sentence, the former method uses human judgements, while the latter applies an algorithm that automatically calculates a score, for example by checking the similarity between the generated sentence and a set of reference sentences.", "labels": [], "entities": []}, {"text": "Extrinsic methods measure the performance of a system by evaluating the system's output with respect to its ability to accomplish the task for which it was developed.", "labels": [], "entities": []}, {"text": "An example of extrinsic evaluation methods is that used to evaluate the STOP system (.", "labels": [], "entities": [{"text": "STOP", "start_pos": 72, "end_pos": 76, "type": "TASK", "confidence": 0.697003960609436}]}, {"text": "STOP generates \"short tailored smoking cessation letters, based on responses to a four-page smoking questionnaire\" (p.", "labels": [], "entities": []}, {"text": "41) with the aim of helping people to give up smoking.", "labels": [], "entities": []}, {"text": "This system was evaluated \"by recruiting 2553 smokers, sending 1/3 of them letters produced by STOP and the other 2/3 control letters, and then measuring how many people in each group managed to stop smoking\" 3 . In this case the system was evaluated in the real world to see whether it has the desired effect, of helping people to quit smoking.", "labels": [], "entities": [{"text": "STOP", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.7562176585197449}]}, {"text": "The results showed that there were no relevant differences between the STOP letters and the control letters.", "labels": [], "entities": [{"text": "STOP letters", "start_pos": 71, "end_pos": 83, "type": "TASK", "confidence": 0.43171271681785583}]}, {"text": "the frequency of use of intrinsic compared to extrinsic methods, confirms the trend identified in.", "labels": [], "entities": []}, {"text": "Gkatzia and Mahamood found out that the 74.7% of the papers used the intrinsic evaluation method.", "labels": [], "entities": []}, {"text": "In our analysis we found that 83% of the papers used this methodology.", "labels": [], "entities": []}, {"text": "However, we note that with respect to Gkatzia and Mahamood's results, we have an inverted trend between the use of an extrinsic method compared to both intrinsic and extrinsic.", "labels": [], "entities": []}, {"text": "Indeed, Gkatzia and Mahamood found that 15.2% of the papers used extrinsic methods, against the 6% we get in our analysis, and 10.1% of the papers used both methodologies, where our analysis shows that 11% of the papers use a combination of both.", "labels": [], "entities": []}, {"text": "Furthermore, our analysis confirms the trend between the use of automatic compared to human intrinsic evaluation methodologies.", "labels": [], "entities": []}, {"text": "In the authors report that in 45.4% of the cases human evaluation is used, whereas in 38.2% of the cases automatic evaluation were adopted.", "labels": [], "entities": []}, {"text": "Similarly, our analysis shows that between the papers that prefer intrinsic evaluation methods, 45% used human evaluation, 32% used automatic evaluation and 23% used both human and automatic evaluation.", "labels": [], "entities": []}, {"text": "shows that in the period since 2016, there has been a considerable increase in the number of publications in this area.", "labels": [], "entities": []}, {"text": "It therefore makes sense to ask whether this increase has been accompanied with a change in the evaluation methodologies used.", "labels": [], "entities": []}, {"text": "shows how the range of evaluation methodologies used has changed.", "labels": [], "entities": []}, {"text": "Between the years 2013 -2015 only intrinsic evaluation methodologies were used -with 75% of papers using human evaluation, 12.5% using automatic evaluation and 12.5% using both methodologiesfor the years between 2016 -2018 extrinsic evaluation methods have also been introduced.", "labels": [], "entities": []}, {"text": "Indeed, although the majority of the papers in this period (79%) used intrinsic evaluation methods, 7% of papers used extrinsic evaluation methods and 14% used both the methodologies.", "labels": [], "entities": []}, {"text": "We can also see a change in the tendency to use intrinsic methods.", "labels": [], "entities": []}, {"text": "Between the years 2016 -2018, 35% of the papers used human evaluation (a decrease of 40% from the years between 2013 -2015), 39% of the papers used automatic evaluation (a 26.5% increase on the years between 2013 -2015) and 26% of the papers used both methodologies (a 13.5% increase on the years between 2013 -2015).", "labels": [], "entities": []}, {"text": "presents a list of automatic metrics used in the papers studied in the present research.", "labels": [], "entities": []}, {"text": "From our analysis it turns out that the most used automatic metric is BLEU followed by METEOR.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9992333650588989}, {"text": "METEOR", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9933860898017883}]}, {"text": "Note that only describes those that use the specified metrics; other papers use metrics that are defined for the specific aims described in the paper that introduces them.: Automatic metrics used.", "labels": [], "entities": []}, {"text": "In our survey we found out that 31% of the papers used just a single metric, whereas the other 69% used more than one.", "labels": [], "entities": []}, {"text": "The average is 2 metrics per paper, with a minimum of 1 metric (6 papers) and a maximum of 5 metrics (1 paper).", "labels": [], "entities": []}, {"text": "In almost 50% of cases (9 papers), 3 metrics were used.", "labels": [], "entities": []}, {"text": "We noticed that only a single paper used an embedding based metric (see).", "labels": [], "entities": []}, {"text": "Ina majority of studies, word-overlap based metrics were used (see).", "labels": [], "entities": []}, {"text": "In the last few years, many studies in NLG have shed light on the correlation between human judgement and automatic metrics.", "labels": [], "entities": []}, {"text": "The results, which have shown how this correspondence is somewhat weak 4 , shed doubt on the feasibility of using these metrics for evaluating the overall quality of a system.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, the area of AQG is currently missing a study which aims to verify the correlation between human judgement and automatic metrics . Such research would have two merits: on one hand, this kind of meta-evaluation study would give a better characterisation of the general problem.", "labels": [], "entities": []}, {"text": "On the other hand, the research could provide guidance to researchers about which metric is most appropriate in evaluating a particular model or system.", "labels": [], "entities": []}, {"text": "In conclusion, we believe that research in AQG would benefit from a systematic study that aims to clarify the relation between different evaluation methodologies.", "labels": [], "entities": []}, {"text": "Among the various human evaluation methodologies, eliciting quality judgments is most common: human annotators are asked to assess the quality of a question based on criteria such as question's grammaticality and fluency.", "labels": [], "entities": []}, {"text": "Only two papers used a preference judgement methodology, in which the human annotators are asked either to assess pairwise preference between questions or given a couple of questions, one human generated and one automatically generated, assess which one is automatically generated (or which one is the the human generated).", "labels": [], "entities": []}, {"text": "One of these papers also used the other methodology of eliciting quality judgements.", "labels": [], "entities": []}, {"text": "Quality judgment methodologies typically ask annotators to use Likert or rating scales to record their judgements.", "labels": [], "entities": [{"text": "Quality judgment", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7040628790855408}]}, {"text": "In our analysis, we found that 56% of the papers used some kind of numerical scale.", "labels": [], "entities": []}, {"text": "For example, human annotators were often asked to assess the grammaticality of a question on a scale from 1 (worst) to 5 (best).", "labels": [], "entities": []}, {"text": "On the other hand, 44% of the papers used a linguistic (or semantic) scale.", "labels": [], "entities": []}, {"text": "In these cases, human annotators were typically asked to classify the questions in some category such as coherent, somewhat coherent or incoherent.", "labels": [], "entities": []}, {"text": "The number of categories used in the Likert or rating scales by the papers that adopted quality judgment methodologies are shown in.", "labels": [], "entities": []}, {"text": "Only three papers used more than one scale in the evaluation.", "labels": [], "entities": []}, {"text": "One of these uses a free scale in which the annotators have to choose a positive integer to count the inference steps necessary for answer a question.", "labels": [], "entities": []}, {"text": "As shown in, extrinsic evaluation methodologies are rare in the area.", "labels": [], "entities": []}, {"text": "As reported by this is generally true for NLG tasks.", "labels": [], "entities": []}, {"text": "Amongst the papers that have chosen to use this kind of evaluation technique, human judges were used in 4 times out of the 6.", "labels": [], "entities": []}, {"text": "In the papers where human judges were not used, the Question Generation (QG) system was tested as a component of a Question Answering (QA) system.", "labels": [], "entities": [{"text": "Question Generation (QG)", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.7457712888717651}, {"text": "Question Answering (QA)", "start_pos": 115, "end_pos": 138, "type": "TASK", "confidence": 0.810169643163681}]}, {"text": "The performance was evaluated by checking the difference between the QA system without the use of the QG system against the performance of the QA system with the use of the QG system.", "labels": [], "entities": []}, {"text": "The aim of those papers was to improve QA systems by creating more accurate question/answer pairs to be used for training purposes.", "labels": [], "entities": [{"text": "QA", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9886003136634827}]}, {"text": "As a consequence of the different tasks in play, the other papers used humans in different ways.", "labels": [], "entities": []}, {"text": "We can find tasks such as: answer the generated questions or use the generated questions in a web page and then answer a survey about the utility of those questions.", "labels": [], "entities": []}, {"text": "Or also: engage in a conversation with a chatbot which involves a question-based dialogue, and then rate the conversations.", "labels": [], "entities": []}, {"text": "Also in this case, the number of humans involved in the evaluation varies from paper to paper, ranging from 2 to 81.", "labels": [], "entities": []}, {"text": "In contrast to the case of intrinsic human evaluation, in this case the IAA is not reported.", "labels": [], "entities": [{"text": "IAA", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.985129714012146}]}, {"text": "We note that human agreement in extrinsic evaluation is not as relevant as in the case of intrinsic evaluation.", "labels": [], "entities": []}, {"text": "Indeed, for intrinsic evaluations, agreement is required to have a reliability and validity measure of the evaluation scheme and guidelines.", "labels": [], "entities": [{"text": "validity", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.8684868812561035}]}, {"text": "The agreement measure should gather evidence that different humans can make similar judgements about the questions evaluated.", "labels": [], "entities": []}, {"text": "This fact, following should allow us to answer the question of: \"how much the resulting data can be trusted to represent something real\"?", "labels": [], "entities": []}, {"text": "In human intrinsic evaluation, the agreement can be seen as a measure of the replicability of the results.", "labels": [], "entities": []}, {"text": "For example, Carletta (1996, p. 1) wrote: At onetime, it was considered sufficient.", "labels": [], "entities": []}, {"text": "to show examples based on the authors' interpretation.", "labels": [], "entities": []}, {"text": "Research was judged according to whether or not the reader found the explanation plausible.", "labels": [], "entities": []}, {"text": "Now, researchers are beginning to require evidence that people besides the authors themselves can understand and make the judgments underlying the research reliably.", "labels": [], "entities": []}, {"text": "This is a reasonable requirement because if researchers can't even show that different people can agree about the judgments on which their research is based, then there is no chance of replicating the research results.", "labels": [], "entities": []}, {"text": "In the case of extrinsic methods, the evaluation aim is to check if the generated sentences fulfil the task for which they were generated.", "labels": [], "entities": []}, {"text": "To test this, humans need to use those sentences in real contexts.", "labels": [], "entities": []}, {"text": "Now, humans make use of the same tools in different ways, and similarly they answer questions in different ways.", "labels": [], "entities": []}, {"text": "For this reason, it is not expected that humans reach similar results in areal context of language use.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of papers per year describing  question generation systems.", "labels": [], "entities": [{"text": "question generation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7944380939006805}]}, {"text": " Table 2: Number of papers per conference pro- ceedings or journal.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation methodologies used.", "labels": [], "entities": []}, {"text": " Table 4: Variation of the evaluation methodolo- gies used between 2013 -2015 and between 2016  -2018.", "labels": [], "entities": []}, {"text": " Table 5: Automatic metrics used.", "labels": [], "entities": []}, {"text": " Table 6: Number of categories used in the Likert  or rating scales.", "labels": [], "entities": []}, {"text": " Table 7. 13% of the pa- pers (3 studies) used both specific criteria and an  overall criterion. As", "labels": [], "entities": []}, {"text": " Table 8: Measures of Inter-Annotator Agreement.", "labels": [], "entities": []}]}