{"title": [{"text": "Unsupervised Source Hierarchies for Low-Resource Neural Machine Translation", "labels": [], "entities": [{"text": "Low-Resource Neural Machine Translation", "start_pos": 36, "end_pos": 75, "type": "TASK", "confidence": 0.636509545147419}]}], "abstractContent": [{"text": "Incorporating source syntactic information into neural machine translation (NMT) has recently proven successful (Eriguchi et al., 2016; Luong et al., 2016).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.7505993247032166}]}, {"text": "However, this is generally done using an outside parser to syntactically annotate the training data, making this technique difficult to use for languages or domains for which a reliable parser is not available.", "labels": [], "entities": []}, {"text": "In this paper, we introduce an unsupervised tree-to-sequence (tree2seq) model for neural machine translation; this model is able to induce an unsupervised hierarchical structure on the source sentence based on the downstream task of neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.6805684963862101}, {"text": "neural machine translation", "start_pos": 233, "end_pos": 259, "type": "TASK", "confidence": 0.7790526549021403}]}, {"text": "We adapt the Gumbel tree-LSTM of Choi et al.", "labels": [], "entities": [{"text": "Gumbel tree-LSTM of Choi et al", "start_pos": 13, "end_pos": 43, "type": "DATASET", "confidence": 0.9482951362927755}]}, {"text": "(2018) to NMT in order to create the encoder.", "labels": [], "entities": [{"text": "NMT", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.9797556400299072}]}, {"text": "We evaluate our model against sequential and supervised parsing baselines on three low-and medium-resource language pairs.", "labels": [], "entities": []}, {"text": "For low-resource cases, the unsuper-vised tree2seq encoder significantly out-performs the baselines; no improvements are seen for medium-resource translation.", "labels": [], "entities": [{"text": "medium-resource translation", "start_pos": 130, "end_pos": 157, "type": "TASK", "confidence": 0.7437537908554077}]}], "introductionContent": [{"text": "Neural machine translation (NMT) is a widely used approach to machine translation that is often trained without outside linguistic information.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8020161390304565}, {"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7615821957588196}]}, {"text": "In NMT, sentences are typically modeled using recurrent neural networks (RNNs), so they are represented in a continuous space, alleviating the sparsity issue that afflicted many previous machine translation approaches.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.7488628923892975}]}, {"text": "As a result, NMT is state-of-the-art for many language pairs.", "labels": [], "entities": []}, {"text": "Despite these successes, there is room for improvement.", "labels": [], "entities": []}, {"text": "RNN-based NMT is sequential, whereas natural language is hierarchical; thus, RNNs may not be the most appropriate models for language.", "labels": [], "entities": []}, {"text": "In fact, these sequential models do not fully learn syntax (.", "labels": [], "entities": []}, {"text": "In addition, although NMT performs well on high-resource languages, it is less successful in low-resource scenarios (.", "labels": [], "entities": []}, {"text": "As a solution to these challenges, researchers have incorporated syntax into NMT, particularly on the source side.", "labels": [], "entities": []}, {"text": "Notably, introduced a tree-to-sequence (tree2seq) NMT model in which the RNN encoder was augmented with a tree long short-term memory (LSTM) network (.", "labels": [], "entities": []}, {"text": "This and related techniques have yielded improvements in NMT; however, injecting source syntax into NMT requires parsing the training data with an external parser, and such parsers maybe unavailable for low-resource languages.", "labels": [], "entities": [{"text": "NMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.84086012840271}]}, {"text": "Adding syntactic source information may improve low-resource NMT, but we would need away of doing so without an external parser.", "labels": [], "entities": []}, {"text": "We would like to mimic the improvements that come from adding source syntactic hierarchies to NMT without requiring syntactic annotations of the training data.", "labels": [], "entities": []}, {"text": "Recently, there have been some proposals to induce unsupervised hierarchies based on semantic objectives for sentiment analysis and natural language inference (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.9651809632778168}]}, {"text": "Here, we apply these hierarchical sentence representations to lowresource neural machine translation.", "labels": [], "entities": [{"text": "lowresource neural machine translation", "start_pos": 62, "end_pos": 100, "type": "TASK", "confidence": 0.6015210375189781}]}, {"text": "In this work, we adapt the Gumbel tree-LSTM of to low-resource NMT, allowing unsupervised hierarchies to be injected into the encoder.", "labels": [], "entities": []}, {"text": "We compare this model to sequential neural machine translation, as well as to NMT enriched with information from an external parser.", "labels": [], "entities": [{"text": "sequential neural machine translation", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.772892951965332}]}, {"text": "Our proposed model yields significant improvements in very low-resource NMT without requiring outside data or parsers beyond what is used in standard NMT; in addition, this model is not significantly slower to train than RNN-based models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Amount of parallel sentences for each  language pair after preprocessing.", "labels": [], "entities": [{"text": "Amount", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9402574896812439}]}, {"text": " Table 2: BLEU for the baseline and the unsuper- vised tree2seq systems on *\u2192EN translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990020394325256}]}, {"text": " Table 3: BLEU for the baselines and the unsuper- vised tree2seq systems on EN\u2192* translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990395903587341}]}, {"text": " Table 5: Recombined subwords in the test data.", "labels": [], "entities": []}]}