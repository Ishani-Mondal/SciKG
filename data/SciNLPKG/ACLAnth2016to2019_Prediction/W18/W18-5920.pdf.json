{"title": [{"text": "CLaC at SMM4H Task 1, 2, and 4", "labels": [], "entities": [{"text": "SMM4H Task 1", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.643443743387858}]}], "abstractContent": [{"text": "CLaC Labs participated in Tasks 1, 2, and 4 using the same base architecture for all tasks with various parameter variations.", "labels": [], "entities": [{"text": "CLaC Labs", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9469222128391266}]}, {"text": "This was our first exploration of this data and the SMM4H Tasks, thus a unified system was useful to compare the behavior of our architecture over the different datasets and how they interact with different linguistic features.", "labels": [], "entities": [{"text": "SMM4H Tasks", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.8483375310897827}]}, {"text": "1 Base system The base system is a feed-forward neural network with a recurrent neuron.", "labels": [], "entities": []}, {"text": "We decided to explore that architecture for independent purposes and used the SMM4H tasks to compare performance on different datasets and task descriptions.", "labels": [], "entities": [{"text": "SMM4H", "start_pos": 78, "end_pos": 83, "type": "TASK", "confidence": 0.9071699976921082}]}, {"text": "We considered three variations of this architecture: Full: A recurrent neuron that outputs a 20 dimensional vector is followed by a 3 layer feed-forward neural net, all embedded in two decision neurons with soft-max activations.", "labels": [], "entities": []}, {"text": "The feed-forward network has 50, 25 and 12 neurons in first, second and third layers respectively.", "labels": [], "entities": []}, {"text": "Unless otherwise mentioned, the network has been trained for 100 epochs.", "labels": [], "entities": []}, {"text": "The recurrent neuron consists of an LSTM cell using tanh activations [Hochreiter and Schmidhu-ber, 1997].", "labels": [], "entities": []}, {"text": "The activation functions for the feed-forward networks are also tanh.", "labels": [], "entities": []}, {"text": "NR: Only the recurrent neuron and the decision neurons are used, the feed-forward (N)etwork is (R)emoved.", "labels": [], "entities": []}, {"text": "Full+At: Attention is added to the full architecture.", "labels": [], "entities": []}, {"text": "In contrast to Full, where the LSTM cell outputs a single vector, in Full+At, the recurrent neu-ron outputs the sequence of each time step.", "labels": [], "entities": []}, {"text": "We used the Keras package [Chollet and others , 2015] to implement the neural networks using TensorFlow as backend [Abadi et al., 2015].", "labels": [], "entities": []}, {"text": "1.1 Input parameters Tweets are normalized to a size of 25, padded with leading zeros or shortened from the end as required.", "labels": [], "entities": []}, {"text": "The input per tweet consists thus of 25 word vectors of size 100 compiled by the Word2Vec method [Mikolov et al., 2013] over the training data.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.9305470585823059}]}, {"text": "The Gensim package [ \u02c7 Reh\u016f\u0159ek and Sojka, 2010] is used for the training of word vectors.", "labels": [], "entities": []}, {"text": "The minimum number of occurrences fora word to be considered in the vocabulary is 1 and the window size has been set to 5.", "labels": [], "entities": []}, {"text": "Other parameters involved in word vector training were left to the default values of the Gensim package.", "labels": [], "entities": [{"text": "word vector training", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.8011489113171896}, {"text": "Gensim package", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.8565499186515808}]}, {"text": "Tweet representations are then binned to a batch size of 5, unless otherwise indicated.", "labels": [], "entities": []}, {"text": "2 Text features and knowledge sources We also experimented with a few linguistic text features and a gazetteer list to see whether they might influence the results.", "labels": [], "entities": []}, {"text": "2.1 Gazetteer Inspired by Task 1, detection of drug mentions, we scraped the name field of product fields in Drug-Bank [Wishart et al., 2017] to compile a gazetteer list for drugs.", "labels": [], "entities": [{"text": "detection of drug mentions", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.8638792484998703}, {"text": "Drug-Bank [Wishart et al., 2017]", "start_pos": 109, "end_pos": 141, "type": "DATASET", "confidence": 0.8592439964413643}]}, {"text": "Due to time constraints, this resource was only minimally refined and contained many multi-word drug names such as One A Day and dosage specifications (Aspirin 80mg).", "labels": [], "entities": []}, {"text": "The gazetteer information was appended to the word vector.", "labels": [], "entities": []}, {"text": "Runs that use the gazetteer are identified as +Gaz.", "labels": [], "entities": [{"text": "Gaz", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.8692123293876648}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Training and validation results for Task 1", "labels": [], "entities": []}, {"text": " Table 2: Official Task 1 results for CLaC Difference  between best and last system score for this task was  0.1399", "labels": [], "entities": [{"text": "CLaC", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.7347823977470398}, {"text": "Difference", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9149538278579712}]}, {"text": " Table 3: Official Task 2 micro-averaged results", "labels": [], "entities": []}, {"text": " Table 4: Official Task 4 results for CLaC.", "labels": [], "entities": [{"text": "CLaC", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.8818411827087402}]}]}