{"title": [{"text": "Task Proposal -The TL;DR challenge", "labels": [], "entities": []}], "abstractContent": [{"text": "The TL;DR challenge fosters research in abstractive summarization of informal text, the largest and fastest-growing source of textual data on the web, which has been overlooked by summarization research so far.", "labels": [], "entities": [{"text": "TL;DR", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.5704132119814554}, {"text": "abstractive summarization of informal text", "start_pos": 40, "end_pos": 82, "type": "TASK", "confidence": 0.7320928871631622}, {"text": "summarization", "start_pos": 180, "end_pos": 193, "type": "TASK", "confidence": 0.9658721089363098}]}, {"text": "The challenge owes its name to the frequent practice of social media users to supplement long posts with a \"TL;DR\"-for \"too long; didn't read\"-followed by a short summary as a courtesy to those who would otherwise reply with the exact same abbreviation to indicate they did not care to read a post for its apparent length.", "labels": [], "entities": [{"text": "TL;DR\"-for \"too long; didn't read\"-followed by a short summary as a courtesy to those who would otherwise reply with the exact same abbreviation to indicate they did not care to read a post", "start_pos": 108, "end_pos": 297, "type": "Description", "confidence": 0.7601822898501441}]}, {"text": "Posts featuring TL;DR summaries form an excellent ground truth for summarization, and by tapping into this resource for the first time, we have mined millions of training examples from social media, opening the door to all kinds of generative models.", "labels": [], "entities": [{"text": "TL;DR summaries", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.4713948592543602}, {"text": "summarization", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.9906179308891296}]}], "introductionContent": [], "datasetContent": [{"text": "To determine the winners of the TL;DR challenge, we will deploy a two step process involving both automatic measures and a thorough human evaluation of the generated summaries.", "labels": [], "entities": [{"text": "TL;DR challenge", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.7728102207183838}]}, {"text": "Content selection evaluation metrics such as ROUGE, BLEU, and METEOR, will be reported to provide participants with a first impression of the coherence and information capturing capabilities of their models.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9964955449104309}, {"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9980385899543762}, {"text": "METEOR", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9873412251472473}]}, {"text": "Additionally, embedding based metrics such as cosine similarity of word and sentence representations of the generated summaries will be reported against the reference summaries.", "labels": [], "entities": []}, {"text": "For qualitative evaluation, human annotators recruited via Amazon Mechanical Turk will read the candidate summaries and rate them based on the standard summary evaluation criteria established by the DUC competitions (.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 59, "end_pos": 81, "type": "DATASET", "confidence": 0.9462528427441915}, {"text": "DUC competitions", "start_pos": 199, "end_pos": 215, "type": "DATASET", "confidence": 0.9267500340938568}]}, {"text": "Each generated summary will be judged by at least three annotators to ensure accuracy; annotators will rate individual summaries, as well as pairs of summaries from different participants to establish preference and break ties.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9990062117576599}]}, {"text": "The final ranking, and the winner of the TL;DR challenge, will be derived from the human annotators' quality judgments.", "labels": [], "entities": [{"text": "TL;DR challenge", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.5471470132470131}]}, {"text": "The crowdsourcing evaluation phase will employ 200 test samples not used during the automatic evaluation phase.", "labels": [], "entities": []}, {"text": "Based on the number of submitted summarization systems, participation in the crowd evaluation phase maybe limited to the top performers on the automatic evaluation leaderboard-based on our projections, up to approximately thirty submissions will be considered for crowd evaluation.", "labels": [], "entities": [{"text": "crowd evaluation", "start_pos": 264, "end_pos": 280, "type": "TASK", "confidence": 0.8458127379417419}]}, {"text": "Some time after the conclusion of the competition, all testing data and annotator decisions will be made available to the research community at large; we expect that the analysis of the resulting data, and how it correlates with automatically computed ROUGE scores, will benefit the development of better evaluation metrics.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 252, "end_pos": 257, "type": "METRIC", "confidence": 0.894558310508728}]}, {"text": "Outside of the ranking, we intend to offer evaluation scenarios in constrained summarization, such as generating summaries that include the topic of the underlying discussion, summaries in the form of questions, or wacky summaries deliberately including off-color vocabulary.", "labels": [], "entities": []}, {"text": "We envision such scenarios to gain interest as summarization technology becomes integrated into conversational agents.", "labels": [], "entities": [{"text": "summarization", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9745118618011475}]}], "tableCaptions": []}