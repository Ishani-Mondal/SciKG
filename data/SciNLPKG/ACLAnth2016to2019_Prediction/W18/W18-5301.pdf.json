{"title": [{"text": "Results of the sixth edition of the BioASQ Challenge", "labels": [], "entities": [{"text": "BioASQ Challenge", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.6947130560874939}]}], "abstractContent": [{"text": "This paper presents the results of the sixth edition of the BioASQ challenge.", "labels": [], "entities": [{"text": "BioASQ challenge", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.596715897321701}]}, {"text": "The BioASQ challenge aims at the promotion of systems and methodologies through the organization of a challenge on two tasks: semantic indexing and question answering.", "labels": [], "entities": [{"text": "semantic indexing", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.7015616446733475}, {"text": "question answering", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.8379803597927094}]}, {"text": "In total, 26 teams with more than 90 systems participated in this year's challenge.", "labels": [], "entities": []}, {"text": "As in previous years, the best systems were able to outperform the strong baselines.", "labels": [], "entities": []}, {"text": "This suggests that state-of-the-art systems are continuously improving, pushing the frontier of research.", "labels": [], "entities": []}], "introductionContent": [{"text": "The aim of this paper is twofold.", "labels": [], "entities": []}, {"text": "First, we aim to give an overview of the data issued during the BioASQ challenge in 2018.", "labels": [], "entities": [{"text": "BioASQ challenge in 2018", "start_pos": 64, "end_pos": 88, "type": "DATASET", "confidence": 0.8063057214021683}]}, {"text": "In addition, we aim to present the systems that participated in the challenge and evaluate their performance.", "labels": [], "entities": []}, {"text": "To achieve these goals, we begin by giving a brief overview of the tasks, which took place from February to May 2018, and the challenge's data.", "labels": [], "entities": []}, {"text": "Thereafter, we provide an overview of the systems that participated in the challenge.", "labels": [], "entities": []}, {"text": "Detailed descriptions of some of the systems are given in workshop proceedings.", "labels": [], "entities": []}, {"text": "The evaluation of the systems, which was carried out using state-of-the-art measures or manual assessment, is the last focal point of this paper, with remarks regarding the results of each task.", "labels": [], "entities": []}, {"text": "The conclusions sum up this year's challenge.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics on test datasets for Task 6a.", "labels": [], "entities": []}, {"text": " Table 2: Statistics on the training and test datasets of  Task 6b. All the numbers for the documents and snip- pets refer to averages.", "labels": [], "entities": []}, {"text": " Table 5: Average system ranks across the batches of the Task 6a. A hyphenation symbol (-) is used whenever the  system participated in fewer than 4 tests in the batch. Systems with fewer than 4 participations in all batches are  omitted.", "labels": [], "entities": []}, {"text": " Table 6: Results for snippet retrieval in batch 3 of phase A of Task 6b. Only the top-10 systems are presented.", "labels": [], "entities": [{"text": "snippet retrieval", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.8702462017536163}]}, {"text": " Table 7: Results for document retrieval in batch 3 of phase A of Task 6b. Only the top-10 systems are presented.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7969659268856049}]}, {"text": " Table 8: Results for batch 4 for exact answers in phase B of Task 6b.", "labels": [], "entities": []}]}