{"title": [{"text": "Ontology-Based Retrieval & Neural Approaches for BioASQ Ideal Answer Generation", "labels": [], "entities": [{"text": "Ontology-Based Retrieval", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7548786401748657}, {"text": "BioASQ Ideal Answer Generation", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.7337543815374374}]}], "abstractContent": [{"text": "The ever-increasing magnitude of biomedi-cal information sources makes it difficult and time-consuming fora human researcher to find the most relevant documents and pinpointed answers fora specific question or topic when using only a traditional search engine.", "labels": [], "entities": []}, {"text": "Biomedical Question Answering systems automatically identify the most relevant documents and pinpointed answers, given an information need expressed as a natural language question.", "labels": [], "entities": [{"text": "Biomedical Question Answering", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.748262902100881}]}, {"text": "Generating a non-redundant, human-readable summary that satisfies the information need of a given biomedical question is the focus of the Ideal Answer Generation task, part of the BioASQ challenge.", "labels": [], "entities": [{"text": "Ideal Answer Generation task", "start_pos": 138, "end_pos": 166, "type": "TASK", "confidence": 0.8498682975769043}]}, {"text": "This paper presents a system for ideal answer generation (using ontology-based retrieval and a neu-ral learning-to-rank approach, combined with extractive and abstractive summarization techniques) which achieved the highest ROUGE score of 0.659 on the BioASQ 5b batch 2 test.", "labels": [], "entities": [{"text": "answer generation", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.9038804471492767}, {"text": "ROUGE score", "start_pos": 224, "end_pos": 235, "type": "METRIC", "confidence": 0.9857217371463776}, {"text": "BioASQ 5b batch 2 test", "start_pos": 252, "end_pos": 274, "type": "DATASET", "confidence": 0.923234474658966}]}], "introductionContent": [{"text": "In this paper, we describe our attempts to address the Ideal Answer Generation task of the sixth edition of the BioASQ challenge, 1 which is a large-scale semantic indexing and question answering challenge in the biomedical domain.", "labels": [], "entities": [{"text": "Ideal Answer Generation task", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.8890818804502487}, {"text": "question answering challenge", "start_pos": 177, "end_pos": 205, "type": "TASK", "confidence": 0.7807470262050629}]}, {"text": "In particular, the sub-task of Phase B of this annual challenge is to develop a system for queryoriented summarization.", "labels": [], "entities": [{"text": "queryoriented summarization", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.6601530015468597}]}, {"text": "Traditionally, there are two classes of summarization techniques, each having their own merits and pitfalls: (1) extractive and (2) abstractive.", "labels": [], "entities": [{"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9782481789588928}]}, {"text": "While extractive techniques patch relevant sentences together enabling them to generate grammatically robust summaries, they flounder on maintaining coherence and readability.", "labels": [], "entities": []}, {"text": "On the contrary, abstractive techniques extract relevant information from the original text, * denotes equal contribution 1 bioasq.org which is then used to generate a novel natural language summary.", "labels": [], "entities": []}, {"text": "While abstractive techniques are more succinct and coherent, automatic text generation is prone to grammatical error.", "labels": [], "entities": [{"text": "text generation", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.7598567605018616}]}, {"text": "This directly implies that extractive summarization techniques should perform well on automatic evaluation metrics (such as ROUGE), but do less well on human evaluation measures which account for precision, repetition and readability.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.6199701726436615}, {"text": "ROUGE", "start_pos": 124, "end_pos": 129, "type": "METRIC", "confidence": 0.9751434326171875}, {"text": "precision", "start_pos": 196, "end_pos": 205, "type": "METRIC", "confidence": 0.998857855796814}, {"text": "repetition", "start_pos": 207, "end_pos": 217, "type": "METRIC", "confidence": 0.9853031635284424}]}, {"text": "We explore the hypothesis that a combination of these techniques will provide better overall performance on the ideal answer task, when compared with either approach used in isolation.", "labels": [], "entities": []}, {"text": "The dataset we use for development of the current work is released as apart of the sixth edition of the annual BioASQ challenge ().", "labels": [], "entities": [{"text": "BioASQ challenge", "start_pos": 111, "end_pos": 127, "type": "TASK", "confidence": 0.49853986501693726}]}, {"text": "The main categories of answers in this data include summary, factoid, list and yes/no.", "labels": [], "entities": []}, {"text": "There area total of 2,251 questions, each of which is accompanied by a list of relevant documents and a list of relevant snippets extracted from each of these documents.", "labels": [], "entities": []}, {"text": "Our model is an extension to the highest ROUGE scoring model in the final test batch of the fifth edition of the BioASQ challenge (, which is based on Maximal Marginal Relevance (MMR)).", "labels": [], "entities": [{"text": "ROUGE scoring", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.9832696616649628}]}, {"text": "In addition, we attempted abstractive techniques that are scoped to improve the readability and coherence aspect of the problem.", "labels": [], "entities": []}, {"text": "We made 4 submissions to the challenge.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 describes our overall system architecture and the implementation details.", "labels": [], "entities": []}, {"text": "Experiments and results are discussed in Section 3 followed by conclusion and future work in 4.", "labels": [], "entities": [{"text": "conclusion", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9619575142860413}]}], "datasetContent": [{"text": "This section describes the experiments conducted to evaluate each of the components in the question answering pipeline.", "labels": [], "entities": [{"text": "question answering pipeline", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.8645660281181335}]}, {"text": "All the experiments have been conducted on batch 2 of the fifth edition of BioASQ and evaluated using the official Oracle 2 developed by the organizers of the task.", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.7888018488883972}]}], "tableCaptions": [{"text": " Table 1: Results on 5b batch 2", "labels": [], "entities": [{"text": "5b batch", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9213089644908905}]}, {"text": " Table 3: BioASQ Results for Task 6B", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9548644423484802}]}]}