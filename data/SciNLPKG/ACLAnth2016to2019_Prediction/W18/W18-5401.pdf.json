{"title": [{"text": "When does deep multi-task learning work for loosely related document classification tasks?", "labels": [], "entities": [{"text": "document classification tasks", "start_pos": 60, "end_pos": 89, "type": "TASK", "confidence": 0.8031245668729147}]}], "abstractContent": [{"text": "This work aims to contribute to our understanding of when multi-task learning through parameter sharing in deep neural networks leads to improvements over single-task learning.", "labels": [], "entities": []}, {"text": "We focus on the setting of learning from loosely related tasks, for which no theoretical guarantees exist.", "labels": [], "entities": []}, {"text": "We therefore approach the question empirically, studying which properties of datasets and single-task learning characteristics correlate with improvements from multi-task learning.", "labels": [], "entities": []}, {"text": "We are the first to study this in a text classification setting and across more than 500 different task pairs.", "labels": [], "entities": [{"text": "text classification", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7296318709850311}]}], "introductionContent": [{"text": "Multi-task learning is a set of techniques for exploiting synergies between related tasks, and in natural language processing (NLP), where there is an overwhelming number of related problems, and different ways to represent these problems, multitask learning seems well-motivated.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 98, "end_pos": 131, "type": "TASK", "confidence": 0.7137757837772369}]}, {"text": "Since multitask learning, by exploiting related tasks, also reduces the need for labeled data, multi-task learning is also often seen as away to obtain more robust NLP for more domains and languages.", "labels": [], "entities": []}, {"text": "Multi-task learning has seen a revival in recent years, amplified by the success of deep learning techniques.", "labels": [], "entities": [{"text": "Multi-task learning", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8464747965335846}]}, {"text": "Multi-task learning algorithms have been proven to lead to better performance for similar tasks, e.g.,, such as models of individual patients in healthcare, but recently multi-task learning has been applied to more loosely related sets of tasks in artificial intelligence.", "labels": [], "entities": []}, {"text": "Examples include machine translation and syntactic parsing or fixation prediction and sentence compression.", "labels": [], "entities": [{"text": "machine translation and syntactic parsing or fixation prediction", "start_pos": 17, "end_pos": 81, "type": "TASK", "confidence": 0.7017054185271263}, {"text": "sentence compression", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.7506904006004333}]}, {"text": "Reported results * This work was done, when the third author was affiliated with Dpt. of Computer Science, University of Copenhagen.", "labels": [], "entities": []}, {"text": "have been promising, but in the case of loosely related tasks, often also with different label spaces, we have no guarantees that multi-task learning will work.", "labels": [], "entities": []}, {"text": "Recent studies have tried to study empirically when multi-task learning leads to improvements.", "labels": [], "entities": []}, {"text": "These preliminary studies have arguedBingel and S\u00f8gaard (2017) most clearly -that multi-task learning is particularly effective when the target task otherwise plateaus faster than the auxiliary task.", "labels": [], "entities": []}, {"text": "This study compliments these studies, considering new tasks and architectures, and our findings are largely supportive of this conclusion.", "labels": [], "entities": []}, {"text": "In text classification, however, performance also depends crucially on the divergence between the marginal distributions of words in the target and auxiliary task.", "labels": [], "entities": [{"text": "text classification", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8015281558036804}]}, {"text": "Document classification comes in many different flavors, including spam detection, sentiment analysis, customer support ticket routing, and diagnosis support based on patient records, but in this paper we focus on topic-level multi-way classification.", "labels": [], "entities": [{"text": "Document classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9182111620903015}, {"text": "spam detection", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.9181796610355377}, {"text": "sentiment analysis", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.9558888375759125}, {"text": "customer support ticket routing", "start_pos": 103, "end_pos": 134, "type": "TASK", "confidence": 0.6221818029880524}, {"text": "topic-level multi-way classification", "start_pos": 214, "end_pos": 250, "type": "TASK", "confidence": 0.623028298219045}]}, {"text": "We use the 20 Newsgroups dataset, a corpus of newsgroup posts that are labeled by the topics of the newsgroups.", "labels": [], "entities": [{"text": "20 Newsgroups dataset", "start_pos": 11, "end_pos": 32, "type": "DATASET", "confidence": 0.7426532904307047}]}, {"text": "One key challenge in document classification is the high number of feature dimensions introduced by n-gram features, often outnumbering the number of document instances in the training corpus.", "labels": [], "entities": [{"text": "document classification", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.7818827331066132}]}, {"text": "Specifically, it is easy to overfit to the training corpus in high dimensions.", "labels": [], "entities": []}, {"text": "Multi-task learning has strong regularization effects and can therefore potentially make our models less prone to overfitting.", "labels": [], "entities": []}, {"text": "Previous empirical meta-studies of multi-task learning have focused on sequence tagging problems and recurrent neural networks, but there is no guarantee that results extend to document classification.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.682471752166748}, {"text": "document classification", "start_pos": 177, "end_pos": 200, "type": "TASK", "confidence": 0.696600005030632}]}, {"text": "This work, which extends previous work on recur-rent neural networks, is thus motivated by a) an interest in whether previous findings generalize to document classification algorithms -in our case, multi-layered perceptrons, b) a practical consideration that any recommendations coming out of a study of document classification would be helpful to a wider audience.", "labels": [], "entities": [{"text": "document classification", "start_pos": 149, "end_pos": 172, "type": "TASK", "confidence": 0.7096730321645737}, {"text": "document classification", "start_pos": 304, "end_pos": 327, "type": "TASK", "confidence": 0.7382871508598328}]}, {"text": "As already said, our focus on topic-level classification is motivated by the observation that this is an extremely common problem, and key to structuring content on websites, customer support ticket routing, intelligent email, etc.", "labels": [], "entities": [{"text": "topic-level classification", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6831973493099213}, {"text": "customer support ticket routing", "start_pos": 175, "end_pos": 206, "type": "TASK", "confidence": 0.5944247245788574}]}, {"text": "Also, the 20 Newsgroups corpus uses a set of 20 labels that are hierarchically organized (see, which we can exploit to extract a large set of task pairs.", "labels": [], "entities": [{"text": "20 Newsgroups corpus", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.7235611279805502}]}, {"text": "The problem that we consider is the following: If we have two topic-level classification datasets that are loosely related -i.e, contrasts the same upper level classes in the hierarchy in and we have run single-task experiments for each of these, when does multi-task learning help, keeping hyper-parameters fixed?", "labels": [], "entities": []}, {"text": "We approach this as a prediction problem, trying to predict gains or losses based on meta features such as dataset characteristics and features of the single-task learning curves.", "labels": [], "entities": []}, {"text": "This approach was first introduced in).", "labels": [], "entities": []}], "datasetContent": [{"text": "We run single-task and multi-task learning experiments for all pairs of main and auxiliary tasks, as described in Section 3.2.", "labels": [], "entities": []}, {"text": "We then extract data characteristics and features from the logs of the single-task learning experiments.", "labels": [], "entities": []}, {"text": "We train a metalearning model to predict gains from doing multitask learning over single-task performance using the above features.", "labels": [], "entities": []}, {"text": "Then we build a final model to predict gains from multi-task learning using these pairs as instances.", "labels": [], "entities": []}, {"text": "We use the 20 Newsgroups for both RELATED TOPICS and UNRELATED TOP-ICS, as explained above.", "labels": [], "entities": [{"text": "RELATED TOPICS", "start_pos": 34, "end_pos": 48, "type": "METRIC", "confidence": 0.7502156496047974}, {"text": "UNRELATED TOP-ICS", "start_pos": 53, "end_pos": 70, "type": "METRIC", "confidence": 0.6412681043148041}]}, {"text": "We use 200 topics for each class for training, and the rest of each dataset for testing (5-700 data points, depending on the topics).", "labels": [], "entities": []}, {"text": "We train single-task models for all tasks, as well as multi-task learning models for all combinations of target and auxiliary tasks.", "labels": [], "entities": []}, {"text": "We report the F 1 gains obtained for multi-task learning over single-task learning below.", "labels": [], "entities": [{"text": "F 1", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.9888274669647217}]}, {"text": "Our real aim, however, is to try to predict the gains one can get from doing multi-task learning.", "labels": [], "entities": []}, {"text": "This is a meta-learning problem, and here, the above experiments are our instances, i.e., one instance for each of the main-auxiliary task pairs, meaning that we have 52 instances for RELATED TOPICS and 516 for UNRELATED TOPICS.", "labels": [], "entities": [{"text": "RELATED", "start_pos": 184, "end_pos": 191, "type": "METRIC", "confidence": 0.8373677134513855}]}, {"text": "In order to compensate for the small number of training instances, we repeat our RELATED TOPICS experiments five times with random initializations, and report means over the results.", "labels": [], "entities": [{"text": "RELATED TOPICS", "start_pos": 81, "end_pos": 95, "type": "METRIC", "confidence": 0.7140993922948837}]}, {"text": "We use the same procedure for UNRELATED TOPICS, also.", "labels": [], "entities": [{"text": "UNRELATED TOPICS", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.5014746785163879}]}, {"text": "F 1 scores, obtained by a logistic regression model over 100 runs using a 5-fold cross-validation procedure, are reported at the end of the next section.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9468568364779154}]}], "tableCaptions": [{"text": " Table 2. The results  show that generally, features extracted from the  loss curves are more predictive of gains than any  other features. This confirms findings in", "labels": [], "entities": []}, {"text": " Table 2: Mean performance across 100 runs of 5- fold CV logistic regression.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9894157648086548}]}, {"text": " Table 3: Average inverse ranks and average logistic regression coefficients of various predictors of gains  from multi-task learning", "labels": [], "entities": []}]}