{"title": [{"text": "Generating Descriptions for Sequential Images with Local-Object Attention and Global Semantic Context Modelling", "labels": [], "entities": [{"text": "Global Semantic Context Modelling", "start_pos": 78, "end_pos": 111, "type": "TASK", "confidence": 0.5745318755507469}]}], "abstractContent": [{"text": "In this paper, we propose an end-to-end CNN-LSTM model for generating descriptions for sequential images with a local-object attention mechanism.", "labels": [], "entities": []}, {"text": "To generate coherent descriptions, we capture global semantic context using a multi-layer perceptron, which learns the dependencies between sequential images.", "labels": [], "entities": []}, {"text": "A par-alleled LSTM network is exploited for decoding the sequence descriptions.", "labels": [], "entities": []}, {"text": "Experimental results show that our model out-performs the baseline across three different evaluation metrics on the datasets published by Microsoft.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, automatically generating image descriptions has attracted considerable interest in the fields of computer vision and nature language processing.", "labels": [], "entities": [{"text": "automatically generating image descriptions", "start_pos": 10, "end_pos": 53, "type": "TASK", "confidence": 0.7026659846305847}, {"text": "nature language processing", "start_pos": 127, "end_pos": 153, "type": "TASK", "confidence": 0.683676560719808}]}, {"text": "Such a task is easy to humans but highly non-trivial for machines as it requires not only capturing the semantic information from images (e.g., objects and actions) but also needs to generate human-like natural language descriptions.", "labels": [], "entities": []}, {"text": "Existing approaches to generating image description are dominated by neural network-based methods, which mostly focus on generating description fora single image (.", "labels": [], "entities": [{"text": "generating image description", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.7769637505213419}]}, {"text": "Generating descriptions for sequential images, in contrast, is much more challenging, i.e., the information of both individual images as well as the dependencies between images in a sequence needs to be captured.", "labels": [], "entities": []}, {"text": "introduce the first sequential vision-to-language dataset and exploit Gated Recurrent Units (GRUs) () based encoder and decoder for the task of visual storytelling.", "labels": [], "entities": [{"text": "visual storytelling", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.7506146132946014}]}, {"text": "However, their approach only considers image information of a sequence at the first time step of the decoder, where the local attention mechanism is ignored which is important for capturing the correlation between the features of an individual image and the corresponding words in a description sentence.", "labels": [], "entities": []}, {"text": "propose a hierarchically-attentive Recurrent Neural Nets (RNNs) for album summarisation and storytelling.", "labels": [], "entities": [{"text": "album summarisation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.5939192920923233}]}, {"text": "To generate descriptions for an image album, their hierarchical framework selects representative images from several image sequences of the album, where the selected images might not necessary have correlation to each other.", "labels": [], "entities": []}, {"text": "In this paper, we propose an end-to-end CNN-LSTM model with a local-object attention mechanism for generating story-like descriptions for multiple images of a sequence.", "labels": [], "entities": []}, {"text": "To improve the coherence of the generated descriptions, we exploit a paralleled long short-terms memory (LSTM) network and learns global semantic context by embedding the global features of sequential images as an initial input to the hidden layer of the LSTM model.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our model on the task of generating story-like descriptions for an image sequence on the sequencein-sequence (SIS) dataset published by Microsoft.", "labels": [], "entities": [{"text": "SIS) dataset published by Microsoft", "start_pos": 141, "end_pos": 176, "type": "DATASET", "confidence": 0.786466067035993}]}, {"text": "We hypothesise that by taking into account global context, our model can also generate better descriptions for individual images.", "labels": [], "entities": []}, {"text": "Therefore, in another set of experiments, we further test our model on the Descriptions of Images-in-Isolation (DII) dataset for generating descriptions for each individual image of a sequence.", "labels": [], "entities": []}, {"text": "Experimental results show that our model outperforms a baseline developed based on the state-of-the-art image captioning model () in terms of BLEU, METEOR and ROUGE, and can generate sequential descriptions which preserve the dependencies between sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9986617565155029}, {"text": "METEOR", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9531006813049316}, {"text": "ROUGE", "start_pos": 159, "end_pos": 164, "type": "METRIC", "confidence": 0.98647540807724}]}, {"text": "There is also a surge of research interest in visual storytelling).", "labels": [], "entities": []}, {"text": "collect stories using Mechanical Turk and translate a sequence of images into story-like descriptions by extending a GRU-GRU framework.", "labels": [], "entities": []}, {"text": "utilise a hierarchically-attentive structures with combined RNNs for photo selection and story generation.", "labels": [], "entities": [{"text": "photo selection", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.79972705245018}, {"text": "story generation", "start_pos": 89, "end_pos": 105, "type": "TASK", "confidence": 0.7184920310974121}]}, {"text": "However, the above mentioned approaches for generating descriptions of sequential images do not explicitly capture the dependencies between each individual images of a sequence, which is the gap that we try to address in this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Both the SIS and DII datasets are published by Microsoft 1 , which have a similar data structure, (1) me and my lover went on a vacation to see some sights.", "labels": [], "entities": [{"text": "DII datasets", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.813673198223114}]}, {"text": "here we are getting something to eat.", "labels": [], "entities": []}, {"text": "we liked the food but the place was rather crowded for our tastes.", "labels": [], "entities": []}, {"text": "here is a view of the city from our hotel.", "labels": [], "entities": []}, {"text": "(3) it was so lovely to lookout every night as the sun went down.", "labels": [], "entities": []}, {"text": "another shot from high up.", "labels": [], "entities": []}, {"text": "(4) it was breathtaking to watch the city light up as the sun went down.", "labels": [], "entities": []}, {"text": "we wherein line fora ferris wheel.", "labels": [], "entities": []}, {"text": "i thought that this would make a good pic, and i think it came out well.: Evaluation of the quality of descriptions generated for sequential images.", "labels": [], "entities": []}, {"text": "i.e., each image sequence consists of five images and their corresponding descriptions.", "labels": [], "entities": []}, {"text": "The key difference is that descriptions of SIS consider the dependencies between images, whereas the descriptions of DII are generated for each individual image, i.e., no dependencies are considered.", "labels": [], "entities": []}, {"text": "As the full DII and SIS datasets are quite large, we only used part of both datasets for our initial experiments, where the dataset statistics are shown in.", "labels": [], "entities": [{"text": "SIS datasets", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.754909873008728}]}, {"text": "We compare our model with the sequence-to-sequence baseline (cnn-att-lstm) with attention mechanism ().", "labels": [], "entities": []}, {"text": "The cnnatt-lstm baseline only utilises the local attention mechanism which combines visual concepts of an image with the corresponding words in a sentence.", "labels": [], "entities": []}, {"text": "Our model, apart from adopting a local-object attention, can further model global semantic context for capturing the correlation between sequential images.", "labels": [], "entities": []}, {"text": "shows the experimental results of our model on the task of generating descriptions for sequential images with three popular evaluation metrics, i.e. BLEU, Meteor and ROUGE.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9982441663742065}, {"text": "ROUGE", "start_pos": 166, "end_pos": 171, "type": "METRIC", "confidence": 0.982650101184845}]}, {"text": "It can be observed from that our model outperforms the baseline on both SIS and DII datasets for all evaluation metrics.", "labels": [], "entities": [{"text": "DII datasets", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.7463896721601486}]}, {"text": "It is also observed that the scores of the evaluation metric are generally higher for the DII dataset than the SIS dataset.", "labels": [], "entities": [{"text": "DII dataset", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.9144815802574158}, {"text": "SIS dataset", "start_pos": 111, "end_pos": 122, "type": "DATASET", "confidence": 0.8598322868347168}]}, {"text": "The main reason is that the SIS dataset contains more sentences descriptions in a sequence and more abstract content descriptions such as \"breathtaking\" and \"excited\" which are difficult to understand and prone to overfitting.", "labels": [], "entities": [{"text": "SIS dataset", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.7778969407081604}]}, {"text": "shows an example sequence of five images as well as their corresponding descriptions generated by our model, the baseline (cnn-attlstm), and the ground truth.", "labels": [], "entities": []}, {"text": "For the SIS dataset, it can observed that our model can capture more coherent story-like descriptions.", "labels": [], "entities": [{"text": "SIS dataset", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.9179107844829559}]}, {"text": "For instance, our model can learn the social word \"family\" to connect the whole story and learn the emotional words \"great time\" to summarise the description.", "labels": [], "entities": []}, {"text": "However, the baseline model failed to capture such important information.", "labels": [], "entities": []}, {"text": "Our model can learn dependencies of visual scenes between images even on the DII dataset.", "labels": [], "entities": [{"text": "DII dataset", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.9226253926753998}]}, {"text": "For example, compared to the descriptions generated by cnn-att-lstm, our model can learn the visual word \"beach\" in image 1 by reasoning from the visual word \"water\" in image 4.", "labels": [], "entities": []}, {"text": "Our model can generally achieve good results by capturing the global semantics of an image sequence such as the example in the first row of.", "labels": [], "entities": []}, {"text": "However, our model also has difficulties in generating meaningful descriptions in a number of cases.", "labels": [], "entities": []}, {"text": "For instance, our model generates fairly abstractive descriptions such as \"a great time\" due to severe overfitting, as shown in the second row of.", "labels": [], "entities": []}, {"text": "We suppose the issue of overfitting is likely to be alleviated by adding more training data or using more effective algorithm for image feature extraction.", "labels": [], "entities": [{"text": "image feature extraction", "start_pos": 130, "end_pos": 154, "type": "TASK", "confidence": 0.6884915431340536}]}], "tableCaptions": [{"text": " Table 2: Evaluation of the quality of descriptions  generated for sequential images.", "labels": [], "entities": []}]}