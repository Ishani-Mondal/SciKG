{"title": [{"text": "Affordance Extraction and Inference based on Semantic Role Labeling", "labels": [], "entities": [{"text": "Affordance", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9259059429168701}, {"text": "Semantic Role Labeling", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.7431161006291708}]}], "abstractContent": [{"text": "Common-sense reasoning is becoming increasingly important for the advancement of Natural Language Processing.", "labels": [], "entities": [{"text": "Common-sense reasoning", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8061434626579285}, {"text": "Natural Language Processing", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.630719393491745}]}, {"text": "While word embeddings have been very successful, they cannot explain which aspects of 'coffee' and 'tea' make them similar, or how they could be related to 'shop'.", "labels": [], "entities": []}, {"text": "In this paper, we propose an explicit word representation that builds upon the Distributional Hypothesis to represent meaning from semantic roles, and allow inference of relations from their meshing, as supported by the affordance-based Indexical Hypothesis.", "labels": [], "entities": []}, {"text": "We find that our model improves the state-of-the-art on unsupervised word similarity tasks while allowing for direct inference of new relations from the same vector space.", "labels": [], "entities": []}], "introductionContent": [{"text": "The word representations used more recently in Natural Language Processing (NLP) have been based on the Distributional Hypothesis (DH)) -\"words that occur in the same contexts tend to have similar meanings\".", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 47, "end_pos": 80, "type": "TASK", "confidence": 0.7148009439309438}]}, {"text": "This simple idea has led to the development of powerful word embedding models, starting with Latent Semantic Analysis (LSA)) and later, the popular word2vec () and GloVe () models.", "labels": [], "entities": []}, {"text": "Although, effective at quantifying the similarity between words (and phrases) such as 'tea' and 'coffee', they cannot relate that judgement to the fact that both can be sold, for instance.", "labels": [], "entities": []}, {"text": "Furthermore, current representations can't inform us about possible relations between words occurring in mostly distinct contexts, such as using a 'newspaper' to cover a 'face'.", "labels": [], "entities": []}, {"text": "While there have been substantial improvements to word embedding models over the years, these shortcomings have endured).", "labels": [], "entities": []}], "datasetContent": [{"text": "The A2Avecs model introduced in this paper is used to generate 155,183 word vectors of 18,179 affordance dimensions.", "labels": [], "entities": []}, {"text": "This section compares our model with lexical-based models (word2vec (), GloVe () and fastText () and other syntactic-based models (Deps () and OpenIE ().", "labels": [], "entities": []}, {"text": "We're using Deps and OpenIE embeddings that the respective authors trained on a Wikipedia corpus and distributed online.", "labels": [], "entities": []}, {"text": "Lexical models were trained using the same parameters, wherever applicable: Wikipedia corpus from April 2010 (same as mentioned in section 2.1); minimum word frequency of 100; window size of 2; 300 vector dimensions; 15 negative samples; and ngrams sized from 3 to 6 characters.", "labels": [], "entities": [{"text": "Wikipedia corpus from April 2010", "start_pos": 76, "end_pos": 108, "type": "DATASET", "confidence": 0.9688910484313965}]}, {"text": "We also show that our approach can make use of high-quality pretrained embeddings.", "labels": [], "entities": []}, {"text": "We experiment with a fastText model pretrained on 600B tokens, referred to as 'fastText 600B' in contrast with the fastText model trained on Wikipedia.", "labels": [], "entities": []}], "tableCaptions": []}