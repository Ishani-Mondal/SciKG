{"title": [{"text": "Towards Automatic Fake News Detection: Cross-Level Stance Detection in News Articles", "labels": [], "entities": [{"text": "Fake News Detection", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7272564570109049}, {"text": "Cross-Level Stance Detection", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.8029225071271261}]}], "abstractContent": [{"text": "In this paper, we propose to adapt the four-staged pipeline proposed by Zubiaga et al.", "labels": [], "entities": []}, {"text": "(2018) for the Rumor Verification task to the problem of Fake News Detection.", "labels": [], "entities": [{"text": "Rumor Verification task", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.8522555232048035}, {"text": "Fake News Detection", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7817568182945251}]}, {"text": "We show that the recently released FNC-1 corpus covers two of its steps, namely the Tracking and the Stance Detection task.", "labels": [], "entities": [{"text": "FNC-1 corpus", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.927227795124054}, {"text": "Tracking", "start_pos": 84, "end_pos": 92, "type": "TASK", "confidence": 0.9702985882759094}, {"text": "Stance Detection task", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.9312803149223328}]}, {"text": "We identify asymmetry in length in the input to be a key characteristic of the latter step, when adapted to the framework of Fake News Detection, and propose to handle it as a specific type of Cross-Level Stance Detection.", "labels": [], "entities": [{"text": "Fake News Detection", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.7301832834879557}, {"text": "Cross-Level Stance Detection", "start_pos": 193, "end_pos": 221, "type": "TASK", "confidence": 0.6638605693976084}]}, {"text": "Inspired by theories from the field of Journalism Studies, we implement and test two architectures to successfully model the internal structure of an article and its interactions with a claim.", "labels": [], "entities": []}], "introductionContent": [{"text": "The rise of social media platforms, which allow for real-time posting of news with very little (or none at all) editorial review at the source, is responsible for an unprecedented growth in the amount of the information available to the public.", "labels": [], "entities": []}, {"text": "While this constitutes an invaluable source of free information, it also facilitates the spread of misinformation.", "labels": [], "entities": []}, {"text": "In particular, the literature distinguishes between rumors, i.e., pieces of information which are unverified at the time of posting and therefore can turnout to be true or false, and fake news (or hoaxes), i.e., false stories which are instrumentally made up with the intent to mislead the readers and spread disinformation.", "labels": [], "entities": []}, {"text": "Both Rumor Verification (RV) and Fake News Detection (FND) constitute very difficult tasks even for trained professionals.", "labels": [], "entities": [{"text": "Rumor Verification (RV)", "start_pos": 5, "end_pos": 28, "type": "TASK", "confidence": 0.7583865106105805}, {"text": "Fake News Detection (FND)", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.7067331423362097}]}, {"text": "Therefore, approaching them in an end-to-end fashion has generally been avoided.", "labels": [], "entities": []}, {"text": "Both tasks, however, can be easily split into a number of sub-steps.", "labels": [], "entities": []}, {"text": "For instance,  proposed a model for RV which consists of four stages: a rumor detection stage, where potentially rumorous posts are identified, followed by a tracking stage, where posts concerning the identified rumor are collected; after determining the orientation expressed in each post with respect to the rumor (stance detection), the final truth value of the rumor is obtained by aggregating those single stance judgments (veracity classification).", "labels": [], "entities": [{"text": "RV", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9817797541618347}, {"text": "rumor detection", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.6953358054161072}]}, {"text": "As shown in Figure 1, this pipeline can be naturally adapted to FND.", "labels": [], "entities": [{"text": "FND", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.6646347641944885}]}, {"text": "In recent years, several efforts have been made by the research community toward the automatization of some of these stages, in order to provide effective tools to enhance the performance of human journalists in rumor and fake news debunking (.", "labels": [], "entities": [{"text": "rumor and fake news debunking", "start_pos": 212, "end_pos": 241, "type": "TASK", "confidence": 0.6740652441978454}]}, {"text": "Concerning FND, recently released a dataset for the Stance Detection step in the framework of the Fake News Challenge 1 (FNC-1).", "labels": [], "entities": [{"text": "FND", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.8132367730140686}, {"text": "Stance Detection", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.9535898864269257}, {"text": "Fake News Challenge 1 (FNC-1)", "start_pos": 98, "end_pos": 127, "type": "DATASET", "confidence": 0.7815915601594108}]}, {"text": "The core of the corpus is constituted by a collection of articles discussing 566 claims, 300 of which come from the EMERGENT dataset.", "labels": [], "entities": [{"text": "EMERGENT dataset", "start_pos": 116, "end_pos": 132, "type": "DATASET", "confidence": 0.960921049118042}]}, {"text": "Each article is summarized in a headline and labeled as agreeing (AGR), disagreeing (DSG) or discussing (DSC) the claim.", "labels": [], "entities": []}, {"text": "Additionally, unrelated (UNR) samples were created by pairing headlines with random articles.", "labels": [], "entities": []}, {"text": "The goal of the challenge was to classify the pairs constituted by a headline and an article as AGR, DSG, DSC or UNR.", "labels": [], "entities": [{"text": "AGR", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.5302343368530273}, {"text": "UNR", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.5803439021110535}]}, {"text": "The red rectangle indicates steps covered by the FNC-1 corpus.", "labels": [], "entities": [{"text": "FNC-1 corpus", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.9707119464874268}]}, {"text": "Note that the amount of semantic understanding needed for the second task is much higher than for the first.", "labels": [], "entities": []}, {"text": "In fact, even humans struggle in the related sample classification, as empirically demonstrated by: the interannotator agreement of five human judges drops from Fleiss' \u03ba of .686 to .218, after filtering out the UNR samples.", "labels": [], "entities": [{"text": "interannotator agreement", "start_pos": 104, "end_pos": 128, "type": "METRIC", "confidence": 0.8997391164302826}, {"text": "Fleiss' \u03ba", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.9352290332317352}, {"text": "UNR samples", "start_pos": 212, "end_pos": 223, "type": "DATASET", "confidence": 0.9709633588790894}]}, {"text": "For this reason, we concentrate on the stance detection step, and we make the following contributions: 1.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.9359239041805267}]}, {"text": "We identify asymmetry in length between headlines and articles as a key characteristic of the FNC-1 corpus: on average, an article contains more than 30 times the number of words contained in its associated headline.", "labels": [], "entities": [{"text": "FNC-1 corpus", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.9207692742347717}]}, {"text": "This is peculiar with respect to most of the commonly used datasets for stance detection ( and require the development of architectures specifically tailored to this considerable asymmetry.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.9491575062274933}]}, {"text": "Following on the terminology introduced by for Semantic Similarity, we propose to handle the problem as a CrossLevel Stance Detection task.", "labels": [], "entities": [{"text": "Semantic Similarity", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7571928203105927}, {"text": "CrossLevel Stance Detection task", "start_pos": 106, "end_pos": 138, "type": "TASK", "confidence": 0.7426348477602005}]}, {"text": "To our knowledge, it is the first time that this task is investigated in isolation.", "labels": [], "entities": []}, {"text": "2. Inspired by theoretical principles in the field of Journalism Studies, we propose two simple neural architectures to model the argumentative structure of an article, and its complex interplay with a headline.", "labels": [], "entities": [{"text": "Journalism Studies", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7065493166446686}]}, {"text": "We demonstrate that our systems can beat a strong feature-based baseline, based on one of the FNC-1 winning architectures, and that they can successfully model the internal structure of a news article and its relations with a claim, leveraging only word embeddings as input.", "labels": [], "entities": []}], "datasetContent": [{"text": "As we are not considering the UNR samples, the FNC-1 score would not constitute a good metric, as it distinguishes between related and unrelated samples for scoring . Following Zubiaga et al. and, we use macro-average precision, recall and F 1 measure, which is less affected by the high class unbalance.", "labels": [], "entities": [{"text": "UNR samples", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.9585451483726501}, {"text": "FNC-1 score", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9516887664794922}, {"text": "precision", "start_pos": 218, "end_pos": 227, "type": "METRIC", "confidence": 0.86540687084198}, {"text": "recall", "start_pos": 229, "end_pos": 235, "type": "METRIC", "confidence": 0.9994608759880066}, {"text": "F 1 measure", "start_pos": 240, "end_pos": 251, "type": "METRIC", "confidence": 0.9908185005187988}]}, {"text": "We also consider the accuracy with respect to the single AGR, DSG and DSC classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9997169375419617}, {"text": "DSG", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.8140543699264526}, {"text": "DSC", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.6126145124435425}]}], "tableCaptions": [{"text": " Table 1: Label distribution for the FNC-1 dataset, con- sidering all classes, or only the related samples.", "labels": [], "entities": [{"text": "FNC-1 dataset", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.9429638981819153}]}, {"text": " Table 2: Asymmetry in length between headlines and  articles in the FNC-1 corpus.", "labels": [], "entities": [{"text": "FNC-1 corpus", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.9626875817775726}]}, {"text": " Table 4: Results of experiments using double-conditional encoding or co-matching attention. Best results for each  encoding type are shown in bold. Best results overall are indicated with an asterisk.", "labels": [], "entities": []}]}