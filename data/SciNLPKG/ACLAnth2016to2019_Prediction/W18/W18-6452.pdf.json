{"title": [{"text": "Findings of the WMT 2018 Shared Task on Automatic Post-Editing", "labels": [], "entities": [{"text": "WMT 2018 Shared Task", "start_pos": 16, "end_pos": 36, "type": "DATASET", "confidence": 0.6615454852581024}]}], "abstractContent": [{"text": "We present the results from the fourth round of the WMT shared task on MT Automatic Post-Editing.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.5248196919759115}, {"text": "MT Automatic Post-Editing", "start_pos": 71, "end_pos": 96, "type": "DATASET", "confidence": 0.6966675519943237}]}, {"text": "The task consists in automatically correcting the output of a \"black-box\" machine translation system by learning from human corrections.", "labels": [], "entities": [{"text": "correcting the output of a \"black-box\" machine translation", "start_pos": 35, "end_pos": 93, "type": "TASK", "confidence": 0.5821495592594147}]}, {"text": "Keeping the same general evaluation setting of the three previous rounds, this year we focused on one language pair (English-German) and on domain-specific data (Information Technology), with MT outputs produced by two different paradigms: phrase-based (PBSMT) and neural (NMT).", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 192, "end_pos": 202, "type": "TASK", "confidence": 0.8970032930374146}]}, {"text": "Five teams submitted respectively 11 runs for the PBSMT subtask and 10 runs for the NMT sub-task.", "labels": [], "entities": [{"text": "PBSMT", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.907639741897583}, {"text": "NMT sub-task", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.895613968372345}]}, {"text": "In the former subtask, characterized by original translations of lower quality, top results achieved impressive improvements , up to-6.24 TER and +9.53 BLEU points over the baseline \"do-nothing\" system.", "labels": [], "entities": [{"text": "TER", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.9991962313652039}, {"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9873611927032471}]}, {"text": "The NMT subtask proved to be more challenging due to the higher quality of the original translations and the availability of less training data.", "labels": [], "entities": [{"text": "NMT subtask", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7851567566394806}]}, {"text": "In this case, top results show smaller improvements up to-0.38 TER and +0.8 BLEU points.", "labels": [], "entities": [{"text": "TER", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9996334314346313}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9982272982597351}]}], "introductionContent": [{"text": "The WMT shared task on MT Automatic PostEditing (APE), this year at its fourth round, aims to evaluate systems for the automatic correction of errors in a machine-translated text.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.537551204363505}, {"text": "MT Automatic PostEditing (APE)", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.8838986357053121}, {"text": "automatic correction of errors in a machine-translated text", "start_pos": 119, "end_pos": 178, "type": "TASK", "confidence": 0.7864990271627903}]}, {"text": "The 2018 round of the task proposed participants with the same evaluation setting of the three previous editions, in which the output of an unknown \"black box\" MT engine has to be automatically corrected by learning from human revisions of translations produced by the same engine.", "labels": [], "entities": []}, {"text": "This year, the task focused on one language pair 1 (English-German) and, in continuity with the 2016 and 2017 rounds, on data coming from the Information Technology domain.", "labels": [], "entities": []}, {"text": "The main novelty was represented by the use of training/test data including, for the same source sentences, translations produced by two different MT technologies: phrase-based (in continuity with and neural (for the first time).", "labels": [], "entities": [{"text": "MT", "start_pos": 147, "end_pos": 149, "type": "TASK", "confidence": 0.9564345479011536}]}, {"text": "On one side, keeping language and domain unchanged was meant to measure the technology progress over the past.", "labels": [], "entities": []}, {"text": "On the other side, extending the evaluation to NMTderived data was meant to explore the effectiveness of APE techniques, which now migrated to the neural paradigm, to correct data obtained with the same paradigm.", "labels": [], "entities": [{"text": "NMTderived data", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.8363995552062988}, {"text": "APE", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.6919670701026917}]}, {"text": "In terms of participants and submitted runs, 5 teams produced respectively 11 runs for the PB-SMT subtask and 10 runs for the NMT subtask.", "labels": [], "entities": [{"text": "PB-SMT subtask", "start_pos": 91, "end_pos": 105, "type": "DATASET", "confidence": 0.9554480612277985}, {"text": "NMT subtask", "start_pos": 126, "end_pos": 137, "type": "DATASET", "confidence": 0.9699771702289581}]}, {"text": "All submissions were produced by neural APE systems.", "labels": [], "entities": []}, {"text": "All the teams experimented with the Transformer architecture (), either directly or by adapting it to the task (see Section 2.1).", "labels": [], "entities": []}, {"text": "The two synthetic corpora provided as additional training material (see Section 2.1) were also extensively used.", "labels": [], "entities": []}, {"text": "In terms of results, on PBSMT data, the last year's trend is confirmed: the migration to the neural approach to APE yielded significant quality gains to the output of phrase-based MT systems.", "labels": [], "entities": [{"text": "PBSMT data", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.878243088722229}, {"text": "MT", "start_pos": 180, "end_pos": 182, "type": "TASK", "confidence": 0.8244361877441406}]}, {"text": "However, while in 2017 the largest improvements with respect to the baseline were respectively -4.9 TER and +7.6 BLEU, this year the distance is even larger: -6.24 TER and +9.53.", "labels": [], "entities": [{"text": "TER", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9670734405517578}, {"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9970378875732422}, {"text": "distance", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9787934422492981}, {"text": "TER", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.9833824038505554}]}, {"text": "On NMT data, the gains are less evident, with the largest improvements over the baseline of -0.38 TER and +0.8 BLEU.", "labels": [], "entities": [{"text": "NMT data", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.92197185754776}, {"text": "TER", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9245448708534241}, {"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9879122376441956}]}, {"text": "The large difference in terms of quality gains yield by APE can be explained in several ways.", "labels": [], "entities": [{"text": "APE", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.36712342500686646}]}, {"text": "One is the different amount of in-domain training data available: in the PBSMT subtask, they comprise 28,000 instances while, in the NMT subtask, they are less than 14,000.", "labels": [], "entities": [{"text": "NMT subtask", "start_pos": 133, "end_pos": 144, "type": "DATASET", "confidence": 0.9415890574455261}]}, {"text": "Another reason is the different MT output quality in the two datasets.", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9722803235054016}]}, {"text": "Indeed, TER and BLEU scores for the PBSMT test set are respectively 24.24 and 62.99 while, in the NMT test set, they reach considerably better values of 16.84 and 74.73.", "labels": [], "entities": [{"text": "TER", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9994333386421204}, {"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9981075525283813}, {"text": "PBSMT test set", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.9542800188064575}, {"text": "NMT test set", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.9771894216537476}]}, {"text": "Altogether, these differences contributed to make the NMT subtask more challenging, participants' scores concentrated in small TER/BLEU ranges close to the baseline and the overall results harder to interpret.", "labels": [], "entities": [{"text": "TER", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.9931964874267578}, {"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.7447326183319092}]}], "datasetContent": [{"text": "System performance was evaluated both by means of automatic metrics and manually.", "labels": [], "entities": []}, {"text": "Automatic metrics were used to compute the distance between automatic and human post-edits of the machine-translated sentences present in the test sets (i.e. for each of the target sentences in the PBSMT and NMT test sets).", "labels": [], "entities": [{"text": "NMT test sets", "start_pos": 208, "end_pos": 221, "type": "DATASET", "confidence": 0.9194554885228475}]}, {"text": "To this aim, TER and BLEU (case-sensitive) were respectively used as primary and secondary evaluation metrics.", "labels": [], "entities": [{"text": "TER", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9992175102233887}, {"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9985073208808899}]}, {"text": "Systems were ranked based on the average TER calculated on the test set by using the TERcom 5 software: lower average TER scores correspond to higher ranks.", "labels": [], "entities": [{"text": "TER", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9965051412582397}, {"text": "TERcom 5 software", "start_pos": 85, "end_pos": 102, "type": "DATASET", "confidence": 0.8561194737752279}, {"text": "TER", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.993242084980011}]}, {"text": "BLEU was computed using the multi-bleu.perl package 6 available in MOSES.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9816059470176697}, {"text": "MOSES", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.8936464190483093}]}, {"text": "Manual evaluation was conducted via direct human assessment ( ) performed by professional translators and proficient translation students, as discussed in Section 6.", "labels": [], "entities": []}, {"text": "In order to complement the automatic evaluation of APE submissions, a manual evaluation of the primary systems submitted (five in total) was conducted.", "labels": [], "entities": [{"text": "APE submissions", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.8226649761199951}]}, {"text": "Similarly to the manual evaluation conducted for last year APE shared task, it was carried out following the direct assessment (DA) approach (.", "labels": [], "entities": [{"text": "APE shared task", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.7807761033376058}]}, {"text": "In this Section, we present the evaluation procedure as well as the results obtained.", "labels": [], "entities": []}, {"text": "The manual evaluation carried out this year involved 12 native German speakers with full professional proficiency in English in the IT domain, with a third of the evaluators being students in translation technologies from Saarland University and the remaining ones researchers and engineers from DFKI.", "labels": [], "entities": [{"text": "DFKI", "start_pos": 296, "end_pos": 300, "type": "DATASET", "confidence": 0.9480945467948914}]}, {"text": "Each evaluator was introduced to the evaluation task through a set of slides and a testing phase of the evaluation platform in order to be familiar with the user interface and its functionalities.", "labels": [], "entities": []}, {"text": "A screenshot of the evaluation interface is presented in.", "labels": [], "entities": []}, {"text": "A single assessment consists in assigning a score to a German sentence indicating how much of the meaning from a source sentence in English is expressed.", "labels": [], "entities": []}, {"text": "In other words, the adequacy of a translation is directly evaluated on a scale from 0 to 100 given the source.", "labels": [], "entities": []}, {"text": "The evaluators are free to conduct as many assessments as they want and free to schedule their own evaluation sessions.", "labels": [], "entities": []}, {"text": "In addition, there was no requirement regarding a minimum amount of assessments to perform.", "labels": [], "entities": []}, {"text": "The evaluation took place over a period of a month and  was conducted in two sessions: a first one focusing on the PBSMT subtask and a second one on the NMT subtask.", "labels": [], "entities": [{"text": "PBSMT subtask", "start_pos": 115, "end_pos": 128, "type": "DATASET", "confidence": 0.9322325587272644}, {"text": "NMT subtask", "start_pos": 153, "end_pos": 164, "type": "DATASET", "confidence": 0.9538830816745758}]}, {"text": "For each subtask, the submitted post-edited test sets from the participants were presented to the evaluators one sentence at a time along with the corresponding source sentence.", "labels": [], "entities": []}, {"text": "In order to define a baseline and an upperbound for this manual evaluation, the baseline (no post-edits) and the human post-edited MT output were added to the pool of submissions to evaluate, leading to a total of 14, 000 and 7, 161 pairs of segments to evaluate for the PBSMT and NMT subtasks respectively.", "labels": [], "entities": [{"text": "PBSMT", "start_pos": 271, "end_pos": 276, "type": "DATASET", "confidence": 0.90977543592453}]}, {"text": "However, it was possible to take advantage of the fact that multiple systems can produce identical outputs, allowing us to combine them and reduce the total number of source-target pairs to evaluate.", "labels": [], "entities": []}, {"text": "contains the statistics relative to the numbers of translations in total for all systems, as well as savings in terms of assessment effort that was gained by combining identical system outputs prior to running the evaluation.", "labels": [], "entities": []}, {"text": "Based on the direct assessment scores provided by the evaluators, two scores were computed for each system.", "labels": [], "entities": []}, {"text": "A first score is the average of the segments direct assessment scores (noted \"Avg %\").", "labels": [], "entities": [{"text": "Avg", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9958140254020691}]}, {"text": "For the second score (noted \"Avg z\"), human assessments for translations were first standardized according to each individual human assessor's overall mean and standard deviation score.", "labels": [], "entities": [{"text": "Avg z", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9723494946956635}, {"text": "translations", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.965609610080719}]}, {"text": "Average standardized scores for individual segments belonging to a given system are then computed, before the final overall DA score for that system is computed as the average of its segment scores.", "labels": [], "entities": [{"text": "DA score", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.8394010663032532}]}, {"text": "The twelve human evaluators spent a total of 64 hours on the DA task with an average of 17.2 and 17.5 seconds per assessment for the NMT and PB-SMT subtasks respectively.", "labels": [], "entities": [{"text": "DA task", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.8766424655914307}, {"text": "NMT", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.7361245155334473}]}, {"text": "More details about the    man post-edits with a difference of 2.6%.", "labels": [], "entities": [{"text": "a difference", "start_pos": 46, "end_pos": 58, "type": "METRIC", "confidence": 0.8901797235012054}]}, {"text": "The ranking of primary submissions for the PBSMT subtask is similar to the one obtained with the automatic metrics evaluation, where all primary systems were ranked above the baseline.", "labels": [], "entities": [{"text": "PBSMT subtask", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.864939421415329}]}, {"text": "For the DFKI-MLT system, TER indicates a nonsignificant difference with the baseline while DA scores leads to this system being significantly higher than the baseline.", "labels": [], "entities": [{"text": "TER", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9994533658027649}, {"text": "DA", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.9946596026420593}]}, {"text": "The results of DA for the NMT subtask are presented in.", "labels": [], "entities": [{"text": "DA", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.8824888467788696}]}, {"text": "Similarly to the results obtained with automatic metrics, the baseline is ranked above two and below three primary submissions.", "labels": [], "entities": []}, {"text": "However, none of the submissions are ranked significantly higher or lower than the baseline according to DA scores and all five submissions are placed in the same cluster.", "labels": [], "entities": [{"text": "DA", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9449592232704163}]}, {"text": "The human post-edited MT output reaches an averaged DA score of 96.13%, ranked above the first system (MS UEdin) with an averaged DA score of 91.11%.", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9797737002372742}, {"text": "DA score", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9862480461597443}, {"text": "MS UEdin", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.7685058414936066}, {"text": "DA score", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9831898510456085}]}, {"text": "The range of averaged DA scores for the NMT subtask is smaller) compared to the PBSMT subtask (), which is observed in the results obtained with automatic metrics as well.", "labels": [], "entities": []}, {"text": "This indicates a higher translation adequacy for the NMT subtask and is supported by the averaged DA score obtained by the baseline system (no post-edits).", "labels": [], "entities": [{"text": "DA score", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9788170456886292}]}, {"text": "In addition, the human post-edited MT output reaches a higher averaged DA score for the NMT compared to the PB-SMT subtask (similarly to automatic metrics results), which could indicate a higher overall translation quality of the final translation after manually post-editing the baseline NMT output compared to a baseline PBSMT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9479838013648987}, {"text": "DA score", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9852877855300903}]}, {"text": "However, more experiments involving larger test sets and a larger pool of evaluators are necessary to validate this obser-722 vation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Repetition Rate and translation quality (TER/BLEU of TGT) of the WMT15, WMT16, WMT17 and WMT18 APE  task data. Grey columns refer to data covering different language pairs and domains with respect to this year's evaluation round.", "labels": [], "entities": [{"text": "Repetition Rate", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9429512023925781}, {"text": "TER/BLEU of TGT)", "start_pos": 51, "end_pos": 67, "type": "METRIC", "confidence": 0.7573995292186737}, {"text": "WMT15", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.9218825101852417}, {"text": "WMT16", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.6411757469177246}, {"text": "WMT17", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.6104941964149475}, {"text": "WMT18 APE  task data", "start_pos": 99, "end_pos": 119, "type": "DATASET", "confidence": 0.7388302087783813}]}, {"text": " Table 4: Results for the WMT18 APE PBSMT subtask -average TER (\u2193), BLEU score (\u2191). The symbol \" \u2020\" indicates  a difference from the MT baseline that is not statistically significant. The symbol \"*\" indicates a late submission by the  USAAR DFKI team.", "labels": [], "entities": [{"text": "WMT18 APE PBSMT subtask", "start_pos": 26, "end_pos": 49, "type": "DATASET", "confidence": 0.7408775687217712}, {"text": "TER", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.720579206943512}, {"text": "BLEU score", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9842290580272675}, {"text": "MT baseline", "start_pos": 133, "end_pos": 144, "type": "DATASET", "confidence": 0.7540629506111145}, {"text": "USAAR DFKI team", "start_pos": 235, "end_pos": 250, "type": "DATASET", "confidence": 0.9644151131312052}]}, {"text": " Table 5: Results for the WMT18 APE NMT subtask -average TER (\u2193), BLEU score (\u2191). The symbol \" \u2020\" indicates a  difference from the MT baseline that is not statistically significant.", "labels": [], "entities": [{"text": "WMT18 APE NMT subtask", "start_pos": 26, "end_pos": 47, "type": "DATASET", "confidence": 0.7539938688278198}, {"text": "TER", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.6632579565048218}, {"text": "BLEU score", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9840212464332581}, {"text": "MT baseline", "start_pos": 131, "end_pos": 142, "type": "DATASET", "confidence": 0.7596139311790466}]}, {"text": " Table 8: Data statistics per subtask with the total number of  assessments prior to and after combination of identical target  segments for each source.", "labels": [], "entities": []}, {"text": " Table 9: Direct assessments statistics indicating the number  of assessments carried out per subtask and the average dura- tion in seconds per assessment for the twelve evaluators in- volved in the manual evaluation.", "labels": [], "entities": [{"text": "dura- tion in seconds", "start_pos": 118, "end_pos": 139, "type": "METRIC", "confidence": 0.9478885889053345}]}, {"text": " Table 10: DA Human evaluation results for the PBSMT sub- task in terms of average raw DA (Ave %) and average stan- dardized scores (Ave z). Dashed lines between systems in- dicate clusters according to Wilcoxon signed-rank test at p- level p \u2264 0.05.", "labels": [], "entities": [{"text": "DA", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9581059217453003}, {"text": "stan- dardized scores (Ave z)", "start_pos": 110, "end_pos": 139, "type": "METRIC", "confidence": 0.7176322788000107}]}]}