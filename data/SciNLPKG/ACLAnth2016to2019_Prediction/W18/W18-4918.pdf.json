{"title": [{"text": "All Roads Lead to UD: Converting Stanford and Penn Parses to English Universal Dependencies with Multilayer Annotations", "labels": [], "entities": [{"text": "Stanford and Penn Parses", "start_pos": 33, "end_pos": 57, "type": "DATASET", "confidence": 0.7924439907073975}]}], "abstractContent": [{"text": "We describe and evaluate different approaches to the conversion of gold standard corpus data from Stanford Typed Dependencies (SD) and Penn-style constituent trees to the latest English Universal Dependencies representation (UD 2.2).", "labels": [], "entities": [{"text": "English Universal Dependencies representation (UD 2.2)", "start_pos": 178, "end_pos": 232, "type": "DATASET", "confidence": 0.8496566936373711}]}, {"text": "Our results indicate that pure SD to UD conversion is highly accurate across multiple genres, resulting in around 1.5% errors, but can be improved further to fewer than 0.5% errors given access to annotations beyond the pure syntax tree, such as entity types and coreference resolution, which are necessary for correct generation of several UD relations.", "labels": [], "entities": [{"text": "UD conversion", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.8084561228752136}, {"text": "coreference resolution", "start_pos": 263, "end_pos": 285, "type": "TASK", "confidence": 0.9094508290290833}]}, {"text": "We show that constituent-based conversion using CoreNLP (with automatic NER) performs substantially worse in all genres, including when using gold constituent trees, primarily due to underspecification of phrasal grammatical functions.", "labels": [], "entities": [{"text": "constituent-based conversion", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.6701952368021011}]}], "introductionContent": [{"text": "In the past two years, the Universal Dependencies project (UD,), offering freely available dependency treebanks with a unified annotation scheme in over 50 languages, has grown rapidly, allowing for cross-linguistic comparison and computational linguistics applications.", "labels": [], "entities": [{"text": "cross-linguistic comparison", "start_pos": 199, "end_pos": 226, "type": "TASK", "confidence": 0.7632681429386139}]}, {"text": "At the same time, because of its rapid growth and the need to negotiate annotation schemes across languages, annotating large resources from scratch in the latest UD standard is challenging, not only because of the annotation effort, but also because guidelines may change mid-way, and data and annotator training must be revisited to match the latest developments.", "labels": [], "entities": []}, {"text": "Instead, a large number of projects within UD capitalize on existing treebanks converted from constituent treebanks (in English usually using CoreNLP, ) or other dependency schemes, meaning that for those projects that are not annotated directly in UD, changes to the UD guidelines generally mean adapting an existing converter framework.", "labels": [], "entities": []}, {"text": "In this paper, we concentrate on English dependency treebanking, which has been dominated by data converted from Penn Treebank-style constituent trees (cf..", "labels": [], "entities": [{"text": "Penn Treebank-style constituent trees", "start_pos": 113, "end_pos": 150, "type": "DATASET", "confidence": 0.9736817181110382}]}, {"text": "We compare results of constituent treebank conversions with results from converting English dependency data annotated using the older (and by now frozen) Stanford Typed Depenendencies (hence SD, de Marneffe and Manning 2013).", "labels": [], "entities": []}, {"text": "Specifically, we will be working with the freely available Georgetown University Multilayer corpus (GUM, http://corpling.uis.georgetown.edu/gum/), which we have converted to the latest UD standard (as of UD version 2.2).", "labels": [], "entities": [{"text": "Georgetown University Multilayer corpus (GUM", "start_pos": 59, "end_pos": 103, "type": "DATASET", "confidence": 0.9443387190500895}]}, {"text": "The paper has several goals: We will show that while rule-based SD to UD conversion is already highly accurate, it must also rely on multiple annotation layers outside of the parse proper if the full range of dependencies is targeted.", "labels": [], "entities": [{"text": "rule-based SD to UD conversion", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.5677061438560486}]}, {"text": "For the third goal in particular, our evaluation of the converted UD product reveals that 'native' dependency data in English differs from converted constituents in several ways, including the presence of some rare labels and the proportion of non-projective dependencies.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare UD conversions from SD and constituent annotations in several scenarios on a total of 8,300 tokens, comprising just over 1,000 tokens from each genre in GUM, or about 10% of the corpus, for which we created manually checked gold UD parses.", "labels": [], "entities": [{"text": "UD conversions", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.9045297205448151}]}, {"text": "To evaluate constituent to UD ('C2UD') conversion accuracy, we created three constituent parsed versions of the same data using the Stanford Parser: one based on gold-tokenized plain text, one from data with gold POS tags, and the third, also parsed from gold POS tags, but then manually corrected for errors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9643976092338562}, {"text": "Stanford Parser", "start_pos": 132, "end_pos": 147, "type": "DATASET", "confidence": 0.9244203567504883}]}, {"text": "The manually corrected constituent parses do not introduce empty categories such as PRO or traces, but douse function labels that maybe critical for conversion, such as S-TPC (for fronted direct speech, common e.g. in fiction) and NP-VOC, NP-TMP and NP-ADV for vocative, temporal and other adverbial NPs.", "labels": [], "entities": []}, {"text": "8 C2UD conversion was carried out using CoreNLP 3.9.1, which uses built-in NER and heuristic time expression recognition, but is not completely up-to-date with the current UD standard.", "labels": [], "entities": [{"text": "time expression recognition", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.622098445892334}]}, {"text": "We therefore apply trivial renaming of labels where needed and two heuristic corrections: all coordinating conjunctions (labeled cc) are attached to the original target of the conj relation, so that they point right to left; and all nominal modifiers of verbs (labeled nmod) are re-labeled as obl.", "labels": [], "entities": []}, {"text": "In scoring correct conversion we focus on two metrics: attachment accuracy ignoring punctuation tokens (since punctuation is automatically attached using udapi,, and errors are therefore by-products of other attachment errors), and label accuracy, including punctuation (since some punctuation symbols are occasionally used for non-punctuation functions).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.8193375468254089}, {"text": "accuracy", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.6157216429710388}]}, {"text": "Because there are some differences in the label subtypes produced by CoreNLP and GUM (e.g. obl:tmod, nmod:npmod), we ignore subtypes for the evaluation and focus on main label types.", "labels": [], "entities": []}, {"text": "shows boxplots for the range of error rates across documents from different genres in five scenarios (tokenwise micro-averaged global means are given in blue diamonds), each splits into two metrics: head and label accuracy.", "labels": [], "entities": [{"text": "head and label accuracy", "start_pos": 199, "end_pos": 222, "type": "METRIC", "confidence": 0.5636537149548531}]}], "tableCaptions": []}