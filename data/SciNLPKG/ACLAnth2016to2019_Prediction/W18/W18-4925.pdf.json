{"title": [{"text": "Edition 1.1 of the PARSEME Shared Task on Automatic Identification of Verbal Multiword Expressions", "labels": [], "entities": [{"text": "PARSEME Shared Task on Automatic Identification of Verbal Multiword Expressions", "start_pos": 19, "end_pos": 98, "type": "TASK", "confidence": 0.6050035774707794}]}], "abstractContent": [{"text": "This paper describes the PARSEME Shared Task 1.1 on automatic identification of verbal multi-word expressions.", "labels": [], "entities": [{"text": "PARSEME Shared Task", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7317052880922953}, {"text": "automatic identification of verbal multi-word expressions", "start_pos": 52, "end_pos": 109, "type": "TASK", "confidence": 0.80888003607591}]}, {"text": "We present the annotation methodology, focusing on changes from last year's shared task.", "labels": [], "entities": []}, {"text": "Novel aspects include enhanced annotation guidelines, additional annotated data for most languages, corpora for some new languages, and new evaluation settings.", "labels": [], "entities": []}, {"text": "Corpora were created for 20 languages, which are also briefly discussed.", "labels": [], "entities": []}, {"text": "We report organizational principles behind the shared task and the evaluation metrics employed for ranking.", "labels": [], "entities": []}, {"text": "The 17 participating systems, their methods and obtained results are also presented and analysed.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes edition 1.1 of the PARSEME Shared Task, which builds on this momentum.", "labels": [], "entities": [{"text": "PARSEME Shared Task", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.5970029433568319}]}, {"text": "We amalgamated organizational experience from last year's task, a more polished version of the annotation methodology and an extended set of linguistic data, yielding an event that attracted 12 teams from 9 countries.", "labels": [], "entities": []}, {"text": "Novel aspects in this year's task include additional annotated data for most of the languages, some new languages with annotated datasets and enhanced annotation guidelines.", "labels": [], "entities": []}, {"text": "The structure of the paper is the following.", "labels": [], "entities": []}, {"text": "First, related work is presented, then details on the annotation methodology are described, focusing on changes from last year's shared task.", "labels": [], "entities": []}, {"text": "We have annotated corpora for 20 languages, which are briefly discussed.", "labels": [], "entities": []}, {"text": "Main organizational principles behind the shared task, as well as the evaluation metrics are reported next.", "labels": [], "entities": []}, {"text": "Finally, participating systems are introduced and their results are discussed before we draw our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of the evaluation measures is to represent the quality of system predictions when compared to the human-annotated gold standard fora given language.", "labels": [], "entities": []}, {"text": "As in edition 1.0, we define two types of evaluation measures: a strict per-VMWE score (in which each VMWE in gold is either deemed predicted or not, in a binary fashion); and a fuzzy per-token score (which takes partial matches into account).", "labels": [], "entities": []}, {"text": "For each of these two, we can calculate precision (P), recall (R) and F 1 -scores (F).", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.9392137676477432}, {"text": "recall (R)", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9505305290222168}, {"text": "F 1 -scores (F)", "start_pos": 70, "end_pos": 85, "type": "METRIC", "confidence": 0.9587451560156686}]}, {"text": "Orthogonally to the type of measure, there is the choice of what subset of VMWEs to take into account from gold and system predictions.", "labels": [], "entities": []}, {"text": "As in the previous edition, we calculate a general category-agnostic measure (both per-VMWE and per-token) based on the totality of VMWEs in both gold and system predictions -this measure only considers whether each VMWE has been properly predicted, regardless of category.", "labels": [], "entities": []}, {"text": "We also calculate category-specific measures (both per-VMWE and per-token), where we consider only the subset of VMWEs associated with a given category.", "labels": [], "entities": []}, {"text": "We additionally consider the following phenomenon-specific measures, which focus on some of the challenging phenomena specifically relevant to MWEs (Constant et al., 2017): \u2022 MWE continuity: We calculate per-VMWE scores for two different subsets: continuous e.g. TR istifa edecek 'resignation will-do'\u21d2'he/she will resign', and discontinuous VMWEs e.g. SL imajo investicijske na\u010drte 'they-have investment plans'\u21d2'they have investment plans'.", "labels": [], "entities": [{"text": "MWEs", "start_pos": 143, "end_pos": 147, "type": "TASK", "confidence": 0.9559867978096008}, {"text": "continuity", "start_pos": 179, "end_pos": 189, "type": "METRIC", "confidence": 0.7276615500450134}]}, {"text": "\u2022 MWE length: We calculate per-VMWE scores for two different subsets: single-token, e.g. DE anfangen 'at-catch'\u21d2'begin', ES abstenerse 'abstain-REFL'\u21d2'abstain', and multi-token VMWEs e.g. FA 'eye throw'\u21d2'to look at'.", "labels": [], "entities": []}, {"text": "\u2022 MWE novelty: We calculate per-VMWE scores for two subsets: seen and unseen VMWEs.", "labels": [], "entities": []}, {"text": "We consider a VMWE in the (gold or prediction) test corpus as seen if a VMWE with the same multiset of lemmas is annotated at least once in the training corpus.", "labels": [], "entities": []}, {"text": "Other VMWEs are deemed unseen.", "labels": [], "entities": [{"text": "VMWEs", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.7362656593322754}]}, {"text": "For instance, given the occurrence of EN has anew look in the training corpus, the occurrence of EN had a look of innocence and of EN having a look at this report in the test corpus would be considered seen and unseen, respectively.", "labels": [], "entities": [{"text": "innocence", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9900574088096619}]}, {"text": "\u2022 MWE variability: We calculate per-VMWE scores for the subset of VMWEs that are variants of VMWEs from the training corpus.", "labels": [], "entities": []}, {"text": "A VMWE is considered a variant if: (i) it is deemed as a seen VMWE, as defined above, and (2) it is not identical to another VMWE, i.e. the training corpus does not contain the sequence of surface-form tokens as seen in this VMWE (including non-lexicalized components in between, in the case of discontinuous VMWEs).", "labels": [], "entities": []}, {"text": "E.g., BG \u043d\u0430\u043a\u0440\u0438\u0432\u043e \u043b\u0438 \u0431\u0435\u0448\u0435 \u0441\u0442\u044a\u043f\u0438\u043b is a variant of \u0441\u0442\u044a\u043f\u044f \u043d\u0430\u043a\u0440\u0438\u0432\u043e 'to step to the side'\u21d2'to lose (one's) footing'.", "labels": [], "entities": [{"text": "BG", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.777519702911377}]}, {"text": "Systems may predict VMWEs for all languages in the shared task, and the aforementioned measures are independently calculated for each language.", "labels": [], "entities": []}, {"text": "Additionally, we calculate a macro-average score based on all of the predictions.", "labels": [], "entities": []}, {"text": "In this case, the precision P fora given measure (e.g. for continuous VMWEs) is the average of the precisions for all 19 languages.", "labels": [], "entities": [{"text": "precision P", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9704357087612152}, {"text": "precisions", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9816482067108154}]}, {"text": "Arabic is not considered due to delays in the corpus release.", "labels": [], "entities": []}, {"text": "Missing system predictions are assumed to have P = R = 0.", "labels": [], "entities": [{"text": "P = R", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.7580107053120931}]}, {"text": "The recall R is averaged in the same manner, and the average F score is calculated from these averaged P and R scores.", "labels": [], "entities": [{"text": "recall R", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9849341511726379}, {"text": "F score", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9894692897796631}]}], "tableCaptions": [{"text": " Table 1: Per-language inter-annotator agreement on a sample of S sentences, with A 1 and A 2 VMWEs  annotated by each annotator. F span is the F-measure between annotators, \u03ba span is the agreement on the  annotation span and \u03ba cat is the agreement on the VMWE category. EL, EN and HI provided corpora  annotated by more than 2 annotators. We report the highest scores among all possible annotator pairs.", "labels": [], "entities": [{"text": "F span", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9591846168041229}]}, {"text": " Table 2: Statistics on the training (train), development (dev), and test corpora. Number of sentences  (Sent.), number of tokens (Tok.), average sentence length in number of tokens (Sent. length), total  number of annotated VMWEs (VMWE), and number of annotated VMWEs broken down by category  (VID, IRV, . . . )", "labels": [], "entities": []}, {"text": " Table 4: Results for continuous MWEs.", "labels": [], "entities": []}, {"text": " Table 5: Results for discontinuous MWEs.", "labels": [], "entities": [{"text": "MWEs", "start_pos": 36, "end_pos": 40, "type": "TASK", "confidence": 0.6982525587081909}]}, {"text": " Table 6: Results for multi-token MWEs.", "labels": [], "entities": [{"text": "multi-token MWEs", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.4784029424190521}]}, {"text": " Table 7: Results for single-token MWEs.", "labels": [], "entities": [{"text": "single-token MWEs", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.4854103624820709}]}, {"text": " Table 8: Results for seen-in-train MWEs.", "labels": [], "entities": [{"text": "seen-in-train MWEs", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.4292035847902298}]}, {"text": " Table 9: Results for unseen-in-train MWEs.", "labels": [], "entities": []}, {"text": " Table 10: Results for identical-to-train MWEs.", "labels": [], "entities": [{"text": "identical-to-train MWEs", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.4686862379312515}]}, {"text": " Table 11: Results for variant-of-train MWEs.", "labels": [], "entities": [{"text": "variant-of-train MWEs", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.636731892824173}]}]}