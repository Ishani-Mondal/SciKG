{"title": [{"text": "OpenNMT System Description for WNMT 2018: 800 words/sec on a single-core CPU", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a system description of the OpenNMT Neural Machine Translation entry for the WNMT 2018 evaluation.", "labels": [], "entities": [{"text": "OpenNMT Neural Machine Translation", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.7262329906225204}, {"text": "WNMT 2018 evaluation", "start_pos": 88, "end_pos": 108, "type": "DATASET", "confidence": 0.8884514371554056}]}, {"text": "In this work, we developed a heavily optimized NMT inference model targeting a high-performance CPU system.", "labels": [], "entities": []}, {"text": "The final system uses a combination of four techniques , all of them leading to significant speed-ups in combination: (a) sequence distillation, (b) architecture modifications, (c) pre-computation, particularly of vocabulary , and (d) CPU targeted quantiza-tion.", "labels": [], "entities": []}, {"text": "This work achieves the fastest performance of the shared task, and led to the development of new features that have been integrated to OpenNMT and made available to the community.", "labels": [], "entities": []}], "introductionContent": [{"text": "As neural machine translation becomes more widely deployed in production environments, it becomes also increasingly important to serve translations models in away as fast and as memory-efficient as possible, both on dedicated GPU and on standard CPU hardwares.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.6706574559211731}]}, {"text": "The WN-MT 2018 shared task 1 focused on comparing different systems on both accuracy and computational efficiency.", "labels": [], "entities": [{"text": "WN-MT 2018 shared task 1", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.7396247267723084}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.998955249786377}]}, {"text": "This paper describes the entry for the OpenN-MT system to this competition.", "labels": [], "entities": []}, {"text": "Our specific interest was to explore the different techniques for training and optimizing CPU models for very high throughput while preserving highest possible accuracy compared to state-of-the-art.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9978886246681213}]}, {"text": "While we did not put real focus on memory and docker size foot- print, we applied basic optimization techniques to reduce the final size of our models.", "labels": [], "entities": []}, {"text": "Inference techniques and quantization are described in Section 3.", "labels": [], "entities": []}, {"text": "Our experiments compare the different approaches in terms of speed and accuracy.", "labels": [], "entities": [{"text": "speed", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9982307553291321}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9987344145774841}]}, {"text": "A meta question of this work is to decide which models represent interesting and useful points on the Pareto curve.", "labels": [], "entities": [{"text": "Pareto curve", "start_pos": 102, "end_pos": 114, "type": "DATASET", "confidence": 0.8311686515808105}]}, {"text": "The main results are described in 1.", "labels": [], "entities": []}, {"text": "From these results we highlight two models which were submitted to the shared task: Our first system distill-small-opt is a 2-layer LST-M network that is only -2.19 BLEU points behind the reference transformer model, but with a speedup of x18.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.9981797933578491}]}, {"text": "Our second system achieves 23.11 on WNMT 2018 English-German newstest2014 (-4.85 behind the reference model) but with an additional decoding speed-up of x8.", "labels": [], "entities": [{"text": "WNMT 2018 English-German newstest2014", "start_pos": 36, "end_pos": 73, "type": "DATASET", "confidence": 0.9282793998718262}]}, {"text": "The final model reaches 800 words/sec on the dedicated evaluation CPU hardware and is the fastest CPU model submitted . We additionally report several new results about recent work in NMT and model efficiency: (a) we show that distillation of transformer model to a simple RNN outperforms direct training of a strong RNN model -extending findings of who reported that student models could outperform their teacher for reference RNN-based model.", "labels": [], "entities": []}, {"text": "(b) We compare quantitatively different quantizations, and (c) we give an improved algorithm to dynamically select target vocabulary fora given batch.", "labels": [], "entities": []}, {"text": "Finally, we also report several complementary experiments that resulted in systems inside of the pareto convex border.", "labels": [], "entities": []}, {"text": "For instance, we compare using 8-bit quantization to 16-bit quantization.", "labels": [], "entities": []}], "datasetContent": [{"text": "We explored several other methods which largely resulted in negative results but could be interesting for other contexts.", "labels": [], "entities": []}, {"text": "Decrease the sentence length: For BPE preprocessing, we try using 64K merges which can generate shorter sentences.", "labels": [], "entities": [{"text": "BPE preprocessing", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.642548143863678}]}, {"text": "The average sentence length for 32K BPE is about 29.1 and for 64K BPE, it is about 27.7 tokens.", "labels": [], "entities": []}, {"text": "Assuming the same efficiency in the vocabulary mapping, increasing the size of the vocabulary could therefore have again of about 5% additional speed-up just by the reduction of the sentence length.", "labels": [], "entities": [{"text": "vocabulary mapping", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.709716334939003}]}, {"text": "However, the tradeoff here was not clearly a win.", "labels": [], "entities": []}, {"text": "8-bit quantization: To reduce further the system size, we also considered use of gemmlowp . gemmlowp is a library allowing quantization to unsigned 8 bits integer through dynamic offset/multiplier/shift parameters.", "labels": [], "entities": []}, {"text": "Like our implementation of 16-bit quantization, the low precision is only for storing the parameters, the dot product is using larger register for accumulating intermediate result of the operation.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9922782182693481}]}, {"text": "gemmlowp usage is for embedded application where speed but also power usage is critical.", "labels": [], "entities": []}, {"text": "The idea was tempting, but it was not clear if such quantization schema could actually outperform quantization using SIMD extended instruction set on modern processors.", "labels": [], "entities": []}, {"text": "We ran comparative tests using AVX2 instructions set and found out that for multiplications of large matrixes (20, 1024) * (1024, 512) -optimized IN-T16 implementation was about 3 times faster than gemmlowp UINT8 implementation", "labels": [], "entities": [{"text": "AVX2 instructions set", "start_pos": 31, "end_pos": 52, "type": "DATASET", "confidence": 0.9098909497261047}]}], "tableCaptions": [{"text": " Table 1: Evaluations of n-gram vocabulary mappings on newstest2014.", "labels": [], "entities": []}, {"text": " Table 2: Configurations for the different systems presented in the paper", "labels": [], "entities": []}, {"text": " Table 3: Evaluations on NMT systems (the suffix \"-opt\" means it is the final submission)", "labels": [], "entities": []}]}