{"title": [{"text": "Concept Transfer Learning for Adaptive Language Understanding", "labels": [], "entities": [{"text": "Concept Transfer Learning", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7601562142372131}, {"text": "Adaptive Language Understanding", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.8203801512718201}]}], "abstractContent": [{"text": "Concept definition is important in language understanding (LU) adaptation since literal definition difference can easily lead to data sparsity even if different data sets are actually semantically correlated.", "labels": [], "entities": [{"text": "Concept definition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7612755000591278}, {"text": "language understanding (LU) adaptation", "start_pos": 35, "end_pos": 73, "type": "TASK", "confidence": 0.8839599887530009}]}, {"text": "To address this issue, in this paper, a novel concept transfer learning approach is proposed.", "labels": [], "entities": []}, {"text": "Here, substructures within literal concept definition are investigated to reveal the relationship between concepts.", "labels": [], "entities": []}, {"text": "A hierarchical semantic representation for concepts is proposed, where a semantic slot is represented as a composition of atomic concepts.", "labels": [], "entities": []}, {"text": "Based on this new hierarchical representation, transfer learning approaches are developed for adaptive LU.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.9008220136165619}]}, {"text": "The approaches are applied to two tasks: value set mismatch and domain adaptation , and evaluated on two LU benchmarks: ATIS and DSTC 2&3.", "labels": [], "entities": [{"text": "value set mismatch", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.6357430219650269}, {"text": "domain adaptation", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7572205662727356}, {"text": "ATIS", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.8826223611831665}, {"text": "DSTC 2&3", "start_pos": 129, "end_pos": 137, "type": "DATASET", "confidence": 0.942097470164299}]}, {"text": "Thorough empirical studies validate both the efficiency and effectiveness of the proposed method.", "labels": [], "entities": []}, {"text": "In particular, we achieve state-of-the-art performance (F 1-score 96.08%) on ATIS by only using lexicon features.", "labels": [], "entities": [{"text": "F 1-score 96.08", "start_pos": 56, "end_pos": 71, "type": "METRIC", "confidence": 0.9739555517832438}, {"text": "ATIS", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8522189855575562}]}], "introductionContent": [{"text": "The language understanding (LU) module is a key component of dialogue system (DS), parsing user's utterances into corresponding semantic concepts (or semantic slots 1 ).", "labels": [], "entities": [{"text": "dialogue system (DS)", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.6597253799438476}]}, {"text": "For example, the utterance \"Show me flights from Boston to New York\" can be parsed into (from city=Boston, to city=New York) (.", "labels": [], "entities": []}, {"text": "Typically, the LU is seen as a plain slot filling task.", "labels": [], "entities": [{"text": "plain slot filling task", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.6867721900343895}]}, {"text": "With sufficient in-domain data and deep learning models (e.g. recurrent neural networks, bidirectional long-short term memory network), statistical methods have achieved satisfactory performance in the slot filling task recently (.", "labels": [], "entities": [{"text": "slot filling task", "start_pos": 202, "end_pos": 219, "type": "TASK", "confidence": 0.8959259986877441}]}, {"text": "However, retrieving sufficient in-domain data for training LU model () is unrealistic, especially when the semantic slot extends or dialogue domain changes.", "labels": [], "entities": []}, {"text": "The ability of LU approaches to cope with changed domains and limited data is a key to the deployment of commercial dialogue systems (e.g. Apple Siri, Amazon Alexa, Google Home, Microsoft Cortana etc).", "labels": [], "entities": []}, {"text": "In this paper, we investigate substructure of semantic slots to find out slot relations and promote data reuse.", "labels": [], "entities": []}, {"text": "We represent semantic slots with a hierarchical structure based on atomic concept tuple, as shown in.", "labels": [], "entities": []}, {"text": "Each semantic slot is composed of different atomic concepts, e.g. slot \"from city\" can be defined as a tuple of atoms [\"from location\", \"city name\"], FC refers to \"from city\".", "labels": [], "entities": []}, {"text": "TC refers to \"to city\". and \"date of birth\" can be defined as.", "labels": [], "entities": [{"text": "TC", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.6498053669929504}]}, {"text": "Unlike the traditional slot definition on a plain level, modeling on the atomic concepts helps identify linguistic patterns of related slots by atom sharing, and even decrease the required amount of training data.", "labels": [], "entities": []}, {"text": "For example, the training and test sets are unmatched in, whereas the patterns of atomic concepts (e.g. \"from\", \"to\", \"city\") can be shared.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the slot filling task switching from plain slots to hierarchical structures by proposing the novel atomic concept tuples which are constructed manually.", "labels": [], "entities": [{"text": "slot filling task", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.861647387345632}]}, {"text": "For comparison, we also introduce a competitive method which automatically learns slot representation from the word sequence of each slot name.", "labels": [], "entities": [{"text": "slot representation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7790839672088623}]}, {"text": "Our methods are applied to value set mismatch and domain adaptation problems on ATIS ( and DSTC 2&3 () respectively.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7182402610778809}, {"text": "ATIS", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.969657838344574}, {"text": "DSTC 2&3", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.9486261010169983}]}, {"text": "As shown in the experimental results, the slot-filling based on concept transfer learning is effective in solving the value set mismatch and domain adaptation problems.", "labels": [], "entities": [{"text": "concept transfer learning", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.7787441611289978}, {"text": "domain adaptation", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.7070351392030716}]}, {"text": "The concept transfer learning method especially achieves state-of-theart performance (F 1 -score 96.08%) on the ATIS task.", "labels": [], "entities": [{"text": "F 1 -score 96.08", "start_pos": 86, "end_pos": 102, "type": "METRIC", "confidence": 0.977109146118164}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section is about the relation to prior work.", "labels": [], "entities": []}, {"text": "The atomic concept tuple is introduced in section 3.", "labels": [], "entities": []}, {"text": "The proposed concept transfer learning is then described in section 4.", "labels": [], "entities": [{"text": "concept transfer learning", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.7140990495681763}]}, {"text": "Section 5 describes a competitive method with slot embedding derived from the literal descriptions of slot names.", "labels": [], "entities": []}, {"text": "In section 6, the proposed approach is evaluated on the value set mismatch and domain adaptation problems.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7014911323785782}]}, {"text": "Finally, our conclusions are presented in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our atomic-concept methods on two tasks: value set mismatch and domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7056562155485153}]}, {"text": "Value set mismatch task evaluates the generalization capability of different slot filling models.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.7273851633071899}]}, {"text": "Ina language understanding (LU) system, each slot has a value set with all possible values which can be assigned to it.", "labels": [], "entities": []}, {"text": "Since the semantically annotated data is always limited, only apart of values is seen in the training data.", "labels": [], "entities": []}, {"text": "Will the slot filling model perform well on the unseen values?", "labels": [], "entities": [{"text": "slot filling", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.8707650005817413}]}, {"text": "To answer this question, we synthesize a test set by the values mismatched with the training set of ATIS corpus.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 100, "end_pos": 111, "type": "DATASET", "confidence": 0.922924816608429}]}, {"text": "Our methods may take advantages of the prior knowledge about slot relations based on the atomic concepts and the literal descriptions of slot names.", "labels": [], "entities": []}, {"text": "Domain adaptation task evaluates the adaptation capability of our methods when they meet new slots in the target domain.", "labels": [], "entities": [{"text": "Domain adaptation task", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8122485478719076}]}, {"text": "In this task, a seed training set of the target domain is provided.", "labels": [], "entities": []}, {"text": "However, it is very limited: 1) some new slots may not be covered; 2) not all contexts are covered for each new slot.", "labels": [], "entities": []}, {"text": "The atomic-concepts based method would alleviate this problem.", "labels": [], "entities": []}, {"text": "Each slot is defined as a tuple of atomic concepts in our method.", "labels": [], "entities": []}, {"text": "Therefore, it is possible to learn an unseen slot of the target domain if its atomic concepts exist in the data of the source domain and the seed data of the target domain.", "labels": [], "entities": []}, {"text": "It is also possible to see more contexts fora new slot if its atomic concepts exist in the source domain which has much more data.", "labels": [], "entities": []}, {"text": "We randomly selected 80% of the training data for model training and the remaining 20% for validation.", "labels": [], "entities": []}, {"text": "We deal with unseen words in the test set by marking any words with only one single occurrence in the training set as unk.", "labels": [], "entities": []}, {"text": "We also converted sequences of numbers to the string DIGIT, e.g. 1990 is converted to.", "labels": [], "entities": [{"text": "DIGIT", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.8577852845191956}]}, {"text": "Regarding BLSTM model, we set the dimension of word embeddings to 100 and the number of hidden units to 100.", "labels": [], "entities": []}, {"text": "For training, the network parameters are randomly initialized in accordance with the uniform distribution (-0.2, 0.2).", "labels": [], "entities": []}, {"text": "Stochastic gradient descent (SGD) is used for updating parameters.", "labels": [], "entities": [{"text": "Stochastic gradient descent (SGD)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7641774266958237}]}, {"text": "The dropout with a probability of 0.5 is applied to the non-recurrent connections during the training stage.", "labels": [], "entities": []}, {"text": "We try different learning rates by grid-search in range of [0.008, 0.04].", "labels": [], "entities": []}, {"text": "We keep the learning rate for 100 epochs and save the parameters that give the best performance on the validation set.", "labels": [], "entities": []}, {"text": "Finally, we report the F 1 -score of the semantic slots on the test set with parameters that have achieved the best F 1 -score on the validation set.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9875317066907883}, {"text": "F 1 -score", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.9753498286008835}]}, {"text": "The F 1 -score is calculated using CoNLL evaluation script.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.98834528028965}, {"text": "CoNLL evaluation script", "start_pos": 35, "end_pos": 58, "type": "DATASET", "confidence": 0.8891654213269552}]}, {"text": "summarizes the recently published results on the ATIS slot filling task and compares them with the results of our proposed methods on the standard ATIS test set.", "labels": [], "entities": [{"text": "ATIS slot filling task", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.7248896136879921}, {"text": "ATIS test set", "start_pos": 147, "end_pos": 160, "type": "DATASET", "confidence": 0.9628725647926331}]}, {"text": "We can see that RNN outperforms CRF because of the ability to capture long-term dependencies.", "labels": [], "entities": []}, {"text": "LSTM beats RNN by solving the problem of vanishing or exploding gradients.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6802287101745605}]}, {"text": "BLSTM further improves the result by considering both the past and future features.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7411931157112122}]}, {"text": "Encoder-decoder achieves the state-of-theart performance by modeling the label dependencies.", "labels": [], "entities": []}, {"text": "Encoder-labeler is a similar method to the Encoder-decoder.", "labels": [], "entities": [{"text": "Encoder-decoder", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.9550207257270813}]}, {"text": "These systems are designed to predict the plain semantic slots traditionally.", "labels": [], "entities": []}, {"text": "The experimental settings are similar to the ATIS's, whereas the seed data in DSTC 3 is also used for validation.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.7758482098579407}, {"text": "DSTC 3", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.9484852254390717}]}, {"text": "The performance of our methods in the DSTC 2&3 task is illustrated in.", "labels": [], "entities": [{"text": "DSTC 2&3 task", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.6336104869842529}]}, {"text": "We can see that: 1) By incorporating the data of the source domain (dstc2 train), PS and AC achieve improvements respectively.", "labels": [], "entities": [{"text": "PS", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9469533562660217}]}, {"text": "2) AC gains more than PS by modeling the plain semantic slot as atomic concepts.", "labels": [], "entities": []}, {"text": "The atomic concepts promote the associated slots to share input features for the same atoms.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison with the published results  on the standard ATIS task, and evaluation on  ATIS X test. ( *  denotes our implementation.)", "labels": [], "entities": [{"text": "ATIS task", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.7566646337509155}, {"text": "ATIS X test", "start_pos": 95, "end_pos": 106, "type": "DATASET", "confidence": 0.844581405321757}]}, {"text": " Table 4: The performance of our methods evalu- ated on the DSTC3 S test.", "labels": [], "entities": [{"text": "DSTC3 S test", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.855637768904368}]}]}