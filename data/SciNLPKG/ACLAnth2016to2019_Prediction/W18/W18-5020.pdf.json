{"title": [{"text": "A Context-aware Convolutional Natural Language Generation model for Dialogue Systems", "labels": [], "entities": [{"text": "Context-aware Convolutional Natural Language Generation", "start_pos": 2, "end_pos": 57, "type": "TASK", "confidence": 0.5911773204803467}]}], "abstractContent": [{"text": "Natural language generation (NLG) is an important component in spoken dialog systems (SDSs).", "labels": [], "entities": [{"text": "Natural language generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7659198542435964}]}, {"text": "A model for NLG involves sequence to sequence learning.", "labels": [], "entities": []}, {"text": "State-of-the-art NLG models are built using recurrent neural network (RNN) based sequence to sequence models (Du\u0161ek and Jurcicek, 2016a).", "labels": [], "entities": []}, {"text": "Convolutional sequence to sequence based models have been used in the domain of machine translation but their application as natural language generators in dialogue systems is still unex-plored.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7710902094841003}]}, {"text": "In this work, we propose a novel approach to NLG using convolutional neu-ral network (CNN) based sequence to sequence learning.", "labels": [], "entities": []}, {"text": "CNN-based approach allows to build a hierarchical model which encapsulates dependencies between words via shorter path unlike RNNs.", "labels": [], "entities": []}, {"text": "In contrast to recurrent models, convolutional approach allows for efficient utilization of computational resources by parallelizing computations overall elements, and eases the learning process by applying constant number of nonlinearities.", "labels": [], "entities": []}, {"text": "We also propose to use CNN-based reranker for obtaining responses having semantic correspondence with input dialogue acts.", "labels": [], "entities": []}, {"text": "The proposed model is capable of entrainment.", "labels": [], "entities": []}, {"text": "Studies using a standard dataset shows the effectiveness of the proposed CNN-based approach to NLG.", "labels": [], "entities": []}], "introductionContent": [{"text": "In task-specific spoken dialogue systems (SDS), the function of natural language generation (NLG) components is to generate natural language response from a dialogue act (DA) (.", "labels": [], "entities": [{"text": "task-specific spoken dialogue systems (SDS)", "start_pos": 3, "end_pos": 46, "type": "TASK", "confidence": 0.7024223293576922}, {"text": "natural language generation (NLG)", "start_pos": 64, "end_pos": 97, "type": "TASK", "confidence": 0.8105984429518381}]}, {"text": "DA is a meaning representation specifying actions along with various attributes and their values.", "labels": [], "entities": []}, {"text": "NLG plays a very important role in realizing the overall quality of the SDS.", "labels": [], "entities": [{"text": "NLG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9032004475593567}]}, {"text": "Entrainment to users way of speaking is essential for generating more natural and high quality natural language responses.", "labels": [], "entities": []}, {"text": "Most of the approaches for incorporating entrainment are rule-based models.", "labels": [], "entities": []}, {"text": "Recent advances have been in the direction of developing a fully trainable context aware NLG model).", "labels": [], "entities": []}, {"text": "However, all these approaches are based on recurrent sequence to sequence architecture.", "labels": [], "entities": []}, {"text": "Convolutional neural networks are largely unexplored in the domain of NLG for SDS inspite of having several advantages (.", "labels": [], "entities": []}, {"text": "Recurrent networks depend on the computations of previous time step and thus inhibits parallelization within a sequence.", "labels": [], "entities": []}, {"text": "Convolutional networks on the other hand, allows parallelization within a sequence resulting in efficient use of GPUs and other computational resources (.", "labels": [], "entities": []}, {"text": "Multi-block (multilayer) convolutional networks enable controlling the upper bound on the effective context size and form a hierarchical structure.", "labels": [], "entities": []}, {"text": "In contrast to the sequential structure of RNNs, hierarchical structure provides shorter paths for modeling longrange dependencies.", "labels": [], "entities": []}, {"text": "Recurrent networks apply variable number of nonlinearities to the inputs, whereas convolutional networks apply fixed number of nonlinearities which simplifies the learning (.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel approach of using convolutional sequence to sequence model (ConvSeq2Seq) for the task of NLG.", "labels": [], "entities": []}, {"text": "ConvSeq2Seq generator is an encoder decoder model where convolutional neural networks (CNNs) are used to build both encoder and decoder states.", "labels": [], "entities": []}, {"text": "It uses multi-step attention mechanism.", "labels": [], "entities": []}, {"text": "In the decoding phase, beam search is implemented and nbest natural language responses are chosen.", "labels": [], "entities": [{"text": "beam search", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.8223543167114258}]}, {"text": "The n-best beam search responses from ConvSeq2Seq generator may have some missing and/or irrelevant information.", "labels": [], "entities": [{"text": "ConvSeq2Seq generator", "start_pos": 38, "end_pos": 59, "type": "DATASET", "confidence": 0.9047363102436066}]}, {"text": "To address this, we propose to rank the n-best outputs from ConvSeq2Seq generator using convolutional reranker (CNN reranker).", "labels": [], "entities": []}, {"text": "CNN reranker implements one dimensional convolution on beam search responses and generates binary vectors.", "labels": [], "entities": [{"text": "CNN reranker", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8967428505420685}]}, {"text": "These binary vectors are used to penalize the responses having missing and/or irrelevant information.", "labels": [], "entities": []}, {"text": "We evaluate our model on the Alex Context natural language generation (NLG) dataset of and demonstrate that our model outperforms the RNNbased model of Du\u0161ek and Jurcicek (2016a) (TGen model) in automatic metrics.", "labels": [], "entities": [{"text": "Alex Context natural language generation (NLG) dataset", "start_pos": 29, "end_pos": 83, "type": "DATASET", "confidence": 0.6654142704274919}]}, {"text": "Training time of proposed model is observed to be significantly lower than TGen model.", "labels": [], "entities": [{"text": "Training time", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9230962097644806}]}, {"text": "The main contributions of this work are (i) ConvSeq2Seq generator for NLG and (ii) CNN-based reranker for ranking n-best beam search responses for obtaining semantically appropriate responses with respect to input DA.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief review of different approaches to NLG.", "labels": [], "entities": [{"text": "NLG", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.896226167678833}]}, {"text": "In Section 3, proposed convolutional natural language generator (ConvSeq2Seq) is described along with CNN reranker.", "labels": [], "entities": [{"text": "CNN reranker", "start_pos": 102, "end_pos": 114, "type": "DATASET", "confidence": 0.8653190732002258}]}, {"text": "The experimental studies are presented in Section 4 and conclusions are given in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The studies in this work are performed on Alex Context natural language generation (NLG) dataset).", "labels": [], "entities": [{"text": "Alex Context natural language generation (NLG) dataset", "start_pos": 42, "end_pos": 96, "type": "DATASET", "confidence": 0.8057919674449496}]}, {"text": "This dataset is intended for fully trainable NLG systems in task-oriented spoken dialogue systems (SDS).", "labels": [], "entities": [{"text": "task-oriented spoken dialogue systems (SDS)", "start_pos": 60, "end_pos": 103, "type": "TASK", "confidence": 0.667635841029031}]}, {"text": "It is in the domain of public transport information and has four dialogue act (DA) types namely request, inform, iconfirm and inform no match.", "labels": [], "entities": []}, {"text": "It contains 1859 data instances each having 3 target responses.", "labels": [], "entities": []}, {"text": "Each data instance consists of a preceding context (user utterance), source meaning representation and target natural language responses/sentences.", "labels": [], "entities": []}, {"text": "Data is delexicalized and split into training, validation and test sets as done by.", "labels": [], "entities": []}, {"text": "For training and validation, the three paraphrases are used as separate instances.", "labels": [], "entities": [{"text": "validation", "start_pos": 17, "end_pos": 27, "type": "TASK", "confidence": 0.9681487083435059}]}, {"text": "For evaluation they are used as three target references.", "labels": [], "entities": []}, {"text": "Input to our ConvSeq2Seq generator is a DA prepended with user utterance.", "labels": [], "entities": []}, {"text": "This allows entrainment of the model to the user utterances.", "labels": [], "entities": []}, {"text": "A single dictionary is used for context utterances and DA tokens.", "labels": [], "entities": [{"text": "context utterances", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7007561922073364}]}, {"text": "Our model is trained by minimizing cross-entropy error using Nesterov Accelerated Gradient (NAG) optimizer.", "labels": [], "entities": [{"text": "Nesterov Accelerated Gradient (NAG) optimizer", "start_pos": 61, "end_pos": 106, "type": "METRIC", "confidence": 0.8204913820539202}]}, {"text": "The hyper-parameters are chosen by crossvalidation method.", "labels": [], "entities": []}, {"text": "Based on our experiments on validation set, we use maximum sentences per batch 20, learning rate 0.07, minimum learning rate 0.00001, maximum number of epochs 2000, learning rate shrink factor 0.5, clip-norm 0.5, encoder embedding dimension 100, decoder embedding dimension 100, decoder output embedding dimension 100 and dropout 0.3.", "labels": [], "entities": [{"text": "learning rate shrink factor 0.5", "start_pos": 165, "end_pos": 196, "type": "METRIC", "confidence": 0.8593159794807435}]}, {"text": "Encoder part includes 10 layers/blocks, each having 100 units and kernel width of 7. Decoder part includes 10 layers, each having 100 units and kernel width of 7.", "labels": [], "entities": []}, {"text": "For generating outputs on test set, we choose batch size 128 and beam size 20.", "labels": [], "entities": []}, {"text": "For our CNN reranker, all the possible combinations of DA tokens and its values are considered as classes.", "labels": [], "entities": []}, {"text": "We have 19 such classes.", "labels": [], "entities": []}, {"text": "Each input is a natural language sentence and each output is a set of class labels.", "labels": [], "entities": []}, {"text": "Training is done by minimizing cross-entropy loss using Adam optimizer.", "labels": [], "entities": []}, {"text": "Cross-entropy error is measured on validation set after every 100 steps.", "labels": [], "entities": []}, {"text": "Misclassification penalty for CNN reranker is set to 100.", "labels": [], "entities": [{"text": "Misclassification penalty", "start_pos": 0, "end_pos": 25, "type": "METRIC", "confidence": 0.9577340483665466}, {"text": "CNN reranker", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.8834398090839386}]}, {"text": "Based on our experiments, we choose embedding dimension 128, filter sizes (3,5,7,9), number of filters 64, dropout keep probability 0.5, batch size 100, number of epochs 100 and L2 regularization, \u03bb=0.05.", "labels": [], "entities": [{"text": "dropout keep probability", "start_pos": 107, "end_pos": 131, "type": "METRIC", "confidence": 0.8171663681666056}]}, {"text": "The performance of the proposed ConvSeq2Seq model for NLG is compared with that of TGen model).", "labels": [], "entities": []}, {"text": "For comparison, we have considered NIST), BLEU (), METEOR), ROUGE L () and CIDEr metrics (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9983327984809875}, {"text": "METEOR", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.991289496421814}, {"text": "ROUGE L", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9702714681625366}]}, {"text": "For this study, we have considered script \"mtevalv13a-sig.pl\" (version 13a) that implements these metrics.", "labels": [], "entities": []}, {"text": "This script was used for E2E NLG challenge (.", "labels": [], "entities": [{"text": "E2E NLG challenge", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.7869739929835001}]}, {"text": "We focus on the evaluations using this version.", "labels": [], "entities": []}, {"text": "Our model has also been evaluated using the metric script \"mtevalv11b.pl\" (version 11b) to compare our results with those stated in).", "labels": [], "entities": []}, {"text": "The 13a version takes into account the closest reference length with respect to candidate length for calculation of brevity penalty.", "labels": [], "entities": []}, {"text": "This is in accordance with IBM BLEU.", "labels": [], "entities": [{"text": "IBM", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.6371888518333435}, {"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9485343098640442}]}, {"text": "On the contrary, 11b version takes shortest reference length for measuring brevity penalty.", "labels": [], "entities": []}, {"text": "This is the reason behind higher BLEU scores in the 11b version when compared to 13a version.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9990058541297913}]}, {"text": "Both the models have been evaluated on five different metrics, with NIST and BLEU scores being of atmost importance.", "labels": [], "entities": [{"text": "NIST", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.6637433171272278}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9970166683197021}]}, {"text": "We have used N-gram match reranker with the weight \u03c9 set to 1 based on experiments done on validation set.", "labels": [], "entities": []}, {"text": "When using 11b version for evaluating automatic metrics, weight \u03c9 is set to 5.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Different automatic metric scores of TGen model (RNN+RNN) and proposed model  (CNN+CNN) on test data using evaluation metric version 13a.", "labels": [], "entities": [{"text": "CNN+CNN", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.7620794971783956}]}, {"text": " Table 2: Different automatic metric scores of TGen model (RNN+RNN) and proposed model  (CNN+CNN) on test data using evaluation metric version 11b.", "labels": [], "entities": [{"text": "CNN+CNN", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.7608832120895386}]}]}