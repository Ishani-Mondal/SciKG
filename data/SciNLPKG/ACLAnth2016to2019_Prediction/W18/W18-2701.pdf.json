{"title": [{"text": "Findings of the Second Workshop on Neural Machine Translation and Generation", "labels": [], "entities": [{"text": "Neural Machine Translation and Generation", "start_pos": 35, "end_pos": 76, "type": "TASK", "confidence": 0.7266008138656617}]}], "abstractContent": [{"text": "This document describes the findings of the Second Workshop on Neural Machine Translation and Generation, held in concert with the annual conference of the Association for Computational Linguistics (ACL 2018).", "labels": [], "entities": [{"text": "Neural Machine Translation and Generation", "start_pos": 63, "end_pos": 104, "type": "TASK", "confidence": 0.7768744230270386}, {"text": "Association for Computational Linguistics (ACL 2018)", "start_pos": 156, "end_pos": 208, "type": "TASK", "confidence": 0.6432300359010696}]}, {"text": "First, we summarize the research trends of papers presented in the proceedings, and note that there is particular interest in linguistic structure, domain adaptation, data augmentation, handling inadequate resources, and analysis of models.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.7267595678567886}]}, {"text": "Second, we describe the results of the workshop's shared task on efficient neural machine translation (NMT), where participants were tasked with creating NMT systems that are both accurate and efficient.", "labels": [], "entities": [{"text": "efficient neural machine translation (NMT)", "start_pos": 65, "end_pos": 107, "type": "TASK", "confidence": 0.8199623823165894}]}], "introductionContent": [{"text": "Neural sequence to sequence models) are now a workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simplification.", "labels": [], "entities": [{"text": "machine translation, generation", "start_pos": 133, "end_pos": 164, "type": "TASK", "confidence": 0.6874407678842545}, {"text": "summarization", "start_pos": 166, "end_pos": 179, "type": "TASK", "confidence": 0.9052045345306396}]}, {"text": "The 2nd Workshop on Neural Machine Translation and Generation) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization (, NLG from structured data, dialog response generation (), among others).", "labels": [], "entities": [{"text": "Neural Machine Translation and Generation", "start_pos": 20, "end_pos": 61, "type": "TASK", "confidence": 0.7368940830230712}, {"text": "machine translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7860180139541626}, {"text": "summarization", "start_pos": 196, "end_pos": 209, "type": "TASK", "confidence": 0.984095573425293}, {"text": "dialog response generation", "start_pos": 239, "end_pos": 265, "type": "TASK", "confidence": 0.7316482663154602}]}, {"text": "Overall, the workshop was held with two goals: First, it aimed to synthesize the current state of knowledge in neural machine translation and generation: This year we will continue to encourage submissions that not only advance the state of the art through algorithmic advances, but also analyze and understand the current state of the art, pointing to future research directions.", "labels": [], "entities": [{"text": "neural machine translation and generation", "start_pos": 111, "end_pos": 152, "type": "TASK", "confidence": 0.7063035666942596}]}, {"text": "Towards this goal, we received a number of high-quality research contributions on the topics of linguistic structure, domain adaptation, data augmentation, handling inadequate resources, and analysis of models, which are summarized in Section 2.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.7460583448410034}, {"text": "data augmentation", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.7877841889858246}]}, {"text": "Second, it aimed to expand the research horizons in NMT: Based on panel discussions from the first workshop, we organized a shared task.", "labels": [], "entities": [{"text": "NMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.6488698124885559}]}, {"text": "Specifically, the shared task was on \"Efficient NMT\".", "labels": [], "entities": [{"text": "Efficient", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9225033521652222}]}, {"text": "The aim of this task was to focus on not only accuracy, but also memory and computational efficiency, which are paramount concerns in practical deployment settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9981669187545776}]}, {"text": "The workshop provided a set of baselines for the task, and elicited contributions to help push forward the Pareto frontier of both efficiency and accuracy.", "labels": [], "entities": [{"text": "Pareto frontier", "start_pos": 107, "end_pos": 122, "type": "DATASET", "confidence": 0.6845625042915344}, {"text": "efficiency", "start_pos": 131, "end_pos": 141, "type": "METRIC", "confidence": 0.9860529899597168}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9887332916259766}]}, {"text": "The results of the shared task are summarized in Section 3", "labels": [], "entities": []}], "datasetContent": [{"text": "The first step to the evaluation was deciding what we want to measure.", "labels": [], "entities": []}, {"text": "In the case of the shared task, we used metrics to measure several different aspects connected to how good the system is.", "labels": [], "entities": []}, {"text": "These were measured for systems that were run on CPU, and also systems that were run on GPU.", "labels": [], "entities": []}, {"text": "Accuracy Measures: As a measure of translation accuracy, we used BLEU () and NIST) scores.", "labels": [], "entities": [{"text": "Accuracy Measures", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.8957033157348633}, {"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9474575519561768}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.8228592872619629}, {"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9987115859985352}]}, {"text": "Computational Efficiency Measures: We measured the amount of time it takes to translate the entirety of the test set on CPU or GPU.", "labels": [], "entities": [{"text": "Computational Efficiency Measures", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6295560598373413}]}, {"text": "Time for loading models was measured by having the model translate an empty file, then subtracting this from the total time to translate the test set file.", "labels": [], "entities": []}, {"text": "Memory Efficiency Measures: We measured: (1) the size on disk of the model, (2) the number of parameters in the model, and (3) the peak consumption of the host memory and GPU memory.", "labels": [], "entities": [{"text": "Memory Efficiency Measures", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7141512831052145}]}, {"text": "These metrics were measured by having participants submit a container for the virtualization environment Docker 1 , then measuring from outside the container the usage of computation time and memory.", "labels": [], "entities": []}, {"text": "All evaluations were performed on dedicated instances on Amazon Web Services 2 , specifically of type m5.large for CPU evaluation, and p3.2xlarge (with a NVIDIA Tesla V100 GPU).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Image file sizes of submitted systems.", "labels": [], "entities": []}, {"text": " Table 2: Time consumption and MT evaluation metrics (CPU systems).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.8847483694553375}]}, {"text": " Table 3: Time consumption and MT evaluation metrics (GPU systems).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9229780733585358}]}]}