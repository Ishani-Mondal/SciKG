{"title": [{"text": "A Multi-Context Character Prediction Model fora Brain-Computer Interface", "labels": [], "entities": [{"text": "Multi-Context Character Prediction", "start_pos": 2, "end_pos": 36, "type": "TASK", "confidence": 0.6229724784692129}]}], "abstractContent": [{"text": "Brain-computer interfaces and other aug-mentative and alternative communication devices introduce language-modeing challenges distinct from other character-entry methods.", "labels": [], "entities": []}, {"text": "In particular, the acquired signal of the EEG (electroencephalogram) signal is noisier, which, in turn, makes the user intent harder to decipher.", "labels": [], "entities": []}, {"text": "In order to adapt to this condition , we propose to maintain ambiguous history for every time step, and to employ, apart from the character language model, word information to produce a more robust prediction system.", "labels": [], "entities": []}, {"text": "We present preliminary results that compare this proposed Online-Context Language Model (OCLM) to current algorithms that are used in this type of setting.", "labels": [], "entities": []}, {"text": "Evaluations on both perplexity and predictive accuracy demonstrate promising results when dealing with ambiguous histories in order to provide to the front end a distribution of the next character the user might type.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9294792413711548}]}], "introductionContent": [{"text": "Augmentative and alternative communication (AAC) devices are aimed at individuals facing communication disabilities, and aim to enable them to interact with their environment, assisting with both comprehension as well as expression of the individual; American Speech Language Hearing).", "labels": [], "entities": [{"text": "Augmentative and alternative communication (AAC)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.6978223664419991}, {"text": "American Speech Language Hearing", "start_pos": 251, "end_pos": 283, "type": "DATASET", "confidence": 0.7858686596155167}]}, {"text": "In particular, a Brain-Computer Interface (BCI) has been employed as an aiding device for patients who have experienced loss of motor control, and who might struggle with producing spoken or written language) (as in e.g. Amyotrophic Lateral Sclerosis).", "labels": [], "entities": []}, {"text": "A BCI system measures a user's brain's electrical activity, typically using electroencephalography (EEG), and attempts to infer intent, often in response to stimuli such as row-column scanning.", "labels": [], "entities": []}, {"text": "Communication rates in BCI systems tend to be quite slow.", "labels": [], "entities": []}, {"text": "To reduce the prediction time of the next letter in the sequence, one approach is to incorporate language models into AAC, and BCI in particular).", "labels": [], "entities": []}, {"text": "The potential advantage of combining a language model with EEG information in a BCI system is twofold: achieving higher accuracy of target predictions, as well as gains in the speed of the process of typing the user's intended string).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9972424507141113}]}, {"text": "It may also help with reducing the number of necessary stimuli required for symbol selection.", "labels": [], "entities": [{"text": "symbol selection", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.8953766524791718}]}, {"text": "Mora- describe different types of EEG responses common when employing a typing-based BCI, and in the review of, event-related potentials (ERP) seem to be the prevailing choice in particular.", "labels": [], "entities": []}, {"text": "In the RSVP-Keyboard (, an ERP signal is elicited in response to a rapid display of letters.", "labels": [], "entities": []}, {"text": "Other systems such as the P300) present a user with a grid of letters and the ERP signal is retrieved in response to a flash of each row or column.", "labels": [], "entities": [{"text": "P300", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.9368769526481628}]}, {"text": "In such systems, a language model provides the prior distribution that serves as a bias to the EEG evidence when computing the posterior distribution of a symbol as shown in Equation 1 Traditionally, such BCI systems commit to a single decision (given their posterior results), which introduces a problem once the decision is not aligned with the user.", "labels": [], "entities": []}, {"text": "One option is to allow fora backspace character, though this poses complex modeling challenges).", "labels": [], "entities": []}, {"text": "In our system we propose to partly commit to a decision with regard to the user intent, and maintain more than one candidate for every letter selection.", "labels": [], "entities": []}, {"text": "This way, the best path is frequently re-evaluated and as a result can revise previously typed strings.", "labels": [], "entities": []}, {"text": "Maintaining more than one candidate may reduce frustration among users, and minimize time spent correcting errors.", "labels": [], "entities": []}, {"text": "We also, as recommended by and, explicitly incorporate word-level information and mix word-level and character-level language models to improve letter level prediction.", "labels": [], "entities": [{"text": "letter level prediction", "start_pos": 144, "end_pos": 167, "type": "TASK", "confidence": 0.6527377267678579}]}, {"text": "Notably, this approach allows our system to support out-of-vocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "Ambiguous input is widely used in traditional Automatic Speech Recognition (Jelinek, 1997) systems along with the Viterbi algorithm to find the best path of the utterance bottomup, from acoustic phoneme units (often triphones) to characters and words (often represented as the C \u2022 L \u2022 G composition), and recently in keyboard settings as well by swiping (.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (Jelinek, 1997)", "start_pos": 46, "end_pos": 90, "type": "TASK", "confidence": 0.799212384968996}]}, {"text": "To the best of our knowledge, it has rarely been explored in the area of BCI, with a notable exception being the work of, who later found it to be intractable due to complexity issues (.", "labels": [], "entities": [{"text": "BCI", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.7175464630126953}]}, {"text": "In this work, we present the Online-Context Language Model (OCLM) and describe the process of producing prior distributions given EEG evidence as part of our BCI system.", "labels": [], "entities": []}, {"text": "Our contributions are specifically in the context of BCI as we integrate word language models (within the system), allow for OOV words to be typed, and optimize over word-level paths to compute optimal priors.", "labels": [], "entities": [{"text": "BCI", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.7040457725524902}]}, {"text": "Our mixed-context approach achieves superior performance in the face of noisy input compared with a purely character-level model.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate the OCLM, we constructed a simulated character selection task to compare the performance of three different algorithms.", "labels": [], "entities": [{"text": "OCLM", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.6528589129447937}, {"text": "character selection task", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.7724147737026215}]}, {"text": "The first is a smoothed character 5-gram model (\"NGRAM\"), 2 in which prediction is conducted by intersecting the history (EEG evidence, plus the wildcard \u03c3) with the LM lattice.", "labels": [], "entities": []}, {"text": "In this and our other n-gram models, we used Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "The second algorithm we evaluated against was a \"prefix language model\" that included a character language model (PreLM, Equation 6) explicitly trained on character prefixes of in-vocabulary words (F wp ) combined with a closure over the NGRAM model.", "labels": [], "entities": [{"text": "PreLM", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.8005214929580688}]}, {"text": "This model used an identical prediction mechanism as did the NGRAM model, but was designed to be better-equipped to handle in-progress words.", "labels": [], "entities": []}, {"text": "The third is the OCLM described in Section 2.", "labels": [], "entities": [{"text": "OCLM", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.4705606698989868}]}, {"text": "All models were trained on the Brown corpus, and data split was 80% for training and 20% for testing.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.9775710701942444}]}, {"text": "The results are on the test set.", "labels": [], "entities": []}, {"text": "The test set contained 10, 265 sentences and 176, 280 words.", "labels": [], "entities": []}, {"text": "Our simulated task was as follows.", "labels": [], "entities": []}, {"text": "For each test sentence, we proceed character by character.", "labels": [], "entities": []}, {"text": "At each point in the sentence, we provide the algorithms with some representation of the history of the sentence up to that point, and ask the models to predict the following character target.", "labels": [], "entities": []}, {"text": "We evaluated each model in terms of the traditional average character-level perplexity.", "labels": [], "entities": []}, {"text": "Since our model is intended to be used to support character prediction, we also measured the reciprocal rank of the ground-truth target letter, both overall as well as by position within a word (to compare performance earlier in words vs. later).", "labels": [], "entities": [{"text": "character prediction", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.8933777809143066}]}, {"text": "We also looked at the proportion of predictions for which the model placed the correct target letter in the top 10 sorted guesses (ACC@10).", "labels": [], "entities": [{"text": "ACC", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.9597210884094238}]}, {"text": "We evaluated under two conditions: the first simulating a \"deterministic\" history, in which we assume that the EEG signal is reliable, and an \"ambiguous\" history, in which we take the top-n letters from the EEG signal at each time point as possible selections.", "labels": [], "entities": [{"text": "EEG signal", "start_pos": 207, "end_pos": 217, "type": "DATASET", "confidence": 0.9090521335601807}]}], "tableCaptions": [{"text": " Table 1: Evaluation Results (n=1)", "labels": [], "entities": []}, {"text": " Table 2: Evaluation Results (n=2, n=3)", "labels": [], "entities": []}]}