{"title": [{"text": "Deep Graph Convolutional Encoders for Structured Data to Text Generation", "labels": [], "entities": [{"text": "Structured Data to Text Generation", "start_pos": 38, "end_pos": 72, "type": "TASK", "confidence": 0.5810322821140289}]}], "abstractContent": [{"text": "Most previous work on neural text generation from graph-structured data relies on standard sequence-to-sequence methods.", "labels": [], "entities": [{"text": "neural text generation", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.6454601089159647}]}, {"text": "These approaches linearise the input graph to be fed to a recurrent neural network.", "labels": [], "entities": []}, {"text": "In this paper, we propose an alternative encoder based on graph convolutional networks that directly exploits the input structure.", "labels": [], "entities": []}, {"text": "We report results on two graph-to-sequence datasets that empirically show the benefits of explicitly encoding the input graph structure.", "labels": [], "entities": []}], "introductionContent": [{"text": "Data-to-text generators produce a target natural language text from a source data representation.", "labels": [], "entities": []}, {"text": "Recent neural generation approaches () build on encoder-decoder architectures proposed for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7860265672206879}]}, {"text": "The source data, differently from the machine translation task, is a structured representation of the content to be conveyed.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.7927727500597636}]}, {"text": "Generally, it describes attributes and events about entities and relations among them.", "labels": [], "entities": []}, {"text": "In this work we focus on two generation scenarios where the source data is graph structured.", "labels": [], "entities": []}, {"text": "One is the generation of multi-sentence descriptions of Knowledge Base (KB) entities from RDF graphs (; Gardent et al., 2017a,b), namely the WebNLG task.", "labels": [], "entities": []}, {"text": "The number of KB relations modelled in this scenario is potentially large and generation involves solving various subtasks (e.g. lexicalisation and aggregation).", "labels": [], "entities": []}, {"text": "shows and example of source RDF graph and target natural language description.", "labels": [], "entities": []}, {"text": "The other is the linguistic realisation of the meaning expressed by a source dependency graph), namely the SR11Deep generation task.", "labels": [], "entities": [{"text": "SR11Deep generation task", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.7949496905008951}]}, {"text": "In this task, the semantic relations are linguistically motivated and their number is smaller.", "labels": [], "entities": []}, {"text": "Figure (1b) illustrates a source dependency graph and the corresponding target text.", "labels": [], "entities": []}, {"text": "Most previous work casts the graph structured data to text generation task as a sequence-tosequence problem (.", "labels": [], "entities": [{"text": "text generation", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.7007909417152405}]}, {"text": "They rely on recurrent data encoders with memory and gating mechanisms (LSTM; (Hochreiter and Schmidhuber, 1997)).", "labels": [], "entities": []}, {"text": "Models based on these sequential encoders have shown good results although they do not directly exploit the input structure but rather rely on a separate linearisation step.", "labels": [], "entities": []}, {"text": "In this work, we compare with a model that explicitly encodes structure and is trained end-to-end.", "labels": [], "entities": []}, {"text": "Concretely, we use a Graph Convolutional Network (GCN;) as our encoder.", "labels": [], "entities": []}, {"text": "GCNs area flexible architecture that allows explicit encoding of graph data into neural networks.", "labels": [], "entities": [{"text": "GCNs", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.825359582901001}]}, {"text": "Given their simplicity and expressiveness they have been used to encode dependency syntax and predicate-argument structures in neural machine translation (.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 127, "end_pos": 153, "type": "TASK", "confidence": 0.7058473229408264}]}, {"text": "In contrast to previous work, we do not exploit the sequential information of the input (i.e., with an LSTM), but we solely rely on a GCN for encoding the source graph structure.", "labels": [], "entities": []}, {"text": "The main contribution of this work is showing that explicitly encoding structured data with  GCNs is more effective than encoding a linearized version of the structure with LSTMs.", "labels": [], "entities": []}, {"text": "We evaluate the GCN-based generator on two graph-tosequence tasks, with different level of source content specification.", "labels": [], "entities": []}, {"text": "In both cases, the results we obtain show that GCNs encoders outperforms standard LSTM encoders.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our models on the WebNLG and SR11Deep datasets.", "labels": [], "entities": [{"text": "WebNLG", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.9869958758354187}, {"text": "SR11Deep datasets", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.9161413609981537}]}, {"text": "The WebNLG dataset contains 18102 training and 871 development datatext pairs.", "labels": [], "entities": [{"text": "WebNLG dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9857993423938751}]}, {"text": "The test dataset is split in two sets, test Seen (971 pairs) and a test set with new unseen categories for KB entities.", "labels": [], "entities": []}, {"text": "As here we are interested only in the modelling aspects of the structured input data we focus on our evaluation only on the test partition with seen categories.", "labels": [], "entities": []}, {"text": "The dataset covers 373 distinct relations from DBPedia.", "labels": [], "entities": [{"text": "DBPedia", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.9779586791992188}]}, {"text": "The SR11Deep dataset contains 39279, 1034 and 2398 examples in the training, development and test partitions, respectively.", "labels": [], "entities": [{"text": "SR11Deep dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9741459488868713}]}, {"text": "It covers 117 distinct dependency relations.", "labels": [], "entities": []}, {"text": "Sequential Encoders For both WebNLG and SR11Deep tasks we used a standard sequenceto-sequence model () with an LSTM encoder as baseline.", "labels": [], "entities": [{"text": "WebNLG", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.9589040875434875}]}, {"text": "Both take as input a linearised version of the source graph.", "labels": [], "entities": []}, {"text": "For the WebNLG baseline, we use the linearisation scripts provided by).", "labels": [], "entities": [{"text": "WebNLG baseline", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.9670432209968567}]}, {"text": "For the SR11Deep baseline we follow a similar linearisation procedure as proposed for AMR graphs ().", "labels": [], "entities": [{"text": "SR11Deep baseline", "start_pos": 8, "end_pos": 25, "type": "DATASET", "confidence": 0.9418429732322693}]}, {"text": "We built a linearisation based on a depth first traversal of the input graph.", "labels": [], "entities": []}, {"text": "Siblings are traversed in random order (they are anyway shuffled in the given dataset).", "labels": [], "entities": []}, {"text": "We repeat a child node when anode is revisited by a cycle or has more than one parent.", "labels": [], "entities": []}, {"text": "The baseline model for the WebNLG task uses one layer bidirectional LSTM encoder and one layer LSTM decoder with embeddings and hidden units set to 256 dimensions . For the SR11Deep task we used the same architecture with 500-dimensional hidden states and embeddings.", "labels": [], "entities": []}, {"text": "All hyperparameters tuned on the development set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test results WebNLG task.", "labels": [], "entities": [{"text": "WebNLG", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.9472953081130981}]}, {"text": " Table 2: Test results SR11Deep task.", "labels": [], "entities": []}, {"text": " Table 4: GCN ablation study (layers (L) and skip- connections: none, residual(res) and dense(den)).  Average and standard deviation of BLEU scores  over three runs on the WebNLG dev. set. Number", "labels": [], "entities": [{"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.8939040899276733}, {"text": "WebNLG dev. set", "start_pos": 172, "end_pos": 187, "type": "DATASET", "confidence": 0.9847384293874105}]}]}