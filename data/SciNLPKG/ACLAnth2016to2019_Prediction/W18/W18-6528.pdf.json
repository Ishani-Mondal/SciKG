{"title": [], "abstractContent": [{"text": "Hypertagging, or supertagging for surface realization, is the process of assigning lexical categories to nodes in an input semantic graph.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7973132729530334}]}, {"text": "Previous work has shown that hypertagging significantly increases realization speed and quality by reducing the search space of the realizer.", "labels": [], "entities": []}, {"text": "Building on recent work using LSTMs to improve accuracy on supertagging for parsing, we develop an LSTM hypertagging method for OpenCCG, an open source NLP toolkit for CCG.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9980577826499939}, {"text": "parsing", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.9717731475830078}, {"text": "OpenCCG", "start_pos": 128, "end_pos": 135, "type": "DATASET", "confidence": 0.9406548738479614}]}, {"text": "Our results show significant improvements in both hypertagging accuracy and downstream realization performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.969254732131958}]}], "introductionContent": [{"text": "Hypertagging, or supertagging for surface realization, is the process of assigning lexical categories to nodes in a semantic graph as a step in grammar-based surface realization.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7925540506839752}, {"text": "grammar-based surface realization", "start_pos": 144, "end_pos": 177, "type": "TASK", "confidence": 0.7306026021639506}]}, {"text": "It significantly increases realization speed and quality by reducing the search space of the realizer.", "labels": [], "entities": [{"text": "realization", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.9598332643508911}]}, {"text": "built the original single-stage maximum entropy hypertagger for CCG in OpenCCG, which obtained a single-best hypertagging accuracy of 93.6%.", "labels": [], "entities": [{"text": "OpenCCG", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9542062282562256}, {"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9669939875602722}]}, {"text": "This hypertagger was later expanded into a two-stage model, which obtained a hypertagging accuracy of 95.1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9443069100379944}]}, {"text": "Recent work has used neural networks to make significant improvements in supertagging for parsing (i.e., predicting lexical categories fora sequence of words), improving upon earlier work with maximum entropy supertagging).", "labels": [], "entities": [{"text": "predicting lexical categories fora sequence of words", "start_pos": 105, "end_pos": 157, "type": "TASK", "confidence": 0.8226259691374642}]}, {"text": "used a feedforward neural network to obtain 91.3% category accuracy in the CCGBank (Hockenmaier 1 http://openccg.sf.net and Steedman, 2007) development section (Section 00).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.8599211573600769}, {"text": "CCGBank", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.9549651741981506}, {"text": "Hockenmaier 1 http://openccg.sf.net and Steedman, 2007) development section", "start_pos": 84, "end_pos": 159, "type": "DATASET", "confidence": 0.887287030617396}]}, {"text": "improved on this result using an LSTM, obtaining 94.9% category accuracy on the development section and 94.7% on the test section (Section 23) of the CCGBank.", "labels": [], "entities": [{"text": "category", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9083356857299805}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.8539117574691772}, {"text": "CCGBank", "start_pos": 150, "end_pos": 157, "type": "DATASET", "confidence": 0.8930736780166626}]}, {"text": "Our work uses techniques from to implement an improved hypertagger.", "labels": [], "entities": []}, {"text": "To do so, we first linearize the input graph using a method adapted from approach to generating from Abstract Meaning Representations (AMRs).", "labels": [], "entities": []}, {"text": "Unlike in their work though, we found that the input ordering method substantially impacted hypertagging accuracy, with an English-like ordering yielding substantial improvements over random ordering while substantially trailing oracle ordering.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9929250478744507}]}, {"text": "We evaluated the LSTM hypertagger on both tagging accuracy and its downstream effect on realization performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9445741176605225}]}, {"text": "Our results show significant improvement over the original hypertagger on both.", "labels": [], "entities": [{"text": "hypertagger", "start_pos": 59, "end_pos": 70, "type": "METRIC", "confidence": 0.9783174991607666}]}, {"text": "We obtained 96.47% on tagging accuracy (up from 95.1%) and an increase in realization BLEU scores from 0.8429 with the original hypertagger to 0.8683 with the neural hypertagger.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9489737153053284}, {"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9864441752433777}]}, {"text": "As expected, the LSTM hypertagger yielded large gains inaccuracy on the difficult cases of unseen predicates and predicates not seen with the gold tag in training, helping to achieve a 7.8% increase in sentences with grammatically complete derivations.", "labels": [], "entities": []}, {"text": "A human evaluation confirmed that using the LSTM hypertagger yielded significant improvements inadequacy and fluency, especially in cases where the LSTM hypertagger was essential for obtaining a complete derivation.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides background on surface realization with CCG, the maximum entropy hypertagger and LSTM supertagging.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7124860286712646}]}, {"text": "Section 3 describes our fea-tures and model along with our approach to input linearization.", "labels": [], "entities": []}, {"text": "The results and analysis appear in Section 4.", "labels": [], "entities": []}, {"text": "Related work is discussed in Section 5, including where grammar-based realization stands in the current research landscape.", "labels": [], "entities": []}], "datasetContent": [{"text": "We also did a targeted human evaluation to determine whether the improvements in hypertagging accuracy generally led to noticeable improvements in realization quality, especially where it enabled a complete realization to be found.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.93878573179245}]}, {"text": "Our procedure was as follows.", "labels": [], "entities": []}, {"text": "We randomly chose 100 sentences from the devset where the realizations differed between using the LSTM hypertagger and the baseline two-stage MaxEnt hypertagger.", "labels": [], "entities": []}, {"text": "50 of the sentences were ones for which one system got a complete realization but the other did not-where we expected to find substantial differences-while the other 50 were ones for which either both systems got a complete realization or both did not get a complete realization.", "labels": [], "entities": []}, {"text": "We generated a spreadsheet with the following columns: Reference sentence, Realization A, Realization B, Adequacy, and Fluency.", "labels": [], "entities": [{"text": "Adequacy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9615783095359802}, {"text": "Fluency", "start_pos": 119, "end_pos": 126, "type": "METRIC", "confidence": 0.9936756491661072}]}, {"text": "Which system was A for each sentence was randomized,: Comparison in realization performance between systems using the LSTM and two-stage MaxEnt hypertaggers.", "labels": [], "entities": []}, {"text": "The 'Complete' column indicates the percentage of logical forms realized with a complete (non-fragmentary) derivation, while the 'Exact' column indicates the percentage of realizations that exactly matched the reference sentence.", "labels": [], "entities": []}, {"text": "as was the order of sentences.", "labels": [], "entities": []}, {"text": "A separate key file kept track of which system was A for each sentence, and which sentence belonged to which of the above sets of 50.", "labels": [], "entities": []}, {"text": "Two linguists who had no familiarity with the research evaluated the realizations, marking in the Adequacy and Fluency columns whether they believed A or B was better inadequacy and fluency, respectively, or whether they were equally good for one or both measures.", "labels": [], "entities": [{"text": "Adequacy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9797044992446899}, {"text": "Fluency", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.7167357206344604}]}, {"text": "Raw agreement was relatively high, with the two judges agreeing on adequacy 70% of the time and fluency 73% of the time.", "labels": [], "entities": [{"text": "agreement", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.7684211730957031}]}, {"text": "However, nearly all the disagreements involved cases where only one judge found the pair of realizations to be the same on adequacy or fluency; on the subset of items where neither judge found the pair to be equal, agreement was 96% for adequacy and 95% for fluency.", "labels": [], "entities": []}, {"text": "The results of the human evaluations are summarized in.", "labels": [], "entities": []}, {"text": "For the sentences where only one system produced a complete realization (Set 1), the LSTM hypertagger system outperformed the baseline one most of the time on both adequacy and fluency.", "labels": [], "entities": []}, {"text": "For the other sentences (Set 2), the two systems were mostly tied on adequacy and fluency, but when the realizations were of distinct quality, the LSTM hypertagger system usually outperformed the original one.", "labels": [], "entities": []}, {"text": "All differences in the counts of Better/Worse judgments were highly significant (p < 0.001, sign test).", "labels": [], "entities": [{"text": "sign test", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.939891517162323}]}, {"text": "Examples of the changes yielded by the LSTM hypertagger appear in, where the first two examples improve both adequacy and fluency, the next example makes adequacy and fluency worse, and the final one leaves adequacy and fluency the same.", "labels": [], "entities": []}, {"text": "With wsj 0080.21, it seems that the twostage MaxEnt system failed to match the subject with the verb, yielding a realization where respond doesn't have a subject and them is not clearly linked with its antecedent.", "labels": [], "entities": []}, {"text": "With wsj 0004.8, the LSTM system switched yields and nevertheless, making a realization that's different from the original sentence, but still grammatical.", "labels": [], "entities": []}, {"text": "The two-stage MaxEnt system seemed to have trouble combining the words yields, nevertheless, and may, making a realization that gives an awkward order for these words and splits the sentence with the phrase said Brenda Malizia Negus at an awkward place; moreover, with may appearing initially, the sentence can be read as a wish rather than a declarative statement.", "labels": [], "entities": []}, {"text": "With wsj 0097.19, the LSTM system incorrectly inverts the main subject and verb, and makes several other word order mistakes.", "labels": [], "entities": []}, {"text": "Finally, wsj 0037.9 is an example where leaving out the complementizer that or the contraction does not substantially affect adequacy or fluency (though the reference sentence arguably makes the best choices here).", "labels": [], "entities": []}, {"text": "More generally, while the choice whether to include a that-complementizer occasionally made a crucial difference, they were a frequent source of insubstantial differences, along with contractions and adverbial placement.", "labels": [], "entities": []}, {"text": "There were also cases where both realizations made distinct but important mistakes that yielded equally bad realizations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of tagging accuracies between LSTM hypertagger and published single-stage  MaxEnt hypertagger on the development section (Section 00) of CCGBank. Results (in percentages) are  for per-predicate tagging accuracies.", "labels": [], "entities": [{"text": "CCGBank", "start_pos": 158, "end_pos": 165, "type": "DATASET", "confidence": 0.8221666812896729}]}, {"text": " Table 2: Comparison of tagging accuracies between LSTM hypertagger and unpublished two-stage  MaxEnt hypertagger on hard cases in the development set, namely predicates that are not seen in training  and predicates that are seen in training but not with correct supertag.", "labels": [], "entities": []}, {"text": " Table 3: Comparison in realization performance between systems using the LSTM and two-stage Max- Ent hypertaggers. The 'Complete' column indicates the percentage of logical forms realized with a com- plete (non-fragmentary) derivation, while the 'Exact' column indicates the percentage of realizations that  exactly matched the reference sentence.", "labels": [], "entities": []}, {"text": " Table 4: Results (counts of judgments) of human evaluations of realizations, which indicate how often  the new system produced better, same, and worse realizations for the given aspect. Set 1 is the set of  sentences in which one system had a complete realization while the other did not. Set 2 is the set of  sentences in which either both systems had complete realizations or both did not.", "labels": [], "entities": []}]}