{"title": [{"text": "Extracting Syntactic Trees from Transformer Encoder Self-Attentions", "labels": [], "entities": []}], "abstractContent": [], "introductionContent": [{"text": "Interpreting neural networks is a popular topic, and there are many works focusing on analyzing networks with respect to learning syntax (.", "labels": [], "entities": [{"text": "Interpreting neural networks", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8570538957913717}]}, {"text": "In particular, showed that the self-attentions in their Transformer architecture maybe directly interpreted as syntactic dependencies between tokens.", "labels": [], "entities": []}, {"text": "However, there is a potential problem in the fact that the attention mechanism on deeper layers operates on the previous-layer neurons, which already comprise mixed information from multiple source tokens.", "labels": [], "entities": []}, {"text": "Our goal is to infer source sentence tree structures form the encoder's self-attention energies used in the Transfomer neural machine translation (NMT) system.", "labels": [], "entities": [{"text": "Transfomer neural machine translation (NMT)", "start_pos": 108, "end_pos": 151, "type": "TASK", "confidence": 0.705884073461805}]}, {"text": "We would like to visualize how the self-attention mechanism connects individual words (or wordpieces) of the sentence, to create various tree structures (e.g. constituency trees, undirected trees, dependency trees), and to discuss their characteristics with respect to the existing syntactic theories and annotations.", "labels": [], "entities": []}, {"text": "We would also like to discuss results across various languages and natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "In this abstract, we present our preliminary results, analyzing the encoder in English-to-German NMT within the NeuralMonkey toolkit).", "labels": [], "entities": []}, {"text": "We introduce aggregation of self-attention through layers to get a distribution over the input tokens for each encoder position and layer (Section 2).", "labels": [], "entities": []}, {"text": "We then propose algorithms for constructing two types of syntactic trees (Sections 3 and 4), apply them to 42 sentences sampled from PennTB (, and compare the resulting structures to established syntax annotation styles, such as that of PennTB, UD (Nivre et al., 2016), or PDT (.", "labels": [], "entities": [{"text": "PennTB", "start_pos": 133, "end_pos": 139, "type": "DATASET", "confidence": 0.9659131765365601}, {"text": "PennTB", "start_pos": 237, "end_pos": 243, "type": "DATASET", "confidence": 0.9781451225280762}]}], "datasetContent": [], "tableCaptions": []}