{"title": [{"text": "Autonomous Sub-domain Modeling for Dialogue Policy with Hierarchical Deep Reinforcement Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "Solving composites tasks, which consist of several inherent sub-tasks, remains a challenge in the research area of dialogue.", "labels": [], "entities": []}, {"text": "Current studies have tackled this issue by manually decomposing the composite tasks into several sub-domains.", "labels": [], "entities": []}, {"text": "However, much human effort is inevitable.", "labels": [], "entities": []}, {"text": "This paper proposes a dialogue framework that autonomously models meaningful sub-domains and learns the policy over them.", "labels": [], "entities": []}, {"text": "Our experiments show that our framework outperforms the baseline without sub-domains by 11% in terms of success rate, and is competitive with that with manually defined sub-domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modeling a composite dialogue (, which consists of several inherent subtasks, is in high demand due to the complexity of human conversation.", "labels": [], "entities": []}, {"text": "For instance, a composite dialogue of making a hotel reservation involves several sub-tasks, such as looking fora hotel that meets the user's constraints, booking the room, and paying for the room.", "labels": [], "entities": []}, {"text": "The completion of a composite dialogue requires the fulfillment of all involved sub-tasks.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the development of a dialogue agent that can discover inherent sub-tasks autonomously from a composite domain, learn a policy to fulfill each sub-task, and learn a policy among these sub-tasks to solve the composite task.", "labels": [], "entities": []}, {"text": "Composite dialogues are different from multi-domain dialogues.", "labels": [], "entities": []}, {"text": "In multidomain dialogue systems, each dialogue typically involves one domain, and consequently, its fulfillment does not need policy across domains.", "labels": [], "entities": []}, {"text": "To develop a dialogue agent that can handle a composite task, using standard flat reinforcement learning (RL), which are often used for dialogues with a simple task (, might be inappropriate.", "labels": [], "entities": []}, {"text": "Flat RL methods, such as DQN (, could suffer from the curse of dimensionality, that is the number of parameters to be learned grows exponentially with the size of any compact encoding of system state.", "labels": [], "entities": [{"text": "DQN", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.8462557196617126}]}, {"text": "Therefore, flat RL is unable to learn reliable value functions () fora composite task.", "labels": [], "entities": []}, {"text": "A composite task has a larger state space and action set, longer trajectory, and more sparse rewards than a simple task.", "labels": [], "entities": []}, {"text": "Hierarchical reinforcement learning (HRL)) is a technique to model complex dialogues. and  used the options framework () to solve the above problems in composite dialogues and showed its superiority over flat RL.", "labels": [], "entities": [{"text": "Hierarchical reinforcement learning (HRL))", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7449922412633896}]}, {"text": "In their work, however, each option (i.e. sub-task) and its property (e.g. starting and terminating conditions, and valid action set) had to be manually defined.", "labels": [], "entities": []}, {"text": "Such handcrafted options ease the policy learning in a composite task, but much human effort is inevitable.", "labels": [], "entities": [{"text": "policy learning", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.9298298060894012}]}, {"text": "To solve the above problems, we propose to model sub-domains autonomously without any human intervention.", "labels": [], "entities": []}, {"text": "The modeled sub-domains imitate the intentions to fulfill sub-tasks in a dialogue, which consequently can be reused by similar yet different domains.", "labels": [], "entities": []}, {"text": "Challenges to achieve such autonomous sub-domain modeling include (i) how to discover meaningful sub-domains and their properties (i.e. starting conditions, terminating conditions, and the policies), and (ii) how to have a coherent interaction among these subdomains so that the dialogue agent can accomplish a dialogue goal efficiently.", "labels": [], "entities": []}, {"text": "To tackle these challenges, we propose a unified framework that integrates option discovery () with HRL to learn the opti-mal policies over options.", "labels": [], "entities": [{"text": "option discovery", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.7132778018712997}]}, {"text": "With an evaluation involving a task of reserving hotel room, we confirm that our framework achieves a significant improvement over flat RL by 11% in terms of success rate, and is competitive with the framework with manually defined options ( ).", "labels": [], "entities": [{"text": "RL", "start_pos": 136, "end_pos": 138, "type": "METRIC", "confidence": 0.9094679951667786}]}], "datasetContent": [{"text": "We conducted three evaluations on (i) the effectiveness of our autonomous sub-domain modeling compared to the manual sub-domain modeling, (ii) the performance difference between flat RL (i.e. without modeling) and the HRL with autonomous modeling, and (iii) the impact of using PVFs in discovering meaningful sub-domains.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Highest SR of dialogue agents.", "labels": [], "entities": [{"text": "SR", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9622159004211426}]}, {"text": " Table 2: Average turn distance between activation  of user's sub-domains and agent's", "labels": [], "entities": [{"text": "Average turn distance between activation  of user's sub-domains and agent's", "start_pos": 10, "end_pos": 85, "type": "Description", "confidence": 0.7227788964907328}]}, {"text": " Table 3: Sample dialogue by HRL-OC PVF (S:  agent, U: user). Different font styles and colors  indicate different sub-domains activated by agent.", "labels": [], "entities": []}]}