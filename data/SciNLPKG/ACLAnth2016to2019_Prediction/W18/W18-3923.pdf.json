{"title": [{"text": "Identification of Differences between Dutch Language Varieties with the VarDial2018 Dutch-Flemish Subtitle Data", "labels": [], "entities": [{"text": "Identification of Differences between Dutch Language Varieties", "start_pos": 0, "end_pos": 62, "type": "TASK", "confidence": 0.8961540801184518}, {"text": "VarDial2018 Dutch-Flemish Subtitle Data", "start_pos": 72, "end_pos": 111, "type": "DATASET", "confidence": 0.8588060438632965}]}], "abstractContent": [{"text": "With the goal of discovering differences between Belgian and Netherlandic Dutch, we participated as Team Taurus in the Dutch-Flemish Subtitles task of VarDial2018.", "labels": [], "entities": [{"text": "VarDial2018", "start_pos": 151, "end_pos": 162, "type": "DATASET", "confidence": 0.5501784086227417}]}, {"text": "We used a rather simple marker-based method, but with a wide range of features, including lexical, lexico-syntactic and syntactic ones, and achieved a second position in the ranking.", "labels": [], "entities": []}, {"text": "Inspection of highly distinguishing features did point towards differences between the two language varieties, but because of the nature of the experimental data, we have to treat our observations as very tentative and in need of further investigation.", "labels": [], "entities": []}], "introductionContent": [{"text": "The main area where the Dutch Language is spoken is in The Netherlands and the Northern part of Belgium (Flanders).", "labels": [], "entities": []}, {"text": "Although there are quite strong dialects in regions in both countries, the standard version of Dutch is shared.", "labels": [], "entities": []}, {"text": "In fact, there is a joint Dutch Language Union that promotes and supports standard Dutch.", "labels": [], "entities": []}, {"text": "Still, many native speakers have the feeling that there are subtle differences between the Northern and the Southern variety of standard Dutch, and that these differences are not limited to just pronunciation.", "labels": [], "entities": []}, {"text": "We would like to verify whether this intuition is correct, by investigating (qualitatively and quantitatively) the language use in corpus material that is meant to represent standard Dutch (and not, e.g., the language used on social media as that tends to contain high levels of dialect in various regions) and that is balanced in all factors apart from the language variety.", "labels": [], "entities": []}, {"text": "However, such a corpus is hard to come by.", "labels": [], "entities": []}, {"text": "The Spoken Dutch Corpus () contains both varieties, but the content is necessarily biased by location.", "labels": [], "entities": [{"text": "Spoken Dutch Corpus", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.6759369174639384}]}, {"text": "The same can be said about SoNaR (Stevin Dutch Reference Corpus;).", "labels": [], "entities": [{"text": "Stevin Dutch Reference Corpus", "start_pos": 34, "end_pos": 63, "type": "DATASET", "confidence": 0.832411140203476}]}, {"text": "We were therefore pleased to find that one of the tasks of VarDial 2018 (, namely Discriminating between Dutch and Flemish in Subtitles (DFS), appeared to provide exactly what we were looking for: a corpus of subtitles of international movies and tv shows, produced by the Dutch and Belgian branches of Broadcast Text International (BTI Studios) and decided to participate, as Team Taurus.", "labels": [], "entities": [{"text": "Discriminating between Dutch and Flemish in Subtitles (DFS)", "start_pos": 82, "end_pos": 141, "type": "TASK", "confidence": 0.6664001971483231}, {"text": "Broadcast Text International (BTI Studios)", "start_pos": 303, "end_pos": 345, "type": "DATASET", "confidence": 0.8040176289422172}]}, {"text": "As can be deduced from the introduction above, our main goal is not the highest possible recognition score; rather, we want to establish if the two varieties differ from each other and if so, in what respect.", "labels": [], "entities": []}, {"text": "Of course, a score higher than chance is required to show that indeed there are differences between the varieties, but mostly we are interested in which features are apparently used in distinguishing between Northern and Southern Dutch.", "labels": [], "entities": []}, {"text": "This means that we are limited in our choice of recognition methods.", "labels": [], "entities": []}, {"text": "For example Support Vector Machines, although very strong in recognition quality, are not suited for our purpose as the transformation of the feature space makes evaluation per feature impossible.", "labels": [], "entities": []}, {"text": "Instead, we chose a very simple marker-based method, which allows us to see directly how much each feature contributes.", "labels": [], "entities": []}, {"text": "As for recognition features, our main interest lies in syntax, even for text classification already more than two decades (.", "labels": [], "entities": [{"text": "text classification", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7547842264175415}]}, {"text": "Still, we chose as wide a range as we could extract in the time allotted to this project, ranging from character n-grams to syntactic rewrites.", "labels": [], "entities": []}, {"text": "In the sections below we will first describe some related work (Section 2) and then the experimental data and our preprocessing (Section 3).", "labels": [], "entities": []}, {"text": "Next we describe our features in more detail (Section 4).", "labels": [], "entities": []}, {"text": "Then we proceed with the recognition method and the recognition quality (Section 5).", "labels": [], "entities": []}, {"text": "Our investigation on variety-distinguishing features in this paper will be restricted to token unigrams (Section 6) and syntactic features (Section 7).", "labels": [], "entities": []}, {"text": "We conclude with a more general discussion of the results (Section 8).", "labels": [], "entities": []}], "datasetContent": [{"text": "The data for the DFS shared task of VarDial2018 () originate from the SUBTIEL Corpus (van der.", "labels": [], "entities": [{"text": "VarDial2018", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.5917801260948181}, {"text": "SUBTIEL Corpus", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.8746476471424103}]}, {"text": "They consist of Dutch subtitles for movies and tv shows, produced by the Dutch and the Belgian branch of the company Broadcast Text International (BTI Studios).", "labels": [], "entities": [{"text": "Broadcast Text International (BTI Studios)", "start_pos": 117, "end_pos": 159, "type": "DATASET", "confidence": 0.8430700046675546}]}, {"text": "For the shared task, the subtitles have been marked as either Belgian or Netherlandic Dutch depending on the market for which they were prepared.", "labels": [], "entities": []}, {"text": "This is likely, but not guaranteed, to correspond to subtitling in the corresponding branch by a native speaker of the corresponding variety.", "labels": [], "entities": []}, {"text": "Sequences of subtitles comprising a number of full subtitles and containing about 30-35 words were selected randomly from the whole data set to be used as task items.", "labels": [], "entities": []}, {"text": "150,000 training items and 250 development items for each variety were provided beforehand, and 20,000 test items a few days before the submission deadline.", "labels": [], "entities": []}, {"text": "As we did not use a tuning step in our training, we merged the development data with the training data.", "labels": [], "entities": []}, {"text": "The training and test data were both selected completely randomly from the full data set.", "labels": [], "entities": []}, {"text": "There was no overlap in items, but it was possible that there were test items belonging to the same movies or tv shows as training items.", "labels": [], "entities": []}, {"text": "As we will see below, this had a substantial influence on the nature of the task.", "labels": [], "entities": []}, {"text": "When inspecting the data, we found several artefacts of earlier preprocessing steps.", "labels": [], "entities": []}, {"text": "Most notably, all characters with diacritics were removed (e.g. \u00e9\u00e9n (\"a\") became n), or alternatively the diacritic was removed but also a space was inserted (e.g. ru\u00efne (\"ruin\") became ru ine).", "labels": [], "entities": []}, {"text": "Furthermore, periods in numbers had spaces inserted next to them (e.g. 20.000 (\"20,000\") became 20. 000).", "labels": [], "entities": []}, {"text": "Also apostrophes in words like z'n (zijn, \"his\") were removed; apparently, some correction had already been applied, but this also produced non-existing forms like zeen.", "labels": [], "entities": []}, {"text": "Now, these artefacts would not be a problem for character or token n-gram recognition.", "labels": [], "entities": [{"text": "character or token n-gram recognition", "start_pos": 48, "end_pos": 85, "type": "TASK", "confidence": 0.6919033885002136}]}, {"text": "However, we were planning to use POS tagging and syntactic parsing, for which these artefacts would most certainly lead to errors.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.6863985061645508}, {"text": "syntactic parsing", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7293352782726288}]}, {"text": "We therefore decided to include a preprocessing step in which we tried to correct most of these artefacts.", "labels": [], "entities": [{"text": "preprocessing", "start_pos": 34, "end_pos": 47, "type": "METRIC", "confidence": 0.9650530815124512}]}, {"text": "For the diacritics, it would have been easiest to compare to a Dutch word list in which diacritics were included, but we did not manage to acquire such a resource quickly enough.", "labels": [], "entities": []}, {"text": "Instead, we inspected derived word counts and the text itself manually, and build a list of about 270 regular expression substitutes, such as As we spent only limited time on this, we missed cases, even (in retrospect) obvious ones like financi ele (financi\u00eble, \"financial\"), leading to non-words like ele in the observations below.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: VarDial2018-DFS scores for various approaches", "labels": [], "entities": []}]}