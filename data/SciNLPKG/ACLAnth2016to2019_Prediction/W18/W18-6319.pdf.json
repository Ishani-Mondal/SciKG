{"title": [{"text": "A Call for Clarity in Reporting BLEU Scores", "labels": [], "entities": [{"text": "BLEU Scores", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.8922722935676575}]}], "abstractContent": [{"text": "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7687496244907379}]}, {"text": "Although people refer to \"the\" BLEU score, BLEU is in fact a param-eterized metric whose values can vary wildly with changes to these parameters.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9768289625644684}, {"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9973334074020386}]}, {"text": "These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9986962676048279}]}, {"text": "I quantify this variation, finding differences as high as 1.8 between commonly used configurations.", "labels": [], "entities": []}, {"text": "The main culprit is different tokeniza-tion and normalization schemes applied to the reference.", "labels": [], "entities": []}, {"text": "Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide anew tool, SACREBLEU, 1 to facilitate this.", "labels": [], "entities": [{"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.979131817817688}, {"text": "machine translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.8155632615089417}, {"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9975927472114563}, {"text": "annual Conference on Machine Translation (WMT)", "start_pos": 132, "end_pos": 178, "type": "TASK", "confidence": 0.7830341681838036}]}], "introductionContent": [{"text": "Science is the process of formulating hypotheses, making predictions, and measuring their outcomes.", "labels": [], "entities": []}, {"text": "In machine translation research, the predictions are made by models whose development is the focus of the research, and the measurement, more often than not, is done via BLEU ().", "labels": [], "entities": [{"text": "machine translation research", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.8555887937545776}, {"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9969046711921692}]}, {"text": "BLEU's relative language independence, its ease of computation, and its reasonable correlation with human judgments have led to its adoption as the dominant metric for machine translation research.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9766744375228882}, {"text": "machine translation research", "start_pos": 168, "end_pos": 196, "type": "TASK", "confidence": 0.8275021910667419}]}, {"text": "On the whole, it has been a boon to the community, providing a fast and cheap way for researchers to gauge the performance of their models.", "labels": [], "entities": []}, {"text": "Together with larger-scale controlled manual evaluations, BLEU has shep-herded the field through a decade and a half of quality improvements (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9897618889808655}]}, {"text": "This is of course not to claim there are no problems with BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9560490846633911}]}, {"text": "Its weaknesses abound, and much has been written about them (cf.;).", "labels": [], "entities": []}, {"text": "This paper is not, however, concerned with the shortcomings of BLEU as a proxy for human evaluation of quality; instead, our goal is to bring attention to the relatively narrower problem of the reporting of BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9807382225990295}, {"text": "BLEU", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.8910374641418457}]}, {"text": "This problem can be summarized as follows: \u2022 BLEU is not a single metric, but requires a number of parameters ( \u00a72.1).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9983392953872681}]}, {"text": "\u2022 Preprocessing schemes have a large effect on scores ( \u00a72.2).", "labels": [], "entities": [{"text": "scores", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9561694264411926}]}, {"text": "Importantly, BLEU scores computed against differently-processed references are not comparable.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9984904527664185}]}, {"text": "\u2022 Papers vary in the hidden parameters and schemes they use, yet often do not report them ( \u00a72.3).", "labels": [], "entities": []}, {"text": "Even when they do, it can be hard to discover the details.", "labels": [], "entities": []}, {"text": "Together, these issues make it difficult to evaluate and compare BLEU scores across papers, which impedes comparison and replication.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9864650964736938}]}, {"text": "I quantify these issues and show that they are serious, with variances bigger than many reported gains.", "labels": [], "entities": []}, {"text": "After introducing the notion of user-versus metricsupplied tokenization, I identify user-supplied reference tokenization as the main cause of this incompatibility.", "labels": [], "entities": []}, {"text": "In response, I suggest the community use only metric-supplied reference tokenization when sharing scores, 2 following the annual Conference on Machine Translation (.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.7781345248222351}]}, {"text": "In support of this, I release a Python package, SACREBLEU, 3 which automatically downloads and stores references for common test sets, thus introducing a \"protective layer\" between them and the user.", "labels": [], "entities": []}, {"text": "It also provides a number of other features, such as reporting aversion string which records the parameters used and which can be included in published papers.", "labels": [], "entities": []}, {"text": "2 Problem Description 2.1 Problem: BLEU is underspecified \"BLEU\" does not signify a single concrete method, but a constellation of parameterized methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9970200657844543}, {"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9946967363357544}]}, {"text": "Among these parameters are: \u2022 The number of references used; \u2022 for multi-reference settings, the computation of the length penalty; \u2022 the maximum n-gram length; and \u2022 smoothing applied to 0-count n-grams.", "labels": [], "entities": []}, {"text": "Many of these are not common problems in practice.", "labels": [], "entities": []}, {"text": "Most often, there is only one reference, and the length penalty calculation is therefore moot.", "labels": [], "entities": [{"text": "length penalty calculation", "start_pos": 49, "end_pos": 75, "type": "METRIC", "confidence": 0.9495029052098592}]}, {"text": "The maximum n-gram length is virtually always set to four, and since BLEU is corpus level, it is rare that there are any zero counts.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9938434362411499}]}, {"text": "But it is also true that people use BLEU scores as very rough guides to MT performance across test sets and languages (comparing, for example, translation performance into English from German and Chinese).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9981046915054321}, {"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.997018575668335}]}, {"text": "Apart from the wide intralanguage scores between test sets, the number of references included with a test set has a large effect that is often not given enough attention.", "labels": [], "entities": []}, {"text": "For example, WMT 2017 includes two references for English-Finnish.", "labels": [], "entities": [{"text": "WMT 2017", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.9533053636550903}]}, {"text": "Scoring the online-B system with one reference produces a BLEU score of 22.04, and with two, 25.25.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9817761480808258}]}, {"text": "As another example, the NIST OpenMT Arabic-English and ChineseEnglish test sets 4 provided four references and consequently yielded BLEU scores in the high 40s (and now, low 50s).", "labels": [], "entities": [{"text": "NIST OpenMT Arabic-English and ChineseEnglish test sets 4", "start_pos": 24, "end_pos": 81, "type": "DATASET", "confidence": 0.8983728513121605}, {"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9993952512741089}]}, {"text": "Since these numbers are all gathered together under the label \"BLEU\", overtime, they leave an impression in people's minds of very high BLEU scores for some language pairs or test sets relative to others, but without this critical distinguishing detail.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9970360994338989}, {"text": "BLEU scores", "start_pos": 136, "end_pos": 147, "type": "METRIC", "confidence": 0.9779500663280487}]}], "datasetContent": [{"text": "Other tricky details exist in the management of datasets.", "labels": [], "entities": []}, {"text": "It has been common over the past few years to report results on the English\u2192German arc of the WMT'14 dataset.", "labels": [], "entities": [{"text": "English\u2192German arc of the WMT'14 dataset", "start_pos": 68, "end_pos": 108, "type": "DATASET", "confidence": 0.6774647310376167}]}, {"text": "It is unfortunate, therefore, that for this track (and this track alone), there are actually two such datasets.", "labels": [], "entities": []}, {"text": "One of them, released for the evaluation, has only 2,737 sentences, having removed about 10% of the original data after problems were discovered during the evaluation.", "labels": [], "entities": []}, {"text": "The second, released after the evaluation, restores this missing data (after correcting the problem) and has 3,004 sentences.", "labels": [], "entities": []}, {"text": "Many researchers are unaware of this fact, and do not specify which version they use when reporting, which itself contributes to variance.", "labels": [], "entities": [{"text": "variance", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9731475710868835}]}, {"text": "depicts the ideal process for computing sharable scores.", "labels": [], "entities": []}, {"text": "Reference tokenization must identical in order for scores to be comparable.", "labels": [], "entities": []}, {"text": "The widespread use of user-supplied reference preprocessing prevents this, needlessly complicating comparisons.", "labels": [], "entities": []}, {"text": "The lack of details about preprocessing pipelines exacerbates this problem.", "labels": [], "entities": []}, {"text": "This situation should be fixed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU score variation across WMT'17 language arcs for cased (top) and uncased (bottom) BLEU. Each  column varies the processing of the \"online-B\" system output and its references. basic denotes basic user-supplied  tokenization, split adds compound splitting, unk replaces words not appearing at least twice in the training data  with UNK, and metric denotes the metric-supplied tokenization used by WMT. The range row lists the difference  between the smallest and largest scores, excluding unk.", "labels": [], "entities": [{"text": "BLEU score variation", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9434507091840109}, {"text": "WMT'17 language arcs", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.8567891319592794}, {"text": "UNK", "start_pos": 344, "end_pos": 347, "type": "DATASET", "confidence": 0.8404998779296875}, {"text": "WMT", "start_pos": 409, "end_pos": 412, "type": "DATASET", "confidence": 0.9566329121589661}]}]}