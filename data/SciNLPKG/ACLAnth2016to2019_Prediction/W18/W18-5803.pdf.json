{"title": [{"text": "Acoustic word disambiguation with phonological features in Danish ASR", "labels": [], "entities": [{"text": "Acoustic word disambiguation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6353971362113953}, {"text": "ASR", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.5182055234909058}]}], "abstractContent": [{"text": "Phonological features can indicate word class and we can use word class information to disambiguate both homophones and homo-graphs in automatic speech recognition (ASR).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 135, "end_pos": 169, "type": "TASK", "confidence": 0.8013210395971934}]}, {"text": "We show Danish st\u00f8d can be predicted from speech and used to improve ASR.", "labels": [], "entities": [{"text": "Danish st\u00f8d", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.567307248711586}, {"text": "ASR", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.969383180141449}]}, {"text": "We discover which acoustic features contain the signal of st\u00f8d, how to use these features to predict st\u00f8d and how we can make use of st\u00f8d and st\u00f8d-predictive acoustic features to improve overall ASR accuracy and decoding speed.", "labels": [], "entities": [{"text": "ASR", "start_pos": 195, "end_pos": 198, "type": "TASK", "confidence": 0.988248348236084}, {"text": "accuracy", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.9542695879936218}]}, {"text": "In the process, we discover acoustic features that are novel to the phonetic characterisation of st\u00f8d.", "labels": [], "entities": []}], "introductionContent": [{"text": "St\u00f8d ( in IPA notation) is usually described as (a kind of) creaky voice or as laryngealisation.", "labels": [], "entities": []}, {"text": "St\u00f8d can distinguish homophones and homographs and can identify word class by its presence.", "labels": [], "entities": []}, {"text": "Danish vi ? ser is a noun that translates to clock dial, but pronounced without st\u00f8d -viser -it can also be a verb that means to show.", "labels": [], "entities": []}, {"text": "The presence of st\u00f8d can change the meaning of an utterance, e.g. de kendte folk can mean the famous people if kendte is pronounced as and can also mean they knew people if kendte is pronounced as.", "labels": [], "entities": []}, {"text": "St\u00f8d is robust against some types of reduction and is an acoustic cue that can help distinguish one vs. none in colloquial Danish: [e ? n] and.", "labels": [], "entities": []}, {"text": "Phonological features can often be determined from grammar and morphology) but st\u00f8d may not occur in read-aloud or spontaneous speech when predicted by morphology and grammar, and st\u00f8d can be difficult to perceive in both visualisations of spectrograms and in speech.", "labels": [], "entities": []}, {"text": "St\u00f8d is not highly frequent in either readaloud or spontaneous speech, but st\u00f8d and similar phonological features like e.g. tones are interesting for two main reasons: 1.", "labels": [], "entities": []}, {"text": "Relatively small languages like Nordic languages do not have large speech corpora available like English, Chinese etc.", "labels": [], "entities": []}, {"text": "We should exploit all signals in the data to improve ASR performance for these languages.", "labels": [], "entities": [{"text": "ASR", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9883338212966919}]}, {"text": "2. The semantic disambiguation at both sentence and lexical level is appealing because ASR errors that disturb the meaning of an utterance are less acceptable for human consumers of ASR output).", "labels": [], "entities": [{"text": "ASR", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.9664592146873474}]}, {"text": "Our contributions are to: \u2022 Show that st\u00f8d annotation is reliable when annotated by trained phoneticians and can be the basis of statistical analyses.", "labels": [], "entities": []}, {"text": "\u2022 Discover novel audio features that are predictive of st\u00f8d in speech.", "labels": [], "entities": []}, {"text": "\u2022 Demonstrate we can predict st\u00f8d in speech as phone variant discrimination.", "labels": [], "entities": []}, {"text": "\u2022 Integrate st\u00f8d in ASR and improve WER on read-aloud and spontaneous speech.", "labels": [], "entities": [{"text": "ASR", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.8177069425582886}, {"text": "WER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9845390915870667}]}], "datasetContent": [{"text": "Speech genre Location Speakers Duration Types Tokens 2h 51 min 1075 21170 and evaluated on a test set from the same corpus and we demonstrate that the findings do not generalise to a multi-speaker setting.", "labels": [], "entities": [{"text": "Speech genre Location Speakers Duration Types Tokens", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7467578138623919}]}, {"text": "No single feature extracted from audio of spoken Danish can predict the presence of st\u00f8d like F0 estimation can predict pitch.", "labels": [], "entities": [{"text": "F0 estimation", "start_pos": 94, "end_pos": 107, "type": "METRIC", "confidence": 0.8899613618850708}]}, {"text": "Because st\u00f8d is related to irregular vibration of the vocal folds, previous research has focused on harmonics-to-noise (HNR) ratio, the difference between the first two harmonics in a spectrum (H1:H2) and diplophony (H1:H1 1 2 1 ) as well as F0 and intensity, but this is the first large scale quantitative study of st\u00f8d.", "labels": [], "entities": [{"text": "harmonics-to-noise (HNR) ratio", "start_pos": 100, "end_pos": 130, "type": "METRIC", "confidence": 0.7258886933326721}, {"text": "F0", "start_pos": 242, "end_pos": 244, "type": "METRIC", "confidence": 0.9983524084091187}, {"text": "intensity", "start_pos": 249, "end_pos": 258, "type": "METRIC", "confidence": 0.9749196171760559}]}, {"text": "St\u00f8d can be audibly heard yet not be visible in a spectrogram to an experienced researcher.", "labels": [], "entities": []}, {"text": "Consequently, the annotation of st\u00f8d is subject to annotator perception.", "labels": [], "entities": []}, {"text": "Annotators need a considerable amount of training to be able to annotate st\u00f8d and the high cost of annotation in terms of training and annotation time coupled with potential bias from annotator training or the specific annotator has been a barrier to quantitative studies of st\u00f8d.", "labels": [], "entities": []}, {"text": "We show that expert st\u00f8d is reliable in Section 4.", "labels": [], "entities": []}, {"text": "Like st\u00f8d in Danish, Tone 1 and Tone 2 in Norwegian and Swedish are the only difference between some homographs and homophones.", "labels": [], "entities": []}, {"text": "Swedish and Norwegian are pitch accent languages that use tones to distinguish lexical items that would otherwise be homophones and homographs, e.g. tanken 1 vs. tanken 2 (the tank vs. the thought -subscript indicates Tone 1 and Tone 2) ().", "labels": [], "entities": []}, {"text": "Some theories suggest that st\u00f8d originated from tones and the distribution of st\u00f8d and Tone 1 & 2 also show similarities (.", "labels": [], "entities": []}, {"text": "describe st\u00f8d as atonal pattern but this is refuted in a reply in.", "labels": [], "entities": []}, {"text": "In tonal languages like Mandarin Chinese, tones or tonal contours disambiguate monosyllabic words as in the famous example of ma which has five different meanings depending on the tonal contour.", "labels": [], "entities": []}, {"text": "ASR for tonal languages add suprasegmental information to ASR models either by extending the acoustic feature input (embedded) or rescoring word lattices (explicit)).", "labels": [], "entities": [{"text": "ASR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9524996280670166}]}, {"text": "Embedded modelling requires that tones are modelled in the lexicon either as tonal variations of the same phoneme () or as separate phonemes (.", "labels": [], "entities": []}, {"text": "St\u00f8d is related to irregular vibration of the vocal folds which occurs frequently in Danish with no connection to st\u00f8d and we do not explore explicit modelling.", "labels": [], "entities": []}, {"text": "The duration of the st\u00f8d-bearing (semi-)vowel or syllable has been considered important in previous literature.", "labels": [], "entities": [{"text": "duration", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9674767851829529}]}, {"text": "We do not consider duration in this paper for 2 reasons: 1) HMM-based ASR is the target application and implicitly model duration with self-loops in the HMM and 2) the investigations of duration where conducted in lab conditions with elicited speech in the Standard Copenhagen dialect.", "labels": [], "entities": [{"text": "duration", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9872801899909973}, {"text": "HMM-based ASR", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.4489242732524872}, {"text": "duration", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9681957960128784}, {"text": "duration", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.9866413474082947}, {"text": "Copenhagen dialect", "start_pos": 266, "end_pos": 284, "type": "DATASET", "confidence": 0.8794342875480652}]}, {"text": "We use several corpora that cover most Danish dialects, also dialects that typically do not use st\u00f8d.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: Section 3 presents the data used and Section 4 presents the study of st\u00f8d annotation.", "labels": [], "entities": [{"text": "st\u00f8d annotation", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.7409396767616272}]}, {"text": "In Section 5 we discover novel acoustic features that are predictive of st\u00f8d.", "labels": [], "entities": []}, {"text": "We test and evaluate how well acoustic features predict st\u00f8d in Section 6 and perform phone variant discrimination where we jointly predict phone and st\u00f8d.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 64, "end_pos": 73, "type": "DATASET", "confidence": 0.9023022055625916}, {"text": "phone variant discrimination", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.5878081023693085}]}, {"text": "In Section 7, we adapt an ASR recipe for Danish and train several ASR systems to determine the best way to use st\u00f8d to improve ASR.", "labels": [], "entities": [{"text": "ASR", "start_pos": 127, "end_pos": 130, "type": "TASK", "confidence": 0.9645986557006836}]}, {"text": "shows the corpus statistics for all corpora used in the rest of this paper.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Annotator competence scores for all items and  st\u00f8d-bearing items.", "labels": [], "entities": [{"text": "Annotator competence scores", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.787447432676951}]}, {"text": " Table 4: Acoustic features. Features marked with *  also include 1st order and 2nd order derivatives.", "labels": [], "entities": []}, {"text": " Table 5: 5-fold cross validation on the training data  across 40 phone variant pairs and mean classification  accuracy on JHP across 5 pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.933885931968689}, {"text": "JHP", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.7950991988182068}]}, {"text": " Table 6: The 6 conditions we test in this set of experi- ments.", "labels": [], "entities": []}, {"text": " Table 7: %WER performance on SPTEST. The best performance for each AM is in bold. Statistical significance  over the condition in the column to the left is denoted by * if p < 0.05,  \u2020 if p < 0.01 and  \u2021 if p < 0.001.", "labels": [], "entities": [{"text": "WER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.999101996421814}, {"text": "SPTEST", "start_pos": 30, "end_pos": 36, "type": "TASK", "confidence": 0.47894686460494995}, {"text": "Statistical significance", "start_pos": 83, "end_pos": 107, "type": "METRIC", "confidence": 0.9399314522743225}]}, {"text": " Table 8: 2x denote the number of phonetic transcrip- tions that can be mapped to two words, 3x denotes the  number of phonetic transcriptions that can be mapped  to three words, etc.", "labels": [], "entities": []}, {"text": " Table 10: WER performance using the same decoder parameters for each test set. WER under the RT F < 1  constraint are in parentheses if different. Statistical significance is compared to M. These numbers are not directly  comparable to Table 7.", "labels": [], "entities": [{"text": "WER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.5202608704566956}, {"text": "WER", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.995259702205658}, {"text": "RT F < 1  constraint", "start_pos": 94, "end_pos": 114, "type": "METRIC", "confidence": 0.9369741320610047}, {"text": "Statistical significance", "start_pos": 148, "end_pos": 172, "type": "METRIC", "confidence": 0.7789040505886078}]}]}