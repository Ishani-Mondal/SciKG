{"title": [{"text": "Improving Neural Language Models with Weight Norm Initialization and Regularization", "labels": [], "entities": [{"text": "Improving Neural Language Models", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8146639466285706}, {"text": "Norm Initialization", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7182611972093582}, {"text": "Regularization", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.8307723999023438}]}], "abstractContent": [{"text": "Embedding and projection matrices are commonly used in neural language models (NLM) as well as in other sequence processing networks that operate on large vocabularies.", "labels": [], "entities": []}, {"text": "We examine such matrices in fine-tuned language models and observe that a NLM learns word vectors whose norms are related to the word frequencies.", "labels": [], "entities": []}, {"text": "We show that by initializing the weight norms with scaled log word counts, together with other techniques, lower perplexi-ties can be obtained in early epochs of training.", "labels": [], "entities": []}, {"text": "We also introduce a weight norm regular-ization loss term, whose hyperparameters are tuned via a grid search.", "labels": [], "entities": []}, {"text": "With this method, we are able to significantly improve perplexities on two word-level language modeling tasks (without dynamic evaluation): from 54.44 to 53.16 on Penn Treebank (PTB) and from 61.45 to 60.13 on WikiText-2 (WT2).", "labels": [], "entities": [{"text": "word-level language modeling tasks", "start_pos": 75, "end_pos": 109, "type": "TASK", "confidence": 0.6529208794236183}, {"text": "Penn Treebank (PTB)", "start_pos": 163, "end_pos": 182, "type": "DATASET", "confidence": 0.9706697344779969}]}], "introductionContent": [{"text": "A language model (LM) measures how likely a certain sequence of words is fora given language.", "labels": [], "entities": []}, {"text": "It does so by calculating the probability of occurrence of that sequence, which can be learned from monolingual text data.", "labels": [], "entities": []}, {"text": "Many models in machine translation and automatic speech recognition benefit from the use of a LM (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8025135099887848}, {"text": "automatic speech recognition", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.6758260826269785}]}, {"text": "While count-based LMs provided the best results in the past, substantial improvements were achieved with the introduction of neural networks in the field of language modeling (.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.7393918931484222}]}, {"text": "Different types of architectures such as feedforward neural networks and recurrent neural networks ( have since been used for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.7334749549627304}]}, {"text": "Currently, variants of long short-term memory * Equal contribution.", "labels": [], "entities": []}, {"text": "Ordering determined by coin flipping.", "labels": [], "entities": [{"text": "Ordering", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9609451293945312}, {"text": "coin flipping", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.8612598478794098}]}, {"text": "(LSTM)) networks give the best results on popular language modeling tasks ().", "labels": [], "entities": []}, {"text": "In natural language processing, words are typically represented by high-dimensional one-hot vectors.", "labels": [], "entities": []}, {"text": "To reduce dimensionality and to be able to learn relationships between words, they are mapped into a lower-dimensional, continuous embedding space.", "labels": [], "entities": []}, {"text": "Mathematically, this is done by multiplying the one-hot vector with the embedding matrix.", "labels": [], "entities": []}, {"text": "Similarly, to receive a probability distribution over the vocabulary, a mapping from an embedding space is performed by a projection matrix followed by a softmax operation.", "labels": [], "entities": []}, {"text": "These two matrices can be tied together in order to reduce the number of parameters and improve the results of.", "labels": [], "entities": []}, {"text": "Since the row vectors in the embedding and projection matrices are effectively word vectors in a continuous space, we investigate such weight vectors in well-trained and fine-tuned NLMs.", "labels": [], "entities": []}, {"text": "We observe that the learned word vector generally has a greater norm fora frequent word than an infrequent word.", "labels": [], "entities": []}, {"text": "We then specifically examine the weight vector norm distribution and design initialization and normalization strategies to improve NLMs.", "labels": [], "entities": []}, {"text": "Our contribution is twofold: \u2022 We identify that word vectors learned by NLMs have a weight norm distribution that resembles logarithm of the word counts.", "labels": [], "entities": []}, {"text": "We then correspondingly develop a weight initialization strategy to aid NLM training.", "labels": [], "entities": [{"text": "NLM training", "start_pos": 72, "end_pos": 84, "type": "TASK", "confidence": 0.911247044801712}]}, {"text": "\u2022 We design a weight norm regularization loss term that increases the generalization ability of the model.", "labels": [], "entities": []}, {"text": "Applying this loss term, we achieve state-of-the-art results on Penn Treebank (PTB) and WikiText-2 (WT2) language modeling tasks.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 64, "end_pos": 83, "type": "DATASET", "confidence": 0.9637178778648376}, {"text": "WikiText-2 (WT2) language modeling tasks", "start_pos": 88, "end_pos": 128, "type": "TASK", "confidence": 0.5767227326120649}]}, {"text": "investigated different NLM architectures and regularization methods with the use of a black-box hyperparameter tuner.", "labels": [], "entities": [{"text": "regularization", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.9632905721664429}]}, {"text": "In particular, the LSTM architecture was compared to two more recent recurrent approaches, namely recurrent highway networks (Zilly et al., 2017) and neural architecture search.", "labels": [], "entities": []}, {"text": "They found that the standard LSTM architecture outperforms other models, if properly regularized.", "labels": [], "entities": []}, {"text": "used various regularization methods such as activation regularization) in a LSTM model.", "labels": [], "entities": []}, {"text": "They also introduced a variant of the averaged stochastic gradient method, where the averaging trigger is not tuned by the user but relies on a non-monotonic condition instead.", "labels": [], "entities": []}, {"text": "With these and further regularization and optimization methods, improved results on PTB and WT2 were achieved.", "labels": [], "entities": [{"text": "PTB", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.9488682150840759}, {"text": "WT2", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.8173109292984009}]}, {"text": "To further improve this network architecture, introduced the mixture of softmaxes (MoS) model, claiming that the calculation of the output probabilities with a single softmax layer is a bottleneck.", "labels": [], "entities": []}, {"text": "In their approach, several output probabilities are calculated and then combined via a weighted sum.", "labels": [], "entities": []}, {"text": "The LSTM-MoS architecture provides state-of-the-art results on PTB and WT2 at the time of writing and is used as the baseline model for comparisons in this work.", "labels": [], "entities": [{"text": "PTB", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.964047372341156}, {"text": "WT2", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.6741079688072205}]}, {"text": "Other works proposed to tie the embedding and projection matrices.", "labels": [], "entities": []}, {"text": "investigated the effects of weight tying, analyzed update rules after tying and showed that tied matrices evolve in a similar way as the projection matrix. were motivated by the fact that with a classification setup over the vocabulary, inter-word information is not utilized to its full potential.", "labels": [], "entities": [{"text": "weight tying", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.7007870525121689}]}, {"text": "They also provided theoretical justification on why it is appropriate to tie the above-mentioned matrices.", "labels": [], "entities": []}, {"text": "Besides using the word embedding matrix, there are other approaches to represent word sequences.", "labels": [], "entities": []}, {"text": "proposed anew embedding method called fixed-sized ordinally-forgetting encoding (FOFE), which allows them to encode variable-length sentences into fixed-length vectors almost uniquely.", "labels": [], "entities": []}, {"text": "Additionally, introduced a weight normalization reparametrization trick on weight matrices, which separates the norm and the angle of a vector.", "labels": [], "entities": []}, {"text": "This can speedup the convergence of stochastic gradient descent and also allows for explicit scaling of gradients in the amplitude and direction.", "labels": [], "entities": [{"text": "stochastic gradient descent", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.7220351099967957}]}, {"text": "They also discussed the connections between weight normalization and batch normalization.", "labels": [], "entities": [{"text": "weight normalization", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.673062264919281}]}, {"text": "On top of one-hot representations of words, used additional information to represent word sequences.", "labels": [], "entities": []}, {"text": "It is shown that the use of long-context bag-of-words as additional feature for language modeling can narrow the gap between feed-forward NLMs and recurrent NLMs.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.6825643181800842}]}], "datasetContent": [{"text": "The experiments are conducted on two popular language modeling datasets.", "labels": [], "entities": []}, {"text": "The number of tokens and size of vocabulary for each dataset are summarized in   The smaller one is the PTB corpus with preprocessing from, which has a comparatively small vocabulary size of 10k.", "labels": [], "entities": [{"text": "PTB corpus", "start_pos": 104, "end_pos": 114, "type": "DATASET", "confidence": 0.964564174413681}]}, {"text": "With a smaller number of sentences, this dataset is a good choice for performing optimization of hyperparameters.", "labels": [], "entities": []}, {"text": "The second corpus WT2, which was introduced by, has over three times the vocabulary size of PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.9544644951820374}]}, {"text": "We use the network structure introduced by with the same hyper-parameter values to ensure comparability.", "labels": [], "entities": []}, {"text": "Several regularization techniques are used in this setup, such as dropout and weight decay.", "labels": [], "entities": []}, {"text": "Furthermore, the embedding and projection matrices are tied by default.", "labels": [], "entities": []}, {"text": "For optimization, we adopt the same strategy as described in.", "labels": [], "entities": []}, {"text": "That is, a conservative non-monotonic criterion is used to switch from stochastic gradient descent (SGD) to averaged stochastic gradient descent (ASGD).", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD)", "start_pos": 71, "end_pos": 104, "type": "TASK", "confidence": 0.7353038390477499}]}, {"text": "For more details of the network structure refer to ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Perplexity (ppl) improvement using weight norm initialization (wni) in early epochs on Penn  Treebank and WikiText-2. ppl reduction is around 10% after the first epoch on both tasks, and decays  to approximately 1% after 40 epochs. The wni model has slightly higher perplexities than the baseline  model from around 50 epochs onward.", "labels": [], "entities": [{"text": "Penn  Treebank", "start_pos": 97, "end_pos": 111, "type": "DATASET", "confidence": 0.9961663782596588}]}, {"text": " Table 3: Single model perplexity on the Penn Treebank test and validation sets. Baseline results are  obtained from (Yang et al., 2018).  \u2020 indicates the use of dynamic evaluation.", "labels": [], "entities": [{"text": "Penn Treebank test", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.9938067396481832}, {"text": "Baseline", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9477859735488892}]}, {"text": " Table 4: Single model perplexity on the WikiText-2 test and validation sets. Baseline results are obtained  from (", "labels": [], "entities": [{"text": "WikiText-2 test and validation sets", "start_pos": 41, "end_pos": 76, "type": "DATASET", "confidence": 0.8600932121276855}]}]}