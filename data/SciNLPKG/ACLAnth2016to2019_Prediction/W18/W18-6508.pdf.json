{"title": [], "abstractContent": [{"text": "This paper presents SimpleNLG-NL, an adaptation of the SimpleNLG surface re-alisation engine for the Dutch language.", "labels": [], "entities": []}, {"text": "It describes a novel method for determining and testing the grammatical constructions to be implemented, using target sentences sampled from a treebank.", "labels": [], "entities": []}], "introductionContent": [{"text": "SimpleNLG is a Java-based surface realisation library aimed at practical applications (.", "labels": [], "entities": [{"text": "SimpleNLG", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8910945057868958}]}, {"text": "It is meant to be simple to use, and in an architecture redesign in version 4 of the software it was also made easy for developers to alter its code.", "labels": [], "entities": []}, {"text": "Over the years, SimpleNLG has been adapted for several languages other than English, such as German (, Brazilian-Portuguese) and Italian (.", "labels": [], "entities": []}, {"text": "In this paper we present anew version of SimpleNLG for surface realisation in Dutch, called SimpleNLG-NL.", "labels": [], "entities": [{"text": "surface realisation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7023376524448395}]}, {"text": "Like most adaptations for other languages, it is based on SimpleNLG-EnFr (Vaudry and Lapalme, 2013): a bilingual version of SimpleNLG that supports both English and French, based on SimpleNLG version 4.2.", "labels": [], "entities": [{"text": "SimpleNLG-EnFr (Vaudry and Lapalme, 2013)", "start_pos": 58, "end_pos": 99, "type": "DATASET", "confidence": 0.8724914789199829}]}, {"text": "The architecture of SimpleNLG-EnFr was split into language independent and language dependent parts, making it relatively easy to add new languages.", "labels": [], "entities": []}, {"text": "As Dutch is closely related to German, Simple-NLG for German) might seem a more obvious starting point for SimpleNLG-NL.", "labels": [], "entities": []}, {"text": "However, SimpleNLG for German is based on the differently structured version 3 of SimpleNLG, which made it unsuitable to build on.", "labels": [], "entities": [{"text": "SimpleNLG", "start_pos": 82, "end_pos": 91, "type": "DATASET", "confidence": 0.9342333078384399}]}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the method we used for developing SimpleNLG-NL, followed in Section 3 by an overview of the main characteristics of Dutch and how we implemented them.", "labels": [], "entities": [{"text": "SimpleNLG-NL", "start_pos": 59, "end_pos": 71, "type": "DATASET", "confidence": 0.8274410367012024}]}, {"text": "In Section 4 we present the current coverage of SimpleNLG-NL over a set of test sentences.", "labels": [], "entities": [{"text": "SimpleNLG-NL", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.8550974726676941}]}, {"text": "We end with conclusions and directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To determine the coverage of SimpleNLG-NL, each sentence generated in one of the four rounds described in Section 2 was judged on correctness.", "labels": [], "entities": [{"text": "SimpleNLG-NL", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.7893187403678894}]}, {"text": "Since the number of sentences to be evaluated was small, using automated evaluation metrics such as BLEU () would not have made much sense; moreover, these would not take into account that word order in Dutch is relatively free.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9975857734680176}]}, {"text": "Therefore we chose to manually evaluate the sentences.", "labels": [], "entities": []}, {"text": "We considered a sentence generated by SimpleNLG-NL to be generated \"correctly\" if the output met at least one of the following criteria: \u2022 The output matched the target sentence exactly, including punctuation; or \u2022 The output only differed from the target in terms of punctuation (commas and quotation marks), with no change in meaning; or \u2022 The output differed from the target in terms of word order, but without making the sentence unwellformed or causing a change in meaning.", "labels": [], "entities": []}, {"text": "The criteria are ordered by inclusiveness, with the first being the preferred outcome (\"exact match\").", "labels": [], "entities": [{"text": "exact match", "start_pos": 88, "end_pos": 99, "type": "METRIC", "confidence": 0.9423590302467346}]}, {"text": "The final coverage by SimpleNLG-NL of the test sentences according to these criteria, after all four rounds of generate-evaluate-revise, is shown in.", "labels": [], "entities": [{"text": "SimpleNLG-NL", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.7643519043922424}]}, {"text": "Results round 1: Out of 12 sentences, 11 were generated correctly (91.7%).", "labels": [], "entities": []}, {"text": "The result counting only exact matches is 8 out of 12 (66.7%).", "labels": [], "entities": []}, {"text": "Of the three accepted mismatches, one missed some nonmandatory commas, and two had acceptable differences in word order from their target sentences.", "labels": [], "entities": []}, {"text": "(One lacked topicalisation, which is currently unsupported, and the other placed the past participle at the end of the sentence, a merely stylistic difference.)", "labels": [], "entities": []}, {"text": "SimpleNLG-NL could not reproduce the longest sentence from Round 1 (26 words).", "labels": [], "entities": [{"text": "SimpleNLG-NL", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8615063428878784}]}, {"text": "This was due to several problems.", "labels": [], "entities": []}, {"text": "First, SimpleNLG cannot handle clauses without verbs, in this case an enumeration (\"tasks such as X, Y and Z\").", "labels": [], "entities": []}, {"text": "Second, the sentence contained a verb cluster as well as an attributively used infinitive, neither of which SimpleNLG-NL could handle.", "labels": [], "entities": [{"text": "SimpleNLG-NL", "start_pos": 108, "end_pos": 120, "type": "DATASET", "confidence": 0.8798828125}]}, {"text": "Results round 2: In Round 2, all 37 short test sentences were generated correctly, as exact matches (100%).", "labels": [], "entities": []}, {"text": "Results round 3: Of the 11 medium-length sentences, 9 were generated as exact matches (81.8%) and the same number were accepted as correct.", "labels": [], "entities": []}, {"text": "The two incorrectly generated sentences both had problems with modifier ordering.", "labels": [], "entities": []}, {"text": "Of the 10 long sentences, 7 were generated correctly (70.0%).", "labels": [], "entities": []}, {"text": "This includes two sentences that did not match exactly.", "labels": [], "entities": []}, {"text": "One accepted mismatch added an unnecessary (but acceptable) comma, the other positioned the preverb of an SCV at the end of the sentence.", "labels": [], "entities": []}, {"text": "While that position is acceptable, it can be stylistically preferable to reduce the distance between the main verb and the preverb.", "labels": [], "entities": []}, {"text": "However, SimpleNLG-NL does not yet support such a stylistic mechanism.", "labels": [], "entities": [{"text": "SimpleNLG-NL", "start_pos": 9, "end_pos": 21, "type": "DATASET", "confidence": 0.8939710855484009}]}, {"text": "The problems with the three incorrectly generated sentences involved incorrect ordering of modifiers and a verb cluster (te gaan wonen, lit.", "labels": [], "entities": []}, {"text": "\"to go live\"), and lack of support for main clauses connected by a semi-colon.", "labels": [], "entities": []}, {"text": "Results round 4: Of the 16 varieties of the same sentence, 10 were generated as exact matches.", "labels": [], "entities": []}, {"text": "There were no mismatches accepted as correct.", "labels": [], "entities": []}, {"text": "The incorrect sentences all had an incorrect word order.", "labels": [], "entities": []}, {"text": "Active sentences in the future perfect and the conditional tenses incorrectly positioned the auxiliary verb before the object.", "labels": [], "entities": []}, {"text": "In passive sentences in the perfect form, the order of the verb and the two or three auxiliary verbs was incorrect (e.g., zal zijn geweest gegooid should be zal gegooid zijn geweest \"will have been thrown\").", "labels": [], "entities": []}, {"text": "Overall results: In total, 74 out of 86 test sentences (86.0%) were generated correctly.", "labels": [], "entities": []}, {"text": "Of these, 69 (80.2%) are exact matches.", "labels": [], "entities": [{"text": "exact", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9623706936836243}]}, {"text": "If we only look at the 33 treebank sentences from Rounds 1 and 3, then 28 (84.8%) were generated correctly, with 22 (66,7%) exact matches.", "labels": [], "entities": []}, {"text": "The open part of the lexicon gained 59 entries during the development rounds.", "labels": [], "entities": []}, {"text": "Combined with the closed part, the final lexicon contained 165 entries.", "labels": [], "entities": []}, {"text": "This lexicon was later replaced by a more extensive one, containing over 8000 entries (see Section 3.5).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The final coverage of SimpleNLG-NL after development and testing. Generated sentences were  \"accepted as correct\" if they met the criteria described in Section 4.", "labels": [], "entities": [{"text": "SimpleNLG-NL", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.8223155736923218}]}]}