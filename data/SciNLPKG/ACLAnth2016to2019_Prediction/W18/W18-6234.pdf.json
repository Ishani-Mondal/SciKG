{"title": [{"text": "EmotiKLUE at IEST 2018: Topic-Informed Classification of Implicit Emotions", "labels": [], "entities": [{"text": "Topic-Informed Classification of Implicit Emotions", "start_pos": 24, "end_pos": 74, "type": "TASK", "confidence": 0.8287141561508179}]}], "abstractContent": [{"text": "EmotiKLUE is a submission to the Implicit Emotion Shared Task.", "labels": [], "entities": [{"text": "Implicit Emotion Shared Task", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.7304834872484207}]}, {"text": "It is a deep learning system that combines independent representations of the left and right contexts of the emotion word with the topic distribution of an LDA topic model.", "labels": [], "entities": []}, {"text": "EmotiKLUE achieves a macro average F 1 score of 67.13%, significantly outperforming the baseline produced by a simple ML classifier.", "labels": [], "entities": [{"text": "EmotiKLUE", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8661696314811707}, {"text": "macro average F 1 score", "start_pos": 21, "end_pos": 44, "type": "METRIC", "confidence": 0.7673754394054413}]}, {"text": "Further enhancements after the evaluation period lead to an improved F 1 score of 68.10%.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9896758993466696}]}], "introductionContent": [{"text": "The aim of the Implicit Emotion Shared Task (IEST;) is to infer emotion from the context of emotion words.", "labels": [], "entities": [{"text": "Implicit Emotion Shared Task (IEST", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.6829086740811666}]}, {"text": "The working definition of emotion for the shared task implies that emotion is triggered by the interpretation of a stimulus event (, i. e. the cause of the emotion.", "labels": [], "entities": []}, {"text": "Consequently, the data for the shared task have been compiled with the aim of including a description of the cause of the emotion.", "labels": [], "entities": []}, {"text": "This has been accomplished by using distant supervision: The organizers collected tweets that contain exactly one of 21 emotion words belonging to six emotions (anger, fear, disgust, joy, sadness, surprise), where the emotion word has to be followed by that, because or when as likely indicators fora description of the cause of the emotion.", "labels": [], "entities": []}, {"text": "The corpus collected this way comprises more than 190.000 tweets and is split into three data sets: 80% training, 5% trial and 15% test.", "labels": [], "entities": []}, {"text": "The emotion words in the tweets are masked and participants of the shared task have to predict the emotion of the masked emotion word from its context.", "labels": [], "entities": []}, {"text": "EmotiKLUE, our submission to the shared task, is a deep learning system that learns independent representations of the left and right contexts of the emotion word, similar to, who use n-gram representations for both the right and the left context around triggerwords in aspectbased opinion mining.", "labels": [], "entities": [{"text": "aspectbased opinion mining", "start_pos": 270, "end_pos": 296, "type": "TASK", "confidence": 0.6056413253148397}]}, {"text": "Our intuition is that the distribution of the emotions is dependent on the topics of the tweets, therefore we train a Twitterspecific LDA topic model and explore different ways of combining the topic distributions with the left and right contexts in order to predict the emotions.", "labels": [], "entities": []}, {"text": "EmotiKLUE is available on GitHub.", "labels": [], "entities": [{"text": "EmotiKLUE", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9567751288414001}]}], "datasetContent": [{"text": "We have three different network architectures that differ in the way they use LDA topic distributions.", "labels": [], "entities": []}, {"text": "We have four sets of embeddings that differ in size and training objective.", "labels": [], "entities": [{"text": "training objective", "start_pos": 56, "end_pos": 74, "type": "METRIC", "confidence": 0.9471921026706696}]}, {"text": "And we have three options for the training data (only the official training data, only our additional data, or training on the latter and retraining on the former).", "labels": [], "entities": []}, {"text": "In order to quantify the impact of the individual choices, we train and evaluate all 36 possible models.", "labels": [], "entities": []}, {"text": "Results for models using skip-gram-based embeddings are shown in and results for models using CBOWbased embeddings in.", "labels": [], "entities": []}, {"text": "The evaluation metric used is the macro average of the F 1 scores of the six classes.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9751101533571879}]}, {"text": "The exact numbers listed in should not betaken too seriously as they are subject to some small amout of random variation due  to differences in the initialization of the weights and the shuffling of the training data.", "labels": [], "entities": []}, {"text": "However, since all the individual options have been used at least nine times, we can still make some fairly reliable claims about their usefulness.", "labels": [], "entities": []}, {"text": "The most obvious observation is that the official training data lead to much better results than our additional data (+12.97 on average).", "labels": [], "entities": []}, {"text": "This is probably due to two reasons: We only use a subset of the emotion words that have been used in the official data sets and, more importantly, we use all instances of the emotion words and not only those that are followed by something that is likely to be a description of the cause of the emotion.", "labels": [], "entities": [{"text": "official data sets", "start_pos": 106, "end_pos": 124, "type": "DATASET", "confidence": 0.813116192817688}]}, {"text": "However, first training the model on the additional data and then retraining it on the official training data is benefitial (+1.96).", "labels": [], "entities": []}, {"text": "We can also see that word embeddings based on the skip-gram approach consistently outperform those based on the CBOW approach (+2.55).", "labels": [], "entities": []}, {"text": "300-dimensional embeddings are notably better than 100-dimensional embeddings, an effect that is more pronounced for the skip-grambased embeddings (+1.57) than for the CBOWbased ones (+0.80).", "labels": [], "entities": []}, {"text": "The 95%-confidence interval for the performance of the add+train-skip300-ldafeat model on the test data is 67.12 \u00b1 0.34, for example (estimated from 20 instances of the model).", "labels": [], "entities": []}, {"text": "The LDA topic distributions only have a positive effect when used as additional features alongside the LSTM output -and even then the effect is small and only positive for models using skipgram-based embeddings (+0.08) and negative for models using CBOW-based embeddings (\u22120.24).", "labels": [], "entities": []}, {"text": "Using the LDA topic distribution as a filter usually has a negative effect (\u22120.76).", "labels": [], "entities": [{"text": "LDA topic distribution", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.7324775656064352}]}, {"text": "Consequently, for our submission to the shared task, we chose the second network architecture (LDA topic distribution as feature), used 300-dimensional skip-gram embeddings and trained the model first on our additional data and retrained it on the official training and trial data.", "labels": [], "entities": []}, {"text": "That model achieved a macro average F 1 score of 67.13 on the test data and took the tenth place in the shared task.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9148818651835123}]}, {"text": "For comparison, report that human performance on this task is approximately 45%, the MaxEnt uni-and bigram classifier used as a baseline system achieved 59.88% and the best submission ( 71.45%.", "labels": [], "entities": []}, {"text": "The analysis in the previous section has shown that our system performs better on the more frequent words that we used for compiling our additional data than on the less frequent words.", "labels": [], "entities": []}, {"text": "Therefore, we recompile our additional data as described in Section 3.3 but for all of the 21 emotion words that occur in the official data.", "labels": [], "entities": []}, {"text": "After balancing the data, this results in approximately 163.000 items per class.", "labels": [], "entities": []}, {"text": "We take the model versions from Section 4.1 that are the basis for our submission and replace the additional data with the updated version.", "labels": [], "entities": []}, {"text": "The new models (prefixed with \"add2\" in) improve on the old ones both when using only the additional data (+1.66) and when retraining on the official training data (.", "labels": [], "entities": []}, {"text": "It is also worth pointing out that so far we have not fine-tuned the hyperparameters of our model.", "labels": [], "entities": []}, {"text": "As a first step in that direction, we try to use more units in the hidden layers and increase the size of all hidden layers to 300 units (models prefixed with \"300-add\" in).", "labels": [], "entities": []}, {"text": "This boosts the performance both when using only the additional data (+2.03) and when retraining on the official training data (+0.85).", "labels": [], "entities": []}, {"text": "Combining the recompiled additional data and the larger hidden layers yields further improvements (models prefixed with \"300-add2\" in).", "labels": [], "entities": []}, {"text": "The retrained model is approximately 1 point better than our submission and would have taken the eighth place in the shared task.", "labels": [], "entities": []}, {"text": "A further error analysis shows that the additional training data indeed yield the desired effect: Recall for category angry improves from 61% to 66%, largely due to better recall in the case of the triggerword furious (rising from 57% to 65%).", "labels": [], "entities": [{"text": "Recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9985135197639465}, {"text": "recall", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.9990692734718323}]}, {"text": "Further improvements can be found in almost all categories, namely for fear (69% to 72%, especially frightened: 49% to 53%), joy (78% to 79%, with recall for joyful rising from 61% to 65% and for cheerful from 64% to 70%), and sad (59% to 62%, triggerword depressed up two points from 46% to 48%).", "labels": [], "entities": [{"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.999352753162384}]}, {"text": "However, the additional training data had an adverse effect on category surprise; here recall falls from 68% to 65%, with almost all triggerwords dropping a couple of points, the worst being surprised, falling from 74% to 69%.", "labels": [], "entities": [{"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.999321699142456}]}, {"text": "Finally, we want to take a closer look at the contribution of the LDA topic distribution.", "labels": [], "entities": [{"text": "LDA topic distribution", "start_pos": 66, "end_pos": 88, "type": "DATASET", "confidence": 0.6768275499343872}]}, {"text": "To this end, we have trained 20 instances of the 300-add2+train-skip300-ldafeat and 300-add2+train-skip300-nolda models and have calculated the means and 95%-confidence intervals.", "labels": [], "entities": [{"text": "means", "start_pos": 144, "end_pos": 149, "type": "METRIC", "confidence": 0.9823920130729675}]}, {"text": "As it turns out, both model variants perform identically on the trial data.", "labels": [], "entities": []}, {"text": "On the test data, there are some minor differences but the performance means lie within one standard deviation of each other.", "labels": [], "entities": []}, {"text": "This means that our choice of concatenating the LDA topic distribution of the tweet to the LSTM does not have a statistically significant result..", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for models using skip-gram-based em- beddings (macro F 1 )", "labels": [], "entities": []}, {"text": " Table 2: Results for models using CBOW-based word  embeddings (macro F 1 )", "labels": [], "entities": []}, {"text": " Table 3: Confusion Matrix for the six predicted emotion categories (columns) for each real emotion and each  triggerword (rows) in the test data", "labels": [], "entities": []}, {"text": " Table 4: Results for the post-analysis experiments  (macro F 1 )", "labels": [], "entities": []}]}