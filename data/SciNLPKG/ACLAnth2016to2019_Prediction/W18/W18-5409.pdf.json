{"title": [{"text": "Linguistic representations in multi-task neural networks for ellipsis resolution", "labels": [], "entities": []}], "abstractContent": [{"text": "Sluicing resolution is the task of identifying the antecedent to a question ellipsis.", "labels": [], "entities": [{"text": "Sluicing resolution", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9509691596031189}]}, {"text": "Antecedents are often sentential constituents, and previous work has therefore relied on syntactic parsing, together with complex linguistic features.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7756073176860809}]}, {"text": "A recent model instead used partial parsing as an auxiliary task in sequential neu-ral network architectures to inject syntactic information.", "labels": [], "entities": []}, {"text": "We explore the linguistic information being brought to bear by such networks, both by defining subsets of the data exhibiting relevant linguistic characteristics, and by examining the internal representations of the network.", "labels": [], "entities": []}, {"text": "Both perspectives provide evidence for substantial linguistic knowledge being deployed by the neural networks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sluices are questions where material beyond the wh-word is missing and must be retrieved from context.", "labels": [], "entities": [{"text": "Sluices", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9418644905090332}]}, {"text": "Consider the following example from: (1) If [this is not practical], explain why.", "labels": [], "entities": []}, {"text": "Here, the antecedent is the complete sentential constituent, this is not practical.", "labels": [], "entities": []}, {"text": "Anand and Hardt (2016) present a sluice resolution system, in which candidate antecedents are required to be sentential constituents.", "labels": [], "entities": [{"text": "sluice resolution", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.8088853061199188}]}, {"text": "Furthermore, each candidate is represented by features manually defined over syntactic dependency structures.", "labels": [], "entities": []}, {"text": "Anand and Hardt report an accuracy of antecedent selection of 0.72, and a token-level F1 score 0.72, applied to a dataset based on news content.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9996203184127808}, {"text": "F1 score 0.72", "start_pos": 86, "end_pos": 99, "type": "METRIC", "confidence": 0.9538666208585104}]}, {"text": "show that neural network architectures with multi-task learning are able to achieve comparable results to Anand and Hardt, without relying on structured syntactic annotation or handcrafted features.", "labels": [], "entities": []}, {"text": "On a slightly different version of the news dataset, R\u00f8nning et al. report a tokenlevel F1 score of 0.70, compared to 0.67 for Anand and Hardt's system.", "labels": [], "entities": [{"text": "news dataset", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.7539245784282684}, {"text": "F1 score", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.8739833831787109}]}, {"text": "Furthermore, it is far superior to Anand and Hardt's system at adapting from the newswire to a dialogue dataset.", "labels": [], "entities": []}, {"text": "This is quite surprising as sluicing is traditionally understood to be constituent-based.", "labels": [], "entities": []}, {"text": "Two explanations present themselves; first, the traditional view might simply be wrong -that is, linguistic structure is not actually needed for ellipsis resolution.", "labels": [], "entities": [{"text": "ellipsis resolution", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7496995627880096}]}, {"text": "The second, and perhaps more reasonable, explanation is that R\u00f8nning et al.'s multi-task neural network architectures have learned to extract and incorporate the relevant linguistic representations.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the linguistic knowledge learned implicitly in the experiments in.", "labels": [], "entities": []}, {"text": "We take two approaches to this: 1.", "labels": [], "entities": []}, {"text": "We select linguistically-defined subsets of the data, and examine the output of different systems on these subsets; and 2.", "labels": [], "entities": []}, {"text": "we examine activations of the networks, focusing in particular on the activations associated with the wh-word that identifies a sluice, to assess how well the network notices, remembers, and classifies them.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Token-F1 Score on complete test set.", "labels": [], "entities": [{"text": "Token-F1 Score", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.6036607772111893}]}, {"text": " Table 2: F1 score for Adjacent vs. Non-adjacent  Sluice Antecedents", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9760677814483643}, {"text": "Adjacent vs. Non-adjacent  Sluice Antecedents", "start_pos": 23, "end_pos": 68, "type": "TASK", "confidence": 0.5790578305721283}]}, {"text": " Table 3: F1 score for punctuation as boundary to- ken for antecedent", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9760706722736359}]}, {"text": " Table 4: Degree of token continuity", "labels": [], "entities": []}, {"text": " Table 5: Accuracy of classifying adjacent an- tecedents from non-adjacent antecedents.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9834980368614197}, {"text": "classifying adjacent an- tecedents", "start_pos": 22, "end_pos": 56, "type": "TASK", "confidence": 0.781474232673645}]}, {"text": " Table 8: Euclidean distance between the antecedent left boundary activation and avg. sluice word vector  representation. Distances compared to average Euclidean norm distance between word representations  and activations separated by the same number of tokens as the antecedent sluice pair.", "labels": [], "entities": []}]}