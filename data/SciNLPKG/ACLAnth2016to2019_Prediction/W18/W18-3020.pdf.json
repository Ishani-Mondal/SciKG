{"title": [{"text": "Learning Hierarchical Structures On-The-Fly with a Recurrent-Recursive Model for Sequences", "labels": [], "entities": [{"text": "Sequences", "start_pos": 81, "end_pos": 90, "type": "TASK", "confidence": 0.7009368538856506}]}], "abstractContent": [{"text": "We propose a hierarchical model for sequential data that learns a tree on-the-fly, i.e. while reading the sequence.", "labels": [], "entities": []}, {"text": "In the model, a recurrent network adapts its structure and reuses recurrent weights in a recursive manner.", "labels": [], "entities": []}, {"text": "This creates adaptive skip-connections that ease the learning of long-term dependencies.", "labels": [], "entities": []}, {"text": "The tree structure can either be inferred without supervision through reinforcement learning, or learned in a supervised manner.", "labels": [], "entities": []}, {"text": "We provide preliminary experiments in a novel Math Expression Evaluation (MEE) task, which is explicitly crafted to have a hierarchical tree structure that can be used to study the effectiveness of our model.", "labels": [], "entities": [{"text": "Math Expression Evaluation (MEE) task", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.8000521489552089}]}, {"text": "Additionally , we test our model in a well-known propositional logic and language modelling tasks.", "labels": [], "entities": []}, {"text": "Experimental results show the potential of our approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many kinds of sequential data such as language or math expressions naturally come with a hierarchical structure.", "labels": [], "entities": []}, {"text": "Sometimes the structure is hidden deep in the semantics of the sequence, like the syntax tree in natural language; Other times the structure is more explicit, as in math expressions, where the tree is determined by the parentheses.", "labels": [], "entities": []}, {"text": "Recurrent neural networks (RNNs) have shown tremendous success in modeling sequential data, such as natural language.", "labels": [], "entities": []}, {"text": "However, RNNs process the observed data as a linear sequence of observations: * Equal contribution (Ordering determined by coin flip).", "labels": [], "entities": [{"text": "Equal contribution", "start_pos": 80, "end_pos": 98, "type": "METRIC", "confidence": 0.9653893113136292}]}, {"text": "Corresponding authors: zhouhan.lin@umontreal.ca, apjacob@edu.uwaterloo.ca \u2020 Work done while at Microsoft Research, Montreal.", "labels": [], "entities": []}, {"text": "the length of the computational path between any two words is a function of their position in the observed sequence, instead of their semantic or syntactic roles, leading to the appearance of difficultto-learn long-term dependencies and stimulating research on strategies to deal with that ().", "labels": [], "entities": []}, {"text": "Hierarchical, tree-like structures may alleviate this problem by creating shortcuts between distant inputs and by simulating compositionality of the sequence, compressing the sequence into higher-level abstractions.", "labels": [], "entities": []}, {"text": "Models that use tree as prior knowledge (e.g. () have shown improved performances over sequential models, validating the value of tree structure.", "labels": [], "entities": []}, {"text": "For example,) learns a bottom-up encoder, but requires the model to have access to the entire sentence as well as its parse tree before encoding it, which limits its application in some cases, e.g. language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 198, "end_pos": 215, "type": "TASK", "confidence": 0.7348186373710632}]}, {"text": "There has been various efforts to learn the tree structure as a supervised training target (, which free the model from relying on an external parser.", "labels": [], "entities": []}, {"text": "More recent efforts learn the best tree structure without supervision, by minimizing the log likelihood of the observed corpus, or by optimizing over a downstream task).", "labels": [], "entities": []}, {"text": "These models usually take advantage of a binary tree assumption on the inferred tree, which imposes restrictions on the flexibility of inferred tree structure, for example, Gumbel TreeLSTM (.", "labels": [], "entities": [{"text": "Gumbel TreeLSTM", "start_pos": 173, "end_pos": 188, "type": "DATASET", "confidence": 0.8639220297336578}]}, {"text": "We propose a model that reads sequences using a hierarchical, tree-structured process (: it creates a tree on-the-fly, in a top-down fashion.", "labels": [], "entities": []}, {"text": "Our model sits in between fully recursive models that have access to the whole sequence, such as is a sample model structure resulted from a sequence of decisions.", "labels": [], "entities": []}, {"text": "\"R\", \"S\" and \"M\" stand for recurrent cell, split cell, and merge cell, respectively.", "labels": [], "entities": []}, {"text": "Note that the \"S\" and \"M\" node can take inputs in datasets where splitting and merging signals are part of the sequence.", "labels": [], "entities": []}, {"text": "For example, in math data the splitting signal are related to the brackets.", "labels": [], "entities": []}, {"text": "(e) is the tree inferred from (d).", "labels": [], "entities": []}, {"text": "TreeLSTMs (, and vanilla recurrent models that \"flatten\" input sequence, such as LSTMs (.", "labels": [], "entities": []}, {"text": "At each time-step in the sequence, the model chooses either to create anew sub-tree (split), to return and merge information into the parent node (merge), or to predict the next word in the sequence (recur).", "labels": [], "entities": []}, {"text": "On split, anew sub-tree is created which takes control on which operation to perform.", "labels": [], "entities": []}, {"text": "Merge operations end the current computation and return a representation of the current sub-tree to the parent node, which composes it with the previously available information on the same level.", "labels": [], "entities": []}, {"text": "Recurrent operations use information from the siblings to perform predictions.", "labels": [], "entities": []}, {"text": "Operations at every level in the tree use shared weights, thus sharing the recursive nature of TreeLSTMs.", "labels": [], "entities": []}, {"text": "In contrast to TreeLSTMs however, the tree is created on-the-fly, which establishes skip-connections with previous tokens in the sequence and forms compositional representations of the past.", "labels": [], "entities": []}, {"text": "The branching decisions can either be trained through supervised learning, by providing the true branching signals, or by policy gradients which maximizes the log-likelihood of the observed sequence.", "labels": [], "entities": []}, {"text": "As opposed to previous models, these three operations constantly change the structure of the model in an online manner.", "labels": [], "entities": []}, {"text": "Experimental evaluation aims to analyze various aspects of the model such as: how does the model generalize on sequences of different lengths than those seen during training?", "labels": [], "entities": []}, {"text": "how hard is the tree learning problem?", "labels": [], "entities": [{"text": "tree learning", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.7615991234779358}]}, {"text": "To answer those questions, we propose a novel multi-operation math expression evaluation (MEE) dataset, with a standard set of tasks with varying levels of difficulty, where the difficulty scales up with respect to the length of the sequence.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct our experiments on a math induction task, a propositional logic inference task (Bowman et al., 2016) and language modelling.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.794128030538559}]}, {"text": "First of all, we aim to investigate whether a) our hierarchical model may help in tasks that explicitly exhibit hierarchical structure, and then b) whether the trees learned without supervision correspond to the ground-truth trees, c) how our model fare with respect to hierarchical models that have access to the whole sequence with a pre-determined tree structure and finally, d) are there any limitations for models that are not capable of learning hierarchical structures on-the-fly.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Prediction accuracy on MEE dataset.", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9576844573020935}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9150645136833191}, {"text": "MEE dataset", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.8651724755764008}]}]}