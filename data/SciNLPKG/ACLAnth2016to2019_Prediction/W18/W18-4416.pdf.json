{"title": [{"text": "Merging datasets for aggressive text identification", "labels": [], "entities": [{"text": "Merging datasets", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.7786589562892914}, {"text": "aggressive text identification", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.6371429959932963}]}], "abstractContent": [{"text": "This paper presents the approach of the team \"groutar\" to the shared task on Aggression Identification , considering the test sets in English, both from Facebook and general Social Media.", "labels": [], "entities": [{"text": "Aggression Identification", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.8941639065742493}]}, {"text": "This experiment aims to test the effect of merging new datasets in the performance of classification models.", "labels": [], "entities": []}, {"text": "We followed a standard machine learning approach with training, validation, and testing phases, and considered features such as part-of-speech, frequencies of insults, punctuation, sentiment, and capitalization.", "labels": [], "entities": []}, {"text": "In terms of algorithms, we experimented with Boosted Logistic Regression, Multi-Layer Perceptron, Parallel Random Forest and eXtreme Gradient Boosting.", "labels": [], "entities": []}, {"text": "One question appearing was how to merge datasets using different classification systems (e.g. aggression vs. toxicity).", "labels": [], "entities": []}, {"text": "Other issue concerns the possibility to generalize models and apply them to data from different social networks.", "labels": [], "entities": []}, {"text": "Regarding these, we merged two datasets, and the results showed that training with similar data is an advantage in the classification of social networks data.", "labels": [], "entities": []}, {"text": "However, adding data from different platforms, allowed slightly better results in both Facebook and Social Media, indicating that more generalized models can bean advantage.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the last few years, we have witnessed a growing number of online platforms where users can post content.", "labels": [], "entities": []}, {"text": "As the number of platforms has increased, so has the number of aggressive interactions, such as cyberbullying or hate speech.", "labels": [], "entities": []}, {"text": "The goal of our work is to contribute to the automatic identification of this type of communication through the participation in the Shared Task on Aggression Identification in text ().", "labels": [], "entities": [{"text": "Shared Task on Aggression Identification in text", "start_pos": 133, "end_pos": 181, "type": "TASK", "confidence": 0.6976776335920606}]}, {"text": "The task consisted in developing a classifier that could make a 3-way classification between Overtly Aggressive (OAG), Covertly Aggressive (CAG) and Non-aggressive (NAG) text data.", "labels": [], "entities": []}, {"text": "The organizers provided a dataset of 15,000 aggression-annotated Facebook posts for training and validating the classification systems.", "labels": [], "entities": []}, {"text": "Each team was allowed to test up to three systems and to use additional data for training, as long as the data would be publicly available before submission of the system paper.", "labels": [], "entities": []}, {"text": "Different competitions were available with variations in language and data sources.", "labels": [], "entities": []}, {"text": "It was possible to classify aggression in English and Hindi and also using data from Facebook or other Social Media platform unknown at submission time.", "labels": [], "entities": []}, {"text": "In our approach, we focused on understanding the effects of merging new datasets for training models.", "labels": [], "entities": []}, {"text": "We used the Toxicity dataset, from the Toxic comment classification challenge in Kaggle, as an additional source of data, and we proceeded with the conversion from toxicity to aggression.", "labels": [], "entities": [{"text": "Toxicity dataset", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.793840229511261}, {"text": "Toxic comment classification challenge in Kaggle", "start_pos": 39, "end_pos": 87, "type": "TASK", "confidence": 0.7084272503852844}]}, {"text": "We built and compared two systems, one using only the original data for training, and the second using also toxic data.", "labels": [], "entities": []}, {"text": "We extracted some classic features and studied different machine learning classification algorithms using a methodology of training, validation, and testing.", "labels": [], "entities": [{"text": "machine learning classification", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.6488246917724609}]}, {"text": "Our approach focused on English and in both test sets provided, Facebook and Social Media.", "labels": [], "entities": []}, {"text": "In the next sections, we present the related work (Section 2), our method (Section 3), the results (Section 4), and finally our conclusions (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "The provided training datasets () contained Facebook text messages for English and Hindi.", "labels": [], "entities": []}, {"text": "From those messages, 12,000 were for training and 3,000 for development (dev).", "labels": [], "entities": []}, {"text": "This last was a dataset for testing before submitting final results.", "labels": [], "entities": []}, {"text": "Regarding the test set, was the data available for final classification and final submission for ranking of solutions in the contest.", "labels": [], "entities": []}, {"text": "Several scenarios were available for testing the final models.", "labels": [], "entities": []}, {"text": "Besides different languages (English and Hindi), the teams could classify diverse message sources (Facebook and general Social Media).", "labels": [], "entities": []}, {"text": "For the annotation of the datasets, there were three classes described solely as Overtly Aggressive (OAG), Covertly Aggressive (CAG) and Non-aggressive (NAG) and no additional information provided.", "labels": [], "entities": []}, {"text": "This lack of deeper definitions opposes to previous recommendations (, which pointed out the importance to clearly define concepts before addressing problems like hate speech identification.", "labels": [], "entities": [{"text": "hate speech identification", "start_pos": 163, "end_pos": 189, "type": "TASK", "confidence": 0.7684741616249084}]}, {"text": "Aiming to improve the available definitions, we tried to manually inspect some data, so that we could better understand the differences among the three types of messages.", "labels": [], "entities": []}, {"text": "We concluded that it is not easy to distinguish the classes and that they overlap ().", "labels": [], "entities": []}, {"text": "For example, a text like \"Nonsense\" is marked as overtly aggressive (OAG), while \"No respect for him now\" is marked as non-aggressive (NAG).", "labels": [], "entities": []}, {"text": "Outside of the challenge, the definitions of the classes became available in the article presenting the dataset ().", "labels": [], "entities": []}, {"text": "According to these authors, overt aggression is any speech or text in which aggression is overtly expressed, either through the use of specific kind of lexical items, lexical features or certain syntactic structures, considered aggressive.", "labels": [], "entities": []}, {"text": "On the other hand, covert aggression is any text in which aggression is not overtly expressed.", "labels": [], "entities": []}, {"text": "It is an indirect attack against the victim and is often packaged as insincere polite expressions, through the use of conventionalised polite structures.", "labels": [], "entities": []}, {"text": "For instance, cases of satire or rhetorical questions maybe classified as covert aggression.", "labels": [], "entities": []}, {"text": "Considering the opportunities this task enabled, we decided to conduct our experiment only for English When we try to match the Aggression dataset with the Toxicity dataset, we are in the presence of two unequal classification systems.", "labels": [], "entities": [{"text": "Aggression dataset", "start_pos": 128, "end_pos": 146, "type": "DATASET", "confidence": 0.9309248924255371}, {"text": "Toxicity dataset", "start_pos": 156, "end_pos": 172, "type": "DATASET", "confidence": 0.820997804403305}]}, {"text": "Therefore, we conducted a procedure for converting the classes of the Toxicity dataset into aggression).", "labels": [], "entities": [{"text": "Toxicity dataset", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.8147099614143372}]}, {"text": "The steps followed are:  Regarding the features, we used the NLTK 3.3 library (Bird et al., 2009) for extracting: \u2022 Parts of speech (POS).", "labels": [], "entities": [{"text": "NLTK 3.3 library", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.8361892501513163}]}, {"text": "\u2022 Combination of POS and sentiment analysis.", "labels": [], "entities": [{"text": "POS", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.7198199033737183}, {"text": "sentiment analysis", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9382740557193756}]}, {"text": "The procedure consisted in tokenization and extraction of parts of speech (POS) with Penn Treebank style.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.9670603275299072}, {"text": "Penn Treebank style", "start_pos": 85, "end_pos": 104, "type": "DATASET", "confidence": 0.9714194734891256}]}, {"text": "Regarding sentiment, we considered Vader (Valence aware Dictionary and sentiment reasoner), a lexicon and rule-based sentiment analysis tool specialized in social media (.", "labels": [], "entities": [{"text": "rule-based sentiment analysis", "start_pos": 106, "end_pos": 135, "type": "TASK", "confidence": 0.6741464833418528}]}, {"text": "It produces four sentiment metrics, namely: positive, negative, neutral and compound.", "labels": [], "entities": []}, {"text": "Additionally, to these metrics, we extracted the counts of negative words, and the counts of negative adjectives, combining both POS and sentiment analysis.", "labels": [], "entities": [{"text": "POS", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.7129591107368469}, {"text": "sentiment analysis", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.8294633030891418}]}, {"text": "We also measured the frequencies of capitalized words in a message and marked with a boolean full capitalized messages.", "labels": [], "entities": []}, {"text": "Punctuation patterns were obtained as explained in?.", "labels": [], "entities": []}, {"text": "Finally, we mapped the frequencies of insults, using a dictionary 1 with 350 words.", "labels": [], "entities": []}, {"text": "The total number of features in each group is presented in?.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Extracted features based on punctuation and regular expression used.", "labels": [], "entities": []}, {"text": " Table 3: The total number of features by group.", "labels": [], "entities": []}, {"text": " Table 5: Results for the English (Facebook) task, comparing the use of aggressive data (FB ag rf) with  the use of aggressive data plus Toxicity dataset (FB ag tox rf) for training using Random Forests for  classification.", "labels": [], "entities": [{"text": "FB ag rf)", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.7926402688026428}]}, {"text": " Table 6: Results for the English (Social Media) task, comparing the use of aggressive data (Sm ag rf)  with the use of aggressive data plus Toxicity dataset (Sm ag tox rf) for training using Random Forests  for classification.", "labels": [], "entities": [{"text": "English (Social Media) task", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.5969743380943934}]}]}