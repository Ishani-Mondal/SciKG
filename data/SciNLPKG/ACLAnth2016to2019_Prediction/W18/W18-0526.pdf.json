{"title": [{"text": "Modeling Second-Language Learning from a Psychological Perspective", "labels": [], "entities": [{"text": "Modeling Second-Language Learning from a Psychological Perspective", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.8261189035006932}]}], "abstractContent": [{"text": "Psychological research on learning and memory has tended to emphasize small-scale laboratory studies.", "labels": [], "entities": []}, {"text": "However, large datasets of people using educational software provide opportunities to explore these issues from anew perspective.", "labels": [], "entities": []}, {"text": "In this paper we describe our approach to the Duolingo Second Language Acquisition Modeling (SLAM) competition which was run in early 2018.", "labels": [], "entities": [{"text": "Duolingo Second Language Acquisition Modeling (SLAM) competition", "start_pos": 46, "end_pos": 110, "type": "TASK", "confidence": 0.7578886018859016}]}, {"text": "We used a well-known class of algorithms (gradi-ent boosted decision trees), with features partially informed by theories from the psychological literature.", "labels": [], "entities": []}, {"text": "After detailing our mod-eling approach and a number of supplementary simulations, we reflect on the degree to which psychological theory aided the model, and the potential for cognitive science and pre-dictive modeling competitions to gain from each other.", "labels": [], "entities": []}], "introductionContent": [{"text": "Educational software that aims to teach people new skills, languages, and academic subjects have become increasingly popular.", "labels": [], "entities": []}, {"text": "The wide-spread deployment of these tools has created interesting opportunities to study the process of learning in large samples.", "labels": [], "entities": []}, {"text": "The Duolingo shared task on Second Lanugage Acquisition Modeling (SLAM) was a competitive modeling challenge run in early 2018 ( . The challenge, organized by Duolingo 1 , a popular second language learning app, was to use log data from thousands of users completing millions of exercises to predict patterns of future translation mistakes in heldout data.", "labels": [], "entities": [{"text": "Second Lanugage Acquisition Modeling (SLAM)", "start_pos": 28, "end_pos": 71, "type": "TASK", "confidence": 0.7590390656675611}]}, {"text": "The data was divided into three sets covering Spanish speakers learning English (en es), English speakers learning Spanish (es en), and English speakers learning French (fr en).", "labels": [], "entities": []}, {"text": "This paper reports the approach used by our team, http://duolingo.com which finished in third place for the en es data set, second place for es en, and third place for fr en.", "labels": [], "entities": [{"text": "en es data set", "start_pos": 108, "end_pos": 122, "type": "DATASET", "confidence": 0.670961894094944}]}, {"text": "Learning and memory has been a core focus of psychological science for over 100 years.", "labels": [], "entities": []}, {"text": "Most of this work has sought to build explanatory theories of human learning and memory using relatively small-scale laboratory studies.", "labels": [], "entities": []}, {"text": "Such studies have identified a number of important and apparently robust phenomena in memory including the nature of the retention curve (, the advantage for spaced over massed practice, the testing effect), and retrieval-induced forgetting.", "labels": [], "entities": [{"text": "retention curve", "start_pos": 121, "end_pos": 136, "type": "METRIC", "confidence": 0.9563007950782776}, {"text": "retrieval-induced forgetting", "start_pos": 212, "end_pos": 240, "type": "TASK", "confidence": 0.7044327855110168}]}, {"text": "The advent of large datasets such as the one provided in the Duolingo SLAM challenge may offer anew perspective and approach which may prove complementary to laboratory scale science.", "labels": [], "entities": [{"text": "Duolingo SLAM challenge", "start_pos": 61, "end_pos": 84, "type": "DATASET", "confidence": 0.7890829046567281}]}, {"text": "First, the much larger sample sizes may help to better identify parameters of psychological models.", "labels": [], "entities": []}, {"text": "Second, datasets covering more naturalistic learning situations may allow us to test the predictive accuracy of psychological theories in a more generalizable fashion.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9047211408615112}]}, {"text": "Despite these promising opportunities, it remains unclear how much of current psychological theory might be important for tasks such as the Duolingo SLAM challenge.", "labels": [], "entities": [{"text": "Duolingo SLAM challenge", "start_pos": 140, "end_pos": 163, "type": "TASK", "confidence": 0.7087681492169698}]}, {"text": "In the field of education data mining, researchers trying to build predictive models of student learning have typically relied on traditional, and interpretable, models and approaches that are rooted in cognitive science (e.g.,.", "labels": [], "entities": []}, {"text": "However, a recent paper found that state-of-the-art results could be achieved using deep neural networks with little or no cognitive theory builtin (so called \"deep knowledge tracing\",.", "labels": [], "entities": [{"text": "deep knowledge tracing", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.7036333481470743}]}, {"text": "compared deep knowledge tracing (DKT) to more standard \"Bayesian knowledge tracing\" (BKT) models and showed that it was possible to equate the performance of the BKT model by additional features and parameters that represent core aspects of the psychology of learning and memory such as forgetting and individual abilities ().", "labels": [], "entities": [{"text": "deep knowledge tracing (DKT)", "start_pos": 9, "end_pos": 37, "type": "TASK", "confidence": 0.7936670780181885}, {"text": "Bayesian knowledge tracing\" (BKT)", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.7916997585977826}]}, {"text": "An ongoing debate remains in this community whether using flexible models with lots of data can improve over more heavily structured, theory-based models (.", "labels": [], "entities": []}, {"text": "For our approach to the SLAM competition, we decided to use a generic and fairly flexible model structure that we provided with hand-coded, psychologically inspired features.", "labels": [], "entities": [{"text": "SLAM competition", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.9312432408332825}]}, {"text": "We therefore positioned our entry to SLAM somewhat in between the approaches mentioned above.", "labels": [], "entities": [{"text": "SLAM", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.8663259148597717}]}, {"text": "Specifically, we used gradient boosting decision trees (GBDT,) for the model structure, which is a powerful classification algorithm that is known to perform well across various kinds of data sets.", "labels": [], "entities": []}, {"text": "Like deep learning, GBDT can extract complex interactions among features, but it has some advantages including faster training and easier integration of diverse inputs.", "labels": [], "entities": []}, {"text": "We then created a number of new psychologically-grounded features for the SLAM dataset covering aspects such as user perseverance, learning processes, contextual factors, and cognate similarity.", "labels": [], "entities": [{"text": "SLAM dataset", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.8298760056495667}]}, {"text": "After finding a model that provided the best held-out performance on the test data set, we conducted a number of \"lesioning\" studies where we selectively removed features from the model and re-estimated the parameters in order to assess the contribution of particular types of features.", "labels": [], "entities": []}, {"text": "We begin by describing our overall modeling approach, and then discuss some of the lessons learned from our analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "To better understand which features or groups of features were most important to our model's predictions, we conducted a set of experiments in which we lesioned (i.e., removed) a group of features and re-trained the model on the train set, evaluating performance on the dev set.", "labels": [], "entities": []}, {"text": "For simplicity, we ran each of the lesioned models on all language data and report the average performance.", "labels": [], "entities": []}, {"text": "We did not run individual-language models as we did for our primary model.", "labels": [], "entities": []}, {"text": "The results of the lesion experiments are shown in.", "labels": [], "entities": []}, {"text": "The models are as follows.", "labels": [], "entities": []}, {"text": "none: All features are included.", "labels": [], "entities": []}, {"text": "temporal: Temporal information, including number and timing of past encounters with the word and error tracking information, is removed.", "labels": [], "entities": []}, {"text": "Interestingly, we found that for both user-level and word-level features, the bulk of the model's predictive power could be achieved using ID's alone, represented as high-cardinality categorical features.", "labels": [], "entities": []}, {"text": "Removing other word features, such as morphological features and part of speech, created only a small degradation of performance.", "labels": [], "entities": []}, {"text": "In the case of users, removing features such as entropy and average exercise burst length led to a tiny increase of performance.", "labels": [], "entities": [{"text": "average exercise burst length", "start_pos": 60, "end_pos": 89, "type": "METRIC", "confidence": 0.5826444998383522}]}, {"text": "In the case of both users and words, though, we find that in the absence of ID features the other features are helpful and lead to better performance than removing all features.", "labels": [], "entities": []}, {"text": "We also found that removing all information about neighboring words and the dependency-parse root word degraded performance.", "labels": [], "entities": []}, {"text": "This confirms that word context matters, and suggests that users commonly make errors in word order, subject-verb matching and other grammatical rules.", "labels": [], "entities": []}, {"text": "Our external word features-Levenshtein distance to translation, frequency, and age of acquisition-provided a slight boost to model performance, showing the benefit of considering what makes a word hard to learn from a psychological and linguistic perspective.", "labels": [], "entities": []}, {"text": "Adding temporal features about past encounters and errors helped the models, but not as much as we expected.", "labels": [], "entities": []}, {"text": "While not included in the final model, we had also tried augmenting the temporal feature set with more features related to massing and spacing of encounters with a word, but found it did not improve performance.", "labels": [], "entities": []}, {"text": "This is perhaps not surprising given how small the benefit of the existing temporal features are in our model.", "labels": [], "entities": []}, {"text": "Though not plotted above, we also ran a model lesioning exercise-level features including client, session type, format, and exercise duration.", "labels": [], "entities": []}, {"text": "This model achieved an AUROC of .787, far lower than any other lesion.", "labels": [], "entities": [{"text": "AUROC", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.994452178478241}]}, {"text": "This points to the fact that the manner in which memory is assessed often affects observed performance (e.g., the large literature in psychology on the difference between recall and recognition memory,).", "labels": [], "entities": [{"text": "recall", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.9810118675231934}]}], "tableCaptions": [{"text": " Table 1: Parameters of final LightGBM models. See LightGBM documentation for more information; all other  parameters were left at their default values.", "labels": [], "entities": []}]}