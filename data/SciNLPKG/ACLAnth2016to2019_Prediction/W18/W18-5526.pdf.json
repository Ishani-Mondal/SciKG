{"title": [{"text": "QED: A Fact Verification System for the FEVER Shared Task", "labels": [], "entities": [{"text": "FEVER Shared Task", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.6868832111358643}]}], "abstractContent": [{"text": "This paper describes our system submission to the 2018 Fact Extraction and VERification (FEVER) shared task.", "labels": [], "entities": [{"text": "Fact Extraction and VERification (FEVER) shared task", "start_pos": 55, "end_pos": 107, "type": "TASK", "confidence": 0.6433834897147285}]}, {"text": "The system uses a heuristics-based approach for evidence extraction and a modified version of the inference model by Parikh et al.", "labels": [], "entities": [{"text": "evidence extraction", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8712508082389832}]}, {"text": "(2016) for classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.9734242558479309}]}, {"text": "Our process is broken down into three modules: potentially relevant documents are gathered based on key phrases in the claim, then any possible evidence sentences inside those documents are extracted, and finally our classifier discards any evidence deemed irrelevant and uses the remaining to classify the claim's veracity.", "labels": [], "entities": []}, {"text": "Our system beats the shared task baseline by 12% and is successful at finding correct evidence (evidence retrieval F1 of 62.5% on the development set).", "labels": [], "entities": [{"text": "evidence retrieval F1", "start_pos": 96, "end_pos": 117, "type": "METRIC", "confidence": 0.5216414928436279}]}], "introductionContent": [{"text": "The FEVER shared task () sets outwith the goal of creating a system which can take a factual claim and either verify or refute it based on a database of Wikipedia articles.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9702597856521606}]}, {"text": "The system is evaluated on the correct labeling of the claims as \"Supports,\" \"Refutes,\" or \"Not Enough Info\" (NEI) as well as on valid evidence to support the label (except in the case of \"NEI\").", "labels": [], "entities": [{"text": "Refutes,\" or \"Not Enough Info\" (NEI)", "start_pos": 78, "end_pos": 114, "type": "METRIC", "confidence": 0.7384582974693992}]}, {"text": "Each claim can have multiple evidence sets, but only one set needs to be found so long as the correct label is applied.", "labels": [], "entities": []}, {"text": "gives an example of a claim along with the evidence sets that support it, as well as a claim and the evidence that refutes it.", "labels": [], "entities": []}, {"text": "We split the task into three distinct modules, with each module building on the data of the previous one.", "labels": [], "entities": []}, {"text": "The first module is a document finder finding key terms in the claim which correspond to the titles of the Wikipedia articles, and returning those articles.", "labels": [], "entities": []}, {"text": "The second module takes each document found and finds all sentences which are close enough to the claim to be considered evi-\"Supports\" Claim: Ann Richards was professionally involved in politics.", "labels": [], "entities": []}, {"text": "Evidence set 1: Dorothy Ann Willis Richards () was an American politician and 45th Governor of Texas.", "labels": [], "entities": []}, {"text": "Evidence set 2: A Democrat, she first came to national attention as the Texas State Treasurer, when she delivered the keynote address at the 1988 Democratic National Convention.", "labels": [], "entities": [{"text": "1988 Democratic National Convention", "start_pos": 141, "end_pos": 176, "type": "TASK", "confidence": 0.554135650396347}]}, {"text": "\"Refutes\" Claim: Andrew Kevin Walker is only Chinese.", "labels": [], "entities": [{"text": "Refutes\" Claim", "start_pos": 1, "end_pos": 15, "type": "TASK", "confidence": 0.576136181751887}]}, {"text": "Evidence set: Andrew Kevin Walker (born August) is an American BAFTA-nominated screenwriter. dence.", "labels": [], "entities": []}, {"text": "Finally, all sentences retrieved fora given claim are classified using an inference system as supporting or refuting the claim, or as \"NEI\".", "labels": [], "entities": []}, {"text": "In the following sections, we detail each module, providing results on the FEVER development set which consists of 19,998 claims (6,666 in each class).", "labels": [], "entities": [{"text": "FEVER development set", "start_pos": 75, "end_pos": 96, "type": "DATASET", "confidence": 0.7669465442498525}]}, {"text": "Our system focuses on finding evidence sets composed of only one sentence.", "labels": [], "entities": []}, {"text": "Of the 13,332 verifiable (\"Supports\" or \"Refutes\") claims in the development set, only 9% cannot be satisfied with an evidence set consisting of only one sentence.", "labels": [], "entities": []}, {"text": "The code for our system is available at https: //github.com/jluken/FEVER.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9975916147232056}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Scores on the FEVER development and test sets. Baseline is the system from (Thorne et al., 2018). The  results are prior to human evaluation of the evidence.", "labels": [], "entities": [{"text": "FEVER development and test sets", "start_pos": 24, "end_pos": 55, "type": "DATASET", "confidence": 0.6981076836585999}, {"text": "Baseline", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9684566855430603}]}, {"text": " Table 2: Contingency matrix (percentage) in the devel- opment set.", "labels": [], "entities": []}]}