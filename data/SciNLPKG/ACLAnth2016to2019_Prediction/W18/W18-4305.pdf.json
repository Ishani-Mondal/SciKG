{"title": [{"text": "An Evaluation of Information Extraction Tools for Identifying Health Claims in News Headlines", "labels": [], "entities": [{"text": "Identifying Health Claims in News Headlines", "start_pos": 50, "end_pos": 93, "type": "TASK", "confidence": 0.8851530154546102}]}], "abstractContent": [{"text": "This study evaluates the performance of four information extraction tools (extractors) on identifying health claims in health news headlines.", "labels": [], "entities": [{"text": "identifying health claims in health news headlines", "start_pos": 90, "end_pos": 140, "type": "TASK", "confidence": 0.735140894140516}]}, {"text": "A health claim is defined as a triplet: IV (what is being manipulated), DV (what is being measured) and their relation.", "labels": [], "entities": []}, {"text": "Tools that can identify health claims provide the foundation for evaluating the accuracy of these claims against authoritative resources.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.995804488658905}]}, {"text": "The evaluation result shows that 26% headlines do not include health claims, and all extractors face difficulty separating them from the rest.", "labels": [], "entities": []}, {"text": "For those with health claims, OPENIE-5.0 performed the best with F-measure at 0.6 level for extracting \"IV-relation-DV\".", "labels": [], "entities": [{"text": "OPENIE-5.0", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9161150455474854}, {"text": "F-measure", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9989511966705322}]}, {"text": "However, the characteristic linguistic structures in health news headlines, such as incomplete sentences and non-verb relations, pose particular challenge to existing tools.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We created a benchmark data set for evaluating the information extraction tools.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.8204032480716705}]}, {"text": "This section describes the process of data collection, annotation, and validation.", "labels": [], "entities": [{"text": "data collection", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7599934339523315}]}, {"text": "After the inter-coder reliability check, the disagreements were resolved through discussion.", "labels": [], "entities": []}, {"text": "Then one annotator annotated the remaining news headlines.", "labels": [], "entities": []}, {"text": "Among the 564 headlines, 416 (74%) describe health claims and 148 (26%) do not.", "labels": [], "entities": []}, {"text": "Each of the 416 headlines with health claims was annotated as one \"IV-relation-DV\" triplet with a few exceptions.", "labels": [], "entities": []}, {"text": "Five headlines were annotated with \"Multiple IVs\" (e.g., \"Little to no association between butter consumption, chronic disease or total mortality\"); two described more than one \"IV-relation-DV\" triplet (e.g., \"Epigenetic modification increases susceptibility to obesity and predicts fatty liver\").", "labels": [], "entities": []}, {"text": "We have also identified six types of linguistic structures in health news headlines that might confuse the extractors: \u2022 Non-verb relation: such as \"New potential treatment for cancer metastasis identified\".", "labels": [], "entities": []}, {"text": "\u2022 Relation with modal verb: such as \"Smoking can hamper common treatment for breast cancer\".", "labels": [], "entities": [{"text": "Relation", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9093486666679382}]}, {"text": "\u2022 IVs and DVs with prepositional phrases: such as \"Sugars in Western diets\" in \"Sugars in Western diets increase risk for breast cancer tumors and metastasis\".", "labels": [], "entities": []}, {"text": "\u2022 Multiple IVs or DVs in parallel phrases: parallel phrases mean to contain more than one IVs or DVs, such as \"breast, ovarian cancer\" in \"Breast, ovarian cancer may have similar origins, study finds\".", "labels": [], "entities": []}, {"text": "\u2022 Headlines containing both reporting verb and another verb to describe the relation: such as \"find\" and \"treat\" in \"Scientists find 'outlier' enzymes, potential new targets to treat diabetes, inflammation\".", "labels": [], "entities": []}, {"text": "\u2022 Headlines as incomplete sentences: such as \"Sugar-sweetened drinks linked to increased visceral fat\".", "labels": [], "entities": [{"text": "Headlines", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9094734787940979}]}, {"text": "The 416 headlines with health claims include 113 with incomplete sentence structure (27%), 39 with reporting verbs (9%), 39 non-verb relations (9%), 108 with modal verbs (26%), 107 with prepositional phrases in IVs (26%), 182 with prepositional phrases in DVs (44%), 18 with parallel phrases in IVs (4%), 42 with parallel phrases in DVs (10%).", "labels": [], "entities": []}, {"text": "One headline may include multiple characteristics.", "labels": [], "entities": []}, {"text": "As a robustness check, we further compared the linguistic characteristics of the headlines about two diseases: \"breast cancer\" and \"diabetes\", and found no significant difference (see.", "labels": [], "entities": []}, {"text": "Therefore, we consider the linguistic characteristics of health news headlines independent of disease types.", "labels": [], "entities": []}, {"text": "Four systems were compared in terms of their performance in extracting \"IV-relation-DV\" from health news headlines.", "labels": [], "entities": [{"text": "extracting \"IV-relation-DV\" from health news headlines", "start_pos": 60, "end_pos": 114, "type": "TASK", "confidence": 0.8492637798190117}]}, {"text": "Section 4.1 describes the evaluation method.", "labels": [], "entities": []}, {"text": "Section 4.2 evaluates performance on identifying headlines without health claims, and Section 4.3 on headlines with health claims.", "labels": [], "entities": []}, {"text": "Section 4.4 evaluates performance on cases with special linguistic structures.", "labels": [], "entities": []}, {"text": "We chose two methods for this evaluation: the first one calculates the precision, recall and F-measure by manually comparing the machine annotations against the gold standard; the second method automatically calculates the BLEU scores between machine annotations and the gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9997404217720032}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9992751479148865}, {"text": "F-measure", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9987342953681946}, {"text": "BLEU", "start_pos": 223, "end_pos": 227, "type": "METRIC", "confidence": 0.9993317723274231}]}, {"text": "Precision, Recall and F-measure are traditional evaluation methods in information retrieval.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9850314855575562}, {"text": "Recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9749606251716614}, {"text": "F-measure", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9964824914932251}, {"text": "information retrieval", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.7991164922714233}]}, {"text": "For information extraction task, Fader et al.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.9277096688747406}]}, {"text": "(2011) and Etzioni et al.", "labels": [], "entities": []}, {"text": "(2011) defined precision as the fraction of returned extractions that are correct, and recall as the fraction of correct extractions in the total corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9993333220481873}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9991135001182556}]}, {"text": "For each extracted triplet, we manually check whether it is corrector not.", "labels": [], "entities": []}, {"text": "The correct extraction is defined as keywords or phrases match in IV, DV and relation.", "labels": [], "entities": [{"text": "IV", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9194577932357788}]}, {"text": "For robustness check we choose the second evaluation method as the BLEU score, which can be automatically calculated.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9579603374004364}]}, {"text": "As a popular measure in machine translation, it evaluates the similarity between the machine translations and the gold standard.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8242031335830688}, {"text": "similarity", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9551697969436646}]}, {"text": "The BLEU score divides each translated sentence into several n-grams and compares differences between a machine translation and a professional human translation based on those n-grams (.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9628521502017975}]}, {"text": "In our task, we consider each manually-annotated \"IV-relation-DV\" triplet as a reference translation, and each machine-extracted triplet as a candidate translation.", "labels": [], "entities": []}, {"text": "For each extractor, we will calculate a BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.976852536201477}]}, {"text": "The higher BLEU score is, the better performance an extractor would achieve.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9803634583950043}]}, {"text": "The maximum number of n-grams varies from 2 to 4 and the weights for each n-gram are equal.", "labels": [], "entities": []}, {"text": "In addition, we apply \"Add-one Smoothing\" technique proposed in to avoid the BLEU score being 0 when n grows bigger.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9843316972255707}]}], "tableCaptions": [{"text": " Table 1: Confusion matrix from whether a headline describe a relation.", "labels": [], "entities": []}, {"text": " Table 2: Confusion matrix from IV, DV and relation annotations.", "labels": [], "entities": []}, {"text": " Table 3: Confusion matrix from different tools.  Information Extractor Precision Recall F-measure  REVERB  .35  .77  .48  OLLIE  .42  .52  .46  OPENIE-5.0  .50  .41  .45  SemRep  .31  .78  .44", "labels": [], "entities": [{"text": "Information Extractor Precision Recall F-measure  REVERB  .", "start_pos": 50, "end_pos": 109, "type": "METRIC", "confidence": 0.6510291312422071}, {"text": "OLLIE", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.9881388545036316}, {"text": "OPENIE-5.0", "start_pos": 145, "end_pos": 155, "type": "METRIC", "confidence": 0.9579293131828308}]}, {"text": " Table 4: Precision, Recall and F-measure for different tools on headlines without health claim.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9988105297088623}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9972307085990906}, {"text": "F-measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.999311089515686}]}, {"text": " Table 5: Precision, Recall and F-measure for different tools on headlines with health claim.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9987877011299133}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9971952438354492}, {"text": "F-measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9993273019790649}]}, {"text": " Table 6: BLEU scores for different tools on headlines with health claim.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988569021224976}]}, {"text": " Table 7: Performance on different linguistic structures.", "labels": [], "entities": []}]}