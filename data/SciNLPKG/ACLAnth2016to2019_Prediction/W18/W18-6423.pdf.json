{"title": [{"text": "Association for Computa-tional Linguistics. Philipp Koehn, and Christof Monz. 2018. Find-ings of the 2018 conference on machine translation (WMT18)", "labels": [], "entities": [{"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.6729120165109634}, {"text": "WMT18)", "start_pos": 141, "end_pos": 147, "type": "DATASET", "confidence": 0.6541593074798584}]}], "abstractContent": [{"text": "The paper describes the development process of the Tilde's NMT systems that were submitted for the WMT 2018 shared task on news translation.", "labels": [], "entities": [{"text": "Tilde's NMT systems", "start_pos": 51, "end_pos": 70, "type": "DATASET", "confidence": 0.9189469516277313}, {"text": "WMT 2018 shared task on news translation", "start_pos": 99, "end_pos": 139, "type": "TASK", "confidence": 0.6710290568215507}]}, {"text": "We describe the data filtering and pre-processing workflows, the NMT system training architectures, and automatic evaluation results.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.7065965980291367}]}, {"text": "For the WMT 2018 shared task, we submitted seven systems (both constrained and unconstrained) for English-Estonian and Estonian-English translation directions.", "labels": [], "entities": [{"text": "WMT 2018 shared task", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.5264485999941826}, {"text": "Estonian-English translation directions", "start_pos": 119, "end_pos": 158, "type": "TASK", "confidence": 0.6500480969746908}]}, {"text": "The submitted systems were trained using Transformer models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) is a rapidly changing research area.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.886445144812266}]}, {"text": "Since 2016 when NMT systems first showed to achieve significantly better results than statistical machine translation (SMT) systems, the dominant neural network (NN) architectures for NMT have changed on a yearly (and even more frequent) basis.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 86, "end_pos": 123, "type": "TASK", "confidence": 0.7877324223518372}]}, {"text": "The state-of-the-art in 2016 were shallow attention-based recurrent neural networks (RNN) with gated recurrent units (GRU)) in recurrent layers.", "labels": [], "entities": []}, {"text": "In 2017 (, multiplicative long short-term memory (MLSTM) units () and deep GRU () models were introduced in NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.8056430220603943}]}, {"text": "The same year, selfattentional (Transformer) models were introduced (.", "labels": [], "entities": []}, {"text": "Consequently, in 2018, most of the top scoring systems in the shared task on news translation of the Third Conference on Machine Translation (WMT) were trained using Transformer models . However, it is already evident that the state-of-the-art architectures will All 14 of the best automatically scored systems according to the information provided by participants in the official submission portal http://matrix.statmt.org were indicated as being based on Transformer models.", "labels": [], "entities": [{"text": "news translation of the Third Conference on Machine Translation (WMT)", "start_pos": 77, "end_pos": 146, "type": "TASK", "confidence": 0.7617859691381454}]}, {"text": "be pushed even further in.", "labels": [], "entities": []}, {"text": "For instance, have recently proposed RNMT+ models that combine deep LSTM-based models with multi-head attention and showed that the models outperform Transformer models.", "labels": [], "entities": []}, {"text": "In WMT 2017, Tilde participated with MLSTM-based NMT systems (.", "labels": [], "entities": [{"text": "WMT 2017", "start_pos": 3, "end_pos": 11, "type": "TASK", "confidence": 0.526960164308548}, {"text": "MLSTM-based NMT", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.7719570696353912}]}, {"text": "In this paper, we compare the MLSTMbased models with Transformer models for English-Estonian and Estonian-English and we show that the state-of-the-art of WMT 2017 is well behind the new models.", "labels": [], "entities": [{"text": "WMT 2017", "start_pos": 155, "end_pos": 163, "type": "DATASET", "confidence": 0.7993835210800171}]}, {"text": "Therefore, for WMT 2018, Tilde submitted NMT systems that were trained using Transformer models.", "labels": [], "entities": [{"text": "WMT 2018", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.6295169591903687}, {"text": "Tilde", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.88105708360672}]}, {"text": "The paper is further structured as follows: Section 2 provides an overview of systems submitted for the WMT 2018 shared task on news translation, Section 3 describes the data used to train the NMT systems and the data pre-processing workflows, Section 4 describes all NMT systems trained and experiments on handling of named entities and combination of systems, Section 5 provides automatic evaluation results, and Section 6 concludes the paper.", "labels": [], "entities": [{"text": "WMT 2018 shared task on news translation", "start_pos": 104, "end_pos": 144, "type": "TASK", "confidence": 0.6661919866289411}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Training data statistics (sentence counts) before and after filtering", "labels": [], "entities": []}, {"text": " Table 3: NMT system training configuration (all other parameters were set to the default values of the respective  toolkits (Nematus or Sockeye)", "labels": [], "entities": [{"text": "Sockeye", "start_pos": 137, "end_pos": 144, "type": "DATASET", "confidence": 0.5553115606307983}]}, {"text": " Table 4: Automatic evaluation results", "labels": [], "entities": []}]}