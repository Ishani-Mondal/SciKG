{"title": [{"text": "Detecting Code-Switching between Turkish-English Language Pair", "labels": [], "entities": [{"text": "Detecting Code-Switching", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.842449963092804}]}], "abstractContent": [{"text": "Code-switching (usage of different languages within a single conversation context in an alternative manner) is a highly increasing phenomenon in social media and colloquial usage which poses different challenges for natural language processing.", "labels": [], "entities": []}, {"text": "This paper introduces the first study for the detection of Turkish-English code-switching and also a small test data collected from social media in order to smooth the way for further studies.", "labels": [], "entities": []}, {"text": "The proposed system using character level n-grams and conditional random fields (CRFs) obtains 95.6% micro-averaged F1-score on the introduced test data set.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9191179275512695}]}], "introductionContent": [{"text": "Code-switching is a common linguistic phenomenon generally attributed to bilingual communities but also highly observed among white collar employees.", "labels": [], "entities": []}, {"text": "It is also treated as related to higher education in some regions of the world (e.g. due to foreign language usage at higher education).", "labels": [], "entities": []}, {"text": "Although the social motivation of code-switching usage has been still under investigation and there exist different reactions to it), the challenges caused by its increasing usage in social media are not negligible for natural language processing studies focusing on this domain.", "labels": [], "entities": []}, {"text": "Social media usage has increased tremendously, bringing with it several problems.", "labels": [], "entities": []}, {"text": "Analysis and information retrieval from social media sources are difficult, due to usage of a noncanonical language ().", "labels": [], "entities": [{"text": "Analysis", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9640249609947205}, {"text": "information retrieval from social media", "start_pos": 13, "end_pos": 52, "type": "TASK", "confidence": 0.8103184580802918}]}, {"text": "The noisy character of social media texts often require text normalization, in order to prepare social media texts for data analysis.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7233349233865738}]}, {"text": "Eryi\u02d8 git and Toruno\u02d8 gluSelamet (2017) is the first study which introduces asocial media text normalization approach for Turkish.", "labels": [], "entities": [{"text": "asocial media text normalization", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.6410404443740845}]}, {"text": "In this study, similar to Han and Baldwin (2011) their candidate word (solution) generation stage comes after an initial ill-formed word detection stage where they use a Turkish morphological analyzer as the language validator.", "labels": [], "entities": [{"text": "candidate word (solution) generation", "start_pos": 55, "end_pos": 91, "type": "TASK", "confidence": 0.6304411689440409}, {"text": "word detection", "start_pos": 132, "end_pos": 146, "type": "TASK", "confidence": 0.7606170773506165}]}, {"text": "Although this approach works quite well for Turkish posts, it is obvious that it would encounter difficulties in case of code-switching where the language validator would detect every foreign word as ill-formed and the normalizer would try to propose a candidate correction for each of these.", "labels": [], "entities": []}, {"text": "A similar situation maybe observed at the behavior of spelling checkers within text editors.", "labels": [], "entities": [{"text": "spelling checkers within text editors", "start_pos": 54, "end_pos": 91, "type": "TASK", "confidence": 0.8426673173904419}]}, {"text": "These also detect the foreign words (purposely written) as out of vocabulary and insist on proposing a candidate correction which makes the detection of actual spelling errors difficult for the users.", "labels": [], "entities": []}, {"text": "In recent years, the use of code-switching between Turkish and English has also become very frequent specifically in daily life conversations and social media posts of white collars and youth population.", "labels": [], "entities": []}, {"text": "(1) introduces such an example which is not unusual to see.", "labels": [], "entities": []}, {"text": "(1) Original code-switched version Serverlar\u0131m\u0131z\u0131n update is\u00b8lemleriis\u00b8lemleri i\u00e7in bu domaindeki expert aray\u0131s\u00b8\u0131m\u0131zaray\u0131s\u00b8\u0131m\u0131z devam etmektedir.", "labels": [], "entities": []}, {"text": "Turkish version and literal translation Sunucular\u0131m\u0131z\u0131n (of our servers) g\u00fcncelleme (update) is\u00b8lemleriis\u00b8lemleri (process) i\u00e7in (for), bu (this) alandaki (on domain) uzman (expert) aray\u0131s\u00b8\u0131m\u0131zaray\u0131s\u00b8\u0131m\u0131z (search) devam etmektedir (continues).", "labels": [], "entities": []}, {"text": "English version For the update processes of our servers, we keep on searching an expert on this domain.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first study working on automatic detection of codeswitching between Turkish and English.", "labels": [], "entities": [{"text": "automatic detection of codeswitching between Turkish and English", "start_pos": 65, "end_pos": 129, "type": "TASK", "confidence": 0.699889775365591}]}, {"text": "We introduce a small test data set composed of 391 social media posts each consisting of code-switched sentences and their word-by-word manual annotation stating either the word is Turkish or English.", "labels": [], "entities": []}, {"text": "The paper presents our first results on this data set which is quite promising with a 95.6% micro average F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.8401092290878296}]}, {"text": "Our proposed system uses conditional random fields using character n-grams and word look-up features from monolingual corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our token-level language identification models using precision, recall and F 1 measures calculated separately for both language classes (Turkish and English).", "labels": [], "entities": [{"text": "token-level language identification", "start_pos": 16, "end_pos": 51, "type": "TASK", "confidence": 0.6563956340154012}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9996168613433838}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9992420673370361}, {"text": "F 1", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.996143251657486}]}, {"text": "We also provide micro and macro averaged F 1 measures for each model.", "labels": [], "entities": [{"text": "F 1", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.8653198480606079}]}, {"text": "the baseline model which assigns the label \"Turkish\" to all tokens and the second row provides the results of a rule-based lexicon lookup (base LL) which assigns the language label for each word by searching it in TNC and ETD used as Turkish and English dictionaries.", "labels": [], "entities": []}, {"text": "If a word occurs in both or none of these dictionaries, it is tagged as Turkish by default.", "labels": [], "entities": []}, {"text": "We observe from the results that the characterlevel n-gram models trained on a formal data set (TNC) fall behind our second baseline (with 88.7% macro avg.", "labels": [], "entities": []}, {"text": "F 1 ) whereas the one trained on social media data (TTC) performs better (91.1%).", "labels": [], "entities": []}, {"text": "It can also be observed that the performances of character n-gram language models turned out to be considerably high, aided by the fact that Turkish and English are morphologically distant languages and contain differing alphabetical characters such as \"s \u00b8,\u02d8 g,\u00a8 u,\u00a8 o,c \u00b8,\u0131\"in Turkish and \"q,w,x\" in English.", "labels": [], "entities": []}, {"text": "CRF models' performances are calculated via 10 fold cross-validation over code-switched corpus (CSC).", "labels": [], "entities": []}, {"text": "One may see from the table that all of our CRF models perform higher than our baselines and character n-gram models.", "labels": [], "entities": []}, {"text": "The best performances (95.6% micro and 94.5% macro avg.", "labels": [], "entities": []}, {"text": "F 1 ) are obtained with CRF \u03c6 trained with LEX + LM + APOS features.", "labels": [], "entities": [{"text": "LEX", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9218460321426392}, {"text": "APOS", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.5627005100250244}]}, {"text": "Contrary to the above findings with character level n-gram models, we see that CRF \u03c6 performs better when TNC is used for character-level n-gram training and look-up.", "labels": [], "entities": [{"text": "CRF \u03c6", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.8474425375461578}]}, {"text": "The use of TTC (monolingual Turkish data collected from social media) was revealing better results in Ch.n-gram models and similar results in CRF \u2020 . This maybe attributed to the fact that our hypothesis regarding the use of apostrophes in codeswitching of Turkish reveals a good point and the validation of the word sub-part before the apostrophe sign (from a formally written corpus -TNC) brings out a better modeling.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Token-level language identification results.  LM/Dict. refers to the data used as dictionaries and training data for n-gram language models.", "labels": [], "entities": [{"text": "Token-level language identification", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.798825999101003}]}]}