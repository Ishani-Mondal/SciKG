{"title": [{"text": "The JHU Parallel Corpus Filtering Systems for WMT 2018", "labels": [], "entities": [{"text": "JHU", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9472376704216003}, {"text": "WMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9139556884765625}]}], "abstractContent": [{"text": "This work describes our submission to the WMT18 Parallel Corpus Filtering shared task.", "labels": [], "entities": [{"text": "WMT18 Parallel Corpus Filtering shared task", "start_pos": 42, "end_pos": 85, "type": "TASK", "confidence": 0.6000870565573374}]}, {"text": "We use a slightly modified version of the Zip-porah Corpus Filtering toolkit (Xu and Koehn, 2017), which computes an adequacy score and a fluency score on a sentence pair, and use a weighted sum of the scores as the selection criteria.", "labels": [], "entities": []}, {"text": "This work differs from Zipporah in that we experiment with using the noisy corpus to be filtered to compute the combination weights, and thus avoids generating synthetic data as in standard Zipporah.", "labels": [], "entities": []}], "introductionContent": [{"text": "Todays machine translation systems require large amounts of training data inform of sentences paired with their translation, which are often compiled from online sources.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7708058655261993}]}, {"text": "This has not changed fundamentally with the move from statistical machine translation to neural machine translation, also we observed that neural models require more training data and are more sensitive to noise.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6516662140687307}, {"text": "neural machine translation", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.7174449364344279}]}, {"text": "Thus both the acquisition of more training data such as indiscriminate web crawling and corpus filtering will have large impact on the quality of state-of-the-art machine translation systems.", "labels": [], "entities": [{"text": "corpus filtering", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.7651733756065369}, {"text": "machine translation", "start_pos": 163, "end_pos": 182, "type": "TASK", "confidence": 0.7037698179483414}]}, {"text": "The JHU submission to the WMT18 Parallel Corpus Filtering shared task uses a modified version of the Zipporah Corpus Filtering toolkit (.", "labels": [], "entities": [{"text": "JHU", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9148605465888977}, {"text": "WMT18 Parallel Corpus Filtering shared task", "start_pos": 26, "end_pos": 69, "type": "TASK", "confidence": 0.6714007059733073}]}, {"text": "For a sentence pair, Zipporah uses a bag-of-words model to generate an adequacy score, and an n-gram language model to generate fluency score.", "labels": [], "entities": []}, {"text": "The two scores are combined based on weights trained in order to separate clean data from noisy data.", "labels": [], "entities": []}, {"text": "The original version of Zipporah generates artificial noisy training data to train such classifier, in this submission we also treat the Paracrawl corpus as the negative examples.", "labels": [], "entities": [{"text": "Paracrawl corpus", "start_pos": 137, "end_pos": 153, "type": "DATASET", "confidence": 0.9504781067371368}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results of our Zipporah variants, compared to the submission with the best average test score.", "labels": [], "entities": []}]}