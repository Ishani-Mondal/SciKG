{"title": [{"text": "Comparative Analysis of Neural QA models on SQuAD", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 44, "end_pos": 49, "type": "TASK", "confidence": 0.5070936679840088}]}], "abstractContent": [{"text": "The task of Question Answering has gained prominence in the past few decades for testing the ability of machines to understand natural language.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7827823162078857}]}, {"text": "Large datasets for Machine Reading have led to the development of neural models that cater to deeper language understanding compared to information retrieval tasks.", "labels": [], "entities": [{"text": "Machine Reading", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.8634123504161835}]}, {"text": "Different components in these neural architectures are intended to tackle different challenges.", "labels": [], "entities": []}, {"text": "As a first step towards achieving generalization across multiple domains, we attempt to understand and compare the peculiarities of existing end-to-end neu-ral models on the Stanford Question Answering Dataset (SQuAD) by performing quantitative as well as qualitative analysis of the results attained by each of them.", "labels": [], "entities": [{"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 174, "end_pos": 217, "type": "DATASET", "confidence": 0.8076363461358207}]}, {"text": "We observed that prediction errors reflect certain model-specific biases, which we further discuss in this paper.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine Reading is a task in which a model reads apiece of text and attempts to formally represent it or performs a downstream task like Question Answering (QA).", "labels": [], "entities": [{"text": "Machine Reading", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7746375799179077}, {"text": "Question Answering (QA)", "start_pos": 137, "end_pos": 160, "type": "TASK", "confidence": 0.8404476463794708}]}, {"text": "Neural approaches to the latter have gained a lot of prominence especially owing to the recent spur in developing and publicly releasing large datasets on Machine Reading and Comprehension (MRC).", "labels": [], "entities": [{"text": "Machine Reading and Comprehension (MRC)", "start_pos": 155, "end_pos": 194, "type": "TASK", "confidence": 0.8408316118376595}]}, {"text": "These datasets are created from different underlying sources such as web resources in MS MARCO (; trivia and web in QUASAR-S and QUASAR-T (), SearchQA (,; news articles in CNN/Daily Mail (, NewsQA ( and stories in NarrativeQA.", "labels": [], "entities": [{"text": "MS MARCO", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.8428401052951813}, {"text": "QUASAR-T", "start_pos": 129, "end_pos": 137, "type": "DATASET", "confidence": 0.7239589095115662}, {"text": "CNN/Daily Mail", "start_pos": 172, "end_pos": 186, "type": "DATASET", "confidence": 0.8201283365488052}, {"text": "NewsQA", "start_pos": 190, "end_pos": 196, "type": "DATASET", "confidence": 0.9555097222328186}]}, {"text": "Another common source is large unstructured text documents from Wikipedia such as in SQuAD (),) and WikiHop (.", "labels": [], "entities": []}, {"text": "These different sources implicitly affect the nature and properties of questions and answers in these datasets.", "labels": [], "entities": []}, {"text": "Based on the dataset, certain neural models capitalize on these biases while others are unable to.", "labels": [], "entities": []}, {"text": "The ability to generalize across different sources and domains is a desirable characteristic for any machine reading system.", "labels": [], "entities": []}, {"text": "Evaluating and analyzing systems on QA tasks can lead to insights for advancements in machine reading and natural language understanding, and have also previously worked on this.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 106, "end_pos": 136, "type": "TASK", "confidence": 0.6614578167597452}]}, {"text": "One of the first large MRC datasets (over 100k QA pairs) is the Stanford Question Answering Dataset (SQuAD) ().", "labels": [], "entities": [{"text": "MRC", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9498390555381775}, {"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 64, "end_pos": 107, "type": "DATASET", "confidence": 0.8639411330223083}]}, {"text": "For its collection, different sets of crowd-workers formulated questions and answers using passages obtained from \u223c500 Wikipedia articles.", "labels": [], "entities": []}, {"text": "The answer to each question is a span in the given passage, and many effective neural QA models have been developed for this dataset.", "labels": [], "entities": []}, {"text": "Our main focus in this work is to perform comparative subjective and empirical analysis of errors in answer predictions by four top performing models on the SQuAD leaderboard . We focused on Bi-Directional Attention Flow (BiDAF) (, Gated Self-Matching Networks (R-Net) (, Document Reader (DrQA), MultiParagraph Reading Comprehension (DocQA), and the Logistic Regression baseline model ( We mainly choose these models since they have comparable high performance on the evaluation metrics and it is easy to replicate their results due to availability of open source implementations.", "labels": [], "entities": [{"text": "SQuAD leaderboard", "start_pos": 157, "end_pos": 174, "type": "DATASET", "confidence": 0.8437474071979523}]}, {"text": "While we limit ourselves to in-domain analysis of the performance of these models on SQuAD in this paper, similar principles can be used to extend this work to study biases of combinations of different models on different datasets and thereby understand the generalization capabilities of these neural architectures.", "labels": [], "entities": []}, {"text": "The organization of the paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a comprehensive overview of the models that are compared in further sections.", "labels": [], "entities": []}, {"text": "Section 3 describes the different experiments we conducted, and discusses our observations.", "labels": [], "entities": []}, {"text": "In Section 4, we summarize our main conclusions from this work and describe our vision for the future.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained the aforementioned end-to-end neural models and compare their performance on the SQuAD development set which contains 10,570 question-answer pairs based on Wikipedia articles.", "labels": [], "entities": [{"text": "SQuAD development set", "start_pos": 92, "end_pos": 113, "type": "DATASET", "confidence": 0.8770142793655396}]}], "tableCaptions": [{"text": " Table 1.  The DocQA model gives the best overall perfor- mance which aligns well with our expectation, ow- ing to the usage of and improvements in the prior  mechanisms introduced in BiDAF and R-Net.", "labels": [], "entities": []}, {"text": " Table 1: Span and Sentence Level Performance", "labels": [], "entities": []}, {"text": " Table 2: Incorrect Answer Overlap (%)", "labels": [], "entities": [{"text": "Incorrect Answer Overlap", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.5027585526307424}]}]}