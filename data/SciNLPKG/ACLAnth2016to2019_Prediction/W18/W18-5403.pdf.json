{"title": [{"text": "Explaining non-linear Classifier Decisions within Kernel-based Deep Architectures", "labels": [], "entities": [{"text": "Explaining non-linear Classifier Decisions", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.848544642329216}]}], "abstractContent": [{"text": "Nonlinear methods such as deep neural networks achieve state-of-the-art performances in several semantic NLP tasks.", "labels": [], "entities": []}, {"text": "However episte-mologically transparent decisions are not provided as for the limited interpretability of the underlying acquired neural models.", "labels": [], "entities": []}, {"text": "In neural-based semantic inference tasks epistemologi-cal transparency corresponds to the ability of tracing back causal connections between the linguistic properties of a input instance and the produced classification output.", "labels": [], "entities": [{"text": "semantic inference tasks epistemologi-cal transparency", "start_pos": 16, "end_pos": 70, "type": "TASK", "confidence": 0.596742445230484}]}, {"text": "In this paper, we propose the use of a methodology , called Layerwise Relevance Propagation , over linguistically motivated neural ar-chitectures, namely Kernel-based Deep Archi-tectures (KDA), to guide argumentations and explanation inferences.", "labels": [], "entities": [{"text": "explanation inferences", "start_pos": 222, "end_pos": 244, "type": "TASK", "confidence": 0.7602976858615875}]}, {"text": "In such away, each decision provided by a KDA can be linked to real examples, linguistically related to the input instance: these can be used to motivate the network output.", "labels": [], "entities": []}, {"text": "Quantitative analysis shows that richer explanations about the semantic and syntagmatic structures of the examples characterize more convincing arguments in two tasks, i.e. question classification and semantic role labeling.", "labels": [], "entities": [{"text": "question classification", "start_pos": 173, "end_pos": 196, "type": "TASK", "confidence": 0.8115234375}, {"text": "semantic role labeling", "start_pos": 201, "end_pos": 223, "type": "TASK", "confidence": 0.6924852331479391}]}], "introductionContent": [{"text": "Nonlinear methods such as deep neural networks achieve state-of-the-art performances in several challenging problems, such as image classification or natural language processing (NLP).", "labels": [], "entities": [{"text": "image classification", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.8068334758281708}, {"text": "natural language processing (NLP)", "start_pos": 150, "end_pos": 183, "type": "TASK", "confidence": 0.7523406942685446}]}, {"text": "However the traditional AI criticism still holds: they are not epistemologically transparent, as for the limited interpretability of the neural inferences.", "labels": [], "entities": []}, {"text": "Ina question classification (QC) task, e.g. (), this is particularly evident.", "labels": [], "entities": [{"text": "question classification (QC) task", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.8408298542102178}]}, {"text": "The category describing the target of a request is relevant in question answering to optimize the later stages of search and answer detection, and its interpretation depends on a variety of semantic and syntactic properties of the question.", "labels": [], "entities": [{"text": "question answering", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.759242832660675}, {"text": "answer detection", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.7273822277784348}]}, {"text": "Epistemological transparency corresponds hereto the ability of tracing back the connections between linguistic properties of the input question and the proposed question category.", "labels": [], "entities": [{"text": "Epistemological transparency", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7330108284950256}]}, {"text": "An example-driven machine learning model should be able to provide causal relations between the input semantic aspect and the properties of the question.", "labels": [], "entities": []}, {"text": "For example, given the prediction \"What is the capital of Zimbabwe?\" refers to a Location, we would like the system to motivate it with a sentence such as: Since it seems similar to \"What is the capital of California?\" which also refers to a Location.", "labels": [], "entities": []}, {"text": "Notice how in neural learning, as for example in Multilayer Perceptrons, Long Short-Term Memory Networks,, or the more recent Attention-based Networks (, the network parameters have no clear conceptual counterpart.", "labels": [], "entities": []}, {"text": "Using the Layerwise Relevance Propagation (LRP) () approach, the classification decisions of a multilayer perceptron are decomposed backward across the network layers, and evidence about the contribution of individual input fragments (i.e. layer 0) to the final decision is gathered.", "labels": [], "entities": []}, {"text": "Evaluation against images (i.e. the MNIST and ILSVRC data sets) suggests that LRP activates meaningful associations between input and output fragments, and this corresponds to tracing back meaningful causal connections.", "labels": [], "entities": [{"text": "MNIST", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.913883626461029}, {"text": "ILSVRC data sets", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9037893414497375}]}, {"text": "In this paper, we propose the use of a similar mechanism over the linguistically motivated network architectures, as they have been recently proposed in: Kernel-based Deep network architectures aim at integrating syntactic/semantic information derived from the adoption of Tree Kernels) within neural-based learning.", "labels": [], "entities": []}, {"text": "Here, we show that the inferences of such architectures can be motivated by simply applying the LRP method, which allows to trace back causal associations between the semantic classification and the examples expressed by parse tree-based metrics.", "labels": [], "entities": []}, {"text": "Evaluation of the LRP algorithm to the problem of explaining the system decisions allows to demonstrate the meaningful impact of LRP on semantic transparency: users faced with explanations are better oriented to accept or reject the system decisions, thus improving the impact on the overall application accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 304, "end_pos": 312, "type": "METRIC", "confidence": 0.9644530415534973}]}, {"text": "In the rest of the paper, section 2 reports related works.", "labels": [], "entities": []}, {"text": "In section 3 we describe the Kernelbased Deep Architecture (KDA) while section 4 illustrates the details of LRP and how it connects to KDAs.", "labels": [], "entities": []}, {"text": "In section 5 we propose both a novel model to generate explanations of a network prediction and an evaluation methodology.", "labels": [], "entities": []}, {"text": "In section 6 we provide experimental evidences of the overall system's effectiveness against two semantic tasks, question classification and frame-based argument classification in the semantic role labeling chain.", "labels": [], "entities": [{"text": "question classification", "start_pos": 113, "end_pos": 136, "type": "TASK", "confidence": 0.7672132253646851}, {"text": "frame-based argument classification", "start_pos": 141, "end_pos": 176, "type": "TASK", "confidence": 0.6418717801570892}, {"text": "semantic role labeling chain", "start_pos": 184, "end_pos": 212, "type": "TASK", "confidence": 0.7191413193941116}]}, {"text": "Lastly, in section 7 conclusions are derived.", "labels": [], "entities": []}], "datasetContent": [{"text": "The effectiveness of the proposed approach has been measured against two different semantic processing tasks,i.e. question classification and argument classification in semantic role labeling.", "labels": [], "entities": [{"text": "question classification", "start_pos": 114, "end_pos": 137, "type": "TASK", "confidence": 0.7148602604866028}, {"text": "argument classification", "start_pos": 142, "end_pos": 165, "type": "TASK", "confidence": 0.7302889376878738}, {"text": "semantic role labeling", "start_pos": 169, "end_pos": 191, "type": "TASK", "confidence": 0.6568276584148407}]}, {"text": "The Nystrom projection has been implemented in the KeLP framework (Filice et al., 2018) 1 , the neural network and LRP have been implemented in Tensorflow 2 , with 1 and 2 hidden layers, respectively, whose dimensionality corresponds to the number of involved Nystrom landmarks (500 and 200, re- 0.05 0.95  spectively, randomly selected 3 ), and the adoption of dropout regularization in hidden and final layers.", "labels": [], "entities": [{"text": "KeLP framework", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.900155633687973}]}, {"text": "For both tasks, hyper-parameters have been optimized via grid-search.", "labels": [], "entities": []}, {"text": "The Adam optimizer has been applied to minimize the cross-entropy loss function, with a multi-epoch (500) training, each fed with batches of size 256.", "labels": [], "entities": []}, {"text": "We adopted an early stop strategy, where the best model was selected according to the performance over the development set.", "labels": [], "entities": []}, {"text": "For evaluating our explanation method, we defined five quality categories and associated them to values for the posteriori probability P (C|s, e), as shown in.", "labels": [], "entities": []}, {"text": "We gathered into explanation datasets hundreds of explanations from the three models for each task and presented them to a pool of annotators (further details in related subsections) for independent labeling; annotators had no information of the correctness of the system emissions but just knowledge about the dataset entropy.", "labels": [], "entities": []}, {"text": "We addressed their consensus by measuring a weighted Cohen's Kappa.", "labels": [], "entities": [{"text": "Cohen's Kappa", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.6411395569642385}]}], "tableCaptions": [{"text": " Table 1: Posterior probabilities w.r.t. quality categories", "labels": [], "entities": []}, {"text": " Table 2: Weights for the Cohen's Kappa \u03ba w statistics", "labels": [], "entities": [{"text": "Weights", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9526705145835876}]}, {"text": " Table 1. We gathered into explana- tion datasets hundreds of explanations from the  three models for each task and presented them to  a pool of annotators (further details in related sub- sections) for independent labeling; annotators had  no information of the correctness of the system  emissions but just knowledge about the dataset en- tropy. We addressed their consensus by measuring  a weighted Cohen's Kappa.", "labels": [], "entities": []}]}