{"title": [{"text": "Can You Spot the Semantic Predicate in this Video?", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a method to improve human activity recognition in video by leveraging semantic information about the target activities from an expert-defined linguistic resource, VerbNet.", "labels": [], "entities": [{"text": "human activity recognition", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.6293984850247701}, {"text": "VerbNet", "start_pos": 174, "end_pos": 181, "type": "DATASET", "confidence": 0.9363596439361572}]}, {"text": "Our hypothesis is that activities that share similar event semantics, as defined by the semantic predicates of VerbNet, will be more likely to share some visual components.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.9146593809127808}]}, {"text": "We use a deep convolutional neural network approach as a baseline and incorporate linguistic information from VerbNet through multi-task learning.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.9194629192352295}]}, {"text": "We present results of experiments showing the added information has negligible impact on recognition performance.", "labels": [], "entities": [{"text": "recognition", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.9581283330917358}]}, {"text": "We discuss how this maybe because the lexical semantic information defined by VerbNet is generally not visually salient given the video processing approach used here, and how we may handle this in future approaches.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9293167591094971}]}], "introductionContent": [{"text": "Human activity recognition is a crucial component of comprehensive, multimodal event detection and identification as well as a prerequisite for more complex tasks, such as establishing timelines from video and video caption generation.", "labels": [], "entities": [{"text": "Human activity recognition", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6293900807698568}, {"text": "multimodal event detection and identification", "start_pos": 68, "end_pos": 113, "type": "TASK", "confidence": 0.6957727491855621}, {"text": "video caption generation", "start_pos": 210, "end_pos": 234, "type": "TASK", "confidence": 0.7566501299540201}]}, {"text": "In this work, we attempt to improve the performance of activity recognition in video by considering the event semantics associated with the activity types.", "labels": [], "entities": [{"text": "activity recognition", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.7644563317298889}]}, {"text": "have done this for images by leveraging a large dataset that is thoroughly labeled with the activity performed in each image, the actors involved, and the roles the actors play in the activity.", "labels": [], "entities": []}, {"text": "While their method works well, it comes at the cost of obtaining and labeling the dataset.", "labels": [], "entities": []}, {"text": "In this work, we eschew the use of an expensive labeled dataset and instead leverage the lexical semantic information found in VerbNet (VN) and only a small amount of manually annotated data.", "labels": [], "entities": [{"text": "VerbNet (VN)", "start_pos": 127, "end_pos": 139, "type": "DATASET", "confidence": 0.8151925355195999}]}, {"text": "Our hypothesis is that activities that share similar event semantics will be more likely to share some visual components.", "labels": [], "entities": []}, {"text": "To begin to explore this hypothesis, we must first select the type and specificity of semantics that should be coupled with an activity.", "labels": [], "entities": []}, {"text": "Here, we use VN to obtain semantic representations in the form of composed predicates, which apply to classes of verbs denoting more or less similar event types.", "labels": [], "entities": []}, {"text": "The semantic representations therefore provide a level of generalization over somewhat distinct events.", "labels": [], "entities": []}, {"text": "This extra information comes at the activity level rather than the sample level, and thus provides relatively weak supervision.", "labels": [], "entities": []}, {"text": "Nonetheless, we feel our hypothesis intuitively holds promise and, if supported, would enable efficient improvement in activity recognition with less training data.", "labels": [], "entities": [{"text": "activity recognition", "start_pos": 119, "end_pos": 139, "type": "TASK", "confidence": 0.8104060888290405}]}, {"text": "Specifically, the ability to detect similar visual components across an event type could allow for generalizing from the recognition of one activity type (e.g., baseball pitch) to another that is semantically similar (e.g., throw discus).", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the two-stream convolutional network approach of Simonyan and Zisserman (2014) as a baseline model.", "labels": [], "entities": []}, {"text": "In this model, two neural networks are trained to classify videos.", "labels": [], "entities": []}, {"text": "The first is trained on the raw frames and the second on optical flow features extracted from the frames.", "labels": [], "entities": []}, {"text": "Both networks are trained to classify the activities from small portions of the video (the visible network is trained on single frames, while the motion network is trained on five-frame segments).", "labels": [], "entities": []}, {"text": "To test a video, 25 equally spaced frames are passed through the visual network and 25 equally spaced five-frame segments are passed through the motion network.", "labels": [], "entities": []}, {"text": "The video is then classified as the activity with the highest average probability.", "labels": [], "entities": []}, {"text": "We alter the approach of Simonyan and Zisserman (2014) by injecting information from VN with multi-task learning.", "labels": [], "entities": []}, {"text": "Multi-task learning is the process of simultaneously training for several objectives.", "labels": [], "entities": [{"text": "Multi-task learning", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8076126873493195}]}, {"text": "In our case, we train the networks to classify the VN class or meta-class the activities belong to in addition to the categories of the activities themselves.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sample of annotation examples showing which UCF activities correspond to which event type.  The \"maybe\" indicates some Bowling video clips that include the ball hitting pins. Annotations were  completed for all 101 UCF activities, only six of these are shown here.", "labels": [], "entities": []}, {"text": " Table 2: Performance comparison of our method to the baseline method (Baseline) which does not use  any VN information. We evaluate our method using all four VN objectives at once (All), all at once  but doubly weighted (All 2x Weight), and with one VN objective at a time (Vehicle Motion, Throwing,  Hitting, Group Motion). All values denote the classification accuracy on the 3783 videos.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 363, "end_pos": 371, "type": "METRIC", "confidence": 0.9366734027862549}]}]}