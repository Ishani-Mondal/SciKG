{"title": [{"text": "Controlling Personality-Based Stylistic Variation with Neural Natural Language Generators", "labels": [], "entities": []}], "abstractContent": [{"text": "Natural language generators for task-oriented dialogue must effectively realize system dialogue actions and their associated semantics.", "labels": [], "entities": []}, {"text": "In many applications, it is also desirable for generators to control the style of an utterance.", "labels": [], "entities": []}, {"text": "To date, work on task-oriented neural generation has primarily focused on semantic fidelity rather than achieving stylistic goals, while work on style has been done in contexts where it is difficult to measure content preservation.", "labels": [], "entities": [{"text": "task-oriented neural generation", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.8242000540097555}]}, {"text": "Here we present three different sequence-to-sequence models and carefully test how well they disentangle content and style.", "labels": [], "entities": []}, {"text": "We use a statistical generator , PERSONAGE, to synthesize anew corpus of over 88,000 restaurant domain utterances whose style varies according to models of personality, giving us total control over both the semantic content and the stylistic variation in the training data.", "labels": [], "entities": [{"text": "PERSONAGE", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9045032262802124}]}, {"text": "We then vary the amount of explicit stylistic supervision given to the three models.", "labels": [], "entities": []}, {"text": "We show that our most explicit model can simultaneously achieve high fidelity to both semantic and stylistic goals: this model adds a context vector of 36 stylistic parameters as input to the hidden state of the en-coder at each time step, showing the benefits of explicit stylistic supervision, even when the amount of training data is large.", "labels": [], "entities": []}], "introductionContent": [{"text": "The primary aim of natural language generators (NLGs) for task-oriented dialogue is to effectively realize system dialogue actions and their associated content parameters.", "labels": [], "entities": []}, {"text": "This requires training data that allows the NLG to learn how to map semantic representations for system dialogue acts to one or more possible outputs (see,).", "labels": [], "entities": []}, {"text": "Because neural generators often make semantic errors such as deleting, repeating or hallucinating content, to date previous work on task-oriented neural generation has primarily focused on faithfully rendering the meaning of the system's dialogue act).", "labels": [], "entities": []}, {"text": "However, in many applications it is also desirable for generators to control the style of an utterance independently of its content.", "labels": [], "entities": []}, {"text": "For example, in, the first output uses more formal language and complex syntactic structures, as one might see in written language, while the other uses simpler syntax and pragmatic markers characteristic of oral language.", "labels": [], "entities": []}, {"text": "In this paper, we create several different sequenceto-sequence models and compare how well they can disentangle content and style.", "labels": [], "entities": []}, {"text": "Controlling the style of the output requires disentangling the content from the style, but previous neural models aimed at achieving stylistic goals have not focused on task-oriented dialogue where specific semantic attributes and values must be communicated (as per the MR in), and where semantic fi- Let's see what we can find on Fitzbillies.", "labels": [], "entities": []}, {"text": "I see, well it is a pub with a decent rating, it isn't kid friendly and it's moderately priced near The Sorrento and an Italian restaurant in riverside.", "labels": [], "entities": [{"text": "The Sorrento", "start_pos": 100, "end_pos": 112, "type": "DATASET", "confidence": 0.9183365702629089}, {"text": "riverside", "start_pos": 142, "end_pos": 151, "type": "DATASET", "confidence": 0.9491333961486816}]}, {"text": "UNCONSCIENTIOUS Oh god yeah, I don't know.", "labels": [], "entities": [{"text": "UNCONSCIENTIOUS", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9416877031326294}]}, {"text": "Fitzbillies is a pub with a decent rating, also it is moderately priced near The Sorrento and an Italian place in riverside and it isn't kid friendly.", "labels": [], "entities": [{"text": "Fitzbillies", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9613717794418335}, {"text": "The Sorrento", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.8043291866779327}]}, {"text": "EXTRAVERT Basically, Fitzbillies is an Italian place near The Sorrento and actually moderately priced in riverside, it has a decent rating, it isn't kid friendly and it's a pub, you know.", "labels": [], "entities": [{"text": "EXTRAVERT", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9023376107215881}, {"text": "Fitzbillies", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.9324997067451477}, {"text": "The Sorrento", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9190423488616943}]}, {"text": "One of the main challenges is the lack of parallel corpora realizing the same content with different styles.", "labels": [], "entities": []}, {"text": "Thus we create a large, novel parallel corpus with specific style parameters and specific semantics, by using an existing statistical generator, PERSONAGE, to synthesize over 88,000 utterances in the restaurant domain that vary in style according to psycholinguistic models of personality.", "labels": [], "entities": [{"text": "PERSONAGE", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9546930193901062}]}, {"text": "PERSONAGE can generate a very large number of stylistic variations for any given dialogue act, thus yielding, to our knowledge, the largest style-varied NLG training corpus in existence.", "labels": [], "entities": []}, {"text": "The strength of this new corpus is that: (1) we can use the PERSONAGE generator to generate as much training data as we want; (2) it allows us to systematically vary a specific set of stylistic parameters and the network architectures; (3) it allows us to systematically test the ability of different models to generate outputs that faithfully realize both the style and content of the training data.", "labels": [], "entities": []}, {"text": "We develop novel neural models that vary the amount of explicit stylistic supervision given to the network, and we explore, for the first time, explicit control of multiple interacting stylistic parameters.", "labels": [], "entities": []}, {"text": "We show that the no-supervision (NO-SUP) model, a baseline sequence-to-sequence model, produces semantically correct outputs, but We leave a detailed review of related work to Section 6.", "labels": [], "entities": []}, {"text": "Our stylistic variation for NLG corpus is available at: nlds.soe.ucsc.edu/stylistic-variation-nlg 3 Section 4 quantifies the naturalness of PERSONAGE outputs.", "labels": [], "entities": [{"text": "NLG corpus", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.8234652280807495}]}, {"text": "eliminates much of the stylistic variation that it saw in the training data.", "labels": [], "entities": []}, {"text": "MODEL TOKEN provides minimal supervision by allocating a latent variable in the encoding as a label for each style, similar to the use of language labels in machine translation.", "labels": [], "entities": [{"text": "MODEL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5278134942054749}, {"text": "TOKEN", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.7059453129768372}, {"text": "machine translation", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.7181233465671539}]}, {"text": "This model learns to generate coherent and stylistically varied output without explicit exposure to language rules, but makes more semantic errors.", "labels": [], "entities": []}, {"text": "MODEL CONTEXT adds another layer to provide an additional encoding of individual stylistic parameters to the network.", "labels": [], "entities": [{"text": "MODEL CONTEXT", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.6902927160263062}]}, {"text": "We show that it performs best on both measures of semantic fidelity and stylistic variation.", "labels": [], "entities": []}, {"text": "The results suggest that neural architectures can benefit from explicit stylistic supervision, even with a large training set.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Percentage of the MRs in training and  test in terms of number of attributes in the MR", "labels": [], "entities": []}, {"text": " Table 3. The parameters  for each reference text are encoded as a boolean  vector, and a feed-forward network is added as a  context encoder, taking the vector as input to the  hidden state of the encoder and making the param- eters available at every time step to a multiplica- tive attention unit. The activations of the fully  connected nodes are represented as an additional", "labels": [], "entities": []}, {"text": " Table 4: Automated Metric Evaluation", "labels": [], "entities": [{"text": "Automated Metric Evaluation", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.6326677799224854}]}, {"text": " Table 5: Ratio of Model Errors by Personality", "labels": [], "entities": []}, {"text": " Table 6: Shannon Text Entropy", "labels": [], "entities": [{"text": "Shannon Text Entropy", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6954137086868286}]}, {"text": " Table 7: Correlations between PERSONAGE and  models for pragmatic markers in", "labels": [], "entities": [{"text": "PERSONAGE", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.884656548500061}]}, {"text": " Table 8: Correlations between PERSONAGE and  models for aggregation operations in", "labels": [], "entities": []}, {"text": " Table 9: Percentage of Correct Items and Average  Ratings and Naturalness Scores for Each Person- ality (PERSONAGE vs. MODEL CONTEXT)", "labels": [], "entities": [{"text": "PERSONAGE", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9796145558357239}]}, {"text": " Table 10: Multiple-Personality Generation Out- put based on DISAGREEABLE", "labels": [], "entities": [{"text": "Multiple-Personality Generation", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.8002806007862091}, {"text": "DISAGREEABLE", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.5614604353904724}]}]}