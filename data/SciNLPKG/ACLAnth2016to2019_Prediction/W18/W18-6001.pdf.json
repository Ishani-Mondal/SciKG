{"title": [{"text": "Assessing the Impact of Incremental Error Detection and Correction. A Case Study on the Italian Universal Dependency Treebank", "labels": [], "entities": [{"text": "Incremental Error Detection and Correction", "start_pos": 24, "end_pos": 66, "type": "TASK", "confidence": 0.6682805180549621}, {"text": "Italian Universal Dependency Treebank", "start_pos": 88, "end_pos": 125, "type": "DATASET", "confidence": 0.925973430275917}]}], "abstractContent": [{"text": "Detection and correction of errors and inconsistencies in \"gold treebanks\" are becoming more and more central topics of corpus annotation.", "labels": [], "entities": [{"text": "corpus annotation", "start_pos": 120, "end_pos": 137, "type": "TASK", "confidence": 0.7038163840770721}]}, {"text": "The paper illustrates anew incremental method for enhancing treebanks, with particular emphasis on the extension of error patterns across different textual genres and registers.", "labels": [], "entities": []}, {"text": "Impact and role of corrections have been assessed in a dependency parsing experiment carried outwith four different parsers, whose results are promising.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8107506036758423}]}, {"text": "For both evaluation datasets, the performance of parsers increases , in terms of the standard LAS and UAS measures and of a more focused measure taking into account only relations involved in error patterns, and at the level of individual dependencies .", "labels": [], "entities": [{"text": "LAS", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.8123149871826172}, {"text": "UAS", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.4548241198062897}]}], "introductionContent": [{"text": "Over the last years, many approaches to detect errors and inconsistencies in treebanks have been devised.", "labels": [], "entities": []}, {"text": "They can be categorized in two main groups, depending on whether the proposed quality check procedure relies on heuristic patterns) or on statistical methods).", "labels": [], "entities": []}, {"text": "More recently, the Universal Dependencies (UD) initiative has yielded a renewed interest as shown by the methods and tools introduced by;;.", "labels": [], "entities": [{"text": "Universal Dependencies (UD)", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.7253452479839325}]}, {"text": "A number of reasons prompted the importance of these methods: they can be useful to check the internal coherence of the newly created treebanks with respect to other treebanks created fora same language or to the annotation guidelines.", "labels": [], "entities": []}, {"text": "The risk of inconsistencies or errors is considerable if we consider that 70% of the released UD treebanks originate from a conversion process and only 29% of them has been manually revised after automatic conversion.", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.8220390975475311}]}, {"text": "In this paper, we extend the method proposed by for error detection and correction in \"gold treebanks\" and we evaluate its impact on parsing results.", "labels": [], "entities": [{"text": "error detection and correction", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.7203104123473167}, {"text": "parsing", "start_pos": 133, "end_pos": 140, "type": "TASK", "confidence": 0.9773010611534119}]}], "datasetContent": [{"text": "In order to test the impact of the result of our incremental treebank enhancement approach, we compared the dependency parsing results achieved using IUDT versions 2.0 vs 2.3 for training.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.6972822993993759}]}, {"text": "Although the overall size of IUDT changed across the 2.0 and 2.3 versions, we used two equivalent training sets of 265,554 tokens to train the parsers, containing exactly the same texts but different annotations.", "labels": [], "entities": [{"text": "IUDT", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.6892910599708557}]}, {"text": "For both sets of experiments, parser performances were tested against a dev(elopment) set of 10,490 tokens and a test set of 7,545 tokens, differing again at the annotation level only.", "labels": [], "entities": []}, {"text": "Four different parsers were selected for the experiments, differing at the level of the used parsing algorithm.", "labels": [], "entities": []}, {"text": "The configurations of the parsers were kept the same across all experiments.", "labels": [], "entities": []}, {"text": "DeSR MLP is a transition-based parser that uses a Multi-Layer Perceptron), selected as representative of transition-based parsers.", "labels": [], "entities": [{"text": "DeSR MLP", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8358557522296906}]}, {"text": "The best configuration for UD, which uses a rich set of features including third order ones and a graph score, is described in.", "labels": [], "entities": []}, {"text": "We trained it on 300 hidden variables, with a learning rate of 0.01, and early stopping when validation accuracy reaches 99.5%.) is a graphbased parser that uses third-order feature models and a specialized accelerated dual decomposition algorithm for making non-projective parsing computationally feasible.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.8786242604255676}]}, {"text": "It was used in configuration \"full\", enabling all third-order features.", "labels": [], "entities": []}, {"text": "Mate is a graph-based parser that uses passive aggressive perceptron and exploits a rich feature set.", "labels": [], "entities": []}, {"text": "Among the configurable parameters, we set to 25 the numbers of iterations.", "labels": [], "entities": []}, {"text": "Mate was used in the pure graph version.", "labels": [], "entities": []}, {"text": "UDPipe is a trainable pipeline for tokenization, tagging, lemmatization and dependency parsing.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9297018647193909}, {"text": "tokenization", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.9647179841995239}, {"text": "dependency parsing", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.8005302548408508}]}, {"text": "The transition-based parser provided with the pipeline is based on a non-recurrent neural network, with just one hidden layer, with locally normalized scores.", "labels": [], "entities": []}, {"text": "We used the parser in the basic configuration provided for the CoNLL 2017 Shared Task on Dependency Parsing.", "labels": [], "entities": [{"text": "CoNLL 2017 Shared Task on Dependency Parsing", "start_pos": 63, "end_pos": 107, "type": "TASK", "confidence": 0.8278671247618539}]}, {"text": "The performance of parsers was assessed in terms of the standard evaluation metrics of dependency parsing, i.e. Labeled Attachment Score (LAS) and Unlabeled Attachment Score (UAS).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7834779322147369}, {"text": "Labeled Attachment Score (LAS)", "start_pos": 112, "end_pos": 142, "type": "METRIC", "confidence": 0.7770131379365921}, {"text": "Unlabeled Attachment Score (UAS)", "start_pos": 147, "end_pos": 179, "type": "METRIC", "confidence": 0.7792113423347473}]}, {"text": "To assess the impact of the correction of systematic errors, we devised anew metric inspired by the Content-word Labeled Attachment Score (CLAS) introduced for the).", "labels": [], "entities": [{"text": "Content-word Labeled Attachment Score (CLAS)", "start_pos": 100, "end_pos": 144, "type": "METRIC", "confidence": 0.7022091363157544}]}, {"text": "Similarly to CLAS, the new metric focuses on a selection of dependencies: whereas CLAS focuses on relations between content words only, our metric is computed by only considering those dependencies directly or indirectly involved in the pattern-based error correction process.", "labels": [], "entities": [{"text": "pattern-based error correction", "start_pos": 237, "end_pos": 267, "type": "TASK", "confidence": 0.6795619328816732}]}, {"text": "reports the list of UD dependencies involved in error patterns: it includes both modified and modifying dependencies occurring in the rewriting rules formalizing error patterns.", "labels": [], "entities": []}, {"text": "Henceforth, we will refer to this metric as Selected Labeled Attachment Score (SLAS).", "labels": [], "entities": [{"text": "Selected Labeled Attachment Score (SLAS)", "start_pos": 44, "end_pos": 84, "type": "METRIC", "confidence": 0.7464951617377145}]}], "tableCaptions": [{"text": " Table 1: Evaluation of the parsers against the IUDT test and development sets version 2.0 and 2.3.", "labels": [], "entities": [{"text": "IUDT test", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.9363734722137451}]}, {"text": " Table 2: Statistics of individual dependencies involved in an error pattern in the test and development sets of  IUDT 2.0 and 2.3 (gold). sys refers to the number of predicted dependencies by the MATE parser and correct to  the correct predictions.", "labels": [], "entities": [{"text": "IUDT 2.0", "start_pos": 114, "end_pos": 122, "type": "DATASET", "confidence": 0.833111584186554}]}, {"text": " Table 3: F1 scores and differences for a selection of individual dependencies involved in error patterns by the  MATE parser trained on IUDT 2.0 and 2.3.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9676772058010101}, {"text": "MATE", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.6883960366249084}, {"text": "IUDT 2.0", "start_pos": 137, "end_pos": 145, "type": "DATASET", "confidence": 0.8634013831615448}]}]}