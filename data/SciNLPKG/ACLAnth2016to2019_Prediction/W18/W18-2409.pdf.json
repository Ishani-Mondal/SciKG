{"title": [{"text": "Report of NEWS 2018 Named Entity Transliteration Shared Task", "labels": [], "entities": [{"text": "Entity Transliteration Shared", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.6112540165583292}]}], "abstractContent": [{"text": "This report presents the results from the Named Entity Transliteration Shared Task conducted as part of The Seventh Named Entities Workshop (NEWS 2018) held at ACL 2018 in Melbourne, Australia.", "labels": [], "entities": [{"text": "Named Entity Transliteration Shared Task", "start_pos": 42, "end_pos": 82, "type": "TASK", "confidence": 0.7150318682193756}, {"text": "Named Entities Workshop (NEWS 2018) held at ACL 2018 in Melbourne, Australia", "start_pos": 116, "end_pos": 192, "type": "TASK", "confidence": 0.5990555147329967}]}, {"text": "Similar to previous editions of NEWS, the Shared Task featured 19 tasks on proper name transliteration, including 13 different languages and two different Japanese scripts.", "labels": [], "entities": [{"text": "proper name transliteration", "start_pos": 75, "end_pos": 102, "type": "TASK", "confidence": 0.6811118622620901}]}, {"text": "A total of 6 teams from 8 different institutions participated in the evaluation, submitting 424 runs, involving different transliteration methodologies.", "labels": [], "entities": []}, {"text": "Four performance metrics were used to report the evaluation results.", "labels": [], "entities": []}, {"text": "The NEWS shared task on machine transliteration has successfully achieved its objectives by providing a common ground for the research community to conduct comparative evaluations of state-of-the-art technologies that will benefit the future research and development in this area.", "labels": [], "entities": [{"text": "NEWS", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7496576309204102}, {"text": "machine transliteration", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.589767649769783}]}], "introductionContent": [{"text": "Names play an important role in the performance of most natural language processing and information retrieval applications.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.6752381920814514}]}, {"text": "They are also critical in cross-lingual applications such as machine translation and cross-language information retrieval, as it has been shown that system performance correlates positively with the quality of name conversion across languages).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8421412706375122}, {"text": "cross-language information retrieval", "start_pos": 85, "end_pos": 121, "type": "TASK", "confidence": 0.6834030946095785}, {"text": "name conversion", "start_pos": 210, "end_pos": 225, "type": "TASK", "confidence": 0.7101905941963196}]}, {"text": "Bilingual dictionaries constitute the traditional source of information for name conversion across languages, however they offer very limited support as inmost languages names are continuously emerging and evolving.", "labels": [], "entities": [{"text": "name conversion", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.8018370568752289}]}, {"text": "All of the above points to the critical need for robust machine transliteration methods and systems.", "labels": [], "entities": []}, {"text": "Significant efforts has been conducted by the research community to address the problem of machine transliteration).", "labels": [], "entities": [{"text": "machine transliteration", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.751402735710144}]}, {"text": "These efforts fall into three main categories: grapheme-based, phoneme-based and hybrid methods.", "labels": [], "entities": []}, {"text": "Grapheme based methods () treat transliteration as a direct orthographic mapping and only uses orthographyrelated features while phoneme-based methods) make use of phonetic correspondences to generate the transliteration.", "labels": [], "entities": []}, {"text": "The hybrid approach refers to the combination of several different models or knowledge sources to support the transliteration generation process.", "labels": [], "entities": [{"text": "transliteration generation", "start_pos": 110, "end_pos": 136, "type": "TASK", "confidence": 0.8675634562969208}]}, {"text": "Recently, neural network approaches have been explored with varying successes, depending on the size of the training data.", "labels": [], "entities": []}, {"text": "The first machine transliteration shared task () was organized and conducted as part of NEWS 2009 at ACL-IJCNLP 2009.", "labels": [], "entities": [{"text": "machine transliteration shared task", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.7798252552747726}, {"text": "NEWS 2009 at ACL-IJCNLP 2009", "start_pos": 88, "end_pos": 116, "type": "DATASET", "confidence": 0.741490113735199}]}, {"text": "It was the first time that common benchmarking data in diverse language pairs was provided for evaluating state-of-the-art machine transliteration.", "labels": [], "entities": []}, {"text": "While the focus of the 2009 shared task was on establishing the quality metrics and on setting up a baseline for transliteration quality based on those metrics, the 2010 shared task () fo-cused on expanding the scope of the transliteration generation task to about a dozen languages and on exploring the quality of the task depending on the direction of transliteration.", "labels": [], "entities": [{"text": "transliteration generation task", "start_pos": 224, "end_pos": 255, "type": "TASK", "confidence": 0.8263254563013712}]}, {"text": "In NEWS 2011 (), the focus was on significantly increasing the hand-crafted parallel corpora of named entities to include 14 different language pairs from 11 language families, and on making them available as the common dataset for the shared task.", "labels": [], "entities": [{"text": "NEWS 2011", "start_pos": 3, "end_pos": 12, "type": "DATASET", "confidence": 0.9405198097229004}]}, {"text": "The NEWS 2018 Shared Task on Named Entity Transliteration has been a continued effort for evaluating machine transliteration performance following the NEWS edition of 2012 (), 2015 () and 2016 (.", "labels": [], "entities": [{"text": "NEWS 2018 Shared Task on Named Entity Transliteration", "start_pos": 4, "end_pos": 57, "type": "TASK", "confidence": 0.7583300769329071}]}, {"text": "In this paper, we present in full detail the results of NEWS 2018 Named Entity Transliteration Shared Task.", "labels": [], "entities": [{"text": "NEWS 2018 Named Entity Transliteration Shared Task", "start_pos": 56, "end_pos": 106, "type": "TASK", "confidence": 0.7187954527991158}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides as short review of the main characteristics of the machine transliteration task and the corpora used for it.", "labels": [], "entities": []}, {"text": "Section 3 reviews the four metrics used for the evaluations.", "labels": [], "entities": []}, {"text": "Section 4 reports specific details about participation in the shared task, and section 5 presents and discusses the evaluation results.", "labels": [], "entities": []}, {"text": "Finally, section 6 presents our main conclusions and future plans.", "labels": [], "entities": []}], "datasetContent": [{"text": "The participants have been asked to submit standard and, optionally, non-standard runs.", "labels": [], "entities": []}, {"text": "One of the standard runs must be named as the primary submission, which was the one used for the performance summary.", "labels": [], "entities": []}, {"text": "Each run must contain a ranked list of up to ten candidate transliterations for each source name.", "labels": [], "entities": []}, {"text": "The submitted results are compared to the ground truth (reference transliterations) using four evaluation metrics capturing different aspects of transliteration performance.", "labels": [], "entities": []}, {"text": "The four considered evaluation metrics are \u2022 Word Accuracy in Top-1 (ACC), \u2022 Fuzziness in Top-1 (Mean F-score) (Powers 2011), \u2022 Mean Reciprocal Rank (MRR)), and \u2022 Mean Average Precision (MAP ref )).", "labels": [], "entities": [{"text": "Word Accuracy in Top-1 (ACC)", "start_pos": 45, "end_pos": 73, "type": "METRIC", "confidence": 0.7353659697941372}, {"text": "Fuzziness in Top-1 (Mean F-score)", "start_pos": 77, "end_pos": 110, "type": "METRIC", "confidence": 0.7696357028824943}, {"text": "Mean Reciprocal Rank (MRR))", "start_pos": 128, "end_pos": 155, "type": "METRIC", "confidence": 0.9359977046648661}, {"text": "Mean Average Precision (MAP ref ))", "start_pos": 163, "end_pos": 197, "type": "METRIC", "confidence": 0.9605239459446498}]}, {"text": "In the next subsections, we present a brief description of the four considered evaluation metrics.", "labels": [], "entities": []}], "tableCaptions": []}