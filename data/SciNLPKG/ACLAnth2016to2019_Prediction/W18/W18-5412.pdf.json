{"title": [{"text": "Can LSTM Learn to Capture Agreement? The Case of Basque", "labels": [], "entities": [{"text": "LSTM Learn to Capture Agreement", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.5632276952266693}]}], "abstractContent": [{"text": "Sequential neural networks models are powerful tools in a variety of Natural Language Processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "The sequential nature of these models raises the questions: to what extent can these models implicitly learn hierarchical structures typical to human language, and what kind of grammatical phenomena can they acquire?", "labels": [], "entities": []}, {"text": "We focus on the task of agreement prediction in Basque, as a case study fora task that requires implicit understanding of sentence structure and the acquisition of a complex but consistent morphological system.", "labels": [], "entities": [{"text": "agreement prediction in Basque", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.8163329660892487}]}, {"text": "Analyzing experimental results from two syntactic prediction tasks-verb number prediction and suffix recovery-we find that sequential models perform worse on agreement prediction in Basque than one might expect on the basis of a previous agreement prediction work in English.", "labels": [], "entities": [{"text": "syntactic prediction tasks-verb number prediction", "start_pos": 40, "end_pos": 89, "type": "TASK", "confidence": 0.8539879322052002}, {"text": "suffix recovery-we", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7112446874380112}, {"text": "agreement prediction", "start_pos": 158, "end_pos": 178, "type": "TASK", "confidence": 0.7399318963289261}]}, {"text": "Tentative findings based on diagnostic classifiers suggest the network makes use of local heuristics as a proxy for the hierarchical structure of the sentence.", "labels": [], "entities": []}, {"text": "We propose the Basque agreement prediction task as challenging benchmark for models that attempt to learn regularities inhuman language.", "labels": [], "entities": [{"text": "Basque agreement prediction task", "start_pos": 15, "end_pos": 47, "type": "TASK", "confidence": 0.812386155128479}]}], "introductionContent": [{"text": "In recent years, recurrent neural network (RNN) models have emerged as a powerful architecture fora variety of NLP tasks.", "labels": [], "entities": []}, {"text": "In particular, gated versions, such as Long Short-Term Networks (LSTMs)) and Gated Recurrent Units (GRU) () achieve state-of-the-art results in tasks such as language modeling, parsing, and machine translation.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 158, "end_pos": 175, "type": "TASK", "confidence": 0.7288158386945724}, {"text": "parsing", "start_pos": 177, "end_pos": 184, "type": "TASK", "confidence": 0.9290927052497864}, {"text": "machine translation", "start_pos": 190, "end_pos": 209, "type": "TASK", "confidence": 0.8018636405467987}]}, {"text": "RNNs were shown to be able to capture longterm dependencies and statistical regularities in input sequences).", "labels": [], "entities": []}, {"text": "An adequate evaluation of the ability of RNNs to capture syntactic structure requires a use of established benchmarks.", "labels": [], "entities": []}, {"text": "A common approach is the use of an annotated corpus to learn an explicit syntax-oriented task, such as parsing or shallow parsing ( . While such an approach does evaluate the ability of the model to learn syntax, it has several drawbacks.", "labels": [], "entities": [{"text": "parsing or shallow parsing", "start_pos": 103, "end_pos": 129, "type": "TASK", "confidence": 0.6301724910736084}]}, {"text": "First, the annotation process relies on human experts and is thus demanding in term of resources.", "labels": [], "entities": []}, {"text": "Second, by its very nature, training a model on such a corpus evaluates it on a human-dictated notion of grammatical structure, and is tightly coupled to a linguistic theory.", "labels": [], "entities": []}, {"text": "Lastly, the supervised training process on such a corpus provides the network with explicit grammatical labels (e.g. a parse tree).", "labels": [], "entities": []}, {"text": "While this is sometimes desirable, in some instances we would like to evaluate the ability of the model to implicitly acquire hierarchical representations.", "labels": [], "entities": []}, {"text": "Alternatively, one can train language model (LM) to model the probability distribution of a language, and use common measures for quality such as perplexity as an indication of the model's ability to capture regularities in language.", "labels": [], "entities": []}, {"text": "While this approach does not suffer from the above discussed drawbacks, it conflates syntactical capacity with other factors such as world knowledge and frequency of lexical items.", "labels": [], "entities": []}, {"text": "Furthermore, the LM task does not provide one clear answer: one cannot be \"right\" or \"wrong\" in language modeling, only softly worse or better than other systems.", "labels": [], "entities": []}, {"text": "A different approach is testing the model on a grammatical task that does not require an extensive grammatical annotation, but is yet indicative of syntax comprehension.", "labels": [], "entities": []}, {"text": "Specifically, previous works () used the task of predicting agreement, which requires detecting hierarchal relations between sentence constituents.", "labels": [], "entities": [{"text": "predicting agreement", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.8912651240825653}]}, {"text": "Labeled data for such a task requires only the collection of sentences that exhibit agreement from an unannotated corpora.", "labels": [], "entities": []}, {"text": "However, those works have focused on relatively small set of languages: several Indo-European languages and a Semitic language (Hebrew).", "labels": [], "entities": []}, {"text": "As we show, drawing conclusions on the model's abilities from a relatively small subset of languages can be misleading.", "labels": [], "entities": []}, {"text": "In this work, we test agreement prediction in a substantially different language, Basque, which is a language with ergative-absolutive alignment, rich morphology, relatively free word order, and polypersonal agreement (see Section 3).", "labels": [], "entities": [{"text": "agreement prediction", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.6988092064857483}]}, {"text": "We propose two tasks, verb-number prediction (Section 6) and suffix prediction (Section 7), and show that agreement prediction in Basque is indeed harder for RNNs.", "labels": [], "entities": [{"text": "verb-number prediction", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7349081635475159}, {"text": "suffix prediction", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7070148289203644}, {"text": "agreement prediction in Basque", "start_pos": 106, "end_pos": 136, "type": "TASK", "confidence": 0.7330368533730507}]}, {"text": "We thus propose Basque agreement as a challenging benchmark for the ability of models to capture regularities inhuman language.", "labels": [], "entities": []}], "datasetContent": [{"text": "In contrast to more explicit grammatical tasks (e.g. tagging, parsing), the data needed for training a model on agreement prediction task does not require annotated data and can be derived relatively easily from unannotated sentences.", "labels": [], "entities": [{"text": "parsing", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.7795573472976685}, {"text": "agreement prediction task", "start_pos": 112, "end_pos": 137, "type": "TASK", "confidence": 0.816542367140452}]}, {"text": "We have used the text of the Basque Wikipedia.", "labels": [], "entities": [{"text": "Basque Wikipedia", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.7951158285140991}]}, {"text": "A considerable number of the articles in Basque Wikipedia appear to be bot-generated; we have tried to filter these from the data according to keywords.", "labels": [], "entities": []}, {"text": "The data consists of 1,896,371 sentences; we have used 935,730 sentences for training, 129,375 for validation and 259,215 for evaluation.", "labels": [], "entities": []}, {"text": "We make the data publicly available . We use the Apertium morphological analyzer () to extract the parts-of-speech (POS) and morphological marking of all words.", "labels": [], "entities": [{"text": "Apertium morphological analyzer", "start_pos": 49, "end_pos": 80, "type": "DATASET", "confidence": 0.7234126925468445}]}, {"text": "The POS information was used to detect verbs, nouns and adjectives, but was not incorporated in the word embeddings.", "labels": [], "entities": [{"text": "POS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9189175367355347}]}, {"text": "For section 7.1, grammatical generalization, we used the Basque Universal Dependencies treebank () to extract human-annotated POS, case, number and dependency edge labels.", "labels": [], "entities": [{"text": "grammatical generalization", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.6928263902664185}, {"text": "Basque Universal Dependencies treebank", "start_pos": 57, "end_pos": 95, "type": "DATASET", "confidence": 0.8286130875349045}]}, {"text": "We have used their train:dev:test division, resulting in 5,173 training sentences, 1,719 development sentences and 1,719 test sentences.", "labels": [], "entities": []}, {"text": "Word Representation We represent each word with an embedding vector.", "labels": [], "entities": [{"text": "Word Representation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6294227838516235}]}, {"text": "To account for the rich morphology of Basque, our word embeddings combine the word identity, its lemma 5 as determined by the morphological analyzer, and character ngrams of lengths 1 to 5.", "labels": [], "entities": []}, {"text": "Let E t , E land Eng betoken, lemma and n-gram embedding matrices, and let t w , l wand {ng w } be the word token, the lemma and the set of all n-grams of lengths 1 to 5, fora given word w.", "labels": [], "entities": []}, {"text": "The final vector representation of w, e w , is given by . We use embedding vectors of size 150.", "labels": [], "entities": []}, {"text": "We recorded the 100,000 most common words, n-grams and lemmas, and used them to calculate the vector representation of words.", "labels": [], "entities": []}, {"text": "Out-of-vocabulary words, ngrams and lemmas are replaced by a unk token.", "labels": [], "entities": []}, {"text": "Model In previous studies, the agreement was between two elements, and the model was tasked with predicting a morphological property of the second one, based on a property encoded on the . In the verb prediction task, the BiLSTM encodes the verb in the context of the entire sentence, and the numbers of the ergative, absolutive and datives are predicted by 3 independent multilayer perceptrons (MLPs) with a single hidden layer of size 128, that receive as an input the hidden state of the BiLSTM over the verb token.", "labels": [], "entities": [{"text": "verb prediction task", "start_pos": 196, "end_pos": 216, "type": "TASK", "confidence": 0.7824375629425049}]}, {"text": "In the suffix prediction task, the prediction of the case suffix is performed by a MLP of size 128, that receives as an input the hidden state of the BiLSTM over each word in the sentence.", "labels": [], "entities": [{"text": "suffix prediction task", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.8337370951970419}, {"text": "prediction of the case suffix", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.7307122826576233}]}, {"text": "The whole model, including the embedding, is trained end-to-end with the Adam optimizer ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Model performance according to closest-verb grammatical connection to the verb, for sentences that contain the verb", "labels": [], "entities": []}, {"text": " Table 4: Summary of verb number prediction results for ac-", "labels": [], "entities": [{"text": "verb number prediction", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.6752120653788248}]}, {"text": " Table 5: nuclear case prediction results.", "labels": [], "entities": [{"text": "nuclear case prediction", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7799635926882426}]}, {"text": " Table 6: Summary of F1 scores for suffix prediction.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7534286379814148}, {"text": "F1 scores", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9546891152858734}, {"text": "suffix prediction", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.9357288181781769}]}]}