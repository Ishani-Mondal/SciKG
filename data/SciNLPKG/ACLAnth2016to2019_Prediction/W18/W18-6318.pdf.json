{"title": [{"text": "On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation", "labels": [], "entities": [{"text": "Multi-Head Attention-Based Neural Machine Translation", "start_pos": 28, "end_pos": 81, "type": "TASK", "confidence": 0.7411670804023742}]}], "abstractContent": [{"text": "This work investigates the alignment problem in state-of-the-art multi-head attention models based on the transformer architecture.", "labels": [], "entities": []}, {"text": "We demonstrate that alignment extraction in transformer models can be improved by augmenting an additional alignment head to the multi-head source-to-target attention component.", "labels": [], "entities": [{"text": "alignment extraction", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.9361280202865601}]}, {"text": "This is used to compute sharper attention weights.", "labels": [], "entities": []}, {"text": "We describe how to use the alignment head to achieve competitive performance.", "labels": [], "entities": []}, {"text": "To study the effect of adding the alignment head, we simulate a dictionary-guided translation task, where the user wants to guide translation using pre-defined dictionary entries.", "labels": [], "entities": [{"text": "dictionary-guided translation task", "start_pos": 64, "end_pos": 98, "type": "TASK", "confidence": 0.686903049548467}]}, {"text": "Using the proposed approach, we achieve up to 3.8% BLEU improvement when using the dictionary, in comparison to 2.4% BLEU in the baseline case.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9996318817138672}, {"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9986628293991089}]}, {"text": "We also propose alignment pruning to speedup decoding in alignment-based neural machine translation (ANMT), which speeds up translation by a factor of 1.8 without loss in translation performance.", "labels": [], "entities": [{"text": "alignment-based neural machine translation (ANMT)", "start_pos": 57, "end_pos": 106, "type": "TASK", "confidence": 0.7398228900773185}]}, {"text": "We carryout experiments on the shared WMT 2016 English\u2192Romanian news task and the BOLT Chinese\u2192English discussion forum task.", "labels": [], "entities": [{"text": "WMT 2016 English\u2192Romanian news task", "start_pos": 38, "end_pos": 73, "type": "DATASET", "confidence": 0.9397262930870056}, {"text": "BOLT Chinese\u2192English discussion forum task", "start_pos": 82, "end_pos": 124, "type": "TASK", "confidence": 0.5691261504377637}]}], "introductionContent": [{"text": "Attention-based neural machine translation (NMT) () uses an attention layer to determine which part of the input sequence to focus on during decoding.", "labels": [], "entities": [{"text": "Attention-based neural machine translation (NMT)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7027725960527148}]}, {"text": "This component eliminates the need for explicit alignment modeling.", "labels": [], "entities": [{"text": "alignment modeling", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8970252871513367}]}, {"text": "In conventional phrase-based statistical machine translation (, word alignment is modeled explicitly, making it clear which word or phrase is being translated.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 16, "end_pos": 60, "type": "TASK", "confidence": 0.5967109054327011}, {"text": "word alignment", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.739966407418251}]}, {"text": "The lack of explicit alignment use in attention-based models makes it harder to determine which target words are generated using which source words.", "labels": [], "entities": []}, {"text": "While this is not necessarily needed for translation itself, alignments can be useful in certain applications, e.g. when the customer wants to enforce specific translation of certain words.", "labels": [], "entities": [{"text": "translation", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.9628103971481323}]}, {"text": "One simple solution is to use maximum attention weights to extract the alignment, but this can result in wrong alignments in the case where the maximum attention weight is not pointing to the word being translated.", "labels": [], "entities": []}, {"text": "Such cases are not uncommon in NMT, making the use of attention weights as alignment replacement non-trivial (.", "labels": [], "entities": []}, {"text": "Alignment extraction is even less clear for transformer models, which currently produce state-of-the-art results.", "labels": [], "entities": [{"text": "Alignment extraction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9253578782081604}]}, {"text": "These models use multiple attention components for each of the stacked decoder layers.", "labels": [], "entities": []}, {"text": "In this work we focus our study on these models since they usually outperform singleattention-head recurrent neural network (RNN) attention models.", "labels": [], "entities": []}, {"text": "Alignment-based NMT () uses neural models trained using explicit hard alignments to generate translation.", "labels": [], "entities": []}, {"text": "These systems include explicit alignment modeling, making them more convenient for tasks where the source-totarget alignment is needed.", "labels": [], "entities": [{"text": "alignment modeling", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9165709018707275}]}, {"text": "However, it is not clear whether these systems are able to compete with strong attention-based NMT systems.", "labels": [], "entities": []}, {"text": "present results for alignmentbased neural machine translation (ANMT) using models trained on CPUs, limiting them to small models of 200-node layers, and they only investigate RNN models.", "labels": [], "entities": [{"text": "alignmentbased neural machine translation (ANMT)", "start_pos": 20, "end_pos": 68, "type": "TASK", "confidence": 0.8579673681940351}]}, {"text": "present results using only one RNN encoder layer, and do not include attention layers in their models.", "labels": [], "entities": []}, {"text": "In this work, we investigate the performance of large and deep state-of-the-art transformer models.", "labels": [], "entities": []}, {"text": "We keep the multi-head attention component and propose to augment it with an additional alignment head, to: An example from the Chinese\u2192English system.", "labels": [], "entities": []}, {"text": "The figures illustrate the accumulated attention weights of the baseline transformer model (left), the alignment-assisted transformer model (middle), and the alignment-assisted model guided by a dictionary entry.", "labels": [], "entities": []}, {"text": "We simulate a scenario where the user wants to translate the Chinese word \"\u5f3a\u5927\" to \"powerful\".", "labels": [], "entities": []}, {"text": "Both the baseline and alignment-assisted transformer models generate the translation \"strong\" instead.", "labels": [], "entities": []}, {"text": "To enforce the translation, we use the maximum attention weight to determine the source word being translated.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9752352833747864}]}, {"text": "Left: The maximum attention of the baseline case incorrectly points to the sentence end when translating the designated Chinese word, therefore we cannot enforce the translation in this case.", "labels": [], "entities": []}, {"text": "Middle: The alignment looks sharper because the system has an augmented alignment head.", "labels": [], "entities": [{"text": "Middle", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9659454226493835}]}, {"text": "In this case the maximum attention is pointing to the correct Chinese word.", "labels": [], "entities": []}, {"text": "Right: using the maximum attention, the translation \"strong\" is successfully replaced with the translation \"powerful\" as suggested by the user using our proposed alignment-assisted transformer.", "labels": [], "entities": []}, {"text": "combine the benefits of the two.", "labels": [], "entities": []}, {"text": "We demonstrate that we can train these models to achieve competitive results in comparison to strong state-of-theart baselines.", "labels": [], "entities": []}, {"text": "Moreover, we demonstrate that this variant has clear advantage in tasks that require alignments such as dictionary-guided translation.", "labels": [], "entities": [{"text": "dictionary-guided translation", "start_pos": 104, "end_pos": 133, "type": "TASK", "confidence": 0.6287694871425629}]}, {"text": "Translation in NMT can be performed without explicit alignment.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9506111145019531}]}, {"text": "However, there are tasks where translation needs to be constrained given specific user requirements.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9752098917961121}]}, {"text": "Examples include interactive machine translation, and scenarios where customers demand domain-specific words or phrases to be translated according to a pre-defined dictionary.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7154190540313721}]}, {"text": "We demonstrate that the explicit use of alignment in ANMT can be leveraged to generate guided translation.", "labels": [], "entities": [{"text": "ANMT", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.9520048499107361}, {"text": "guided translation", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.6750023663043976}]}, {"text": "The figures are generated using attention weights averaged overall attention components in each system.", "labels": [], "entities": []}, {"text": "The contribution of this work is as follows.", "labels": [], "entities": []}, {"text": "First, we propose a method to integrate alignment information into the multi-head attention component of the transformer model (Section 3.1).", "labels": [], "entities": [{"text": "alignment information", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.8861476480960846}]}, {"text": "We describe how such models can be trained to maintain the strong baseline performance while also using external alignment information (Section 3.3).", "labels": [], "entities": []}, {"text": "We also introduce alignment models that use selfattentive layers for faster evaluation (Section 3.2).", "labels": [], "entities": []}, {"text": "Second, we introduce alignment pruning during search to speedup evaluation without affecting translation quality (Section 4).", "labels": [], "entities": []}, {"text": "Third, we describe how to extract alignments from multi-head attention models (Section 5), and demonstrate that alignment-assisted transformer systems perform better than baseline systems in dictionary-guided translation tasks (Section 7).", "labels": [], "entities": []}, {"text": "We present speed and performance results in Section 6.", "labels": [], "entities": [{"text": "speed", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9975122213363647}]}], "datasetContent": [{"text": "We run experiments on the WMT 2016 English\u2192Romanian news task, 2 and on BOLT Chinese\u2192English which is a discussion forum task.", "labels": [], "entities": [{"text": "WMT 2016 English\u2192Romanian news task", "start_pos": 26, "end_pos": 61, "type": "DATASET", "confidence": 0.9445074966975621}, {"text": "BOLT", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.805837094783783}]}, {"text": "The corpora statistics are shown in.", "labels": [], "entities": []}, {"text": "All transformer models use 6 encoder and 6 decoder self-attentive layers.", "labels": [], "entities": []}, {"text": "We use 8 scaled dot product attention heads and augment an additional alignment head to the source-to-target attention component.", "labels": [], "entities": []}, {"text": "We use an embedding size of 512.", "labels": [], "entities": []}, {"text": "The size of feedforward layers is 2048 nodes.", "labels": [], "entities": []}, {"text": "We use source and target weight tying for the WMT English\u2192Romanian task, and no tying for BOLT Chinese\u2192English.", "labels": [], "entities": [{"text": "WMT English\u2192Romanian task", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.5403453707695007}, {"text": "BOLT", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9322200417518616}]}, {"text": "The structure of the RNN models is as follows.", "labels": [], "entities": []}, {"text": "The English\u2192Romanian lexical and alignment models use 1 bidirectional encoder layer.", "labels": [], "entities": []}, {"text": "The  Translation results for the WMT 2016 English\u2192Romanian task and the BOLT Chinese\u2192English task.", "labels": [], "entities": [{"text": "Translation", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.8093562722206116}, {"text": "WMT 2016 English\u2192Romanian task", "start_pos": 33, "end_pos": 63, "type": "DATASET", "confidence": 0.7813145319620768}, {"text": "BOLT Chinese\u2192English task", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.6051773309707642}]}, {"text": "We include the lexical model perplexities.", "labels": [], "entities": []}, {"text": "Chinese\u2192English models have 1 bidirectional encoder and 3 stacked unidirectional encoder layers.", "labels": [], "entities": []}, {"text": "All models use 2 decoder layers.", "labels": [], "entities": []}, {"text": "The baseline attention models have similar structures.", "labels": [], "entities": []}, {"text": "We use LSTM layers of 1000 nodes and embeddings of size 620.", "labels": [], "entities": []}, {"text": "We train using the Adam optimizer (.", "labels": [], "entities": []}, {"text": "All alignment models predict source jumps of maximum width of 100 source positions (forward and backward).", "labels": [], "entities": []}, {"text": "The alignments used during training are the result of IBM1/HMM/IBM4 training using GIZA++.", "labels": [], "entities": []}, {"text": "All results are measured in case-insensitive BLEU[%] ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9881700277328491}]}, {"text": "TER[%] scores are computed with TERCom ().", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9795302748680115}, {"text": "TERCom", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9751087427139282}]}, {"text": "We implement the models in Sockeye (, which allows efficient training of large models on GPUs.", "labels": [], "entities": [{"text": "Sockeye", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.8349839448928833}]}], "tableCaptions": [{"text": " Table 3: Improvements after using the dictionary of the development sets. The tokenized references of  the English\u2192Romanian and Chinese\u2192English development sets have 26.7K and 46.6K running words  respectively.", "labels": [], "entities": []}]}