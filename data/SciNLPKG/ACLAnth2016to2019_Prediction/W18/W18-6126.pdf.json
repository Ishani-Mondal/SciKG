{"title": [{"text": "A Case Study on Learning a Unified Encoder of Relations", "labels": [], "entities": []}], "abstractContent": [{"text": "Typical relation extraction models are trained on a single corpus annotated with a pre-defined relation schema.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.8812279105186462}]}, {"text": "An individual corpus is often small, and the models may often be biased or overfitted to the corpus.", "labels": [], "entities": []}, {"text": "We hypothesize that we can learn a better representation by combining multiple relation datasets.", "labels": [], "entities": []}, {"text": "We attempt to use a shared encoder to learn the unified feature representation and to augment it with reg-ularization by adversarial training.", "labels": [], "entities": []}, {"text": "The additional corpora feeding the encoder can help to learn a better feature representation layer even though the relation schemas are different.", "labels": [], "entities": []}, {"text": "We use ACE05 and ERE datasets as our case study for experiments.", "labels": [], "entities": [{"text": "ACE05", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.9199825525283813}, {"text": "ERE datasets", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.7126293480396271}]}, {"text": "The multi-task model obtains significant improvement on both datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relations represent specific semantic relationships between two entities.", "labels": [], "entities": []}, {"text": "For example, there is Physical.Located relationship between Smith and Brazil in the sentence: Smith went to a conference in Brazil.", "labels": [], "entities": []}, {"text": "Relation extraction is a crucial task for many applications such as knowledge base population.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9588510394096375}]}, {"text": "Several relation schemas and annotated corpora have been developed such as the Automatic Content Extraction (ACE), and the Entities, Relations and Events (ERE) annotation ().", "labels": [], "entities": [{"text": "Automatic Content Extraction (ACE)", "start_pos": 79, "end_pos": 113, "type": "TASK", "confidence": 0.649103025595347}]}, {"text": "These schemas share some similarity, but differ in details.", "labels": [], "entities": []}, {"text": "A relation type may exist in one schema but not in another.", "labels": [], "entities": []}, {"text": "An example might be annotated as different types in different datasets.", "labels": [], "entities": []}, {"text": "For example, Part-whole.Geographical relations in ACE05 are annotated as Physcial.Located relations in ERE.", "labels": [], "entities": [{"text": "ACE05", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.9487099051475525}, {"text": "ERE", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.7490452527999878}]}, {"text": "Most of these corpora are relatively small.", "labels": [], "entities": []}, {"text": "Models trained on a single corpus maybe biased or overfitted towards the corpus.", "labels": [], "entities": []}, {"text": "Despite the difference in relation schemas, we hypothesize that we can learn a more general representation with a unified encoder.", "labels": [], "entities": []}, {"text": "Such a representation could have better out-of-domain or lowresource performance.", "labels": [], "entities": []}, {"text": "We develop a multi-task model to learn a representation of relations in a shared relation encoder.", "labels": [], "entities": []}, {"text": "We use separate decoders to allow different relation schemas.", "labels": [], "entities": []}, {"text": "The shared encoder accesses more data, learning less overfitted representation.", "labels": [], "entities": []}, {"text": "We then regularize the representation with adversarial training in order to further enforce the sharing between different datasets.", "labels": [], "entities": []}, {"text": "In our experiments, we take ACE05 1 and ERE 2 datasets as a case study.", "labels": [], "entities": [{"text": "ERE 2 datasets", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.6285092731316885}]}, {"text": "Experimental results show that the model achieves higher performance on both datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "To apply the multi-task learning, we need at least two datasets.", "labels": [], "entities": []}, {"text": "We pick ACE05 and ERE for our case study.", "labels": [], "entities": [{"text": "ACE05", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.6656490564346313}, {"text": "ERE", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9868703484535217}]}, {"text": "set, and the other half of bc, cts and wl as the test sets.", "labels": [], "entities": []}, {"text": "We followed their split of documents and their split of the relation types for asymmetric relations.", "labels": [], "entities": []}, {"text": "The ERE dataset has a similar relation schema to ACE05, but is different in some annotation guidelines.", "labels": [], "entities": [{"text": "ERE dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9003711342811584}, {"text": "ACE05", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.8970419764518738}]}, {"text": "It also has more data than ACE05, which we expect to be helpful in the multi-task learning.", "labels": [], "entities": [{"text": "ACE05", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.8190650343894958}]}, {"text": "It contains documents from newswire and discussion forums.", "labels": [], "entities": []}, {"text": "We did not find an existing split of this dataset, so we randomly split the documents into train (80%), dev (10%) and test (10%).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Multi-task Learning and Regularization.", "labels": [], "entities": []}, {"text": " Table 2: Multi-task Learning with extra features on ACE05.", "labels": [], "entities": [{"text": "ACE05", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.9604313373565674}]}]}