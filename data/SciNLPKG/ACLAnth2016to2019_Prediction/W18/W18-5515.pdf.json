{"title": [{"text": "UCL Machine Reading Group: Four Factor Framework For Fact Finding (HexaF)", "labels": [], "entities": [{"text": "UCL Machine Reading Group", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.7775942534208298}, {"text": "Fact Finding (HexaF)", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.7139149665832519}]}], "abstractContent": [{"text": "In this paper we describe our 2 nd place FEVER shared-task system that achieved a FEVER score of 62.52% on the provisional test set (without additional human evaluation), and 65.41% on the development set.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9582537412643433}, {"text": "FEVER score", "start_pos": 82, "end_pos": 93, "type": "METRIC", "confidence": 0.9865305423736572}]}, {"text": "Our system is a four stage model consisting of document retrieval, sentence retrieval, natural language inference and aggregation.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7431881427764893}, {"text": "sentence retrieval", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7609737813472748}]}, {"text": "Retrieval is performed leveraging task-specific features, and then a natural language inference model takes each of the retrieved sentences paired with the claimed fact.", "labels": [], "entities": []}, {"text": "The resulting predictions are aggregated across retrieved sentences with a Multi-Layer Perceptron, and re-ranked corresponding to the final prediction.", "labels": [], "entities": []}], "introductionContent": [{"text": "We often hear the word \"Fake News\" these days.", "labels": [], "entities": [{"text": "Fake News\"", "start_pos": 24, "end_pos": 34, "type": "TASK", "confidence": 0.6288173198699951}]}, {"text": "Recently, Russian meddling, for example, has been blamed for the prevalence of inaccurate news stories on social media, 1 but even the reporting on this topic often turns out to be fake news.", "labels": [], "entities": []}, {"text": "An abundance of incorrect information can plant wrong beliefs in individual citizens and lead to a misinformed public, undermining the democratic process.", "labels": [], "entities": []}, {"text": "In this context, technology to automate fact-checking and source verification) is of great interest to both media consumers and publishers.", "labels": [], "entities": []}, {"text": "The Fact Extraction and Verification (FEVER) shared task provides a benchmark for such tools, testing the ability to assess textual claims against a corpus of around 5.4M Wikipedia articles.", "labels": [], "entities": [{"text": "Fact Extraction and Verification (FEVER) shared task", "start_pos": 4, "end_pos": 56, "type": "TASK", "confidence": 0.6998056140210893}]}, {"text": "Each claim is labeled as SUPPORTS, REFUTES or NOT ENOUGH INFO, depending on whether relevant evidence from the corpus can support/refute it.", "labels": [], "entities": [{"text": "SUPPORTS", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8508392572402954}, {"text": "REFUTES", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9940779209136963}, {"text": "NOT ENOUGH", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.7272553443908691}, {"text": "INFO", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.5276092290878296}]}, {"text": "Systems are evaluated on the proportion of claims for which both the predicted label is correct and a complete set of relevant evidence sentences has been identified.", "labels": [], "entities": []}, {"text": "The original dataset description paper) evaluates a simple baseline system that achieves a score of \u223c33% on this metric, using tf-idf based retrieval to find the relevant evidence and a natural language inference (NLI) model to classify the relation between the returned evidence and the claim.", "labels": [], "entities": []}, {"text": "Our system attempts to improve on this baseline by addressing two major weaknesses.", "labels": [], "entities": []}, {"text": "Firstly, the original retrieval component only finds a full evidence set for 55% of claims.", "labels": [], "entities": []}, {"text": "While tf-idf is an effective task agnostic approach to information retrieval, we find that a simple linear model using task-specific features is able to achieve much stronger performance.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.764777809381485}]}, {"text": "Secondly, the NLI component uses an overly simplistic strategy for aggregating retrieved evidence, by simply concatenating all the sentences into a single paragraph.", "labels": [], "entities": []}, {"text": "Instead, we employ an explicit aggregation step to combine the knowledge gained from each evidence sentence.", "labels": [], "entities": []}, {"text": "These improvements allow us to achieve a FEVER score of 65.41% on the development set, and 62.52% on the test set.", "labels": [], "entities": [{"text": "FEVER score", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9866445362567902}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Development set scores for different aggregation methods, all numbers in percent.", "labels": [], "entities": []}, {"text": " Table 2: Confusion matrix on the development set.", "labels": [], "entities": []}]}