{"title": [{"text": "UnibucKernel: A kernel-based learning method for complex word identification", "labels": [], "entities": [{"text": "complex word identification", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.655974010626475}]}], "abstractContent": [{"text": "In this paper, we present a kernel-based learning approach for the 2018 Complex Word Identification (CWI) Shared Task.", "labels": [], "entities": [{"text": "Complex Word Identification (CWI) Shared Task", "start_pos": 72, "end_pos": 117, "type": "TASK", "confidence": 0.755766723304987}]}, {"text": "Our approach is based on combining multiple low-level features, such as character n-grams, with high-level semantic features that are either automatically learned using word embeddings or extracted from a lexical knowledge base, namely WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 236, "end_pos": 243, "type": "DATASET", "confidence": 0.9674839377403259}]}, {"text": "After feature extraction, we employ a kernel method for the learning phase.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7232005596160889}]}, {"text": "The feature matrix is first transformed into a normalized kernel matrix.", "labels": [], "entities": []}, {"text": "For the binary classification task (simple versus complex), we employ Support Vector Machines.", "labels": [], "entities": [{"text": "binary classification task", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.7632996539274851}]}, {"text": "For the regression task, in which we have to predict the complexity level of a word (a word is more complex if it is labeled as complex by more annotators), we employ \u03bd-Support Vector Regression.", "labels": [], "entities": []}, {"text": "We applied our approach only on the three English", "labels": [], "entities": []}], "introductionContent": [{"text": "A key role in reading comprehension by nonnative speakers is played by lexical complexity.", "labels": [], "entities": []}, {"text": "To date, researchers in the Natural Language Processing (NLP) community have developed several systems to simply texts for non-native speakers as well as native speakers with reading disabilities () or low literacy levels.", "labels": [], "entities": []}, {"text": "The first task that needs to be addressed by text simplification methods is to identify which words are likely to be considered complex.", "labels": [], "entities": []}, {"text": "The complex word identification (CWI) task raised a lot of attention in the NLP community, as it has been addressed as a stand-alone task by some researchers.", "labels": [], "entities": [{"text": "complex word identification (CWI) task", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.8000411433832986}]}, {"text": "More recently, researchers even organized shared tasks on CWI.", "labels": [], "entities": []}, {"text": "The goal of the 2018 CWI Shared Task ( is to predict which words can be difficult fora non-native speaker, based on annotations collected from a mixture of native and non-native speakers.", "labels": [], "entities": []}, {"text": "Although the task features a multilingual data set, we participated only in the English monolingual track, due to time constraints.", "labels": [], "entities": []}, {"text": "In this paper, we describe the approach of our team, UnibucKernel, for the English monolingual track of the 2018 CWI Shared Task ().", "labels": [], "entities": []}, {"text": "We present results for both classification (predicting if a word is simple or complex) and regression (predicting the complexity level of a word) tasks.", "labels": [], "entities": []}, {"text": "Our approach is based on a standard machine learning pipeline that consists of two phases: (i) feature extraction and (ii) classification/regression.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7097211629152298}]}, {"text": "In the first phase, we combine multiple low-level features, such as character n-grams, with highlevel semantic features that are either automatically learned using word embeddings) or extracted from a lexical knowledge base, namely WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 232, "end_pos": 239, "type": "DATASET", "confidence": 0.9723223447799683}]}, {"text": "After feature extraction, we employ a kernel method for the learning phase.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7232005596160889}]}, {"text": "The feature matrix is first transformed into a normalized kernel matrix, using either the inner product between pairs of samples (computed by the linear kernel function) or an exponential transformation of the inner product (computed by the Gaussian kernel function).", "labels": [], "entities": []}, {"text": "For the binary classification task, we employ Support Vector Machines (SVM), while for the regression task, we employ \u03bd-Support Vector Regression (SVR) ().", "labels": [], "entities": [{"text": "binary classification task", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.7569652994473776}]}, {"text": "We applied our approach only on the three English monolingual data sets containing documents from Wikipedia, WikiNews and News domains.", "labels": [], "entities": [{"text": "English monolingual data sets containing documents from Wikipedia", "start_pos": 42, "end_pos": 107, "type": "DATASET", "confidence": 0.8012743443250656}]}, {"text": "Our best result during the competition was the third place on the English Wikipedia data set.", "labels": [], "entities": [{"text": "English Wikipedia data set", "start_pos": 66, "end_pos": 92, "type": "DATASET", "confidence": 0.9783397763967514}]}, {"text": "Nonetheless, in this paper, we also report better post-competition results.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Related work on complex word identification is presented in Section 2.", "labels": [], "entities": [{"text": "complex word identification", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6056297620137533}]}, {"text": "Our method is presented in Section 3.", "labels": [], "entities": []}, {"text": "Our experiments and results are presented in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we draw our conclusions and discuss future work in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: A summary with the number of samples in  each data set of the English monolingual track of the  2018 CWI Shared Task.", "labels": [], "entities": []}, {"text": " Table 2: Classification results on the three data sets of the English monolingual track of the 2018 CWI Shared  Task. The methods are evaluated in terms of the classification accuracy and the F 1 -score. The results marked with  an asterisk are obtained during the competition. The other results are obtained after the competition.", "labels": [], "entities": [{"text": "English monolingual track of the 2018 CWI Shared  Task", "start_pos": 63, "end_pos": 117, "type": "DATASET", "confidence": 0.7932068208853403}, {"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9754739999771118}, {"text": "F 1 -score", "start_pos": 193, "end_pos": 203, "type": "METRIC", "confidence": 0.9882211983203888}]}, {"text": " Table 3: Regression results on the three data sets of the English monolingual track of the 2018 CWI Shared Task.  The methods are evaluated in terms of the mean absolute error (MAE). The reported results are obtained after the  competition.", "labels": [], "entities": [{"text": "Regression", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9473087787628174}, {"text": "English monolingual track of the 2018 CWI Shared Task", "start_pos": 59, "end_pos": 112, "type": "DATASET", "confidence": 0.6920031209786733}, {"text": "mean absolute error (MAE)", "start_pos": 157, "end_pos": 182, "type": "METRIC", "confidence": 0.8510535359382629}]}]}