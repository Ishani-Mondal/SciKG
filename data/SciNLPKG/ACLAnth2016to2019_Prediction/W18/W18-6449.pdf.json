{"title": [{"text": "Neural Machine Translation with the Transformer and Multi-Source Romance Languages for the Biomedical WMT 2018 task", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7905548016230265}, {"text": "Biomedical WMT 2018 task", "start_pos": 91, "end_pos": 115, "type": "DATASET", "confidence": 0.6266874969005585}]}], "abstractContent": [{"text": "The Transformer architecture has become the state-of-the-art in Machine Translation.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.8510099053382874}]}, {"text": "This model, which relies on attention-based mechanisms , has outperformed previous neural machine translation architectures in several tasks.", "labels": [], "entities": []}, {"text": "In this system description paper, we report details of training neural machine translation with multi-source Romance languages with the Transformer model and in the evaluation frame of the biomedical WMT 2018 task.", "labels": [], "entities": [{"text": "training neural machine translation", "start_pos": 55, "end_pos": 90, "type": "TASK", "confidence": 0.6473598256707191}, {"text": "WMT 2018 task", "start_pos": 200, "end_pos": 213, "type": "TASK", "confidence": 0.605265736579895}]}, {"text": "Using multi-source languages from the same family allows improvements of over 6 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9991257786750793}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) () proved to be competitive with the encoder-decoder architecture based on recurrent neural networks and attention.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7898875772953033}]}, {"text": "After this architecture, new proposals based on convolutional neural networks) or only attention-based mechanisms (  appeared.", "labels": [], "entities": []}, {"text": "The latter architecture has achieved great success in Machine Translation (MT) and it has already been extended to other tasks such as Parsing ( , Speech Recognition 1 , Speech Translation (), Chatbots (Costa-juss\u00e0 et al., 2018) among others.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.8481011986732483}, {"text": "Speech Recognition 1", "start_pos": 147, "end_pos": 167, "type": "TASK", "confidence": 0.780159572760264}, {"text": "Speech Translation", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.7801428139209747}]}, {"text": "However, training with low resources is still a big drawback for neural architectures and NMT is not an exception.", "labels": [], "entities": []}, {"text": "To face low resource scenarios, several techniques have been proposed, like using multi-source, multiple languages (Johnson et al., 2017) or unsupervised techniques (), among many others.", "labels": [], "entities": []}, {"text": "In this paper, we use the Transformer enhanced with the multi-source technique to participate in the Biomedical WMT 2018 task, which can be somehow considered a low-resourced task, given the large quantity of data that it is required for NMT.", "labels": [], "entities": [{"text": "Biomedical WMT 2018 task", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.5760859996080399}]}, {"text": "Our multi-source enhancement is done only with Romance languages.", "labels": [], "entities": []}, {"text": "The fact of using similar languages in a multi-source system maybe a factor towards improving the final system which ends up with over 6 BLEU points of improvement over the single source system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.999202311038971}]}], "datasetContent": [{"text": "In this section we report details on the database, training parameters and results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Corpus Statistics (number of segments).", "labels": [], "entities": [{"text": "Corpus Statistics", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.8760811388492584}]}, {"text": " Table 1: Trained systems results for WMT17 and WMT18 official test sets.", "labels": [], "entities": [{"text": "WMT17", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.9359137415885925}, {"text": "WMT18 official test sets", "start_pos": 48, "end_pos": 72, "type": "DATASET", "confidence": 0.8899817019701004}]}, {"text": " Table 2: Spanish/Portuguese/French to English examples for WMT18", "labels": [], "entities": [{"text": "WMT18", "start_pos": 60, "end_pos": 65, "type": "TASK", "confidence": 0.7232166528701782}]}]}