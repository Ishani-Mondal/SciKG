{"title": [{"text": "LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation", "labels": [], "entities": [{"text": "Explaining Recurrent Neural Network Judgments", "start_pos": 6, "end_pos": 51, "type": "TASK", "confidence": 0.8161734700202942}]}], "abstractContent": [{"text": "Recurrent neural networks (RNNs) are temporal networks and cumulative in nature that have shown promising results in various natural language processing tasks.", "labels": [], "entities": [{"text": "Recurrent neural networks (RNNs)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6337145070234934}]}, {"text": "Despite their success, it still remains a challenge to understand their hidden behavior.", "labels": [], "entities": []}, {"text": "In this work, we analyze and interpret the cumulative nature of RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while decision making.", "labels": [], "entities": []}, {"text": "We demonstrate (1) LISA: \"How an RNN accumulates or builds semantics during its sequential processing fora given text example and expected response\" (2) Example2pattern: \"How the saliency patterns look like for each category in the data according to the network in decision making\".", "labels": [], "entities": []}, {"text": "We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the network.", "labels": [], "entities": []}, {"text": "We employ two relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the LISA and example2pattern.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.7439775168895721}, {"text": "SemEval 10 Task", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.68513156970342}, {"text": "TAC KBP", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.8363200724124908}]}], "introductionContent": [{"text": "The interpretability of systems based on deep neural network is required to be able to explain the reasoning behind the network prediction(s), that offers to (1) verify that the network works as expected and identify the cause of incorrect decision(s) (2) understand the network in order to improve data or model with or without human intervention.", "labels": [], "entities": []}, {"text": "There is along line of research in techniques of interpretability of Deep Neural networks (DNNs) via different aspects, such as explaining network decisions, data generation, etc.;; and focused on model aspects to interpret neural networks via activation maximization approach by finding inputs that maximize activations of given neurons.", "labels": [], "entities": [{"text": "data generation", "start_pos": 158, "end_pos": 173, "type": "TASK", "confidence": 0.7522418797016144}]}, {"text": "interprets by generating adversarial examples.", "labels": [], "entities": []}, {"text": "However, and; explain neural network predictions by sensitivity analysis to different input features and decomposition of decision functions, respectively.", "labels": [], "entities": []}, {"text": "Recurrent neural networks (RNNs) are temporal networks and cumulative in nature to effectively model sequential data such as text or speech.", "labels": [], "entities": []}, {"text": "RNNs and their variants such as LSTM have shown success in several natural language processing (NLP) tasks, such as entity extraction, relation extraction (), language modeling (, slot filling (), machine translation (, sentiment analysis (, semantic textual similarity) and dynamic topic modeling ().", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.7746677994728088}, {"text": "relation extraction", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.7809827923774719}, {"text": "language modeling", "start_pos": 159, "end_pos": 176, "type": "TASK", "confidence": 0.7899374961853027}, {"text": "slot filling", "start_pos": 180, "end_pos": 192, "type": "TASK", "confidence": 0.7480126023292542}, {"text": "machine translation", "start_pos": 197, "end_pos": 216, "type": "TASK", "confidence": 0.8583455085754395}, {"text": "sentiment analysis", "start_pos": 220, "end_pos": 238, "type": "TASK", "confidence": 0.8736601173877716}, {"text": "dynamic topic modeling", "start_pos": 275, "end_pos": 297, "type": "TASK", "confidence": 0.6918687621752421}]}, {"text": "Past works have mostly analyzed deep neural network, especially CNN in the field of computer vision to study and visualize the features learned by neurons.", "labels": [], "entities": []}, {"text": "Recent studies have investigated visualization of RNN and its variants.", "labels": [], "entities": []}, {"text": "visualized the memory vectors to understand the behavior of LSTM and gated recurrent unit (GRU) in speech recognition task.", "labels": [], "entities": [{"text": "speech recognition task", "start_pos": 99, "end_pos": 122, "type": "TASK", "confidence": 0.817386527856191}]}, {"text": "For given words in a sentence,  employed heat maps to study sensitivity and meaning composition in recurrent networks.", "labels": [], "entities": []}, {"text": "proposed a tool, RNNVis to visualize hidden states based on RNN's expected response to  inputs.", "labels": [], "entities": []}, {"text": "studied the internal states of deep bidirectional language model to learn contextualized word representations and observed that the higher-level hidden states capture word semantics, while lower-level states capture syntactical aspects.", "labels": [], "entities": []}, {"text": "Despite the possibility of visualizing hidden state activations and performancebased analysis, there still remains a challenge for humans to interpret hidden behavior of the\"black box\" networks that raised questions in the NLP community as to verify that the network behaves as expected.", "labels": [], "entities": []}, {"text": "In this aspect, we address the cumulative nature of RNN with the text input and computed response to answer \"how does it aggregate and build the semantic meaning of a sentence word byword at each time point in the sequence for each category in the data\".", "labels": [], "entities": []}], "datasetContent": [{"text": "The relation classification dataset of the Semantic Evaluation 2010 (SemEval10) shared task 8) consists of 19 relations (9 directed relations and one artificial class Other), 8,000 training and 2,717 testing sentences.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7468109130859375}, {"text": "Semantic Evaluation 2010 (SemEval10) shared task 8", "start_pos": 43, "end_pos": 93, "type": "DATASET", "confidence": 0.6943239801459842}]}, {"text": "We split the training data into train (6.5k) and development (1.5k) sentences to optimize the C-BRNN  We investigate another dataset from TAC KBP Slot Filling (SF) shared task, where we use the relation classification dataset by in the context of slot filling.", "labels": [], "entities": [{"text": "TAC KBP Slot Filling (SF) shared task", "start_pos": 138, "end_pos": 175, "type": "TASK", "confidence": 0.644375377231174}, {"text": "relation classification", "start_pos": 194, "end_pos": 217, "type": "TASK", "confidence": 0.7206572890281677}, {"text": "slot filling", "start_pos": 247, "end_pos": 259, "type": "TASK", "confidence": 0.8186072707176208}]}, {"text": "We have selected the two slots: per:loc of birth and per:spouse out of 24 types.", "labels": [], "entities": []}, {"text": "LISA Analysis: Following Section 4.1, we analyse the C-BRNN for LISA using sentences S8 and S9.", "labels": [], "entities": [{"text": "LISA Analysis", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6655066311359406}]}, {"text": "demonstrate the cumulative nature of recurrent neural network, where we observe that the salient patterns born in <e2> and </e1> married e2 lead to correct decision making for S8 and S9, respectively.", "labels": [], "entities": []}, {"text": "Interestingly for S8, we see a decrease in prediction score from 0.59 to 0.52 on including terms in the subsequence, following the term in.", "labels": [], "entities": [{"text": "prediction score", "start_pos": 43, "end_pos": 59, "type": "METRIC", "confidence": 0.9543915688991547}]}, {"text": "Saliency Patterns via example2pattern Transformation: Following Section 3 and Algorithm 1, we demonstrate the example2pattern transformation of sentences S8 and S9 in with tirgrams.", "labels": [], "entities": []}, {"text": "In addition, shows the tri-gram salient patterns extracted for the two slots.", "labels": [], "entities": []}], "tableCaptions": []}