{"title": [{"text": "Classification of medication-related tweets using stacked bidirectional LSTMs with context-aware attention", "labels": [], "entities": [{"text": "Classification of medication-related tweets", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8440472185611725}]}], "abstractContent": [{"text": "This paper describes the system that team UChicagoCompLx developed for the 2018 Social Media Mining for Health Applications (SMM4H) Shared Task.", "labels": [], "entities": [{"text": "Social Media Mining for Health Applications (SMM4H) Shared Task", "start_pos": 80, "end_pos": 143, "type": "TASK", "confidence": 0.7184142063964497}]}, {"text": "We use a variant of the Message-level Sentiment Analysis (MSA) model of (Baziotis et al., 2017), a word-level stacked bidirectional Long Short-Term Memory (LSTM) network equipped with attention , to classify medication-related tweets in the four subtasks of the SMM4H Shared Task.", "labels": [], "entities": [{"text": "Message-level Sentiment Analysis (MSA)", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.7720159590244293}, {"text": "SMM4H Shared Task", "start_pos": 262, "end_pos": 279, "type": "TASK", "confidence": 0.8591376741727194}]}, {"text": "Without any subtask-specific tuning, the model is able to achieve competitive results across all subtasks.", "labels": [], "entities": []}, {"text": "We make the datasets, model weights, and code publicly available 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "The Shared Task of the 2018 Social Media Mining for Health Applications (SMM4H) workshop proposed four subtasks in the domain of social media mining for health monitoring and surveillance.", "labels": [], "entities": [{"text": "Social Media Mining for Health Applications (SMM4H) workshop", "start_pos": 28, "end_pos": 88, "type": "TASK", "confidence": 0.6934259295463562}]}, {"text": "From a Natural Language Processing (NLP) viewpoint, these tasks present a considerable challenge since the nature of social media posts requires dealing with both a significant level of language variation and a widespread presence of noise (spelling mistakes, syntactic errors etc).", "labels": [], "entities": []}, {"text": "Any classifier designed for this textual domain should take into account the above intricacies and should, furthermore, be able to deal with with semantic complexities in the various ways people express medication-related concepts and outcomes.", "labels": [], "entities": []}, {"text": "To address these challenges, we use a variant of the Message-level Sentiment Analysis (MSA) model of (, originally developed for sentiment analysis of Twitter posts, to classify tweets in all four subtasks.", "labels": [], "entities": [{"text": "Message-level Sentiment Analysis (MSA)", "start_pos": 53, "end_pos": 91, "type": "TASK", "confidence": 0.7282528678576151}, {"text": "sentiment analysis of Twitter posts", "start_pos": 129, "end_pos": 164, "type": "TASK", "confidence": 0.8524885535240173}]}, {"text": "The model is a word-level stacked bidirectional LSTM (BiLSTM) with context-aware attention that uses word-embeddings pretrained by () on a corpus of \u2248 330M tweets.", "labels": [], "entities": []}, {"text": "Without additional hyperparameter tuning or subtask-specific modifications, the model outperforms the average of all submitted systems in subtasks 1 and 4 and achieves first place (by a F1-score margin of 0.234 from the next team) in subtask 2.", "labels": [], "entities": [{"text": "F1-score margin", "start_pos": 186, "end_pos": 201, "type": "METRIC", "confidence": 0.9770537614822388}]}, {"text": "In subtask 3 our model was placed 6th out of 9 systems.", "labels": [], "entities": []}, {"text": "In the following sections, we introduce the datasets, discuss preprocessing steps we took, present the model and its training setup, report results, and conclude with potential avenues for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the datasets of each subtask.", "labels": [], "entities": []}, {"text": "Subtasks 1, 3 and 4 are binary classification problems while subtask 2 is a three-class classification problem.", "labels": [], "entities": []}, {"text": "The data was manually annotated by the organizers.", "labels": [], "entities": []}, {"text": "Subtask 1 is about the automatic detection of posts mentioning the name of a drug or dietary supplement, as defined by the United States Food and Drug Administration (FDA).", "labels": [], "entities": [{"text": "automatic detection of posts mentioning the name", "start_pos": 23, "end_pos": 71, "type": "TASK", "confidence": 0.7460488932473319}]}, {"text": "A tweet is assigned label 1 if it contains the name of one or more drugs or supplements and 0 otherwise.", "labels": [], "entities": []}, {"text": "Subtask 2 poses the challenge of automatic classification of posts describing medication intake.", "labels": [], "entities": [{"text": "automatic classification of posts describing medication intake", "start_pos": 33, "end_pos": 95, "type": "TASK", "confidence": 0.7746887121881757}]}, {"text": "A tweet is assigned label 1 if \"the user clearly expresses a personal medication intake/consumption\", 2 if the tweet suggests (without certainty) that \"the user may have taken the medication\", and 3 if the tweet mentions medication names but does not indicate personal intake.", "labels": [], "entities": []}, {"text": "Subtask 3 concerns the automatic classification of posts mentioning an adverse drug reaction (ADR).", "labels": [], "entities": [{"text": "automatic classification of posts mentioning an adverse drug reaction (ADR)", "start_pos": 23, "end_pos": 98, "type": "TASK", "confidence": 0.8105657895406088}]}, {"text": "A tweet is assigned label 1 if it men- tions an ADR and 0 otherwise.", "labels": [], "entities": [{"text": "ADR", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.905295729637146}]}, {"text": "Finally, Subtask 4 deals with the automatic detection of posts mentioning vaccination behavior related to influenza vaccines.", "labels": [], "entities": [{"text": "automatic detection of posts mentioning vaccination behavior related to influenza vaccines", "start_pos": 34, "end_pos": 124, "type": "TASK", "confidence": 0.8061138933355158}]}, {"text": "The annotators were asked the question \"Does this message indicate that someone received, or intended to receive, a flu vaccine?\" and a tweet was assigned label 1 if the answer was affirmative and 0 otherwise.", "labels": [], "entities": []}, {"text": "Subtasks 1, 3 and 4 are evaluated using the F1-score for the positive class while subtask 2 uses the micro-averaged F1-score for classes 1 and 2.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.999152421951294}, {"text": "F1-score", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9654414653778076}]}, {"text": "Subtask 1 is additionally evaluated on precision and recall for the positive class.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9996398687362671}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9995477795600891}]}, {"text": "Due to Twitter privacy policies, the training set for any subtask did not contain the actual tweet text.", "labels": [], "entities": []}, {"text": "To obtain said text, participants were provided with the tweet ID of each dataset example along with a script to use for downloading the text using this ID.", "labels": [], "entities": []}, {"text": "The process inevitably resulted in fewer tweets than the number of IDs contained in the original dataset, primarily because a number of tweets had been removed (either by the users themselves, or by Twitter because e.g. the user deleted his account) while others failed to download (due to e.g. lag issues when requesting the HTML of the tweet).", "labels": [], "entities": []}, {"text": "To avoid such issues in the evaluation datasets, the organizers decided to provide the tweet text along with the ID.", "labels": [], "entities": []}, {"text": "provides a short summary of the number of tweets that were available to our team for each subtask.", "labels": [], "entities": []}, {"text": "The model was developed using Keras 2 with the Tensorflow ( backend.", "labels": [], "entities": []}, {"text": "For data preparation and processing we use Scikit-learn).", "labels": [], "entities": [{"text": "data preparation", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8440523445606232}]}, {"text": "Given the small size of the datasets, we do not use GPUs for training the model.", "labels": [], "entities": []}, {"text": "A standard 8-core CPU is sufficient.", "labels": [], "entities": []}, {"text": "Finally, for designing the network architecture, we use part of the code released by (Baziotis et al., 2017) 3 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples per split per task. CV indicates  cross-validation, so no validation set was held out.", "labels": [], "entities": []}, {"text": " Table 2: Results on the evaluation set for subtasks 1  and 4. Average score of all participating systems in  parentheses. Metric is F1-score for class 1. For sub- task 1, precision and recall for class 1 are also used for  evaluation.", "labels": [], "entities": [{"text": "Average score", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.9781722724437714}, {"text": "Metric", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9924894571304321}, {"text": "F1-score", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9974614381790161}, {"text": "precision", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9994205236434937}, {"text": "recall", "start_pos": 186, "end_pos": 192, "type": "METRIC", "confidence": 0.9981278777122498}]}, {"text": " Table 3: Subtask 2 final leaderboard. Metric is micro- averaged F1-score for classes 1 and 2.", "labels": [], "entities": [{"text": "Metric", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9928955435752869}, {"text": "F1-score", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.7681273818016052}]}, {"text": " Table 4: Subtask 3 final leaderboard. Metric is F1- score for class 1.", "labels": [], "entities": [{"text": "Metric", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9961897730827332}, {"text": "F1- score", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.988309363524119}]}]}