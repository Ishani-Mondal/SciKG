{"title": [{"text": "Learning Semantic Textual Similarity from Conversations", "labels": [], "entities": [{"text": "Semantic Textual Similarity from Conversations", "start_pos": 9, "end_pos": 55, "type": "TASK", "confidence": 0.6893875777721405}]}], "abstractContent": [{"text": "We present a novel approach to learn representations for sentence-level semantic similarity using conversational data.", "labels": [], "entities": [{"text": "sentence-level semantic similarity", "start_pos": 57, "end_pos": 91, "type": "TASK", "confidence": 0.6237778862317404}]}, {"text": "Our method trains an unsupervised model to predict conversational responses.", "labels": [], "entities": []}, {"text": "The resulting sentence embeddings perform well on the Semantic Textual Similarity (STS) Benchmark and SemEval 2017's Community Question Answering (CQA) question similarity subtask.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.728483627239863}, {"text": "SemEval 2017's Community Question Answering (CQA) question similarity", "start_pos": 102, "end_pos": 171, "type": "TASK", "confidence": 0.6959627812558954}]}, {"text": "Performance is further improved by introducing multi-task training, combining conversational response prediction and natural language inference.", "labels": [], "entities": [{"text": "conversational response prediction", "start_pos": 78, "end_pos": 112, "type": "TASK", "confidence": 0.7389655113220215}]}, {"text": "Extensive experiments show the proposed model achieves the best performance among all neural models on the STS Benchmark and is competitive with the state-of-the-art feature engineered and mixed systems for both tasks.", "labels": [], "entities": [{"text": "STS Benchmark", "start_pos": 107, "end_pos": 120, "type": "DATASET", "confidence": 0.9521420300006866}]}], "introductionContent": [{"text": "We propose a novel approach to sentence-level semantic similarity based on unsupervised learning from conversational data.", "labels": [], "entities": [{"text": "sentence-level semantic similarity", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.6757544974486033}]}, {"text": "We observe that semantically similar sentences have a similar distribution of potential conversational responses, and that a model trained to predict conversational responses should implicitly learn useful semantic representations.", "labels": [], "entities": []}, {"text": "As illustrated in, \"How old are you?\" and \"What is your age?\" are both questions about age, which can be answered by similar responses such as \"I am 20 years old\".", "labels": [], "entities": []}, {"text": "In contrast, \"How are you?\" and \"How old are you?\" use similar words but have different meanings and lead to different responses.", "labels": [], "entities": []}, {"text": "Deep learning models have been shown to predict conversational responses with increasingly good accuracy (; Kannan et al., 2016).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9985859394073486}]}, {"text": "The internal representations of such models resolve the semantics necessary to predict the correct response across abroad selection of input messages.", "labels": [], "entities": []}, {"text": "Meaning similarity between sentences then can be obtained by comparing the sentence-level representations learned by such models.", "labels": [], "entities": []}, {"text": "We follow this approach, and assess the quality of the resulting similarity scores on the Semantic Textual Similarity (STS)) and a question similarity subtask from SemEval 2017's Community Question Answering (CQA) evaluation.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS))", "start_pos": 90, "end_pos": 124, "type": "TASK", "confidence": 0.6730334709088007}, {"text": "SemEval 2017's Community Question Answering (CQA)", "start_pos": 164, "end_pos": 213, "type": "TASK", "confidence": 0.812406427330441}]}, {"text": "The STS benchmark scores sentence pairs based on their degree of meaning similarity.", "labels": [], "entities": []}, {"text": "The Community Question Answering (CQA) subtask B ( ranks questions based on their similarity with a target question.", "labels": [], "entities": [{"text": "Community Question Answering (CQA)", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.7381685078144073}]}, {"text": "We first assess representations learned from unsupervised conversational input-response pairs.", "labels": [], "entities": []}, {"text": "We then explore augmenting our model with multi-task training over a combination of unsupervised conversational response prediction and supervised training on Natural Language Inference (NLI) data, as training to NLI has been shown to independently yield useful general purpose representations (.", "labels": [], "entities": [{"text": "conversational response prediction", "start_pos": 97, "end_pos": 131, "type": "TASK", "confidence": 0.6785163680712382}]}, {"text": "Unsupervised training over conversational data yields represen-: The conversational response selection problem attempts to identify the correct response from a collection of candidate responses.", "labels": [], "entities": [{"text": "represen-", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9723538756370544}, {"text": "conversational response selection", "start_pos": 69, "end_pos": 102, "type": "TASK", "confidence": 0.6608265439669291}]}, {"text": "We train using batch negatives with each candidate response serving as a positive example for one input and a negative sample for the remaining inputs.", "labels": [], "entities": []}, {"text": "tations that perform well on STS and CQA question similarity.", "labels": [], "entities": [{"text": "STS", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.7019917964935303}, {"text": "similarity", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9265257120132446}]}, {"text": "The addition of supervised SNLI data leads to further improvements and reaches state-of-the-art performance for neural STS models, surpassing training on NLI data alone.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first evaluate the different encoders on the response prediction task.", "labels": [], "entities": [{"text": "response prediction task", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.8975309729576111}]}, {"text": "For the multitask models, we then examine their performance on SNLI.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.7300693392753601}]}, {"text": "Finally, we evaluate the encoders on the STS Benchmark () and on SemEval 2017 Community Question Answering (CQA) subtask B (.", "labels": [], "entities": [{"text": "STS Benchmark", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.9629723429679871}, {"text": "SemEval 2017 Community Question Answering (CQA)", "start_pos": 65, "end_pos": 112, "type": "TASK", "confidence": 0.7569368928670883}]}, {"text": "We refer to the model trained over Reddit input-response pairs as Reddit and the multitask model as Reddit+SNLI.", "labels": [], "entities": [{"text": "Reddit", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.9507702589035034}]}], "tableCaptions": [{"text": " Table 1: Precision at N (P@N) results on the Red- dit response predication test set for models built  using the DAN and Transformer encoders. Mod- els attempt to select the true response for an input  against 99 randomly selected negatives.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9874251484870911}]}, {"text": " Table 2: SNLI classification performance for the  Reddit+SNLI model using the transformer en- coder with reference evaluation numbers from  prior work. We note that similar to InferSent, our  goal is to use SNLI to obtain better sentence rep- resentations rather than achieving state-of-the-art  performance on the SNLI task itself.", "labels": [], "entities": [{"text": "SNLI classification", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.9473476707935333}, {"text": "sentence rep- resentations", "start_pos": 230, "end_pos": 256, "type": "TASK", "confidence": 0.5984931066632271}]}, {"text": " Table 3: Pearson's r on the STS Benchmark.", "labels": [], "entities": [{"text": "Pearson's r on the STS Benchmark", "start_pos": 10, "end_pos": 42, "type": "DATASET", "confidence": 0.6433175206184387}]}, {"text": " Table 4: Pearson's r of the proposed models on the STS Benchmark with a breakdown by category.", "labels": [], "entities": [{"text": "STS Benchmark", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9800456762313843}]}, {"text": " Table 5: Example model and human similarity scores on pairs from the STS Benchmark. System scores  are reported as the negative angular distance between the sentence embeddings. The scores can range  from 0 to \u2212\u03c0, but in practice are typically between 0 and -1  2 \u03c0.", "labels": [], "entities": [{"text": "STS Benchmark", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.9654455482959747}]}, {"text": " Table 6: Mean Average Precision (MAP) on Com- munity Question Answering (CQA) subtask B.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.9500297705332438}, {"text": "Com- munity Question Answering (CQA)", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.6857900321483612}]}]}