{"title": [{"text": "An Operation Sequence Model for Explainable Neural Machine Translation", "labels": [], "entities": [{"text": "Explainable Neural Machine Translation", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.639900729060173}]}], "abstractContent": [{"text": "We propose to achieve explainable neural machine translation (NMT) by changing the output representation to explain itself.", "labels": [], "entities": [{"text": "explainable neural machine translation (NMT)", "start_pos": 22, "end_pos": 66, "type": "TASK", "confidence": 0.7696064114570618}]}, {"text": "We present a novel approach to NMT which generates the target sentence by monotonically walking through the source sentence.", "labels": [], "entities": []}, {"text": "Word reordering is modeled by operations which allow setting markers in the target sentence and move a target-side write head between those markers.", "labels": [], "entities": [{"text": "Word reordering", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7416046857833862}]}, {"text": "In contrast to many modern neural models , our system emits explicit word alignment information which is often crucial to practical machine translation as it improves explain-ability.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.7140148431062698}, {"text": "machine translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7567537128925323}]}, {"text": "Our technique can outperform a plain text system in terms of BLEU score under the recent Transformer architecture on Japanese-English and Portuguese-English, and is within 0.5 BLEU difference on Spanish-English.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9824223518371582}, {"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9989245533943176}]}], "introductionContent": [{"text": "Neural machine translation (NMT) models ( are remarkably effective in modelling the distribution over target sentences conditioned on the source sentence, and yield superior translation performance compared to traditional statistical machine translation (SMT) on many language pairs.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.757578507065773}, {"text": "statistical machine translation (SMT)", "start_pos": 222, "end_pos": 259, "type": "TASK", "confidence": 0.8205280800660452}]}, {"text": "However, it is often difficult to extract a comprehensible explanation for the predictions of these models as information in the network is represented by real-valued vectors or matrices (.", "labels": [], "entities": []}, {"text": "In contrast, the translation process in SMT is 'transparent' as it can identify the source word which caused a target word through word alignment.", "labels": [], "entities": [{"text": "SMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9876517057418823}]}, {"text": "Most NMT models do not use the concept of word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.7601872682571411}]}, {"text": "It is tempting to interpret encoder-decoder attention matrices () in neural models as (soft) alignments, but previous work has found that the attention weights in NMT are often erratic () and differ significantly from traditional word alignments ().", "labels": [], "entities": []}, {"text": "We will discuss the difference between attention and alignment in detail in Sec.", "labels": [], "entities": []}, {"text": "4. The goal of this paper is explainable NMT by developing a transparent translation process for neural models.", "labels": [], "entities": [{"text": "NMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.8853818774223328}]}, {"text": "Our approach does not change the neural architecture, but represents the translation together with its alignment as a linear sequence of operations.", "labels": [], "entities": []}, {"text": "The neural model predicts this operation sequence, and thus simultaneously generates a translation and an explanation for it in terms of alignments from the target words to the source words that generate them.", "labels": [], "entities": []}, {"text": "The operation sequence is \"selfexplanatory\"; it does not explain an underlying NMT system but is rather a single representation produced by the NMT system that can be used to generate translations along with an accompanying explanatory alignment to the source sentence.", "labels": [], "entities": []}, {"text": "We report competitive results of our method on Spanish-English, Portuguese-English, and Japanese-English, with the benefit of producing hard alignments for better interpretability.", "labels": [], "entities": []}, {"text": "We discuss the theoretical connection between our approach and hierarchical SMT by showing that an operation sequence can be seen as a derivation in a formal grammar.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.883335292339325}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Generation of the target sentence \"stable operation of 2000 hr was confirmed\" from the source  sentence \"2000 hr \". The neural model produces the linear sequence  of operations in the first column. The positions of the source-side read head and the target-side write  head are highlighted. The marker in the target sentence produced by the i-th SET MARKER operation is  denoted with 'X i+1 '; X 1 is the initial marker. We denote INSERT(t) operations as t to simplify notation.", "labels": [], "entities": [{"text": "INSERT", "start_pos": 440, "end_pos": 446, "type": "METRIC", "confidence": 0.9867219924926758}]}, {"text": " Table 4: Generating training alignments on the  subword level.", "labels": [], "entities": [{"text": "Generating training alignments", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.8063122630119324}]}, {"text": " Table 5: Frequency of invalid OSNMT sequences  produced by an unconstrained decoder on the ja- en test set.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9246180057525635}, {"text": "ja- en test set", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.6546007931232453}]}, {"text": " Table 7: Examples of Portuguese-English translations together with their (subword-)alignments induced  by the operation sequence. Alignment links from source words consisting of multiple subwords were  mapped to the final subword in the training data, visible for 'temperamento' and 'pennisetum'.", "labels": [], "entities": []}, {"text": " Table 8: Comparison between OSNMT and using the attention matrix from forced decoding with a  recurrent model.", "labels": [], "entities": []}]}