{"title": [], "abstractContent": [{"text": "Semantic networks and semantic spaces have been two prominent approaches to represent lexical semantics.", "labels": [], "entities": []}, {"text": "While a unified account of the lexical meaning relies on one being able to convert between these representations, in both directions, the conversion direction from semantic networks into semantic spaces started to attract more attention recently.", "labels": [], "entities": []}, {"text": "In this paper we present a methodology for this conversion and assess it with a case study.", "labels": [], "entities": [{"text": "conversion", "start_pos": 48, "end_pos": 58, "type": "TASK", "confidence": 0.974621593952179}]}, {"text": "When it is applied over WordNet, the performance of the resulting embeddings in a mainstream semantic similarity task is very good, substantially superior to the performance of word embeddings based on very large collections of texts like word2vec.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.9193980693817139}]}], "introductionContent": [{"text": "The study of lexical semantics has been at the core of the research on language science and technology as the meaning of linguistic forms results from the meaning of their lexical units and from the way these are combined.", "labels": [], "entities": []}, {"text": "How to represent lexical semantics has thus been a central topic of inquiry.", "labels": [], "entities": [{"text": "represent lexical semantics", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.8194742401440939}]}, {"text": "Three broad families of approaches have emerged in this respect, namely those advocating that lexical semantics is represented as a semantic network, a feature-based model, or a semantic space.", "labels": [], "entities": []}, {"text": "In terms of data structures, under a semantic network approach, the meaning of a lexical unit is represented as anode in a graph whose edges between nodes encode different types of semantic relations holding among the units (e.g. hypernymy, meronymy, etc.).", "labels": [], "entities": []}, {"text": "Ina feature-based model, the semantics of a lexicon is represented by a hash table where a key is the lexical unit of interest and the respective value is a set of other units denoting typical characteristics of the denotation of the unit in the key (e.g. role, usage or shape, etc.).", "labels": [], "entities": []}, {"text": "Under a semantic space perspective, in turn, the meaning of a lexical unit is represented by a vector in a high-dimensional space, where each component is based on some frequency level of co-occurrence with the other units in contexts of language usage.", "labels": [], "entities": []}, {"text": "The motivation for these three families of lexical representation is to be found in their different suitability and success in explaining a wide range of empirical phenomena, in terms of how these are manifest in ordinary language usage and how they are elicited in laboratory experimentation.", "labels": [], "entities": []}, {"text": "These phenomena are related to the acquisition, storage and retrieval of lexical knowledge (e.g. the spread activation effect, the fan effect, among many others) and to how this knowledge interacts with other cognitive faculties or tasks, including categorization, reasoning, problem solving, learning, etc.", "labels": [], "entities": [{"text": "acquisition, storage and retrieval of lexical knowledge", "start_pos": 35, "end_pos": 90, "type": "TASK", "confidence": 0.7595236450433731}, {"text": "problem solving", "start_pos": 276, "end_pos": 291, "type": "TASK", "confidence": 0.6928753703832626}]}, {"text": "In the scope of the formal and computational modeling of lexical semantics, these approaches have inspired a number of initiatives to build repositories of lexical knowledge.", "labels": [], "entities": []}, {"text": "Popular examples of such repositories are, for semantic networks, WordNet, for featurebased models, Small World of Words, and for the semantic space, word2vec), among many others.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9276075959205627}]}, {"text": "Interestingly, to achieve the highest quality, repositories of different types typically resort to different empirical sources of data.", "labels": [], "entities": []}, {"text": "For instance, WordNet is constructed on the basis of systematic lexical intuitions handled by human experts; the informa-tion encoded in Small World of Words is evoked from laypersons; and word2vec is built on the basis of the co-occurrence frequency of lexical units in a collection of documents.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.9573310017585754}]}, {"text": "Even when motivated in the first place by psycholinguistic research goals, these repositories of lexical knowledge have been extraordinarily important for language technology.", "labels": [], "entities": []}, {"text": "They have been instrumental for major advances in language processing tasks and applications such as word sense disambiguation, part-of-speech tagging, named entity recognition, sentiment analysis (e.g. (), parsing (e.g. (), textual entailment (e.g. ( ), discourse analysis (e.g. (), among many others.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 101, "end_pos": 126, "type": "TASK", "confidence": 0.6795651912689209}, {"text": "part-of-speech tagging", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.7229682803153992}, {"text": "named entity recognition", "start_pos": 152, "end_pos": 176, "type": "TASK", "confidence": 0.6102167864640554}, {"text": "sentiment analysis", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.8358185589313507}, {"text": "discourse analysis", "start_pos": 255, "end_pos": 273, "type": "TASK", "confidence": 0.7912178039550781}]}, {"text": "The proliferation of different types of representation for the same object of research is common in science, and searching fora unified rendering of a given research domain has been a major goal in many disciplines.", "labels": [], "entities": []}, {"text": "To a large extent, such search focuses on finding ways of converting from one type of representation into another.", "labels": [], "entities": []}, {"text": "Once this is made possible, it brings not only the theoretical satisfaction of getting a better unified insight into the research object, but also important instrumental rewards of reapplying results, resources and tools that had been obtained under one representation to the other representations, thus opening the potential for further research advances.", "labels": [], "entities": []}, {"text": "This is the case also in what concerns the research on lexical semantics.", "labels": [], "entities": []}, {"text": "Establishing whether and how any given lexical representation can be converted into another representation is important fora more unified account of it.", "labels": [], "entities": []}, {"text": "On the language science side, this will likely enhance the plausibility of our empirical modeling about how the mindbrain handles lexical meaning.", "labels": [], "entities": []}, {"text": "On the language technology side, in turn, this will permit to reuse resources and find new ways to combine different sources of lexical information for better application results.", "labels": [], "entities": []}, {"text": "In the present paper, we seek to contribute towards a unified account of lexical semantics.", "labels": [], "entities": []}, {"text": "We report on the methodology we used to convert from a semantic network based representation of lexical meaning into a semantic space based one, and on the successful evaluation results obtained when applying that methodology.", "labels": [], "entities": []}, {"text": "We resorted to Princeton WordNet version 3 as a repository of the lexical semantics of the English language, represented as a semantic graph, and converted a subgraph of it with half of its concepts into wnet2vec, a collection of vectors in a high-dimension space.", "labels": [], "entities": [{"text": "Princeton WordNet version 3", "start_pos": 15, "end_pos": 42, "type": "DATASET", "confidence": 0.9192313104867935}]}, {"text": "These WordNet embeddings were evaluated under the same conditions that semantic space based repositories like word2vec are, namely under the processing task of determining the semantic similarity between pairs of lexical units.", "labels": [], "entities": []}, {"text": "The evaluation results obtained for wnet2vec are around 15% superior to the results obtained for word2vec with the same mainstream evaluation data set SimLex-999 ().", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Performance of wnet2vec in similarity  task over SimLex-999 given by Spearman's coeffi- cient. First row indicates the sizes of the matrices  supported by specific subgraphs.", "labels": [], "entities": []}, {"text": " Table 4: Performance of different models in the semantic similarity (simil) and relatedness (relat) tasks  over different data sets measured by Spearman's and Pearson's coefficients. Models used: word2vec  (w2vec); wnet2vec with the random 60k subgraph (n2vec 60k r); and wnet2vec with the best specific  13k subgraph (n2vec 13k s), cf. Subsection 4.3. Overlap with the vocabulary of wnet2vec 60k random  appears in the fourth column.", "labels": [], "entities": []}]}