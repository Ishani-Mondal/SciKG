{"title": [{"text": "Using context to identify the language of face-saving", "labels": [], "entities": []}], "abstractContent": [{"text": "We created a corpus of utterances that attempt to save face from parliamentary debates and use it to automatically analyze the language of reputation defence.", "labels": [], "entities": [{"text": "reputation defence", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.7368906140327454}]}, {"text": "Our proposed model that incorporates information regarding threats to reputation can predict reputation defence language with high confidence.", "labels": [], "entities": [{"text": "reputation defence language", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.7893385887145996}]}, {"text": "Further experiments and evaluations on different datasets show that the model is able to generalize to new utterances and can predict the language of reputation defence in anew dataset.", "labels": [], "entities": [{"text": "reputation defence", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.7465933561325073}]}], "introductionContent": [{"text": "Goffman (1967) defines face, or reputation, as \"the positive social value a person effectively claims for himself by the line others assume he has taken during a particular contact.", "labels": [], "entities": []}, {"text": "Face is an image of self delineated in terms of approved social attributes\".", "labels": [], "entities": []}, {"text": "Criticisms and persuasive attacks pose threats to reputation or face and they are common in all social interactions.", "labels": [], "entities": []}, {"text": "Allegations are often made against organizations (e.g., companies and governments) and individuals (e.g., medical practitioners and politicians), and various argumentation tactics and persuasive strategies are used in response to these allegations to attempt to defend the respondent's reputation and thereby save face.", "labels": [], "entities": []}, {"text": "Previous studies on reputation defence mostly use manual content analysis, such as the studies by and on political cases, and by and Tracy (2011) on courtroom cases.", "labels": [], "entities": [{"text": "reputation defence", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.906008243560791}]}, {"text": "While these studies reveal much about reputation defence strategies in various social settings, they do not analyze in detail the actual language used in the defence of reputation.", "labels": [], "entities": [{"text": "reputation defence", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7872395217418671}, {"text": "defence of reputation", "start_pos": 158, "end_pos": 179, "type": "TASK", "confidence": 0.862459401289622}]}, {"text": "Here, we examine political speeches and investigate whether we can detect the language of reputation defence.", "labels": [], "entities": [{"text": "reputation defence", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7351017445325851}]}, {"text": "We created a corpus of reputation defence, 1 in which the annotations are based on the structure of parliamentary debate.", "labels": [], "entities": [{"text": "reputation defence", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.6861681640148163}]}, {"text": "This corpus is based on the oral question period of a Westminster-style parliamentary system, specifically that of Canada, where the government of the day is held accountable for its actions and tries to defend its reputation.", "labels": [], "entities": []}, {"text": "Using this naturally annotated data lets us avoid the subjectivity of manual analysis, any interpretation by the annotators, and any annotation inconsistencies.", "labels": [], "entities": []}, {"text": "We investigate whether we can predict the language of reputation defence and whether the context in which the reputation defence occurs can help in identifying this language.", "labels": [], "entities": [{"text": "predict the language of reputation defence", "start_pos": 30, "end_pos": 72, "type": "TASK", "confidence": 0.5759818702936172}, {"text": "reputation defence", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.7251812815666199}]}, {"text": "We first perform experiments on a sampled dataset from Canadian parliamentary proceedings of 1994-2014.", "labels": [], "entities": [{"text": "sampled dataset from Canadian parliamentary proceedings of 1994-2014", "start_pos": 34, "end_pos": 102, "type": "DATASET", "confidence": 0.7608720250427723}]}, {"text": "We then explore the performance of our approaches on two different governments.", "labels": [], "entities": []}, {"text": "We show that the context of reputation defence is effective in its recognition.", "labels": [], "entities": [{"text": "reputation defence", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8619325160980225}]}], "datasetContent": [{"text": "We approach the recognition of the face-saving language as a binary supervised classification task.", "labels": [], "entities": []}, {"text": "Our baselines are majority class (which is always answers given to the opposition questions), an SVM model trained with answer unigram vectors (weighted using tf-idf, represented with the notation '-Answers' in the result tables), and one layer of GRU to model answer sequences.", "labels": [], "entities": []}, {"text": "Since reputation defence is expressed in response to the reputation threat, we further considered the question as the context of the reputation defence and trained an SVM model with question and answer unigrams (weighted using tf-idf, represented by the notation '-Questions&Answers' in the result tables).", "labels": [], "entities": []}, {"text": "For comparison, we further include the results of an SVM model trained on only unigrams from questions ('-Questions').", "labels": [], "entities": []}, {"text": "We also use one layer of GRU to model the concatenation of question and answer pairs as one sequence.", "labels": [], "entities": []}, {"text": "The SVM model trained on word pairs is represented with the notation '-Questions\u00d7Answers' in the result tables.", "labels": [], "entities": []}, {"text": "For all datasets and models, we randomly used 10% of the training data as the development set.", "labels": [], "entities": []}, {"text": "We evaluated the performance of reputation defence classification using the metrics Accuracy, Precision, Recall, and F 1 . shows the results of five-fold crossvalidation on a balanced set from all parliaments in the period 1994-2014, on just the Liberal governments, and on just the Conservative governments.", "labels": [], "entities": [{"text": "reputation defence classification", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.8787152568499247}, {"text": "Accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9984239339828491}, {"text": "Precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9695157408714294}, {"text": "Recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9905053973197937}, {"text": "F 1", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9959450662136078}]}, {"text": "Both CNN and LSTM models improve the classification compared to the baselines.", "labels": [], "entities": [{"text": "CNN", "start_pos": 5, "end_pos": 8, "type": "DATASET", "confidence": 0.9022226333618164}]}, {"text": "In general, we can see that all the models that rely only on the answer or reputation defence perform poorer than the models that rely also on the questions.", "labels": [], "entities": []}, {"text": "The best model achieves an accuracy and F 1 measure of above 98% on the parliaments with Conservative governments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9997954964637756}, {"text": "F 1 measure", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.9886823892593384}]}, {"text": "The highest accuracy and F 1 measure on the Liberal dataset is above 95% and 97%, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.999693751335144}, {"text": "F 1", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9951529800891876}, {"text": "Liberal dataset", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.9755039215087891}]}, {"text": "shows the results of the crossparliament classification.", "labels": [], "entities": [{"text": "crossparliament classification", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.7872410416603088}]}, {"text": "We trained the models on all Liberal parliaments, and tested them on all Conservative governments, and then vice versa.", "labels": [], "entities": []}, {"text": "The SVM model trained using questionand-answer unigrams is a strong baseline.", "labels": [], "entities": []}, {"text": "Both the CNN and LSTM models improved F 1 measure compared to the baseline models.", "labels": [], "entities": [{"text": "CNN", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.9046459197998047}, {"text": "F 1 measure", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.9855629007021586}]}, {"text": "On the cross-parliament classification setting, again the models trained on both questions and answers perform better.", "labels": [], "entities": [{"text": "cross-parliament classification", "start_pos": 7, "end_pos": 38, "type": "TASK", "confidence": 0.8159627318382263}]}, {"text": "The overall performance of the neural net models across parliaments is poorer than the classification performance within parliaments.", "labels": [], "entities": []}, {"text": "This can be explained by the differences in framing strategies used in the language of defence by the two parties, which each defend their actions and choices from their own point of view.", "labels": [], "entities": []}, {"text": "The SVM model trained on the words extracted from the cross-product of questions and answers (wordpairs) achieves the best accuracy, reaching an accuracy and F 1 measure above 92% across parliaments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9992926120758057}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.999646782875061}, {"text": "F 1", "start_pos": 158, "end_pos": 161, "type": "METRIC", "confidence": 0.9945386350154877}]}, {"text": "These results show that reputation defence language can be detected with high accuracy regardless of differences in ideologies and framing strategies.", "labels": [], "entities": [{"text": "reputation defence language", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.9237794081370035}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9978600144386292}]}, {"text": "An error analysis shows that most errors occurred in the classification of answers to nonthreat questions.", "labels": [], "entities": [{"text": "classification of answers to nonthreat questions", "start_pos": 57, "end_pos": 105, "type": "TASK", "confidence": 0.851809432109197}]}, {"text": "One reason for this is that while the government ministers do not defend themselves in the answers in response to the government backbenchers, they do try to enhance their image.", "labels": [], "entities": []}, {"text": "Consider the following example 9 : Example 7.1 Q.", "labels": [], "entities": []}, {"text": "Mr. Speaker, my question is for the Minister of the Environment.", "labels": [], "entities": []}, {"text": "Over the weekend, the leader of the Bloc Qu\u00e9b\u00e9cois had the temerity to claim that the 2005 budget did not serve the interests of the people in Quebec.", "labels": [], "entities": []}, {"text": "I know full well that the environment is very important to the people in my riding.", "labels": [], "entities": []}, {"text": "Could the minister tell the House how the environmental initiatives contained in the budget will benefit Quebec?", "labels": [], "entities": []}, {"text": "A. Mr. Speaker, Quebeckers are impatiently awaiting the greenest budget since Confederation.", "labels": [], "entities": [{"text": "Confederation", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9294275641441345}]}, {"text": "Very successful contacts have been established with the Government of Quebec for the use of the partnership fund.", "labels": [], "entities": []}, {"text": "Projects are sprouting up allover for the climate fund, for new investments, for national parks and for investment in renewable and wind energy.", "labels": [], "entities": []}, {"text": "Mayors are waiting for green investments for cities and municipalities through the new deal, the green municipal fund, the EnerGuide program for cities and soon.", "labels": [], "entities": []}, {"text": "Quebec must not be blocked, but greened even more.", "labels": [], "entities": [{"text": "Quebec", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9558312296867371}]}, {"text": "We further examined the cases where a reputation defence was erroneously assigned a nondefence label.", "labels": [], "entities": []}, {"text": "These cases require real-world knowledge to determine that they are indeed reputation defence.", "labels": [], "entities": [{"text": "reputation defence", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.9009830355644226}]}, {"text": "Here is an example 10 : Example 7.2 Q.", "labels": [], "entities": []}, {"text": "Mr. Speaker, this country was built upon common interests by and for the people here.", "labels": [], "entities": []}, {"text": "We cannot allow the House of Commons to introduce a bill which, in reality, provides a recipe for destroying this country.", "labels": [], "entities": []}, {"text": "Does the government realize that this draft bill is an avowal of failure by this government as far as the future of the federation is concerned?", "labels": [], "entities": []}, {"text": "A. No, Mr. Speaker.", "labels": [], "entities": [{"text": "A.", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9879746437072754}]}, {"text": "This bill is a follow-up to the Supreme Court judgment referring back to the political stakeholders the responsibility to establish the conditions of clarity under which they would agree to negotiate the secession of a province from Canada, and it seems tome that one of those stakeholders is the Canadian House of Commons.", "labels": [], "entities": []}, {"text": "The models that rely on only the answer have particular difficulty in distinguishing these cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics; Party shows the govern- ing party; Opposition shows the number of questions  asked by the opposition members and their respective  answers, Government shows the number of questions  asked by the government backbenchers and their re- spective answers.", "labels": [], "entities": []}, {"text": " Table 3: The performance of different models for binary classification of reputation defence language using five- fold cross-validation on (1) a balanced set from 1994-2014; (2) three Liberal governments; (3) three Conservative  governments.", "labels": [], "entities": [{"text": "binary classification of reputation defence language", "start_pos": 50, "end_pos": 102, "type": "TASK", "confidence": 0.7344008187452952}]}, {"text": " Table 4: The performance of different models for binary classification of reputation defence in the cross-parliament  setting. Opp shows the number of opposition members' questions and their respective answers and Gov shows the  number of government backbenchers' questions and their respective answers.", "labels": [], "entities": [{"text": "reputation defence", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.6984919458627701}]}, {"text": " Table 5: The performance of different models for binary classification of reputation defence in the cross-parliament  setting with the balanced data (1700 instances of each class).", "labels": [], "entities": [{"text": "binary classification of reputation defence", "start_pos": 50, "end_pos": 93, "type": "TASK", "confidence": 0.5593368411064148}]}, {"text": " Table 6: Confusion matrix for the best performing  model that relies only on features extracted from an- swers, including unigrams and bigrams, NRC emo- tions (anger+pos+neg), and vagueness cues. Trained  on 36,37,38 (3,400 instances) and tested on 39,40,41  (3,400 instances).", "labels": [], "entities": []}, {"text": " Table 7: Confusion matrix for the model trained on  word pairs. Trained on 36,37,38 (3,400 instances) and  tested on 39,40,41 (3,400 instances).", "labels": [], "entities": []}]}