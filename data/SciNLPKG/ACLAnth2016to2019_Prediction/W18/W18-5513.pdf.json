{"title": [{"text": "Where is your Evidence: Improving Fact-checking by Justification Modeling", "labels": [], "entities": [{"text": "Improving Fact-checking", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.8087290227413177}, {"text": "Justification Modeling", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7046041488647461}]}], "abstractContent": [{"text": "Fact-checking is a journalistic practice that compares a claim made publicly against trusted sources of facts.", "labels": [], "entities": []}, {"text": "Wang (2017) introduced a large dataset of validated claims from the POLITIFACT.com website (LIAR dataset), enabling the development of machine learning approaches for fact-checking.", "labels": [], "entities": [{"text": "POLITIFACT.com website (LIAR dataset)", "start_pos": 68, "end_pos": 105, "type": "DATASET", "confidence": 0.8298090845346451}]}, {"text": "However, approaches based on this dataset have fo-cused primarily on modeling the claim and speaker-related metadata, without considering the evidence used by humans in labeling the claims.", "labels": [], "entities": []}, {"text": "We extend the LIAR dataset by automatically extracting the justification from the fact-checking article used by humans to label a given claim.", "labels": [], "entities": [{"text": "LIAR dataset", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.7300276011228561}]}, {"text": "We show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half true, mostly true, true).", "labels": [], "entities": []}], "introductionContent": [{"text": "Fact-checking is the process of assessing the veracity of claims.", "labels": [], "entities": []}, {"text": "It requires identifying evidence from trusted sources, understanding the context, and reasoning about what can be inferred from the evidence.", "labels": [], "entities": []}, {"text": "Several organizations such as FACTCHECK.org, POLITIFACT.com and FULL-FACT.org are devoted to such activities, and the final verdict can reflect varying degrees of truth (e.g., POLITIFACT labels claims as true, mostly true, half true, mostly false, false and pants on fire).", "labels": [], "entities": [{"text": "FACTCHECK.org", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.7899609804153442}, {"text": "FULL-FACT.org", "start_pos": 64, "end_pos": 77, "type": "METRIC", "confidence": 0.6791456937789917}]}, {"text": "Until recently, the bottleneck for developing automatic methods for fact-checking has been the lack of large datasets for building machine learning models.", "labels": [], "entities": []}, {"text": "provide a survey of current datasets and models for factchecking (e.g.,). has introduced a large dataset (LIAR) of claims from POLITIFACT, the associated metadata for each claim and the verdict (6 class labels).", "labels": [], "entities": []}, {"text": "Most work on the LIAR dataset has focused on modeling the content of the claim (including hedging, sentiment and emotion analysis) and the speaker-related metadata.", "labels": [], "entities": [{"text": "LIAR dataset", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.8349906206130981}, {"text": "hedging, sentiment and emotion analysis", "start_pos": 90, "end_pos": 129, "type": "TASK", "confidence": 0.6516174723704656}]}, {"text": "However, these approaches do not use the evidence and the justification provided by humans to predict the label.", "labels": [], "entities": []}, {"text": "Extracting evidence from (trusted) sources for fact-checking or for argument mining is a difficult task (.", "labels": [], "entities": [{"text": "Extracting evidence from (trusted) sources", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8368204065731594}, {"text": "argument mining", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.8026891052722931}]}, {"text": "For the purpose of our paper, we rely on the fact-checking article associated with the claim.", "labels": [], "entities": []}, {"text": "We extend the original LIAR dataset by automatically extracting the justification given by humans for labeling the claim, from the fact-checking article (Section 2).", "labels": [], "entities": [{"text": "LIAR dataset", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.8234369456768036}]}, {"text": "We release the extended LIAR dataset (LIAR-PLUS) to the community 1 . The main contribution of this paper is to show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half-true, mostly true, true) (Section 4).", "labels": [], "entities": [{"text": "LIAR dataset (LIAR-PLUS)", "start_pos": 24, "end_pos": 48, "type": "DATASET", "confidence": 0.803956788778305}]}, {"text": "We provide a detailed error analysis and per-class results.", "labels": [], "entities": []}, {"text": "Our work complements the recent work on providing datasets and models that enable the development of an end-to-end pipeline for fact-checking ) for English and () for Arabic).", "labels": [], "entities": []}, {"text": "We are primarily concerned on showing the impact of modeling the human-provided justification for predicting the veracity of a claim.", "labels": [], "entities": [{"text": "predicting the veracity of a claim", "start_pos": 98, "end_pos": 132, "type": "TASK", "confidence": 0.7759672005971273}]}, {"text": "In addition, our task aims to capture the varying degrees of truth that some claims might have and that are usually labeled as such by professionals (rather than binary true vs. false labels).", "labels": [], "entities": []}], "datasetContent": [{"text": "The LIAR dataset introduced by consists of 12,836 short statements taken from POLITIFACT and labeled by humans for truthfulness, subject, context/venue, speaker, state, party, and prior history.", "labels": [], "entities": [{"text": "LIAR dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.6565628498792648}]}, {"text": "For truthfulness, the LIAR dataset has six labels: pants-fire, false, mostlyfalse, half-true, mostly-true, and true.", "labels": [], "entities": [{"text": "LIAR dataset", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.8764050006866455}]}, {"text": "These six label sets are relatively balanced in size.", "labels": [], "entities": []}, {"text": "The statements were collected from a variety of broadcasting mediums, like TV interviews, speeches, tweets, debates, and they cover abroad range of topics such as the economy, healthcare, taxes and election.", "labels": [], "entities": []}, {"text": "We extend the LIAR dataset to the LIAR-PLUS dataset by automatically extracting for each claim the justification that humans have provided in the fact-checking article associated with the claim.", "labels": [], "entities": [{"text": "LIAR dataset", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.8348768353462219}, {"text": "LIAR-PLUS dataset", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.922219306230545}]}, {"text": "Most of the articles end with a summary that has a headline \"our ruling\" or \"summing up\".", "labels": [], "entities": []}, {"text": "This summary usually has several justification sentences that are related to the statement.", "labels": [], "entities": []}, {"text": "We extract all sentences in these summary sections, or the last five sentences in the fact-checking article when no summary exists.", "labels": [], "entities": []}, {"text": "We filter out the sentence that has the verdict and related words.", "labels": [], "entities": []}, {"text": "These extracted sentences can support or contradict the statement, which is expected to enhance the accuracy of the classification approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9991875290870667}]}, {"text": "Excerpt from the LIAR-PLUS dataset is shown in.", "labels": [], "entities": [{"text": "LIAR-PLUS dataset", "start_pos": 17, "end_pos": 34, "type": "DATASET", "confidence": 0.9099039137363434}]}], "tableCaptions": [{"text": " Table 3: F1 Score Per Class on Validation Set", "labels": [], "entities": [{"text": "F1 Score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9683797657489777}]}, {"text": " Table 4: F1 Score Per Class on Test Set", "labels": [], "entities": [{"text": "F1 Score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9782595932483673}]}]}