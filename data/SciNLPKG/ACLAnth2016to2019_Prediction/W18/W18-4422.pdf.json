{"title": [{"text": "Combining Shallow and Deep Learning for Aggressive Text Detection", "labels": [], "entities": [{"text": "Aggressive Text Detection", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.59776637951533}]}], "abstractContent": [{"text": "We describe the participation of team TakeLab in the aggression detection shared task at the TRAC1 workshop for English.", "labels": [], "entities": [{"text": "aggression detection shared task", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.8754774481058121}, {"text": "TRAC1 workshop", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.8634655773639679}]}, {"text": "Aggression manifests in a variety of ways.", "labels": [], "entities": [{"text": "Aggression", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9662023186683655}]}, {"text": "Unlike some forms of aggression that are impossible to prevent in day-today life, aggressive speech abounding on social networks could in principle be prevented or at least reduced by simply disabling users that post aggressively worded messages.", "labels": [], "entities": []}, {"text": "The first step in achieving this is to detect such messages.", "labels": [], "entities": []}, {"text": "The task, however, is far from being trivial, as what is considered as aggressive speech can be quite subjective, and the task is further complicated by the noisy nature of user-generated text on social networks.", "labels": [], "entities": []}, {"text": "Our system learns to distinguish between open aggression, covert aggression , and non-aggression in social media texts.", "labels": [], "entities": []}, {"text": "We tried different machine learning approaches, including traditional (shallow) machine learning models, deep learning models, and a combination of both.", "labels": [], "entities": []}, {"text": "We achieved respectable results, ranking 4th and 8th out of 31 submissions on the Facebook and Twitter test sets, respectively.", "labels": [], "entities": [{"text": "Facebook and Twitter test sets", "start_pos": 82, "end_pos": 112, "type": "DATASET", "confidence": 0.7590049028396606}]}], "introductionContent": [{"text": "Violence has always been present inhuman society.", "labels": [], "entities": []}, {"text": "As it evolved and technology improved overtime, forms of violence changed as well.", "labels": [], "entities": []}, {"text": "While only a few decades ago most of the physical and psychological abuse occurred face-to-face, today a lot of psychological violence gets through to the victim via the Internet, and inmost cases, social networks.", "labels": [], "entities": []}, {"text": "This kind of violence might even be worse, as it can take place at anytime, regardless of the physical distance between the victim and the perpetrator.", "labels": [], "entities": []}, {"text": "Moreover, social consequences that a person would endure for aggressive speech in real life are virtually absent on the Internet, lowering the inhibitions of potential perpetrators.", "labels": [], "entities": []}, {"text": "Although it is next to impossible to prevent people from being rude in real-life conversations, violence through social networks might be alleviated by simply making it impossible to send or share offensive content.", "labels": [], "entities": []}, {"text": "The first step on that path is figuring out which among the millions of messages or posts are indeed aggressive.", "labels": [], "entities": []}, {"text": "While it is always possible to rely on the users themselves, e.g., by allowing them to report offensive and inappropriate content, the most common and fastest way to detect aggressive posts is by using supervised machine learning.", "labels": [], "entities": []}, {"text": "One option is to frame the task as a binary classification problem and learn a model that discerns between aggressive and non-aggressive speech.", "labels": [], "entities": []}, {"text": "In this context, we can define aggressive speech as any kind of text which is offensive or inappropriate.", "labels": [], "entities": []}, {"text": "A more ambitious alternative is to subcategorize the aggressive speech into specific types such as racism, sexism, homophobia, trolling, cyberbullying, 1 etc.", "labels": [], "entities": []}, {"text": "A successful system employing either approach would obviously be of significant practical value, as it would facilitate detecting and intercepting the aggressive texts before they reach their intended victim, as well as allow implementation of disciplinary measures to discourage aggressive behaviour.", "labels": [], "entities": [{"text": "detecting and intercepting the aggressive texts before they reach their intended victim", "start_pos": 120, "end_pos": 207, "type": "TASK", "confidence": 0.8103776375452677}]}, {"text": "This offers considerable motivation for pursuing this strand of research.", "labels": [], "entities": []}, {"text": "Building a system for aggressive text detection using machine learning was the goal at the TRAC1 shared task on aggression detection (.", "labels": [], "entities": [{"text": "aggressive text detection", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6442877948284149}, {"text": "TRAC1 shared task", "start_pos": 91, "end_pos": 108, "type": "DATASET", "confidence": 0.7933617830276489}, {"text": "aggression detection", "start_pos": 112, "end_pos": 132, "type": "TASK", "confidence": 0.6459331661462784}]}, {"text": "The goal was to build a system that can label messages from a given dataset as openly aggressive, covertly aggressive, or not aggressive.", "labels": [], "entities": []}, {"text": "Open aggression in face-to-face conversation implies yelling, swearing, insulting and dominant attitude, while convert aggression refers to gossiping, inappropriate sarcasm and non-constructive criticism.", "labels": [], "entities": []}, {"text": "The typology is arguably challenging when it comes to applying it to online communication, as some of the nuances required to recognize aggression are difficult to discern from text alone.", "labels": [], "entities": []}, {"text": "The task turned out to be challenging for human annotators, even though they could refer to context of each message.", "labels": [], "entities": []}, {"text": "Expectedly, the task turned out to be even more challenging for automated systems.", "labels": [], "entities": []}, {"text": "In this paper we describe our submissions to the shared task.", "labels": [], "entities": []}, {"text": "We tackled the task using traditional (shallow) machine learning models, namely logistic regression and support vector machine (SVM), as well as deep learning models, namely convolutional neural networks (CNNs) and long short-term memory networks (LSTMs).", "labels": [], "entities": []}, {"text": "To get the best of both worlds, we experimented with a combination of shallow and the deep learning models.", "labels": [], "entities": []}, {"text": "We achieved respectable performance, ranking 4th and 8th out of 31 teams on the Facebook and Twitter test sets, respectively.", "labels": [], "entities": [{"text": "Facebook and Twitter test sets", "start_pos": 80, "end_pos": 110, "type": "DATASET", "confidence": 0.7637101173400879}]}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we give a brief overview of existing work on aggression detection and related tasks.", "labels": [], "entities": [{"text": "aggression detection", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.8205865323543549}]}, {"text": "Section 3 presents the data set, while the machine learning models we use are explained in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5 we present and discuss the results, followed by a conclusion and ideas for future improvements in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The shared task dataset consists of 15,000 Facebook messages (train and validation portion combined), out of which 3,419 are labelled as openly aggressive, 5,297 as covertly aggressive, and 6,284 as notaggressive.", "labels": [], "entities": []}, {"text": "lists some examples from the dataset.", "labels": [], "entities": []}, {"text": "As mentioned in the introduction, data labeling was a challenging task as there is a certain degree of subjectivity present when determining the aggression type.", "labels": [], "entities": [{"text": "data labeling", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.712175190448761}]}, {"text": "From our experience, the most problematic are the openly vs. covertly aggressive cases, in particular the messages that contain no swear words, since swear words usually imply open aggression.", "labels": [], "entities": []}, {"text": "Their absence, on the other hand, does not mean that the message is not openly aggressive.", "labels": [], "entities": []}, {"text": "We also noticed that many of the texts had grammatical and typing errors, typical of user-generated content.", "labels": [], "entities": []}, {"text": "Moreover, a smaller number of texts were not in English.", "labels": [], "entities": []}, {"text": "Finally, an additional difficulty is posed by the fact that sometimes broader context is required to correctly classify a message.", "labels": [], "entities": []}, {"text": "For example, the text from the third row of need not necessarily be covertly aggressive; whether this is the case depends on what the speaker is referring to.", "labels": [], "entities": []}, {"text": "For the above reasons, performing this task manually is not trivial at all.", "labels": [], "entities": []}, {"text": "This is also reflected in the final scores of machine learning algorithms, which are relatively low -the top performing systems on the shared task attained 0.64 and 0.60 weighted F1 measure on the Facebook and Twitter test sets, respectively.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 179, "end_pos": 189, "type": "METRIC", "confidence": 0.9843913614749908}, {"text": "Facebook and Twitter test sets", "start_pos": 197, "end_pos": 227, "type": "DATASET", "confidence": 0.750096982717514}]}, {"text": "The dataset provided by the task organizers had already been split into a train and development part.", "labels": [], "entities": []}, {"text": "However, since it is not uncommon for the test data in such competitions to be unrepresentative of train data, we decided to use the official development set as held-out test data for all preliminary experiments to avoid overfitting our models and to obtain a realistic performance estimate.", "labels": [], "entities": []}, {"text": "We first evaluate our models using 5-fold cross-validation on the train data (the official train set).", "labels": [], "entities": [{"text": "official train set", "start_pos": 82, "end_pos": 100, "type": "DATASET", "confidence": 0.7217559814453125}]}, {"text": "In each iteration of the cross-validation, models are fitted on four out of five folds, and they are used to label the remaining fifth fold, which is also considered a validation fold.", "labels": [], "entities": []}, {"text": "Model hyperparameters (details below) are chosen (in each iteration separately) to maximize weighted F1-score on this validation fold, making the obtained cross-validation score an optimistic estimate of true model performance.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9914312362670898}]}, {"text": "This also produces five trained models, each trained on 4/5 of the train data.", "labels": [], "entities": []}, {"text": "Labels on the held-out data, i.e., the official development set, are derived for each model by voting of these five models, which could be considered a type of bagging.", "labels": [], "entities": []}, {"text": "The main motivation for this slightly atypical setup is ensuring that deep learning-based models have access to a validation fold in each iteration for regularization via early stopping.", "labels": [], "entities": []}, {"text": "For producing the official test set labels, we used an identical setup, but used the union of train and development sets as the train data and the official test set as the held-out test data.", "labels": [], "entities": []}, {"text": "Before submitting the final results to the shared task organizers, we ran a number of preliminary evaluations.", "labels": [], "entities": []}, {"text": "The results of our models using both 5-fold cross-validation on the train set as well as on the development set are given in.", "labels": [], "entities": []}, {"text": "In general, results of BiLSTM and logistic regression are comparable, while the standalone models perform slightly worse.", "labels": [], "entities": []}, {"text": "Among the standalone models, the best results were obtained with the BiLSTM, which is not surprising since this model is considered by many to be the state of the art.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.545949399471283}]}, {"text": "The second-best result was achieved by logistic regression with word bigrams as features, which is an interesting indicator that, although logistic regression is considered to be among the simplest traditional machine learning models, it can yield satisfactory results on this task.", "labels": [], "entities": []}, {"text": "Results on the development set were slightly different, and this time logistic regression achieved the best results.", "labels": [], "entities": []}, {"text": "Moreover, what we find the most interesting is the fact that both combinations of the models (voting and stacking with an SVM metaclassifier) yield somewhat better results than any of the seven standalone models, indicating that combining models is useful.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results using cross-validation on the official train set (the first column) and on the official  development set (the second column). The designations next to the deep learning models refer to the  type of embeddings they were given as input.", "labels": [], "entities": [{"text": "official train set", "start_pos": 48, "end_pos": 66, "type": "DATASET", "confidence": 0.8197273214658102}, {"text": "official  development set", "start_pos": 97, "end_pos": 122, "type": "DATASET", "confidence": 0.6373546123504639}]}, {"text": " Table 3: Test set results of models which where in top 5 on either dataset. Our (TakeLab) model refers  to the the BiLSTM-common model and the SVM metaclassifier model for the Facebook and Twitter test  sets, respectively.", "labels": [], "entities": [{"text": "Facebook and Twitter test  sets", "start_pos": 177, "end_pos": 208, "type": "DATASET", "confidence": 0.7375608384609222}]}, {"text": " Table 4: Tweets, true labels, prediction of the voting classifier, and predictions of all the seven models,  where O stands for open aggression, C stands for covert aggression and N for non-aggressive speech.", "labels": [], "entities": []}]}