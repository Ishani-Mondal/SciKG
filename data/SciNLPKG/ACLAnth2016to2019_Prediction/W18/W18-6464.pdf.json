{"title": [{"text": "UAlacant machine translation quality estimation at WMT 2018: a simple approach using phrase tables and feed-forward neural networks", "labels": [], "entities": [{"text": "UAlacant machine translation quality estimation", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7333367347717286}, {"text": "WMT 2018", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8011375367641449}]}], "abstractContent": [{"text": "We describe the Universitat d'Alacant submissions to the word-and sentence-level machine translation (MT) quality estimation (QE) shared task at WMT 2018.", "labels": [], "entities": [{"text": "word-and sentence-level machine translation (MT) quality estimation (QE) shared task at WMT 2018", "start_pos": 57, "end_pos": 153, "type": "TASK", "confidence": 0.7718892255250145}]}, {"text": "Our approach to word-level MT QE builds on previous work to mark the words in the machine-translated sentence as OK or BAD, and is extended to determine if a word or sequence of words need to be inserted in the gap after each word.", "labels": [], "entities": [{"text": "MT QE", "start_pos": 27, "end_pos": 32, "type": "TASK", "confidence": 0.922846108675003}, {"text": "BAD", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9492535591125488}]}, {"text": "Our sentence-level submission simply uses the edit operations predicted by the word-level approach to approximate TER.", "labels": [], "entities": [{"text": "sentence-level submission", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.6584271341562271}, {"text": "TER", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9595621824264526}]}, {"text": "The method presented ranked first in the sub-task of identifying insertions in gaps for three out of the six datasets, and second in the rest of them.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the Universitat d'Alacant submissions to the word-and sentence-level machine translation (MT) quality estimation (QE) shared task at WMT 2018 (.", "labels": [], "entities": [{"text": "word-and sentence-level machine translation (MT) quality estimation (QE) shared task at WMT 2018", "start_pos": 66, "end_pos": 162, "type": "TASK", "confidence": 0.7796450064462774}]}, {"text": "Our approach is an extension of a previous approach Esp\u00ec a-) in which we simply marked the words t j of a machine-translated segment T as OK (no changes are needed) or as BAD (needing editing).", "labels": [], "entities": [{"text": "BAD", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.9902674555778503}]}, {"text": "Now we also mark the gaps \u03b3 j after each word t j as OK (no insertions are needed) or as BAD (needing the insertion of one or more words).", "labels": [], "entities": [{"text": "OK", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9848524928092957}, {"text": "BAD", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9977748990058899}]}, {"text": "In addition, we use the edit operations predicted at the word level to estimate quality at the sentence level.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: section 2 briefly reviews previous work on word-level MT QE; section 3 describes the method used to label words and gaps, paying special attention to the features extracted (sections 3.1 and 3.2) and the neural network (NN) architecture and its training (section 3.3); section 4 describes the datasets used; section 5 shows the main results; and, finally, section 6 closes the paper with concluding remarks.", "labels": [], "entities": [{"text": "MT QE", "start_pos": 89, "end_pos": 94, "type": "TASK", "confidence": 0.9007810652256012}]}], "datasetContent": [{"text": "Six datasets were provided for the shared task on MT QE at WMT 2018 (, covering four language pairs -English-German (EN-DE), German-English (DE-EN), English-Latvian (EN-LV), and English-Czech (EN-CS)-and two MT systems -statistical MT (SMT) and neural MT (NMT).", "labels": [], "entities": [{"text": "MT QE", "start_pos": 50, "end_pos": 55, "type": "TASK", "confidence": 0.921190083026886}, {"text": "WMT", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.6228928565979004}, {"text": "MT", "start_pos": 208, "end_pos": 210, "type": "TASK", "confidence": 0.9545661211013794}, {"text": "statistical MT (SMT", "start_pos": 220, "end_pos": 239, "type": "TASK", "confidence": 0.615817666053772}]}, {"text": "Each dataset is split into training, development and test sets.", "labels": [], "entities": []}, {"text": "From the data provided by the organizers of the shared task, the approach in this paper used: 1.", "labels": [], "entities": []}, {"text": "set of segments S in source language, 2.", "labels": [], "entities": []}, {"text": "set of translations T of the SL segment produced by an MT system, 3.", "labels": [], "entities": []}, {"text": "word-level MT QE gold predictions for each word and gap in each translation T , and 4.", "labels": [], "entities": [{"text": "MT QE gold predictions", "start_pos": 11, "end_pos": 33, "type": "METRIC", "confidence": 0.7744548544287682}]}, {"text": "baseline features 8 for word-level MT QE.", "labels": [], "entities": [{"text": "MT QE", "start_pos": 35, "end_pos": 40, "type": "TASK", "confidence": 0.9183719754219055}]}, {"text": "Regarding the baseline features, the organizers provided 28 features per word in the dataset, from which we only used the 14 numeric features plus the part-of-speech category (one-hot encoded).", "labels": [], "entities": []}, {"text": "This was done for the sake of simplicity of our architecture.", "labels": [], "entities": []}, {"text": "It is worth mentioning that no valid baseline features were provided for the EN-LV datasets.", "labels": [], "entities": [{"text": "EN-LV datasets", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.950858473777771}]}, {"text": "In addition, the large number of part-ofspeech categories in the EN-CS dataset led us to discard this feature in this case.", "labels": [], "entities": [{"text": "EN-CS dataset", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9641795754432678}]}, {"text": "As a result, 121 baseline features were obtained for EN-DE (SMT), 122 for EN-DE (NMT), 123 for DE-EN (SMT), 14 for EN-CS (SMT), and 0 for EN-LV (SMT) and EN-LV (NMT).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for sentence-level MT QE (columns 2-5) in terms of the Pearson's correlation r, MAE, RMSE,  and Sperman's correlation \u03c1 (for ranking). Results for the task of word labelling (columns 6-8) and gap labelling  (columns 9-11) in terms of the F 1 score for class BAD (F BAD ), the F 1 score for class OK (F OK ) and the product of  both (F MULTI ).", "labels": [], "entities": [{"text": "MT QE", "start_pos": 37, "end_pos": 42, "type": "TASK", "confidence": 0.8742141723632812}, {"text": "Pearson's correlation r", "start_pos": 73, "end_pos": 96, "type": "METRIC", "confidence": 0.8263563215732574}, {"text": "MAE", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.971498429775238}, {"text": "RMSE", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9239892363548279}, {"text": "Sperman's correlation \u03c1", "start_pos": 114, "end_pos": 137, "type": "METRIC", "confidence": 0.7335762679576874}, {"text": "word labelling", "start_pos": 177, "end_pos": 191, "type": "TASK", "confidence": 0.6996625065803528}, {"text": "F 1 score for class BAD (F BAD )", "start_pos": 256, "end_pos": 288, "type": "METRIC", "confidence": 0.6717130243778229}, {"text": "F 1 score", "start_pos": 294, "end_pos": 303, "type": "METRIC", "confidence": 0.9591667652130127}, {"text": "F MULTI )", "start_pos": 351, "end_pos": 360, "type": "METRIC", "confidence": 0.824258029460907}]}]}