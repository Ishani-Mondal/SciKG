{"title": [{"text": "Deep Learning for social media health text classification", "labels": [], "entities": [{"text": "health text classification", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.6713667015234629}]}], "abstractContent": [{"text": "This paper describes the systems developed for 1st and 2nd tasks of the 3rd Social Media Mining for Health Applications Shared Task at EMNLP 2018.", "labels": [], "entities": [{"text": "Social Media Mining for Health Applications Shared Task at EMNLP 2018", "start_pos": 76, "end_pos": 145, "type": "TASK", "confidence": 0.7516645762053403}]}, {"text": "The first task focuses on automatic detection of posts mentioning a drug name or dietary supplement, a binary classification.", "labels": [], "entities": [{"text": "automatic detection of posts mentioning a drug name", "start_pos": 26, "end_pos": 77, "type": "TASK", "confidence": 0.8134807907044888}]}, {"text": "The second task is about distinguishing the tweets that present personal medication intake, possible medication intake and non-intake.", "labels": [], "entities": []}, {"text": "We performed extensive experiments with various classifiers like Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDT) and deep learning architectures such as Long Short-Term Memory Networks (LSTM), jointed Convolutional Neural Networks (CNN) and LSTM architecture, and attention based LSTM architecture both at word and character level.", "labels": [], "entities": []}, {"text": "We have also explored using various pre-trained embeddings like Global Vectors for Word Representation (GloVe), Word2Vec and task-specific embeddings learned using CNN-LSTM and LSTMs.", "labels": [], "entities": [{"text": "Word Representation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.6829668134450912}, {"text": "Word2Vec", "start_pos": 112, "end_pos": 120, "type": "DATASET", "confidence": 0.9344354271888733}]}], "introductionContent": [{"text": "The tasks involve NLP challenges on social media mining for health monitoring and surveillance and in particular pharmaco-vigilance.", "labels": [], "entities": []}, {"text": "This requires processing noisy, real-world, and substantially creative language expressions from social media.", "labels": [], "entities": []}, {"text": "The proposed systems should be able to deal with many linguistic variations and semantic complexities in various ways people express medication-related concepts and outcomes.", "labels": [], "entities": []}, {"text": "The tasks present several interesting challenges including the noisy nature of the data, the informal language of the user posts, misspellings, and data imbalance.", "labels": [], "entities": []}, {"text": "Deep learning has the potential to improve analysis of social media text because of its ability to learn patterns from unlabelled data (.", "labels": [], "entities": [{"text": "analysis of social media text", "start_pos": 43, "end_pos": 72, "type": "TASK", "confidence": 0.8786703586578369}]}, {"text": "This property has enabled deep learning to produce breakthroughs in the domain of image, text and speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7168962359428406}]}, {"text": "Moreover, deep learning has the ability to generalize learnt patterns beyond data similar to the training data, which can be advantageous while dealing with social media text.", "labels": [], "entities": []}, {"text": "Despite the breakthroughs brought by deep learning, improvements are still to be made to further optimise it and improve its performance (.", "labels": [], "entities": []}, {"text": "This paper proposes to explore how the emerging advantages of deep learning can be expanded upon to address the pertinent challenges for social media text analysis.", "labels": [], "entities": [{"text": "social media text analysis", "start_pos": 137, "end_pos": 163, "type": "TASK", "confidence": 0.6296291723847389}]}, {"text": "For Task 1, tweets are required to be distinguished those that mention any drug names or dietary supplement.", "labels": [], "entities": []}, {"text": "For Task 2, the data-set contains tweets mentioning a drug and the objective is to classify the tweet into three classes.", "labels": [], "entities": []}, {"text": "The class descriptions are as follows: personal medication intake tweets in which the user clearly expresses a personal medication intake/consumption; possible medication intake tweets that are ambiguous but suggest that the user may have taken the medication; non-intake tweets that mention medication names but do not indicate personal intake.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section details how the proposed approach is applied to Task 1 and Task 2 data sets.", "labels": [], "entities": []}, {"text": "Task 1 is a binary classification problem and task 2 is a multiclass classification problem.", "labels": [], "entities": [{"text": "multiclass classification problem", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.774107942978541}]}, {"text": "The dataset statistics are given in.", "labels": [], "entities": []}, {"text": "The dataset for each task includes training data and test data.", "labels": [], "entities": []}, {"text": "As baselines, we experimented with several classifiers like Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDT).", "labels": [], "entities": []}, {"text": "We have used TF-IDF to extract the feature values.", "labels": [], "entities": []}, {"text": "We then used the CNN-LSTM and attention based LSTM networks and are trained (fine-tuned) using labeled data with backpropagation.", "labels": [], "entities": []}, {"text": "We have also experimented with CNN-LSTM and attention based LSTM networks by using pre-trained embeddings such as GloVE and Word2vec for word level and we have also experimented them at character level.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 124, "end_pos": 132, "type": "DATASET", "confidence": 0.9448692798614502}]}, {"text": "These networks also learn task-specific word embeddings.", "labels": [], "entities": []}, {"text": "Therefore, for each of the networks, we also experimented by using these embeddings as features and trained various classifiers like Logistic Regression, Random Forest, SVMs, GBDT.", "labels": [], "entities": [{"text": "GBDT", "start_pos": 175, "end_pos": 179, "type": "DATASET", "confidence": 0.7460970878601074}]}], "tableCaptions": [{"text": " Table 1: Task 1 Data Statistics", "labels": [], "entities": []}, {"text": " Table 2: Task 2 Data Statistics", "labels": [], "entities": []}, {"text": " Table 3: Validation Data Results for Task 1", "labels": [], "entities": []}, {"text": " Table 4: Test Data Results for Task 1", "labels": [], "entities": []}]}