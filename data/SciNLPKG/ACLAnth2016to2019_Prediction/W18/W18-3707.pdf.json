{"title": [{"text": "Chinese Grammatical Error Diagnosis using Statistical and Prior Knowledge driven Features with Probabilistic Ensemble Enhancement \u2020", "labels": [], "entities": [{"text": "Chinese Grammatical Error Diagnosis", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7477381378412247}]}], "abstractContent": [{"text": "This paper describes our system at NLPTEA-2018 Task #1: Chinese Grammatical Error Diagnosis.", "labels": [], "entities": [{"text": "Chinese Grammatical Error Diagnosis", "start_pos": 56, "end_pos": 91, "type": "TASK", "confidence": 0.859661877155304}]}, {"text": "Grammatical Error Diagnosis is one of the most challenging NLP tasks\uff0cwhich is to locate grammar errors and tell error types.", "labels": [], "entities": [{"text": "Grammatical Error Diagnosis", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9202443162600199}]}, {"text": "Our system is built on the model of bidirectional Long Short-Term Memory with a conditional random field layer (BiLSTM-CRF) but integrates with several new features.", "labels": [], "entities": []}, {"text": "First, richer features are considered in the BiLSTM-CRF model; second, a prob-abilistic ensemble approach is adopted; third, Template Matcher are used during a post-processing to bring inhuman knowledge.", "labels": [], "entities": [{"text": "Template Matcher", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.698580875992775}]}, {"text": "In official evaluation, our system obtains the highest F 1 scores at identifying error types and locating error positions, the second highest F 1 score at sentence level error detection.", "labels": [], "entities": [{"text": "F 1", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9938737154006958}, {"text": "F 1 score", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9852224985758463}, {"text": "sentence level error detection", "start_pos": 155, "end_pos": 185, "type": "TASK", "confidence": 0.5694440826773643}]}, {"text": "We also recommend error corrections for specific error types and achieve the best F1 performance among all participants.", "labels": [], "entities": [{"text": "F1", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9996063113212585}]}], "introductionContent": [{"text": "Chinese Language is commonly regarded as one of the most complicated languages.", "labels": [], "entities": []}, {"text": "Its sentence structures are not so strict like English.", "labels": [], "entities": []}, {"text": "Also, word segmentation usually has to be processed before deeper analysis, since word boundaries are not explicitly given in Chinese which is also different from English.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.766539454460144}]}, {"text": "In recent years, more and more people coming from overseas become interested in learning Chinese as a second language.", "labels": [], "entities": []}, {"text": "The complicatedness of Chinese language makes it challenging to learn it well for the ones with different language and knowledge background.", "labels": [], "entities": []}, {"text": "The learners are unavoidable to make grammatical errors during learning.", "labels": [], "entities": []}, {"text": "Therefore, it is necessary to develop automated tools help identifying and correcting grammatical errors.", "labels": [], "entities": [{"text": "identifying and correcting grammatical errors", "start_pos": 59, "end_pos": 104, "type": "TASK", "confidence": 0.7760740101337433}]}, {"text": "Such tools not only benefit learners also release the burden of teachers.", "labels": [], "entities": []}, {"text": "Deep Learning-based models has recently become popular due to its powerful capability of capturing features automatically, which demonstrates its excellency in many areas especially in huge-scale data mining.", "labels": [], "entities": [{"text": "data mining", "start_pos": 196, "end_pos": 207, "type": "TASK", "confidence": 0.7078310549259186}]}, {"text": "Such models also gain superior performance in previous Grammatical Error Diagnosis system (.", "labels": [], "entities": [{"text": "Grammatical Error Diagnosis", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.8770101269086202}]}, {"text": "However, prior knowledge is also important\uff0cespecially when the scale of available data is limited.", "labels": [], "entities": []}, {"text": "This paper introduces our system at NLPTEA-2018 Chinese Grammatical Error Diagnosis task.", "labels": [], "entities": [{"text": "NLPTEA-2018 Chinese Grammatical Error Diagnosis task", "start_pos": 36, "end_pos": 88, "type": "TASK", "confidence": 0.791287769873937}]}, {"text": "We will describe how to combine the knowledge that learned from large scale text data and handcraft heuristics with deep learning framework.", "labels": [], "entities": []}, {"text": "Different ensemble strategies are also discussed, which have different preferences and achieves variant performances.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Validation Results using single models and ensemble methods. \"S\" denotes for SGD-based  single model, \"A\" denotes for Adam-based single model, \"P\" denotes for probabilistic-ensemble  method, \"M\" denotes for simply merge-all, \"RM\" denotes for ranking-based output ensemble.", "labels": [], "entities": []}, {"text": " Table 2: Matcher and ePMI Performances of Single model on our Validation dataset. The baseline  model is the basic BiLSTM-CRF model described in this article without ePMI feature.", "labels": [], "entities": [{"text": "Matcher", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9727336764335632}, {"text": "Validation dataset", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.8344151973724365}]}, {"text": " Table 3: Performances of Submitted Runs on Official Evaluation Testing datasets. Yellow-labelled  scores represent the best scores we have achieved among all participant teams. \"Best Team\" row rec- ords the best scores among all participant teams at each task-specific evaluating metric.", "labels": [], "entities": [{"text": "Official Evaluation Testing datasets", "start_pos": 44, "end_pos": 80, "type": "DATASET", "confidence": 0.6609927713871002}]}]}