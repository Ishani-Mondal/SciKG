{"title": [{"text": "Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information", "labels": [], "entities": []}], "abstractContent": [{"text": "How do neural language models keep track of number agreement between subject and verb?", "labels": [], "entities": []}, {"text": "We show that 'diagnostic classifiers', trained to predict number from the internal states of a language model, provide a detailed understanding of how, when, and where this information is represented.", "labels": [], "entities": []}, {"text": "Moreover, they give us insight into when and where number information is corrupted in cases where the language model ends up making agreement errors.", "labels": [], "entities": []}, {"text": "To demonstrate the causal role played by the representations we find, we then use agreement information to influence the course of the LSTM during the processing of difficult sentences.", "labels": [], "entities": []}, {"text": "Results from such an intervention reveal a large increase in the language model's accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9983208775520325}]}, {"text": "Together, these results show that diagnostic classifiers give us an unrivalled detailed look into the representation of linguistic information in neural models, and demonstrate that this knowledge can be used to improve their performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine learning models for estimating the probabilities of potential next words (and hence, for predicting the next word) in a running text have seen enormous improvements in performance over the last few years (.", "labels": [], "entities": [{"text": "predicting the next word) in a running text", "start_pos": 97, "end_pos": 140, "type": "TASK", "confidence": 0.8608654273880852}]}, {"text": "These newer models-all based on deep learning techniques such as LSTMs (Hochreiter and Schmidhuber, 1997)-allow some language technologies, such as speech recognisers, to reach 'human parity'.", "labels": [], "entities": [{"text": "speech recognisers", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.7039884924888611}]}, {"text": "From their high accuracy and from further analysis, it is clear that LSTM-based language models have learned a great deal about both short and long distance relations in sentences and discourse.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9977318644523621}]}, {"text": "In particular, report that for several languages, their LSTM-based language model performs remarkably well on a set of long-distance number agreement tasks.", "labels": [], "entities": []}, {"text": "The Gulordava study, however, does not clarify which components of the LSTM are responsible for storing or processing syntactic features, and how such features are represented.", "labels": [], "entities": []}, {"text": "Understanding how trained recurrent networks such as LSTMs might represent syntax and other structural information is currently a key area of research.", "labels": [], "entities": []}, {"text": "Popular approaches include visualising the state space of these networks, performing ablations to the network, or using the internal states of the networks for some auxiliary task (e.g.,.", "labels": [], "entities": []}, {"text": "In this paper, we analyse the phenomenon of subject-verb agreement in English using the diagnostic classification approach of.", "labels": [], "entities": [{"text": "diagnostic classification", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.6398485898971558}]}, {"text": "We start with replicating the results of on English, and we then show that diagnostic classifiers can be used to give a fine-grained analysis of how neural language models capture structural dependencies.", "labels": [], "entities": []}, {"text": "In particular, we examine how information about subject-verb agreement is represented by an LSTM (Section 4), (ii) how that information varies across timesteps (Section 5), and (iii) where and how the problems arise that let the model commit agreement errors (Section 5 and 6).", "labels": [], "entities": []}, {"text": "Finally, to demonstrate how precisely and accurately this method can identify the network's internal representations, we (iv) show that we can alter the representation to strongly improve the models ability to predict verb number (Section 7).", "labels": [], "entities": []}, {"text": "In the next section, after discussing subject-verb agreement, we outline the data used throughout our experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the experiments described in this paper we use two different datasets.", "labels": [], "entities": []}, {"text": "The first is the one introduced by, which contains 410 sentences with at least three tokens occurring between subject head and verb.", "labels": [], "entities": []}, {"text": "For each of 41 original sentences, nine 'nonce' variants were generated by substituting each context word in the sentence by a random word with the same partof-speech tag and morphological features.", "labels": [], "entities": []}, {"text": "This data construction method is motivated by the fact that grammaticality judgements should not be influenced by the meaningfulness of a sentence, and ensures that frequency-based confounds are avoided.", "labels": [], "entities": []}, {"text": "Every sentence in the dataset is annotated with the correct and incorrect verb forms, the morphological features of the former, the position of the subject head and of the verb, the number of agreement attractors, and the type of construction spanning the long-distance dependency.", "labels": [], "entities": []}, {"text": "Additionally, we extract different subsets of the Universal Dependency (UD) corpus (ca. 1.5 million sentences) for our experiments.", "labels": [], "entities": [{"text": "Universal Dependency (UD) corpus", "start_pos": 50, "end_pos": 82, "type": "DATASET", "confidence": 0.5421272317568461}]}, {"text": "The large amount of annotated sentences in this dataset allows us to retrieve sets of sentences that satisfy specific conditions relevant to subject-verb agreement.", "labels": [], "entities": []}, {"text": "In particular, we can extract sentences with specific context sizes, and fixed numbers of words before the subject and after the verb.", "labels": [], "entities": []}, {"text": "We are also able to specify whether the sentences in the set should have an attractor and-if so-at which index (or, in our terminology, timestep) the attractor should appear.", "labels": [], "entities": []}, {"text": "Similarly, we can ensure that there is no other noun between subject and verb that has the same number as the subject (we call these helpful nouns).", "labels": [], "entities": []}, {"text": "As we will see, this allows us to examine the dynamic effect of attractors in the way the LSTM processes subject-verb agreement.", "labels": [], "entities": []}, {"text": "In this paper, the specific subset of the universal dependency dataset we use varies from experiment to experiment, as different experiments require different constraints.", "labels": [], "entities": []}, {"text": "We will specify our selection of data for each experiment in the relevant sections.", "labels": [], "entities": []}, {"text": "To clarify which subset of the UD corpus is used in an experiment, we use the following notation: UD-Kk-Ll-Mm-Aa, where k refers to the minimal number of words appearing before the subject, l to the number of words between the subject and verb (the context size), m to the minimal number of words after the verb, and a to the position of the attractor relative to the subject.", "labels": [], "entities": []}, {"text": "We use an asterisk to indicate that no restrictions are placed on one of the above mentioned variables; e.g., A* indicates that there mayor may not bean attractor.", "labels": [], "entities": [{"text": "A", "start_pos": 110, "end_pos": 111, "type": "METRIC", "confidence": 0.980632483959198}]}, {"text": "Finally, we denote datasets of sentences that have no attractor with a minus following the attractor index (i.e., A\u2212).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: LM accuracy on both English sets from", "labels": [], "entities": [{"text": "LM", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.7477967143058777}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9524203538894653}]}, {"text": " Table 2: Mean accuracy of DCs (correct/wrong) across timesteps, averaged over datasets drawn from  different context sizes and attractor positions (with K = 0, M = 0, 5 \u2264 L \u2264 7 and with a variable number  of attractors at different positions).", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9922075867652893}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.778981626033783}]}, {"text": " Table 4: Accuracy of the LSTM on the Gulordava  et al. (2018) agreement test, with and without an  intervention at the subject timestep.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9932748079299927}, {"text": "LSTM", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.6919807195663452}]}]}