{"title": [{"text": "Limitations in learning an interpreted language with recurrent models", "labels": [], "entities": []}], "abstractContent": [{"text": "In this submission I report work in progress on learning simplified interpreted languages by means of recurrent models.", "labels": [], "entities": []}, {"text": "The data is constructed to reflect core properties of natural language as modeled informal syntax and semantics.", "labels": [], "entities": []}, {"text": "Preliminary results suggest that LSTM networks do generalise to compo-sitional interpretation, albeit only in the most favorable learning setting.", "labels": [], "entities": []}, {"text": "Despite showing impressive performance on certain tasks, neural networks are still far from showing natural language understanding at a human level, cf. Paperno et al.", "labels": [], "entities": []}, {"text": "Ina sense, it is not even clear what kind of neural architecture is capable of learning natural language semantics in all its complexity, with recurrent and convolutional models being currently tried on various tasks.", "labels": [], "entities": []}, {"text": "One can hope to make progress towards the challenging goal of natural language understanding by taking into account what is known about language structure and language processing in humans.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.6864327589670817}]}, {"text": "With this in mind, it is possible to formulate certain preliminary desiderata for an adequate natural language understanding model.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 94, "end_pos": 124, "type": "TASK", "confidence": 0.6962152322133383}]}, {"text": "First, language processing in humans is known to be sequential; people process and interpret linguistic input on the fly, without any lookahead and without waiting for the linguistic structure to be completed.", "labels": [], "entities": []}, {"text": "This property, which has serious potential consequences for the cognitive architecture (Christiansen and Chater, 2016), gives a certain degree of cognitive plausibility to unidirectional recurrent models compared to other neural archi-tectures, at last in their current implementations.", "labels": [], "entities": []}, {"text": "Second, natural language can exploit recursive structures: natural language syntax consists of constructions, represented informal grammars as rewrite rules, which can recursively embed other", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: System accuracy as a function of the lan- guage, curriculum and data complexity. Random base- line is .25.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9959717392921448}, {"text": "Random base- line", "start_pos": 91, "end_pos": 108, "type": "METRIC", "confidence": 0.9764121174812317}]}, {"text": " Table 2: Percentage of complexity 3 data included in  training data vs. average test accuracy over 10 runs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9453182816505432}]}]}