{"title": [{"text": "A Neural Morphological Analyzer for Arapaho Verbs Learned from a Finite State Transducer", "labels": [], "entities": []}], "abstractContent": [{"text": "We experiment with training an encoder-decoder neural model for mimicking the behavior of an existing handwritten finite-state morphological grammar for Arapaho verbs, a polysynthetic language with a highly complex verbal inflection system.", "labels": [], "entities": []}, {"text": "After adjusting for ambiguous parses, we find that the system is able to generalize to unseen forms with accuracies of 98.68% (unam-biguous verbs) and 92.90% (all verbs).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.982437789440155}]}], "introductionContent": [{"text": "One of the clear successes in computational modeling of linguistic patterns has been that of finite state transducer (FST) models for morphological analysis and generation.", "labels": [], "entities": [{"text": "morphological analysis and generation", "start_pos": 134, "end_pos": 171, "type": "TASK", "confidence": 0.7447890490293503}]}, {"text": "Given enough linguistic expertise and investment in developing such models, FSTs provide the capability to analyze any well-formed word in a language.", "labels": [], "entities": [{"text": "FSTs", "start_pos": 76, "end_pos": 80, "type": "TASK", "confidence": 0.8383651375770569}]}, {"text": "Although FST models generally rely on lexicons, they can also be extended to handle complex inflected word forms outside the lexicon as long as morphophonological regularities are obeyed.", "labels": [], "entities": [{"text": "FST", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9646266102790833}]}, {"text": "Even ill-formed words can be mapped to a \"closest plausible reading\" through FST engineering.", "labels": [], "entities": []}, {"text": "On the downside, developing a robust FST model fora given language is very time-consuming and requires knowledge of both the language and finite-state modeling tools.", "labels": [], "entities": [{"text": "FST", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9276377558708191}]}, {"text": "Development of a finite-state grammar tends to follow a Pareto-style tradeoff where the bulk of the grammar can be developed very quickly, and the long tail of remaining effort tends to focus on lexicon expansion and difficult corner cases.", "labels": [], "entities": []}, {"text": "In this paper we describe an experiment in training a neural encoder-decoder model to replicate the behavior of an existing morphological analyzer for the Arapaho language (.", "labels": [], "entities": []}, {"text": "Our purpose is to evaluate the feasibility of bootstrapping a neural analyzer with a hand-developed FST grammar, particularly if we train from an incomplete selection of word forms in the hand-developed grammar.", "labels": [], "entities": [{"text": "FST grammar", "start_pos": 100, "end_pos": 111, "type": "TASK", "confidence": 0.6533542275428772}]}, {"text": "A successful morphological analyzer is essential for downstream applications, such as speech recognition and machine translation, that could provide Arapaho speakers access to common tools similar to Siri or Google Translate that might support and accelerate language revitalization efforts.", "labels": [], "entities": [{"text": "morphological analyzer", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.6915861368179321}, {"text": "speech recognition", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7731048464775085}, {"text": "machine translation", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.7539853155612946}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Description of non-self-explanatory tags used in the parser", "labels": [], "entities": []}]}