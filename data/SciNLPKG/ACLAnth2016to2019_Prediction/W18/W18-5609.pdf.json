{"title": [{"text": "Unsupervised Identification of Study Descriptors in Toxicology Research: An Experimental Study", "labels": [], "entities": []}], "abstractContent": [{"text": "Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks.", "labels": [], "entities": []}, {"text": "In this paper we address the question of identifying data elements in an unsupervised manner.", "labels": [], "entities": []}, {"text": "Specifically , provided a set of criteria describing specific study parameters, such as species, route of administration, and dosing regimen, we develop an unsupervised approach to identify text segments (sentences) relevant to the criteria.", "labels": [], "entities": []}, {"text": "A binary classifier trained to identify publications that met the criteria performs better when trained on the candidate sentences than when trained on sentences randomly picked from the text, supporting the intuition that our method is able to accurately identify study de-scriptors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Extracting data elements such as study descriptors from publication full texts is an essential step in a number of tasks including systematic review preparation (, construction of reference databases, and knowledge discovery).", "labels": [], "entities": [{"text": "systematic review preparation", "start_pos": 131, "end_pos": 160, "type": "TASK", "confidence": 0.6948214371999105}, {"text": "knowledge discovery", "start_pos": 205, "end_pos": 224, "type": "TASK", "confidence": 0.80438432097435}]}, {"text": "These tasks typically involve domain experts identifying relevant literature pertaining to a specific research question or a topic being investigated, identifying passages in the retrieved articles that discuss the sought after information, and extracting structured data from these passages.", "labels": [], "entities": []}, {"text": "The extracted data is then analyzed, for example to assess adherence to existing guidelines ().", "labels": [], "entities": []}, {"text": "shows an example text excerpt with information relevant to a specific task (assessment of adherence to existing guidelines () highlighted..", "labels": [], "entities": []}, {"text": "The text in this example was manually annotated by one of the authors to highlight information relevant to guidelines for performing uterotrophic bioassays set forth by.", "labels": [], "entities": []}, {"text": "Extracting the data elements needed in these tasks is a time-consuming and at present a largely manual process which requires domain expertise.", "labels": [], "entities": []}, {"text": "For example, in systematic review preparation, information extraction generally constitutes the most time consuming task).", "labels": [], "entities": [{"text": "systematic review preparation", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.7877183953921}, {"text": "information extraction", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.880348801612854}]}, {"text": "This situation is made worse by the rapidly expanding body of potentially relevant literature with more than one million papers added into PubMed each year.", "labels": [], "entities": []}, {"text": "Therefore, data annotation and extraction presents an important challenge for automation.", "labels": [], "entities": [{"text": "data annotation and extraction", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.8147722333669662}]}, {"text": "A typical approach to automated identification of relevant information in biomedical texts is to infer a prediction model from labeled training datasuch a model can then be used to assign predicted labels to new data instances.", "labels": [], "entities": [{"text": "automated identification of relevant information in biomedical texts", "start_pos": 22, "end_pos": 90, "type": "TASK", "confidence": 0.8123659193515778}]}, {"text": "However, obtaining training data for creating such prediction models can be very costly as it involves the step which these models are trying to automate -manual data extraction.", "labels": [], "entities": []}, {"text": "Furthermore, depending on the task at hand, the types of information being extracted may vary significantly.", "labels": [], "entities": []}, {"text": "For example, in systematic reviews of randomized controlled trials this information generally includes the patient group, the intervention being tested, the comparison, and the outcomes of the study (PICO elements)).", "labels": [], "entities": []}, {"text": "In toxicology research the extraction may focus on routes of exposure, dose, and necropsy timing (.", "labels": [], "entities": []}, {"text": "Previous work has largely focused on identifying specific pieces of information such as biomedical events ( or PICO elements (.", "labels": [], "entities": []}, {"text": "However, depending on the domain and the end goal of the extraction, these maybe insufficient to comprehensively describe a given study.", "labels": [], "entities": []}, {"text": "Therefore, in this paper we focus on unsupervised methods for identifying text segments (such as sentences or fixed length sequences of words) relevant to the information being extracted.", "labels": [], "entities": [{"text": "identifying text segments (such as sentences or fixed length sequences of words", "start_pos": 62, "end_pos": 141, "type": "TASK", "confidence": 0.6132696866989136}]}, {"text": "We develop a model that can be used to identify text segments from text documents without labeled data and that only requires the current document itself, rather than an entire training corpus linked to the target document.", "labels": [], "entities": []}, {"text": "More specifically, we utilize representation learning methods (, where words or phrases are embedded into the same vector space.", "labels": [], "entities": []}, {"text": "This allows us to compute semantic relatedness among text fragments, in particular sentences or text segments in a given document and a short description of the type of information being extracted from the document, by using similarity measures in the feature space.", "labels": [], "entities": []}, {"text": "The model has the potential to speedup identification of relevant segments in text and therefore to expedite annotation of domain specific information without reliance on costly labeled data.", "labels": [], "entities": []}, {"text": "We have developed and tested our approach on a reference database of rodent uterotropic bioassays 2 () which are labeled according to their adherence to test guidelines set forth in.", "labels": [], "entities": []}, {"text": "Each study in the database is assigned a label determining whether or not it met each of six main criteria defined by the guidelines; however, the database does not contain sentence-level annotations or any information about where the criteria was mentioned in each publication.", "labels": [], "entities": []}, {"text": "Due to the lack of fine-grained annotations, supervised learning methods cannot be easily applied to aid annotating new publications or to annotate related but distinct types of studies.", "labels": [], "entities": []}, {"text": "This database therefore presents an ideal use-case for unsupervised approaches.", "labels": [], "entities": []}, {"text": "While our approach doesn't require any labeled data to work, we use the labels available in the dataset to evaluate the approach.", "labels": [], "entities": []}, {"text": "We train a binary classification model for identifying publications which satisfied given criteria and show the model performs better when trained on relevant sentences identified by our method than when trained on sentences randomly picked from the text.", "labels": [], "entities": []}, {"text": "Furthermore, for three out of the six criteria, a model trained solely on the relevant sentences outperforms a model which utilizes full text.", "labels": [], "entities": []}, {"text": "The results of our evaluation support the intuition that semantic relatedness to criteria descriptions can help in identifying text sequences discussing sought after information.", "labels": [], "entities": [{"text": "identifying text sequences discussing sought after information", "start_pos": 115, "end_pos": 177, "type": "TASK", "confidence": 0.8378819056919643}]}, {"text": "There are two main contributions of this work.", "labels": [], "entities": []}, {"text": "We present an unsupervised method that employs representation learning to identify text segments from publication full text which are relevant to/contain specific sought after information (such as number of dose groups).", "labels": [], "entities": []}, {"text": "In addition, we explore anew dataset which hasn't been previously used in the field of information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.8518709242343903}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the following section we provide more details of the task and the dataset used in this study.", "labels": [], "entities": []}, {"text": "In Section 3 we describe our approach.", "labels": [], "entities": []}, {"text": "In Section 4 we evaluate our model and discuss our results.", "labels": [], "entities": []}, {"text": "In Section 5 we compare our work to existing approaches.", "labels": [], "entities": []}, {"text": "Finally, in Section 6 we provide ideas for further study.", "labels": [], "entities": []}], "datasetContent": [{"text": "The version of the database which contains both GL and non-GL studies consists of 670 publications (spanning the years 1938 through 2014) with results from 2,615 uterotrophic bioassays.", "labels": [], "entities": []}, {"text": "Specifically, each entry in the database describes one study, and studies are linked to publications using PubMed reference numbers (PMIDs).", "labels": [], "entities": [{"text": "PubMed reference numbers (PMIDs)", "start_pos": 107, "end_pos": 139, "type": "METRIC", "confidence": 0.662764290968577}]}, {"text": "Each study  is assigned seven 0/1 labels -one for each of the minimum criteria and one for the overall GL/non-GL label.", "labels": [], "entities": []}, {"text": "The database also contains more detailed subcategories for each label (for example \"species\" label for MC 1) which were not used in this study.", "labels": [], "entities": []}, {"text": "The publication PDFs were provided to us by the database creators.", "labels": [], "entities": []}, {"text": "We have used the Grobid 3 library to convert the PDF files into structured text.", "labels": [], "entities": [{"text": "Grobid 3 library", "start_pos": 17, "end_pos": 33, "type": "DATASET", "confidence": 0.8788409034411112}]}, {"text": "After removing documents with missing PDF files and documents which were not converted successfully, we were left with 624 full text documents.", "labels": [], "entities": []}, {"text": "Each publication contains on average 3.7 studies (separate bioassays), 194 publications contain a single study, while the rest contain two or more studies (with 82 being the most bioassays per publication).", "labels": [], "entities": []}, {"text": "The following excerpt shows an example sentence mentioning multiple bioassays (with different study protocols): With the exception of the first study (experiment 1), which had group sizes of 12, all other studies had group sizes of 8.", "labels": [], "entities": []}, {"text": "For this experiment we did not distinguish between publications describing a single or multiple studies.", "labels": [], "entities": []}, {"text": "Instead, our focus was on retrieving all text segments (which maybe related to multiple studies) relevant to each of the criteria.", "labels": [], "entities": []}, {"text": "For   each MC, if a document contained multiple studies with different labels, we discarded that document from our analysis of that criteria; if a document contained multiple studies with the same label, we simply combine all those labels into a single label.", "labels": [], "entities": []}, {"text": "shows the final size of the dataset.", "labels": [], "entities": []}, {"text": "The goal of this experiment was to explore empirically whether our approach truly identifies mentions of the minimum criteria in text.", "labels": [], "entities": []}, {"text": "As we did not have any fine-grained annotations that could be used to directly evaluate whether our model identifies the correct sequences, we have used a different methodology.", "labels": [], "entities": []}, {"text": "We have utilized the existing 0/1 labels which were available in the database (these were discussed in Section 2) to train one binary classifier for each MC.", "labels": [], "entities": []}, {"text": "The task of each of the classifiers is to determine whether a publication met the given criteria or not.", "labels": [], "entities": []}, {"text": "We have then compared a baseline classifier trained on all full text with three other models: \u2022 A model which, instead of all full text, utilized only the top k sentences most similar to the given MC.", "labels": [], "entities": []}, {"text": "The top k sentences were identified using our model introduced in the previous section.", "labels": [], "entities": []}, {"text": "\u2022 A model which utilized only the k least similar sentences.: Annotations generated using our method for the abstract from.", "labels": [], "entities": []}, {"text": "The sentence which was found to be the most similar to the description for \"MC 1: Animal model\" is highlighted in yellow and the most similar sequence of words within that sentence is highlighted in red.", "labels": [], "entities": []}, {"text": "The text we are looking for is highlighted with bold underlined text.", "labels": [], "entities": []}, {"text": "For this example we ran our method on the abstract of the target document rather than the full text and highlighted only the single most similar sentence.", "labels": [], "entities": []}, {"text": "\u2022 A model which utilized only k random sentences (but none of the top or bottom k sentences -the sentences were chosen at random from the interval (k, n \u2212 k) where n is the number of sentences in the document and where sentences are sorted from the most similar to the least similar).", "labels": [], "entities": []}, {"text": "The only difference between the four models is which sentences from each document are passed to the classifier for training and testing.", "labels": [], "entities": []}, {"text": "The intuition is that a classifier utilizing the correct sentences should outperform both other models.", "labels": [], "entities": []}, {"text": "To avoid selecting the same sentences across the three models we removed documents which contained less than 3 * k sentences, row Number of documents shows how many documents satisfied this condition).", "labels": [], "entities": []}, {"text": "In all of the experiments presented in this section, the publication full text was tokenized, lower-cased, stemmed, and stop words were removed.", "labels": [], "entities": []}, {"text": "All models used a Bernoulli Na\u00a8\u0131veNa\u00a8\u0131ve Bayes classifier (scikit-learn implementation which used a uniform class prior) trained on binary occurrence matrices created using 1-3-grams extracted from the publications, with ngrams appearing in only one document removed.", "labels": [], "entities": []}, {"text": "The complete results obtained from leave-one-out cross validation are shown in.", "labels": [], "entities": []}, {"text": "In all cases we report classification accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.946894645690918}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9572020769119263}]}, {"text": "In the case of the random-k sentences model the accuracy was averaged over 10 runs of the model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9995972514152527}]}, {"text": "We compare the results to two baselines: (1) a baseline obtained by classifying all documents as belonging to the majority class (baseline 1 in Table 3) and (2) a baseline obtained using the same setup (features and classification algorithm) as in the case of the top-/random-/bottom-k sentences models but which utilized all full text instead of selected sentences extracted from the text only (baseline 2 in).", "labels": [], "entities": []}, {"text": "shows that for four out of the six criteria (MC 1, MC 4, MC 5, and MC 6) the top-k sentences model outperforms baseline 1 as well the bottom-k and the random-k sentences models by a significant margin.", "labels": [], "entities": []}, {"text": "Furthermore, for three of the   six criteria (MC 4, MC 5, and MC 6) the top-k sentences model also outperforms the baseline 2 model (model which utilized all full text).", "labels": [], "entities": []}, {"text": "This seems to confirm our hypothesis that semantic relatedness of sentences to the criteria descriptions helps in identifying sentences discussing the criteria.", "labels": [], "entities": []}, {"text": "These seems to be the case especially given that for three of the six criteria the top-k sentences model outperforms the model which utilizes all full text (baseline 2) despite being given less information to learn from (selected sentences only in the case of the top-k sentences model vs. all full text in the case of the baseline 2 model).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Minimum criteria for guideline-like studies. The descriptions are reprinted here from (Kleinstreuer et al.,  2016).", "labels": [], "entities": []}, {"text": " Table 2: Label statistics. Column 0 shows number of  publications per MC which did not meet the criteria and  column 1 shows number of publications which met the  criteria. The last column in the table shows proportion  of positive (i.e. criteria met) labels.", "labels": [], "entities": []}]}