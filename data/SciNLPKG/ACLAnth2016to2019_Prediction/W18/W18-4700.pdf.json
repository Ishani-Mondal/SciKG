{"title": [{"text": "Proceedings 14th Joint ACL -ISO Workshop on Interoperable Semantic Annotation (isa-14)", "labels": [], "entities": [{"text": "ACL -ISO Workshop on Interoperable Semantic Annotation", "start_pos": 23, "end_pos": 77, "type": "TASK", "confidence": 0.4714258089661598}]}], "abstractContent": [{"text": "We introduce a dataset containing human-authored descriptions of target locations in an \"endof-trip in a taxi ride\" scenario.", "labels": [], "entities": []}, {"text": "We describe our data collection method and a novel annotation scheme that supports understanding of such descriptions of target locations.", "labels": [], "entities": []}, {"text": "Our dataset contains target location descriptions for both synthetic and real-world images as well as visual annotations (ground truth labels, dimensions of vehicles and objects, coordinates of the target location, distance and direction of the target location from vehicles and objects) that can be used in various visual and language tasks.", "labels": [], "entities": []}, {"text": "We also perform a pilot experiment on how the corpus could be applied to visual reference resolution in this domain.", "labels": [], "entities": [{"text": "visual reference resolution", "start_pos": 73, "end_pos": 100, "type": "TASK", "confidence": 0.6868725220362345}]}], "introductionContent": [{"text": "The last few utterances in atypical taxi ride are the passengers directing the driver to stop their ride at the desired target location.", "labels": [], "entities": []}, {"text": "\"Stop right next to the white car\", \"behind the big tree should work\", \"drop me off in front of the second black pickup truck\" are all examples of such utterances.", "labels": [], "entities": []}, {"text": "Resolving these requests, while a simple task for the human drivers, assumes complex vision and language understanding capabilities.", "labels": [], "entities": []}, {"text": "The purpose of this work is to build a dataset that comprises of such utterances and build an annotation scheme supporting the understanding of such utterances.", "labels": [], "entities": []}, {"text": "We introduce a novel dataset which contains the human-authored natural language descriptions of the desired target location in an end-of-trip taxi ride scenario with synthetic images and real street images.", "labels": [], "entities": []}, {"text": "We describe the annotation scheme for these descriptions which comprises of referents, directional descriptions, and actions, and show that the inter-annotator agreement is high.", "labels": [], "entities": []}, {"text": "Our dataset contains the images with the ground-truth target location coordinates that are described by the users.", "labels": [], "entities": []}, {"text": "The image annotations also contain object ground-truth labels, coordinates, dimensions along with the distance and direction of the target location with respect to the objects that are present in the image.", "labels": [], "entities": []}, {"text": "We refer to the position of the target location as a function of 'r' and '\u2713' where 'r' is the magnitude of the vector, and \u2713 is the direction between the referent and the target location.", "labels": [], "entities": []}, {"text": "This quantification provides the capability to predict the target location coordinates using natural language sentences given the visual context.", "labels": [], "entities": []}, {"text": "shows an example where the combination of rand \u2713 determines the target location with respect to the referent(s).", "labels": [], "entities": []}, {"text": "The contributions of this work are: 1) A novel corpus containing user descriptions of target locations for synthetic and real-world street images.", "labels": [], "entities": []}, {"text": "2) The natural language description annotations along with the visual annotations for the task of target location prediction.", "labels": [], "entities": [{"text": "target location prediction", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.6579441428184509}]}, {"text": "3) A baseline model for the task of identification of referents from user descriptions.", "labels": [], "entities": [{"text": "identification of referents from user descriptions", "start_pos": 36, "end_pos": 86, "type": "TASK", "confidence": 0.8068953255812327}]}, {"text": "r: Example from the synthetic section of the dataset.", "labels": [], "entities": []}, {"text": "The annotation labels of rand \u2713 define the target location for an example utterance \"drop me off in front of the cop car\".", "labels": [], "entities": []}, {"text": "The green arrow shows the direction of motion of the taxi.", "labels": [], "entities": []}], "datasetContent": [{"text": "The recruited users were given images (digital photographs) sampled from the Visual Genome data set () which in turn were sampled from the MSCOCO data set ().", "labels": [], "entities": [{"text": "Visual Genome data set", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.8589245080947876}, {"text": "MSCOCO data set", "start_pos": 139, "end_pos": 154, "type": "DATASET", "confidence": 0.9747587045033773}]}, {"text": "The photos selected from the sampled image data sets were based on observations of 200 random request submissions from Zhopped and Reddit Photoshop forums.", "labels": [], "entities": []}, {"text": "The forum submissions were often about eight high-level categories of images: animals, city scenes, food, nature/landscapes, indoor scenes, people, sports, and vehicles.", "labels": [], "entities": []}, {"text": "Thus we selected images from the MSCOCO data set that fit into at least one of these eight categories.", "labels": [], "entities": [{"text": "MSCOCO data set", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.9539827704429626}]}, {"text": "Users were given one photograph from each category in an experiment session.", "labels": [], "entities": []}, {"text": "They were given time to think about the changes they wanted to perform before the dialogue session, and were informed about the tool that was going to be used and the fact that it did not support complex functionality.", "labels": [], "entities": []}, {"text": "If they were unsure of what functionality was supported they were instructed to ask the wizard.", "labels": [], "entities": []}, {"text": "Users were asked to perform as many edits as they desired per image.", "labels": [], "entities": []}, {"text": "Participants were encouraged (but not required) to participate for 40 minutes, and communicated via remote voice call.", "labels": [], "entities": []}, {"text": "Users did not have the freedom to perform the edits themselves.", "labels": [], "entities": []}, {"text": "Any edits they wished to be performed on the image had to be conveyed to the wizard through voice.", "labels": [], "entities": []}, {"text": "The wizard responded to the requests in a natural human-like manner.", "labels": [], "entities": []}, {"text": "The screen share feature was enabled on the wizard's screen so that the user could see in real time the wizard's edits on the image.", "labels": [], "entities": []}, {"text": "While users were not explicitly told that the wizard was human, this was obvious due to the naturalness of the conversation.", "labels": [], "entities": []}, {"text": "The interaction typically started with the user describing a given image to the wizard.", "labels": [], "entities": []}, {"text": "The wizard was not aware of the images provided to the user.", "labels": [], "entities": []}, {"text": "The wizard chose the image from the available images based on the user description; following user confirmation, the image was then loaded for editing.", "labels": [], "entities": []}, {"text": "The image editing session generally began with the user describing desired changes to the image in natural language.", "labels": [], "entities": [{"text": "image editing", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.8095352351665497}]}, {"text": "The wizard interpreted the request provided by the user and performed these edits on the image.", "labels": [], "entities": []}, {"text": "The interaction continued until the user was satisfied with the final outcome.", "labels": [], "entities": []}, {"text": "shows an example of an interaction between the user and the wizard.", "labels": [], "entities": []}, {"text": "We performed machine learning experiments to automatically predict the annotation labels in our corpus.", "labels": [], "entities": []}, {"text": "We build a separate classifier for SOC, POC, and \"other\" labels (3 classifiers in total).", "labels": [], "entities": []}, {"text": "This is because these labels are annotated independent of one another.", "labels": [], "entities": []}, {"text": "Each classifier could output one of the corresponding labels or a \"null\" label.", "labels": [], "entities": []}, {"text": "We use logistic regression in Weka (, and since no prior work exists a majority baseline for comparison.", "labels": [], "entities": []}, {"text": "The data were preprocessed before the classification was performed.", "labels": [], "entities": []}, {"text": "We used the NLTK toolkit for lemmatization) and removed stop words.", "labels": [], "entities": []}, {"text": "The features that we used were just words.", "labels": [], "entities": []}, {"text": "We report the results on 10-fold cross validation performed on the user sentences.", "labels": [], "entities": []}, {"text": "We predict the labels in two separate experiments: (i) \"unsegmented\" and (ii) \"segmented\".", "labels": [], "entities": []}, {"text": "For both settings we use the same set of features.", "labels": [], "entities": []}, {"text": "In the \"unsegmented\" version, we predict the classification labels using the complete sentences.", "labels": [], "entities": []}, {"text": "Each complete sentence is forwarded to the 3 classifiers and each classifier outputs one of its corresponding labels or the \"null\" label.", "labels": [], "entities": []}, {"text": "For the \"segmented\" approach we segment the sentences and use each segment as an input to each classifier.", "labels": [], "entities": []}, {"text": "Again each classifier outputs one of its corresponding labels or the \"null\" label.", "labels": [], "entities": []}, {"text": "In the \"segmented\" approach we assume oracle (perfect) segmentation of the user sentences before classification.", "labels": [], "entities": []}, {"text": "In future experiments we plan to perform the segmentation automatically and then predict the label.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.9629219174385071}]}, {"text": "Note however that the annotations can overlap, which means that an \"other\" label can be inside a section of the sentence annotated with a SOC or POC label.", "labels": [], "entities": []}, {"text": "Similarly, POC and SOC labels can overlap.", "labels": [], "entities": []}, {"text": "Hence we use 3 types of segmentations the output of which would be forwarded to each one of the 3 classifiers.", "labels": [], "entities": []}, {"text": "The segmentation for the SOC classifier would be: SEG1: Hi there SEG2: I would like to lose weight but exercising didn't help me much The segmentation for the POC classifier would be: SEG1: Hi there I would like to lose weight but SEG2: exercising SEG3: didn't help me much The segmentation for the \"other\" label classifier would be:: Classification results.", "labels": [], "entities": [{"text": "SOC classifier", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.809215247631073}]}, {"text": "The differences between the unsegmented and the segmented accuracies as well as the differences between the unsegmented and segmented accuracies and the majority baseline are significant (p < .05).", "labels": [], "entities": []}, {"text": "SEG1: Hi there SEG2: I would like to SEG3: lose weight SEG4: but exercising didn't help me much shows our results for each classifier: \"unsegmented\" majority baseline and accuracy using the \"segmented\" and \"unsegmented\" approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9995328187942505}]}, {"text": "We observe that the \"segmented\" approach results in higher classification accuracies.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Example annotations of dialogue acts, actions, and entities.", "labels": [], "entities": []}, {"text": " Table 3: Percentages of words and of utterance segments for each dialogue act type; \"0.0\" values are  close to 0.", "labels": [], "entities": []}, {"text": " Table 2: Statistics of event coreference clusters and cluster-level event relations in SW100. For brevity,  we use a prefix with 3 or 4 characters to refer to each domain.", "labels": [], "entities": []}, {"text": " Table 3: Inter-annotator agreement (Fleiss' Kappa) on event relations.", "labels": [], "entities": [{"text": "Fleiss' Kappa)", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.8318142493565878}]}, {"text": " TableTalk  4  2F/2M  2072  TableTalk  5  3F/2M  4740", "labels": [], "entities": [{"text": "TableTalk  4  2F/2M  2072  TableTalk  5  3F/2M  4740", "start_pos": 1, "end_pos": 53, "type": "DATASET", "confidence": 0.8119805703560511}]}, {"text": " Table 3: Floor occupancy in percentage (seconds in brackets) in chat and chunk for all conversations", "labels": [], "entities": [{"text": "Floor occupancy", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.785892128944397}, {"text": "chunk", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.9170517325401306}]}, {"text": " Table 1: Annotation of the Past Tense", "labels": [], "entities": []}, {"text": " Table 1: Statistics of the corpus collected.", "labels": [], "entities": []}, {"text": " Table 4: Classification results. The differences between the unsegmented and the segmented accuracies  as well as the differences between the unsegmented and segmented accuracies and the majority baseline  are significant (p < .05).", "labels": [], "entities": []}, {"text": " Table 1: Statistics of the dataset collected across the synthetic and the real-world images. The \"com- bined\" section contains the total unique tokens and common tokens shared across the descriptions in the  synthetic and real-world. We observe that the difference in the language is mainly related to the referent  descriptions.", "labels": [], "entities": []}]}