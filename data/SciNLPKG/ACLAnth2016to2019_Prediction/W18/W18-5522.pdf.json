{"title": [{"text": "DeFactoNLP: Fact Verification using Entity Recognition, TFIDF Vector Comparison and Decomposable Attention", "labels": [], "entities": [{"text": "Fact Verification", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.9431634843349457}]}], "abstractContent": [{"text": "In this paper, we describe DeFactoNLP 1 , the system we designed for the FEVER 2018 Shared Task.", "labels": [], "entities": [{"text": "FEVER 2018 Shared Task", "start_pos": 73, "end_pos": 95, "type": "DATASET", "confidence": 0.8158570230007172}]}, {"text": "The aim of this task was to conceive a system that cannot only automatically assess the veracity of a claim but also retrieve evidence supporting this assessment from Wikipedia.", "labels": [], "entities": []}, {"text": "In our approach, the Wikipedia documents whose Term Frequency-Inverse Document Frequency (TFIDF) vectors are most similar to the vector of the claim and those documents whose names are similar to those of the named entities (NEs) mentioned in the claim are identified as the documents which might contain evidence.", "labels": [], "entities": []}, {"text": "The sentences in these documents are then supplied to a textual entailment recognition module.", "labels": [], "entities": [{"text": "textual entailment recognition", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.6353374123573303}]}, {"text": "This module calculates the probability of each sentence supporting the claim, contradicting the claim or not providing any relevant information to assess the veracity of the claim.", "labels": [], "entities": []}, {"text": "Various features computed using these probabilities are finally used by a Random Forest clas-sifier to determine the overall truthfulness of the claim.", "labels": [], "entities": [{"text": "Random Forest clas-sifier", "start_pos": 74, "end_pos": 99, "type": "DATASET", "confidence": 0.8666621843973795}]}, {"text": "The sentences which support this classification are returned as evidence.", "labels": [], "entities": []}, {"text": "Our approach achieved 2 a 0.4277 evidence F1-score, a 0.5136 label accuracy and a 0.3833 FEVER score 3.", "labels": [], "entities": [{"text": "evidence F1-score", "start_pos": 33, "end_pos": 50, "type": "METRIC", "confidence": 0.7600571513175964}, {"text": "label", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9304882884025574}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.5892773270606995}, {"text": "FEVER score", "start_pos": 89, "end_pos": 100, "type": "METRIC", "confidence": 0.9815891981124878}]}, {"text": "* Work was completed while the author was a student at the Birla Institute of Technology and Science, India and was interning at SDA Research.", "labels": [], "entities": [{"text": "SDA Research", "start_pos": 129, "end_pos": 141, "type": "DATASET", "confidence": 0.7178584039211273}]}, {"text": "1 https://github.com/DeFacto/DeFactoNLP 2 The scores and ranks reported in this paper are provisional and were determined prior to any human evaluation of those evidences that were retrieved by the proposed systems but were not identified in the previous rounds of annotation.", "labels": [], "entities": [{"text": "DeFacto", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.9512895345687866}]}, {"text": "The organizers of the task plan to update these results after an additional round of annotation.", "labels": [], "entities": []}, {"text": "3 FEVER score measures the fraction of claims for which at least one complete set of evidences have been retrieved by the fact verification system.", "labels": [], "entities": [{"text": "FEVER score", "start_pos": 2, "end_pos": 13, "type": "METRIC", "confidence": 0.9828637540340424}]}], "introductionContent": [{"text": "Given the current trend of massive fake news propagation on social media, the world is desperately in need of automated fact checking systems.", "labels": [], "entities": [{"text": "fake news propagation on social media", "start_pos": 35, "end_pos": 72, "type": "TASK", "confidence": 0.8459991912047068}, {"text": "fact checking", "start_pos": 120, "end_pos": 133, "type": "TASK", "confidence": 0.7996830642223358}]}, {"text": "Automatically determining the authenticity of a fact is a challenging task that requires the collection and assimilation of a large amount of information.", "labels": [], "entities": [{"text": "Automatically determining the authenticity of a fact", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7912975123950413}]}, {"text": "To perform the task, a system is required to find relevant documents, detect and label evidences, and finally output a score which represents the truthfulness of the given claim.", "labels": [], "entities": []}, {"text": "The numerous design challenges associated with such systems are discussed by  and.", "labels": [], "entities": []}, {"text": "The Fact Extraction and Verification (FEVER) dataset ) is the first publicly available large-scale dataset designed to facilitate the training and testing of automated fact verification systems.", "labels": [], "entities": [{"text": "Fact Extraction and Verification (FEVER)", "start_pos": 4, "end_pos": 44, "type": "TASK", "confidence": 0.7427747036729541}]}, {"text": "The FEVER 2018 Shared Task required us to design such systems using this dataset.", "labels": [], "entities": [{"text": "FEVER 2018 Shared Task", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8532573282718658}]}, {"text": "The organizers had provided us a preprocessed version of the June 2017 Wikipedia dump in which the pages only contained the introductory sections of the respective Wikipedia pages.", "labels": [], "entities": []}, {"text": "Given a claim, we were asked to build systems which could determine if there were sentences supporting the claim (labelled as \"SUPPORTS\") or sentences refuting it (labelled as \"REFUTES\").", "labels": [], "entities": [{"text": "REFUTES", "start_pos": 177, "end_pos": 184, "type": "METRIC", "confidence": 0.978675365447998}]}, {"text": "If conclusive evidence either supporting or refuting the claim could not be found in the dump, the system should report the same (labelled \"NOT ENOUGH INFO\").", "labels": [], "entities": [{"text": "NOT", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.7960723638534546}, {"text": "ENOUGH", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.5927724838256836}, {"text": "INFO", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.6387154459953308}]}, {"text": "However, if conclusive evidence was found, it should also retrieve the sentences which either support or refute the claim.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: FEVER SNLI-style Dataset split sizes for EN- TAILMENT, CONTRADICTION and NEUTRAL classes", "labels": [], "entities": [{"text": "FEVER", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9831762313842773}, {"text": "EN- TAILMENT", "start_pos": 51, "end_pos": 63, "type": "METRIC", "confidence": 0.6091990272204081}]}, {"text": " Table 2: Macro and class-specific F1 scores achieved  on the FEVER SNLI-style test set", "labels": [], "entities": [{"text": "F1", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9790663123130798}, {"text": "FEVER", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9521248936653137}, {"text": "SNLI-style test set", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.7660373349984487}]}]}