{"title": [{"text": "UBC-NLP at IEST 2018: Learning Implicit Emotion With an Ensemble of Language Models", "labels": [], "entities": [{"text": "UBC-NLP at IEST 2018", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.8159885406494141}]}], "abstractContent": [{"text": "We describe UBC-NLP contribution to IEST-2018, focused at learning implicit emotion in Twitter data.", "labels": [], "entities": [{"text": "IEST-2018", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.720276415348053}]}, {"text": "Among the 30 participating teams, our system ranked the 4th (with 69.3% F-score).", "labels": [], "entities": [{"text": "F-score", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9992330074310303}]}, {"text": "Post competition, we were able to score slightly higher than the 3rd ranking system (reaching 70.7%).", "labels": [], "entities": []}, {"text": "Our system is trained on top of a pre-trained language model (LM), fine-tuned on the data provided by the task organizers.", "labels": [], "entities": []}, {"text": "Our best results are acquired by an average of an ensemble of language models.", "labels": [], "entities": []}, {"text": "We also offer an analysis of system performance and the impact of training data size on the task.", "labels": [], "entities": []}, {"text": "For example, we show that training our best model for only one epoch with < 40% of the data enables better performance than the baseline reported by Klinger et al.", "labels": [], "entities": []}, {"text": "(2018) for the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Emotion is essential inhuman experience and communication, lending special significance to natural language processing systems aimed at learning it.", "labels": [], "entities": []}, {"text": "Emotion detection systems can be applied in a host of domains, including health and well-being, user profiling, education, and marketing.", "labels": [], "entities": [{"text": "Emotion detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9217415153980255}]}, {"text": "There is a small, yet growing, body of NLP literature on emotion.", "labels": [], "entities": []}, {"text": "Early works focused on creating and manually labeling datasets.", "labels": [], "entities": []}, {"text": "The SemEval 2007 Affective Text task and are two examples that target the news and blog domains respectively.", "labels": [], "entities": [{"text": "SemEval 2007 Affective Text task", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.5701907932758331}]}, {"text": "In these works, data were labeled for the 6 basic emotions of Ekman.", "labels": [], "entities": [{"text": "Ekman", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.8122898936271667}]}, {"text": "More recent works exploit distant supervision) to automatically acquire emotion data for training systems.", "labels": [], "entities": []}, {"text": "More specifically, a number of works use hashtags like #happy and #sad, especially occurring finally in Twitter data, as a proxy of emotion (.", "labels": [], "entities": []}, {"text": "report state-of-the-art results using a large dataset acquired with hashtags.", "labels": [], "entities": []}, {"text": "Other works exploit emojis to capture emotion carrying data.", "labels": [], "entities": [{"text": "capture emotion carrying data", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.7265078350901604}]}, {"text": "introduce a third effective approach that leverages firstperson seed phrases like \"I'm happy that\" to collect emotion data.", "labels": [], "entities": []}, {"text": "propose yet a fourth method for collecting emotion data that depends on the existence of the expression \"emotion-word + one of the following words (when, that or because)\" in a tweet, regardless of the position of the emotion word.", "labels": [], "entities": []}, {"text": "In the \"Implicit Emotion\" shared task 1 , participants were provided data representing the 6 emotions in the set (anger, disgust, fear, joy, sad, surprise).", "labels": [], "entities": []}, {"text": "The trigger word was removed from each tweet.", "labels": [], "entities": []}, {"text": "To illustrate, the task is to predict the emotion in a tweet like \"Boys who like Starbucks make me because we can goon cute coffee dates\" (with the triggered word labeled as joy).", "labels": [], "entities": []}, {"text": "In this paper, we describe our system submitted as part of the competition.", "labels": [], "entities": []}, {"text": "Overall, our submission ranked the 4th out of the 30 participating teams.", "labels": [], "entities": []}, {"text": "With further experiments, we were able to acquire better results, which would rank our model at top 3 (70.7% F-score).", "labels": [], "entities": [{"text": "F-score", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9984315037727356}]}, {"text": "The rest of the paper is organized as follows: Section 2 describes the data.", "labels": [], "entities": []}, {"text": "Section 3 offers a description of the methods employed in our work.", "labels": [], "entities": []}, {"text": "Section 3 is where we present our results, and we perform an analysis of these results in Section 5.", "labels": [], "entities": []}, {"text": "We list negative experiments in Section 6 and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed a number of negative experiments that we report briefly here.", "labels": [], "entities": []}, {"text": "Our intuition is that training our models with Twitter-specific data should help classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.980609118938446}]}, {"text": "For this reason, we trained ULMFiT with 4.5 million tweets with the same settings reported in.", "labels": [], "entities": []}, {"text": "We did not find this set of experiments to yield gains over the results reported in, however.", "labels": [], "entities": []}, {"text": "For example, an Fwd LM trained on Twitter domain data yields 67.9% F-score, which is 1.4% less than the Fscore obtained by the Wikipedia-trained Fwd LM in 3.", "labels": [], "entities": [{"text": "Fwd LM trained on Twitter domain data", "start_pos": 16, "end_pos": 53, "type": "DATASET", "confidence": 0.7252721658774784}, {"text": "F-score", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9991570711135864}, {"text": "Fscore", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9978530406951904}]}, {"text": "The loss might be due to the smaller size of the Twitter data we train on, as compared to the Wikipedia data the ULMFiT is originally trained on (i.e., > 103 million tokens).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Network architecture and hyper-parameters  for experiments with simple pre-trained embeddings  with fastText 3.2.1 and ELMo 3.2.2 across our LSTM  and BiLSTM networks.", "labels": [], "entities": []}, {"text": " Table 2: Hyper-parameters for our submitted system  exploiting fine-tuned language models from Howard  and Ruder (2018).", "labels": [], "entities": []}, {"text": " Table 3: Results: BiLM refers to an ensemble of both  the Fwd and Bwd LMs.", "labels": [], "entities": [{"text": "BiLM", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9762580990791321}, {"text": "Fwd and Bwd LMs", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.795690655708313}]}]}