{"title": [{"text": "Discourse Embellishment Using a Deep Encoder-Decoder Network", "labels": [], "entities": [{"text": "Discourse Embellishment", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7079923152923584}]}], "abstractContent": [{"text": "We suggest anew NLG task in the context of the discourse generation pipeline of computational storytelling systems.", "labels": [], "entities": [{"text": "discourse generation pipeline", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.7824048399925232}]}, {"text": "This task, textual embellishment, is defined by taking a text as input and generating a semantically equivalent output with increased lexical and syntactic complexity.", "labels": [], "entities": [{"text": "textual embellishment", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.6998780965805054}]}, {"text": "Ideally, this would allow the authors of computational storytellers to implement just lightweight NLG systems and use a domain-independent embellishment module to translate its output into more literary text.", "labels": [], "entities": []}, {"text": "We present promising first results on this task using LSTM Encoder-Decoder networks trained on the Wiki-Large dataset.", "labels": [], "entities": [{"text": "Wiki-Large dataset", "start_pos": 99, "end_pos": 117, "type": "DATASET", "confidence": 0.9785815477371216}]}, {"text": "Furthermore, we introduce \"Compiled Computer Tales\", a corpus of computationally generated stories, that can be used to test the capabilities of embellishment algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "Narratives can be analyzed to consist of at least two layers: plot-what is told-and discoursehow it is told.", "labels": [], "entities": []}, {"text": "Usually, computational systems first generate the events of the plot, and then decide how to render these in text.", "labels": [], "entities": []}, {"text": "This means that a tight coupling exists between the knowledge bases that are used for plot generation, and the NLG modules for discourse rendering.", "labels": [], "entities": [{"text": "plot generation", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.8476974368095398}, {"text": "discourse rendering", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7696811854839325}]}, {"text": "The result is that it is hard to share these modules between systems, while the implementation of a custom module to generate literary discourse requires significant effort and linguistic expertise.", "labels": [], "entities": []}, {"text": "Indeed, our subjective feeling is that often it is possible to recognize repeating textual patterns when reading several stories created by the same system.", "labels": [], "entities": []}, {"text": "To alleviate this problems we suggest anew NLG task, textual embellishment (TE), with the goal to automatically increase the lexical and syntactic complexity of a text.", "labels": [], "entities": [{"text": "textual embellishment (TE)", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.7349265277385711}]}, {"text": "Textual embellishment can be understood as the inverse task to textual simplification (TS), which has been researched for at least two decades.", "labels": [], "entities": [{"text": "Textual embellishment", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7839857041835785}, {"text": "textual simplification (TS)", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.8378417134284973}]}, {"text": "Recent TS systems are trained on vast corpora using machine learning techniques and work independent of domains or handcrafted rules.", "labels": [], "entities": [{"text": "TS", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9764118194580078}]}, {"text": "These results open the intriguing possibility for an equally general resource for TE.", "labels": [], "entities": [{"text": "TE", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.944275438785553}]}, {"text": "This could allow the authors of storytelling systems to safe time by implementing only thin NLG modules and perform subsequent embellishment.", "labels": [], "entities": []}, {"text": "We would like to strongly caution that TE most likely will prove to be a more difficult task then TS.", "labels": [], "entities": [{"text": "TE", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.8416781425476074}, {"text": "TS", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.4830532371997833}]}, {"text": "While the latter typically results in removing information in a text, the former might lead to an automated adding of information which can introduce semantic contradictions.", "labels": [], "entities": []}, {"text": "However, on a syntactic level TE ideally just results in periphrastic constructions with no added information.", "labels": [], "entities": [{"text": "TE", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.8555454015731812}]}], "datasetContent": [], "tableCaptions": []}