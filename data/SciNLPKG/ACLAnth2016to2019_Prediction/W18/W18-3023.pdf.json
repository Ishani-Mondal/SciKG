{"title": [{"text": "Multilingual seq2seq training with similarity loss for cross-lingual document classification", "labels": [], "entities": [{"text": "similarity loss", "start_pos": 35, "end_pos": 50, "type": "METRIC", "confidence": 0.9522020220756531}, {"text": "cross-lingual document classification", "start_pos": 55, "end_pos": 92, "type": "TASK", "confidence": 0.6739487747351328}]}], "abstractContent": [{"text": "In this paper we continue the line of work where neural machine translation training is used to produce joint cross-lingual fixed-dimensional sentence embeddings.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.7073012789090475}]}, {"text": "In this framework we introduce a simple method of adding a loss to the learning objective which penalizes distance between representations of bilingually aligned sentences.", "labels": [], "entities": []}, {"text": "We evaluate cross-lingual transfer using two approaches, cross-lingual similarity search on an aligned corpus (Eu-roparl) and cross-lingual document classification on a recently published benchmark Reuters corpus, and we find the similarity loss significantly improves performance on both.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.7837618887424469}, {"text": "cross-lingual similarity search", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.5864574015140533}, {"text": "cross-lingual document classification", "start_pos": 126, "end_pos": 163, "type": "TASK", "confidence": 0.6270409822463989}, {"text": "Reuters corpus", "start_pos": 198, "end_pos": 212, "type": "DATASET", "confidence": 0.9249331653118134}]}, {"text": "Our cross-lingual transfer performance is competitive with state-of-the-art, even while there is potential to further improve by investing in a better in-language baseline.", "labels": [], "entities": []}, {"text": "Our results are based on a set of 6 European languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many real-world services collect data in many languages, and machine learning models on text need to support these languages.", "labels": [], "entities": []}, {"text": "In practice, however, it is often only the top one or two dominant languages (usually English) which are supported because it is expensive to collect labeled training data for the task in every language.", "labels": [], "entities": []}, {"text": "It is desirable, therefore, to obtain a representation of sequences of text that is joint across all languages, which allows for cross-lingual transfer on the languages without labeled data.", "labels": [], "entities": []}, {"text": "These representations typically take the form of a fixed-size embedding representing a complete sentence or document.", "labels": [], "entities": []}, {"text": "Previous work has focused on several approaches in this setting, all of which rely on parallel corpora.", "labels": [], "entities": []}, {"text": "In), a predictive auto-encoder is used to reconstruct the featurized representation of a pair of sentences.) constructs a bilingual sentence embedding by minimizing the squared distance between the embeddings of parallel sentences.", "labels": [], "entities": []}, {"text": "() learns a common representation by simultaneously predicting n-grams in both languages from a common vector.", "labels": [], "entities": []}, {"text": "In, a similarity measure is used to minimize distance on both the sentence embeddings, and the average of the word embeddings of a pair of sentences.", "labels": [], "entities": []}, {"text": "A method is also proposed to apply this approach to label-aligned corpora in the absence of sentence-aligned corpora by doing a pre-alignment.", "labels": [], "entities": []}, {"text": "Finally, multilingual representations can be learned using a sequence-to-sequence encoderdecoder neural machine translation (NMT) architecture, such as the one introduced in).", "labels": [], "entities": [{"text": "sequence-to-sequence encoderdecoder neural machine translation (NMT)", "start_pos": 61, "end_pos": 129, "type": "TASK", "confidence": 0.7751955166459084}]}, {"text": "Multilingual encoders have been successfully demonstrated in the NMT setting.", "labels": [], "entities": []}, {"text": "Recently ( ) has proposed using this framework for generating multilingual sentence representations and apply it to cross-lingual document classification.", "labels": [], "entities": [{"text": "cross-lingual document classification", "start_pos": 116, "end_pos": 153, "type": "TASK", "confidence": 0.7134552597999573}]}, {"text": "In this paper, we combine this NMT approach with the pairwise similarity approach to obtain better representations.", "labels": [], "entities": []}, {"text": "In section 2 we describe our framework.", "labels": [], "entities": []}, {"text": "Then in section 4 we present an evaluation of our method based on measuring similarity on the multiply aligned Europarl corpus).", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 111, "end_pos": 126, "type": "DATASET", "confidence": 0.991724282503128}]}, {"text": "Section 5 contains our cross-lingual document classification experiments on the balanced version of the Reuters Corpus Volume 2 dataset (RCV2b), recently published by resampling from the Reuters Corpus Volume 2 to have a balanced distribution of languages and a similar label distribution for each language.", "labels": [], "entities": [{"text": "cross-lingual document classification", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.6144136190414429}, {"text": "Reuters Corpus Volume 2 dataset (RCV2b)", "start_pos": 104, "end_pos": 143, "type": "DATASET", "confidence": 0.9674778655171394}, {"text": "Reuters Corpus Volume 2", "start_pos": 187, "end_pos": 210, "type": "DATASET", "confidence": 0.9588582068681717}]}], "datasetContent": [{"text": "Evaluation paradigm Mean accuracy Zero-shot transfer 74.6 Targeted transfer 75.6) using word embeddings and sentence embeddings; however, these are not directly comparable given we are using significantly more training data.", "labels": [], "entities": [{"text": "Mean accuracy", "start_pos": 20, "end_pos": 33, "type": "METRIC", "confidence": 0.6872670948505402}, {"text": "Zero-shot transfer 74.6 Targeted transfer", "start_pos": 34, "end_pos": 75, "type": "TASK", "confidence": 0.6651765942573548}]}, {"text": "Finally, shows a t-SNE representation of the document embeddings over the four classes on a sample of RCVBalanced dataset.", "labels": [], "entities": [{"text": "RCVBalanced dataset", "start_pos": 102, "end_pos": 121, "type": "DATASET", "confidence": 0.9811974465847015}]}, {"text": "We also try \"leaving one out\" (LOO) where we pool training data overall languages except the target to augment training data, while tuning to the English development set.", "labels": [], "entities": []}, {"text": "However results do not improve over the best single-language transfer numbers (last row in table 4).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SentEval results: performance as a sentence encoder in English", "labels": [], "entities": [{"text": "SentEval", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.9319993257522583}]}, {"text": " Table 2: Europarl (5k) similarity retrieval accuracy from training { without encoder regularization / with  encoder weight regularization / with similarity loss }. Some combinations are omitted for space.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.9285403490066528}, {"text": "similarity retrieval", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.6544376015663147}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9516647458076477}]}, {"text": " Table 3: Example top 3 retrieved sentences in Europarl 5k: the correctly retrieved sentence is omitted.", "labels": [], "entities": [{"text": "Europarl 5k", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.9860432744026184}]}, {"text": " Table 5: Comparison of aggregation methods for  document embedding (RCV2Balanced)", "labels": [], "entities": []}, {"text": " Table 4: Cross-lingual document classification results (RCV2Balanced): from training { without encoder  regularization / with similarity loss }. Zero-shot paradigm. Bold indicates best result for target language.", "labels": [], "entities": [{"text": "Cross-lingual document classification", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.6628389159838358}]}]}