{"title": [{"text": "RUSE: Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation", "labels": [], "entities": [{"text": "RUSE", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.7592186331748962}, {"text": "Automatic Machine Translation Evaluation", "start_pos": 46, "end_pos": 86, "type": "TASK", "confidence": 0.7076815664768219}]}], "abstractContent": [{"text": "We introduce the RUSE 1 metric for the WMT18 metrics shared task.", "labels": [], "entities": [{"text": "RUSE 1 metric", "start_pos": 17, "end_pos": 30, "type": "METRIC", "confidence": 0.9780771334966024}, {"text": "WMT18 metrics shared task", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.5930338501930237}]}, {"text": "Sentence em-beddings can capture global information that cannot be captured by local features based on character or word N-grams.", "labels": [], "entities": []}, {"text": "Although training sentence embeddings using small-scale translation datasets with manual evaluation is difficult, sentence embeddings trained from large-scale data in other tasks can improve the automatic evaluation of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 219, "end_pos": 238, "type": "TASK", "confidence": 0.7669963836669922}]}, {"text": "We use a multi-layer perceptron re-gressor based on three types of sentence em-beddings.", "labels": [], "entities": []}, {"text": "The experimental results of the WMT16 and WMT17 datasets show that the RUSE metric achieves a state-of-the-art performance in both segment-and system-level metrics tasks with embedding features only.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.932793378829956}, {"text": "WMT17 datasets", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.9098689258098602}, {"text": "RUSE", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.8171961307525635}]}], "introductionContent": [{"text": "This study describes a segment-level metric for automatic machine translation evaluation (MTE).", "labels": [], "entities": [{"text": "machine translation evaluation (MTE)", "start_pos": 58, "end_pos": 94, "type": "TASK", "confidence": 0.8564637899398804}]}, {"text": "The MTE metrics with a high correlation with human evaluation enable the continuous integration and deployment of a machine translation (MT) system.", "labels": [], "entities": [{"text": "MTE", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.919450044631958}, {"text": "machine translation (MT)", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.8464777708053589}]}, {"text": "Various MTE metrics have been proposed in the metrics task of the Workshops on Statistical Machine Translation (WMT) that was started in 2008.", "labels": [], "entities": [{"text": "MTE", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9617867469787598}, {"text": "Statistical Machine Translation (WMT)", "start_pos": 79, "end_pos": 116, "type": "TASK", "confidence": 0.7625564833482107}]}, {"text": "However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference based on the character or word N-grams, such as SentBLEU (), which is a smoothed version of BLEU (), Blend (, MEANT 2.0 (, and chrF++.", "labels": [], "entities": [{"text": "MTE", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9768242239952087}, {"text": "BLEU", "start_pos": 200, "end_pos": 204, "type": "METRIC", "confidence": 0.9904835224151611}, {"text": "Blend", "start_pos": 209, "end_pos": 214, "type": "METRIC", "confidence": 0.6259235739707947}]}, {"text": "Therefore, they can exploit only limited information for the segment-level MTE.", "labels": [], "entities": [{"text": "MTE", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.7834064960479736}]}, {"text": "In other words, the MTE metrics based on character or word N-grams cannot make full use of sentence embeddings.", "labels": [], "entities": [{"text": "MTE", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.8716474771499634}]}, {"text": "They only check for word matches.", "labels": [], "entities": []}, {"text": "We extend our previous work) and propose a segment-level MTE metric using universal sentence embeddings capable of capturing global information that cannot be captured by local features based on character or word N-grams.", "labels": [], "entities": [{"text": "MTE", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9113526940345764}]}, {"text": "The experimental results in both segment-and system-level metrics tasks conducted using the datasets for to-English language pairs on WMT16 and WMT17 indicated that the proposed regression model using sentence embeddings, RUSE, achieves the best performance.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 134, "end_pos": 139, "type": "DATASET", "confidence": 0.9885857701301575}, {"text": "WMT17", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.743312418460846}, {"text": "RUSE", "start_pos": 222, "end_pos": 226, "type": "METRIC", "confidence": 0.8571690917015076}]}, {"text": "The main contributions of the study are summarized below: \u2022 We propose a novel supervised regression model for the segment-level MTE based on universal sentence embeddings.", "labels": [], "entities": [{"text": "MTE", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.8104552626609802}]}, {"text": "\u2022 We achieved a state-of-the-art performance in segment-and system-level metrics tasks on the WNT16 and WMT17 datasets for to-English language pairs without using any complex features.), using ranking SVM to train parameters of each metric score.", "labels": [], "entities": [{"text": "WNT16", "start_pos": 94, "end_pos": 99, "type": "DATASET", "confidence": 0.9772588014602661}, {"text": "WMT17 datasets", "start_pos": 104, "end_pos": 118, "type": "DATASET", "confidence": 0.8035937547683716}]}, {"text": "DPMF evaluates the syntactic similarity between an MT hypothesis and a reference translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.955289363861084}]}, {"text": "REDp evaluates an MT hypothesis based on the dependency tree of the reference translation that comprises both lexical and syntactic information.", "labels": [], "entities": [{"text": "MT", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9902442097663879}]}, {"text": "ENTFp ( evaluates the fluency of an MT hypothesis. After the success of DPMF comb , Blend) achieved the best performance in the WMT17 metrics task (.", "labels": [], "entities": [{"text": "ENTFp", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6243875622749329}, {"text": "MT hypothesis.", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.9223476946353912}, {"text": "Blend", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.877112090587616}, {"text": "WMT17 metrics task", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.47363056739171344}]}, {"text": "Similar to DPMF comb , Blend is essentially an SVR model with RBF kernel that uses the scores of various metrics as features.", "labels": [], "entities": [{"text": "Blend", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9091532826423645}]}, {"text": "It incorporates 25 lexical metrics provided by the Asiya MT evaluation toolkit, as well as four other metrics, namely BEER, Charac-TER (, DPMF, and ENTFp.", "labels": [], "entities": [{"text": "Asiya MT evaluation toolkit", "start_pos": 51, "end_pos": 78, "type": "DATASET", "confidence": 0.5915800780057907}, {"text": "BEER", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9980923533439636}]}, {"text": "BEER is a linear model based on character Ngrams and replacement trees.", "labels": [], "entities": [{"text": "BEER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9108996391296387}]}, {"text": "CharacTER evaluates an MT hypothesis based on character-level edit distance.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9894946217536926}]}, {"text": "DPMF comb is trained through relative ranking (RR) of human evaluation data in terms of relative ranking (RR).", "labels": [], "entities": [{"text": "relative ranking (RR)", "start_pos": 88, "end_pos": 109, "type": "METRIC", "confidence": 0.8699036002159118}]}, {"text": "The quality of five MT hypotheses of the same source segment is ranked from 1 to 5 via a comparison with the reference translation.", "labels": [], "entities": [{"text": "MT hypotheses", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.8954207897186279}]}, {"text": "In contrast, Blend is trained through direct assessment (DA) of human evaluation data.", "labels": [], "entities": [{"text": "Blend", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.4976673424243927}]}, {"text": "DA provides the absolute quality scores of hypotheses by measuring to what extent a hypothesis adequately expresses the meaning of the reference translation.", "labels": [], "entities": []}, {"text": "The experiment results in the segment-level MTE conducted using the datasets for to-English language pairs on WMT16 showed that Blend achieved a performance better than DPMF comb . In this study, as with Blend, we propose a regression model trained using DA human evaluation data.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 110, "end_pos": 115, "type": "DATASET", "confidence": 0.9858209490776062}, {"text": "Blend", "start_pos": 128, "end_pos": 133, "type": "METRIC", "confidence": 0.9635809063911438}]}, {"text": "Instead of using local and lexical features, ReVal proposes using sentence-level features.", "labels": [], "entities": []}, {"text": "It is a metric using Tree-LSTM () for training and capturing the holistic information of sentences.", "labels": [], "entities": []}, {"text": "It is trained using datasets of pseudo similarity scores generated by translating RR data and out-domain datasets of similarity scores of SICK 5 . However, the training dataset used in this metric consists of approximately 21,000 sentences; thus, the learning of Tree-LSTM is unstable, and accurate learning is difficult.", "labels": [], "entities": []}, {"text": "We use sentence embeddings trained using various RNN and Transformer as sentence information.", "labels": [], "entities": []}, {"text": "Furthermore, we apply universal sentence embeddings to this task.", "labels": [], "entities": []}, {"text": "These embeddings were trained using large-scale data obtained in other tasks.", "labels": [], "entities": []}, {"text": "Therefore, the proposed approach avoids the problem of using a small dataset for training sentence embeddings.", "labels": [], "entities": []}, {"text": "cs-en de-en fi-en lv-en ro-en ru-en tr-en zh-en WMT15 500 500 500 --500 --WMT16 560 560 560 -560 560 560 -WMT17 560 560 560 560 -560 560 560", "labels": [], "entities": [{"text": "WMT15 500 500 500 --500 --WMT16 560 560 560 -560 560 560 -WMT17 560 560 560 560 -560", "start_pos": 48, "end_pos": 132, "type": "DATASET", "confidence": 0.8007048381411511}]}], "datasetContent": [{"text": "We performed experiments using the evaluation datasets of the WMT metrics task to verify the performance of the proposed metric.", "labels": [], "entities": [{"text": "WMT metrics task", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.7117616931597391}]}], "tableCaptions": [{"text": " Table 1: Number of segment-level DA human evaluation datasets for to-English language pairs 10 in  WMT15 (Stanojevi\u00b4cStanojevi\u00b4c et al., 2015), WMT16 (Bojar et al., 2016), and WMT17 (Bojar et al., 2017).", "labels": [], "entities": [{"text": "WMT15", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.8896780610084534}, {"text": "WMT16", "start_pos": 145, "end_pos": 150, "type": "DATASET", "confidence": 0.937831461429596}, {"text": "WMT17", "start_pos": 177, "end_pos": 182, "type": "DATASET", "confidence": 0.947273313999176}]}, {"text": " Table 2: Number of MT systems and system-level DA human evaluation datasets for to-English language pairs in  WMT16 (Bojar et al., 2016) and WMT17 (Bojar et al., 2017).", "labels": [], "entities": [{"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.977756142616272}, {"text": "WMT16", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.9464383721351624}, {"text": "WMT17", "start_pos": 142, "end_pos": 147, "type": "DATASET", "confidence": 0.959317147731781}]}, {"text": " Table 4: Segment-level Pearson correlation of metric scores and DA human evaluation scores for to-English lan- guage pairs in WMT17. IS: InferSent; QT: Quick-Thought; and USE: Universal Sentence Encoder.", "labels": [], "entities": [{"text": "Segment-level Pearson correlation of metric scores", "start_pos": 10, "end_pos": 60, "type": "METRIC", "confidence": 0.7306085626284281}, {"text": "DA human evaluation scores", "start_pos": 65, "end_pos": 91, "type": "METRIC", "confidence": 0.864516094326973}, {"text": "WMT17", "start_pos": 127, "end_pos": 132, "type": "DATASET", "confidence": 0.9398632049560547}, {"text": "USE", "start_pos": 172, "end_pos": 175, "type": "DATASET", "confidence": 0.7186447978019714}]}, {"text": " Table 7: Ablation analysis on the segment-level dataset in WMT17.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9635094404220581}, {"text": "WMT17", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.5002970695495605}]}, {"text": " Table 8: Ablation analysis on the system-level dataset in WMT17.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9551723003387451}, {"text": "WMT17", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.7348839044570923}]}]}