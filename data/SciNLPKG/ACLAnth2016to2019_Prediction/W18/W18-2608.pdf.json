{"title": [{"text": "RECIPE: Applying Open Domain Question Answering to Privacy Policies", "labels": [], "entities": [{"text": "RECIPE", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.7974398732185364}, {"text": "Applying Open Domain Question Answering", "start_pos": 8, "end_pos": 47, "type": "TASK", "confidence": 0.6593354701995849}]}], "abstractContent": [{"text": "We describe our experiences in using an open domain question answering model (Chen et al., 2017) to evaluate an out-of-domain QA task of assisting in analyzing privacy policies of companies.", "labels": [], "entities": [{"text": "open domain question answering", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6600457057356834}]}, {"text": "Specifically, Relevant CI Parameters Ex-tractor (RECIPE) seeks to answer questions posed by the theory of contextual integrity (CI) regarding the information flows described in the privacy statements.", "labels": [], "entities": []}, {"text": "These questions have a simple syntactic structure and the answers are factoids or descriptive in nature.", "labels": [], "entities": []}, {"text": "The model achieved an F1 score of 72.33, but we noticed that combining the results of this model with a neural dependency parser based approach yields a significantly higher F1 score of 92.35 compared to manual annotations.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9881903231143951}, {"text": "F1 score", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9878520667552948}]}, {"text": "This indicates that future work which incorporates signals from parsing like NLP tasks more explicitly can generalize better on out-of-domain tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open domain question answering approaches offer a promising glimpse into a future in which machines are able to perform sophisticated cognitive tasks on behalf of a human.", "labels": [], "entities": [{"text": "question answering", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7047840505838394}]}, {"text": "Recent advances in deep neural networks applied to reading comprehension and document retrieval) have achieved competitive results for answering questions based on a large and diverse corpus of documents.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.7606543302536011}]}, {"text": "These models are trained on Wikipedia text and are capable of answering various factoid and descriptive questions.", "labels": [], "entities": []}, {"text": "Specifically, the distribution of the interrogative words used, lexical and syntactic variations, reasoning across multiple sentences and ambiguous statements in such Wikipedia datasets () results in a robust QA model.", "labels": [], "entities": []}, {"text": "Motivated by their success, we set to apply one such model to evaluate an outof-domain QA task to assist in analysis and understanding of privacy policies.", "labels": [], "entities": []}, {"text": "The text in privacy policies is notoriously cumbersome, confusing and hard to comprehend even for legal experts (.", "labels": [], "entities": []}, {"text": "So when it comes to reading privacy policies, users often miss important information or skip reading them altogether.", "labels": [], "entities": []}, {"text": "Past efforts have applied NLP (), Machine Learning () and crowdsourcing () techniques to identify important and relevant privacy statements.", "labels": [], "entities": []}, {"text": "However, identifying paragraphs that mention sensitive information in privacy policies only takes us halfway because we still need to understand who collects that information, who receives it, and under what conditions the collection happens.", "labels": [], "entities": []}, {"text": "To support this finer-grained analysis, we present a case for using an machine comprehension model for answering questions posed by the theory of contextual integrity (CI).", "labels": [], "entities": []}, {"text": "To understand the privacy implications, the CI theory (see Section 2) calls for answering to the following questions: who are the actors (sender, recipient, subject) involved in the information flow?", "labels": [], "entities": []}, {"text": "What are the type of information (attribute), and condition (transmission principle) under which information is exchanged?", "labels": [], "entities": []}, {"text": "The answers are used to formulate information flows (CI flows) into a five element tuple (sender, attribute, receiver, subject, transmission principle).", "labels": [], "entities": []}, {"text": "Unfortunately, because privacy policies are not written with CI in mind, manually identifying CI flows in text is a time consuming exercise.", "labels": [], "entities": []}, {"text": "It requires substantial cognitive effort to understand and answer CI related questions.", "labels": [], "entities": []}, {"text": "Automating this task is not trivial either.", "labels": [], "entities": []}, {"text": "The syntax of privacy statements varies a lot, some relevant information might not be specified at all.", "labels": [], "entities": []}, {"text": "So, the answers are not always obvious and cannot be identified using a simple model.", "labels": [], "entities": []}, {"text": "Even with the simple syntactic structure of the questions, the trained model needs to take into account the variations and complexity in syntax and semantics used in privacy policies.", "labels": [], "entities": []}, {"text": "To help address these issues, we are designing a Relevant CI Parameters Extractor (RECIPE) that uses a pre-trained 3 layer bi-directional LSTM reading comprehension (RC) model.", "labels": [], "entities": []}, {"text": "Our experiments show that, in itself, the model achieved an F1 score of 72.33 for identifying answers in the text.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9870725274085999}]}, {"text": "However, the results significantly improved by combining the results from the model and that of a neural dependency parser.", "labels": [], "entities": []}, {"text": "The combination of the two approaches yielded an overall F1 score of 92.35 against the baseline of six manually annotated privacy policies.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9861792922019958}]}], "datasetContent": [{"text": "For our evaluation, we analyze the ensemble algorithm against each of its sub-algorithms using manually annotated policies as a baseline.", "labels": [], "entities": []}, {"text": "We use OPP-115 Corpus () that contains website privacy policies in natural text alongside annotations (done bylaw students) specifying the corresponding text data prac- shows the F1 scores of the reading comprehension model alone.", "labels": [], "entities": [{"text": "OPP-115 Corpus", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.9161825776100159}, {"text": "F1", "start_pos": 179, "end_pos": 181, "type": "METRIC", "confidence": 0.996491014957428}]}, {"text": "We see that subject, receiver and sender have relatively low F1 scores and this is due to the absence of entities in the paragraphs as they are usually referred to as pronouns.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9795902073383331}]}, {"text": "We noticed that the reading comprehension model outputs such pronoun answers only for 15% of these parameters and prefers to answer with often incorrect entities for these questions.", "labels": [], "entities": []}, {"text": "shows the results from using the dependency parser on its own.", "labels": [], "entities": []}, {"text": "Since dependency parsing is restricted to a single sentence, it misses out on some contextual information spread across sentences as shown in, leading to a loss inaccuracy, even after manual validation.  by each of the individual models (\"Parsing only\", \"RC only\") demonstrate how each model is able to capture syntactic and semantic features respectively, which the other could not.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.8155897557735443}]}, {"text": "Column \"DP \u2229 RC\" shows that many parameters are extracted by both the approaches used to formulate CI parameter extraction.", "labels": [], "entities": [{"text": "formulate CI parameter extraction", "start_pos": 89, "end_pos": 122, "type": "TASK", "confidence": 0.8348827213048935}]}, {"text": "This confirms that the task of CI parameter extraction is non-trivial and composes semantic and syntactic relationship extraction within it.", "labels": [], "entities": [{"text": "CI parameter extraction", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.695673664410909}, {"text": "syntactic relationship extraction", "start_pos": 96, "end_pos": 129, "type": "TASK", "confidence": 0.7384860714276632}]}, {"text": "Finally, \"DP \u222a RC\" achieves the highest F1 score, significantly improving over both of the component scores individually.", "labels": [], "entities": [{"text": "DP \u222a RC", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8322041432062784}, {"text": "F1 score", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9922366440296173}]}, {"text": "shows the number of incorrect answers yielded by the hybrid approach, i.e., the answers where neither RC nor DP performed well.", "labels": [], "entities": [{"text": "DP", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.8838708400726318}]}], "tableCaptions": [{"text": " Table 3: Validated F1 score of manually validated  reading comprehension based annotation", "labels": [], "entities": [{"text": "F1 score", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9769962728023529}]}, {"text": " Table 4: F1 scores of dependency parsing based  annotation", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9989991784095764}, {"text": "dependency parsing based  annotation", "start_pos": 23, "end_pos": 59, "type": "TASK", "confidence": 0.8222233057022095}]}, {"text": " Table 5: Validated F1 score by ensembling", "labels": [], "entities": [{"text": "Validated", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.542130708694458}, {"text": "F1 score", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9224079847335815}]}]}