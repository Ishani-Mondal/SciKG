{"title": [{"text": "The JHU Machine Translation Systems for WMT 2018 Anonymous EMNLP submission", "labels": [], "entities": [{"text": "JHU Machine Translation", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8580768704414368}, {"text": "WMT 2018 Anonymous EMNLP submission", "start_pos": 40, "end_pos": 75, "type": "DATASET", "confidence": 0.6099091947078705}]}], "abstractContent": [{"text": "We report on the efforts of the Johns Hopkins University to develop neural machine translation systems for the shared task for news translation organized around the Conference for Machine Translation (WMT) 2018.", "labels": [], "entities": [{"text": "news translation organized around the Conference for Machine Translation (WMT)", "start_pos": 127, "end_pos": 205, "type": "TASK", "confidence": 0.7664744580785433}]}, {"text": "We developed systems for German-English, English-German, and Russian-English.", "labels": [], "entities": []}, {"text": "Our novel contributions are iterative back-translation and fine-tuning on test sets from prior years.", "labels": [], "entities": []}], "introductionContent": [{"text": "We carried out two relatively independent efforts on German-English language directions and Russian-English, using the Marian and Sockeye neural machine translation toolkits, respectively.", "labels": [], "entities": [{"text": "German-English language directions", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.5719672739505768}, {"text": "Sockeye neural machine translation", "start_pos": 130, "end_pos": 164, "type": "TASK", "confidence": 0.68573297560215}]}, {"text": "The German-English systems outperformed last year's best result (37.0 vs. 35.1 (+1.9) for German-English, 29.1 vs. 28.3 (+0.8) for English-German), but fell short against this year's best performing systems (45.3 vs. 48.4 (-3.1) and 43.4 vs. 48.3 (-4.9), respectively) . The best models this year used the Transformer model instead of the recurrent neural networks that our models are based on.", "labels": [], "entities": []}, {"text": "Our novel contributions are iterative back-translation and fine-tuning on prior test sets.", "labels": [], "entities": []}, {"text": "For Russian-English, we carried out extensive hyperparameter search, with different numbers of layers, embedding and hidden state sizes, and drop-out settings.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Shallow and deep systems for German-English with Marian. 4 independent training runs, with  checkpoint ensemble, and merging the checkpoint ensemble into a single model (averaging parameters).  Ensemble of the runs, with right-to-left reranking (4 independent right-to-left runs).", "labels": [], "entities": []}, {"text": " Table 1. Merging  the checkpoint models worked better for German-", "labels": [], "entities": [{"text": "Merging", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8003082871437073}]}, {"text": " Table 2: System development with deep models us- ing iterative back-translation.", "labels": [], "entities": []}, {"text": " Table 3: Impact of the quality of the back- translation system on the final system performace.", "labels": [], "entities": []}, {"text": " Table 4: Final refinements: a model trained with  the unfiltered Paracrawl corpus, an ensemble of  the 4 iterative back-translation models, plus the", "labels": [], "entities": [{"text": "Paracrawl corpus", "start_pos": 66, "end_pos": 82, "type": "DATASET", "confidence": 0.9466868340969086}]}, {"text": " Table 5. The BLEU scores for  continued training after 3 or 9 epochs are shown,  along with their difference \u2206 against base. Con- tinued training with few epochs improve results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.99950110912323}]}, {"text": " Table 6.  The difference \u2206 is gain with respect to the best  single model (a), with BLEU 31.7.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9982594847679138}]}]}