{"title": [{"text": "A Characterwise Windowed Approach to Hebrew Morphological Segmentation", "labels": [], "entities": [{"text": "Hebrew Morphological Segmentation", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.843932588895162}]}], "abstractContent": [{"text": "This paper presents a novel approach to the segmentation of orthographic word forms in contemporary Hebrew, focusing purely on splitting without carrying out morphological analysis or disambiguation.", "labels": [], "entities": [{"text": "segmentation of orthographic word forms", "start_pos": 44, "end_pos": 83, "type": "TASK", "confidence": 0.8200735211372375}]}, {"text": "Casting the analysis task as character-wise binary classification and using adjacent character and word-based lexicon-lookup features, this approach achieves over 98% accuracy on the benchmark SPMRL shared task data for Hebrew, and 97% accuracy on anew out of domain Wikipedia dataset, an improvement of \u22484% and 5% over previous state of the art performance.", "labels": [], "entities": [{"text": "character-wise binary classification", "start_pos": 29, "end_pos": 65, "type": "TASK", "confidence": 0.7073937853177389}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9988969564437866}, {"text": "SPMRL shared task data", "start_pos": 193, "end_pos": 215, "type": "DATASET", "confidence": 0.656885914504528}, {"text": "accuracy", "start_pos": 236, "end_pos": 244, "type": "METRIC", "confidence": 0.9991347193717957}, {"text": "Wikipedia dataset", "start_pos": 267, "end_pos": 284, "type": "DATASET", "confidence": 0.8384337425231934}]}], "introductionContent": [{"text": "Hebrew is a morphologically rich language in which, as in Arabic and other similar languages, space-delimited word forms contain multiple units corresponding to what other languages (and most POS tagging/parsing schemes) consider multiple words.", "labels": [], "entities": [{"text": "POS tagging/parsing", "start_pos": 192, "end_pos": 211, "type": "TASK", "confidence": 0.8670721352100372}]}, {"text": "To avoid confusion, the rest of this paper will use the term 'super-token' for large units such as the entire contents of (1) and 'subtoken' for the smaller units.", "labels": [], "entities": []}, {"text": "Beyond this, three issues complicate Hebrew segmentation further.", "labels": [], "entities": [{"text": "Hebrew segmentation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8457328677177429}]}, {"text": "Firstly, much like Arabic and similar languages, Hebrew uses a consonantal script which disregards most vowels.", "labels": [], "entities": []}, {"text": "For example in (2) the sub-tokens ve 'and' and \u0161e 'that' are spelled as single letters wand \u0161.", "labels": [], "entities": []}, {"text": "While some vowels are represented (mainly word-final or etymologically long vowels), these are denoted by the same letters as consonants, e.g. the final -u in ve\u0161emtsa'uhu, spelled as w, the same letter used for ve 'and'.", "labels": [], "entities": []}, {"text": "This means that syllable structure, a crucial cue for segmentation, is obscured.", "labels": [], "entities": []}, {"text": "A second problem is that unlike Arabic, Hebrew has morpheme reductions creating segments which are pronounced, but not distinguished in the orthography.", "labels": [], "entities": []}, {"text": "This affects the definite article after some prepositions, as in (3) and (4), which mean 'in a house' and 'in the house' respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since comparable segmentation systems, including the previous state of the art, do insert reconstructed articles and carryout base form transformations (i.e. they aim to produce the gold format in  previously published scores.", "labels": [], "entities": []}, {"text": "All systems were rerun on both datasets, and no errors were counted where these resulted from data alterations.", "labels": [], "entities": []}, {"text": "Specifically, other systems are never penalized for reconstructing or omitting inserted units such as articles, and when constructing clitics or alternate base forms of nouns or verbs, only the presence or absence of a split was checked, not whether inferred noun or verb lemmas are correct.", "labels": [], "entities": []}, {"text": "This leads to higher scores than previously reported, but makes results comparable across systems on the pure segmentation task.", "labels": [], "entities": []}, {"text": "shows the results of several systems on both datasets.", "labels": [], "entities": []}, {"text": "The column '% perf' indicates the proportion of perfectly segmented super-tokens, while the next three columns indicate precision, recall and F-score for boundary detection, not including the trivial final position characters.", "labels": [], "entities": [{"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9995841383934021}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9992043375968933}, {"text": "F-score", "start_pos": 142, "end_pos": 149, "type": "METRIC", "confidence": 0.998320996761322}, {"text": "boundary detection", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.7000120878219604}]}, {"text": "The first baseline strategy of not segmenting anything is given in the first row, and unsurprisingly gets many cases right, but performs badly overall.", "labels": [], "entities": [{"text": "segmenting anything", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.872445821762085}]}, {"text": "A more intelligent baseline is provided by UDPipe (; retrained on the SPMRL data), which, for super-tokens in morphologically rich languages such as Hebrew, implements a 'most common segmentation' baseline An anonymous reviewer suggested that it would also be interesting to add an unsupervised pure segmentation system such as Morfessor () to evaluate on this task.", "labels": [], "entities": [{"text": "SPMRL data", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.8408516347408295}]}, {"text": "This would certainly bean interesting comparison, but due to the brief response period it was not possible to add this experiment before publication.", "labels": [], "entities": []}, {"text": "It can however be expected that results would be substantially worse than yap, given the centrality of the lexicon representation to this task, which can be seen in detail in the ablation tests in Section 7.", "labels": [], "entities": []}, {"text": "(i.e. each super-token is given its most common segmentation from training data, forgoing segmentation for OOV items).", "labels": [], "entities": []}, {"text": "8 Results for yap represent pure segmentation performance from the previous state of the art (More and Tsarfaty, 2016).", "labels": [], "entities": []}, {"text": "The best two approaches in the present paper are represented next: the Extra Trees Random Forest variant, 9 called RFTokenizer, is labeled RF and the DNN-based system is labeled DNN.", "labels": [], "entities": []}, {"text": "Surprisingly, while the DNN is a close runner up, the best performance is achieved by the RFTokenizer, de-spite not having access to word embeddings.", "labels": [], "entities": [{"text": "DNN", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.9194958209991455}, {"text": "RFTokenizer", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.8321288824081421}]}, {"text": "Its high performance on the SPMRL dataset makes it difficult to converge to a better solution using the DNN, though it is conceivable that substantially more data, a better feature representation and/or more hyperparameter tuning could equal or surpass the RFTokenizer's performance.", "labels": [], "entities": [{"text": "SPMRL dataset", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.7443161159753799}]}, {"text": "Coupled with a lower cost in system resources and external dependencies, and the ability to forgo large model files to store word embeddings, we consider the RFTokenizer solution to be better given the current training data size.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: System performance on the SPMRL and  Wiki5K datasets.", "labels": [], "entities": [{"text": "SPMRL", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.8055669069290161}, {"text": "Wiki5K datasets", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.8189069032669067}]}, {"text": " Table 3: Effects of removing features on performance,  ordered by descending F-score impact on SPMRL.", "labels": [], "entities": [{"text": "F-score impact", "start_pos": 78, "end_pos": 92, "type": "METRIC", "confidence": 0.9738753736019135}]}]}