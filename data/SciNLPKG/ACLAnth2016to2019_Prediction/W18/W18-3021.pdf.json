{"title": [{"text": "Limitations of cross-lingual learning from image search", "labels": [], "entities": []}], "abstractContent": [{"text": "Cross-lingual representation learning is an important step in making NLP scale to all the world's languages.", "labels": [], "entities": [{"text": "Cross-lingual representation learning", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7242794036865234}]}, {"text": "Previous work on bilingual lexicon induction suggests that it is possible to learn cross-lingual representations of words based on similarities between images associated with these words.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.6577261586983999}]}, {"text": "However, that work focused (almost exclusively) on the translation of nouns only.", "labels": [], "entities": [{"text": "translation of nouns", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.910033126672109}]}, {"text": "Here, we investigate whether the meaning of other parts-of-speech (POS), in particular adjectives and verbs, can be learned in the same way.", "labels": [], "entities": []}, {"text": "Our experiments across five language pairs indicate that previous work does not scale to the problem of learning cross-lingual representations beyond simple nouns.", "labels": [], "entities": []}], "introductionContent": [{"text": "Typically, cross-lingual word representations are learned from word alignments, sentence alignments, from aligned, comparable documents (, or from monolingual corpora using seed dictionaries (.", "labels": [], "entities": []}, {"text": "However, for many languages such resources are not available.", "labels": [], "entities": []}, {"text": "Bergsma and Van Durme (2011) introduced an alternative idea, namely to learn bilingual representations from image data collected via web image search.", "labels": [], "entities": []}, {"text": "The idea behind their approach is to represent words in a visual space and find valid translations between words based on similarities between their visual representations.", "labels": [], "entities": []}, {"text": "Representations of words in the visual space are built by rep-resenting a word by a set of images that are associated with that word, i.e., the word is a semantic tag for the images in the set.", "labels": [], "entities": []}, {"text": "improve performance for the same task using a feature representation extracted from convolutional networks.", "labels": [], "entities": []}, {"text": "However, both works only consider nouns, leaving open the question of whether learning cross-lingual representations for other POS from images is possible.", "labels": [], "entities": []}, {"text": "In order to evaluate whether this work scales to verbs and adjectives, we compile wordlists containing these POS in several languages.", "labels": [], "entities": []}, {"text": "We collect image sets for each image word and represent all words in a visual space.", "labels": [], "entities": []}, {"text": "Then, we rank translations computing similarities between image sets and evaluate performance on this task.", "labels": [], "entities": []}, {"text": "Another field of research that exploits image data for NLP applications is the induction of multi-modal embeddings, i.e. semantic representations that are learned from textual and visual information jointly.", "labels": [], "entities": []}, {"text": "The work presented in our paper differs from these approaches, in that we do not use image data to improve semantic representations, but use images as a resource to learn crosslingual representations.", "labels": [], "entities": []}, {"text": "Even though lexicon induction from text resources might be more promising in terms of performance, we think that lexicon induction from visual data is worth exploring as it might give insights in the way that language is grounded in visual context.", "labels": [], "entities": []}], "datasetContent": [{"text": "Ranking performance is evaluated by computing the Mean Reciprocal Rank (MRR) as M is the number of words to be translated and rank(w s , wt ) is the position the correct translation wt for source word w sis ranked on.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 50, "end_pos": 76, "type": "METRIC", "confidence": 0.9645499189694723}]}, {"text": "In addition to MRR, we also evaluate the crosslingual representations by means of precision at k (P@k).", "labels": [], "entities": [{"text": "MRR", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.5056492686271667}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9990013241767883}]}, {"text": "We run experiments for 5 language pairs EnglishGerman, English-Spanish, English-French, English-Russian and English-Italian.", "labels": [], "entities": []}, {"text": "We evaluate the representations computed from image data and compare the different methods for similarity computation described in 3.", "labels": [], "entities": []}, {"text": "For each English word, we rank all the words in the corresponding target languages based on similarities between image sets and evaluate the models' ability to identify correct translations, i.e. to rank the correct translation on a position near the top.", "labels": [], "entities": []}, {"text": "We compare 4 settings that differ in the set of English words that are translated.", "labels": [], "entities": []}, {"text": "In the setting ALL, all English words in the wordlist are translated.", "labels": [], "entities": []}, {"text": "NN, VB and ADJ refer to the settings where only nouns, verbs and adjectives are translated.", "labels": [], "entities": []}, {"text": "displays results averaged overall language pairs.", "labels": [], "entities": []}, {"text": "7 First, comparing the different methods to compute similarities between image sets, AVGMAX outperforms the other methods in almost all cases.", "labels": [], "entities": [{"text": "AVGMAX", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.7860273718833923}]}, {"text": "Most importantly, we witness a very significant drop in performance when moving from nouns to verbs and adjectives.", "labels": [], "entities": []}, {"text": "For verbs, we rarely pick the right translation based on the image-based word representations.", "labels": [], "entities": []}, {"text": "This behavior applies across all methods for similarity computation.", "labels": [], "entities": [{"text": "similarity computation", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.8024270832538605}]}, {"text": "Further, we see small improvements if we cluster the image sets prior to applying the KNN method, which might indicate that the clustering helps in finding translations for polysemous words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of POS tags in the datasets  used to compile the final wordlist.", "labels": [], "entities": []}, {"text": " Table 2: Results for translation ranking with images represented by CNN features averaged over 5 lan- guage pairs. KNN and KNN-C do not produce a ranking, hence we only provide P@1 values.", "labels": [], "entities": [{"text": "translation ranking", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.956366628408432}]}, {"text": " Table 3: English image words associated with  the image sets with highest and lowest dispersion  scores d.", "labels": [], "entities": []}]}