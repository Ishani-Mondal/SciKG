{"title": [{"text": "Adding the Third Dimension to Spatial Relation Detection in 2D Images", "labels": [], "entities": [{"text": "Spatial Relation Detection", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.9045264919598898}]}], "abstractContent": [{"text": "Detection of spatial relations between objects in images is currently a popular subject in image description research.", "labels": [], "entities": [{"text": "Detection of spatial relations between objects in images", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.8686788529157639}, {"text": "image description", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.7255942970514297}]}, {"text": "A range of different language and geometric object features have been used in this context , but methods have not so far used explicit information about the third dimension (depth), except when manually added to annotations.", "labels": [], "entities": []}, {"text": "The lack of such information hampers detection of spatial relations that are inherently 3D.", "labels": [], "entities": []}, {"text": "In this paper, we use a fully automatic method for creating a depth map of an image and derive several different object-level depth features from it which we add to an existing feature set to test the effect on spatial relation detection.", "labels": [], "entities": [{"text": "spatial relation detection", "start_pos": 211, "end_pos": 237, "type": "TASK", "confidence": 0.683556338151296}]}, {"text": "We show that performance increases are obtained from adding depth features in all scenarios tested.", "labels": [], "entities": []}], "introductionContent": [{"text": "Image description aims to produce a summarising description, in structured natural language, of an image (region), typically involving the prioritisation of more important elements and relationships between elements.", "labels": [], "entities": [{"text": "Image description", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8386674523353577}]}, {"text": "Work in this area is most commonly motivated in terms of accessibility and data management, and has a range of distinct application tasks.", "labels": [], "entities": [{"text": "data management", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.746916264295578}]}, {"text": "Research in image description and understanding is booming, with relation detection currently a particular focus.", "labels": [], "entities": [{"text": "image description and understanding", "start_pos": 12, "end_pos": 47, "type": "TASK", "confidence": 0.7956636697053909}, {"text": "relation detection", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.961880624294281}]}, {"text": "The input to spatial relation detection is usually a set of secondary, abstract features derived from region boundaries and labels.", "labels": [], "entities": [{"text": "spatial relation detection", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.6822565197944641}]}, {"text": "A range of different language and geometric features have been used in existing work, but none that explicitly encode information about the third dimension (depth), except via manual annotations.", "labels": [], "entities": []}, {"text": "This is an issue for spatial relation detection, because many spatial relations involve three dimensions, some obviously so (e.g. in front of, behind), some less so (beyond, outside, across, etc.).", "labels": [], "entities": [{"text": "spatial relation detection", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.6202048063278198}]}, {"text": "Existing methods in effect try to guess 3D relations from 2D information.", "labels": [], "entities": []}, {"text": "In the experiments in this paper, we use a fully automatic method to generate a depth map from an image, derive different object-level abstract features from the depth values associated with pixels within object bounding boxes, and test the effect of adding such features on the performance of spatial relation detection methods.", "labels": [], "entities": [{"text": "spatial relation detection", "start_pos": 294, "end_pos": 320, "type": "TASK", "confidence": 0.7020354171593984}]}, {"text": "Below, we start by reviewing related research (Section 2) and describing the existing dataset and associated features we use in our experiments (Section 3).", "labels": [], "entities": []}, {"text": "We next describe the depth map generation method we used, and the features we derive from depth maps (Section 4).", "labels": [], "entities": [{"text": "depth map generation", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.6522293289502462}]}, {"text": "We then describe the classifier methods we use in experiments (Section 5), and report results from experiments involving different classifier methods and combinations of depth features (Section 6).", "labels": [], "entities": []}, {"text": "We conclude with some discussion and a look to the future (Section 7).", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out experiments for all ML methods above, and for the following feature combinations: (i) the 18 geometrical features ('G' in results tables) from Section 3, (ii) the language features derived from the object labels ('L' in the tables), (iii) average depth ('avg' in tables), (iv) RWA ('rwa' in tables) and (V) human-estimated depth ('man' in tables).", "labels": [], "entities": [{"text": "ML", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.954937219619751}, {"text": "RWA", "start_pos": 292, "end_pos": 295, "type": "METRIC", "confidence": 0.988014280796051}]}, {"text": "For each of (iii), (iv) and (v) we considered depth of object 1 ('d1' in tables), depth of object 2 ('d2' in tables), and the difference between the latter two depths ('dd' in tables).", "labels": [], "entities": []}, {"text": "shows system-level weighted aver-  The highest increase is 8.9% when added to geometric features, and 8.3% when added to both language and geometric features.", "labels": [], "entities": []}, {"text": "AVG and RWA features perform equally well, and less well than the human-estimated depths.", "labels": [], "entities": [{"text": "AVG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5393997430801392}, {"text": "RWA", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.5697848796844482}]}, {"text": "Out of the three depth features, the difference in depth (dd = d1 \u2212 d2) has the most pronounced positive effect on scores individually; however, the overall highest scores are obtained when all three (d1, d2 and dd).", "labels": [], "entities": [{"text": "depth", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9854656457901001}]}, {"text": "Out of the different classifier modesl, the RF model resulted in the highest scores followed by LR, SVM, DT and NB.", "labels": [], "entities": [{"text": "LR", "start_pos": 96, "end_pos": 98, "type": "METRIC", "confidence": 0.9602330923080444}]}, {"text": "However, the NB model registered the highest increase in scores resulting from depth features: 12.5% when added to geometric features.", "labels": [], "entities": [{"text": "NB", "start_pos": 13, "end_pos": 15, "type": "DATASET", "confidence": 0.818918764591217}]}, {"text": "This could indicate that the other models are learning more about depth from the other features.", "labels": [], "entities": []}, {"text": "shows per-preposition weighted average recall results.", "labels": [], "entities": [{"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.7757890224456787}]}, {"text": "In this set of results we examine the effect of adding depth information on individual prepositions, looking at which combinations of features increase or decrease the recall per preposition.", "labels": [], "entities": [{"text": "recall", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.998489260673523}]}, {"text": "The table is split into two halves.", "labels": [], "entities": []}, {"text": "The top half shows changes from adding depth features to (just) the geometric features (G), while the bottom half shows changes from adding depth features to the union of geometric and language features (G,L).", "labels": [], "entities": []}, {"text": "Some prepositions fare better with depth information: au niveau de (\"at the level of\"), derriere (\"behind\"), devant (\"in front of\"), sur (\"on\").", "labels": [], "entities": []}, {"text": "Results for others worsen: ` a c\u00f4t\u00e9 de (\"next to\"), en face de (\"facing\"), sous (\"under\").", "labels": [], "entities": []}, {"text": "For some, the results are inconclusive (contre (\"against\"), dans (\"in\"), loin de (\"far from\"), pr\u00e8s de (\"near\")), while others are not affected (au dessus de (\"above\"), autour de (\"around\")).", "labels": [], "entities": []}, {"text": "The row labelled 'G,L' shows the effect of just adding language features to the geometric set.", "labels": [], "entities": []}, {"text": "Some prepositions (most notably autour de, contre and dans) benefit substantially from language features while others benefit more from depth features.", "labels": [], "entities": []}, {"text": "Some (au niveau de, oin de) fare worse when language features are added.", "labels": [], "entities": []}, {"text": "The biggest improvement when depth information is added to geometric features is 33% for derriere (\"behind\"); the highest when depth is added to both geometrical and language is 55%, for au niveau de (\"at the level of, at equal distance from the viewer\").", "labels": [], "entities": []}, {"text": "Getting improvements for clearly 3D prepositions such as derriere, devant and au niveau de is as expected, but there are clear improvements for other prepositions too.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SpatialVOC2K: Weighted Average Re- call for all feature combinations (for explanation  of abbreviations, see in text).", "labels": [], "entities": [{"text": "Weighted Average Re- call", "start_pos": 24, "end_pos": 49, "type": "METRIC", "confidence": 0.924711549282074}]}, {"text": " Table 2: SpatialVOC2K: Percentage increase in recall per preposition for the RF model. Figures in top  half relative to geometric features; lower half relative to both geometric and language features.", "labels": [], "entities": [{"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.975241482257843}]}]}