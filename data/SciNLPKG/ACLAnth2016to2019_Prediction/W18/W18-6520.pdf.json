{"title": [{"text": "Self-Learning Architecture for Natural Language Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose a self-learning architecture for generating natural language templates for conversational assistants.", "labels": [], "entities": []}, {"text": "Generating templates to coverall the combinations of slots in an intent is time consuming and labor-intensive.", "labels": [], "entities": []}, {"text": "We examine three different models based on our proposed architecture-Rule-based model, Sequence-to-Sequence (Seq2Seq) model and Semantically Conditioned LSTM (SC-LSTM) model for the IoT domain-to reduce the human labor required for template generation.", "labels": [], "entities": [{"text": "template generation", "start_pos": 232, "end_pos": 251, "type": "TASK", "confidence": 0.7477642595767975}]}, {"text": "We demonstrate the feasibility of template generation for the IoT domain using our self-learning architecture.", "labels": [], "entities": [{"text": "template generation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7310526669025421}]}, {"text": "In both automatic and human evaluation , the self-learning architecture outper-forms previous works trained with a fully human-labeled dataset.", "labels": [], "entities": []}, {"text": "This is promising for commercial conversational assistant solutions.", "labels": [], "entities": [{"text": "conversational assistant", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.8832723200321198}]}], "introductionContent": [{"text": "Intelligent Conversational Assistants are prevalent now.", "labels": [], "entities": [{"text": "Intelligent Conversational Assistants", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6793563465277354}]}, {"text": "They have been integrated into a wide range of IoT devices.", "labels": [], "entities": []}, {"text": "Various recent studies on stochastic language generation have been conducted in NLG.", "labels": [], "entities": [{"text": "stochastic language generation", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.6295894682407379}]}, {"text": "Although these stochastic approaches outperform traditional LSTM language models, they have drawbacks including the requirement of large training datasets, low accuracy of trained models and lack of naturalness of generated sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9977518916130066}]}, {"text": "Therefore, Most of commercial conversational assistant services adopt template-based approach) to implement natural language generation.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.7406463225682577}]}, {"text": "This approach is robust and feasible for commercialization, but requires creating a large number of templates.", "labels": [], "entities": []}, {"text": "In this approach, one needs to manually generate NLG templates that coverall the possible combinations of intents and slots.", "labels": [], "entities": []}, {"text": "There are statistical approaches for generating templates with 2, 3 and 4 slots), but they suffer from exponential complexity as the number of slots increases.", "labels": [], "entities": []}, {"text": "In addition, substitution-based implementation with SimpleNLG ( can handle some of the cases, but this is not versatile enough.", "labels": [], "entities": [{"text": "SimpleNLG", "start_pos": 52, "end_pos": 61, "type": "DATASET", "confidence": 0.5954022407531738}]}, {"text": "The number of templates required to coverall possible combinations of slots is: where n is the total number of possible slots.", "labels": [], "entities": []}, {"text": "The value is exponential as shown in equation 1.", "labels": [], "entities": []}, {"text": "Manually generating an exponential number of templates is undesirable, especially when the intents get more complex.", "labels": [], "entities": []}, {"text": "We propose a self-learning architecture for NLG which solves the problem of generating an exponential number of templates.", "labels": [], "entities": []}, {"text": "In order to generate informative and natural sentences, arguably, it is more important to generate consistent system responses with limited syntactic information than to generate error prone system responses with more variation in their grammatical form.", "labels": [], "entities": []}, {"text": "In our proposed solution, we start with an initial training set containing less than or equal to 2 slots per intent, and iteratively build our model to increase its ability to cover more complex inputs.", "labels": [], "entities": []}, {"text": "Thus, the re-1 Throughout this paper, intent denotes the intention of user utterance and slot denotes the variable part (slot value) and its name if any (slot name).", "labels": [], "entities": [{"text": "slot", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9575238823890686}]}, {"text": "Slot can be replaced by another phrase in user utterances or responses.", "labels": [], "entities": [{"text": "Slot", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.748145580291748}]}, {"text": "NLG will be generated from dialog act and it will be written as Intent( SlotN ame1 = SlotV alue1; SlotN ame2 = SlotV alue2 ; \u00b7 \u00b7 \u00b7).", "labels": [], "entities": [{"text": "Intent", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9926995635032654}]}, {"text": "For example, from dialog act up(functionname=tem perature;devicename='airconditioner';location='bedroom'), response \"I turned up the temperature of the airconditioner in the bedroom\" is generated.", "labels": [], "entities": []}, {"text": "quired number of human generated templates decreases from 2 n \u2212 1 ton", "labels": [], "entities": [{"text": "quired number", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9233810603618622}]}], "datasetContent": [{"text": "We use a small domain-specific dataset for our experiment.", "labels": [], "entities": []}, {"text": "Our dataset is derived from the one used by, focused on the domain of IoT Home Appliances.", "labels": [], "entities": []}, {"text": "We extract a subset of the system responses, and extend them for increased coverage, and generate DialogAct, Response pairs.", "labels": [], "entities": []}, {"text": "The generated DialogAct, Response pairs are given to our   model as the training set.", "labels": [], "entities": []}, {"text": "In the first training iteration, each DialogAct, Response pair in the dataset consists of a maximum of two slots.", "labels": [], "entities": []}, {"text": "For each training iteration, we split the available training dataset into 4 : 1 partitions for training and validation respectively.", "labels": [], "entities": []}, {"text": "For testing, we use human generated DialogAct, Response pairs which contain one more slot compared to the dataset used in the training step.", "labels": [], "entities": []}, {"text": "For comparison, we also trained our neural models using human generated test pairs with less thank slots combined with original train dataset.", "labels": [], "entities": []}, {"text": "They are also evaluated using the test dataset which contains k + 1 slots per pair.", "labels": [], "entities": []}, {"text": "To evaluate our models, we measure BLEU scores and use a human evaluation.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9766782522201538}]}, {"text": "The BLEU score is the n-gram similarity between the reference response and the generated response.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9824427366256714}]}, {"text": "Higher BLEU scores indicate a better model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.999724805355072}]}, {"text": "The whole test set was used to measure BLEU scores.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9688914120197296}]}, {"text": "For human rating, we asked 21 human evaluators to rank the outputs of all five models, considering the grammatical correctness and informativeness of the generated responses.", "labels": [], "entities": []}, {"text": "Then we calculated the Mean Rank for each model.", "labels": [], "entities": [{"text": "Mean Rank", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9920185804367065}]}, {"text": "Lower Mean Rank indicates a better model.", "labels": [], "entities": [{"text": "Mean Rank", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9877544045448303}]}, {"text": "Randomly chosen 10 responses with 3 to 5 slots were used for human rating.", "labels": [], "entities": [{"text": "human rating", "start_pos": 61, "end_pos": 73, "type": "TASK", "confidence": 0.8027167320251465}]}], "tableCaptions": [{"text": " Table 1: Sample data from IoT dataset. The first line of each example is Dialog Act and the next line is the corresponding", "labels": [], "entities": [{"text": "IoT dataset", "start_pos": 27, "end_pos": 38, "type": "DATASET", "confidence": 0.8989672958850861}, {"text": "Dialog Act", "start_pos": 74, "end_pos": 84, "type": "DATASET", "confidence": 0.8419669270515442}]}, {"text": " Table 2: BLEU score Results. Test results for k = 2 to k = 5", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9700004458427429}]}]}