{"title": [{"text": "Extracting In-domain Training Corpora for Neural Machine Translation Using Data Selection Methods", "labels": [], "entities": [{"text": "Extracting In-domain Training Corpora", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8522368669509888}, {"text": "Neural Machine Translation", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.6442234913508097}]}], "abstractContent": [{"text": "Data selection is a process used in selecting a subset of parallel data for the training of machine translation (MT) systems, so that 1) resources for training might be reduced, 2) trained models could perform better than those trained with the whole corpus, and/or 3) trained models are more tailored to specific domains.", "labels": [], "entities": [{"text": "Data selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6806920170783997}, {"text": "machine translation (MT)", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.860016942024231}]}, {"text": "It has been shown that for statistical MT (SMT), the use of data selection helps improve the MT performance significantly.", "labels": [], "entities": [{"text": "MT (SMT)", "start_pos": 39, "end_pos": 47, "type": "TASK", "confidence": 0.8630561083555222}, {"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9935530424118042}]}, {"text": "In this study, we reviewed three data selection approaches for MT, namely Term Frequency-Inverse Document Frequency, Cross-Entropy Difference and Feature Decay Algorithm, and conducted experiments on Neural Machine Translation (NMT) with the selected data using the three approaches.", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9925445318222046}, {"text": "Term Frequency-Inverse Document Frequency", "start_pos": 74, "end_pos": 115, "type": "TASK", "confidence": 0.5971699953079224}, {"text": "Cross-Entropy Difference", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.6998164802789688}, {"text": "Neural Machine Translation (NMT)", "start_pos": 200, "end_pos": 232, "type": "TASK", "confidence": 0.8440271417299906}]}, {"text": "The results showed that for NMT systems, using data selection also improved the performance, though the gain is not as much as for SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 131, "end_pos": 134, "type": "TASK", "confidence": 0.988351047039032}]}], "introductionContent": [{"text": "Data selection is a technology used to improve Machine Translation (MT) performance by choosing a subset of the corpus for the training of MT systems.", "labels": [], "entities": [{"text": "Data selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6871008425951004}, {"text": "Machine Translation (MT)", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.8593961715698242}, {"text": "MT", "start_pos": 139, "end_pos": 141, "type": "TASK", "confidence": 0.9663884043693542}]}, {"text": "There are additional benefits using subsets instead of the whole corpus for MT training.", "labels": [], "entities": [{"text": "MT training", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.9316207468509674}]}, {"text": "Firstly, the training time could be reduced significantly.", "labels": [], "entities": []}, {"text": "In some application scenarios, a much shorter training time would be very useful.", "labels": [], "entities": []}, {"text": "Secondly, we could select data with the aim to make trained systems perform well for specific domains.", "labels": [], "entities": []}, {"text": "In MT, models built with in-domain data perform better, as the vocabulary and sentence structures used in one domain (e.g. legal) differs from another unrelated domain (e.g. biotechnology).", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9900506138801575}]}, {"text": "There are several studies on data selection methods for SMT, showing good improvements over the baselines in which the whole corpora were used for training.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.997147262096405}]}, {"text": "A popular data selection method is cross-entropy difference (CED)).", "labels": [], "entities": [{"text": "cross-entropy difference (CED))", "start_pos": 35, "end_pos": 66, "type": "METRIC", "confidence": 0.7353237450122834}]}, {"text": "In particular its bilingual variant) showed a positive impact of data selection for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9914154410362244}]}, {"text": "Term Frequency-Inverse Document Frequency (TF-IDF)) has also been used as a baseline data selection method in the literature.", "labels": [], "entities": [{"text": "Term Frequency-Inverse Document Frequency", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.6780274584889412}]}, {"text": "Data selection with cleaning was proposed to improve the robustness of training with divergent sentences.", "labels": [], "entities": [{"text": "Data selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.5949166715145111}]}, {"text": "Feature Decay Algorithms (FDA) are data selection methods that try to extract the subset of sentences by which the coverage of target language features is maximized.", "labels": [], "entities": [{"text": "Feature Decay Algorithms (FDA)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7848437130451202}]}, {"text": "It has been used to select sentences from parallel data for SMT and NMT () in order to obtain a subset of data that is more tailored to a given test set.", "labels": [], "entities": [{"text": "SMT", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9767257571220398}]}, {"text": "Most of these results focused on comparing training of models from scratch for use in specific domains.", "labels": [], "entities": []}, {"text": "The aforementioned papers do not include a focus on the impact of such techniques in finetuning the resulting trained model, which could be useful in the case where a baseline model works as an initialization and can be reused for any domain and thus reduce the time required to train the models for specific domains (van der.", "labels": [], "entities": []}, {"text": "In this paper we evaluate the impact of data selection methods on Neural Machine Translation (NMT) systems.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.8014762699604034}]}, {"text": "We would like to answer the following questions: Do data selection approaches improve domain NMT performance?", "labels": [], "entities": []}, {"text": "Which of the three commonly used methods delivers the best results on data selection for NMT?", "labels": [], "entities": [{"text": "NMT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.8523254990577698}]}, {"text": "How does the size of the seed and the selected training sentences affect the performance?", "labels": [], "entities": []}, {"text": "The paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we give an overview of data selection approaches.", "labels": [], "entities": [{"text": "data selection", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7298640012741089}]}, {"text": "Experimental setup and results are presented in Section 3 and Section 4.", "labels": [], "entities": []}, {"text": "Conclusions and future work are given in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present MT results using the three data selection methods and then use the best of the three methods to conduct a series of experiments to assess the impact of data selection on NMT models.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9875871539115906}]}, {"text": "We present two evaluation scores, BLEU) and Translation Error Rate (TER) (), in the tables.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9996026158332825}, {"text": "Translation Error Rate (TER)", "start_pos": 44, "end_pos": 72, "type": "METRIC", "confidence": 0.9597094456354777}]}, {"text": "These scores give an estimation of how good the translation is: For BLEU, higher scores indicate better translations, while for TER, as it measures an error rate, lower scores indicate better translation performance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9970452189445496}, {"text": "TER", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.9942317605018616}]}, {"text": "We performed three different experiments: \u2022 A comparison of the three data selection methods introduced in this paper (Section 4.1).", "labels": [], "entities": []}, {"text": "\u2022 A comparison of the data selection methods using different seeds (Section 4.2).", "labels": [], "entities": []}, {"text": "\u2022 The impact of the best data selection method in NMT (Section 4.3)  We also conducted a human evaluation using Unbabel's quality control system.", "labels": [], "entities": [{"text": "NMT", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.8538051247596741}]}, {"text": "For each language pair, translation direction and domain, 150 sentences were chosen randomly for evaluation.", "labels": [], "entities": [{"text": "translation direction", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.9403901696205139}]}, {"text": "We then shuffled the content and provided it to evaluators ( professional linguists) for Fluency and Adequacy assessment.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 89, "end_pos": 96, "type": "TASK", "confidence": 0.5390660762786865}, {"text": "Adequacy assessment", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.667085662484169}]}, {"text": "This assessment is done by rating each sentence from 1 to 5, and then computing the average for each model.", "labels": [], "entities": []}, {"text": "The evaluators were not provided with the information as to which model was used to generate sentences.", "labels": [], "entities": []}, {"text": "The definitions of Fluency and Adequacy, as used by the Unbabel Quality Team, are as follows.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9972997307777405}, {"text": "Adequacy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9813279509544373}, {"text": "Unbabel Quality Team", "start_pos": 56, "end_pos": 76, "type": "DATASET", "confidence": 0.9165486892064413}]}, {"text": "Fluency addresses the linguistic wellformedness and naturalness of the text.", "labels": [], "entities": []}, {"text": "Fluency errors include grammar, spelling or unintelligible text, sentence structure and word order issues, etc.", "labels": [], "entities": []}, {"text": "In sum, these errors affect the reading and the comprehension of the text.", "labels": [], "entities": []}, {"text": "The evaluation is done on the resulting translations without revealing their source sentences to the evaluators, to avoid biasing Fluency scores.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 130, "end_pos": 137, "type": "METRIC", "confidence": 0.9599055647850037}]}, {"text": "Adequacy addresses the relationship of the target text to the source text and can only be assessed by providing both translations and their source sentences to the editors.", "labels": [], "entities": []}, {"text": "In other words, Adequacy addresses the extent to which a target text accurately renders the meaning of a source text.", "labels": [], "entities": []}, {"text": "Adequacy errors include changes in intended meaning, addition and omission of content or any type of mistranslation, etc.", "labels": [], "entities": []}, {"text": "In sum, Adequacy measures if the target text accurately reflect the meaning conveyed in the source text.", "labels": [], "entities": [{"text": "Adequacy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9906298518180847}]}, {"text": "The results of human evaluation on Fluency and Adequacy are presented in.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.5496784448623657}]}, {"text": "The results show that with fine-tuning of the training of models, Fluency is improved, especially for the EMEA models.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9992166757583618}, {"text": "EMEA", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.875083863735199}]}, {"text": "Adequacy is also significantly improved in both EN-to-FR and EN-to-ES models.", "labels": [], "entities": []}, {"text": "It shows very clear that data selection does improve the performance of all MT systems evaluated in this paper, in both Adequacy and Fluency.", "labels": [], "entities": [{"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.9854669570922852}, {"text": "Adequacy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9924559593200684}, {"text": "Fluency", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9782292246818542}]}, {"text": "It was also shown in and that for EN-to-FR, BLEU .452 of MT translated French sentences approximately corresponds to Fluency 4.25, and for EN-to-ES, BLEU .485 of MT translated Spanish sentences approximately corresponds to Fluency 4.50.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9988064765930176}, {"text": "MT translated French sentences", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.5186989158391953}, {"text": "Fluency", "start_pos": 117, "end_pos": 124, "type": "METRIC", "confidence": 0.8279457688331604}, {"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.998502254486084}, {"text": "Fluency", "start_pos": 223, "end_pos": 230, "type": "METRIC", "confidence": 0.7555587291717529}]}, {"text": "In the future, we would like to make more comparisons between human evaluation metrics, e.g. Adequacy and Fluency as defined by Unbabel Quality Team, with commonly used MT performance metrics, e.g. BLEU and TER.", "labels": [], "entities": [{"text": "Adequacy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9632743000984192}, {"text": "Fluency", "start_pos": 106, "end_pos": 113, "type": "METRIC", "confidence": 0.4994478225708008}, {"text": "Unbabel Quality Team", "start_pos": 128, "end_pos": 148, "type": "DATASET", "confidence": 0.8715824286142985}, {"text": "MT", "start_pos": 169, "end_pos": 171, "type": "TASK", "confidence": 0.9382569193840027}, {"text": "BLEU", "start_pos": 198, "end_pos": 202, "type": "METRIC", "confidence": 0.9921504259109497}, {"text": "TER", "start_pos": 207, "end_pos": 210, "type": "METRIC", "confidence": 0.9625571966171265}]}], "tableCaptions": [{"text": " Table 1: Results of running three different data selection methods on different selection sizes for EMEA EN\u2192FR. Both BLEU", "labels": [], "entities": [{"text": "EMEA EN\u2192FR", "start_pos": 101, "end_pos": 111, "type": "DATASET", "confidence": 0.741804838180542}, {"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9973413348197937}]}, {"text": " Table 2: Results of running different data selection methods on different seed sizes for EMEA EN\u2192FR. The top result for", "labels": [], "entities": [{"text": "EMEA EN\u2192FR", "start_pos": 90, "end_pos": 100, "type": "DATASET", "confidence": 0.6773568391799927}]}, {"text": " Table 3: BLEU and TER scores for NMT training with different slices of selected data, using FDA for data selection. The top", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993371367454529}, {"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.99866783618927}, {"text": "NMT training", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.9019524455070496}]}, {"text": " Table 4: Fine-tuning approach for NMT training with data selection. The top two results for each column are shaded, with the", "labels": [], "entities": [{"text": "NMT training", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.9431813061237335}]}, {"text": " Table 5: Human evaluation of Adequacy (AD) and Fluency (FL) for top scores in previous experiments in", "labels": [], "entities": [{"text": "Adequacy (AD) and Fluency (FL)", "start_pos": 30, "end_pos": 60, "type": "METRIC", "confidence": 0.8409933348496755}]}]}