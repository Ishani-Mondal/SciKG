{"title": [{"text": "Belittling the Source: Trustworthiness Indicators to Obfuscate Fake News on the Web", "labels": [], "entities": []}], "abstractContent": [{"text": "With the growth of the internet, the number of fake-news online has been proliferating every year.", "labels": [], "entities": []}, {"text": "The consequences of such phenomena are manifold, ranging from lousy decision-making process to bullying and violence episodes.", "labels": [], "entities": []}, {"text": "Therefore, fact-checking algorithms became a valuable asset.", "labels": [], "entities": []}, {"text": "To this aim, an important step to detect fake-news is to have access to a credibility score fora given information source.", "labels": [], "entities": []}, {"text": "However, most of the widely used Web indicators have either been shutdown to the public (e.g., Google PageRank) or are not free for use (Alexa Rank).", "labels": [], "entities": []}, {"text": "Further existing databases are short-manually curated lists of online sources, which do not scale.", "labels": [], "entities": []}, {"text": "Finally, most of the research on the topic is theoretical-based or explore confidential data in a restricted simulation environment.", "labels": [], "entities": []}, {"text": "In this paper we explore current research, highlight the challenges and propose solutions to tackle the problem of classifying websites into a credibility scale.", "labels": [], "entities": []}, {"text": "The proposed model automatically extracts source reputation cues and computes a credibility factor, providing valuable insights which can help in belittling dubious and confirming trustful unknown web-sites.", "labels": [], "entities": []}, {"text": "Experimental results outperform state of the art in the 2-classes and 5-classes setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the enormous daily growth of the Web, the number of fake-news sources have also been increasing considerably (.", "labels": [], "entities": []}, {"text": "This social network era has provoked a communication revolution that boosted the spread of misinformation, hoaxes, lies and questionable claims.", "labels": [], "entities": []}, {"text": "The proliferation of unregulated sources of information allows any person to become an opinion provider with + Work was completed while the author was a student at the Birla Institute of Technology and Science, India and was interning at SDA Research.", "labels": [], "entities": []}, {"text": "* These two authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "For instance, websites spreading manipulative political content or hoaxes can be persuasive.", "labels": [], "entities": []}, {"text": "To tackle this problem, different fact-checking tools and frameworks have been proposed (, mainly divided into two categories: fact-checking over natural language claims and fact-checking over knowledge bases, i.e., triplebased approaches (.", "labels": [], "entities": []}, {"text": "Overall, fact-checking algorithms aim at determining the veracity of claims, which is considered a very challenging task due to the nature of underlying steps, from natural language understanding (e.g. argumentation mining) to common-sense verification (i.e., humans have prior knowledge that makes far easier to judge which arguments are plausible and which are not).", "labels": [], "entities": [{"text": "natural language understanding (e.g. argumentation mining)", "start_pos": 165, "end_pos": 223, "type": "TASK", "confidence": 0.684119526296854}]}, {"text": "Yet an important underlying fact-checking step relies upon computing the credibility of sources of information, i.e. indicators that allow answering the question: \"How reliable is a given provider of information?\".", "labels": [], "entities": []}, {"text": "Due to the obvious importance of the Web and the negative impact that misinformation can cause, methods to demote the importance of websites also become a valuable asset.", "labels": [], "entities": []}, {"text": "In this sense the high number of new websites appearing at everyday, make straightforward approaches -such as blacklists and whitelists -impractical.", "labels": [], "entities": []}, {"text": "Moreover, such approaches are not designed to compute credibility scores fora given website but rather to binary label them.", "labels": [], "entities": []}, {"text": "Thus, they aim at detecting mostly \"fake\" (threatening) websites; e.g., phishing detection, which is out of scope of this work.", "labels": [], "entities": [{"text": "phishing detection", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7961342632770538}]}, {"text": "Thus, open credibility models have a great importance, especially due to the increase of fake news being propagated.", "labels": [], "entities": []}, {"text": "There is much research into credibility factors.", "labels": [], "entities": []}, {"text": "However, they are mostly grouped as follows: (1) theoretical research on psychological aspects of credibility and (2) experiments performed over private and confidential users information, mostly from web browser activities (strongly supported by private companies).", "labels": [], "entities": []}, {"text": "Therefore, while (1) lacks practical results (2) report findings which are not much appealing to the broad open-source community, given the non-open characteristic of the conducted experiments and data privacy.", "labels": [], "entities": []}, {"text": "Finally, recent research on credibility has also pointed out important drawbacks, as follows: 1.", "labels": [], "entities": []}, {"text": "Manual (human) annotation of credibility indicators fora set of websites is costly (Haas and Unkel, 2017).", "labels": [], "entities": []}, {"text": "2. Search engine results page (SERP) do not provide more than few information cues (URL, title and snippet) and the dominant heuristic happens to be the search engine (SE) rank itself (Haas and Unkel, 2017).", "labels": [], "entities": []}, {"text": "3. Only around 42.67% of the websites are covered by the credibility evaluation knowledge base, where most domains have a low credibility confidence ( Therefore, automated credibility models play an important role in the community -although not broadly explored yet, in practice.", "labels": [], "entities": []}, {"text": "In this paper, we focus on designing computational models to predict the credibility of a given website rather than performing sociological experiments or experiments with end users (simulations).", "labels": [], "entities": []}, {"text": "In this scenario, we expect that a website from a domain such as bbc.com gets a higher trustworthiness score compared to one from wordpress.com, for instance.", "labels": [], "entities": []}], "datasetContent": [{"text": "Microsoft Dataset ( consists of thousands of URLs and their credibility ratings (five-point Likert Scale 8 ), ranging from 1 (\"very non-credible\") to 5 (\"very credible\").", "labels": [], "entities": [{"text": "Microsoft Dataset", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9804560542106628}]}, {"text": "In this study, participants were asked to rate the websites as credible following the definition: \"A credible webpage is one whose information one can accept as the truth without needing to look elsewhere\".", "labels": [], "entities": []}, {"text": "Studies by () use this dataset for evaluation.", "labels": [], "entities": []}, {"text": "Content Credibility Corpus (C3) 9 is the most recent and the largest credibility dataset currently publicly available for research (.", "labels": [], "entities": [{"text": "Content Credibility Corpus (C3) 9", "start_pos": 0, "end_pos": 33, "type": "DATASET", "confidence": 0.7874037538255964}]}, {"text": "It contains 15.750 evaluations of 5.543 URLs from 2.041 participants with some additional information about website characteristics and basic demographic features of users.", "labels": [], "entities": []}, {"text": "Among many metadata information existing in the dataset, in this work we are only interested in the URLs and their re-spective five-point Likert scale, so that we obtain the same information available in the Microsoft dataset.", "labels": [], "entities": [{"text": "Microsoft dataset", "start_pos": 208, "end_pos": 225, "type": "DATASET", "confidence": 0.9456764161586761}]}, {"text": "Previous research proposes two application settings w.r.t. the classification itself, as follows: (A.1) casting the credibility problem as a classification problem and (A.2) evaluating the credibility on a five-point Likert scale (regression).", "labels": [], "entities": []}, {"text": "In the classification scenario, the models are evaluated both w.r.t. the 2-classes as well as 3-classes.", "labels": [], "entities": []}, {"text": "In the 2-classes scenario, websites ranging from 1 to 3 are labeled as \"low\" whereas 4 and 5 are labeled as \"high\" (credibility).", "labels": [], "entities": []}, {"text": "Analogously, in the 3-classes scenario, websites labeled as 1 and 2 are converted to \"low\", 3 remains as \"medium\" while 4 and 5 are grouped into the \"high\" class.", "labels": [], "entities": []}, {"text": "We first explore the impact of the bag-of-tags strategy.", "labels": [], "entities": []}, {"text": "We encode and convert the tags into a sequence of tags, similar to a sequence of sentences (looking for opening and closing tags, e.g., <a>and </a>).", "labels": [], "entities": []}, {"text": "Therefore, we perform document classification over the resulting vectors.", "labels": [], "entities": [{"text": "document classification", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.7518506348133087}]}, {"text": "show results of this strategy for both 2 and 3-classes scenarios.", "labels": [], "entities": []}, {"text": "The x-axis is the log scale of the paddings (i.e., the offset of HTML tags we retrieved from w, ranging from 25 to 10.000).", "labels": [], "entities": []}, {"text": "The charts reveal an interesting pattern in both goldstandard datasets (Microsoft Dataset and C3 Corpus): the first tags are the most relevant to predict the credibility class.", "labels": [], "entities": [{"text": "Microsoft Dataset and C3 Corpus", "start_pos": 72, "end_pos": 103, "type": "DATASET", "confidence": 0.7893633604049682}]}, {"text": "Although this strategy does not achieve state of the art performance , it presents reasonable performance by just inspecting website metadata: F1-measures = 0.690 and 0.571 for the 2-classes and 3-classes settings, respectively.", "labels": [], "entities": [{"text": "F1-measures", "start_pos": 143, "end_pos": 154, "type": "METRIC", "confidence": 0.9972550272941589}]}, {"text": "However, it is worth mentioning that the main advantage of this approach lies in the fact that it is language agnostic (while current research shows detailed results for both datasets (2-classes, 3-classes and 5-classes configurations, respectively).", "labels": [], "entities": []}, {"text": "For 5-class regression, we found that the best pad = 100 for the Microsoft dataset and best pad = 175 for the C3 Corpus.", "labels": [], "entities": [{"text": "Microsoft dataset", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.9588994383811951}, {"text": "C3 Corpus", "start_pos": 110, "end_pos": 119, "type": "DATASET", "confidence": 0.9101405739784241}]}, {"text": "We preceded the computing of both classification and regression models with feature selection according to a percentile of the highest scoring features (SelectKBest).", "labels": [], "entities": []}, {"text": "We tested the choice of 3, 5, 10, 25, 50 75 and K=100 percentiles (thus, no selection) of features and did not find a unique K value for every case.", "labels": [], "entities": []}, {"text": "It is worth noticing that in general it is easy to detect high credible sources (F1 for \"high\" class around 0.80 in all experiments and both datasets) but recall of \"low\" credible sources is still an issue.", "labels": [], "entities": [{"text": "F1", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9979318380355835}, {"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.9926746487617493}]}, {"text": "depicts the impact of the credibility model in the fact-checking context.", "labels": [], "entities": []}, {"text": "We collected a small subset of 186 URLs from the FactBench dataset and manually annotated 15 the credibility for each URL (following the Likert scale).", "labels": [], "entities": [{"text": "FactBench dataset", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.9830707609653473}]}, {"text": "The model corrected labeled around 80% of the URLs associated with a positive claim and, more importantly, 70% of non-credible websites linked to false claims were correctly identified.", "labels": [], "entities": []}, {"text": "This helps to minimize the number of non-credible information providers that contain information that supports a false claim.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Text+HTML2Seq features (3-class): best  classifier performance", "labels": [], "entities": []}, {"text": " Table 3: Text+HTML2Seq: regression measures  (5-class). Selecting top K lexical features.", "labels": [], "entities": []}, {"text": " Table 4: FactBench: Web sites collected from  claims.", "labels": [], "entities": [{"text": "FactBench", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.703691840171814}]}, {"text": " Table 5: FactBench Dataset: analyzing the per- formance of the credibility model in the fact- checking task.", "labels": [], "entities": [{"text": "FactBench Dataset", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.8422296941280365}, {"text": "fact- checking task", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7894790917634964}]}]}