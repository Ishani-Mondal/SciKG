{"title": [{"text": "Creating a Dataset for Multilingual Fine-grained Emotion-detection Using Gamification-based Annotation", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces a gamified framework for fine-grained sentiment analysis and emotion detection.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.8112129867076874}, {"text": "emotion detection", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7829627394676208}]}, {"text": "We present a flexible tool, Sen-timentator, that can be used for efficient annotation based on crowd sourcing and a self-perpetuating gold standard.", "labels": [], "entities": [{"text": "crowd sourcing", "start_pos": 95, "end_pos": 109, "type": "TASK", "confidence": 0.6971570253372192}]}, {"text": "We also present a novel dataset with multi-dimensional annotations of emotions and sentiments in movie subtitles that enables research on sentiment preservation across languages and the creation of robust multilingual emotion detection tools.", "labels": [], "entities": [{"text": "sentiment preservation", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.812136709690094}, {"text": "multilingual emotion detection", "start_pos": 205, "end_pos": 235, "type": "TASK", "confidence": 0.7574213941891988}]}, {"text": "The tools and datasets are public and open-source and can easily be extended and applied for various purposes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis and emotion detection is a crucial component in many practical applications but also defines a great challenge in natural language processing and artificial intelligence.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9691349267959595}, {"text": "emotion detection", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7689421772956848}]}, {"text": "Detecting emotions is crucial in human-computer interaction, and human behavior in communication is to a large degree affected by the emotional states that are created in a message.", "labels": [], "entities": []}, {"text": "These states are typically fine-grained and fuzzy covering various dimensions of human feelings and attitudes.", "labels": [], "entities": []}, {"text": "Nevertheless, it is often the practice to consider sentiments and emotions as very coarse and discrete features that can be detected with simple classifiers on a small scale of a few classes.", "labels": [], "entities": []}, {"text": "In our work, we focus on a high-dimensional model of emotions that allows a more natural and fine-grained classification and, furthermore, we tackle emotion detection in a multilingual setting.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 149, "end_pos": 166, "type": "TASK", "confidence": 0.7214689254760742}]}, {"text": "One of the biggest issues that stand in the way of creating reliable emotion detection algorithms is the lack of properly annotated datasets for training and testing purposes, especially in the case of the dimensionality that we consider and the multilingual support that we envision.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7630181610584259}]}, {"text": "This is the reason why we created Sentimentator, anew annotation tool that facilitates the efficient creation of appropriate datasets).", "labels": [], "entities": []}, {"text": "The main contribution of the paper is the framework based on a gamified environment that we develop to efficiently build large-scale resources.", "labels": [], "entities": []}, {"text": "Our setup results in a self-perpetuating gold standard, which is initialized by seed sentences that are annotated by experts and augmented by crowd annotators.", "labels": [], "entities": []}, {"text": "A combination of correlation-based scoring and ranking makes it possible to build datasets with weighted judgments based on the annotator confidence that we measure.", "labels": [], "entities": []}, {"text": "Initial rankings are based on the comparison to seed annotation only but they will be adjusted dynamically once the correlation between crowd annotators allows to estimate further reliability scores.", "labels": [], "entities": []}, {"text": "The main idea is that we can trust annotators that provide identical or at least similar judgments as other reliable annotators.", "labels": [], "entities": []}, {"text": "With this scheme, we can move away from the use of limited seed sentences for confidence estimation to a more dynamic and selfperpetuating gold standard.", "labels": [], "entities": [{"text": "confidence estimation", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.728734165430069}]}, {"text": "Another fundamental decision in our setup is the use of multilingual material on which to base our annotations.", "labels": [], "entities": []}, {"text": "We are interested in the crosslingual use of emotions and the development of multilingual classifiers).", "labels": [], "entities": []}, {"text": "Therefore, we start with sentences extracted from movie subtitles (English originals in our case) for which we also have plenty of translations into a large number of languages.", "labels": [], "entities": []}, {"text": "Movies contain a lot of emotional content and, as aside effect, it is interesting to see how that is reflected in subtitles and their translations.", "labels": [], "entities": []}, {"text": "Before presenting Sentimentator itself, we will first discuss related work and the theoretical framework we work with.", "labels": [], "entities": []}, {"text": "The presentation of the seed/pilot dataset and its application for emotion detection follows the description of the tool and 25 ends with a concluding discussion.", "labels": [], "entities": [{"text": "emotion detection", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7894008755683899}]}], "datasetContent": [{"text": "For the seed data, we used the following procedure: On completed expert annotation, another expert annotates the same sentences with the data order randomized.", "labels": [], "entities": []}, {"text": "Ambiguous sentences were reviewed and the correct class was agreed upon.", "labels": [], "entities": []}, {"text": "In some cases where no agreement could be reached the sentence was excluded from the seed dataset.", "labels": [], "entities": [{"text": "seed dataset", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.7568645775318146}]}, {"text": "Our data collection will be unique in that it will provide a fine-grained multi-dimensional open source dataset for sentiment analysis and emotion detection in various languages.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.9749554395675659}, {"text": "emotion detection", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.7649272084236145}]}, {"text": "Annotation is on-going and the first real dataset will be available later in 2018.", "labels": [], "entities": []}, {"text": "For now we have a set of sentences with validated annotations that we will use as our seed data to get the gamified annotation started.", "labels": [], "entities": []}, {"text": "This dataset has already been used to investigate sentiment preservation in.", "labels": [], "entities": [{"text": "sentiment preservation", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.9593333899974823}]}, {"text": "We wanted to make the dataset as useful as possible to as many researchers as possible from the beginning.", "labels": [], "entities": []}, {"text": "This is why we selected an open parallel corpus, namely the OPUS movie subtitles corpus.", "labels": [], "entities": [{"text": "OPUS movie subtitles corpus", "start_pos": 60, "end_pos": 87, "type": "DATASET", "confidence": 0.8996723145246506}]}, {"text": "From this collection, approximately 9,000 English sentences were annotated into the following emotion classes: anger, anticipation, disgust, joy, fear, sadness, surprise, and trust.", "labels": [], "entities": [{"text": "surprise", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9546054601669312}]}, {"text": "This yielded a preliminary dataset of between 649 to 908 sentences per class (see).", "labels": [], "entities": []}, {"text": "For the classification experiments, we did not take into account the measure of emotion intensity that we introduce later in our annotation framework, which is also part of the seed sentence annotation.", "labels": [], "entities": []}, {"text": "ang ant dis fea joy sad sur tru: Emotion distribution in seed data We also keep the metadata and therefore all sentences can be paired with a particular movie, genre, time period, as well as its counterpart in another language.", "labels": [], "entities": []}, {"text": "This is valuable information for future research avenues.", "labels": [], "entities": []}, {"text": "We expect to have a full dataset by the end of the year as we will be collecting at least 100 000 annotations in September-October of 2018 from crowd annotators (students).", "labels": [], "entities": []}, {"text": "suggest using four non-experts to match the quality of one expert annotator, however, gamification, seed sentences, and rank-validation means that fewer annotations per sentence might be sufficient using our platform.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement is on average around 70-90% depending on the type of annotation and who is doing the annotation work (expert vs. non-expert), and this is where annotator agreement was for our data as well, varying between classes.", "labels": [], "entities": []}, {"text": "Based on initial timed annotations, atypical annotator can be expected to annotate up to 10 sentences per minute.", "labels": [], "entities": []}, {"text": "This means that it only takes just over one hour to annotate around 600 sentences.", "labels": [], "entities": []}, {"text": "If every annotator is asked to annotate 1000 sentences, this should not take more than a few hours each on average taking the learning curve into account.", "labels": [], "entities": []}, {"text": "The students are encouraged to annotate in languages other than English as well, resulting in at least two or three separate datasets with an expected minimum of 40 000 annotations each.", "labels": [], "entities": []}, {"text": "We currently have preliminary datasets for English, Italian, French, and Finnish.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracies achieved by previous studies", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9994551539421082}]}, {"text": " Table 2: Emotion distribution in seed data", "labels": [], "entities": []}, {"text": " Table 4: Confusion matrix for NB-based classification  of the test set.", "labels": [], "entities": [{"text": "NB-based classification", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7504287660121918}]}]}