{"title": [{"text": "Comparison of Assorted Models for Transliteration", "labels": [], "entities": [{"text": "Transliteration", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.7540165781974792}]}], "abstractContent": [{"text": "We report the results of our experiments in the context of the NEWS 2018 Shared Task on Transliteration.", "labels": [], "entities": [{"text": "NEWS 2018 Shared Task on Transliteration", "start_pos": 63, "end_pos": 103, "type": "TASK", "confidence": 0.7411699891090393}]}, {"text": "We focus on the comparison of several diverse systems, including three neural MT models.", "labels": [], "entities": []}, {"text": "A combination of discriminative, generative, and neural models obtains the best results on the development sets.", "labels": [], "entities": []}, {"text": "We also put forward ideas for improving the shared task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transliteration is the conversion of names and words between distinct writing scripts.", "labels": [], "entities": [{"text": "conversion of names and words between distinct writing scripts", "start_pos": 23, "end_pos": 85, "type": "TASK", "confidence": 0.757504959901174}]}, {"text": "It is an interesting and well-defined task, which is suitable for testing sequence-to-sequence models.", "labels": [], "entities": []}, {"text": "In this edition of the NEWS Shared Task on Machine Transliteration, we tested a number of different approaches on all provided languages and datasets.", "labels": [], "entities": [{"text": "NEWS Shared Task on Machine Transliteration", "start_pos": 23, "end_pos": 66, "type": "TASK", "confidence": 0.6069261481364568}]}, {"text": "Because of the sheer number of tested models, only minimal tuning was conducted.", "labels": [], "entities": []}, {"text": "The results demonstrate that, on average, the neural models perform better than other systems, and that a combination of neural and non-neural models further improves the results.", "labels": [], "entities": []}, {"text": "However, no individual system is clearly superior on all datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We divided the available data into three parts for training, validation, and development testing.", "labels": [], "entities": []}, {"text": "We created the validation sets for each language pair by randomly selecting instances from the provided training sets.", "labels": [], "entities": []}, {"text": "Our validation sets had the same size as the provided development sets: 1000 instances for each language pair, except 500 for EnVi.", "labels": [], "entities": [{"text": "EnVi", "start_pos": 126, "end_pos": 130, "type": "DATASET", "confidence": 0.9355304837226868}]}, {"text": "We trained the models on the remaining instances in the training sets.", "labels": [], "entities": []}, {"text": "We used the provided development sets for development testing, as well as for selecting the SEQUITUR model order, and tuning the linear combinations coefficients.", "labels": [], "entities": [{"text": "SEQUITUR", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.753083348274231}]}, {"text": "shows the development results (on the left).", "labels": [], "entities": []}, {"text": "The average word accuracy is computed across all 19 language pairs, using a result of 0% for runs which could not be completed (N/A).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9139889478683472}]}, {"text": "On average, our two neural systems outperform the other individual systems, with RL-NMT better than NMT inmost cases.", "labels": [], "entities": [{"text": "RL-NMT", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9608607292175293}]}, {"text": "Surprisingly, one of the two non-neural systems is the most accurate on about half of the datasets, even though DIRECTL+ (DTL) was not properly tuned, and SEQUITUR (SEQ) could not be run on three datasets.", "labels": [], "entities": [{"text": "DIRECTL+ (DTL)", "start_pos": 112, "end_pos": 126, "type": "METRIC", "confidence": 0.8679227590560913}]}, {"text": "On the other hand, the OpenNMT tool is well below the other systems, and completely fails on EnVi and EnKo.", "labels": [], "entities": [{"text": "EnVi", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.9444178342819214}, {"text": "EnKo", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.8319298028945923}]}, {"text": "Arguably, the most interesting outcome is that the linear combination (LC) of three diverse systems, DIRECTL+, SEQUITUR and RL-NMT substantially improves over the best-performing individual system on all datasets.", "labels": [], "entities": [{"text": "DIRECTL", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.9158270955085754}, {"text": "SEQUITUR", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.8802716732025146}, {"text": "RL-NMT", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.8602813482284546}]}, {"text": "We conjecture that traditional ML approaches perform better than neural networks on datasets with fewer training instances.", "labels": [], "entities": [{"text": "ML", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9833700656890869}]}, {"text": "The average training size for the sets on which the former surpass the latter is approximately 13 thousand vs. 20 thousand instances for the remaining sets.", "labels": [], "entities": []}, {"text": "Further evidence is provided by, which shows that SEQUITUR outperforms RL-NMT when the training set contains fewer than 400 instances.", "labels": [], "entities": [{"text": "SEQUITUR", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.8850258588790894}, {"text": "RL-NMT", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.7512773275375366}]}], "tableCaptions": [{"text": " Table 1: Transliteration word accuracy on the development and test sets of the shared task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.952113151550293}]}, {"text": " Table 2: The non-standard results of DTLM, and  the corresponding standard baseline.", "labels": [], "entities": [{"text": "DTLM", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.6010519862174988}]}]}