{"title": [{"text": "A Compositional Bayesian Semantics for Natural Language", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a compositional Bayesian semantics that interprets declarative sentences in a natural language by assigning them probability conditions.", "labels": [], "entities": []}, {"text": "These are conditional probabilities that estimate the likelihood that a competent speaker would endorse an assertion, given certain hypotheses.", "labels": [], "entities": []}, {"text": "Our semantics is implemented in a functional programming language.", "labels": [], "entities": []}, {"text": "It estimates the marginal probability of a sentence through Markov Chain Monte Carlo (MCMC) sampling of objects in vector space models satisfying specified hypotheses.", "labels": [], "entities": []}, {"text": "We apply our semantics to examples with several predicates and generalised quantifiers, including higher-order quantifiers.", "labels": [], "entities": []}, {"text": "It captures the vagueness of predication (both gradable and non-gradable), without positing a precise boundary for classifier application.", "labels": [], "entities": []}, {"text": "We present a basic account of semantic learning based on our semantic system.", "labels": [], "entities": [{"text": "semantic learning", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.8338022530078888}]}, {"text": "We compare our proposal to other current theories of probabilistic semantics, and we show that it offers several important advantages over these accounts.", "labels": [], "entities": []}], "introductionContent": [{"text": "In classical model theoretic semantics) the interpretation of a declarative sentence is given as a set of truth conditions with Boolean values.", "labels": [], "entities": []}, {"text": "This excludes vagueness from semantic interpretation, and it does not provide a natural framework for explaining semantic learning.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.8183183073997498}]}, {"text": "Indeed, semantic learning involves the acquisition of classifiers (predicates), which seems to require probabilistic learning.", "labels": [], "entities": [{"text": "semantic learning", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.8391023576259613}]}, {"text": "Recently several theories of probabilistic semantics for natural language have been proposed to accommodate both phenomena.", "labels": [], "entities": []}, {"text": "These accounts offer interesting ways of expressing vagueness, and suggestive approaches to semantic learning.", "labels": [], "entities": [{"text": "expressing vagueness", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.8106172978878021}, {"text": "semantic learning", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.8966743052005768}]}, {"text": "They also suffer from a number of serious shortcomings, some of which we briefly discuss in Section 4.", "labels": [], "entities": []}, {"text": "In this paper we propose a compositional Bayesian semantics for natural language in which we assign probability rather than truth conditions to declarative sentences.", "labels": [], "entities": []}, {"text": "We estimate the conditional probability of a sentence as the likelihood that an idealised competent speaker of the language would accept the assertion that the sentence expresses, given fixed interpretations of generalised quantifiers and certain other terms, and a set of specified hypotheses, p S (A | H).", "labels": [], "entities": []}, {"text": "S is a competent speaker of the language, A is the assertion that the sentence expresses, and H is the set of hypotheses on which we are conditioning the likelihood that S will endorse A.", "labels": [], "entities": []}, {"text": "On this approach assessing the probability of a sentence in the circumstances defined by the hypotheses is an instance of evaluating the application of a classifier acquired through supervised learning, to anew argument (set of arguments).", "labels": [], "entities": []}, {"text": "Our semantics interprets sentences as probabilistic programs).", "labels": [], "entities": []}, {"text": "Section 2 gives a detailed description of our implementation.", "labels": [], "entities": []}, {"text": "It involves encoding objects and properties as vectors in vector space models.", "labels": [], "entities": []}, {"text": "Our system uses Markov Chain Monte Carlo (MCMC) sampling, as implemented 2 in WebPPL (, a lightweight version of Church (, and it estimates the marginal probabilities of predications and quantified sentences relative to the models satisfying the constraints of an asserted set of hypotheses (p S (A | H)).", "labels": [], "entities": [{"text": "WebPPL", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.9324856996536255}]}, {"text": "We give examples of inferences involving several generalised quantifiers, including higher-order quantifiers (in the sense of) like most.", "labels": [], "entities": []}, {"text": "Our semantics uses the same vector space models and sampling mechanism to express both the vagueness of gradable predicates, like tall, and of ordinary property terms, such as red and chair.", "labels": [], "entities": []}, {"text": "Our semantic framework does not require extensive lexically specified content or pragmatic knowledge statements to estimate the parameters of our vector space models.", "labels": [], "entities": []}, {"text": "It also does not posit boundary values (hard coded or contextually specified) for the application of a predicate to an argument.", "labels": [], "entities": []}, {"text": "The system that we describe here is a prototype that offers a proof of concept for our approach.", "labels": [], "entities": []}, {"text": "A robust, wide coverage version of this system will be useful fora variety of tasks.", "labels": [], "entities": []}, {"text": "Three examples are as follows.", "labels": [], "entities": []}, {"text": "First, we intend to encode both semantic and real world knowledge as priors in our models.", "labels": [], "entities": []}, {"text": "These will sustain probabilistic inferencing that will support text understanding and question answering in away analogous to that in which Bayesian Networks are used for inference and knowledge representation in restricted domains.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.8000392019748688}, {"text": "question answering", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7727968692779541}, {"text": "knowledge representation", "start_pos": 185, "end_pos": 209, "type": "TASK", "confidence": 0.7414986491203308}]}, {"text": "Second, we envisage an integration of visual and other non-linguistic vector representations into our models.", "labels": [], "entities": []}, {"text": "This will facilitate the evaluation of candidate descriptions of images and scenes.", "labels": [], "entities": []}, {"text": "It will also allow us to assess the relative accuracy of statements concerning these scenes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9977715015411377}]}, {"text": "Finally, our system could be used as a filter on machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7879791557788849}]}, {"text": "Source and target sentences are expected to share the same probability values for the same models.", "labels": [], "entities": []}, {"text": "The success which our framework achieves in these applications will provide criteria for evaluating it.", "labels": [], "entities": []}, {"text": "In Section 3 we present an outline of our implemented system for semantic learning, that extends our compositional semantics to the probabilistic acquisition of classifiers.", "labels": [], "entities": [{"text": "semantic learning", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.8061557412147522}]}, {"text": "In Section 4 we compare our system to recent work in probabilistic semantics.", "labels": [], "entities": []}, {"text": "Finally, in Section 5 we state the main conclusions of our research, and we indicate the issues that we will address in future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}