{"title": [{"text": "Neural Machine Translation of Logographic Languages Using Sub-character Level Information", "labels": [], "entities": [{"text": "Neural Machine Translation of Logographic Languages", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8213979005813599}]}], "abstractContent": [{"text": "Recent neural machine translation (NMT) systems have been greatly improved by encoder-decoder models with attention mechanisms and sub-word units.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 7, "end_pos": 39, "type": "TASK", "confidence": 0.8670106728871664}]}, {"text": "However, important differences between languages with logographic and alphabetic writing systems have long been overlooked.", "labels": [], "entities": []}, {"text": "This study fo-cuses on these differences and uses a simple approach to improve the performance of NMT systems utilizing decomposed sub-character level information for logographic languages.", "labels": [], "entities": []}, {"text": "Our results indicate that our approach not only improves the translation capabilities of NMT systems between Chinese and English, but also further improves NMT systems between Chinese and Japanese, because it utilizes the shared information brought by similar sub-character units.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation () (NMT) systems based on sequence-to-sequence models) have recently become the de facto standard architecture.", "labels": [], "entities": [{"text": "Neural machine translation () (NMT)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7131355830601284}]}, {"text": "The models use attention mechanisms ( to keep records of all encoding results, and can focus on particular parts of these results during decoding, so that the model can produce longer and more accurate translations.", "labels": [], "entities": []}, {"text": "Sub-word units are another technique first introduced by application of the byte pair encoding (BPE) algorithm, and are used to breakup words in both source and target sentences into sequences of smaller units, learned without supervision.", "labels": [], "entities": []}, {"text": "This alleviates the risk of producing <unk> symbols when the model encounters infrequent \"unknown\" words, also known as the out-of-vocabulary (OOV) problem.", "labels": [], "entities": []}, {"text": "Moreover, sub-word units, which can be viewed as learned stems and affixes, can help the NMT model better encode the source sentence and decode the target sentence, particularly when the source and target languages share some similarities.", "labels": [], "entities": []}, {"text": "Almost all of the methods used to improve NMT systems were developed for alphabetic languages such as English, French, and German as either the source or target language, or both.", "labels": [], "entities": [{"text": "NMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9447058439254761}]}, {"text": "An alphabetic language typically uses an alphabet: a small set of letters (basic writing symbols) that each roughly represents a phoneme in the spoken language.", "labels": [], "entities": []}, {"text": "Words are composed by ordered letters, and sentences are composed by spacesegmented ordered words.", "labels": [], "entities": []}, {"text": "However, in other major writing systems-namely, logographic (or character-based) languages such as Chinese, Japanese, and traditional Korean-strokes are used to construct ideographs; ideographs are used to construct characters, which are the basic units for meaningful words.", "labels": [], "entities": []}, {"text": "Words can then further compose sentences.", "labels": [], "entities": []}, {"text": "In alphabetic languages, sub-word units are easy to identify, whereas in logographic languages, a similar effect can be achieved only if sub-character level information is taken into consideration.", "labels": [], "entities": []}, {"text": "Having noticed this significant difference between these two writing systems,,,, and used stroke-level information for logographic languages when constructing word embeddings; used visual information for strokes and Japanese Kanji Taking the ASPEC corpus as an example, the average word lengths are roughly 1.5 characters (Chinese words, tokenized by Jieba tokenizer), 1.7 characters (Japanese words, tokenized by MeCab tokenizer), and 5.7 characters (English words, tokenized by Moses tokenizer), respectively.", "labels": [], "entities": [{"text": "ASPEC corpus", "start_pos": 242, "end_pos": 254, "type": "DATASET", "confidence": 0.861214280128479}]}, {"text": "Therefore, when a sub-word model of similar vocabulary size is applied directly, English sub-words usually contain several letters, which are more effective in facilitating NMT, whereas Chinese and Japanese sub-words are largely just characters.", "labels": [], "entities": [{"text": "NMT", "start_pos": 173, "end_pos": 176, "type": "TASK", "confidence": 0.8906014561653137}]}, {"text": "radicals in a text classification task.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.845211942990621}]}, {"text": "Some studies have performed NMT tasks using various sub-word \"equivalents\".", "labels": [], "entities": [{"text": "NMT tasks", "start_pos": 28, "end_pos": 37, "type": "TASK", "confidence": 0.9009409546852112}]}, {"text": "For instance, trained factored NMT models using \"Pinyin\" 3 sequences on the source side.", "labels": [], "entities": []}, {"text": "Unfortunately, they did not apply a BPE algorithm during training, and their model also cannot perform factored decoding.", "labels": [], "entities": [{"text": "BPE", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.8138598203659058}]}, {"text": "directly applied a BPE algorithm to character sequences before building NMT models.", "labels": [], "entities": [{"text": "BPE", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.8810654878616333}]}, {"text": "However, they did not take advantage of sub-character level information during the training of sub-word and NMT models.", "labels": [], "entities": []}, {"text": "also attempted to use a factored encoder for Chinese NMT systems using radical data.", "labels": [], "entities": []}, {"text": "It is worth noting that although the idea of using ideographs and strokes in NLP tasks (particularly in NMT tasks) is not new, no previous NMT research has focused on the decoding process.", "labels": [], "entities": []}, {"text": "If it is also possible to construct an ideograph/stroke decoder, we can further investigate translations between logographic languages.", "labels": [], "entities": []}, {"text": "Additionally, no NMT research has previously used stroke data.", "labels": [], "entities": []}, {"text": "To summarize, there are three potential information gaps associated with current studies on NMT systems for logographic languages using sub-character level data: 1) no research has been performed on the decoding process; 2) no studies have trained models using sub-character level sub-words; and 3) no studies have attempted to build NMT models for logographic language pairs, despite their sharing many similarities.", "labels": [], "entities": []}, {"text": "This study investigates whether sub-character information can facilitate both encoding and decoding in NMT systems and between logographic language pairs, and aims to determine the best sub-character unit granularity for each setting.", "labels": [], "entities": []}, {"text": "The main contributions of this study are threefold: 1.", "labels": [], "entities": []}, {"text": "We create a sub-character database of Chinese character-based languages, and conduct MT experiments using various types of subcharacter NMT models.", "labels": [], "entities": [{"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.992605447769165}]}], "datasetContent": [{"text": "To answer our research questions, we setup a series of experiments to compare NMT models of logographic languages trained on word sequences, character-level sub-word unit sequences, and ideograph-and stroke-level subword unit sequences.", "labels": [], "entities": []}, {"text": "We performed two lines of experiments: 1.", "labels": [], "entities": []}, {"text": "We trained NMT models between logographic language and alphabetic language combinations, i.e., Japanese/Chinese and English.", "labels": [], "entities": []}, {"text": "In each model, we varied the data granularity for the logographic language, using \"character level\" or \"sub-character level\" (ideograph level and stroke level) granularities.", "labels": [], "entities": []}, {"text": "We used the character level NMT models as our baselines, and investigated whether the sub-character level NMT models could outperform the baseline models.", "labels": [], "entities": []}, {"text": "2. We trained NMT models between combinations of two logographic languages, i.e., Chinese and Japanese.", "labels": [], "entities": []}, {"text": "Similarly, we used data sets with different granularities: 1) Models lacking sub-character level data.", "labels": [], "entities": []}, {"text": "2) Models having sub-character level data on both sides (to confirm the results of the previous experiment).", "labels": [], "entities": []}, {"text": "For the experiments, the models will have both source and target sides.", "labels": [], "entities": []}, {"text": "The models will use sub-character level data with/without shared vocabularies (namely, ideograph models, stroke models, ideograph-stroke models, stroke-ideograph models, and ideograph/stroke models with shared vocabularies).", "labels": [], "entities": []}, {"text": "3) Pinyin baselines according to (, where both Pinyin word sequences with tones and character sequences with Pinyin factors are used with the encoder.", "labels": [], "entities": []}, {"text": "We trained our baselines and experiments using Chinese, Japanese, and English.", "labels": [], "entities": []}, {"text": "The Asian Scientific Paper Excerpt Corpus (ASPEC (Nakazawa et al., 2016)) and Casia2015 12 corpus were used for this purpose.", "labels": [], "entities": [{"text": "Asian Scientific Paper Excerpt Corpus (ASPEC (Nakazawa et al., 2016))", "start_pos": 4, "end_pos": 73, "type": "DATASET", "confidence": 0.902986867087228}, {"text": "Casia2015 12 corpus", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.8209807276725769}]}, {"text": "ASPEC contains a Japanese-English paper abstract corpus of 3 million parallel sentences (ASPEC-JE) and a Japanese-Chinese paper excerpt corpus of 680,000 parallel sentences (ASPEC-JC).", "labels": [], "entities": [{"text": "ASPEC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9425328373908997}]}, {"text": "We used the first million confidently aligned parallel sentences in ASPEC-JE and all of the ASPEC-JC data to cover Japanese-English and Japanese-Chinese language pairs.", "labels": [], "entities": [{"text": "ASPEC-JE", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.8875309824943542}, {"text": "ASPEC-JC data", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.8674807846546173}]}, {"text": "The Casia2015 corpus contains approximately 1 million parallel Chinese-English sentences.", "labels": [], "entities": [{"text": "Casia2015 corpus", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8491723537445068}]}, {"text": "All data in the Casia2015 corpus were used to cover Chinese-English language pairs.", "labels": [], "entities": [{"text": "Casia2015 corpus", "start_pos": 16, "end_pos": 32, "type": "DATASET", "confidence": 0.9307882785797119}]}, {"text": "During training, the maximum length hyperparameter was adjusted to ensure 90% coverage of the training data.", "labels": [], "entities": [{"text": "coverage", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.973193883895874}]}, {"text": "For development and testing, the ASPEC corpus has an official split between the development set and test set; however, because the Casia2015 corpus is not similarly split, we made random selections from the development set and test set of 1,000 sentences each.", "labels": [], "entities": [{"text": "ASPEC corpus", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.7805885672569275}, {"text": "Casia2015 corpus", "start_pos": 131, "end_pos": 147, "type": "DATASET", "confidence": 0.9007227122783661}]}], "tableCaptions": []}