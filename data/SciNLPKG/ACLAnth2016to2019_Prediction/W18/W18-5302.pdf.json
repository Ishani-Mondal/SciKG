{"title": [{"text": "Semantic role labeling tools for biomedical question answering: a study of selected tools on the BioASQ datasets", "labels": [], "entities": [{"text": "Semantic role labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7444775104522705}, {"text": "biomedical question answering", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.6011346379915873}, {"text": "BioASQ datasets", "start_pos": 97, "end_pos": 112, "type": "DATASET", "confidence": 0.9250611960887909}]}], "abstractContent": [{"text": "Question answering (QA) systems usually rely on advanced natural language processing components to precisely understand the questions and extract the answers.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9292571783065796}]}, {"text": "Semantic role labeling (SRL) is known to boost performance for QA, but its use for biomedical texts has not yet been fully studied.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7995584309101105}, {"text": "QA", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9609153866767883}]}, {"text": "We analyzed the performance of three SRL tools (BioKIT, BIOSMILE and PathLSTM) on 1776 questions from the BioASQ challenge.", "labels": [], "entities": [{"text": "SRL", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.941412627696991}, {"text": "BIOSMILE", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.890534520149231}]}, {"text": "We compared the systems regarding the coverage of the questions and snippets, as well as based on pre-defined criteria, such as easiness of installation , supported formats and usability.", "labels": [], "entities": []}, {"text": "Finally , we integrated two of the tools in a simple QA system to further evaluate their performance over the official BioASQ test sets.", "labels": [], "entities": [{"text": "BioASQ test sets", "start_pos": 119, "end_pos": 135, "type": "DATASET", "confidence": 0.9265259106953939}]}], "introductionContent": [{"text": "Question answering (QA) is one of the most complex applications of natural language processing (NLP).", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.944458258152008}, {"text": "natural language processing (NLP)", "start_pos": 67, "end_pos": 100, "type": "TASK", "confidence": 0.7897183994452158}]}, {"text": "QA systems need to precisely understand questions, in order to infer which information is being requested, and usually include steps such as question type and expected answer detection (.", "labels": [], "entities": [{"text": "expected answer detection", "start_pos": 159, "end_pos": 184, "type": "TASK", "confidence": 0.5825494527816772}]}, {"text": "Likewise, the candidate documents or snippets that potentially contain the answers also need to be analyzed to extract the requested answer.", "labels": [], "entities": []}, {"text": "Therefore, such systems usually rely on various NLP components, such as named-entity recognition, part-of-speech tagging and semantic parsing.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.7402723729610443}, {"text": "part-of-speech tagging", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.702696219086647}, {"text": "semantic parsing", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.7107893228530884}]}, {"text": "Semantic role labeling (SRL) is one of the most popular tools to support QA systems.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8226306736469269}]}, {"text": "It consists of automatically identifying predicates and their arguments, the so-called predicate-argument structures (PAS).", "labels": [], "entities": []}, {"text": "For instance, * Current address: German Federal Institute for Risk Assessment, Diedersdorfer Weg 1, Berlin 12277, Germany for the question \"How many genes does the human hoxD cluster contain?\",, an SRL tool for biomedicine, correctly identified the following PAS: the predicate contains and two arguments (Arg0 -the human hoxD cluster and Arg1 -How many genes).", "labels": [], "entities": [{"text": "Diedersdorfer Weg 1, Berlin 12277", "start_pos": 79, "end_pos": 112, "type": "DATASET", "confidence": 0.8774384607871374}, {"text": "Arg0", "start_pos": 306, "end_pos": 310, "type": "METRIC", "confidence": 0.9860720038414001}, {"text": "Arg1", "start_pos": 339, "end_pos": 343, "type": "METRIC", "confidence": 0.989993155002594}]}, {"text": "SRL is known for its potential to boost QA performance when extracting PAS from both the question and the text (e.g., snippets of sentences).", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.61435866355896}, {"text": "QA", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.8752446174621582}]}, {"text": "Ideally, the same (or semantically related) PAS should be found in both of them in order to effectively support QA applications.", "labels": [], "entities": []}, {"text": "Hence, a good coverage is an important requirement fora tool to be suitable for QA.", "labels": [], "entities": [{"text": "QA", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9085344672203064}]}, {"text": "For our given example question, one of the answer snippets provided by the BioASQ challenge) was The human HOXD complex contains nine genes HOXD1, HOXD3, HOXD4, HOXD8, HOXD9, HOXD10, HOXD11, HOXD12 and HOXD13, which are clustered from.", "labels": [], "entities": []}, {"text": "The following PAS was detected by BioKIT: the predicate contains and the arguments Arg0 the human HOXD complex and Arg1 nine genes.", "labels": [], "entities": [{"text": "PAS", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.7274982929229736}, {"text": "BioKIT", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.869482696056366}, {"text": "Arg0", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9034379124641418}]}, {"text": "In this example, there is a perfect match between the predicates from the question and the snippet.", "labels": [], "entities": []}, {"text": "Further, the values for the argument Arg0 are similar and could be considered as a match too.", "labels": [], "entities": [{"text": "Arg0", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9885421991348267}]}, {"text": "The answer nine genesis indeed contained in Arg1, which also matches the argument type of the question word of the sentence.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9929967522621155}]}, {"text": "This example demonstrates how QA systems can benefit from PASs that were automatically detected by an SRL tool.", "labels": [], "entities": []}, {"text": "However, language is more complex than reflected in this example.", "labels": [], "entities": []}, {"text": "Thus, besides performing SRL, further challenges arise to integrate SRL and gain significant advantages in QA systems.", "labels": [], "entities": [{"text": "SRL", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9911885261535645}, {"text": "SRL", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9695290923118591}]}, {"text": "We are not aware of a comprehensive evaluation of available SRL tools on the BioASQ dataset, which is the most comprehensive dataset on biomedical QA ().", "labels": [], "entities": [{"text": "SRL", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9593706130981445}, {"text": "BioASQ dataset", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.9368208050727844}]}, {"text": "We investigated three SRL tools, two of which were specifically developed for the biomedical domain, namely, BioKIT and BIOSMILE ), and one which is based on deep learning, i.e.,.", "labels": [], "entities": [{"text": "SRL", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9858312010765076}, {"text": "BioKIT", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.6487596035003662}, {"text": "BIOSMILE", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.8962852358818054}]}, {"text": "The latter has neither been trained nor tuned to biomedicine but has recently achieved promising results on SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.609810471534729}]}, {"text": "Our contribution in this work is three-fold: (i) we provide a comprehensive overview on SRL for biomedicine and QA; (ii) we perform a comparison of selected tools regarding pre-defined criteria based on hands-on experiments; and (iii) we evaluated the selected SRL tools on the BioASQ datasets regarding their PAS coverage and performance in a QA system.", "labels": [], "entities": [{"text": "SRL", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9921247363090515}, {"text": "BioASQ datasets", "start_pos": 278, "end_pos": 293, "type": "DATASET", "confidence": 0.9546455442905426}]}, {"text": "In the next section we provide an overview on previous work on SRL for biomedical QA, followed by the methodology we defined for the selection, comparison and evaluation of the SRL tools.", "labels": [], "entities": [{"text": "SRL", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9887781739234924}, {"text": "biomedical QA", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.6194114536046982}, {"text": "SRL", "start_pos": 177, "end_pos": 180, "type": "TASK", "confidence": 0.9538289308547974}]}, {"text": "In section 4 we present our results and discussion, followed by the conclusions of this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We utilized the training dataset of 1,776 questions made available for the BioASQ challenge in 2017 ().", "labels": [], "entities": [{"text": "BioASQ challenge", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.5508681833744049}]}, {"text": "It combines test sets from the first four challenges.", "labels": [], "entities": []}, {"text": "This dataset contains questions and the corresponding snippets of text which include the answer to the questions.", "labels": [], "entities": []}, {"text": "The BioASQ dataset addresses four types of questions: factoid, list, summary and yes/no. shows statistics on the number of questions and snippets.", "labels": [], "entities": [{"text": "BioASQ dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8290423154830933}]}, {"text": "We considered all four question types in our analysis.", "labels": [], "entities": []}, {"text": "We installed each tool (or accessed it via web service) and ran them on the questions and corresponding snippets of the BioASQ training dataset.", "labels": [], "entities": [{"text": "BioASQ training dataset", "start_pos": 120, "end_pos": 143, "type": "DATASET", "confidence": 0.8436150948206583}]}, {"text": "The BIOSMILE API was rather slow and unstable, therefore, we did not manage to annotate the questions and snippets of the 4th year of the BioASQ challenge with BIOSMILE, which is part of the training dataset.", "labels": [], "entities": [{"text": "BIOSMILE", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.7127304673194885}, {"text": "BIOSMILE", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.47756996750831604}]}, {"text": "Hence, we were only able to evaluate 1,308 questions and the corresponding 16,791 snippets for BIOSMILE.", "labels": [], "entities": [{"text": "BIOSMILE", "start_pos": 95, "end_pos": 103, "type": "DATASET", "confidence": 0.721638023853302}]}, {"text": "However, this should not significantly compromise the comparison between the tools, given that the BioASQ dataset appears to be very homogeneous.", "labels": [], "entities": [{"text": "BioASQ dataset", "start_pos": 99, "end_pos": 113, "type": "DATASET", "confidence": 0.8909800946712494}]}, {"text": "We analyzed the tools with on three approaches: (a) an assessment based on pre-defined criteria (cf. Section 3.4); (b) an evaluation of the coverage by counting the numbers of questions and snippets for which PASs were found; and (c) performance of the tools in a simple QA system (cf. Section 3.5).", "labels": [], "entities": []}, {"text": "We also analyzed the tools regarding some selected criteria: Installation.", "labels": [], "entities": []}, {"text": "It checks whether the tool could be easily installed or whether it required advanced skills for building, as well as whether we experienced any issues related to missing or outdated dependencies.", "labels": [], "entities": []}, {"text": "This is important fora smooth integration into a QA system, given that the latter should not suffer from alack of portability or maintainability after the integration.", "labels": [], "entities": []}, {"text": "Support of standardized web API.", "labels": [], "entities": []}, {"text": "It checks whether the tool offers a Web service and whether it could be accessed and used via API calls following standards, e.g., REST.", "labels": [], "entities": []}, {"text": "This is important if no source or binaries are available to download.", "labels": [], "entities": []}, {"text": "Additionally, this functionality constitutes a straightforward and easy way of integration without the use of own computational resources.", "labels": [], "entities": []}, {"text": "It specifies the supported standard input formats, e.g., XML, JSON or CoNLL.", "labels": [], "entities": []}, {"text": "Standardized input formats can facilitate the integration process independent from the system's platform.", "labels": [], "entities": []}, {"text": "Similar to standardized input formats, it specifies the supported standard output formats, e.g., XML, JSON or CoNLL.", "labels": [], "entities": []}, {"text": "It is our subjective rating on how easy it was to parse the content to and from the supported input and output formats.", "labels": [], "entities": [{"text": "parse the content", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.8782402475674947}]}, {"text": "It specifies whether the tool is able to handle special characters or if it runs into errors at presence of certain characters in the input text.", "labels": [], "entities": []}, {"text": "It assesses the tool's time performance for annotating questions and answer snippets.", "labels": [], "entities": []}, {"text": "This should give an idea to which degree an integration of the respective SRL tool could slowdown the whole QA system.", "labels": [], "entities": []}, {"text": "It indicates how reliable the SRL tool behaves with regard to stability and accessibility.", "labels": [], "entities": [{"text": "SRL", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9354875087738037}]}, {"text": "Issues with robustness might, for instance, be caused by the input or the unresponsiveness of a web service.", "labels": [], "entities": []}, {"text": "We present an evaluation of the three selected SRL tools on the previously defined criteria (cf) and provide a detailed discussion on our impressions for each tool.", "labels": [], "entities": [{"text": "SRL", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9707156419754028}]}, {"text": "It does not support a binary, executable package nor a Web service and, therefore, it needed to be built on the Linux operating system.", "labels": [], "entities": []}, {"text": "It was admittedly very hard to build and compile BioKIT, given that it is mainly written in Python and C but also depends on other packages and languages, such as Fortran.", "labels": [], "entities": []}, {"text": "Many dependencies were outdated or missing and had to be searched in the Web.", "labels": [], "entities": []}, {"text": "Therefore, simply following the installation instructions was not sufficient as some of the dependencies were themselves hard to build.", "labels": [], "entities": []}, {"text": "Further, parsing the CoNLL format was more challenging than parsing XML or JSON into an objectoriented representation because the PASs had to be extracted by dynamically matching row and column indexes of the presented predicates and arguments.", "labels": [], "entities": []}, {"text": "Additionally, BioKIT failed at handling special characters which led to annoying runtime errors.", "labels": [], "entities": [{"text": "BioKIT", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.75645512342453}]}, {"text": "Usually, BioKIT's preprocessing pipeline was meant to eliminate problematic characters but some symbols (e.g., \"ae\", \"\u00a8 o\" or \"\u02d9 o\") were not handled by the system.", "labels": [], "entities": [{"text": "BioKIT", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.8107784986495972}]}, {"text": "As a result, BioKIT crashed with an error after processing thousands of sentences without returning any result when it ran into a special character.", "labels": [], "entities": [{"text": "BioKIT", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.4699244499206543}]}, {"text": "We collected a set of almost 20 of such characters that we eliminated in an own script-based preprocessing step.", "labels": [], "entities": []}, {"text": "Depending on the length of the question or snippet, the processing of one question or snippet took at least 600 milliseconds or few seconds.", "labels": [], "entities": []}, {"text": "This could be rated as a fast performance, but only when labeling many questions at once.", "labels": [], "entities": []}, {"text": "If BioKIT was just used to process a single question, its runtime exceeded one minute, given the necessary time to load models into memory.", "labels": [], "entities": [{"text": "BioKIT", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.5209226608276367}]}, {"text": "In spite of the problem with special characters, we found BioKIT to be reliable and stable.", "labels": [], "entities": [{"text": "BioKIT", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.6353036165237427}]}, {"text": "It is not available to download in anyway (source code, binaries or executables).", "labels": [], "entities": []}, {"text": "Therefore, we accessed it via a Web service with the REST API.", "labels": [], "entities": []}, {"text": "The input and output were both formatted as XML, which facilitated parsing with standard XML parsing libraries.", "labels": [], "entities": [{"text": "parsing", "start_pos": 67, "end_pos": 74, "type": "TASK", "confidence": 0.9714568853378296}]}, {"text": "Further, we experienced no problems regarding special characters.", "labels": [], "entities": []}, {"text": "However, with regard to the processing speed, the web service was rather unstable.", "labels": [], "entities": []}, {"text": "In rare cases, it was possible to annotate a sentence in about a second but there were many problems regarding the robustness of the service.", "labels": [], "entities": []}, {"text": "Frequently, it was not possible to send more than five requests in a row without waiting several minutes in between, otherwise the Web service became unresponsive fora longtime.", "labels": [], "entities": []}, {"text": "At some point, the service became so slow and had so many down times that we did not manage to annotate the data of the 4th year of the BioASQ challenge.", "labels": [], "entities": [{"text": "BioASQ challenge", "start_pos": 136, "end_pos": 152, "type": "TASK", "confidence": 0.49496041238307953}]}, {"text": "Installing PathLSTM was not as hard as BioKIT but there were still some timeconsuming issues.", "labels": [], "entities": [{"text": "BioKIT", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.7126196026802063}]}, {"text": "The tool can be build via Maven but it was underdevelopment during the time that we were using it (as of June/2017).", "labels": [], "entities": []}, {"text": "The code actually contained missing or wrong Maven dependencies and even a bug due to an outdated or not committed class.", "labels": [], "entities": []}, {"text": "Hence, the installation process required certain research and code review to find out that an earlier git commit was working properly.", "labels": [], "entities": []}, {"text": "Recently, the developers of PathLSTM published a more stable package but we did not check its feasibility yet.", "labels": [], "entities": []}, {"text": "The input and output formats also followed the CoNLL format and, hence, were very similar to BioKIT.", "labels": [], "entities": [{"text": "CoNLL format", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.8878590166568756}, {"text": "BioKIT", "start_pos": 93, "end_pos": 99, "type": "DATASET", "confidence": 0.9207738637924194}]}, {"text": "The CoNLL parser for BioKIT could be reused with small adaptations.", "labels": [], "entities": [{"text": "BioKIT", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.6450784802436829}]}, {"text": "PathLSTM had similar issues with special characters and could therefore reuse the preprocessing script that was created for BioKIT.", "labels": [], "entities": [{"text": "PathLSTM", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9357898235321045}, {"text": "BioKIT", "start_pos": 124, "end_pos": 130, "type": "DATASET", "confidence": 0.9139207601547241}]}, {"text": "When annotating many questions or answer snippets at once, PathLSTM could reach an annotation rate as low as 300 milliseconds per sentence.", "labels": [], "entities": []}, {"text": "We considered it as being very fast, in comparison to the other tools.", "labels": [], "entities": []}, {"text": "But if trying to annotate a single sentence, PathLSTM had the same issue as BioKIT and needed almost one and a half minute to load the models into memory.", "labels": [], "entities": [{"text": "BioKIT", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.8210576176643372}]}, {"text": "This section compares the three SRL tools with regard to the usefulness and completeness of the detected PASs for the QA task.", "labels": [], "entities": [{"text": "SRL", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9521918296813965}, {"text": "QA task", "start_pos": 118, "end_pos": 125, "type": "TASK", "confidence": 0.8156082332134247}]}, {"text": "As pointed out in, the PAS coverage of SRL systems is an important factor when trying to successfully integrate SRL into QA.", "labels": [], "entities": [{"text": "SRL", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9245337247848511}, {"text": "SRL", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9275057911872864}]}, {"text": "Therefore, we compared the PAS coverage of each tool for questions and answer snippets from the BioASQ datasets.", "labels": [], "entities": [{"text": "PAS coverage", "start_pos": 27, "end_pos": 39, "type": "METRIC", "confidence": 0.9093038737773895}, {"text": "BioASQ datasets", "start_pos": 96, "end_pos": 111, "type": "DATASET", "confidence": 0.9555579721927643}]}, {"text": "With special regard to the QA task, we analyzed the PAS matching coverage between questions and corresponding answer snippets.", "labels": [], "entities": [{"text": "QA task", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.8409699499607086}, {"text": "PAS matching coverage", "start_pos": 52, "end_pos": 73, "type": "METRIC", "confidence": 0.7597485184669495}]}, {"text": "The PAS matching coverage is defined as the percentage of questions for which a PAS match could be found in any of the corresponding answer snippets.", "labels": [], "entities": [{"text": "PAS matching coverage", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.7178451418876648}]}, {"text": "PAS coverage for questions and answer snippets.", "labels": [], "entities": []}, {"text": "gives an overview on the PAS coverage reached by the respective tools for the various types of questions and for all answer snippets in general.", "labels": [], "entities": [{"text": "PAS", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8054063320159912}]}, {"text": "Answer snippets are not presented by question types because they do not differ by question type.", "labels": [], "entities": []}, {"text": "When comparing the coverage of different question types, the lowest coverage values were reached for summary questions, while the highest coverage values were reached for yes/no questions.", "labels": [], "entities": [{"text": "coverage", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9649239778518677}]}, {"text": "Only BIOSMILE performed better on factoid and list questions than on yes/no questions.", "labels": [], "entities": [{"text": "BIOSMILE", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9677696824073792}]}, {"text": "This is probably due to predicate stems such as door have that are widely used in yes/no questions which not part of the predicates supported by BIOSMILE.", "labels": [], "entities": [{"text": "BIOSMILE", "start_pos": 145, "end_pos": 153, "type": "DATASET", "confidence": 0.8956209421157837}]}, {"text": "BIOSMILE obtained the lowest coverage results of the three tools, especially when looking at the answer snippets, which only 15.2% of them had PAS annotations.", "labels": [], "entities": [{"text": "BIOSMILE", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.5904372930526733}, {"text": "coverage", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9883988499641418}]}, {"text": "For list questions, BIOSMILE reached a coverage of 65.1%, which was slightly higher than the coverage of BioKIT in the same category (61.8%).", "labels": [], "entities": [{"text": "BIOSMILE", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9089622497558594}, {"text": "coverage", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9906588792800903}, {"text": "BioKIT", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.8902806639671326}]}, {"text": "The main reason for the low coverage of BIOSMILE is most probably the limited set of 82 biomedical predicates.", "labels": [], "entities": [{"text": "coverage", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9603636860847473}, {"text": "BIOSMILE", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.5718227624893188}]}, {"text": "BioKIT reached the maximum coverage for yes/no questions (99.8%).", "labels": [], "entities": [{"text": "BioKIT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8462535738945007}, {"text": "coverage", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.996704638004303}]}, {"text": "This could be due to the fact that do (727) and be (247) are the top predicate stems detected in the questions.", "labels": [], "entities": []}, {"text": "In comparison to this, PathLSTM only labeled do 27 times as a predicate and never labeled be.", "labels": [], "entities": []}, {"text": "Additionally, BioKIT also labeled auxiliary verbs as predicates, which appear very often in yes/no questions, and might explain its high coverage.", "labels": [], "entities": [{"text": "BioKIT", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.873183012008667}, {"text": "coverage", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9491878747940063}]}, {"text": "Unfortunately, auxiliary verbs like has or has been are not known to have much semantic value.", "labels": [], "entities": []}, {"text": "Hence, this high coverage might not be seen as an advantage for PathLSTM.", "labels": [], "entities": [{"text": "coverage", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.983439028263092}]}, {"text": "In general, PathLSTM obtained significantly higher coverage values than the other tools.", "labels": [], "entities": [{"text": "coverage", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9770761132240295}]}, {"text": "In contrast to leaving out auxiliary verbs, the high coverage of PathLSTM can be explained by detecting about three times as many distinct predicates as BioKIT.", "labels": [], "entities": [{"text": "PathLSTM", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.7567550539970398}, {"text": "BioKIT", "start_pos": 153, "end_pos": 159, "type": "DATASET", "confidence": 0.6400492787361145}]}, {"text": "In general, reaching a higher coverage might be good for QA, but by looking at some of the annotations, we found that PathLSTM labeled many nouns (as predicates) that did not even had in a verb form and most likely did not represent a predicate.", "labels": [], "entities": [{"text": "QA", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.8623366355895996}]}, {"text": "For example, the most frequent predicates found by PathLSTM were nouns such as disease or syndrome, none of which are regularly used as predicates.", "labels": [], "entities": [{"text": "PathLSTM", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.7945604920387268}]}, {"text": "PAS matching coverage between questions and answer snippets We evaluated two levels of PAS matching coverage between questions and answer snippets.", "labels": [], "entities": [{"text": "PAS matching coverage between questions and answer snippets", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.7113150097429752}, {"text": "PAS matching", "start_pos": 87, "end_pos": 99, "type": "TASK", "confidence": 0.8859748542308807}]}, {"text": "The first level, which is presented in, is the proportion of questions for which any answer snippet contained a PAS with the same predicate stem.", "labels": [], "entities": [{"text": "PAS", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9205471873283386}]}, {"text": "The second level, which is presented in, requires that both predicate argument structures, from the question and from the particular answer snippet, share a similar argument type besides the predicate stem.", "labels": [], "entities": []}, {"text": "The: Coverage of the questions (in %) for which a PAS match between the question and any of the related snippets was found.", "labels": [], "entities": [{"text": "PAS match", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9697754979133606}]}, {"text": "A PAS match was counted, if the predicate stem and any of the related argument types matched.", "labels": [], "entities": [{"text": "PAS", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9501007795333862}]}, {"text": "* BIOSMILE was only evaluated on data from the first three years of the BioASQ challenge.", "labels": [], "entities": [{"text": "BIOSMILE", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9713003039360046}, {"text": "BioASQ challenge", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.48612983524799347}]}, {"text": "argument type and the predicate stem have to be related by the same predicate.", "labels": [], "entities": []}, {"text": "On both PAS matching levels, BIOSMILE reached very poor results, between 2% and 4% of PAS matching coverage.", "labels": [], "entities": [{"text": "PAS matching", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.8988004922866821}, {"text": "BIOSMILE", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.5362321734428406}, {"text": "PAS matching", "start_pos": 86, "end_pos": 98, "type": "TASK", "confidence": 0.8134949207305908}]}, {"text": "It appears that the same questions of found by BIOSMILE also fulfill the requirements of, which might indicate that the found PAS matches are of a good quality.", "labels": [], "entities": [{"text": "BIOSMILE", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.5343870520591736}]}, {"text": "Nevertheless, the coverage is so low that BIOSMILE might only be considered in combination with other tools with a higher coverage in order to be efficiently used for biomedical QA.", "labels": [], "entities": [{"text": "coverage", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9932079911231995}, {"text": "BIOSMILE", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.8554782867431641}, {"text": "biomedical QA", "start_pos": 167, "end_pos": 180, "type": "TASK", "confidence": 0.617666631937027}]}, {"text": "It would be pointless to exclusively rely on a tool that can only contribute to answering up to 4% of the questions.", "labels": [], "entities": []}, {"text": "In contrast, PathLSTM reached the highest PAS matching coverage values on both levels.", "labels": [], "entities": [{"text": "PathLSTM", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.7576161623001099}, {"text": "PAS matching coverage", "start_pos": 42, "end_pos": 63, "type": "METRIC", "confidence": 0.8496265411376953}]}, {"text": "Table 5 shows that PathLSTM obtained 82.5% PAS matching coverage for list questions with matching predicate stems.", "labels": [], "entities": [{"text": "PAS matching coverage", "start_pos": 43, "end_pos": 64, "type": "METRIC", "confidence": 0.9152538776397705}]}, {"text": "Further, shows that for 72.2% of the questions, a matching argument type was present.", "labels": [], "entities": []}, {"text": "On the one hand, the high coverage of PathLSTM might lead to a high recall when implementing a QA system on top of the annotations.", "labels": [], "entities": [{"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9994269609451294}]}, {"text": "On the other hand, our previous analysis of PathLSTM's predicates (cf. above) showed that they might be of poor quality.", "labels": [], "entities": []}, {"text": "The PAS matching coverage for BioKIT were not as high as the results reached by PathLSTM but superior than those from BIOSMILE.", "labels": [], "entities": [{"text": "PAS matching coverage", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.8998255133628845}, {"text": "BioKIT", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.691453754901886}, {"text": "PathLSTM", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.8116475343704224}, {"text": "BIOSMILE", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.892744243144989}]}, {"text": "BioKIT leaves some space for improvement regarding coverage by finding matches for about one third of the factoid, list and yes/no questions and less than one sixth for summary questions.", "labels": [], "entities": [{"text": "BioKIT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7773841619491577}]}, {"text": "It is striking that the differences between the PAS matching coverage values of both levels are not very large (below 2.5%).", "labels": [], "entities": [{"text": "PAS matching coverage", "start_pos": 48, "end_pos": 69, "type": "METRIC", "confidence": 0.7233391801516215}]}, {"text": "In contrast to PathLSTM, this might bean indicator that PAS matches found by BioKIT are actually of a good quality and semantically relevant, and not just simply include matching terms that are not even real predicates and hence have no related arguments.", "labels": [], "entities": []}, {"text": "Finally, PathLSTM reached PAS matching coverage values which are in average more than twice as large as those from BioKIT, but the quality and usefulness of the PAS matches from PathLSTM are still dubious.", "labels": [], "entities": [{"text": "PAS matching coverage", "start_pos": 26, "end_pos": 47, "type": "METRIC", "confidence": 0.587531715631485}, {"text": "BioKIT", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.8965125679969788}]}, {"text": "We compared BioKIT and PathLSTM regarding their performance on the fourth year of the BioASQ challenge.", "labels": [], "entities": [{"text": "BioKIT", "start_pos": 12, "end_pos": 18, "type": "DATASET", "confidence": 0.8251764178276062}, {"text": "PathLSTM", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.8431494235992432}]}, {"text": "This dataset is composed of five batches of 100 questions and we provide detailed results for each batch.", "labels": [], "entities": []}, {"text": "The results were obtained by uploading JSON result files to the BioASQ Oracle evaluation system 7 . The BIOS-MILE system was not further evaluated due to (i) its low PAS matching coverage (cf., which were very unpromising, and (ii) the instability of the Web service, which did not allow us to obtain results for this test set.", "labels": [], "entities": [{"text": "BioASQ Oracle evaluation system 7", "start_pos": 64, "end_pos": 97, "type": "DATASET", "confidence": 0.8871948957443238}, {"text": "PAS matching coverage", "start_pos": 166, "end_pos": 187, "type": "METRIC", "confidence": 0.9111937284469604}]}, {"text": "To measure the impact of the SRL components added to the QA system, we included a baseline QA solution (NOSRL) which did not rely on SRL but simply only on the fall-back solutions (cf. Section 3.5).", "labels": [], "entities": []}, {"text": "As we could not propose appropriate fall-back solutions for the factoid questions, we evaluated the NOSRL baseline only for yes/no and list questions.", "labels": [], "entities": [{"text": "NOSRL baseline", "start_pos": 100, "end_pos": 114, "type": "DATASET", "confidence": 0.7529914379119873}]}, {"text": "Yes/no. evaluates the accuracy of the SRL-based QA and the NOSRL systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.999574601650238}, {"text": "SRL-based QA", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.7667617797851562}, {"text": "NOSRL", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.7765396237373352}]}, {"text": "The latter simply answered yes to all questions.", "labels": [], "entities": []}, {"text": "For all 5 batches of the fourth year of the BioASQ challenge, the SRL-based systems did not provide any answers different than yes.", "labels": [], "entities": [{"text": "BioASQ challenge", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6507009863853455}]}, {"text": "Therefore, all systems achieved the same accuracy values.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9990839958190918}]}, {"text": "Obviously, our rules failed to match any of the valid noanswers in the fourth year's dataset.", "labels": [], "entities": [{"text": "fourth year's dataset", "start_pos": 71, "end_pos": 92, "type": "DATASET", "confidence": 0.7117644250392914}]}, {"text": "Subsequently, we cannot provide insight with respect to the performance of the SRL tools.", "labels": [], "entities": [{"text": "SRL", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9298143982887268}]}, {"text": "compares the performance on factoid questions for the different SRL-based QA systems by means of the mean reciprocal rank measure (MRR).", "labels": [], "entities": [{"text": "mean reciprocal rank measure (MRR)", "start_pos": 101, "end_pos": 135, "type": "METRIC", "confidence": 0.8754145588193621}]}, {"text": "The results show that the BioKIT-based QA system performed much better than the PathLSTM-based version.", "labels": [], "entities": [{"text": "BioKIT-based QA", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.527140349149704}]}, {"text": "For the second and third batch, the PathLSTM-based system did not find any correct answer.", "labels": [], "entities": []}, {"text": "shows the mean average F-measure results for the five batches.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9982120990753174}]}, {"text": "In general, BioKIT performed better than the NOSRL system, while: Evaluation of the mean average F-measure (in %) for list questions on the 5 batches of the fourth BioASQ challenge.", "labels": [], "entities": [{"text": "BioKIT", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.7358672022819519}, {"text": "NOSRL", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.6544645428657532}, {"text": "F-measure", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.935370683670044}]}, {"text": "the PathLSTM-based system performed worse than the latter.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the BioASQ training dataset for  each question type.", "labels": [], "entities": [{"text": "BioASQ training dataset", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.6395278970400492}]}, {"text": " Table 2: Overview on the PAS matching levels and corresponding weights for the various question types.", "labels": [], "entities": [{"text": "PAS", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.6873995661735535}]}, {"text": " Table 4: PAS coverage (in %) for the various types of questions and for answer snippets. * BIOSMILE was only  evaluated on data from the first three years of the BioASQ challenge.", "labels": [], "entities": [{"text": "PAS coverage", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.8424426913261414}, {"text": "BIOSMILE", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9851858019828796}, {"text": "BioASQ challenge", "start_pos": 163, "end_pos": 179, "type": "TASK", "confidence": 0.5554691255092621}]}, {"text": " Table 5: Coverage of the questions (in %) for which a  predicate match between the question and any of the  related answer snippets was found. * BIOSMILE was  only evaluated on data from the first three years of the  BioASQ challenge.", "labels": [], "entities": [{"text": "BIOSMILE", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9888737201690674}]}, {"text": " Table 6: Coverage of the questions (in %) for which a  PAS match between the question and any of the related  snippets was found. A PAS match was counted, if  the predicate stem and any of the related argument  types matched. * BIOSMILE was only evaluated on  data from the first three years of the BioASQ challenge.", "labels": [], "entities": [{"text": "PAS", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9782668352127075}, {"text": "PAS", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.982894778251648}, {"text": "BIOSMILE", "start_pos": 229, "end_pos": 237, "type": "METRIC", "confidence": 0.976252555847168}]}, {"text": " Table 7: Evaluation of the accuracy (in %) for yes/no  questions on the 5 batches of the fourth BioASQ chal- lenge.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9995706677436829}, {"text": "BioASQ chal- lenge", "start_pos": 97, "end_pos": 115, "type": "DATASET", "confidence": 0.840646967291832}]}, {"text": " Table 8: Evaluation of the mean reciprocal rank (in  %) for factoid questions on the 5 batches of the fourth  BioASQ challenge.", "labels": [], "entities": [{"text": "mean reciprocal rank", "start_pos": 28, "end_pos": 48, "type": "METRIC", "confidence": 0.7026938796043396}, {"text": "BioASQ challenge", "start_pos": 111, "end_pos": 127, "type": "TASK", "confidence": 0.5178594440221786}]}, {"text": " Table 9: Evaluation of the mean average F-measure  (in %) for list questions on the 5 batches of the fourth  BioASQ challenge.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9500585794448853}, {"text": "BioASQ challenge", "start_pos": 110, "end_pos": 126, "type": "TASK", "confidence": 0.47839275002479553}]}]}