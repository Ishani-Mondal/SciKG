{"title": [{"text": "Generating High-Quality Surface Realizations Using Data Augmentation and Factored Sequence Models", "labels": [], "entities": [{"text": "Generating High-Quality Surface Realizations", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.6179229095578194}]}], "abstractContent": [{"text": "This work presents state of the art results in reconstruction of surface realiza-tions from obfuscated text.", "labels": [], "entities": []}, {"text": "We identify the lack of sufficient training data as the major obstacle to training high-performing models, and solve this issue by generating large amounts of synthetic training data.", "labels": [], "entities": []}, {"text": "We also propose preprocessing techniques which make the structure contained in the input features more accessible to sequence models.", "labels": [], "entities": []}, {"text": "Our models were ranked first on all evaluation metrics in the En-glish portion of the 2018 Surface Realization shared task.", "labels": [], "entities": [{"text": "Surface Realization shared task", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.8109768033027649}]}], "introductionContent": [{"text": "Contextualized Natural Language Generation (NLG) is a long-standing goal of Natural Language Processing (NLP) research.", "labels": [], "entities": [{"text": "Contextualized Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7500841617584229}]}, {"text": "The task of generating text, conditioned on knowledge about the world, is applicable to almost any domain.", "labels": [], "entities": []}, {"text": "However, despite recent advances on some tasks, NLG models still produce relatively low quality outputs in many settings.", "labels": [], "entities": []}, {"text": "Representing the context in a consistent manner is still a challenge: how can we condition output on a stateful structure such as a graph or a tree?", "labels": [], "entities": []}, {"text": "Several shared tasks have recently explored NLG from inputs with graph-like structures; RDF triples (, dialogue act-based meaning representations ( and abstract meaning representations.", "labels": [], "entities": []}, {"text": "In each of these challenges, the input has structure beyond simple linear sequences; however, to date, the top results in these tasks have consistently been achieved using relatively standard sequence-to-sequence models.", "labels": [], "entities": []}, {"text": "The surface realization task () is a conceptually simple challenge: given shuffled input, where tokens are represented by their lemmas, parts of speech, and dependency features, can we train a model to reconstruct the original text?", "labels": [], "entities": [{"text": "surface realization task", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8167220155398051}]}, {"text": "A model that performs well at this task is likely to be a good starting point for solving more complex tasks, such as NLG from Resource Description Framework (RDF) graphs or Abstract Meaning Representation (AMR) structures.", "labels": [], "entities": []}, {"text": "In addition, training data for the surface realization task can also be generated in a fully-automated manner.", "labels": [], "entities": [{"text": "surface realization task", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.7887657781442007}]}, {"text": "In this work, we show that training dataset size maybe the major obstacle preventing current sequence-to-sequence models from doing well at NLG from structured inputs.", "labels": [], "entities": []}, {"text": "Although inputting the structures themselves is theoretically appealing (, for some tasks it maybe enough to use sequential inputs by flattening structures, and providing structural information via input factors, as long as the training dataset is sufficiently large.", "labels": [], "entities": []}, {"text": "By augmenting training data using a large corpus of unannotated data, we obtain anew state of the art in the surface realization task using off-the-shelf sequence to sequence models.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.7334000468254089}]}, {"text": "In addition, we show that information about the output word order, implicitly available in the universal dependency fields, provides essential information about the word order of correct output sequences, confirming that structural information cannot be discarded without a large drop in performance.", "labels": [], "entities": []}, {"text": "The main contributions of this work are: 1.", "labels": [], "entities": []}, {"text": "We show how training datasets can be augmented with synthetic data 2.", "labels": [], "entities": []}, {"text": "We apply preprocessing steps to simplify the universal dependency structures, making the structure more explicit 3.", "labels": [], "entities": []}, {"text": "We evaluate copy attention models for the surface realization task", "labels": [], "entities": [{"text": "copy attention", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.861220121383667}, {"text": "surface realization", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7642699778079987}]}], "datasetContent": [{"text": "To augment the SR training data, we used sentences from the WikiText corpus (.", "labels": [], "entities": [{"text": "SR training", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.8416818380355835}, {"text": "WikiText corpus", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.9650300741195679}]}, {"text": "Each of these sentences was parsed using UDPipe ( to obtain the same features provided by the SR organizers.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.7573655247688293}]}, {"text": "We then filtered this data, keeping only sentences with at least 95% vocabulary overlap with the in-domain SR training data.", "labels": [], "entities": [{"text": "SR training data", "start_pos": 107, "end_pos": 123, "type": "DATASET", "confidence": 0.7648773590723673}]}, {"text": "Note that the input vocabulary for this task is word lemmas, so at least 95% of the tokens in each instance in our additional training data are lemmas which are also found in the in-domain data.", "labels": [], "entities": []}, {"text": "The order of tokens in each instance of this additional dataset is then randomly shuffled to simulate the random input order in the SR data.", "labels": [], "entities": []}, {"text": "We thus obtain 642,960 additional training instances, which are added to the 12,375 instances supplied by the SR shared task organizers.", "labels": [], "entities": []}, {"text": "We experiment with many different combinations of input features and training data, in order to understand which elements of the representation have the largest impact upon performance.", "labels": [], "entities": []}, {"text": "We limit vocabulary size during training to enable the network to generalize to unknown tokens attest time.", "labels": [], "entities": []}, {"text": "When using just the SR training data we train word embeddings for the 15,000 most frequent tokens from a possible 23,650 unique tokens.", "labels": [], "entities": [{"text": "SR training data", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.7075673540433248}]}, {"text": "When using the combined SR training data and filtered WikiText dataset we use the 30,000 most frequent tokens from a possible 106,367 unique tokens.", "labels": [], "entities": [{"text": "SR training data", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.6592883268992106}, {"text": "WikiText dataset", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.9364019930362701}]}, {"text": "We trained on a single Tesla K40 GPU.", "labels": [], "entities": []}, {"text": "Training time was approximately 1 minute per epoch for the SR data and 1 hour per epoch for the combined SR data and filtered WikiText.", "labels": [], "entities": [{"text": "SR data", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.8115715980529785}, {"text": "SR data", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.8099494278430939}, {"text": "WikiText", "start_pos": 126, "end_pos": 134, "type": "DATASET", "confidence": 0.8288723230361938}]}], "tableCaptions": [{"text": " Table 1: The features used in the factored models, along with the number of possible values the feature  may take, and the respective embedding size.", "labels": [], "entities": []}, {"text": " Table 2: An example from the training data, containing all features we use as input factors.", "labels": [], "entities": []}, {"text": " Table 3: Ablation study with BLEU scores for dif- ferent configurations on the shallow task develop- ment set", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9657147526741028}, {"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9994179010391235}]}, {"text": " Table 4: Official results of the surface realization  shared task using BLEU, DIST and NIST as eval- uation metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9939149022102356}, {"text": "NIST", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.8769314885139465}]}]}