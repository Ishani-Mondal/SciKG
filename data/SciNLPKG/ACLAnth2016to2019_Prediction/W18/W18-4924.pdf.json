{"title": [{"text": "Improving Domain Independent Question Parsing with Synthetic Treebanks", "labels": [], "entities": [{"text": "Improving Domain Independent Question Parsing", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.9310256361961364}]}], "abstractContent": [{"text": "Automatic syntactic parsing for question constructions is a challenging task due to the paucity of training examples inmost treebanks.", "labels": [], "entities": [{"text": "syntactic parsing for question constructions", "start_pos": 10, "end_pos": 54, "type": "TASK", "confidence": 0.6419052183628082}]}, {"text": "The near absence of question constructions is due to the dominance of the news domain in treebanking efforts.", "labels": [], "entities": []}, {"text": "In this paper, we compare two synthetic low-cost question treebank creation methods with a conventional manual high-cost annotation method in the context of three domains (news questions, political talk shows, and chatbots) for Modern Standard Arabic, a language with relatively low resources and rich morphology.", "labels": [], "entities": []}, {"text": "Our results show that synthetic methods can be effective at significantly reducing parsing errors fora target domain without having to invest large resources on manual annotation; and the combination of manual and synthetic methods is our best domain-independent performer.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic syntactic parsing for questions is a challenging task since most treebanks are built from news domain articles that contain few questions).", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7228327691555023}]}, {"text": "Consequently, parsers can have difficulty with parsing questions.", "labels": [], "entities": [{"text": "parsing questions", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.9136412739753723}]}, {"text": "This is a problem, especially in low resource languages like Arabic where the main gold standard treebank, the Penn Arabic TreeBank (PATB) (), contains only 428 questions out of 12k annotated sentences (PATB Part 3).", "labels": [], "entities": [{"text": "Penn Arabic TreeBank (PATB)", "start_pos": 111, "end_pos": 138, "type": "DATASET", "confidence": 0.9518610636393229}, {"text": "PATB Part 3", "start_pos": 203, "end_pos": 214, "type": "DATASET", "confidence": 0.7963685592015585}]}, {"text": "In addition to the issue of sparse question data, parsing accuracy is further affected by the difference between the training data domain and the test data domain.", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.9842981100082397}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9612624049186707}]}, {"text": "The cost of manually building and labeling a treebank can be prohibitive; so different methods to maximize the impact of available human resources are utilized.", "labels": [], "entities": []}, {"text": "One of these methods is model adaptation through using the output of a model trained on one domain (e.g., news) to annotate data from a different target domain (e.g., science fiction) (.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7321606129407883}]}, {"text": "The automatically annotated data is then used to train anew model that has improved accuracy in the target domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9985951781272888}]}, {"text": "There are published efforts that deal specifically with adapting models for question parsing.", "labels": [], "entities": [{"text": "question parsing", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.8524218797683716}]}, {"text": "In this paper, we evaluate the efficacy of two low-cost synthetic treebank generation methods at improving the parsing accuracy of questions for Arabic in a number of domains.", "labels": [], "entities": [{"text": "parsing", "start_pos": 111, "end_pos": 118, "type": "TASK", "confidence": 0.959274411201477}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.8470860123634338}]}, {"text": "One of the methods we use relies on existing treebanks and uses phrase structure transformations to create questions from statements.", "labels": [], "entities": []}, {"text": "The second method elicits generic unlexicalized templates from human users of a specific application.", "labels": [], "entities": []}, {"text": "Another contribution of this paper is the creation of a manually annotated treebank of 988 questions (5.3k words) in two question-heavy domains to allow us to evaluate the three methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "For all evaluations, the Stanford parser (Version 3.8.0) was used to train models for various combinations of train sets which were evaluated against all test sets.", "labels": [], "entities": []}, {"text": "The parser's command-line maxLength argument was set to 30 so as to exclude longer sentences.", "labels": [], "entities": []}, {"text": "All other configuration variables were left to default.", "labels": [], "entities": []}, {"text": "Given the complexity of evaluating both segmentation and syntactic parsing together (, we used the gold segmentation but the POS tags were predicted.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.6719909310340881}]}, {"text": "shows the distribution of treebanks between train and test sets, which are explained in the next subsections.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Number of annotated templates and values.", "labels": [], "entities": []}, {"text": " Table 5. The Average Q row is a macro average of the three question test sets (PATBQ,  TalkShow, and Chatbot).", "labels": [], "entities": [{"text": "Average Q row", "start_pos": 14, "end_pos": 27, "type": "METRIC", "confidence": 0.9183525443077087}, {"text": "PATBQ", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.750307023525238}, {"text": "TalkShow", "start_pos": 88, "end_pos": 96, "type": "DATASET", "confidence": 0.8684177398681641}]}, {"text": " Table 5: F-scores for Baseline to All models. C1-C6 and CQ represent the different combination of  training sets which are shown in the second column.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9950156807899475}]}]}