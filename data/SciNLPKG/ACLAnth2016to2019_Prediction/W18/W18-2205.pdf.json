{"title": [{"text": "Semi-Supervised Neural Machine Translation with Language Models", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.654161254564921}]}], "abstractContent": [{"text": "Training neural machine translation models is notoriously slow and requires abundant parallel corpora and computational power.", "labels": [], "entities": [{"text": "Training neural machine translation models", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7451853632926941}]}, {"text": "In this work we propose an approach of transferring knowledge from separately trained language models to translation systems, also we investigate several techniques to improve translation quality when there is alack of parallel data and computational resources.", "labels": [], "entities": []}, {"text": "Our method is based on fusion between translation system and language model, and initialization of translation system with weights from pretrained language models.", "labels": [], "entities": []}, {"text": "We show that this technique gives +2.2 BLEU score on En-Fr pair of WMT europarl-7 dataset and allows us to reach 20.7 BLEU on 20k parallel sentences in less than 8 hours of training on a single NVIDIA GeForce GTX 1080 Ti.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9988852143287659}, {"text": "WMT europarl-7 dataset", "start_pos": 67, "end_pos": 89, "type": "DATASET", "confidence": 0.9169382254282633}, {"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9954973459243774}, {"text": "NVIDIA GeForce GTX 1080 Ti", "start_pos": 194, "end_pos": 220, "type": "DATASET", "confidence": 0.7277852952480316}]}, {"text": "We specifically note, that for this advance we use nothing but monolingual corpora for source and target languages.", "labels": [], "entities": []}, {"text": "Significant part of presented results was obtained during DeepHack.Babel hackathon on low-resource machine translation organized by iPavlov Lab.", "labels": [], "entities": [{"text": "DeepHack.Babel hackathon", "start_pos": 58, "end_pos": 82, "type": "DATASET", "confidence": 0.9028180241584778}, {"text": "machine translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.6625072360038757}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) is now a state-of-the-art approach for building translation systems.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8024255931377411}]}, {"text": "The reason behind this is both the invention of new techniques () and availability of massive amounts of training data.", "labels": [], "entities": []}, {"text": "Despite of abundance of parallel datasets for some popular language pairs we still lack such learning opportunities for many others.", "labels": [], "entities": []}, {"text": "That's why there is a growing interest in techniques which allow us to train translation systems in semi-supervised or completely unsupervised fashion.", "labels": [], "entities": []}, {"text": "Several notable approaches () in this direction were provided in recent years, but supervised techniques still highly outperform unsupervised ones.", "labels": [], "entities": []}, {"text": "In our work we investigate a semi-supervised approach of combining translation model with language models in two ways.", "labels": [], "entities": [{"text": "translation model", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.9115690886974335}]}, {"text": "First, we propose an approach of initializing a translation model with language models and show how it can be used to advance learning of the whole translation system.", "labels": [], "entities": [{"text": "initializing a translation model", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.7560076862573624}]}, {"text": "Second, we revise the technique of shallow fusion by by using two separate gates to combine predictions of the translation model with predictions of the language model.", "labels": [], "entities": [{"text": "shallow fusion", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.7655091285705566}]}, {"text": "Using both of these techniques we were able to obtain +2.2 BLEU on WMT europarl-7 data for En-Fr pair in comparison to the strong baseline model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9988430738449097}, {"text": "WMT europarl-7 data", "start_pos": 67, "end_pos": 86, "type": "DATASET", "confidence": 0.9281745354334513}]}, {"text": "Most of the current work was preformed during the DeepHack.Babel hackathon 1 on unsupervised machine translation, organized by iPavlov Lab 2 . Participants were given the task to build semi-supervised translation system.", "labels": [], "entities": [{"text": "DeepHack.Babel hackathon 1", "start_pos": 50, "end_pos": 76, "type": "DATASET", "confidence": 0.9100152850151062}, {"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.6984711289405823}]}, {"text": "Dataset consisted of 1M monolingual sentences for each of two languages (source and target) and 50k parallel sentences.", "labels": [], "entities": []}, {"text": "The hackathon had an unusual format: instead of Kaggle-like submissions, where participants are given both training and test data and are asked to send predictions for the test set, we had to pack our models into Docker 3 containers and send it to the submission server, which did all the training and testing on its own side.", "labels": [], "entities": []}, {"text": "The purpose of this was to prevent participants from deviating from the hackathon rules by using additional parallel datasets or incorporating some language-dependent techniques.", "labels": [], "entities": []}, {"text": "As the datasets used during the hackathon are private and not available for public use, we rerun all our experiments on a dataset, specially generated from WMT'14 En-Fr data (see section 4), and all the reported results are obtained from it.", "labels": [], "entities": [{"text": "WMT'14 En-Fr data", "start_pos": 156, "end_pos": 173, "type": "DATASET", "confidence": 0.8971400856971741}]}, {"text": "In such away, we manage to build a system which learns to translate from English to French in less than 8 hours of training on NVIDIA GeForce GTX 1080 Ti by using only 20k parallel sentences and 300k monolingual corpora.", "labels": [], "entities": [{"text": "translate from English to French", "start_pos": 58, "end_pos": 90, "type": "TASK", "confidence": 0.8483932971954345}, {"text": "NVIDIA GeForce GTX 1080 Ti", "start_pos": 127, "end_pos": 153, "type": "DATASET", "confidence": 0.8218520164489747}]}, {"text": "Our system obtains 20.7 BLEU score, showing improvement of 2.2 BLEU in comparison to the strong baseline model, which does not use monolingual corpora.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9994214773178101}, {"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9989235997200012}]}], "datasetContent": [{"text": "To introduce repeatable results, we reproduced all experiments on famous WMT europarl-v7 for En-Fr pair.", "labels": [], "entities": [{"text": "WMT europarl-v7", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.9148094058036804}]}, {"text": "This corpus consists of almost 2M parallel sentences, so we were able to extract parallel data, test set, and monolingual corporas for language models from it.", "labels": [], "entities": []}, {"text": "Important thing here is that we choose monolingual corporas from different parts of original parallel corpora, so they do not contain similar information.", "labels": [], "entities": []}, {"text": "Also we made experiments with different sizes of parallel corpora.", "labels": [], "entities": []}, {"text": "We name datasets according to the size of the parallel corpus and language pair, see table 1.", "labels": [], "entities": []}, {"text": "At the same time we used private dataset from DeepHack hackathon, which consists of descriptions of hotels in English and Russian as parallel corporas and comments about hotels as monolingual ones.", "labels": [], "entities": [{"text": "DeepHack hackathon", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.9410437643527985}]}, {"text": "The size of the parallel corpora for this set was originally 50k, and monolingual corporas were 1M each, but we reduced them to be the same size as En-Fr datasets.", "labels": [], "entities": []}, {"text": "We use validation set to prevent overfitting and reduce training time.", "labels": [], "entities": []}, {"text": "Training process stops when the BLEU score on validation set does not improve for five epochs.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9774292707443237}]}, {"text": "The model with best validation BLEU at that moment is a final one.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.993023157119751}]}, {"text": "We report final result on 5k parallel sentences used as test set.", "labels": [], "entities": []}, {"text": "We start from evaluating four main models on En-Fr-20k and En-Ru-20k datasets.", "labels": [], "entities": [{"text": "En-Fr-20k", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.9435193538665771}, {"text": "En-Ru-20k datasets", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.961608350276947}]}, {"text": "The training progress for En-Fr pair is shown on.", "labels": [], "entities": [{"text": "En-Fr pair", "start_pos": 26, "end_pos": 36, "type": "DATASET", "confidence": 0.7481141686439514}]}, {"text": "The final results for both pairs on a test set are listed in table 2.", "labels": [], "entities": []}, {"text": "We can notice, that models with initialization perform reasonably better than models without it.", "labels": [], "entities": []}, {"text": "Also we see that initialization is essential if we use fusion.", "labels": [], "entities": [{"text": "initialization", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.9541234970092773}]}, {"text": "All training process for 20k data fits in 5 hours on GTX 1080 Ti, and a huge part of this time is used by validation after each epoch.: Results on 20k datasets  During this series of experiments we see again that proposed approach improves BLEU score.", "labels": [], "entities": [{"text": "GTX 1080 Ti", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.9620997111002604}, {"text": "BLEU score", "start_pos": 240, "end_pos": 250, "type": "METRIC", "confidence": 0.9751814603805542}]}, {"text": "Also it is worth to mention that for fused model this initialization is essential to achieve any reasonable score.", "labels": [], "entities": []}, {"text": "Finally it is important to notice, that due to extremely small parallel corpora models tend to do a lot of factual mistakes in translations, which is shown well by BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9990184307098389}]}], "tableCaptions": [{"text": " Table 2: Results on 20k datasets", "labels": [], "entities": [{"text": "20k datasets", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.6903370320796967}]}, {"text": " Table 3: Results on 10k datasets", "labels": [], "entities": [{"text": "10k datasets", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.7287681698799133}]}]}