{"title": [{"text": "Adversarial Learning of Task-Oriented Neural Dialog Models", "labels": [], "entities": [{"text": "Adversarial Learning of Task-Oriented Neural Dialog", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.6225002408027649}]}], "abstractContent": [{"text": "In this work, we propose an adversarial learning method for reward estimation in reinforcement learning (RL) based task-oriented dialog models.", "labels": [], "entities": [{"text": "reward estimation", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7025928646326065}, {"text": "reinforcement learning (RL) based task-oriented dialog", "start_pos": 81, "end_pos": 135, "type": "TASK", "confidence": 0.73707614839077}]}, {"text": "Most of the current RL based task-oriented dialog systems require the access to a reward signal from either user feedback or user ratings.", "labels": [], "entities": [{"text": "RL based task-oriented dialog", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.8861412107944489}]}, {"text": "Such user ratings, however, may not always be consistent or available in practice.", "labels": [], "entities": []}, {"text": "Furthermore, online dialog policy learning with RL typically requires a large number of queries to users, suffering from sample efficiency problem.", "labels": [], "entities": [{"text": "online dialog policy learning", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.6888329908251762}, {"text": "RL", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.7503042221069336}]}, {"text": "To address these challenges , we propose an adversarial learning method to learn dialog rewards directly from dialog samples.", "labels": [], "entities": []}, {"text": "Such rewards are further used to optimize the dialog policy with policy gradient based RL.", "labels": [], "entities": []}, {"text": "In the evaluation in a restaurant search domain, we show that the proposed adversarial dialog learning method achieves advanced dialog success rate comparing to strong baseline methods.", "labels": [], "entities": []}, {"text": "We further discuss the covariate shift problem in online adversarial dialog learning and show how we can address that with partial access to user feedback.", "labels": [], "entities": [{"text": "online adversarial dialog learning", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.6521032750606537}]}], "introductionContent": [{"text": "Task-oriented dialog systems are designed to assist user in completing daily tasks, such as making reservations and providing customer support.", "labels": [], "entities": []}, {"text": "Comparing to chit-chat systems that are usually modeled with single-turn context-response pairs (, taskoriented dialog systems () involve retrieving information from external resources and reasoning over multiple dialog turns.", "labels": [], "entities": []}, {"text": "This makes it especially important fora system to be able to learn interactively from users.", "labels": [], "entities": []}, {"text": "Recent efforts on task-oriented dialog systems focus on learning dialog models from a datadriven approach using human-human or humanmachine conversations.", "labels": [], "entities": []}, {"text": "designed a hybrid supervised and reinforcement learning end-to-end dialog agent.", "labels": [], "entities": []}, {"text": "proposed an RL based model for information access that can learn online via user interactions.", "labels": [], "entities": [{"text": "information access", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.6990294307470322}]}, {"text": "Such systems assume the model has access to a reward signal at the end of a dialog, either in the form of a binary user feedback or a continuous user score.", "labels": [], "entities": []}, {"text": "A challenge with such learning systems is that user feedback maybe inconsistent ( and may not always be available in practice.", "labels": [], "entities": []}, {"text": "Further more, online dialog policy learning with RL usually suffers from sample efficiency issue ( , which requires an agent to make a large number of feedback queries to users.", "labels": [], "entities": [{"text": "online dialog policy learning", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.7017176896333694}]}, {"text": "To reduce the high demand for user feedback in online policy learning, solutions have been proposed to design or to learn a reward function that can be used to generate a reward in approximation to a user feedback.", "labels": [], "entities": [{"text": "online policy learning", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.6492002805074056}]}, {"text": "Designing a good reward function is not easy ( as it typically requires strong domain knowledge.", "labels": [], "entities": []}, {"text": "El proposed a learning based reward function that is trained with task completion transfer learning.", "labels": [], "entities": [{"text": "task completion transfer learning", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.6729993000626564}]}, {"text": "proposed an online active learning method for reward estimation using Gaussian process classification.", "labels": [], "entities": [{"text": "reward estimation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7812816500663757}, {"text": "Gaussian process classification", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.5874020258585612}]}, {"text": "These methods still require annotations of dialog ratings by users, and thus may also suffer from the rating consistency and learning efficiency issues.", "labels": [], "entities": []}, {"text": "To address the above discussed challenges, we investigate the effectiveness of learning dialog rewards directly from dialog samples.", "labels": [], "entities": []}, {"text": "Inspired by the success of adversarial training in computer vi-sion ( and natural language generation (, we propose an adversarial learning method for task-oriented dialog systems.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.7754289905230204}]}, {"text": "We jointly train two models, a generator that interacts with the environment to produce task-oriented dialogs, and a discriminator that marks a dialog sample as being successful or not.", "labels": [], "entities": []}, {"text": "The generator is a neural network based task-oriented dialog agent.", "labels": [], "entities": []}, {"text": "The environment that the dialog agent interacts with is the user.", "labels": [], "entities": []}, {"text": "Quality of a dialog produced by the agent and the user is measured by the likelihood that it fools the discriminator to believe that the dialog is a successful one conducted by a human agent.", "labels": [], "entities": []}, {"text": "We treat dialog agent optimization as a reinforcement learning problem.", "labels": [], "entities": [{"text": "dialog agent optimization", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.7625016768773397}]}, {"text": "The output from the discriminator serves as a reward to the dialog agent, pushing it towards completing a task in away that is indistinguishable from how a human agent completes it.", "labels": [], "entities": []}, {"text": "In this work, we discuss how the adversarial learning reward function compares to designed reward functions in learning a good dialog policy.", "labels": [], "entities": []}, {"text": "Our experimental results in a restaurant search domain show that dialog agents that are optimized with the proposed adversarial learning method achieve advanced task success rate comparing to strong baseline methods.", "labels": [], "entities": []}, {"text": "We discuss the impact of the size of annotated dialog samples to the effectiveness of dialog adversarial learning.", "labels": [], "entities": []}, {"text": "We further discuss the covariate shift issue in interactive adversarial learning and show how we can address that with partial access to user feedback.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use data from the second Dialog State Tracking Challenge (DSTC2) () in the restaurant search domain for our model training and evaluation.", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge (DSTC2)", "start_pos": 28, "end_pos": 67, "type": "TASK", "confidence": 0.7346876348767962}]}, {"text": "We add entity information to each dialog sample in the original DSTC2 dataset.", "labels": [], "entities": [{"text": "DSTC2 dataset", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.9566015899181366}]}, {"text": "This makes entity information apart of the model training process, enabling the agent to handle entities during interactive evaluation with users.", "labels": [], "entities": []}, {"text": "Different from the agent action definition used in DSTC2, actions in our system are produced by concatenating the act for D-steps do", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of DSTC2 dataset.", "labels": [], "entities": [{"text": "DSTC2 dataset", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9611360728740692}]}, {"text": " Table 2: Performance of different discriminator  model design, on prediction accuracy and proba- bilities assigned to successful and failed dialogs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9550295472145081}]}]}