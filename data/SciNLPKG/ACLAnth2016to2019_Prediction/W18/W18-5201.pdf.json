{"title": [{"text": "Argumentative Link Prediction using Residual Networks and Multi-Objective Learning", "labels": [], "entities": [{"text": "Argumentative Link Prediction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7545566360155741}]}], "abstractContent": [{"text": "We explore the use of residual networks for ar-gumentation mining, with an emphasis on link prediction.", "labels": [], "entities": [{"text": "ar-gumentation mining", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.7036487609148026}, {"text": "link prediction", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.7545493543148041}]}, {"text": "The method we propose makes no assumptions on document or argument structure.", "labels": [], "entities": []}, {"text": "We evaluate it on a challenging dataset consisting of user-generated comments collected from an online platform.", "labels": [], "entities": []}, {"text": "Results show that our model outperforms an equivalent deep network and offers results comparable with state-of-the-art methods that rely on domain knowledge.", "labels": [], "entities": []}], "introductionContent": [{"text": "Argumentation mining is a growing sub-area of artificial intelligence and computational linguistics whose aim is to automatically extract arguments from generic textual corpora ().", "labels": [], "entities": [{"text": "Argumentation mining", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8181286752223969}]}, {"text": "The problem is typically broken down into focused sub-problems such as the identification of sentences containing argument components like claims and premises, of the boundaries of argument components within such sentences, and the prediction of the argumentative structure of the document at hand.", "labels": [], "entities": [{"text": "identification of sentences containing argument components like claims and premises", "start_pos": 75, "end_pos": 158, "type": "TASK", "confidence": 0.7642996132373809}]}, {"text": "In spite of significant results achieved in component identification tasks, such as claim/evidence detection (), classification (Eckle-) and boundary detection (, comparatively less progress has been made in the arguably more challenging argument structure prediction task.", "labels": [], "entities": [{"text": "component identification tasks", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.7884261608123779}, {"text": "claim/evidence detection", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.5828272998332977}, {"text": "boundary detection", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.716694712638855}, {"text": "argument structure prediction task", "start_pos": 238, "end_pos": 272, "type": "TASK", "confidence": 0.7982856705784798}]}, {"text": "Again due to the challenging nature of the general argumentation mining problem, solutions have typically addressed a specific genre or application domain, such as legal texts), persuasive essays (, or Wikipedia articles () and have heavily relied on domain knowledge.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.847103476524353}]}, {"text": "One particular aspect of the domain is the argument model.", "labels": [], "entities": []}, {"text": "While argumentation as a discipline has developed rather sophisticated argument models, such as, the majority of the available argumentation mining data sets refer to ad-hoc, usually simpler argument models, often in an effort to obtain a reasonable inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Another crucial aspect is the document structure.", "labels": [], "entities": []}, {"text": "For instance, in some domains, certain argument components occupy a specific position in the document.", "labels": [], "entities": []}, {"text": "Moreover, until recently, approaches have mostly used traditional methods such as support vector machines, logistic regression and naive Bayes classifiers.", "labels": [], "entities": []}, {"text": "Only in the last couple of years the field has started to look more systematically into neural network-based architectures, such as long short-memory networks and convolutional neural networks, and structured output classifiers.", "labels": [], "entities": []}, {"text": "The aim of this study is to investigate the application of residual networks-a deep neural network architecture not previously applied to this domainto a challenging structure prediction task, namely link prediction.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 200, "end_pos": 215, "type": "TASK", "confidence": 0.8138561844825745}]}, {"text": "Our ambition is to define a model that does not exploit domain-specific, highly engineered features, or information on the underlying argument model, and could thus be, at least in principle, of general applicability.", "labels": [], "entities": []}, {"text": "Our results match those of state-of-the-art methods that rely on domain knowledge, but use much less a-priori information.", "labels": [], "entities": []}, {"text": "The next section reviews recent applications of neural networks to argumentation mining.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.9360851645469666}]}, {"text": "Section 3 presents our model, Section 4 the benchmark, and Section 5 discusses results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our model against the Cornell eRulemaking Corpus (CDCP) (.", "labels": [], "entities": [{"text": "Cornell eRulemaking Corpus (CDCP)", "start_pos": 35, "end_pos": 68, "type": "DATASET", "confidence": 0.9493694206078848}]}, {"text": "This consists of 731 user comments from a eRulemaking website, fora total of about 4,700 propositions, all considered to be argumentative.", "labels": [], "entities": []}, {"text": "The argument model adopted is the one proposed by, where links are constrained to form directed graphs.", "labels": [], "entities": []}, {"text": "Propositions are divided into 5 classes: POLICY (17%), VALUE (45%), FACT (16%), TESTIMONY (21%) and REFERENCE (1%).", "labels": [], "entities": [{"text": "POLICY", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9928186535835266}, {"text": "VALUE", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9951598048210144}, {"text": "FACT", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9377120733261108}, {"text": "TESTIMONY", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9889660477638245}, {"text": "REFERENCE", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9963610768318176}]}, {"text": "Links are divided between REASON (97%) and EVIDENCE (3%).", "labels": [], "entities": [{"text": "REASON", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9104539752006531}, {"text": "EVIDENCE", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.7674945592880249}]}, {"text": "shows an annotated document from the CDCP corpus.", "labels": [], "entities": [{"text": "CDCP corpus", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9446781277656555}]}, {"text": "Link prediction is a particularly difficult task in the CDCP dataset, where only 3% of all the possible proposition pairs (more than 43,000) are linked.", "labels": [], "entities": [{"text": "Link prediction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9258692860603333}, {"text": "CDCP dataset", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9687234461307526}]}, {"text": "A preliminary analysis of the data suggests that the number of propositions separating source and target (distance) could be a relevant feature, since most linked propositions are not far from each other.", "labels": [], "entities": []}, {"text": "Indeed, as shows, around 70% of links are between adjacent propositions.", "labels": [], "entities": []}, {"text": "We tokenized documents using a hand-crafted parser based on the progressive splitting of the tokens and search within the GloVe vocabulary.", "labels": [], "entities": [{"text": "GloVe vocabulary", "start_pos": 122, "end_pos": 138, "type": "DATASET", "confidence": 0.8929892480373383}]}, {"text": "We preferred not to use existing tools because of the nature of the data, since the CDCP documents often do not follow proper writing conventions (such as the blank space after the period mark), leading in some cases to a wrong tokenization.", "labels": [], "entities": []}, {"text": "As a result, the number of tokens not contained in the GloVe dictionary dramatically reduced from 384, originally obtained with the software provided by, to 84.", "labels": [], "entities": [{"text": "GloVe dictionary", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.9380293190479279}]}, {"text": "Each of these tokens was mapped into a randomly-generated embedding.", "labels": [], "entities": []}, {"text": "We created a validation set by randomly selecting documents from the original training split with 10% probability.", "labels": [], "entities": []}, {"text": "We used the remaining documents as training data and the original test split as is. reports the statistics related to the three splits.", "labels": [], "entities": []}, {"text": "We defined the learning problem as a multiobjective optimization problem, whose loss function is given by the weighted sum of four different components: the categorical cross-entropy on three labels (source and target categories, link relation category) and an L 2 regularization on the network parameters.", "labels": [], "entities": []}, {"text": "The weights of these components were, respectively, 1, 1, 10, 10 \u22124 . We performed mini-batch optimization using Adam () with parameters b 1 = 0.9 and b 2 = 0.9999, and by applying proportional decay of the initial learning rate \u03b1 0 = 5 \u00d7 10 \u22123 . Training was early-stopped after 200 epochs with no improvements on the validation data.", "labels": [], "entities": []}, {"text": "We chose the numerous hyper-parameters of the architecture and of the learning model after an initial experimental setup phase, based on the performance on the validation set for the link prediction task.", "labels": [], "entities": [{"text": "link prediction task", "start_pos": 183, "end_pos": 203, "type": "TASK", "confidence": 0.8141037225723267}]}, {"text": "Results obtained in this phase confirmed that the presence of the deep embedder block and of the distance feature lead to better results.", "labels": [], "entities": []}, {"text": "We compared the results of the residual network model against an equivalent deep network with the same number of layers and the same hyperparameters, but without the shortcut that characterize the residual network block.", "labels": [], "entities": []}, {"text": "We applied two different training procedures for both this deep network baseline and the residual network.", "labels": [], "entities": []}, {"text": "In particular, as the criterion for early stopping we used once the error on link prediction and once the error on proposition classification.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.6799545139074326}, {"text": "link prediction", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.7159662991762161}, {"text": "proposition classification", "start_pos": 115, "end_pos": 141, "type": "TASK", "confidence": 0.8796970546245575}]}, {"text": "In the presentation of our results we will refer to these two models as link-guided (LG) and proposition-guided (PG).", "labels": [], "entities": []}, {"text": "Following (, we measured the performance of the models by computing the F 1 score for links, propositions, and the average between the two, in order to provide a summary evaluation.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9848817388216654}]}, {"text": "More specifically, for the links we measured the F 1 of the positive classes (as the harmonic mean between precision and recall), whereas for the propositions we used the score of each class and then we computed the macroaverage.", "labels": [], "entities": [{"text": "F 1", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9917423725128174}, {"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9977961778640747}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9786432385444641}]}, {"text": "We also reported the F 1 score for each direct relation class, alongside with their macroaverage.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9894988934199015}]}, {"text": "Since each proposition is involved in many pairs, both as a source and as a target, its classification is performed multiple times.", "labels": [], "entities": []}, {"text": "To classify it uniquely, we considered the average probability score assigned to each class and we have assigned the most probable class.", "labels": [], "entities": []}, {"text": "That is of course not the only option.", "labels": [], "entities": []}, {"text": "Another possibility could be to assign the class that results to be the most probable inmost of the cases, thus relying on a majority vote.", "labels": [], "entities": []}, {"text": "A further option could be to simply consider the label with highest confidence.", "labels": [], "entities": []}, {"text": "However, this procedure might be more sensitive to outliers, because the misclassification of a sentence in just one pair would lead to the misclassification of the sentence, regardless of all the other pairs.", "labels": [], "entities": []}, {"text": "A deeper analysis of different techniques to address this issues is left to future research.", "labels": [], "entities": []}, {"text": "summarizes the evaluation of baselines and residual networks, 4 also showing the best scores obtained by the structured learning configurations presented in (.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental dataset composition.", "labels": [], "entities": []}, {"text": " Table 2: F 1 scores computed on the test set. For each class, the number of instances is reported in parenthesis.  For the comparison with structured learning, the best scores obtained by any of the structured configurations are  reported.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9760861396789551}]}]}