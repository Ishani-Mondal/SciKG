{"title": [{"text": "E2E NLG Challenge Submission: Towards Controllable Generation of Diverse Natural Language", "labels": [], "entities": [{"text": "E2E NLG Challenge", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9309205810228983}]}], "abstractContent": [{"text": "In natural language generation (NLG), the task is to generate utterances from a more abstract input, such as structured data.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.8275211453437805}]}, {"text": "An added challenge is to generate utterances that contain an accurate representation of the input, while reflecting the fluency and variety of human-generated text.", "labels": [], "entities": []}, {"text": "In this paper, we report experiments with NLG models that can be used in task oriented dialogue systems.", "labels": [], "entities": []}, {"text": "We explore the use of additional input to the model to encourage diversity and control of outputs.", "labels": [], "entities": []}, {"text": "While our submission does not rank highly using automated metrics, qualitative investigation of generated utterances suggests the use of additional information in neural network NLG systems to be a promising research direction.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language Generation (NLG) is abroad field, ranging from text-to-text translation to experiments in computational poetry (.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7716236809889475}, {"text": "text-to-text translation", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.7687491476535797}]}, {"text": "Whether the task is to summarize, translate, or entertain, a core challenge is doing so in a manner that is compatible with human needs and preferences.", "labels": [], "entities": [{"text": "summarize, translate", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.8630649050076803}]}, {"text": "Formally, NLG systems aim to create utterances from a set of abstract inputs.", "labels": [], "entities": []}, {"text": "These inputs can be closely aligned, e.g. machine translation (), or require significant abstractive reasoning, as in summarization or datato-text tasks).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7814545035362244}, {"text": "summarization", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.9655643701553345}]}, {"text": "Traditionally NLG systems have followed a rule-based approach).", "labels": [], "entities": []}, {"text": "While robust, these systems are noted to generate repetitive and stilted output, which can Meaning Representation name eatType food priceRange area familyFriendly near additionalWords[looking adults offerings really try good prices situated] Generated utterance If you're looking for an adults only Japanese restaurant, try The Wrestlers.", "labels": [], "entities": [{"text": "Meaning Representation", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.9131052494049072}]}, {"text": "It is really good and situated near Raja Indian Cuisine.", "labels": [], "entities": [{"text": "Raja Indian Cuisine", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.8266260425249735}]}, {"text": "The prices are more than \u00a330.: Utterance generated with a novel dialogue act containing additional words make interacting with rule based systems a tedious experience.", "labels": [], "entities": []}, {"text": "Data driven models using deep neural networks have achieved state-of-the-art results in many NLG tasks/datasets such as RoboCup, Weathergov, SF Hotels/Restaurants and AMR-to-text (.", "labels": [], "entities": [{"text": "Weathergov", "start_pos": 129, "end_pos": 139, "type": "DATASET", "confidence": 0.9201686382293701}]}, {"text": "However notes that high performance on datasets such as's SF Restaurant indicates they no longer pose a sufficient challenge and that the community ought to progress to using larger and more complex datasets.", "labels": [], "entities": [{"text": "'s SF Restaurant", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.696635882059733}]}, {"text": "Two new crowd sourced datasets, each containing tens of thousands of examples and focusing on complex sentence structures, have been recently released;.", "labels": [], "entities": []}, {"text": "This paper focuses on the E2E dataset which was created using anew methodology to maximize both the quality of collected utterances as well as their naturalness and variety.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9489142894744873}]}, {"text": "note that neural networks learning from highly unaligned datasets have trouble choosing between equally plausible outputs and tend towards short and less meaningful outputs.", "labels": [], "entities": []}, {"text": "They suggest that the number of plausible outputs can be decreased by providing additional information to the model.", "labels": [], "entities": []}, {"text": "In we augment the meaning representation (MR) with a novel dialogue act (DA) containing additional words to be included in the generated utterance.", "labels": [], "entities": []}, {"text": "By conditioning the output on these words the model has managed to generate an utterance with a complex sentence structure and wide vocabulary.", "labels": [], "entities": []}, {"text": "Our contribution is to propose a pipeline system.", "labels": [], "entities": []}, {"text": "Additional words are sampled from a secondary model which uses DAs from a given MR as inputs.", "labels": [], "entities": []}, {"text": "These additional words are put into anew DA and added to the existing MR, as shown in.", "labels": [], "entities": [{"text": "DA", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9613725543022156}, {"text": "MR", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9823789000511169}]}, {"text": "The augmented MR is then used as input to a model which generates the final utterance.", "labels": [], "entities": []}, {"text": "The approach of augmenting the source sequence takes inspiration from recent work in paraphrase generation () and generating structured queries from natural language (.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.8810403048992157}]}, {"text": "As noted by delexicalization can often lead to grammatically incorrect sentences.", "labels": [], "entities": []}, {"text": "We opt instead to use a pointer network () which allows the model to copy tokens directly from the source sequence into the generated utterance.", "labels": [], "entities": []}, {"text": "The model does not perform well relative to the baseline and this is possibly due to the failure of the secondary model to generate appropriate additional words.", "labels": [], "entities": []}, {"text": "Improving upon the pipeline system remains an area of active research for us.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data set was tokenized using the NLTK port of the moses tokenizer with aggressive hyphen splitting.", "labels": [], "entities": []}, {"text": "For each DA a custom start and stop token was added to the source sequence; e.g. name start The Vaults name end The models used were from the OpenNMT-py library (.", "labels": [], "entities": []}, {"text": "Our model architecture contains 2 layers of bidirectional recurrent neural networks (RNN) with long short-term memory (LSTM) cells).", "labels": [], "entities": []}, {"text": "We use 500 hidden units for the encoder and decoder layer, and 500 units for the word vectors which are learned jointly across the whole model.", "labels": [], "entities": []}, {"text": "We add dropout of 0.3 applied between the LSTM stacks.", "labels": [], "entities": [{"text": "dropout", "start_pos": 7, "end_pos": 14, "type": "METRIC", "confidence": 0.9663642644882202}]}, {"text": "The models are trained using) with learning rate 0.001 and learning rate decay of 0.5 applied after 8 epochs.", "labels": [], "entities": [{"text": "learning rate 0.001", "start_pos": 35, "end_pos": 54, "type": "METRIC", "confidence": 0.9662018616994222}, {"text": "learning rate decay", "start_pos": 59, "end_pos": 78, "type": "METRIC", "confidence": 0.970190187295278}]}, {"text": "The models were trained for 10 epochs and the best performing checkpoint on the development set was chosen.", "labels": [], "entities": []}, {"text": "The exploration and choice of hyperparameters was aided by the use of Bayesian hyperparameter optimization platform SigOpt (2014).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Dev set results", "labels": [], "entities": []}, {"text": " Table 5: Test set results", "labels": [], "entities": []}, {"text": " Table 6: True skill clusters", "labels": [], "entities": []}]}