{"title": [{"text": "On hapax legomena and morphological productivity", "labels": [], "entities": []}], "abstractContent": [{"text": "Quantifying and predicting morphological productivity is a long-standing challenge in corpus linguistics and psycholinguistics.", "labels": [], "entities": [{"text": "Quantifying and predicting morphological productivity", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.6946070432662964}]}, {"text": "The same challenge reappears in natural language processing in the context of handling words that were not seen in the training set (out-of-vocabulary, or OOV, words).", "labels": [], "entities": []}, {"text": "Prior research showed that a good indicator of the productivity of a morpheme is the number of words involving it that occur exactly once (the ha-pax legomena).", "labels": [], "entities": []}, {"text": "A technical connection was adduced between this result and Good-Turing smoothing, which assigns probability mass to unseen events on the basis of the simplifying assumption that word frequencies are stationary.", "labels": [], "entities": []}, {"text": "Ina large-scale study of 133 affixes in Wikipedia, we develop evidence that success in fact depends on tapping the frequency range in which the assumptions of Good-Turing are violated.", "labels": [], "entities": []}], "introductionContent": [{"text": "The productivity of a morpheme is understood as the extent to which a language uses it actively in novel combinations.", "labels": [], "entities": []}, {"text": "This vexed concept has multiple interpretations, of which two will concern us here.", "labels": [], "entities": []}, {"text": "One views productivity as the cognitive propensity to create anew word involving a morpheme.", "labels": [], "entities": []}, {"text": "The other infers productivity from the likelihood that new word types with a morpheme will be found when a corpus is expanded.", "labels": [], "entities": []}, {"text": "The two can differ because the likelihood of finding a word depends not only on its creation, but also on the extent to which the word is learned and reused by others, and ultimately noted by an observer.", "labels": [], "entities": []}, {"text": "One might suppose that a morpheme found in many different combinations would be more flexible in entering into novel ones, as in the rationale for Witten-Bell smoothing,); if so, the type count of the morpheme would be a good index of productivity.", "labels": [], "entities": [{"text": "Witten-Bell smoothing", "start_pos": 147, "end_pos": 168, "type": "TASK", "confidence": 0.6108982861042023}]}, {"text": "However, the type count correlates poorly with human intuitions about productivity and with the number of OOV words found in test sets).", "labels": [], "entities": []}, {"text": "Working with corpora that are small by current standards, corpus linguists in the 1990s observed that the number of hapax legomena (or hapaxes) that contain a given morpheme is a much better predictor (.", "labels": [], "entities": []}, {"text": "This finding is argued to follow from assumptions about the cognitive system that make Good-Turing smoothing applicable, which we explain in the following section.", "labels": [], "entities": []}, {"text": "This paper systematically explores hapax counts as an indicator of productivity fora set of 133 morphemes that meet objective inclusion criteria fora much larger corpus than was used previously.", "labels": [], "entities": []}, {"text": "This is the August 2013 download of Wikipedia that has 1.24 billion word tokens.", "labels": [], "entities": []}, {"text": "We address several questions: Is the measure successful when exercised at a larger scale?", "labels": [], "entities": []}, {"text": "Are the simplifying assumptions put forward to justify the measure valid?", "labels": [], "entities": []}, {"text": "What does the behaviour of the measure tell us about the lexical system?", "labels": [], "entities": []}, {"text": "We address these questions with numerical experiments.", "labels": [], "entities": []}, {"text": "We define \"pseudohapax\" sets as sets of words in the full corpus that would be expected to occur exactly once in five nominal corpora having sizes used in classic studies.", "labels": [], "entities": []}, {"text": "We explore how well the pseudohapax sets predict the distribution of morphemes amongst extremely rare words.", "labels": [], "entities": []}, {"text": "We also downsample the corpus to create hapax sets from subcorpora matching the nominal corpus sizes.", "labels": [], "entities": []}, {"text": "This approach allows us to separate the influence of several factors: sparse sampling, variation across morphological families in the shape of the rank-frequency distribution, and the actual frequencies of words that appear as hapaxes in corpora of classic size.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Banding scheme for five conditions, ex- pressed as a percentage of the total corpus size. Fre- quency band for the pseudohapaxes (PBand), total  number of pseudohapax types containing any of the  prefixes or suffixes (PTypes), number of down-bands  available before reaching the far tail (DB). Average  number of hapaxes (HTypes).", "labels": [], "entities": [{"text": "Fre- quency band", "start_pos": 100, "end_pos": 116, "type": "METRIC", "confidence": 0.9692564755678177}, {"text": "DB", "start_pos": 299, "end_pos": 301, "type": "METRIC", "confidence": 0.8777145743370056}, {"text": "Average", "start_pos": 304, "end_pos": 311, "type": "METRIC", "confidence": 0.9664971828460693}]}, {"text": " Table 3: Summary of regression parameters. For minimum and maximum values, the indicated affix is the one  that was held out.", "labels": [], "entities": []}]}