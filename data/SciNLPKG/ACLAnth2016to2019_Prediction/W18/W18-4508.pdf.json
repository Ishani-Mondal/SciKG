{"title": [{"text": "One Size Fits All? A simple LSTM for Non-literal Token-and Construction-level Classification", "labels": [], "entities": [{"text": "Token-and Construction-level Classification", "start_pos": 49, "end_pos": 92, "type": "TASK", "confidence": 0.5576801200707754}]}], "abstractContent": [{"text": "We tackle four different tasks of non-literal language classification: token and construction level metaphor detection, classification of idiomatic use of infinitive-verb compounds, and classification of non-literal particle verbs.", "labels": [], "entities": [{"text": "non-literal language classification", "start_pos": 34, "end_pos": 69, "type": "TASK", "confidence": 0.6582803428173065}, {"text": "token and construction level metaphor detection", "start_pos": 71, "end_pos": 118, "type": "TASK", "confidence": 0.6747366189956665}, {"text": "classification of idiomatic use of infinitive-verb compounds", "start_pos": 120, "end_pos": 180, "type": "TASK", "confidence": 0.816953352519444}, {"text": "classification of non-literal particle verbs", "start_pos": 186, "end_pos": 230, "type": "TASK", "confidence": 0.8007747769355774}]}, {"text": "One of the tasks operates on the token level, while the three other tasks classify constructions such as \"hot topic\" or \"stehen lassen\" (to allow sth. to stand vs. to abandon so.).", "labels": [], "entities": []}, {"text": "The two metaphor detection tasks are in English, while the two non-literal language detection tasks are in German.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.8365558087825775}]}, {"text": "We propose a simple context-encoding LSTM model and show that it outperforms the state-of-the-art on two tasks.", "labels": [], "entities": []}, {"text": "Additionally, we experiment with different embeddings for the token level metaphor detection task and find that 1) their performance varies according to the genre, and 2) Mikolov et al.", "labels": [], "entities": [{"text": "token level metaphor detection task", "start_pos": 62, "end_pos": 97, "type": "TASK", "confidence": 0.7341255962848663}]}, {"text": "(2013) embeddings perform best on 3 out of 4 genres, despite being one of the simplest tested models.", "labels": [], "entities": []}, {"text": "In summary, we present a large-scale analysis of a neural model for non-literal language classification (i) at different granularities, (ii) in different languages, (iii) over different non-literal language phenomena.", "labels": [], "entities": [{"text": "non-literal language classification", "start_pos": 68, "end_pos": 103, "type": "TASK", "confidence": 0.7319521506627401}]}], "introductionContent": [{"text": "Computational research of non-literal phenomena, e.g., metonymy, idiom, and prominently metaphor detection (, has been plentiful.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7944578528404236}]}, {"text": "For metaphor detection, most works name the Conceptual Metaphor Theory ( as their underlying framework, in which metaphors are modeled as cognitive mappings of concepts from a source to a target domain.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9273691773414612}]}, {"text": "However, the datasets created and used in these works often follow no unified annotation guidelines and), or even no disclosed guidelines at all, e.g.,, or annotate metaphors at different levels of granularity (.", "labels": [], "entities": []}, {"text": "This is also true for many works in more general non-literal language detection.", "labels": [], "entities": [{"text": "non-literal language detection", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.6448169449965159}]}, {"text": "Consequently, methods are seldom compared on related tasks.", "labels": [], "entities": []}, {"text": "Neural networks have been successfully applied to various natural language processing tasks, but few have applied them to metaphor detection) or detection of non-literal and figurative language in general.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.8127112090587616}]}, {"text": "In this paper, we test whether the same simple generic neural network approach is effective for four different non-literal language detection tasks: token and construction level metaphor detection, idiom classification and classification of literal and non-literal German particle verbs.", "labels": [], "entities": [{"text": "token and construction level metaphor detection", "start_pos": 149, "end_pos": 196, "type": "TASK", "confidence": 0.663233295083046}, {"text": "idiom classification", "start_pos": 198, "end_pos": 218, "type": "TASK", "confidence": 0.6906084269285202}, {"text": "classification of literal and non-literal German particle verbs", "start_pos": 223, "end_pos": 286, "type": "TASK", "confidence": 0.7627227231860161}]}, {"text": "We train a neural model using LSTMs to encode the context of a metaphor candidate or non-literal compound.", "labels": [], "entities": []}, {"text": "We show that our approach outperforms existing state-of-the-art models on two tasks, while producing competitive results on another task, independent of the mode of classification (e.g., token vs. construction classification).", "labels": [], "entities": [{"text": "token vs. construction classification", "start_pos": 187, "end_pos": 224, "type": "TASK", "confidence": 0.5941488072276115}]}, {"text": "In demonstrating the applicability of the same, simple neural network architecture to different non-literal language tasks, we lay the foundation fora more integrative approach.", "labels": [], "entities": []}, {"text": "A joint modeling of these tasks, through data concatenation and multi-task learning, is investigated in Do.", "labels": [], "entities": []}, {"text": "Given enough training data, our model renders many of the handcrafted features employed in previous work unnecessary.", "labels": [], "entities": []}, {"text": "This includes e.g., abstractness values to model source and target concepts, selectional preference violations or topic modeling ().", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 114, "end_pos": 128, "type": "TASK", "confidence": 0.7110321521759033}]}, {"text": "In contrast, because they are the only external resource we utilize, we investigate the influence of an important hyper-parameter of our network-different pre-trained embeddings-on the token-level metaphor detection task and show the genre-specific effects of these embedding models.", "labels": [], "entities": [{"text": "token-level metaphor detection task", "start_pos": 185, "end_pos": 220, "type": "TASK", "confidence": 0.8047740608453751}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Investigated tasks and datasets. Size describes labeled tokens in case of token level metaphor de- tection (content tokens), and labeled constructions for the other tasks respectively, M denotes percentage  of non-literal labels. Non-literal use of tokens/constructions in the examples is marked bold.", "labels": [], "entities": []}, {"text": " Table 4: Classification of idiomatically  used infinitive-verb compounds. Accuracy  values for Horbach et al. (2016) and LSTM  (averaged over 50 configurations).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9992600083351135}, {"text": "LSTM", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.5336645245552063}]}, {"text": " Table 6: System precision (P), recall (R), and F 1 -score for the VUAMC using different embeddings.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 17, "end_pos": 30, "type": "METRIC", "confidence": 0.9095093756914139}, {"text": "recall (R)", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9642076194286346}, {"text": "F 1 -score", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9838560819625854}, {"text": "VUAMC", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.9112659692764282}]}]}