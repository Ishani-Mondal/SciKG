{"title": [{"text": "More or less controlled elicitation of argumentative text: Enlarging a microtext corpus via crowdsourcing", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an extension of an annotated corpus of short argumentative texts that had originally been builtin a controlled text production experiment.", "labels": [], "entities": []}, {"text": "Our extension more than doubles the size of the corpus by means of crowdsourc-ing.", "labels": [], "entities": []}, {"text": "We report on the setup of this experiment and on the consequences that crowdsourcing had for assembling the data, and in particular for annotation.", "labels": [], "entities": []}, {"text": "We labeled the argumentative structure by marking claims, premises, and relations between them, following the scheme used in the original corpus, but had to make a few modifications in response to interesting phenomena in the data.", "labels": [], "entities": []}, {"text": "Finally, we report on an experiment with the automatic prediction of this argumentation structure: We first repli-cated the approach of an earlier study on the original corpus, and compare the performance to various settings involving the extension.", "labels": [], "entities": []}], "introductionContent": [{"text": "As with most areas in NLP, progress on Argumentation Mining hinges on the availability of data, and in the case of this field, this is generally taken to be annotated data.", "labels": [], "entities": [{"text": "Argumentation Mining", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.9174160361289978}]}, {"text": "Up to now, only few corpora labelled with full argumentation structure (i.e., argument components and relations between them) are available; prominent ones are the persuasive essay corpus of, the web text corpus of, and the argumentative microtext corpus of Peldszus and Stede.", "labels": [], "entities": []}, {"text": "The latter is interesting because it has in parallel been annotated with various other linguistic layers, as will be described in Section 2.", "labels": [], "entities": []}, {"text": "The microtexts are relatively \"clean\" text, and the annotation of argumentation structure was generally easy, leading to reasonable annotator agreement, as reported by Peldszus and 1 Many other corpora are available with more lean or more specific annotations; see Section 4 of (.", "labels": [], "entities": []}, {"text": "However, a drawback is the relatively small corpus size: 112 texts of about five argumentative text units on average.", "labels": [], "entities": []}, {"text": "While this data has proven to be useful for various purposes (see Section 2), for machine learning it is clearly desirable to have a larger corpus of this kind.", "labels": [], "entities": []}, {"text": "In this paper, we turn to crowdsourcing as a means to generate more text.", "labels": [], "entities": []}, {"text": "We used essentially the same instructions as used by , and recruited writers via Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 81, "end_pos": 103, "type": "DATASET", "confidence": 0.9220000306765238}]}, {"text": "Naturally, the set of resulting texts is not identical in nature to the original ones, and thus the first contribution of this paper is an analysis of how the different text elicitation scenarios influences the outcome, i.e., to evaluate the pros and cons of crowdsourcing for this type of task.", "labels": [], "entities": []}, {"text": "The second contribution is an evaluation of the annotation scheme that was used for argumentation: Which modifications are necessary in order to produce adequate analyses of the text?", "labels": [], "entities": [{"text": "argumentation", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.9704627394676208}]}, {"text": "Finally, the third contribution is to report on results of an automatic classification experiment: We replicated the Minimum Spanning Tree approach proposed by, and we compare the results that have already been achieved on the original corpus to those stemming from the new sections of the corpus.", "labels": [], "entities": []}, {"text": "We regard this as valuable information on the influence of corpus size on classification results.", "labels": [], "entities": []}, {"text": "In the following, as background we briefly describe the original corpus, and then explain our approach to crowdsourcing the text production task.", "labels": [], "entities": []}, {"text": "This is followed by a description of the annotation phase, and the lessons learned.", "labels": [], "entities": []}, {"text": "Finally, we report on the classification experiment, and then sum up.", "labels": [], "entities": [{"text": "classification", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.9854614734649658}]}, {"text": "The new corpus data, with its annotation of argumentation structure, is available on the website of the arg-microtext corpus (see below).", "labels": [], "entities": [{"text": "arg-microtext corpus", "start_pos": 104, "end_pos": 124, "type": "DATASET", "confidence": 0.7946125864982605}]}, {"text": "), a freely available 2 parallel corpus of 112 short texts with 576 argumentative discourse units (henceforth: segments).", "labels": [], "entities": []}, {"text": "The texts are authentic discussions of controversial issues, which were given to the writers as prompts.", "labels": [], "entities": []}, {"text": "They were originally written in German and have been professionally translated to English, preserving the segmentation and if possible the usage of discourse markers.", "labels": [], "entities": []}, {"text": "The texts have been collected in a controlled text generation experiment, in a classroom setting with students, using a short instruction.", "labels": [], "entities": [{"text": "text generation", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7404878437519073}]}, {"text": "This had the result that all of the texts fulfill the following criteria: (i) The length of each text is about 5 segments; (ii) one segment explicitly states the central claim; (iii) each segment is argumentatively relevant; (iv) at least one objection to the central claim is considered (in order to produce more interesting argumentation).", "labels": [], "entities": []}, {"text": "Finally, all texts have been checked for spelling and grammatical problems, which have been corrected by the annotators.", "labels": [], "entities": []}, {"text": "The reason underlying this decision was the intended role of the corpus as a resource for studying argumentation in connection with other linguistic phenomena (see Section 2.3), where plain errors can lead to undesired complications for parsers, etc.", "labels": [], "entities": []}, {"text": "Hence, \"authenticity\" on this level was considered as less important.", "labels": [], "entities": [{"text": "authenticity", "start_pos": 8, "end_pos": 20, "type": "METRIC", "confidence": 0.9683172702789307}]}, {"text": "In this respect the corpus differs from web-text corpora that have been collected for argumentation mining purposes, such as the Internet Argument Corpus (, the ABCD corpus ( and others.", "labels": [], "entities": [{"text": "Internet Argument Corpus", "start_pos": 129, "end_pos": 153, "type": "DATASET", "confidence": 0.7182501753171285}, {"text": "ABCD corpus", "start_pos": 161, "end_pos": 172, "type": "DATASET", "confidence": 0.9011746942996979}]}], "datasetContent": [{"text": "In many cases, annotation decisions turned out to be dependent on whether a certain modality or a positive or negative evaluation is added to a segment by the annotator's interpretation.", "labels": [], "entities": []}, {"text": "In the (partial) text below, segment 2, 3 and 4 are annotated as supporting the claim 1.", "labels": [], "entities": []}, {"text": "In turn, 5 supports 4, given that the annotator interprets \"a heart rate that gets going\" as a positive state of affairs, brought about by the desire to keep weight.", "labels": [], "entities": []}, {"text": "In the following, we will describe our experiments on automatically identifying the argumentative structures.", "labels": [], "entities": []}, {"text": "This has already been done on the original version of the corpus, e.g., recently by.", "labels": [], "entities": []}, {"text": "In our experiments we replicate their approach, and test it on the texts we acquired and annotated as described above.", "labels": [], "entities": []}, {"text": "Our aim is to get an understanding of how much the old and the new data sets differ in terms of achievable predictions, and to assess possible improvements by extending the size of the corpus.", "labels": [], "entities": []}, {"text": "Regarding the new phenomena pointed out in Section 4, we chose to ignore non-argumentative segments for the purposes of this experiment, similar as if they had been filtered out in a prior step of a pipeline.", "labels": [], "entities": []}, {"text": "For one thing, this concerns only three texts, and, more importantly, if we want to compare our results to the earlier work, we should work with the same representations.", "labels": [], "entities": []}, {"text": "Second, implicit claims that have been made explicit by the annotators are included when we predict argumentation structures.", "labels": [], "entities": []}, {"text": "The predictions of these local models are then combined into a single edge score and decoded to a structure by selecting the minimum spanning tree (MST).", "labels": [], "entities": []}, {"text": "This approach has been shown to yield competitive results when compared to ILP decoders; seethe original papers for more details.", "labels": [], "entities": []}, {"text": "Similar to previous work, our experiment uses the argumentation graphs in aversion that is con-verted to dependency structures.", "labels": [], "entities": []}, {"text": "Also, the set of relations is reduced to merely 'support' and 'attack' by conflating the subcategories.", "labels": [], "entities": []}, {"text": "This step is done in order to be compatible with earlier work (no other corpora use this set of fine-grained distinctions of support and attack so far) and to alleviate a potential sparse-data problem per specific relation.", "labels": [], "entities": []}, {"text": "Restatements (which tend to occur only for the main claim) exist in the new data set but not in the original one; for compatibility, we converted them to support relations in order to maintain compatibility with the old corpus.", "labels": [], "entities": []}, {"text": "Again, this is a purely technical decision made in order to allow a comparison with prior and related work.", "labels": [], "entities": []}, {"text": "As an alternative, experiments with the fine-grained set of relation have been done (on the original corpus) by.", "labels": [], "entities": []}, {"text": "We adopt the evaluation procedure of previous work, i.e., we use 50 train-test splits, resulting from 10 randomized repetitions of 5-fold cross validation.", "labels": [], "entities": []}, {"text": "For evaluations on the original corpus we use the published splits, for the new corpus we derive splits analogously.", "labels": [], "entities": []}, {"text": "The correctness of predicted structures is measured separately for the four subtasks, reported as macro averaged F1, and more unified in a labelled attachment score (LAS) as it is commonly used for evaluation in dependency parsing (see, ch. 6.1).", "labels": [], "entities": [{"text": "macro averaged F1", "start_pos": 98, "end_pos": 115, "type": "METRIC", "confidence": 0.751361350218455}, {"text": "labelled attachment score (LAS)", "start_pos": 139, "end_pos": 170, "type": "METRIC", "confidence": 0.8841557204723358}, {"text": "dependency parsing", "start_pos": 212, "end_pos": 230, "type": "TASK", "confidence": 0.8319655358791351}]}, {"text": "For significance testing, we use the Wilcoxon signed-rank test.", "labels": [], "entities": [{"text": "significance testing", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.9475165605545044}]}, {"text": "We compare the results on the original dataset and those on the new one using three evaluation scenarios: Single Corpus This is the standard scenario for evaluating the model on one single corpus, from which both training and test sets are sampled.", "labels": [], "entities": []}, {"text": "We reproduce the results on the original corpus, and produce new results for the new corpus.", "labels": [], "entities": []}, {"text": "Comparing these scores gives a first, but only tentative, impression whether the structures annotated in the new corpus are as easy or as hard to recognize as in the original corpus.", "labels": [], "entities": []}, {"text": "Cross Corpus When we train the model exclusively on one corpus and test it on the other, we can investigate the degree of generalization of the model.", "labels": [], "entities": []}, {"text": "This is especially interesting, since the new corpus had different prompts and thus covers different topics.", "labels": [], "entities": []}, {"text": "We expect a decrease in performance when compared to in-domain results as in the single-corpus setting.", "labels": [], "entities": []}, {"text": "Extend Corpus Finally, we use one corpus as additional training data when evaluating on the other.", "labels": [], "entities": []}, {"text": "This helps us to understand to which degree new data can help achieve better results for the four subtasks and overall for the prediction of full structures.", "labels": [], "entities": []}, {"text": "We expect improvements here, when compared with the single-corpus setting.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation scores for the predicted structures reported as macro avg. F1 for the cc, ro, fu, and at levels,  and as labelled attachment score (LAS). Results marked with a dagger are significant improvements over the  corresponding 'single' score, with  \u2020 for p < 0.05 and  \u2021 for p < 0.01.", "labels": [], "entities": [{"text": "F1", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.9284059405326843}, {"text": "labelled attachment score (LAS)", "start_pos": 126, "end_pos": 157, "type": "METRIC", "confidence": 0.8902221421400706}]}]}