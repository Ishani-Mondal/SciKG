{"title": [{"text": "Does it care what you asked? Understanding Importance of Verbs in Deep Learning QA System", "labels": [], "entities": [{"text": "Importance", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9051480293273926}]}], "abstractContent": [{"text": "In this paper we present the results of an investigation of the importance of verbs in a deep learning QA system trained on SQuAD data-set.", "labels": [], "entities": [{"text": "SQuAD data-set", "start_pos": 124, "end_pos": 138, "type": "DATASET", "confidence": 0.8921854496002197}]}, {"text": "We show that main verbs in questions carry little influence on the decisions made by the system-in over 90% of researched cases swapping verbs for their antonyms did not change system decision.", "labels": [], "entities": []}, {"text": "We track this phenomenon down to the insides of the net, analyzing the mechanism of self-attention and values contained in hidden layers of RNN.", "labels": [], "entities": []}, {"text": "Finally , we recognize the characteristics of the SQuAD dataset as the source of the problem.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.8189838230609894}]}, {"text": "Our work refers to the recently popular topic of adversarial examples in NLP, combined with investigating deep net structure.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent advances in interpretability for NLP focus on the problem of adversarial examples () (Jia and Liang, 2017) which lead systems to mistakenly change output.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.9681780338287354}]}, {"text": "In case of QA systems, either questions or contexts are modified, and it is shown that seemingly small changes in semantics flip system decisions.", "labels": [], "entities": []}, {"text": "In this paper we take a different approach: we create heavy differences in meaning by generating questions with their meaning negated, and observe system outputs.", "labels": [], "entities": []}, {"text": "Our initial hypothesis was that verbs together with nouns should be of paramount importance to the system, as they are the main creators of meaning in language.", "labels": [], "entities": []}, {"text": "We find that reversing verb meaning disturbs system output in 9.5% of cases, with little influence on decision certainty.", "labels": [], "entities": [{"text": "certainty", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.849753201007843}]}, {"text": "We then proceed to explain this phenomenon by observing the behavior of deep net architecture and the characteristics of the SQuAD dataset itself.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 125, "end_pos": 138, "type": "DATASET", "confidence": 0.7387697845697403}]}, {"text": "As a basis of our research we use the QA system described in.", "labels": [], "entities": []}, {"text": "We pick this mo-*Both authors contributed equally.", "labels": [], "entities": []}, {"text": "del for its good performance and state-of-the-art approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "Attempting to understand the behavior of the system we take inspiration from works focusing on visualizing deep net internals ().", "labels": [], "entities": []}, {"text": "We apply measures specific to the mechanisms present in our tested system: question self-attention and hidden layers of the RNN.", "labels": [], "entities": []}, {"text": "We run experiments on SQuAD development set.   words.", "labels": [], "entities": [{"text": "SQuAD development set", "start_pos": 22, "end_pos": 43, "type": "DATASET", "confidence": 0.768271287282308}]}, {"text": "As shown in, indeed question attention learned to devalue verbs.", "labels": [], "entities": []}, {"text": "Statistical importance of differences between distributions (in particular, of verbs vs. nouns) was confirmed with Kolmogorov-Smirnov test, which showed pvalues smaller than 0.001.", "labels": [], "entities": []}, {"text": "Next, we analyze 3-layer LSTM RNN, whose outputs are used to compute question attention.", "labels": [], "entities": []}, {"text": "We gather the outputs of all layers and visualize them using heatmaps, as in.", "labels": [], "entities": []}, {"text": "We observe that variances in numbers appearing in lower layers are distinctively smaller than in the third layer.", "labels": [], "entities": []}, {"text": "Furthermore, nouns (in particular named entities) exhibit greater variances than other parts of speech, which aligns with observations for attention scores.", "labels": [], "entities": []}, {"text": "Indeed, correlation between entropy scores counted for last hidden layer vectors and attention scores equals -0.91 Pearson's r, and and appropriate variance-attention correlation equals 0.85 Pearson's rand 0.96 Spearman's correlation, as displayed in.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 115, "end_pos": 126, "type": "METRIC", "confidence": 0.8464277386665344}, {"text": "Pearson's rand 0.96 Spearman's correlation", "start_pos": 191, "end_pos": 233, "type": "METRIC", "confidence": 0.6064360567501613}]}, {"text": "It suggests that importance of parts of speech is encoded already by the LSTM network.", "labels": [], "entities": []}, {"text": "We observe that in fact the system correctly aligned to the characteristics of the contexts appearing in SQuAD.", "labels": [], "entities": []}, {"text": "Most often a specific noun (commonly a named entity, or a combination thereof) appears in a single sentence in single context, so contrasting verbs is not needed to extract the answer.", "labels": [], "entities": []}, {"text": "To combat this problem, enhancement of the dataset would be ne-  eded to include more sentences with repeating nouns (subjects and objects) and varying verbs describing their actions and relations.", "labels": [], "entities": []}, {"text": "We observe low importance of verbs in QA system training on SQuAD dataset and identify shortcomings in the underlying data.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.8303970992565155}]}, {"text": "Our findings have confirmation in values yielded by network itself.", "labels": [], "entities": []}, {"text": "We show that values in hidden layers and attention scores are correlated with importance of words in the question.", "labels": [], "entities": []}, {"text": "This work confirms the usefulness of visualization and explanation of deep learning NLP models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average absolute attention scores for parts  of speech. We show scores for all verbs, all nouns,  all PoS other than nouns and verbs, auxiliary verbs  (AUX Verbs), all verbs other than auxiliary (Non-AUX  Verbs), all nouns which are not named entities (Non- NE Nouns) and nouns which are named entities (NE  Nouns).", "labels": [], "entities": []}]}