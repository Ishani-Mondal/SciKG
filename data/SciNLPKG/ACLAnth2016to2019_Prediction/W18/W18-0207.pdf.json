{"title": [{"text": "Tracking Typological Traits of Uralic Languages in Distributed Language Representations", "labels": [], "entities": [{"text": "Tracking Typological Traits of Uralic Languages in Distributed Language Representations", "start_pos": 0, "end_pos": 87, "type": "TASK", "confidence": 0.758761179447174}]}], "abstractContent": [{"text": "Although linguistic typology has along history, computational approaches have only recently gained popularity.", "labels": [], "entities": []}, {"text": "The use of distributed representations in computational linguistics has also become increasingly popular.", "labels": [], "entities": []}, {"text": "A recent development is to learn distributed representations of language, such that typologically similar languages are spatially close to one another.", "labels": [], "entities": []}, {"text": "Although empirical successes have been shown for such language representations, they have not been subjected to much typological probing.", "labels": [], "entities": []}, {"text": "In this paper, we first look at whether this type of language representations are empirically useful for model transfer between Uralic languages in deep neural networks.", "labels": [], "entities": [{"text": "model transfer between Uralic languages", "start_pos": 105, "end_pos": 144, "type": "TASK", "confidence": 0.7802555620670318}]}, {"text": "We then investigate which typological features are encoded in these representations by attempting to predict features in the World Atlas of Language Structures, at various stages of fine-tuning of the representations.", "labels": [], "entities": [{"text": "World Atlas of Language Structures", "start_pos": 125, "end_pos": 159, "type": "DATASET", "confidence": 0.9536624550819397}]}, {"text": "We focus on Uralic languages, and find that some typological traits can be automatically inferred with accuracies well above a strong baseline.", "labels": [], "entities": []}, {"text": "Tiivistelm\u00e4 Vaikka kielitypologialla on pitk\u00e4 historia, siihen liittyv\u00e4t laskennalliset mene-telm\u00e4t ovat vasta viime aikoina saavuttaneet suosiota.", "labels": [], "entities": []}, {"text": "My\u00f6s hajautettujen repre-sentaatioiden k\u00e4ytt\u00f6 laskennallisessa kielitieteess\u00e4 on tullut yh\u00e4 suositummak-si.", "labels": [], "entities": []}, {"text": "Viimeaikainen kehitys alalla on oppia kielest\u00e4 hajautettu representaatio, jo-ka esitt\u00e4\u00e4 samankaltaiset kielet l\u00e4hell\u00e4 toisiaan.", "labels": [], "entities": []}, {"text": "Vaikka kyseiset representaatiot nauttivatkin empiirist\u00e4 menestyst\u00e4, ei niit\u00e4 ole huomattavasti tutkittu typologi-sesti.", "labels": [], "entities": []}, {"text": "T\u00e4ss\u00e4 artikkelissa tutkitaan, ovatko t\u00e4llaiset kielirepresentaatiot empiirises-ti k\u00e4ytt\u00f6kelpoisia uralilaisten kielten v\u00e4lisiss\u00e4 mallimuunnoksissa syviss\u00e4 neuro-verkoissa.", "labels": [], "entities": []}, {"text": "Pyrkim\u00e4ll\u00e4 ennustamaan piirteit\u00e4 World Atlas of Language Structures-tietokannassa tutkimme, mit\u00e4 typologisia ominaisuuksia n\u00e4m\u00e4 representaatiot si-s\u00e4lt\u00e4v\u00e4t.", "labels": [], "entities": [{"text": "World Atlas of Language Structures-tietokannassa tutkimme", "start_pos": 33, "end_pos": 90, "type": "DATASET", "confidence": 0.9306637644767761}]}, {"text": "Keskityimme uralilaisiin kieliin ja huomasimme, ett\u00e4 jotkin typologiset ominaisuudet voidaan automaattisesti p\u00e4\u00e4tell\u00e4 tarkkuudella, joka ylitt\u00e4\u00e4 selv\u00e4sti vahvan perustason.", "labels": [], "entities": []}, {"text": "This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": []}], "introductionContent": [{"text": "For more than two and a half centuries, linguistic typologists have studied languages with respect to their structural and functional properties, thereby implicitly classifying languages as being more or less similar to one another, by virtue of such properties.", "labels": [], "entities": []}, {"text": "Although typology has along history), computational approaches have only recently gained popularity.", "labels": [], "entities": []}, {"text": "One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures.", "labels": [], "entities": [{"text": "World Atlas of Language Structures", "start_pos": 171, "end_pos": 205, "type": "DATASET", "confidence": 0.911582350730896}]}, {"text": "A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings).", "labels": [], "entities": []}, {"text": "These language embeddings encode typological properties of language, reminiscent of the sparse features in WALS, or even of parameters in Chomsky's Principles and Parameters framework.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the usefulness of explicitly modelling similarities between languages in deep neural networks using language embeddings.", "labels": [], "entities": []}, {"text": "To do so, we view NLP tasks for multiple Uralic languages as different aspects of the same problem and model them in one model using multilingual transfer in a multi-task learning model.", "labels": [], "entities": []}, {"text": "Multilingual models frequently follow a hard parameter sharing regime, where all hidden layers of a neural network are shared between languages, with the language either being implicitly coded in the input string, given as a language ID in a one-hot encoding, or as a language embedding.", "labels": [], "entities": []}, {"text": "In this paper, we both explore multilingual modelling of Uralic languages, and probe the language embeddings obtained from such modelling in order to gain novel insights about typological traits of Uralic languages.", "labels": [], "entities": []}, {"text": "We aim to answer the following three research questions (RQs).", "labels": [], "entities": []}], "datasetContent": [{"text": "We approach the task of PoS tagging using a fairly standard bi-directional LSTM architecture, based on Plank et al.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.960152804851532}]}, {"text": "The system is implemented using DyNet ( ).", "labels": [], "entities": []}, {"text": "We train using the Adam optimisation algorithm () over a maximum of 10 epochs, using early stopping.", "labels": [], "entities": []}, {"text": "We make two modifications to the bi-LSTM architecture of.", "labels": [], "entities": []}, {"text": "First of all, we do not use any atomic embedded word representations, but rather use only character-based word representations.", "labels": [], "entities": []}, {"text": "This choice was made so as to encourage the model not to rely on languagespecific vocabulary.", "labels": [], "entities": []}, {"text": "Additionally, we concatenate a pre-trained language embedding to each word representation.", "labels": [], "entities": []}, {"text": "That is to say, in the original bi-LSTM formulation of, each word w is represented as \u20d7 w + LST M c (w), where \u20d7 w is an embedded word representation, and LST M c (w) is the final states of a character bi-LSTM running over the characters in a word.", "labels": [], "entities": []}, {"text": "In our formulation, each word win language l is represented as LST M c (w) + \u20d7 l, where LST M c (w) is defined as before, and \u20d7 l is an embedded language representation.", "labels": [], "entities": []}, {"text": "We use a two-layer deep bi-LSTM, with 100 units in each layer.", "labels": [], "entities": []}, {"text": "The character embeddings used also have 100 dimensions.", "labels": [], "entities": []}, {"text": "We update the language representations, \u20d7 l, during training.", "labels": [], "entities": []}, {"text": "The language representations are 64-dimensional, and are initialised using the language embeddings from.", "labels": [], "entities": []}, {"text": "All PoS tagging results reported are the average of five runs, each with different initialisation seeds, so as to minimise random effects in our results.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.728792816400528}]}], "tableCaptions": []}