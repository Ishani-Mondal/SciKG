{"title": [{"text": "Feature Engineering for Second Language Acquisition Modeling", "labels": [], "entities": [{"text": "Second Language Acquisition Modeling", "start_pos": 24, "end_pos": 60, "type": "TASK", "confidence": 0.6880052238702774}]}], "abstractContent": [{"text": "Knowledge tracing serves as a keystone in delivering personalized education.", "labels": [], "entities": [{"text": "Knowledge tracing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7623196244239807}]}, {"text": "However, few works attempted to model students' knowledge state in the setting of Second Language Acquisition.", "labels": [], "entities": [{"text": "Second Language Acquisition", "start_pos": 82, "end_pos": 109, "type": "TASK", "confidence": 0.6222180326779684}]}, {"text": "The Duolingo Shared Task on Second Language Acquisition Modeling (Set-tles et al., 2018) provides students' trace data that we extensively analyze and engineer features from for the task of predicting whether a student will correctly solve a vocabulary exercise.", "labels": [], "entities": [{"text": "Second Language Acquisition Modeling (Set-tles et al., 2018)", "start_pos": 28, "end_pos": 88, "type": "TASK", "confidence": 0.7041642259467732}, {"text": "predicting whether a student will correctly solve a vocabulary exercise", "start_pos": 190, "end_pos": 261, "type": "TASK", "confidence": 0.5009212583303452}]}, {"text": "Our analyses of students' learning traces reveal that factors like exercise format and engagement impact their exercise performance to a large extent.", "labels": [], "entities": []}, {"text": "Overall, we extracted 23 different features as input to a Gradient Tree Boosting framework, which resulted in an AUC score of between 0.80 and 0.82 on the official test set.", "labels": [], "entities": [{"text": "Gradient Tree Boosting framework", "start_pos": 58, "end_pos": 90, "type": "DATASET", "confidence": 0.7739789113402367}, {"text": "AUC", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.8392571806907654}]}], "introductionContent": [{"text": "Knowledge Tracing plays a crucial role in providing adaptive learning to students: by estimating a student's current knowledge state and predicting her performance in future interactions, students can receive personalized learning materials (e.g. on the topics the student is estimated to know the least about).", "labels": [], "entities": []}, {"text": "Over the years, various knowledge tracing techniques have been proposed and studied, including Bayesian Knowledge Tracing, Performance Factor Analysis), Learning Factors Analysis () and Deep Knowledge Tracing (.", "labels": [], "entities": [{"text": "knowledge tracing", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.8002063035964966}, {"text": "Deep Knowledge Tracing", "start_pos": 186, "end_pos": 208, "type": "TASK", "confidence": 0.5976346333821615}]}, {"text": "Notable is that most of the existing works focus on learning performance within mathematics in elementary school and high school due to the availability of sufficiently large datasets in this domain, e.g. ASSISTment and OLI.", "labels": [], "entities": []}, {"text": "The generalization to other learning scenarios and domains remains under-explored.", "labels": [], "entities": []}, {"text": "Particularly, there are few studies attempted to explore knowledge tracing in the setting of Second Language Acquisition (SLA).", "labels": [], "entities": [{"text": "knowledge tracing", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7519349455833435}, {"text": "Second Language Acquisition (SLA)", "start_pos": 93, "end_pos": 126, "type": "TASK", "confidence": 0.7974055906136831}]}, {"text": "Recent studies showed that SLA is becoming increasingly important in people's daily lives and should gain more research attention to facilitate their learning process.", "labels": [], "entities": [{"text": "SLA", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9810417294502258}]}, {"text": "It remains an open question whether the existing knowledge tracing techniques can be directly applied to SLA modeling-the release of the Duolingo challenge datasets now enables us to investigate this very question.", "labels": [], "entities": [{"text": "knowledge tracing", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7609071135520935}, {"text": "SLA modeling-the", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.9905450940132141}, {"text": "Duolingo challenge datasets", "start_pos": 137, "end_pos": 164, "type": "DATASET", "confidence": 0.9527503450711569}]}, {"text": "Thus, our work is guided by the following research question: What factors impact students' language learning performance?", "labels": [], "entities": []}, {"text": "To answer the question, we first formulate six research hypotheses which are built on previous studies in SLA.", "labels": [], "entities": [{"text": "SLA", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9044971466064453}]}, {"text": "We perform extensive analyses on the three SLA Duolingo datasets ( to determine to what extent they hold.", "labels": [], "entities": [{"text": "SLA Duolingo datasets", "start_pos": 43, "end_pos": 64, "type": "DATASET", "confidence": 0.8488974571228027}]}, {"text": "Subsequently, we engineer a set of 23 features informed by the analyses and use them as input fora state-of-the-art machine learning model, Gradient Tree Boosting (, to estimate the likelihood of whether a student will correctly solve an exercise.", "labels": [], "entities": []}, {"text": "We contribute the following major findings: (i) students who are heavily engaged with the learning platform are more likely to solve words correctly; (ii) contextual factors like the device being used and learning format impact students' performance considerably; (iii) repetitive practice is a necessary step for students towards mastery; (iv) Gradient Tree Boosting are demonstrated to bean effective method for predicting students' future performance in SLA.", "labels": [], "entities": [{"text": "Gradient Tree Boosting", "start_pos": 345, "end_pos": 367, "type": "TASK", "confidence": 0.5567626456419627}]}], "datasetContent": [{"text": "In this section, we first describe our experimental setup and then present the results.", "labels": [], "entities": []}, {"text": "Each of the three Duolingo datasets consists of three parts: TRAIN and DEV sets for offline experimentations and one TEST set for the final evaluation.", "labels": [], "entities": [{"text": "Duolingo datasets", "start_pos": 18, "end_pos": 35, "type": "DATASET", "confidence": 0.9341162145137787}, {"text": "TRAIN", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9980871677398682}, {"text": "DEV", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.8943524956703186}, {"text": "TEST", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9901520609855652}]}, {"text": "We use the TRAIN and DEV sets to explore features that are useful in predicting a student's exercise performance and then combine TRAIN and DEV sets to train the GTB model; we report the model's performance on the TEST set.", "labels": [], "entities": [{"text": "TRAIN", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9149467945098877}, {"text": "TEST set", "start_pos": 214, "end_pos": 222, "type": "DATASET", "confidence": 0.775775820016861}]}, {"text": "We trained the GTB model using XGBoost, a scalable machine learning system for tree boosting.", "labels": [], "entities": [{"text": "tree boosting", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.7187616527080536}]}, {"text": "All model parameters were optimized through grid search and are reported in.", "labels": [], "entities": []}, {"text": "For a detailed explanation of the parameters, please refer to https://github.com/dmlc/xgboost/blob/ v0.71/doc/parameter.md.", "labels": [], "entities": []}, {"text": "We also report the official baseline provided by the benchmark organizers as comparison.", "labels": [], "entities": []}, {"text": "The baseline is a logistic regression model which takes six features as input, which include student ID, word, format and three morpho-syntactic features of the word (e.g., Part of Speech).", "labels": [], "entities": []}, {"text": "As suggested by the benchmark organizers, we use the AUC and F1 scores as our evaluation metrics.", "labels": [], "entities": [{"text": "AUC", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.5891899466514587}, {"text": "F1", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9920169115066528}]}], "tableCaptions": [{"text": " Table 1: Statistics of the datasets.", "labels": [], "entities": []}, {"text": " Table 2: Avg. student-level accuracy (%) and  the number of mastered words of students living  in different locations (approximated by the coun- tries from which students have finished the exer- cises). Significant differences (compared to Avg.,  according to Mann-Whitney) are marked with  *   (p < 0.001).", "labels": [], "entities": [{"text": "Avg", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9666943550109863}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9381240010261536}, {"text": "Avg.", "start_pos": 241, "end_pos": 245, "type": "METRIC", "confidence": 0.7830995917320251}]}, {"text": " Table 5: Average exercise-level accuracy (%) in  different contextual conditions. Significant dif- ferences (compared to Avg., according to Mann- Whitney) are marked with  * (p < 0.001).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9540352821350098}, {"text": "Significant dif- ferences", "start_pos": 83, "end_pos": 108, "type": "METRIC", "confidence": 0.5899091586470604}, {"text": "Avg.", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.985293984413147}]}, {"text": " Table 6: Avg. word-level accuracy (%) of words  with different number of exposures.", "labels": [], "entities": [{"text": "Avg", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9904458522796631}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.7805671095848083}]}, {"text": " Table 7: Pearson Correlation between student per- formance and the number of previous attempts and  the amount of time elapsed since the last attempt  for a word.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9847044944763184}, {"text": "Correlation", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.6593817472457886}]}, {"text": " Table 9: Model parameters of the GTB model; de- termined by using grid search per dataset.", "labels": [], "entities": []}, {"text": " Table 10: Experimental results reported in AUC on  ES-EN. Each row indicates a feature added to the  GBT feature space; the model of row 1 has three  features.", "labels": [], "entities": [{"text": "AUC", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.8894297480583191}, {"text": "ES-EN", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.6915974617004395}, {"text": "GBT feature space", "start_pos": 102, "end_pos": 119, "type": "DATASET", "confidence": 0.8872237404187521}]}, {"text": " Table 11: Final prediction results on the TEST  data. Significant differences (compared to Base- line, according to paired t-test) are marked with  *   (p < 0.001).", "labels": [], "entities": [{"text": "TEST  data", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.823475569486618}, {"text": "Base- line", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9186176856358846}]}]}