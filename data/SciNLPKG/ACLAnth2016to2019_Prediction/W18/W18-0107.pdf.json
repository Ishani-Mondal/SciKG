{"title": [{"text": "Experiential, Distributional and Dependency-based Word Embeddings have Complementary Roles in Decoding Brain Activity", "labels": [], "entities": []}], "abstractContent": [{"text": "We evaluate 8 different word embedding models on their usefulness for predicting the neural activation patterns associated with concrete nouns.", "labels": [], "entities": []}, {"text": "The models we consider include an experiential model, based on crowd-sourced association data, several popular neural and distributional models, and a model that reflects the syntactic context of words (based on dependency parses).", "labels": [], "entities": []}, {"text": "Our goal is to assess the cognitive plausibility of these various embedding models, and understand how we can further improve our methods for interpreting brain imaging data.", "labels": [], "entities": [{"text": "interpreting brain imaging", "start_pos": 142, "end_pos": 168, "type": "TASK", "confidence": 0.8832666476567587}]}, {"text": "We show that neural word embedding models exhibit superior performance on the tasks we consider, beating experiential word representation model.", "labels": [], "entities": []}, {"text": "The syntactically informed model gives the overall best performance when predicting brain activation patterns from word embeddings; whereas the GloVe distri-butional method gives the overall best performance when predicting in the reverse direction (words vectors from brain images).", "labels": [], "entities": []}, {"text": "Interestingly , however, the error patterns of these different models are markedly different.", "labels": [], "entities": []}, {"text": "This may support the idea that the brain uses different systems for processing different kinds of words.", "labels": [], "entities": []}, {"text": "Moreover, we suggest that taking the relative strengths of different embedding models into account will lead to better models of the brain activity associated with words.", "labels": [], "entities": []}], "introductionContent": [{"text": "How are word meanings represented in the human brain?", "labels": [], "entities": []}, {"text": "Is there a single amodal semantic system or are there multiple responsible for representing meanings of different classes of words?", "labels": [], "entities": []}, {"text": "Recently, a series of studies have emerged showing that a combination of methods from machine learning, computational linguistics and cognitive neuroscience are useful for addressing such questions.", "labels": [], "entities": []}, {"text": "() pioneered the use of corpusderived word representations to predict patterns of neural activation's when subjects are exposed to a stimulus word.", "labels": [], "entities": []}, {"text": "Using their framework, a series of papers have evaluated various techniques of computing word representation models based on different assumptions, as we review in section 2.", "labels": [], "entities": [{"text": "word representation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.6889993399381638}]}, {"text": "Since these early successes, a range of new word embedding methods have been proposed and successfully used in a variety of NLP tasks, including methods based on deep learning with neural networks.", "labels": [], "entities": []}, {"text": "() and () present systematic studies, showing that also behavioural data from psycholinguistics can be modelled effectively using neural word embedding models such as GloVe() and word2vec(.", "labels": [], "entities": []}, {"text": "At the same time, studies in the area of vision have shown that deep learning models fit very well to the neocortical data ( and they can help to better understand the sensory cortical system.", "labels": [], "entities": []}, {"text": "To investigate how well the new word embedding models, and in particular the deep learning models, fare in helping to understand neural activation patterns in the domain of language, we now present a systematic evaluation of 8 word embedding models, listed in section 3, against the neuroimaging data from, following the experiments and primary results in.", "labels": [], "entities": []}, {"text": "To address this goal, we take word embedding models designed based on different assumptions of how meanings of words can be represented and evaluate their performance on either the task of predicting brain data from word embeddings or the reverse, predicting word embeddings from brain data.", "labels": [], "entities": []}, {"text": "The basic assumption here is that the better the performance of a model is the more probable it is that the way the word embedding model is built reflects what happens in the human brain to understand a meaning of a word.", "labels": [], "entities": []}, {"text": "In our experiments, we compare modern neural word embedding models with traditional approaches that are based on manually assigned linguistic word attributes, and neuro-inspired techniques based on sensory-motor features.", "labels": [], "entities": []}, {"text": "Besides a large-scale evaluation of various word embedding models, we conduct a detailed error analysis to understand the differences between them.", "labels": [], "entities": []}, {"text": "The first research question we investigate is: How well does each word embedding model allow us to predict neural activation patterns inhuman brain?", "labels": [], "entities": []}, {"text": "To answer this we measure how well different word embedding models can predict the brain imaging data.", "labels": [], "entities": []}, {"text": "Taking this one step further, we also train our models in the reverse direction: to directly predict word embeddings from brain data.", "labels": [], "entities": []}, {"text": "The second research question that we investigate is: What is the best word embedding model for predicting brain activation for different (classes of) nouns?", "labels": [], "entities": []}, {"text": "Maybe human brain uses different processes to understand meanings of different kind of words ().", "labels": [], "entities": []}, {"text": "We do a qualitative analysis of our results to see whether different word embedding models are good in predicting the brain activation for different categories of nouns.", "labels": [], "entities": []}, {"text": "The third question we address is Which are the most predictable voxels in the brain for each word embedding model?", "labels": [], "entities": []}, {"text": "By answering this question we want to test the hypothesis that different areas of the brain are responsible for processing different aspect of the meaning of nouns.", "labels": [], "entities": []}, {"text": "If different models have different performance either for different noun pairs or for different brain areas, the next step would be to find away to integrate different models to build a model that better fits the brain data.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main task in our experiments is to use a regression model to map word representations to brain activation patterns or vice versa.", "labels": [], "entities": []}, {"text": "As the regression model, we employ a single layer neural network with tanh activation.", "labels": [], "entities": []}, {"text": "To avoid over-fitting we use drop-connect () with a keeping rate of 0.7 beside L2 regularization with \u03bb=0.001.", "labels": [], "entities": []}, {"text": "In all the experiments we train the models for each subject separately.", "labels": [], "entities": []}, {"text": "The training and evaluation are done with the leave-2-out method as suggested in ().", "labels": [], "entities": []}, {"text": "Where we train the model on all except 2 pairs and then evaluate the performance of the model on the left-out pairs.", "labels": [], "entities": []}, {"text": "We do this for all possible combinations of pairs.", "labels": [], "entities": []}, {"text": "Neuroimaging Data Our experiments are conducted on the data from which is publicly available . This is a collection of fMRI data that is gathered from 9 participants while exposed to distinctive stimuli.", "labels": [], "entities": []}, {"text": "The stimuli consisted of 60 nouns and corresponding line drawings.", "labels": [], "entities": []}, {"text": "Each stimulus was displayed six times for 3 seconds in random order, adding to a total of 360 fMRI images per participant.", "labels": [], "entities": []}, {"text": "Word Embedding Models In order to get insights about how human mental lexicon is built, we use a wide variety of recently proposed word representation models.", "labels": [], "entities": [{"text": "word representation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.7145362496376038}]}, {"text": "The word embedding models that we are exploring in our experiments are in two (non-exclusive) categories: experiential or distributional.", "labels": [], "entities": []}, {"text": "In the experiential model, the meanings of the words are coded to reflect how the corresponding concept is experienced by humans through their senses.", "labels": [], "entities": []}, {"text": "In the distributional models, the meaning of words is represented based on their co-occurrence with other words.", "labels": [], "entities": []}, {"text": "These models can be either count-based or predictive ().", "labels": [], "entities": []}, {"text": "The word representation models we will use are: \u2022 Experiential word representations: Experiential word representations are suggested based on the fact that humans remember the meaning of things as they experience them.", "labels": [], "entities": []}, {"text": "In (Binder et al., 2016) a set of 65 features are defined and crowdsourcing is used to rate the relatedness of each feature for each word.", "labels": [], "entities": []}, {"text": "Thus, instead of computing the value of features using statistical data from textual corpora they use actual human ratings.", "labels": [], "entities": []}, {"text": "We use the dataset introduce in.", "labels": [], "entities": []}, {"text": "Since it contains only about 50% of the nouns in Tom Mitchell et al dataset, some of the experiments we report are with this limited noun set.", "labels": [], "entities": [{"text": "Tom Mitchell et al dataset", "start_pos": 49, "end_pos": 75, "type": "DATASET", "confidence": 0.6894901096820831}]}, {"text": "\u2022 Distributional word embedding models: -Word2Vec: Word2vec basically is a shallow, two layer, neural network that reconstructs the context of a given word.", "labels": [], "entities": []}, {"text": "In our experiments, we use the skip gram word2vec model trained on Wikipedia ().", "labels": [], "entities": []}, {"text": "-Fasttext: Fasttext is a modification of word2vec that takes morphological information into account (.", "labels": [], "entities": []}, {"text": "-Dependency-based word2vec: The dependency-based word2vec introduced in () is a word2vec model in which the context of the words is computed based on the dependency relations.", "labels": [], "entities": []}, {"text": "-GloVe: GloVe is a count-based method.", "labels": [], "entities": []}, {"text": "It does a dimensionality reduction on the co-occurrence matrix().", "labels": [], "entities": []}, {"text": "-LexVec: LexVec is also a count based method.", "labels": [], "entities": [{"text": "LexVec", "start_pos": 1, "end_pos": 7, "type": "DATASET", "confidence": 0.936578094959259}, {"text": "LexVec", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.8850273489952087}]}, {"text": "It is a matrix factorization method that combines ideas from different models.", "labels": [], "entities": []}, {"text": "It minimizes the reconstruction loss function that weights frequent co-occurrences heavily while taking into account negative co-occurrence (Salle et al., 2016b,a).", "labels": [], "entities": []}, {"text": "\u2022 25 verb features: Similar to experiential word representations, this model is based on the idea that the neural representation of nouns is grounded in sensory-motor features.", "labels": [], "entities": []}, {"text": "They have manually picked 25 verbs and suggested to use the co-occurrence counts of nouns with these 25 verbs to form the word representations ().", "labels": [], "entities": []}, {"text": "\u2022 non-distributional word vector representation: have constructed a nondistributional word representation model employing linguistic resources such as WordNet, FrameNet( etc.", "labels": [], "entities": [{"text": "non-distributional word vector representation", "start_pos": 2, "end_pos": 47, "type": "TASK", "confidence": 0.6441416442394257}, {"text": "WordNet", "start_pos": 151, "end_pos": 158, "type": "DATASET", "confidence": 0.9604007601737976}]}, {"text": "In this model, words are presented as binary vectors where each element of the vector indicates whether the represented word has or does not have a specific feature.", "labels": [], "entities": []}, {"text": "As a result, the vectors are highly sparse.", "labels": [], "entities": []}, {"text": "The advantage of this model to distributional word representations is the interpretability of its dimensions.", "labels": [], "entities": [{"text": "distributional word representations", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.6121543248494467}]}, {"text": "4 How well does each word embedding model allow us to predict neural activation patterns inhuman brain?", "labels": [], "entities": []}, {"text": "To address the first research question, we train a separate regression model for each word representation model to compute the average brain activation corresponding to each word fora particular subject.", "labels": [], "entities": []}, {"text": "illustrates the results of evaluating these models on the brain activation prediction task, using the leave-2-out methodology we discussed in section 3.", "labels": [], "entities": [{"text": "brain activation prediction task", "start_pos": 58, "end_pos": 90, "type": "TASK", "confidence": 0.7670088708400726}]}, {"text": "For the sake of including the experiential word representations from () in our evaluations, we also conducted a set of experiments with only the nouns that were included in the experiential word representation collection.", "labels": [], "entities": []}, {"text": "The good news is that all the models we are evaluating perform significantly above chance.", "labels": [], "entities": []}, {"text": "The fact that the ranking of the models differs per subject makes it difficult to make general conclusions about the best model.", "labels": [], "entities": []}, {"text": "Overall, dependency-based word2vec, GloVe and 25 features model are the top-ranked models for at least one of the subjects.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.7060661315917969}]}, {"text": "Among neural word embedding models, dependency-based word2vec is achieving the best accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9979064464569092}]}, {"text": "This is inline with the results from (Murphy et al., 2012), where they showed that the corpus-based model considering the dependency relationships has the highest performance among corpus-based models.", "labels": [], "entities": []}, {"text": "These authors report an accuracy of 83.1 (with 1000 dimensional word vectors).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9996041655540466}]}, {"text": "Somewhat higher still than the best dependency based word2vec, and the highest performance reported in the literature until now fora corpus-based model.", "labels": [], "entities": []}, {"text": "The fact that fasttext and dependency based word2vec are performing better than word2vec might reflect the importance of morphological and dependency information.", "labels": [], "entities": []}, {"text": "Comparing predictive models with count-based models, although countbased methods like GloVe and LexVec are beating simple word2vec, looking at the performances of fasttext and dependency based word2vec, we can conclude that the context prediction models can potentially perform better.", "labels": [], "entities": [{"text": "LexVec", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.9133602976799011}, {"text": "context prediction", "start_pos": 228, "end_pos": 246, "type": "TASK", "confidence": 0.7300683259963989}]}, {"text": "Moreover, comparing the performance of the Experiential Model with 25 feature model, we see that the Experiential Model is doing slightly better on average while their ranking is different per subject.", "labels": [], "entities": []}, {"text": "Either the higher number of features or the way feature values are computed could have led to the slight improvement inaccuracy for the experiential model.", "labels": [], "entities": []}, {"text": "In both sets of experiments in the non-distributional word representation model has the lowest performance.", "labels": [], "entities": []}, {"text": "The very high dimensionality of the brain imaging data versus the sparseness of nondistributional word vectors make training the regression model with these vectors much harder and this might be the primary reason for its low performance.", "labels": [], "entities": []}, {"text": "Next, instead of predicting brain activation patterns, we train the regression model to predict the word representation given a brain activation.", "labels": [], "entities": []}, {"text": "Thus, we want to predict the stimulus word from the neural activation pattern in the brain.", "labels": [], "entities": []}, {"text": "Evaluation is still based on the leave-2-out setup (so we still evaluate with 2 brain images and 2 word embeddings at each instance, making quantitative results comparable across experiments).", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We expected the performance of the models on the reversed task, predicting word features from brain activation, to be somewhat similar to their performance on the main task, predicting brain activation patterns from word vectors.", "labels": [], "entities": []}, {"text": "However, the results are surprising.", "labels": [], "entities": []}, {"text": "For the 25 features model, the accuracy on the reversed task is much lower.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.999581515789032}]}, {"text": "This maybe because of the way the feature vector for nouns is distributed in the space in this model.", "labels": [], "entities": []}, {"text": "Or it could be that neural activation patterns do not encode all the necessary information to approximate these feature values.", "labels": [], "entities": []}, {"text": "This could indicate that while the 25 features model is pretty useful in interpreting brain activation patterns it is not a plausible model to simulate how nouns are represented in the human brain.", "labels": [], "entities": [{"text": "interpreting brain activation patterns", "start_pos": 73, "end_pos": 111, "type": "TASK", "confidence": 0.8552545607089996}]}, {"text": "On the other hand, it seems that it is very easy to construct GloVe word vectors from brain activation patterns; this model achieves an accuracy of 90 percent.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9993798732757568}]}, {"text": "In (Sudre et al., 2012) accuracy of 91.19 percent is reported on the similar task on MEG data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9996694326400757}, {"text": "MEG data", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.9269300401210785}]}, {"text": "GloVe is based on the distributional semantics hypothesis, and it is achieved by learning to predict the global co-occurrence statistics of words in a corpus.", "labels": [], "entities": []}, {"text": "Hence, obtaining a high accuracy in the word prediction task using GloVe, supports the fact that the context of the words have a major role in the way we learn the meanings of the words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9983975291252136}, {"text": "word prediction task", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.8215163548787435}]}, {"text": "The important thing  to notice is that of course the more information we encode in the word representation the more powerful it becomes in predicting neural activation patterns as far as that information are relevant to some extent.", "labels": [], "entities": [{"text": "predicting neural activation patterns", "start_pos": 139, "end_pos": 176, "type": "TASK", "confidence": 0.8752722293138504}]}, {"text": "However, this alone doesn't imply that the exact same information is encoded in the neural activation patterns.", "labels": [], "entities": []}, {"text": "As we can see in our results, compared to GloVe, it's not that easy to reconstruct the Fasttext and dependency based word vectors from the brain activation patterns.", "labels": [], "entities": []}, {"text": "What we can conclude, for now, is that while morphological and dependency information is helpful in learning word representations that are to some extent more similar to the neural representation of nouns in our brain.", "labels": [], "entities": []}, {"text": "This information is not explicitly encoded in the brain activation patterns.", "labels": [], "entities": []}, {"text": "In the end, only comparing the accuracy of these models does not reveal much about the differences between them and does not mean that the model with the highest accuracy can replace all the others.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9986492991447449}, {"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9728516340255737}]}, {"text": "beetle ant  5 What is the best word embedding model for predicting brain activation for different (classes of) nouns?", "labels": [], "entities": []}, {"text": "In order to get more insights about the differences between the models, we look into the errors they make.", "labels": [], "entities": []}, {"text": "It is informative to see whether each of these models is good at predicting neural activation pattern fora different group of noun pairs.", "labels": [], "entities": []}, {"text": "We want to test the hypothesis of whether human brain uses different mechanisms for understanding meanings of different categories of words ().", "labels": [], "entities": []}, {"text": "To investigate this we look into the miss matched noun pairs for each of the word representation models.", "labels": [], "entities": []}, {"text": "We want to see which are the most confusing noun pairs for each model and measure the overlap between the errors the models make.", "labels": [], "entities": []}, {"text": "This will reveal if these models are actually encoding different kinds of information.", "labels": [], "entities": []}, {"text": "show the overlap between mismatched pairs for different models for subject 1.", "labels": [], "entities": []}, {"text": "In these plots, the red color corresponds to the first model mentioned in the caption, the blue colour corresponds to the second model and the purple colour indicates the overlaps.", "labels": [], "entities": []}, {"text": "While there is some overlap between the mistakes of the 25 features model and the experiential model, considerable number of mismatched pairs are not in common between them.", "labels": [], "entities": []}, {"text": "One interesting fact about the 25 features model is that for some specific nouns ie. \"bear\", \"foot\", \"chair\", and \"dresser\", no matter what is its pair, discrimination performance is poor. eg. \"bear\" is not only confused with other animals, but also with somebody parts, places and etc.", "labels": [], "entities": []}, {"text": "We do not notice similar phenomena for the experiential model.", "labels": [], "entities": []}, {"text": "This could be aside effect of using co-occurrence statistics from corpora to learn word representations and could show that for some reason the representations learned for these nouns are not distinguishable from other nouns.", "labels": [], "entities": []}, {"text": "Looking into the noun pair mismatches of the experiential model and the dependency based word2vec in, again we see a considerable amount of overlap.", "labels": [], "entities": []}, {"text": "They both perform equally for discriminating among animals.", "labels": [], "entities": []}, {"text": "But the experiential model makes more mistake about \"body parts\" and \"insects\".", "labels": [], "entities": []}, {"text": "Comparing the dependency based word2vec with simple word2vec, in we observe similar patterns to.", "labels": [], "entities": []}, {"text": "As illustrated in the plot, discriminating some words eg. \"chair\" is difficult for word2vec while it's not the case for dependency based word2vec.", "labels": [], "entities": []}, {"text": "It seems like both experiential attributes of nouns and the dependency information is helping in learning more distinguishable representations for nouns.", "labels": [], "entities": []}], "tableCaptions": []}