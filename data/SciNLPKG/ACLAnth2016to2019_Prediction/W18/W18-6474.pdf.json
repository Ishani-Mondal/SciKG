{"title": [{"text": "A hybrid pipeline of rules and machine learning to filter web-crawled parallel corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "A hybrid pipeline comprising rules and machine learning is used to filter a noisy web English-German parallel corpus for the Parallel Corpus Filtering task.", "labels": [], "entities": [{"text": "Parallel Corpus Filtering task", "start_pos": 125, "end_pos": 155, "type": "TASK", "confidence": 0.724448561668396}]}, {"text": "The core of the pipeline is a module based on the logistic regression algorithm that returns the probability that a translation unit is accepted.", "labels": [], "entities": []}, {"text": "The training set for the logistic regression is created by automatic annotation.", "labels": [], "entities": []}, {"text": "The quality of the automatic annotation is estimated by manually labeling the training set.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task \"Parallel Corpus Filtering\" presented a noisy web crawled parallel corpora (EnglishGerman) whose English side contains one billion words.", "labels": [], "entities": []}, {"text": "The participants had to select two \"clean\" subsets consisting of 10 million words and 100 million words, respectively.", "labels": [], "entities": []}, {"text": "The quality of the two subsets was determined by the BLEU score of a statistical machine translation (based on Moses) and a neural machine translation system (Marian) trained on these subsets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9992659687995911}, {"text": "statistical machine translation", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.6913945078849792}]}, {"text": "The BLEU scores were computed for multiple not disclosed sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9980447292327881}]}, {"text": "The parallel corpus filtering task bears similarity to translation memory cleaning task and Quality estimation task.", "labels": [], "entities": [{"text": "parallel corpus filtering", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.5903537472089132}, {"text": "translation memory cleaning task", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.882356584072113}, {"text": "Quality estimation", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7481625080108643}]}, {"text": "Some systems that spot false translation units in translation memories are surveyed in.", "labels": [], "entities": []}, {"text": "One of the most successful systems is trained not only on features related to translation quality, but also on features related to grammatical errors and features related to fluency and lexical choice.", "labels": [], "entities": []}, {"text": "Given the similarity between the translation memory cleaning task and this task we have adapted part of our system for cleaning the translation memories.", "labels": [], "entities": [{"text": "translation memory cleaning task", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.897278368473053}]}, {"text": "The system requires supervision and word alignment knowledge.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.8148714303970337}]}, {"text": "However, the \"Parallel Corpus Filtering\" task specifications restrict the usage of external parallel corpora and allow minimum alignment information.", "labels": [], "entities": [{"text": "Parallel Corpus Filtering\" task", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.6853700697422027}]}, {"text": "Therefore, we had to re-engineer the above mentioned system and produce a pipeline that respects the task requirements.", "labels": [], "entities": []}, {"text": "In the next section we present the re-engineered pipeline.", "labels": [], "entities": []}, {"text": "The section 3 shows an in-house evaluation and in the last section we draw the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have manually annotated the automatically annotated pairs used to train the logistic regression algorithm.", "labels": [], "entities": []}, {"text": "A non-native German language speaker has annotated this set with the label \"1\" if the translation unit is accurate and \"0\" otherwise.", "labels": [], "entities": []}, {"text": "Two examples of annotated translation units are given bellow.", "labels": [], "entities": []}, {"text": "\u2022 A correctly automatic annotated translation unit -In a nutshell: the usage of the machinery for sifting, to loosen and rasp, or to prepare powdery substances and hygroscopic materials.", "labels": [], "entities": []}, {"text": "-Kurz: \u00a8 Uberall zum maschinellen Passieren, Auflockern und Raspeln oder zum Aufbereiten pulverf\u00f6rmiger Massen und hygroskopischer Materialien.", "labels": [], "entities": [{"text": "\u00a8", "start_pos": 7, "end_pos": 8, "type": "METRIC", "confidence": 0.9807078242301941}]}, {"text": "\u2022 An incorrectly automatic annotated translation unit -Large swimming pool and gym, for those who want to combine open air and relaxing activities with indoor training -Die R\u00e4ume liegen direkt neben dem gro\u00dfen Pool und dem Fitnessraum, f\u00fcr all diejenigen die zu den vielz\u00e4hligen Outdoor-Aktivit\u00e4ten ein Trainigsprogramm in den Innenr\u00e4umen kombinieren m\u00f6chten.", "labels": [], "entities": []}, {"text": "In both examples the Hunalign score is higher than the fixed threshold but only the first example is correctly annotated automatically.", "labels": [], "entities": [{"text": "Hunalign score", "start_pos": 21, "end_pos": 35, "type": "METRIC", "confidence": 0.9842610061168671}]}, {"text": "The automatic annotator is a binary classifier and we can evaluate this classifier as is customary by comparing its annotation with a gold standard (the manual annotation).", "labels": [], "entities": []}, {"text": "As one can see from the confusion matrix in table 1 the training set is imbalanced with only 27 percent negative examples.", "labels": [], "entities": []}, {"text": "The precision, recall, F1-score and the balanced accuracy for the positive and negative classes are shown in  that the heuristic based on Hunalign threshold is a good one.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996813535690308}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9997490048408508}, {"text": "F1-score", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9996953010559082}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9987185001373291}, {"text": "Hunalign threshold", "start_pos": 138, "end_pos": 156, "type": "METRIC", "confidence": 0.9172112345695496}]}, {"text": "However, one should also consider that the automatically annotated set is not a representative sample of the test set provided by the organizers of the task.", "labels": [], "entities": []}, {"text": "To have a representative sample much more translation units should have been annotated.", "labels": [], "entities": []}, {"text": "The annotation errors are mitigated by the fact that the Logistic regression classifier trained on the automatically annotated set will return the probability of the positive class.", "labels": [], "entities": []}, {"text": "If the probability correlates with translation unit quality, then some translation units, even if not perfect, could be useful for training machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.7311238944530487}]}, {"text": "We counted some cases when the sentence in one language translates the sentence in the other language, but, at the same time, is more informative, as it contains another part for which there is no translation in the other language.", "labels": [], "entities": []}, {"text": "Another worth making remark is the existence of many Bible passages, at least in the set we have manually annotated.", "labels": [], "entities": []}, {"text": "They have lexical, morphological and syntactic characteristics which are specific to this kind of writing and which, when applied to other kinds of writing, will give inappropriate results.", "labels": [], "entities": []}, {"text": "Although accepted as useful for MT in this task, they are probably good only for translating similar kinds of texts (i.e., religious ones).", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9972217082977295}]}, {"text": "A much better evaluation is provided by the task organizers.", "labels": [], "entities": []}, {"text": "They have determined the quality of the cleaning performed by the teams by the BLEU score of a statistical machine translation (based on Moses) and a neural machine translation system (Marian) trained on two subsets as explained in the introduction section.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9994064569473267}]}, {"text": "There were 48 submissions and our system ranked in the range 22 -31 depending on the subset and machine translation system used in the evaluation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.6688947975635529}]}, {"text": "For details regarding shared task preparation, the official results table and a survey of the methods used by the partic-ipating systems one should consult ().", "labels": [], "entities": []}], "tableCaptions": []}