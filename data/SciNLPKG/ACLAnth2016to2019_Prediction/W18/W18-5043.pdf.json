{"title": [{"text": "Maximizing SLU Performance with Minimal Training Data Using Hybrid RNN Plus Rule-based Approach", "labels": [], "entities": [{"text": "Maximizing SLU", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.74697145819664}]}], "abstractContent": [{"text": "Spoken language understanding (SLU) by using recurrent neural networks (RNN) achieves good performances for large training data sets, but collecting large training datasets is a challenge, especially for new voice applications.", "labels": [], "entities": [{"text": "Spoken language understanding (SLU)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8656265139579773}]}, {"text": "Therefore, the purpose of this study is to maximize SLU performances, especially for small training data sets.", "labels": [], "entities": [{"text": "SLU", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9458249807357788}]}, {"text": "To this aim, we propose a novel CRF-based dialog act selector which chooses suitable dialog acts from outputs of RNN SLU and rule-based SLU.", "labels": [], "entities": []}, {"text": "We evaluate the selector by using DSTC2 corpus when RNN SLU is trained by less than 1,000 training sentences.", "labels": [], "entities": [{"text": "DSTC2 corpus", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.9612027108669281}, {"text": "RNN SLU", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.7611641585826874}]}, {"text": "The evaluation demonstrates the selector achieves Micro F1 better than both RNN and rule-based SLUs.", "labels": [], "entities": [{"text": "Micro", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.499824583530426}, {"text": "F1", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.6622238755226135}]}, {"text": "In addition, it shows the selector achieves better Macro F1 than RNN SLU and the same Macro F1 as rule-based SLU.", "labels": [], "entities": [{"text": "F1", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.8339874744415283}, {"text": "F1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.8088998198509216}]}, {"text": "Thus, we confirmed our method offers advantages in SLU performances for small training data sets.", "labels": [], "entities": [{"text": "SLU", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9503324031829834}]}], "introductionContent": [{"text": "Spoken language understanding (SLU) was further researched by using rule-based methods and machine learning (ML) (.", "labels": [], "entities": [{"text": "Spoken language understanding (SLU)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9219627877076467}]}, {"text": "ML achieves good SLU performances for large training data sets.", "labels": [], "entities": [{"text": "ML", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.5663467645645142}]}, {"text": "However, MLbased SLU with small training data results in poor performances.", "labels": [], "entities": [{"text": "MLbased SLU", "start_pos": 9, "end_pos": 20, "type": "DATASET", "confidence": 0.6841659843921661}]}, {"text": "Therefore, if we want to launch anew spoken dialog service as fast as possible, we cannot use ML-based SLUs as there is no time to prepare sufficient training data.", "labels": [], "entities": []}, {"text": "The goal of this study is to maximize SLU performances especially when the training data size is small.", "labels": [], "entities": [{"text": "SLU", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9365679621696472}]}, {"text": "To achieve this objective, we propose a selection method which chooses a suitable SLU either from rule-based or MLbased SLUs depending on SLU output reliability.", "labels": [], "entities": []}, {"text": "While researchers have studied selection methods to choose a suitable SLU result from plural SLUs by applying several algorithms (), most of them focused on selectors that improve SLU performances for large training data sets.", "labels": [], "entities": []}, {"text": "However, their selection methods did not take into account the impact on performance for different training data sizes, specifically, how a selector would work on small training data.", "labels": [], "entities": []}, {"text": "Previous studies have evaluated SLU performances by metrics such as Micro F1.", "labels": [], "entities": [{"text": "SLU", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.8692318201065063}, {"text": "Micro F1", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.7052730470895767}]}, {"text": "Nevertheless, performance evaluation by only Micro F1 is not suitable for practical dialog systems as these systems must recognize all dialog acts that users can say.", "labels": [], "entities": [{"text": "Micro F1", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.8635015487670898}]}, {"text": "In practical dialog systems, the distribution of dialog acts for actual user utterances is usually uneven.", "labels": [], "entities": []}, {"text": "On this scenario, even if SLU completely fails to recognize some rare dialog acts, the Micro F1 remains almost unchanged and that is the main reason why systems cannot exclusively rely on this metric.", "labels": [], "entities": [{"text": "Micro F1", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.6652815341949463}]}, {"text": "Macro F1 is another common major metric in SLU.", "labels": [], "entities": [{"text": "F1", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.7816629409790039}]}, {"text": "Macro F1 computes an averaged Micro F1 of all dialog acts and decreases drastically when it fails to recognize rare dialog acts.", "labels": [], "entities": [{"text": "Micro F1", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.6956875622272491}]}, {"text": "Thus, we evaluate Macro F1 as a better metric to confirm that a selector can recognize all dialog acts.", "labels": [], "entities": [{"text": "F1", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.7860124111175537}]}, {"text": "This paper brings the following contributions to the SLU subject.", "labels": [], "entities": [{"text": "SLU subject", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.7048444896936417}]}, {"text": "First, we propose a conditional random fields (CRF) based selector which chooses suitable SLU outputs either from rule-based or ML-based SLUs.", "labels": [], "entities": []}, {"text": "Second, we assess our selection method with different sizes of training data for recurrent neural network (RNN) based SLU.", "labels": [], "entities": []}, {"text": "Finally, unlike most of previous studies, we evaluate SLU ^ Steps of SLU after user utterance using semantic FST.: Rule-based SLU by using semantic finite state transducers.", "labels": [], "entities": []}, {"text": "<arb> is a symbol that accepts any word.", "labels": [], "entities": []}, {"text": "\u03b5 means no dialog acts are output.", "labels": [], "entities": []}, {"text": "performances by using not only Micro F1 but also Macro F1.", "labels": [], "entities": [{"text": "Micro F1", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.6440788060426712}]}, {"text": "Experiments validate our novel approach and demonstrate that the proposed selector produces better SLU performances (up to 10.1% Micro F1 and 19.2% Macro F1) than ML-based for small training data sets and achieves \"upper bound\" of SLU performances regardless of training data size.", "labels": [], "entities": [{"text": "Micro F1", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.6580915153026581}, {"text": "Macro F1", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.6070826053619385}]}, {"text": "This result confirms that our selector helps to improve ML-based SLU performance even if we utilize very limited training data.", "labels": [], "entities": [{"text": "ML-based SLU", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.767314225435257}]}], "datasetContent": [{"text": "We used a corpus from the Dialog State Tracking Challenge 2 (DSTC2), to evaluate our method ().", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge 2 (DSTC2)", "start_pos": 26, "end_pos": 67, "type": "TASK", "confidence": 0.6594273447990417}]}, {"text": "This corpus contains transcribed sentences of user utterances for restaurant reservation dialogs.", "labels": [], "entities": []}, {"text": "The sentences have sentence-level dialog acts.", "labels": [], "entities": []}, {"text": "From the sentencelevel dialog acts, we manually annotated wordlevel dialog acts.", "labels": [], "entities": []}, {"text": "The DSTC2 corpus has a training set of 11,677 sentences, a development set of 3,934, and a test set of 9,890.", "labels": [], "entities": [{"text": "DSTC2 corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9798156023025513}]}, {"text": "From the training set, we randomly chose sentences to create training sets with various sentence sizes (100-10,000).", "labels": [], "entities": []}, {"text": "Distribution of dialog acts in DSTC2 corpus is skewed; only 25% of dialog acts appeared in 90% of sentences for both training and test sets.", "labels": [], "entities": [{"text": "DSTC2 corpus", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.927031546831131}]}, {"text": "The DSTC2 corpus has an \"ontology\" which defines all dialog acts that user may say.", "labels": [], "entities": [{"text": "DSTC2 corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9650724232196808}]}, {"text": "This ontology defines 659 dialog acts.", "labels": [], "entities": []}, {"text": "649 dialog acts are defined in forms of intent(slot=value), e.g., inform(food=chinese), deny(area=west), and confirm(pricerange=cheap).", "labels": [], "entities": [{"text": "confirm", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9744241237640381}]}, {"text": "Other 10 dialog acts are defined by only intent, e.g., affirm(), negate(), and hello().", "labels": [], "entities": [{"text": "affirm", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9393995404243469}]}, {"text": "shows the configuration of GRU for RNN SLU.", "labels": [], "entities": [{"text": "GRU", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9651280045509338}, {"text": "RNN SLU", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.6826101541519165}]}, {"text": "The GRU receives an embedded word vector with 1-hot POS tag vector.", "labels": [], "entities": []}, {"text": "The embedding weights are initialized with normally distributed random numbers.", "labels": [], "entities": []}, {"text": "The hidden states of a GRU are converted to an output vector with dialog acts probabilities, by multiplying a linear matrix and softmax function.", "labels": [], "entities": []}, {"text": "The dimension of an output vector is 538 (537 acts and \"no act\" class) because the largest training set (10k sentences) contains only 537 dialog acts.", "labels": [], "entities": []}, {"text": "The hyper parameters for RNN is determined based on SLU performance in the development set.", "labels": [], "entities": [{"text": "RNN", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.6637995839118958}]}, {"text": "We terminate RNN training when Micro F1 on the development set is maximized.", "labels": [], "entities": [{"text": "Micro", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.8689037561416626}, {"text": "F1", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.41087132692337036}]}], "tableCaptions": [{"text": " Table 1: Parameters of GRU RNN.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9132101535797119}, {"text": "GRU RNN", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.8287433087825775}]}]}