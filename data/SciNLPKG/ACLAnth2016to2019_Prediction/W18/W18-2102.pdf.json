{"title": [{"text": "Lightweight Word-Level Confidence Estimation for Neural Interactive Translation Prediction", "labels": [], "entities": [{"text": "Word-Level Confidence Estimation", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.5855986376603445}, {"text": "Neural Interactive Translation Prediction", "start_pos": 49, "end_pos": 90, "type": "TASK", "confidence": 0.7339223995804787}]}], "abstractContent": [{"text": "In neural interactive translation prediction, a system provides translation suggestions (\"auto-complete\" functionality) for human translators.", "labels": [], "entities": [{"text": "neural interactive translation prediction", "start_pos": 3, "end_pos": 44, "type": "TASK", "confidence": 0.7041984498500824}]}, {"text": "These translation suggestions maybe rejected by the translator in predictable ways; being able to estimate confidence in the quality of translation suggestions could be useful in providing additional information for users of the system.", "labels": [], "entities": []}, {"text": "We show that a very small set of features (which are already generated as byprod-ucts of the process of translation prediction) can be used in a simple model to estimate confidence for interactive translation prediction.", "labels": [], "entities": [{"text": "translation prediction", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.9685204327106476}, {"text": "interactive translation prediction", "start_pos": 185, "end_pos": 219, "type": "TASK", "confidence": 0.6635114053885142}]}], "introductionContent": [{"text": "In neural interactive translation prediction, a human translator interacts with machine translation output by accepting or rejecting suggestions as they type a translation from beginning to end.", "labels": [], "entities": [{"text": "neural interactive translation prediction", "start_pos": 3, "end_pos": 44, "type": "TASK", "confidence": 0.7132461220026016}]}, {"text": "By accepting a system suggestion, the translator implicitly provides an \"OK\" quality label for that token.", "labels": [], "entities": [{"text": "OK\" quality label", "start_pos": 73, "end_pos": 90, "type": "METRIC", "confidence": 0.8397126197814941}]}, {"text": "Similarly, by rejecting a suggestion (and providing a correction), they implicitly provide a \"BAD\" quality label for the system's suggestion.", "labels": [], "entities": [{"text": "BAD\" quality label", "start_pos": 94, "end_pos": 112, "type": "METRIC", "confidence": 0.9517257660627365}]}, {"text": "The system's suggestions maybe wrong (\"BAD\") in predictable ways.", "labels": [], "entities": [{"text": "BAD", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9975001215934753}]}, {"text": "For example, if one suggestion is incorrect, the subsequent suggestion may then be more likely to be incorrect.", "labels": [], "entities": []}, {"text": "We seek to show that using these implicit labels and model scores we can predict whether subsequent tokens will be accepted as \"OK\" or rejected as \"BAD\" by the translator.", "labels": [], "entities": [{"text": "BAD", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.9863246083259583}]}, {"text": "This confidence estimation has a twofold purpose.", "labels": [], "entities": []}, {"text": "First, if we can detect potentially \"BAD\" tokens before showing them to the translator, we maybe able to increase translator trust in suggestions and reduce time spent reading incorrect suggestions, either by indicating confidence (by color, shading, or some other visual indication), providing multiple alternate translation options, or by simply not showing low-confidence predictions to the user.", "labels": [], "entities": []}, {"text": "Second, if we can identify \"BAD\" tokens, we can save on computation.", "labels": [], "entities": []}, {"text": "If we are confident that a prediction is wrong, we can wait to predict subsequent tokens until the human translator provides a correction rather than completing a translation that is likely to be rejected.", "labels": [], "entities": []}, {"text": "Computer aided translation (CAT) tools such as Lilt 1 or CASMACAT 2 typically provide the translator with either full sentence predictions or predictions consisting of several tokens, which need to be recomputed each time the system is found to have made an erroneous prediction.", "labels": [], "entities": [{"text": "Computer aided translation (CAT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7799175282319387}]}, {"text": "Speed is of the essence in interactive translation prediction; predictions (of several tokens or a full sentence) must be computed quickly enough that the translator does not experience lag in the user interface.", "labels": [], "entities": [{"text": "Speed", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9384741187095642}, {"text": "interactive translation prediction", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.6899504462877909}]}, {"text": "For this reason, we focus on confidence estimation using a very small set of features that can be collected naturally in the process of the interactive translation prediction computation.", "labels": [], "entities": [{"text": "translation prediction computation", "start_pos": 152, "end_pos": 186, "type": "TASK", "confidence": 0.8922499020894369}]}, {"text": "We present results based on a simulation using reference text.", "labels": [], "entities": []}], "datasetContent": [{"text": "In addition to using thresholding or simple heuristics with the features, we train logistic regression classifiers with scikit-learn (Pedregosa et al., 2011) on the WMT 2016 data set, using class weighting (with a weight of 2 on \"BAD\").", "labels": [], "entities": [{"text": "WMT 2016 data set", "start_pos": 165, "end_pos": 182, "type": "DATASET", "confidence": 0.9813733845949173}]}, {"text": "All other parameters are set to defaults, including the threshold.", "labels": [], "entities": []}, {"text": "We report results on the WMT 2017 test sets in.", "labels": [], "entities": [{"text": "WMT 2017 test sets", "start_pos": 25, "end_pos": 43, "type": "DATASET", "confidence": 0.9577870815992355}]}, {"text": "We find that the Current Token Model Score feature drastically outperforms all other features when thresholded, obtaining the best results in terms of F 1 -BAD on train and test data.", "labels": [], "entities": [{"text": "F 1 -BAD", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9451389759778976}]}, {"text": "The logistic regression model that includes it and all other features shows slight improvements in terms of F 1 -mult (at the cost of slight losses to F 1 -BAD).", "labels": [], "entities": [{"text": "F 1 -mult", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9662188589572906}, {"text": "F 1 -BAD", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9263484179973602}]}, {"text": "If we restrict ourselves to the features available before the new token is predicted, we find that the logistic regression model (without the Current Token Model Score) outperforms baselines in terms of F 1 -BAD and the threshold score difference baseline in terms of F 1 -mult on the en-cs and: Results on WMT 2017 test data.", "labels": [], "entities": [{"text": "Current Token Model Score", "start_pos": 142, "end_pos": 167, "type": "DATASET", "confidence": 0.8233771175146103}, {"text": "F 1 -BAD", "start_pos": 203, "end_pos": 211, "type": "METRIC", "confidence": 0.9490808844566345}, {"text": "F 1 -mult", "start_pos": 268, "end_pos": 277, "type": "METRIC", "confidence": 0.9151047319173813}, {"text": "WMT 2017 test data", "start_pos": 307, "end_pos": 325, "type": "DATASET", "confidence": 0.9650455713272095}]}, {"text": "We show baselines and models built with and without the Current Token Model Score.", "labels": [], "entities": [{"text": "Current Token Model Score", "start_pos": 56, "end_pos": 81, "type": "DATASET", "confidence": 0.8384276181459427}]}, {"text": "The first value is F 1 -BAD, and the value in parentheses is F 1 -mult.", "labels": [], "entities": [{"text": "F 1 -BAD", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.8958266079425812}, {"text": "F 1 -mult", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9186277389526367}]}, {"text": "For the en-de and de-en data, we find that it outperforms the threshold score difference baseline in terms of F 1 -mult and the correctness of previous prediction baseline in terms of F 1 -BAD.", "labels": [], "entities": [{"text": "threshold score difference baseline", "start_pos": 62, "end_pos": 97, "type": "METRIC", "confidence": 0.8954576849937439}, {"text": "F 1 -mult", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9554390162229538}, {"text": "F 1 -BAD", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9479664117097855}]}], "tableCaptions": [{"text": " Table 1: Word prediction accuracy (WPA) of neu- ral interactive translation prediction with beam  size 1 and BLEU score for standard neural ma- chine translation decoding with beam size 1 on  WMT 2017 test set.", "labels": [], "entities": [{"text": "Word prediction", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6598634421825409}, {"text": "accuracy (WPA)", "start_pos": 26, "end_pos": 40, "type": "METRIC", "confidence": 0.89387546479702}, {"text": "neu- ral interactive translation prediction", "start_pos": 44, "end_pos": 87, "type": "TASK", "confidence": 0.5623847842216492}, {"text": "BLEU score", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.9816411435604095}, {"text": "WMT 2017 test set", "start_pos": 193, "end_pos": 210, "type": "DATASET", "confidence": 0.9781120866537094}]}]}