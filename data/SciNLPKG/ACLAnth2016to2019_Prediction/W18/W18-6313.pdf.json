{"title": [{"text": "Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation", "labels": [], "entities": [{"text": "Analyze Domain Adaptation", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.62335205078125}, {"text": "Neural Machine Translation", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.642362376054128}]}], "abstractContent": [{"text": "To better understand the effectiveness of continued training, we analyze the major components of a neural machine translation system (the encoder, decoder, and each embedding space) and consider each component's contribution to, and capacity for, domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 247, "end_pos": 264, "type": "TASK", "confidence": 0.7359894216060638}]}, {"text": "We find that freezing any single component during continued training has minimal impact on performance, and that performance is surprisingly good when a single component is adapted while holding the rest of the model fixed.", "labels": [], "entities": []}, {"text": "We also find that continued training does not move the model very far from the out-of-domain model, compared to a sensitivity analysis metric, suggesting that the out-of-domain model can provide a good generic ini-tialization for the new domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) has supplanted Phrase-Based Machine Translation (PBMT) as the standard for high-resource machine translation.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7639090170462927}, {"text": "Phrase-Based Machine Translation (PBMT)", "start_pos": 48, "end_pos": 87, "type": "TASK", "confidence": 0.795456459124883}, {"text": "high-resource machine translation", "start_pos": 108, "end_pos": 141, "type": "TASK", "confidence": 0.6963555614153544}]}, {"text": "This has necessitated new domain adaptation methods, because PBMT adaptation methods primarily rely on adapting the language model and phrase table using interpolation or back-off schemes (see \u00a72).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7575854659080505}, {"text": "PBMT adaptation", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.9092869162559509}]}, {"text": "Continued training (, also referred to as fine-tuning, is one of the most popular methods for NMT adaptation, due to its strong performance.", "labels": [], "entities": [{"text": "NMT adaptation", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.9846296310424805}]}, {"text": "In contrast to the PBMT literature, little research has focused on why continued training is effective or on what happens to NMT models during continued training.", "labels": [], "entities": []}, {"text": "Motivated by domain adaptation analysis in PBMT (, this work proposes a simple freezing subnetworks technique and uses it to gain insight into how the various components of an NMT system behave during continued training.", "labels": [], "entities": [{"text": "domain adaptation analysis", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.8175220688184103}, {"text": "PBMT", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8206902146339417}]}, {"text": "We segment the model into five subnetworks, which we refer to as components, denoted in Figure 1: the source embeddings, encoder, decoder (which includes the attention mechanism), the softmax (used to denote the decoder output embeddings and biases), and the target embeddings.", "labels": [], "entities": []}, {"text": "We freeze components one at a time during continued training to see how much the adaptation depends on each component.", "labels": [], "entities": []}, {"text": "We also experiment with freezing everything except one component to determine each component's capacity to adapt to the new domain on its own.", "labels": [], "entities": []}, {"text": "In order to further analyze continued training, we examine the magnitude of change in model components during continued training of the network, under both normal and freezing training conditions.", "labels": [], "entities": []}, {"text": "We also conduct sensitivity analysis of each component to assist in interpreting these magnitudes.", "labels": [], "entities": []}, {"text": "Our NMT adaptation experiments are performed across three languages: we translate from German,", "labels": [], "entities": [{"text": "NMT adaptation", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.954413115978241}]}], "datasetContent": [{"text": "For all language pairs, we train systems on the out-of-domain data and select the best model parameters based on perplexity on the out-of-domain development set.", "labels": [], "entities": []}, {"text": "We then adapt the systems into our smaller, in-domain training sets.", "labels": [], "entities": []}, {"text": "We select the best model based on the WIPO development set perplexity and report results on the WIPO test sets.", "labels": [], "entities": [{"text": "WIPO development set", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.8746083378791809}, {"text": "WIPO test sets", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.977155844370524}]}], "tableCaptions": [{"text": " Table 1.  While training the out-of-domain models, we  apply dropout with 10% probability on the RNN  layers. We apply label smoothing of 0.1. We use  ADAM (", "labels": [], "entities": [{"text": "ADAM", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.646363377571106}]}, {"text": " Table 4: Euclidean distance moved by each compo- nent when components are adapted jointly.", "labels": [], "entities": []}, {"text": " Table 5: Euclidean distance moved by each compo- nent when components are adapted individually.", "labels": [], "entities": []}]}