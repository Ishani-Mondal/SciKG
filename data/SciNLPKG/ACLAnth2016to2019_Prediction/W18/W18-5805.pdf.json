{"title": [{"text": "String Transduction with Target Language Models and Insertion Handling", "labels": [], "entities": [{"text": "String Transduction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7475912868976593}]}], "abstractContent": [{"text": "Many character-level tasks can be framed as sequence-to-sequence transduction, where the target is a word from a natural language.", "labels": [], "entities": []}, {"text": "We show that leveraging target language models derived from unannotated target corpora, combined with a precise alignment of the training data, yields state-of-the art results on cognate projection, inflection generation, and phoneme-to-grapheme conversion.", "labels": [], "entities": [{"text": "cognate projection", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.737939864397049}, {"text": "inflection generation", "start_pos": 199, "end_pos": 220, "type": "TASK", "confidence": 0.7264370620250702}, {"text": "phoneme-to-grapheme conversion", "start_pos": 226, "end_pos": 256, "type": "TASK", "confidence": 0.7455651760101318}]}], "introductionContent": [{"text": "Many natural language tasks, particularly those involving character-level operations, can be viewed as sequence-to-sequence transduction).", "labels": [], "entities": []}, {"text": "Although these tasks are often addressed in isolation, they share a common objective -in each case, the output is a word in the target language.", "labels": [], "entities": []}, {"text": "The hypothesis that we investigate in this paper is that a single task-and language-independent system can achieve state-of-the-art results by leveraging unannotated target language corpora that contain thousands of valid target word types.", "labels": [], "entities": []}, {"text": "We focus on low-data scenarios, which present a challenge to neural sequence-to-sequence models because sufficiently large parallel datasets are often difficult to obtain.", "labels": [], "entities": []}, {"text": "To reinforce transduction models trained on modest-sized collections of source-target pairs, we leverage monolingual text corpora that are freely available for hundreds of languages.", "labels": [], "entities": []}, {"text": "Our approach is based on discriminative string transduction, where a learning algorithm assigns weights to features defined on aligned source and target pairs.", "labels": [], "entities": []}, {"text": "At test time, an input sequence is converted into the highest-scoring output sequence.", "labels": [], "entities": []}, {"text": "Advantages of discriminative transduction include an aptitude to derive effective models from small training sets, as wells as the capability to incorporate diverse sets of features.", "labels": [], "entities": []}, {"text": "Specifically, we build Sequence-to-sequence Transduction: Illustration of four character-level sequenceto-sequence prediction tasks.", "labels": [], "entities": [{"text": "Sequence-to-sequence Transduction", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.9343163669109344}, {"text": "character-level sequenceto-sequence prediction", "start_pos": 79, "end_pos": 125, "type": "TASK", "confidence": 0.6434973776340485}]}, {"text": "In each case, the output is a word in the target language.", "labels": [], "entities": []}, {"text": "upon DIRECTL+ (), a string transduction tool which was originally designed for grapheme-to-phoneme conversion.", "labels": [], "entities": [{"text": "grapheme-to-phoneme conversion", "start_pos": 79, "end_pos": 109, "type": "TASK", "confidence": 0.7505472898483276}]}, {"text": "We present anew system, DTLM, that combines discriminative transduction with character and word language models (LMs) derived from large unannotated corpora.", "labels": [], "entities": []}, {"text": "Target language modeling is particularly important in low-data scenarios, where the limited transduction models often produce many ill-formed output candidates.", "labels": [], "entities": [{"text": "Target language modeling", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7588071823120117}]}, {"text": "We avoid the error propagation problem which is inherent in pipeline approaches by incorporating the LM feature sets directly into the transducer.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.6794600933790207}]}, {"text": "In addition, we bolster the quality of transduction by employing a novel alignment method, which we refer to as precision alignment.", "labels": [], "entities": [{"text": "precision alignment", "start_pos": 112, "end_pos": 131, "type": "METRIC", "confidence": 0.8241938352584839}]}, {"text": "The idea is to allow null substrings (nulls) on the source side during the alignment of the training data, and then apply a separate aggregation algorithm to merge them nulls with adjacent non-empty substrings.", "labels": [], "entities": []}, {"text": "This method yields precise many-to-many alignment links that lead to improved transduction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9763913750648499}]}, {"text": "The contributions of this paper include the following.", "labels": [], "entities": []}, {"text": "(1) A novel method of incorporating strong target language models directly into discriminative transduction.", "labels": [], "entities": []}, {"text": "(2) A novel approach to unsupervised alignment that is particularly beneficial in low-resource settings.", "labels": [], "entities": [{"text": "unsupervised alignment", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.5998934805393219}]}, {"text": "(3) An extensive experimental comparison to previous models on multiple tasks and languages, which includes state-of-the-art results on inflection generation, cognate projection, and phonemeto-grapheme generation.", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 136, "end_pos": 157, "type": "TASK", "confidence": 0.7659299969673157}, {"text": "cognate projection", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.7295503914356232}, {"text": "phonemeto-grapheme generation", "start_pos": 183, "end_pos": 212, "type": "TASK", "confidence": 0.7578482925891876}]}, {"text": "(4) Publicly available implementation of the proposed methods.", "labels": [], "entities": []}, {"text": "Three new datasets for cognate projection.", "labels": [], "entities": [{"text": "cognate projection", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7419620156288147}]}], "datasetContent": [{"text": "In this section, we present the results of our experiments on four different character-level sequenceto-sequence tasks: transliteration, inflection generation, cognate projection, and phoneme-tographeme conversion (P2G).", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 137, "end_pos": 158, "type": "TASK", "confidence": 0.7439154088497162}, {"text": "cognate projection", "start_pos": 160, "end_pos": 178, "type": "TASK", "confidence": 0.716353252530098}, {"text": "phoneme-tographeme conversion", "start_pos": 184, "end_pos": 213, "type": "TASK", "confidence": 0.7310685813426971}]}, {"text": "In order to demonstrate the generality of our approach, the experiments involve multiple systems and datasets, in both low-data and high-data scenarios.", "labels": [], "entities": []}, {"text": "Where low-data resources do not already exist, we simulate a low-data environment by sampling an existing larger training set.", "labels": [], "entities": []}, {"text": "Low-data training sets consist of 100 training examples, 1000 development examples, and 1000 held-out examples, except for cognate projection, where we limit the development set to 100 training examples, and the held-out set to the remaining examples.", "labels": [], "entities": []}, {"text": "An output is considered correct if it exactly matches any of the targets in the reference data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word-level accuracy on transliteration (in %)  with 100 training instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9654825925827026}]}, {"text": " Table 2: Word-level accuracy on transliteration (in %)  with complete training sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.966624915599823}]}, {"text": " Table 3: Word-level accuracy (in %) on inflection gen- eration with 100 training instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9738222360610962}, {"text": "inflection gen- eration", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.6285639256238937}]}, {"text": " Table 4: Word-level accuracy (in %) on cognate pro- jection with 100 training instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9690659642219543}]}, {"text": " Table 5: Word-level accuracy (in %) on large-scale  cognate projection.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9680559039115906}, {"text": "large-scale  cognate projection", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.7295314272244772}]}, {"text": " Table 6: Word-level accuracy (in %) on phoneme-to- grapheme conversion with 100 training instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9787737727165222}, {"text": "phoneme-to- grapheme conversion", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.6687219738960266}]}, {"text": " Table 7: Word-level accuracy (in %) on phoneme-to- grapheme conversion with large training sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9776572585105896}, {"text": "phoneme-to- grapheme conversion", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.679488331079483}]}, {"text": " Table 8: Ablation test on P2G data with 100 training  instances.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9855600595474243}, {"text": "P2G data", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.7309604585170746}]}]}