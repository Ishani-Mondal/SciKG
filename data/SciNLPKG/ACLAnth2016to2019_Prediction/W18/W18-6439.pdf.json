{"title": [{"text": "The MeMAD Submission to the WMT18 Multimodal Translation Task", "labels": [], "entities": [{"text": "MeMAD Submission", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.6474256813526154}, {"text": "WMT18 Multimodal Translation", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6923850576082865}]}], "abstractContent": [{"text": "This paper describes the MeMAD project entry to the WMT Multimodal Machine Translation Shared Task.", "labels": [], "entities": [{"text": "WMT Multimodal Machine Translation Shared Task", "start_pos": 52, "end_pos": 98, "type": "TASK", "confidence": 0.7925421297550201}]}, {"text": "We propose adapting the Transformer neu-ral machine translation (NMT) architecture to a multi-modal setting.", "labels": [], "entities": [{"text": "Transformer neu-ral machine translation (NMT)", "start_pos": 24, "end_pos": 69, "type": "TASK", "confidence": 0.8170539140701294}]}, {"text": "In this paper , we also describe the preliminary experiments with text-only translation systems leading us up to this choice.", "labels": [], "entities": [{"text": "text-only translation", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.6457249373197556}]}, {"text": "We have the top scoring system for both English-to-German and English-to-French, according to the automatic metrics for flickr18.", "labels": [], "entities": [{"text": "flickr18", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.7507779002189636}]}, {"text": "Our experiments show that the effect of the visual features in our system is small.", "labels": [], "entities": []}, {"text": "Our largest gains come from the quality of the underlying text-only NMT system.", "labels": [], "entities": []}, {"text": "We find that appropriate use of additional data is effective.", "labels": [], "entities": []}], "introductionContent": [{"text": "In multi-modal translation, the task is to translate from a source sentence and the image that it describes, into a target sentence in another language.", "labels": [], "entities": [{"text": "multi-modal translation", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.7308711111545563}]}, {"text": "As both automatic image captioning systems and crowd captioning efforts tend to mainly yield descriptions in English, multi-modal translation can be useful for generating descriptions of images for languages other than English.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.746024876832962}, {"text": "crowd captioning", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7297240197658539}]}, {"text": "In the MeMAD project 1 , multi-modal translation is of interest for creating textual versions or descriptions of audio-visual content.", "labels": [], "entities": [{"text": "multi-modal translation", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7299984693527222}]}, {"text": "Conversion to text enables both indexing for multi-lingual image and video search, and increased access  to the audio-visual materials for visually impaired users.", "labels": [], "entities": [{"text": "multi-lingual image and video search", "start_pos": 45, "end_pos": 81, "type": "TASK", "confidence": 0.6035929203033448}]}, {"text": "We adapt 2 the Transformer ( architecture to use global image features extracted from Detectron, a pre-trained object detection and localization neural network.", "labels": [], "entities": [{"text": "object detection and localization neural", "start_pos": 111, "end_pos": 151, "type": "TASK", "confidence": 0.7967109918594361}]}, {"text": "We use two additional training corpora: MS-COCO () and OpenSubtitles2018.", "labels": [], "entities": [{"text": "MS-COCO", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.8530759215354919}, {"text": "OpenSubtitles2018", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.9083989262580872}]}, {"text": "MS-COCO is multi-modal, but not multi-lingual.", "labels": [], "entities": []}, {"text": "We extended it to a synthetic multi-modal and multilingual training set.", "labels": [], "entities": []}, {"text": "OpenSubtitles is multilingual, but does not include associated images, and was used as text-only training data.", "labels": [], "entities": [{"text": "OpenSubtitles", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9288462400436401}]}, {"text": "This places our entry in the unconstrained category of the WMT shared task.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.8579075932502747}]}, {"text": "Details on the architecture used in this work can be found in Section 4..", "labels": [], "entities": []}, {"text": "Further details on the synthetic data are presented in Section 2.", "labels": [], "entities": []}, {"text": "Data sets are summarized in.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our first aim was to select the text-based MT system to base our multi-modal extensions on.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9308514595031738}]}, {"text": "We tried a wide range of models, but only include results with the two strongest systems: Marian NMT with the amun model (, and OpenNMT () with the Transformer model.", "labels": [], "entities": [{"text": "Marian NMT", "start_pos": 90, "end_pos": 100, "type": "DATASET", "confidence": 0.9114658534526825}]}, {"text": "We also studied the effect of additional training data.", "labels": [], "entities": []}, {"text": "Our initial experiments showed that movie subtitles and their translations work rather well to augment the given training data.", "labels": [], "entities": []}, {"text": "Therefore, we included parallel subtitles from the OpenSubtitles2018 corpus to train better text-only MT models.", "labels": [], "entities": [{"text": "OpenSubtitles2018 corpus", "start_pos": 51, "end_pos": 75, "type": "DATASET", "confidence": 0.9321449100971222}, {"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.8901945948600769}]}, {"text": "For these experiments, we apply the Marian amun model, an attentional encoder-decoder model with bidirectional LSTM's on the encoder side.", "labels": [], "entities": []}, {"text": "In our first series of experiments, we observed that domain-tuning is very important when using Marian.", "labels": [], "entities": []}, {"text": "The domain-tuning was accomplished by a second training step on in-domain data after training the model on the entire data set.", "labels": [], "entities": []}, {"text": "shows the scores on development data.", "labels": [], "entities": []}, {"text": "We also tried decoding with an ensemble of three independent runs, which also pushed the performance a bit.", "labels": [], "entities": []}, {"text": "Furthermore, we tried to artificially increase the amount of in-domain data by translating existing English image captions to German and French.", "labels": [], "entities": []}, {"text": "For this purpose, we used the large MS-COCO data set with its 100,000 images that have five image captions each.", "labels": [], "entities": [{"text": "MS-COCO data set", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.9637909928957621}]}, {"text": "We used our best multidomain model (see) to translate all of those captions and used them as additional training data.", "labels": [], "entities": []}, {"text": "This procedure also transfers the knowledge learned by the multidomain model into the caption translations, which helps us to improve the coverage of the system with less out-of-domain data.", "labels": [], "entities": []}, {"text": "Hence, we filtered the large collection of translated movie subtitles to a smaller portion of reliable sentence pairs (one million in the experiment we report) and could train on a smaller data set with better results.", "labels": [], "entities": []}, {"text": "We experimented with two filtering methods.", "labels": [], "entities": []}, {"text": "Initially, we implemented a basic heuristic filter (subs H ), and later we improved on this with a language model filter (subs LM ).", "labels": [], "entities": []}, {"text": "Both procedures consider each sentence pair, assign it a quality score, and then select the highest scoring 1, 3, or 6 million pairs, discarding the rest.", "labels": [], "entities": []}, {"text": "The subs H method counts terminal punctuation) in the source and target sentences, initializing the score as the negative of the absolute value of the difference between these counts.", "labels": [], "entities": []}, {"text": "Afterwards, it further decrements the score by 1 for each occurrence of terminal punctuation beyond the first in each of the sentences.", "labels": [], "entities": []}, {"text": "The subs LM method first preprocesses the data by filtering samples by length and ratio of lengths, applying a rulebased noise filter, removing all characters not present in the Multi30k set, and deduplicating samples.", "labels": [], "entities": [{"text": "Multi30k set", "start_pos": 178, "end_pos": 190, "type": "DATASET", "confidence": 0.9266814291477203}]}, {"text": "Afterwards, target sentences in the remaining pairs are scored using a characterbased deep LSTM language model trained on the Multi30k data.", "labels": [], "entities": [{"text": "Multi30k data", "start_pos": 126, "end_pos": 139, "type": "DATASET", "confidence": 0.9436427652835846}]}, {"text": "Both selection procedures are intended for noise filtering, and subs LM additionally acts as domain adaptation.", "labels": [], "entities": [{"text": "noise filtering", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7229512333869934}, {"text": "domain adaptation", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7158010751008987}]}, {"text": "Table 3 lists the scores we obtained on development data.", "labels": [], "entities": []}, {"text": "To make a distinction between automatically translated captions, subtitle translations and human-translated image captions, we also introduced domain labels that we added as special tokens to the beginning of the input sequence.", "labels": [], "entities": []}, {"text": "In this way, the model can use explicit information about the domain when deciding how to translate given input.", "labels": [], "entities": []}, {"text": "However, the effect of such labels is not consistent between systems.", "labels": [], "entities": []}, {"text": "For Marian amun, the effect is negligible as we can see in.", "labels": [], "entities": []}, {"text": "For the Transformer, domain labels had little effect on BLEU but were clearly beneficial according to chrF-1.0.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9916098713874817}]}, {"text": "Our first attempt to add multi-modal information to the translation model includes the  We tried two models for the integration of those captions: (1) a dual attention multisource model that adds another input sequence with its own decoder attention and (2) a concatenation model that adds auto captions at the end of the original input string separated by a special token.", "labels": [], "entities": []}, {"text": "In the second model, attention takes care of learning how to use the additional information and previous work has shown that this, indeed, is possible (.", "labels": [], "entities": []}, {"text": "For both models, we applied Marian NMT that already includes a working implementation of dual attention translations.", "labels": [], "entities": [{"text": "Marian NMT", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.9019647240638733}, {"text": "dual attention translations", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.6412679453690847}]}, {"text": "summarizes the scores on the three development test sets for English-French and English-German.", "labels": [], "entities": []}, {"text": "We can see that the dual attention model does notwork at all and the scores slightly drop.", "labels": [], "entities": []}, {"text": "The concatenation approach works better probably because the common attention model learns interactions between the different types of input.", "labels": [], "entities": []}, {"text": "However, the improvements are small if any and the model basically learns to ignore the auto captions, which are often very different from the original input.", "labels": [], "entities": []}, {"text": "The attention pattern in the example of shows one of the very rare cases where we observe at least some attention to the automatic captions.: Attention layer visualization for an example whereat least one of the attention weights for the last part of the sentence, which corresponds to the automatically generated captions, obtains a value above 0.3  One benefit of NMT, in addition to its strong performance, is its flexibility in enabling different information sources to be merged.", "labels": [], "entities": []}, {"text": "Different strategies to include image features both on the encoder and decoder side have been explored.", "labels": [], "entities": []}, {"text": "We are inspired by the recent success of the Transformer architecture to adapt some of these strategies for use with the Transformer.", "labels": [], "entities": []}, {"text": "Recurrent neural networks start their processing from some initial hidden state.", "labels": [], "entities": []}, {"text": "Normally, a zero vector or a learned parameter vector is used, but the initial hidden state is also a natural location to introduce additional context e.g. from other modalities.", "labels": [], "entities": []}, {"text": "Initializing can be applied in either the encoder (IMG E ) or decoder (IMG D ) ().", "labels": [], "entities": []}, {"text": "These approaches are not directly applicable to the Transformer, as it is not a recurrent model, and lacks a comparable initial hidden state.", "labels": [], "entities": []}, {"text": "Double attention is another popular choice, used by e.g..", "labels": [], "entities": []}, {"text": "In this approach, two attention mechanisms are used, one for each modality.", "labels": [], "entities": []}, {"text": "The attentions can be separate or hierarchical.", "labels": [], "entities": []}, {"text": "While it would be possible to use double attention with the Transformer, we did not explore it in this work.", "labels": [], "entities": []}, {"text": "The multiple multi-head attention mechanisms in the Transformer leave open many challenges in how this integration would be done.", "labels": [], "entities": []}, {"text": "Multi-task learning has also been used, e.g. in the Imagination model), where the auxiliary task consists of reconstructing the visual features from the source encoding.", "labels": [], "entities": []}, {"text": "Imagination could also have been used with the Transformer, but we did not explore it in this work.", "labels": [], "entities": []}, {"text": "The source sequence itself is also a possible location for including the visual information.", "labels": [], "entities": []}, {"text": "In the IMG W approach, the visual features are encoded as a pseudo-word embedding concatenated to the word embeddings of the source sentence.", "labels": [], "entities": [{"text": "IMG W", "start_pos": 7, "end_pos": 12, "type": "TASK", "confidence": 0.6702176928520203}]}, {"text": "When the encoder is a bidirectional recurrent network, as in, it is beneficial to add the pseudo-word both at the beginning and the end to make it available for both encoder directions.", "labels": [], "entities": []}, {"text": "This is unnecessary in the Transformer, as it has equal access to all parts of the source in the deeper layers of the encoder.", "labels": [], "entities": []}, {"text": "Therefore, we add the pseudo-word only to the beginning of the sequence.", "labels": [], "entities": []}, {"text": "We use an affine projection of the image features V \u2208 R 80 into a pseudo-word embedding x I \u2208 R 512 In the LIUM trg-mul, the target embeddings and visual features are interacted through elementwise multiplication.", "labels": [], "entities": []}, {"text": "y \u2032 j = y j \u2299 tanh(W dec mul \u00b7 V ) Our initial gating approach resembles trg-mul.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Adding subtitle data and domain tuning  for image caption translation (BLEU% scores). All  results with Marian Amun.", "labels": [], "entities": [{"text": "image caption translation", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.814533660809199}, {"text": "BLEU% scores", "start_pos": 81, "end_pos": 93, "type": "METRIC", "confidence": 0.9729540546735128}]}, {"text": " Table 3: Using automatically translated image  captions and domain labels (BLEU% scores). A is  short for Amun, T for Transformer.", "labels": [], "entities": [{"text": "BLEU% scores", "start_pos": 76, "end_pos": 88, "type": "METRIC", "confidence": 0.9674892822901408}, {"text": "A", "start_pos": 91, "end_pos": 92, "type": "METRIC", "confidence": 0.9629974365234375}]}, {"text": " Table 4: Adding automatic image captions (only  the best one or all 5). The table shows BLEU  scores in %. All results with Marian Amun.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.998616099357605}]}, {"text": " Table 5:  Comparison of strategies for in- tegrating visual information (BLEU% scores).  All results using Transformer, Multi30k+MS- COCO+subs3M LM , Detectron mask surface, and  domain labeling.", "labels": [], "entities": [{"text": "BLEU% scores", "start_pos": 74, "end_pos": 86, "type": "METRIC", "confidence": 0.9720794359842936}, {"text": "domain labeling", "start_pos": 180, "end_pos": 195, "type": "TASK", "confidence": 0.7045364826917648}]}, {"text": " Table 6: Ablation experiments (BLEU% scores).  The row subs3M LM detectron shows our best sin- gle model. Individual components or data choices  are varied one by one. + stands for adding a com- ponent, and \u2212 for removing a component or data  set. Multiple modifications are indicated by in- creasing the indentation.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9956049919128418}, {"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.999004065990448}]}]}