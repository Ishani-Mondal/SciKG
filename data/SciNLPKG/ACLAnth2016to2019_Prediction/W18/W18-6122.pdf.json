{"title": [{"text": "Learning to Define Terms in the Software Domain", "labels": [], "entities": [{"text": "Learning to Define Terms", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5922369137406349}]}], "abstractContent": [{"text": "One way to test a person's knowledge of a domain is to ask them to define domain-specific terms.", "labels": [], "entities": []}, {"text": "Here, we investigate the task of automatically generating definitions of technical terms by reading text from the technical domain.", "labels": [], "entities": []}, {"text": "Specifically, we learn definitions of software entities from a large corpus built from the user forum Stack Overflow.", "labels": [], "entities": []}, {"text": "To model definitions , we train a language model and incorporate additional domain-specific information like word co-occurrence, and ontological category information.", "labels": [], "entities": []}, {"text": "Our approach improves previous baselines by 2 BLEU points for the definition generation task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9992940425872803}, {"text": "definition generation task", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.9410598079363505}]}, {"text": "Our experiments also show the additional challenges associated with the task and the shortcomings of language-model based architectures for definition generation .", "labels": [], "entities": [{"text": "definition generation", "start_pos": 140, "end_pos": 161, "type": "TASK", "confidence": 0.9139045178890228}]}], "introductionContent": [{"text": "Dictionary definitions have been previously used in various Natural Language Processing (NLP) pipelines like knowledge base population (, relationship extraction, and extracting semantic information (.", "labels": [], "entities": [{"text": "relationship extraction", "start_pos": 138, "end_pos": 161, "type": "TASK", "confidence": 0.7959500253200531}]}, {"text": "Creating dictionaries in anew domain is time consuming, often requiring hand curation by domain experts with significant expertise.", "labels": [], "entities": []}, {"text": "Developing systems to automatically learn and generate definitions of words can lead to greater time-efficiency ().", "labels": [], "entities": []}, {"text": "Additionally, it helps accelerate resource-building efforts for any new domain.", "labels": [], "entities": []}, {"text": "In this paper, we study the task of generating definitions of domain-specific entities.", "labels": [], "entities": []}, {"text": "In particular, our goal is to generate definitions for technical terms with the freely available Stack Overflow 1 (SO) as our primary corpus.", "labels": [], "entities": []}, {"text": "Stack Overflow is a technical question-and-answer forum aimed https://stackoverflow.com at supporting programmers in various aspects of computer science.", "labels": [], "entities": []}, {"text": "Each question is tagged with associated entities or \"tags\", and the top answers are ranked based on user upvotes (de).", "labels": [], "entities": []}, {"text": "shows a screenshot from the forum of a question and the entities tagged with the question.", "labels": [], "entities": []}, {"text": "Our work explores the challenge of generating definitions of entities in SO using the background data of question-answer pairs and their associated tags.", "labels": [], "entities": [{"text": "SO", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.6890220642089844}]}, {"text": "Our base definition generation model is adapted from, a Recurrent Neural Network (RNN) language model to learn to generate definitions for common English words using embeddings trained on Google News Corpus.", "labels": [], "entities": [{"text": "base definition generation", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6453306178251902}, {"text": "Google News Corpus", "start_pos": 188, "end_pos": 206, "type": "DATASET", "confidence": 0.7891038060188293}]}, {"text": "Over this base model, we leverage the distributed word information via the embeddings trained on domain specific Stack Overflow corpus.", "labels": [], "entities": []}, {"text": "We improve this model to additionally incorporate domain-specific information such as co-occurring entities and domain ontology in the definition generation process.", "labels": [], "entities": [{"text": "definition generation", "start_pos": 135, "end_pos": 156, "type": "TASK", "confidence": 0.917291909456253}]}, {"text": "Our model also uses an additional loss function to reconstruct the entity word representation from the generated sequence.", "labels": [], "entities": []}, {"text": "Our best model can generate definitions in software domain with a BLEU score of 10.91, improving upon the baseline by 2 points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9996229410171509}]}, {"text": "In summary, our contributions are as follows, 1.", "labels": [], "entities": []}, {"text": "We propose anew dataset of entities in the software domain and their corresponding definitions, for the definition generation task.", "labels": [], "entities": [{"text": "definition generation", "start_pos": 104, "end_pos": 125, "type": "TASK", "confidence": 0.9538336098194122}]}, {"text": "2. We provide ways to incorporate domainspecific knowledge such as co-occurring entities and ontology information into a language model trained for the definition generation task.", "labels": [], "entities": [{"text": "definition generation task", "start_pos": 152, "end_pos": 178, "type": "TASK", "confidence": 0.9247761964797974}]}, {"text": "3. We study the effectiveness of the model using the BLEU () metric and present the results and discussion about our results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9981710910797119}]}, {"text": "Section 4 of this paper presents the dataset.", "labels": [], "entities": []}, {"text": "Section 5 discusses the model in detail.", "labels": [], "entities": []}, {"text": "In Section 6 and 7 we present the experimental details and results of our experiments.", "labels": [], "entities": []}, {"text": "Section 8 provides an analysis and discussion of the results and the generated definitions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In SO, users can associate a question with a 'tag', such as 'Java' or 'machine learning', to help other users find and answer it.", "labels": [], "entities": [{"text": "SO", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9437118172645569}]}, {"text": "These tags are nearly always names of domain specific entities.", "labels": [], "entities": []}, {"text": "Each tag  has a definition on SO.", "labels": [], "entities": []}, {"text": "For the definitions, we created a dataset of 25K software entities (tags from SO) and their definitions on SO.", "labels": [], "entities": []}, {"text": "The data collection and pre-processing for the task is similar to cloze-style software questions collected in.", "labels": [], "entities": []}, {"text": "The definitions dataset was built from the definitional \"excerpt\" entry for each tag (entity) on Stack Overflow.", "labels": [], "entities": []}, {"text": "For example, the excerpt for the \"java\" tag is, \"Java is a general purpose object-oriented programming language designed to be used in conjunction with the Java Virtual Machine (JVM).\"", "labels": [], "entities": []}, {"text": "The dataset statistics can be seen in.", "labels": [], "entities": []}, {"text": "This dataset is used for training our definition generation models.", "labels": [], "entities": [{"text": "definition generation", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.8226700127124786}]}, {"text": "Examples of definitions in the dataset are shown in.", "labels": [], "entities": []}, {"text": "We use a background corpus of top 50 threads 3 tagged with every entity on Stack Overflow) and attempt to learn definitions of entities from this data.", "labels": [], "entities": []}, {"text": "We use this background corpus for training word embeddings and to give us tag co-occurrences.", "labels": [], "entities": []}, {"text": "In SO, a particular question can have multiple tags associated with it, which we call 'co-occurring tags'.", "labels": [], "entities": [{"text": "SO", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9767183661460876}]}, {"text": "We extracted the top 50 question posts for each tag, along with any answer-post responses and metadata (tags, au-3 A question along with the answers provided by other users is collectively called a thread.", "labels": [], "entities": []}, {"text": "The threads are ranked in terms of votes from the community.", "labels": [], "entities": []}, {"text": "thorship, comments) using Scrapy . From each thread, we used all text that is not marked as code and segmented them into sentences.", "labels": [], "entities": []}, {"text": "Each sentence is truncated to 2048 characters, lower-cased and tokenized using a custom tokenizer compatible with special characters in software terms (e.g. .net, c++).", "labels": [], "entities": []}, {"text": "The background corpus for our task consists of 27 million sentences.", "labels": [], "entities": []}, {"text": "To train the model, we use the 25k definitions dataset built as described in Section 4.", "labels": [], "entities": []}, {"text": "We split the data randomly into 90:5:5 train, test and validation sets as shown in.", "labels": [], "entities": []}, {"text": "The words being defined are mutually exclusive across the three sets, and thus our experiments evaluate how well the models generalize to new words.", "labels": [], "entities": []}, {"text": "All of the models utilize the same set of fixed word embeddings from two different corpora.", "labels": [], "entities": []}, {"text": "The first set of vectors are trained entirely on the Stack Overflow background corpus and the other set are pre-trained open domain word embeddings . Both these embeddings are concatenated and we use this as the representation for each word.", "labels": [], "entities": [{"text": "Stack Overflow background corpus", "start_pos": 53, "end_pos": 85, "type": "DATASET", "confidence": 0.620537705719471}]}, {"text": "For the embeddings trained on Stack Overflow corpus, we use the Word2Vec () implementation of Gensim 8 toolkit.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.9481717348098755}]}, {"text": "In the corpus, we prepend to every sentence in a question-answer pair, every tag it is associated with.", "labels": [], "entities": []}, {"text": "We further eliminated stopwords from the corpus and set a larger context window size of 10.", "labels": [], "entities": []}, {"text": "For the model, we use a 2 layer LSTM with 500 hidden unit size for both the forward and reconstruction layers of the models.", "labels": [], "entities": []}, {"text": "The size of the em-: Experimental results of our baselines on common English words dataset bedding layer is set to 300 dimension.", "labels": [], "entities": [{"text": "English words dataset bedding layer", "start_pos": 69, "end_pos": 104, "type": "DATASET", "confidence": 0.7870788931846618}]}, {"text": "For training, we minimize the cross-entropy and reconstruction loss using Adam () optimizer with a learning rate of 0.001 and gradient clip of 5.", "labels": [], "entities": []}, {"text": "We evaluate the task using BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9989909529685974}]}], "tableCaptions": [{"text": " Table 1: Sample definitions from the dataset", "labels": [], "entities": [{"text": "Sample definitions", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9177870452404022}]}, {"text": " Table 3: Experimental results for our different models. *SO Emb = Embeddings learned on Stack Overflow. **Eng  Emb = Embeddings learned on general open domain English dataset", "labels": [], "entities": []}, {"text": " Table 4: Experimental results of our baselines on com- mon English words dataset", "labels": [], "entities": [{"text": "com- mon English words dataset", "start_pos": 51, "end_pos": 81, "type": "DATASET", "confidence": 0.6887185871601105}]}]}