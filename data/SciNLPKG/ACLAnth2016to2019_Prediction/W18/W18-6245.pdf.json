{"title": [{"text": "Super Characters: A Conversion from Sentiment Classification to Image Classification", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.9014091193675995}, {"text": "Image Classification", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.6759448051452637}]}], "abstractContent": [{"text": "We propose a method named Super Characters for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.9580603241920471}]}, {"text": "This method converts the sentiment classification problem into image classification problem by projecting texts into images and then applying CNN models for classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.9269302487373352}, {"text": "image classification", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.7369717359542847}]}, {"text": "Text features are extracted automatically from the generated Super Characters images, hence there is no need of any explicit step of embedding the words or characters into numerical vector representations.", "labels": [], "entities": []}, {"text": "Experimental results on large social media corpus show that the Super Characters method consistently outperforms other methods for sentiment classification and topic classification tasks on ten large social media datasets of millions of contents in four different languages, including Chinese, Japanese, Korean and English.", "labels": [], "entities": [{"text": "sentiment classification and topic classification", "start_pos": 131, "end_pos": 180, "type": "TASK", "confidence": 0.7499899566173553}]}], "introductionContent": [{"text": "Sentiment classification is an interesting topic that has been studied for many years (.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9791288673877716}]}, {"text": "Word embedding is a widely used technique for sentiment classification tasks, which embeds the words into numerical vector representation before the sentences are fed into models for classification ().", "labels": [], "entities": [{"text": "sentiment classification tasks", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.9403756658236185}]}, {"text": "For sequential input, RNNs are usually used and have very good results for text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.8603513439496359}]}, {"text": "Recently, there are also works using Convolutional Neural Networks (CNN) for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8344041407108307}]}, {"text": "CNN models have feature extraction and classification in a whole model, which require no need of manually extracting features from images and are proved to be successful in image classification tasks (.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7296731472015381}, {"text": "image classification", "start_pos": 173, "end_pos": 193, "type": "TASK", "confidence": 0.731372132897377}]}, {"text": "There are also works on character level text classifications (.", "labels": [], "entities": [{"text": "character level text classifications", "start_pos": 24, "end_pos": 60, "type": "TASK", "confidence": 0.653469443321228}]}, {"text": "However, the input for CNNs are still using the embedding vectors.", "labels": [], "entities": [{"text": "CNNs", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.9285393953323364}]}, {"text": "had studied the different ways of encoding Chinese, Japanese, Korean (CJK) and English languages for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.8453608453273773}]}, {"text": "These encoding mechanisms include Onehot encoding, embedding and images of character glyphs.", "labels": [], "entities": [{"text": "Onehot encoding", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.7500059306621552}]}, {"text": "Comparisons with linear models, fastText (, and convolutional networks were provided.", "labels": [], "entities": []}, {"text": "This work studied 473 models, using 14 large-scale text classification datasets in 4 languages including Chinese, English, Japanese and Korean.", "labels": [], "entities": []}, {"text": "Our work in this paper is based on the datasets provided in and downloadable at.", "labels": [], "entities": []}, {"text": "Different from existing methods, our method has no explicit step of embedding the text into numerical vector representations.", "labels": [], "entities": []}, {"text": "Instead, we project the text into images and then directly feed the images into CNN models to classify the sentiments.", "labels": [], "entities": []}, {"text": "Before introducing the details of our solution, let us first look at how humans read text and do sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.9605425894260406}]}, {"text": "Humans read sentences and can immediately understand the sentiment of the text; Humans can also read multiple lines at a first sight of paragraphs and get the general idea instantly.", "labels": [], "entities": []}, {"text": "This fast process consists of two steps.", "labels": [], "entities": []}, {"text": "First, the texts are perceived by human's eyes as a whole picture of text, while the details of this picture are block-built by many characters.", "labels": [], "entities": []}, {"text": "Second, the image containing the texts are fed into the brain.", "labels": [], "entities": []}, {"text": "And then the human brain processes the image of texts to output the sentiment classification results.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.9340404868125916}]}, {"text": "During the processing, the human brain may recognize words and phrases as the intermediate results, in order to further analyze the sentiment.", "labels": [], "entities": []}, {"text": "However, if we treat the human brain as a black box system, its input is the image of texts received by the eyes, and its output is the sentiment classification result.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 136, "end_pos": 160, "type": "TASK", "confidence": 0.9171113669872284}]}, {"text": "In this paper, we propose a two-step method that is similar to how humans do sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.93766850233078}]}, {"text": "We tested our method using the datasets provided by on text classification tasks for social media contents from different countries in four languages, including English, Chinese, Japanese, and Korean.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.7830329934755961}]}, {"text": "And compared with other existing methods, including fastText, EmbedNet, OnehotNet, and linear models.", "labels": [], "entities": []}, {"text": "The results show our method consistently outperforms other method on these datasets for sentiment classification tasks.", "labels": [], "entities": [{"text": "sentiment classification tasks", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.9533795515696207}]}], "datasetContent": [{"text": "Ten of 14 datasets provided by were tested on, a brief description of which is provided: Dianping: Chinese restaurant reviews were evenly split as follows: 4 and 5 star reviews were assigned to the positive class while 1-3 star reviews were in the negative class.", "labels": [], "entities": []}, {"text": "JD Full: Chinese shopping reviews wer evenly split for predicting full five stars.", "labels": [], "entities": [{"text": "JD Full", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8539218902587891}, {"text": "Chinese shopping reviews", "start_pos": 9, "end_pos": 33, "type": "DATASET", "confidence": 0.769646406173706}]}, {"text": "JD Binary Chinese shopping reviews are evenly split into positive (4-and-5 star reviews) and negative (1-and-2 star reviews) sentiments, ignoring 3-star reviews.", "labels": [], "entities": [{"text": "JD Binary Chinese shopping reviews", "start_pos": 0, "end_pos": 34, "type": "DATASET", "confidence": 0.9631025314331054}]}, {"text": "Rakuten Full Japanese shopping reviews were evenly split into predicting full five stars.", "labels": [], "entities": [{"text": "Rakuten Full Japanese shopping reviews", "start_pos": 0, "end_pos": 38, "type": "DATASET", "confidence": 0.9633134365081787}]}, {"text": "Rakuten Binary Japanese shopping reviews were evenly split into positive (4-and-5 star reviews) and negative (1-and-2 star reviews) sentiments, removing duplicates and ignoring 3-star reviews.", "labels": [], "entities": [{"text": "Rakuten Binary Japanese shopping reviews", "start_pos": 0, "end_pos": 40, "type": "DATASET", "confidence": 0.9483071088790893}]}, {"text": "11st Full Korean shopping reviews were evenly split into predicting full five stars.", "labels": [], "entities": [{"text": "11st Full Korean shopping reviews", "start_pos": 0, "end_pos": 33, "type": "DATASET", "confidence": 0.7466071009635925}]}, {"text": "11st Binary Korean shopping reviews were evenly split into identifying positive (4-and-5 star reviews) or negative (1-3 star reviews) sentiments.", "labels": [], "entities": [{"text": "11st Binary Korean shopping reviews", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.6224877238273621}]}, {"text": "Amazon Full: English shopping reviews were evenly split into predicting full five stars.", "labels": [], "entities": [{"text": "Amazon Full", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.943798154592514}]}, {"text": "Ifeng: First paragraphs of Chinese news articles from 2006-2016 were evenly split into 5 news channels.", "labels": [], "entities": []}, {"text": "Chinanews: Chinese news articles from 2008-2016 were evenly split into 7 news channels, removing duplicates.", "labels": [], "entities": [{"text": "Chinanews", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9650663733482361}]}, {"text": "The statistics of these datasets are given in.", "labels": [], "entities": []}, {"text": "We can see that eight out of the ten datasets has more than millions of samples in training, and the largest datasets have 4 millions of samples in training set.", "labels": [], "entities": []}, {"text": "The test datasets are in the range of 1/4 to 1/16 of the training datasets respectively.", "labels": [], "entities": []}, {"text": "The languages used in these datasets include Chinese, Japanese, Korean, and English.", "labels": [], "entities": []}, {"text": "And the number of classes ranges from 2, 5 to 7.", "labels": [], "entities": []}, {"text": "For each dataset, we generate Super Characters images first.", "labels": [], "entities": []}, {"text": "We draw text with the Python Imaging Library (PIL), and set all the Super Character image sizes to 224x224 pixels, the background set to black.", "labels": [], "entities": []}, {"text": "For long text inputs such as paragraphs or articles, the length of which is different so we set a cut-length from the beginning of the news article.", "labels": [], "entities": []}, {"text": "Although we may forcely cut the input and ignore the rest, this cutlength still works well since the first few sentences usually convey the general information about the whole contents.", "labels": [], "entities": []}, {"text": "For other text sources and tasks, the starting point of the text for the cut-length may change accordingly.", "labels": [], "entities": []}, {"text": "For each experiment, we determine the estimated cut-length by using a threshold on sentence lengths.", "labels": [], "entities": []}, {"text": "We have only tried one cut-length of 14x14=196 for every experimental data set.", "labels": [], "entities": []}, {"text": "We set the size of each character as 224/14=16 square pixels.", "labels": [], "entities": []}, {"text": "And then, we feed the generated Super Characters to train CNN models.", "labels": [], "entities": []}, {"text": "We use successful pretrained model SENet-154 ( ) in the ImageNet competition (, which is the winner in ImageNet2017 competition and achieves 81.32% Top1 accuracy and 95.53% Top5 accuracy.", "labels": [], "entities": [{"text": "SENet-154", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.8720327615737915}, {"text": "ImageNet", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.8884086012840271}, {"text": "Top1", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9241662621498108}, {"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.8798742294311523}, {"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.8676005005836487}]}, {"text": "We used pretrained model downloadable at (Hu, 2017) because it gave a good initialization for transfer learning tasks.", "labels": [], "entities": []}, {"text": "We changed the last layer to the corresponding number of categories in each data set to train on the Super Characters images.", "labels": [], "entities": []}, {"text": "The sentiment classification results on test datasets are shown in.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.938536673784256}]}, {"text": "The accuracy numbers for the models of OnehotNet, EmbedNet, Linear Model, and fastText are given by.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994650483131409}]}, {"text": "Note that in (, each model is tried with different encoding methods.", "labels": [], "entities": []}, {"text": "For example, OnehotNet uses 4 different encodings, EmbedNet uses 10, Linear Models uses 11, and fastText uses 10.", "labels": [], "entities": []}, {"text": "We only listed the best results for each existing method across different encodings.", "labels": [], "entities": []}, {"text": "And compare our results with the best of them.", "labels": [], "entities": []}, {"text": "That means we compare our results with the finetuned best encoding of each existing model in 2.", "labels": [], "entities": []}, {"text": "From the results we can see that our Super Characters method (short as S.C.) consistently outperforms other methods, even with their best encodings.", "labels": [], "entities": []}, {"text": "THUCTC () was provided by the Tsinghua University NLP lab in 2016.", "labels": [], "entities": [{"text": "THUCTC", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9511640667915344}, {"text": "Tsinghua University NLP lab", "start_pos": 30, "end_pos": 57, "type": "DATASET", "confidence": 0.8030361533164978}]}, {"text": "It totals 836075 documents after downloaded, covering 14 topics including 24373 Sports and 154398 Stocks.", "labels": [], "entities": []}, {"text": "The majority of the documents are long articles with hundreds or sometimes thousands of characters in multiple sentences or paragraphs.", "labels": [], "entities": []}, {"text": "We use a cutlength of 28x28=784, each having an 8x8 pixel size and utilize simhei font for Super Characters on the THUCTC data.", "labels": [], "entities": [{"text": "THUCTC data", "start_pos": 115, "end_pos": 126, "type": "DATASET", "confidence": 0.9847411215305328}]}, {"text": "In, we showed our Super Character method using ResNet-50 (SC+ResNet50) attained an accuracy of 94.85% and our Super Character method using ResNet-152 (SC+ResNet152) attained an accuracy of 94.35%, while the result given by achieved only an accuracy of 88.6% using LibLinear.) implements linear SVMs and logistic regression models trained using a coordinate descent algorithm.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9956008195877075}, {"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9948222637176514}, {"text": "accuracy", "start_pos": 240, "end_pos": 248, "type": "METRIC", "confidence": 0.9816412925720215}]}, {"text": "Our models reduce the error by 50.4% compared to this existing model.", "labels": [], "entities": [{"text": "error", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9911207556724548}]}, {"text": "The Fudan corpus (Li, 2011) contains 9804 documents of long sentences and paragraphs in 20 categories.", "labels": [], "entities": [{"text": "Fudan corpus (Li, 2011)", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.9412069490977696}]}, {"text": "We use the same split as () in selecting the same 5 categories: 1218 environmental, 1022 agricultural, 1601 economical, 1025 political and 1254 sport documents; 70% of the total data is used for training and the rest for testing.", "labels": [], "entities": []}, {"text": "\u2022 SC+ResNet-50: Using a ResNet-50 model pretrained on the ImageNet dataset, we finetuned the transfer learning model on the new generated super character dataset.", "labels": [], "entities": [{"text": "ImageNet dataset", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.9701443612575531}]}, {"text": "\u2022 SC+ResNet-50-THUCTC: Using a ResNet-50 model pretrained on THUCTC data, we fine-tuned the trasfere learning model on the new generated super character dataset.", "labels": [], "entities": [{"text": "THUCTC data", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.797884464263916}]}, {"text": "We used a cut-length of 28x28=784 and words of pixel size 8x8 with the simhei font for our Super Characters in this experiment.", "labels": [], "entities": []}, {"text": "In, the first 7 rows of model accuracies for different algorithms are given by).", "labels": [], "entities": []}, {"text": "We can see that our SC+ResNet-50-THUCTC model attained an accuracy of 97.8% while the best existing method achieved only a 95.3% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9994164705276489}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9944702386856079}]}, {"text": "Our SC+ResNet50-THUCTC model reduces the error by 53.2% compared with the best existing model.", "labels": [], "entities": [{"text": "error", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9877231121063232}]}, {"text": "The SC+ResNet-50 model with 95.7% accuracy also outperforms the best existing model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9991500377655029}]}, {"text": "The pretrained model on THUCTC dataset gives 2.1% accuracy improvement than SC+ResNet-50 model, which means pretrained models on the same language and a larger dataset will help fora better initialization and better model.", "labels": [], "entities": [{"text": "THUCTC dataset", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.9715201556682587}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9991579055786133}]}, {"text": "For this data set, we did not delete the non-Chinese characters as did.", "labels": [], "entities": []}, {"text": "The result shows that our simple projection from text to Super Characters image is easy to implement and very robust.", "labels": [], "entities": []}, {"text": "Users do not even need to perform complicated preprocessing techniques for the data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Datasets statistics used in Table 2 and short names used for convenience.", "labels": [], "entities": []}, {"text": " Table 2: Results of our Super Character (SC) method against other models on datasets provided  by (Zhang and LeCun, 2017).", "labels": [], "entities": []}, {"text": " Table 3: Results of our Super Character (SC) method  against other models on THUCTC data set.", "labels": [], "entities": [{"text": "THUCTC data set", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.988676389058431}]}, {"text": " Table 4: Results of our Super Character (SC) method  against other models on the Fudan dataset.", "labels": [], "entities": [{"text": "Fudan dataset", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.9805662930011749}]}, {"text": " Table 5: Cut-length Impact on Accuracy.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9290177226066589}]}]}