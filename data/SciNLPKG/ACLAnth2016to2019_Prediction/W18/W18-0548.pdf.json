{"title": [{"text": "Automatic Distractor Suggestion for Multiple-Choice Tests Using Concept Embeddings and Information Retrieval", "labels": [], "entities": [{"text": "Automatic Distractor Suggestion", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7955124179522196}, {"text": "Information Retrieval", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.722996324300766}]}], "abstractContent": [{"text": "Developing plausible distractors (wrong answer options) when writing multiple-choice questions has been described as one of the most challenging and time-consuming parts of the item-writing process.", "labels": [], "entities": []}, {"text": "In this paper we propose a fully automatic method for generating distractor suggestions for multiple-choice questions used in high-stakes medical exams.", "labels": [], "entities": []}, {"text": "The system uses a question stem and the correct answer as an input and produces a list of suggested distractors ranked based on their similarity to the stem and the correct answer.", "labels": [], "entities": []}, {"text": "To do this we use a novel approach of combining concept embeddings with information retrieval methods.", "labels": [], "entities": []}, {"text": "We frame the evaluation as a prediction task where we aim to \"predict\" the human-produced distractors used in large sets of medical questions, i.e. if a distractor generated by our system is good enough it is likely to feature among the list of distractors produced by the human item-writers.", "labels": [], "entities": []}, {"text": "The results reveal that combining concept embeddings with information retrieval approaches significantly improves the generation of plausible distractors and enables us to match around 1 in 5 of the human-produced distractors.", "labels": [], "entities": []}, {"text": "The approach proposed in this paper is generalis-able to all scenarios where the distractors refer to concepts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multiple-choice tests are one of the most widely used forms of both formative and summative assessment and area probably the most prominent feature of high-stakes standardized exams (.", "labels": [], "entities": [{"text": "summative assessment", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.878195583820343}]}, {"text": "Administering such exams requires the development of a large number of good-quality multiple-choice questions (MCQs).", "labels": [], "entities": []}, {"text": "To illustrate the need to have a large number of questions, report that a 40-item computer adaptive test for high-stakes examination administered twice a year would require a bank with 2,000 items and estimate that the cost of developing an item bank of this size would be between 3,000,000 and 5,000,000 USD.", "labels": [], "entities": []}, {"text": "Naturally, this creates the incentive to automate the test production as much as possible and has resulted in a large number of papers on the topic of automatic MCQ generation.", "labels": [], "entities": [{"text": "MCQ generation", "start_pos": 161, "end_pos": 175, "type": "TASK", "confidence": 0.8747971653938293}]}, {"text": "An important aspect of MCQ development is the generation of plausible distractors (wrong answer options), as they can help control for the difficulty of the item, reduce random guessing and discriminate properly between different levels of student ability.", "labels": [], "entities": [{"text": "MCQ development", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.9507405161857605}]}, {"text": "This task poses a challenge to both humans and machines and is especially demanding in the field of medical exams.", "labels": [], "entities": []}, {"text": "For example, an analysis of 514 human-produced items including 2056 options (1542 distractors and 514 correct responses), administered to undergraduate nursing students, indicated that \"Only 52.2% (n = 805) of all distractors were functioning effectively and 10.2% (n = 158) had a choice frequency of 0.\".", "labels": [], "entities": []}, {"text": "Items with more functioning distractors were found to be more difficult and more discriminating.", "labels": [], "entities": []}, {"text": "A particular challenge for the automatic development of MCQ distractors for the medical domain is the coverage of the ontologies, which could be too narrow in some cases, and too broad in others, and the need to rank the candidates in order to select the best ones.", "labels": [], "entities": [{"text": "MCQ distractors", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7840310633182526}]}, {"text": "At the same time, this domain is of particular need of automated assistance, as the requirement fora very specialized knowledge makes the recruitment of item-writers and the test development procedure even more costly.", "labels": [], "entities": []}, {"text": "To address this issue we propose a method to fully automatically suggest distractors for MCQs given a stem and a correct answer.", "labels": [], "entities": []}, {"text": "The data used in this study features two sets of 1,441 MCQs and 369 MCQs from the United States Medical Licensing Examination (USMLE) for which we have the stem, all answer options and information on which the correct answer is.", "labels": [], "entities": [{"text": "United States Medical Licensing Examination (USMLE)", "start_pos": 82, "end_pos": 133, "type": "DATASET", "confidence": 0.5418807975947857}]}, {"text": "We compare two approaches to suggesting distractors based on: i) concept embeddings only and ii) concept embeddings reranked using information retrieval techniques.", "labels": [], "entities": []}, {"text": "The evaluation of these approaches is formulated as a prediction task, where each system uses the stem and the correct answer as an input and tries to predict the existing distractor options for each item as an output.", "labels": [], "entities": []}, {"text": "The contributions of this study are as follows: \u2022 We propose a novel method for distractor generation and selection based on concept embeddings reranked using information retrieval, which can successfully suggest relevant distractors given an item stem and the correct answer option.", "labels": [], "entities": [{"text": "distractor generation", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.7875202894210815}]}, {"text": "\u2022 We show that the ranking based on information-retrieval methods improves the distractor prediction significantly.", "labels": [], "entities": [{"text": "distractor prediction", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.5609382539987564}]}, {"text": "\u2022 The approach used in this study is generalisable to all scenarios where the answer options refer to concepts.", "labels": [], "entities": []}, {"text": "Furthermore, it can generate distractors for any item given that the correct answer features as an entry in the ontology, as opposed to only items generated by a specific method.", "labels": [], "entities": []}, {"text": "The rest of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "The next section presents related work on automatic item generation with special emphasis on distractor generation and evaluation.", "labels": [], "entities": [{"text": "automatic item generation", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.5964118639628092}, {"text": "distractor generation", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.6939156204462051}]}, {"text": "Section 3 describes the data sets used in this study and Section 4 describes our method.", "labels": [], "entities": []}, {"text": "The results are reported in Section 6, discussed in Section 6 and summarised in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate our approach and the usefulness of the suggestions in the generated list, we describe an evaluation procedure where our system takes existing items together with all their options and tries to \"predict\" one or more of the existing distractors.", "labels": [], "entities": []}, {"text": "In other words, if the system comes up with one or more of the same distractors as the ones produced by the human item-writers, then the approach could be considered useful for the generation of suitable distractor suggestions for new items.", "labels": [], "entities": []}, {"text": "To do this, for each item, we get the first n concepts that are most similar to the combination of stem and the correct answer, and see how many of these concepts actually feature as distractors in that item (hits).", "labels": [], "entities": []}, {"text": "The number of hits provides an estimation of the usefulness of the suggested list.", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "Within that table, Applicable items are the ones whose correct answers could produce distractor candidates using the specific ontology relation.", "labels": [], "entities": [{"text": "Applicable", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9835475087165833}]}, {"text": "Number of all candidates reflects the number of candidates suggested by the specific ontology relation.", "labels": [], "entities": []}, {"text": "Maximum number of hits refers to the number of hits if all the suggested candidates are considered, Random N hits is the number of hits if random N candidates are picked for each item.", "labels": [], "entities": []}, {"text": "Recall at N signifies the total number of hits if the top N candidates are considered, divided by the total number of distractors that also feature in UMLS.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9875150918960571}, {"text": "UMLS", "start_pos": 151, "end_pos": 155, "type": "DATASET", "confidence": 0.8633456230163574}]}, {"text": "In terms of ontology relations, SIB includes only candidates that are considered to be the siblings of the correct answer (according to UMLS).", "labels": [], "entities": [{"text": "SIB", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.8838887214660645}, {"text": "UMLS", "start_pos": 136, "end_pos": 140, "type": "DATASET", "confidence": 0.9273366928100586}]}, {"text": "RN RB means that only candidates that share a broader or narrower concept with the correct answer are considered, and STY means that all candidates that share the same semantic type with the correct answer are considered.", "labels": [], "entities": [{"text": "RN RB", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7733573913574219}, {"text": "STY", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9633785486221313}]}, {"text": "The precision and recall relation is presented in, while present the recall for the private and public data sets respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994404911994934}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9978443384170532}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9969965219497681}]}, {"text": "As can be seen from, the suggested list outperformed the baseline of random hits in all three types of relations (SIB, RB RN and STY), where best result (in terms of trade off between precision and recall) is achieved for the top 20 hits.", "labels": [], "entities": [{"text": "RB RN", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.9313860535621643}, {"text": "STY", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.750803530216217}, {"text": "precision", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9978132247924805}, {"text": "recall", "start_pos": 198, "end_pos": 204, "type": "METRIC", "confidence": 0.9960248470306396}]}, {"text": "Using the broadest ontology relation, namely  same semantic type (STY), performs as well as the sibling (SIB) relation for the top 10 hits (i.e. 76 vs. 75, respectively, for the public data set and 325 vs. 333 for the private one).", "labels": [], "entities": []}, {"text": "From the top 20 hits onwards, the STY relation outperforms SIB (i.e. for 20 hits we have STY hits= 142 and SIB = 82 for the public data set and STY = 572 and SIB = 382 for the private data set).", "labels": [], "entities": []}, {"text": "An example of a question and the list of generated distractors and their ranking is presented in.", "labels": [], "entities": []}, {"text": "As can be seen from the table, the   information-retrieval ranking improves the number of hits in all types of relations (SIB, RB RN, and STY).", "labels": [], "entities": []}, {"text": "It should be noted that the improvement we notice in this example is not as significant in other examples but the general trend is the same.", "labels": [], "entities": []}, {"text": "The average improvement across of all items can be seen in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Item characteristics for the two data sets", "labels": [], "entities": []}, {"text": " Table 3: Cosine similarity between different item-part configurations calculated using Freebase vectors, using the  private dataset.", "labels": [], "entities": [{"text": "Cosine similarity", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.8109427094459534}]}, {"text": " Table 4: Cosine similarity between different item-part configurations calculated using UMLS vectors, using the  private dataset.", "labels": [], "entities": [{"text": "Cosine similarity", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.7682884335517883}]}, {"text": " Table 6: Evaluation results: distractor hits.", "labels": [], "entities": []}]}