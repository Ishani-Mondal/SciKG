{"title": [{"text": "Syntactic Manipulation for Generating More Diverse and Interesting Texts", "labels": [], "entities": [{"text": "Syntactic Manipulation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9341767728328705}]}], "abstractContent": [{"text": "Natural Language Generation plays an important role in the domain of dialogue systems as it determines how users perceive the system.", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5984707276026408}]}, {"text": "Recently, deep-learning based systems have been proposed to tackle this task, as they generalize better and require less amounts of manual effort to implement them for new domains.", "labels": [], "entities": []}, {"text": "However , deep learning systems usually adapt a very homogeneous sounding writing style which expresses little variation.", "labels": [], "entities": []}, {"text": "In this work, we present our system for Natural Language Generation where we control various aspects of the surface realization in order to increase the lexical variability of the utterances, such that they sound more diverse and interesting.", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.6555009682973226}]}, {"text": "For this, we use a Semantically Controlled Long Short-term Memory Network (SC-LSTM), and apply its specialized cell to control various syntactic features of the generated texts.", "labels": [], "entities": []}, {"text": "We present an in-depth human evaluation where we show the effects of these surface manipulation on the perception of potential users.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we describe our end-to-end trainable neural network for producing natural language descriptions of restaurants from meaning representations (MR).", "labels": [], "entities": []}, {"text": "Recently, data-driven natural language generation (NLG) systems have shown great promise, especially as they can be easily adapted to new data or domains.", "labels": [], "entities": [{"text": "data-driven natural language generation (NLG)", "start_pos": 10, "end_pos": 55, "type": "TASK", "confidence": 0.7933547241347176}]}, {"text": "End-toend systems based on deep learning can jointly learn sentence planning and sentence realization from unaligned data.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7330272197723389}, {"text": "sentence realization", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.7063134759664536}]}, {"text": "However, a recurrent problem, which we found with the existing solutions for NLG, is that the generated utterances express a very homogeneous writing style.", "labels": [], "entities": []}, {"text": "More precisely, most utterances start by using the restaurant name, the follow-up sentences usually begin with the pronoun \"It\", and each attribute-value pair is expressed using the same formulation across different utterances (see).", "labels": [], "entities": []}, {"text": "Green Man is a family friendly japanese restaurant in riverside near Express by Holiday Inn.", "labels": [], "entities": [{"text": "Express by Holiday Inn", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.7496106177568436}]}, {"text": "Clowns is a pub near Crowne Plaza Hotel with a customer rating of 5 out of 5.", "labels": [], "entities": [{"text": "Clowns", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9029150605201721}, {"text": "Crowne Plaza Hotel", "start_pos": 21, "end_pos": 39, "type": "DATASET", "confidence": 0.985023578008016}]}, {"text": "Wildwood is an italian pub located near Raja Indian Cuisine in the city centre.", "labels": [], "entities": [{"text": "Wildwood", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9693175554275513}, {"text": "Raja Indian Cuisine", "start_pos": 40, "end_pos": 59, "type": "DATASET", "confidence": 0.8967269659042358}]}, {"text": "The Cricketers provides chinese food in the 20-25 price range.", "labels": [], "entities": [{"text": "Cricketers", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.773300051689148}]}, {"text": "It is located in the riverside.", "labels": [], "entities": []}, {"text": "It is near All Bar One.", "labels": [], "entities": [{"text": "All Bar One", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.9832854668299357}]}, {"text": "Its customer rating is high.: Examples to highlight the homogeneity of the utterances generated by state-of-the-art systems.", "labels": [], "entities": []}, {"text": "The publicly available E2E dataset by) provides pairs of Meaning Representations (MR's) and several human generated reference utterances for the restaurant-domain.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.9444832801818848}]}, {"text": "It is the first dataset to provide large amounts of training data with an open vocabulary, complex syntactic structures, and more variabilty in expressing the attributes.", "labels": [], "entities": []}, {"text": "In this work, we exploit these characteristics of the dataset to generate utterances which express a higher diversity in their writing style.", "labels": [], "entities": []}, {"text": "For this, we extend the Semantically Conditioned Long Short-term Memory Network (SC-LSTM) proposed by) with surface features to control the manipulation of the surface realization.", "labels": [], "entities": []}, {"text": "Since the data contains a large variety of formulations for an attribute-value pair, a simple delexicalization of the utterance is not possible.", "labels": [], "entities": []}, {"text": "This fact also increases the difficulty of evaluating the utterances for their correctness.", "labels": [], "entities": []}, {"text": "Thus, we introduce a semantic reranking procedure based on classification algorithms trained to rate whether the attributes are rendered correctly.", "labels": [], "entities": []}, {"text": "We evaluate our model on the E2E dataset and report the BLEU, NIST, METEOR, ROUGE-L and CIDEr scores.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.9723254442214966}, {"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9995893836021423}, {"text": "NIST", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.8851025104522705}, {"text": "METEOR", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9903523325920105}, {"text": "ROUGE-L", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9938886165618896}, {"text": "CIDEr", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9118086695671082}]}, {"text": "We measure the diversity of the generated utterances by counting the number of different uni-and bi-grams.", "labels": [], "entities": []}, {"text": "Further, to evaluate the correctness of the generated utterances, we employ a soft metric based on the aforementioned classifiers.", "labels": [], "entities": []}, {"text": "Finally, we present an in-depth human evaluation where we measured the effects of these more diverse utterances on the perceptions of potential users.", "labels": [], "entities": []}, {"text": "More precisely, humans evaluated the quality and naturalness of an utterance, which of the attributes comprehensible, concise, elegant, and professional fits to the text, and which of the different systems generated the most preferred outputs.", "labels": [], "entities": []}, {"text": "We release the code and all the scripts.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal for our application is to generate descriptions for restaurants.", "labels": [], "entities": []}, {"text": "The dataset from (Novikova et al., 2017) contains 50k utterances for 5,751 different MRs.", "labels": [], "entities": []}, {"text": "On average, each MR is composed of 5.43 attributes and there are 8.1 different references for each MR on average.", "labels": [], "entities": []}, {"text": "For the evaluation, we report various corpus-based metrics: BLEU-4 (), NIST) METEOR (, ROUGE-L, and CIDEr ().", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9983069896697998}, {"text": "METEOR", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9008666276931763}, {"text": "ROUGE-L", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.9917005896568298}]}, {"text": "Furthermore, we report various measures for lexical diversity: number of different tokens (#tokens), the type-token ratio (TTR), the moving average type-token ratio (MSTTR), and the measure of lexical diversity(MLTD)).", "labels": [], "entities": [{"text": "type-token ratio (TTR)", "start_pos": 105, "end_pos": 127, "type": "METRIC", "confidence": 0.8746363401412964}, {"text": "moving average type-token ratio (MSTTR)", "start_pos": 133, "end_pos": 172, "type": "METRIC", "confidence": 0.7342765203544072}, {"text": "lexical diversity(MLTD))", "start_pos": 193, "end_pos": 217, "type": "METRIC", "confidence": 0.6456063508987426}]}, {"text": "Finally, we perform a human evaluation to measure the effect of the proposed manipulations on the user's perception.", "labels": [], "entities": []}, {"text": "Preprocessing Each utterance is treated as a string of characters, where each character is represented as a one-hot encoded vector.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.946431040763855}]}, {"text": "We replace the name and near values with the tokens 'X-name\" and \"X-near\" respectively.", "labels": [], "entities": []}, {"text": "The high diversity of the various formulations found for the attributevalue pairs, impedes us from replacing other attributes with placeholders.", "labels": [], "entities": []}, {"text": "To generate the lexical features, we apply the Spacy-API 3 for word and sentence tokenization.", "labels": [], "entities": [{"text": "word and sentence tokenization", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.5709076821804047}]}, {"text": "System Setup We train the SC-LSTM and the classifiers using AdaDelta to optimize the loss function.", "labels": [], "entities": [{"text": "AdaDelta", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9129126071929932}]}, {"text": "We apply a softmax with decreasing temperature as proposed in ( to approximate the discrete representation, which is used as input to the LSTM during the  in.", "labels": [], "entities": []}, {"text": "For the classifiers we use a 2-layer CNN with 256 kernels of length 3.", "labels": [], "entities": []}, {"text": "We use our character-based version of the SC-LSTM (vanilla) as well as the sequence-tosequence model by (tgen) as baseline.", "labels": [], "entities": []}, {"text": "We evaluate different versions of our model: the model where we control only the first word of the utterance (utt-fw), the model where we only control the first words of the follow-up sentences (follow-fw), the model where we only control the formulations of the attributevalue pairs (form), and the model where we control all three factors (full).", "labels": [], "entities": []}, {"text": "Output Generation The input to the system is a meaning representation (MR) which is converted into the MR-vector d 0 . For each MR, the system samples the syntactic control values at random, i.e. it samples the first word of the utterance, the first words of each of the follow-up sentences and the formulation for each attribute-value pair randomly from the list of their respective possibilities.", "labels": [], "entities": [{"text": "Output Generation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7410382032394409}]}, {"text": "Then, these syntactic features are encoded into the onehot format as described above.", "labels": [], "entities": []}, {"text": "The input to the SC-LSTM is composed of both the MR-vector and the syntactic control vector.", "labels": [], "entities": []}, {"text": "To ensure that the sampling of the syntactic features did not introduce semantic error, the system samples 10 different values for each of the three control types and produces one utterance for each combination, e.g. the full system produces 1000 sentences for each MR.", "labels": [], "entities": []}, {"text": "We then use the classifiers (previously trained to evaluate if the utterance rendered the MR correctly) to rank the 1000 utterances w.r.t. their correctness.", "labels": [], "entities": []}, {"text": "Finally, the system samples the final utterance from the set of utterances with the highest score (as there can be multiple utterances with the same score).: Error Rate for each system, best system is highlighted in bold.", "labels": [], "entities": [{"text": "Error Rate", "start_pos": 158, "end_pos": 168, "type": "METRIC", "confidence": 0.9826609194278717}]}, {"text": "The sc subscript denotes the scores computed by the classifiers.", "labels": [], "entities": []}, {"text": "We report the scores for the automatic evaluation.", "labels": [], "entities": []}, {"text": "This includes the metrics BLEU, ROUGE-L, ME-TEOR, NIST, and CIDEr score, which rely on the comparison between the predicted utterance and multiple reference utterances.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9990161657333374}, {"text": "ROUGE-L", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9966408014297485}, {"text": "ME-TEOR", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9772332310676575}, {"text": "NIST", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.7332273721694946}, {"text": "CIDEr score", "start_pos": 60, "end_pos": 71, "type": "METRIC", "confidence": 0.9179263710975647}]}, {"text": "shows that the surface manipulation leads to a decrease in all of these scores.", "labels": [], "entities": []}, {"text": "The best scores for each metric is achieved by the tgen system.", "labels": [], "entities": []}, {"text": "Its BLEU score is 3 points above the score achieved by vanilla.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9806007146835327}]}, {"text": "The full system achieved the lowest scores in each metric.", "labels": [], "entities": []}, {"text": "Generally speaking, the deeper the impact of the syntactic manipulation the lower the wordoverlap based score.", "labels": [], "entities": []}, {"text": "This behaviour is explained by the fact that the baseline systems generate utterances which are syntactically similar to the most used structure in the gold-standard.", "labels": [], "entities": []}, {"text": "The other systems generate sentences whose style and structure is much rarer in the gold-standard.", "labels": [], "entities": []}, {"text": "For example, 59% of the reference utterances start with the standard pattern, whereas only 3% of the sentences generated by the full system follow this pattern.", "labels": [], "entities": []}, {"text": "Although there are multiple reference utterances, it is not likely that one of these follows the syntactic choices of the syntactically controlled systems.", "labels": [], "entities": []}, {"text": "human-written texts display the highest diversity across all scores.", "labels": [], "entities": []}, {"text": "The full system achieves the highest scores out of all systems.", "labels": [], "entities": []}, {"text": "Furthermore, both the vanilla and the tgen system obtain the lowest scores, thus, showing that the syntactic control mechanisms generate more diverse texts.", "labels": [], "entities": []}, {"text": "In two representative (cherry picked) examples are shown.", "labels": [], "entities": []}, {"text": "For one MR we compare the outputs of all systems.", "labels": [], "entities": []}, {"text": "In both examples the tgen and vanilla system produce utterances which follow the trivial pattern.", "labels": [], "entities": []}, {"text": "The uff-fw and full systems produce a different style of utterance by starting the sentence with a preposition.", "labels": [], "entities": []}, {"text": "The follow-fw system adds more variability to the utterance by starting the follow-up sentences with verbs (e.g. \"Located\") or nouns (\"Children\") instead of pronouns referring to the restaurant name.", "labels": [], "entities": []}, {"text": "The form system adds more variability by using different ways of phrasing an attribute-value pair (e.g. replacing \"high price range\" with \"expensive\").", "labels": [], "entities": []}, {"text": "We added a list of randomly sampled (non-cherrypicked) examples in Appendix B.  Here, * implies a statistical significant difference between a system and the tgen system, measured with two-tailed Student's t-test with p < 0.05  To measure the effectiveness of our approach, we performed an extensive human evaluation.", "labels": [], "entities": []}, {"text": "For this, we recruited judges from the Figure-Eight 4 platform.", "labels": [], "entities": []}, {"text": "For each experiment the sentence is rated by three different judges.", "labels": [], "entities": []}, {"text": "Quality and Naturalness To show that the syntactic manipulations do not deteriorate the utterances, we evaluated the quality and naturalness of the utterances produced by the different systems.", "labels": [], "entities": []}, {"text": "Here, quality is defined to measure the grammatical correctness, the fluency and the correctness of the content, whereas naturalness measures the likelihood that the utterance was written by a human.", "labels": [], "entities": []}, {"text": "For this, we sampled 250 MR's and generated the respective utterances for each system.", "labels": [], "entities": []}, {"text": "The judges rated all utterances on a Likert scale from 1 to 5 for quality and on a scale from 1 to 3 for naturalness 5 . shows the results for both the quality and naturalness evaluation.", "labels": [], "entities": []}, {"text": "Statistical significance is measured by means of a two-tailed Student's t-test between the tgen system and the other systems.", "labels": [], "entities": []}, {"text": "For quality there is no statistically significant difference between the tgen system and any other system.", "labels": [], "entities": []}, {"text": "For naturalness there is no statistically significant between tgen and the syntactically controlled systems.", "labels": [], "entities": []}, {"text": "However, there is a significant difference between tgen and vanilla.", "labels": [], "entities": []}, {"text": "In fact, the vanilla system is rated significantly higher in terms of naturalness than any other system.", "labels": [], "entities": []}, {"text": "For both metrics, the scores of all systems are very high, thus, we conclude that the syntactical control mechanisms do not deteriorate the utterances.", "labels": [], "entities": []}, {"text": "Subjective Analysis The main goal of the human evaluation is to understand how humans perceive the new utterances.", "labels": [], "entities": [{"text": "Subjective Analysis", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8979964554309845}]}, {"text": "For this, we compare the utterances of tgen and the full system by first sampling a MR, generate the utterance for each system, and let the human judges decide which of the two utterances they prefer.", "labels": [], "entities": []}, {"text": "Since preference is a very subjective measure that might not give complete insight, we asked the judges to also state which utterance they find more comprehensible (is the utterance easier to understand), more concise (does the utterance convey the information clearly with as little text as possible), more elegant (is the utterance more nicely written, more poetic, display higher variability) and more professional (could this text be written by an experienced and well trained writer).", "labels": [], "entities": []}, {"text": "shows the ratio at which the system was selected for each of the five aforementioned categories alongside the interannotator agreement computed with the Fleiss' \u03ba.", "labels": [], "entities": []}, {"text": "The results show that none of the two systems is significantly preferred by the judges, nor is any of the two systems rated as being more comprehensible.", "labels": [], "entities": []}, {"text": "However, the judges perceive the full system to produce significantly more elegant and professional utterances, i.e. in 71.6% of the comparisons the utterances by the full system were rated as more elegant and in 66.6% as being more professional.", "labels": [], "entities": []}, {"text": "On the other hand, the judges rate the utterances of the tgen system as being significantly more concise (75%).", "labels": [], "entities": []}, {"text": "There is a moderate correlation between the preference of an utterance and the elegance (Spearman's Rho \u03c1 = 0.557 ) or professionalism ( \u03c1 = 0.569 ).", "labels": [], "entities": [{"text": "Rho \u03c1", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.877326101064682}]}, {"text": "Furthermore, there is a weak correlation between comprehensibility of a system and the professionalism ( \u03c1 = 0.468).", "labels": [], "entities": []}, {"text": "However, we found that there is a strong correlation between the preference and the comprehensibility of an utterance (\u03c1 = 0.719).", "labels": [], "entities": []}, {"text": "The evaluation shows that although the utterances by the full system are rated as being more elegant and professional, they are not necessarily preferred.", "labels": [], "entities": []}, {"text": "The strongest indicator for preference is, thus, the comprehensibility.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Scores achieved for the corpus-based metrics by the  different systems. The value of the best system for each score  is highlighted in bold.", "labels": [], "entities": []}, {"text": " Table 4: Validation Accuracy scores for each classifier.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.8565245866775513}]}, {"text": " Table 5: Error Rate for each system, best system is high- lighted in bold. The sc subscript denotes the scores computed  by the classifiers.", "labels": [], "entities": [{"text": "Error Rate", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9854913055896759}]}, {"text": " Table 6: Diversity scores for each system and the human  texts. The highest score of a system is marked in bold.", "labels": [], "entities": []}, {"text": " Table 7: Quality and naturalness results from the user study.", "labels": [], "entities": []}, {"text": " Table 8: Sample output of the vanilla SC-LSTM (V) and the First Word Control (F) for four different MRs where one attribute- value is changed.", "labels": [], "entities": [{"text": "First Word Control (F)", "start_pos": 59, "end_pos": 81, "type": "METRIC", "confidence": 0.7017760177453359}]}, {"text": " Table 10: Results of the non-native speaking preference test.", "labels": [], "entities": []}, {"text": " Table 11: The most important formulations that appear in the training set for each attribute-value pair. Pairs with just a single  formulation were omitted.", "labels": [], "entities": []}]}