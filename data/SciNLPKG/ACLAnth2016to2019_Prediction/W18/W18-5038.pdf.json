{"title": [{"text": "Feudal Dialogue Management with Jointly Learned Feature Extractors", "labels": [], "entities": [{"text": "Feudal Dialogue Management", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8220734596252441}]}], "abstractContent": [{"text": "Reinforcement learning (RL) is a promising dialogue policy optimisation approach, but traditional RL algorithms fail to scale to large domains.", "labels": [], "entities": [{"text": "Reinforcement learning (RL)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8653263211250305}]}, {"text": "Recently, Feudal Dialogue Management (FDM), has shown to increase the scalability to large domains by decomposing the dialogue management decision into two steps, making use of the domain ontology to abstract the dialogue state in each step.", "labels": [], "entities": [{"text": "Feudal Dialogue Management (FDM)", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8003680954376856}]}, {"text": "In order to abstract the state space, however, previous work on FDM relies on handcrafted feature functions.", "labels": [], "entities": [{"text": "FDM", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.7499555945396423}]}, {"text": "In this work, we show that these feature functions can be learned jointly with the policy model while obtaining similar performance, even outperform-ing the handcrafted features in several environments and domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "In task-oriented Spoken Dialogue Systems (SDS), the Dialogue Manager (DM) (or policy) is the module in charge of deciding the next action in each dialogue turn.", "labels": [], "entities": [{"text": "Spoken Dialogue Systems (SDS)", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.7330921689669291}]}, {"text": "One of the most popular approaches to model the DM is Reinforcement Learning (RL)), having been studied for several years (.", "labels": [], "entities": []}, {"text": "However, as the dialogue state space increases, the number of possible trajectories needed to be explored grows exponentially, making traditional RL methods not scalable to large domains.", "labels": [], "entities": []}, {"text": "Recently, Feudal Dialogue Management (FDM) () has shown to increase the scalability to large domains.", "labels": [], "entities": [{"text": "Feudal Dialogue Management (FDM)", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8010307153066}]}, {"text": "This approach is based on Feudal RL (, * Currently at PolyAI, inigo@poly-ai.com a hierarchical RL method that divides a task spatially rather than temporally, decomposing the decisions into several steps and using different levels of abstraction for each sub-decision.", "labels": [], "entities": [{"text": "Feudal RL", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.7365439534187317}, {"text": "PolyAI", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.9721541404724121}]}, {"text": "When applied to domains with large state and action spaces, FDM showed an impressive performance increase compared to traditional RL policies.", "labels": [], "entities": [{"text": "FDM", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.6943169236183167}]}, {"text": "However, the method presented in, named FDQN 1 , relied on handcrafted feature functions in order to abstract the state space.", "labels": [], "entities": [{"text": "FDQN 1", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.741110622882843}]}, {"text": "These functions, named Domain Independent Parametrisation (DIP) (, are used to transform the belief of each slot into a fixed size representation using a large set of rules.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate that the feature functions needed to abstract the belief state in each sub-decision can be jointly learned with the policy.", "labels": [], "entities": []}, {"text": "We introduce two methods to do it, based on feed forward neural networks and recurrent neural networks respectively.", "labels": [], "entities": []}, {"text": "A modification of the original FDQN architecture is also introduced which stabilizes learning, avoiding overfitting of the policy to a single action.", "labels": [], "entities": [{"text": "FDQN", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.8231860995292664}]}, {"text": "Policies with jointly learned feature functions achieve similar performance to those using handcrafted ones, with superior performance in several environments and domains.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Sumarised description of the domains and  environments used in the experiments. Refer to  (Casanueva et al., 2017) for a detailed description.", "labels": [], "entities": []}, {"text": " Table 2: Reward after 4000 training dialogues  for FDQN+AF and FACER using DIP, FFN and  RNN features, compared to FDQN and the hand- crafted policy presented in the PyDial benchmarks  (HDC). The best performing model is highlighted  in bold while the best performing model with  jointly learned features is highlighted in red.", "labels": [], "entities": [{"text": "FDQN+AF", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.6716281572977701}, {"text": "FACER", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.6650756597518921}, {"text": "FDQN", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.8983293175697327}, {"text": "PyDial benchmarks  (HDC)", "start_pos": 167, "end_pos": 191, "type": "DATASET", "confidence": 0.7962229371070861}]}]}