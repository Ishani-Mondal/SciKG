{"title": [{"text": "Automated learning of templates for data-to-text generation: comparing rule-based, statistical and neural methods", "labels": [], "entities": [{"text": "data-to-text generation", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.7791247665882111}]}], "abstractContent": [{"text": "The current study investigated novel techniques and methods for trainable approaches to data-to-text generation.", "labels": [], "entities": [{"text": "data-to-text generation", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.7533749043941498}]}, {"text": "Neu-ral Machine Translation was explored for the conversion from data to text as well as the addition of extra templatization steps of the data input and text output in the conversion process.", "labels": [], "entities": [{"text": "Neu-ral Machine Translation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8320398728052775}]}, {"text": "Evaluation using BLEU did not find the Neural Machine Translation technique to perform any better compared to rule-based or Statistical Machine Translation, and the templatization method seemed to perform similarly or sometimes worse compared to direct data-to-text conversion.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9870534539222717}, {"text": "Neural Machine Translation", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.6974726716677347}, {"text": "Statistical Machine Translation", "start_pos": 124, "end_pos": 155, "type": "TASK", "confidence": 0.6212995052337646}]}, {"text": "However, the human evaluation metrics indicated that Neural Machine Translation yielded the highest quality output and that the templatization method was able to increase text quality in multiple situations.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.7491297523180643}]}], "introductionContent": [{"text": "Most approaches to data-to-text generation fall into one of two broad categories: rule-based or trainable (.", "labels": [], "entities": [{"text": "data-to-text generation", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.7658763229846954}]}, {"text": "Rule-based systems are often characterised by a templatebased design: texts with gaps that can be filled with information.", "labels": [], "entities": []}, {"text": "The application of these templates generally results in high quality text (e.g. van).", "labels": [], "entities": []}, {"text": "The text quality of trainable systems -e.g. statistical models that select content based on what is the most likely realization according to probability -is generally lower and their development slower (.", "labels": [], "entities": []}, {"text": "However, trainable systems use data-driven algorithms and do not rely on manually written resources for text generation, while most template systems require manually written templates and rules for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 104, "end_pos": 119, "type": "TASK", "confidence": 0.7596102356910706}, {"text": "text generation", "start_pos": 198, "end_pos": 213, "type": "TASK", "confidence": 0.7512454986572266}]}, {"text": "This makes trainable systems potentially more adaptable and maintainable.", "labels": [], "entities": []}, {"text": "Different approaches have been tried to decrease the building time and cost of data-to-text systems associated with trainable approaches, while limiting the drop in output quality compared to rule-based data-totext systems (e.g. Adeyanju, 2012; by experimenting with the trainable method.", "labels": [], "entities": []}, {"text": "The goal of the current study was to explore the combination of template and trainable approaches by giving statistical and deep learning-based systems templatized input to create templatized output.", "labels": [], "entities": []}, {"text": "The more homogeneous nature of this templatized form was expected to make production of output that is fluent and clear as well as an accurate representation of the data more feasible compared to their untemplatized counterpart, generally used for trainable approaches.", "labels": [], "entities": []}, {"text": "Furthermore, the usage of statistical and deep learning methods reduces the reliance on manually written resources that is associated with most template based systems.", "labels": [], "entities": []}, {"text": "The approach of the current study was tested on four corpora in the sports and weather domain, each with divergent characteristics, to assess the usefulness in different situations.", "labels": [], "entities": []}, {"text": "The output of these systems is compared using automated metrics (i.e. BLEU) as well as human evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.996783971786499}]}], "datasetContent": [{"text": "A total of four different datasets were used in the current study, two datasets contain weather reports and two contain sports reports.", "labels": [], "entities": []}, {"text": "Furthermore, one weather dataset and one sports dataset contain texts that resulted from (mainly) rule-based data-to-text generation, while the other weather and sports datasets contain human-written texts.", "labels": [], "entities": [{"text": "rule-based data-to-text generation", "start_pos": 98, "end_pos": 132, "type": "TASK", "confidence": 0.6902286410331726}]}, {"text": "Characteristics of these datasets are described in and below.", "labels": [], "entities": []}, {"text": "This dataset contains human-written texts on wind data.", "labels": [], "entities": []}, {"text": "The dataset contains a total of 601 lines.", "labels": [], "entities": []}, {"text": "The original input vector representation was also modified to a tagged input representation inspired by the tagged input vector of.", "labels": [], "entities": []}, {"text": "The quality of the generated sentences was assessed using NLTK's corpus bleu that calculates BLEU scores based on 1-grams to 4-grams with equal weights and accounts fora micro-average precision score based on  -for translation tasks, which the current task is in someway.", "labels": [], "entities": [{"text": "NLTK's corpus bleu", "start_pos": 58, "end_pos": 76, "type": "DATASET", "confidence": 0.9439449459314346}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9990116357803345}, {"text": "precision score", "start_pos": 184, "end_pos": 199, "type": "METRIC", "confidence": 0.9373957812786102}]}, {"text": "Furthermore, correlations have been found between automated metrics and human ratings (e.g.).", "labels": [], "entities": []}, {"text": "Therefore, the BLEU scores were seen as a first step to investigate differences between methods and corpora.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9983267188072205}]}, {"text": "The BLEU scores show that the computergenerated corpora yielded the best results, with Weather.gov showing the best performance compared to the other corpora with BLEU scores for the lexicalized output varying from 34.52 (retrieval using the templatization method) to 78.90 (NMT using the direct method).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9987730383872986}, {"text": "BLEU", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9969179630279541}]}, {"text": "This seems intuitively logical since the Weather.gov corpus is relatively large, and the sentences are also the most homogeneous out of the corpora, which makes producing output similar to the training data a feasible task.", "labels": [], "entities": [{"text": "Weather.gov corpus", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.9587431848049164}]}, {"text": "Results for the smaller Robocup soccer corpus are decent, but not as good as Weather.gov with BLEU scores for the lexicalized output ranging from 22.38 (retrieval using the direct method) to 39.04 (SMT using the direct method).", "labels": [], "entities": [{"text": "Robocup soccer corpus", "start_pos": 24, "end_pos": 45, "type": "DATASET", "confidence": 0.8228812019030253}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9991177916526794}, {"text": "SMT", "start_pos": 198, "end_pos": 201, "type": "TASK", "confidence": 0.9405901432037354}]}, {"text": "While Prodigy-METEO is human-written, its sentence structure is still quite consistent, which might explain why its BLEU scores are not that far removed from those for computer-generated corpora with scores for the lexicalized output between 23.66 (retrieval using the direct method) and 30.37 (SMT using the direct method).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.998710036277771}, {"text": "SMT", "start_pos": 295, "end_pos": 298, "type": "TASK", "confidence": 0.94831383228302}]}, {"text": "Low BLEU scores were obtained for sentences from the Dutch Soccer corpus, with lexicalized output ranging from 0.90 (SMT using the templatization method) to 4.99 (Retrieval using the direct method).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9990983009338379}, {"text": "Dutch Soccer corpus", "start_pos": 53, "end_pos": 72, "type": "DATASET", "confidence": 0.956636925538381}]}, {"text": "The low BLEU scores might indicate two things.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 8, "end_pos": 19, "type": "METRIC", "confidence": 0.9630354642868042}]}, {"text": "First, it is possible that the systems struggle with the heterogeneous nature of the Dutch Soccer texts which results in low text quality output.", "labels": [], "entities": [{"text": "Dutch Soccer texts", "start_pos": 85, "end_pos": 103, "type": "DATASET", "confidence": 0.9641531904538473}]}, {"text": "However, the same heterogeneous nature might also make it difficult to use BLEU scores as an indication for text quality, since it is known to be difficult to find a good gold standard for corpora with diverse language.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9974120259284973}]}, {"text": "BLEU scores for techniques do not show large differences: especially the sentences generated by SMT and NMT obtained close BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9892873764038086}, {"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.8196572661399841}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9989908337593079}]}, {"text": "Interestingly, the sentences produced using cosine similarity based retrieval seems to be consistently outperformed by the translation methods, with the exception of the Dutch Soccer corpus, which suggests that text generation is preferred oversimple retrieval.", "labels": [], "entities": [{"text": "Dutch Soccer corpus", "start_pos": 170, "end_pos": 189, "type": "DATASET", "confidence": 0.9666144450505575}, {"text": "text generation", "start_pos": 211, "end_pos": 226, "type": "TASK", "confidence": 0.6991927772760391}]}, {"text": "The templatized (filled) and direct methods also scored roughly equal.", "labels": [], "entities": []}, {"text": "The exception involves the Weather.gov corpus, where the direct method resulted in much higher BLEU scores compared to its templatized counterpart.", "labels": [], "entities": [{"text": "Weather.gov corpus", "start_pos": 27, "end_pos": 45, "type": "DATASET", "confidence": 0.9620640873908997}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9988694787025452}]}, {"text": "Although the results are equal, the metrics show a large decrease in BLEU scores when lexicalizing the templates.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.999136745929718}]}, {"text": "This means that the templatization method has the potential to significantly outperform the direct method if the quality of the lexicalization step is improved.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Characteristics of the (text-part of the) corpora used  in this study.", "labels": [], "entities": []}, {"text": " Table 6: MOSES parameters per corpus.", "labels": [], "entities": [{"text": "MOSES parameters per corpus", "start_pos": 10, "end_pos": 37, "type": "DATASET", "confidence": 0.5708525031805038}]}, {"text": " Table 7: OpenNMT parameters per corpus.", "labels": [], "entities": []}, {"text": " Table 8: BLEU scores obtained for the different corpora with the techniques used in this study.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992546439170837}]}]}