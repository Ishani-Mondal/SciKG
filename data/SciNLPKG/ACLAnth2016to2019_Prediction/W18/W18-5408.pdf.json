{"title": [{"text": "Understanding Convolutional Neural Networks for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.786975085735321}]}], "abstractContent": [{"text": "We present an analysis into the inner workings of Convolutional Neural Networks (CNNs) for processing text.", "labels": [], "entities": []}, {"text": "CNNs used for computer vision can be interpreted by projecting filters into image space, but for discrete sequence inputs CNNs remain a mystery.", "labels": [], "entities": []}, {"text": "We aim to understand the method by which the networks process and classify text.", "labels": [], "entities": []}, {"text": "We examine common hypotheses to this problem: that filters, accompanied by global max-pooling, serve as ngram detectors.", "labels": [], "entities": []}, {"text": "We show that filters may capture several different semantic classes of ngrams by using different activation patterns, and that global max-pooling induces behavior which separates important ngrams from the rest.", "labels": [], "entities": []}, {"text": "Finally, we show practical use cases derived from our findings in the form of model interpretability (explaining a trained model by deriving a concrete identity for each filter, bridging the gap between visualization tools in vision tasks and NLP) and prediction inter-pretability (explaining predictions).", "labels": [], "entities": []}], "introductionContent": [{"text": "Convolutional Neural Networks (CNNs), originally invented for computer vision, have been shown to achieve strong performance on text classification tasks () as well as other traditional Natural Language Processing (NLP) tasks), even when considering relatively simple one-layer models.", "labels": [], "entities": [{"text": "Convolutional Neural Networks (CNNs)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6671269585688909}, {"text": "text classification tasks", "start_pos": 128, "end_pos": 153, "type": "TASK", "confidence": 0.7985237240791321}]}, {"text": "As with other architectures of neural networks, explaining the learned functionality of CNNs is still an active research area.", "labels": [], "entities": []}, {"text": "The ability to interpret neural models can be used to increase trust in model predictions, analyze errors or improve the model (.", "labels": [], "entities": []}, {"text": "The problem of interpretability in machine learning can be divided into two concrete tasks: Given a trained model, model interpretability aims to supply a structured explanation which captures what the model has learned.", "labels": [], "entities": []}, {"text": "Given a trained model and a single example, prediction interpretability aims to explain how the model arrived at its prediction.", "labels": [], "entities": [{"text": "prediction interpretability", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.9654086232185364}]}, {"text": "These can be further divided into white-box and black-box techniques.", "labels": [], "entities": []}, {"text": "While recent works have begun to supply the means of interpreting predictions, interpreting neural NLP models remains an under-explored area.", "labels": [], "entities": [{"text": "interpreting predictions", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.9270170032978058}, {"text": "interpreting neural NLP", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.905234714349111}]}, {"text": "Accompanying their rising popularity, CNNs have seen multiple advances in interpretability when used for computer vision tasks).", "labels": [], "entities": []}, {"text": "These techniques unfortunately do not trivially apply to discrete sequences, as they assume a continuous input space used to represent images.", "labels": [], "entities": []}, {"text": "Intuitions about how CNNs work on an abstract level also may not carryover from image inputs to text-for example, pooling in CNNs has been used to induce deformation invariance (, which is likely different than the role it has when processing text.", "labels": [], "entities": []}, {"text": "In this work, we examine and attempt to understand how CNNs process text, and then use this information for the more practical goals of improving model-level and prediction-level explanations.", "labels": [], "entities": []}, {"text": "We identify and refine current intuitions as to how CNNs work.", "labels": [], "entities": []}, {"text": "Specifically, current common wisdom suggests that CNNs classify text by working through the following steps (Goldberg, 2016): 1) 1-dimensional convolving filters are used as ngram detectors, each filter specializing in a closely-related family of ngrams.", "labels": [], "entities": []}, {"text": "2) Max-pooling overtime extracts the relevant ngrams for making a decision.", "labels": [], "entities": []}, {"text": "3) The rest of the network classifies the text based on this information.", "labels": [], "entities": []}, {"text": "We refine items 1 and 2 and show that: \u2022 Max-pooling induces a thresholding behavior, and values below a given threshold are ignored when (i.e. irrelevant to) making a prediction.", "labels": [], "entities": []}, {"text": "Specifically, we show an experiment for which 40% of the pooled ngrams on average can be dropped with no loss of performance (Section 4).", "labels": [], "entities": []}, {"text": "\u2022 Filters are not homogeneous, i.e. a single filter can, and often does, detect multiple distinctly different families of ngrams (Section 5.3).", "labels": [], "entities": []}, {"text": "\u2022 Filters also detect negative items in ngramsthey not only select fora family of ngrams but often actively suppress a related family of negated ngrams (Section 5.4).", "labels": [], "entities": []}, {"text": "We also show that the filters are trained to work with naturally-occurring ngrams, and can be easily misled (made to produce values substantially larger than their expected range) by selected nonnatural ngrams.", "labels": [], "entities": []}, {"text": "These findings can be used for improving model-level and prediction-level interpretability (Section 6).", "labels": [], "entities": [{"text": "prediction-level interpretability", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.5577140599489212}]}, {"text": "Concretely: 1) We improve model interpretability by deriving a useful summary for each filter, highlighting the kinds of structures it is sensitive to.", "labels": [], "entities": []}, {"text": "2) We improve prediction interpretability by focusing on informative ngrams and taking into account also the negative cues.", "labels": [], "entities": [{"text": "prediction interpretability", "start_pos": 14, "end_pos": 41, "type": "TASK", "confidence": 0.9545302391052246}]}], "datasetContent": [{"text": "For our empirical experiments and results presented in this work we use three text classification datasets for Sentiment Analysis, which involves classifying the input text (user reviews in all cases) between positive and negative.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.9371660053730011}]}, {"text": "The specific datasets were chosen for their relative variety in size and domain as well as for the relative simplicity and interpretability of the binary sentiment analysis task.", "labels": [], "entities": [{"text": "binary sentiment analysis task", "start_pos": 147, "end_pos": 177, "type": "TASK", "confidence": 0.7727817073464394}]}, {"text": "The three datasets are: a) MR: sentence polarity dataset v1.0 introduced by, containing 10k evenly split short (sentences or snippets) movie reviews.", "labels": [], "entities": [{"text": "MR", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9740930795669556}]}, {"text": "b) Elec: electronic product reviews for sentiment classification introduced by, assembled from the Amazon review dataset For word embeddings, we use the pre-trained GloVe Wikipedia 2014-Gigaword 5 embeddings (), which we fine-tune with the model.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.9642342925071716}, {"text": "Amazon review dataset", "start_pos": 99, "end_pos": 120, "type": "DATASET", "confidence": 0.7473477125167847}, {"text": "GloVe Wikipedia 2014-Gigaword 5 embeddings", "start_pos": 165, "end_pos": 207, "type": "DATASET", "confidence": 0.8965725302696228}]}, {"text": "We use embedding dimension of 50, filter sizes of \u2208 {2, 3, 4} words, and m \u2208 {10, 50} filters.", "labels": [], "entities": []}, {"text": "Models are implemented in PyTorch and trained with the Adam optimizer.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top ngrams and words by filter from a sample of nine filters from the Elec model. The average difference  between the top natural ngram activation and the top possible ngram activation for this model is 2.5, or a 30%  average reduction.", "labels": [], "entities": []}, {"text": " Table 2: Top-k words by slot scores and top-k ngrams by filter scores from the Elec model. In bold are words from  the top-k ngrams which appear in the top-k slot words -i.e. words which maximize their slot.", "labels": [], "entities": []}, {"text": " Table 3: Example clustering results on the Yelp dataset.  After applying thresholds, the ngrams for this filter  were split into two clusters of sizes 83% and 17% re- spectively. The table shows top-scoring ngrams for this  filter with their clustering results, sorted by their acti- vation strength.", "labels": [], "entities": [{"text": "Yelp dataset", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.9842714667320251}]}, {"text": " Table 4: Top-scoring ngrams from one filter from a  model trained on the Elec dataset, and their accompa- nying lowest-scoring negative ngrams. We selected a  hamming distance of 1 word. Bold ngrams are Case 2  negative ngrams.", "labels": [], "entities": [{"text": "Elec dataset", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.9188346862792969}]}]}