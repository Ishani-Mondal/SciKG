{"title": [{"text": "TencentFmRD Neural Machine Translation for WMT18", "labels": [], "entities": [{"text": "TencentFmRD Neural Machine Translation", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.5066586583852768}, {"text": "WMT18", "start_pos": 43, "end_pos": 48, "type": "TASK", "confidence": 0.5203297734260559}]}], "abstractContent": [{"text": "This paper describes the Neural Machine Translation (NMT) system of TencentFmRD for Chinese\u2194English news translation tasks of WMT 2018.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.7446159541606903}, {"text": "Chinese\u2194English news translation", "start_pos": 84, "end_pos": 116, "type": "TASK", "confidence": 0.6350778579711914}, {"text": "WMT 2018", "start_pos": 126, "end_pos": 134, "type": "DATASET", "confidence": 0.8480775952339172}]}, {"text": "Our systems are neural machine translation systems trained with our original system TenTrans.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.7815661430358887}, {"text": "TenTrans", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.9563388824462891}]}, {"text": "TenTrans is an improved NMT system based on Transformer self-attention mechanism.", "labels": [], "entities": [{"text": "TenTrans", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9216723442077637}]}, {"text": "In addition to the basic settings of Transformer training, TenTrans uses multi-model fusion techniques, multiple features reranking, different segmentation models and joint learning.", "labels": [], "entities": []}, {"text": "Finally, we adopt some data selection strategies to fine-tune the trained system and achieve a stable performance improvement.", "labels": [], "entities": []}, {"text": "Our Chinese\u2192English system achieved the second best BLEU scores and fourth best cased BLEU scores among all WMT18 submitted systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9988183379173279}, {"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.8907386660575867}, {"text": "WMT18", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.801818311214447}]}], "introductionContent": [{"text": "End-to-end neural machine translation () based on self-attention mechanism (, the Transformer, has become promising paradigm in field of machine translation academia and industry.", "labels": [], "entities": [{"text": "End-to-end neural machine translation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6118727698922157}, {"text": "machine translation academia", "start_pos": 137, "end_pos": 165, "type": "TASK", "confidence": 0.7957693239053091}]}, {"text": "Experiments show Transformer, which does not rely on any convolutional or recurrent networks, to be superior in translation performance while being more parallelizable and requiring significantly less time to train.", "labels": [], "entities": [{"text": "translation", "start_pos": 112, "end_pos": 123, "type": "TASK", "confidence": 0.9620704650878906}]}, {"text": "The training part of this paper is an improvement on the tensor2tensor 1 open source project based on the Transformer architecture, and the inference part is completely original, and we called this system TenTrans.", "labels": [], "entities": [{"text": "TenTrans", "start_pos": 205, "end_pos": 213, "type": "DATASET", "confidence": 0.9142188429832458}]}, {"text": "We participated in two directions of translation tasks: English\u2192Chinese and Chinese\u2192English.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.9075115621089935}]}, {"text": "We divide TenTrans system into three parts to introduce in this paper.", "labels": [], "entities": []}, {"text": "First, we introduce how 1 https://github.com/tensorflow/tensor2tensor to train better translation model, that is, the training phase.", "labels": [], "entities": []}, {"text": "Second, we describe how good models can generate better translation candidates, that is, the inference phase.", "labels": [], "entities": []}, {"text": "Finally, we describe Nbest rescoring phase, which ensures that translation results which are closer to the expression typically produced by users are chosen.", "labels": [], "entities": []}, {"text": "Our experimental setup is based on recent promising techniques in NMT, including using Byte Pair Encoding (BPE) () and mixed word/character segmentation rather than words as modeling units to achieve open-vocabulary translation (, using backtranslation () method and joint training () applied to make use of monolingual data to enhance training data.", "labels": [], "entities": [{"text": "NMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9424210786819458}, {"text": "Byte Pair Encoding (BPE)", "start_pos": 87, "end_pos": 111, "type": "METRIC", "confidence": 0.6574623684088389}, {"text": "word/character segmentation", "start_pos": 125, "end_pos": 152, "type": "TASK", "confidence": 0.7337478995323181}, {"text": "open-vocabulary translation", "start_pos": 200, "end_pos": 227, "type": "TASK", "confidence": 0.724939838051796}]}, {"text": "And we also improve the performance using an ensemble based on six variants of the same network, which are trained with different parameter settings.", "labels": [], "entities": []}, {"text": "In addition, we design multi-dimensional features for strategic integration to select the best candidate from n-best translation lists.", "labels": [], "entities": []}, {"text": "Then we perform minimum error rate training (MERT)) on validation set to give different features corresponding reasonable weights.", "labels": [], "entities": [{"text": "minimum error rate training (MERT))", "start_pos": 16, "end_pos": 51, "type": "METRIC", "confidence": 0.8542451092175075}]}, {"text": "And we process named entities, such as person name, location name and organization name into generalization types in order to improve the performance of unknown named entity translation (.", "labels": [], "entities": [{"text": "unknown named entity translation", "start_pos": 153, "end_pos": 185, "type": "TASK", "confidence": 0.8017917275428772}]}, {"text": "Finally, we adopt some data selection strategies () to fine-tune the trained system and achieve a stable performance improvement.", "labels": [], "entities": []}, {"text": "Our Chinese\u2192English system achieved the second best BLEU () scores and fourth best cased BLEU scores among all WMT18 submitted systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.998703122138977}, {"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9324607253074646}, {"text": "WMT18", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.8184090852737427}]}, {"text": "The remainder of this paper is organized as follows: Section 2 describes the system architecture of TenTrans.", "labels": [], "entities": [{"text": "TenTrans", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.9271640181541443}]}, {"text": "Section 3 states all experimental techniques used in WMT18 news translation tasks.", "labels": [], "entities": [{"text": "WMT18 news translation tasks", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.8171893656253815}]}, {"text": "Sections 4 shows designed features for reranking n-best lists.", "labels": [], "entities": [{"text": "reranking n-best lists", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.8343558311462402}]}, {"text": "Section 5 shows experimental settings and results.", "labels": [], "entities": []}, {"text": "Finally, we conclude in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section mainly introduces the techniques used in training and inference phase.", "labels": [], "entities": []}, {"text": "This section focuses on the features designed to help choose translation results which are closer to the way normal user expressions -that is, it focuses on the N -best rescoring phase.", "labels": [], "entities": []}, {"text": "Several features designed in this work can be seen in the left part of third phase in.", "labels": [], "entities": []}, {"text": "In all experiments, we report case-sensitive and detokenized BLEU using the NIST BLEU scorer.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9757037162780762}, {"text": "NIST", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.6840590238571167}, {"text": "BLEU scorer", "start_pos": 81, "end_pos": 92, "type": "METRIC", "confidence": 0.9400907158851624}]}, {"text": "For Chinese output, we split to characters using the script supplied for WMT18 before running BLEU.", "labels": [], "entities": [{"text": "WMT18", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.9696512222290039}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.989471435546875}]}, {"text": "In training and decoding phase, the Chinese sentences are segmented using NiuTrans () Segmenter.", "labels": [], "entities": [{"text": "NiuTrans", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.9279812574386597}]}, {"text": "For English sentences, we use the Moses ( tokenizer . We used all the training data of WMT2018 Chinese\u2194English Translation tasks, firstly filtering out bilingual sentences with unrecognizable code, large length ratio difference, duplications and wrong language coding, then filtering out bilingual sentences with poor mutual translation rate by using fast-align toolkit.", "labels": [], "entities": [{"text": "WMT2018 Chinese\u2194English Translation", "start_pos": 87, "end_pos": 122, "type": "TASK", "confidence": 0.845167624950409}]}, {"text": "After data cleaning, 18.5 million sentence pairs remained.", "labels": [], "entities": []}, {"text": "We used beam search with abeam size of 12, length penalty \u03b1 = 0.8 for Chinese\u2192English system and length penalty \u03b1 = 1.0 for English\u2192Chinese system.", "labels": [], "entities": [{"text": "length penalty \u03b1", "start_pos": 43, "end_pos": 59, "type": "METRIC", "confidence": 0.9799745480219523}, {"text": "length penalty \u03b1", "start_pos": 97, "end_pos": 113, "type": "METRIC", "confidence": 0.970460832118988}]}, {"text": "In order to recover the case information, we use Moses toolkit to train SMT-based recaser on English corpus.", "labels": [], "entities": [{"text": "SMT-based recaser", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.807854413986206}]}, {"text": "In addition, we also use some simple rules to restore the case information of the results.", "labels": [], "entities": []}, {"text": "The size of the Chinese vocabulary and English vocabulary is 64k and 50k respectively after BPE operation.", "labels": [], "entities": [{"text": "BPE", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.5535807013511658}]}, {"text": "shows the Chinese\u2194English translation results on development set of WMT2018.", "labels": [], "entities": [{"text": "WMT2018", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.8672032356262207}]}, {"text": "Wherein reranking refers to multi-feature based rescore method mentioned above.", "labels": [], "entities": []}, {"text": "The submitted system in has slightly better performance than is seen in the previous experiment because we have manually written some rules.", "labels": [], "entities": []}, {"text": "As can be seen from the, when we increase the size of n-best from 12 to 100, the performance is improved by 0.41 BLEU after reranking based on multiple features.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.999406099319458}]}], "tableCaptions": [{"text": " Table 1: Chinese\u2194English BLEU results on WMT18  validation set. The \"C\" and \"E\" denotes Chinese and  English respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9952060580253601}, {"text": "WMT18  validation set", "start_pos": 42, "end_pos": 63, "type": "DATASET", "confidence": 0.9045433004697164}]}]}