{"title": [{"text": "Dual Conditional Cross-Entropy Filtering of Noisy Parallel Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "In this work we introduce dual conditional cross-entropy filtering for noisy parallel data.", "labels": [], "entities": []}, {"text": "For each sentence pair of the noisy parallel corpus we compute cross-entropy scores according to two inverse translation models trained on clean data.", "labels": [], "entities": []}, {"text": "We penalize divergent cross-entropies and weigh the penalty by the cross-entropy average of both models.", "labels": [], "entities": []}, {"text": "Sorting or thresholding according to these scores results in better subsets of parallel data.", "labels": [], "entities": []}, {"text": "We achieve higher BLEU scores with models trained on parallel data filtered only from Paracrawl than with models trained on clean WMT data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9993550181388855}, {"text": "Paracrawl", "start_pos": 86, "end_pos": 95, "type": "DATASET", "confidence": 0.9853809475898743}, {"text": "WMT data", "start_pos": 130, "end_pos": 138, "type": "DATASET", "confidence": 0.768921971321106}]}, {"text": "We further evaluate our method in the context of the WMT2018 shared task on parallel corpus filtering and achieve the overall highest ranking scores of the shared task, scoring top in three out of four subtasks.", "labels": [], "entities": [{"text": "WMT2018 shared task", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.6485992272694906}, {"text": "parallel corpus filtering", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.5633240838845571}]}], "introductionContent": [{"text": "Recently, large web-crawled parallel corpora which are meant to rival non-public resources held by popular machine translation providers have been made publicly available to the research community inform of the Paracrawl corpus.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7633787095546722}, {"text": "Paracrawl corpus", "start_pos": 211, "end_pos": 227, "type": "DATASET", "confidence": 0.9220522940158844}]}, {"text": "At the same time, it has been shown that neural translation models are far more sensitive to noisy parallel training data than phrase-based statistical machine translation methods (.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.722125306725502}, {"text": "phrase-based statistical machine translation", "start_pos": 127, "end_pos": 171, "type": "TASK", "confidence": 0.5914528369903564}]}, {"text": "This creates the need for data selection methods that can filter harmful sentence pairs from these large resources.", "labels": [], "entities": []}, {"text": "In this paper, we introduce dual conditional cross-entropy filtering, a simple but effective data selection method for noisy parallel corpora.", "labels": [], "entities": [{"text": "dual conditional cross-entropy filtering", "start_pos": 28, "end_pos": 68, "type": "TASK", "confidence": 0.6000047326087952}]}, {"text": "We think of it as the missing adequacy component to the fluency aspects of cross-entropy difference filtering by.", "labels": [], "entities": [{"text": "cross-entropy difference filtering", "start_pos": 75, "end_pos": 109, "type": "TASK", "confidence": 0.5996938447157542}]}, {"text": "Similar to Moore-Lewis filtering for monolingual data, we https://paracrawl.eu directly select samples that have the potential to improve perplexity (and in our case translation performance) of models trained with the filtered data.", "labels": [], "entities": []}, {"text": "This is different from Axelrod et al.", "labels": [], "entities": []}, {"text": "(2011) who simply expand Moore and Lewis filtering to both sides of the parallel corpus.", "labels": [], "entities": []}, {"text": "We use conditional probability distributions and enforce agreement between inverse translation directions.", "labels": [], "entities": []}, {"text": "In most cases, neural translation models are trained to minimize perplexity (or cross-entropy) on a training set.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7360434830188751}]}, {"text": "Our selection criterion includes the optimization criterion of neural machine translation which we approximate by using neural translation models pre-trained on clean seed data.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.7058861056963602}]}, {"text": "We evaluated our method in the context of the WMT2018 Shared Task on Parallel Corpus Filtering ( ) and submitted our best method to the task.", "labels": [], "entities": [{"text": "WMT2018 Shared Task on Parallel Corpus Filtering", "start_pos": 46, "end_pos": 94, "type": "TASK", "confidence": 0.6561829703194755}]}, {"text": "Although we only optimized for one of the four subtasks of the shared task, our submission scored highest for three out of four subtasks and third for the fourth subtask; there were 48 submissions to each subtask in total.", "labels": [], "entities": []}], "datasetContent": [{"text": "As required by the shared task, we use Marian () to train our development systems.", "labels": [], "entities": []}, {"text": "We follow the recommended settings quite closely in terms of model architecture, but change training settings, favoring hyperparameters that lead to quicker convergence during our own development phase.", "labels": [], "entities": []}, {"text": "We switched off synchronous ADAM in favor of asynchronous ADAM, increased the evaluation frequency to once per 5000 updates and increased work-space size to 5000MB per GPU.", "labels": [], "entities": []}, {"text": "We also set the initial learningrate to 0.0003 instead of 0.0001 and used an inverse square-root decaying scheme for the learning rate () that started after 16,000 updates.", "labels": [], "entities": []}, {"text": "We removed dropout of source and target words and decreased variational dropout from 0.2 to 0.1 (.", "labels": [], "entities": []}, {"text": "With these settings, our models usually converged within 10 to 15 hours of training on four NVidia Titan Xp GPUs.", "labels": [], "entities": [{"text": "NVidia Titan Xp GPUs", "start_pos": 92, "end_pos": 112, "type": "DATASET", "confidence": 0.9493626654148102}]}, {"text": "Convergence was assumed if perplexity did not improve for 5 consecutive evaluation steps.", "labels": [], "entities": []}, {"text": "We evaluated on the provided WMT2016 and WMT2017 test sets.", "labels": [], "entities": [{"text": "WMT2016", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.9554334282875061}, {"text": "WMT2017 test sets", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.9510697921117147}]}, {"text": "We produce a single score f (x, y) per sentence pair (x, y) as the product of partial scores f i (x, y): Partial scores take values between 0 and 1, as does the total score f . Partial scores that might generate values outside that range are clipped.", "labels": [], "entities": []}, {"text": "We assume that sentence pairs with a score of 0 are excluded from the training data.", "labels": [], "entities": []}, {"text": "In this section, we describe the scores explored in this work and present results on the development data.", "labels": [], "entities": []}, {"text": "Following the training recipe in Section 2.2, we first trained a model (\"WMT18-full\" in) on the admissible parallel WMT18 data for GermanEnglish (excluding Paracrawl).", "labels": [], "entities": [{"text": "WMT18 data", "start_pos": 116, "end_pos": 126, "type": "DATASET", "confidence": 0.8441649377346039}, {"text": "GermanEnglish", "start_pos": 131, "end_pos": 144, "type": "DATASET", "confidence": 0.7767654657363892}]}, {"text": "This model is only used for the computation of reference BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9773632287979126}]}, {"text": "Next, we trained a German-English model on randomly scored Paracrawl data only (\"random\" in).", "labels": [], "entities": [{"text": "Paracrawl data", "start_pos": 59, "end_pos": 73, "type": "DATASET", "confidence": 0.8964751362800598}]}, {"text": "The random scores -uniformly sampled values between 0 and 1 -were used to select representative data consisting of 100M words from unprocessed Paracrawl while using the thresholdbased selection tool provided by the shared task organizers.", "labels": [], "entities": [{"text": "Paracrawl", "start_pos": 143, "end_pos": 152, "type": "DATASET", "confidence": 0.9613052010536194}]}, {"text": "Results for WMT16 and WMT17 test sets for both systems are shown in.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.9093561768531799}, {"text": "WMT17 test sets", "start_pos": 22, "end_pos": 37, "type": "DATASET", "confidence": 0.9330039024353027}]}, {"text": "The Paracrawl-trained systems (random) has dramatically worse BLEU scores than the WMT18-trained system.", "labels": [], "entities": [{"text": "Paracrawl-trained", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8971137404441833}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9994207620620728}, {"text": "WMT18-trained", "start_pos": 83, "end_pos": 96, "type": "DATASET", "confidence": 0.7449830174446106}]}, {"text": "Upon manual inspection, we see many untranslated and partially copied sentences in the case of the randomly-selected Paracrawl system.", "labels": [], "entities": [{"text": "Paracrawl system", "start_pos": 117, "end_pos": 133, "type": "DATASET", "confidence": 0.9067407548427582}]}], "tableCaptions": [{"text": " Table 3: Top-5 out of 48 submissions for each of the four sub-tasks and total sum", "labels": [], "entities": []}]}