{"title": [], "abstractContent": [{"text": "Vector space models of words in NLP-word embeddings-have been recently shown to reliably encode semantic information , offering capabilities such as solving proportional analogy tasks such as man:woman::king:queen.", "labels": [], "entities": []}, {"text": "We study how well these distributional properties carryover to similarly learned phoneme embed-dings, and whether phoneme vector spaces align with articulatory distinctive features, using several methods of obtaining such continuous-space representations.", "labels": [], "entities": []}, {"text": "We demonstrate a statistically significant correlation between distinctive feature spaces and vector spaces learned with word-context PPMI+SVD and word2vec, showing that many distinctive feature contrasts are implicitly present in phoneme distributions.", "labels": [], "entities": []}, {"text": "Furthermore, these distributed representations allow us to solve proportional analogy tasks with phonemes, such asp is to b as t is to X, where the solution is that X = d.", "labels": [], "entities": [{"text": "proportional analogy tasks", "start_pos": 65, "end_pos": 91, "type": "TASK", "confidence": 0.765287866195043}]}, {"text": "This effect is even stronger when a supervision signal is added where we extract phoneme representations from the embedding layer of an recurrent neu-ral network that is trained to solve a word inflection task, i.e. a model that is made aware of word relatedness.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional word representations, or word embeddings, have attracted much attention in NLP, and their success is considered a vindication of the distributional hypothesis for lexical semantics espoused much earlier by the likes of Wittgenstein, Firth, Harris, and other contemporaries.", "labels": [], "entities": [{"text": "Distributional word representations", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6467673579851786}]}, {"text": "Often overlooked is that this hypothesis among linguists has extended itself much wider to include phonology and grammar: \"all elements of speech (phonological, lexical, and grammatical) are now to be defined and classified in terms of their relations to one another\".", "labels": [], "entities": []}, {"text": "Given the successes of distributional models not only in specifying semantic similarity, but also addressing proportional analogy tasks, we want to investigate if distributional representations of phonemes induce a similarly coherent space as lexical items do, and if the properties of such spaces conform to linguistic expectations, a question posed in another form as early as in.", "labels": [], "entities": []}, {"text": "We address these questions using three different methods of obtaining vector representations of phonemes; two unsupervised models that associate phonemes and the contexts (neighboring phonemes) they occur in (word2vec, PPMI+SVD), and one model where we are given lemmas and inflected forms as supervised data fora recurrent neural network (RNN) encoder-decoder model that is trained to perform such inflections from where we extract the vector representations in the embedding layer.", "labels": [], "entities": []}, {"text": "The latter method has a weak supervision signal in that the system knows which words are truly related forms, since it is trained only on word pairs where one of the words is a result of transforming the other one according to inflectional features.", "labels": [], "entities": []}, {"text": "Hence, this latter model can presumably better pickup on phonological alternations along distinctive feature lines that occur between phonemes; for example, knowledge of relatedness helps in determining that two forms exhibit a voicing alternation, as in the Finnish pair mato ('worm', nominative) \u223c madon ('worm', genitive), and hence, the segments t and d should align themselves in the embedding space parallel to other voiceless/voiced pairs that alternate similarly.", "labels": [], "entities": []}, {"text": "We perform experiments in three languages: Finnish, Turkish, and Spanish.", "labels": [], "entities": []}], "datasetContent": [{"text": "Correlation Our first experiment investigates the relationship between the geometries of embedding space and the distinctive feature space.", "labels": [], "entities": []}, {"text": "Let the embedding for phoneme p be emb(p), its distinctive feature vector feat(p), and cosine similarity of vectors u and v be given by Equation 2.", "labels": [], "entities": []}, {"text": "We measure the linear correlation of sim(emb(p), emb(q)) and sim(feat(p), feat(q)) overall unordered pairs of phonemes {p, q} (where p = q) using Pearson's r.", "labels": [], "entities": []}, {"text": "As a baseline, we compute the correlation of similarities of feature representations and random embeddings remb(p).", "labels": [], "entities": []}, {"text": "These are derived by randomly permuting the embeddings of phonemes.", "labels": [], "entities": []}, {"text": "That is, remb(p) = emb(q) for some random phoneme q.", "labels": [], "entities": []}, {"text": "Analogy Our second experiment investigates phoneme analogies.", "labels": [], "entities": [{"text": "phoneme analogies", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7256432473659515}]}, {"text": "We first score four-tuples (a, b, c, d) of phonemes using cosine similarity in embedding space as defined by Equation 3.", "labels": [], "entities": []}, {"text": "This corresponds to a proportional analogy a:b::c:d.", "labels": [], "entities": []}, {"text": "We then evaluate the top 15, 30 and 100 fourtuples w.r.t. phonological analogy in distinctive feature space.", "labels": [], "entities": []}, {"text": "Our evaluation is based on applying the transformation defined by the first two phonemes a and b on the third phoneme c and measuring the Hamming distance of the result and the feature representation of d.", "labels": [], "entities": []}, {"text": "For example, given tuple (p,b,t,d), we get Hamming distance 0.", "labels": [], "entities": [{"text": "Hamming distance 0", "start_pos": 43, "end_pos": 61, "type": "METRIC", "confidence": 0.8280555804570516}]}, {"text": "This happens because p is transformed to b by changing the value of feature voice from 0 to 1.", "labels": [], "entities": []}, {"text": "When the same transformation is applied tot, the result is d, which obviously has Hamming distance 0 with", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Correlation between feature similarities sim(feat(x), feat(y)) and embedding similarities  sim(emb(x), emb(y)) for all unordered pairs of phonemes {x, y} (where x = y). All correlations are  significantly higher (with p-value < 0.01) than ones obtained using a random assignment of embedding  vectors to phonemes.", "labels": [], "entities": []}]}