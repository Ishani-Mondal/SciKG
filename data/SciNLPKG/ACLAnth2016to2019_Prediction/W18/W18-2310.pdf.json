{"title": [{"text": "A Neural Autoencoder Approach for Document Ranking and Query Refinement in Pharmacogenomic Information Retrieval", "labels": [], "entities": [{"text": "Document Ranking", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.9349027574062347}, {"text": "Pharmacogenomic Information Retrieval", "start_pos": 75, "end_pos": 112, "type": "TASK", "confidence": 0.6611195305983225}]}], "abstractContent": [{"text": "In this study, we investigate learning-to-rank and query refinement approaches for information retrieval in the pharmacoge-nomic domain.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.7588858306407928}]}, {"text": "The goal is to improve the information retrieval process of biomedi-cal curators, who manually build knowledge bases for personalized medicine.", "labels": [], "entities": []}, {"text": "We study how to exploit the relationships between genes, variants, drugs, diseases and outcomes as features for document ranking and query refinement.", "labels": [], "entities": [{"text": "query refinement", "start_pos": 133, "end_pos": 149, "type": "TASK", "confidence": 0.7698045372962952}]}, {"text": "For a supervised approach, we are faced with a small amount of annotated data and a large amount of unannotated data.", "labels": [], "entities": []}, {"text": "Therefore, we explore ways to use a neural document auto-encoder in a semi-supervised approach.", "labels": [], "entities": []}, {"text": "We show that a combination of established algorithms, feature-engineering and a neural auto-encoder model yield promising results in this setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Personalized medicine strives to relate genomic detail to patient phenotypic conditions (such as disease, adverse reactions to treatment) and to assess the effectiveness of available treatment options).", "labels": [], "entities": []}, {"text": "For computerassisted decision making, knowledge bases need to be compiled from published scientific evidence.", "labels": [], "entities": [{"text": "computerassisted decision making", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.648248294989268}]}, {"text": "They describe biomarker relationships between key entity types: Disease, Protein/Gene, Variant/Mutation, Drug, and Patient Outcome (Outcome).", "labels": [], "entities": []}, {"text": "While automated information extraction has been applied to simple relationships -such as Drug-Drug ( or Protein-Protein (;; () interactionwith adequate precision and recall, clinically actionable biomarkers need to satisfy rigorous quality criteria set by physicians and therefore call upon manual data curation by domain experts.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.7382860779762268}, {"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9977297186851501}, {"text": "recall", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.9910377264022827}]}, {"text": "To ascertain the timeliness of information, curators are faced with the labor-intensive task to identify relevant articles in a steadily growing flow of publications ().", "labels": [], "entities": []}, {"text": "In our scenario, curators iteratively refine search queries in an electronic library, such as PubMed.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 94, "end_pos": 100, "type": "DATASET", "confidence": 0.9512871503829956}]}, {"text": "The information the curators search for, are biomarkerfacts in the form of {Gene(s) -Variant(s) -Drug(s) -Disease(s) -Outcome}.", "labels": [], "entities": []}, {"text": "This process is then repeated until, theoretically, all published literature regarding the gene PIK3CA has been screened.", "labels": [], "entities": []}, {"text": "Our goal is to reduce the amount of documents which domain experts need to examine.", "labels": [], "entities": []}, {"text": "To achieve this, an information retrieval system should rank documents high that are relevant to the query and should facilitate the identification of relevant entities to refine the query.", "labels": [], "entities": []}, {"text": "Classic approaches for document ranking, like tf-idf;, or bm25 (, and, for example, the Relevance Model () for query refinement are established techniques in this setting.", "labels": [], "entities": [{"text": "document ranking", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7329082190990448}, {"text": "query refinement", "start_pos": 111, "end_pos": 127, "type": "TASK", "confidence": 0.7339264154434204}]}, {"text": "They are known to be robust and do not require data for training.", "labels": [], "entities": []}, {"text": "However, as they are based on a bag-of-words model (BOW), they cannot represent a semantic relationship of entities in a document.", "labels": [], "entities": []}, {"text": "This, for example, yields search results with highly ranked review articles that only list query terms, without the desired relationship between them.", "labels": [], "entities": []}, {"text": "Therefore, we investigate approaches that model the semantic relationships between biomarker entities.", "labels": [], "entities": []}, {"text": "This can either be addressed by combining BOW with rule-based filtering, or by supervised learning, i.e. learningto-rank (LTR).", "labels": [], "entities": [{"text": "BOW", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.7791049480438232}]}, {"text": "Our goal is, to tailor document ranking and query refinement to the task of the curator.", "labels": [], "entities": [{"text": "document ranking", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.6658912897109985}]}, {"text": "This means that a document ranking model should assign a high rank to a document that contains the query entities in a biomarker relationship.", "labels": [], "entities": []}, {"text": "A query refinement model should suggest additional query terms, i.e. biomarker entities, to the curator that are relevant to the current query.", "labels": [], "entities": []}, {"text": "Given the complexity of entity relationships and the high variety of textual realizations this requires either effective feature engineering, or large amounts of training data fora supervised approach.", "labels": [], "entities": []}, {"text": "The in-house data set of Molecular Health consists of 5833 labeled biomarker-facts, and 24 million unlabeled text documents from PubMed.", "labels": [], "entities": [{"text": "Molecular Health", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.4253215342760086}, {"text": "PubMed", "start_pos": 129, "end_pos": 135, "type": "DATASET", "confidence": 0.9702444076538086}]}, {"text": "Therefore, a good solution is to exploit the large amount of unlabeled data in a semi-supervised approach.", "labels": [], "entities": []}, {"text": "have shown that a neural auto-encoder with LSTMs can encode the syntactics and semantics of a text in a dense vector representation.", "labels": [], "entities": []}, {"text": "We show that this representation can be effectively used as a feature for semi-supervised learning-to-rank and query refinement.", "labels": [], "entities": [{"text": "query refinement", "start_pos": 111, "end_pos": 127, "type": "TASK", "confidence": 0.76754429936409}]}, {"text": "In this paper, we describe a feature engineering approach and a semi-supervised approach.", "labels": [], "entities": []}, {"text": "In our experiments we show that the two approaches are, in comparison, almost on par in terms of performance and even improve in a joint model.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the neural auto-encoder, and then proceed in Section 3 to describe our models for document ranking and in Section 4 the models for query refinement.", "labels": [], "entities": [{"text": "document ranking", "start_pos": 107, "end_pos": 123, "type": "TASK", "confidence": 0.7571362853050232}, {"text": "query refinement", "start_pos": 156, "end_pos": 172, "type": "TASK", "confidence": 0.8120585978031158}]}], "datasetContent": [{"text": "In this section, we first explain our evaluation strategy to assess the performance of the respective models for document ranking and query refinement.", "labels": [], "entities": [{"text": "document ranking", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.730756402015686}, {"text": "query refinement", "start_pos": 134, "end_pos": 150, "type": "TASK", "confidence": 0.7388328313827515}]}, {"text": "Subsequently, we describe the settings for the data and the results of the experiments that we have conducted.", "labels": [], "entities": []}, {"text": "Document ranking models are evaluated by their ability to rank relevant documents higher than irrelevant documents.", "labels": [], "entities": [{"text": "Document ranking", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8233238160610199}]}, {"text": "Query refinement models are evaluated both, by their ability to rank relevant query terms high, and by the recall of retrieved relevant documents when the query automatically is refined by the 1st, 2nd and 3rd proposed query term.", "labels": [], "entities": [{"text": "Query refinement", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8398553133010864}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.993119478225708}]}, {"text": "We evaluate our models using mean average precision (MAP) (, Chapter 11) and normalized discounted cumulative gain (nDCG)).", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 29, "end_pos": 57, "type": "METRIC", "confidence": 0.9428006907304128}, {"text": "normalized discounted cumulative gain (nDCG))", "start_pos": 77, "end_pos": 122, "type": "METRIC", "confidence": 0.850740270955222}]}, {"text": "For both the document ranking and query refinement approach we interpret a biomarker-fact as a perfect query and the corresponding papers as the true-positive (or relevant) papers associated with this query.", "labels": [], "entities": [{"text": "query refinement", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7904902696609497}]}, {"text": "In this way, we use the curated facts as document level annotation for our approach.", "labels": [], "entities": []}, {"text": "Because we want to assist the iterative approach of curators in which they refine an initially broad query to ever narrower searches, we need to create valid partial queries and associated relevant documents to mimic this procedure.", "labels": [], "entities": []}, {"text": "Therefore, we generate sub-queries, which are partials of the facts.", "labels": [], "entities": []}, {"text": "We generated two data sets: one for document ranking and one for query refinement.", "labels": [], "entities": [{"text": "document ranking", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7240279614925385}, {"text": "query refinement", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7456210553646088}]}, {"text": "For document ranking, we generated all distinct subsets of the facts.", "labels": [], "entities": [{"text": "document ranking", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7327347993850708}]}, {"text": "For query refinement, we defined the eliminated entities (of the sub-query generation process) as true-positive refinement terms.", "labels": [], "entities": [{"text": "query refinement", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8618458807468414}]}, {"text": "For both data sets, we use all associated documents, of the original biomarker-fact, as true-positive relevant documents.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics about the train/validation/test  splits", "labels": [], "entities": []}, {"text": " Table 2: Query and document representation for ranking models", "labels": [], "entities": []}, {"text": " Table 3: Test Scores Document Ranking", "labels": [], "entities": []}, {"text": " Table 4: Ranked Entity Scores for KRAS Validation and PIK3CA Testing", "labels": [], "entities": [{"text": "KRAS Validation", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.709073930978775}]}, {"text": " Table 5: Refinement Recall Scores for KRAS Validation and PIK3CA Testing", "labels": [], "entities": [{"text": "Refinement Recall Scores", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.6709336439768473}, {"text": "KRAS Validation", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.6957482546567917}]}]}