{"title": [{"text": "Towards one-shot learning for rare-word translation with external experts", "labels": [], "entities": [{"text": "rare-word translation", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.7059883922338486}]}], "abstractContent": [{"text": "Neural machine translation (NMT) has significantly improved the quality of automatic translation models.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7999468247095743}, {"text": "automatic translation", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.7209668457508087}]}, {"text": "One of the main challenges in current systems is the translation of rare words.", "labels": [], "entities": [{"text": "translation of rare words", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.9026795923709869}]}, {"text": "We present a generic approach to address this weakness by having external models annotate the training data as Experts, and control the model-expert interaction with a pointer network and reinforcement learning.", "labels": [], "entities": []}, {"text": "Our experiments using phrase-based models to simulate Experts to complement neural machine translation models show that the model can be trained to copy the annotations into the output consistently.", "labels": [], "entities": []}, {"text": "We demonstrate the benefit of our proposed framework in out-of-domain translation scenarios with only lexical resources, improving more than 1.0 BLEU point in both translation directions English\u2192Spanish and German\u2192English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9982936978340149}]}], "introductionContent": [{"text": "Sequence to sequence models have recently become the state-of-the-art approach for machine translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.8106653094291687}]}, {"text": "This model architecture can directly approximate the conditional probability of the target sequence given a source sequence using neural networks.", "labels": [], "entities": []}, {"text": "As a result, not only do they model a smoother probability distribution () than the sparse phrase tables in statistical machine translation (), but they can also jointly learn translation models, language models and even alignments in a single model (.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 108, "end_pos": 139, "type": "TASK", "confidence": 0.6470206677913666}]}, {"text": "One of the main weaknesses of neural machine translation models is poor handling of low frequency events.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.676249106725057}]}, {"text": "Neural models tend to prioritize output fluency over translation adequacy, and faced with rare words either silently ignore input ( or fall into underor over-translation ().", "labels": [], "entities": []}, {"text": "Examples of these situations include named entities, dates, and rare morphological forms.", "labels": [], "entities": []}, {"text": "Improper handling of rare events can be harmful to industrial systems (, where translation mistakes can have serious ramifications.", "labels": [], "entities": []}, {"text": "Similarly, translating in specific domains such as information technology or biology, a slight change in vocabulary can drastically alter meaning.", "labels": [], "entities": []}, {"text": "It is important, then, to address translation of rare words.", "labels": [], "entities": [{"text": "translation of rare words", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.9082541763782501}]}, {"text": "While domain-specific parallel corpora can be used to adapt translation models efficiently ( , parallel corpora for many domains can be difficult to collect, and this requires continued training.", "labels": [], "entities": []}, {"text": "Translation lexicons, however, are much more commonly available.", "labels": [], "entities": [{"text": "Translation lexicons", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8952556252479553}]}, {"text": "In this work, we introduce a strategy to incorporate external lexical knowledge, dubbed \"Expert annotation,\" into neural machine translation models.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 114, "end_pos": 140, "type": "TASK", "confidence": 0.7141086856524149}]}, {"text": "First, we annotate the lexical translations directly into the source side of the parallel data, so that the information is exposed during both training and inference.", "labels": [], "entities": []}, {"text": "Second, inspired by CopyNet (, we utilize a pointer network ( to introduce a copy distribution over the source sentence, to increase the generation probability of rare words.", "labels": [], "entities": []}, {"text": "Given that the expert annotation can differ from the reference, in order to encourage the model to copy the annotation we use reinforcement learning to guide the search, giving rewards when the annotation is used.", "labels": [], "entities": []}, {"text": "Our work is motivated to be able to achieve One-Shot learning, which can help the model to accurately translate the events that are annotated during inference.", "labels": [], "entities": []}, {"text": "Such ability can be transferred from an Expert which is capable of learning to translate lexically with one or few examples, such as dictionaries, or phrase-tables, or even human annotators.", "labels": [], "entities": []}, {"text": "We realize our proposed framework with experiments on English\u2192Spanish and German\u2192English translation tasks.", "labels": [], "entities": [{"text": "German\u2192English translation tasks", "start_pos": 74, "end_pos": 106, "type": "TASK", "confidence": 0.7228349447250366}]}, {"text": "We focus on translation of rare events using translation suggestions from an Expert, here simulated by an additional phrase table.", "labels": [], "entities": [{"text": "translation of rare events", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.89960877597332}]}, {"text": "Specifically, we annotate rare words in our parallel data with best candidates from a phrase table before training, so that rare events are provided with suggested translations.", "labels": [], "entities": []}, {"text": "Our model can be explicitly trained to copy the annotation approximately 90% of the time, and it outperformed the baselines on translation accuracy of rare words, reaching up to 97% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.8307822942733765}, {"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9969322681427002}]}, {"text": "Also importantly, this performance is maintained when translating data in a different domain.", "labels": [], "entities": []}, {"text": "Further analysis was done to verify the potential of our proposed framework.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the experiments, we realise the generic framework described in Section 3 with the tasks of translating from English\u2192Spanish and German\u2192English.", "labels": [], "entities": [{"text": "translating from English\u2192Spanish", "start_pos": 94, "end_pos": 126, "type": "TASK", "confidence": 0.8740850329399109}]}, {"text": "For both language pairs, we used data from Europarl (version 7) () and IWSLT17 () to train our neural networks.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.9930476546287537}, {"text": "IWSLT17", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9300592541694641}]}, {"text": "For validation, we use the IWSLT validation set (dev2010) to select the best models based on perplexity (for cross-entropy loss) and BLEU score (for reinforcement learning).", "labels": [], "entities": [{"text": "IWSLT validation set", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.8665458559989929}, {"text": "BLEU score", "start_pos": 133, "end_pos": 143, "type": "METRIC", "confidence": 0.983556479215622}]}, {"text": "For evaluation, we use IWSLT tst2010 as the indomain test set.", "labels": [], "entities": [{"text": "IWSLT tst2010", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.760154515504837}]}, {"text": "We also evaluate our models on out-of-domain corpora.", "labels": [], "entities": []}, {"text": "For English\u2192Spanish an additional Business dataset is used.", "labels": [], "entities": []}, {"text": "The corpus statistics can be seen on.", "labels": [], "entities": []}, {"text": "The out-ofdomain experiments for the German\u2192English are carried out on the medical domain, in which we use the UFAL Medical Corpus v1.0 corpus (2.2 million sentences) to train the Expert and the Oracle system.", "labels": [], "entities": [{"text": "UFAL Medical Corpus v1.0 corpus", "start_pos": 111, "end_pos": 142, "type": "DATASET", "confidence": 0.9393212080001831}, {"text": "Expert", "start_pos": 180, "end_pos": 186, "type": "DATASET", "confidence": 0.9422522783279419}]}, {"text": "The test data for this task is the HIML2017 dataset with 1517 sentences.", "labels": [], "entities": [{"text": "HIML2017 dataset", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.979831337928772}]}, {"text": "We preprocess all the data using standard tokenization, true-casing and BPE splitting with 40K joined operations.", "labels": [], "entities": [{"text": "BPE", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.962811291217804}]}, {"text": "To serve the research questions above, we use the following evaluation metrics: \u2022 BLEU: score for general translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9994090795516968}]}, {"text": "\u2022 SUGGESTION (SUG): The overlap between the hypothesis and the phrase-table (on word level), showing how much the expert content is used by the model.", "labels": [], "entities": [{"text": "SUGGESTION (SUG)", "start_pos": 2, "end_pos": 18, "type": "METRIC", "confidence": 0.5020128339529037}]}, {"text": "\u2022 SUGGESTION ACCURACY (SAC): The intersection between the hypothesis, the phrase-table suggestions and the reference.", "labels": [], "entities": [{"text": "SUGGESTION ACCURACY (SAC)", "start_pos": 2, "end_pos": 27, "type": "METRIC", "confidence": 0.8532283902168274}]}, {"text": "This metrics shows us the accuracy of the system on the rare-words which are suggested by the phrase-table.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9997150301933289}]}, {"text": "Discussion The SUG metric shows the consistency of the model on the copy mechanism.", "labels": [], "entities": [{"text": "consistency", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9899793863296509}]}, {"text": "Models with lower SUG are not necessarily worse, and models with high SUG can potentially have very low recall on rare-word translation by systematically copying bad suggestions and failing to translate rare-words where the annotator is incorrect.", "labels": [], "entities": [{"text": "SUG", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9692327976226807}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9987536668777466}, {"text": "rare-word translation", "start_pos": 114, "end_pos": 135, "type": "TASK", "confidence": 0.6742428988218307}]}, {"text": "However, we argue that a high SUG system can be used reliably with a high quality expert.", "labels": [], "entities": [{"text": "SUG", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9442724585533142}]}, {"text": "For example, in censorship management or name translation which is strictly sensitive, this quality can help reducing output inconsistency.", "labels": [], "entities": [{"text": "censorship management", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.945529967546463}, {"text": "name translation", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.8373746871948242}]}, {"text": "On the other hand, the SAC metrics show improvement on rareword translation, but only on the intersection of the phrase table and the reference.", "labels": [], "entities": [{"text": "rareword translation", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.7888931035995483}]}, {"text": "This subset is our main focus.", "labels": [], "entities": []}, {"text": "General rare-word translation quality requires additional effort to find the reference aligned to the rare words in the source sentences, which we consider for future work.", "labels": [], "entities": [{"text": "rare-word translation", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.6262955069541931}]}, {"text": "English\u2192Spanish Results for this task are presented on table 2.", "labels": [], "entities": []}, {"text": "First, the main difference between the settings is the SUG and SAC figures for all test sets.", "labels": [], "entities": [{"text": "SUG", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.993963897228241}, {"text": "SAC", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.989698588848114}]}, {"text": "Both of them increase dramatically from baseline to annotation, and also increase according to the level of supervision in our model proposals.", "labels": [], "entities": []}, {"text": "While the copy mechanism can help us to copy more from the annotation, the REINFORCE models are successfully trained to: The results of German\u2192English on various domains: TEDTalks and Biomedical.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9371179938316345}, {"text": "TEDTalks", "start_pos": 171, "end_pos": 179, "type": "DATASET", "confidence": 0.7472231984138489}]}, {"text": "We use AN for using annotations from the phrase table, RF for using REINFORCE (\u03b1= 0.5) and CP for using the Copy mechanism.", "labels": [], "entities": [{"text": "AN", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.9983304142951965}, {"text": "RF", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9931574463844299}, {"text": "REINFORCE", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.997177004814148}, {"text": "CP", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.9453301429748535}]}, {"text": "make the model copy more consistently.", "labels": [], "entities": []}, {"text": "Their combination helps us achieve the desired behavior, in which almost all of the annotations given are copied, and we achieve 100% accuracy on the rare-words section that the phrase table covers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9994316697120667}]}, {"text": "As mentioned in the discussion above, the SAC and SUG figures, while being not enough to quantitatively prove that the total number of rare words translated, show that the phrase table is complementary to the neural machine translation, and the more coverage the expert has, the more benefit this method can bring.", "labels": [], "entities": [{"text": "SAC", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9415212869644165}, {"text": "SUG", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.7046760320663452}, {"text": "neural machine translation", "start_pos": 209, "end_pos": 235, "type": "TASK", "confidence": 0.7608020106951395}]}, {"text": "We notice an improvement of 1 BLEU point on dev2010 but only slight changes compared to the baseline on tst2010.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9896388053894043}]}, {"text": "On the out-of-domain set, however, the improved rare-word performance leads to an increase of 1.7 BLEU points over the baseline without annotation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9968330264091492}]}, {"text": "Our models, despite training on a noisier dataset, are able to improve translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.974791944026947}]}, {"text": "German\u2192English Results are shown in Table 3.", "labels": [], "entities": []}, {"text": "On the dev2010 and tst2010 in-domain datasets, we observe similar phenomena to the EnEs direction.", "labels": [], "entities": []}, {"text": "Rare-word performance increases with the number of words copied, and the combination of the copy mechanism and REINFORCE help us copy consistently.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.998128354549408}]}, {"text": "Surprisingly, however, the BLEU score drops with annotations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.966338038444519}]}, {"text": "This maybe because of the relative morphologically complexity of the German words compared to the English, making it harder to generate the correct word form.", "labels": [], "entities": []}, {"text": "In the experiments with an out-of-domain test set (HIML), we use annotations from that domain to simulate a domain-expert.", "labels": [], "entities": []}, {"text": "For comparison, we also trained an NMT model adapted to the UFAL corpus, which we call the Oracle model.", "labels": [], "entities": [{"text": "UFAL corpus", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.9288182258605957}]}, {"text": "In this domain, our models show the same behavior, in which almost every word annotated is copied to the output.", "labels": [], "entities": []}, {"text": "The annotation efficiently improves translation quality by 1.7 BLEU points over the baseline without annotation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.998677670955658}]}, {"text": "The adapted model has a higher BLEU score, but here performs worse than our annotated model in terms of phrase-table overlap and rare-word translation accuracy for words in this set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9832179248332977}, {"text": "rare-word translation", "start_pos": 129, "end_pos": 150, "type": "TASK", "confidence": 0.6655905842781067}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.862761914730072}]}, {"text": "Our model shows significantly better rare word handling than the baseline.", "labels": [], "entities": [{"text": "rare word handling", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.61397918065389}]}, {"text": "Though the best obtainable system is adapted to the in-domain data, this requires parallel text: this experiment shows the high potential to improve NMT on out-of-domain scenarios using only lexical-level materials.", "labels": [], "entities": [{"text": "NMT", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.927703320980072}]}, {"text": "We notice a surprising drop of 1.0 BLEU points for the RE-INFORCE model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9981936812400818}, {"text": "RE-INFORCE", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.965649425983429}]}, {"text": "Possible reasons include inefficient beam search on REINFORCE models, or the GLEU signal was out-weighted by the HIT one during training, which is known for the difficulty (.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9945490956306458}, {"text": "HIT", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.7965862154960632}]}], "tableCaptions": [{"text": " Table 1: Phrase-table coverage statistics. The out-of-domain section in English-Spanish is Business  and Biomedical in German-English. We show the total number of rare words detected by frequency (in  parentheses) and the percentage covered by the Experts (intersecting with the reference).", "labels": [], "entities": []}, {"text": " Table 2: The results of English -Spanish on various domains: TEDTalks and Business. We use AN for  using annotations from the phrase table, RF for using REINFORCE (\u03b1= 0.5) and CP for using the Copy  mechanism.", "labels": [], "entities": [{"text": "TEDTalks", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.8019546866416931}, {"text": "AN", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9981589913368225}, {"text": "RF", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.9792160391807556}, {"text": "REINFORCE", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9927668571472168}]}, {"text": " Table 3: The results of German\u2192English on various domains: TEDTalks and Biomedical. We use AN  for using annotations from the phrase table, RF for using REINFORCE (\u03b1= 0.5) and CP for using the  Copy mechanism.", "labels": [], "entities": [{"text": "TEDTalks", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.8055158853530884}, {"text": "AN", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9985160231590271}, {"text": "RF", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.9819091558456421}, {"text": "REINFORCE", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9931859970092773}, {"text": "CP", "start_pos": 177, "end_pos": 179, "type": "METRIC", "confidence": 0.9457843899726868}]}, {"text": " Table 4. Higher \u03b1  values emphasize the signal to copy the source an- notation, as can be seen from the increase in terms  of Accuracy and Suggestion utilization across the  values. As expected, as \u03b1 goes toward 1.0, the  model gradually loses the signal needed to main- tain translation quality and finally diverges.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.999464213848114}, {"text": "Suggestion utilization", "start_pos": 140, "end_pos": 162, "type": "METRIC", "confidence": 0.9583099186420441}]}, {"text": " Table 4: Performances w.r.t to different alpha val- ues. Metrics shown are BLEU (BL), ACCURACY  (AC) and SUGGESTION (SUG)", "labels": [], "entities": [{"text": "BLEU (BL)", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9217558354139328}, {"text": "ACCURACY  (AC)", "start_pos": 87, "end_pos": 101, "type": "METRIC", "confidence": 0.9390487223863602}]}]}