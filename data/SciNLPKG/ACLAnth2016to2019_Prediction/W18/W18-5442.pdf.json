{"title": [], "abstractContent": [{"text": "In this paper, we propose a method of calibrating a word embedding, so that the semantic it conveys becomes more relevant to the context.", "labels": [], "entities": []}, {"text": "Our method is novel because the output shows clearly which senses that were originally presented in a target word embedding become stronger or weaker.", "labels": [], "entities": []}, {"text": "This is possible by utilizing the technique of using sparse coding to recover senses that comprises a word embedding .", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we propose a method of generating contextualized word embeddings.", "labels": [], "entities": []}, {"text": "What we mean by 'contextualized' is that standard embeddings such as Skip-gram and GloVe are modified to reflect their contexts.", "labels": [], "entities": []}, {"text": "For instance, apple appeared in fruit-implying context should become more similar to fruit than it was in the prior state.", "labels": [], "entities": []}, {"text": "We need contextualized embeddings because not all information contained in an embedding is helpful for modelling accurately the meaning of a word in context (e.g. company-related senses of apple in fruit-implying context).", "labels": [], "entities": []}, {"text": "Since word embeddings are trained on unconstrained variation of contexts, using word embeddings as-is is like taking the risk of feeding our subsequent models (e.g. classifiers) with noises that are not relevant to the given context.", "labels": [], "entities": []}, {"text": "We formulate our task as calibrating senses contained in embeddings so that contextually relevant senses (e.g. fruit-ness) becomes stronger and the others (e.g. company-ness) become weaker.", "labels": [], "entities": []}, {"text": "To achieve this we utilize the technique of recovering standard word embeddings as linear composition of different senses, proposed by.", "labels": [], "entities": []}, {"text": "After applying the technique word embeddings are transformed into high dimensional (e.g. 2,500) and sparse (only small portion of dimensions are nonzero) embeddings.", "labels": [], "entities": []}, {"text": "This makes our method interpretable since extracted senses can give us \"a succinct description of which other words cooccur with a specific word sense\".", "labels": [], "entities": []}, {"text": "More detailed explanation is presented in Section 3.", "labels": [], "entities": []}, {"text": "Using the technique we first decompose word embeddings of a target word (to be contextualized) and context words into linear composition of senses, then identify strong senses extracted from context and regard them as contextually relevant.", "labels": [], "entities": []}, {"text": "Finally we use the contextually relevant senses for calibrating the senses contained in a target word.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}