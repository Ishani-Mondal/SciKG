{"title": [{"text": "Parameter Sharing Methods for Multilingual Self-Attentional Translation Models", "labels": [], "entities": [{"text": "Parameter Sharing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9322773516178131}, {"text": "Multilingual Self-Attentional Translation", "start_pos": 30, "end_pos": 71, "type": "TASK", "confidence": 0.6122498412926992}]}], "abstractContent": [{"text": "In multilingual neural machine translation, it has been shown that sharing a single translation model between multiple languages can achieve competitive performance, sometimes even leading to performance gains over bilingually trained models.", "labels": [], "entities": [{"text": "multilingual neural machine translation", "start_pos": 3, "end_pos": 42, "type": "TASK", "confidence": 0.6099753752350807}]}, {"text": "However, these improvements are not uniform; often multilingual parameter sharing results in a decrease inaccuracy due to translation models not being able to accommodate different languages in their limited parameter space.", "labels": [], "entities": []}, {"text": "In this work, we examine parameter sharing techniques that strike a happy medium between full sharing and individual training, specifically focusing on the self-attentional Transformer model.", "labels": [], "entities": []}, {"text": "We find that the full parameter sharing approach leads to increases in BLEU scores mainly when the target languages are from a similar language family.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.9759969115257263}]}, {"text": "However, even in the case where target languages are from different families where full parameter sharing leads to a noticeable drop in BLEU scores, our proposed methods for partial sharing of parameters can lead to substantial improvements in translation accuracy.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 136, "end_pos": 147, "type": "METRIC", "confidence": 0.9779331982135773}, {"text": "translation", "start_pos": 244, "end_pos": 255, "type": "TASK", "confidence": 0.9527932405471802}, {"text": "accuracy", "start_pos": 256, "end_pos": 264, "type": "METRIC", "confidence": 0.8059553503990173}]}], "introductionContent": [{"text": "Neural machine translation (NMT; ; ) is now the de-facto standard in MT research due to its relative simplicity of implementation, ability to perform end-to-end training, and high translation accuracy.", "labels": [], "entities": [{"text": "Neural machine translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6999481976032257}, {"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9925091862678528}, {"text": "accuracy", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.8580277562141418}]}, {"text": "Early approaches to NMT used recurrent neural networks (RNNs), usually LSTMs (Hochreiter and Schmidhuber, 1997), in their encoder and decoder layers, with the addition of an attention mechanism (Bahdanau et al.,) to focus more on specific encoded source words when deciding the next translation target output.", "labels": [], "entities": []}, {"text": "Recently, Data and code of this paper is available at: https://github.com/DevSinghSachan/multilingual_nmt (a) Shared encoder, separate decoder ().", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, first, we describe the datasets used in this work and the evaluation criteria.", "labels": [], "entities": []}, {"text": "Then, we describe the training regimen followed in all our experiments.", "labels": [], "entities": []}, {"text": "All of our models were implemented in PyTorch framework ( and were trained on a single GPU.", "labels": [], "entities": [{"text": "PyTorch framework", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.9200783967971802}]}, {"text": "To perform multilingual translation experiments, we select six language pairs from the openly available TED talks dataset (Qi et al., 2018) whose statistics are mentioned in.", "labels": [], "entities": [{"text": "multilingual translation", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.5958813428878784}, {"text": "TED talks dataset", "start_pos": 104, "end_pos": 121, "type": "DATASET", "confidence": 0.9040800929069519}]}, {"text": "This dataset already contains predefined splits for training, development, and test sets.", "labels": [], "entities": []}, {"text": "Among these languages, Romanian (RO) and French (FR) are Romance languages, German (DE) and Dutch (NL) are Germanic languages while Turkish (TR) and Japanese (JA) are unrelated languages that come from distant language families.", "labels": [], "entities": []}, {"text": "For all language pairs, tokenization was carried out using the Moses tokenizer, 2 except for Japanese, where word segmentation was performed using the KyTea tokenizer (Neubig et al., 2011).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7170422077178955}]}, {"text": "To select training examples, we filter sentences with a maximum length of 70 tokens.", "labels": [], "entities": []}, {"text": "For evaluation, we report the model's performance using the standard BLEU score metric ().", "labels": [], "entities": [{"text": "BLEU score metric", "start_pos": 69, "end_pos": 86, "type": "METRIC", "confidence": 0.9767748117446899}]}, {"text": "We use the mtevalv14.pl script from the Moses toolkit to compute the tokenized BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9870904088020325}]}], "tableCaptions": [{"text": " Table 1: Number of sentences in the training, dev,  and test splits for each language pair used in our ex- periments. The languages are represented by their  ISO 639-1 codes En:English, Fr:French, Nl:Dutch,  De:German, Ja:Japanese, Tr:Turkish.", "labels": [], "entities": []}, {"text": " Table 2: BLEU scores for various parameter sharing strategies when the target languages either belong  to the same family ({RO, FR}, {DE, NL}) or to distant families (DE, TR, JA). \u03b8 ENC denotes that all the  encoder parameters are shared between the models; \u03b8 DEC denotes that all the decoder parameters are  shared between the models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9975935816764832}, {"text": "ENC", "start_pos": 183, "end_pos": 186, "type": "METRIC", "confidence": 0.9323596954345703}]}, {"text": " Table 3: BLEU scores for different models for one-to-many translation task. NS: No Sharing corresponds  to the bilingual models when the two language pairs are trained independently; FS: Full Sharing means  one model is used for the translation of all the language pairs; PS: Partial Sharing means that the  embedding, encoder, decoder's key, and value weights are shared between the two models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990948438644409}, {"text": "FS", "start_pos": 184, "end_pos": 186, "type": "METRIC", "confidence": 0.9830896854400635}]}]}