{"title": [], "abstractContent": [{"text": "Natural language text exhibits hierarchical structure in a variety of respects.", "labels": [], "entities": []}, {"text": "Ideally, we could incorporate our prior knowledge of this hierarchical structure into unsupervised learning algorithms that work on text data.", "labels": [], "entities": []}, {"text": "Recent work by Nickel and Kiela (2017) proposed using hyperbolic instead of Euclidean embedding spaces to represent hierarchical data and demonstrated encouraging results when embedding graphs.", "labels": [], "entities": []}, {"text": "In this work, we extend their method with a re-parameterization technique that allows us to learn hyperbolic embeddings of arbitrarily parameterized objects.", "labels": [], "entities": []}, {"text": "We apply this framework to learn word and sentence embeddings in hyperbolic space in an unsupervised manner from text corpora.", "labels": [], "entities": []}, {"text": "The resulting embeddings seem to encode certain intuitive notions of hierarchy, such as word-context frequency and phrase constituency.", "labels": [], "entities": []}, {"text": "However, the implicit continuous hierarchy in the learned hyperbolic space makes interrogating the model's learned hierarchies more difficult than for models that learn explicit edges between items.", "labels": [], "entities": []}, {"text": "The learned hyperbolic em-beddings show improvements over Euclidean embeddings in some-but not all-downstream tasks, suggesting that hierarchical organization is more useful for some tasks than others.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many real-world datasets exhibit hierarchical structure, either explicitly in ontologies like WordNet, or implicitly in social networks) and natural language sentences.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 94, "end_pos": 101, "type": "DATASET", "confidence": 0.9436590671539307}]}, {"text": "When learning representations of such datasets, hyperbolic spaces have recently been advocated as alternatives to the standard Euclidean spaces in order to better represent the hierarchical structure; Cham- * Work done while interning at Google Brain.", "labels": [], "entities": []}, {"text": "berlain et al., 2017).", "labels": [], "entities": []}, {"text": "Hyperbolic spaces are nonEuclidean geometric spaces that naturally represent hierarchical relationships; for example, they can be viewed as continuous versions of trees (.", "labels": [], "entities": []}, {"text": "Indeed, showed improved reconstruction error and link prediction when embedding WordNet and scientific collaboration networks into a hyperbolic space of small dimension compared to a Euclidean space of much larger dimension.", "labels": [], "entities": [{"text": "reconstruction", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.9217307567596436}, {"text": "error", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.48333853483200073}, {"text": "link prediction", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7837609946727753}]}, {"text": "In this work, we explore the use of hyperbolic spaces for embedding natural language data, which has natural hierarchical structure in terms of specificity.", "labels": [], "entities": []}, {"text": "For example, sub-phrases in a sentence can be arranged into a consituency-based parse tree where each node is semantically more specific than its parent.", "labels": [], "entities": []}, {"text": "This hierarchical structure is not usually annotated in text corpora.", "labels": [], "entities": []}, {"text": "Instead, we hypothesize that this structure is implicitly encoded in the range of natural language contexts in which a concept appears: semantically general concepts will occur in a wider range of contexts than semantically specific ones.", "labels": [], "entities": []}, {"text": "We use this intuition to formulate unsupervised objectives for learning hyperbolic embeddings of text objects.", "labels": [], "entities": []}, {"text": "By contrast, only embedded graphs with an explicit hierarchical structure.", "labels": [], "entities": []}, {"text": "Further, only considered the non-parametric case where each object to be embedded is assigned its representation from a lookup table . This approach is impractical for embedding natural language because there are too many sentences and phrases for such a table to fit in memory.", "labels": [], "entities": []}, {"text": "For natural language, we must adopt a parametric approach where we learn the parameters \u03b8 of an encoder function f \u03b8 that maps sequences of text to their embeddings.", "labels": [], "entities": []}, {"text": "When training their non-parametric model, relied on a projection step to keep their embeddings within their model of hyperbolic space.", "labels": [], "entities": []}, {"text": "Specifically, they embedded their data in the Poincar\u00e9 ball model of hyperbolic space, which consists of points in the unit ball B d = {x \u2208 Rd : x < 1}, but their Reimannian gradientdescent algorithm was not guaranteed to keep their embeddings within the unit ball.", "labels": [], "entities": []}, {"text": "To address this issue, they applied a projection step after each gradient step to force the embeddings back into the unit ball, but this projection is not possible when the representations are the output of an encoder f \u03b8 . Our main contribution is to propose a simpler parametrization of hyperbolic embeddings that allows us to train parametric encoders.", "labels": [], "entities": []}, {"text": "We avoid the need fora projection step by separately parameterizing the direction and norm of each embedding and applying a sigmoid activation function to the norm.", "labels": [], "entities": []}, {"text": "This ensures that embeddings always satisfy e < 1 (as required by the Poincar\u00e9 ball model of hyperbolic space), even after arbitrary gradient steps.", "labels": [], "entities": []}, {"text": "Once the embeddings are constrained in this way, all that is needed to induce hyperbolic embeddings is an appropriate distance metric (see Equation 1) in the loss function in place of the commonly used Euclidean or cosine distance metrics.", "labels": [], "entities": []}, {"text": "In addition to allowing parametric encoders, this parameterization has an added benefit that instead of Riemannian-SGD (as used in, we can use any of the popular optimization methods in deep learning, such as Adam ().", "labels": [], "entities": []}, {"text": "We show that re-parameterizing in this manner leads to comparable reconstruction error to the method of when learning nonparametric embeddings of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 146, "end_pos": 153, "type": "DATASET", "confidence": 0.9542892575263977}]}, {"text": "We test our framework by learning unsuper-vised embeddings for two types of natural language data.", "labels": [], "entities": []}, {"text": "First, we embed a graph of word co-occurrences extracted from a large text corpus.", "labels": [], "entities": []}, {"text": "The resulting embeddings are hierarchically organized such that words occurring in many contexts are placed near the origin and words occurring in few contexts are placed near the boundary of the space.", "labels": [], "entities": []}, {"text": "Using these embeddings, we see improved performance on a lexical entailment task, which supports our hypothesis that co-occurrence frequency is indicative of semantic specificity.", "labels": [], "entities": []}, {"text": "However, this improvement comes at the cost of worse performance on a word similarity task.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.8093812863032023}]}, {"text": "In the second experiment, we learn embeddings of sentences (and sub-sentence sequences) by applying the hyperbolic metric to a modified version of the Skip-Thoughts model () that uses embeddings to predict local context in a text corpus.", "labels": [], "entities": []}, {"text": "Since most sentences are unique, there is no clear notion of co-occurrence frequency in this case.", "labels": [], "entities": []}, {"text": "However, we find a high correlation (0.67) between the norms of embedded constituent phrases from Penn Treebank ( and the height at which those phrases occur in their parse trees.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.995158463716507}]}, {"text": "We conclude that hyperbolic sentence embeddings encode some of the hierarchical structure represented by parse trees, without being trained to do so.", "labels": [], "entities": []}, {"text": "However, experiments on downstream tasks do not show consistent improvements over baseline Euclidean embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our re-parameterized Poincar\u00e9 embeddings we used a batch size 1024, learning rate 0.005, and no burn-in period.", "labels": [], "entities": [{"text": "learning rate 0.005", "start_pos": 72, "end_pos": 91, "type": "METRIC", "confidence": 0.958556334177653}]}, {"text": "The loss was optimized using the Adam optimizer.", "labels": [], "entities": []}, {"text": "Embeddings were initialized in U[\u22120.001, 0.001].", "labels": [], "entities": []}, {"text": "We sampled 10 negatives on the fly during training independently for each positive sample.", "labels": [], "entities": []}, {"text": "We clipped gradients to a norm of 5.", "labels": [], "entities": []}, {"text": "Embeddings were initialized to a small norm around \u03c3(\u22125).", "labels": [], "entities": []}, {"text": "The TEXT8 corpus contains around 17M tokens preprocessed such that all tokens are lowercase, numbers are spelled out, and any characters not in a-z are replaced by whitespace.", "labels": [], "entities": [{"text": "TEXT8 corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9329418540000916}]}, {"text": "We removed stopwords and constructed the word-cooccurrence graph G by adding an edge between words appearing within 5 tokens of each other in the resulting corpus.", "labels": [], "entities": []}, {"text": "We used c = 0.25 for subsampling frequent edges, and trained our embedding model using the Adam optimizer with batch size 512 and learning rate 0.005.", "labels": [], "entities": []}, {"text": "We sampled 50 negatives per step for the loss.", "labels": [], "entities": []}, {"text": "We initialized the norms of the word embeddings around \u03c3(\u22125).", "labels": [], "entities": []}, {"text": "All hyperparameters were tuned to maximize performance on the word similarity task.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.788348118464152}]}, {"text": "During preprocessing, only the top 20,000 most frequent types were retained and the rest were replaced with the UNK type.", "labels": [], "entities": [{"text": "UNK", "start_pos": 112, "end_pos": 115, "type": "DATASET", "confidence": 0.836219072341919}]}, {"text": "We optimized the loss function using Adam optimizer with a batch size of 64.", "labels": [], "entities": []}, {"text": "The initial learning rate was tuned between 0.005, 0.0008, 0.0001 which was then decayed exponentially to half its value in 100,000 steps.", "labels": [], "entities": []}, {"text": "When decoding we utilize a local context from a window of K = 2 words around the target word.", "labels": [], "entities": []}, {"text": "The embedding norms are initialized around \u03c3(\u22122).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Reconstruction errors for various embedding di-", "labels": [], "entities": []}, {"text": " Table 3: Words in order of increasing hyperbolic norm which contain the substring indicated in the top row. Their counts in", "labels": [], "entities": []}, {"text": " Table 4: Spearman's \u03c1 correlation coefficient for Word Simi-", "labels": [], "entities": [{"text": "\u03c1 correlation coefficient", "start_pos": 21, "end_pos": 46, "type": "METRIC", "confidence": 0.9117478728294373}, {"text": "Word Simi-", "start_pos": 51, "end_pos": 61, "type": "TASK", "confidence": 0.7257964611053467}]}, {"text": " Table 6: Held out set perplexity and downstream task performance for sentence embeddings of various sizes.  *  Perplexity of", "labels": [], "entities": []}]}