{"title": [{"text": "NICT Self-Training Approach to Neural Machine Translation at NMT-2018", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.6161250571409861}, {"text": "NMT-2018", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.6699681878089905}]}], "abstractContent": [{"text": "This paper describes the NICT neural machine translation system submitted at the NMT-2018 shared task.", "labels": [], "entities": [{"text": "NICT neural machine translation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.6018602922558784}, {"text": "NMT-2018 shared task", "start_pos": 81, "end_pos": 101, "type": "DATASET", "confidence": 0.7654785315195719}]}, {"text": "A characteristic of our approach is the introduction of self-training.", "labels": [], "entities": []}, {"text": "Since our self-training does not change the model structure, it does not influence the efficiency of translation, such as the translation speed.", "labels": [], "entities": []}, {"text": "The experimental results showed that the translation quality improved not only in the sequence-to-sequence (seq-to-seq) models but also in the transformer models.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this study, we introduce the NICT neural translation system at the Second Workshop on Neural Machine Translation and Generation (NMT-2018) . A characteristic of the system is that translation qualities are improved by introducing self-training, using open-source neural translation systems and defined training data.", "labels": [], "entities": [{"text": "NICT neural translation", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.6564907232920328}, {"text": "Neural Machine Translation and Generation (NMT-2018)", "start_pos": 89, "end_pos": 141, "type": "TASK", "confidence": 0.7818953320384026}]}, {"text": "The self-training method discussed herein is based on the methods proposed by and, and they are applied to a self training strategy.", "labels": [], "entities": []}, {"text": "It extends only the source side of the training data to increase variety.", "labels": [], "entities": [{"text": "variety", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9347131848335266}]}, {"text": "The merit of the proposed self-training strategy is that it does not influence the efficiency of the translation, such as the translation speed, because it does not change the model structure.", "labels": [], "entities": []}, {"text": "(However, the training time increases due to an increase in the training data size.)", "labels": [], "entities": []}, {"text": "The proposed approach can be applied to any translation method.", "labels": [], "entities": []}, {"text": "However, we want to confirm on which model our approach is practically effective.", "labels": [], "entities": []}, {"text": "This paper verifies the effect of our selftraining method in the following two translation models: \u2022 Sequence-to-sequence (seq-to-seq) models) based on recurrent neural networks (RNNs).", "labels": [], "entities": []}, {"text": "Herein, we use OpenNMT ( as an implementation of the seqto-seq model.", "labels": [], "entities": [{"text": "OpenNMT", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.91134113073349}]}, {"text": "\u2022 The transformer model proposed by.", "labels": [], "entities": []}, {"text": "We used Marian NMT ( as an implementation of the transformer model.", "labels": [], "entities": [{"text": "Marian NMT", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.9351591765880585}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the proposed approach.", "labels": [], "entities": []}, {"text": "Section 3 describes the details of our system.", "labels": [], "entities": []}, {"text": "Section 4 explains the results of experiments, and Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe our results for the NMT-2018 shared task in English-German translation.", "labels": [], "entities": [{"text": "NMT-2018 shared task", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.555774986743927}]}, {"text": "Note that the shared task uses the WMT-2014 data set preprocessed by Stanford NLP Group.", "labels": [], "entities": [{"text": "WMT-2014 data set preprocessed by Stanford NLP Group", "start_pos": 35, "end_pos": 87, "type": "DATASET", "confidence": 0.8806536123156548}]}, {"text": "In our experiments, we add two baselines.", "labels": [], "entities": []}, {"text": "One is the model trained from the original parallel corpus only.", "labels": [], "entities": []}, {"text": "Another is the model trained using the synthetic corpus, which contains 1-best generation and did not use the dynamic generation, with the original corpus.", "labels": [], "entities": []}, {"text": "For the inverse temperature parameter 1/\u03c4 , we tested 1.0, 1.2, and 1.5.", "labels": [], "entities": []}, {"text": "This is because the translation quality was better when the diversity was slightly inhibited in our preliminary experiments.", "labels": [], "entities": [{"text": "translation", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.9277194142341614}]}, {"text": "Note that the submitted system was Marian NMT (the transformer model) trained using 1/\u03c4 = 1.0 synthetic corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of Marian NMT (Transformer Model)  The bold values denote the results of the submitted system. (+) and (-) symbols denote results that are  significantly improved and degraded from the \"original parallel corpus only,\" respectively.", "labels": [], "entities": [{"text": "Marian NMT", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.9224249124526978}]}, {"text": " Table 3: Results of OpenNMT (Seq-to-Seq Model)  (+) and (-) symbols denote results that are significantly improved and degraded from the \"original parallel  corpus only,\" respectively.", "labels": [], "entities": []}]}