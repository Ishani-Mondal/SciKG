{"title": [], "abstractContent": [], "introductionContent": [{"text": "Reduplication is a common morphological process in which all or part of a word is copied and added to one side of the word's stem.", "labels": [], "entities": []}, {"text": "An example of reduplication occurring in the language Karao is given in (1): (1) Reduplication in): 'fight each other' (2 people) (>2 people) In the example above, the stem ba is reduplicated to create the affixed form baba.", "labels": [], "entities": []}, {"text": "Berent (2013) discusses four different possibilities for how speakers could represent reduplication in their minds: (i) memorization of which reduplicated forms go with which stems, (ii) learning a function that copies all segments that undergo reduplication, (iii) learning a function that copies all feature values that undergo reduplication, or (iv) learning a function that uses algebraic symbols to copy the appropriate material, regardless of its segmental or featural content.", "labels": [], "entities": []}, {"text": "She concludes that reduplication and similar processes in language involve the fourth possibility, which she labels an identity function.", "labels": [], "entities": []}, {"text": "An identity function for reduplication is illustrated in (2), with \u03b1 acting as a variable that represents the reduplicated sequence.", "labels": [], "entities": []}, {"text": "(2) Reduplication as an algebraic rule \u03b1 \u2192 \u03b1\u03b1 came to a similar conclusion regarding reduplication and identity functions, after showing that infants could learn a reduplication-like pattern and generalize that pattern to novel segments.", "labels": [], "entities": []}, {"text": "They used this as evidence against connectionist models of grammar, which do not typically include explicit variables 1 (see, for example,.", "labels": [], "entities": []}, {"text": "Both feed-forward and simple recurrent neural networks fail at learning generalizable identity functions.", "labels": [], "entities": []}, {"text": "In this paper, we revisit these arguments against variable-free connectionist models in light of recent developments in neural network architecture and training techniques.", "labels": [], "entities": []}, {"text": "Specifically, we test Sequence-to-Sequence models ) with LTSM (Long Short-Term Memory;) and dropout ().", "labels": [], "entities": [{"text": "LTSM", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9574205875396729}]}, {"text": "We find that the scope of generalization for the models is increased from copying segments to copying feature values when dropout is added.", "labels": [], "entities": []}, {"text": "Additionally, we argue that variable-free feature copying is sufficient to model human generalization, contrary to claim that an algebraic identity function is necessary.", "labels": [], "entities": [{"text": "variable-free feature copying", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.7087323864301046}]}], "datasetContent": [{"text": "To test whether reduplication can be modeled by a neural network without explicit variables, we ran a number of simulations in which the model was trained on a reduplication pattern in a toy language and tested on how it generalized that pattern to novel data.", "labels": [], "entities": []}, {"text": "In the experiments presented here, a language's segments were each represented by a unique, randomly-produced vector of 6 features (excluding the simulations in \u00a74.3), with feature values being either -1 or 1 (corresponding to the and used in standard phonological models).", "labels": [], "entities": []}, {"text": "The inventory was divided into consonants and vowels by treating the first feature as, i.e. any of the feature vectors that began with -1 were considered a consonant and any that began with 1 were considered a vowel.", "labels": [], "entities": []}, {"text": "If an inventory had no vowels, one of its consonants was randomly chosen and its value for the feature [syllabic] was changed to 1.", "labels": [], "entities": []}, {"text": "The toy language for any given simulation consisted of all the possible CV syllables that could be made with that simulation's randomly created segment inventory.", "labels": [], "entities": []}, {"text": "Crucially, before the data was given to the model, some portion of it was withheld for testing (see the subsections below for more information on what was withheld in each testing condition).", "labels": [], "entities": []}, {"text": "The mapping that the model was trained on treated each stem (e.g.) as input and each reduplicated form (e.g.) as output.", "labels": [], "entities": []}, {"text": "The model's input and output lengths were fixed to 2 and 4 segments, respectively (reflecting the fact that all the toy languages only had stems that were 2 segments long).", "labels": [], "entities": []}, {"text": "The models were trained for 1000 epochs, with training batches that included all of the learning data (i.e. learning was done in batch).", "labels": [], "entities": []}, {"text": "The loss function that was being minimized was mean-squared error, and the minimization algorithm was RMSprop).", "labels": [], "entities": [{"text": "mean-squared error", "start_pos": 47, "end_pos": 65, "type": "METRIC", "confidence": 0.9435398876667023}]}, {"text": "The models had 2 layers each in the encoder and decoder, with 18 units in each of these layers.", "labels": [], "entities": []}, {"text": "All other parameters were the default values in the deep-learning Python package,).", "labels": [], "entities": []}, {"text": "To test whether the model generalized to withheld data, a relatively strict definition of success was used in testing.", "labels": [], "entities": []}, {"text": "The model was given a withheld stem as input, and the output it predicted was compared to the correct output (i.e. the reduplicated form of the stem it was given).", "labels": [], "entities": []}, {"text": "If every feature value in the predicted output had the same sign (positive/negative) as its counterpart in the correct output, the model was considered to be successfully generalizing the reduplication pattern.", "labels": [], "entities": []}, {"text": "However, if any of the feature values did not have the same sign, that model was considered to be non-generalizing.", "labels": [], "entities": []}, {"text": "While we only report the results from 25 runs in each condition, we ran many more while investigating various hyperparameter settings and possibilities about the construction of the training data.", "labels": [], "entities": []}, {"text": "The results presented here are representative of the general pattern of results.", "labels": [], "entities": []}], "tableCaptions": []}