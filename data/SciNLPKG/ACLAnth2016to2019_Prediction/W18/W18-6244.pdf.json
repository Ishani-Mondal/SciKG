{"title": [{"text": "Learning representations for sentiment classification using Multi-task framework", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.9657847285270691}]}], "abstractContent": [{"text": "Most of the existing state of the art sentiment classification techniques involve the use of pre-trained embeddings.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8194118440151215}]}, {"text": "This paper postulates a generalized representation that collates training on multiple datasets using a Multi-task learning framework.", "labels": [], "entities": []}, {"text": "We incorporate publicly available, pre-trained embeddings with Bidirectional LSTM's to develop the multi-task model.", "labels": [], "entities": []}, {"text": "We validate the representations on an independent test Irony dataset that can contain several sentiments within each sample, with an arbitrary distribution.", "labels": [], "entities": [{"text": "Irony dataset", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.716284990310669}]}, {"text": "Our experiments show a significant improvement in results as compared to the available baselines for individual datasets on which independent models are trained.", "labels": [], "entities": []}, {"text": "Results also suggest superior performance of the representations generated over Irony dataset.", "labels": [], "entities": [{"text": "Irony dataset", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9800511598587036}]}], "introductionContent": [{"text": "Sentiment analysis has attracted substantial research interest, especially in the field of social media, owing to the growing number of data and active users.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9621593952178955}]}, {"text": "In addition, the research community has gravitated towards a pragmatic characterization of language with the division into (and quantification of) specific emotions for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.9513365030288696}]}, {"text": "This approach has come to prominence in recent times as a large number of enterprises (not just social media corporations) now rely on understanding customer sentiments for defining product and marketing strategies (.", "labels": [], "entities": []}, {"text": "Beyond strategic inputs, sentiment analysis also performs a tactical role in the age of rapid (viral) increases and decreases in the visibility of specific events, with magnified consequences for corporations and communities at large.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9678178131580353}]}, {"text": "For example, United Airlines faced significant business impact due to a single (possibly isolated) passengerrelated incident, due to its spread over Twitter . It is conceivable that an automated system quickly alerting the management about the rate and depth of negative sentiments due to the incident, would have enabled them to produce a more amelioratory response from the outset.", "labels": [], "entities": [{"text": "United Airlines", "start_pos": 13, "end_pos": 28, "type": "DATASET", "confidence": 0.8899008929729462}]}, {"text": "Complementary to such motivating incidents is the recent availability of large datasets from social media sources.", "labels": [], "entities": []}, {"text": "Twitter has become a go-to choice for scraping data due to its large user base and the easy accessibility of tweets through its API.", "labels": [], "entities": []}, {"text": "The result is a large corpus of complex sentiments for identification and analysis.", "labels": [], "entities": [{"text": "identification and analysis", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.7296084066232046}]}, {"text": "Tweets (the messages posted on Twitter) are limited to 140 characters, which creates a plethora of challenges as the users find new and innovative ways of condensing the messages using slang, hashtags, and emojis, often defying traditional grammatical rules of the language.", "labels": [], "entities": []}, {"text": "This is further complicated by the fast, localized rise and decay of popular memes, slang, and hashtags.", "labels": [], "entities": []}, {"text": "Traditional sentiment analysis using dictionarybased methods has failed to capture these nuances, as the methods rely on grammatically correct, intact syntactic and semantic structures which are not followed in this space.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.9416989386081696}]}, {"text": "Traditional sentiment analyzers such as () that worked well with well-written texts, face challenges at lexical, syntactic and semantic levels when dealing with tweets as analyzed in.", "labels": [], "entities": [{"text": "sentiment analyzers", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.8720337450504303}]}, {"text": "Bag-of-words models and naive Bayes models are sequence-agnostic, and have therefore failed to generalize over a diverse distribution of sentiments, especially when multiple fine-grained emotions are compressed into a 140 character message.", "labels": [], "entities": []}, {"text": "Word vectors trained on a large corpus to represent the word in dense representations have proved to be efficient in handling sentiment analysis and effective emotions.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.947340190410614}]}, {"text": "Deep learning and specifically Recurrent Neural Networks have been extensively used with word vectors to achieve state of the art results on various sentiment analysis tasks.", "labels": [], "entities": [{"text": "sentiment analysis tasks", "start_pos": 149, "end_pos": 173, "type": "TASK", "confidence": 0.9389535188674927}]}, {"text": "Although there are large datasets available on social media space, deep learning models require annotated data for supervised training.", "labels": [], "entities": []}, {"text": "Annotation for such a large dataset is expensive, since multiple human annotators are required per sample for stable convergence.", "labels": [], "entities": []}, {"text": "A useful research question is how to leverage resources available on social media sites to improve sentiment classification across datasets by leveraging the generic representations and handling the noise present in the space.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.9443888664245605}]}, {"text": "These challenges have led people to use transfer learning and multi-task learning approaches to transfer knowledge across different datasets and languages.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.8859966397285461}]}, {"text": "Recently, neural-network-based models for multitask learning have become very popular, ranging from computer vision ( to natural language processing, since they provide a convenient way of combining information from multiple tasks.", "labels": [], "entities": []}, {"text": "We propose a dual Attention based deep learning model which creates representations using Bidirectional LSTM.", "labels": [], "entities": []}, {"text": "In particular, given an input tweet, our model first uses a pair of bidirectional LSTMs to learn a general representation.", "labels": [], "entities": []}, {"text": "This portion of the model is trained in a multitask framework.", "labels": [], "entities": []}, {"text": "The general sentence representation is then mapped into a task-specific representation through an attention mechanism, so that the most salient parts of the input are selected for each task.", "labels": [], "entities": []}, {"text": "We achieve significant improvement over the baselines and obtain comparable results with the state of the art methods without any feature engineering.", "labels": [], "entities": []}, {"text": "We have selected datasets which classify a text into 3 classes, along with affect dataset.", "labels": [], "entities": []}, {"text": "Affective dimensions provide much more granular analysis over emotions that are being conveyed.", "labels": [], "entities": []}, {"text": "Affective emotions are classified along the valence, arousal and dominance axis according to circumplex model of affect, a well-established system for describing emotional states).", "labels": [], "entities": []}, {"text": "Of these states, valence can directly be mapped to sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.9131512641906738}]}, {"text": "These scales represent valence (or sentiment) and arousal (or intensity), which defines each posts position on the circumplex of the 3 dimension The major contributions of this paper are: \u2022 Generating robust representation of a tweet from three different set of pre-trained embeddings which can handle emoji/smileys and out-of-vocabulary words in the dataset.", "labels": [], "entities": []}, {"text": "\u2022 Multi Task learning framework using Bidirectional Long short Memory Networks (BiLSTM) and attention mechanism to effectively learn the representations across datasets.", "labels": [], "entities": []}, {"text": "We evaluate the effectiveness of the model with respect to both internal and external distribution.", "labels": [], "entities": []}, {"text": "The former refers to the setting where distribution of the test data falls in one of them training tasks, and the latter refers to the setting where task and data are different and we use just the representation to train the task-specific layers.", "labels": [], "entities": []}, {"text": "Rest of the paper is organized as follows, section 2 discusses works related to multi-task learning along the lines of sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.9460799396038055}]}, {"text": "We present our proposed approach in section 3, which details the system architecture and its key components.", "labels": [], "entities": []}, {"text": "Two sets of experiments and results shown in section 4 and 5 respectively.", "labels": [], "entities": []}, {"text": "Finally section 6 concludes the paper with future direction.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to validate our approach we perform two experiments.", "labels": [], "entities": []}, {"text": "In experiment-1 we train our model on mixture of regression and classification tasks and access its performance over the same task by fine tuning it for the same task.", "labels": [], "entities": []}, {"text": "In experiment-2 we accesses the representations that are obtained during experiment-1 on a different task.", "labels": [], "entities": []}, {"text": "We train and evaluate our model on sentiment classification SemEval dataset obtained through shared task and affect emotion dataset from SemEval-2018.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.929332047700882}, {"text": "SemEval dataset", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.8355650007724762}]}, {"text": "These tasks are based on Twitter text and align to our objective of classifying short and noisy text present in the social media space.", "labels": [], "entities": []}, {"text": "Although sentiment and affect task require a varying degree of representation where sentiment classification in positive, negative and neutral space can be relatively easier, representations required for this tasks are not present in the pre-trained embeddings.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.75846928358078}]}, {"text": "For sentiment classification dataset we use SemEval-2017 Task 4 Subtask A dataset.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.9300810992717743}, {"text": "SemEval-2017 Task 4 Subtask A dataset", "start_pos": 44, "end_pos": 81, "type": "DATASET", "confidence": 0.6349500815073649}]}, {"text": "It contains a tweet and its respective label from positive, negative and neutral in english language.", "labels": [], "entities": []}, {"text": "From hereon we refer to this dataset as Sem-3.", "labels": [], "entities": []}, {"text": "The classes presented are imbalance and negative tweets are around 15% in training set and 32% in test set (.", "labels": [], "entities": []}, {"text": "For Affective emotion, we use dataset which was presented as in subtask EI-reg, EI-oc contains tweets specific to 4 emotions namely, Anger, fear, Joy and Sadness for english language.", "labels": [], "entities": [{"text": "Joy", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.9804922342300415}]}, {"text": "Subtask V-reg, V-oc contains the tweets for valence denoting range of positive to negative of sentiment.", "labels": [], "entities": []}, {"text": "In subtasks EI-reg and V-reg, Given a tweet and its corresponding emotion, predict the intensity score of that emotion between 0 to 1, 0 being lowest and 1 being highest.", "labels": [], "entities": [{"text": "V-reg", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.941554844379425}, {"text": "intensity score", "start_pos": 87, "end_pos": 102, "type": "METRIC", "confidence": 0.9495795369148254}]}, {"text": "Whereas, for subtasks EIoc and V-oc we need to classify them into predefined classes, where oc means ordinal classification.", "labels": [], "entities": []}, {"text": "In this dataset, emotions are classified in 4 distinct labels from mildly felt emotion to strongly felt emotion, while valence is classified into 7 distinct classes.", "labels": [], "entities": []}, {"text": "Distribution of the datasets into the train development and test set is presented in table 1.", "labels": [], "entities": []}, {"text": "Predicting intensity for emotions and valence are considered as regression task, while classifying into one of the classes is considered as classification task.", "labels": [], "entities": []}, {"text": "We have 5 regression tasks and 6 classification tasks across these two datasets.", "labels": [], "entities": []}, {"text": "Irony detection in the social media is one such field which is correlated with the sentiment analysis.", "labels": [], "entities": [{"text": "Irony detection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7021536082029343}, {"text": "sentiment analysis", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8695515990257263}]}, {"text": "Although it requires different set of features, sentiment and affective emotions enhances the detection accuracies as reported in.", "labels": [], "entities": []}, {"text": "In this experiment, we apply representations generated earlier to irony classification to access its robustness.", "labels": [], "entities": [{"text": "irony classification", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.9270112216472626}]}, {"text": "We have used irony detection dataset introduced in SemEval-2018 task 3 (Van Hee et al., 2018).", "labels": [], "entities": [{"text": "irony detection", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.8500960767269135}, {"text": "SemEval-2018 task 3", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7632991870244344}]}, {"text": "Dataset was augmented and hashtags used to mine the tweets such as \"#irony\", \"#sarcasm\", etc., were omitted for testing.", "labels": [], "entities": [{"text": "Dataset", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8275420069694519}]}, {"text": "We have removed this hashtags during training as well to keep the dataset consistent.", "labels": [], "entities": []}, {"text": "This task contained two subtask, namely Subtask A and Subtask B. Objective of Subtask A was to classify whether a tweet contains irony or not, while of Subtask B was to classify into verbal irony (V-irony), situational irony (S-irony), other types of irony (O-irony) and non-ironic.", "labels": [], "entities": []}, {"text": "Distribution of the dataset along the training and testing is presented in table 3.", "labels": [], "entities": []}, {"text": "For this we extract the representations from the model trained in experiment 1, specifically we take output of 2 BiLSTM layers.", "labels": [], "entities": []}, {"text": "So for each sample in this dataset we have a 2D matrix of shape \ud97b\udf59n w \u00d7 b1\ud97b\udf59, n w is the maximum sequence length and b1 is the number of hidden units in the BiL-STM layer 1.", "labels": [], "entities": []}, {"text": "Similarly we obtain the representation from BiLSTM layer 2.", "labels": [], "entities": []}, {"text": "We concatenate this representation and pass it onto classification network consisting of single BiLSTM layer and two fully connected layers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Distribution of Irony Dataset across train and  test", "labels": [], "entities": [{"text": "Irony Dataset", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.7824748456478119}]}, {"text": " Table 4: Results of Experiment 1", "labels": [], "entities": []}, {"text": " Table 5: Irony Detection F1 score", "labels": [], "entities": [{"text": "Irony Detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8202688694000244}, {"text": "F1", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.6819589138031006}]}]}