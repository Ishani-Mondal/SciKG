{"title": [{"text": "Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7271768450737}, {"text": "Auxiliary Task Analysis", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.6167845626672109}]}], "abstractContent": [], "introductionContent": [{"text": "Recently, researchers have found that deep LSTMs) trained on tasks like machine translation learn substantial syntactic and semantic information about their input sentences, including part-of-speech (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7833130955696106}]}, {"text": "These findings begin to shed light on why pretrained representations, like ELMo and CoVe, are so beneficial for neural language understanding models (.", "labels": [], "entities": [{"text": "neural language understanding", "start_pos": 112, "end_pos": 141, "type": "TASK", "confidence": 0.6744312743345896}]}, {"text": "We still, though, do not yet have a clear understanding of how the choice of pretraining objective affects the type of linguistic information that models learn.", "labels": [], "entities": []}, {"text": "With this in mind, we compare four objectives-language modeling, translation, skip-thought, and autoencoding-on their ability to induce syntactic and part-of-speech information, holding constant the quantity and genre of the training data, as well as the LSTM architecture.", "labels": [], "entities": [{"text": "translation", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.9651774168014526}]}], "datasetContent": [], "tableCaptions": []}