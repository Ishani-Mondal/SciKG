{"title": [{"text": "A Multi-Stage Memory Augmented Neural Network for Machine Reading Comprehension", "labels": [], "entities": [{"text": "Multi-Stage Memory Augmented Neural Network", "start_pos": 2, "end_pos": 45, "type": "TASK", "confidence": 0.7255271136760711}, {"text": "Machine Reading Comprehension", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.8114606142044067}]}], "abstractContent": [{"text": "Reading Comprehension (RC) of text is one of the fundamental tasks in natural language processing.", "labels": [], "entities": [{"text": "Reading Comprehension (RC) of text", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8344459618840899}, {"text": "natural language processing", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.6450284024079641}]}, {"text": "In recent years, several end-to-end neural network models have been proposed to solve RC tasks.", "labels": [], "entities": []}, {"text": "However, most of these models suffer in reasoning overlong documents.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel Memory Augmented Machine Comprehension Network (MAMCN) to address long-range dependencies present in machine reading comprehension.", "labels": [], "entities": []}, {"text": "We perform extensive experiments to evaluate proposed method with the renowned benchmark datasets such as SQuAD, QUASAR-T, and TriviaQA.", "labels": [], "entities": [{"text": "QUASAR-T", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.5494644045829773}]}, {"text": "We achieve the state of the art performance on both the document-level (QUASAR-T, TriviaQA) and paragraph-level (SQuAD) datasets compared to all the previously published approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reading Comprehension (RC) is essential for understanding human knowledge written in text form.", "labels": [], "entities": [{"text": "Reading Comprehension (RC)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7625242173671722}, {"text": "understanding human knowledge written in text form", "start_pos": 44, "end_pos": 94, "type": "TASK", "confidence": 0.7922840373856681}]}, {"text": "One possible way of measuring RC is by formulating it as answer span prediction style Question Answering (QA) task, which is finding an answer to the question based on the given document(s).", "labels": [], "entities": [{"text": "answer span prediction style Question Answering (QA)", "start_pos": 57, "end_pos": 109, "type": "TASK", "confidence": 0.7580425706174638}]}, {"text": "Recently, influential deep learning approaches have been proposed to solve this QA task.; propose the attention mechanism between question and context for question-aware contextual representation.", "labels": [], "entities": [{"text": "QA task.", "start_pos": 80, "end_pos": 88, "type": "TASK", "confidence": 0.8883174657821655}, {"text": "question-aware contextual representation", "start_pos": 155, "end_pos": 195, "type": "TASK", "confidence": 0.6321821014086405}]}, {"text": "refine these contextual representations by using self-attention to improve the performance.", "labels": [], "entities": []}, {"text": "Even further performance improvement is gained by using contextualized word representations for query and context.", "labels": [], "entities": []}, {"text": "Based on those approaches, several methods have successfully made progress towards reaching human-level performance on SQuAD (.", "labels": [], "entities": []}, {"text": "Each training example in the SQuAD only has the relevant paragraph with the corresponding answer.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.6277943849563599}]}, {"text": "However, most of the documents present in the real-world are long, containing relevant and irrelevant paragraphs, and do not guarantee answer presence.", "labels": [], "entities": [{"text": "answer presence", "start_pos": 135, "end_pos": 150, "type": "TASK", "confidence": 0.7746071219444275}]}, {"text": "Therefore the models proposed to solve SQuAD have difficulty in applying to real-world documents (.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 39, "end_pos": 44, "type": "TASK", "confidence": 0.9154227375984192}]}, {"text": "Recently, QUASAR-T () and TriviaQA () datasets have been proposed to resemble real-world document.", "labels": [], "entities": [{"text": "TriviaQA () datasets", "start_pos": 26, "end_pos": 46, "type": "DATASET", "confidence": 0.6885314484437307}]}, {"text": "These datasets use document-level evidence as training example instead of using only the relevant paragraph and evidence does not guarantee answer presence, which makes them more realistic.", "labels": [], "entities": []}, {"text": "To effectively comprehend long documents present in the QUASAR-T and TriviaQA datasets, the QA models have to resolve long-range dependencies present in these documents.", "labels": [], "entities": [{"text": "QUASAR-T", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.9126943945884705}, {"text": "TriviaQA datasets", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.8720958530902863}]}, {"text": "In this work, we build a QA model that can understand long documents by utilizing Memory Augmented Neural Networks (MANNs) (.", "labels": [], "entities": []}, {"text": "This type of neural networks decouples the memory capacity from the number of model parameters.", "labels": [], "entities": []}, {"text": "While there have been several attempts to use MANNs in managing long-range dependencies, applications are limited to only toy datasets ().", "labels": [], "entities": []}, {"text": "Compared to the previous approaches, we mainly focus on the document-level QA task on QUASAR-T and TriviaQA.", "labels": [], "entities": [{"text": "QA task", "start_pos": 75, "end_pos": 82, "type": "TASK", "confidence": 0.7465450763702393}, {"text": "QUASAR-T", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.8585721850395203}, {"text": "TriviaQA", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.8636404871940613}]}, {"text": "We also apply our model to SQuAD to show that our model even works well on the paragraph-level.", "labels": [], "entities": []}, {"text": "Our contributions in this work are as follows: (", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present our experimental setup for evaluating the performance of our MAMCN model.", "labels": [], "entities": [{"text": "MAMCN model", "start_pos": 89, "end_pos": 100, "type": "DATASET", "confidence": 0.8105100691318512}]}, {"text": "We select different datasets based on their average document length to check the effectiveness of external memory on RC task.", "labels": [], "entities": []}, {"text": "We compare the performance of our model with all the published results and the baseline memory augmented model.", "labels": [], "entities": []}, {"text": "The baseline model is developed by replacing modeling layer in BiDAF () model with the memory controller from DNC ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data statistics of SQuAD, QUASAR- T, and TriviaQA. The Average Document Length  (ADL) represents the average number of words.  In TriviaQA, ADL was calculated after truncating  the documents to the first 1200 words.", "labels": [], "entities": [{"text": "QUASAR- T", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.4915105998516083}, {"text": "Average Document Length  (ADL)", "start_pos": 65, "end_pos": 95, "type": "METRIC", "confidence": 0.9585237900416056}, {"text": "ADL", "start_pos": 150, "end_pos": 153, "type": "METRIC", "confidence": 0.9920110106468201}]}, {"text": " Table 3: Performance results on QUASAR-T dataset.", "labels": [], "entities": [{"text": "QUASAR-T dataset", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.9713766872882843}]}, {"text": " Table 4: Single model results on TriviaQA dataset 3 (Web and Wikipedia). SA: Self-attention, SN: Shared  normalization.", "labels": [], "entities": [{"text": "TriviaQA dataset", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.787485659122467}]}, {"text": " Table 5: Single model results on SQuAD dataset 4 . The last two columns in tables indicate whether mod- els use additional feature and self-attention. AF: Additional feature augmentation for word embedding,  SA: Self-attention, DC: Densely connected embedding block.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.7388009577989578}]}]}