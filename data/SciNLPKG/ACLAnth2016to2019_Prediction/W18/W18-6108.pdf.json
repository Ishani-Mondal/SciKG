{"title": [{"text": "Robust Word Vectors: Context-Informed Embeddings for Noisy Texts", "labels": [], "entities": []}], "abstractContent": [{"text": "We suggest anew language-independent architecture of robust word vectors (RoVe).", "labels": [], "entities": []}, {"text": "It is designed to alleviate the issue of typos, which are common in almost any user-generated content, and hinder automatic text processing.", "labels": [], "entities": [{"text": "text processing", "start_pos": 124, "end_pos": 139, "type": "TASK", "confidence": 0.6972059607505798}]}, {"text": "Our model is morphologically motivated, which allows it to deal with unseen word forms in morphologically rich languages.", "labels": [], "entities": []}, {"text": "We present the results on a number of Natural Language Processing (NLP) tasks and languages for the variety of related architectures and show that proposed architecture is typo-proof.", "labels": [], "entities": []}], "introductionContent": [{"text": "The rapid growth in the usage of mobile electronic devices has increased the number of user input text issues such as typos.", "labels": [], "entities": []}, {"text": "This happens because typing on a small screen and in transport (or while walking) is difficult, and people accidentally hit the wrong keys more often than when using a standard keyboard.", "labels": [], "entities": []}, {"text": "Spell-checking systems widely used in web services can handle this issue, but they can also make mistakes.", "labels": [], "entities": []}, {"text": "Meanwhile, any text processing system is now impossible to imagine without word embeddings -vectors encode semantic and syntactic properties of individual words (.", "labels": [], "entities": []}, {"text": "However, to use these word vectors the user input should be clean (i.e. free of misspellings), because a word vector model trained on clean data will not have misspelled versions of words.", "labels": [], "entities": []}, {"text": "There are examples of models trained on noisy data (, but this approach does not fully solve the problem, because typos are unpredictable and a corpus cannot contain all possible incorrectly spelled versions of a word.", "labels": [], "entities": []}, {"text": "Instead, we suggest that we should make algorithms for word vector modelling robust to noise.", "labels": [], "entities": [{"text": "word vector modelling", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.7906036972999573}]}, {"text": "We suggest anew architecture RoVe (Robust Vectors).", "labels": [], "entities": []}, {"text": "The main feature of this model is open vocabulary.", "labels": [], "entities": []}, {"text": "It encodes words as sequences of symbols.", "labels": [], "entities": []}, {"text": "This enables the model to produce embeddings for out-of-vocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "The idea as such is not new, many other models use character-level embeddings (  or encode the most common ngrams to assemble unknown words from them (.", "labels": [], "entities": []}, {"text": "However, unlike analogous models, RoVe is specifically targeted at typos -it is invariant to swaps of symbols in a word.", "labels": [], "entities": []}, {"text": "This property is ensured by the fact that each word is encoded as a bag of characters.", "labels": [], "entities": []}, {"text": "At the same time, word prefixes and suffixes are encoded separately, which enables RoVe to produce meaningful embeddings for unseen word forms in morphologically rich languages.", "labels": [], "entities": []}, {"text": "Notably, this is done without explicit morphological analysis.", "labels": [], "entities": []}, {"text": "Another feature of RoVe is context dependency -in order to generate an embedding fora word one should encode its context.", "labels": [], "entities": []}, {"text": "The motivation for such architecture is the following.", "labels": [], "entities": []}, {"text": "Our intuition is that when processing an OOV word our model should produce an embedding similar to that of some similar word from the training data.", "labels": [], "entities": []}, {"text": "This behaviour is suitable for typos as well as unseen forms of known words.", "labels": [], "entities": []}, {"text": "In the latter case we want a word to get an embedding similar to the embedding of its initial form.", "labels": [], "entities": []}, {"text": "This process reminds lemmatisation (reduction of a word to its initial form).", "labels": [], "entities": []}, {"text": "Lemmatisation is context-dependent since it often needs to resolve homonymy based on word's context.", "labels": [], "entities": []}, {"text": "By making RoVe model context-dependent we enable it to do such implicit lemmatisation.", "labels": [], "entities": []}, {"text": "We compare RoVe with common word vector tools: word2vec () and fasttext (.", "labels": [], "entities": []}, {"text": "We test the models on three tasks: paraphrase detection, identifi-cation of textual entailment, and sentiment analysis, and three languages with different linguistic properties: English, Russian, and Turkish.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.9206082224845886}, {"text": "sentiment analysis", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.9507983028888702}]}, {"text": "The paper is organised as follows.", "labels": [], "entities": []}, {"text": "In section 2 we review the previous work.", "labels": [], "entities": []}, {"text": "Section 3 contains the description of model's architecture.", "labels": [], "entities": []}, {"text": "In section 4 we describe the experimental setup, and report the results in section 5.", "labels": [], "entities": []}, {"text": "Section 6 concludes and outlines the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We check the performance of word vectors generated with RoVe on three tasks: \u2022 paraphrase detection, \u2022 sentiment analysis, \u2022 identification of text entailment.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.8011618256568909}, {"text": "sentiment analysis", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.9023890495300293}, {"text": "identification of text entailment", "start_pos": 125, "end_pos": 158, "type": "TASK", "confidence": 0.8591152429580688}]}, {"text": "For all tasks we train simple baseline models.", "labels": [], "entities": []}, {"text": "This is done deliberately to make sure that the performance is largely defined by the quality of vectors that we use.", "labels": [], "entities": []}, {"text": "For all the tasks we compare word vectors generated by different modifications of RoVe with vectors produced by word2vec and fasttext models.", "labels": [], "entities": []}, {"text": "We conduct the experiments on datasets for three languages: English (analytical language), Russian (synthetic fusional), and Turkish (synthetic agglutinative).", "labels": [], "entities": []}, {"text": "Affixes have different structures and purposes in these types of languages, and in our experiments we show that our BME representation is effective for all of them.", "labels": [], "entities": []}, {"text": "We did not tune n band n e parameters (lengths of B and E segments of BME).", "labels": [], "entities": []}, {"text": "In all our experiments we set them to 3, following the fact that the average length of affixes in Russian is 2.54.", "labels": [], "entities": []}, {"text": "However, they are not guaranteed to be optimal for English and Turkish.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of the paraphrase detection task in terms of ROC AUC.", "labels": [], "entities": [{"text": "paraphrase detection task", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.9494285384813944}, {"text": "ROC AUC", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.8912101089954376}]}, {"text": " Table 2: Results of the sentiment analysis and textual entailment tasks in terms of ROC AUC.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9693557918071747}, {"text": "textual entailment", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7019978016614914}, {"text": "ROC AUC", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.7022630572319031}]}]}