{"title": [{"text": "Using Paraphrasing and Memory-Augmented Models to Combat Data Sparsity in Question Interpretation with a Virtual Patient Dialogue System", "labels": [], "entities": [{"text": "Question Interpretation", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.7635897099971771}]}], "abstractContent": [{"text": "When interpreting questions in a virtual patient dialogue system, one must inevitably tackle the challenge of along tail of relatively infrequently asked questions.", "labels": [], "entities": [{"text": "interpreting questions", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.8940697312355042}]}, {"text": "To make progress on this challenge, we investigate the use of paraphrasing for data augmentation and neural memory-based classification, finding that the two methods work best in combination.", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7535624802112579}, {"text": "neural memory-based classification", "start_pos": 101, "end_pos": 135, "type": "TASK", "confidence": 0.6827259063720703}]}, {"text": "In particular, we find that the neural memory-based approach not only outperforms a straight CNN classifier on low frequency questions, but also takes better advantage of the augmented data created by paraphrasing, together yielding a nearly 10% absolute improvement inaccuracy on the least frequently asked questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "To develop skills such as taking a patient history and developing a differential diagnosis, medical students interact with actors who play the part of a patient with a specific medical history and pathology, known as Standardized Patients (SPs).", "labels": [], "entities": []}, {"text": "Although SPs remain the standard way to test medical students on such skills, SPs are expensive and can behave inconsistently from student to student.", "labels": [], "entities": [{"text": "SPs", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9550790190696716}]}, {"text": "A virtual patient dialogue system aims to overcome these issues as well as provide a means of supplying automated feedback on the quality of the medical student's interaction with the patient (see.", "labels": [], "entities": []}, {"text": "In previous work,; used a hand-crafted patternmatching system called ChatScript together with a 3D avatar in order to collect chatted dialogues and provide useful student feedback (.", "labels": [], "entities": []}, {"text": "ChatScript matches input text using handwritten patterns and outputs a scripted response for each dialogue turn.", "labels": [], "entities": []}, {"text": "With sufficient pattern-writing skill and effort, pattern matching with ChatScript can achieve relatively high accuracy, but it is unable to easily leverage increasing amounts of training data, somewhat brittle regarding misspellings, and can be difficult to maintain as new questions and patterns are added.", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.874356746673584}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.997319757938385}]}, {"text": "To address these issues, developed an ensemble of word-and characterbased convolutional neural networks (CNNs) for question identification in the system that attained 79% accuracy, comparable to the hand-crafted ChatScript patterns.", "labels": [], "entities": [{"text": "question identification", "start_pos": 115, "end_pos": 138, "type": "TASK", "confidence": 0.7257412970066071}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9990845918655396}]}, {"text": "Moreover, they found that since the CNN ensemble's error profile was very different from the pattern-based approach, combining the two systems yielded a nearly 10% boost in system accuracy and an error reduction of 47% in comparison to using ChatScript alone.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9943148493766785}, {"text": "error reduction", "start_pos": 196, "end_pos": 211, "type": "METRIC", "confidence": 0.9764319658279419}]}, {"text": "Perhaps not surprisingly, the CNN-based classifier outperformed the pattern-matching system on frequently asked questions, but on the least frequently asked questions-where data sparsity was an issuethe CNN performed much worse, only achieving 46.5% accuracy on the quintile of questions asked least often.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 250, "end_pos": 258, "type": "METRIC", "confidence": 0.9987266659736633}]}, {"text": "In this paper, we aim to combat this data sparsity issue by investigating (1) whether paraphrasing can be used to create novel synthetic training items, examining in particular lexical substitution from several resources) and neural MT for back-translation (; and (2) whether neural memory-based approaches developed for one-shot learning) perform better on low-frequency questions.", "labels": [], "entities": []}, {"text": "We find that the two methods work best in combination, as the neural memory-based approach not only outperforms the straight CNN classifier on low frequency questions, but also takes better advantage of the augmented data created by paraphrasing.", "labels": [], "entities": []}, {"text": "Together, the two methods yield nearly 13", "labels": [], "entities": []}], "datasetContent": [{"text": "We train our memory-augmented CNN classifier using a novel episodic training scheme based on the episodic training scheme used in oneshot learning ().", "labels": [], "entities": []}, {"text": "The main difference is that in one-shot learning, most tasks offer a balanced dataset with many classes but small numbers of instances per class.", "labels": [], "entities": []}, {"text": "In our scenario, the dataset is imbalanced, and some classes may have a large number of instances.", "labels": [], "entities": []}, {"text": "Moreover, in evaluation, there are no unseen classes in our case.", "labels": [], "entities": []}, {"text": "We modify the episodic training scheme to accommodate these differences.", "labels": [], "entities": []}, {"text": "In evaluation, we define a support set to be a batch of |C| \u00d7 k samples from the training data.", "labels": [], "entities": []}, {"text": "For a given test set, we first load the memory, then compare each test item to all the entries in the memory in order to generate the memory prediction for the test item based on the most similar memory entry.", "labels": [], "entities": []}, {"text": "This forms the model's 1-shot predictions.", "labels": [], "entities": []}, {"text": "Then we update the memory with the second sample for each class and redo the prediction step.", "labels": [], "entities": []}, {"text": "We now have the model predictions with 2 shots.", "labels": [], "entities": []}, {"text": "We continue to follow this routine until predictions from all k shots have been collected.", "labels": [], "entities": []}, {"text": "Because there is some randomness in how a support set is sampled from the data, we use multiple support sets in evaluation.", "labels": [], "entities": []}, {"text": "Since some of the classes have a large number of instances, each randomly sampled support set tends to be sufficiently different from other support sets that using multiple support sets becomes analogous to ensembling different models.", "labels": [], "entities": []}, {"text": "Finally, letting p be the number of support sets, we have k \u00d7 p predicted labels for each item in the test set.", "labels": [], "entities": []}, {"text": "We use majority voting across all the predicted labels to get the final model prediction.", "labels": [], "entities": []}, {"text": "This capitalizes on the ensembled support sets and reduces the variance of the model predictions.", "labels": [], "entities": []}, {"text": "We use the best model in, namely a stacked convolutional neural network, together with the model proposed in this work (MA-CNN) in all of the experiments.", "labels": [], "entities": []}, {"text": "Our task is to accurately predict a question's label based solely on the typed input from the medical student.", "labels": [], "entities": [{"text": "predict a question's label", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.8055245280265808}]}, {"text": "With improved accuracy, the virtual patient will be able to more coherently answer the students' questions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9984861016273499}]}, {"text": "We shuffle the gold dataset first and use 10-fold cross-validation to evaluate our data augmentation process.", "labels": [], "entities": [{"text": "gold dataset", "start_pos": 15, "end_pos": 27, "type": "DATASET", "confidence": 0.7604986727237701}]}, {"text": "We specifically focus our analysis on rare labels since that is also where we concentrate our data augmentation efforts.", "labels": [], "entities": []}, {"text": "The model we propose here is targeted at improving performance for the rare labels, therefore we are interested in how the model performs on them.", "labels": [], "entities": []}, {"text": "Paraphrases are not added to test sets, and paraphrases derived from those test items are filtered from training.", "labels": [], "entities": []}, {"text": "Finally, we compute significance using the McNemar test.", "labels": [], "entities": [{"text": "significance", "start_pos": 20, "end_pos": 32, "type": "METRIC", "confidence": 0.8300395011901855}]}], "tableCaptions": [{"text": " Table 2: Test results for the stacked CNN ensemble (Jin  et al., 2017) and the memory-augmented CNN clas- sifier (MA-CNN) without any generated paraphrases.  The difference of performance on the rare items is  highly significant (p = 9.5 \u00d7 10 \u22125 , McNemar's test).", "labels": [], "entities": []}, {"text": " Table 3: Test results for the stacked CNN ensem- ble and the memory-augmented CNN classifier (MA- CNN) with the manually filtered paraphrases. The  gain brought by the adding the automatically generated  paraphrases into training data for MA-CNN is highly  significant (p = 1.6 \u00d7 10 \u22124 , McNemar's test).", "labels": [], "entities": []}, {"text": " Table 4: Test results for the memory-augmented  CNN classifier (MA-CNN) with different filtering tech- niques.", "labels": [], "entities": []}, {"text": " Table 5: Test results for the memory-augmented CNN  classifier (MA-CNN) with different subsets of the man- ual filtered paraphrases generated using different para- phrase methods.", "labels": [], "entities": []}, {"text": " Table 6: Test results for the combiner as well as the  two combined subsystems: the stacked CNN ensem- ble trained with gold and the memory-augmented CNN  classifier trained with gold and generated paraphrases.  The gain compared to stacked CNN on full accuracy is  highly significant (p = 1.9 \u00d7 10 \u22129 , McNemar's test).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 254, "end_pos": 262, "type": "METRIC", "confidence": 0.9915988445281982}]}]}