{"title": [{"text": "Using Monolingual Data in Neural Machine Translation: a Systematic Study", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.655896008014679}]}], "abstractContent": [{"text": "Neural Machine Translation (MT) has radically changed the way systems are developed.", "labels": [], "entities": [{"text": "Neural Machine Translation (MT)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8634752730528513}]}, {"text": "A major difference with the previous generation (Phrase-Based MT) is the way mono-lingual target data, which often abounds, is used in these two paradigms.", "labels": [], "entities": [{"text": "Phrase-Based MT)", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.6630445818106333}]}, {"text": "While Phrase-Based MT can seamlessly integrate very large language models trained on billions of sentences , the best option for Neural MT developers seems to be the generation of artificial parallel data through back-translation-a technique that fails to fully take advantage of existing datasets.", "labels": [], "entities": [{"text": "Phrase-Based MT", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.5958435535430908}, {"text": "Neural MT", "start_pos": 129, "end_pos": 138, "type": "TASK", "confidence": 0.7699997127056122}]}, {"text": "In this paper, we conduct a systematic study of back-translation, comparing alternative uses of monolingual data, as well as multiple data generation procedures.", "labels": [], "entities": []}, {"text": "Our findings confirm that back-translation is very effective and give new explanations as to why this is the case.", "labels": [], "entities": []}, {"text": "We also introduce new data simulation techniques that are almost as effective , yet much cheaper to implement.", "labels": [], "entities": []}], "introductionContent": [{"text": "The new generation of Neural Machine Translation (NMT) systems is known to be extremely data hungry).", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.8243562380472819}]}, {"text": "Yet, most existing NMT training pipelines fail to fully take advantage of the very large volume of monolingual source and/or parallel data that is often available.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.868010938167572}]}, {"text": "Making a better use of data is particularly critical in domain adaptation scenarios, where parallel adaptation data is usually assumed to be small in comparison to out-of-domain parallel data, or to in-domain monolingual texts.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7186256498098373}]}, {"text": "This situation sharply contrasts with the previous generation of statistical MT engines, which could seamlessly integrate very large amounts of nonparallel documents, usually with a large positive effect on translation quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.6886799335479736}]}, {"text": "Such observations have been made repeatedly and have led to many innovative techniques to integrate monolingual data in NMT, that we review shortly.", "labels": [], "entities": []}, {"text": "The most successful approach to date is the proposal of, who use monolingual target texts to generate artificial parallel data via backward translation (BT).", "labels": [], "entities": [{"text": "backward translation (BT)", "start_pos": 131, "end_pos": 156, "type": "TASK", "confidence": 0.7730135679244995}]}, {"text": "This technique has since proven effective in many subsequent studies.", "labels": [], "entities": []}, {"text": "It is however very computationally costly, typically requiring to translate large sets of data.", "labels": [], "entities": []}, {"text": "Determining the \"right\" amount (and quality) of BT data is another open issue, but we observe that experiments reported in the literature only use a subset of the available monolingual resources.", "labels": [], "entities": [{"text": "BT data", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.7939961850643158}]}, {"text": "This suggests that standard recipes for BT might be sub-optimal.", "labels": [], "entities": [{"text": "BT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9813393950462341}]}, {"text": "This paper aims to better understand the strengths and weaknesses of BT and to design more principled techniques to improve its effects.", "labels": [], "entities": [{"text": "BT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.6473639607429504}]}, {"text": "More specifically, we seek to answer the following questions: since there are many ways to generate pseudo parallel corpora, how important is the quality of this data for MT performance?", "labels": [], "entities": [{"text": "MT", "start_pos": 171, "end_pos": 173, "type": "TASK", "confidence": 0.9927763938903809}]}, {"text": "Which properties of back-translated sentences actually matter for MT quality?", "labels": [], "entities": [{"text": "MT quality", "start_pos": 66, "end_pos": 76, "type": "TASK", "confidence": 0.8576079308986664}]}, {"text": "Does BT act as some kind of regularizer?", "labels": [], "entities": [{"text": "BT", "start_pos": 5, "end_pos": 7, "type": "DATASET", "confidence": 0.8532788157463074}]}, {"text": "Can BT be efficiently simulated?", "labels": [], "entities": [{"text": "BT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9304383397102356}]}, {"text": "Does BT data play the same role as a target-side language modeling, or are they complementary?", "labels": [], "entities": [{"text": "BT data", "start_pos": 5, "end_pos": 12, "type": "DATASET", "confidence": 0.784127950668335}]}, {"text": "BT is often used for domain adaptation: can the effect of having more indomain data be sorted out from the mere increase of training material)?", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.8208397626876831}]}, {"text": "For studies related to the impact of varying the size of BT data, we refer the readers to the recent work of.", "labels": [], "entities": [{"text": "BT data", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.8976739943027496}]}, {"text": "To answer these questions, we have reimplemented several strategies to use monolingual data in NMT and have run experiments on two language pairs in a very controlled setting (see \u00a7 2).", "labels": [], "entities": []}, {"text": "Our main results (see \u00a7 4 and \u00a7 5) suggest promising directions for efficient domain adaptation with cheaper techniques than conventional BT.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.724375531077385}]}, {"text": "We are mostly interested with the following training scenario: a large out-of-domain parallel corpus, and limited monolingual in-domain data.", "labels": [], "entities": []}, {"text": "We focus hereon the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus 1 () for two translation directions: English\u2192German and English\u2192French.", "labels": [], "entities": [{"text": "Europarl domain", "start_pos": 20, "end_pos": 35, "type": "DATASET", "confidence": 0.9538963735103607}, {"text": "Europarl corpus 1", "start_pos": 127, "end_pos": 144, "type": "DATASET", "confidence": 0.9518328507741293}]}, {"text": "As we study the benefits of monolingual data, most of our experiments only use the target side of this corpus.", "labels": [], "entities": []}, {"text": "The rationale for choosing this domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario.", "labels": [], "entities": []}, {"text": "For both language pairs, we use the Europarl tests from 2007 and 2008 2 for evaluation purposes, keeping test 2006 for development.", "labels": [], "entities": [{"text": "Europarl tests from 2007", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.9791969656944275}]}, {"text": "When measuring out-of-domain performance, we will use the WMT newstest 2014.", "labels": [], "entities": [{"text": "WMT newstest 2014", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.9773856004079183}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance wrt. different BT qualities", "labels": [], "entities": []}, {"text": " Table 3: BLEU scores for (backward) translation into English", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995181560516357}]}, {"text": " Table 4: Selection strategies for BT data (English-French)", "labels": [], "entities": []}, {"text": " Table 6: Performance wrt. different GAN setups", "labels": [], "entities": []}, {"text": " Table 8: BLEU scores with selective parameter freezing", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9983848333358765}]}]}