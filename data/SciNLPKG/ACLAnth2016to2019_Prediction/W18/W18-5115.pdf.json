{"title": [{"text": "Learning Representations for Detecting Abusive Language", "labels": [], "entities": [{"text": "Detecting Abusive Language", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.9270966649055481}]}], "abstractContent": [{"text": "This paper discusses the question whether it is possible to learn a generic representation that is useful for detecting various types of abusive language.", "labels": [], "entities": []}, {"text": "The approach is inspired by recent advances in transfer learning and word embed-dings, and we learn representations from two different datasets containing various degrees of abusive language.", "labels": [], "entities": []}, {"text": "We compare the learned representation with two standard approaches; one based on lexica, and one based on data-specific n-grams.", "labels": [], "entities": []}, {"text": "Our experiments show that learned representations do contain useful information that can be used to improve detection performance when training data is limited.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abusive language is prevalent on the Internet of today.", "labels": [], "entities": []}, {"text": "Many users of social media can attest to the frequent occurrence of negative slurs, racist and sexist comments, hate speech, cyberbullying, and outright threats.", "labels": [], "entities": []}, {"text": "Commentary fields of news outlets, discussion forums, blogs, and normal websites are overflooded by abusive language, forcing administrators to restrict the possibility to comment on content, and in many cases removing this possibility altogether.", "labels": [], "entities": []}, {"text": "As unfortunate as this development maybe, it is hardly surprising.", "labels": [], "entities": []}, {"text": "Our current information landscape has been designed to maximize the effectiveness of human communication, and factors such as transparency, trust and credibility have remained of peripheral concern for service providers.", "labels": [], "entities": []}, {"text": "The combination of accessibility and anonymity of many online services provides the perfect conditions for \"dark triad\" behavior () to flourish.", "labels": [], "entities": []}, {"text": "Even traditional news media, which have been seen as the last bastion for credibility and trust, are nowadays driven by the need for fast updates, sensationalism, and the hunt for clicks.", "labels": [], "entities": []}, {"text": "It should serve as a cautionary observation that even fringe phenomena such as trolling fit comfortably in the current media landscape on the Internet.", "labels": [], "entities": []}, {"text": "Our research focus in this paper is the question whether an inherently abusive environment such as a discussion forum or a white supremacist website can be used to learn a generic representation of abusive language, and whether such a representation can be used in supervised methods for detecting abusive language.", "labels": [], "entities": []}, {"text": "Our work is inspired on the one hand by recent advances in transfer learning and pre-training of deep neural networks, and on the other hand the use of embeddings as representation layer in text classification (.", "labels": [], "entities": [{"text": "text classification", "start_pos": 190, "end_pos": 209, "type": "TASK", "confidence": 0.7964870035648346}]}, {"text": "We use two different data sources for learning representations (Stormfront and Reddit, further detailed in Section 4.1) and three different representation learning mechanisms (character-enhanced word embeddings, document-enhanced word embeddings, and a character-level language model, all further detailed in Section 3.3).", "labels": [], "entities": []}, {"text": "We compare the proposed approaches with standard lexiconbased classification as well as supervised classification using Bag-of-Words n-gram representations.", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.7098302245140076}]}], "datasetContent": [{"text": "In order to compare the viability for detecting abusive language of the representations described in the previous sections, we use two different datasets for building embeddings, and four different datasets for validating classifiers based on the representations.", "labels": [], "entities": []}, {"text": "Since our main focus in this paper is to study the effect of the representations rather than pursuing state of the art results, we use the same supervised classifier in all cases; a Logistic Regression classifier with L2 penalization.", "labels": [], "entities": []}, {"text": "Before turning to the results, we describe the various datasets used in the experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Ratios of occurrences of lexicon items in  Stormfront vs. Reddit.", "labels": [], "entities": []}, {"text": " Table 2: Datasets used for evaluating the representations in supervised classification of abusive language.", "labels": [], "entities": [{"text": "supervised classification of abusive language", "start_pos": 62, "end_pos": 107, "type": "TASK", "confidence": 0.7661237001419068}]}, {"text": " Table 3: Results for the various representations on the datasets used in these experiments. The baseline is based on  random guessing in proportion to the class distributions, and the lexica use simple Boolean matching (i.e. presence  or absence of lexicon terms). All other results are produced by feeding the representations to a Logistic Regression  classifer with L2 penalization.", "labels": [], "entities": []}]}