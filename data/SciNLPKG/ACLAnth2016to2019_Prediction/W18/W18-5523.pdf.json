{"title": [{"text": "An End-to-End Multi-task Learning Model for Fact Checking", "labels": [], "entities": [{"text": "Fact Checking", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9471084177494049}]}], "abstractContent": [{"text": "With huge amount of information generated everyday on the web, fact checking is an important and challenging task which can help people identify the authenticity of most claims as well as providing evidences selected from knowledge source like Wikipedia.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.9175208806991577}]}, {"text": "Here we decompose this problem into two parts: an entity linking task (retrieving relative Wikipedia pages) and recognizing textual entailment between the claim and selected pages.", "labels": [], "entities": []}, {"text": "In this paper , we present an end-to-end multi-task learning with bi-direction attention (EMBA) model to classify the claim as \"supports\", \"refutes\" or \"not enough info\" with respect to the pages retrieved and detect sentences as evidence at the same time.", "labels": [], "entities": []}, {"text": "We conduct experiments on the FEVER (Fact Extraction and VERification) paper test dataset and shared task test dataset, anew public dataset for verification against tex-tual sources.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9974888563156128}]}, {"text": "Experimental results show that our method achieves comparable performance compared with the baseline system.", "labels": [], "entities": []}], "introductionContent": [{"text": "When we got news from newspapers and TVs which was thoroughly investigated and written by professional journalists, most of these messages are well-found and trustworthy.", "labels": [], "entities": []}, {"text": "However, with the popularity of the internet, there are 2.5 quintillion bytes of data created each day at our current pace . Everyone online is a producer as well as a recipient of these emerging information, and some of them are incorrect, fabricated or even with some evil purposes.", "labels": [], "entities": []}, {"text": "Most time it is difficult for us to figure out the truth of those emerging news without professional background and enough investigation.", "labels": [], "entities": []}, {"text": "Fact checking, which firstly has been produced and received a lot of attention in the indus-try of journalism, mainly verifying the speeches of public figures, is also important for other domains, e.g. wrong common-sense correction, rumor detection, content review etc.", "labels": [], "entities": [{"text": "Fact checking", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.75344318151474}, {"text": "wrong common-sense correction", "start_pos": 202, "end_pos": 231, "type": "TASK", "confidence": 0.5878621935844421}, {"text": "rumor detection", "start_pos": 233, "end_pos": 248, "type": "TASK", "confidence": 0.8671916425228119}]}, {"text": "With the increasing demand for automatic claim verification, several datasets for fact checking have been produced in recent years. are the first to release a public fake news detection and fact-checking dataset from two fact checking websites, the fact checking blog of Channel 4 2 and the True-O-Meter from PolitiFact . This dataset only includes 221 statements.", "labels": [], "entities": [{"text": "automatic claim verification", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.6807193954785665}, {"text": "fact checking", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.8419626355171204}]}, {"text": "Similarly, from PolitiFact via its API,  collected LIAR dataset with 12.8K manually labeled short statements, which permits machine learning based methods used on this dataset.", "labels": [], "entities": [{"text": "LIAR dataset", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.8245941400527954}]}, {"text": "Both dataset don't include the original justification and evidence as it was not machinereadable.", "labels": [], "entities": []}, {"text": "However, just verifying the claim based on the claim itself and without referring to any evidence sources is not reasonable and convincing.", "labels": [], "entities": []}, {"text": "In 2015, Silverman launched the Emergent Project 4 , a real-time rumor tracker, part of a research project with the Tow Center for Digital Journalism 5 at Columbia University.", "labels": [], "entities": []}, {"text": "firstly proposed to use the data from Emergent Project as Emergent dataset for rumor debunking, which contains 300 rumored claims and 2,595 associated news articles.", "labels": [], "entities": [{"text": "Emergent Project", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.8716491162776947}, {"text": "Emergent dataset", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.8675653040409088}, {"text": "rumor debunking", "start_pos": 79, "end_pos": 94, "type": "TASK", "confidence": 0.802250474691391}]}, {"text": "In 2017, the Fake news challenge () consisted of 50K labeled claimarticle pairs similarly derived from the Emergent Project.", "labels": [], "entities": [{"text": "Fake news challenge", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.6155453523000082}, {"text": "Emergent Project", "start_pos": 107, "end_pos": 123, "type": "DATASET", "confidence": 0.8137544393539429}]}, {"text": "These two dataset stemmed from Emergent Project alleviate the fact checking task by detecting the relationship between claim-article pairs.", "labels": [], "entities": [{"text": "Emergent Project", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.8304137289524078}, {"text": "fact checking task", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.8807149330774943}]}, {"text": "However, in more common situation, we are dealing with plenty of claims themselves online without associated articles which can help to verify the claims.", "labels": [], "entities": []}, {"text": "Fact Extraction and VERification (FEVER) dataset consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages and classified as SUPPORTED, REFUTED or NOTENOUGH-INFO.", "labels": [], "entities": [{"text": "Fact Extraction and VERification (FEVER) dataset", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.6741206757724285}, {"text": "REFUTED", "start_pos": 176, "end_pos": 183, "type": "METRIC", "confidence": 0.981582760810852}, {"text": "NOTENOUGH-INFO", "start_pos": 187, "end_pos": 201, "type": "DATASET", "confidence": 0.8222004771232605}]}, {"text": "For the first two classes, the dataset provides combination of sentences forming the necessary evidences supporting or refuting the claim.", "labels": [], "entities": []}, {"text": "Obviously, this dataset is more difficult than existing fact-checking datasets.", "labels": [], "entities": []}, {"text": "In order to achieve higher FEVER score, a fact-checking system is required to classify the claim correctly as well as retrieving sentences among more than 5 million Wikipedia pages jointed as correct evidence supporting the judgement.", "labels": [], "entities": [{"text": "FEVER score", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9580315351486206}]}, {"text": "The baseline method of this task comprises of three components: document retrieval, sentencelevel evidence selection and textual entailment.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7673712074756622}, {"text": "sentencelevel evidence selection", "start_pos": 84, "end_pos": 116, "type": "TASK", "confidence": 0.7108637889226278}, {"text": "textual entailment", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.7389912605285645}]}, {"text": "For the first two retrieval components, the baseline method uses document retrieval component of DrQA () which only relies on the unigram and bigram TF-IDF with vector similarity and don't understand semantics of the claim and pages.", "labels": [], "entities": [{"text": "DrQA", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.9420660138130188}]}, {"text": "So, we find that it extracts lots of Wikipedia pages which are unrelated to the entities described in claims.", "labels": [], "entities": []}, {"text": "Besides, similarity-based method prefer extracting supporting evidences than refuting evidences.", "labels": [], "entities": []}, {"text": "For the recognizing textual entailment (RTE) module, on one hand, the previous retrieval results limit the performance of the RTE model.", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE)", "start_pos": 8, "end_pos": 44, "type": "TASK", "confidence": 0.7968558569749197}]}, {"text": "On the other hand, the selected sentences concatenated as evidences may also confuse the RTE model due to some contradictory information.", "labels": [], "entities": [{"text": "RTE", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.6667875647544861}]}, {"text": "In this paper, we introduce an end-to-end multitask learning with bi-direction attention (EMBA) model for FEVER task.", "labels": [], "entities": [{"text": "bi-direction attention (EMBA)", "start_pos": 66, "end_pos": 95, "type": "METRIC", "confidence": 0.6984882891178131}, {"text": "FEVER", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.8551479578018188}]}, {"text": "We utilize the multi-task framework to jointly extract evidences and verify the claim because these two sub-tasks can be accomplished at the same time.", "labels": [], "entities": []}, {"text": "For example, after selecting relative pages, we carefully scan these pages to find supporting or refuting evidences.", "labels": [], "entities": []}, {"text": "If we find some, the claim can be labeled as SUP-PORTS or REFUTES immediately.", "labels": [], "entities": [{"text": "SUP-PORTS", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9566364288330078}, {"text": "REFUTES", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9849265813827515}]}, {"text": "If not, the claim will be classified as NOTENOUGHINFO after we read pages completely.", "labels": [], "entities": [{"text": "NOTENOUGHINFO", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.855712354183197}]}, {"text": "Our model is trained on claim-pages pairs by using attention mechanism in both directions, claim-to-pages and pages-to-claim, which provides complimentary information to each other.", "labels": [], "entities": []}, {"text": "We obtain claim-aware sentence representation to predict the correct evidence position and the pages-aware claim representation to detect the relationship between the claim and the pages.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our model on FEVER paper test dataset and shared task test dataset.", "labels": [], "entities": [{"text": "FEVER paper test dataset", "start_pos": 42, "end_pos": 66, "type": "DATASET", "confidence": 0.7194366157054901}]}, {"text": "We use the official evaluation script to compute the evidence F1 score, label accuracy and FEVER score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9096190929412842}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.8133684396743774}, {"text": "FEVER score", "start_pos": 91, "end_pos": 102, "type": "METRIC", "confidence": 0.9873001873493195}]}, {"text": "As shown in, our method achieves comparable performance on FEVER paper test dataset comparing with the baseline method on FEVER paper dev dataset.", "labels": [], "entities": [{"text": "FEVER paper test dataset", "start_pos": 59, "end_pos": 83, "type": "DATASET", "confidence": 0.8515234738588333}, {"text": "FEVER paper dev dataset", "start_pos": 122, "end_pos": 145, "type": "DATASET", "confidence": 0.9180853813886642}]}, {"text": "The result shows that jointly verifying a claim and retrieving evidences at same time can be as good as pipelined model.", "labels": [], "entities": []}, {"text": "Our method results on the FEVER paper shared task test dataset is showed in.", "labels": [], "entities": [{"text": "FEVER paper shared task test dataset", "start_pos": 26, "end_pos": 62, "type": "DATASET", "confidence": 0.8722340563933054}]}, {"text": "Besides, We calculate and present the confusion matrix of claim classification results on the FEVER paper test dataset in.", "labels": [], "entities": [{"text": "confusion", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9624075889587402}, {"text": "claim classification", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.6268787384033203}, {"text": "FEVER paper test dataset", "start_pos": 94, "end_pos": 118, "type": "DATASET", "confidence": 0.9128475785255432}]}, {"text": "Our model isn't good at identifying the unrelated relationship between claim and pages retrieved.", "labels": [], "entities": []}, {"text": "Our model sentence selection performance is recorded in.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.655802920460701}]}, {"text": "We can see that our model doesn't perform well for retrieving evidence.", "labels": [], "entities": []}, {"text": "Though with low evidence precision, our model average accuracy without requirement to provide correct evidence (51.97%) is similar to 52.09% accuracy of baseline method, which means that claim verification module and the sentence extraction module are relatively independent in our model.: sentences retrieval performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9878080487251282}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9816694855690002}, {"text": "sentence extraction", "start_pos": 221, "end_pos": 240, "type": "TASK", "confidence": 0.7303159087896347}, {"text": "sentences retrieval", "start_pos": 290, "end_pos": 309, "type": "TASK", "confidence": 0.7649799287319183}]}], "tableCaptions": [{"text": " Table 1: our EMBA model results on the FEVER  paper test dataset, Baseline method results on the  paper dev dataset.", "labels": [], "entities": [{"text": "FEVER  paper test dataset", "start_pos": 40, "end_pos": 65, "type": "DATASET", "confidence": 0.8407832533121109}, {"text": "paper dev dataset", "start_pos": 99, "end_pos": 116, "type": "DATASET", "confidence": 0.6314124266306559}]}, {"text": " Table 2: Model results on the FEVER shared task  test dataset.", "labels": [], "entities": [{"text": "FEVER shared task  test dataset", "start_pos": 31, "end_pos": 62, "type": "DATASET", "confidence": 0.6504331648349762}]}, {"text": " Table 3: confusion matrix for claim classification.  (NEI = \"not enough info\")", "labels": [], "entities": [{"text": "confusion", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9129548668861389}, {"text": "claim classification", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.9871885180473328}, {"text": "NEI", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9418524503707886}]}, {"text": " Table 4: sentences retrieval performance.", "labels": [], "entities": [{"text": "sentences retrieval", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8953925371170044}]}]}