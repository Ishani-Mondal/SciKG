{"title": [{"text": "State Gradients for RNN Memory Analysis", "labels": [], "entities": [{"text": "RNN Memory Analysis", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.9040606816609701}]}], "abstractContent": [{"text": "We present a framework for analyzing what the state in RNNs remembers from its input embeddings.", "labels": [], "entities": []}, {"text": "We compute the gradients of the states with respect to the input embeddings and decompose the gradient matrix with Singular Value Decomposition to analyze which directions in the embedding space are best transferred to the hidden state space, characterized by the largest singular values.", "labels": [], "entities": []}, {"text": "We apply our approach to LSTM language models and investigate to what extent and for how long certain classes of words are remembered on average fora certain corpus.", "labels": [], "entities": []}, {"text": "Additionally, the extent to which a specific property or relationship is remembered by the RNN can be tracked by comparing a vector characterizing that property with the direction(s) in embedding space that are best preserved in hidden state space.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recurrent neural networks (RNNs) are the current state of the art in many speech and language technology applications, but they are often called 'black-box' models since it is hard for humans to interpret what exactly the network has learned.", "labels": [], "entities": []}, {"text": "We present a framework to investigate what the states of RNNs remember from their input and for how long.", "labels": [], "entities": []}, {"text": "We apply our approach to the current state of the art in language modeling, long short-term memory) (LSTM)), but it can be applied to other types of RNNs too and to other models with continuous word representations as input.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}