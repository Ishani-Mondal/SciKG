{"title": [{"text": "Predictive Embeddings for Hate Speech Detection on Twitter", "labels": [], "entities": [{"text": "Hate Speech Detection", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.7822075088818868}]}], "abstractContent": [{"text": "We present a neural-network based approach to classifying online hate speech in general, as well as racist and sexist speech in particular.", "labels": [], "entities": [{"text": "classifying online hate speech", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.872673362493515}]}, {"text": "Using pre-trained word embeddings and max/mean pooling from simple, fully-connected transformations of these embed-dings, we are able to predict the occurrence of hate speech on three commonly used publicly available datasets.", "labels": [], "entities": []}, {"text": "Our models match or out-perform state of the art F1 performance on all three datasets using significantly fewer parameters and minimal feature preprocessing compared to previous methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "The increasing popularity of social media platforms like Twitter for both personal and political communication has seen a well-acknowledged rise in the presence of toxic and abusive speech on these platforms.", "labels": [], "entities": []}, {"text": "Although the terms of services on these platforms typically forbid hateful and harassing speech, enforcing these rules has proved challenging, as identifying hate speech speech at scale is still a largely unsolved problem in the NLP community., for example, identify many ambiguities in classifying abusive communications, and highlight the difficulty of clearly defining the parameters of such speech.", "labels": [], "entities": []}, {"text": "This problem is compounded by the fact that identifying abusive or harassing speech is a challenge for humans as well as automated systems.", "labels": [], "entities": [{"text": "identifying abusive or harassing speech", "start_pos": 44, "end_pos": 83, "type": "TASK", "confidence": 0.8368614673614502}]}, {"text": "Despite the lack of consensus around what constitutes abusive speech, some definition of hate speech must be used to build automated systems to address it.", "labels": [], "entities": []}, {"text": "We rely on's definition of hate speech, specifically: \"language that is used to express hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.\"", "labels": [], "entities": []}, {"text": "In this paper, we present a neural classification system that uses minimal preprocessing to take advantage of a modified Simple Word Embeddingsbased to predict the occurrence of hate speech.", "labels": [], "entities": []}, {"text": "Our classifier features: \u2022 A simple deep learning approach with few parameters enabling quick and robust training \u2022 Significantly better performance than two other state of the art methods on publicly available datasets \u2022 An interpretable approach facilitating analysis of results In the following sections, we discuss related work on hate speech classification, followed by a description of the datasets, methods and results of our study.", "labels": [], "entities": [{"text": "hate speech classification", "start_pos": 335, "end_pos": 361, "type": "TASK", "confidence": 0.7357013622919718}]}], "datasetContent": [{"text": "We tokenize the data using Spacy (. We use 300 Dimensional Glove Common Crawl Embeddings (840B Token)) and fine tune them for the task.", "labels": [], "entities": []}, {"text": "We experimented extensively with preprocessing variants and our results showed better performance without lemmatization and lowercasing (see supplement for details).", "labels": [], "entities": []}, {"text": "We pad each input to 50 words.", "labels": [], "entities": []}, {"text": "We train using RMSprop with a learning rate of .001 and a batch size of 512.", "labels": [], "entities": []}, {"text": "We add dropout with a drop rate of 0.1 in the final layer to reduce overfitting (), batch size, and input length empirically through random hyperparameter search.", "labels": [], "entities": []}, {"text": "All of our results are produced from 10-fold cross validation to allow comparison with previous results.", "labels": [], "entities": []}, {"text": "We trained a logistic regression baseline model (line 1 in) using character ngrams and word unigrams using TF*IDF weighting, to provide a baseline since HAR has no reported results.", "labels": [], "entities": [{"text": "TF*IDF weighting", "start_pos": 107, "end_pos": 123, "type": "METRIC", "confidence": 0.6941694021224976}]}, {"text": "For the SR and HATE datasets, the authors reported their trained best logistic regression model's 2 results on their respective datasets.", "labels": [], "entities": [{"text": "SR", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.651220977306366}, {"text": "HATE datasets", "start_pos": 15, "end_pos": 28, "type": "DATASET", "confidence": 0.9304287135601044}]}], "tableCaptions": [{"text": " Table 2: F1 Results 3", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9890581965446472}]}, {"text": " Table 4: Average Validation Loss for each Preprocess- ing Scheme", "labels": [], "entities": [{"text": "Average Validation Loss", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8941903909047445}]}]}