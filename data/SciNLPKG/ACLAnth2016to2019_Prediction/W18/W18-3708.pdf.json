{"title": [{"text": "A Hybrid System for Chinese Grammatical Error Diagnosis and Correction", "labels": [], "entities": [{"text": "Chinese Grammatical Error Diagnosis", "start_pos": 20, "end_pos": 55, "type": "TASK", "confidence": 0.8466092646121979}]}], "abstractContent": [{"text": "This paper introduces the DM NLP team's system for NLPTEA 2018 shared task of Chinese Grammatical Error Diagnosis (CGED), which can be used to detect and correct grammatical errors in texts written by Chinese as a Foreign Language (CFL) learners.", "labels": [], "entities": [{"text": "DM NLP team", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.925650397936503}, {"text": "Chinese Grammatical Error Diagnosis (CGED)", "start_pos": 78, "end_pos": 120, "type": "TASK", "confidence": 0.6615645374570575}, {"text": "correct grammatical errors in texts written by Chinese as a Foreign Language (CFL) learners", "start_pos": 154, "end_pos": 245, "type": "TASK", "confidence": 0.6815815847367048}]}, {"text": "This task aims at not only detecting four types of grammatical errors including redundant words (R), missing words (M), bad word selection (S) and disordered words (W), but also recommending corrections for errors of M and S types.", "labels": [], "entities": []}, {"text": "We proposed a hybrid system including four models for this task with two stages: the detection stage and the correction stage.", "labels": [], "entities": [{"text": "correction", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.8830893635749817}]}, {"text": "In the detection stage, we first used a BiLSTM-CRF model to tag potential errors by sequence labeling, along with some handcraft features.", "labels": [], "entities": []}, {"text": "Then we designed three Grammatical Error Correction (GEC) models to generate corrections , which could help to tune the detection result.", "labels": [], "entities": []}, {"text": "In the correction stage, candidates were generated by the three GEC models and then merged to output the final corrections for M and S types.", "labels": [], "entities": [{"text": "GEC", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.9275769591331482}]}, {"text": "Our system reached the highest precision in the correction subtask, which was the most challenging part of this shared task, and got top 3 on F1 scores for position detection of errors .", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9994693398475647}, {"text": "F1 scores", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9740463197231293}, {"text": "position detection of errors", "start_pos": 156, "end_pos": 184, "type": "TASK", "confidence": 0.7679750174283981}]}], "introductionContent": [{"text": "More and more people are learning a second or third language as an interest, a career plus, or even a challenge to oneself.", "labels": [], "entities": []}, {"text": "Chinese is one of the oldest and most versatile languages in the world.", "labels": [], "entities": []}, {"text": "Many * Equal Contribution \u2020 This work was done while the author at Alibaba Group people choose to learn Chinese, and the number of CFL leaner grows rapidly.", "labels": [], "entities": [{"text": "Equal Contribution", "start_pos": 7, "end_pos": 25, "type": "METRIC", "confidence": 0.944695770740509}, {"text": "Alibaba Group", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.9818371832370758}]}, {"text": "However, it would be difficult to learn Chinese, because Chinese has a lot of differences from other languages.", "labels": [], "entities": []}, {"text": "For example, Chinese has neither the change of singular and plural, nor the tense change of the verb.", "labels": [], "entities": []}, {"text": "It has quite flexible expressions and loose structural grammar.", "labels": [], "entities": []}, {"text": "These traits bring a lot of trouble to CFL learners, so the demands for Chinese Grammatical Error Diagnosis (CGED) as well as Correction (CGEC) is growing rapidly.", "labels": [], "entities": [{"text": "Chinese Grammatical Error Diagnosis (CGED)", "start_pos": 72, "end_pos": 114, "type": "TASK", "confidence": 0.6168393109525953}]}, {"text": "GEC for English has been studied for many years, with many shared tasks such as and), while those kinds of studies on Chinese is less yet.", "labels": [], "entities": [{"text": "GEC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6094639301300049}]}, {"text": "This CGED shared task ( gives researchers an opportunity to build the system and exchange opinions in this field.", "labels": [], "entities": [{"text": "CGED", "start_pos": 5, "end_pos": 9, "type": "DATASET", "confidence": 0.8909690380096436}]}, {"text": "It could make the community more flourish which benefits all CFL learners.", "labels": [], "entities": []}, {"text": "Compared with previous years, this year's NLPTEA CGED shared task requests participants to generate candidate corrections for errors of M and S types.", "labels": [], "entities": [{"text": "NLPTEA CGED shared task", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.718456044793129}]}, {"text": "This correction subtask is more challenging and valuable, so we focused on this subtask and got the highest precision in this subtask.", "labels": [], "entities": [{"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9988347887992859}]}, {"text": "This paper is organized as follows: Section 2 describes some related works in English as well as Chinese.", "labels": [], "entities": []}, {"text": "Dataset will be described in Section 3.", "labels": [], "entities": [{"text": "Dataset", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8581636548042297}]}, {"text": "Section 4 illustrates our hybrid system with two stages, including four models.", "labels": [], "entities": []}, {"text": "Section 5 shows the evaluation and discussion of the hybrid model.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper and discusses the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset is provided by the 5th Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA) 2018 with a Shared Task for CGED.", "labels": [], "entities": [{"text": "CGED", "start_pos": 152, "end_pos": 156, "type": "DATASET", "confidence": 0.9075637459754944}]}, {"text": "The NLPTEA CGED has been held since 2014, and it provides several sets of training data for this field.", "labels": [], "entities": [{"text": "NLPTEA CGED", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9361407458782196}]}, {"text": "Each instance in the CGED training dataset is composed of an original sentence with a unique sentence number 'sid', some 'target edits', and a correction sentence.", "labels": [], "entities": [{"text": "CGED training dataset", "start_pos": 21, "end_pos": 42, "type": "DATASET", "confidence": 0.9318695465723673}]}, {"text": "The original sentence contains grammatical errors in Chinese sentences written by CFL learners.", "labels": [], "entities": []}, {"text": "All errors are divided into four types, including redundant words (denoted as R), missing words (M), word selection errors (S), and word ordering errors (W).", "labels": [], "entities": []}, {"text": "Some typical examples are shown in.", "labels": [], "entities": []}, {"text": "Each edit in the 'target edits' indicates the error type and the position at which it occurs in the original sentence.", "labels": [], "entities": []}, {"text": "If an input sentence contains one or more grammatical errors, the 'target edits' will include many items, each of which is in the form of, where start-off and end-off respectively denote the starting and ending position of the grammatical error, and the error-type is in the set of R, M, S, and W.", "labels": [], "entities": []}, {"text": "For each original sentence given in the test dataset, the developed system should predict the 'target edits' in the format which is same as the training set, and for the error type of Sand M, the system should predict the candidate corrections.", "labels": [], "entities": []}, {"text": "We also used an external dataset Lang-8 1 to train our GEC models, which contains more than 700,000 items, and each item consists of an original sentence and corresponding corrected sentences.", "labels": [], "entities": [{"text": "GEC", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.9470654129981995}]}, {"text": "Each original sentence has k correction   To train the BiLSTM-CRF model, we collected several datasets of CGED, which are.", "labels": [], "entities": [{"text": "BiLSTM-CRF", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.7426607012748718}, {"text": "CGED", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.9265912771224976}]}, {"text": "We split 20% of the 2017 training data as the validation dataset, which is denoted as '17-dev', and all the rest as training.", "labels": [], "entities": []}, {"text": "We used the character embeddings and word embeddings pre-trained on the Gigawords and fixed them.", "labels": [], "entities": [{"text": "Gigawords", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.9633087515830994}]}, {"text": "For other parameters, we initialized them randomly.", "labels": [], "entities": []}, {"text": "To train our GEC models, we used the external Lang-8 dataset as explained in Section 3.", "labels": [], "entities": [{"text": "GEC", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.9031845331192017}, {"text": "Lang-8 dataset", "start_pos": 46, "end_pos": 60, "type": "DATASET", "confidence": 0.9799671471118927}]}, {"text": "Because each original sentence could have more than one corrected sentences, we used two approaches to generate parallel data pairs to train our GEC models.", "labels": [], "entities": [{"text": "GEC", "start_pos": 145, "end_pos": 148, "type": "DATASET", "confidence": 0.7747941017150879}]}, {"text": "The first choice is to use only the correct sentence whose edit distance is smallest from the original sentence.", "labels": [], "entities": []}, {"text": "The training data generated by the first choice is denoted as data ed . The second choice is to use all the correct sentences of the corresponding original sentence.", "labels": [], "entities": []}, {"text": "The training data generated by the first choice is denoted as data all . For the NMT model, we used the pre-trained embedding in different parts of the model.", "labels": [], "entities": []}, {"text": "The first choice was to use it for the whole model, which forced the model to learn a proper embedding by itself.", "labels": [], "entities": []}, {"text": "Considering the dataset is not large enough for the model to learn the embedding from scratch, we also tested the pre-trained embedding phrase CGED+NLPCC data all used for both encoder and decoder parts.", "labels": [], "entities": [{"text": "CGED+NLPCC data", "start_pos": 143, "end_pos": 158, "type": "DATASET", "confidence": 0.8395366072654724}]}, {"text": "But the embedding was trained on the Gigaword (, which was quite different from the sentences written by CFL learners, so we also used the pre-trained embedding only in the decoder part.", "labels": [], "entities": []}, {"text": "The configurations of our four different NMT GEC models N j , j \u2208 {1, 2, 3, 4} are shown in.", "labels": [], "entities": []}, {"text": "For the 'Network' column, the 'BiL-STM' means bi-directional LSTM, and for the 'Embed' column, the 'enc-dec' means using pre-trained embedding for both encoder and decoder part in our model.", "labels": [], "entities": [{"text": "BiL-STM", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9604341983795166}]}, {"text": "For the SMT model, we trained the language model part on different corpora, including the Gigaword, the Chinese Wikipedia corpus), and the corpus consists of CGED as well as Lang-8 correct sentences which are constructed by ourselves.", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.993141233921051}, {"text": "Chinese Wikipedia corpus", "start_pos": 104, "end_pos": 128, "type": "DATASET", "confidence": 0.7600702941417694}]}, {"text": "Besides, we also tested different granularities of the model, which means, used char-level or phrase-level translation model.", "labels": [], "entities": [{"text": "phrase-level translation", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.6435089856386185}]}, {"text": "It is worth to mention that we found that using data all outperformed data ed significantly, so we only did detailed experiments on data all because of the time limitation of the contest.", "labels": [], "entities": []}, {"text": "The configurations of our six different SMT models S j , j \u2208 {1, 2, 3, 4, 5, 6} are shown in Many excellent tools can emancipate us from the heavy burden of implementing models from scratch.", "labels": [], "entities": [{"text": "SMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9855732321739197}]}, {"text": "For those NMT GEC models, we implemented it with the OpenNMT () toolkit, and for those SMT GEC models, we implemented the language model with) toolkit and translation model with Moses (.", "labels": [], "entities": [{"text": "SMT GEC", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.8314106166362762}]}, {"text": "For the Lang-8 dataset, we found that in those 717,241 lines data, 474,638 lines contained traditional Chinese.", "labels": [], "entities": [{"text": "Lang-8 dataset", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.8960490822792053}]}, {"text": "The traditional Chinese cannot convey more information than its corresponding simplified Chinese, but will make the size of vocabulary much larger.", "labels": [], "entities": []}, {"text": "So, we used the opencc: Experiments of Grammatical Error Detection on 17-dev dataset by merging eleven models.", "labels": [], "entities": [{"text": "Grammatical Error Detection", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.7007739841938019}]}, {"text": "The corresponding configuration of the models in 'NMT-type' and 'SMT-type' can be found in.", "labels": [], "entities": []}, {"text": "The values for 'Detection', 'Identification', and 'Position' columns are all F 1 values.", "labels": [], "entities": [{"text": "F 1", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9447270333766937}]}, {"text": "NMT-type SMT-type FP-rate Detection Identification Position: Experiments of Grammatical Error Detection on 17-dev dataset by voting eleven models.", "labels": [], "entities": [{"text": "NMT-type SMT-type FP-rate Detection Identification Position", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.7242188602685928}, {"text": "Grammatical Error Detection", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.6359732349713644}]}, {"text": "The corresponding configuration of the models in 'NMT-type' and 'SMT-type' can be found in and toolkit to convert all the traditional Chinese to simplified Chinese.", "labels": [], "entities": []}, {"text": "The evaluation metrics for NLPTEA CGED shared task consists of four subtasks: 'Detection' (determine if the sentence contains errors), 'Identification' (determine the error types), 'Position' (determine the position of errors), and 'Correction' (determine the candidate corrected words for M and S error types).", "labels": [], "entities": [{"text": "NLPTEA CGED shared task", "start_pos": 27, "end_pos": 50, "type": "DATASET", "confidence": 0.7766688168048859}]}, {"text": "Those four subtasks are from easy to hard, and the last metric is the most valuable, which will be paid more attention by us.", "labels": [], "entities": []}, {"text": "The former three metrics are related to the detection stage, and the last metric is related to the correction stage.", "labels": [], "entities": [{"text": "correction", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9647029042243958}]}], "tableCaptions": [{"text": " Table 4: Experiments of Grammatical Error Detection on 17-dev dataset by merging eleven models. The  corresponding configuration of the models in 'NMT-type' and 'SMT-type' can be found in", "labels": [], "entities": [{"text": "Grammatical Error Detection", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.8687029282251993}]}, {"text": " Table 2 and  Table 3. The values for 'Detection', 'Identification', and 'Position' columns are all F 1 values.", "labels": [], "entities": [{"text": "Identification", "start_pos": 52, "end_pos": 66, "type": "METRIC", "confidence": 0.8780669569969177}, {"text": "F 1", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9166510105133057}]}, {"text": " Table 3. The values for 'Detection', 'Identification', and 'Position' columns are all F 1 values.  Threshold NMT-type SMT-type FP-rate Detection Identification Position", "labels": [], "entities": [{"text": "SMT-type FP-rate Detection Identification Position", "start_pos": 119, "end_pos": 169, "type": "TASK", "confidence": 0.6536498844623566}]}, {"text": " Table 8: Ablation Tests of Correction Subtask  Method  Precision Recall  F 1", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9924260377883911}, {"text": "Precision Recall", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.7911399304866791}]}]}