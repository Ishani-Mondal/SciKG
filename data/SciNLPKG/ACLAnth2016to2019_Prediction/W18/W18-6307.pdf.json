{"title": [{"text": "A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in Neural Machine Translation", "labels": [], "entities": [{"text": "Context-Aware Pronoun Translation", "start_pos": 45, "end_pos": 78, "type": "TASK", "confidence": 0.6913250088691711}, {"text": "Neural Machine Translation", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.6275383730729421}]}], "abstractContent": [{"text": "The translation of pronouns presents a special challenge to machine translation to this day, since it often requires context outside the current sentence.", "labels": [], "entities": [{"text": "translation of pronouns", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8795919219652811}, {"text": "machine translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7526515126228333}]}, {"text": "Recent work on models that have access to information across sentence boundaries has seen only moderate improvements in terms of automatic evaluation metrics such as BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9940453767776489}]}, {"text": "However, metrics that quantify the overall translation quality are illequipped to measure gains from additional context.", "labels": [], "entities": []}, {"text": "We argue that a different kind of evaluation is needed to assess how well models translate inter-sentential phenomena such as pronouns.", "labels": [], "entities": []}, {"text": "This paper therefore presents a test suite of contrastive translations focused specifically on the translation of pronouns.", "labels": [], "entities": []}, {"text": "Furthermore, we perform experiments with several contextaware models.", "labels": [], "entities": []}, {"text": "We show that, while gains in BLEU are moderate for those systems, they outperform baselines by a large margin in terms of accuracy on our contrastive test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9985185265541077}, {"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9992725253105164}]}, {"text": "Our experiments also show the effectiveness of parameter tying for multiencoder architectures.", "labels": [], "entities": [{"text": "parameter tying", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7203754484653473}]}], "introductionContent": [{"text": "Even though machine translation has improved considerably with the advent of neural machine translation (NMT) (, the translation of pronouns remains a major issue.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7896757125854492}, {"text": "neural machine translation", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.7839435736338297}]}, {"text": "They are notoriously hard to translate since they often require context outside the current sentence.", "labels": [], "entities": []}, {"text": "As an example, consider the sentences in Figure 1.", "labels": [], "entities": []}, {"text": "In both languages, there is a pronoun in the * Work performed prior to joining Amazon.", "labels": [], "entities": []}, {"text": "EN However, the European Central Bank took an interest in it.", "labels": [], "entities": [{"text": "EN", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.9770122766494751}, {"text": "European Central Bank", "start_pos": 16, "end_pos": 37, "type": "DATASET", "confidence": 0.9723276893297831}]}, {"text": "It describes bitcoin as \"the most successful virtual currency\".", "labels": [], "entities": []}, {"text": "DE Dennoch hat die Europ\u00e4ische Zentralbank (EZB) Interesse hierf\u00fcr gezeigt.", "labels": [], "entities": [{"text": "Europ\u00e4ische Zentralbank (EZB) Interesse", "start_pos": 19, "end_pos": 58, "type": "DATASET", "confidence": 0.8010205725828806}]}, {"text": "Sie beschreibt Bitcoin als \"die virtuelle W\u00e4hrung mit dem gr\u00f6ssten Erfolg\".", "labels": [], "entities": []}, {"text": "second sentence that refers to the European Central Bank.", "labels": [], "entities": [{"text": "European Central Bank", "start_pos": 35, "end_pos": 56, "type": "DATASET", "confidence": 0.9682808121045431}]}, {"text": "When the second sentence is translated from English to German, the translation of the pronoun it is ambiguous.", "labels": [], "entities": []}, {"text": "This ambiguity can only be resolved with context awareness: if a translation system has access to the previous English sentence, the previous German translation, or both, it can determine the antecedent the pronoun refers to.", "labels": [], "entities": []}, {"text": "In this German sentence, the antecedent Europ\u00e4ische Zentralbank dictates the feminine gender of the pronoun sie.", "labels": [], "entities": []}, {"text": "It is unfortunate, then, that current NMT systems generally operate on the sentence level (.", "labels": [], "entities": []}, {"text": "Documents are translated sentenceby-sentence for practical reasons, such as linebased processing in a pipeline and reduced computational complexity.", "labels": [], "entities": []}, {"text": "Furthermore, improvements of larger-context models over baselines in terms of document-level metrics such as BLEU or RIBES have been moderate, so that their computational overhead does not seem justified, and so that it is hard to develop more effective context-aware architectures and empirically validate them.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9861225485801697}]}, {"text": "To address this issue, we present an alternative way of evaluating larger-context models on a test set that allows to specifically measure a model's capability to correctly translate pronouns.", "labels": [], "entities": []}, {"text": "The test suite consists of pairs of source and target sentences, in combination with contrastive translation variants (for evaluation by model scoring) and additional linguistic and contextual information (for further analysis).", "labels": [], "entities": []}, {"text": "The resource is freely available.", "labels": [], "entities": []}, {"text": "Additionally, we evaluate several context-aware models that have recently been proposed in the literature on this test set, and extend existing models with parameter tying.", "labels": [], "entities": []}, {"text": "The main contributions of our paper are: \u2022 We present a large-scale test set to evaluate the accuracy with which NMT models translate the English pronoun it to its German counterparts es, sie and er.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9993858337402344}]}, {"text": "\u2022 We evaluate several context-aware systems and show how targeted, contrastive evaluation is an effective tool to measure improvement in pronoun translation.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.7261952459812164}]}, {"text": "\u2022 We empirically demonstrate the effectiveness of parameter tying in multi-encoder contextaware models.", "labels": [], "entities": [{"text": "parameter tying", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7231855690479279}]}, {"text": "Section 2 explains how our paper relates to existing work on context-aware models and the evaluation of pronoun translation.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7052616775035858}]}, {"text": "Section 3 describes our test suite.", "labels": [], "entities": []}, {"text": "The context-aware models we use in our experiments are detailed in Section 4.", "labels": [], "entities": []}, {"text": "We discuss our experiments in Section 5 and the results in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Pronouns can serve a variety of functions with complex cross-lingual variation, and hand-picked, manually annotated test suites have been presented for the evaluation of pronoun translation (.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 170, "end_pos": 189, "type": "TASK", "confidence": 0.7745761275291443}]}, {"text": "While suitable for analysis, the small size of the test suites makes it hard to make statistically confident comparisons between systems, and the hand-picked nature of the test suites introduces biases.", "labels": [], "entities": []}, {"text": "To overcome these problems, we opted fora fully automatic approach to constructing a large-scale test suite.", "labels": [], "entities": []}, {"text": "Conceptually, our test set is most similar to the \"cross-lingual pronoun prediction\" task held at DiscoMT and WMT in recent years (: participants are asked to fill a gap in a target sentence, where gaps correspond to pronouns.", "labels": [], "entities": [{"text": "cross-lingual pronoun prediction", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.6702723900477091}, {"text": "DiscoMT", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9570398330688477}, {"text": "WMT", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.7822904586791992}]}, {"text": "The first edition of the task focused on English\u2192French, and it was found that local context (such as the verb group) was a strong signal for pronoun prediction.", "labels": [], "entities": [{"text": "pronoun prediction", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.7717959880828857}]}, {"text": "Hence, future editions only provided target-side lemmas instead of fully inflected forms, which makes the task less suitable to evaluate end-to-end neural machine translation systems, although such systems have been trained on the task ( . do not report on the proportion of intra-sentential and inter-sentential anaphora in their test set, but the two top-performing systems only made use of intrasentential information.", "labels": [], "entities": []}, {"text": "Our test suite focuses on allowing the comparison of end-to-end contextaware NMT systems, and we thus extract a large number of inter-sentential anaphora, with meta-data allowing fora focus on inter-sentential anaphora with along distance between the pronoun and its antecedent.", "labels": [], "entities": []}, {"text": "Our focus on evaluating end-to-end NMT systems also relieves us from having to provide annotated training sets, and reduces pressure to achieve balance and full coverage of phenomena.", "labels": [], "entities": [{"text": "balance", "start_pos": 144, "end_pos": 151, "type": "METRIC", "confidence": 0.9522761106491089}]}, {"text": "An alternative approach to automatically evaluate pronoun translation are reference-based methods that produce a score based on word alignment between source, translation output, and reference translation, and identification of pronouns in them, such as AutoPRF ( and APT).", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7244674116373062}]}, {"text": "Guillou and Hardmeier (2018) perform a human meta-evaluation and show substantial disagreement between reference-based metrics and human judges, especially because there often exist valid alternative translations that use different pronouns than the reference.", "labels": [], "entities": []}, {"text": "Our test set, and our protocol of generating contrastive examples, is focused on selected pronouns to minimize the risk of producing contrastive examples that are actually valid translations.", "labels": [], "entities": []}, {"text": "Contrastive evaluation is different from conventional evaluation of machine translation in that it does not require any translation.", "labels": [], "entities": [{"text": "Contrastive evaluation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8674289584159851}, {"text": "machine translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7044610679149628}]}, {"text": "Rather than testing a model's ability to translate, it is a method to test a model's ability to discriminate between given good and bad translations.", "labels": [], "entities": []}, {"text": "8 There are some cases where the antecedent is listed as it in the test set.", "labels": [], "entities": []}, {"text": "This is our fallback behaviour if the coreference chain does not contain any noun.", "labels": [], "entities": []}, {"text": "In that case, we do not know the true antecedent.", "labels": [], "entities": []}, {"text": "We exploit the fact that NMT systems are in fact language models of the target language, conditioned on source text.", "labels": [], "entities": []}, {"text": "Like language models, NMT systems can be used to compute a model score (the negative log probability) for an existing translation.", "labels": [], "entities": []}, {"text": "Contrastive evaluation, then, means to compare the model score of two pairs of inputs: (actual source, reference translation) and (actual source, contrastive translation).", "labels": [], "entities": [{"text": "Contrastive evaluation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8139866590499878}]}, {"text": "If the model score of the actual reference translation is higher, we assume that this model can detect wrong pronoun translations.", "labels": [], "entities": []}, {"text": "However, this does not mean that systems actually produce the reference translation when given the source sentence for translation.", "labels": [], "entities": []}, {"text": "An entirely different target sequence might rank higher in the system's beam during decoding.", "labels": [], "entities": []}, {"text": "The only conclusion permitted by contrastive evaluation is whether or not the reference translation is more probable than a contrastive variant.", "labels": [], "entities": []}, {"text": "If the model score of the reference is indeed higher, we refer to this outcome as a \"correct decision\" by the model.", "labels": [], "entities": []}, {"text": "The model's decision is only correct if the reference translation has a higher score than any contrastive translation.", "labels": [], "entities": []}, {"text": "In our evaluation, we aggregate model decisions on the whole test set and report the overall percentage of correct decisions as accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9984845519065857}]}, {"text": "During scoring, the model is provided with reference translations as target context, while during translation, the model needs to predict the full sequence.", "labels": [], "entities": [{"text": "translation", "start_pos": 98, "end_pos": 109, "type": "TASK", "confidence": 0.9671607613563538}]}, {"text": "It is an open question to what extent performance deteriorates when context is itself predicted, and thus noisy.", "labels": [], "entities": []}, {"text": "We highlight that the same problem arises for sentence-level NMT, and has been addressed with alternative training strategies ().", "labels": [], "entities": []}, {"text": "We train all models on the data from the WMT 2017 English\u2192German news translation shared task (\u223c 5.8 million sentence pairs).", "labels": [], "entities": [{"text": "WMT 2017 English\u2192German news translation shared task", "start_pos": 41, "end_pos": 93, "type": "DATASET", "confidence": 0.8758803208669027}]}, {"text": "These corpora do not have document boundaries, therefore a small fraction of sentences will be paired with wrong context, but we expect the model to be robust against occasional random context (see also).", "labels": [], "entities": []}, {"text": "Experimental setups for the RNN and Transformer models are different, and we describe them separately.", "labels": [], "entities": [{"text": "RNN", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.8250306248664856}]}, {"text": "All RNN-based models are trained with Nematus ().", "labels": [], "entities": []}, {"text": "We learn a joint BPE model with 89.5k merge operations.", "labels": [], "entities": []}, {"text": "We train shallow models with an embedding size of 512, a hidden layer size of 1024 and layer normalization.", "labels": [], "entities": []}, {"text": "Models are trained with Adam (, with an initial learning rate of 0.0001.", "labels": [], "entities": []}, {"text": "We apply early stopping based on validation perplexity.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.7627178728580475}]}, {"text": "The batch size for training is 80, and the maximum length of training sequences is 100 (if input sentences are concatenated) or 50 (if input lines are single sentences).", "labels": [], "entities": []}, {"text": "For our Transformer-based experiments, we use a custom implementation and follow the hyperparameters from;.", "labels": [], "entities": []}, {"text": "Systems are trained on lowercased text that was encoded using BPE (32k merge operations).", "labels": [], "entities": [{"text": "BPE", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.8377628326416016}]}, {"text": "Models consist of 6 encoder and decoder layers with 8 attention heads.", "labels": [], "entities": []}, {"text": "The hidden state size is 512, the size of feedforward layers is 2048.", "labels": [], "entities": []}, {"text": "Model performance is evaluated in terms of BLEU, on newstest2017, newstest2018 and all sentence pairs from our pronoun test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9990370273590088}, {"text": "newstest2018", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.9193852543830872}]}, {"text": "We compute scores with SacreBLEU.", "labels": [], "entities": []}, {"text": "Evaluation with BLEU is done mainly to control for overall translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9962496161460876}, {"text": "translation", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.9516226649284363}]}, {"text": "To evaluate pronoun translation, we perform contrastive evaluation and report the accuracy of models on our contrastive test set.", "labels": [], "entities": [{"text": "pronoun translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7323916107416153}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9988309741020203}]}, {"text": "The BLEU scores in show a moderate improvement for most context-aware systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9978572726249695}]}, {"text": "This suggests that the architectural changes for the context-aware models do not degrade overall translation quality.", "labels": [], "entities": []}, {"text": "The contrastive evaluation on our test set on the other hand shows a clear increase in the accuracy of pronoun translation: The best model s-hier-to-2.tied achieves a total of +16 percentage points accuracy on the test set over the baseline, see.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9990422129631042}, {"text": "pronoun translation", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.716203585267067}, {"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9850872755050659}]}, {"text": "shows that context-aware models perform better than the baseline when the antecedent is outside the current sentence.", "labels": [], "entities": []}, {"text": "In our experiments, all context-aware models consider one preceding sentence as context.", "labels": [], "entities": []}, {"text": "The evaluation according to the distance of the antecedent in confirms that the subset of sentences 9 Our (cased) SacreBLEU signature is BLEU+c.mixed+ l.en-de+#.1+s.exp+t.wmt{17,18}+tok.13a+ v. with antecedent distance 1 benefits most from the tested context-aware models (up to +20 percentage points accuracy).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9984196424484253}, {"text": "accuracy", "start_pos": 301, "end_pos": 309, "type": "METRIC", "confidence": 0.9637758731842041}]}, {"text": "However, we note two surprising patterns: \u2022 For inter-sentential anaphora, the performance of all systems, including the baseline, improves with increasing antecedent distance.", "labels": [], "entities": []}, {"text": "\u2022 Context-aware systems that consider one preceding sentence also improve on intrasentential anaphora, and on pronouns whose antecedent is outside the context window.", "labels": [], "entities": []}, {"text": "The first observation can be explained by the distribution of German pronouns in the test set.", "labels": [], "entities": []}, {"text": "The further away the antecedent, the higher the percentage of it\u2192es cases, which are the majority class, and thus the class that will be predicted most often if evidence for other classes is lacking.", "labels": [], "entities": []}, {"text": "We speculate that this is due to our more permissive extraction heuristics for it\u2192es.", "labels": [], "entities": []}, {"text": "We attribute the second observation to the existence of coreference chains where the preceding sentence contains a pronoun that refers to the same nominal antecedent as the pronoun in the current sentence.", "labels": [], "entities": []}, {"text": "Consider the example in: The nominal antecedent of it in the current sentence is door, T\u00fcr in German with feminine gender.", "labels": [], "entities": []}, {"text": "The nominal antecedent occurs two sentences before the current sentence, but the German sentence in between contains the pronoun sie, which is a useful signal for the context-aware models, even though they cannot know the nominal antecedent.", "labels": [], "entities": []}, {"text": "Note that only models aware of target-side context can benefit from such circumstances: The shier models as well as the Transformer model by only see source side context, which results in lower accuracy if the distance to the antecedent is >1, see.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 194, "end_pos": 202, "type": "METRIC", "confidence": 0.9981793165206909}]}, {"text": "While such coreference chains complicate the interpretation of the results, we note that improvements on inter-sentential anaphora with antecedent distance > 1 are relatively small (compared to distance 1), and that performance is still relatively poor (especially for the minority classes er and sie).", "labels": [], "entities": []}, {"text": "We encourage evaluation of widercontext models on this subset, which is still large thanks to the size of the full test set.", "labels": [], "entities": []}, {"text": "Regarding       -Ist es abgeschlossen?: Example where 1) antecedent distance is >1 and 2) the context given contains another pronoun as an additional hint.", "labels": [], "entities": []}, {"text": "effectiveness of parameter sharing between the main encoder (or decoder) and the contextual encoder.", "labels": [], "entities": []}, {"text": "We observe an improvement of 5 percentage points from s-hier-to-2 to s-hier-to-2.tied, and 4 percentage points from s-t-hier to s-t-hier.tied.", "labels": [], "entities": []}, {"text": "Context encoders introduce a large number of extra parameters, while inter-sentential context is only relevant fora relatively small number of predictions.", "labels": [], "entities": []}, {"text": "We hypothesize that the training signal is thus too weak to train a strong contextual encoder in an end-to-end fashion without parameter sharing.", "labels": [], "entities": []}, {"text": "Our results also confirm the finding by that multi-encoder architectures, specifically s-hier-to-2(.tied), can outperform a simple concatenation system in the translation of coreferential pronouns.", "labels": [], "entities": []}, {"text": "The Transformer-based models perform strongest on pronouns with intra-segmental antecedent, outperforming the recurrent baseline by 9-18 percentage points.", "labels": [], "entities": []}, {"text": "This is likely an effect of increased model depth and the self-attentional architecture in this set of experiments.", "labels": [], "entities": []}, {"text": "The model by) only uses source context, and outperforms the most comparable RNN system, s-hier.tied.", "labels": [], "entities": []}, {"text": "However, the Transformerbased concat22 slightly underperforms the RNN-based concat22, and we consider it future research how to better exploit target context with Transformer-based models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Frequency and probability of alignments  of it in the training data of our systems (all  data from the WMT 2017 news translation task).  Alignments are produced by a fast_align model.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9893314838409424}, {"text": "WMT 2017 news translation task", "start_pos": 113, "end_pos": 143, "type": "TASK", "confidence": 0.8420406699180603}]}, {"text": " Table 3: Example sentence pair with contrastive translations. An antecedent distance of 1 means that the  antecedent is in the immediately preceding sentence.", "labels": [], "entities": []}, {"text": " Table 4: Test set frequencies of pronoun pairs and  antecedent distance (measured in sentences).", "labels": [], "entities": []}, {"text": " Table 5: English\u2192German BLEU scores on newstest2017, newstest2018 and all sentence pairs from our  pronoun test set. Case-sensitive and case-insensitive (uncased) scores are reported. Higher is better, and  the best scores are marked in bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9940348267555237}]}, {"text": " Table 6: Accuracy on contrastive test set (N=4000  per pronoun) with regard to reference pronoun.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990010857582092}]}, {"text": " Table 7: Accuracy on contrastive test set with re- gard to antecedent location (within segment vs.  outside segment).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9959473013877869}, {"text": "re- gard", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9067453543345133}]}, {"text": " Table 8: Accuracy on contrastive test set with regard to antecedent distance of antecedent (in sentences).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9891352653503418}]}]}