{"title": [{"text": "CUNI Transformer Neural MT System for WMT18", "labels": [], "entities": [{"text": "CUNI Transformer Neural MT", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5506855472922325}, {"text": "WMT18", "start_pos": 38, "end_pos": 43, "type": "TASK", "confidence": 0.6839402914047241}]}], "abstractContent": [{"text": "We describe our NMT system submitted to the WMT2018 shared task in news translation.", "labels": [], "entities": [{"text": "WMT2018 shared task", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.6239625414212545}, {"text": "news translation", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.6612625867128372}]}, {"text": "Our system is based on the Transformer model (Vaswani et al., 2017).", "labels": [], "entities": []}, {"text": "We use an improved technique of backtranslation, where we iterate the process of translating monolingual data in one direction and training an NMT model for the opposite direction using synthetic parallel data.", "labels": [], "entities": []}, {"text": "We apply a simple but effective filtering of the synthetic data.", "labels": [], "entities": []}, {"text": "We pre-process the input sentences using coreference resolution in order to disambiguate the gender of pro-dropped personal pronouns.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.871546059846878}]}, {"text": "Finally, we apply two simple post-processing substitutions on the translated output.", "labels": [], "entities": []}, {"text": "Our system is significantly (p < 0.05) better than all other English-Czech and Czech-English systems in WMT2018.", "labels": [], "entities": [{"text": "WMT2018", "start_pos": 104, "end_pos": 111, "type": "DATASET", "confidence": 0.9326746463775635}]}], "introductionContent": [{"text": "The quality of Neural Machine Translation (NMT) depends heavily on the amount and quality of the training parallel sentences as well as on various training tricks, which are sometimes surprisingly simple and effective.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 15, "end_pos": 47, "type": "TASK", "confidence": 0.7729981392621994}]}, {"text": "In this paper, we describe our NMT system \"CUNI Transformer\" (Charles University version of Transformer), submitted to the English\u2192Czech and Czech\u2192English news translation shared task within WMT2018.", "labels": [], "entities": [{"text": "Charles University version of Transformer)", "start_pos": 62, "end_pos": 104, "type": "DATASET", "confidence": 0.908005158106486}, {"text": "Czech\u2192English news translation shared task", "start_pos": 141, "end_pos": 183, "type": "TASK", "confidence": 0.6486982703208923}, {"text": "WMT2018", "start_pos": 191, "end_pos": 198, "type": "DATASET", "confidence": 0.8876064419746399}]}, {"text": "We describe five techniques, which helped to improve our system, so that it outperformed all other systems in these two translation directions: training data filtering (Section 3), improved backtranslation (Section 4), tuning two separate models based on the original language of the text to be translated (Section 5), coreference pre-processing (Section 6) and post-processing using regular expressions (Section 7).", "labels": [], "entities": [{"text": "training data filtering", "start_pos": 144, "end_pos": 167, "type": "TASK", "confidence": 0.6420096059640249}]}, {"text": "Our system significantly outperformed all other systems in WMT2018 evaluation", "labels": [], "entities": [{"text": "WMT2018", "start_pos": 59, "end_pos": 66, "type": "TASK", "confidence": 0.49992597103118896}]}], "datasetContent": [{"text": "Our training data is constrained to the data allowed in the WMT2018 shared task.", "labels": [], "entities": [{"text": "WMT2018 shared task", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.7474056283632914}]}, {"text": "Parallel (authentic) data are: CzEng 1.7, Europarl v7, News Commentary v11 and CommonCrawl.", "labels": [], "entities": [{"text": "CzEng 1.7", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.902010440826416}, {"text": "Europarl", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9604239463806152}, {"text": "CommonCrawl", "start_pos": 79, "end_pos": 90, "type": "DATASET", "confidence": 0.9775412678718567}]}, {"text": "In our backtranslation experiments (Section 4), we used synthetic data translated by backtranslation of monolingual data: Czech and (a subset of) English NewsCrawl articles.", "labels": [], "entities": []}, {"text": "3% of sentences from the synthetic data (Section 3).", "labels": [], "entities": []}, {"text": "Data sizes are reported in.", "labels": [], "entities": []}, {"text": "Note that usually the amount of available monolingual data is orders of magnitude larger than the available parallel data, but in our case it is comparable (58M parallel vs. 65M/48M monolingual).", "labels": [], "entities": []}, {"text": "We used all the Czech monolingual data allowed in the constrained task.", "labels": [], "entities": []}, {"text": "We used the Transformer self-attentional sequence-to-sequence model ( implemented in the Tensor2Tensor framework.", "labels": [], "entities": []}, {"text": "We followed the training setup and tips of, but we trained our models with the Adafactor optimizer) instead of the default Adam: We used T2T version 1.6.0, transformer_big and hyper-parameters learning_rate_schedule=rsqrt_decay, learning_rate_warmup_steps=8000, batch_size=2900, max_length=150, layer_prepostprocess_dropout=0, optimizer=Adafactor.", "labels": [], "entities": []}, {"text": "For decoding, we used alpha=1.", "labels": [], "entities": []}, {"text": "We stored model checkpoints each hour and averaged the last eight checkpoints.", "labels": [], "entities": []}, {"text": "We used eight GTX 1080 Ti GPUs.", "labels": [], "entities": []}, {"text": "The three reported automatic metrics are: casesensitive (cased) BLEU, case-insensitive (uncased) BLEU and a character-level metric chrF2.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9851239323616028}, {"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9896597266197205}]}, {"text": "We compute all the three metrics with sacreBLEU.", "labels": [], "entities": []}, {"text": "The reported cased and uncased variants of BLEU differ also in the tokenization.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9840618968009949}, {"text": "tokenization", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.959557831287384}]}, {"text": "The cased variant uses the default (ASCII-only) for better comparability with the results at http://matrix.statmt.org.", "labels": [], "entities": []}, {"text": "The uncased variant uses the international tokenization, which has higher correlation with humans We performed a small-scale manual evaluation on newstest2017 and noticed that in many cases the human reference translation is actually worse than our Transformer output.", "labels": [], "entities": []}, {"text": "Thus the results of BLEU (or any other automatic metric comparing similarity with references) maybe misleading.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9970026612281799}]}, {"text": "the reports results of all English\u2194Czech systems submitted to WMT2018, according to both automatic and manual evaluation.", "labels": [], "entities": [{"text": "WMT2018", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.8943105340003967}]}, {"text": "For the automatic evaluation, we use the same three metrics as in the previous section (just with wmt18 instead of wmt17).", "labels": [], "entities": []}, {"text": "For the manual evaluation, we report the reference-based direct assessment (refDA) scores, provided by the WMT organizers.", "labels": [], "entities": [{"text": "reference-based direct assessment (refDA) scores", "start_pos": 41, "end_pos": 89, "type": "METRIC", "confidence": 0.7898996685232434}, {"text": "WMT organizers", "start_pos": 107, "end_pos": 121, "type": "DATASET", "confidence": 0.7734299898147583}]}, {"text": "Our Transformer is the best system in English\u2192Czech and Czech\u2192English WMT2018 news task.", "labels": [], "entities": [{"text": "WMT2018 news task", "start_pos": 70, "end_pos": 87, "type": "DATASET", "confidence": 0.8772883613904318}]}, {"text": "It is significantly (p < 0.05) better than the second-best system -UEdin NMT, in both translation directions and both according to BLEU bootstrap resampling test) and according to refDA Wilcoxon rank-sum test.", "labels": [], "entities": [{"text": "UEdin NMT", "start_pos": 67, "end_pos": 76, "type": "DATASET", "confidence": 0.8330506384372711}, {"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9796000123023987}]}], "tableCaptions": [{"text": " Table 1: Training data sizes (in thousands).", "labels": [], "entities": []}, {"text": " Table 2: Automatic evaluation on (English\u2192Czech) newstest2017. The three scores in parenthesis show  BLEU difference relative to the previous line.", "labels": [], "entities": [{"text": "Czech) newstest2017", "start_pos": 43, "end_pos": 62, "type": "DATASET", "confidence": 0.46178391575813293}, {"text": "BLEU difference", "start_pos": 102, "end_pos": 117, "type": "METRIC", "confidence": 0.9660678505897522}]}, {"text": " Table 3: WMT2018 automatic (BLEU, chrF2) and manual (refDA = reference-based direct assessment) evaluation  on newstest2018.", "labels": [], "entities": [{"text": "WMT2018", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.5912572145462036}, {"text": "automatic", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9339356422424316}, {"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9584933519363403}]}]}