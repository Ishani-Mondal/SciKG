{"title": [{"text": "Initial Experiments in Data-Driven Morphological Analysis for Finnish", "labels": [], "entities": [{"text": "Data-Driven Morphological Analysis", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.6668513119220734}]}], "abstractContent": [{"text": "This paper presents initial experiments in data-driven morphological analysis for Finnish using deep learning methods.", "labels": [], "entities": [{"text": "data-driven morphological analysis", "start_pos": 43, "end_pos": 77, "type": "TASK", "confidence": 0.7113030056158701}]}, {"text": "Our system uses a character based bidirectional LSTM and pretrained word embeddings to predict a set of morphological analyses for an input word form.", "labels": [], "entities": []}, {"text": "We present experiments on morphological analysis for Finnish.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.8996922373771667}]}, {"text": "We learn to mimic the output of the OMorFi analyzer on the Finnish portion of the Universal Dependency treebank collection.", "labels": [], "entities": [{"text": "Universal Dependency treebank collection", "start_pos": 82, "end_pos": 122, "type": "DATASET", "confidence": 0.8142667263746262}]}, {"text": "The results of the experiments are encouraging and show that the current approach has potential to serve as an extension to existing rule-based analyzers.", "labels": [], "entities": []}, {"text": "Tiivistelm\u00e4 Esittelemme kokeita aineistol\u00e4ht\u00f6isell\u00e4 syv\u00e4oppimismenetelmiin perustuval-la suomen kielen morfologisella analysaattorilla.", "labels": [], "entities": []}, {"text": "Esittelem\u00e4mme j\u00e4rjestelm\u00e4 pe-rustuu merkkipohjaisiin LSTM-malleihin ja esiopetettuihin sanaupotuksiin.", "labels": [], "entities": []}, {"text": "J\u00e4r-jestelm\u00e4mme oppii matkimaan OMorFi-j\u00e4sennint\u00e4, joka on suomen kielen morfo-loginen analysaattori.", "labels": [], "entities": []}, {"text": "Teemme kokeita Universal Dependency-puupankkikoko-elman suomenkielisell\u00e4 osuudella.", "labels": [], "entities": []}, {"text": "Kokeemme osoittavat, ett\u00e4 koneoppimismene-telm\u00e4t tarjoavat lupaavan l\u00e4hestymistavan suomen kielen morfologiseen analyy-siin.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of morphological analysis consists of providing a word form with the complete set of morphological readings it can attain (see).", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.8313151895999908}]}, {"text": "It is a cornerstone in the development of natural language processing (NLP) utilities for morphologically complex languages such as the Uralic languages.", "labels": [], "entities": []}, {"text": "It is a necessary preprocessing task because of the high type-to-token ratio, which is prevalent in morphologically complex languages.", "labels": [], "entities": []}, {"text": "Additionally, phenomena like compounding and derivation, which frequently produce previously unseen lexemes, necessitate the use of morphological analyzers.", "labels": [], "entities": []}, {"text": "Hand-crafted analyzers are the gold standard for morphological analysis.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.8628053665161133}]}, {"text": "Creation of such analyzers is, however, a labor intensive process and requires expertise in linguistics, the target language and the rule formalisms used to create these analyzers.", "labels": [], "entities": []}, {"text": "Moreover, analyzers need to be continuously updated with new lexemes in order to maintain high coverage on running text.", "labels": [], "entities": [{"text": "coverage", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9632580876350403}]}, {"text": "In this paper, we investigate an alternative to hand-crafted analyzers, namely, data-driven morphological analyzers which are learned from annotated training data.", "labels": [], "entities": []}, {"text": "In our case, the training data consists of words and complete sets of analyses.", "labels": [], "entities": []}, {"text": "During test time, the system takes a Finnish word such as kisaan ('into the competition' or 'I am competing') as input and gives a set of analyses {Noun+Sg+Ill, Verb+Act+Indv+Pres+Sg1} as output.", "labels": [], "entities": []}, {"text": "We present experiments in data-driven morphological analysis of Finnish.", "labels": [], "entities": [{"text": "morphological analysis of Finnish", "start_pos": 38, "end_pos": 71, "type": "TASK", "confidence": 0.7603999599814415}]}, {"text": "We learn to mimic the OMorFi analyzer () on the Finnish portion of the Universal Dependency treebank collection ().", "labels": [], "entities": [{"text": "Universal Dependency treebank collection", "start_pos": 71, "end_pos": 111, "type": "DATASET", "confidence": 0.7594139948487282}]}, {"text": "The data sets and OMorFi analyzer are further discussed in Section 3.", "labels": [], "entities": [{"text": "OMorFi analyzer", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.5157257616519928}]}, {"text": "We use a deep learning model encompassing a character-level recurrent model, which maps words onto sets of analyses as explained in Section 4.", "labels": [], "entities": []}, {"text": "Our results, described in Section 5, show that this line of research is encouraging.", "labels": [], "entities": []}, {"text": "We present related work in Section 2 and present concluding remarks in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments on the UD_Finnish treebank as explained above.", "labels": [], "entities": [{"text": "UD_Finnish treebank", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.9063554555177689}]}, {"text": "We train the system on the training data and report performance on the held-out test set.", "labels": [], "entities": []}, {"text": "The system was implemented using the Dynet toolkit (.", "labels": [], "entities": []}, {"text": "We set all hyper-parameters using the development set and optimize the network using Adam () with learning rate 0.0001 and beta values \u03b2 1 = 0.9; \u03b2 2 = 0.999.", "labels": [], "entities": []}, {"text": "We train the system for 50 epochs.", "labels": [], "entities": []}, {"text": "We use word embeddings and character-based embeddings of dimension 200.", "labels": [], "entities": []}, {"text": "Character-based embeddings are computed in the following way: We set the hidden state dimension of the character-based LSTM to 100 and use a single layer bidirectional LSTM network.", "labels": [], "entities": []}, {"text": "We concatenate the final 100 dimensional cell states of the forward and backward component of the bidirectional LSTM.", "labels": [], "entities": []}, {"text": "This gives us one 200 dimensional character-based embedding vector for the input word.", "labels": [], "entities": []}, {"text": "As explained in Section 4, the word embedding and character-based embedding are then summed.", "labels": [], "entities": []}, {"text": "During training, we employ 50% dropout on recurrent connections in the characterbased LSTM networks.", "labels": [], "entities": []}, {"text": "We use pretrained word vectors to initialize word embeddings.", "labels": [], "entities": []}, {"text": "These were trained using the word2vec () implementation in the gensim toolkit.", "labels": [], "entities": []}, {"text": "In order to train the unknown word embedding discussed in Section 4, we randomly replace word embeddings during training with the unknown word embedding with probability 2%.", "labels": [], "entities": []}, {"text": "The system is evaluated with regard to accuracy for full analysis sets as well as recall, precision and f-score of analyses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9995680451393127}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9996770620346069}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9994231462478638}, {"text": "f-score", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9873552918434143}]}, {"text": "Full analysis accuracy defined as C/A, where C is the number of test set tokens, which received exactly the correct set of analyses, and A is the count of all tokens in the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9981974959373474}]}, {"text": "Recall is defined r = T P /T , where T P is the amount of correct analyses that the system recovered and T is the total amount of correct analyses in the gold standard test set.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9814645648002625}, {"text": "gold standard test set", "start_pos": 154, "end_pos": 176, "type": "DATASET", "confidence": 0.7964023500680923}]}, {"text": "Similarly, precision is defined asp = T P /P , where P is the total amount of analyses returned by the system.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.999624490737915}]}, {"text": "As familiar, f-score is defined as 2pr/(p + r).", "labels": [], "entities": []}, {"text": "Results of experiments are shown in.", "labels": [], "entities": []}, {"text": "We present results separately for all tokens in the test set and OOV tokens, which were not present in the training set.", "labels": [], "entities": [{"text": "OOV", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.8789107203483582}]}], "tableCaptions": [{"text": " Table 1: Description of data sets used in experiments. Average ambiguity refers to  the average count of distinct analyses for tokens recognized by OMorFi.", "labels": [], "entities": [{"text": "Average ambiguity", "start_pos": 56, "end_pos": 73, "type": "METRIC", "confidence": 0.9468623101711273}, {"text": "OMorFi", "start_pos": 149, "end_pos": 155, "type": "DATASET", "confidence": 0.896446943283081}]}, {"text": " Table 2: Results of experiments.", "labels": [], "entities": []}]}