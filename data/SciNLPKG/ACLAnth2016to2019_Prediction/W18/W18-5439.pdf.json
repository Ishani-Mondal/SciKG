{"title": [{"text": "Predicting and interpreting embeddings for out of vocabulary words in downstream tasks", "labels": [], "entities": [{"text": "Predicting", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9327126741409302}]}], "abstractContent": [{"text": "We propose a novel way to handle out of vocabulary (OOV) words in downstream natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "handle out of vocabulary (OOV) words in downstream natural language processing (NLP) tasks", "start_pos": 26, "end_pos": 116, "type": "TASK", "confidence": 0.6537984416765326}]}, {"text": "We implement a network that predicts useful em-beddings for OOV words based on their morphology and on the context in which they appear.", "labels": [], "entities": []}, {"text": "Our model also incorporates an attention mechanism indicating the focus allocated to the left context words, the right context words or the word's characters, hence making the prediction more interpretable.", "labels": [], "entities": []}, {"text": "The model is a \"drop-in\" module that is jointly trained with the downstream task's neural network, thus producing embeddings specialized for the task at hand.", "labels": [], "entities": []}, {"text": "When the task is mostly syntactical, we observe that our model aims most of its attention on surface form characters.", "labels": [], "entities": []}, {"text": "On the other hand, for tasks more semantical, the network allocates more attention to the surrounding words.", "labels": [], "entities": []}, {"text": "In all our tests, the module helps the network to achieve better performances in comparison to the use of simple random em-beddings.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We evaluate the performance gain that our module can offer by solving two sequence labeling tasks, NER and POS tagging, using the CoNLL 2003 shared task dataset.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 107, "end_pos": 118, "type": "TASK", "confidence": 0.6285905838012695}, {"text": "CoNLL 2003 shared task dataset", "start_pos": 130, "end_pos": 160, "type": "DATASET", "confidence": 0.9464616179466248}]}, {"text": "We compare our module to a baseline where OOV words are assigned random embeddings.", "labels": [], "entities": []}, {"text": "shows the results we obtain.", "labels": [], "entities": []}, {"text": "We can observe the clear advantage of proper handling of OOV words can provide.", "labels": [], "entities": []}, {"text": "For both tasks, we gain a significant margin on the baseline, with more than 3% of the F1 score for NER.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9903202056884766}, {"text": "NER", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.48818641901016235}]}, {"text": "We can see from that the network focuses more on the context fora semantic task such as NER.", "labels": [], "entities": []}, {"text": "An interesting phenomenon is a focus on the right context when the entity is of type B and on the left context when the entity is of type I.", "labels": [], "entities": []}, {"text": "We can also note that for the syntactic task (POS), the network tends to focus on the context for proper nouns (NNP), which corroborates our observations for the NER task.", "labels": [], "entities": [{"text": "NER task", "start_pos": 162, "end_pos": 170, "type": "TASK", "confidence": 0.8916043341159821}]}, {"text": "However, morphology plays a more important role to predict embeddings for other lexical categories.", "labels": [], "entities": []}, {"text": "Embeddings for quantities (CD) are mostly predicted from their numerical characters.", "labels": [], "entities": []}, {"text": "We further qualitatively analyze the behavior of the network fora given OOV word appearing in different contexts in.", "labels": [], "entities": []}, {"text": "When the target OOV word langmore is preceded by john or australian, the network gives high importance to these context words.", "labels": [], "entities": []}, {"text": "However, an interesting phenomenon happens when a sentence begins with this word: the network shifts its attention from the left context to the right one and also assigns more importance to the morphology of the word, thus showing the network has truly learned where it can extract useful information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of our OOV embeddings pre- diction scheme against random embeddings.", "labels": [], "entities": []}, {"text": " Table 2: Average weights assigned to word charac- ters, left context and right context by the attention  mechanism for NER and for POS tagging.", "labels": [], "entities": [{"text": "NER", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.7872495651245117}, {"text": "POS tagging", "start_pos": 132, "end_pos": 143, "type": "TASK", "confidence": 0.7799432575702667}]}]}