{"title": [{"text": "Transliteration Better than Translation? Answering Code-mixed Questions over a Knowledge Base", "labels": [], "entities": [{"text": "Answering Code-mixed Questions over a Knowledge Base", "start_pos": 41, "end_pos": 93, "type": "TASK", "confidence": 0.8336215615272522}]}], "abstractContent": [{"text": "Humans can learn multiple languages.", "labels": [], "entities": []}, {"text": "If they know a fact in one language, they can answer a question in another language they understand.", "labels": [], "entities": []}, {"text": "They can also answer Code-mix (CM) questions: questions which contain both languages.", "labels": [], "entities": []}, {"text": "This ability is attributed to the unique learning ability of humans.", "labels": [], "entities": []}, {"text": "Our task aims to study if machines can achieve this.", "labels": [], "entities": []}, {"text": "We demonstrate how effectively a machine can answer CM questions.", "labels": [], "entities": [{"text": "CM questions", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.8242228627204895}]}, {"text": "In this work, we adopt a two-step approach: candidate generation and candidate re-ranking to answer questions.", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.809388667345047}]}, {"text": "We propose a Triplet-Siamese-Hybrid CNN (TSHCNN) to re-rank candidate answers.", "labels": [], "entities": []}, {"text": "We show experiments on the SimpleQuestions dataset.", "labels": [], "entities": [{"text": "SimpleQuestions dataset", "start_pos": 27, "end_pos": 50, "type": "DATASET", "confidence": 0.9559989869594574}]}, {"text": "Our network is trained only on English questions provided in this dataset and noisy Hindi translations of these questions and can answer English-Hindi CM questions effectively without the need of translation into English.", "labels": [], "entities": []}, {"text": "Back-transliterated CM questions outperform their lexical and sentence level translated counterparts by 5% & 35% respectively, highlighting the efficacy of our approach in a resource-constrained setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question Answering (QA) has received significant attention in the Natural Language (NLP) community.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9231696963310242}]}, {"text": "There are many variations (opendomain, knowledge bases, reading comprehension) as well as datasets () for the question answering task.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 110, "end_pos": 133, "type": "TASK", "confidence": 0.8295975923538208}]}, {"text": "However, many approaches () attempted in QA so far have been focused on monolingual questions.", "labels": [], "entities": [{"text": "QA", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9301202893257141}]}, {"text": "This is true for both methods and techniques as well as resources.", "labels": [], "entities": []}, {"text": "Code-mixing (referred to as CM) refers to the phenomenon of \"embedding of linguistic units such as phrases, words and morphemes of one language into an utterance of another language\").", "labels": [], "entities": [{"text": "CM)", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.6672476381063461}]}, {"text": "People in multilingual societies commonly use code-mixed sentences in conversations (, to search on the web ( and to ask questions ().", "labels": [], "entities": []}, {"text": "However, current Question Answering (QA) systems do not support CM and are only designed to work with a single language.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.86463942527771}]}, {"text": "This limitation makes it unsuitable for multilingual users to naturally interact with the QA system, specifically in scenarios wherein they do not know the right word in the target language.", "labels": [], "entities": []}, {"text": "CM presents serious challenges for the language processing community (), including parsing, Machine Translation (MT), automatic speech recognition (ASR), information retrieval (IR) and extraction (IE), and semantic processing.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.8352979600429535}, {"text": "automatic speech recognition (ASR)", "start_pos": 118, "end_pos": 152, "type": "TASK", "confidence": 0.8189729054768881}, {"text": "information retrieval (IR) and extraction (IE)", "start_pos": 154, "end_pos": 200, "type": "TASK", "confidence": 0.8749530166387558}, {"text": "semantic processing", "start_pos": 206, "end_pos": 225, "type": "TASK", "confidence": 0.7964546084403992}]}, {"text": "Even for problems such as language identification, or part of speech tagging, that are considered solved for monolingual languages, performance degrades when mixed-language is present.", "labels": [], "entities": [{"text": "language identification", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7150374352931976}, {"text": "speech tagging", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.7254614233970642}]}, {"text": "Lack of language resources such as annotated corpora, part-of-speech taggers and parsers poses a considerable challenge for automated processing and analysis of CM languages.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.72523432970047}]}, {"text": "This further amplifies the challenge for CM QA.", "labels": [], "entities": [{"text": "CM QA", "start_pos": 41, "end_pos": 46, "type": "TASK", "confidence": 0.6092391610145569}]}, {"text": "This CM question answering task is challenging not just because of having multiple languages with different semantics but also because of the different word order of source language and CM, making it difficult to extract essential features from the input text.", "labels": [], "entities": [{"text": "CM question answering task", "start_pos": 5, "end_pos": 31, "type": "TASK", "confidence": 0.8527513444423676}]}, {"text": "We base our work on the premise that humans can answer CM questions easily provided they understand the languages used in the question.", "labels": [], "entities": []}, {"text": "They require no additional training in the form of CM questions to comprehend a CM question.", "labels": [], "entities": []}, {"text": "So, one way to tackle CM questions is to translate them into a single language and use monolingual QA systems ().", "labels": [], "entities": []}, {"text": "Machine Translation systems perform poorly on CM sentences.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7840250134468079}]}, {"text": "The only other viable option is lexical translation (word byword translation).", "labels": [], "entities": [{"text": "lexical translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7610308527946472}, {"text": "word byword translation", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.6376995046933492}]}, {"text": "Lexical translation requires language identification, which show to be solved.", "labels": [], "entities": [{"text": "Lexical translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9229441285133362}, {"text": "language identification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7241283506155014}]}, {"text": "We show that our model trained on both English and Hindi can perform better on CM question directly than its lexical translation.", "labels": [], "entities": []}, {"text": "This removes the need to obtain a large bilingual mapping of words for lexical translation.", "labels": [], "entities": [{"text": "lexical translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7737168967723846}]}, {"text": "Also, such a sizeable bilingual mapping maybe hard to obtain for low-resource languages.", "labels": [], "entities": []}, {"text": "Knowledge Bases (KBs) like Freebase (Google, 2017) and DBpedia 1 contain avast wealth of information.", "labels": [], "entities": [{"text": "Freebase (Google, 2017)", "start_pos": 27, "end_pos": 50, "type": "DATASET", "confidence": 0.881810317436854}]}, {"text": "Information is structured in the form of tuples, i.e. a combination of subject, predicate and object (s, p, o) in these KBs.", "labels": [], "entities": []}, {"text": "Such KBs contain information predominately in English, and low resource languages tend to lose out on having a rich information source.", "labels": [], "entities": []}, {"text": "We use bilingual embeddings to fill the gaps due to lack of resources.", "labels": [], "entities": []}, {"text": "We also develop a K-Nearest Bilingual Embedding Transformation (KNBET) which exploits bilingual embeddings to outperform the performance of lexical translation.", "labels": [], "entities": [{"text": "K-Nearest Bilingual Embedding Transformation (KNBET", "start_pos": 18, "end_pos": 69, "type": "TASK", "confidence": 0.613449881474177}]}, {"text": "We overcome challenges discussed above in our paper and develop a CM QA system over KB, named CMQA, using only monolingual data from individual languages.", "labels": [], "entities": []}, {"text": "We demonstrate our system with Hinglish (Matrix language: Hindi, Embedded language: English) CM questions.", "labels": [], "entities": []}, {"text": "Our evaluation shows promising results given that no CM data was used to 1 http://dbpedia.org/ train our model.", "labels": [], "entities": []}, {"text": "This shows promise that we do not need CM data but can use monolingual data to train a CM QA system.", "labels": [], "entities": []}, {"text": "Our results show that our system is much more useful as compared to translating a CM question.", "labels": [], "entities": [{"text": "translating a CM question", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.750417023897171}]}, {"text": "Our contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "We show how we can answer CM questions given an English corpus, noisy Hindi supervision and imperfect bilingual embeddings.", "labels": [], "entities": []}, {"text": "2. We introduce a Triplet-SiameseHybrid Convolutional Neural Network (TSHCNN) that jointly learns to rank candidate answers.", "labels": [], "entities": []}], "datasetContent": [{"text": "English CM questions to researchers.", "labels": [], "entities": []}, {"text": "This dataset is mapped with Freebase tuples and English questions from the SimpleQuestions dataset.", "labels": [], "entities": [{"text": "SimpleQuestions dataset", "start_pos": 75, "end_pos": 98, "type": "DATASET", "confidence": 0.9580644965171814}]}, {"text": "To the best of our knowledge, we are the first to tackle the problem of End-to-End CodeMixed Question Answering over Knowledge Bases in a resource-constrained setting.", "labels": [], "entities": [{"text": "End-to-End CodeMixed Question Answering over Knowledge Bases", "start_pos": 72, "end_pos": 132, "type": "TASK", "confidence": 0.6610358783176967}]}, {"text": "Earlier approaches for CM QA () require a bilingual dictionary to translate words to English and an existing Google like-search engine to get answers, which we do not require.", "labels": [], "entities": [{"text": "CM QA", "start_pos": 23, "end_pos": 28, "type": "TASK", "confidence": 0.6595801413059235}]}, {"text": "The rest of the paper is structured as follows: We survey related work in Section 2 and describe the task description in Section 3.", "labels": [], "entities": []}, {"text": "We explain our system in Section 4.", "labels": [], "entities": []}, {"text": "We describe experiments in Section 5 and provide a detailed analysis and discussion in Section 6 and conclude in Section 7.", "labels": [], "entities": []}, {"text": "We use the SimpleQuestions () dataset which comprises 75.9k/10.8k/21.7k training/validation/test questions.", "labels": [], "entities": [{"text": "SimpleQuestions () dataset", "start_pos": 11, "end_pos": 37, "type": "DATASET", "confidence": 0.6851850152015686}]}, {"text": "Each question is associated with an answer, i.e. a tuple (subject, predicate, object) from a Freebase (Google, 2017) subset (FB2M or FB5M).", "labels": [], "entities": [{"text": "FB2M", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.8619408011436462}, {"text": "FB5M", "start_pos": 133, "end_pos": 137, "type": "DATASET", "confidence": 0.8090052008628845}]}, {"text": "The subject is given as a MID 5 and we obtain its corresponding entity name by processing the Freebase data dumps.", "labels": [], "entities": [{"text": "Freebase data dumps", "start_pos": 94, "end_pos": 113, "type": "DATASET", "confidence": 0.9796926975250244}]}, {"text": "We were unable to obtain entity name mappings for some MIDs, and these were removed from our final set.", "labels": [], "entities": [{"text": "MIDs", "start_pos": 55, "end_pos": 59, "type": "TASK", "confidence": 0.9475212097167969}]}, {"text": "We also obtain Hindi translations for all questions in SimpleQuestions using Google Translate.", "labels": [], "entities": []}, {"text": "Note, these translations are not perfect and serve as a noisy input to the network.", "labels": [], "entities": []}, {"text": "Also, we only translate the questions, and the answers remain in English.", "labels": [], "entities": []}, {"text": "As with previous work, we show results over the 2M-subset of Freebase (FB2M).", "labels": [], "entities": [{"text": "Freebase (FB2M)", "start_pos": 61, "end_pos": 76, "type": "DATASET", "confidence": 0.8031162023544312}]}, {"text": "We use pre-trained word embeddings 6 provided by Fasttext ( and use alignment matrices 7 provided by to obtain English-Hindi bilingual embeddings.", "labels": [], "entities": []}, {"text": "use a small set of 5000 words to obtain the alignment matrices.", "labels": [], "entities": []}, {"text": "The provided Hindi embeddings are in Devanagari script.", "labels": [], "entities": []}, {"text": "We use randomly initialised embeddings between [-0.25, 0.25] for words without embeddings.", "labels": [], "entities": []}, {"text": "We have prepared a dataset of Hindi-English CM questions fora smaller set of 250 tuples obtained from the test split of SimpleQuestions dataset.", "labels": [], "entities": [{"text": "SimpleQuestions dataset", "start_pos": 120, "end_pos": 143, "type": "DATASET", "confidence": 0.9352993965148926}]}, {"text": "We gathered these questions from Hindi-English speakers, who were asked to form a natural language CM question, shown a tuple.", "labels": [], "entities": []}, {"text": "Further, for every tuple we obtained CM questions from 5 different annotators and pick one at random for the final test set, to ensure multiple variations.", "labels": [], "entities": []}, {"text": "Each CM question is in Roman script, and annotators anglicise (or transliterate) Hindi words (Devanagari script) to Roman script, to the best of their ability.", "labels": [], "entities": []}, {"text": "This introduces variations in spellings and posses a challenge for the network and also back-transliteration.", "labels": [], "entities": []}, {"text": "A unique ID referring to an entity in Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.962925910949707}]}, {"text": "We report results using the standard evaluation criteria (, in terms of path-level accuracy, which is the percentage of questions for which the top-ranked candidate fact is correct.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9281671643257141}]}, {"text": "A prediction is correct if the system correctly retrieved the subject and the relationship.", "labels": [], "entities": []}, {"text": "Since there is no earlier work on CM QA over KBs, we compare the different ways a CM question can be answered using our QA sys- We also report results for the English questions in SimpleQuestions on our model trained only on English.", "labels": [], "entities": []}, {"text": "This serves as a benchmark for our model as compared to other work on SimpleQuestions (.", "labels": [], "entities": []}, {"text": "Network parameters and decisions are presented in.", "labels": [], "entities": []}, {"text": "We train our model until the validation loss on the validation set stops improving further for 3 epochs.", "labels": [], "entities": [{"text": "validation", "start_pos": 29, "end_pos": 39, "type": "TASK", "confidence": 0.940645694732666}]}, {"text": "We report the results on the epoch with the best validation loss.", "labels": [], "entities": []}, {"text": "We use K = 200 for the initial candidate generation step.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Network Parameters  Parameter  Value  Batch Size  100  Non-linearity  Relu  CNN Filters &  Width", "labels": [], "entities": [{"text": "Relu", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9186856746673584}]}, {"text": " Table 2: End-to-End Answer Accuracy for En- glish Questions  Model  Acc.  Bordes et al. (2015)  62.7  Golub and He (2016)  70.9  Lukovnikov et al. (2017)  71.2  Yin et al. (2016)  76.4  Yu et al. (2017)  77.0  Ture and Jojic (2017)  86.8  Ours: Candidate Generation  68.5  Ours: Candidate Re-Ranking 77.0", "labels": [], "entities": [{"text": "Acc", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9790937304496765}]}, {"text": " Table 3: Candidate generation results: Recall  of top-k answer candidates for each question  type", "labels": [], "entities": [{"text": "Candidate generation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7766962945461273}]}]}