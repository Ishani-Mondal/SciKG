{"title": [{"text": "The NiuTrans Machine Translation System for WMT18", "labels": [], "entities": [{"text": "NiuTrans Machine Translation", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.7995171944300333}, {"text": "WMT18", "start_pos": 44, "end_pos": 49, "type": "TASK", "confidence": 0.6199180483818054}]}], "abstractContent": [{"text": "This paper describes the submission of the NiuTrans neural machine translation system for the WMT 2018 Chinese \u2194 En-glish news translation tasks.", "labels": [], "entities": [{"text": "NiuTrans neural machine translation", "start_pos": 43, "end_pos": 78, "type": "TASK", "confidence": 0.7698111236095428}, {"text": "WMT 2018 Chinese \u2194 En-glish news translation tasks", "start_pos": 94, "end_pos": 144, "type": "TASK", "confidence": 0.7124873325228691}]}, {"text": "Our baseline systems are based on the Transformer architecture.", "labels": [], "entities": []}, {"text": "We further improve the translation performance 2.4-2.6 BLEU points from four aspects, including architectural improvements, diverse ensemble decoding, reranking, and post-processing.", "labels": [], "entities": [{"text": "translation", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.9528776407241821}, {"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9941431879997253}]}, {"text": "Among constrained submissions, we rank 2nd out of 16 submitted systems on Chinese \u2192 En-glish task and 3rd out of 16 on English \u2192 Chinese task, respectively.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) exploits an encoder-decoder framework to model the whole translation process in an end-to-end fashion, and has achieved state-of-the-art performance in many language pairs ().", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8377426167329153}]}, {"text": "This paper describes the submission of the NiuTrans neural machine translation system for the WMT 2018 Chinese \u2194 English news translation tasks.", "labels": [], "entities": [{"text": "NiuTrans neural machine translation", "start_pos": 43, "end_pos": 78, "type": "TASK", "confidence": 0.78403240442276}, {"text": "WMT 2018 Chinese \u2194 English news translation tasks", "start_pos": 94, "end_pos": 143, "type": "TASK", "confidence": 0.6628151386976242}]}, {"text": "Our baseline systems are based on the Transformer model due to the excellent translation performance and fast training thanks to the self-attention mechanism.", "labels": [], "entities": [{"text": "translation", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.9581072926521301}]}, {"text": "Then we enhance it with checkpoint ensemble () that averages the last N checkpoints of a single training run.", "labels": [], "entities": []}, {"text": "To enable openvocabulary translation, all the words are segmented via byte pair encoding (BPE)) for both Chinese and English.", "labels": [], "entities": [{"text": "openvocabulary translation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6976098418235779}, {"text": "byte pair encoding (BPE))", "start_pos": 70, "end_pos": 95, "type": "METRIC", "confidence": 0.6652963757514954}]}, {"text": "Also, we use back-translation technique) to leverage the rich monolingual resource.", "labels": [], "entities": []}, {"text": "Beyond the baseline, we achieve further improvement from four aspects, including architectural improvements, diverse ensemble decoding, reranking and post-processing.", "labels": [], "entities": []}, {"text": "For architectural improvements, we add relu dropout and attention dropout to improve the generalization ability and increase the inner dimension of feed-forward neural network to enlarge the model capacity ().", "labels": [], "entities": []}, {"text": "We also use the novel Swish activation function and self-attention with relative positional representations (.", "labels": [], "entities": []}, {"text": "Next, we explore more diverse ensemble decoding via increasing the number of models and using the models generated by different ways.", "labels": [], "entities": []}, {"text": "Furthermore, at most 17 features tuned by MIRA () are used to rerank the N-best hypotheses.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9056587219238281}]}, {"text": "At last, a post-processing algorithmic is proposed to correct the inconsistent English literals between the source and target sentence.", "labels": [], "entities": []}, {"text": "Through these techniques, we can achieve 2.4-2.6 BLEU points improvement over the baselines.", "labels": [], "entities": [{"text": "BLEU points improvement", "start_pos": 49, "end_pos": 72, "type": "METRIC", "confidence": 0.9486290216445923}]}, {"text": "As a result, our systems rank the second out of 16 submitted systems on Chinese \u2192 English task and the third out of 16 on English \u2192 Chinese task among constrained submissions, respectively.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: BLEU scores [%] on newsdev2018 Chinese-English translation. * denotes the submitted system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9983358979225159}, {"text": "newsdev2018 Chinese-English translation", "start_pos": 29, "end_pos": 68, "type": "DATASET", "confidence": 0.9010070164998373}]}, {"text": " Table 4: BLEU scores [%] on newsdev2018 English \u2192 Chinese translation. * denotes the submitted  system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984209537506104}]}]}