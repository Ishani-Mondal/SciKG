{"title": [{"text": "Reference-less Quality Estimation of Text Simplification Systems", "labels": [], "entities": [{"text": "Text Simplification", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.6588487029075623}]}], "abstractContent": [{"text": "The evaluation of text simplification (TS) systems remains an open challenge.", "labels": [], "entities": [{"text": "text simplification (TS)", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.8488298296928406}]}, {"text": "As the task has common points with machine translation (MT), TS is often evaluated using MT metrics such as BLEU.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.7967142701148987}, {"text": "TS", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9319154620170593}, {"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9959415793418884}]}, {"text": "However , such metrics require high quality reference data, which is rarely available for TS.", "labels": [], "entities": [{"text": "TS", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.969550371170044}]}, {"text": "TS has the advantage over MT of being a monolingual task, which allows for direct comparisons to be made between the simplified text and its original version.", "labels": [], "entities": [{"text": "TS", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.6099038124084473}, {"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.8912813067436218}]}, {"text": "In this paper, we compare multiple approaches to reference-less quality estimation of sentence-level text simplification systems, based on the dataset used for the QATS 2016 shared task.", "labels": [], "entities": [{"text": "reference-less quality estimation of sentence-level text simplification", "start_pos": 49, "end_pos": 120, "type": "TASK", "confidence": 0.5656255951949528}, {"text": "QATS 2016 shared task", "start_pos": 164, "end_pos": 185, "type": "DATASET", "confidence": 0.7978798449039459}]}, {"text": "We distinguish three different dimensions: gram-maticality, meaning preservation and simplicity.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.7134896665811539}, {"text": "simplicity", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9916292428970337}]}, {"text": "We show that n-gram-based MT metrics such as BLEU and METEOR correlate the most with human judgment of grammaticality and meaning preservation, whereas simplicity is best evaluated by basic length-based metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9631953835487366}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9979673027992249}, {"text": "METEOR", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9523177742958069}, {"text": "meaning preservation", "start_pos": 122, "end_pos": 142, "type": "TASK", "confidence": 0.7242169231176376}, {"text": "simplicity", "start_pos": 152, "end_pos": 162, "type": "METRIC", "confidence": 0.9834497570991516}]}], "introductionContent": [{"text": "Text simplification (hereafter TS) has received increasing interest by the scientific community in recent years.", "labels": [], "entities": [{"text": "Text simplification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8308600783348083}, {"text": "TS)", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9099155962467194}]}, {"text": "It aims at producing a simpler version of a source text that is both easier to read and to understand, thus improving the accessibility of text for people suffering from a range of disabilities such as aphasia () or dyslexia (, as well as for second language learners ( and people with low literacy (.", "labels": [], "entities": []}, {"text": "This topic has been researched fora variety of languages such as English (,),), Portuguese (Specia, 2010), Italian () and Japanese (.", "labels": [], "entities": []}, {"text": "One of the main challenges in TS is finding an adequate automatic evaluation metric, which is necessary to avoid the time-consuming human evaluation.", "labels": [], "entities": [{"text": "TS", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9928985834121704}]}, {"text": "Any TS evaluation metric should take into account three properties expected from the output of a TS system, namely: \u2022 Grammaticality: how grammatically correct is the TS system output?", "labels": [], "entities": [{"text": "TS evaluation", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.8679404258728027}]}, {"text": "\u2022 Meaning preservation: how well is the meaning of the source sentence preserved in the TS system output?", "labels": [], "entities": [{"text": "Meaning preservation", "start_pos": 2, "end_pos": 22, "type": "TASK", "confidence": 0.85519078373909}]}, {"text": "\u2022 Simplicity: how simple is the TS system output?", "labels": [], "entities": [{"text": "TS", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.8483132719993591}]}, {"text": "TS is often reduced to a sentence-level problem, whereby one sentence is transformed into a simpler version containing one or more sentences.", "labels": [], "entities": [{"text": "TS", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9760767817497253}]}, {"text": "In this paper, we shall make use of the terms source (sentence) and (TS system) output to respectively denote a sentence given as an input to a TS system and the simplified, single or multi-sentence output produced by the system.", "labels": [], "entities": []}, {"text": "TS, seen as a sentence-level problem, is often viewed as a monolingual variant of (sentencelevel) MT.", "labels": [], "entities": [{"text": "TS", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7823945879936218}]}, {"text": "The standard approach to automatic TS evaluation is therefore to view the task as a translation problem and to use machine translation (MT) evaluation metrics such as BLEU ().", "labels": [], "entities": [{"text": "TS evaluation", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.9671967327594757}, {"text": "machine translation (MT) evaluation", "start_pos": 115, "end_pos": 150, "type": "TASK", "confidence": 0.7986076772212982}, {"text": "BLEU", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9972856044769287}]}, {"text": "However, MT evaluation metrics rely on the existence of parallel corpora of source sentences and manually produced reference translations, which are available on a large scale for many language pairs.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.9543337523937225}]}, {"text": "TS datasets are less numerous and smaller.", "labels": [], "entities": [{"text": "TS datasets", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8414106667041779}]}, {"text": "Moreover, they are often automatically extracted from comparable corpora rather than strictly parallel corpora, which results in noisier reference data.", "labels": [], "entities": []}, {"text": "For example, the PWKP dataset ( consists of 100,000 sentences from the English Wikipedia automatically aligned with sentences from the Simple English Wikipedia based on term-based similarity metrics.", "labels": [], "entities": [{"text": "PWKP dataset", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.9664657711982727}, {"text": "Simple English Wikipedia", "start_pos": 135, "end_pos": 159, "type": "DATASET", "confidence": 0.7527833580970764}]}, {"text": "It has been shown by that many of PWKP's \"simplified\" sentences are in fact not simpler or even not related to their corresponding source sentence.", "labels": [], "entities": []}, {"text": "Even if better quality corpora such as Newsela do exist (, they are costly to create, often of limited size, and not necessarily open-access.", "labels": [], "entities": [{"text": "Newsela", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9725474715232849}]}, {"text": "This creates a challenge for the use of referencebased MT metrics for TS evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9358289837837219}, {"text": "TS evaluation", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.9575195908546448}]}, {"text": "However, TS has the advantage of being a monolingual translation-like task, the source being in the same language as the output.", "labels": [], "entities": [{"text": "TS", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9576793909072876}]}, {"text": "This allows for new, nonconventional ways to use MT evaluation metrics, namely by using them to compare the output of a TS system with the source sentence, thus avoiding the need for reference data.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.9058817923069}]}, {"text": "However, such an evaluation method can only capture at most two of the three above-mentioned dimensions, namely meaning preservation and, to a lesser extent, grammaticality.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 112, "end_pos": 132, "type": "TASK", "confidence": 0.8315748274326324}]}, {"text": "Previous works on reference-less TS evaluation includ\u011b, who compare the behaviour of six different MT metrics when used between the source sentence and the corresponding simplified output.", "labels": [], "entities": [{"text": "TS evaluation", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.8717689514160156}, {"text": "MT", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.9558634757995605}]}, {"text": "They evaluate these metrics with respect to meaning preservation and grammaticality.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.8201596736907959}]}, {"text": "We extend their work in two directions.", "labels": [], "entities": []}, {"text": "Firstly, we extend the comparison to include the degree of simplicity achieved by the system.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9922630786895752}]}, {"text": "Secondly, we compare additional features, including those used by\u0160tajnerby\u02c7by\u0160tajner et al.", "labels": [], "entities": []}, {"text": "(2016a), both individually, as elementary metrics, and within multi-feature metrics.", "labels": [], "entities": []}, {"text": "To our knowledge, no previous work has provided as thorough a comparison across such a wide range and combination of features for the reference-less evaluation of TS.", "labels": [], "entities": [{"text": "TS", "start_pos": 163, "end_pos": 165, "type": "TASK", "confidence": 0.9046335220336914}]}, {"text": "First we review available text simplification evaluation methods and traditional quality estimation features.", "labels": [], "entities": [{"text": "text simplification evaluation", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.7920004924138387}]}, {"text": "We then present the QATS shared task and the associated dataset, which we use for our experiments.", "labels": [], "entities": []}, {"text": "Finally we compare all methods in a reference-less setting and analyze the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "2.1 Using MT metrics to compare the output and a reference TS can be considered as a monolingual translation task.", "labels": [], "entities": []}, {"text": "As a result, MT metrics such as BLEU (), which compare the output of an MT system to a reference translation, have been extensively used for TS).", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.975283682346344}, {"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9986199140548706}, {"text": "TS", "start_pos": 141, "end_pos": 143, "type": "TASK", "confidence": 0.9836033582687378}]}, {"text": "Other successful MT metrics include TER (), ROUGE) and METEOR (), but they have not gained much traction in the TS literature.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.979198694229126}, {"text": "TER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9968031644821167}, {"text": "ROUGE", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9951915740966797}, {"text": "METEOR", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9901493787765503}, {"text": "TS", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.8341436386108398}]}, {"text": "These metrics rely on good quality references, something which is often not available in TS, as discussed by.", "labels": [], "entities": [{"text": "TS", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.5229742527008057}]}, {"text": "Moreover, \u02c7 Stajner et al.", "labels": [], "entities": []}, {"text": "(2015) and showed that using BLEU to compare the system output with a reference is not a good way to perform TS evaluation, even when good quality references are available.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9952641725540161}, {"text": "TS evaluation", "start_pos": 109, "end_pos": 122, "type": "TASK", "confidence": 0.9584448635578156}]}, {"text": "This is especially true when the TS system produces more than one sentence fora single source sentence.", "labels": [], "entities": [{"text": "TS", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.8651260137557983}]}, {"text": "Evaluation of elementary metrics We rank all features by comparing their behaviour with human judgments on the training set.", "labels": [], "entities": []}, {"text": "We first compute for each elementary metric the Pearson correlation between its results and the manually assigned labels for each of the three dimensions.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 48, "end_pos": 67, "type": "METRIC", "confidence": 0.9587450623512268}]}, {"text": "We then rank our elementary metrics according to the absolute value of the Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 75, "end_pos": 94, "type": "METRIC", "confidence": 0.7180752754211426}]}, {"text": "6 Training and evaluation of a combined metric We use our elementary metrics as features to train classifiers on the training set, and evaluate their performance on the test set.", "labels": [], "entities": []}, {"text": "We therefore scale them and reduce the dimensionality with a 25-component PCA 7 , then train several regression algorithms 8 and classification algorithms 9 using scikit-learn (Pedregosa et al., 2011).", "labels": [], "entities": []}, {"text": "For each dimension, we keep the two models performing best on the test set and add them in the leaderboard of the QATS shared task (TABLE 4), naming them with the name of the regression algorithm they were built with.", "labels": [], "entities": [{"text": "TABLE", "start_pos": 132, "end_pos": 137, "type": "METRIC", "confidence": 0.8263460397720337}]}, {"text": "Grammaticality N -gram based MT metrics have the highest correlation with human grammaticality judgments.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.7601305842399597}]}, {"text": "METEOR seems to be the best, probably because of its robustness to synonymy, followed by smoothed BLEU (BLEUSmoothed in 2).", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8537853360176086}, {"text": "smoothed", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9414947628974915}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9223803281784058}, {"text": "BLEUSmoothed", "start_pos": 104, "end_pos": 116, "type": "METRIC", "confidence": 0.9936892986297607}]}, {"text": "This indicates that relevant grammaticality information can be derived from the source sentence.", "labels": [], "entities": []}, {"text": "We were expecting that information contained in a language model would help achieving better results (AvgLMProbsOutput), but MT metrics correlate better with human judgments.", "labels": [], "entities": [{"text": "AvgLMProbsOutput", "start_pos": 102, "end_pos": 118, "type": "METRIC", "confidence": 0.9502689838409424}, {"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9595685601234436}]}, {"text": "We deduce that the grammaticality information contained in the source is more specific and more helpful for evaluation than what is learned by the language model.", "labels": [], "entities": []}, {"text": "We will release our code on github.", "labels": [], "entities": []}, {"text": "We used PCA instead of feature selection because it performed better on the validation set.", "labels": [], "entities": []}, {"text": "The number of component was tuned on the validation set as well.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Brief description of 30 of our most relevant elementary metrics", "labels": [], "entities": []}, {"text": " Table 3: Pearson correlation with human judgments of elementary metrics ranked by absolute value on  training set (15 best metrics for each dimension).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8344006538391113}]}, {"text": " Table 4: QATS leaderboard. Results in bold are our additions to the original leaderboard. We only select  the two models that rank highest during cross-validation.", "labels": [], "entities": [{"text": "QATS leaderboard", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.7005905956029892}]}]}