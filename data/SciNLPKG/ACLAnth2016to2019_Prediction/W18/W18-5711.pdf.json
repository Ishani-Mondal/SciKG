{"title": [{"text": "Exploring Named Entity Recognition As an Auxiliary Task for Slot Filling in Conversational Language Understanding", "labels": [], "entities": [{"text": "Exploring Named Entity Recognition", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7149622961878777}, {"text": "Slot Filling in Conversational Language Understanding", "start_pos": 60, "end_pos": 113, "type": "TASK", "confidence": 0.890427827835083}]}], "abstractContent": [{"text": "Slot filling is a crucial task in the Natural Language Understanding (NLU) component of a dialogue system.", "labels": [], "entities": [{"text": "Slot filling", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9581389427185059}]}, {"text": "Most approaches for this task rely solely on the domain-specific datasets for training.", "labels": [], "entities": []}, {"text": "We propose a joint model of slot filling and Named Entity Recognition (NER) in a multi-task learning (MTL) setup.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.8713402152061462}, {"text": "Named Entity Recognition (NER)", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.7625451137622198}]}, {"text": "Our experiments on three slot filling datasets show that using NER as an auxiliary task improves slot filling performance and achieve competitive performance compared with state-of-the-art.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.7688263952732086}, {"text": "slot filling", "start_pos": 97, "end_pos": 109, "type": "TASK", "confidence": 0.8453767895698547}]}, {"text": "In particular, NER is effective when supervised at the lower layer of the model.", "labels": [], "entities": [{"text": "NER", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9785957336425781}]}, {"text": "For low-resource scenarios, we found that MTL is effective for one dataset.", "labels": [], "entities": [{"text": "MTL", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9409165382385254}]}], "introductionContent": [{"text": "Most of the current dialogue systems depend on an NLU component to extract semantic information from an utterance.", "labels": [], "entities": []}, {"text": "Such semantic information is often represented as a semantic frame which contains the domain, intent of the user, and predefined attributes (slots).", "labels": [], "entities": []}, {"text": "Each word of the utterance is labeled with a slot, which defines a particular attribute (an entity, time, etc) of the utterance.", "labels": [], "entities": []}, {"text": "shows an example of a semantic frame for the sentence \"Show me the prices of all flights from Atlanta to Washington DC\" with Begin/In/Out (BIO) representation.", "labels": [], "entities": []}, {"text": "We focus on slot filling, a task of automatically extracting slots fora given utterance.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.9110638201236725}]}, {"text": "This task can be treated as a sequence labeling problem and the most successful approach is to employ a conditional random fields (CRF) on top of a deep recurrent neural networks (RNN).", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.6749244928359985}]}, {"text": "In general, there are two ways of training a slot filling model: (i) train a domain-specific model (   et al.,.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.9028464257717133}]}, {"text": "One popular transfer learning technique is multi-task learning (MTL) in which a joint model is trained on a target (main) task and several auxiliary tasks simultaneously to learn better feature representations across tasks.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.8851540684700012}, {"text": "multi-task learning (MTL)", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.7231714963912964}]}, {"text": "This technique has shown potential on various NLP tasks and offer flexibility as it allows transfer learning across different domains and tasks).", "labels": [], "entities": []}, {"text": "On slot filling, Jaech et al.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 3, "end_pos": 15, "type": "TASK", "confidence": 0.9790665805339813}]}, {"text": "(2016) train a single slot filling model on different domains and show that MTL is particulary useful in low resource scenarios.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.732182040810585}, {"text": "MTL", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9405616521835327}]}, {"text": "Identifying beneficial auxiliary task for the target task is important when applying MTL.", "labels": [], "entities": [{"text": "MTL", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.8288338780403137}]}, {"text": "In this work, we investigate the effectiveness of Named Entity Recognition (NER) as an auxiliary task for slot filling.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.77804667254289}, {"text": "slot filling", "start_pos": 106, "end_pos": 118, "type": "TASK", "confidence": 0.9215661883354187}]}, {"text": "We propose NER because of two main reasons.", "labels": [], "entities": [{"text": "NER", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.8751101493835449}]}, {"text": "First, the slot values are typically named entities, for example airline name, city name, etc.", "labels": [], "entities": []}, {"text": "Second, the state of the art performance of models for NER have been relatively high (.", "labels": [], "entities": [{"text": "NER", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.8174206018447876}]}, {"text": "Therefore, we expect that the learned features of NER can improve the slot filling performance.", "labels": [], "entities": [{"text": "NER", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.7445568442344666}, {"text": "slot filling", "start_pos": 70, "end_pos": 82, "type": "TASK", "confidence": 0.8611954748630524}]}, {"text": "Finally, NER corpus is relatively easier to obtain compared to domain specific slot filling datasets.", "labels": [], "entities": [{"text": "NER corpus", "start_pos": 9, "end_pos": 19, "type": "DATASET", "confidence": 0.8278018832206726}]}, {"text": "We are interested to answer the following questions: \u2022 Does NER help the performance of slot filling in the MTL setup?", "labels": [], "entities": [{"text": "NER", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.856593668460846}, {"text": "slot filling", "start_pos": 88, "end_pos": 100, "type": "TASK", "confidence": 0.8190310597419739}, {"text": "MTL", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.9354574680328369}]}, {"text": "As NER labels are usually more coarse-grained than slot filling labels, predicted NER label might provide good signal to the more fine-grained slot labels.", "labels": [], "entities": []}, {"text": "For example, the location LOC label in NER can be a strong indicator for slots fromloc.city name or toloc.city name and filter out other slot labels which are not related to location.", "labels": [], "entities": [{"text": "NER", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.9300556182861328}]}, {"text": "We hope the model can learn more general knowledge first and transfer such knowledge to predict more specific slot information using MTL.", "labels": [], "entities": [{"text": "MTL", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.6917238831520081}]}, {"text": "\u2022 What is the effect of supervising NER on the lower layer of the MTL model to the slot filling performance?", "labels": [], "entities": [{"text": "slot filling", "start_pos": 83, "end_pos": 95, "type": "TASK", "confidence": 0.8790087401866913}]}, {"text": "Inspired by recent work of S\u00f8gaard and Goldberg (2016), we investigate the effect of supervising NER on different layers of the model.", "labels": [], "entities": []}, {"text": "Our hypothesis is that a more \"general\" feature is better learned on the lower layer in order to support a task which depends on a more \"specific\" feature.", "labels": [], "entities": []}, {"text": "In addition, we also experiment on crossdomain slot filling models by jointly training slot filling datasets from similar domains using a MTL setup.", "labels": [], "entities": [{"text": "crossdomain slot filling", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.6589771807193756}]}, {"text": "We explore two techniques to measure similarity between domains: domain similarity by and label embedding mapping by.", "labels": [], "entities": []}, {"text": "We experiment with three datasets from different domains.", "labels": [], "entities": []}, {"text": "Our experiments show that for all datasets, using NER as an auxiliary task is beneficial for the slot filling performance.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 97, "end_pos": 109, "type": "TASK", "confidence": 0.8944614231586456}]}, {"text": "NER is consistently helpful when it is supervised at the lower layer.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8195131421089172}]}, {"text": "On the low resource scenario, we found mixed results, in which MTL is only effective for 1 dataset.", "labels": [], "entities": [{"text": "MTL", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.7164125442504883}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Statistics of the datasets. For each dataset, number of sentence in train/dev/test set, the number of unique token and", "labels": [], "entities": []}, {"text": " Table 4: F1 scores comparison between MTL, STL, and previous published results on each dataset. \"Most Similar\" auxiliary", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9995335340499878}, {"text": "MTL", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.5684609413146973}]}, {"text": " Table 5: The effect of the label filtering on MTL perfor-", "labels": [], "entities": [{"text": "label filtering", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.719207838177681}]}, {"text": " Table 6: Performance comparison between STL and MTL", "labels": [], "entities": [{"text": "MTL", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.7967210412025452}]}]}