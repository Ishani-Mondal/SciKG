{"title": [{"text": "New Baseline in Automatic Speech Recognition for Northern S\u00e1mi", "labels": [], "entities": [{"text": "Automatic Speech Recognition", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.6813190877437592}]}], "abstractContent": [{"text": "Automatic speech recognition has gone through many changes in recent years.", "labels": [], "entities": [{"text": "Automatic speech recognition", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7606263756752014}]}, {"text": "Advances both in computer hardware and machine learning have made it possible to develop systems far more capable and complex than the previous state-of-the-art.", "labels": [], "entities": []}, {"text": "However, almost all of these improvements have been tested in major well-resourced languages.", "labels": [], "entities": []}, {"text": "In this paper, we show that these techniques are capable of yielding improvements even in a small data scenario.", "labels": [], "entities": []}, {"text": "We experiment with different deep neural network architectures for acoustic modeling for Northern S\u00e1mi and report up to 50% relative error rate reductions.", "labels": [], "entities": [{"text": "acoustic modeling", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.8165473639965057}, {"text": "relative error rate", "start_pos": 124, "end_pos": 143, "type": "METRIC", "confidence": 0.7555344800154368}]}, {"text": "We also run experiments to compare the performance of subwords as language modeling units in Northern S\u00e1mi.", "labels": [], "entities": []}, {"text": "Tiivistelm\u00e4 Automaattinen puheentunnistus on kehittynyt viime vuosina merkitt\u00e4v\u00e4sti.", "labels": [], "entities": []}, {"text": "Uudet innovaatiot sek\u00e4 laitteistossa ett\u00e4 koneoppimisessa ovat mahdollistaneet entist\u00e4 paljon tehokkaammat ja monimutkaisemmat j\u00e4rjestelm\u00e4t.", "labels": [], "entities": []}, {"text": "Suurin osa n\u00e4is-t\u00e4 parannuksista on kuitenkin testattu vain valtakielill\u00e4, joiden kehitt\u00e4miseen on tarjolla runsaasti aineistoja.", "labels": [], "entities": []}, {"text": "T\u00e4ss\u00e4 paperissa n\u00e4yt\u00e4mme ett\u00e4 n\u00e4m\u00e4 tekniikat tuot-tavat parannuksia my\u00f6s kielill\u00e4, joista aineistoa on v\u00e4h\u00e4n.", "labels": [], "entities": []}, {"text": "Kokeilemme ja ver-tailemme erilaisia syvi\u00e4 neuroverkkoja pohjoissaamen akustisina malleina ja on-nistumme v\u00e4hent\u00e4m\u00e4\u00e4n tunnistusvirheit\u00e4 jopa 50%:lla.", "labels": [], "entities": []}, {"text": "Tutkimme my\u00f6s tapoja pilkkoa sanoja pienempiin osiin pohjoissaamen kielimalleissa.", "labels": [], "entities": []}], "introductionContent": [{"text": "The field of automatic speech recognition (ASR) has advanced rapidly in the last couple of years, in large part thanks to deep neural networks (DNNs).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 13, "end_pos": 47, "type": "TASK", "confidence": 0.7853520462910334}]}, {"text": "For decades there has been active research trying to replace Gaussian mixture models (GMM) with various neural network configurations.", "labels": [], "entities": []}, {"text": "Yet, only after 2010 the full power of neural networks started to be noticed when multiple groups started reporting huge improvements in their implementations (.", "labels": [], "entities": []}, {"text": "At the same time, the computational power of modern graphics processing units (GPU) has made it feasible to utilize very large DNNs with very large training data sets.", "labels": [], "entities": []}, {"text": "For speech recognition, this has meant that the decades-old best practices are quickly being replaced by new and more powerful methods.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9154582917690277}]}, {"text": "In this paper, we have documented our work to build anew baseline for Northern S\u00e1mi.", "labels": [], "entities": []}, {"text": "Using DNNs for acoustic modeling has provided large improvements for wellresourced Uralic languages, but for under-resourced languages, the applicability has yet to be tested.", "labels": [], "entities": [{"text": "acoustic modeling", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7817582786083221}]}, {"text": "For broadcast news data sets, the latest improvements for applying neural networks instead of GMM-based acoustic models have been in the range of 14% smaller relative word error rate (WER) for Finnish and 6% for Estonian ().", "labels": [], "entities": [{"text": "relative word error rate (WER)", "start_pos": 158, "end_pos": 188, "type": "METRIC", "confidence": 0.8273360303470066}]}, {"text": "In languages with a rich morphological structure it is difficult to build statistical language models using words.", "labels": [], "entities": []}, {"text": "If using n-gram word models, the vocabulary size becomes computationally challenging, and even worse, the growing lexicon decreases out-of-vocabulary (OOV) rate rather slowly.", "labels": [], "entities": [{"text": "out-of-vocabulary (OOV) rate", "start_pos": 132, "end_pos": 160, "type": "METRIC", "confidence": 0.927579915523529}]}, {"text": "Furthermore, the lack of data for underresourced languages makes building a large lexicon and n-gram difficult.", "labels": [], "entities": []}, {"text": "For Finnish, Estonian, Arabic and Turkish it is common to use subword units such as morphs) or syllables () instead of words.", "labels": [], "entities": []}, {"text": "In this work we follow this tradition and apply statistical morphs as subword units for Northern S\u00e1mi.", "labels": [], "entities": []}, {"text": "Because the pronunciation in Northern S\u00e1mi can be rather well covered by rules, a simple grapheme-to-phoneme conversion can be applied for our lexicon.", "labels": [], "entities": []}, {"text": "This gives Northern S\u00e1mi and other such languages a significant advantage in ASR, since building a proper lexicon is one of the most arduous data preparation tasks for speech recognition.", "labels": [], "entities": [{"text": "ASR", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9956919550895691}, {"text": "speech recognition", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.7907052636146545}]}, {"text": "We will use a popular open-source toolkit for speech recognition, Kaldi, and document the building of a speech recognizer.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.8238951563835144}, {"text": "speech recognizer", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.6719270646572113}]}, {"text": "In addition to DNN-based acoustic modeling, we test new methods of subword modeling for morphologically rich languages, originally developed for Finnish.", "labels": [], "entities": [{"text": "DNN-based acoustic modeling", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.7811401883761088}, {"text": "subword modeling", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.7046764492988586}]}, {"text": "The main focus of the paper is to demonstrate these new techniques in building anew baseline for Northern S\u00e1mi for further research and comparison.", "labels": [], "entities": []}, {"text": "We will compare our results to the previous Northern S\u00e1mi baseline results from.", "labels": [], "entities": [{"text": "Northern S\u00e1mi baseline", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.7127816677093506}]}], "datasetContent": [{"text": "We start by demonstrating the improvements obtained without DNNs by Kaldi and WFST-based decoding in relation to the AaltoASR and token-passing decoding.", "labels": [], "entities": [{"text": "AaltoASR", "start_pos": 117, "end_pos": 125, "type": "DATASET", "confidence": 0.8105477690696716}]}, {"text": "We continue by comparing different subword boundary markings and choose the overall best for the next experiments, where we compare different types of DNN architectures for acoustic modeling.", "labels": [], "entities": [{"text": "acoustic modeling", "start_pos": 173, "end_pos": 190, "type": "TASK", "confidence": 0.7486203610897064}]}, {"text": "Finally, we show the effects of increasing the size of the language model training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Lexicon and language model sizes for word models and subword models with  different boundary marking styles.", "labels": [], "entities": []}, {"text": " Table 4: Comparison between AaltoASR (Smit et al., 2016) and Kaldi with 10-gram LM  based on TRAIN+WIKI and 2.5h of audio for both speakers.", "labels": [], "entities": [{"text": "AaltoASR", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.7761034965515137}, {"text": "TRAIN+WIKI", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.8168720801671346}]}, {"text": " Table 5: Error Rates for different subword boundary markings. All models were  trained with the TRAIN+WIKI corpus and 2.5h of audio.", "labels": [], "entities": [{"text": "Error Rates", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9756491184234619}, {"text": "TRAIN", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.978648841381073}, {"text": "WIKI corpus", "start_pos": 103, "end_pos": 114, "type": "DATASET", "confidence": 0.773547500371933}]}, {"text": " Table 6: Error Rates between TRAIN+WIKI and the BIG language model. Same acous- tic data was used in all models. AaltoASR results are from Smit et al. (2016).", "labels": [], "entities": [{"text": "Error Rates", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.974995881319046}, {"text": "BIG language model", "start_pos": 49, "end_pos": 67, "type": "DATASET", "confidence": 0.7063274582227071}, {"text": "AaltoASR", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9844481945037842}]}, {"text": " Table 7: Error Rates between different boundary marking styles using the BIG lan- guage model. TDNN was used in all recognizers.", "labels": [], "entities": [{"text": "Error Rates", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9509182274341583}, {"text": "BIG", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.7787103056907654}]}]}