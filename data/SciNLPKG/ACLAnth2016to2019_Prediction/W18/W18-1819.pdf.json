{"title": [], "abstractContent": [{"text": "Tensor2Tensor is a library for deep learning models that is very well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.6706945598125458}]}, {"text": "1 Neural Machine Translation Background Machine translation using deep neural networks achieved great success with sequence-to-sequence models Sutskever et al.", "labels": [], "entities": [{"text": "Neural Machine Translation Background Machine translation", "start_pos": 2, "end_pos": 59, "type": "TASK", "confidence": 0.7198053201039633}]}, {"text": "(2014); Bahdanau et al.", "labels": [], "entities": []}, {"text": "(2014); Cho et al.", "labels": [], "entities": []}, {"text": "(2014) that used recurrent neural networks (RNNs) with LSTM cells Hochreiter and Schmidhuber (1997).", "labels": [], "entities": []}, {"text": "The basic sequence-to-sequence architecture is composed of an RNN encoder which reads the source sentence one token at a time and transforms it into a fixed-sized state vector.", "labels": [], "entities": []}, {"text": "This is followed by an RNN decoder, which generates the target sentence, one token at a time, from the state vector.", "labels": [], "entities": []}, {"text": "While a pure sequence-to-sequence recurrent neural network can already obtain good translation results Sutskever et al.", "labels": [], "entities": []}, {"text": "(2014); Cho et al.", "labels": [], "entities": []}, {"text": "(2014), it suffers from the fact that the whole input sentence needs to be encoded into a single fixed-size vector.", "labels": [], "entities": []}, {"text": "This clearly manifests itself in the degradation of translation quality on longer sentences and was partially overcome in Bahdanau et al.", "labels": [], "entities": []}, {"text": "(2014) by using a neural model of attention.", "labels": [], "entities": []}, {"text": "Convolutional architectures have been used to obtain good results in word-level neural machine translation starting from Kalchbrenner and Blunsom (2013) and later in Meng et al.", "labels": [], "entities": [{"text": "word-level neural machine translation", "start_pos": 69, "end_pos": 106, "type": "TASK", "confidence": 0.6487469971179962}]}, {"text": "These early models used a standard RNN on top of the convolution to generate the output, which creates a bottleneck and hurts performance.", "labels": [], "entities": []}, {"text": "Fully convolutional neural machine translation without this bottleneck was first achieved in Kaiser and Bengio (2016) and Kalchbrenner et al.", "labels": [], "entities": [{"text": "convolutional neural machine translation", "start_pos": 6, "end_pos": 46, "type": "TASK", "confidence": 0.7653855383396149}]}, {"text": "The model in Kaiser and Bengio (2016) (Extended Neural GPU) used a recurrent stack of gated convolutional layers, while the model in Kalchbrenner et al.", "labels": [], "entities": [{"text": "Extended Neural GPU)", "start_pos": 39, "end_pos": 59, "type": "DATASET", "confidence": 0.7576679289340973}]}, {"text": "(2016) (ByteNet) did away with recursion and used left-padded convolutions in the decoder.", "labels": [], "entities": []}, {"text": "This idea, introduced in WaveNet van den Oord et al.", "labels": [], "entities": []}, {"text": "(2016), significantly improves efficiency of the model.", "labels": [], "entities": []}, {"text": "The same technique was improved in a number of neural translation models recently, including Gehring et al.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7583059370517731}]}, {"text": "(2017) and Kaiser et al.", "labels": [], "entities": []}, {"text": "2 Self-Attention Instead of convolutions, one can use stacked self-attention layers.", "labels": [], "entities": []}, {"text": "This was introduced in the Transformer model Vaswani et al.", "labels": [], "entities": []}, {"text": "(2017) and has significantly improved state-of-the-art in machine translation and language modeling while also improving the speed of training.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.8223369121551514}, {"text": "language modeling", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.6827831864356995}]}, {"text": "Research continues in applying the model in more domains and exploring the space of self-attention mechanisms.", "labels": [], "entities": []}, {"text": "It is clear that self-attention is a powerful tool in general-purpose sequence mod-eling.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}