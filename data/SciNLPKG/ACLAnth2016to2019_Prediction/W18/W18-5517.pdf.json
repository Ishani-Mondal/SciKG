{"title": [{"text": "Team Papelo: Transformer Networks at FEVER", "labels": [], "entities": [{"text": "FEVER", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.45357444882392883}]}], "abstractContent": [{"text": "We develop a system for the FEVER fact extraction and verification challenge that uses a high precision entailment classifier based on transformer networks pretrained with language modeling, to classify abroad set of potential evidence.", "labels": [], "entities": [{"text": "FEVER fact extraction and verification challenge", "start_pos": 28, "end_pos": 76, "type": "TASK", "confidence": 0.80763179063797}]}, {"text": "The precision of the entail-ment classifier allows us to enhance recall by considering every statement from several articles to decide upon each claim.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9992245435714722}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9978718757629395}]}, {"text": "We include not only the articles best matching the claim text by TFIDF score, but read additional articles whose titles match named entities and capitalized expressions occurring in the claim text.", "labels": [], "entities": [{"text": "TFIDF score", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.9380240440368652}]}, {"text": "The entailment module evaluates potential evidence one statement at a time, together with the title of the page the evidence came from (providing a hint about possible pronoun antecedents).", "labels": [], "entities": []}, {"text": "In preliminary evaluation, the system achieves .5736 FEVER score, .6108 label accuracy, and .6485 evidence F1 on the FEVER shared task test set.", "labels": [], "entities": [{"text": "FEVER score", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9810804724693298}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.7885112166404724}, {"text": "evidence F1", "start_pos": 98, "end_pos": 109, "type": "METRIC", "confidence": 0.8564345836639404}, {"text": "FEVER shared task test set", "start_pos": 117, "end_pos": 143, "type": "DATASET", "confidence": 0.7222637832164764}]}], "introductionContent": [{"text": "The release of the FEVER fact extraction and verification dataset) provides a large-scale challenge that tests a combination of retrieval and textual entailment capabilities.", "labels": [], "entities": [{"text": "FEVER fact extraction", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.720725397268931}]}, {"text": "To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia.", "labels": [], "entities": []}, {"text": "Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem.", "labels": [], "entities": []}, {"text": "Systems are evaluated on the accuracy of the claim predictions, with credit only given when correct evidence is submitted.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.998970627784729}]}, {"text": "As entailment data, premises in FEVER data differ substantially from those in the image caption data used as the basis for the Stanford Natural Language Inference (SNLI) dataset.", "labels": [], "entities": [{"text": "FEVER data", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.70381960272789}, {"text": "Stanford Natural Language Inference (SNLI) dataset", "start_pos": 127, "end_pos": 177, "type": "DATASET", "confidence": 0.6001788899302483}]}, {"text": "Sentences are longer (31 compared to 14 words on average), vocabulary is more abstract, and the prevalence of named entities and out-ofvocabulary terms is higher.", "labels": [], "entities": []}, {"text": "The retrieval aspect of FEVER is not straightforward either.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.7565776705741882}]}, {"text": "A claim may have small word overlap with the relevant evidence, especially if the claim is refuted by the evidence.", "labels": [], "entities": []}, {"text": "Our approach to FEVER is to fix the most obvious shortcomings of the baseline approaches to retrieval and entailment, and to train a sharp entailment classifier that can be used to filter abroad set of retrieved potential evidence.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 16, "end_pos": 21, "type": "TASK", "confidence": 0.45146527886390686}]}, {"text": "For the entailment classifier we compare Decomposable Attention ( as implemented in the official baseline, ESIM, and a transformer network with pre-trained weights.", "labels": [], "entities": [{"text": "ESIM", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.7582956552505493}]}, {"text": "The transformer network naturally supports out-of-vocabulary words and gives substantially higher performance than the other methods.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Effect of adding titles to premises.", "labels": [], "entities": []}, {"text": " Table 2: Concatenating evidence or not.", "labels": [], "entities": []}, {"text": " Table 3: Percentage of evidence retrieved from first half of development set. Single-evidence claims only.", "labels": [], "entities": []}, {"text": " Table 4: FEVER Score of various systems. All use NE+Film retrieval.", "labels": [], "entities": [{"text": "FEVER Score", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9616106450557709}, {"text": "NE+Film retrieval", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.848186194896698}]}]}