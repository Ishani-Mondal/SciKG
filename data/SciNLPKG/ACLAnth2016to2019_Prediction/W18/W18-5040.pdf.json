{"title": [{"text": "Coherence Modeling Improves Implicit Discourse Relation Recognition", "labels": [], "entities": [{"text": "Coherence Modeling Improves Implicit Discourse Relation Recognition", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.9209578803607396}]}], "abstractContent": [{"text": "The research described in this paper examines how to learn linguistic knowledge associated with discourse relations from un-labeled corpora.", "labels": [], "entities": []}, {"text": "We introduce an unsu-pervised learning method on text coherence that could produce numerical representations that improve implicit discourse relation recognition in a semi-supervised manner.", "labels": [], "entities": [{"text": "discourse relation recognition", "start_pos": 131, "end_pos": 161, "type": "TASK", "confidence": 0.6233527064323425}]}, {"text": "We also empirically examine two variants of coherence modeling: order-oriented and topic-oriented negative sampling , showing that, of the two, topic-oriented negative sampling tends to be more effective.", "labels": [], "entities": []}], "introductionContent": [{"text": "Shallow discourse parsing aims to automatically identify discourse relations (e.g., comparisons) between adjacent sentences.", "labels": [], "entities": [{"text": "Shallow discourse parsing", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6973916292190552}]}, {"text": "When connectives such as however explicitly appear, discourse relations are relatively easy to classify, as connectives provide strong cues.", "labels": [], "entities": []}, {"text": "In contrast, it remains challenging to identify discourse relations across sentences that have no connectives.", "labels": [], "entities": []}, {"text": "One reason for this inferior performance is a shortage of labeled instances, despite the diversity of natural language discourses.", "labels": [], "entities": []}, {"text": "Collecting annotations about implicit relations is highly expensive because it requires linguistic expertise.", "labels": [], "entities": [{"text": "Collecting annotations about implicit relations", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8570339322090149}]}, {"text": "A variety of semi-supervised or unsupervised methods have been explored to alleviate this issue.", "labels": [], "entities": []}, {"text": "proposed generating synthetic instances by removing connectives from sentence pairs.", "labels": [], "entities": []}, {"text": "This idea has been extended in many works The Penn Discourse Treebank (PDTB) 2.0 corpus (, which is the current largest corpus for discourse relation recognition, contains only about 16K annotated instances in total. and remains a core approach in the field (.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB) 2.0 corpus", "start_pos": 46, "end_pos": 87, "type": "DATASET", "confidence": 0.9498907998204231}, {"text": "discourse relation recognition", "start_pos": 131, "end_pos": 161, "type": "TASK", "confidence": 0.6980926891167959}]}, {"text": "However, these methods rely on automatically detecting connectives in unlabeled corpora beforehand, which makes it almost impossible to utilize parts of unlabeled corpora in which no connectives appear.", "labels": [], "entities": []}, {"text": "In addition, as discovered, it is difficult to obtain a generalized model by training on synthetic data due to domain shifts.", "labels": [], "entities": []}, {"text": "Though several semi-supervised methods do not depend on detecting connectives, these methods are restricted to manually selected features, linear models, or word-level knowledge transfer.", "labels": [], "entities": [{"text": "word-level knowledge transfer", "start_pos": 157, "end_pos": 186, "type": "TASK", "confidence": 0.5863930185635885}]}, {"text": "In this paper, our research question is how to exploit unlabeled corpora without explicitly detecting connectives to learn linguistic knowledge associated with implicit discourse relations.", "labels": [], "entities": []}, {"text": "Our core hypothesis is that unsupervised learning about text coherence could produce numerical representations related to discourse relations.", "labels": [], "entities": []}, {"text": "Sentences that compose a coherent document should be connected with syntactic or semantic relations.", "labels": [], "entities": []}, {"text": "In particular, we expect that there should be latent relations among local sentences.", "labels": [], "entities": []}, {"text": "In this study, we hypothesize that parameters learned through coherence modeling could contain useful information for identifying (implicit) discourse relations.", "labels": [], "entities": []}, {"text": "To verify this hypothesis, we develop a semi-supervised system whose parameters are first optimized for coherence modeling and then transferred to implicit discourse relation recognition.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 147, "end_pos": 186, "type": "TASK", "confidence": 0.5788837894797325}]}, {"text": "We also empirically examine two variants of coherence mod-: An example of order-oriented and topic-oriented negative sampling in coherence modeling.", "labels": [], "entities": []}, {"text": "eling: (1) order-oriented negative sampling and (2) topic-oriented negative sampling.", "labels": [], "entities": []}, {"text": "An example is shown in.", "labels": [], "entities": []}, {"text": "Our experimental results demonstrate that coherence modeling improves Macro F 1 on implicit discourse relation recognition by about 3 points on first-level relation classes and by about 5 points on second-level relation types.", "labels": [], "entities": [{"text": "Macro F 1", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.6093636155128479}, {"text": "implicit discourse relation recognition", "start_pos": 83, "end_pos": 122, "type": "TASK", "confidence": 0.6040433645248413}]}, {"text": "Coherence modeling is particularly effective for relation categories with few labeled instances, such as temporal relations.", "labels": [], "entities": [{"text": "Coherence modeling", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9105824828147888}]}, {"text": "In addition, we find that topic-oriented negative sampling tends to be more effective than the order-oriented counterpart, especially on firstlevel relation classes.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The results of implicit discourse relation recognition (multi-class classification) and coher- ence modeling (binary classification). IRel and O/T-Coh denote that the model is trained on implicit  discourse relation recognition and order/topic-oriented coherence modeling respectively. \"Small\" and  \"large\" correspond to the relative size of the used unlabeled corpus: 37K (WSJ) and 22M (BLLIP)  positive instances, respectively.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 25, "end_pos": 64, "type": "TASK", "confidence": 0.574180968105793}, {"text": "IRel", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.932080090045929}, {"text": "discourse relation recognition", "start_pos": 207, "end_pos": 237, "type": "TASK", "confidence": 0.7041107614835104}, {"text": "BLLIP)  positive instances", "start_pos": 398, "end_pos": 424, "type": "METRIC", "confidence": 0.9188987463712692}]}, {"text": " Table 2: Comparison with previous works that  exploit unlabeled corpora on first-level relation  classes. An asterisk indicates that word embed- dings are fine-tuned (which slightly decreases per- formance on second-level relation types due to  overfitting).", "labels": [], "entities": []}, {"text": " Table 3: Results on one-vs.-others binary clas- sification in implicit discourse relation recogni- tion. The evaluation metric is Macro F 1 (%).  We evaluate on the first-level relation classes:  Expansion, Contingency, Comparison,  and Temporal.", "labels": [], "entities": [{"text": "Macro F 1", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.7816935976346334}]}]}