{"title": [{"text": "An Adaption of BIOASQ Question Answering dataset for Machine Reading systems by Manual Annotations of Answer Spans", "labels": [], "entities": [{"text": "BIOASQ Question Answering", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.6588631172974905}, {"text": "Machine Reading", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7353160679340363}]}], "abstractContent": [{"text": "BIOASQ Task B Phase B challenge focuses on extracting answers from snippets fora given question.", "labels": [], "entities": [{"text": "BIOASQ Task B Phase B challenge", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.6726556122303009}, {"text": "extracting answers from snippets fora given question", "start_pos": 43, "end_pos": 95, "type": "TASK", "confidence": 0.7847728899547032}]}, {"text": "The dataset provided by the organizers contains answers, but not all their variants.", "labels": [], "entities": []}, {"text": "Henceforth a manual annotation was performed to extract all forms of correct answers.", "labels": [], "entities": []}, {"text": "This article shows the impact of using all occurrences of correct answers for training on the evaluation scores which are improved significantly .", "labels": [], "entities": []}], "introductionContent": [{"text": "BIOASQ 1 challenge is a large-scale biomedical semantic indexing and question answering task ( which has been successful for 5 years.", "labels": [], "entities": [{"text": "BIOASQ 1 challenge", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.5979515314102173}, {"text": "biomedical semantic indexing and question answering task", "start_pos": 36, "end_pos": 92, "type": "TASK", "confidence": 0.7350326010159084}]}, {"text": "The challenge proposes several tasks using Biomedical data.", "labels": [], "entities": [{"text": "Biomedical data", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.7537911534309387}]}, {"text": "One of the tasks focuses on Biomedical question answering (Task B Phase B -we further refer it as B) where the goal is to extract answers fora given question from relevant snippets.", "labels": [], "entities": [{"text": "Biomedical question answering", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.8460651636123657}]}, {"text": "Several teams have participated actively, and a noticeable aspect is that the results of the task B are much lower compared to open domain QA evaluations, as in SQUAD 2 . Some reasons can be the low dataset size and the format of the answers provided by the organizers.", "labels": [], "entities": []}, {"text": "Bioasq provides only certain answer forms in the gold standard data and not all the variants of the answers in the given snippets.", "labels": [], "entities": [{"text": "Bioasq", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9316108822822571}, {"text": "gold standard data", "start_pos": 49, "end_pos": 67, "type": "DATASET", "confidence": 0.7998406092325846}]}, {"text": "In this paper, we study the influence of enriching the training data by manually annotated variants of gold standard answers on the evaluation performance.", "labels": [], "entities": []}, {"text": "We show the impact of the enriched data by experimenting on 5B and 6B training datasets.", "labels": [], "entities": []}, {"text": "Our method outperforms the best-1 http://bioasq.org/ 2 https://rajpurkar.github.io/SQuAD-explorer/ performing systems from Bioasq 5B by 7.3% on strict accuracy and 18% on lenient accuracy.", "labels": [], "entities": [{"text": "Bioasq 5B", "start_pos": 123, "end_pos": 132, "type": "DATASET", "confidence": 0.8330510258674622}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9071090817451477}, {"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9837797284126282}]}], "datasetContent": [{"text": "Bioasq 6 is the sixth challenge and the evaluation measures for Bioasq task B has always been the same.", "labels": [], "entities": [{"text": "Bioasq 6", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8322276473045349}]}, {"text": "Strict Accuracy, Lenient Accuracy and Mean Reciprocal Rank (MRR) are the 3 evaluation measures used.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.61416095495224}, {"text": "Lenient Accuracy", "start_pos": 17, "end_pos": 33, "type": "METRIC", "confidence": 0.6816000938415527}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 38, "end_pos": 64, "type": "METRIC", "confidence": 0.9771555662155151}]}, {"text": "To compute the scores, the exact match of strings between the predictions and the gold standard answers is used to decide if a system answer is correct.", "labels": [], "entities": []}, {"text": "Strict accuracy is the rate of top 1 exact answers.", "labels": [], "entities": [{"text": "Strict", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.928864598274231}, {"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.7562556266784668}]}, {"text": "Lenient accuracy is the rate of exact answers in top 5 predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.6897035241127014}]}, {"text": "MRR is the mean reciprocal rank computed on the top 5 system answers.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7849398255348206}]}, {"text": "These measures have been the same since the 1st challenge, although the first four challenges had triples and concepts along with snippets in the data.", "labels": [], "entities": []}, {"text": "In the last two challenges, only relevant snippets for questions are released.", "labels": [], "entities": []}, {"text": "Similar evaluations are performed in machine reading tasks like in SQUAD where top 1 accuracy and F1 scores are computed by comparing exact matching strings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9714013338088989}, {"text": "F1 scores", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9827569127082825}]}, {"text": "One main assumption in machine reading task is that the answer strings are substrings of the snippets, which implies that answers have to be extracted from the snippets.", "labels": [], "entities": [{"text": "machine reading task", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.842780868212382}]}, {"text": "In Bioasq, the answers are curated by human experts by analyzing the triples, concepts, and snippets (or paragraphs).", "labels": [], "entities": []}, {"text": "Thus, the Bioasq dataset and evaluation measures are very similar to that of machine reading task, but the major difference apart from the dataset size are the answers instances provided as gold standard which does not contain all the occurrences, abbreviations, different forms of answers which are present in the snippets.", "labels": [], "entities": [{"text": "Bioasq dataset", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.9107384085655212}]}, {"text": "In (), the authors transform Bioasq Phase B as a machine reading task with domain adaptation.", "labels": [], "entities": []}, {"text": "Gold standard answer strings and their offsets are automatically searched in the snippets for exact match and treated as answers if only they are found in the snippets, i.e., the answer string must be a substring of the snippet.", "labels": [], "entities": [{"text": "exact match", "start_pos": 94, "end_pos": 105, "type": "METRIC", "confidence": 0.8961665034294128}]}, {"text": "By doing so the dataset size is reduced to 65% of Bioasq 5 train set which was suitable for adaptation.", "labels": [], "entities": [{"text": "Bioasq 5 train set", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.9728375971317291}]}, {"text": "Other 35% of the questions did not have matching answers in the snippets, because of different variants of answers in the snippets, missing abbreviations, or irrelevant snippets.", "labels": [], "entities": []}, {"text": "This snippet annotation method can result in: \u2022 False positive: an answer mentioned in the snippet which does not answer the question.", "labels": [], "entities": [{"text": "False positive", "start_pos": 48, "end_pos": 62, "type": "METRIC", "confidence": 0.9723254442214966}]}, {"text": "\u2022 False negative: a snippet answers the question but does not have the exact string compared to the gold standard string.", "labels": [], "entities": [{"text": "False negative", "start_pos": 2, "end_pos": 16, "type": "METRIC", "confidence": 0.9732720851898193}]}, {"text": "We found that in Bioasq 6B training dataset for factoid questions, 205 out of 619 questions have false negative answers (33% of the dataset) which may result in some problems: \u2022 Less data for learning; \u2022 The model does not learn to extract all the variants; \u2022 Evaluation is done using such gold standard data which will lower the results even though the model is performing well.", "labels": [], "entities": [{"text": "Bioasq 6B training dataset", "start_pos": 17, "end_pos": 43, "type": "DATASET", "confidence": 0.8881211131811142}]}, {"text": "Below are some examples for which the answers returned from a reference system is correct (when evaluated manually) but the automatic evaluation classifies it as incorrect.", "labels": [], "entities": []}, {"text": "Q: Which calcium channels does ethosuximide target?", "labels": [], "entities": []}, {"text": "P: ..neuropathic pain is blocked by ethosuximide, known to block T-type calcium channels,..", "labels": [], "entities": []}, {"text": "The goal of our experiments is to study the impact of the data augmentation on training and evaluating a system.", "labels": [], "entities": []}, {"text": "Henceforth, we follow the process of (Wiese et al., 2017b) and use a machine reading model developed by) that is pre-trained on SQUAD dataset () for open domain questions and fine tuned to biomedical questions.", "labels": [], "entities": [{"text": "SQUAD dataset", "start_pos": 128, "end_pos": 141, "type": "DATASET", "confidence": 0.8685086667537689}]}, {"text": "To study the impact on the training process and the evaluations, we train the models using separately the automatically annotated data and the fully manually annotated data.", "labels": [], "entities": []}, {"text": "We also evaluate them using both kinds of data separately.", "labels": [], "entities": []}, {"text": "We perform fine-tuning on two datasets namely \u2022 BIOASQ 5B training set, which contains the 4B training data + the answers of the 4B test data -We term it as 5B.", "labels": [], "entities": [{"text": "BIOASQ 5B training set", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.7819049507379532}]}, {"text": "\u2022 BIOASQ 6B training set, which contains the 5B training data + the answers of the 5B test set -We term it as 6B.", "labels": [], "entities": [{"text": "BIOASQ 6B training set", "start_pos": 2, "end_pos": 24, "type": "DATASET", "confidence": 0.7647766023874283}]}, {"text": "We term the automatically annotated training data as Gold, and manually annotated training data as Anno.", "labels": [], "entities": []}, {"text": "The pre-trained model on open domain QA data is fine-tuned on the above listed Bioasq datasets separately.", "labels": [], "entities": [{"text": "Bioasq datasets", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.9719553291797638}]}, {"text": "Evaluation is performed by K-fold cross validation because of the small scale of the data, and on the official test sets of Bioasq 5B, which were separated from the training data while fine-tuning.", "labels": [], "entities": [{"text": "Bioasq 5B", "start_pos": 124, "end_pos": 133, "type": "DATASET", "confidence": 0.9014866352081299}]}, {"text": "The explanation of scores reported in table 2 and 3 along with the corresponding experiments on the datasets listed above, is as follows.", "labels": [], "entities": []}, {"text": "On the data of Trainset mentioned in the first row, we fine-tune it with Finetune data on the second rowwhich is Gold or Anno.", "labels": [], "entities": [{"text": "Finetune data", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.927810788154602}]}, {"text": "The official evaluation measures 5 using Gold or Anno.", "labels": [], "entities": [{"text": "Gold", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9922645092010498}, {"text": "Anno", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9609566926956177}]}, {"text": "version of the answers are highlighted in the third row.", "labels": [], "entities": []}, {"text": "The strict and lenient accuracies along with the MRR are reported.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9749967455863953}, {"text": "MRR", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.964813232421875}]}, {"text": "Gold version of 5B data contains 313 questions and Gold version of 6B data contains 428 questions.", "labels": [], "entities": [{"text": "6B data", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.8061728477478027}]}, {"text": "We consider the remaining questions with no matching answers as incorrectly answered, hence evaluating overall the questions of the datasets (5B -486 questions, 6B -618 questions).", "labels": [], "entities": []}, {"text": "Annotated 5B data contains 483 questions and 6B data contains 618 questions.", "labels": [], "entities": [{"text": "Annotated 5B data", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.737447182337443}, {"text": "6B data", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.7964864671230316}]}, {"text": "Overall results of 5B test sets presented in are evaluated on 150 questions from the test sets of 5B challenge whose gold standard answers are present in 6B challenge train set.", "labels": [], "entities": [{"text": "6B challenge train set", "start_pos": 154, "end_pos": 176, "type": "DATASET", "confidence": 0.816970944404602}]}, {"text": "To compare our scores with the ones reported in () and also since the size of the dataset is small, we perform K-Fold (5) evaluations which are reported in.", "labels": [], "entities": []}, {"text": "To compare with previously reported official test scores in Bioasq 5, we train on 5B training set and test on 5B test sets which are reported in.", "labels": [], "entities": [{"text": "Bioasq 5", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9242605566978455}]}], "tableCaptions": [{"text": " Table 2: K-fold evaluation on different train sets with Gold and Anno data. DeepQA scores are presented by  (Wiese et al., 2017a)", "labels": [], "entities": [{"text": "Gold", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9589077830314636}]}, {"text": " Table 3: Overall results calculated on official test sets from 5B task. Scores from (Wiese et al., 2017b) and Lab  Zhu, Fudan Univer are reported in Bioasq 5.", "labels": [], "entities": [{"text": "Fudan Univer", "start_pos": 121, "end_pos": 133, "type": "DATASET", "confidence": 0.9553979337215424}, {"text": "Bioasq 5", "start_pos": 150, "end_pos": 158, "type": "DATASET", "confidence": 0.8692954182624817}]}, {"text": " Table 4: MRR results calculated batchwise on 5B official test sets.", "labels": [], "entities": [{"text": "MRR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.4955427348613739}, {"text": "5B official test sets", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.7950569838285446}]}]}