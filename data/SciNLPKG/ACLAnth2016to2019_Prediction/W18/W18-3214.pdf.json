{"title": [{"text": "Bilingual Character Representation for Efficiently Addressing Out-of-Vocabulary Words in Code-Switching Named Entity Recognition", "labels": [], "entities": [{"text": "Bilingual Character Representation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7753520011901855}, {"text": "Efficiently Addressing Out-of-Vocabulary Words in Code-Switching Named Entity Recognition", "start_pos": 39, "end_pos": 128, "type": "TASK", "confidence": 0.5841429101096259}]}], "abstractContent": [{"text": "We propose an LSTM-based model with hierarchical architecture on named entity recognition from code-switching Twitter data.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.6778250336647034}]}, {"text": "Our model uses bilingual character representation and transfer learning to address out-of-vocabulary words.", "labels": [], "entities": [{"text": "bilingual character representation", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.6036014457543691}]}, {"text": "In order to mitigate data noise, we propose to use token replacement and normalization.", "labels": [], "entities": [{"text": "token replacement", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.8278533518314362}]}, {"text": "In the 3rd Workshop on Computational Approaches to Linguistic Code-Switching Shared Task, we achieved second place with 62.76% harmonic mean F1-score for English-Spanish language pair without using any gazetteer and knowledge-based information .", "labels": [], "entities": [{"text": "F1-score", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.559843122959137}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) predicts which word tokens refer to location, people, organization, time, and other entities from a word sequence.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER) predicts which word tokens refer to location, people, organization, time, and other entities from a word sequence", "start_pos": 0, "end_pos": 144, "type": "Description", "confidence": 0.7537762434394272}]}, {"text": "Deep neural network models have successfully achieved the state-of-the-art performance in NER tasks (Cohen;) using monolingual corpus.", "labels": [], "entities": []}, {"text": "However, learning from code-switching tweets data is very challenging due to several reasons: (1) words may have different semantics in different context and language, for instance, the word \"cola\" can be associated with product or \"queue\" in Spanish (2) data from social media are noisy, with many inconsistencies such as spelling mistakes, repetitions, and informalities which eventually points to Out-ofVocabulary (OOV) words issue (3) entities may appear in different language other than the matrix language.", "labels": [], "entities": []}, {"text": "For example \"todos los Domingos en Westland Mall\" where \"Westland Mall\" is an English named entity.", "labels": [], "entities": []}, {"text": "Our contributions are two-fold: (1) bilingual character bidirectional RNN is used to capture character-level information and tackle OOV words issue (2) we apply transfer learning from monolingual pre-trained word vectors to adapt the model with different domains in a bilingual setting.", "labels": [], "entities": []}, {"text": "In our model, we use LSTM to capture long-range dependencies of the word sequence and character sequence in bilingual character RNN.", "labels": [], "entities": []}, {"text": "In our experiments, we show the efficiency of our model in handling OOV words and bilingual word context.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiment, we use English-Spanish (ENG-SPA) Tweets data from Twitter provided by.", "labels": [], "entities": []}, {"text": "There are nine different named-entity labels.", "labels": [], "entities": []}, {"text": "The labels use IOB format (Inside, Outside, Beginning) where every token is labeled as B-label in the beginning and follows with I-label if it is inside a named entity, or O otherwise.", "labels": [], "entities": [{"text": "I-label", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.9227949976921082}]}, {"text": "For example \"Kendrick Lamar\" is represented as B-PER I-PER. and show the statistics of the dataset.", "labels": [], "entities": [{"text": "B-PER I-PER.", "start_pos": 47, "end_pos": 59, "type": "METRIC", "confidence": 0.9065651893615723}]}, {"text": "\"Person\", \"Location\", and \"Product\" are the most frequent entities in the dataset, and the least common ones are \"Time\", \"Event\", and \"Other\" categories.", "labels": [], "entities": []}, {"text": "'Other\" category is the least trivial among all because it is not well clustered like others.", "labels": [], "entities": []}, {"text": "We trained our LSTM models with a hidden size of 200.", "labels": [], "entities": []}, {"text": "We used batch size equals to 64.", "labels": [], "entities": []}, {"text": "The sentences were sorted by length in descending order.", "labels": [], "entities": []}, {"text": "Our embedding size is 300 for word and 150 for characters.) of 0.4 was applied to all LSTMs.", "labels": [], "entities": []}, {"text": "Adam Optimizer was chosen with an initial learning rate of 0.01.", "labels": [], "entities": []}, {"text": "We applied time-based decay of \u221a 2 decay rate and stop after two consecutive epochs without improvement.", "labels": [], "entities": []}, {"text": "We tuned our model with the development set and evaluated our best model with the test set using harmonic mean F1-score metric with the script provided by. shows the results for ENG-SPA tweets.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.7903974056243896}]}, {"text": "Adding pre-trained word vectors and characterlevel features improved the performance.", "labels": [], "entities": []}, {"text": "Interestingly, our initial attempts at adding character-level features did not improve the overall performance, until we apply dropout to the Char-RNN.", "labels": [], "entities": []}, {"text": "The performance of the model improves significantly after transfer learning with FastText word vectors while it also reduces the number of OOV words in the development and test set.", "labels": [], "entities": []}, {"text": "The margin between ours and first place model is small, approximately 1%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: OOV words rates on ENG-SPA dataset before and after preprocessing  Train  Dev  Test  All  Entity  All  Entity  All  Corpus  - - 18.91% 31.84% 49.39%  FastText (eng) (Mikolov et al., 2018) 62.62% 16.76% 19.12% 3.91% 54.59%  + FastText (spa) (Grave et al., 2018) 49.76% 12.38% 11.98% 3.91% 39.45%  + token replacement  12.43% 12.35% 7.18%  3.91%  9.60%  + token normalization  7.94% 8.38% 5.01% 1.67% 6.08%", "labels": [], "entities": [{"text": "ENG-SPA dataset", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.7862125337123871}, {"text": "token replacement", "start_pos": 308, "end_pos": 325, "type": "TASK", "confidence": 0.7699521481990814}]}, {"text": " Table 2: Data Statistics for ENG-SPA Tweets  Train  Dev  Test  # Words 616,069 9,583 183,011", "labels": [], "entities": [{"text": "ENG-SPA Tweets  Train  Dev  Test  # Words 616,069 9,583 183,011", "start_pos": 30, "end_pos": 93, "type": "DATASET", "confidence": 0.6750379145145416}]}, {"text": " Table 3: Entity Statistics for ENG-SPA Tweets  Entities Train Dev  # Person 4701  75  # Location 2810  10  # Product 1369  16  # Title  824  22  # Organization  811  9  # Group  718  4  # Time  577  6  # Event  232  4  # Other  324  6", "labels": [], "entities": []}, {"text": " Table 4: Results on ENG-SPA Dataset ( \u2021 result(s) from the shared task organizer (Aguilar et al., 2018)   \u2020 without token normalization)", "labels": [], "entities": [{"text": "ENG-SPA Dataset", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.781189888715744}]}]}