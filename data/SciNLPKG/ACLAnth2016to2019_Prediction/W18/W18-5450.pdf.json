{"title": [{"text": "Interpretable Structure Induction Via Sparse Attention", "labels": [], "entities": []}], "abstractContent": [], "introductionContent": [{"text": "Neural network methods are experiencing wide adoption in NLP, thanks to their empirical performance on many tasks.", "labels": [], "entities": []}, {"text": "Modern neural architectures go way beyond simple feedforward and recurrent models: they are complex pipelines that perform soft, differentiable computation instead of discrete logic.", "labels": [], "entities": []}, {"text": "Inspired by pioneering work by, e.g.;;, such modern differentiable architectures include neural memories () and attention mechanisms (.", "labels": [], "entities": []}, {"text": "The price of such soft computing is the introduction of dense dependencies, which make it hard to disentangle the patterns that trigger a prediction.", "labels": [], "entities": []}, {"text": "Our recent work on sparse and structured latent computation) presents a promising avenue for enhancing interpretability of such neural pipelines.", "labels": [], "entities": []}, {"text": "Through this extended abstract, we aim to discuss and explore the potential and impact of our methods.", "labels": [], "entities": []}, {"text": "The principle of parsimony suggests that simpler explanations are more plausible and interpretable.", "labels": [], "entities": []}, {"text": "Our perspective is similar to prior work on regularizing model weights (, but with a twist: instead of model sparsity that tells us which \"static\" groups of variables are relevant fora task, we now have a \"dynamic\" form of sparsity that tells us, fora particular input object, where we should attend to produce a decision.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}