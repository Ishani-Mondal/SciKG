{"title": [{"text": "The Task Matters: Comparing Image Captioning and Task-Based Dialogical Image Description", "labels": [], "entities": [{"text": "Comparing Image Captioning", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.6919117470582327}, {"text": "Task-Based Dialogical Image Description", "start_pos": 49, "end_pos": 88, "type": "TASK", "confidence": 0.610663503408432}]}], "abstractContent": [{"text": "Image captioning models are typically trained on data that is collected from people who are asked to describe an image, without being given any further task context.", "labels": [], "entities": [{"text": "Image captioning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7809862196445465}]}, {"text": "As we argue here, this context independence is likely to cause problems for transferring to task settings in which image description is bound by task demands.", "labels": [], "entities": []}, {"text": "We demonstrate that careful design of data collection is required to obtain image descriptions which are contextually bounded to a particular meta-level task.", "labels": [], "entities": []}, {"text": "As a task, we use MeetUp!, a text-based communication game where two players have the goal of finding each other in a visual environment.", "labels": [], "entities": []}, {"text": "To reach this goal, the players need to describe images representing their current location.", "labels": [], "entities": []}, {"text": "We analyse a dataset from this domain and show that the nature of image descriptions found in MeetUp! is diverse, dynamic and rich with phenomena that are not present in descriptions obtained through a simple image captioning task, which we ran for comparison.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic description generation from real-world images has emerged as a key task in vision & language in recent years (, and datasets like),) or Microsoft CoCo ( are typically considered to be general benchmarks for visual and linguistic image understanding.", "labels": [], "entities": [{"text": "Automatic description generation from real-world images", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.8313916822274526}, {"text": "visual and linguistic image understanding", "start_pos": 217, "end_pos": 258, "type": "TASK", "confidence": 0.8414190769195556}]}, {"text": "By exploiting these sizeable data collections and recent advances in computer vision (e.g. ConvNets, attention, etc.), image description models have achieved impressive performance, at least for indomain training and testing on existing benchmarks.", "labels": [], "entities": [{"text": "image description", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.677878275513649}]}, {"text": "Nevertheless, the actual linguistic definition and foundation of image description as a task remains unclear and is a matter of ongoing debate, e.g. see) fora conceptual discussion of the task from a cross-lingual perspective.", "labels": [], "entities": [{"text": "image description", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7306715101003647}]}, {"text": "According to, image description generation involves generating a textual description (typically a sentence) that verbalizes the most salient aspects of the image.", "labels": [], "entities": [{"text": "image description generation", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.7736876010894775}]}, {"text": "In practice, however, researchers have observed that eliciting descriptions from naive subjects (i.e. mostly crowd-workers) at a consistent level of quality is a non-trivial task, as workers seem to interpret the task in different ways.", "labels": [], "entities": []}, {"text": "Thus, previous works have developed relatively elaborate instructions and quality checking conventions for being able to systematically collect image descriptions.", "labels": [], "entities": []}, {"text": "In this paper, we argue that problems result from the fact that the task is typically put to the workers without providing any further context.", "labels": [], "entities": []}, {"text": "This entirely monological setting essentially suggests that determining the salient aspects of an image (like highly important objects, object properties, scene properties) can be solved in a general, \"neutral\" way, by humans and systems.", "labels": [], "entities": []}, {"text": "We present ongoing work on collecting image descriptions in task-oriented dialogue where descriptions are generated collaboratively by two players.", "labels": [], "entities": []}, {"text": "Importantly, in our setting (which we call the MeetUp!", "labels": [], "entities": [{"text": "MeetUp!", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.9389779269695282}]}, {"text": "environment), image descriptions serve the purpose of solving a higher-level task (meeting in a room, which in the game translates to determining whether an image that is seen is the same as the one that the partner sees).", "labels": [], "entities": []}, {"text": "Hence, our participants need not be instructed explicitly to produce image descriptions.", "labels": [], "entities": []}, {"text": "In this collaborative setting, we observe that the notion of saliency is non-static throughout a dialogue.", "labels": [], "entities": []}, {"text": "Depending on the history of the interaction, and the current state, speakers seem to flexibly adjust their descriptions (ranging from short scene descriptions to specific object descriptions) to achieve their common goal.", "labels": [], "entities": []}, {"text": "Moreover, the descriptions are more factual than those collected in a monological setting.", "labels": [], "entities": []}, {"text": "We believe that this opens up new perspectives for image captioning models, which can be trained on data that is bounded to its contextual use.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7284818589687347}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Excerpt from a dialogue involving image description (of the image shown in Figure 1)", "labels": [], "entities": [{"text": "image description", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7041666805744171}]}, {"text": " Table 2: Description of gameboards", "labels": [], "entities": []}, {"text": " Table 3: Analysis of image descriptions", "labels": [], "entities": []}, {"text": " Table 4: First 10 most frequent adjectives in both  MDs (left) and DDs (right)", "labels": [], "entities": []}]}