{"title": [{"text": "Identifying Explicit Discourse Connectives in German", "labels": [], "entities": [{"text": "Identifying Explicit Discourse Connectives", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.9057836979627609}]}], "abstractContent": [{"text": "We are working on an end-to-end Shallow Discourse Parsing system for German and in this paper focus on the first subtask: the identification of explicit connectives.", "labels": [], "entities": [{"text": "Shallow Discourse Parsing", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.6221194863319397}, {"text": "identification of explicit connectives", "start_pos": 126, "end_pos": 164, "type": "TASK", "confidence": 0.8472130596637726}]}, {"text": "Starting with the feature set from an En-glish system and a Random Forest classi-fier, we evaluate our approach on a (rel-atively small) German annotated corpus, the Potsdam Commentary Corpus.", "labels": [], "entities": [{"text": "Random Forest classi-fier", "start_pos": 60, "end_pos": 85, "type": "DATASET", "confidence": 0.8896651665369669}, {"text": "Potsdam Commentary Corpus", "start_pos": 166, "end_pos": 191, "type": "DATASET", "confidence": 0.9018638332684835}]}, {"text": "We introduce new features and experiment with including additional training data obtained through annotation projection and achieve an f-score of 83.89.", "labels": [], "entities": [{"text": "f-score", "start_pos": 135, "end_pos": 142, "type": "METRIC", "confidence": 0.9978086352348328}]}], "introductionContent": [{"text": "A task central to the field of Discourse Processing is the uncovering of coherence relations that hold between individual (elementary) units of a text.", "labels": [], "entities": []}, {"text": "When discourse relations are explicitly signaled in a text, the explicit markers are called (discourse) connectives.", "labels": [], "entities": []}, {"text": "Connectives can be two-way ambiguous in the sense of having either a discourse or a sentential reading, and if they have a discourse reading, many can assign multiple senses.", "labels": [], "entities": []}, {"text": "Further, connectives form a syntactically heterogeneous group and include coordinating and subordinating conjunctions, adverbials, and depending on the definition maintained, also certain prepositions.", "labels": [], "entities": []}, {"text": "In our experiments, we adopt the definition of where X is a connective if X cannot be inflected, the meaning of X is a two-place relation, the arguments of X are propositional structures and the expressions of the arguments of X can be sentential structures.", "labels": [], "entities": []}, {"text": "Following, we include prepositions that have a discourse function.", "labels": [], "entities": []}, {"text": "Recent approaches toward end-to-end shallow discourse parsing (SDP) have focused on a pipeline approach where the identification of discourse connectives is the first step, followed by the extraction of the arguments of the connective and the classification of the sense.", "labels": [], "entities": [{"text": "end-to-end shallow discourse parsing (SDP)", "start_pos": 25, "end_pos": 67, "type": "TASK", "confidence": 0.8285172922270638}, {"text": "identification of discourse connectives", "start_pos": 114, "end_pos": 153, "type": "TASK", "confidence": 0.7532147914171219}]}, {"text": "This pipeline architecture has dominated the CONLL 2015 1 and 2016 2 shared tasks on SDP.", "labels": [], "entities": [{"text": "CONLL 2015 1", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.8958210945129395}]}, {"text": "We will adopt it for our goal, viz.", "labels": [], "entities": []}, {"text": "developing an end-to-end discourse parser for German.", "labels": [], "entities": []}, {"text": "This paper focuses on the first step in the pipeline and introduces a connective identification module for German.", "labels": [], "entities": [{"text": "connective identification", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.773025244474411}]}, {"text": "We train a classifier using annotated data (Section 3), investigate and extend the feature set (Section 4), discuss and evaluate the results (Section 5) and summarize in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results for the different setups are illustrated in.", "labels": [], "entities": []}, {"text": "We use a micro-averaged f1 score for all experiments.", "labels": [], "entities": []}, {"text": "We compare performance of the classifier to a majority vote baseline, where each instance is assigned its most frequent label.", "labels": [], "entities": []}, {"text": "Using the base feature set results in an f-score of 81.90 (second row of).", "labels": [], "entities": [{"text": "f-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9942195415496826}]}, {"text": "Using extra training data generated through annotation projection on EuroParl yields a negative result (below the baseline) and fscore decreases considerably, to 65.98 (third row).", "labels": [], "entities": [{"text": "EuroParl", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.9870741367340088}, {"text": "fscore", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.999466598033905}]}, {"text": "This decrease can be explained by the susceptibility of this approach to error propagation.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7055450826883316}]}, {"text": "The English classifier, trained on the PDTB (f-score of 93.64) is applied to another domain (EuroParl), word-alignments introduce errors, and the additional German training data is again from another domain (EuroParl) than the test set (news commentary).", "labels": [], "entities": [{"text": "PDTB", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.9179952144622803}, {"text": "f-score", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9551812410354614}, {"text": "EuroParl", "start_pos": 93, "end_pos": 101, "type": "DATASET", "confidence": 0.9683150053024292}, {"text": "EuroParl", "start_pos": 208, "end_pos": 216, "type": "DATASET", "confidence": 0.9827118515968323}]}, {"text": "The extra training data obtained in this way (18,853 instances) apparently does not compensate for this.", "labels": [], "entities": []}, {"text": "We note that the scores resulting from annotation projection data are comparable to the f-score of 68.7 reported by.", "labels": [], "entities": [{"text": "f-score", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9862577319145203}]}, {"text": "This may suggest an upper-limit in performance when using data obtained through annotation projection, but more research is needed to verify this.", "labels": [], "entities": []}, {"text": "Since the PCC has gold annotations for syntax trees, we used these for part-of-speech tag and other syntactic features, in order to establish the impact of parsing errors.", "labels": [], "entities": [{"text": "PCC", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.9682804942131042}]}, {"text": "As shown in the first row, this mainly impacts precision and leads to an increase of almost 5 points for the f-score (using the base feature set).", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9996623992919922}]}, {"text": "However, because having access to gold parses is not feasible in an end-to-end scenario, we consider this an estimation of the impact of parsing errors and continue using automatically generated parse trees for the other experiments.", "labels": [], "entities": []}, {"text": "The best results were obtained using the extended feature set (see Section 4) and are displayed in the last row of.", "labels": [], "entities": []}, {"text": "Inspecting the individual scores, we found that in particular 'auch' ('also') and 'als' ('as/than') were difficult to classify (with f-scores of 27.03 and 28.57, respectively), despite being relatively frequent (208 and 147 examples in the PCC).", "labels": [], "entities": [{"text": "f-scores", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9573875665664673}, {"text": "PCC", "start_pos": 240, "end_pos": 243, "type": "DATASET", "confidence": 0.9620197415351868}]}, {"text": "Although they are not connectives in the majority of cases (ratios of 0.13 ('auch') and 0.08 ('als')), some connectives with similar ratios yet significantly lower frequencies have higher f-scores, such as 'so' ('so/thus'); frequency of 108, ratio of 0.11 and f-score of 72.00) and 'damit' ('in order to/thereby'); frequency of 30, ratio of 0.19 and f-score of 60.00.", "labels": [], "entities": [{"text": "frequency", "start_pos": 315, "end_pos": 324, "type": "METRIC", "confidence": 0.956670880317688}, {"text": "f-score", "start_pos": 350, "end_pos": 357, "type": "METRIC", "confidence": 0.9825238585472107}]}, {"text": "When using separate classifiers for the different syntactic categories (a setup which did not result in improved performance), the conjunctions performed best (with 91.81 for coordinating and 90.25 for subordinating conjunctions) and prepositions worst (51.55), but groupinternally the differences were equally large, with some prepositions having above-average scores and some having scores close to 0.", "labels": [], "entities": []}, {"text": "Further attempts at increasing the overall f-score quickly led to looking into solutions for individual connectives and came with the risk of over-fitting to the data set.", "labels": [], "entities": [{"text": "f-score", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9012282490730286}]}, {"text": "To put our score for German into perspective, we performed a set of experiments with different amounts of training data for English.", "labels": [], "entities": []}, {"text": "shows the f-score (y-axis) when gradually increasing the number of training instances (x-axis).: Results for binary connective classification on PCC for gold trees and automatically generated trees blue line represents the curve for English, starting with 1,000 instances randomly sampled from the total of 278k instances in the 2016 CONLL shared task data.", "labels": [], "entities": [{"text": "binary connective classification", "start_pos": 109, "end_pos": 141, "type": "TASK", "confidence": 0.6532983779907227}, {"text": "CONLL shared task data", "start_pos": 334, "end_pos": 356, "type": "DATASET", "confidence": 0.8535397946834564}]}, {"text": "Recall that using this full set, the f-score using the same feature set and classification algorithm (RandomForest) is 93.64.", "labels": [], "entities": []}, {"text": "The orange triangle represents performance for German, using all available instances from the PCC.", "labels": [], "entities": [{"text": "PCC", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.9644601345062256}]}, {"text": "While we have no explanation for the dentin the curve at 10,000 instances (and the smaller one around 20,000), we focus on the German score and note that with 81.90, this is 1.8 points higher than the corresponding score for English (80.09).", "labels": [], "entities": [{"text": "German score", "start_pos": 127, "end_pos": 139, "type": "DATASET", "confidence": 0.7679378390312195}]}, {"text": "This comparison suggests that the problem of connective identification is not significantly more or less challenging for German than it is for English.", "labels": [], "entities": [{"text": "connective identification", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.8824505507946014}]}, {"text": "In fact, seeing that we also include the syntactic category of prepositions (which is not included in the PDTB connectives), and this group scored the worst in our separate-classifier setup, it suggests that for the other categories, performance is better for German than it is for English.", "labels": [], "entities": []}, {"text": "When leaving out prepositions altogether, f-score increased to 85.99.", "labels": [], "entities": [{"text": "f-score", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9866870641708374}]}, {"text": "But because it was a conscious decision to include prepositions, the most straightforward means of improving performance for the problem at hand seems to be adding more (in-domain) training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for binary connective classification on PCC for gold trees and automatically generated  trees", "labels": [], "entities": [{"text": "binary connective classification", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.5995992521444956}]}]}