{"title": [{"text": "Whitepaper on NEWS 2018 Shared Task on Machine Transliteration", "labels": [], "entities": []}], "abstractContent": [{"text": "Transliteration is defined as the phonetic translation of names across languages.", "labels": [], "entities": [{"text": "phonetic translation of names across languages", "start_pos": 34, "end_pos": 80, "type": "TASK", "confidence": 0.8342844843864441}]}, {"text": "Transliteration of Named Entities (NEs) is a necessary subtask in many applications , such as machine translation, corpus alignment, cross-language IR, information extraction and automatic lexicon acquisition.", "labels": [], "entities": [{"text": "Transliteration of Named Entities (NEs)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.759126569543566}, {"text": "machine translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.8157480657100677}, {"text": "corpus alignment", "start_pos": 115, "end_pos": 131, "type": "TASK", "confidence": 0.7782467007637024}, {"text": "cross-language IR", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.5751843452453613}, {"text": "information extraction", "start_pos": 152, "end_pos": 174, "type": "TASK", "confidence": 0.8439887166023254}, {"text": "automatic lexicon acquisition", "start_pos": 179, "end_pos": 208, "type": "TASK", "confidence": 0.6433931787808737}]}, {"text": "All such systems call for high-performance transliteration, which is the focus of the shared task in NEWS 2018.", "labels": [], "entities": [{"text": "NEWS 2018", "start_pos": 101, "end_pos": 110, "type": "DATASET", "confidence": 0.9008264243602753}]}], "introductionContent": [], "datasetContent": [{"text": "Training Data (Parallel) Paired names between source and target languages; size 3K -41K.", "labels": [], "entities": []}, {"text": "Training data is used for training a basic transliteration system.", "labels": [], "entities": []}, {"text": "As in previous editions of the shared task, the quality of the submitted results will be evaluated by using the following 4 metrics.", "labels": [], "entities": []}, {"text": "Each individual name result might include up to 10 output candidates in a ranked list.", "labels": [], "entities": []}, {"text": "Since a given source name may have multiple correct target transliterations, all these alternatives are treated equally in the evaluation.", "labels": [], "entities": []}, {"text": "That is, any of these alternatives are considered as a correct transliteration, and the first correct transliteration in the ranked list is accepted as a correct hit.", "labels": [], "entities": []}, {"text": "The following notation is further assumed: N : Total number of names (source words) in the test set.", "labels": [], "entities": []}, {"text": "n i : Number of reference transliterations for i-th name in the test set (n i \u2265 1).", "labels": [], "entities": []}, {"text": "r i,j : j-th reference transliteration for i-th name in the test set.", "labels": [], "entities": []}, {"text": "c i,k : k-th candidate transliteration (system output) for i-th name in the test set (1 \u2264 k \u2264 10).", "labels": [], "entities": []}, {"text": "K i : Number of candidate transliterations produced by a transliteration system.", "labels": [], "entities": []}, {"text": "1. Word Accuracy in Top-1 (ACC) Also known as Word Error Rate.", "labels": [], "entities": [{"text": "Word Accuracy in Top-1 (ACC)", "start_pos": 3, "end_pos": 31, "type": "METRIC", "confidence": 0.6582485863140651}, {"text": "Word Error Rate", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.5852585136890411}]}, {"text": "It measures correctness of the first transliteration candidate in the candidate list produced by a transliteration system.", "labels": [], "entities": []}, {"text": "ACC = 1 means that all top candidates are correct transliterations i.e. they match one of the references, and ACC = 0 means that none of the top candidates are correct.", "labels": [], "entities": [{"text": "ACC", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.930371880531311}, {"text": "ACC = 0", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.9591849644978842}]}, {"text": "2. Fuzziness in Top-1 (Mean F-score) The mean F-score measures how different, on average, the top transliteration candidate is from its closest reference.", "labels": [], "entities": [{"text": "Fuzziness", "start_pos": 3, "end_pos": 12, "type": "METRIC", "confidence": 0.9588279128074646}, {"text": "Mean F-score)", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.814581036567688}, {"text": "F-score", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9304600358009338}]}, {"text": "F-score for each source word is a function of Precision and Recall and equals 1 when the top candidate matches one of the references, and 0 when there are no common characters between the candidate and any of the references.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9893835186958313}, {"text": "Precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9984169006347656}, {"text": "Recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9939841628074646}]}, {"text": "Precision and Recall are calculated based on the length of the Longest Common Subsequence between a candidate and a reference: where ED is the edit distance and |x| is the length of x.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9911085367202759}, {"text": "Recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9953248500823975}, {"text": "ED", "start_pos": 133, "end_pos": 135, "type": "METRIC", "confidence": 0.9869146943092346}]}, {"text": "For example, the longest common subsequence between \"abcd\" and \"afcde\" is \"acd\" and its length is 3.", "labels": [], "entities": []}, {"text": "The best matching reference, that is, the reference for which the edit distance has the minimum value, is taken for calculation.", "labels": [], "entities": []}, {"text": "If the best matching reference is given by then Recall, Precision and F-score for i-th word are calculated as follows: \u2022 The length is computed in distinct Unicode characters.", "labels": [], "entities": [{"text": "Recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9987446069717407}, {"text": "Precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9993624091148376}, {"text": "F-score", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9984821677207947}]}, {"text": "\u2022 No distinction is made among different character types of a language (e.g. vowel vs. consonants vs. combining diereses etc.)", "labels": [], "entities": []}, {"text": "3. Mean Reciprocal Rank (MRR) Measures traditional MRR for any right answer produced by the system, from among the candidates.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 3, "end_pos": 29, "type": "METRIC", "confidence": 0.9632169604301453}, {"text": "MRR", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.7460805177688599}]}, {"text": "1/M RR tells approximately the average rank of the correct transliteration.", "labels": [], "entities": [{"text": "RR", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.7214033007621765}]}, {"text": "MRR closer to 1 implies that the correct answer is mostly produced close to the top of the n-best lists.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9521684646606445}]}, {"text": "4. MAP ref Measures tightly the precision in the n-best candidates for i-th source name, for which reference transliterations are available.", "labels": [], "entities": [{"text": "MAP", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.8660261631011963}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9992367029190063}]}, {"text": "If all of the references are produced, then the MAP is 1.", "labels": [], "entities": [{"text": "MAP", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9895591735839844}]}, {"text": "Let's denote the number of correct candidates for the i-th source word in k-best list as num(i, k).", "labels": [], "entities": []}, {"text": "MAP ref is then given by All data will be made available in XML formats as illustrated in.", "labels": [], "entities": [{"text": "MAP", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.6428075432777405}]}, {"text": "\u2022 Data Encoding Formats: The data will be in Unicode UTF-8 encoding files without byte-order mark, and in the XML format specified.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Source and target languages for the shared task on transliteration.", "labels": [], "entities": []}]}