{"title": [{"text": "T \u00a8 ubingen-Oslo Team at the VarDial 2018 Evaluation Campaign: An Analysis of N-gram Features in Language Variety Identification", "labels": [], "entities": [{"text": "T \u00a8 ubingen-Oslo Team at the VarDial 2018 Evaluation Campaign", "start_pos": 0, "end_pos": 61, "type": "DATASET", "confidence": 0.7658830612897873}, {"text": "Language Variety Identification", "start_pos": 97, "end_pos": 128, "type": "TASK", "confidence": 0.6673763493696848}]}], "abstractContent": [{"text": "This paper describes our systems for the VarDial 2018 evaluation campaign.", "labels": [], "entities": [{"text": "VarDial 2018 evaluation campaign", "start_pos": 41, "end_pos": 73, "type": "DATASET", "confidence": 0.7099701762199402}]}, {"text": "We participated in all language identification tasks, namely, Arabic dialect identification (ADI), German dialect identification (GDI), Discriminating between Dutch and Flemish in Subtitles (DFS), and Indo-Aryan Language Identification (ILI).", "labels": [], "entities": [{"text": "language identification", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.7506861090660095}, {"text": "Arabic dialect identification (ADI)", "start_pos": 62, "end_pos": 97, "type": "TASK", "confidence": 0.8207162618637085}, {"text": "German dialect identification (GDI)", "start_pos": 99, "end_pos": 134, "type": "TASK", "confidence": 0.7030089298884074}, {"text": "Discriminating between Dutch and Flemish in Subtitles (DFS)", "start_pos": 136, "end_pos": 195, "type": "TASK", "confidence": 0.7943651616573334}, {"text": "Indo-Aryan Language Identification (ILI)", "start_pos": 201, "end_pos": 241, "type": "TASK", "confidence": 0.7793656289577484}]}, {"text": "In all of the tasks, we only used textual transcripts (not using audio features for ADI).", "labels": [], "entities": []}, {"text": "We submitted system runs based on support vector machine classifiers (SVMs) with bag of character and word n-grams as features, and gated bidirectional recurrent neural networks (RNNs) using units of characters and words.", "labels": [], "entities": []}, {"text": "Our SVM models outperformed our RNN models in all tasks, obtaining the first place on the DFS task, third place on the ADI task, and second place on others according to the official rankings.", "labels": [], "entities": []}, {"text": "As well as describing the models we used in the shared task participation, we present an analysis of the n-gram features used by the SVM models in each task, and also report additional results (that were run after the official competition deadline) on the GDI surprise dialect track.", "labels": [], "entities": [{"text": "GDI surprise dialect track", "start_pos": 256, "end_pos": 282, "type": "DATASET", "confidence": 0.8141502887010574}]}], "introductionContent": [{"text": "Identifying the language of a text or speech is an important step for many (multi-lingual) natural language processing applications.", "labels": [], "entities": [{"text": "Identifying the language of a text or speech", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8988512381911278}]}, {"text": "At least for written text, the language identification is a 'mostly-solved' problem.", "labels": [], "entities": [{"text": "language identification", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7152771204710007}]}, {"text": "High accuracy values can be obtained with relatively simple machine learning models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9977356195449829}]}, {"text": "One challenging issue, however, is identifying closely related languages or dialects, which is an interesting research question as well as being relevant to practical NLP applications.", "labels": [], "entities": []}, {"text": "The series of VarDial evaluation campaigns) included tasks of identifying closely related languages and dialects from written or spoken language data.", "labels": [], "entities": []}, {"text": "This paper is a description of our efforts in the VarDial 2018 shared task, which featured four dialect/language identification tasks, as well as one morphosyntactic tagging task.", "labels": [], "entities": [{"text": "VarDial 2018 shared task", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6487298607826233}, {"text": "dialect/language identification tasks", "start_pos": 96, "end_pos": 133, "type": "TASK", "confidence": 0.7006286442279815}]}, {"text": "We only participated in the language identification tasks.", "labels": [], "entities": [{"text": "language identification tasks", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.8514819542566935}]}, {"text": "The aim of the ADI task, which was also part of the earlier two VarDial evaluation campaigns, is to recognize the five varieties of Arabic (Egyptian, Gulf, Levantine, North-African, and Modern Standard Arabic) from spoken language samples.", "labels": [], "entities": []}, {"text": "The task provides transcribed text as well as pre-extracted audio features and raw audio recordings.", "labels": [], "entities": []}, {"text": "The DFS task, introduced this year, is on discriminating Dutch and Flemish subtitles.", "labels": [], "entities": [{"text": "DFS task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.4985168129205704}]}, {"text": "The GDI is another task that was present in earlier VarDial shared tasks, where the aim is to identify four Swiss German Dialects (Basel, Bern, Lucerne, Zurich).", "labels": [], "entities": [{"text": "GDI", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.88074791431427}]}, {"text": "This year's edition also included a surprise dialect.", "labels": [], "entities": []}, {"text": "Finally, the ILI task, another newcomer, is about identifying five closely related Indo-Aryan languages.", "labels": [], "entities": []}, {"text": "A simple approach to language/dialect identification is to treat it as a text (or document) classification task.", "labels": [], "entities": [{"text": "language/dialect identification", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6261607855558395}, {"text": "text (or document) classification task", "start_pos": 73, "end_pos": 111, "type": "TASK", "confidence": 0.7479027083941868}]}, {"text": "Two well-known methods for solving this task are linear classifiers with bag-of-n-gram representations, and, recently popularized, recurrent neural networks.", "labels": [], "entities": []}, {"text": "As in our participation at earlier VarDial evaluation campaigns (C \u00b8 \u00a8 oltekin and Rama, 2016; C \u00b8 \u00a8 oltekin and Rama, 2017), we experiment with both the methods.", "labels": [], "entities": []}, {"text": "Although our main participation is based on the SVM models, we also report the performance of the RNN models for comparison, and provide further analyses regarding the feature sets used in the SVM models.", "labels": [], "entities": []}, {"text": "We outline our approach in the next section, describing the models used briefly and explaining the strategies we used for the GDI surprise dialect track.", "labels": [], "entities": [{"text": "GDI surprise dialect track", "start_pos": 126, "end_pos": 152, "type": "TASK", "confidence": 0.6727079749107361}]}, {"text": "Section 3 introduces the data, explains the experimental procedure used, and presents the analyses and results from the experiments run during and after the shared task.", "labels": [], "entities": []}, {"text": "After a general discussion in Section 4, Section 5 concludes with pointers to potential future improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "For all results submitted, we combined the training and development sets, and tuned the hyperparameters of the models with 5-fold cross validation.", "labels": [], "entities": []}, {"text": "For the bag-of-n-grams models, we used an exhaustive grid search over the range of hyperparameter settings.", "labels": [], "entities": []}, {"text": "For the RNN models, we used random search, since the RNN models required higher run times, and a full grid search was not feasible.", "labels": [], "entities": []}, {"text": "The ranges of parameter values used during random or grid search are listed in.", "labels": [], "entities": []}, {"text": "The bag-of-n-gram features always include all n-gram sizes from unigrams to the n-grams of the specified order for the systems used in the shared participation.", "labels": [], "entities": []}, {"text": "We use the same set of parameters for the backward and forward RNNs in our bidirectional RNN models.", "labels": [], "entities": []}, {"text": "The random search is run for approximately 40 different parameter settings for each data set (task).", "labels": [], "entities": []}, {"text": "presents the parameter settings that yielded best average macro-averaged F 1 score based on 5-fold cross validation on the combined training and development sets.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9484924475351969}]}, {"text": "Although we report these values for the purpose of reproducibility, a rather large range of values result in similar performance scores.", "labels": [], "entities": []}, {"text": "The distributions of F 1 scores for both models are presented in.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9635157783826193}]}, {"text": "The central tendency inbox plots presented in indicate that the F 1 score for many parameter settings for the SVM models are close to the top score.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9838759700457255}]}, {"text": "Hence, a large range of 'reasonable' hyperparameter settings yield the scores similar to the top performing setting.", "labels": [], "entities": []}, {"text": "The scores of the RNNs distributed more evenly (symmetrically), since they include a smaller number of randomly selected hyperparameter configurations.", "labels": [], "entities": []}, {"text": "We tune the hyperparameters which interact, for instance, both decreasing C and increasing min df may reduce overfitting.", "labels": [], "entities": []}, {"text": "As a result, it is difficult to observe global trends of hyperparameter settings that yields better performance in a particular task.", "labels": [], "entities": []}, {"text": "However, in general, frequency thresholds seems to hurt systems' performances.", "labels": [], "entities": []}, {"text": "The best performing hyperparameter settings used all features regardless of their frequencies (hence, frequency thresholds are not shown in).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. The DFS data set has a perfectly balanced label distribution, while the other data sets show  slight label imbalance. The reader is referred to the shared task description paper (", "labels": [], "entities": [{"text": "DFS data set", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.9554417133331299}]}, {"text": " Table 1: The number of instances in the training (train), development (dev), and test sets followed by  the length of texts in each data set. The text-length statistics are calculated on combined training and  development sets.", "labels": [], "entities": []}, {"text": " Table 2: The range of values used for hyper-parameter search for the SVM and the RNN models. Case  normalization only applies to the DFS data set. The last row corresponds to 4 separate parameters (used  both for forward-and backward-RNNs).", "labels": [], "entities": [{"text": "DFS data set", "start_pos": 134, "end_pos": 146, "type": "DATASET", "confidence": 0.9641838868459066}]}, {"text": " Table 3: Best parameters for both SVM and RNN models tuned with 5-fold cross validation on the  combined training and development data. The abbreviations for the parameters are explained in Table 2.", "labels": [], "entities": []}, {"text": " Table 5: The best F1-scores obtained during tuning with k-fold CV and using development set, as well  as the official score. Clearly, there is a discrepancy between the full training set (k-fold results) and the  test set for ADI and GDI. In the case of DFS, the development set seems to be closer to the training set.  For ILI, the extent of the discrepancy seems to be smaller, but also development set is not necessarily  closer to the test set than the training set.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9983659386634827}]}, {"text": " Table 6: Best macro-averaged F 1 scores obtained on the combined training/development set with 5-fold  cross validation with the SVM model. The column 'c' indicates the maximum character n-gram size and  the column 'w' indicates maximum word n-gram size that yielded the corresponding score.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.8511915802955627}]}, {"text": " Table 7: Number of predictions for the surprise dialect 'XY' and macro-averaged precision, recall and  F1-score of the SVM model for different decision criteria for predicting XY. We calculated the scores on  the full gold-standard test set including the surprise dialect. The test set contains 790 XY samples out of  5542 samples total.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9630721807479858}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9276686310768127}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9992496371269226}, {"text": "F1-score", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9994235038757324}]}]}