{"title": [{"text": "BrainT at IEST 2018: Fine-tuning Multiclass Perceptron For Implicit Emotion Classification", "labels": [], "entities": [{"text": "BrainT at IEST 2018", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.7975177019834518}, {"text": "Implicit Emotion Classification", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.716032882531484}]}], "abstractContent": [{"text": "We present BrainT, a multi-class, averaged perceptron tested on implicit emotion prediction of tweets.", "labels": [], "entities": [{"text": "implicit emotion prediction of tweets", "start_pos": 64, "end_pos": 101, "type": "TASK", "confidence": 0.7692322850227356}]}, {"text": "We show that the dataset is linearly separable and explore ways in fine-tuning the baseline classifier.", "labels": [], "entities": []}, {"text": "Our results indicate that the bag-of-words features benefit the model moderately and prediction can be improved with bigrams, trigrams, skip-one-tetra-grams and POS-tags.", "labels": [], "entities": [{"text": "prediction", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9743497967720032}]}, {"text": "Furthermore, we find preprocessing of the n-grams, including stemming , lowercasing, stopword filtering, emoji and emoticon conversion generally not useful.", "labels": [], "entities": [{"text": "emoticon conversion", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7289387881755829}]}, {"text": "The model is trained on an annotated corpus of 153,383 tweets and predictions on the test data were submitted to the WASSA-2018 Implicit Emotion Shared Task.", "labels": [], "entities": [{"text": "WASSA-2018 Implicit Emotion Shared Task", "start_pos": 117, "end_pos": 156, "type": "TASK", "confidence": 0.5458556652069092}]}, {"text": "BrainT 1 attained a Macro F-score of 0.63.", "labels": [], "entities": [{"text": "BrainT 1", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.91339111328125}, {"text": "F-score", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9208782911300659}]}], "introductionContent": [{"text": "Our task is to predict emotions of tweets in a dataset where words explicitly mentioning the emotion are masked.", "labels": [], "entities": []}, {"text": "Following the definition of Ekman (1992), there are six \"basic\" emotions, these tweets have the labels joy, fear, surprise, disgust, anger or sadness.", "labels": [], "entities": []}, {"text": "As the model has no access to the explicit emotion word, it has to detect it from its implicit context, i.e. the situational or causal description of the event.", "labels": [], "entities": []}, {"text": "This aspect of the task makes it comparable to centre word prediction from context words.", "labels": [], "entities": [{"text": "centre word prediction from context words", "start_pos": 47, "end_pos": 88, "type": "TASK", "confidence": 0.7413065433502197}]}, {"text": "Twitter language distinguishes itself by a heterogeneous variety of internet vernaculars, abundance of abbreviations, emojis, hashtags and deviation from conventional spelling, grammar, syntax and lexicon.", "labels": [], "entities": []}, {"text": "This makes recognition of emotions intricate even for human readers as evident from the noticeably low inter-annotator agreement reported by or the  \"testing\" of the IEST dataset on English nativespeakers which resulted in an F-score of 0.45 ().", "labels": [], "entities": [{"text": "recognition of emotions", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.8875129818916321}, {"text": "IEST dataset on English nativespeakers", "start_pos": 166, "end_pos": 204, "type": "DATASET", "confidence": 0.7506998717784882}, {"text": "F-score", "start_pos": 226, "end_pos": 233, "type": "METRIC", "confidence": 0.999258816242218}]}], "datasetContent": [{"text": "The dataset we use is provided by the WASSA 2018 Implicit Emotion Shared Task . It is a corpus of 153,383 tweets annotated with distant supervision where each tweet originally contained one of the six emotion words (joy, fear, surprise, disgust, anger, sadness) or their synonyms.", "labels": [], "entities": [{"text": "WASSA 2018 Implicit Emotion Shared Task", "start_pos": 38, "end_pos": 77, "type": "TASK", "confidence": 0.5286310017108917}]}, {"text": "These words are masked in the dataset, as are usernames and URLs.", "labels": [], "entities": []}, {"text": "The dataset is described in detail in.", "labels": [], "entities": []}, {"text": "We use a test set consisting of 28,757 tweets, provided by the IEST as well.", "labels": [], "entities": [{"text": "IEST", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.9124269485473633}]}, {"text": "We evaluate our model on the test data described in section 4.1.", "labels": [], "entities": []}, {"text": "We consider Macro F-score as the evaluation metric and calculate Precision and Recall scores for each emotion class.", "labels": [], "entities": [{"text": "F-score", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.5865312218666077}, {"text": "Precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9991542100906372}, {"text": "Recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9836733341217041}]}, {"text": "We run our experiments with learning rates ranging from 0.1 to 0.5, but choose for 0.3 in later experiments as the model seems to converge slightly better in this case.", "labels": [], "entities": []}, {"text": "For the initial model we set the number of epochs T = 150, but with averaging of the weights, T = 50 seems reasonable as the learning curve plateaus already after 30-35 epochs.", "labels": [], "entities": [{"text": "T", "start_pos": 94, "end_pos": 95, "type": "METRIC", "confidence": 0.9938194155693054}]}, {"text": "During each epoch we calculate the accuracy of the predictions on the train data (we refer to this measure as Convergence or Conv).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9993591904640198}, {"text": "Convergence or Conv", "start_pos": 110, "end_pos": 129, "type": "METRIC", "confidence": 0.8462348580360413}]}, {"text": "Additionally, after each epoch the model is evaluated on the test data whereby the weights are not adjusted so the test data remains unseen.", "labels": [], "entities": []}, {"text": "With these two measures we can track how the model adapts to the train data in comparison to its performance on the test data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of testing the baseline with unigrams.  T = 150.", "labels": [], "entities": [{"text": "T", "start_pos": 58, "end_pos": 59, "type": "METRIC", "confidence": 0.998753547668457}]}, {"text": " Table 2: Results of reductive preprocessing options us- ing unigram frequencies. T = 50.", "labels": [], "entities": [{"text": "T", "start_pos": 82, "end_pos": 83, "type": "METRIC", "confidence": 0.9983741044998169}]}, {"text": " Table 3: Results of additive preprocessing options us- ing unigram frequencies. T = 50.", "labels": [], "entities": [{"text": "T", "start_pos": 81, "end_pos": 82, "type": "METRIC", "confidence": 0.9985210299491882}]}, {"text": " Table 4: Results of third group of experiments: Feature  sets are added incrementally. T = 50.", "labels": [], "entities": [{"text": "T", "start_pos": 88, "end_pos": 89, "type": "METRIC", "confidence": 0.9988954067230225}]}, {"text": " Table 5: Group 4 experiments: feature set includes only  one n-gram or the POS-tags. T = 50.", "labels": [], "entities": [{"text": "T", "start_pos": 86, "end_pos": 87, "type": "METRIC", "confidence": 0.9970299005508423}]}]}