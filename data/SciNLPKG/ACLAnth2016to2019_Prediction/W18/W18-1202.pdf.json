{"title": [{"text": "Entropy-Based Subword Mining with an Application to Word Embeddings", "labels": [], "entities": [{"text": "Entropy-Based Subword Mining", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6732125778992971}]}], "abstractContent": [{"text": "Recent literature has shown a wide variety of benefits to mapping traditional one-hot representations of words and phrases to lower-dimensional real-valued vectors known as word embeddings.", "labels": [], "entities": []}, {"text": "Traditionally, most word embedding algorithms treat each word as the finest meaningful semantic granularity and perform embedding by learning distinct embedding vectors for each word.", "labels": [], "entities": []}, {"text": "Contrary to this line of thought, technical domains such as scientific and medical literature compose words from subword structures such as prefixes , suffixes, and root-words as well as compound words.", "labels": [], "entities": []}, {"text": "Treating individual words as the finest-granularity unit discards meaningful shared semantic structure between words sharing substructures.", "labels": [], "entities": []}, {"text": "This not only leads to poor embeddings for text corpora that have long-tail distributions, but also heuristic methods for handling out-of-vocabulary words.", "labels": [], "entities": []}, {"text": "In this paper we propose SubwordMine, an entropy-based subword mining algorithm that is fast, unsupervised, and fully data-driven.", "labels": [], "entities": [{"text": "subword mining", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.7374312281608582}]}, {"text": "We show that this allows for great cross-domain performance in identifying semantically meaningful subwords.", "labels": [], "entities": []}, {"text": "We then investigate utilizing the mined subwords within the FastText embedding model and compare performance of the learned representations in a downstream language modeling task.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, distributed continuous word representations have become a popular tool for providing a low-dimensional, alternative representation to traditional one-hot bag of words.", "labels": [], "entities": []}, {"text": "These wordembedding vectors are typically a real-valued vector of dimensionality much smaller than the vocabulary size of a corpus.", "labels": [], "entities": []}, {"text": "In addition to computational efficiency of working with low-dimensional representations, distributed representations have been shown to capture syntactic and semantic regularities and have been shown to boost the performance in tasks such as text classification, sequential classification, sentiment analysis, and machine translation ().", "labels": [], "entities": [{"text": "text classification", "start_pos": 242, "end_pos": 261, "type": "TASK", "confidence": 0.821495532989502}, {"text": "sequential classification", "start_pos": 263, "end_pos": 288, "type": "TASK", "confidence": 0.8526865839958191}, {"text": "sentiment analysis", "start_pos": 290, "end_pos": 308, "type": "TASK", "confidence": 0.9513523280620575}, {"text": "machine translation", "start_pos": 314, "end_pos": 333, "type": "TASK", "confidence": 0.8239202499389648}]}, {"text": "Many different methods have been proposed to derive these continuous representations from large, unlabeled, text corpora.", "labels": [], "entities": []}, {"text": "While distributed continuous representations have helped push the state-of-the-art in a variety of NLP tasks, because most text corpora have long-tail distributions, embeddings in the longtail are often of poor quality due to infrequency.", "labels": [], "entities": []}, {"text": "This is even worse for out-of-vocabulary words which are all given the same constant embedding vector because no information is available to infer a meaningful representation.", "labels": [], "entities": []}, {"text": "To address this deficiency, we propose to mine smaller subword structures that form the base syntactic unit and leverage the discovered subwords for generating better-quality word embeddings.", "labels": [], "entities": []}, {"text": "As seen in, morphologically-rich words often contain semantically meaningful subwords that are shared among many words.", "labels": [], "entities": []}, {"text": "Understanding the semantic meaning of these subwords can be used to infer the meaning of words that contain them.", "labels": [], "entities": []}, {"text": "With this motivation, we propose SubwordMine, an algorithm for mining semantically-meaningful subwords from corpus vocabulary.", "labels": [], "entities": []}, {"text": "We utilize the mined subwords as the base-unit for embedding and combine them to construct a word's distributed vector representation.", "labels": [], "entities": []}, {"text": "The resultant word embeddings are robust to data-sparsity due to word infrequency and can be constructed on many out-of-vocabulary words.", "labels": [], "entities": []}, {"text": "We state and analyze the problem in Section 2, followed by our proposed solution in Section 3 where we present the key components of our solution, subword mining and subword-based word embeddings.", "labels": [], "entities": [{"text": "subword mining", "start_pos": 147, "end_pos": 161, "type": "TASK", "confidence": 0.7549220025539398}]}, {"text": "In Section 4, we review the related work.", "labels": [], "entities": []}, {"text": "Then we evaluate the proposed solution in Section 5, conclude in Section 6, and present future directions in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We introduce the datasets used and methods for comparison.", "labels": [], "entities": []}, {"text": "We then describe our evaluations for both subword extraction and for word embedding performance.", "labels": [], "entities": [{"text": "subword extraction", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.777734249830246}]}, {"text": "We use the following three datasets for evaluation purpose: \u2022 DBLP Abstracts.", "labels": [], "entities": [{"text": "DBLP Abstracts", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.9219129383563995}]}, {"text": "Computer science abstracts containing 529K abstracts, 186K unique words, and 39M tokens.", "labels": [], "entities": []}, {"text": "Titles of computer science papers published in 20 conferences containing 44K titles, 5.5K unique words, and 351K tokens.", "labels": [], "entities": []}, {"text": "Abstracts of research papers obtained from from PubMed Central containing 421K abstracts, 334K unique words and 5.8M tokens.", "labels": [], "entities": [{"text": "PubMed Central", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.9430229067802429}]}, {"text": "For baseline comparison methods to our proposed SubwordMine algorithm we utilize a unigram language model segmentation of 'wordpieces' and byte-pair encoding segmentation as", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy in automatically extracting Greek  and Latin root words.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9941425919532776}, {"text": "automatically extracting Greek  and Latin root words", "start_pos": 22, "end_pos": 74, "type": "TASK", "confidence": 0.8364073378699166}]}, {"text": " Table 2: Test perplexity on the language modeling task  for DBLP titles dataset. Evaluation is performed with  fixed pre-trained embeddings, and embedding tuning.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7014902979135513}, {"text": "DBLP titles dataset", "start_pos": 61, "end_pos": 80, "type": "DATASET", "confidence": 0.8197643558184305}]}]}