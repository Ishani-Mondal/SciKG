{"title": [{"text": "Language Identification and Morphosyntactic Tagging: The Second VarDial Evaluation Campaign", "labels": [], "entities": [{"text": "Language Identification and Morphosyntactic Tagging", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.6452625513076782}]}], "abstractContent": [{"text": "We present the results and the findings of the Second VarDial Evaluation Campaign on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects.", "labels": [], "entities": []}, {"text": "The campaign was organized as part of the fifth edition of the VarDial workshop, collocated with COLING'2018.", "labels": [], "entities": [{"text": "VarDial workshop", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.8408387303352356}, {"text": "COLING'2018", "start_pos": 97, "end_pos": 108, "type": "DATASET", "confidence": 0.6567766070365906}]}, {"text": "This year, the campaign included five shared tasks, including two task reruns -Arabic Dialect Identification (ADI) and German Dialect Identification (GDI)-, and three new tasks-Mor-phosyntactic Tagging of Tweets (MTT), Discriminating between Dutch and Flemish in Subtitles (DFS), and Indo-Aryan Language Identification (ILI).", "labels": [], "entities": [{"text": "Tagging of Tweets (MTT)", "start_pos": 194, "end_pos": 217, "type": "TASK", "confidence": 0.6498665312925974}, {"text": "Discriminating between Dutch and Flemish in Subtitles (DFS)", "start_pos": 219, "end_pos": 278, "type": "TASK", "confidence": 0.7887552797794342}, {"text": "Indo-Aryan Language Identification (ILI)", "start_pos": 284, "end_pos": 324, "type": "TASK", "confidence": 0.7336651384830475}]}, {"text": "A total of 24 teams submitted runs across the five shared tasks, and contributed 22 system description papers, which were included in the VarDial workshop proceedings and are referred to in this report.", "labels": [], "entities": [{"text": "VarDial workshop proceedings", "start_pos": 138, "end_pos": 166, "type": "DATASET", "confidence": 0.8468725085258484}]}], "introductionContent": [{"text": "The interest in applying Natural Language Processing (NLP) methods to similar languages, varieties, and dialects has been growing in recent years.", "labels": [], "entities": []}, {"text": "This is evidenced by the growing number of publications and the organization of well-attended workshops co-located with the major NLP conferences such as LT4CloseLang at EMNLP'2014 and the now well-established VarDial workshop series, which is currently in its fifth edition and has been co-located with conferences such as COLING and EACL.", "labels": [], "entities": [{"text": "EMNLP'2014", "start_pos": 170, "end_pos": 180, "type": "DATASET", "confidence": 0.494005411863327}, {"text": "EACL", "start_pos": 335, "end_pos": 339, "type": "DATASET", "confidence": 0.6818751692771912}]}, {"text": "Since its first edition, shared tasks have been organized as part of VarDial.", "labels": [], "entities": [{"text": "VarDial", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9091734290122986}]}, {"text": "The Discriminating Between Similar Languages (DSL) shared task ( ) was run continuously from 2014 to 2018.", "labels": [], "entities": [{"text": "Discriminating Between Similar Languages (DSL) shared task", "start_pos": 4, "end_pos": 62, "type": "TASK", "confidence": 0.6892501446935866}]}, {"text": "In 2016, the DSL task was split into two sub-tasks: a second iteration of the DSL task and the first iteration of the Arabic Dialect Identification (ADI) shared task ( ).", "labels": [], "entities": [{"text": "Arabic Dialect Identification (ADI) shared task", "start_pos": 118, "end_pos": 165, "type": "TASK", "confidence": 0.6099637299776077}]}, {"text": "In the following year, the organizers decided to broaden the scope of the workshop and to organize an evaluation campaign with four shared tasks (: along with iterations of the ADI and the DSL shared tasks, new tasks were started such as the first German Dialect Identification (GDI) and the shared task on Cross-lingual Dependency Parsing (CLP).", "labels": [], "entities": [{"text": "German Dialect Identification (GDI)", "start_pos": 248, "end_pos": 283, "type": "TASK", "confidence": 0.7272558907667795}, {"text": "Cross-lingual Dependency Parsing (CLP)", "start_pos": 307, "end_pos": 345, "type": "TASK", "confidence": 0.7550906985998154}]}, {"text": "This year, we continue with a similar setup, covering five shared tasks as part of the Second VarDial Evaluation Campaign.", "labels": [], "entities": [{"text": "VarDial Evaluation Campaign", "start_pos": 94, "end_pos": 121, "type": "DATASET", "confidence": 0.5832751592000326}]}, {"text": "The remainder of this paper is organized as follows: Section 2 describes this year's shared tasks, Section 3 presents the teams who participated in each task including references to their system descriptions, Section 4 briefly summarizes the related work on the topics of the campaign and on the previous iterations of the ADI and GDI shared tasks.", "labels": [], "entities": []}, {"text": "Sections 5, 6, 7, 8, and 9, present the data, the task setup, and the results for each of the shared tasks.", "labels": [], "entities": []}, {"text": "Finally, Section 10 concludes this report and points to possible directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data for this task was collected from both hard printed and digital sources.", "labels": [], "entities": []}, {"text": "Printed materials were obtained from different institutions that promote these languages.", "labels": [], "entities": []}, {"text": "We also gathered data from libraries, as well as from local literary and cultural groups.", "labels": [], "entities": []}, {"text": "We collected printed stories, novels and essays in books, magazines, and newspapers.", "labels": [], "entities": []}, {"text": "We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output.", "labels": [], "entities": [{"text": "OCR", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.6010146737098694}]}, {"text": "Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API.", "labels": [], "entities": []}, {"text": "Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did.", "labels": [], "entities": [{"text": "OCR", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.782321572303772}]}, {"text": "We further managed to get some blogs in Magahi and Bhojpuri.", "labels": [], "entities": [{"text": "Magahi", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.9745349884033203}, {"text": "Bhojpuri", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8317858576774597}]}, {"text": "There are several corpora already available for Modern Standard Hindi).", "labels": [], "entities": []}, {"text": "However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels.", "labels": [], "entities": []}, {"text": "Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.", "labels": [], "entities": [{"text": "Modern Standard Hindi data collected", "start_pos": 10, "end_pos": 46, "type": "DATASET", "confidence": 0.8459140658378601}]}, {"text": "The provided datasets consisted of three types of data: (i) standard manually annotated data (standard.train), (ii) automatically annotated web data (web.auto), and (iii) Twitter variety manually annotated data (twitter. * ).", "labels": [], "entities": []}, {"text": "The latter were split into train, dev and test sets, with the test data being withheld for the final evaluation.", "labels": [], "entities": []}, {"text": "We give an overview of the different datasets (in number of tokens) in.", "labels": [], "entities": []}, {"text": "* datasets come from the Janes-Tag manually annotated dataset of Slovene computermediated communication () and the ReLDI-NormTagNER-* manually annotated datasets of Croatian) and Serbian) tweets.", "labels": [], "entities": [{"text": "Janes-Tag manually annotated dataset", "start_pos": 25, "end_pos": 61, "type": "DATASET", "confidence": 0.8120079040527344}, {"text": "ReLDI-NormTagNER-* manually annotated datasets", "start_pos": 115, "end_pos": 161, "type": "DATASET", "confidence": 0.6190703392028809}]}, {"text": "These datasets are all similar in size, with around 40 thousand tokens available for training, 8 thousand for development and 20 thousand for testing.", "labels": [], "entities": []}, {"text": "The standard.train datasets mostly cover the general domain.", "labels": [], "entities": []}, {"text": "While the Slovene and Croatian datasets are similar in size with around 500 thousand tokens, the Serbian dataset is significantly smaller, with just 87 thousand tokens.", "labels": [], "entities": []}, {"text": "The web.auto datasets are large web-based datasets: slWac for Slovene (, hrWaC for Croatian, and srWaC for Serbian).", "labels": [], "entities": [{"text": "web.auto datasets", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.735650822520256}]}, {"text": "They are automatically annotated with state-of-the-art taggers for standard Slovene, Croatian, and Serbian: MTT task: size of the datasets (in number of tokens).", "labels": [], "entities": [{"text": "MTT", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.6560121178627014}]}], "tableCaptions": [{"text": " Table 1: The teams that participated in the VarDial'2018 evaluation campaign.", "labels": [], "entities": [{"text": "VarDial'2018 evaluation campaign", "start_pos": 45, "end_pos": 77, "type": "DATASET", "confidence": 0.7770036260286967}]}, {"text": " Table 2: The ADI data: examples (Ex.) in utterances, duration (Dur.), in number of hours.", "labels": [], "entities": [{"text": "ADI data", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.7658167779445648}, {"text": "duration (Dur.)", "start_pos": 54, "end_pos": 69, "type": "METRIC", "confidence": 0.9349832981824875}]}, {"text": " Table 3. The best result,  an F1 score of 0.589, was achieved by UnibucKernel, 1 followed by safina, with an F1 score of 0.575.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9847380816936493}, {"text": "F1 score", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9838287234306335}]}, {"text": " Table 3: ADI results: ranked taking statistical significance into account.", "labels": [], "entities": [{"text": "ADI", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6798087954521179}, {"text": "statistical significance", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.7794646620750427}]}, {"text": " Table 4: ArchiMob interviews used for the GDI task. The surprise dialect is labeled XY.", "labels": [], "entities": [{"text": "GDI task", "start_pos": 43, "end_pos": 51, "type": "TASK", "confidence": 0.7079537212848663}]}, {"text": " Table 5: GDI results: ranked taking statistical significance into account.", "labels": [], "entities": [{"text": "GDI", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.49990954995155334}, {"text": "statistical significance", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.7693946957588196}]}, {"text": " Table 6: MTT task: size of the datasets (in number of tokens).", "labels": [], "entities": [{"text": "MTT task", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.8162190318107605}]}, {"text": " Table 7: MTT results: ranked taking statistical significance into account.", "labels": [], "entities": [{"text": "MTT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8135076761245728}, {"text": "statistical significance", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.7626728415489197}]}, {"text": " Table 8: DFS results: ranked taking statistical significance into account.", "labels": [], "entities": [{"text": "DFS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.5128946900367737}, {"text": "statistical significance", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.7778656482696533}]}, {"text": " Table 9: ILI results: ranked taking statistical significance into account.", "labels": [], "entities": [{"text": "ILI", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.787051796913147}, {"text": "statistical significance", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.745540589094162}]}]}