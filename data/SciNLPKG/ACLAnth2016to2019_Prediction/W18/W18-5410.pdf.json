{"title": [{"text": "Unsupervised Token-wise Alignment to Improve Interpretation of Encoder-Decoder Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Developing a method for understanding the inner workings of black-box neural methods is an important research endeavor.", "labels": [], "entities": []}, {"text": "Conventionally , many studies have used an attention matrix to interpret how Encoder-Decoder-based models translate a given source sentence to the corresponding target sentence.", "labels": [], "entities": []}, {"text": "However, recent studies have empirically revealed that an attention matrix is not optimal for token-wise translation analyses.", "labels": [], "entities": [{"text": "token-wise translation analyses", "start_pos": 94, "end_pos": 125, "type": "TASK", "confidence": 0.7641667723655701}]}, {"text": "We propose a method that explicitly models the token-wise alignment between the source and target sequences to provide a better analysis.", "labels": [], "entities": []}, {"text": "Experiments show that our method can acquire token-wise alignments that are superior to those of an attention mechanism 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "The Encoder-Decoder model with an attention mechanism (EncDec) has been an epoch-making development that has led to great progress in many natural language generation tasks, such as machine translation, dialog generation (, and headline generation ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 182, "end_pos": 201, "type": "TASK", "confidence": 0.8254017233848572}, {"text": "dialog generation", "start_pos": 203, "end_pos": 220, "type": "TASK", "confidence": 0.8228005170822144}, {"text": "headline generation", "start_pos": 228, "end_pos": 247, "type": "TASK", "confidence": 0.8074475824832916}]}, {"text": "An enormous number of studies have attempted to enhance the ability of EncDec.", "labels": [], "entities": []}, {"text": "Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (.", "labels": [], "entities": []}, {"text": "One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors ().", "labels": [], "entities": []}, {"text": "The assumption behind this interpretation is that the attention matrix has a \"soft\" token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target.", "labels": [], "entities": []}, {"text": "However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering ().", "labels": [], "entities": [{"text": "token-wise alignment", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.6771087348461151}]}, {"text": "In addition, reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner ().", "labels": [], "entities": []}, {"text": "also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs.", "labels": [], "entities": []}, {"text": "From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence.", "labels": [], "entities": []}, {"text": "In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task).", "labels": [], "entities": [{"text": "headline generation task", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.7870748440424601}]}, {"text": "Compared with a machine translation task, which is categorized as a loss-less generation (lossless-gen) task, the headline generation task additionally requires EncDec models to appropriately select salient ideas in given source sentences (.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.7962752282619476}, {"text": "headline generation task", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.7760079105695089}]}, {"text": "Therefore, the lossy-gen task seems to make modeling by EncDec much harder.", "labels": [], "entities": []}, {"text": "In fact, our preliminary experiments revealed that the attention mechanism in EncDec models largely fails to capture token-wise alignments, e.g., less than 10 percent accuracy, even if we use one of the current state-of-the-art EncDec models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9977973699569702}]}, {"text": "To obtain a better analysis of how EncDec models translate a given source sentence to the corre-sponding target sentence in the headline generation task, this paper introduces the Unsupervised tokenwise Alignment Module (UAM), a novel component that can be plugged into EncDec models.", "labels": [], "entities": [{"text": "headline generation task", "start_pos": 128, "end_pos": 152, "type": "TASK", "confidence": 0.8232322533925375}]}, {"text": "Unlike a conventional attention model, the proposed UAM explicitly captures token-wise alignments between the source and target sequences on the final hidden layer.", "labels": [], "entities": []}, {"text": "One can plug the UAM into a EncDec model during a training phase and easily understand the EncDec model's behavior by analyzing the UAM's token-wise alignments.", "labels": [], "entities": []}, {"text": "Moreover, the UAM does not require any gold alignment data.", "labels": [], "entities": [{"text": "UAM", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.8879038095474243}, {"text": "gold alignment", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.6356304883956909}]}, {"text": "To demonstrate the effectiveness of the UAM, we evaluate EncDec models with the UAM in the headline generation task (), a widely used benchmark for EncDec models.", "labels": [], "entities": [{"text": "UAM", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.7415557503700256}, {"text": "headline generation task", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.7894857923189799}]}, {"text": "Our experiments show that (i) EncDec models with a UAM achieve comparable (or even superior) performance to the current state-of-the-art headline generation model, and (ii) the produced token-wise alignment is practical regardless of the absence of gold alignment during its training phase.", "labels": [], "entities": []}], "datasetContent": [{"text": "The origin of the headline generation dataset used in our experiments is identical to that used in.", "labels": [], "entities": [{"text": "headline generation dataset", "start_pos": 18, "end_pos": 45, "type": "DATASET", "confidence": 0.6936300198237101}]}, {"text": "The dataset consists of pairs of the first sentence of each article and its headline from the annotated English Gigaword corpus ().", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 104, "end_pos": 127, "type": "DATASET", "confidence": 0.8811008532842001}]}, {"text": "defined the training, validation and test splits, which contain approximately 3.8M, 200K and 400K source-headline pairs, respectively We used the entire training split for training as in the previous studies.", "labels": [], "entities": []}, {"text": "We randomly sampled 8K instances as validation data and 10K instances as test data from the validation split.", "labels": [], "entities": []}, {"text": "Moreover, we experimented on the test data provided by and for comparison with the reported state-of-the-art performance (.", "labels": [], "entities": []}, {"text": "We refer to those test data sets as Test (Ours), Test (Zhou), and MSR-ATC respectively.", "labels": [], "entities": [{"text": "Test (Zhou)", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.936460480093956}]}, {"text": "Among these test sets, MSR-ATC is the only dataset created by a human worker.", "labels": [], "entities": []}, {"text": "We evaluated the performance by ROUGE-1 (RG-1), ROUGE-2 (RG-2) and ROUGE-L (RG-L) . We report the F1 value as given in a previous study . We computed the ROUGE scores using the official ROUGE script (version 1.5.5).", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9051605463027954}, {"text": "F1", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.9990083575248718}]}], "tableCaptions": [{"text": " Table 1: Configurations used in our experiments", "labels": [], "entities": []}, {"text": " Table 2: Full-length ROUGE F1 evaluation results.  \u2020 is the proposed model.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.8180910348892212}, {"text": "F1", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.4857994318008423}]}, {"text": " Table 3: Alignment Macro Accuracy (%)", "labels": [], "entities": [{"text": "Alignment Macro", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7457274496555328}, {"text": "Accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9094056487083435}]}]}