{"title": [{"text": "OpenSeq2Seq: Extensible Toolkit for Distributed and Mixed Precision Training of Sequence-to-Sequence Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We present OpenSeq2Seq-an open-source toolkit for training sequence-to-sequence models.", "labels": [], "entities": []}, {"text": "The main goal of our toolkit is to allow researchers to most effectively explore different sequence-to-sequence architectures.", "labels": [], "entities": []}, {"text": "The efficiency is achieved by fully supporting distributed and mixed-precision training.", "labels": [], "entities": []}, {"text": "OpenSeq2Seq provides building blocks for training encoder-decoder models for neural machine translation and automatic speech recognition.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.6958739161491394}, {"text": "automatic speech recognition", "start_pos": 108, "end_pos": 136, "type": "TASK", "confidence": 0.623819887638092}]}, {"text": "We plan to extend it with other modalities in the future.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequence-to-Sequence models built around the encoder-decoder paradigm) have been successfully used for natural language processing (NLP) (, imagecaptioning (, and automatic speech recognition (ASR) (.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 103, "end_pos": 136, "type": "TASK", "confidence": 0.7846397260824839}, {"text": "automatic speech recognition (ASR)", "start_pos": 163, "end_pos": 197, "type": "TASK", "confidence": 0.7818637788295746}]}, {"text": "However, implementing a sequenceto-sequence model in a general purpose deep learning framework such as TensorFlow (), CNTK () or PyTorch ( can be challenging, especially with support for distributed training.", "labels": [], "entities": []}, {"text": "Several open-source toolkits have been proposed in recent years in an attempt to tackle this challenge.", "labels": [], "entities": []}, {"text": "Among the most popular ones are: OpenNMT (),), NMT (, and Tensor2Tensor ().", "labels": [], "entities": [{"text": "OpenNMT", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9022833704948425}, {"text": "NMT", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.844380795955658}]}, {"text": "These toolkits make it much easier to reproduce most current state-ofthe-art results and train your own models on new datasets.", "labels": [], "entities": []}, {"text": "OpenSeq2Seq is inspired by these approaches with an additional focus on distributed and mixed-precision training.", "labels": [], "entities": []}, {"text": "In particular, OpenSeq2Seq adds support for mixed precision training as described in.", "labels": [], "entities": [{"text": "OpenSeq2Seq", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.8989462852478027}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9321497082710266}]}, {"text": "It uses the IEEE float16 data format to reduce memory requirements and speedup training on modern deep learning hardware such as NVIDIA's Volta GPUs.", "labels": [], "entities": [{"text": "IEEE float16 data format", "start_pos": 12, "end_pos": 36, "type": "DATASET", "confidence": 0.9136084765195847}]}, {"text": "Furthermore, OpenSeq2Seq supports multi-GPU and multinode distributed training.", "labels": [], "entities": []}, {"text": "OpenSeq2Seq is built using TensorFlow and is available at: https://github.com/ NVIDIA/OpenSeq2Seq.", "labels": [], "entities": []}], "datasetContent": [{"text": "Neural Machine Translation (NMT) is naturally expressed in terms of encoder-decoder paradigm (: Evaluation scores after training using float32 and mixed precision.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8464802205562592}]}, {"text": "We used 2, 4 and 8 GPUs to train Transformer, GNMT and DeepSpeech2 models.", "labels": [], "entities": []}, {"text": "All configs are available on OpenSeq2Seq's GitHub.", "labels": [], "entities": [{"text": "OpenSeq2Seq's GitHub", "start_pos": 29, "end_pos": 49, "type": "DATASET", "confidence": 0.8363298376401266}]}, {"text": "change network topology or any of the hyper parameters.", "labels": [], "entities": []}, {"text": "(A) demonstrates that training loss curves for GNMT-like model using float32 and mixed precision track each other very closely during training (the same is true for Transformer training).", "labels": [], "entities": []}, {"text": "In our experiments, we used WMT 2016 English\u2192German data set obtained by combining the Europarlv7, News Commentary v10, and Common Crawl corpora and resulting in roughly 4.5 million sentence pairs.", "labels": [], "entities": [{"text": "WMT 2016 English\u2192German data set", "start_pos": 28, "end_pos": 60, "type": "DATASET", "confidence": 0.9265618068831307}, {"text": "Europarlv7", "start_pos": 87, "end_pos": 97, "type": "DATASET", "confidence": 0.9871628880500793}, {"text": "Common Crawl corpora", "start_pos": 124, "end_pos": 144, "type": "DATASET", "confidence": 0.8952236970265707}]}, {"text": "compares BLEU scores after training with float32 and mixed precision.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9987138509750366}, {"text": "float32", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.8359653353691101}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9642594456672668}]}, {"text": "These scores are computed using multi-bleu.perl 2 script from Moses against newstest2013.tok.de file.", "labels": [], "entities": []}, {"text": "In our experiments, for Transformer and GNMT-like model, total GPU memory consumption is reduced to about 55% of what it was while using float32.", "labels": [], "entities": []}, {"text": "We also observe performance boosts (around x1.8 for GNMT) which can vary depending on the batch size.", "labels": [], "entities": []}, {"text": "The general rule of thumb is that bigger batch size yields better performance.", "labels": [], "entities": []}, {"text": "Many recent Automated Speech Recognition (ASR) models are built using explicit encoderdecoder topology (.", "labels": [], "entities": [{"text": "Automated Speech Recognition (ASR)", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.7896377344926199}]}, {"text": "However, even for models without explicit encoderdecoder topology, it is easy to re-formulate them as such in our toolkit.", "labels": [], "entities": []}, {"text": "For example, let's consider an encoder-decoder implementation of Deep Speech 2 (DS2) model) in OpenSeq2Seq.", "labels": [], "entities": []}, {"text": "DS2 consists of three convolutional layers, several bidirectional or unidirectional recurrent layers (with LSTMs or GRUs), an optional row convolutional layer, and a fully connected layer followed by a hidden layer which produces logits for the Connectionist Temporal Classification (CTC) loss ().", "labels": [], "entities": []}, {"text": "Logits represent a probability distribution over alphabet characters at each timestep.", "labels": [], "entities": []}, {"text": "A beam search CTC decoder with language model rescorer is usually employed for producing the output characters sequence during inference.", "labels": [], "entities": []}, {"text": "While DS2 doesn't contain explicit encoder and decoder (in seq2seq sense), we can split the model in the following fashion: convolutional, recurrent and fully connected layers are encapsulated in the DeepSpeech2Encoder, and logits' hidden layer together with the CTC decoder are encapsulated in the FullyConnectedCTCDecoder.", "labels": [], "entities": []}, {"text": "The reason behind this split point is simple: it allows the encoder to output a custom-sized representation and it encourages encoder and decoder re-use.", "labels": [], "entities": []}, {"text": "For example, the CTC decoder can be replaced with any decoder from text-to-text models.", "labels": [], "entities": [{"text": "CTC decoder", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.9084056913852692}]}, {"text": "In our experiments, we trained DeepSpeech2-like model on a \"clean\" and \"other\" subsets of LibriSpeech training dataset (.", "labels": [], "entities": [{"text": "LibriSpeech training dataset", "start_pos": 90, "end_pos": 118, "type": "DATASET", "confidence": 0.8133283654848734}]}, {"text": "shows final Word Error Rates (WER) on LibriSpeech \"dev-clean\" subset, obtained after training using float32 and mixed precision.", "labels": [], "entities": [{"text": "Word Error Rates (WER)", "start_pos": 12, "end_pos": 34, "type": "METRIC", "confidence": 0.868401845296224}, {"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9513963460922241}]}, {"text": "Similarly to GNMT experiments, we did not change any of the hyper parameters when training in mixed precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9806268811225891}]}, {"text": "During training in mixed precision, we observed a total memory reduction to around 57% compared to float32 mode.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9437042474746704}, {"text": "memory reduction", "start_pos": 56, "end_pos": 72, "type": "METRIC", "confidence": 0.9550411999225616}]}, {"text": "(B) demonstrates that mixed precision has no effect on convergence behaviour.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.8967278003692627}]}], "tableCaptions": []}