{"title": [{"text": "Amobee at IEST 2018: Transfer Learning from Language Models", "labels": [], "entities": [{"text": "Transfer Learning from Language", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.8446112424135208}]}], "abstractContent": [{"text": "This paper describes the system developed at Amobee for the WASSA 2018 implicit emotions shared task (IEST).", "labels": [], "entities": [{"text": "Amobee", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.8341096043586731}, {"text": "WASSA 2018 implicit emotions shared task (IEST)", "start_pos": 60, "end_pos": 107, "type": "TASK", "confidence": 0.7958181500434875}]}, {"text": "The goal of this task was to predict the emotion expressed by missing words in tweets without an explicit mention of those words.", "labels": [], "entities": []}, {"text": "We developed an ensemble system consisting of language models together with LSTM-based networks containing a CNN attention mechanism.", "labels": [], "entities": []}, {"text": "Our approach represents a novel use of language models-specifically trained on a large Twitter dataset-to predict and classify emotions.", "labels": [], "entities": []}, {"text": "Our system reached 1st place with a macro F 1 score of 0.7145.", "labels": [], "entities": [{"text": "macro F 1 score", "start_pos": 36, "end_pos": 51, "type": "METRIC", "confidence": 0.8158292472362518}]}], "introductionContent": [{"text": "Sentiment analysis (SA) is a sub-field of natural language processing (NLP) that explores the automatic deduction of feelings and attitudes from textual data.", "labels": [], "entities": [{"text": "Sentiment analysis (SA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9100234508514404}, {"text": "natural language processing (NLP)", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.7810074289639791}]}, {"text": "One popular choice of source to study is Twitter, asocial network website where people publish short messages, called tweets, with a maximum length of 280 characters.", "labels": [], "entities": []}, {"text": "People write on various topics, including global and local events, public figures, brands and products.", "labels": [], "entities": []}, {"text": "Twitter data has attracted the interest of both academia and industry for the last several years.", "labels": [], "entities": [{"text": "Twitter data", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.7864307463169098}]}, {"text": "It contains some unique features, such as emojis, misspelling and slang that are of interest to NLP researchers while also containing insights relevant for business intelligence, marketing and e-governance.", "labels": [], "entities": []}, {"text": "The implicit emotions shared task (IEST) is part of the WASSA 2018 workshop, and is concerned with classifying tweets into one of 6 emotions-anger, disgust, fear, joy, sadness and surprise-without an explicit mention of emotion words.", "labels": [], "entities": [{"text": "implicit emotions shared task (IEST)", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.5555781551769802}, {"text": "WASSA 2018 workshop", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.514534463485082}, {"text": "classifying tweets into one of 6 emotions-anger, disgust, fear, joy, sadness and surprise-without an explicit mention of emotion words", "start_pos": 99, "end_pos": 233, "type": "Description", "confidence": 0.7230395972728729}]}, {"text": "There were 30 teams who participated in * These authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "the task; fora description and analysis of the task and the datasets, see.", "labels": [], "entities": []}, {"text": "This paper describes our specially developed system for the shared task; it comprises several ensembles, where our new contribution is the use of a language model as an emotion classifier.", "labels": [], "entities": []}, {"text": "A language model, based on the TransformerDecoder architecture () was trained using a large Twitter dataset, and used to produce probabilities for each of the 6 emotions.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Sections 2 and 3 describe our data sources and the embedding training, Section 4 describes the training and usage of the language models.", "labels": [], "entities": []}, {"text": "In Section 5 we describe the resources that are used as features; Section 6 describes the architecture, broken into smaller components.", "labels": [], "entities": []}, {"text": "Finally, we review and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Distributions of labels in the train, dev and test  datasets.", "labels": [], "entities": []}, {"text": " Table 2: Probability calculation of the sentence \"I'm #{TRIGGERWORD} than you.\" with 3 emotions using the  language models. Notice that the sentences which are grammatically incorrect have much lower probabilities.", "labels": [], "entities": [{"text": "TRIGGERWORD", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.8794199824333191}]}]}