{"title": [{"text": "An Automatic Error Tagger for German", "labels": [], "entities": [{"text": "Automatic Error Tagger", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.6385846634705862}]}], "abstractContent": [{"text": "Automatically classifying errors by language learners facilitates corpus analysis and tool development.", "labels": [], "entities": [{"text": "corpus analysis", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7924707233905792}]}, {"text": "We present a tag set and a rule-based classifier for automatically assigning error tags to edits in learner texts.", "labels": [], "entities": [{"text": "automatically assigning error tags to edits in learner texts", "start_pos": 53, "end_pos": 113, "type": "TASK", "confidence": 0.6690225667423673}]}, {"text": "In our manual evaluation, the tags assigned by the classifier are considered to be the best or close to best fitting tag by both raters in 91% of the cases.", "labels": [], "entities": []}], "introductionContent": [{"text": "For a variety of tasks, it is useful to classify errors by language learners into error types.", "labels": [], "entities": []}, {"text": "E. g. corpora which are annotated with error types can be used to extract examples for compiling teaching material or exercises.", "labels": [], "entities": []}, {"text": "Errors can only be interpreted sensibly with respect to a reconstructed utterance, a so-called target hypothesis (TH)).", "labels": [], "entities": []}, {"text": "An error type characterizes the divergence between the learner utterance and the corresponding TH.", "labels": [], "entities": []}, {"text": "Manually annotating error types is a timeconsuming task and has to be repeated if an error tagging scheme changes.", "labels": [], "entities": []}, {"text": "Therefore, automatic error tagging is desirable and in some use cases even inevitable when manual annotation is not feasible due to the amount of data (e. g. when selecting training data from Wikipedia edits for Grammatical Error Correction (GEC) systems or when evaluating the performance of GEC systems () or due to an interactive setting (automatic error tags could be used as an information source for student modeling and feedback generation if a reliable GEC system is available).", "labels": [], "entities": [{"text": "automatic error tagging", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.6377638280391693}, {"text": "feedback generation", "start_pos": 427, "end_pos": 446, "type": "TASK", "confidence": 0.7855271995067596}]}, {"text": "In addition, automatic annotation has the advantage that it can be used to easily unify error annotations across different corpora as long as  some form of correction is available . Inspired by ERRANT (), a grammatical ERRor ANnotation Toolkit for extracting and classifying edits in English learner texts, we developed an error annotation tool for German: Gerrant.", "labels": [], "entities": [{"text": "extracting and classifying edits in English learner texts", "start_pos": 248, "end_pos": 305, "type": "TASK", "confidence": 0.7050361447036266}]}, {"text": "It classifies edits extracted from already aligned parallel learner corpora and assigns error tags using a rule-based approach.", "labels": [], "entities": []}, {"text": "An example for two edits from the ComiGS corpus and their error tags is shown in in.", "labels": [], "entities": [{"text": "ComiGS corpus", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.9133539497852325}]}, {"text": "We present the system, the error types and the design decisions that lead to this set.", "labels": [], "entities": []}, {"text": "Although we have a rather large and diverse tag set, the assigned tags were regarded as best fitting inmost of the cases in our manual evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate Gerrant, we (the authors) manually rated the tags for 200 randomly chosen edits independently.", "labels": [], "entities": []}, {"text": "One half was from the ComiGS corpus, the other from the FalkoEssayL2v2.4 corpus.", "labels": [], "entities": [{"text": "ComiGS corpus", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.9577296376228333}, {"text": "FalkoEssayL2v2.4 corpus", "start_pos": 56, "end_pos": 79, "type": "DATASET", "confidence": 0.9835960566997528}]}, {"text": "For each of these sets, one half was from the minimal target hypothesis and one was from the extended target hypothesis.", "labels": [], "entities": []}, {"text": "The raters were given the original sentence, the corrected sentence, the edit and the tag assigned by the system.", "labels": [], "entities": [{"text": "edit", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.953711748123169}]}, {"text": "The raters were asked to judge on a 4-point Likert scale how appropriate the error tag is.", "labels": [], "entities": []}, {"text": "Since there can be multiple tags for one coarse tag (combined with \":\") and multiple parts combined in one tag (combined with \" \") and we wanted to give partial credit for partially correct tags, the rating should be given as follows: Strongly agree When the error in the text matches the error type in the description of the error tag exactly and no other tag fits better.", "labels": [], "entities": []}, {"text": "If there are multiple tags combined with \":\", everyone of them fits exactly.", "labels": [], "entities": []}, {"text": "Example 1: If S:DET:NUM CASE is the best fitting tag and Gerrant assigns exactly S:DET:NUM CASE.", "labels": [], "entities": []}, {"text": "Example 2: If Gerrant assigns S:DET:CASE:GEN and both S:DET:CASE and S:DET:GEN fit exactly.", "labels": [], "entities": []}, {"text": "Agree When Gerrant assigns one error type (without combinations of parts with \":\") and the error matches the type but another error type fits better.", "labels": [], "entities": [{"text": "Agree", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.96886146068573}]}, {"text": "Or: When Gerrant assigns a combination of error types (combinations of parts with \":\") and the error matches one of the assigned error types in the description of the error tag, which include the best fitting label.", "labels": [], "entities": []}, {"text": "Example: If S:DET:NUM CASE is the best fitting tag and Gerrant assigns S:DET:NUM CASE:GEN.", "labels": [], "entities": [{"text": "GEN", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.6605069637298584}]}, {"text": "Disagree When the error matches the error type in the description of the error tag without the context.", "labels": [], "entities": []}, {"text": "Considering the sentence context, the tag is incorrect.", "labels": [], "entities": []}, {"text": "Or: If more than one tag was assigned, no label fits perfectly, but parts of the label are correct (e. g. if the assigned tag is S:NOUN:CASE NUM:-, but it is only a S:NOUN:NUM).", "labels": [], "entities": []}, {"text": "Strongly disagree When the error does not match the error type described in the error tag description.", "labels": [], "entities": []}, {"text": "If more than one error tag is assigned, not even partial tags fit.", "labels": [], "entities": []}, {"text": "If none of the above cases apply, the most appropriate rating should be chosen.", "labels": [], "entities": []}, {"text": "In addition to the precise error tags, the raters also evaluated the coarse error tags for the same edits.", "labels": [], "entities": []}, {"text": "The coarse error tag consists of the prefix and the first part of the error tag, e. g. S:NOUN or S:MORPH.", "labels": [], "entities": [{"text": "NOUN", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.6549319624900818}]}, {"text": "The coarse tag for all word order errors is S:WO even if the word error's precise tag classifies the error further as in S:WO:NOUN:CASE.", "labels": [], "entities": [{"text": "NOUN:CASE", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.6283926367759705}]}, {"text": "The evaluation results for both raters are shown in  cise tag in 92.75% of the cases (coarse tag: 95.5%, see ).", "labels": [], "entities": [{"text": "cise tag", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.7034377753734589}, {"text": "coarse tag", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.935931921005249}]}, {"text": "While there is only a small difference between coarse and precise tags if \"strongly agree\" and \"agree\" are considered in sum, there is a considerable drop in \"strongly agree\" (\u221212 percentage points on average) and a considerable increase in \"agree\" (+9.25 percentage points on average).", "labels": [], "entities": [{"text": "agree", "start_pos": 242, "end_pos": 247, "type": "METRIC", "confidence": 0.928533136844635}]}, {"text": "This shows that Gerrant most often assigns the best fitting coarse tag but not as often also the best fitting precise tag but only the close to best.", "labels": [], "entities": [{"text": "precise", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.9883132576942444}]}, {"text": "In only 3% of the cases on average, the precise error tag was considered as not fitting (disagree or strongly disagree), although the coarse tag was considered fitting (strongly agree or agree).", "labels": [], "entities": [{"text": "precise error tag", "start_pos": 40, "end_pos": 57, "type": "METRIC", "confidence": 0.9069910049438477}, {"text": "coarse tag", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.9540196061134338}]}, {"text": "Both raters give the same rating for the precise tags in 91.5% of the cases (coarse tag: 95.5%) and 91% of the precise tags are rated as strongly agree or agree by both annotators.", "labels": [], "entities": []}, {"text": "There are area number of errors which Gerrant can improve on.", "labels": [], "entities": []}, {"text": "Some error types do not behave as expected because Gerrant only extracts differences between the original and the correction, e. g. if the first word of a sentence is moved and the case is changed, this would be classified as an S:WO:ORTH, although technically it is not an orthographic error if the case was correct in the original text.", "labels": [], "entities": [{"text": "ORTH", "start_pos": 234, "end_pos": 238, "type": "METRIC", "confidence": 0.9390996098518372}]}, {"text": "For other error types, the rules can be further refined to match the tags more precisely: E. g. if the verb is changed by inserting the particle zu (\"to\") into the word as in wegfahren \u2192 wegzufahren (\"to drive off\"), Gerrant classifies this as a S:VERB:AVZ, although the separable verb prefix (weg) has not been changed.", "labels": [], "entities": [{"text": "VERB", "start_pos": 248, "end_pos": 252, "type": "METRIC", "confidence": 0.9690595865249634}, {"text": "AVZ", "start_pos": 253, "end_pos": 256, "type": "METRIC", "confidence": 0.55392986536026}]}, {"text": "Currently insertions or deletions of the particle zu as a token on its own when it is not used as a separable verb prefix are classified as OTHER.", "labels": [], "entities": [{"text": "OTHER", "start_pos": 140, "end_pos": 145, "type": "METRIC", "confidence": 0.9413734674453735}]}, {"text": "It might be sensible to introduce an error category PART to coverall cases where the particle zu is deleted or inserted.", "labels": [], "entities": []}, {"text": "When a substitution error has more than one token on any side and the spans are not contiguous, Gerrant makes the simplifying assumption that this is always a word order error and uses S:WO as a prefix, although this might not be a word order error.", "labels": [], "entities": []}, {"text": "Gerrant can classify verb errors which contain more than one verb form on one side or both sides, e. g. for identifying tense errors.", "labels": [], "entities": []}, {"text": "Gerrant classifies verb errors based on the PoS of the original and the correction.", "labels": [], "entities": [{"text": "PoS", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9899926781654358}]}, {"text": "Both sides must contain a verb form in order to check for verb errors.", "labels": [], "entities": []}, {"text": "Because of this, some errors are not classified as verb errors due to the assigned PoS tags (an incorrect participle might be tagged as adjective and therefore is not treated as a verb).", "labels": [], "entities": []}, {"text": "Some improvements can also be made for recognizing ADJ:FORM and ADV:FORM, e. g. check if the adverb is accompanied with a particle (STTS tag: PTKA) or certain words such as mehr (\"more\").", "labels": [], "entities": [{"text": "FORM", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9462814331054688}, {"text": "FORM", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9326610565185547}]}, {"text": "Moreover, Gerrant could narrow down the assigned error tags further by taking more of the sentence context into account when disambiguating tokens.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of evaluation showing how much the human raters agree with the tags assigned by the system (in  percent).", "labels": [], "entities": []}, {"text": " Table 4: Sentence which contains a complex verb error (positions 2 and 8, marked with tokmovid tmid 1) where  two verb forms are jointly replaced by two other verb forms. (ComiGS Corpus, text 2mVs 1)", "labels": [], "entities": [{"text": "ComiGS Corpus", "start_pos": 173, "end_pos": 186, "type": "DATASET", "confidence": 0.9464980959892273}]}]}