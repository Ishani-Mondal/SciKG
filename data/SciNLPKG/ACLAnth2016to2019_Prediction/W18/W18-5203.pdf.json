{"title": [{"text": "ArguminSci: A Tool for Analyzing Argumentation and Rhetorical Aspects in Scientific Writing", "labels": [], "entities": [{"text": "Analyzing Argumentation and Rhetorical Aspects in Scientific Writing", "start_pos": 23, "end_pos": 91, "type": "TASK", "confidence": 0.6326736360788345}]}], "abstractContent": [{"text": "Argumentation is arguably one of the central features of scientific language.", "labels": [], "entities": []}, {"text": "We present ArguminSci, an easy-to-use tool that analyzes argumentation and other rhetorical aspects of scientific writing, which we collectively dub scitorics.", "labels": [], "entities": []}, {"text": "The main aspect we focus on is the fine-grained argumentative analysis of scientific text through identification of argument components.", "labels": [], "entities": [{"text": "argumentative analysis of scientific text", "start_pos": 48, "end_pos": 89, "type": "TASK", "confidence": 0.7834016501903533}]}, {"text": "The functionality of ArguminSci is accessible via three interfaces: as a command line tool, via a RESTful application programming interface, and as a web application.", "labels": [], "entities": []}], "introductionContent": [{"text": "Scientific publications are primary means for convincing scientific communities of the merit of one's scientific work and importance of research findings.", "labels": [], "entities": []}, {"text": "To this end, authors typically present their work by embracing and exploiting established practices and specific tools related to the scientific discourse, such as citations, that facilitate building persuading argumentation lines.", "labels": [], "entities": []}, {"text": "Consequently, scientific texts are abundant with different interrelated rhetorical and argumentative layers.", "labels": [], "entities": []}, {"text": "In this work, we refer to this set of mutually-related rhetorical aspects of scientific writing as scitorics.", "labels": [], "entities": []}, {"text": "Numerous research groups have already proposed computational models for analyzing scientific language with respect to one or multiple of these aspects.", "labels": [], "entities": []}, {"text": "For example,  presented experiments on the automatic assignment of argumentative zones, i.e., sentential discourse roles, to sentences in scientific articles.", "labels": [], "entities": []}, {"text": "Similarly, there has been work on automatic classification of citations with respect to their polarity and purpose).", "labels": [], "entities": [{"text": "automatic classification of citations", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.7297682836651802}]}, {"text": "It has also been shown that through analyses of scitorics higher-level computational tasks can be supported, such as the attribution of scientific statements to authors), identification of research trends, or automatic summarization of scientific articles.", "labels": [], "entities": [{"text": "identification of research trends", "start_pos": 171, "end_pos": 204, "type": "TASK", "confidence": 0.8827807605266571}, {"text": "summarization of scientific articles", "start_pos": 219, "end_pos": 255, "type": "TASK", "confidence": 0.8822225779294968}]}, {"text": "In this work, we present ArguminSci 1 a tool that aims to support the holistic analyses of scientific publications in terms of scitorics, including the identification of argumentative components.", "labels": [], "entities": []}, {"text": "We make ArguminSci publicly available for download.", "labels": [], "entities": []}, {"text": "In its core, it relies on separate neural models based on recurrent neural networks with the long short-term memory cells (LSTM) (Hochreiter and Schmidhuber, 1997) pre-trained for each of the five tasks in the area of scientific publication mining that ArguminSci adresses, namely (1) argumentative component identification, (2) discourse role classification, (3) subjective aspect classification, (4) summary relevance classification, and (5) citation context identification.", "labels": [], "entities": [{"text": "argumentative component identification", "start_pos": 285, "end_pos": 323, "type": "TASK", "confidence": 0.6657490134239197}, {"text": "discourse role classification", "start_pos": 329, "end_pos": 358, "type": "TASK", "confidence": 0.6627795398235321}, {"text": "subjective aspect classification", "start_pos": 364, "end_pos": 396, "type": "TASK", "confidence": 0.6203125218550364}, {"text": "summary relevance classification", "start_pos": 402, "end_pos": 434, "type": "TASK", "confidence": 0.7771143118540446}, {"text": "citation context identification", "start_pos": 444, "end_pos": 475, "type": "TASK", "confidence": 0.7473634282747904}]}, {"text": "ArguminSci is available as a command line tool, through a RESTful HTTP-based application programming interface, and as a web-based graphical user interface (i.e., as a web application).", "labels": [], "entities": [{"text": "ArguminSci", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7985779643058777}]}], "datasetContent": [{"text": "Our system supports the following aspects of rhetorical analysis (i.e., automatic annotation) of scientific writing: (1) argument component identification, (2) discourse role classification, (3) subjective aspect classification, (4) citation context identification, and (5) summary relevance classification.", "labels": [], "entities": [{"text": "rhetorical analysis", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7794014513492584}, {"text": "argument component identification", "start_pos": 121, "end_pos": 154, "type": "TASK", "confidence": 0.653621256351471}, {"text": "discourse role classification", "start_pos": 160, "end_pos": 189, "type": "TASK", "confidence": 0.6387852430343628}, {"text": "subjective aspect classification", "start_pos": 195, "end_pos": 227, "type": "TASK", "confidence": 0.6815097332000732}, {"text": "citation context identification", "start_pos": 233, "end_pos": 264, "type": "TASK", "confidence": 0.7217775781949362}, {"text": "summary relevance classification", "start_pos": 274, "end_pos": 306, "type": "TASK", "confidence": 0.8447175820668539}]}, {"text": "Out of these tasks -in accordance with the structure of the annotations in our training corpus -argument component identification and citation context identification are token-level sequence labeling tasks, whereas the remaining three tasks are cast as sentence-level classification tasks.", "labels": [], "entities": [{"text": "argument component identification", "start_pos": 96, "end_pos": 129, "type": "TASK", "confidence": 0.6757465600967407}, {"text": "citation context identification", "start_pos": 134, "end_pos": 165, "type": "TASK", "confidence": 0.7280407349268595}, {"text": "token-level sequence labeling", "start_pos": 170, "end_pos": 199, "type": "TASK", "confidence": 0.6057974100112915}]}, {"text": "\u2022 Argument Component Identification (ACI): The task is to identify argumentative components in a sentence.", "labels": [], "entities": [{"text": "Argument Component Identification (ACI)", "start_pos": 2, "end_pos": 41, "type": "TASK", "confidence": 0.7373344451189041}]}, {"text": "That is, given a sentence x = (x 1 , . .", "labels": [], "entities": []}, {"text": ", x n ) with individual words xi assign a sequence of labels y aci = (y 1 , . .", "labels": [], "entities": []}, {"text": ", y n ) out of the set of token tags Y aci . The label set is a combination of the standard B-I-O tagging scheme and three types of argumentative components, namely background claim, own claim, and data.", "labels": [], "entities": []}, {"text": "\u2022 Discourse Role Classification (DRC): Given a sentence x the task is to classify the role of the sentence in terms of the discourse structure of the publication.", "labels": [], "entities": [{"text": "Discourse Role Classification (DRC)", "start_pos": 2, "end_pos": 37, "type": "TASK", "confidence": 0.821287473042806}]}, {"text": "The classes are given by the set Y drc = {Background, Unspecified, Challenge, FutureWork, Approach, Outcome}.", "labels": [], "entities": []}, {"text": "\u2022 Subjective Aspect Classification (SAC): Given a sentence x the task is to assign a single class out of eight possible categories in Y sac = {None, Limitation, Advantage, Disadvantage-Advantage, Disadvantage, Common Practice, Novelty, Advantage-Disadvantage}.", "labels": [], "entities": [{"text": "Subjective Aspect Classification (SAC)", "start_pos": 2, "end_pos": 40, "type": "TASK", "confidence": 0.8561667402585348}]}, {"text": "\u2022 Summary Relevance Classification (SRC): Out of the set of possible relevance classes Y src , choose one given a sentence x, with Y src = {Very relevant, Relevant, May appear, Should not appear, Totally irrelevant}.", "labels": [], "entities": [{"text": "Summary Relevance Classification (SRC)", "start_pos": 2, "end_pos": 40, "type": "TASK", "confidence": 0.7871120075384775}]}, {"text": "\u2022 Citation Context Identification (CCI): The task is to identify textual spans corresponding to citation contexts.", "labels": [], "entities": [{"text": "Citation Context Identification (CCI)", "start_pos": 2, "end_pos": 39, "type": "TASK", "confidence": 0.8102361559867859}]}, {"text": "More specifically, given a sentence x = (x 1 , . .", "labels": [], "entities": []}, {"text": ", x n ) the task is to decide on a label for each of the tokens xi . The possible labels are Begin Citation Context, Inside Citation Context, and Outside.", "labels": [], "entities": []}, {"text": "For training our models, we used an extension of the Dr. Inventor Corpus (, which we annotated with finegrained argumentation structures (.", "labels": [], "entities": [{"text": "Dr. Inventor Corpus", "start_pos": 53, "end_pos": 72, "type": "DATASET", "confidence": 0.8190814256668091}]}, {"text": "The corpus consists of 40 scientific publications in the field of computer graphics and, besides our annotations of argumentative components, offers four layers of annotation, three of which are on the sentence level (DRC, SAC, SRC).", "labels": [], "entities": [{"text": "SAC", "start_pos": 223, "end_pos": 226, "type": "METRIC", "confidence": 0.8073347210884094}]}, {"text": "Our argument annotation scheme includes three types of argumentative components: \u2022 Background claim: A statement of argumentative nature, which is about or closely related to the work of others or common practices in a research field or about background facts related to the topic of the publication.", "labels": [], "entities": []}, {"text": "\u2022 Own claim: A statement of argumentative nature, which related to the authors own work and contribution.", "labels": [], "entities": []}, {"text": "\u2022 Data: A fact that serves as evidence pro or against a claim.", "labels": [], "entities": []}, {"text": "More details on the argument-extended corpus we use to train our models can be found in the accompanying resource paper ().", "labels": [], "entities": []}, {"text": "For more details on the original annotation layers of the Dr. Inventor Corpus, we refer the reader to (.", "labels": [], "entities": [{"text": "Dr. Inventor Corpus", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.87173459927241}]}, {"text": "In, we provide the overview of all labels for all five scitorics tasks that ArguminSci is capable of recognizing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Tagging and classification performances.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9855853915214539}]}]}