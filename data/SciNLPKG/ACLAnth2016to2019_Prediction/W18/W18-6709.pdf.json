{"title": [{"text": "ChatEval: A Tool for the Systematic Evaluation of Chatbots", "labels": [], "entities": []}], "abstractContent": [{"text": "Open-domain dialog systems are difficult to evaluate.", "labels": [], "entities": []}, {"text": "The current best practice for analyzing and comparing these dialog systems is the use of human judgments.", "labels": [], "entities": []}, {"text": "However, the lack of standardization in evaluation procedures , and the fact that model parameters and code are rarely published hinder systematic human evaluation experiments.", "labels": [], "entities": []}, {"text": "We introduce a unified framework for human evaluation of chatbots that augments existing chatbot tools, and provides a web-based hub for researchers to share and compare their dialog systems.", "labels": [], "entities": []}, {"text": "Researchers can submit their trained models to the ChatEval web interface and obtain comparisons with baselines and prior work.", "labels": [], "entities": []}, {"text": "The evaluation code is open-source to ensure evaluation is performed in a standardized and transparent way.", "labels": [], "entities": []}, {"text": "In addition, we introduce open-source baseline models and evaluation datasets.", "labels": [], "entities": []}, {"text": "ChatEval can be found at https: //chateval.org.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reproducibility and model assessment for opendomain dialog systems is challenging, as many small variations in the training setup or evaluation technique can result in large differences in perceived model performance.", "labels": [], "entities": []}, {"text": "In addition, as the field has grown, it has become increasingly fragmented.", "labels": [], "entities": []}, {"text": "Papers often focus on novel methods, but insufficient attention has been paid to ensuring that datasets and evaluation remain consistent and reproducible.", "labels": [], "entities": []}, {"text": "For example, while human evaluation of chatbot quality is extremely common, few papers publish the set of prompts used for this evaluation, and almost no papers release their learned model parameters.", "labels": [], "entities": []}, {"text": "Because of this, papers tend to evaluate their methodological improvement against a sequence-to-sequence (Seq2Seq) baseline () rather than against each other.", "labels": [], "entities": []}, {"text": "Seq2Seq was first proposed for dialog generation by   NCM being closed-source, nearly all the papers comparing against it have implemented their own versions, with widely varying performance.", "labels": [], "entities": [{"text": "dialog generation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.9209628105163574}, {"text": "NCM", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.9338924884796143}]}, {"text": "Indeed, we found no model, neither among those we trained nor those available online, that matched the performance of the original NCM, as evaluated by humans.", "labels": [], "entities": []}, {"text": "Another issue is that human evaluation experiments, which are currently the gold standard for model evaluation, are equally fragmented, with almost no two papers by different authors adopting the same evaluation dataset or experimental procedure.", "labels": [], "entities": [{"text": "model evaluation", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.7135758697986603}]}, {"text": "To address these concerns, we have built ChatEval, a scientific framework for evaluating chatbots.", "labels": [], "entities": []}, {"text": "ChatEval consists of two main components: (1) an open-source codebase for conducting automatic and human evaluation of chatbots in a standardized way, and (2) a web portal for accessing model code, trained parameters, and evaluation results, which grows with participation.", "labels": [], "entities": []}, {"text": "In addition, ChatEval includes newly created and cu-rated evaluation datasets with both human annotated and automated baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "The  Human Evaluation A/B comparison tests consist of showing the evaluator a prompt and two possible responses from models which are being compared.", "labels": [], "entities": []}, {"text": "The prompt can consist of a single utterance or a series of utterances.", "labels": [], "entities": []}, {"text": "The user picks the better response or specifies a tie.", "labels": [], "entities": []}, {"text": "When both responses are the same, a tie is automatically recorded.", "labels": [], "entities": [{"text": "tie", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.957136869430542}]}, {"text": "The instructions seen by AMT workers are shown in.", "labels": [], "entities": [{"text": "AMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8724601864814758}]}, {"text": "The evaluation prompts are split into blocks (currently defaulted to 10).", "labels": [], "entities": []}, {"text": "Crowd workers are paid $0.01 per single evaluation.", "labels": [], "entities": []}, {"text": "We used three evaluators per prompt, so, if there are 200 prompt/response pairs, we have 600 ratings and the net cost of the experiment is $6.", "labels": [], "entities": []}, {"text": "On the submission form, we ask researchers to pay for the cost of the AMT experiment.", "labels": [], "entities": [{"text": "AMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9476461410522461}]}, {"text": "The overall inter-annotator agreement (IAA) varies depending on the vagueness of the prompt as well as the similarity of the models.", "labels": [], "entities": [{"text": "inter-annotator agreement (IAA)", "start_pos": 12, "end_pos": 43, "type": "METRIC", "confidence": 0.8838106870651246}]}, {"text": "Out of 18 different experiments run, we found that IAA, as measured by Cohen's weighted kappa, varies between .2 to .54 if we include tie choices.", "labels": [], "entities": [{"text": "IAA", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9985474944114685}]}, {"text": "This is similar to the findings of Yuwono et al. who also found low inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Unfortunately, there are occasionally bad workers, which we automatically remove from our results.", "labels": [], "entities": []}, {"text": "In order to identify such workers, we examine the worker against the other annotators.", "labels": [], "entities": []}, {"text": "We propose using the dataset collected by the dialogue breakdown detection (DBDC) task) as a standard benchmark.", "labels": [], "entities": [{"text": "dialogue breakdown detection (DBDC) task", "start_pos": 46, "end_pos": 86, "type": "TASK", "confidence": 0.8699706367083958}]}, {"text": "The DBDC dataset was created by presenting participants with a short paragraph of context and then asking them to converse with three possible chatbots: TikTok, Iris, and CIC.", "labels": [], "entities": [{"text": "DBDC dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8717265725135803}]}, {"text": "Participants knew that they were speaking with a chatbot, and the conversations reflect this.", "labels": [], "entities": []}, {"text": "We randomly selected 200 human utterances from this dataset, after manually filtering out utterances which were too ambiguous or short to be easily answerable.", "labels": [], "entities": []}, {"text": "As the DBDC dataset does not contain any human-human dialog, we collected reference human responses to each utterance.", "labels": [], "entities": [{"text": "DBDC dataset", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.970848023891449}]}, {"text": "For compatibility with prior work, we also publish random subsets of 200 query-response pairs from the test sets of Twitter and OpenSubtitles.", "labels": [], "entities": []}, {"text": "We also make available the list of 200 prompts used as the evaluation set by in their analysis of the NCM's performance.", "labels": [], "entities": []}, {"text": "The datasets used for chatbot evaluation ought to reflect the goal of the chatbot.", "labels": [], "entities": []}, {"text": "For example, even if a chatbot is trained on Twitter, it only makes sense to evaluate on Twitter if the chatbot's aim is to be skilled at responding to Tweets.", "labels": [], "entities": []}, {"text": "With the DBDC dataset, we emphasize the goal of engaging in text-based interactions with users who know they are speaking with a chatbot.", "labels": [], "entities": [{"text": "DBDC dataset", "start_pos": 9, "end_pos": 21, "type": "DATASET", "confidence": 0.9489103555679321}]}, {"text": "We believe that this dataset best represents the kind of conversations we would expect a user to actually have with a text-based conversational agent.", "labels": [], "entities": []}], "tableCaptions": []}