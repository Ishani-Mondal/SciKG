{"title": [{"text": "Multi-task learning for Joint Language Understanding and Dialogue State Tracking", "labels": [], "entities": [{"text": "Joint Language Understanding", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.7757055163383484}, {"text": "Dialogue State Tracking", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.6198465724786123}]}], "abstractContent": [{"text": "This paper presents a novel approach for multi-task learning of language understanding (LU) and dialogue state tracking (DST) in task-oriented dialogue systems.", "labels": [], "entities": [{"text": "multi-task learning of language understanding (LU)", "start_pos": 41, "end_pos": 91, "type": "TASK", "confidence": 0.7269112579524517}, {"text": "dialogue state tracking (DST)", "start_pos": 96, "end_pos": 125, "type": "TASK", "confidence": 0.7850299328565598}]}, {"text": "Multi-task training enables the sharing of the neural network layers responsible for encoding the user utterance for both LU and DST and improves performance while reducing the number of network parameters.", "labels": [], "entities": []}, {"text": "In our proposed framework, DST operates on a set of candidate values for each slot that has been mentioned so far.", "labels": [], "entities": [{"text": "DST", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9005007743835449}]}, {"text": "These candidate sets are generated using LU slot annotations for the current user utterance, dialogue acts corresponding to the preceding system utterance and the dialogue state estimated for the previous turn, enabling DST to handle slots with a large or unbounded set of possible values and deal with slot values not seen during training.", "labels": [], "entities": []}, {"text": "Furthermore, to bridge the gap between training and inference, we investigate the use of scheduled sampling on LU output for the current user utterance as well as the DST output for the preceding turn.", "labels": [], "entities": []}], "introductionContent": [{"text": "Task-oriented dialogue systems interact with users in natural language to accomplish tasks they have in mind, by providing a natural language interface to a backend (API, database or service).", "labels": [], "entities": []}, {"text": "State of the art approaches to task-oriented dialogue systems typically consist of a language understanding (LU) component, which estimates the semantic parse of each user utterance and a dialogue state tracking (DST) or belief tracking component, which keeps track of the conversation context and the dialogue state (DS).", "labels": [], "entities": [{"text": "dialogue state tracking (DST) or belief tracking", "start_pos": 188, "end_pos": 236, "type": "TASK", "confidence": 0.609458284245597}]}, {"text": "Typically, DST uses the System: Hello!", "labels": [], "entities": [{"text": "DST", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.8518267869949341}]}, {"text": "Acts: greeting User: Hello, book me a table for two at Cascal.", "labels": [], "entities": []}, {"text": "Intent: RESERVE RESTAURANT Acts: greeting, inform(#people), inform(restaurant) State: restaurant=Cascal,#people=two System: I found a table for two at Cascal at 6 pm.", "labels": [], "entities": [{"text": "Intent", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9721722602844238}, {"text": "RESERVE", "start_pos": 8, "end_pos": 15, "type": "METRIC", "confidence": 0.9773010611534119}, {"text": "RESTAURANT", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.7124844789505005}, {"text": "inform", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9678463339805603}]}, {"text": "Acts: offer(time=6 pm) User: 6 pm isn't good for us.", "labels": [], "entities": []}, {"text": "Acts: negate(time), inform(time) State: restaurant=Cascal,#people=two, time=7 pm: A dialogue with user intent, user and system dialogue acts, and dialogue state.", "labels": [], "entities": []}, {"text": "semantic parse generated by LU to update the DS at every dialogue turn.", "labels": [], "entities": [{"text": "semantic parse", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6769849359989166}]}, {"text": "The DS accumulates the preferences specified by the user over the dialogue and is used to make requests to a backend.", "labels": [], "entities": []}, {"text": "The results from the backend and the dialogue state are then used by a dialogue policy module to generate the next system response.", "labels": [], "entities": []}, {"text": "Pipelining dialogue system components often leads to error propagation, hence joint modeling of these components has recently gained popularity (, owing to computational efficiency as well as the potential ability to recover from errors introduced by LU.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7007998377084732}]}, {"text": "However, combining joint modeling with the ability to scale to multiple domains and handle slots with a large set of possible values, potentially containing entities not seen during training, are active areas of research.", "labels": [], "entities": []}, {"text": "In this work, we propose a single, joint model for LU and DST trained with multi-task learning.", "labels": [], "entities": []}, {"text": "Similar to Liu and Lane 2017, our model employs a hierarchical recurrent neural network to encode the dialogue context.", "labels": [], "entities": []}, {"text": "Intermediate feature representations from this network are used for identifying the intent and dialogue acts, and tagging slots Utterance:  in the user utterance.", "labels": [], "entities": []}, {"text": "Slot values obtained using these slot tags (as shown in) are then used to update the set of candidate values for each slot.", "labels": [], "entities": []}, {"text": "Similar to , these candidate values are then scored by a recurrent scoring network which is shared across all slots, thus giving an efficient model for DST which can handle new entities that are not present in the training set -i.e., out-of-vocabulary (OOV) slot values.", "labels": [], "entities": [{"text": "DST", "start_pos": 152, "end_pos": 155, "type": "TASK", "confidence": 0.9740343689918518}]}, {"text": "During inference, the model uses its own predicted slot tags and previous turn dialogue state.", "labels": [], "entities": []}, {"text": "However, ground truth slot tags and dialogue state are used for training to ensure stability.", "labels": [], "entities": []}, {"text": "Aiming to bridge this gap between training and inference, we also propose a novel scheduled sampling) approach to joint language understanding and dialogue state tracking.", "labels": [], "entities": [{"text": "joint language understanding", "start_pos": 114, "end_pos": 142, "type": "TASK", "confidence": 0.6452129582564036}, {"text": "dialogue state tracking", "start_pos": 147, "end_pos": 170, "type": "TASK", "confidence": 0.7030229171117147}]}, {"text": "The paper is organized as follows: Section 2 presents related work, followed by Section 3 describing the architecture of the dialogue encoder, which encodes the dialogue turns to be used as features by different tasks in our framework.", "labels": [], "entities": []}, {"text": "The section also defines and outlines the implementation of the LU and DST tasks.", "labels": [], "entities": []}, {"text": "Section 4 describes our setup for scheduled sampling.", "labels": [], "entities": []}, {"text": "We then conclude with experiments and discussion of results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The major contributions of our work are two-fold.", "labels": [], "entities": []}, {"text": "First, we hypothesize that joint modeling of LU and DST results in a computationally efficient model with fewer parameters without compromising performance.", "labels": [], "entities": []}, {"text": "Second, we propose the use of scheduled sampling to improve the robustness of DST during inference.", "labels": [], "entities": [{"text": "DST", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9672996997833252}]}, {"text": "To this end, we conduct experiments across the following two setups.", "labels": [], "entities": []}, {"text": "Separate vs Joint LU-DST - shows the joint LU-DST setup where parameters in the utterance encoder and state encoder are shared across LU tasks (intent classification, dialogue act classification and slot tagging) and DST (candidate scoring).", "labels": [], "entities": [{"text": "intent classification", "start_pos": 144, "end_pos": 165, "type": "TASK", "confidence": 0.7041251808404922}, {"text": "dialogue act classification", "start_pos": 167, "end_pos": 194, "type": "TASK", "confidence": 0.6113934814929962}, {"text": "slot tagging", "start_pos": 199, "end_pos": 211, "type": "TASK", "confidence": 0.67039754986763}]}, {"text": "As baselines, we also conduct experiments where LU and DST tasks use separate parameters for utterance and state encoders.", "labels": [], "entities": []}, {"text": "Scheduled Sampling -We conduct scheduled sampling (as described in Section 4) experiments in four different setups.", "labels": [], "entities": [{"text": "Scheduled Sampling", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.5434802919626236}]}, {"text": "1. None -Ground truth slot tags (\u00af ct u ) and previous dialogue state ( \u00af D t\u22121 ) are used for training.", "labels": [], "entities": [{"text": "previous dialogue state ( \u00af D t\u22121 )", "start_pos": 46, "end_pos": 81, "type": "METRIC", "confidence": 0.6440708726644516}]}, {"text": "2. Tags -Model samples between ground truth (\u00af ct u ) and predicted (c t u ) slot tags, sticking to ground truth previous state.", "labels": [], "entities": []}, {"text": "3. State -Model samples between ground truth ( \u00af D t\u22121 ) and predicted (D t\u22121 ) previous state, sticking to ground truth slot tags.", "labels": [], "entities": []}, {"text": "4. Both -Model samples between \u00af D t\u22121 and D t\u22121 as well as between \u00af ct u and ct u . In the last three setups, we start sampling from predictions only after k pre = 0.3 k max training steps, as shown in.", "labels": [], "entities": []}, {"text": "We report user intent classification accuracy, F1 score for user dialogue act classification, frame accuracy for slot tagging and joint goal accuracy and slot F1 score for DST.", "labels": [], "entities": [{"text": "user intent classification", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6936107277870178}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.8256370425224304}, {"text": "F1 score", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9907592236995697}, {"text": "user dialogue act classification", "start_pos": 60, "end_pos": 92, "type": "TASK", "confidence": 0.7000591456890106}, {"text": "frame accuracy", "start_pos": 94, "end_pos": 108, "type": "METRIC", "confidence": 0.8121607005596161}, {"text": "slot tagging", "start_pos": 113, "end_pos": 125, "type": "TASK", "confidence": 0.7710034847259521}, {"text": "joint goal accuracy", "start_pos": 130, "end_pos": 149, "type": "METRIC", "confidence": 0.504761149485906}, {"text": "slot F1 score", "start_pos": 154, "end_pos": 167, "type": "METRIC", "confidence": 0.7809672951698303}, {"text": "DST", "start_pos": 172, "end_pos": 175, "type": "TASK", "confidence": 0.781192421913147}]}, {"text": "During DST evaluation, we always use the predicted slot values and the dialogue state in the previous turn.", "labels": [], "entities": [{"text": "DST evaluation", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.9150793254375458}]}, {"text": "Slot frame accuracy is defined as the fraction of turns for which all slot labels are predicted correctly.", "labels": [], "entities": [{"text": "Slot frame", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.7906167805194855}, {"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.729268491268158}]}, {"text": "Similarly, joint goal accuracy is the fraction of turns for which the predicted and ground truth dialogue state match for all slots.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.7252371311187744}]}, {"text": "Since it is a stricter metric than DST slot F1, we use it as the primary metric to identify the best set of parameters on the validation set.", "labels": [], "entities": []}, {"text": "We evaluate our approaches on two datasets: \u2022 Simulated Dialogues 2 -The dataset, described in, contains dialogues from restaurant (Sim-R) and movie (Sim-M) domains across three intents.", "labels": [], "entities": []}, {"text": "A challenging aspect of this dataset is the prevalence of OOV entities e.g. only 13% of the movie names in the dev/test sets also occur in the training data.", "labels": [], "entities": []}, {"text": "\u2022 DSTC2 -We use the top ASR hypothesis and system dialogue acts as inputs.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 2, "end_pos": 7, "type": "DATASET", "confidence": 0.8350799083709717}, {"text": "ASR", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9355826377868652}]}, {"text": "Dialogue act labels are obtained from top SLU hypothesis and state labels for requestable slots.", "labels": [], "entities": []}, {"text": "DS labels are obtained from state labels for informable slots.", "labels": [], "entities": []}, {"text": "We use a semantic dictionary) to obtain ground truth slot tags.", "labels": [], "entities": []}, {"text": "We also use the semantic dictionary to canonicalize the candidate values since the slot values in the dialogue state come from a fixed set in the DSTC2 dialogues and maybe different from those present in the user utterance.", "labels": [], "entities": [{"text": "DSTC2 dialogues", "start_pos": 146, "end_pos": 161, "type": "DATASET", "confidence": 0.9127696752548218}]}], "tableCaptions": [{"text": " Table 2: Reported joint goal accuracy of model  variants on the DSTC2 test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.7216253876686096}, {"text": "DSTC2 test set", "start_pos": 65, "end_pos": 79, "type": "DATASET", "confidence": 0.9724389513333639}]}]}