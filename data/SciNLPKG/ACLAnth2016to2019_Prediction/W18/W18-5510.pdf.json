{"title": [{"text": "Stance Detection in Fake News: A Combined Feature Representation", "labels": [], "entities": [{"text": "Stance Detection in Fake News", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.879542100429535}]}], "abstractContent": [{"text": "With the uncontrolled increasing of fake news and rumors over the Web, different approaches have been proposed to address the problem.", "labels": [], "entities": []}, {"text": "In this paper, we present an approach that combines lexical, word embeddings and n-gram features to detect the stance in fake news.", "labels": [], "entities": []}, {"text": "Our approach has been tested on the Fake News Challenge (FNC-1) dataset.", "labels": [], "entities": [{"text": "Fake News Challenge (FNC-1) dataset", "start_pos": 36, "end_pos": 71, "type": "DATASET", "confidence": 0.8067209550312587}]}, {"text": "Given a news title-article pair, the FNC-1 task aims at determining the relevance of the article and the title.", "labels": [], "entities": []}, {"text": "Our proposed approach has achieved an accurate result (59.6 % Macro F1) that is close to the state-of-the-art result with 0.013 difference using a simple feature representation.", "labels": [], "entities": [{"text": "accurate", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9717791080474854}, {"text": "Macro F1)", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.6958483457565308}]}, {"text": "Furthermore , we have investigated the importance of different lexicons in the detection of the classification labels.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, many phenomena appeared and spread in the Internet, especially with the huge propagation of information and the growth of social networks.", "labels": [], "entities": []}, {"text": "Some of these phenomena are fake news, rumors and misinformation.", "labels": [], "entities": []}, {"text": "In general, the detection of these phenomena is crucial since in many situations they expose the people to danger . Journalism made several efforts in addressing these problems by presenting a validity proof to the audience.", "labels": [], "entities": []}, {"text": "Unfortunately, these manual attempts take much time and effort from the journalists and, at the same time, they cannot cover the rapid spread of these fake news.", "labels": [], "entities": []}, {"text": "Hence, there is the need for addressing the problem from an automatic perspective.", "labels": [], "entities": []}, {"text": "Fake news gained large attention recently from the natural language processing (NLP) research community and many approaches have been proposed.", "labels": [], "entities": [{"text": "Fake news", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9619622826576233}]}, {"text": "These approaches investigated fake news from network and textual perspectives (.", "labels": [], "entities": []}, {"text": "Some of the textual approaches handled the phenomenon from a validity aspect, where they labeled a claim as \"False\", \"True\", or \"Half-True\".", "labels": [], "entities": [{"text": "False", "start_pos": 109, "end_pos": 114, "type": "METRIC", "confidence": 0.9540524482727051}]}, {"text": "Others tried to tackle it from a stance perspective, similar to stance detection works on Twitter () that tried to determine whether a tweet is in favor, against, or neither to a given target entity (person, organization, etc.).", "labels": [], "entities": [{"text": "stance detection", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.7775488197803497}]}, {"text": "Where in fake news, they replaced the tuple of the tweet and the target entity with a claim and an article; also a different stances' set is used (agree, disagree, discuss, and unrelated).", "labels": [], "entities": []}, {"text": "Several shared tasks have been proposed: Fake News Challenge (FNC-1),, CheckThat (, and Fact Extraction and Verification (FEVER) 2 . In FNC-1, the organizers proposed the task to be approached from a stance perspective; the goal is to predict how other articles orient to a specific fact, similarly than in RumorEval (task-A).", "labels": [], "entities": [{"text": "Fact Extraction and Verification (FEVER)", "start_pos": 88, "end_pos": 128, "type": "METRIC", "confidence": 0.6564664968422481}]}, {"text": "While in both RumorEval (task-B) and CheckThat (task-B) a rumor/claim has been submitted and the task objective is to validate the truthfulness of this sentence (true, half-true, or false).", "labels": [], "entities": []}, {"text": "In the first task of CheckThat (task-A) participants were asked to detect claims that are worthy for checking (may have facts), as preliminary step to task B.", "labels": [], "entities": []}, {"text": "Finally, the purpose of FEVER shared task is to evaluate the ability of a system to verify a factual claim using evidences from Wikipedia, where each re-trieved evidence (in case there are many) should be labeled as \"Supported\", \"Refuted\" or \"NotEnoughInfo\" (if there isn't sufficient evidence to either support or refute it).", "labels": [], "entities": [{"text": "FEVER shared task", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.5557411213715872}]}, {"text": "The given attention to fake news and rumors detection in the literature is more than the one gained by detecting worthy claims.", "labels": [], "entities": [{"text": "fake news and rumors detection", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.8154352188110352}]}, {"text": "The orientation in these works was towards inferring these worthy claims using linguistic and stylistic aspects ().", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we tested Support Vector Machines (SVM) (using each Linear and RBF kernels), Gradient Boost, Random Forest and Naive Bayes classifiers but the Neural Network (NN) showed better results 6 . Our NN architecture consists of two hidden layers with rectified linear unit (ReLU) activation function as non-linearity for the hidden layers, and Softmax activation function for the output layer.", "labels": [], "entities": []}, {"text": "Also, we employed the Systems Macro-F1 Majority vote 0.210 FNC-1 baseline 0.454 Talos 0.582 UCLMR ( 0.583 Athene 0.604 stackLSTM 0.609 Our approach 0.596 Cue words 0.250 Word2vec embeddings 0.488 Adam weight optimizer.", "labels": [], "entities": [{"text": "Systems Macro-F1 Majority vote 0.210 FNC-1 baseline 0.454 Talos 0.582 UCLMR", "start_pos": 22, "end_pos": 97, "type": "DATASET", "confidence": 0.6086036888035861}, {"text": "stackLSTM", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.719588577747345}]}, {"text": "The used batch size is 200.", "labels": [], "entities": []}, {"text": "shows the results of our approach and those of the FNC-1 participants.", "labels": [], "entities": [{"text": "FNC-1", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.8787079453468323}]}, {"text": "We investigated the score of each of our features independently.", "labels": [], "entities": []}, {"text": "The word2vec embeddings feature set has achieved 0.488 Macro F1 value, while the cue words achieved 0.25.", "labels": [], "entities": [{"text": "word2vec embeddings feature set", "start_pos": 4, "end_pos": 35, "type": "DATASET", "confidence": 0.6959556192159653}, {"text": "Macro F1 value", "start_pos": 55, "end_pos": 69, "type": "METRIC", "confidence": 0.7644387483596802}]}, {"text": "The extension of the cue words has improved the final result by 2.5%.", "labels": [], "entities": [{"text": "final result", "start_pos": 48, "end_pos": 60, "type": "METRIC", "confidence": 0.8947093486785889}]}, {"text": "The tuples of the \"Unrelated\" class had been created artificially by assigning articles from different documents.", "labels": [], "entities": []}, {"text": "This abnormal distribution can affect the result of the cue words feature when we test it independently; since we extract the cue words feature from the articles part (without the titles) and some articles could be found with different class labels, this can bias the classification process.", "labels": [], "entities": []}, {"text": "As we mentioned previously, the stateof-the-art result was obtained by an approach that combined LSTM with other features (see Section 2).", "labels": [], "entities": []}, {"text": "Our approach achieved 0.596 value of Macro F1 score which is very close to the best result.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9151388108730316}]}, {"text": "The combination of the cue words categories with the other features has improved the overall result.", "labels": [], "entities": []}, {"text": "Each of them had impact in the classification process.", "labels": [], "entities": [{"text": "classification process", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.9139798581600189}]}, {"text": "In, we show the importance of each category using the Information Gain.", "labels": [], "entities": []}, {"text": "We extract it using Gradient Boost classifier as it achieves the highest result comparing to the other decision tree-based classifiers.", "labels": [], "entities": []}, {"text": "The figure clarifies that Report is the category that has the highest importance in the classification process, where Negation and Belief categories have lower importance, whereas both of the Denial and Knowledge categories have the lowest importance.", "labels": [], "entities": []}, {"text": "Surprisingly, both of the Fake and Doubt categories have a lower importance than the other three.", "labels": [], "entities": [{"text": "Fake", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.6937812566757202}, {"text": "Doubt", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.6675068736076355}]}, {"text": "Our intuition was that the Fake category will have the highest importance in discriminating the classes, where this category contains words that: may not appear in the \"Agree\" class records, appear profusely in the \"Disagree\" class (where the title is fake and the article proving that), and a medium appearance amount in the \"Discuss\" class.", "labels": [], "entities": []}, {"text": "Similarly, for the Doubt category, it seems that it may appear frequently in both \"Discuss\" and \"Disagree\" classes where its words normally mentioned when an article discusses a specific idea or when refuse it.", "labels": [], "entities": []}, {"text": "To understand deeper our Information Gain results, we conducted another experiment to infer the importance of each category with respect to each classification class.", "labels": [], "entities": []}, {"text": "To do so, we use SVM classifier coefficients (linear kernel) to extract the most important category to each classification class.", "labels": [], "entities": []}, {"text": "In our initial experiments, the SVM produced a result that is similar to the NN (58% Macro F1), so based on the good performance we used it in this experiment, where we couldn't extract the feature importance using the NN.", "labels": [], "entities": [{"text": "Macro F1)", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.7384650607903799}]}, {"text": "Once the SVM fits the data and creates a hyperplane that uses support vectors to maximize the distance between the classes, the importance of the features can be extracted based on the absolute size of the coefficients (vector coordinates).", "labels": [], "entities": []}, {"text": "In we show the importance of each category by their order.", "labels": [], "entities": []}, {"text": "We can notice that for the \"Agree\" class, generally, the categories that are used when there is a disagreement (Denial, Fake, Negation) tend to be less important than the other categories.", "labels": [], "entities": []}, {"text": "On the contrary, for the \"Disagree\", disagreement categories appear in general in higher order comparing to the \"Agree\" class.", "labels": [], "entities": []}, {"text": "For the \"Discuss\" class, due to the unclear stance towards the title where articles did not show a clear in favor or against stance, we can notice an overlapping in the highest order between the categories that are important for both \"Disagree\" and \"Agree\" classes.", "labels": [], "entities": []}, {"text": "Finally, as we mentioned previously that the articles in the \"Unrelated\" class are created artificially by assigning articles from different titles, the order of the categories is not meaningful.", "labels": [], "entities": []}], "tableCaptions": []}