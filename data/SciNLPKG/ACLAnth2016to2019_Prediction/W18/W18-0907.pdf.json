{"title": [{"text": "A Report on the 2018 VUA Metaphor Detection Shared Task", "labels": [], "entities": [{"text": "VUA Metaphor Detection Shared", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.8450156152248383}]}], "abstractContent": [{"text": "As the community working on computational approaches to figurative language is growing and as methods and data become increasingly diverse, it is important to create widely shared empirical knowledge of the level of system performance in a range of contexts, thus facilitating progress in this area.", "labels": [], "entities": []}, {"text": "One way of creating such shared knowledge is through benchmarking multiple systems on a common dataset.", "labels": [], "entities": []}, {"text": "We report on the shared task on metaphor identification on the VU Amsterdam Metaphor Corpus conducted at the NAACL 2018 Workshop on Figurative Language Processing .", "labels": [], "entities": [{"text": "metaphor identification", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.896783322095871}, {"text": "VU Amsterdam Metaphor Corpus", "start_pos": 63, "end_pos": 91, "type": "DATASET", "confidence": 0.8845083564519882}, {"text": "NAACL 2018 Workshop on Figurative Language Processing", "start_pos": 109, "end_pos": 162, "type": "TASK", "confidence": 0.7184506739888873}]}], "introductionContent": [{"text": "Metaphor use in everyday language is away to relate our physical and familiar social experiences to a multitude of other subjects and contexts (; it is a fundamental way to structure our understanding of the world even without our conscious realization of its presence as we speak and write.", "labels": [], "entities": []}, {"text": "It highlights the unknown using the known, explains the complex using the simple, and helps us to emphasize the relevant aspects of meaning resulting in effective communication.", "labels": [], "entities": []}, {"text": "Consider the following examples of metaphor use in.", "labels": [], "entities": []}, {"text": "Metaphor has been studied in the context of political communication, marketing, mental health, teaching, assessment of English proficiency, among others (Beigman; see chapter 7 in fora recent review.", "labels": [], "entities": []}, {"text": "M: Authority is a chair, it needs legs to stand.", "labels": [], "entities": [{"text": "M", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.7948727011680603}]}, {"text": "I: Authority is useless when it lacks support.", "labels": [], "entities": []}, {"text": "M: In Washington, people change dance partners frequently, but not the dance.", "labels": [], "entities": [{"text": "M", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9557129740715027}]}, {"text": "I: In Washington, people work with one another opportunistically.", "labels": [], "entities": []}, {"text": "M: Robert Muller is like a bulldog -he will get what he wants.", "labels": [], "entities": []}, {"text": "I: Robert Muller will work in a determined and aggressive manner to get what he wants.: Metaphorical sentences (M) characterized by metaphors in bold and their literal interpretations In this paper, we report on the first shared task on automatic metaphor detection.", "labels": [], "entities": [{"text": "automatic metaphor detection", "start_pos": 237, "end_pos": 265, "type": "TASK", "confidence": 0.6560241381327311}]}, {"text": "By making available an easily accessible common dataset and framework for evaluation, we hope to contribute to the consolidation and strengthening of the growing community of researchers working on computational approaches to figurative language.", "labels": [], "entities": []}, {"text": "By engaging a variety of teams to test their systems within a common evaluation framework and share their findings about more or less effective architectures, features, and data sources, we hope to create a shared understanding of the current state of the art, laying a foundation for further work.", "labels": [], "entities": []}, {"text": "This report provides a description of the shared task, dataset and metrics, a brief description of each of the participating systems, a comparative evaluation of the systems, and our observations about trends in designs and performance of the 56 systems that participated in the shared task.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the VU Amsterdam Metaphor Corpus (VUA) as the dataset for our shared task.", "labels": [], "entities": [{"text": "VU Amsterdam Metaphor Corpus (VUA)", "start_pos": 11, "end_pos": 45, "type": "DATASET", "confidence": 0.9372366496494838}]}, {"text": "The dataset consists of 117 fragments sampled across four genres from the British National Corpus: Academic, News, Conversation, and Fiction.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 74, "end_pos": 97, "type": "DATASET", "confidence": 0.9611012736956278}]}, {"text": "Each genre is represented by approximately the same number of tokens, although the number of texts differs greatly, where the news archive has the largest number of texts.", "labels": [], "entities": []}, {"text": "We randomly sampled 23% of the texts from each genre to set aside for testing, while retaining the rest for training.", "labels": [], "entities": []}, {"text": "The data is annotated using the MIP-VU procedure with a strong inter-annotator reliability of \u03ba > 0.8.", "labels": [], "entities": []}, {"text": "It is based on the MIP procedure, extending it to handle metaphoricity through reference (such as marking did as a metaphor in As the weather broke up, so did their friendship) and allow for explicit coding of difficult cases where a group of annotators could not arrive at a consensus.", "labels": [], "entities": [{"text": "MIP", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.8838678598403931}]}, {"text": "The tagset is rich and is organized hierarchically, detecting various types of metaphors, words that flag the presense of metaphors, etc.", "labels": [], "entities": []}, {"text": "In this paper, we consider only the top-level partition, labeling all content words with the tag \"function=mrw\" (metaphor-related word) as metaphors, while all other content words are labeled as non-metaphors.", "labels": [], "entities": []}, {"text": "shows the overall statistics of our training and testing sets.", "labels": [], "entities": []}, {"text": "To facilitate the use of the datasets and evaluation scripts beyond this shared task in future re-  search, the complete set of task instructions and scripts are published on Github 1 . Specifically, we provide a script to parse the original VUAMC.xml, which was not provided in our download bundle due to licensing restriction, to extract the verbs and other content words required for the shared task.", "labels": [], "entities": [{"text": "VUAMC.xml", "start_pos": 242, "end_pos": 251, "type": "DATASET", "confidence": 0.9424917101860046}]}, {"text": "We also provide a set of features used to construct the baseline classification model for prediction of metaphor/non-metaphor classes at the word level, and instructions on how to replicate the baselines.", "labels": [], "entities": [{"text": "prediction of metaphor/non-metaphor classes", "start_pos": 90, "end_pos": 133, "type": "TASK", "confidence": 0.789580891529719}]}], "tableCaptions": [{"text": " Table 2: Verbs and All POS datasets. The table reports the number of text fragments from BNC, number  of tokens and percentage of tokens marked as metaphor group by genres.", "labels": [], "entities": [{"text": "POS datasets", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.7566470503807068}, {"text": "BNC", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.8820362091064453}]}, {"text": " Table 3: Performance and ranking of the best system per team and baselines for the All-POS track,  including split by genre.", "labels": [], "entities": [{"text": "All-POS track", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.892280787229538}]}, {"text": " Table 4: Performance and ranking of the best system per team and baselines for the Verbs track, including  split by genre.", "labels": [], "entities": [{"text": "Verbs track", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.8778519332408905}]}, {"text": " Table 5: Performance (F-score) of the best systems submitted to All-POS track by POS subsets of the test  data. In parentheses, we show the rank of the given POS within all POS for the system. The last column  shows the overall drop in performance from best POS (ranked 1) to worst (ranked 4).", "labels": [], "entities": [{"text": "F-score)", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9560286402702332}]}]}