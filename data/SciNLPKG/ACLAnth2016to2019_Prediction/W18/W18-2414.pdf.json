{"title": [{"text": "Low-Resource Machine Transliteration Using Recurrent Neural Networks of Asian Languages", "labels": [], "entities": []}], "abstractContent": [{"text": "Grapheme-to-phoneme models are key components in automatic speech recognition and text-to-speech systems.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6078740159670512}]}, {"text": "With low-resource language pairs that do not have available and well-developed pronunciation lexicons, grapheme-to-phoneme models are particularly useful.", "labels": [], "entities": []}, {"text": "These models are based on initial alignments between grapheme source and phoneme target sequences.", "labels": [], "entities": []}, {"text": "Inspired by sequence-to-sequence recurrent neural network-based translation methods, the current research presents an approach that applies an alignment representation for input sequences and pre-trained source and target embed-dings to overcome the transliteration problem fora low-resource languages pair.", "labels": [], "entities": [{"text": "sequence-to-sequence recurrent neural network-based translation", "start_pos": 12, "end_pos": 75, "type": "TASK", "confidence": 0.6271745443344117}]}, {"text": "We participated in the NEWS 2018 shared task for the English-Vietnamese transliter-ation task.", "labels": [], "entities": [{"text": "NEWS 2018 shared task", "start_pos": 23, "end_pos": 44, "type": "DATASET", "confidence": 0.8069378435611725}]}], "introductionContent": [{"text": "Transliteration means the phonetic translation of the words in a source language (e.g. English) into equivalent words in a target language (e.g. Vietnamese).", "labels": [], "entities": []}, {"text": "It entails transforming a word from one writing system (the \"source word\") to a phonetically equivalent word in another writing system (the \"target word\").", "labels": [], "entities": []}, {"text": "This transformation requires a large set of rules defined by expert linguists to determine how the phonemes are aligned and to take into account the phonological system of the target language.", "labels": [], "entities": []}, {"text": "Many language pairs have adopted various rules for transliteration overtime, and most transliteration depends on the origin of a word (.", "labels": [], "entities": [{"text": "transliteration overtime", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.9342715740203857}]}, {"text": "In recent work on sequence-to-sequence neural network-based machine translation, the input vocabulary is large.", "labels": [], "entities": [{"text": "sequence-to-sequence neural network-based machine translation", "start_pos": 18, "end_pos": 79, "type": "TASK", "confidence": 0.5698813199996948}]}, {"text": "Moreover, statistics for many words must be sparsely estimated).", "labels": [], "entities": []}, {"text": "To deal with this linguistics aspect, neural network-based approaches use continuous-space representations of words or word embeddings, in which words that occur in similar context tend to be close to each other in representational space.", "labels": [], "entities": []}, {"text": "The benefits of using neural networks, particularly, recurrent neural networks, to deal with sparse problem are very clear.", "labels": [], "entities": []}, {"text": "We have observed that the state-of-the-art grapheme-to-phoneme methods were based on the use of grapheme-phoneme mappings.", "labels": [], "entities": []}, {"text": "However, recurrent neural networks approaches do not require any alignment information.", "labels": [], "entities": []}, {"text": "In this study, we propose a novel method to build a low-resource machine transliteration system, using RNN-based models and alignment information for input sequences.", "labels": [], "entities": []}, {"text": "Given anew word in the source language that does not exist in the bilingual pronunciation dictionary, this system automatically predicts the phonemic representation of a word in the target language.", "labels": [], "entities": []}, {"text": "We are interested in solving out-of-vocabulary words for machine translation systems, such as proper nouns or technical terms, fora low-resource language pair, in this case English and Vietnamese.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.727253794670105}]}, {"text": "The structure of the article is as follows: Section 2 presents the state of the art on machine transliteration.", "labels": [], "entities": [{"text": "machine transliteration", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.6964118778705597}]}, {"text": "In section 3, we describe our proposed approach.", "labels": [], "entities": []}, {"text": "Then, in section 4, we present our experiments, compare our system's performance with other systems.", "labels": [], "entities": []}, {"text": "Finally, in section 5, we present our conclusions and perspectives for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we built a machine transliteration method which was inspired by neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 78, "end_pos": 104, "type": "TASK", "confidence": 0.6923348108927408}]}, {"text": "Hence, we applied different evaluation metrics such as BiLingual Evaluation Understudy (BLEU) (), Translation Error Rate (TER), and Phoneme Error Rate (PER).", "labels": [], "entities": [{"text": "BiLingual Evaluation Understudy (BLEU)", "start_pos": 55, "end_pos": 93, "type": "METRIC", "confidence": 0.8340217769145966}, {"text": "Translation Error Rate (TER)", "start_pos": 98, "end_pos": 126, "type": "METRIC", "confidence": 0.912352979183197}, {"text": "Phoneme Error Rate (PER)", "start_pos": 132, "end_pos": 156, "type": "METRIC", "confidence": 0.7749158690373102}]}, {"text": "To evaluate our proposed approach, we implemented five systems: (1) Baseline system A : phrase-based statistical machine translation (pbSMT).", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 88, "end_pos": 132, "type": "TASK", "confidence": 0.5559776052832603}]}, {"text": "We implemented a pbSMT system with Moses 6 (.", "labels": [], "entities": [{"text": "Moses 6", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9463561177253723}]}, {"text": "We used mGIZA ( to align the corpus at the character level, and SRILM () to create a character-based 5-gram language model for the target language.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.8831076622009277}]}, {"text": "(2) Baseline system B : multi-joint sequence model for grapheme-to-phoneme convertion.", "labels": [], "entities": []}, {"text": "We applied the Sequitur-G2P 7 toolkit to train a transliteration model.", "labels": [], "entities": []}, {"text": "The difference between the two baseline systems' performance is minor.", "labels": [], "entities": []}, {"text": "Baseline system B seems slightly more efficient than baseline system A, with again of +4.40 BLEU points, as well as reduced translation errors (TER), at -3.58 points and phoneme errors (PER), at -6.20 points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9970752000808716}, {"text": "translation errors (TER)", "start_pos": 124, "end_pos": 148, "type": "METRIC", "confidence": 0.9074532270431519}, {"text": "phoneme errors (PER)", "start_pos": 170, "end_pos": 190, "type": "METRIC", "confidence": 0.8261327624320984}]}, {"text": "By comparing the two baseline systems and systems 1, 2 and 3 (our proposed approach), we note significant results up to 68.60 points for BLEU, and reductions in TER and PER up to 15.92 and 30.03 points, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9976367950439453}, {"text": "TER", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.998166561126709}, {"text": "PER", "start_pos": 169, "end_pos": 172, "type": "METRIC", "confidence": 0.9694601893424988}]}, {"text": "In addition, system 3 performed better than systems A and B, with gains of +7.30 and +2.90 BLEU points, reductions of -8.16 and -4.58 TER points, -14.17 and -7.97 PER points, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9985480904579163}, {"text": "TER", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.9973026514053345}, {"text": "PER", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.9979427456855774}]}, {"text": "In general, the proposed approach performed the transliteration task very well, with significant gains, and reduced the phoneme error rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.8784480392932892}]}, {"text": "We observed that the output quality of the proposed approach, based on recurrent neural networks, was more fluid, coherent and had fewer errors than other systems, that use statistical-based approaches.", "labels": [], "entities": []}, {"text": "All the experimental results showed that using the alignment representation and the pre-trained source and target embeddings resulted in significant advances over other methods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of scoring for all systems : BLEU, TER and PER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9984190464019775}, {"text": "TER", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9950183629989624}, {"text": "PER", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9960424900054932}]}]}