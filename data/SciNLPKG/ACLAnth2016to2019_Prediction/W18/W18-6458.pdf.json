{"title": [{"text": "RTM results for Predicting Translation Performance", "labels": [], "entities": [{"text": "RTM", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8802850842475891}, {"text": "Predicting Translation", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.9739458858966827}]}], "abstractContent": [{"text": "With improved prediction combination using weights based on their training performance and stacking and multilayer perceptrons to build deeper prediction models, RTMs become the 3rd system in general at the sentence-level prediction of translation scores and achieve the lowest RMSE in English to German NMT QET results.", "labels": [], "entities": [{"text": "sentence-level prediction of translation", "start_pos": 207, "end_pos": 247, "type": "TASK", "confidence": 0.7714004367589951}, {"text": "RMSE", "start_pos": 278, "end_pos": 282, "type": "METRIC", "confidence": 0.9181385636329651}, {"text": "NMT QET", "start_pos": 304, "end_pos": 311, "type": "DATASET", "confidence": 0.6374880373477936}]}, {"text": "For the document-level task, we compare document-level RTM models with sentence-level RTM models obtained with the concatenation of document sentences and obtain similar results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality estimation task in WMT18 () (QET18) address machine translation performance prediction, where translation quality is predicted without using reference translations, at the sentence-(Task 1), word-(Task 2), phrase-level (Task 3), and document-levels (Task 4).", "labels": [], "entities": [{"text": "WMT18", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.8444099426269531}, {"text": "machine translation performance prediction", "start_pos": 52, "end_pos": 94, "type": "TASK", "confidence": 0.8107119798660278}]}, {"text": "The tasks contain subtasks involving EnglishGerman phrase-based machine translation (SMT) and neural network-based SMT (NMT), GermanEnglish SMT, English-Latvian SMT and NMT, English-Czech SMT, and English-French SMT.", "labels": [], "entities": [{"text": "EnglishGerman phrase-based machine translation (SMT)", "start_pos": 37, "end_pos": 89, "type": "TASK", "confidence": 0.7366912492683956}, {"text": "GermanEnglish SMT", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.6853612959384918}, {"text": "English-Czech SMT", "start_pos": 174, "end_pos": 191, "type": "TASK", "confidence": 0.507382407784462}, {"text": "English-French SMT", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.5511352717876434}]}, {"text": "Task 1 is about predicting HTER (human-targeted translation edit rate) scores), Task 2 is about binary classification of words, Task 3 is about binary classification of phrases, and Task 4 is about predicting multi-dimensional quality metrics (MQM).", "labels": [], "entities": [{"text": "predicting HTER (human-targeted translation edit rate)", "start_pos": 16, "end_pos": 70, "type": "TASK", "confidence": 0.7928772196173668}, {"text": "predicting multi-dimensional quality metrics (MQM)", "start_pos": 198, "end_pos": 248, "type": "TASK", "confidence": 0.7182033700602395}]}, {"text": "We use referential translation machine (RTM)) models for building our prediction models.", "labels": [], "entities": [{"text": "referential translation machine (RTM))", "start_pos": 7, "end_pos": 45, "type": "TASK", "confidence": 0.8240671902894974}]}, {"text": "RTMs predict data translation between the instances in the training set and the test set using interpretants, data close to the task instances.", "labels": [], "entities": [{"text": "RTMs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9390698671340942}, {"text": "data translation", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.678978368639946}]}, {"text": "Interpretants provide context for the prediction task and are used during the derivation of the features measuring the closeness of the  test sentences to the training data, the difficulty of translating them, and to identify translation acts between any two data sets for building prediction models.", "labels": [], "entities": []}, {"text": "With the enlarging parallel and monolingual corpora made available by WMT, the capability of the interpretant datasets selected by RTM models to provide context for the training and test sets improve.", "labels": [], "entities": [{"text": "WMT", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.8319847583770752}]}, {"text": "depicts RTMs and explains the model building process.", "labels": [], "entities": [{"text": "RTMs", "start_pos": 8, "end_pos": 12, "type": "TASK", "confidence": 0.889943540096283}]}, {"text": "RTMs use parfda for instance selection and machine translation performance prediction system (MTPPS)) for generating features.", "labels": [], "entities": [{"text": "machine translation performance prediction", "start_pos": 43, "end_pos": 85, "type": "TASK", "confidence": 0.6944101676344872}]}, {"text": "The total number of features vary depending on the order of n-grams used (e.g. a log of probability score from the language model for each n-gram is used).", "labels": [], "entities": []}, {"text": "We use ridge regression, kernel ridge regression, k-nearest neighors, support vector regression, AdaBoost, gradient tree boosting, extremely randomized trees (), and multi-layer perceptron) as learning models in combination with feature selection (FS) () and partial least squares (PLS) () where most of these models can be found in scikit-learn.", "labels": [], "entities": [{"text": "ridge regression", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.7120404988527298}, {"text": "kernel ridge regression", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.6513592998186747}]}, {"text": "1 Evaluation metrics listed  are Pearson's correlation (r), mean absolute error (MAE), and root mean squared error (RMSE).", "labels": [], "entities": [{"text": "Pearson's correlation (r)", "start_pos": 33, "end_pos": 58, "type": "METRIC", "confidence": 0.9567806422710419}, {"text": "mean absolute error (MAE)", "start_pos": 60, "end_pos": 85, "type": "METRIC", "confidence": 0.9569985369841257}, {"text": "root mean squared error (RMSE)", "start_pos": 91, "end_pos": 121, "type": "METRIC", "confidence": 0.8927611453192574}]}, {"text": "We use Global Linear Models (GLM)) with dynamic learning (GLMd) for word-and phrase-level translation performance prediction.", "labels": [], "entities": [{"text": "word-and phrase-level translation performance prediction", "start_pos": 68, "end_pos": 124, "type": "TASK", "confidence": 0.7228770077228546}]}, {"text": "GLMd uses weights in a range to update the learning rate dynamically according to the error rate.", "labels": [], "entities": [{"text": "GLMd", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8506159782409668}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of instances and interpretants used.", "labels": [], "entities": []}, {"text": " Table 2: Training results on Task 4 with stacking and prediction averaging. FS RR is the top single model for doc  and FS KR for sent where RR is ridge regression and KR is kernel ridge regression.", "labels": [], "entities": [{"text": "prediction averaging", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.9303914308547974}, {"text": "FS RR", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.598799467086792}]}, {"text": " Table 3: Training results on Task 1 with prediction av- eraging.", "labels": [], "entities": [{"text": "prediction av- eraging", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7109587490558624}]}, {"text": " Table 5: Task 4 test RTM results and the top result in the task.", "labels": [], "entities": [{"text": "RTM", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.7344198226928711}]}, {"text": " Table 6: Test results of RTM in Task 1 where numbers  in parentheses show the rank and corresponding top re- sults. RTM achieves the lowest RMSE in en-de NMT  and becomes the 3rd system in general. r P is Pearson's  correlation and r S is Spearman's correlation.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9783024787902832}, {"text": "Pearson's  correlation", "start_pos": 206, "end_pos": 228, "type": "METRIC", "confidence": 0.8573111693064371}, {"text": "Spearman's correlation", "start_pos": 240, "end_pos": 262, "type": "METRIC", "confidence": 0.5600516398747762}]}]}