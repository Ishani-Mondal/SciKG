{"title": [{"text": "Findings of the 2017 conference on machine translation (WMT17)", "labels": [], "entities": [{"text": "machine translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7775735557079315}, {"text": "WMT17)", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.7225607335567474}]}], "abstractContent": [{"text": "This paper describes the University of Maryland's submission to the WMT 2018 Chinese\u2194English news translation tasks.", "labels": [], "entities": [{"text": "WMT 2018 Chinese\u2194English news translation tasks", "start_pos": 68, "end_pos": 115, "type": "TASK", "confidence": 0.7307200059294701}]}, {"text": "Our systems are BPE-based self-attentional Transformer networks with parallel and backtranslated monolingual training data.", "labels": [], "entities": [{"text": "BPE-based self-attentional Transformer", "start_pos": 16, "end_pos": 54, "type": "TASK", "confidence": 0.6460202932357788}]}, {"text": "Using ensembling and reranking, we improve over the Transformer baseline by +1.4 BLEU for Chinese\u2192English and +3.97 BLEU for English\u2192Chinese on newstest2017.", "labels": [], "entities": [{"text": "Transformer baseline", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.8583908379077911}, {"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9986237287521362}, {"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9980892539024353}, {"text": "newstest2017", "start_pos": 144, "end_pos": 156, "type": "DATASET", "confidence": 0.9608338475227356}]}, {"text": "Our best systems reach BLEU scores of 24.4 for Chinese\u2192English and 39.0 for English\u2192Chinese on newstest2018.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.998838484287262}, {"text": "newstest2018", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.9480158090591431}]}], "introductionContent": [{"text": "While machine translation between Chinese and English has long been considered a challenging task, with performance lagging behind other language pairs, neural architectures have helped achieve large improvements.", "labels": [], "entities": [{"text": "machine translation between Chinese and English", "start_pos": 6, "end_pos": 53, "type": "TASK", "confidence": 0.8526753087838491}]}, {"text": "A new state-of-the-art on Chinese\u2192English news translation was recently obtained () using a deep Transformer model in combination with many other techniques including Dual Learning (, joint training of source-to-target and target-to-source models, and Deliberation Networks (.", "labels": [], "entities": [{"text": "Chinese\u2192English news translation", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.63885138630867}]}, {"text": "The resulting high quality translation comes at the cost of large models and complex training pipelines, which make such models difficult to train and deploy with constrained resources.", "labels": [], "entities": []}, {"text": "In this shared task, our goal is to evaluate the performance of systems inspired by but with fewer and smaller components, which require less time and memory at training and decoding time.", "labels": [], "entities": []}, {"text": "Our systems are based on a multi-layer encoder-decoder architecture with attention mechanism.", "labels": [], "entities": []}, {"text": "We experiment with different network architectures, including single-layer RNN, deep Stacked RNN as used in, and self-attentional Transformer networks (.", "labels": [], "entities": []}, {"text": "The best results are obtained with deep Transformer models.", "labels": [], "entities": []}, {"text": "Our best systems reach BLEU scores of 24.4 for Chinese\u2192English and 39.0 for English\u2192Chinese on newstest2018.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.998838484287262}, {"text": "newstest2018", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.9480158090591431}]}, {"text": "Using a combination of backtranslation (Section 2.2), ensembling, and reranking (Section 2.3) we improve over the base Transformer models by +1.4 BLEU (Chinese\u2192English) and +3.97 BLEU (English\u2192Chinese) on newstest2017.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9977278113365173}, {"text": "BLEU", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.9985994696617126}, {"text": "newstest2017", "start_pos": 205, "end_pos": 217, "type": "DATASET", "confidence": 0.9812920093536377}]}, {"text": "We describe each component of the system (Section 2), and its contribution for each language pair (Section 4).", "labels": [], "entities": []}, {"text": "We show that the impact of backtranslation and reranking is not symmetric in the two translation directions, and that, compared to oracle scores, the reranker leaves much room for improvement.", "labels": [], "entities": []}], "datasetContent": [{"text": "To estimate an upper-bound for reranking methods, we build an oracle that returns the translation in the n-best list that gets the highest BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 139, "end_pos": 149, "type": "METRIC", "confidence": 0.9791395962238312}]}, {"text": "shows the comparison of BLEU scores when using the reranker trained with L2R, R2L, and T2S features versus the oracle.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9993078708648682}]}, {"text": "Increasing the beam size from 30 to 100 doesn't improve the results when using the reranker, but improves the oracle scores.", "labels": [], "entities": []}, {"text": "This is consistent with prior findings that beam search only improves translation quality for narrow beams and deteriorates with larger beams (, but differs in that we rerank the n-best lists instead of adopting the 1-best results from beam search.", "labels": [], "entities": [{"text": "beam search", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.8486044108867645}]}, {"text": "The results also show that better translations according to BLEU exist in the n-best lists with larger beam size, but are ranked low by the models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9990384578704834}]}, {"text": "In addition, we find that the oracle scores are always higher than the reranker scores, and the gap increases with beam size.", "labels": [], "entities": []}, {"text": "When comparing the MSR's best system results (28.46 BLEU achieved by Combo-4 in with the oracle, we find that the oracle score is still higher by 4-5 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9882389307022095}, {"text": "BLEU", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.995055079460144}]}, {"text": "The results show that there is room for improvement by introducing more useful rescoring features and warrant further investigation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data sizes for Chinese/English training (train), validation (valid) and test sets respectively. All statistics  are computed after filtering, tokenization, truecasing, and BPE. The Types column shows the number of distinct  tokens in each data set. The OOVs column shows the number of distinct out-of-vocabulary tokens.", "labels": [], "entities": [{"text": "BPE", "start_pos": 182, "end_pos": 185, "type": "METRIC", "confidence": 0.9863170981407166}]}, {"text": " Table 2: BLEU scores for baseline models on  Chinese\u2192English and English\u2192Chinese new- stest2017. The Size column shows the total number  of parameters.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991850256919861}]}, {"text": " Table 3: Chinese \u2192 English Results on newstest2017.  The submitted system is the last one.", "labels": [], "entities": [{"text": "newstest2017", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9617384672164917}]}, {"text": " Table 5: A comparison of BLEU scores when using  the reranker trained with L2R, R2L, and T2S features  versus the oracle, with varying beam sizes.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9988970756530762}]}]}