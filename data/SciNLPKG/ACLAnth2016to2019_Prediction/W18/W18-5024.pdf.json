{"title": [{"text": "Neural Dialogue Context Online End-of-Turn Detection", "labels": [], "entities": [{"text": "Neural Dialogue Context Online End-of-Turn Detection", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.8072173794110616}]}], "abstractContent": [{"text": "This paper proposes a fully neural network based dialogue-context online end-of-turn detection method that can utilize long-range interactive information extracted from both target speaker's and interlocu-tor's utterances.", "labels": [], "entities": [{"text": "dialogue-context online end-of-turn detection", "start_pos": 49, "end_pos": 94, "type": "TASK", "confidence": 0.598274439573288}]}, {"text": "In the proposed method, we combine multiple time-asynchronous long short-term memory recurrent neu-ral networks, which can capture target speaker's and interlocutor's multiple sequential features, and their interactions.", "labels": [], "entities": []}, {"text": "On the assumption of applying the proposed method to spoken dialogue systems, we introduce target speaker's acoustic sequential features and interlocutor's linguistic sequential features, each of which can be extracted in an online manner.", "labels": [], "entities": []}, {"text": "Our evaluation confirms the effectiveness of taking dialogue context formed by the target speaker's utterances and interlocutor's utterances into consideration.", "labels": [], "entities": []}], "introductionContent": [{"text": "In human-like spoken dialogue systems, end-ofturn detection that determines whether a target speaker's utterance is ended or not is an essential technology (.", "labels": [], "entities": [{"text": "end-ofturn detection", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.7358203828334808}]}, {"text": "It is widely known that heuristic end-of-turn detection based on nonspeech duration determined by speech activity detection (SAD) is insufficient for smooth turntaking ().", "labels": [], "entities": [{"text": "speech activity detection (SAD)", "start_pos": 98, "end_pos": 129, "type": "TASK", "confidence": 0.6945425619681677}]}, {"text": "Various methods have been examined for modeling the end-of-turn detection (.", "labels": [], "entities": [{"text": "end-of-turn detection", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.6505063325166702}]}, {"text": "A general approach is discriminative modeling using acoustic or linguistic features extracted from target speaker's current utterance.", "labels": [], "entities": []}, {"text": "In addition, recent studies use recurrent neural networks (RNNs) as they are suitable for directly capturing long-range sequential features without manual specification of fixed length features such as maximum, minimum, average values of acoustic features or bag-of-words features ( We note, however, that interlocutor's utterances are rarely used for end-of-turn detection.", "labels": [], "entities": [{"text": "end-of-turn detection", "start_pos": 352, "end_pos": 373, "type": "TASK", "confidence": 0.6948471963405609}]}, {"text": "In dialogues, target speaker's utterances are definitely impacted by the interlocutor's utterances.", "labels": [], "entities": []}, {"text": "It is expected that we can improve end-of-utterance detection performance by capturing the \"interaction\" between the target speaker and the interlocutor.", "labels": [], "entities": [{"text": "end-of-utterance detection", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.6921317130327225}]}, {"text": "In this paper, we propose a neural dialoguecontext online end-of-turn detection method that can flexibly utilize both target speaker's and interlocutor's utterances.", "labels": [], "entities": [{"text": "dialoguecontext online end-of-turn detection", "start_pos": 35, "end_pos": 79, "type": "TASK", "confidence": 0.530426487326622}]}, {"text": "To the best of our knowledge, this paper is the first study to utilize dialoguecontext information for neural end-of-turn detection.", "labels": [], "entities": [{"text": "neural end-of-turn detection", "start_pos": 103, "end_pos": 131, "type": "TASK", "confidence": 0.6551061471303304}]}, {"text": "Although some natural language processing tasks recently examine dialogue-context modeling (, they cannot handle multiple acoustic and lexical features individually extracted from both target speaker's and interlocutor's utterances.", "labels": [], "entities": [{"text": "dialogue-context modeling", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.7505901753902435}]}, {"text": "In the proposed method, target speaker's and interlocutor's multiple sequential features, and their interactions are captured by stacking multiple time-asynchronous long short-term memory RNNs (LSTM-RNNs).", "labels": [], "entities": []}, {"text": "In order to achieve low-delayed end-of-turn detection in spoken dialogue systems, acoustic sequential features extracted from target speaker's speech and linguistic sequential features extracted from the interlocutor's (system's) responses are used for capturing interactive information.", "labels": [], "entities": []}, {"text": "In our experiments, human-human contact center dialogue data sets are used with the goal of constructing a human-like interactive voice response system.", "labels": [], "entities": []}, {"text": "We show that the proposed method outperforms a variant that uses only target speaker's utterances.", "labels": [], "entities": []}], "datasetContent": [{"text": "This paper employed Japanese simulated contact center dialogue data sets instead of humancomputer dialogue data sets.", "labels": [], "entities": []}, {"text": "The data sets include 330 dialogues and 6 topics.", "labels": [], "entities": []}, {"text": "One dialogue means one telephone call between one operator and one customer, in which each speaker's speech was separately recorded.", "labels": [], "entities": []}, {"text": "In order to simulated interactive voice response applications, we regard the operator as the interlocutor, and the customer as the target speaker.", "labels": [], "entities": []}, {"text": "We divided each data set into speech units and non-speech units using an LSTM-RNN based SAD () trained using various Japanese speech data.", "labels": [], "entities": []}, {"text": "An utterance is defined as a unit surrounded by non-speech units whose   duration is more than 100 ms.", "labels": [], "entities": []}, {"text": "Turn-taking points and backchannel points were manually annotated for all dialogues.", "labels": [], "entities": []}, {"text": "The evaluation used 6-fold cross validation in which training and validation data were 5 topics and test data were 1 topic.", "labels": [], "entities": []}, {"text": "Detailed setups are shown in where #calls, #utter-ances, and #turns represent number of calls, utterances and end-of-turn points, respectively.", "labels": [], "entities": []}, {"text": "To realize a comprehensive evaluation, we examined various conditions.", "labels": [], "entities": []}, {"text": "In the proposed modeling, unit size of LSTM-RNNs was unified to 256.", "labels": [], "entities": []}, {"text": "For training, the mini-batch size was set to 2 calls.", "labels": [], "entities": []}, {"text": "The optimizer was Adam with the default setting.", "labels": [], "entities": []}, {"text": "Note that apart of the training sets were used as the data sets employed for early stopping.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.9050851464271545}]}, {"text": "We constructed five models by varying an initial parameter for individual conditions and evaluated the average performance.", "labels": [], "entities": []}, {"text": "When using either target speaker's utterances or interlocutor's utterances, required components were only used for building the proposed modeling.", "labels": [], "entities": []}, {"text": "We used following sequential features.", "labels": [], "entities": []}, {"text": "F0 represents 2 dimensional sequential features of F0 and \u2206F0; frame shift was set to 5 ms.", "labels": [], "entities": [{"text": "F0", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.6679732799530029}, {"text": "frame shift", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.8722249567508698}]}, {"text": "SENONE represents 256-dimensional senone bottleneck features extracted from 3-layer senone LSTM-RNN with 256 units trained from a corpus of spontaneous Japanese speech ().", "labels": [], "entities": [{"text": "SENONE", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7544508576393127}]}, {"text": "Its frame shift was set to 10 ms, and the bottleneck layer was set to the third LSTM-RNN layer.", "labels": [], "entities": [{"text": "frame shift", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.878711998462677}]}, {"text": "PRON represents pronunciation sequences, and WORD represents word sequences of interlocutor's utterances.", "labels": [], "entities": [{"text": "WORD", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9370590448379517}]}, {"text": "The lexical features were introduced by converting them into 128 dimensional vectors through linear transformation that was also optimized in training.", "labels": [], "entities": []}, {"text": "We used the evaluation metrics of recall, precision, macro F-value, and accuracy.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9996113181114197}, {"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9993466734886169}, {"text": "F-value", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.7527450919151306}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.99857497215271}]}, {"text": "The results gained when using only target speaker's utterances are shown in (1)-(3).", "labels": [], "entities": []}, {"text": "In terms of F-value and accuracy, (3) outperformed and.", "labels": [], "entities": [{"text": "F-value", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9990647435188293}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9997192025184631}]}, {"text": "This confirms that stacked time-asynchronous sequential network based modeling is effective for combining multiple sequential features.", "labels": [], "entities": []}, {"text": "The results gained when using only interlocutor's utterances are shown in (4)-(6).", "labels": [], "entities": []}, {"text": "Among them, (6) attained the best performance although its performance was inferior to (1)-(3).", "labels": [], "entities": []}, {"text": "In fact, (4)-(6) outperformed random end-of-turn decision making.", "labels": [], "entities": [{"text": "decision making", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.8035354018211365}]}, {"text": "This indicates interlocutor's utterances are effective in improving online end-of-turn detection performance.", "labels": [], "entities": [{"text": "online end-of-turn detection", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.565995991230011}]}, {"text": "The proposed method, which takes both target speaker's and interlocutor's utterances into consideration, is shown in (7) and.", "labels": [], "entities": []}, {"text": "In terms of Fvalue and accuracy, (7) outperformed (2) and (5).", "labels": [], "entities": [{"text": "Fvalue", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9992250204086304}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9996315240859985}]}, {"text": "These results indicate that interaction information is effective for detecting end-of-turn points.", "labels": [], "entities": []}, {"text": "The best results were attained by (8), which utilized both multiple target speaker's features and multiple interlocutor's features.", "labels": [], "entities": []}, {"text": "The sign test results verified that (8) achieved statistically significant performance improvement (p < 0.05) over (3).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experimental results: Recall (%), Precision (%), F-value (%), and Accuracy (%).", "labels": [], "entities": [{"text": "Recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9994961023330688}, {"text": "Precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.999441921710968}, {"text": "F-value", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9994065761566162}, {"text": "Accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9996877908706665}]}, {"text": " Table 1: Experimental data sets.", "labels": [], "entities": []}]}