{"title": [{"text": "The Fact Extraction and VERification (FEVER) Shared Task", "labels": [], "entities": [{"text": "Fact Extraction and VERification (FEVER)", "start_pos": 4, "end_pos": 44, "type": "METRIC", "confidence": 0.6799522084849221}]}], "abstractContent": [{"text": "We present the results of the first Fact Extraction and VERification (FEVER) Shared Task.", "labels": [], "entities": [{"text": "Fact Extraction and VERification (FEVER)", "start_pos": 36, "end_pos": 76, "type": "METRIC", "confidence": 0.6675742438861302}]}, {"text": "The task challenged participants to classify whether human-written factoid claims could be SUPPORTED or REFUTED using evidence retrieved from Wikipedia.", "labels": [], "entities": [{"text": "REFUTED", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9279031157493591}]}, {"text": "We received entries from 23 competing teams, 19 of which scored higher than the previously published base-line.", "labels": [], "entities": []}, {"text": "The best performing system achieved a FEVER score of 64.21%.", "labels": [], "entities": [{"text": "FEVER score", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.9877842962741852}]}, {"text": "In this paper, we present the results of the shared task and a summary of the systems, highlighting com-monalities and innovations among participating systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information extraction is a well studied domain and the outputs of such systems enable many natural language technologies such as question answering and text summarization.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8209748864173889}, {"text": "question answering", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.9215228259563446}, {"text": "text summarization", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.7464115619659424}]}, {"text": "However, since information sources can contain errors, there exists an additional need to verify whether the information is correct.", "labels": [], "entities": []}, {"text": "For this purpose, we hosted the first Fact Extraction and VERification (FEVER) shared task to raise interest in and awareness of the task of automatic information verificationa research domain that is orthogonal to information extraction.", "labels": [], "entities": [{"text": "Fact Extraction and VERification (FEVER)", "start_pos": 38, "end_pos": 78, "type": "METRIC", "confidence": 0.6633612300668444}, {"text": "information verificationa research domain", "start_pos": 151, "end_pos": 192, "type": "TASK", "confidence": 0.7765325233340263}, {"text": "information extraction", "start_pos": 215, "end_pos": 237, "type": "TASK", "confidence": 0.7703665792942047}]}, {"text": "This shared task required participants to develop systems to predict the veracity of human-generated textual claims against textual evidence to be retrieved from Wikipedia.", "labels": [], "entities": []}, {"text": "We constructed a purpose-built dataset for this task) that contains 185,445 human-generated claims, manually verified against the introductory sections of Wikipedia pages and labeled as SUPPORTED, REFUTED or NOTENOUGHINFO.", "labels": [], "entities": [{"text": "REFUTED", "start_pos": 197, "end_pos": 204, "type": "METRIC", "confidence": 0.9844471216201782}]}, {"text": "The claims were generated by paraphrasing facts from Wikipedia and mutating them in a variety of ways, some of which were meaning-altering.", "labels": [], "entities": []}, {"text": "For each claim, and without the knowledge of where the claim was generated from, annotators selected evidence in the form of sentences from Wikipedia to justify the labeling of the claim.", "labels": [], "entities": []}, {"text": "The systems participating in the FEVER shared task were required to label claims with the correct class and also return the sentence(s) forming the necessary evidence for the assigned label.", "labels": [], "entities": [{"text": "FEVER shared task", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.6231352587540945}]}, {"text": "Performing well at this task requires both identifying relevant evidence and reasoning correctly with respect to the claim.", "labels": [], "entities": []}, {"text": "A key difference between this task and other textual entailment and natural language inference tasks () is the need to identify the evidence from a large textual corpus.", "labels": [], "entities": []}, {"text": "Furthermore, in comparison to large-scale question answering tasks, systems must reason about information that is not present in the claim.", "labels": [], "entities": [{"text": "question answering tasks", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.7918874323368073}]}, {"text": "We hope that research in these fields will be stimulated by the challenges present in FEVER.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.6402164101600647}]}, {"text": "One of the limitations of using human annotators to identify correct evidence when constructing the dataset was the trade-off between annotation velocity and evidence recall (.", "labels": [], "entities": [{"text": "recall", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.9315377473831177}]}, {"text": "Evidence selected by annotators was often incomplete.", "labels": [], "entities": []}, {"text": "As part of the FEVER shared task, any evidence retrieved by participating systems that was not contained in the original dataset was annotated and used to augment the evidence in the test set.", "labels": [], "entities": [{"text": "FEVER", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9343093633651733}]}, {"text": "In this paper, we present a short description of the task and dataset, present a summary of the submissions and the leader board, and highlight future research directions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Dataset split sizes for SUPPORTED, REFUTED  and NOTENOUGHINFO (NEI) classes", "labels": [], "entities": [{"text": "REFUTED", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9465845823287964}]}, {"text": " Table 2: Results on the test dataset.", "labels": [], "entities": []}]}