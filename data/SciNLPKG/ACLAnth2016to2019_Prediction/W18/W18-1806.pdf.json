{"title": [{"text": "Context Models for OOV Word Translation in Low-Resource Languages", "labels": [], "entities": [{"text": "OOV Word Translation", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.7455730636914571}]}], "abstractContent": [{"text": "Out-of-vocabulary word translation is a major problem for the translation of low-resource languages that suffer from alack of parallel training data.", "labels": [], "entities": [{"text": "Out-of-vocabulary word translation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6988191604614258}]}, {"text": "This paper evaluates the contributions of target-language context models towards the translation of OOV words, specifically in those cases where OOV translations are derived from external knowledge sources, such as dictionaries.", "labels": [], "entities": [{"text": "translation of OOV words", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.8292906135320663}]}, {"text": "We develop both neural and non-neural context models and evaluate them within both phrase-based and self-attention based neural machine translation systems.", "labels": [], "entities": [{"text": "phrase-based and self-attention based neural machine translation", "start_pos": 83, "end_pos": 147, "type": "TASK", "confidence": 0.7037771684782845}]}, {"text": "Our results show that neural language models that integrate additional context beyond the current sentence are the most effective in disambiguating possible OOV word translations.", "labels": [], "entities": [{"text": "OOV word translations", "start_pos": 157, "end_pos": 178, "type": "TASK", "confidence": 0.7001990874608358}]}, {"text": "We present an efficient second-pass lattice-rescoring method for wide-context neural language models and demonstrate performance improvements over state-of-the-art self-attention based neural MT systems in five out of six low-resource language pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Translation of out-of-vocabulary (OOV) words (words occurring in the test data but not in the training data) is of major importance in statistical machine translation (MT).", "labels": [], "entities": [{"text": "Translation of out-of-vocabulary (OOV) words", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8477785502161298}, {"text": "statistical machine translation (MT)", "start_pos": 135, "end_pos": 171, "type": "TASK", "confidence": 0.7692689200242361}]}, {"text": "It is a particularly difficult problem in low-resource languages, i.e., languages for which parallel training data is extremely sparse, requiring recourse to techniques that are complementary to standard statistical machine translation approaches.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 204, "end_pos": 235, "type": "TASK", "confidence": 0.662955234448115}]}, {"text": "The approaches described in this paper were developed for scenarios where the training data comprises at most 100k sentences pairs.", "labels": [], "entities": []}, {"text": "Most previous studies in this area have focused on how to generate translation candidates for OOV words, either by segmentation into subword units, projection from other languages, or by leveraging external knowledge sources like dictionaries.", "labels": [], "entities": []}, {"text": "Often, however, these methods generate multiple candidates for each OOV word, and the MT system is insufficiently trained to choose the appropriate translation according to the context.", "labels": [], "entities": [{"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.931439220905304}]}, {"text": "In this paper we address this problem by utilizing more sophisticated context models based on target-language information.", "labels": [], "entities": []}, {"text": "In particular, we develop wide-context models that incorporate information from context beyond current sentence boundaries to resolve translation ambiguity.", "labels": [], "entities": []}, {"text": "We compare these against models incorporating information from the current sentence only, and evaluate neural models vs. count-based sentence completion and graph reranking models.", "labels": [], "entities": []}, {"text": "All are evaluated within both phrase-based and attention-based neural machine translation models for 6 low-resource language pairs.", "labels": [], "entities": [{"text": "attention-based neural machine translation", "start_pos": 47, "end_pos": 89, "type": "TASK", "confidence": 0.6531251221895218}]}, {"text": "Our paper makes several contributions: \u2022 We evaluate recently proposed neural machine translation (NMT) architectures (purely attention-based neural MT) on low-resource languages and show that, contrary to previous results obtained with sequence-to-sequence models, neural MT performs similarly to phrase-based machine translation (PBMT) in these scenarios, without modifications to the basic model.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 71, "end_pos": 103, "type": "TASK", "confidence": 0.8551385700702667}, {"text": "phrase-based machine translation (PBMT)", "start_pos": 298, "end_pos": 337, "type": "TASK", "confidence": 0.7780154744784037}]}, {"text": "\u2022 We develop and compare several wide-context target-language based models for translation disambiguation and find that document-level neural language models are the most effective at resolving translation ambiguities.", "labels": [], "entities": [{"text": "translation disambiguation", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.9754002690315247}, {"text": "resolving translation ambiguities", "start_pos": 184, "end_pos": 217, "type": "TASK", "confidence": 0.764595607916514}]}, {"text": "\u2022 We present an efficient lattice rescoring algorithm for wide-context neural language models.", "labels": [], "entities": []}, {"text": "\u2022 We compare our approach against directly adding external translation resources to the training data and show that our approach provides small but consistent improvements on five out of six language pairs.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses prior work on OOV translation.", "labels": [], "entities": [{"text": "OOV translation", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.8227920532226562}]}, {"text": "Section 3 describes our general approach and presents various context models for translation disambiguation.", "labels": [], "entities": [{"text": "translation disambiguation", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.9802346527576447}]}, {"text": "Section 4 describes the datasets and baseline systems.", "labels": [], "entities": []}, {"text": "Section 5 provides experimental results followed by a final conclusion in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "As an additional baseline we integrate the externally derived translations by simply adding them to the parallel training data, i.e., each translation pair is treated as an additional 'sentence'.", "labels": [], "entities": []}, {"text": "In this scenario, each of the new translation pairs is seen only once and without context; the final translation choice is still made by the MT system that has models, even when using a larger maximum phrase length., we observe only mild improvements, except for Uighur, where the improvement is more pronounced.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of sentence pairs/documents/target language words in the training, develop- ment and test sets for each language.", "labels": [], "entities": []}, {"text": " Table 2: BLEU/unigram precision on test sets for phrase-based MT (PBMT), Transformer  model, and Transformer model with byte-pair encoding (BPE).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988335967063904}, {"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9425117373466492}]}, {"text": " Table 3: Vocabulary sizes, OOV rates, coverage, accuracy of external translation sources, and  average number of translation candidates per OOV word.", "labels": [], "entities": [{"text": "coverage", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9613721966743469}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9987755417823792}]}, {"text": " Table 4: Results (BLEU/unigram precision) of adding external translations to parallel training  data. Boldface numbers are improvements over the best baseline system from Table 2.", "labels": [], "entities": [{"text": "BLEU/unigram precision", "start_pos": 19, "end_pos": 41, "type": "METRIC", "confidence": 0.5973574370145798}]}, {"text": " Table 5: Perplexities obtained by LSTM vs. DCLM on dev sets.", "labels": [], "entities": []}, {"text": " Table 6: BLEU/unigram precision for lattice rescoring of PBMT output with reference transla- tion included (diagnostic experiment).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988998174667358}, {"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9457127451896667}]}, {"text": " Table 7: BLEU/unigram precision of (1) baseline system without OOV handling; (2) systems  trained with external translations as additional training data; (3) lattice rescoring with context  models; (4) oracle; (5) full system combination of PBMT and Transformer outputs plus OOV  translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989051818847656}, {"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.8872962594032288}, {"text": "lattice rescoring", "start_pos": 159, "end_pos": 176, "type": "TASK", "confidence": 0.8082106113433838}, {"text": "OOV  translation", "start_pos": 276, "end_pos": 292, "type": "TASK", "confidence": 0.6552882045507431}]}]}