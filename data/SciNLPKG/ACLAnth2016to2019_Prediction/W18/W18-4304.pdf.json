{"title": [{"text": "Identifying the Discourse Function of News Article Paragraphs", "labels": [], "entities": [{"text": "Identifying the Discourse Function of News Article Paragraphs", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.8340039104223251}]}], "abstractContent": [{"text": "Discourse structure is a key aspect of all forms of text, providing valuable information both to humans and machines.", "labels": [], "entities": [{"text": "Discourse structure", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7578693628311157}]}, {"text": "We applied the hierarchical theory of news discourse developed by van Dijk (1988) to examine how paragraphs operate as units of discourse structure within news articles-what we refer to here as document-level discourse.", "labels": [], "entities": []}, {"text": "This document-level discourse provides a characterization of the content of each paragraph that describes its relation to the events presented in the article (such as main events, backgrounds, and consequences) as well as to other components of the story (such as commentary and evaluation).", "labels": [], "entities": []}, {"text": "The purpose of a news discourse section is of great utility to story understanding as it affects both the importance and temporal order of items introduced in the text-therefore, if we know the news discourse purpose for different sections, we should be able to better rank events for their importance and better construct timelines.", "labels": [], "entities": [{"text": "story understanding", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7313847243785858}]}, {"text": "We test two hypotheses: first, that people can reliably annotate news articles with van Dijk's theory; second, that we can reliably predict these labels using machine learning.", "labels": [], "entities": []}, {"text": "We show that people have a high degree of agreement with each other when annotating the theory (F 1 > 0.8, \u03ba > 0.6), demonstrating that it can be both learned and reliably applied by human annotators.", "labels": [], "entities": [{"text": "F 1 > 0.8", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9530575275421143}]}, {"text": "Additionally, we demonstrate first steps toward machine learning of the theory, achieving a performance of F 1 = 0.54, which is 65% of human performance.", "labels": [], "entities": [{"text": "F 1", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9919576048851013}]}, {"text": "Moreover , we have generated a gold-standard, adjudicated corpus of 50 documents for document-level discourse annotation based on the ACE Phase 2 corpus (NIST, 2002).", "labels": [], "entities": [{"text": "document-level discourse annotation", "start_pos": 85, "end_pos": 120, "type": "TASK", "confidence": 0.6003087560335795}, {"text": "ACE Phase 2 corpus (NIST, 2002)", "start_pos": 134, "end_pos": 165, "type": "DATASET", "confidence": 0.8880841533342997}]}], "introductionContent": [{"text": "Discourse structure is a key aspect of all forms of text, providing valuable information about the contents of a given span of text.", "labels": [], "entities": [{"text": "Discourse structure", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7146096229553223}]}, {"text": "This is most obvious in academic, legal, and technical texts, which are often clearly delineated into sections containing, for example, introductory, background, or explanatory material, among others-these type of texts are designed to make it easy to find specific information within them quickly.", "labels": [], "entities": []}, {"text": "News articles have a similarly helpful, though implicit, design: they often provide a brief, up-front summary of the important events, relevant background information, comments from both experts and the reporters, and detailed descriptions of the main events.", "labels": [], "entities": []}, {"text": "Events are often not presented in chronological order, but rather structured by importance.", "labels": [], "entities": []}, {"text": "We use an established hierarchical theory of news discourse to model how paragraphs operate as units of discourse structure within news articles to capture the importance of events within a story.", "labels": [], "entities": []}, {"text": "We test two hypotheses: first, that humans can reliably annotate news articles with van Dijk's theory; second, that these discourse labels can be predicted by machine learning.", "labels": [], "entities": []}, {"text": "In our first hypothesis, by reliable we specifically mean that independent people agree with each other when applying van Dijk's theory of news discourse.", "labels": [], "entities": []}, {"text": "We performed an annotation study to answer this question, producing a small corpus of gold-standard, adjudicated annotations in a standoff format based on the Automated Content Extraction (ACE) Phase 2 corpus.", "labels": [], "entities": [{"text": "Automated Content Extraction (ACE) Phase 2 corpus", "start_pos": 159, "end_pos": 208, "type": "DATASET", "confidence": 0.7236954106224908}]}, {"text": "This corpus consists of 50 documents (28,236 words; 644 paragraphs) annotated at the paragraph level.", "labels": [], "entities": []}, {"text": "Agreement was notable, with F 1 > 0.75 and Cohen's \u03ba > 0.60 (see \u00a74.3 for details).", "labels": [], "entities": [{"text": "F 1", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9837614297866821}]}, {"text": "These results show that van Dijk's theory of can be both learned and reliably applied by humans to news article.", "labels": [], "entities": []}, {"text": "To address our second hypothesis, we demonstrate a machine learning approach using support vector machine (SVM) for learning to tag paragraphs with labels from van Dijk's theory.", "labels": [], "entities": []}, {"text": "We achieve a performance of F 1 = 0.54, which is 65% of human performance.", "labels": [], "entities": [{"text": "F 1", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9947299361228943}]}, {"text": "We also demonstrate the performance of other machine learning algorithms (decision tree and random forest) and provide the set of features that perform the best on this task.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first introduce some of the existing related work ( \u00a72), and then provide a definition of van Dijk's theory as was presented to our annotators ( \u00a73).", "labels": [], "entities": []}, {"text": "We next describe the selection of texts used in this study, provide corpus statistics, describe the training and annotation procedures for the study, and describe the results of the annotation study and provide some discussion on these results ( \u00a74).", "labels": [], "entities": []}, {"text": "We then provide an automated method to learn and predict the discourse-structure labels of a plain document ( \u00a75), followed by discussion on the results of label prediction and some remarks of possible future directions ( \u00a76).", "labels": [], "entities": [{"text": "label prediction", "start_pos": 156, "end_pos": 172, "type": "TASK", "confidence": 0.7466380000114441}]}, {"text": "Finally, we summarize our contributions ( \u00a77).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Corpus-wide statistics on the relevant lexical features for annotating the news articles.", "labels": [], "entities": []}, {"text": " Table 2: Microaveraged agreement measures between the annotators (A1, A2), adjudicator (Adj.), and  the merged gold standard (Gold)-including precision (P ), recall (R), balanced F-measure (F 1 ), relative  observed agreement among raters (p 0 ), probability of chance agreement (p e ), and Cohen's kappa (\u03ba,  derived from p 0 and p e ).", "labels": [], "entities": [{"text": "precision (P )", "start_pos": 143, "end_pos": 157, "type": "METRIC", "confidence": 0.9597173035144806}, {"text": "recall (R)", "start_pos": 159, "end_pos": 169, "type": "METRIC", "confidence": 0.9559158384799957}, {"text": "balanced F-measure (F 1 )", "start_pos": 171, "end_pos": 196, "type": "METRIC", "confidence": 0.8351988196372986}]}, {"text": " Table 3: Distribution of the labels within the annotated corpus. The majority of paragraphs fall under the  categories of verbal reactions or circumstances.", "labels": [], "entities": []}, {"text": " Table 4: Results from label prediction using SVM. All results are micro-averaged across instances,  including precision (P ), recall (R), and balanced F-measure (F 1 ). For the final three classifiers, all four  features are described in  \u00a75.1.", "labels": [], "entities": [{"text": "label prediction", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7922145128250122}, {"text": "precision (P )", "start_pos": 111, "end_pos": 125, "type": "METRIC", "confidence": 0.9425526410341263}, {"text": "recall (R)", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.9461133629083633}, {"text": "F-measure (F 1 )", "start_pos": 152, "end_pos": 168, "type": "METRIC", "confidence": 0.9350218057632447}]}, {"text": " Table 5: Per-label F 1 results. Best performance occurs for the lead, circumstances, and verbal reactions.", "labels": [], "entities": [{"text": "Per-label F 1", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.771024207274119}]}]}