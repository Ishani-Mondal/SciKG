{"title": [{"text": "Ling@CASS Solution to the NLP-TEA CGED Shared Task 2018", "labels": [], "entities": [{"text": "NLP-TEA CGED Shared Task 2018", "start_pos": 26, "end_pos": 55, "type": "DATASET", "confidence": 0.7739883184432983}]}], "abstractContent": [{"text": "In this study, we employ the sequence to sequence learning to model the task of grammar error correction.", "labels": [], "entities": [{"text": "grammar error correction", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.588870773712794}]}, {"text": "The system takes potentially erroneous sentences as inputs, and outputs correct sentences.", "labels": [], "entities": []}, {"text": "To breakthrough the bottlenecks of very limited size of manually labeled data, we adopt a semi-supervised approach.", "labels": [], "entities": []}, {"text": "Specifically, we adapt correct sentences written by native Chinese speakers to generate pseudo grammatical errors made by learners of Chinese as a second language.", "labels": [], "entities": []}, {"text": "We use the pseudo data to pre-train the model, and the CGED data to fine-tune it.", "labels": [], "entities": [{"text": "CGED data", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.9497659504413605}]}, {"text": "Being aware of the significance of precision in a grammar error correction system in real scenarios, we use ensembles to boost precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9952308535575867}, {"text": "grammar error correction", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6493849257628123}, {"text": "precision", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9976593255996704}]}, {"text": "When using inputs as simple as Chi-nese characters, the ensembled system achieves a precision at 86.56% in the detection of erroneous sentences, and a precision at 51.53% in the correction of errors of Selection and Missing types.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9993835687637329}, {"text": "precision", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.9985664486885071}]}], "introductionContent": [{"text": "An inter-language is an idiolect developed by a learner of a second language (or L2).", "labels": [], "entities": []}, {"text": "It is characteristic that it preserves some features of the first language (or L1), and can overgeneralize some L2 linguistic rules.", "labels": [], "entities": []}, {"text": "An investigation on the grammatical errors made by L2 learners will disclose the error patterns, which are beneficial to the teaching and learning process.", "labels": [], "entities": []}, {"text": "On the other hand, it will promote the development of systems which can correct grammatical errors made by L2 learners automatically.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 briefly introduces the definition of the NLP-TEA CGED Shared Task 2018.", "labels": [], "entities": [{"text": "NLP-TEA CGED Shared Task 2018", "start_pos": 99, "end_pos": 128, "type": "DATASET", "confidence": 0.6130509734153747}]}, {"text": "Section 3 gives a quick review on previous studies.", "labels": [], "entities": []}, {"text": "Section 4 describes the generation of pseudo data in detail.", "labels": [], "entities": []}, {"text": "Section 5 introduces the modeling of the correction task using sequence to sequence learning.", "labels": [], "entities": []}, {"text": "Section 6 analyses the experimental results.", "labels": [], "entities": []}, {"text": "Finally, conclusions and prospects are drawn in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "As shown in, we have four basic system configurations.", "labels": [], "entities": []}, {"text": "These configurations are different in the use of pseudo corpus and POS tags.", "labels": [], "entities": []}, {"text": "The evaluation in reveals that the use of pseudo data has improved both precision and recall in the correction task of the word selection errors and missing errors, while that of POS tags does not make a significant contribution.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9995256662368774}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9991706609725952}]}, {"text": "In real scenarios of grammar error diagnoses, the evaluation metrics of precision, recall and F1 are not of the same importance.", "labels": [], "entities": [{"text": "grammar error diagnoses", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.6773472229639689}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9995824694633484}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9992094039916992}, {"text": "F1", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.9991793036460876}]}, {"text": "A teacher would always prefers a grammar error correction system with high precision, even if it has a low recall, than a system returns lots of noises.", "labels": [], "entities": [{"text": "grammar error correction", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.5740951399008433}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9978927969932556}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.998502254486084}]}, {"text": "Being aware of the significance of precision in a grammar error correction system in practice, we further use ensembles to boost precisions.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9976164102554321}, {"text": "grammar error correction", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6554400026798248}, {"text": "precisions", "start_pos": 129, "end_pos": 139, "type": "METRIC", "confidence": 0.9962390661239624}]}, {"text": "The tag \"(>1)\" indicates that the correction has been confirmed by at least two basic systems; and \"(>2)\", at least three.", "labels": [], "entities": [{"text": "correction", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9418817758560181}]}, {"text": "The ensembled systems steadily achieve a precision greater than 50%, with a recall greater than 8%.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9996813535690308}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9997667670249939}]}, {"text": "These performances are much higher than the best in CGED 2018 submissions, where the precision is 29.32%, and recall is 1.58%.", "labels": [], "entities": [{"text": "CGED 2018 submissions", "start_pos": 52, "end_pos": 73, "type": "DATASET", "confidence": 0.9052970210711161}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9997532963752747}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9997891783714294}]}, {"text": "The official submission of our team to CGED 2018 is the result of an ensemble of the systems 3 and 4, where the results are simply merged.", "labels": [], "entities": []}, {"text": "Figure 1: Impacts of Pseudo Data We also evaluated the systems on the detections, and the identifications of error types and positions.", "labels": [], "entities": []}, {"text": "shows a detailed analysis on the precision of the identification of error positions for all four types of errors.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9990485310554504}]}, {"text": "It reveals shows that the identification of the positions of these errors is of different difficulties to the systems.", "labels": [], "entities": []}, {"text": "While the ensembled systems are proficient in handling word ordering errors, they have the most difficulties in handling redundant errors.", "labels": [], "entities": [{"text": "word ordering errors", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.7617139418919882}]}, {"text": "shows the ensembled system 1+3 (>1) achieves a False Positive Rate (FPR) at 4.48% and a precision of 86.56% the detection of erroneous sentences, which are better than the best FPR 4.99% and the best precision 82.76% in CGED 2018 submissions, respectively.", "labels": [], "entities": [{"text": "False Positive Rate (FPR)", "start_pos": 47, "end_pos": 72, "type": "METRIC", "confidence": 0.9541490276654562}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9987167119979858}, {"text": "precision", "start_pos": 200, "end_pos": 209, "type": "METRIC", "confidence": 0.9692079424858093}, {"text": "CGED 2018 submissions", "start_pos": 220, "end_pos": 241, "type": "DATASET", "confidence": 0.9539578755696615}]}], "tableCaptions": [{"text": " Table 1: Performances on Corrections", "labels": [], "entities": [{"text": "Corrections", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.5532997250556946}]}, {"text": " Table 2: Performances on Detections, Identifications of Error Types & Positions", "labels": [], "entities": []}]}