{"title": [{"text": "Correcting Length Bias in Neural Machine Translation", "labels": [], "entities": [{"text": "Correcting Length Bias", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5781772633393606}, {"text": "Neural Machine Translation", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.6942692001660665}]}], "abstractContent": [{"text": "We study two problems in neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.8195947706699371}]}, {"text": "First, in beam search, whereas a wider beam should in principle help translation, it often hurts NMT.", "labels": [], "entities": [{"text": "beam search", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8852347433567047}, {"text": "translation", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.9805441498756409}]}, {"text": "Second, NMT has a tendency to produce translations that are too short.", "labels": [], "entities": []}, {"text": "Here, we argue that these problems are closely related and both rooted in label bias.", "labels": [], "entities": []}, {"text": "We show that correcting the brevity problem almost eliminates the beam problem; we compare some commonly-used methods for doing this, finding that a simple per-word reward works well; and we introduce a simple and quick way to tune this reward using the perceptron algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "Although highly successful, neural machine translation (NMT) systems continue to be plagued by a number of problems.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.8213557700316111}]}, {"text": "We focus on two here: the beam problem and the brevity problem.", "labels": [], "entities": [{"text": "beam problem", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.9167332947254181}]}, {"text": "First, machine translation systems rely on heuristics to search through the intractably large space of possible translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7755647301673889}]}, {"text": "Most commonly, beam search is used during the decoding process.", "labels": [], "entities": [{"text": "beam search", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9227120578289032}]}, {"text": "Traditional statistical machine translation systems often rely on large beams to find good translations.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.6309678057829539}]}, {"text": "However, in neural machine translation, increasing the beam size has been shown to degrade performance.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.67292187611262}]}, {"text": "This is the last of the six challenges identified by.", "labels": [], "entities": []}, {"text": "The second problem, noted by several authors, is that NMT tends to generate translations that are too short. and Koehn and Knowles address this by dividing translation scores by their length, inspired by work on audio chords.", "labels": [], "entities": []}, {"text": "A similar method is also used by Google's production system ( . A third simple method used by various authors) is a tunable reward added for each output word.  and propose variations of this reward that enable better guarantees during search.", "labels": [], "entities": []}, {"text": "In this paper, we argue that these two problems are related (as hinted at by and that both stem from label bias, an undesirable property of models that generate sentences word byword instead of all at once.", "labels": [], "entities": []}, {"text": "The typical solution is to introduce a sentencelevel correction to the model.", "labels": [], "entities": [{"text": "sentencelevel correction", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.6835035681724548}]}, {"text": "We show that making such a correction almost completely eliminates the beam problem.", "labels": [], "entities": []}, {"text": "We compare two commonlyused corrections, length normalization and a word reward, and show that the word reward is slightly better.", "labels": [], "entities": [{"text": "length", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9512014985084534}, {"text": "word reward", "start_pos": 99, "end_pos": 110, "type": "METRIC", "confidence": 0.8148518800735474}]}, {"text": "Finally, instead of tuning the word reward using grid search, we introduce away to learn it using a perceptron-like tuning method.", "labels": [], "entities": []}, {"text": "We show that the optimal value is sensitive both to task and beam size, implying that it is important to tune for every model trained.", "labels": [], "entities": []}, {"text": "Fortunately, tuning is a quick posttraining step.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the above methods in four settings, a high-resource German-English system, a medium-resource Russian-English system, and two low-resource French-English and EnglishFrench systems.", "labels": [], "entities": []}, {"text": "For all settings, we show that larger beams lead to large BLEU and METEOR drops if not corrected.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9989510774612427}, {"text": "METEOR", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.987646222114563}]}, {"text": "We also show that the optimal parameters can depend on the task, language pair, training data size, as well as the beam size.", "labels": [], "entities": []}, {"text": "These values can affect performance strongly.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of the high-resource German-English system. Rows: BLEU, METEOR, length = ratio of output  to reference length; \u03b3 = learned parameter value. While baseline performance decreases with beam size due to the  brevity problem, other methods perform more consistently across beam sizes. Length normalization (norm) gets  the best BLEU scores, but similar METEOR scores to the word reward.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9990884065628052}, {"text": "METEOR", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9971317052841187}, {"text": "BLEU", "start_pos": 341, "end_pos": 345, "type": "METRIC", "confidence": 0.9988935589790344}, {"text": "METEOR", "start_pos": 366, "end_pos": 372, "type": "METRIC", "confidence": 0.9848899245262146}]}, {"text": " Table 3: Results of low-resource French-English and English-French systems. Rows: BLEU, METEOR, length =  ratio of output to reference length; \u03b3 = learned parameter value. While baseline performance decreases with beam  size due to the brevity problem, other methods perform more consistently across beam sizes. Word reward gets the  best scores in both directions on METEOR. Length normalization (norm) gets the best BLEU scores in Fra-Eng  due to the slight bias of BLEU towards shorter translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9988191723823547}, {"text": "METEOR", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9445878863334656}, {"text": "METEOR", "start_pos": 369, "end_pos": 375, "type": "DATASET", "confidence": 0.7734072208404541}, {"text": "BLEU", "start_pos": 419, "end_pos": 423, "type": "METRIC", "confidence": 0.9953745007514954}, {"text": "Fra-Eng", "start_pos": 434, "end_pos": 441, "type": "DATASET", "confidence": 0.8760860562324524}, {"text": "BLEU", "start_pos": 469, "end_pos": 473, "type": "METRIC", "confidence": 0.9928750991821289}]}, {"text": " Table 4: Tuning time on top of baseline training time. Times are in minutes on 1000 dev examples (German- English) or 892 dev examples (French-English). Due to the much larger model size, we only looked at beam sizes  up to 75 for German-English.", "labels": [], "entities": [{"text": "Tuning", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9479345083236694}]}, {"text": " Table 5: Varying the size of the Russian-English training dataset results in different optimal word reward scores  (\u03b3). In all settings, the tuned score alleviates the beam problem. As the datasets get smaller, using a tuned larger  beam improves the BLEU score over a smaller tuned beam. This suggests that lower-resource systems are more  susceptible to the beam problem.", "labels": [], "entities": [{"text": "Russian-English training dataset", "start_pos": 34, "end_pos": 66, "type": "DATASET", "confidence": 0.5924932261308035}, {"text": "BLEU", "start_pos": 252, "end_pos": 256, "type": "METRIC", "confidence": 0.9993084669113159}]}]}