{"title": [{"text": "Trivial Transfer Learning for Low-Resource Neural Machine Translation", "labels": [], "entities": [{"text": "Trivial Transfer Learning", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8663643995920817}, {"text": "Low-Resource Neural Machine Translation", "start_pos": 30, "end_pos": 69, "type": "TASK", "confidence": 0.6432696580886841}]}], "abstractContent": [{"text": "Transfer learning has been proven as an effective technique for neural machine translation under low-resource conditions.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9688295722007751}, {"text": "neural machine translation", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.6419185400009155}]}, {"text": "Existing methods require a common target language , language relatedness, or specific training tricks and regimes.", "labels": [], "entities": []}, {"text": "We present a simple transfer learning method, where we first train a \"parent\" model fora high-resource language pair and then continue the training on a low-resource pair only by replacing the training corpus.", "labels": [], "entities": []}, {"text": "This \"child\" model performs significantly better than the baseline trained for low-resource pair only.", "labels": [], "entities": []}, {"text": "We are the first to show this for targeting different languages, and we observe the improvements even for unrelated languages with different alphabets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) has made a big leap in performance and became the unquestionable winning approach in the past few years ().", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7974099318186442}]}, {"text": "The main reason behind the success of NMT in realistic conditions was the ability to handle large vocabulary () and to utilize large monolingual data).", "labels": [], "entities": []}, {"text": "However, NMT still struggles if the parallel data is insufficient (e.g. fewer than 1M parallel sentences), producing fluent output unrelated to the source and performing much worse than phrase-based machine translation.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 186, "end_pos": 218, "type": "TASK", "confidence": 0.6300232708454132}]}, {"text": "Many strategies have been used in MT in the past for employing resources from additional languages, see e.g.,, El, or.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9885678291320801}]}, {"text": "For NMT, a particularly promising approach is transfer learning or \"domain adaptation\" where the \"domains\" are the different languages.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.8928148448467255}, {"text": "domain adaptation\"", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7853188018004099}]}, {"text": "For example, train a \"parent\" model in a high-resource language pair, then use some of the trained weights as the initialization fora \"child\" model and further train it on the low-resource language pair.", "labels": [], "entities": []}, {"text": "In, the parent and child pairs shared the target language (English) and a number of modifications of the training process were needed to achieve an improvement in translation from Hansa, Turkish, and Uzbek into English with the help of FrenchEnglish data.", "labels": [], "entities": [{"text": "Hansa", "start_pos": 180, "end_pos": 185, "type": "DATASET", "confidence": 0.7021331787109375}, {"text": "FrenchEnglish data", "start_pos": 236, "end_pos": 254, "type": "DATASET", "confidence": 0.9632787108421326}]}, {"text": "Nguyen and Chiang (2017) explore a related scenario where the parent language pair is also low-resource but it is related to the child language pair.", "labels": [], "entities": []}, {"text": "They improved the previous approach by using a shared vocabulary of subword units).", "labels": [], "entities": []}, {"text": "Additionally, they used transliteration to improve their results.", "labels": [], "entities": []}, {"text": "In this paper, we contribute empirical evidence that transfer learning for NMT can be simplified even further.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.9216074347496033}]}, {"text": "We leave out the restriction on relatedness of the languages and extend the experiments to parent-child pairs where the target language changes.", "labels": [], "entities": []}, {"text": "Moreover, we do not utilize any special modifications to the training regime or data pre-preprocessing.", "labels": [], "entities": []}, {"text": "In contrast to previous work, we test the method with the Transformer model (, instead of the recurrent approaches ().", "labels": [], "entities": []}, {"text": "As documented in e.g. and anticipated in WMT18, 1 the Transformer model seems superior to other NMT approaches.", "labels": [], "entities": [{"text": "WMT18", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.8141383528709412}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Maximal score reached by ENET child for  decreasing sizes of child training data, trained off an  ENFI parent (all ENFI data are used and models are  trained for 800k steps). The baselines use only the re- duced ENET data.", "labels": [], "entities": [{"text": "Maximal score reached", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9558183550834656}, {"text": "ENET child", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.8182620108127594}, {"text": "ENET data", "start_pos": 222, "end_pos": 231, "type": "DATASET", "confidence": 0.9171948730945587}]}, {"text": " Table 4: Results of child following a parent with  swapped direction. \"Baseline\" is child-only training.  \"Aligned\" is the more natural setup with English ap- pearing on the \"correct\" side of the parent, the numbers  in this column thus correspond to those in", "labels": [], "entities": []}, {"text": " Table 5: Transfer learning with parent and child not  sharing any language.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9207924902439117}]}, {"text": " Table 6: Breakdown of subword vocabulary of exper- iments involving ET, EN and RU.", "labels": [], "entities": [{"text": "RU", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.7748052477836609}]}, {"text": " Table 7: Summary of vocabulary overlaps for the var- ious language sets. All figures in % of the shared vo- cabulary.", "labels": [], "entities": []}, {"text": " Table 8: Various automatic scores on ENET test set.  Scores prefixed \"n\" reported as (1 \u2212 score) to make  higher numbers better.", "labels": [], "entities": [{"text": "ENET test set", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9719046354293823}]}, {"text": " Table 9: Candidate total length, BLEU n-gram preci- sions and brevity penalty (BP). The reference length in  the matching tokenization was 36062.", "labels": [], "entities": [{"text": "Candidate total length", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8600831627845764}, {"text": "BLEU n-gram preci- sions", "start_pos": 34, "end_pos": 58, "type": "METRIC", "confidence": 0.9188152432441712}, {"text": "brevity penalty (BP)", "start_pos": 63, "end_pos": 83, "type": "METRIC", "confidence": 0.9206735610961914}]}]}