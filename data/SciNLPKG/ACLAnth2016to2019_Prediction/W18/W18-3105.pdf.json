{"title": [{"text": "A Simple End-to-End Question Answering Model for Product Information", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.6881881654262543}, {"text": "Product Information", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.6975694298744202}]}], "abstractContent": [{"text": "When evaluating a potential product purchase , customers may have many questions in mind.", "labels": [], "entities": []}, {"text": "They want to get adequate information to determine whether the product of interest is worth their money.", "labels": [], "entities": []}, {"text": "In this paper we present a simple deep learning model for answering questions regarding product facts and specifications.", "labels": [], "entities": []}, {"text": "Given a question and a product specification, the model outputs a score indicating their relevance.", "labels": [], "entities": []}, {"text": "To train and evaluate our proposed model, we collected a dataset of 7,119 questions that are related to 153 different products.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that-despite its simplicity the performance of our model is shown to be comparable to a more complex state-of-the-art baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Customers ask many questions before buying products.", "labels": [], "entities": []}, {"text": "Developing a general question answering system to assist customers is challenging, due to the diversity of questions.", "labels": [], "entities": [{"text": "question answering", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7180188298225403}]}, {"text": "In this paper, we focus on the task of answering questions regarding product facts and specifications.", "labels": [], "entities": []}, {"text": "We formalize the task as follows: Given a question Q about a product P and the list of specifications (s 1 , s 2 , ..., s M ) of P , the goal is to identify the specification that is most relevant to Q.", "labels": [], "entities": []}, {"text": "M is the number of specifications of P , and s i is the i th specification of P . In this formulation, the task is similar to the answer selection problem (.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 130, "end_pos": 146, "type": "TASK", "confidence": 0.9081898331642151}]}, {"text": "'Answers' shall be individual product specifications in this case.", "labels": [], "entities": []}, {"text": "After identifying the most relevant specification, the final response sentence is generated using predefined templates (.", "labels": [], "entities": []}, {"text": "In this paper, we present a simple deep learning model for selecting the product specification that is most relevant to a given question from a set of candidate specifications.", "labels": [], "entities": []}, {"text": "Given a questionspecification pair, the model will output a score indicating their relevance.", "labels": [], "entities": []}, {"text": "To train and evaluate our model, we collected a dataset of 7,119 questions, covering 153 different products.", "labels": [], "entities": []}, {"text": "Despite its simplicity, the performance of our model is shown to be comparable to a more complex state-of-the-art baseline.", "labels": [], "entities": []}], "datasetContent": [{"text": "We setup two different experimental settings.", "labels": [], "entities": []}, {"text": "The only difference between the two settings is the way in which we split up the collected HomeDepot dataset into training set, development set, and test set: 1.", "labels": [], "entities": [{"text": "HomeDepot dataset", "start_pos": 91, "end_pos": 108, "type": "DATASET", "confidence": 0.9482919573783875}]}, {"text": "We divide the dataset so that the test set has no products in common with the training set or the development set.", "labels": [], "entities": []}, {"text": "2. We divide the dataset so that the test set has no specifications in common with the training set or the development set.", "labels": [], "entities": []}, {"text": "This is different from the first setting, because two different products may have some specifications in common.", "labels": [], "entities": []}, {"text": "For example, a chair and a table usually have a same specification called 'Product Weight'.", "labels": [], "entities": []}, {"text": "In both settings, the proportions of the training set, development set, and test set are roughly 80%, 10%, and 10% of the total questions, respectively.", "labels": [], "entities": []}, {"text": "During training, the objective is to minimize the cross entropy of all question-specification pairs in the training set: where Q (i) and S (i) represent a questionspecification pair in the training set, y (i) indicates whether specification S (i) is relevant to question Q (i) , and p \u03b8 is the predicted probability with model weights \u03b8.", "labels": [], "entities": []}, {"text": "We use all possible questionspecification pairs for training.", "labels": [], "entities": []}, {"text": "In other words, if there are k questions about a product and the product hash specifications, there are h \u00d7 k questionspecification examples related to the product, and exactly k of them are positive examples.", "labels": [], "entities": []}, {"text": "During testing, for every question about a product, we sort the specifications of the product in descending order based on the predicted probability of being relevant.", "labels": [], "entities": []}, {"text": "After that, we calculate the precision at 1 (P@1), precision at 1 (P@2), and precision at 3 (P@3) of our model.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9996453523635864}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9995021820068359}, {"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9994643330574036}]}, {"text": "We compare the performance of our model with the unigram model mentioned in () and the IWAN model proposed in).", "labels": [], "entities": []}, {"text": "The unigram model is a simple bag-ofwords model.", "labels": [], "entities": []}, {"text": "It first generates a vector representation for each input sentence by summing over the embeddings of all words in the sentence.", "labels": [], "entities": []}, {"text": "The final output is then determined based on the generated vector representations.", "labels": [], "entities": []}, {"text": "The unigram model is less complicated than our model.", "labels": [], "entities": []}, {"text": "On the other hand, the IWAN model belongs to the CompareAggregate framework, and it is more sophisticated than our model.", "labels": [], "entities": []}, {"text": "In addition to comparing between the fine-grained word representations of the input sentences, the IWAN model also has an inter-weighted layer for evaluating the importance of each word in each sentence.", "labels": [], "entities": []}, {"text": "The IWAN model currently achieves state-of-the-art performance on public datasets such as TrecQA () and WikiQA (.", "labels": [], "entities": []}, {"text": "We make use of the GloVe word embeddings () when training the models.", "labels": [], "entities": []}, {"text": "We did try the word2vec word embeddings (), however they gave worse performances than GloVe.", "labels": [], "entities": []}, {"text": "We tune the hyperparameters of each model using the development set.", "labels": [], "entities": []}, {"text": "shows the performances of all the models in the first setting.", "labels": [], "entities": []}, {"text": "shows the performances of all the models in the second setting.", "labels": [], "entities": []}, {"text": "The IWAN model and our model clearly outperform the unigram model.", "labels": [], "entities": [{"text": "IWAN", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7198048830032349}]}, {"text": "In addition, in both settings, our model's performance is comparable to: Test results in the setting where the test set has no specification in common with the training set or the development set the performance of the IWAN model despite being much simpler.", "labels": [], "entities": []}, {"text": "We measured the speeds of our model and the IWAN model.", "labels": [], "entities": [{"text": "IWAN model", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.8882890045642853}]}, {"text": "Our proposed model is about 8% faster than the IWAN model.", "labels": [], "entities": []}, {"text": "In addition, we see that all models perform worse in the second setting than the first setting.", "labels": [], "entities": []}, {"text": "This may due to the fact that in the first setting two different products in the train set and the test set may still have many specifications in common (e.g., a LG TV and a Samsung TV).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test results in the setting where the  test set has no product in common with the  training set or the development set", "labels": [], "entities": []}, {"text": " Table 2: Test results in the setting where the  test set has no specification in common with  the training set or the development set", "labels": [], "entities": []}]}