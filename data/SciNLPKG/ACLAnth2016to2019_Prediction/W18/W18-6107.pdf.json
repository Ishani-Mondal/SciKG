{"title": [{"text": "Normalization of Transliterated Words in Code-Mixed Data Using Seq2Seq Model & Levenshtein Distance", "labels": [], "entities": [{"text": "Normalization of Transliterated Words", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8339377641677856}, {"text": "Levenshtein Distance", "start_pos": 79, "end_pos": 99, "type": "DATASET", "confidence": 0.8125638663768768}]}], "abstractContent": [{"text": "Building tools for code-mixed data is rapidly gaining popularity in the NLP research community as such data is exponentially rising on social media.", "labels": [], "entities": []}, {"text": "Working with code-mixed data contains several challenges, especially due to grammatical inconsistencies and spelling variations in addition to all the previous known challenges for social media scenarios.", "labels": [], "entities": []}, {"text": "In this article, we present a novel architecture focus-ing on normalizing phonetic typing variations, which is commonly seen in code-mixed data.", "labels": [], "entities": [{"text": "normalizing phonetic typing variations", "start_pos": 62, "end_pos": 100, "type": "TASK", "confidence": 0.8172651678323746}]}, {"text": "One of the main features of our architecture is that in addition to normalizing, it can also be utilized for back-transliteration and word identification in some cases.", "labels": [], "entities": [{"text": "word identification", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.787943571805954}]}, {"text": "Our model achieved an accuracy of 90.27% on the test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9997885823249817}]}], "introductionContent": [{"text": "With rising popularity of social media, the amount of data is rising exponentially.", "labels": [], "entities": [{"text": "amount", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9614630937576294}]}, {"text": "If mined, this data can proof to be useful for various purposes.", "labels": [], "entities": []}, {"text": "In countries where the number of bilinguals are high, we see that users tend to switchback and forth between multiple languages, a phenomenon known as code-mixing or code-switching.", "labels": [], "entities": []}, {"text": "An interesting case is switching between languages which share different native scripts.", "labels": [], "entities": []}, {"text": "On such occasions, one of the two languages is typed in it's phonetically transliterated form in order to use a common script.", "labels": [], "entities": []}, {"text": "Though there are some standard transliteration rules, for example ITRANS 1 , ISO 2 , but it is extremely difficult and un-realistic for people to follow them while typing.", "labels": [], "entities": [{"text": "ITRANS 1", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.8291439116001129}]}, {"text": "This indeed is the case as we see that identical words are being transliterated differently by different people based on their own phonetic judgment influenced by dialects, location, or sometimes even based on the informality or casualness of the situation.", "labels": [], "entities": []}, {"text": "Thus, for cre-ating systems for code-mixed data, post language tagging, normalization of transliterated text is extremely important in order to identify the word and understand it's semantics.", "labels": [], "entities": [{"text": "post language tagging", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.6754809021949768}]}, {"text": "This would help a lot in systems like opinion mining, and is actually necessary for tasks like summarization, translation, etc.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.8523699343204498}, {"text": "summarization", "start_pos": 95, "end_pos": 108, "type": "TASK", "confidence": 0.9867058396339417}, {"text": "translation", "start_pos": 110, "end_pos": 121, "type": "TASK", "confidence": 0.6653539538383484}]}, {"text": "A normalizing module will also be of immense help while making word embeddings for code-mixed data.", "labels": [], "entities": []}, {"text": "In this paper, we present an architecture for automatic normalization of phonetically transliterated words to their standard forms.", "labels": [], "entities": [{"text": "automatic normalization of phonetically transliterated words", "start_pos": 46, "end_pos": 106, "type": "TASK", "confidence": 0.801799034078916}]}, {"text": "The language pair we have worked on is Bengali-English (Bn-En), where both are typed in Roman script, thus the Bengali words are in their transliterated form.", "labels": [], "entities": []}, {"text": "The canonical or normalized form we have considered is the Indian Languages Transliteration (ITRANS) form of the respective Bengali word.", "labels": [], "entities": []}, {"text": "Bengali is an Indo-Aryan language of India where 8.10% 3 of the total population are 1 st language speakers and is also the official language of Bangladesh.", "labels": [], "entities": []}, {"text": "The native script of Bengali is the Eastern Nagari Script 4 . Our architecture utilizes fully char based sequence to sequence learning in addition to Levenshtein distance to give the final normalized form or as close to it as possible.", "labels": [], "entities": [{"text": "Eastern Nagari Script 4", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.8081679344177246}, {"text": "Levenshtein distance", "start_pos": 150, "end_pos": 170, "type": "METRIC", "confidence": 0.957787424325943}]}, {"text": "Some additional advantages of our system is that at an intermediate stage, the back-transliterated form of the word can be fetched (i.e. word identification), which will be very useful in several cases as original tools (i.e. tools using native script) can be utilized, for example emotion lexicons.", "labels": [], "entities": [{"text": "word identification)", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.8030181030432383}]}, {"text": "Some other important contributions of our research are the new lexicons that have been prepared (discussed in Sec 3) which can be used for building various other tools for studying Bengali-English code-mixed data.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our system was evaluated in two ways, one at word level and another at task level.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Prior and post normalization results.", "labels": [], "entities": []}]}