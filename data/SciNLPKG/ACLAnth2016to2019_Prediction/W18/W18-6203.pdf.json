{"title": [{"text": "Implicit Subjective and Sentimental Usages in Multi-sense Word Embeddings", "labels": [], "entities": [{"text": "Implicit Subjective and Sentimental Usages", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7857064843177796}]}], "abstractContent": [{"text": "In multi-sense word embeddings, contextual variations in corpus may cause a univocal word to be embedded into different sense vectors.", "labels": [], "entities": []}, {"text": "(2016) show that this kind of pseudo multi-senses can be eliminated by linear transformations.", "labels": [], "entities": []}, {"text": "In this paper, we show that pseudo multi-senses may come from a uniform and meaningful phenomenon such as subjective and sentimental usage, though they are seemingly redundant.", "labels": [], "entities": []}, {"text": "In this paper, we present an unsupervised algorithm to find a linear transformation which can minimize the transformed distance of a group of sense pairs.", "labels": [], "entities": []}, {"text": "The major shrinking direction of this transformation is found to be related with subjective shift.", "labels": [], "entities": []}, {"text": "Therefore, we cannot only eliminate pseudo multi-senses in multi-sense embeddings, but also identify these subjective senses and tag the subjective and sentimental usage of words in the corpus automatically .", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-sense word embeddings are popular choices to represent polysemous words.", "labels": [], "entities": []}, {"text": "These methods learn senses of words automatically by clustering contexts they appear in.", "labels": [], "entities": []}, {"text": "However, contextual variation in corpus may cause a univocal word be embedded into different senses (.", "labels": [], "entities": []}, {"text": "For example, the context of \"another\" in this sentence is normal and narrative: \"South Trust, another large bank headquartered in Birmingham, was acquired by   In the second sentence, the word \"another\" locates in subjective and emotional context with intense feelings: \"He committed suicide after the woman he loved married another man.\"", "labels": [], "entities": [{"text": "South Trust", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.9927598237991333}]}, {"text": "The word \"another\" in these two sentences have the same meaning, but they are often embedded into two different senses by existing multi-sense word embedding models.", "labels": [], "entities": []}, {"text": "used linear transformation to eliminate the vector differences between corresponding sense pairs with the same meaning, and improved the performance on downstream tasks such as contextual word similarity (.", "labels": [], "entities": []}, {"text": "Such pairs were called pseudo multi-sense pairs.", "labels": [], "entities": []}, {"text": "However, they did not give any explicit explanation of the eliminated vector difference in a pseudo multi-sense pair.", "labels": [], "entities": []}, {"text": "In this paper, we propose to explain the socalled pseudo multi-senses by slightly modifying the linear transformation proposed by.", "labels": [], "entities": []}, {"text": "We find that a large number of pseudo multi-senses can be viewed as pairs of i) a normal sense and ii) a subjective or sentimental sense.", "labels": [], "entities": []}, {"text": "In addition, as shown in, a group of words may have similar normal-subjective/sentimental difference vector, indicating subjectivity and sentiment are general sources of pseudo multi-senses.", "labels": [], "entities": []}, {"text": "In the first step of our approach, we identify the multi-sense pairs that are generated by an uniform contextual variation.", "labels": [], "entities": []}, {"text": "Then we regress a linear transformation which can minimize the average Euclidean distance between two opposite groups in embedding space.", "labels": [], "entities": []}, {"text": "We analyze the major shrinking directions in the embedding space w.r.t. the linear transformation, and find it consistent that one of such directions is relevant to subjective and sentimental usage.", "labels": [], "entities": []}, {"text": "The motivation of our approach is that a group of pseudo multi-senses is often generated systematically, i.e., pseudo multi-senses in the same group come from the same reason.", "labels": [], "entities": []}, {"text": "Therefore, a linear transformation that eliminate the shift and minimize distance between senses may reflect a salient language phenomenon.", "labels": [], "entities": []}, {"text": "In addition to giving explicit explanation to pseudo multi-senses, experimental results also show that our approach can contribute to some NLP tasks such as subjective and sentimental analysis.", "labels": [], "entities": [{"text": "sentimental analysis", "start_pos": 172, "end_pos": 192, "type": "TASK", "confidence": 0.8430465459823608}]}, {"text": "In Section 2, we introduce some related work.", "labels": [], "entities": []}, {"text": "In Section 3, we present a method to mine a linear transformation that eliminates semantic shift generating 'pseudo multi-senses'.", "labels": [], "entities": []}, {"text": "In Section 4, we analyze the language phenomenon represented and eliminated by that linear transformation, namely subjective and sentimental usage, and do some evaluations on the subjective shift.", "labels": [], "entities": []}, {"text": "Finally in Section 6 we draw conclusions and propose future work left to be done.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: The performance of the original MSSG50D  on Wiki Corpus embeddings(Original Space), the  sentimental-stretched space('Subj-stretched'), the  sentimental-eliminated space('Subj-eliminated') and  the original contextual vector in combination with its  projection on sentimental direction('Context + Subj')  on text classification tests including MPQA (", "labels": [], "entities": [{"text": "text classification", "start_pos": 318, "end_pos": 337, "type": "TASK", "confidence": 0.756036251783371}]}]}