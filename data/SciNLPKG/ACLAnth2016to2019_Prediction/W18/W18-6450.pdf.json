{"title": [{"text": "Results of the WMT18 Metrics Shared Task", "labels": [], "entities": [{"text": "WMT18 Metrics Shared", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.6339839299519857}]}], "abstractContent": [{"text": "This paper presents the results of the WMT18 Metrics Shared Task.", "labels": [], "entities": [{"text": "WMT18 Metrics Shared Task", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6264677718281746}]}, {"text": "We asked participants of this task to score the outputs of the MT systems involved in the WMT18 News Translation Task with automatic metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9303135275840759}, {"text": "WMT18 News Translation Task", "start_pos": 90, "end_pos": 117, "type": "TASK", "confidence": 0.7873637229204178}]}, {"text": "We collected scores of 10 metrics and 8 research groups.", "labels": [], "entities": []}, {"text": "In addition to that, we computed scores of 8 standard met-rics (BLEU, SentBLEU, chrF, NIST, WER, PER, TER and CDER) as base-lines.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9985400438308716}, {"text": "WER", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.8740411400794983}, {"text": "PER", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.8195356726646423}, {"text": "TER", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.8026840686798096}]}, {"text": "The collected scores were evaluated in terms of system-level correlation (how well each metric's scores correlate with WMT18 official manual ranking of systems) and in terms of segment-level correlation (how often a metric agrees with humans in judging the quality of a particular sentence relative to alternate outputs).", "labels": [], "entities": [{"text": "WMT18 official manual ranking", "start_pos": 119, "end_pos": 148, "type": "DATASET", "confidence": 0.85813207924366}]}, {"text": "This year, we employ a single kind of manual evaluation: direct assessment (DA).", "labels": [], "entities": [{"text": "direct assessment (DA)", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.5369713842868805}]}], "introductionContent": [{"text": "Accurate machine translation (MT) evaluation is important for measuring improvements in system performance.", "labels": [], "entities": [{"text": "Accurate machine translation (MT) evaluation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.896878182888031}]}, {"text": "Human evaluation can be costly and time consuming, and it is not always available for the language pair of interest.", "labels": [], "entities": []}, {"text": "Automatic metrics can be employed as a substitute for human evaluation in such cases, metrics that aim to measure improvements to systems quickly and at no cost to developers.", "labels": [], "entities": []}, {"text": "In the usual set-up, an automatic metric carries out a comparison of MT system output translations and human-produced reference translations to produce a single overall score for the system.", "labels": [], "entities": [{"text": "MT system output translations", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.8615425527095795}]}, {"text": "1 Since there exists a large number of possible approaches to producing quality scores for translations, it is sensible to carryout a meta-evaluation of metrics with the aim to estimate their accuracy as a substitute for human assessment of translation quality.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.9930242300033569}]}, {"text": "The Metrics Shared Task 2 of WMT annually evaluates the performance of automatic machine translation metrics in their ability to provide a substitute for human assessment of translation quality.", "labels": [], "entities": [{"text": "machine translation metrics", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.7846759259700775}]}, {"text": "Again, we keep the two main types of metric evaluation unchanged from the previous years.", "labels": [], "entities": [{"text": "metric evaluation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.6342687606811523}]}, {"text": "In system-level evaluation, each metric provides a quality score for the whole translated test set (usually a set of documents, in fact).", "labels": [], "entities": []}, {"text": "In segment-level evaluation, a score is assigned by a given metric to every individual sentence.", "labels": [], "entities": []}, {"text": "The underlying texts and MT systems come from the News Translation Task ( in the following).", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9940696954727173}, {"text": "News Translation Task", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.7561996082464854}]}, {"text": "The texts were drawn from the news domain and involve translations to/from Chinese (zh), Czech (cs), German (de), Estonian (et), Finnish (fi), Russian (ru), and Turkish (tr), each paired with English, making a total of 14 language pairs.", "labels": [], "entities": []}, {"text": "A single form of golden truth of translation quality judgement is used this year: \u2022 In Direct Assessment (DA) ( , humans assess the quality of a given MT output translation by comparison with a reference translation (as opposed to the source and reference).", "labels": [], "entities": [{"text": "MT output translation", "start_pos": 151, "end_pos": 172, "type": "TASK", "confidence": 0.8628835280736288}]}, {"text": "DA is the new standard used in WMT News Translation Task evaluation, requiring only monolingual evaluators.", "labels": [], "entities": [{"text": "DA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.593894362449646}, {"text": "WMT News Translation Task evaluation", "start_pos": 31, "end_pos": 67, "type": "TASK", "confidence": 0.9156161904335022}]}, {"text": "As in last year's evaluation, the official method of manual evaluation of MT outputs is no longer \"relative ranking\" (RR, evaluating up to five system outputs on an annotation screen relative to each other) as this was changed in 2017 to DA.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 74, "end_pos": 84, "type": "TASK", "confidence": 0.9126872420310974}, {"text": "DA", "start_pos": 238, "end_pos": 240, "type": "METRIC", "confidence": 0.7439492344856262}]}, {"text": "For system-level evaluation, we thus use the Pearson correlation r of automatic metrics with DA scores.", "labels": [], "entities": [{"text": "Pearson correlation r", "start_pos": 45, "end_pos": 66, "type": "METRIC", "confidence": 0.9816584388415018}]}, {"text": "For segment-level evaluation, we re-interpret DA judgements as relative comparisons and use Kendall's \u03c4 as a substitute, see below for details and references.", "labels": [], "entities": []}, {"text": "Section 2 describes our datasets, i.e. the sets of underlying sentences, system outputs, human judgements of translation quality and also participating metrics.", "labels": [], "entities": []}, {"text": "Sections 3.1 and 3.2 then provide the results of system and segment-level metric evaluation, respectively.", "labels": [], "entities": []}, {"text": "We discuss the results in Section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Absolute Pearson correlation of to-English system-level metrics with DA human as- sessment in newstest2018; correlations of metrics not significantly outperformed by any other  for that language pair are highlighted in bold; ensemble metrics are highlighted in gray.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8157135049502054}]}, {"text": " Table 5: Absolute Pearson correlation of out-of-English system-level metrics with DA human  assessment in newstest2018; correlations of metrics not significantly outperformed by any other  for that language pair are highlighted in bold; ensemble metrics are highlighted in gray.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.812369704246521}]}, {"text": " Table 6: Absolute Pearson correlation of to-English system-level metrics with DA human assess- ment for 10K hybrid super-sampled systems in newstest2018; ensemble metrics are highlighted  in gray.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8180506825447083}, {"text": "DA human assess- ment", "start_pos": 79, "end_pos": 100, "type": "METRIC", "confidence": 0.9135444760322571}]}, {"text": " Table 7: Absolute Pearson correlation of out-of-English system-level metrics with DA human  assessment for 10K hybrid super-sampled systems in newstest2018; ensemble metrics are high- lighted in gray.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8086910247802734}, {"text": "DA human  assessment", "start_pos": 83, "end_pos": 103, "type": "METRIC", "confidence": 0.6969084143638611}]}, {"text": " Table 8: Segment-level metric results for to-English language pairs in newstest2018: absolute  Kendall's Tau formulation of segment-level metric scores with DA scores; correlations of metrics  not significantly outperformed by any other for that language pair are highlighted in bold;  ensemble metrics are highlighted in gray.", "labels": [], "entities": []}, {"text": " Table 9: Segment-level metric results for out-of-English language pairs in newstest2018: absolute  Kendall's Tau formulation of segment-level metric scores with DA scores; correlations of metrics  not significantly outperformed by any other for that language pair are highlighted in bold;  ensemble metrics are highlighted in gray.", "labels": [], "entities": []}]}