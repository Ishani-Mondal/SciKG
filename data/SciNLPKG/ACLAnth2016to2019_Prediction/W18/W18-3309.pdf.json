{"title": [{"text": "DNN Multimodal Fusion Techniques for Predicting Video Sentiment", "labels": [], "entities": [{"text": "Predicting Video Sentiment", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.9017283121744791}]}], "abstractContent": [{"text": "We present our work on sentiment prediction using the benchmark MOSI dataset from the CMU-MultimodalDataSDK.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.972713053226471}, {"text": "MOSI dataset", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.7153400331735611}, {"text": "CMU-MultimodalDataSDK", "start_pos": 86, "end_pos": 107, "type": "DATASET", "confidence": 0.9362345337867737}]}, {"text": "Previous work on multimodal sentiment analysis have been focused on input-level feature fusion or decision-level fusion for multimodal fusion.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.8072861433029175}, {"text": "decision-level fusion", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.7301847338676453}, {"text": "multimodal fusion", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.7323123812675476}]}, {"text": "Here, we propose an intermediate-level feature fusion, which merges weights from each modality (au-dio, video, and text) during training with subsequent additional training.", "labels": [], "entities": []}, {"text": "Moreover, we tested principle component analysis (PCA) for feature selection.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7651160955429077}]}, {"text": "We found that applying PCA increases unimodal performance , and multimodal fusion outper-forms unimodal models.", "labels": [], "entities": []}, {"text": "Our experiments show that our proposed intermediate-level feature fusion outperforms other fusion techniques, and it achieves the best performance with an overall binary accuracy of 74.0% on video+text modalities.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.7176922559738159}]}, {"text": "Our work also improves feature selection for unimodal sentiment analysis, while proposing a novel and effective multi-modal fusion architecture for this task.", "labels": [], "entities": [{"text": "unimodal sentiment analysis", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.6710464954376221}]}], "introductionContent": [{"text": "Sentiment analysis is the study on the underlying attitude that one holds towards a certain entity.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9405399560928345}]}, {"text": "For along time, text-based sentiment analysis has been the staple in this area and only recently are other modalities being considered for sentiment analysis such as vision and speech).", "labels": [], "entities": [{"text": "text-based sentiment analysis", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.7084516485532125}, {"text": "sentiment analysis", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.9352146685123444}]}, {"text": "For text channels, the features usually include information about word sequences and meaning).", "labels": [], "entities": []}, {"text": "However, combining information from multiple modalities can bring additional information to ambiguous cases.", "labels": [], "entities": []}, {"text": "For example, a smile extracted from facial features could help disambiguate cases such us \"This movie is sick\".", "labels": [], "entities": []}, {"text": "Text alone would have trouble interpreting the meaning of the word \"sick\" in this context.", "labels": [], "entities": []}, {"text": "This motivates the research of multimodal sentiment analysis.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.6757632195949554}]}, {"text": "We seek to exploit the inter-dependencies between audio, text, and visual modalities in order to label video segments that exhibit positive or negative sentiment.", "labels": [], "entities": []}, {"text": "In current studies in this field, visual features often involve salient points of the face or body (, while low-level descriptors are collected from the speech signal such as pitch and volume ().", "labels": [], "entities": []}, {"text": "The combination of features which have originated from text, speech and audio is what forms the basis of our multimodal classification work.", "labels": [], "entities": [{"text": "multimodal classification", "start_pos": 109, "end_pos": 134, "type": "TASK", "confidence": 0.7258004546165466}]}, {"text": "Features from each modality are modeled, learned, and eventually fused together at various levels in a classification Deep Neural Network (DNN) system.", "labels": [], "entities": []}, {"text": "When the modalities are fused together, this is called multimodal fusion.", "labels": [], "entities": []}, {"text": "DNN multimodal fusion for binary sentiment classification is an active area of research that continues to gain momentum and spark interest due to the challenging nature of the problem (e.g., ).", "labels": [], "entities": [{"text": "DNN multimodal fusion", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.599378764629364}, {"text": "binary sentiment classification", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.7633039355278015}]}, {"text": "We explore the interplay between three modalities: text, video, and audio.", "labels": [], "entities": []}, {"text": "We focus on three fusion techniques inspired by previous work on multimodal fusion ().", "labels": [], "entities": []}, {"text": "We developed and compared three multimodal fusion architectures: (1) Input-level features fusion, (2) Intermediate features fusion, and (3) Decision-level fusion (late fusion).", "labels": [], "entities": []}, {"text": "The first method refers to fusing information at the level of input features, similar to an unweighted concatenation of feature vectors, and it is the most widely used.", "labels": [], "entities": []}, {"text": "The second method evokes the notion that each modality can be learned using a unimodal DNN.", "labels": [], "entities": []}, {"text": "The weights learned through train-ing each unimodal DNN are concatenated together and training continues before the decision level.", "labels": [], "entities": []}, {"text": "The third method, also known as ensemble fusion or late fusion, fuses multiple modalities at the decision level.", "labels": [], "entities": [{"text": "ensemble fusion or late fusion", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.735875916481018}]}, {"text": "We present our multimodal DNN fusion approaches in detail in our methodology description in Section 3.", "labels": [], "entities": [{"text": "DNN fusion", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.799287348985672}]}, {"text": "where we further analyze the interactions between modalities.", "labels": [], "entities": []}, {"text": "We experimented with combinations of modalities as well as system architectures that attempt to capture the interplay between modalities.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we provide the experiments on the 3 fusion techniques with and without PCA, for predicting the positive/negative sentiment of the 2 commonly employed when there is a need to assess which components explain the most variability in the data, plots available upon request  videos.", "labels": [], "entities": []}, {"text": "We report accuracy for the binary sentiment classification problem.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996507167816162}, {"text": "binary sentiment classification", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.7737245758374532}]}, {"text": "After experimenting with the fusion techniques, we identify the best overall performing systems and further report the 5-class accuracy, F1, and regression MAE and correlation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9530153870582581}, {"text": "F1", "start_pos": 137, "end_pos": 139, "type": "METRIC", "confidence": 0.9993743300437927}, {"text": "regression", "start_pos": 145, "end_pos": 155, "type": "METRIC", "confidence": 0.9543536305427551}, {"text": "MAE", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.528464674949646}, {"text": "correlation", "start_pos": 164, "end_pos": 175, "type": "METRIC", "confidence": 0.9672101736068726}]}], "tableCaptions": [{"text": " Table 2: Unimodal binary accuracy, exploring k  number of PCA components with corresponding  variance threshold (A=audio, V=video, T=text).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9297742247581482}]}, {"text": " Table 2. Our experiment results for early fu- sion are displayed in", "labels": [], "entities": []}, {"text": " Table 3. The top-performing  systems for each modality combination are high- lighted in bold.", "labels": [], "entities": []}, {"text": " Table 3: Bimodal/trimodal binary accuracy for  early fusion. Parameters refer to DNN layers,  dropout rate, segment length.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.8172006011009216}, {"text": "Parameters", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9840953946113586}]}, {"text": " Table 4: Bimodal/trimodal binary accuracy for  intermediate feature fusion. Parameters refer to  DNN layers, dropout rate, segment length.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.7663585543632507}, {"text": "intermediate feature fusion", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.7057835459709167}, {"text": "Parameters", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9756794571876526}]}, {"text": " Table 5: Bimodal/trimodal binary accuracy for  decision-level fusion experiments.", "labels": [], "entities": [{"text": "Bimodal", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9198171496391296}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.6177411675453186}]}, {"text": " Table 6: Top fusion system performance on bi- nary classification, 5-class classification and re- gression.", "labels": [], "entities": []}]}