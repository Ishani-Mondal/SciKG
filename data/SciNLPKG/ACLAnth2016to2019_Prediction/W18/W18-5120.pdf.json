{"title": [{"text": "The Linguistic Ideologies of Deep Abusive Language Classification", "labels": [], "entities": [{"text": "Linguistic Ideologies of Deep Abusive Language Classification", "start_pos": 4, "end_pos": 65, "type": "TASK", "confidence": 0.7180409346308027}]}], "abstractContent": [{"text": "This paper brings together theories from so-ciolinguistics and linguistic anthropology to critically evaluate the so-called \"language ideologies\"-the set of beliefs and ways of speaking about language-in the practices of abusive language classification in modern machine learning-based NLP.", "labels": [], "entities": [{"text": "abusive language classification", "start_pos": 221, "end_pos": 252, "type": "TASK", "confidence": 0.5806662738323212}]}, {"text": "This argument is made at both a conceptual and empirical level, as we review approaches to abusive language from different fields, and use two neural network methods to analyze three datasets developed for abusive language classification tasks (drawn from Wikipedia, Facebook, and Stack-Overflow).", "labels": [], "entities": [{"text": "abusive language classification tasks", "start_pos": 206, "end_pos": 243, "type": "TASK", "confidence": 0.7591710090637207}]}, {"text": "By evaluating and comparing these results, we argue for the importance of incorporating theories of pragmatics and metaprag-matics into both the design of classification tasks as well as in ML architectures.", "labels": [], "entities": [{"text": "ML architectures", "start_pos": 190, "end_pos": 206, "type": "TASK", "confidence": 0.9107935726642609}]}], "introductionContent": [{"text": "Some problems lend themselves more easily to A.I. solutions than others.", "labels": [], "entities": []}, {"text": "So, hate speech is one of the hardest, because determining if something is hate speech is very linguistically nuanced.", "labels": [], "entities": [{"text": "hate speech", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.8315062522888184}]}, {"text": "I'm optimistic that over a fiveto 10-year period, we will have A.I. tools that can get into some of the nuances, the linguistic nuances of different types of content to be more accurate in flagging things for our systems.", "labels": [], "entities": []}, {"text": "This view of linguistic action is in contrast with a tradition of speech-act theory in which the meaning of an utterance is not solely determined by lexical or syntactic structure but by its social context and performative effects.", "labels": [], "entities": []}, {"text": "In studies of hate speech following these latter perspectives, such as those of, words are seen not just as arbitrary lexemes but something capable of actively causing violence to one or more addressees.", "labels": [], "entities": []}, {"text": "The question remains: is detecting hate speech a matter of sufficiently advanced pattern recognition with the right model architecture on a large enough data set of \"hateful\" vs. \"non-hateful\" expressions?", "labels": [], "entities": [{"text": "pattern recognition", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7859080731868744}]}, {"text": "Or is it necessary for any such automated classification to be integrated with longterm ethnographic and contextual immersion in the communities, however problematic, in which said expressions emerge?", "labels": [], "entities": []}, {"text": "To what extent should we expect researchers to attempt to integrate the sociocultural complexities of contemporary online communication into their \"A.I. tools\", and to what extent should we expect successor failure from those tools?", "labels": [], "entities": []}, {"text": "This article assesses the current potential for, and limitations of, machine learning methodologies-both in terms of how the 'gold standard' datasets are normally constructed and how neural networks are applied-for abusive language detection in natural language text.", "labels": [], "entities": [{"text": "abusive language detection in natural language text", "start_pos": 215, "end_pos": 266, "type": "TASK", "confidence": 0.7975024495806012}]}, {"text": "We will, through our own experiments, explore the specific conflicts between the research culture of achieving \"state-of-the-art\" scores with (relatively) black-boxed architectures and theories of the pragmatics (and metapragmatics) of linguistic abuse in practice.", "labels": [], "entities": []}], "datasetContent": [{"text": "The nascent research cluster around NLP and abusive language constitutes not just a 'speech community' but a language community, i.e., \"an organization of people by their orientation to structural (formal) norms for denotational coding (whether explicit or implicit)\".", "labels": [], "entities": []}, {"text": "The combination of linguistic ideologies described above is fully realized in the conventional experimental architecture of the shared task, in which multiple teams of researchers independently attempt to build systems with good classificatory performance by determining the true denotational meaning of utterances which, most commonly, have been excised from their interactional context.", "labels": [], "entities": []}, {"text": "For example, the tasks addressing abusive language typically have as their goal the determination of whether stand-alone utterances should be considered rude, offensive, or abusive.", "labels": [], "entities": []}, {"text": "Training data is provided in the form of utterancelabel pairs, where the label maybe a binary value (i.e. abusive or not) or multi-class (for different categorical and/or ordinal levels of offensiveness).", "labels": [], "entities": []}, {"text": "In order to explore these kinds of tasks directly, in this paper we chose to experiment with 3 datasets: the Kaggle Toxic Comment Classification Challenge 2 , the shared task in the 1st Workshop on Trolling, Aggression and Cyberbullying (TRAC1) 3 , and the StackOverflow dataset from the 2nd EMNLP Abusive Language Workshop.", "labels": [], "entities": [{"text": "Kaggle Toxic Comment Classification Challenge", "start_pos": 109, "end_pos": 154, "type": "TASK", "confidence": 0.6633763670921325}, {"text": "1st Workshop on Trolling, Aggression and Cyberbullying (TRAC1) 3", "start_pos": 182, "end_pos": 246, "type": "TASK", "confidence": 0.5799436544378599}, {"text": "StackOverflow dataset from the 2nd EMNLP Abusive Language Workshop", "start_pos": 257, "end_pos": 323, "type": "DATASET", "confidence": 0.7750875883632236}]}], "tableCaptions": [{"text": " Table 1: Results on test sets of three data sources us- ing two architectures. The numbers next to the data  sources shows the size of each class in the test set. Hy- perparameters were manually tuned using the valida- tion sets. We calculated the micro-averaged F1 score  because of the varied class imbalance in the datasets.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 264, "end_pos": 272, "type": "METRIC", "confidence": 0.966622918844223}]}]}