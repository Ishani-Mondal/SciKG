{"title": [{"text": "Orthogonal Matching Pursuit for Text Classification", "labels": [], "entities": [{"text": "Orthogonal Matching Pursuit", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7059781750043234}, {"text": "Text Classification", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.9499668478965759}]}], "abstractContent": [{"text": "In text classification, the problem of overfitting arises due to the high dimensionality, making regularization essential.", "labels": [], "entities": [{"text": "text classification", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8158213198184967}]}, {"text": "Although classic regularizers provide sparsity, they fail to return highly accurate models.", "labels": [], "entities": []}, {"text": "On the contrary , state-of-the-art group-lasso regularizers provide better results at the expense of low sparsity.", "labels": [], "entities": []}, {"text": "In this paper, we apply a greedy variable selection algorithm, called Orthogonal Matching Pursuit, for the text classification task.", "labels": [], "entities": [{"text": "Orthogonal Matching Pursuit", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.8714045683542887}, {"text": "text classification task", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.891478975613912}]}, {"text": "We also extend standard group OMP by introducing overlapping Group OMP to handle overlapping groups of features.", "labels": [], "entities": []}, {"text": "Empirical analysis verifies that both OMP and overlapping GOMP constitute powerful regularizers, able to produce effective and very sparse models.", "labels": [], "entities": []}, {"text": "Code and data are available online 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "The overall high dimensionality of textual data is of major importance in text classification (also known as text categorization), opinion mining, noisy text normalization and other NLP tasks.", "labels": [], "entities": [{"text": "text classification", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7485340237617493}, {"text": "text categorization)", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.6646992365519205}, {"text": "opinion mining", "start_pos": 131, "end_pos": 145, "type": "TASK", "confidence": 0.8524196147918701}, {"text": "noisy text normalization", "start_pos": 147, "end_pos": 171, "type": "TASK", "confidence": 0.6152954995632172}]}, {"text": "Since inmost cases a high number of words occurs, one can easily fall in the case of overfitting.", "labels": [], "entities": []}, {"text": "Regularization remains a key element for addressing overfitting in tasks like text classification, domain adaptation and neural machine translation.", "labels": [], "entities": [{"text": "Regularization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9434472322463989}, {"text": "text classification", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.78733029961586}, {"text": "domain adaptation", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.7651970684528351}, {"text": "neural machine translation", "start_pos": 121, "end_pos": 147, "type": "TASK", "confidence": 0.6893878976504008}]}, {"text": "Along with better generalization capabilities, a proper scheme of regularization can also introduce sparsity.", "labels": [], "entities": []}, {"text": "Recently, a number of text regularization techniques have been proposed in the context of deep learning.", "labels": [], "entities": [{"text": "text regularization", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7771274447441101}]}, {"text": "Apart from 1 , 2 and elastic net, a very popular method for regularizing text classification is group lasso.", "labels": [], "entities": [{"text": "regularizing text classification", "start_pos": 60, "end_pos": 92, "type": "TASK", "confidence": 0.8766882022221884}]}], "datasetContent": [{"text": "Next, we present the data, setup and results of our empirical analysis on the text classification task.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.8651635646820068}]}, {"text": "From the 20 Newsgroups, so that we can observe the regularization effect clearly.", "labels": [], "entities": []}, {"text": "In our setup, as features we use unigram frequency concatenated with an additional bias term.", "labels": [], "entities": []}, {"text": "We reproduce standard regularizers like lasso, ridge, elastic and state-of-the-art structured regularizers like sentence, LDA, GoW and w2v groups) as baselines and compare them with the proposed OMP and GOMP.", "labels": [], "entities": [{"text": "GOMP", "start_pos": 203, "end_pos": 207, "type": "DATASET", "confidence": 0.83642578125}]}, {"text": "We used pre-trained Google vectors introduced by and apply k-means clustering algorithm with maximum 2000 clusters.", "labels": [], "entities": []}, {"text": "For each word belonging to a cluster, we also keep the top 5 nearest words so that we introduce overlapping groups.", "labels": [], "entities": []}, {"text": "For the learning part we used Matlab and specifically code provided by    set in a stratified manner by 80% for training and 20% for validation.", "labels": [], "entities": [{"text": "Matlab", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.9134407639503479}]}, {"text": "All the hyperparameters are tuned on the development dataset, using accuracy for evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9994677901268005}]}, {"text": "For lasso and ridge regularization, we choose \u03bb from {10 \u22122 , 10 \u22121 , 1, 10, 10 2 }.", "labels": [], "entities": []}, {"text": "For elastic net, we perform grid search on the same set of values as ridge and lasso experiments for \u03bb rid and \u03bb las . For group lasso, OMP and GOMP regularizers, we perform grid search on the same set of parameters as ridge and lasso experiments.", "labels": [], "entities": []}, {"text": "In the case we get the same accuracy on the development data, the model with the highest sparsity is selected.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9991193413734436}]}, {"text": "In GOMP we considered all individual features as separate groups of size one, along with the w2v groups.", "labels": [], "entities": [{"text": "GOMP", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.8316704034805298}]}, {"text": "Last but not least, in both OMP and GOMP the maximum number of features, K(budget), is set to 2000.", "labels": [], "entities": [{"text": "OMP", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.6567983627319336}, {"text": "GOMP", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.7590678334236145}]}, {"text": "reports the results of our experiments on the aforementioned datasets.", "labels": [], "entities": []}, {"text": "The empirical results reveal the advantages of using OMP or GOMP for regularization in the text categorization task.", "labels": [], "entities": []}, {"text": "The OMP regularizer performs systematically better than the baseline ones.", "labels": [], "entities": [{"text": "OMP regularizer", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.5638719350099564}]}, {"text": "More specifically, OMP outperforms the lasso, ridge and elastic net regularizers in all datasets, as regards to the accuracy.", "labels": [], "entities": [{"text": "OMP", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.6907702088356018}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9985384941101074}]}, {"text": "At the same time, the performance of OMP is quite close or even better to that of structured regularizers.", "labels": [], "entities": []}, {"text": "Actually, in the case of electronics data, the model produced by OMP is the one with the highest accuracy.", "labels": [], "entities": [{"text": "OMP", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.7633867859840393}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9971736669540405}]}, {"text": "On the other hand, the proposed overlapping GOMP regularizer outperforms all the other regularizers in 3 out of 10 datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Descriptive statistics of the datasets.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy on the test sets. Bold font marks the best performance for a dataset, while * indicates statistical  significance at p < 0.05 using micro sign test against lasso. For GOMP, we use w2v clusters and add all unigram  features as individual groups.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9903396964073181}, {"text": "GOMP", "start_pos": 186, "end_pos": 190, "type": "DATASET", "confidence": 0.7709517478942871}]}, {"text": " Table 3: Model sizes (percentages of non-zero features in the resulting models). Bold for best, blue for best group.", "labels": [], "entities": []}, {"text": " Table 5: Comparison in test accuracy with state-of- the-art classifiers: CNN (Kim, 2014), FastText (Joulin  et al., 2017) with no pre-trained vectors. The proposed  OMP and GOMP algorithms produce the highest accu- rate model in 4 out of 10 datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9881822466850281}, {"text": "CNN", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.9715536832809448}, {"text": "accu- rate", "start_pos": 210, "end_pos": 220, "type": "METRIC", "confidence": 0.9015080134073893}]}]}