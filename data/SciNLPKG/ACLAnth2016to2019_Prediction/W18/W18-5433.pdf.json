{"title": [], "abstractContent": [{"text": "This paper analyzes the behavior of stack-augmented recurrent neural network (RNN) models.", "labels": [], "entities": []}, {"text": "Due to the architectural similarity between stack RNNs and pushdown transducers , we train stack RNN models on a number of tasks, including string reversal, context-free language modelling, and cumulative XOR evaluation.", "labels": [], "entities": [{"text": "string reversal", "start_pos": 140, "end_pos": 155, "type": "TASK", "confidence": 0.7486977577209473}, {"text": "context-free language modelling", "start_pos": 157, "end_pos": 188, "type": "TASK", "confidence": 0.5865812301635742}]}, {"text": "Examining the behavior of our networks , we show that stack-augmented RNNs can discover intuitive stack-based strategies for solving our tasks.", "labels": [], "entities": []}, {"text": "However, stack RNNs are more difficult to train than classical ar-chitectures such as LSTMs.", "labels": [], "entities": []}, {"text": "Rather than employ stack-based strategies, more complex networks often find approximate solutions by using the stack as unstructured memory.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent work on recurrent neural network (RNN) architectures has introduced a number of models that enhance traditional networks with differentiable implementations of common data structures.", "labels": [], "entities": []}, {"text": "Appealing to their Turing-completeness, view RNNs as computational devices that learn transduction algorithms, and develop a trainable model of random-access memory that can simulate Turing machine computations.", "labels": [], "entities": []}, {"text": "In the domain of natural language processing, the prevalence of contextfree models of natural language syntax has motivated stack-based architectures such as those of and.", "labels": [], "entities": []}, {"text": "By analogy tos Neural Turing Machines, these stack-based models are designed to simulate pushdown transducer computations.", "labels": [], "entities": []}, {"text": "From a practical standpoint, stack-based models maybe seen as away to optimize networks for discovering dependencies of a hierarchical * Equal contribution. nature.", "labels": [], "entities": []}, {"text": "Additionally, stack-based models could potentially facilitate interpretability by imposing structure upon the recurrent state of an RNN.", "labels": [], "entities": []}, {"text": "Classical architectures such as Simple RNNs), Long Short-Term Memory networks, and Gated Recurrent Unit networks (GRU,) represent state as black-box vectors.", "labels": [], "entities": []}, {"text": "In certain cases, these models can learn to implement classical data structures using state vectors ().", "labels": [], "entities": []}, {"text": "However, because state vectors are fixed in size, the inferred data structures must be represented in a fractal encoding requiring arbitrary position.", "labels": [], "entities": []}, {"text": "On the other hand, differentiable stacks typically increase in size throughout the course of the computation, so their performance may better scale to larger inputs.", "labels": [], "entities": []}, {"text": "Since the ability of a differentiable stack to function correctly intrinsically requires that the information it contains be represented in the proper format, examining the contents of a network's stack throughout the course of its computation could reveal hierarchical patterns that the network has discovered in its training data.", "labels": [], "entities": []}, {"text": "This paper systematically explores the behavior of stack-augmented RNNs on simple computational tasks.", "labels": [], "entities": []}, {"text": "While provide an analysis of stack RNNs based on their Multipop Adaptive Computation Stack model, our analysis is based on the existing Neural Stack model of, as well as a novel enhancement thereof.", "labels": [], "entities": []}, {"text": "We consider tasks with optimal strategies requiring either finite-state memory or a stack, or possibly a combination of the two.", "labels": [], "entities": []}, {"text": "We show that Neural Stack networks have the ability to learn to use the stack in an intuitive manner.", "labels": [], "entities": []}, {"text": "However, we find that Neural Stacks are more difficult to train than classical architectures.", "labels": [], "entities": [{"text": "Neural Stacks", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.8418180644512177}]}, {"text": "In particular, our models prefer not to employ stackbased strategies when other forms of memory are Controller Stack available, such as in networks with both LSTM memory and a stack.", "labels": [], "entities": []}, {"text": "A description of our models, including a review of Grefenstette et al.'s Neural Stacks, appears in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 discusses the relationship between stack-augmented RNN models and pushdown transducers, motivating our intuition that Neural Stacks area suitable architecture for learning context-free structure.", "labels": [], "entities": []}, {"text": "The tasks we consider are defined in Section 4, and our experimental paradigm is described in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 presents quantitative evaluation of our models' performance as well as qualitative description of their behavior.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider two tasks that require the network to implement the XOR function.", "labels": [], "entities": []}, {"text": "In the Cumulative XOR Evaluation task, the network reads an input string of 1s and 0s.", "labels": [], "entities": [{"text": "Cumulative XOR Evaluation task", "start_pos": 7, "end_pos": 37, "type": "TASK", "confidence": 0.8874695301055908}]}, {"text": "At each time step, the network must output the XOR of all the input symbols it has seen so far.", "labels": [], "entities": [{"text": "XOR", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.980036735534668}]}, {"text": "The Delayed XOR Evaluation task is similar, except that the most recent input symbol is excluded from the XOR computation.", "labels": [], "entities": []}, {"text": "As shown in the left of, the XOR Evaluation tasks can be computed by a PDT without using the stack.", "labels": [], "entities": []}, {"text": "Thus, we use XOR Evaluation to test the versatility of the stack by assessing whether a feedforward controller can learn to use it as unstructured memory.", "labels": [], "entities": []}, {"text": "The Cumulative XOR Evaluation task presents the linear controller with a theoretical challenge because single-layer linear networks cannot compute the XOR function.", "labels": [], "entities": [{"text": "Cumulative XOR Evaluation task", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.7718033790588379}]}, {"text": "However, in the Delayed XOR Evaluation task, the delay between reading an input symbol and incorporating it into the XOR gives the network two linear layers to compute XOR when unravelled through time.", "labels": [], "entities": []}, {"text": "Therefore, we expect that the linear model should be able to perform the Delayed XOR Evaluation task, but not the Cumulative XOR Evaluation task.", "labels": [], "entities": [{"text": "Cumulative XOR Evaluation task", "start_pos": 114, "end_pos": 144, "type": "TASK", "confidence": 0.6368464902043343}]}, {"text": "The discrepancy between the Cumulative and the Delayed XOR Evaluation tasks for the linear controller highlights the importance of timing in stack algorithms.", "labels": [], "entities": []}, {"text": "Since the our enhanced architecture from Subsection 2.3 can perform \u03b5-transitions, we expect it to perform the Cumulative XOR Evaluation task with a linear controller by learning to introduce the necessary delay.", "labels": [], "entities": [{"text": "Cumulative XOR Evaluation task", "start_pos": 111, "end_pos": 141, "type": "TASK", "confidence": 0.688426561653614}]}, {"text": "Thus, the XOR tasks allow us to test whether our buffered model can learn to optimize the timing of its computation.", "labels": [], "entities": []}, {"text": "In the Boolean Formula Evaluation task, the network reads a boolean formula in reverse Polish notation generated by the following CFG.", "labels": [], "entities": [{"text": "Boolean Formula Evaluation task", "start_pos": 7, "end_pos": 38, "type": "TASK", "confidence": 0.7279998436570168}, {"text": "CFG", "start_pos": 130, "end_pos": 133, "type": "DATASET", "confidence": 0.981111466884613}]}, {"text": "At each time step, the network must output the truth value of the longest sub-formula ending at the input symbol.", "labels": [], "entities": []}, {"text": "The Boolean Formula Evaluation task tests the ability of Neural Stacks to infer complex computations over the stack.", "labels": [], "entities": [{"text": "Boolean Formula Evaluation task", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.7538280040025711}]}, {"text": "In this case, the network must store previously computed values on the stack and evaluate boolean operations over these stored values.", "labels": [], "entities": []}, {"text": "This technique is reminiscent of shift-reduce parsing, making the Boolean Formula Evaluation task a testing ground for the possibility of applying Neural Stacks to natural language parsing.", "labels": [], "entities": [{"text": "shift-reduce parsing", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.6372072398662567}, {"text": "Boolean Formula Evaluation task", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.7133972942829132}, {"text": "natural language parsing", "start_pos": 164, "end_pos": 188, "type": "TASK", "confidence": 0.663305014371872}]}, {"text": "We conducted four experiments designed to assess various aspects of the behavior of Neural Stacks.", "labels": [], "entities": []}, {"text": "In each experiment, models are trained on a generated dataset consisting of 800 input-output string pairings encoded in one-hot representation.", "labels": [], "entities": []}, {"text": "Training occurs in mini-batches containing 10 string pairings each.", "labels": [], "entities": []}, {"text": "At the end of each epoch, the model is evaluated on a generated development set of 100 examples.", "labels": [], "entities": []}, {"text": "Training terminates when five consecutive epochs fail to exceed the highest development accuracy attained.", "labels": [], "entities": [{"text": "development", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.920245885848999}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.6712114214897156}]}, {"text": "The sizes of the LSTM controllers' recurrent state vectors are fixed to 10, and, with the exception of Experiment 2 described below, the sizes of the vectors placed on the stack are fixed to 2.", "labels": [], "entities": []}, {"text": "After training is complete, each trained model is evaluated on a testing set of 1000 generated strings, each of which is at least roughly twice as long as the strings used for training.", "labels": [], "entities": []}, {"text": "10 trials are performed for each set of experimental conditions.", "labels": [], "entities": []}, {"text": "Experiment 1 tests the propensity of trained Neural Stack models to use the stack.", "labels": [], "entities": []}, {"text": "We train both the standard Neural Stack model and our enhanced buffered model from Subsection 2.3 to perform the String Reversal task using the linear controller.", "labels": [], "entities": []}, {"text": "To compare the stack with unstructured memory, we also train the standard Neural Stack model using the LSTM controller as well as an LSTM model without a stack.", "labels": [], "entities": []}, {"text": "Training and development data are obtained from sequences of 0s and 1s randomly generated with an average length of 10.", "labels": [], "entities": []}, {"text": "The testing data have an average length of 20.", "labels": [], "entities": [{"text": "length", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9437195658683777}]}, {"text": "Experiment 2 considers the XOR Evaluation tasks.", "labels": [], "entities": [{"text": "XOR Evaluation", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.7294521331787109}]}, {"text": "We train standard models with a linear controller on the Delayed XOR task and an LSTM controller on the Cumulative XOR task to test the network's ability to use the stack as unstructured state.", "labels": [], "entities": []}, {"text": "We also train both a standard and a buffered model on the Cumulative XOR Evaluation task using the linear controller to test the network's ability to use our buffering mechanism to infer optimal timing for computation steps.", "labels": [], "entities": [{"text": "Cumulative XOR Evaluation task", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.7588520795106888}]}, {"text": "Training and development data are obtained from randomly generated sequences of 0s and 1s fixed to a length of 12.", "labels": [], "entities": []}, {"text": "The testing data are fixed to a length of 24.", "labels": [], "entities": []}, {"text": "The vectors placed on the stack are fixed to a size of 6.", "labels": [], "entities": []}, {"text": "In Experiment 3, we attempt to perform the Parenthesis Prediction task using standard models with various types of memory: a linear controller with no stack, which has no memory; a linear controller with a stack, which has stackstructured memory; an LSTM controller with no stack, which has unstructured memory; and an LSTM controller with a stack, which has both stack-structured and unstructured memory.", "labels": [], "entities": [{"text": "Parenthesis Prediction task", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.955649177233378}]}, {"text": "Sequences of well-nested parentheses are generated by the CFG from the previous section.", "labels": [], "entities": [{"text": "CFG", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.9814659357070923}]}, {"text": "The training and development data are obtained by randomly sampling from the set of strings of derivation depth at most 6, which contains strings of length up to 20.", "labels": [], "entities": []}, {"text": "The testing data are of depth 12 and length up to 110.", "labels": [], "entities": [{"text": "length", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.993644118309021}]}, {"text": "Experiment 4 compares the standard models with linear and LSTM controllers against a baseline consisting of an LSTM controller with no stack.", "labels": [], "entities": []}, {"text": "Whereas Experiments 1-3 presented the network with tasks designed to showcase various features of the Neural Stack architecture, the goal of this experiment is to gauge the extent to which stack-structured memory may improve the network's performance on more sophisticated tasks.", "labels": [], "entities": []}, {"text": "We train the three types of models on the Boolean Formula Evaluation task and the SubjectAuxiliary Agreement task.", "labels": [], "entities": [{"text": "Boolean Formula Evaluation task", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.8110765516757965}]}, {"text": "Data for both tasks are generated by the CFGs given in Section 4.", "labels": [], "entities": [{"text": "CFGs", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.9305585026741028}]}, {"text": "The boolean formulae for training and development are randomly sampled from the set of strings of derivation depth at most 6, having a maximum length of 15, while the testing data are sampled from derivations of depth at most 7, with a maximum length of 31.", "labels": [], "entities": []}, {"text": "The sentence prefixes are of depth 16 and maximum length 23 during the training phase, and depth 32 and maximum length 49 during the final evaluation round.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The minimum, median, and maximum accuracy (%) attained by the 10 models for each experimental  condition during the last epoch of the training phase (left) and the final testing phase (right).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9959368705749512}]}]}