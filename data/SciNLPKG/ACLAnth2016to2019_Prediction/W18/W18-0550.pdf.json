{"title": [], "abstractContent": [{"text": "We investigate the feasibility of cross-lingual content scoring, a scenario where training and test data in an automatic scoring task are from two different languages.", "labels": [], "entities": [{"text": "cross-lingual content scoring", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.6793461342652639}]}, {"text": "Cross-lingual scoring can contribute to educational equality by allowing answers in multiple languages.", "labels": [], "entities": []}, {"text": "Training a model in one language and applying it to another language might also help to overcome data sparsity issues by re-using trained models from other languages.", "labels": [], "entities": []}, {"text": "As there is no suitable dataset available for this new task, we create a comparable bilingual corpus by extending the English ASAP dataset with Ger-man answers.", "labels": [], "entities": [{"text": "English ASAP dataset", "start_pos": 118, "end_pos": 138, "type": "DATASET", "confidence": 0.6261291205883026}]}, {"text": "Our experiments with cross-lingual scoring based on machine-translating either training or test data show a considerable drop in scoring quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatically scoring the content of student answers is a well-established research field (see, e.g.,;;).", "labels": [], "entities": [{"text": "Automatically scoring the content of student answers", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.757868366582053}]}, {"text": "However, content scoring is usually restricted to training a model on labeled answers in one language and then applying it to unseen student answers in the same language.", "labels": [], "entities": [{"text": "content scoring", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.7924696505069733}]}, {"text": "In this paper, we examine how well the scoring models transfer when being applied crosslingually, i.e., whether data in one language can be used for training a model to score data in another language.", "labels": [], "entities": []}, {"text": "The motivation for our study is two-fold: First, cross-lingual scoring can contribute to educational equality.", "labels": [], "entities": []}, {"text": "Ina realistic educational setting, scores assigned to an answer given in the language of instruction can discriminate against non-native students who might conceptually understand the topic in question, but are unable to express their understanding in that language.", "labels": [], "entities": []}, {"text": "One solution to this problem could be that students are allowed to answer a question in a language they are proficient in.", "labels": [], "entities": []}, {"text": "As only the content matters, the form, including the language, is unimportant.", "labels": [], "entities": []}, {"text": "Such a setting would of course require that a teacher scoring an item is also proficient in the language used by the student, which would still restrict the available language options for the student.", "labels": [], "entities": []}, {"text": "In such a scenario, automatic scoring of answers in different languages can help to treat students equally.", "labels": [], "entities": []}, {"text": "Second, cross-lingual scoring can help to overcome data sparsity.", "labels": [], "entities": [{"text": "cross-lingual scoring", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.7649922966957092}]}, {"text": "Existing short-answer datasets have mainly been collected in English.", "labels": [], "entities": []}, {"text": "If a researcher or practitioner wants to work on a different language, little annotated data is available.", "labels": [], "entities": []}, {"text": "Cross-lingual approaches can help in such a scenario to re-use trained models from different languages or to combine data from several languages to train anew model.", "labels": [], "entities": []}, {"text": "In our study, we investigate whether crosslingual scoring is possible using state-of-the-art machine translation techniques.", "labels": [], "entities": [{"text": "crosslingual scoring", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.8366928100585938}, {"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7519052028656006}]}, {"text": "We translate either training or test data from one language to another, such that both training and test data are available in the same language.", "labels": [], "entities": []}, {"text": "We then build prompt-specific models for each prompt and compare the performance to a monolingual approach.", "labels": [], "entities": []}, {"text": "It is likely that machine translation will negatively impact scoring quality due to translation errors.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7691748440265656}]}, {"text": "Additionally, student answers often contain language errors that might further decrease translation quality.", "labels": [], "entities": []}, {"text": "However, translation might also have a positive effect on automatic scoring in case of typos being corrected during translation (e.g. seperate correctly translated as getrennt).", "labels": [], "entities": []}, {"text": "Datasets in more than one language might also differ depending on different teaching or learning traditions in the environments where they are collected, so that anew dataset collection has to be carefully planned to control such influence factors.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we are the first to investigate the feasibility of cross-lingual scoring.", "labels": [], "entities": [{"text": "cross-lingual scoring", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.8156501650810242}]}, {"text": "As our approach relies heavily on the availability of machine-translation methods, we also assess whether state-of-the-art machine translation methods perform well enough to be used in automatic scoring.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7440265715122223}]}, {"text": "To evaluate cross-lingual scoring in a realistic scenario, we collect and release anew dataset ASAP-DE that consists of three prompts from the ASAP corpus for which we collect answers in German.", "labels": [], "entities": [{"text": "ASAP-DE", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.8387026190757751}, {"text": "ASAP corpus", "start_pos": 143, "end_pos": 154, "type": "DATASET", "confidence": 0.9147962927818298}]}, {"text": "In our experiments, we find that cross-lingual scoring using machinetranslation is feasible, but -unsurprisingly-at the cost of a decrease in performance.", "labels": [], "entities": [{"text": "cross-lingual scoring", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.7929845452308655}]}, {"text": "Preliminary analyses showed that his performance drop varies across prompts and is only in part due to artifacts of machine translation, but it rather results from differences between the two datasets involved.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.6975546479225159}]}], "datasetContent": [{"text": "For our cross-lingual experiments, we need a dataset that contains answers to the same prompt in at least two different languages.", "labels": [], "entities": []}, {"text": "As no such dataset is publicly available so far, we decided to create and release anew dataset.", "labels": [], "entities": []}, {"text": "We decided to extend an existing monolingual dataset instead of collecting anew dataset from scratch, as it provides the advantage that larger amounts of data are already available in one language.", "labels": [], "entities": []}, {"text": "The majority of datasets is available in English, so this is a realistic option for the source language.", "labels": [], "entities": []}, {"text": "We use German as the target language due to familiarity with the language, as we need to be able to manually score the new dataset.", "labels": [], "entities": []}, {"text": "Also, the expected translation quality between English and German is rather high providing a good test case for the feasibility of the approach in general.", "labels": [], "entities": []}, {"text": "There is a set of publicly available English datasets that we could base our experiments on: The ASAP-2 short answer scoring dataset 4 , the Powergrading dataset by (, the computer science dataset by, and the SemEval2013 dataset (.", "labels": [], "entities": [{"text": "ASAP-2 short answer scoring dataset", "start_pos": 97, "end_pos": 132, "type": "DATASET", "confidence": 0.5701795816421509}, {"text": "Powergrading dataset", "start_pos": 141, "end_pos": 161, "type": "DATASET", "confidence": 0.9064582288265228}, {"text": "SemEval2013 dataset", "start_pos": 209, "end_pos": 228, "type": "DATASET", "confidence": 0.8616038858890533}]}, {"text": "When deciding fora dataset, we took the following criteria into account: First, all necessary prompt material has to be completely available, including reading texts or connected images.", "labels": [], "entities": []}, {"text": "This requirement rules out the SemEval2013 data, where the prompt contains pictures and graphs (such a drawing of a electrical circuit) that are necessary to answer the questions but that are not included in the dataset.", "labels": [], "entities": [{"text": "SemEval2013 data", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.8323558866977692}]}, {"text": "Second, the prompts should be language and culture-independent so that speakers of a different language or from a different culture have similar chances to answer the questions correctly.", "labels": [], "entities": []}, {"text": "This requirement rules out the Powergrading data, as this dataset contains solely questions from US immigration tests like, If both the President and the Vice President can no longer serve, who becomes President?", "labels": [], "entities": [{"text": "Powergrading data", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.8138091266155243}]}, {"text": "German participants are rather unlikely to correctly answer those questions.", "labels": [], "entities": []}, {"text": "Third, the prompts should be curriculumindependent, i.e., they should not be based on a specific university course, as we expect answers in those settings to be heavily influenced by what exactly was taught in the corresponding course.", "labels": [], "entities": []}, {"text": "Thus, we excluded the computer science dataset, which was targeted at students from a specific computer science class.", "labels": [], "entities": [{"text": "computer science dataset", "start_pos": 22, "end_pos": 46, "type": "DATASET", "confidence": 0.6150589386622111}]}, {"text": "(In addition, the number of only 30 answers per prompt is relatively small.)", "labels": [], "entities": []}, {"text": "Last, in order to be able to score the newly collected data, scoring guidelines for the original dataset have to be available and we must be able to apply them with a reasonable inter-annotatoragreement.", "labels": [], "entities": []}, {"text": "Re-scoring Study The ASAP dataset is the only dataset fulfilling the first two requirements and seems relatively curriculum-independent as well.", "labels": [], "entities": [{"text": "ASAP dataset", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.7348917424678802}]}, {"text": "We tested in an annotation study, whether we are able to apply the available annotation guidelines.", "labels": [], "entities": []}, {"text": "We selected one prompt for each of the three domains covered by the dataset (science, biology, English Language Arts (ELA)).", "labels": [], "entities": []}, {"text": "Two German native speakers with a good command in English annotated a subset of 50 answers for each prompt.", "labels": [], "entities": []}, {"text": "For the science prompt, the pairwise inter-annotator agreement between our two annotators and the original English annotators, measured by quadratically weighted kappa, was between .70 and .79 for the science prompt, between .60 and .78 for biology, and between .26 and .63 for ELA.", "labels": [], "entities": [{"text": "ELA", "start_pos": 278, "end_pos": 281, "type": "DATASET", "confidence": 0.8652390837669373}]}, {"text": "IAA between the two German annotators lies in similar regions.", "labels": [], "entities": [{"text": "IAA", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.5895105004310608}]}, {"text": "The agreement between the two original annotations was .95 for science, .98 for biology and .77 for ELA.", "labels": [], "entities": [{"text": "ELA", "start_pos": 100, "end_pos": 103, "type": "DATASET", "confidence": 0.5758710503578186}]}, {"text": "Based on these numbers, we deemed ELA prompts unsuitable for re-collection.", "labels": [], "entities": [{"text": "ELA prompts", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.5441900789737701}]}, {"text": "As described above, we find the science and biology prompts from ASAP to be suitable for the re-collection process.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.7311868667602539}]}, {"text": "An exploratory data collec-  tion for the three science and two biology prompts revealed that the knowledge tested in the biology prompts was more course-specific than we thought and most participants were unable to answer these questions.", "labels": [], "entities": []}, {"text": "Therefore, we restricted ourselves to the three science prompts, which we translated into German.", "labels": [], "entities": []}, {"text": "We collect answers from the crowdsourcing platform CrowdFlower, 5 as well as by directly asking colleagues and students, with the majority of answer (>90%) originating from CrowdFlower.", "labels": [], "entities": [{"text": "CrowdFlower, 5", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.8404550353686014}]}, {"text": "We excluded answers in any language different from German and obvious non-answers, such as copying the prompt.", "labels": [], "entities": [{"text": "copying the prompt", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.8190394242604574}]}, {"text": "Overall, we collect a total of 301 answers per prompt.", "labels": [], "entities": []}, {"text": "compares the resulting German dataset with the original English one.", "labels": [], "entities": [{"text": "German dataset", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.8528459668159485}]}, {"text": "All answers have been annotated by two German annotators (one being one of the authors of this paper).", "labels": [], "entities": []}, {"text": "We found an inter-annotator agreement per prompt between .58 and .84 quadratically weighted kappa.", "labels": [], "entities": []}, {"text": "shows some exemplary answers from Prompt 1 both for the original English and the newly collected German dataset.", "labels": [], "entities": [{"text": "German dataset", "start_pos": 97, "end_pos": 111, "type": "DATASET", "confidence": 0.8864957690238953}]}, {"text": "We provide a corpus analysis to get further insights into the differences between the two language versions of the dataset.", "labels": [], "entities": []}, {"text": "Label distribution A first indicator as to whether the two language versions are comparable is the label distribution as shown in.", "labels": [], "entities": []}, {"text": "We see that the distribution in the German dataset is skewed towards lower scores, which could bean artifact of our assessment situation.", "labels": [], "entities": [{"text": "German dataset", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.9549048542976379}]}, {"text": "While we tried to avoid questions answerable only by a certain group of learners, it might still be that the original English test taker population was either better prepared or more motivated to answer the ques-ENGLISH QUESTION: After reading the groups procedure, describe what additional information you would need in order to replicate the experiment.", "labels": [], "entities": [{"text": "English test taker population", "start_pos": 118, "end_pos": 147, "type": "DATASET", "confidence": 0.8155520111322403}, {"text": "ques-ENGLISH", "start_pos": 207, "end_pos": 219, "type": "METRIC", "confidence": 0.9303314685821533}]}, {"text": "Make sure to include at least three pieces of information.", "labels": [], "entities": []}, {"text": "After finding in the previous monolingual pilot study that machine translation quality is good enough for our purposes, we now present in this section our cross-lingual experiments.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7776415646076202}]}, {"text": "We assume that training data in one language is used to score test data in another language by means of translating either the test or the training data.", "labels": [], "entities": []}, {"text": "For our scoring experiments, we use a standard supervised machine learning setup with Weka's SVM classifier in standard configuration as classification backbone, implemented using free-text scoring toolkit ESCRITO (.", "labels": [], "entities": []}, {"text": "We use token uni-, bi-and trigrams as well as character bi-to five-grams as features and evaluate our results using accuracy and quadratically weighted Kappa.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9994076490402222}]}, {"text": "The English ASAP dataset comes with an established split into train and test data, which we reuse.", "labels": [], "entities": [{"text": "English ASAP dataset", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7471408049265543}]}, {"text": "The German dataset is very small indirect comparison, so that we cannot use a fixed split into training and test data.", "labels": [], "entities": [{"text": "German dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9472506642341614}]}, {"text": "Therefore, we use 10-fold cross-validation for the German dataset.", "labels": [], "entities": [{"text": "German dataset", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.9640302956104279}]}, {"text": "Experimental conditions We conduct experiments falling into four groups: (1) for the baseline experiments, we train and test models on monolingual datasets and use either the English or German dataset exclusively.", "labels": [], "entities": []}, {"text": "These two datasets have very different sizes.", "labels": [], "entities": []}, {"text": "For the original English data, we have over 2000 answers per prompt.", "labels": [], "entities": [{"text": "English data", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.8302562236785889}]}, {"text": "For the re-collected German set, we only have 300 answers per prompt, 270 of which are used for training in our cross-validation setup.", "labels": [], "entities": [{"text": "German set", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.894201785326004}]}, {"text": "This difference in size might also reflect in different performances.", "labels": [], "entities": []}, {"text": "To eliminate such effects, we conduct experiments on the English training data in a variant that uses only 270 training items, sampled from the training data section.", "labels": [], "entities": [{"text": "English training data", "start_pos": 57, "end_pos": 78, "type": "DATASET", "confidence": 0.6739098429679871}]}, {"text": "For comparison, we also conduct the baseline experiment on the full English train data (EN all ).", "labels": [], "entities": [{"text": "English train data", "start_pos": 68, "end_pos": 86, "type": "DATASET", "confidence": 0.9007027745246887}, {"text": "EN", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.8041617274284363}]}, {"text": "To avoid sampling artifacts, we repeat the experiment 100 times with different splits and report the average of all runs.", "labels": [], "entities": []}, {"text": "(2) In the monolingual condition, we translate both the training and the test data, similar to our experiments in the pilot study, but using data sampling that makes sure that training data sizes are comparable.", "labels": [], "entities": []}, {"text": "Differences to the baseline are thus only due to the machine translation process.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7001402080059052}]}, {"text": "(3) In the translate train experiments, we combine the original English test data with the German training data automatically translated to English, as well as original German test data with the English training data translated to German.", "labels": [], "entities": [{"text": "translate train", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.9028089344501495}]}, {"text": "(4) In the translate test condition, we use test data translated into the other language with the original test data from that language.", "labels": [], "entities": []}, {"text": "In these last two conditions, differences to the baseline result either from machine translation or from differences inherent to the datasets.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.6966053992509842}]}, {"text": "This shows that our manual scoring of the German data set is reliable enough to learn a competitive model.", "labels": [], "entities": [{"text": "German data set", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.9483983318010966}]}, {"text": "In the second monolingual case, when we translate both training and test data, we only observe moderate losses or for some prompts even small improvements compared to the original language version.", "labels": [], "entities": []}, {"text": "When turning towards the cross-lingual results, where we either only translate train or test data, the picture looks quite different: in all four conditions, scoring performance is considerably lower compared to the monolingual settings.", "labels": [], "entities": []}, {"text": "The loss is especially pronounced for prompt 2.", "labels": [], "entities": [{"text": "loss", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9585130214691162}, {"text": "prompt 2", "start_pos": 38, "end_pos": 46, "type": "TASK", "confidence": 0.9021270871162415}]}, {"text": "This difference between prompts cannot be explained by our corpus analysis in Section 3, especially the vocabulary overlap between English and German datasets, which were in the same range for all three prompts (and even slightly higher for prompt 2 than for the other two prompts).", "labels": [], "entities": []}, {"text": "As discussed in the introduction, the difference between the baseline and cross-lingual scoring performance can originate from two sources: dif-ferent learner populations and effects of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.6923887133598328}]}, {"text": "In order to assess the individual contributions of these two factors, we propose a variant of our experiment that operates on only one dataset but still uses machine translation on either the test or the training data, so that the delta in performance is due to translation and not to different learner populations.", "labels": [], "entities": []}, {"text": "We achieve this by doubletranslating the training or test data of the English ASAP dataset, i.e., we have the data automatically translated from English to German and then back to English (marked as EN 2T ).", "labels": [], "entities": [{"text": "English ASAP dataset", "start_pos": 70, "end_pos": 90, "type": "DATASET", "confidence": 0.6555218001206716}]}, {"text": "shows the performance in comparison to the monolingual baseline experiments where we see that double-translating the test data decreases performance considerably while -surprisingly-doubletranslating the training data leaves performance unaffected.", "labels": [], "entities": []}, {"text": "A naive approach to factor out artifacts from translationese, while keeping effects stemming from the differences between the datasets, would be to use translated datasets in the cross-lingual case both for training and testing, i.e., we doubletranslate one dataset and translate the other one only once.", "labels": [], "entities": []}, {"text": "In this setup, shown in, performance benefits only slightly, if at all, from doubletranslation (with the exception of double translated train data in prompt 1).", "labels": [], "entities": []}, {"text": "Consider the following example of an answer from the original English dataset: (A) Plastic type B was the superior in both trial 1 and trial 2.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.8455499410629272}]}, {"text": "(B) Record the weight that was put onto show how much effected each plastic.", "labels": [], "entities": []}, {"text": "Also conducting more trials (.", "labels": [], "entities": []}, {"text": ") After translating the answer automatically to German and back to English it looks like this: Type B plastic was the supervisor in both Trial 1 and Trial 2.", "labels": [], "entities": []}, {"text": "(B) Write down the weight that was put onto show how much each one has made plastic.", "labels": [], "entities": []}, {"text": "Also do more experiments (.", "labels": [], "entities": []}, {"text": ") Apart from obvious translation errors (superiorsupervisor), we see a simplifying effect of translation: record-write down, effect-make, and conduct-do.", "labels": [], "entities": [{"text": "translation", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.9629881381988525}]}, {"text": "Such simplifications might on the one hand normalize over different paraphrases of the same content, but could on the other hand also remove meaningful differences between correct and incorrect answers.: Double translation in cross-lingual setting", "labels": [], "entities": [{"text": "Double translation", "start_pos": 204, "end_pos": 222, "type": "TASK", "confidence": 0.6952383518218994}]}], "tableCaptions": [{"text": " Table 4: Lexical overlap measured on the type level for  the top 1000 unigrams for each prompt.", "labels": [], "entities": [{"text": "overlap", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.6356722116470337}]}, {"text": " Table 5: Content scoring performance measured  in quadratically weighted kappa for different cross- lingual setups.", "labels": [], "entities": []}, {"text": " Table 6: Double translation in monolingual setting", "labels": [], "entities": [{"text": "Double translation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6154991686344147}]}, {"text": " Table 7: Double translation in cross-lingual setting", "labels": [], "entities": [{"text": "Double translation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7413589954376221}]}]}