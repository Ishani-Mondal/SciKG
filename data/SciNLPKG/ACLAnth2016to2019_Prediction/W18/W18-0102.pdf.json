{"title": [{"text": "Predictive power of word surprisal for reading times is a linear function of language model quality", "labels": [], "entities": []}], "abstractContent": [{"text": "Within human sentence processing, it is known that there are large effects of a word's probability in context on how long it takes to read it.", "labels": [], "entities": []}, {"text": "This relationship has been quantified using information-theoretic surprisal, or the amount of new information conveyed by a word.", "labels": [], "entities": []}, {"text": "Here, we compare surprisals derived from a collection of language models derived from n-grams, neural networks, and a combination of both.", "labels": [], "entities": []}, {"text": "We show that the mod-els' psychological predictive power improves as a tight linear function of language model linguistic quality.", "labels": [], "entities": [{"text": "mod-els' psychological predictive", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.5035055577754974}]}, {"text": "We also show that the size of the effect of surprisal is estimated consistently across all types of language models.", "labels": [], "entities": []}, {"text": "These findings point toward surprising robustness of surprisal estimates and suggest that surprisal estimated by low-quality language models are not biased.", "labels": [], "entities": []}], "introductionContent": [{"text": "Decades of work studying human sentence processing have demonstrated that a word's probability in context is strongly related to the amount of time it takes to read it.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7550633251667023}]}, {"text": "This relationship has been quantified by surprisal theory, which states that processing difficulty of a word win context c is proportional to its information-theoretic surprisal, defined as \u2212 log p(w|c).", "labels": [], "entities": []}, {"text": "As a word is more likely to occur in its context, and thus communicates less information, it is read more quickly.", "labels": [], "entities": []}, {"text": "One difficulty in testing such effects of a word's probability in context is the need to construct estimates of a word's probability in context.", "labels": [], "entities": []}, {"text": "One way of estimating such probabilities is to give human subjects a context, have them guess the next word, and estimate p(w|c) as the proportion of participants who guess word win context c.", "labels": [], "entities": []}, {"text": "This method, called a Cloze task, may yield reliable estimates for words that have relatively high probabilities in their context, and it has been used in a number of studies of the effects of probabilities in context on reading.", "labels": [], "entities": []}, {"text": "However, it is an open question whether these human guess-derived proportions maybe biased from objective probabilities in some way).", "labels": [], "entities": []}, {"text": "Problematically for studying surprisal specifically, however, the Cloze task cannot in principle yield reliable estimates of word probabilities in context that are relatively low, say less than 1 in 100, as many word probabilities are, without requiring an extremely large number of participants.", "labels": [], "entities": []}, {"text": "Additionally, it is not practical to use the Cloze task to estimate probabilities for large datasets on which surprisal is often studied, for which there can easily be tens of thousands of contexts that would require estimation.", "labels": [], "entities": []}, {"text": "The alternative is to estimate the probabilities of words in context using computational language models, which are trained on large language corpora to estimate the probabilities of words in context.", "labels": [], "entities": []}, {"text": "Many studies of surprisal have used such language models (e.g..", "labels": [], "entities": []}, {"text": "Unfortunately, however, computational language models are still substantially worse than humans at predicting upcoming words, meaning there is some mismatch between the probabilities p(w|c) being estimated computationally and the implicit probabilities in the brains of readers that humans are using.", "labels": [], "entities": []}, {"text": "This situation raises the question of to what extent we can trust results about the effects of surprisal as estimated by such language models.", "labels": [], "entities": []}, {"text": "To try to get some information about possible biases that might exist in our results based on language models being worse than humans at predicting upcoming words, poor linguistic quality, we can compare a range of computational language models of varying linguistic quality and see how the estimated effects of surprisal change.", "labels": [], "entities": []}, {"text": "If there is a trend in results as the linguistic quality of the language models improves, that would provide evidence that such a trend maybe even more present in language models with human-level linguistic quality.", "labels": [], "entities": []}, {"text": "Additionally, recent years have seen rapid progress in computational language modeling, enabled by recent advances in neural networks.", "labels": [], "entities": [{"text": "computational language modeling", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.6959759394327799}]}, {"text": "As a result, the linguistic quality of contemporary language models is far beyond what has been used in previous work studying surprisal.", "labels": [], "entities": []}, {"text": "In this paper, we address both these concerns by analyzing how the predictive power of these surprisal estimates, their psychological quality, varies as a function of language model linguistic quality and type.", "labels": [], "entities": []}, {"text": "There has also been substantial interest in the shape of the effects of surprisal on reading times, because of theories that predict it to be linear.", "labels": [], "entities": []}, {"text": "A secondary goal of this work is to investigate whether the shape of this effect depends on language model quality or type.", "labels": [], "entities": []}, {"text": "In particular, we compare surprisal estimates using a range of language models of varying linguistic qualities and types, from the n-gram models that have been used inmost previous work on surprisal to state-of-the-art LSTM and interpolated-LSTM models.", "labels": [], "entities": []}, {"text": "We assess the predictive ability and the size and shape of surprisals derived from each language model using generalized additive mixed-effects models fit to a corpus of eye movements in reading.", "labels": [], "entities": []}, {"text": "The plan for the remainder of this paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the set of language models we compare and establishes the linguistic quality of each.", "labels": [], "entities": []}, {"text": "Then, in Section 3 we quantify the ability of surprisals derived from each language model to predict reading times and seethe extent to which this changes with language model type and quality, assuming that effects of surprisal on reading times are linear.", "labels": [], "entities": []}, {"text": "In Section 4 we do the same but allow surprisal to have non-linear effects, and we additionally use the non-linear models to assess whether there is evidence that the shape of the surprisal effect changes with language model type or quality.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Perplexity of language models generated  either as a LSTM, n-grams, or an interpolation  of both the LSTM model as well as the 5-gram  model. Perplexities were calculated for the entire  Dundee corpus (60, 916 tokens) as well as for only  the tokens in the 1b corpus (60, 741 tokens).", "labels": [], "entities": [{"text": "Dundee corpus", "start_pos": 197, "end_pos": 210, "type": "DATASET", "confidence": 0.9903542697429657}]}, {"text": " Table 2: As the perplexity of a language model increases, its improvement over baseline log likelihood  (\u2206LogLik) decreases. The coefficients for both the current and previous words do not bear a consistent  relationship with model perplexity.", "labels": [], "entities": [{"text": "baseline log likelihood  (\u2206LogLik)", "start_pos": 80, "end_pos": 114, "type": "METRIC", "confidence": 0.6611524224281311}]}, {"text": " Table 3: Correlation results for metrics of predic- tors of linear and non-linear GAMMs", "labels": [], "entities": [{"text": "GAMMs", "start_pos": 83, "end_pos": 88, "type": "TASK", "confidence": 0.8583126664161682}]}, {"text": " Table 4: Log likelihood and F statistics for GAMMs with nonlinear smoothers on all covariates", "labels": [], "entities": [{"text": "F", "start_pos": 29, "end_pos": 30, "type": "METRIC", "confidence": 0.9948707818984985}, {"text": "GAMMs", "start_pos": 46, "end_pos": 51, "type": "TASK", "confidence": 0.9635109901428223}]}]}