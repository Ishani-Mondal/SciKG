{"title": [{"text": "Exploring Word Sense Disambiguation Abilities of Neural Machine Translation Systems", "labels": [], "entities": [{"text": "Exploring Word Sense Disambiguation Abilities of Neural Machine Translation", "start_pos": 0, "end_pos": 75, "type": "TASK", "confidence": 0.6934359769026438}]}], "abstractContent": [{"text": "Neural machine translation systems have been shown to achieve state-of-the-art translation performance for many language pairs.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6493760744730631}]}, {"text": "In order to produce a correct translation, MT systems must learn how to disambiguate words with multiple senses and pick the correct translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9778314828872681}]}, {"text": "We explore the extent to which the word embeddings for ambiguous words are able to disambiguate senses at deeper layers of the NMT encoder, which are thought to represent words with surrounding context.", "labels": [], "entities": []}, {"text": "Consistent with previous research, we find that the NMT system fails to translate many ambiguous words correctly.", "labels": [], "entities": []}, {"text": "We provide an evaluation framework to use for proposed improvements to word sense disambiguation abilities of NMT systems.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.6871078411738077}]}], "introductionContent": [{"text": "Neural machine translation systems have to be able to perform many different linguistic tasks successfully in order to obtain good translations.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6698404947916666}]}, {"text": "For example, MT systems have to be able to deal with syntactic reordering, semantic relationships, co-reference, and discourse roles, among other phenomena.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9854955673217773}]}, {"text": "The obvious question that arises is: how well are state-of-the-art NMT systems doing at detecting linguistic features?", "labels": [], "entities": []}, {"text": "This question is not new.", "labels": [], "entities": []}, {"text": "Statistical machine translation (SMT) systems have achieved consistently high BLEU scores because they explicitly try to model features such as word or phrase alignments.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8107586403687795}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9985068440437317}, {"text": "word or phrase alignments", "start_pos": 144, "end_pos": 169, "type": "TASK", "confidence": 0.5958645567297935}]}, {"text": "For lower-resource languages, SMT systems have been shown to outperform NMT systems, but NMT systems overtake SMT once there is enough training data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9918964505195618}, {"text": "SMT", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.9635644555091858}]}, {"text": "Recent work has looked at the ability of neural systems to learn syntactic and morphological features.", "labels": [], "entities": []}, {"text": "Specifically, showed that recurrent neural networks are able to achieve high accuracy on tasks such as predicting morphological or part of speech tags and showed that RNNs follow similar patterns as humans with respect to sentences that are grammatical or ungrammatical in agreement structure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9941266775131226}, {"text": "predicting morphological or part of speech tags", "start_pos": 103, "end_pos": 150, "type": "TASK", "confidence": 0.8176036562238421}]}, {"text": "Additionally, specific RNN cells can be shown to have high correlation with features such as sentence length (, part of speech (, or whether or not the RNN has finished a relative clause (.", "labels": [], "entities": []}, {"text": "Another linguistic issue NMT systems have to deal with is translating words in the source language that might have multiple translations in the target language.", "labels": [], "entities": []}, {"text": "When these words don't differ orthographically, this task is known as word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.7454474369684855}]}, {"text": "Typically, humans can successfully translate these kinds of words by looking at the contexts in which they appear.", "labels": [], "entities": []}, {"text": "If NMT systems are able to successfully translate these words, it seems likely that they would have had to learn something about word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 129, "end_pos": 154, "type": "TASK", "confidence": 0.7197009722391764}]}, {"text": "There has been much research on improving machine translation performance by simultaneously improving word sense disambiguation ( for SMT systems, showing that adding word sense disambiguation to a baseline SMT system greatly improves translation performance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.822851836681366}, {"text": "word sense disambiguation", "start_pos": 102, "end_pos": 127, "type": "TASK", "confidence": 0.6915851930777231}, {"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9929250478744507}]}, {"text": "For NMT, recent work points out that NMT systems are not very reliable at translating rare word senses, but that disambiguation performance can be improved by using sense embeddings either as additional input to the encoder or to extract more structured lexical chains from the training data (, or by using context-aware embeddings (.", "labels": [], "entities": [{"text": "translating rare word senses", "start_pos": 74, "end_pos": 102, "type": "TASK", "confidence": 0.8847450762987137}]}, {"text": "To the best of our knowledge, no work has yet attempted to examine the hidden activations of an NMT system to see whether it is able to disambiguate word senses.", "labels": [], "entities": []}, {"text": "In this paper, we present means for evaluating the word sense disambiguation performance of NMT systems.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.6711280345916748}]}, {"text": "Specifically, we visualize the hidden activations of an NMT encoder to see whether it is able to disambiguate word senses at deeper layers.", "labels": [], "entities": []}, {"text": "We also present metrics that represent how well-disambiguated the senses are, with the hope that these metrics can be used to evaluate the word sense disambiguation performance of NMT systems in the future.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 139, "end_pos": 164, "type": "TASK", "confidence": 0.6165628035863241}]}], "datasetContent": [{"text": "We trained all of our NMT systems using the OpenNMT-py toolkit (, which trains an attentional encoder-decoder model with the attention from.", "labels": [], "entities": []}, {"text": "We tokenized, cleaned, and truecased our data using the standard tools from the Moses toolkit ().", "labels": [], "entities": []}, {"text": "We did not use byte-pair encoding in order to more easily do manual annotation of the data later.", "labels": [], "entities": []}, {"text": "We used the default parameters of the OpenNMT-py toolkit for training, with the exception of the number of encoder layers, which we varied from 1 to 4.", "labels": [], "entities": []}, {"text": "For the current study, we extensively analyzed WSD performance on sentences containing four possible ambiguous words: right, like, last, or case.", "labels": [], "entities": [{"text": "WSD", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9896517395973206}]}, {"text": "We manually annotated English sentences with their most appropriate sense (these were our \"gold\" sense labels), and fed the (un-annotated) sentences into our English-French NMT system.", "labels": [], "entities": []}, {"text": "After feeding in the source sentence, we extracted the hidden activations of the NMT encoder and labeled them with their corresponding \"gold\" sense.", "labels": [], "entities": []}, {"text": "We will refer to these hidden activations as the \"extracted embeddings,\" since they are thought to represent a kind of word-and-context embedding.", "labels": [], "entities": []}, {"text": "We performed principle component analysis (PCA) on all of the extracted embeddings and plotted the first two components, where we marked these points based on their \"gold\" sense label.", "labels": [], "entities": []}, {"text": "We then computed internal cluster evaluation scores for all of our embedding \"clusters.\"", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The SVM classifier accuracy at predicting sense from hidden activations.", "labels": [], "entities": [{"text": "SVM classifier", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.8869251906871796}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9567435383796692}, {"text": "predicting sense from hidden activations", "start_pos": 41, "end_pos": 81, "type": "TASK", "confidence": 0.8795663833618164}]}]}