{"title": [{"text": "ASR-based Features for Emotion Recognition: A Transfer Learning Approach", "labels": [], "entities": [{"text": "ASR-based", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9229089021682739}, {"text": "Emotion Recognition", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.8842463195323944}]}], "abstractContent": [{"text": "During the last decade, the applications of signal processing have drastically improved with deep learning.", "labels": [], "entities": [{"text": "signal processing", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7265021651983261}]}, {"text": "However areas of affecting computing such as emotional speech synthesis or emotion recognition from spoken language remains challenging.", "labels": [], "entities": [{"text": "emotional speech synthesis", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.6774580379327139}, {"text": "emotion recognition from spoken language", "start_pos": 75, "end_pos": 115, "type": "TASK", "confidence": 0.8431161522865296}]}, {"text": "In this paper, we investigate the use of a neural Automatic Speech Recognition (ASR) as a feature extractor for emotion recognition.", "labels": [], "entities": [{"text": "neural Automatic Speech Recognition (ASR)", "start_pos": 43, "end_pos": 84, "type": "TASK", "confidence": 0.7478241409574237}, {"text": "emotion recognition", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.7411096096038818}]}, {"text": "We show that these features outperform the eGeMAPS feature set to predict the valence and arousal emotional dimensions, which means that the audio-to-text mapping learned by the ASR system contains information related to the emotional dimensions in spontaneous speech.", "labels": [], "entities": []}, {"text": "We also examine the relationship between first layers (closer to speech) and last layers (closer to text) of the ASR and valence/arousal.", "labels": [], "entities": [{"text": "ASR", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.9502967000007629}]}], "introductionContent": [{"text": "With the advent of deep learning, areas of signal processing have been drasctically improved.", "labels": [], "entities": [{"text": "signal processing", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7177535593509674}]}, {"text": "In the field of speech synthesis, Wavenet (Van Den Oord et al., 2016), a deep neural network for generating raw audio waveforms, outperforms all previous approaches in terms of naturalness.", "labels": [], "entities": [{"text": "speech synthesis", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7522201240062714}]}, {"text": "One of the remaining challenges in speech synthesis is to control its emotional dimension (happiness, sadness, amusement, etc.).", "labels": [], "entities": [{"text": "speech synthesis", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7221401333808899}]}, {"text": "The work described here is part of a larger project to control as accurately as possible, the emotional state of a sentence being synthesized.", "labels": [], "entities": []}, {"text": "For this, we present here exploratory work regarding the analysis of the relationship between the emotional states and the modalities used to express them in speech.", "labels": [], "entities": []}, {"text": "Indeed one of the main problems to develop such a system is the amount of good quality data (naturalistic emotional speech of synthesis quality, i.e. containing no noise of any sorts).", "labels": [], "entities": []}, {"text": "This is why we are considering solutions such as synthesis by analysis and transfer learning.", "labels": [], "entities": []}, {"text": "Arousal and valence are among the most, if not the most used dimensions for quantizing emotions.", "labels": [], "entities": [{"text": "Arousal", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9729564785957336}]}, {"text": "Valence represents the positivity of the emotion whereas arousal represents its activation.", "labels": [], "entities": []}, {"text": "Since they represent emotional states, these dimensions are linked to several modalities that we use to express emotions (audio, text, facial expressions, etc.).", "labels": [], "entities": []}, {"text": "It has recently been shown that for emotion recognition, deep learning based systems learn features that outperform handcrafted features) (.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.897814929485321}]}, {"text": "The use of context and different modalities has also been studied with deep learning models.", "labels": [], "entities": []}, {"text": "focus on the contextual information among utterances in a video while develop specific architectures to fuse information coming from different modalities.", "labels": [], "entities": []}, {"text": "In this work, with the goal to study the relationship between valence/arousal, and different modalities, we propose to use the internal representation of a speech-to-text system.", "labels": [], "entities": []}, {"text": "An Automatic Speech Recognition (ASR) system or speech-to-text system, learns a mapping between two modalities: an audio speech signal and its corresponding transcription.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 3, "end_pos": 37, "type": "TASK", "confidence": 0.800608848532041}]}, {"text": "We hypothesize that such a system must also be learning representations of emotional expressions since these are contained intrinsically in both speech (variation or the pitch, the energy, etc.) and text (semantic of the words).", "labels": [], "entities": []}, {"text": "In fact, we show here that the activations of certain neurons in an ASR system, are useful to esti-mate the arousal and valence dimensions of an audio speech signal.", "labels": [], "entities": [{"text": "ASR", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.97776198387146}]}, {"text": "In other words, transfer learning is leveraged by using features learned for an automatic speech recognition (ASR) task to estimate valence and arousal.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.9689047932624817}, {"text": "automatic speech recognition (ASR) task", "start_pos": 80, "end_pos": 119, "type": "TASK", "confidence": 0.8153138331004551}]}, {"text": "The advantage of our method is that it allows combining the use of large datasets of speech with transcriptions with limited datasets annotated in emotional dimensions.", "labels": [], "entities": []}, {"text": "An example of transfer learning is the work of.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.967731773853302}]}, {"text": "They trained a multiplicative LTSM ( to predict next character based on the previous ones to design a text generator system.", "labels": [], "entities": []}, {"text": "The dataset used to train their model was the Amazon review dataset presented in.", "labels": [], "entities": [{"text": "Amazon review dataset", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.9465163548787435}]}, {"text": "Then, they used the representation learned by the model to predict sentiment also available in the dataset, and achieved state of the art prediction.", "labels": [], "entities": []}, {"text": "In this paper, we show that the activations of a deep learning-based ASR system trained on a large database can be used as features for the estimation of arousal and valence values.", "labels": [], "entities": [{"text": "ASR", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.8923006653785706}]}, {"text": "The features would therefore be extracted from both the audio and text modalities which the ASR system learned to map.", "labels": [], "entities": [{"text": "ASR", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9532332420349121}]}], "datasetContent": [{"text": "The \"interactive emotional dyadic motion capture database\" (IEMOCAP) dataset () is used in this paper.", "labels": [], "entities": [{"text": "interactive emotional dyadic motion capture database\" (IEMOCAP)", "start_pos": 5, "end_pos": 68, "type": "TASK", "confidence": 0.6706142336130142}]}, {"text": "It consists of audiovisual recordings of 5 sessions of dialogues between male and female subjects.", "labels": [], "entities": []}, {"text": "In total it contains 10 speakers and a total of 12 hours of data.", "labels": [], "entities": []}, {"text": "The data is segmented in utterances.", "labels": [], "entities": []}, {"text": "Each utterance is transcribed and annotated by category of emotions (Ekman, 1992) and a value for emotional dimensions (valence, arousal and dominance) between 1 and 5 representing the dimension's intensity.", "labels": [], "entities": []}, {"text": "In this work, we only use the audio and text modalities as well as the valence and arousal annotations.", "labels": [], "entities": []}, {"text": "In this section, we detail the experiments that we carried out.", "labels": [], "entities": []}, {"text": "The first one is the evaluation of the neural features in terms of MSE and its comparison with a linear regression of the eGeMAPS feature set ().", "labels": [], "entities": [{"text": "eGeMAPS feature set", "start_pos": 122, "end_pos": 141, "type": "DATASET", "confidence": 0.8708852728207906}]}, {"text": "In the second one, we investigate the relationship between the audio and text and modalities and the emotional dimensions.", "labels": [], "entities": []}, {"text": "In this first experiment, we investigate the performance of a linear regression to predict arousal and valence using the neural features.", "labels": [], "entities": []}, {"text": "We compare this with a linear regression using the eGeMAPS feature set.", "labels": [], "entities": [{"text": "eGeMAPS feature set", "start_pos": 51, "end_pos": 70, "type": "DATASET", "confidence": 0.955365518728892}]}, {"text": "The eGeMAPS feature set is a selection of acoustic features that provide a common baseline for evaluation in researches to avoid differences of feature set and implementations.", "labels": [], "entities": [{"text": "eGeMAPS feature set", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8958479166030884}]}, {"text": "Indeed, they also provide their implementation with openS-MILE toolkit) that we used in this work.", "labels": [], "entities": []}, {"text": "The features were selected based on their ability to represent affective physiological nuances in voice production, their proven performance in former research work as well as the possibility to extract them automatically, and their theoretical significance.", "labels": [], "entities": []}, {"text": "The result of this selection is a set of 18 Low-level descriptors (LLDs) related to frequency (pitch, formants etc.), energy (loudness, Harmonics-to-Noise Ratio, etc.) and spectral balance (spectral slopes, ratios between formant energies, etc.).", "labels": [], "entities": []}, {"text": "Then several functionals such as standard deviation and mean are applied to these LLDs to have the final features.", "labels": [], "entities": []}, {"text": "The results obtained from the linear regression in terms of MSE are compared to the annotations for each of the arousal and valence values (between 1 and 5) in: MSE on the prediction of valence and arousal.", "labels": [], "entities": []}, {"text": "We perform a leave-one-speaker-out evaluation scheme with both feature sets for cross-validation.", "labels": [], "entities": []}, {"text": "In other words, each validation set in constituted with the utterances corresponding to one speaker and the corresponding training set with the other speakers.", "labels": [], "entities": []}, {"text": "We train a model with each training set and evaluate it on the validation set in terms of MSE.", "labels": [], "entities": [{"text": "MSE", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.7783584594726562}]}, {"text": "The table contains the mean and standard deviation of the MSEs.", "labels": [], "entities": [{"text": "mean", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9620668292045593}, {"text": "standard", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8385269045829773}]}, {"text": "It is clear from this table that the neural features outperform the eGeMAPS in this experiment.", "labels": [], "entities": []}, {"text": "This confirms the fact that the ASR system learns representations of emotional dimensions in spontaneous speech.", "labels": [], "entities": [{"text": "ASR", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9874659180641174}]}, {"text": "During the data exploration, we noticed that, for some speakers, the layers closer to the speech input were more correlated to arousal and the ones closer to the text output to valence.", "labels": [], "entities": []}, {"text": "An example is shown in.", "labels": [], "entities": []}, {"text": "We present, in this section, preliminary studies regarding this matter.", "labels": [], "entities": []}, {"text": "In order to analyze this phenomenon as precisely as possible, we only considered the utterances from the IEMOCAP database for which the valence/arousal annotators were consistent with each other, leaving us with 7532 utterances in total instead of 10039.", "labels": [], "entities": [{"text": "IEMOCAP database", "start_pos": 105, "end_pos": 121, "type": "DATASET", "confidence": 0.8576290011405945}]}, {"text": "Then we performed linear regression with 4 different sets of feature to study their influence.", "labels": [], "entities": []}, {"text": "For the first set, we select the 100 best features among the 3 first layers of the neural ASR in terms of Fisher score using scikit-learn.", "labels": [], "entities": [{"text": "Fisher score", "start_pos": 106, "end_pos": 118, "type": "METRIC", "confidence": 0.9627489447593689}]}, {"text": "For the second set, we apply the same selection to the 3 last layers.", "labels": [], "entities": []}, {"text": "The third set selection is applied among all neural features.", "labels": [], "entities": []}, {"text": "The last set is the eGeMAPS feature set.", "labels": [], "entities": [{"text": "eGeMAPS feature set", "start_pos": 20, "end_pos": 39, "type": "DATASET", "confidence": 0.9597894350687662}]}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "As expected, the results show, that for the speakers considered, the layers closer to the audio modality outperform the ones closer to the text modality in the ASR architecture for arousal prediction and vice versa for the valence prediction.", "labels": [], "entities": [{"text": "arousal prediction", "start_pos": 181, "end_pos": 199, "type": "TASK", "confidence": 0.720407247543335}, {"text": "valence prediction", "start_pos": 223, "end_pos": 241, "type": "TASK", "confidence": 0.6947756707668304}]}, {"text": "On this we build a hypothesis that the arousal-related features learned are more related to the audio modality than the text and vice versa for the valence-related features.", "labels": [], "entities": []}, {"text": "This hypothesis will be further explored in future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: MSE on the prediction of valence and  arousal.", "labels": [], "entities": [{"text": "prediction of valence", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7752191623051962}]}, {"text": " Table 2: Means and variances of the MSE on the  prediction of valence and arousal.", "labels": [], "entities": []}]}