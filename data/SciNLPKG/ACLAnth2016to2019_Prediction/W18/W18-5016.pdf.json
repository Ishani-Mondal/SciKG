{"title": [{"text": "Language-Guided Adaptive Perception for Efficient Grounded Communication with Robotic Manipulators in Cluttered Environments", "labels": [], "entities": [{"text": "Language-Guided Adaptive Perception", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5769342382748922}]}], "abstractContent": [{"text": "The utility of collaborative manipulators for shared tasks is highly dependent on the speed and accuracy of communication between the human and the robot.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9970734119415283}]}, {"text": "The run-time of recently developed probabilis-tic inference models for situated symbol grounding of natural language instructions depends on the complexity of the representation of the environment in which they reason.", "labels": [], "entities": [{"text": "situated symbol grounding of natural language instructions", "start_pos": 71, "end_pos": 129, "type": "TASK", "confidence": 0.8057196055139814}]}, {"text": "As we move towards more complex bi-directional interactions, tasks, and environments, we need intelligent perception models that can selectively infer precise pose, semantics, and affordances of the objects when inferring exhaustively detailed world models is inefficient and prohibits real-time interaction with these robots.", "labels": [], "entities": []}, {"text": "In this paper we propose a model of language and perception for the problem of adapting the configuration of the robot perception pipeline for tasks where constructing exhaustively detailed models of the environment is inefficient and inconsequential for symbol grounding.", "labels": [], "entities": [{"text": "symbol grounding", "start_pos": 255, "end_pos": 271, "type": "TASK", "confidence": 0.7464930415153503}]}, {"text": "We present experimental results from a synthetic corpus of natural language instructions for robot manipulation in example environments.", "labels": [], "entities": []}, {"text": "The results demonstrate that by adapting perception we get significant gains in terms of run-time for perception and situated symbol grounding of the language instructions without a loss in the accuracy of the latter.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 194, "end_pos": 202, "type": "METRIC", "confidence": 0.998424768447876}]}], "introductionContent": [{"text": "Perception is a critical component of an intelligence architecture that converts raw sensor observations to a suitable representation for the task that the robot is to perform.", "labels": [], "entities": []}, {"text": "Models of environments vary significantly depending on the application.", "labels": [], "entities": []}, {"text": "For example, a robotic manipulator may need to model the objects in its environment with their six degree-of-freedom pose for grasping and dexterous manipulation tasks, whereas a self-driving car may need to model the dynamics of the environment in addition to domain-specific semantics such as stop signs, sidewalks and pedestrians etc.", "labels": [], "entities": []}, {"text": "to safely navigate through the environment.", "labels": [], "entities": []}, {"text": "The ability of robots to perform complex tasks is linked to the richness of the robot's world model.", "labels": [], "entities": []}, {"text": "As inferring exhaustively detailed world representations is impractical, it is common to infer representations which are highly specific to the task that the robot is to perform.", "labels": [], "entities": []}, {"text": "However, in collaborative domains as we move towards more complex bi-directional interactions, manipulation tasks, and the environments, it becomes unclear how to best represent the environment in order to facilitate planning and reasoning fora wide distribution of tasks.", "labels": [], "entities": []}, {"text": "As shown in the, modeling the affordance between the chips can and its lid would be unnecessary for the task of picking up the mustard sauce bottle and vice versa.", "labels": [], "entities": []}, {"text": "Inferring exhaustively detailed models of all of the objects in the environment is computationally expensive and inconsequential for the individual tasks, and inhibits real-time interaction with these collaborative robots.", "labels": [], "entities": []}, {"text": "The utility of collaborative manipulators is also highly dependent on the speed and accuracy of communication between the human operator and the robot.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.997511625289917}]}, {"text": "Natural language interfaces provide intuitive and muti-resolution means to interact with the robots in shared realms.", "labels": [], "entities": []}, {"text": "In this work, we propose learning a model of language and perception that can adapt the configurations of the perception pipeline according to the task in order to infer representations that are necessary and suffi-: On the left is an image showing the Baxter Research Robot in a cluttered tabletop environment in the context of collaborative human-robot tasks.", "labels": [], "entities": [{"text": "Baxter Research Robot", "start_pos": 253, "end_pos": 274, "type": "DATASET", "confidence": 0.9573193788528442}]}, {"text": "A perception system that does not use the context of the instruction when interpreting the observations would inefficiently construct detailed world model that is only partially utilized by the symbol grounding algorithm.", "labels": [], "entities": []}, {"text": "On the right are the adaptively inferred representations using our proposed language perception model for the instructions, \"pick up the leftmost blue gear\" and \"pick up the largest red object\" respectively.", "labels": [], "entities": []}, {"text": "cient to facilitate planning and grounding for the intended task.", "labels": [], "entities": []}, {"text": "e.g. the top-right image in the shows the adaptively inferred world model pertaining to the instruction \"pick up the leftmost blue gear\" which is different than the one inferred for the instruction \"pick up the largest red object\".", "labels": [], "entities": []}], "datasetContent": [{"text": "Herein with our experiments we demonstrate the utility of our language perception model for the task of grounded language understanding of the manipulation instructions.", "labels": [], "entities": []}, {"text": "As shown in the process involves two distinct inferences: Inferring the perceptual groundings given a language instruction ( eq.", "labels": [], "entities": []}, {"text": "8 ), and inferring high level motion planning constraints given the language and the generated world model ( eq.", "labels": [], "entities": []}, {"text": "In this section we describe our assumptions, and define the distinct symbolic representations used in our experiments for each of the above tasks.", "labels": [], "entities": []}, {"text": "We then discuss our instruction corpus and the details of the individual experiments.", "labels": [], "entities": []}, {"text": "We structure our experiments to validate two claims.", "labels": [], "entities": []}, {"text": "The first claim is that adaptively inferring the task optimal representations reduce the perception run-time by avoiding exhaustively detailed uniform modeling of the world.", "labels": [], "entities": []}, {"text": "The second claim is that reasoning in the context of these optimal representations also reduces the inference run-time of the symbol grounding model.", "labels": [], "entities": []}, {"text": "An outline of our experiments is illustrated in.", "labels": [], "entities": []}, {"text": "In the first experiment, we study the root-level inference accuracy of LPM ( groundings expressed at the root level of the phrase ) as a function of the gradual increase in the training fraction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.8922041654586792}]}, {"text": "For each increasing with a step of 0.1, we perform 15 validation experiments.", "labels": [], "entities": []}, {"text": "The training data is sampled randomly for every individual experiment.", "labels": [], "entities": []}, {"text": "Additionally, we perform a leave-one-out cross validation experiment.", "labels": [], "entities": [{"text": "cross validation", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.6389705389738083}]}, {"text": "We use the inferences generated by the leave-one-out cross validation experiments as inputs to drive the adaptive perception for each instruction.", "labels": [], "entities": []}, {"text": "In the second experiment, we compare the cumulative run-time of LPM inference ( eq.", "labels": [], "entities": []}, {"text": "8 ) and adaptive perception ( T 1 +T 2 ) against the run-time for complete perception ( T 4 ) -our baseline, for increasingly complex worlds.", "labels": [], "entities": []}, {"text": "In the third experiment, we compare the inference time of the symbol grounding model reasoning in the context of the adaptively generated optimal world models ( T 3 , eq.", "labels": [], "entities": []}, {"text": "6 ) against the inference time of the same model but when reasoning in the context of the complete world models ( T 5 , eq. 5 ).", "labels": [], "entities": []}, {"text": "We also check whether the planning constraints inferred in both cases match the ground truth or not.", "labels": [], "entities": []}, {"text": "Experiments are performed on a system running a 2.2 GHz Intel Core i7 CPU with 16 GB RAM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Per phrase symbol grounding run-time in  ms ( rounded to the nearest integer ) using adaptive  representations compared against the same when  using complete representations. Deviation mea- sures are 95% confidence interval values.", "labels": [], "entities": [{"text": "Per phrase symbol grounding", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.5861669480800629}, {"text": "mea- sures", "start_pos": 195, "end_pos": 205, "type": "METRIC", "confidence": 0.7127890586853027}]}, {"text": " Table 3: Impact of LPM on average percep- tion run-time per instruction (T P ), average symbol  grounding run-time per instruction (T SG ), and the  symbol grounding accuracy.", "labels": [], "entities": [{"text": "percep- tion run-time per instruction (T P )", "start_pos": 35, "end_pos": 79, "type": "METRIC", "confidence": 0.7819662094116211}, {"text": "T SG )", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.8385356664657593}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9534301161766052}]}]}