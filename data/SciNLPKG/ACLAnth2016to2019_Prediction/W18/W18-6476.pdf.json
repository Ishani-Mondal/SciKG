{"title": [{"text": "Webinterpret Submission to the WMT2018 Shared Task on Parallel Corpus Filtering", "labels": [], "entities": [{"text": "WMT2018 Shared Task", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.522821287314097}, {"text": "Parallel Corpus Filtering", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.5923693080743154}]}], "abstractContent": [{"text": "This paper describes the participation of We-binterpret in the shared task on parallel corpus filtering at the Third Conference on Machine Translation (WMT 2018).", "labels": [], "entities": [{"text": "parallel corpus filtering at the Third Conference on Machine Translation (WMT 2018)", "start_pos": 78, "end_pos": 161, "type": "TASK", "confidence": 0.804289909345763}]}, {"text": "The paper describes the main characteristics of our approach and discusses the results obtained on the data sets published for the shared task.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Participants in the shared task have to submit a file with quality scores, one per line, corresponding to the sentence pairs on the 1 billion word GermanEnglish Paracrawl corpus.", "labels": [], "entities": [{"text": "GermanEnglish Paracrawl corpus", "start_pos": 147, "end_pos": 177, "type": "DATASET", "confidence": 0.9537778894106547}]}, {"text": "Scores do not have to be meaningful, except that higher scores indicate better quality.", "labels": [], "entities": []}, {"text": "The performance of the submissions is evaluated by sub-sampling 10 million and 100 million word corpora based on these scores, training statistical ( and neural MT systems with these corpora, and assessing translation quality on six blind test sets 4 using the BLEU () score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 261, "end_pos": 265, "type": "METRIC", "confidence": 0.9979846477508545}]}, {"text": "displays the score of the best submission of each individual participant institution.", "labels": [], "entities": []}, {"text": "The top plot shows the results for the 10 million token sub-sampled corpus, and the bottom plot shows the results for the 100 million token corpus.", "labels": [], "entities": []}, {"text": "Scores are the aggregation of the BLEU scores of the statistical and neural systems averaged over the six blind test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9982433319091797}]}, {"text": "One first observation we can make is that (almost) all scores are quite close to each other with little variation between them; particularly in the 100 million condition.", "labels": [], "entities": []}, {"text": "Also, the scores for the statistical and neural systems tend to follow the same pattern.", "labels": [], "entities": []}, {"text": "We do not have confidence intervals available which makes difficult to interpret the observed differences between systems.", "labels": [], "entities": []}, {"text": "Still, in the case of 100 million tokens sub-sampling, it seems quite clear that all the systems except for the DCU and UTFPR submissions are of the same quality.", "labels": [], "entities": [{"text": "DCU", "start_pos": 112, "end_pos": 115, "type": "DATASET", "confidence": 0.9298834800720215}]}, {"text": "There is only a 5% relative improvement between the last system of this group and the best submission to the task.", "labels": [], "entities": []}, {"text": "Scores area bit more spread out in the 10 million tokens sub-sampling.", "labels": [], "entities": []}, {"text": "This indicates that 100 million sample neutralizes the differences between the data cleaning methods and allows (almost) all systems to reach a theoretical maximum.", "labels": [], "entities": []}, {"text": "Our submission (Webinterpret) scored 22.5 for statistical and 24.8 for neural MT systems on the 10 million tokens sub-sampling, in comparison to the corresponding scores of 24.5 and 28.6 achieved by the best submission.", "labels": [], "entities": []}, {"text": "For the 100 million condition, we scored 26.1 and 31.2, in comparison to the best system with the respective scores of 26.5 and 32.1.", "labels": [], "entities": []}], "tableCaptions": []}