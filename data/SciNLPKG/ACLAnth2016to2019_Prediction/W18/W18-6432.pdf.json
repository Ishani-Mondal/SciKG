{"title": [{"text": "EVALD Reference-Less Discourse Evaluation for WMT18", "labels": [], "entities": [{"text": "EVALD Reference-Less Discourse Evaluation", "start_pos": 0, "end_pos": 41, "type": "DATASET", "confidence": 0.880756676197052}, {"text": "WMT18", "start_pos": 46, "end_pos": 51, "type": "TASK", "confidence": 0.7827805280685425}]}], "abstractContent": [{"text": "We present the results of automatic evaluation of discourse in machine translation (MT) outputs using the EVALD tool.", "labels": [], "entities": [{"text": "evaluation of discourse in machine translation (MT) outputs", "start_pos": 36, "end_pos": 95, "type": "TASK", "confidence": 0.7357350617647171}]}, {"text": "EVALD was originally designed and trained to assess the quality of human writing, for native speakers and foreign-language learners.", "labels": [], "entities": [{"text": "EVALD", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7082201838493347}]}, {"text": "MT has seen a tremendous leap in translation quality at the level of sentences and it is thus interesting to see if the human-level evaluation is becoming relevant.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9357877373695374}, {"text": "translation", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9593706130981445}]}], "introductionContent": [{"text": "The output quality of machine translation has substantially improved in the last few years thanks to the neural models (NMT).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7718876600265503}]}, {"text": "In some setups, NMT systems may even surpass the quality of human reference translations if evaluated at the level of individual sentences.", "labels": [], "entities": []}, {"text": "The natural next step is (1) to start evaluating MT using larger pieces of texts, e.g. whole documents, and (2) to evaluate using methods suitable for the text quality produced by humans.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9537087678909302}]}, {"text": "Our contribution to the WMT18 test suites responds to both of these goals.", "labels": [], "entities": [{"text": "WMT18 test suites", "start_pos": 24, "end_pos": 41, "type": "DATASET", "confidence": 0.9241158366203308}]}, {"text": "We experiment with the application of automatic, reference-less evaluation of text quality which was originally designed to evaluate texts written by humans.", "labels": [], "entities": []}, {"text": "In this exploratory study, we do not have the human resources fora contrastive manual evaluation of the texts.", "labels": [], "entities": []}, {"text": "We thus limit the comparison to overall MT system quality as provided by WMT.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9869205951690674}, {"text": "WMT", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.9420672655105591}]}, {"text": "In Section 2, we briefly describe the tool we use, EVALD.", "labels": [], "entities": [{"text": "EVALD", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.5213936567306519}]}, {"text": "Section 3 describes the texts and MT system used.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9897938966751099}]}, {"text": "Section 4 provides and discusses the empirical results and we conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply EVALD to all the MT outputs and also to the source.", "labels": [], "entities": [{"text": "EVALD", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.9978142976760864}, {"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9333087801933289}]}, {"text": "No Czech reference is available for the texts, so we take the source as the lower bound:    EVALD, trained for Czech, should very much dislike the original English text.", "labels": [], "entities": [{"text": "EVALD", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.984140157699585}]}, {"text": "The overall EVALD score across the 107 texts produced by each MT system is listed in.", "labels": [], "entities": [{"text": "EVALD score", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.969491571187973}, {"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9571841955184937}]}, {"text": "Clearly, the L1 version of EVALD aimed at native speakers is non-discerning.", "labels": [], "entities": []}, {"text": "All systems get almost the same score.", "labels": [], "entities": []}, {"text": "It is actually the best possible score, but this tells us primarily that the system trained for L1 is not suitable for our setting.", "labels": [], "entities": []}, {"text": "Only the source gets the worst possible score.", "labels": [], "entities": []}, {"text": "The L2 version is more interesting.", "labels": [], "entities": []}, {"text": "As expected, English Source receives the worst rating, 1.0 with no variance at all.", "labels": [], "entities": [{"text": "English Source", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.9871078133583069}, {"text": "variance", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.963702380657196}]}, {"text": "MT systems score around 4 or 5.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.8665716648101807}]}, {"text": "While this is a clear overestimation of the text quality (6 would be the best score and e.g. phrase-based MT Moses gets 4.69), it reveals some differences between the systems.", "labels": [], "entities": []}, {"text": "We thus explore only EVALD L2 in the following.", "labels": [], "entities": [{"text": "EVALD L2", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.8090640008449554}]}, {"text": "the size of the sample in terms of individual scorings and distinct documents, respectively.", "labels": [], "entities": []}, {"text": "We see that all 56 translations of the 7 documents of Creative Writing seemed excellent.", "labels": [], "entities": [{"text": "translations of the 7 documents of Creative Writing", "start_pos": 19, "end_pos": 70, "type": "TASK", "confidence": 0.6320258341729641}]}, {"text": "Again, EVALD is non-discerning in this setting.", "labels": [], "entities": [{"text": "EVALD", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.5982088446617126}]}, {"text": "Other genres exhibit some divergence in scores.", "labels": [], "entities": []}, {"text": "Since all the genres differ from the news texts that the MT systems are geared towards, it is not easy to explain the stability of the score in Creative Writing.", "labels": [], "entities": [{"text": "Creative Writing", "start_pos": 144, "end_pos": 160, "type": "TASK", "confidence": 0.675683468580246}]}, {"text": "Possibly, EVALD is checking many shallow discourse features (e.g. the presence of a certain variety of conjunctions) and our texts in Creative Writing superficially include the required diversity, and this diversity is preserved by all MT systems.   of ranks, as humanities do (HIStory or the mentioned PHIlosophy).", "labels": [], "entities": [{"text": "EVALD", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.820038914680481}]}, {"text": "documents the effect of the mother tongue of the author of the original English text before the translation.", "labels": [], "entities": []}, {"text": "compares EVALD L2 scores in three experimental settings: using only the deep text features (marked discourse-specific in the table), shallow features (marked other) and all features.", "labels": [], "entities": []}, {"text": "Vertical tildes mark differences in rank in comparison with the rank given by the deep text features.", "labels": [], "entities": []}, {"text": "Agreement in five first ranks using the deep features and all features indicates that the full version of EVALD (i.e. using all features) really evaluates the translation systems based on the quality of the text coherence, rather than on the basis of shallow features.", "labels": [], "entities": [{"text": "EVALD", "start_pos": 106, "end_pos": 111, "type": "DATASET", "confidence": 0.788429856300354}]}, {"text": "summarizes the variance of EVALD scores according to individual aspects captured in the previously mentioned tables.", "labels": [], "entities": [{"text": "EVALD", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9099045991897583}]}, {"text": "The highest variance of the scores appeared in the aspect of nativeness of the text author.", "labels": [], "entities": []}, {"text": "The second most diverse results are across MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9841699600219727}]}, {"text": "The evaluation proposed here thus seems as a promising research direction, although a careful analysis of EVALD features and their adaptation will be needed to obtain more discerning evaluation.", "labels": [], "entities": []}, {"text": "Finally, the genre and domain of the original text also play a role but this is always to be expected.", "labels": [], "entities": []}, {"text": "See Section 2 for the list of features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Automatic results of WMT18 English-Czech systems as listed at http://matrix.statmt.org/  matrix/systems_list/1883.", "labels": [], "entities": [{"text": "WMT18", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.6269323229789734}]}, {"text": " Table 3: Overall EVALD scores for individual MT sys- tems. L1: EVALD for native speakers with 5 being the  best mark, L2: EVALD for non-natives with 6 being  the best possible mark.", "labels": [], "entities": [{"text": "EVALD", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.9874633550643921}, {"text": "MT sys- tems", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.8511766940355301}, {"text": "EVALD", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9882199764251709}, {"text": "EVALD", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.9586508274078369}]}, {"text": " Table 4: Results for individual genres.", "labels": [], "entities": []}, {"text": " Table 3.  Clearly, the L1 version of EVALD aimed at na- tive speakers is non-discerning. All systems get  almost the same score. It is actually the best possi- ble score, but this tells us primarily that the system  trained for L1 is not suitable for our setting. Only  the source gets the worst possible score.", "labels": [], "entities": [{"text": "possi- ble score", "start_pos": 154, "end_pos": 170, "type": "METRIC", "confidence": 0.6834159716963768}]}, {"text": " Table 5: Results for individual domains.", "labels": [], "entities": []}, {"text": " Table 6: Results depending on whether the author of  the English original was an English native speaker.", "labels": [], "entities": []}, {"text": " Table 7: Comparison of EVALD L2 scores using discourse-specific (deep) features, other (shallow) features, and  all features. Vertical tildes mark differences in rank in comparison with the rank given by the discourse-specific  features.", "labels": [], "entities": []}, {"text": " Table 8: Variance in EVALD L2 scores across various  aspects of our test suite.", "labels": [], "entities": [{"text": "Variance", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9642361998558044}, {"text": "EVALD L2 scores", "start_pos": 22, "end_pos": 37, "type": "METRIC", "confidence": 0.8446656465530396}]}]}