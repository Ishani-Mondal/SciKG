{"title": [{"text": "Grammar Induction with Neural Language Models: An Unusual Replication", "labels": [], "entities": [{"text": "Grammar Induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7572501301765442}]}], "abstractContent": [], "introductionContent": [{"text": "Grammar induction is the task of learning syntactic structure without the expert-labeled treebanks).", "labels": [], "entities": [{"text": "Grammar induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8256076276302338}]}, {"text": "Recent work on latent tree learning offers anew family of approaches to this problem by inducing syntactic structure using the supervision from a downstream NLP task (.", "labels": [], "entities": []}, {"text": "Ina recent paper published at ICLR, introduce such a model and report near state-ofthe-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing.", "labels": [], "entities": [{"text": "ICLR", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9028174877166748}, {"text": "language modeling", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.734484001994133}, {"text": "constituency parsing", "start_pos": 192, "end_pos": 212, "type": "TASK", "confidence": 0.8697104454040527}]}, {"text": "During the analysis of this model, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set.", "labels": [], "entities": []}, {"text": "Here, we analyze the model under different configurations to understand what it learns and to identify the conditions under which it succeeds.", "labels": [], "entities": []}, {"text": "We find that this model represents the first empirical success for neural network latent tree learning, and that neural language modeling warrants further study as a setting for grammar induction.", "labels": [], "entities": [{"text": "neural network latent tree learning", "start_pos": 67, "end_pos": 102, "type": "TASK", "confidence": 0.6637068510055542}, {"text": "neural language modeling", "start_pos": 113, "end_pos": 137, "type": "TASK", "confidence": 0.6618333260218302}, {"text": "grammar induction", "start_pos": 178, "end_pos": 195, "type": "TASK", "confidence": 0.8084022998809814}]}], "datasetContent": [{"text": "We analyze the Parsing-Reading-PredictNetwork (PRPN;, which uses convolutional networks with a form of structured attention () rather than recursive neural networks) to learn trees while performing straightforward backpropagation training on a language modeling objective.", "labels": [], "entities": []}, {"text": "The structure of the model seems rather suboptimal: Since the parser is trained as part of a language model, it makes parsing greedily, with no access to any words to the right of the point where each parsing decision must be made.", "labels": [], "entities": []}, {"text": "The experiments on language modeling and A crusade of NO to the consumption of drugs is imperative . Parses from PRPN-LM trained on AllNLI.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7782657444477081}, {"text": "PRPN-LM", "start_pos": 113, "end_pos": 120, "type": "DATASET", "confidence": 0.9451475739479065}, {"text": "AllNLI", "start_pos": 132, "end_pos": 138, "type": "DATASET", "confidence": 0.9855090975761414}]}, {"text": "parsing are carried out using different configurations of the model-PRPN-LM tuned for language modeling, and PRPN-UP for (unsupervised) parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9744974970817566}]}, {"text": "PRPN-LM is much larger than PRPN-UP, with embedding layer that is 4 times larger and the number of units per layer that is 3 times larger.", "labels": [], "entities": [{"text": "PRPN-LM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9466913938522339}, {"text": "PRPN-UP", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.8494294285774231}]}, {"text": "In the PRPN-UP experiments, we observe that the WSJ data is not split, such that the test data is used without parse information for training.", "labels": [], "entities": [{"text": "PRPN-UP", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.7439718842506409}, {"text": "WSJ data", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.8338761925697327}]}, {"text": "This implies that the parsing results of PRPN-UP may not be generalizable in the way usually expected of machine learning evaluation results.", "labels": [], "entities": [{"text": "PRPN-UP", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.8057174682617188}]}, {"text": "We train PRPN on sentences from two datasets: The full WSJ and AllNLI, the concatenation of SNLI () and MultiNLI ().", "labels": [], "entities": [{"text": "WSJ", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.8224651217460632}, {"text": "AllNLI", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.6293145418167114}]}, {"text": "We then evaluate the constituency trees produced by these models on the full WSJ, WSJ10 1 , and the MultiNLI development set.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.9647735953330994}, {"text": "WSJ10 1", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.7591531276702881}, {"text": "MultiNLI development set", "start_pos": 100, "end_pos": 124, "type": "DATASET", "confidence": 0.950114925702413}]}, {"text": "shows results with all the models understudy, plus several baselines, on WSJ and WSJ10.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.986493706703186}, {"text": "WSJ10", "start_pos": 81, "end_pos": 86, "type": "DATASET", "confidence": 0.9484787583351135}]}, {"text": "Unexpectedly, the PRPN-LM models achieve higher parsing performance than PRPN-UP.", "labels": [], "entities": [{"text": "PRPN-LM", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.8206466436386108}, {"text": "PRPN-UP", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9202840328216553}]}, {"text": "This shows that any tuning done to separate PRPN-UP from PRPN-LM was not necessary, and that the results described in the paper can be largely reproduced by a unified model in a fair setting.", "labels": [], "entities": []}, {"text": "Moreover, the PRPN models trained on the larger, out-of-domain AllNLI perform better than those trained on WSJ.", "labels": [], "entities": [{"text": "AllNLI", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.9689679145812988}, {"text": "WSJ", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.9641910791397095}]}, {"text": "Surprisingly, PRPN-LM tained on out-of-domain AllNLI achieves the best F1 score on full WSJ among all the models shows that both PRPN-UP models achieve F1 scores of 46.3 and 48.6 respectively on the MultiNLI dev set, setting the state of the art in parsing on this dataset among latent tree models.", "labels": [], "entities": [{"text": "AllNLI", "start_pos": 46, "end_pos": 52, "type": "DATASET", "confidence": 0.801182746887207}, {"text": "F1 score", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9832146465778351}, {"text": "F1", "start_pos": 152, "end_pos": 154, "type": "METRIC", "confidence": 0.9988119602203369}, {"text": "MultiNLI dev set", "start_pos": 199, "end_pos": 215, "type": "DATASET", "confidence": 0.9252453843752543}]}, {"text": "We conclude that PRPN does acquire some substantial knowledge of syntax, and that this knowledge agrees with Penn Treebank (PTB) grammar significantly better than chance.", "labels": [], "entities": [{"text": "Penn Treebank (PTB) grammar", "start_pos": 109, "end_pos": 136, "type": "DATASET", "confidence": 0.9669490456581116}]}], "tableCaptions": [{"text": " Table 2: Unlabeled parsing F1 on the MultiNLI  development set for models trained on AllNLI. F1  wrt. shows F1 with respect to strictly right-and  left-branching (LB/RB) trees and with respect to  the Stanford Parser (SP) trees supplied with the  corpus; The evaluations of SPINN, RL-SPINN,  and ST-Gumbel are from Williams et al. (2018a).  SPINN is a supervised parsing model, and the oth- ers are latent tree models.", "labels": [], "entities": [{"text": "MultiNLI  development set", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.9322342276573181}, {"text": "AllNLI", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.975948691368103}, {"text": "F1", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.8198502063751221}, {"text": "F1", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.9973178505897522}]}]}