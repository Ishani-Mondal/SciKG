{"title": [{"text": "Dynamic encoding of structural uncertainty in gradient symbols", "labels": [], "entities": [{"text": "Dynamic encoding", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8048194050788879}]}], "abstractContent": [{"text": "An important achievement in modeling online language comprehension is the discovery of the relationship between processing difficulty and surprisal (Hale, 2001; Levy, 2008).", "labels": [], "entities": []}, {"text": "However, it is not clear how structural uncertainty can be represented and updated in a continuous-time continuous-state dynamical system model, a reasonable abstraction of neural computation.", "labels": [], "entities": []}, {"text": "In this study, we investigate the Gradient Symbolic Computation (GSC) model (Smolensky et al., 2014) and show how it can dynamically encode and update structural uncertainty via the gradient activation of symbolic constituents.", "labels": [], "entities": []}, {"text": "We claim that surprisal is closely related to the amount of change in the optimal activation state driven by anew word input.", "labels": [], "entities": []}, {"text": "Ina simulation study, we demonstrate that the GSC model implementing a simple probabilistic symbolic grammar can simulate the effect of surprisal on processing time.", "labels": [], "entities": []}, {"text": "Our model provides a mechanistic account of the effect of surprisal, bridging between probabilistic symbolic models and subsymbolic connectionist models.", "labels": [], "entities": []}], "introductionContent": [{"text": "A core computational problem in online language comprehension is to deal with local ambiguity, the one-to-many mapping from a unit symbol wk (e.g., word) to symbol strings containing w at the k-th position W * k = \u00b7 \u00b7 \u00b7 wk \u00b7 \u00b7 \u00b7 and their interpretations S (e.g., sentences and their parses).", "labels": [], "entities": []}, {"text": "Rational models of sentence comprehension solve this problem by computing P (S|W k ), a conditional probability of interpretations given a partial string of symbols (henceforth, prefix) W k = w 1 \u00b7 \u00b7 \u00b7 wk , and updating it discretely for every new symbol input.", "labels": [], "entities": []}, {"text": "We will refer to this class of incremental processing models simply as (structural) probabilistic models.", "labels": [], "entities": []}, {"text": "The probabilistic model has drawn a lot of attention because it predicts processing difficulty in different regions of a sentence based on information-theoretic complexity metrics.", "labels": [], "entities": []}, {"text": "The surprisal hypothesis (e.g., claims that reading time of wk (as a measure of processing difficulty) is proportional to its surprisal, \u2212 log P (w k |W k\u22121 ), or equivalently, the Kullback-Leibler (KL) divergence of P (S|W k ) from P (S|W k\u22121 ) (.", "labels": [], "entities": []}, {"text": "This hypothesis has been supported in many psycholinguistic experiments (e.g.,.", "labels": [], "entities": []}, {"text": "In this study, our goal is to provide a neurallyplausible, mechanistic account of the relationship between surprisal and processing time.", "labels": [], "entities": []}, {"text": "For our purpose, we need a model from which both kinds of information, P (S|W k ) and processing times of wk , can be collected directly without relying on stipulated linking hypotheses.", "labels": [], "entities": []}, {"text": "Since the model is a dynamical system, processing time is directly modeled.", "labels": [], "entities": []}, {"text": "To model the probability P (S|W k ) relevant for rational analysis, we treat the model, primarily developed to study interpretation, as a generator: it is run to equilibrium with no input, producing a sentence parse as output.", "labels": [], "entities": [{"text": "rational analysis", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7221346497535706}]}, {"text": "This is done repeatedly as the dynamical system is stochastic; this gives a probability distribution over generated parses we call * P (S) : this we take to be the knowledge of sentence probabilities that is embodied in the model's dynamics.", "labels": [], "entities": []}, {"text": "Then for any W k , for rational analysis we compute * P (S|W k ) by conditioning * P (S) on W k , i.e., * P (S|W k ) is the proportion of all generated parses that have prefix equal to W k . We can then examine the extent to which the model, when serving as an incremental parser, behaves in accord with rational inference given its knowledge.", "labels": [], "entities": []}, {"text": "The Gradient Symbolic Computation (GSC) framework () serves our goal.", "labels": [], "entities": []}, {"text": "The GSC model is a continuoustime, continuous-state stochastic dynamical system model that computes the representation of a discrete structure gradually.", "labels": [], "entities": []}, {"text": "This framework grew out of the Integrated Connectionist/Symbolic cognitive architecture).", "labels": [], "entities": []}, {"text": "GSC aims to provide an integrated account of the contribution of the continuous dynamics of cognitive processing and the discrete competence that characterizes our knowledge of language.", "labels": [], "entities": [{"text": "GSC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8616572618484497}]}, {"text": "applied the framework to incremental processing problems focusing on transient dynamics during incremental processing and argued that the model can achieve two core computational goals in incremental processing: maintaining multiple context-appropriate and globallycoherent interpretations while rejecting interpretations that are context-inappropriate.", "labels": [], "entities": []}, {"text": "The GSC parser meets these challenges by moving, during the processing of a word, to an intermediate activation state (a blend state) in which multiple symbolic constituents are simultaneously activated to varying partial degrees.", "labels": [], "entities": []}, {"text": "From this state, the parser can reach all activation states representing context-appropriate and globally-coherent structures but does not move to activation states representing context-inappropriate structures (either grammatical or ungrammatical).", "labels": [], "entities": []}, {"text": "The relation between intermediate activation states and probability distributions over discrete parses was briefly discussed but was not investigated systematically.", "labels": [], "entities": []}, {"text": "In this study, we propose aversion of the GSC parser and show how it can be related to other probabilistic sentence-processing models.", "labels": [], "entities": [{"text": "GSC parser", "start_pos": 42, "end_pos": 52, "type": "TASK", "confidence": 0.6223931312561035}]}, {"text": "We argue that the parser's internal state -the activation values of multiple symbolic constituents along with control parameters of the parser -encodes a probability distribution over complete parses (Section 3).", "labels": [], "entities": []}, {"text": "After encountering new input, the parser incrementally changes its internal state to encode anew probability distribution.", "labels": [], "entities": []}, {"text": "The work the parser needs to do to shift this internal state is closely related to the KL divergence between the probability distributions, providing a link between processing time and surprisal (Section 4).", "labels": [], "entities": []}, {"text": "Ina simulation study (Section 5), we demonstrate that the GSC parser can approximate rational inference and report the correlation between processing time and surprisal in our model.", "labels": [], "entities": [{"text": "GSC parser", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.5719708204269409}]}, {"text": "In Section 6, we summarize our results and discuss some implications of our work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}