{"title": [{"text": "Empirical Evaluation of Character-Based Model on Neural Named-Entity Recognition in Indonesian Conversational Texts", "labels": [], "entities": [{"text": "Neural Named-Entity Recognition", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.67380823691686}]}], "abstractContent": [{"text": "Despite the long history of named-entity recognition (NER) task in the natural language processing community, previous work rarely studied the task on conversational texts.", "labels": [], "entities": [{"text": "named-entity recognition (NER) task", "start_pos": 28, "end_pos": 63, "type": "TASK", "confidence": 0.8673867185910543}]}, {"text": "Such texts are challenging because they contain a lot of word variations which increase the number of out-of-vocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "The high number of OOV words poses a difficulty for word-based neural models.", "labels": [], "entities": []}, {"text": "Meanwhile, there is plenty of evidence to the effectiveness of character-based neural models in mitigating this OOV problem.", "labels": [], "entities": [{"text": "OOV", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9214755892753601}]}, {"text": "We report an empirical evaluation of neural sequence labeling models with character embedding to tackle NER task in Indonesian conversational texts.", "labels": [], "entities": [{"text": "NER task", "start_pos": 104, "end_pos": 112, "type": "TASK", "confidence": 0.8972184360027313}]}, {"text": "Our experiments show that (1) character models outper-form word embedding-only models by up to 4 F 1 points, (2) character models perform better in OOV cases with an improvement of as high as 15 F 1 points, and (3) character models are robust against a very high OOV rate.", "labels": [], "entities": [{"text": "F 1", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9473665058612823}]}], "introductionContent": [{"text": "Critical to a conversational agent is the ability to recognize named entities.", "labels": [], "entities": []}, {"text": "For example, in a flight booking application, to book a ticket, the agent needs information about the passenger's name, origin, and destination.", "labels": [], "entities": []}, {"text": "While named-entity recognition (NER) task has a long-standing history in the natural language processing community, most of the studies have been focused on recognizing entities in well-formed data, such as news articles or biomedical texts.", "labels": [], "entities": [{"text": "named-entity recognition (NER) task", "start_pos": 6, "end_pos": 41, "type": "TASK", "confidence": 0.8782884925603867}]}, {"text": "Hence, little is known about the suitability of the available named-entity recognizers for conversational texts.", "labels": [], "entities": []}, {"text": "In this work, we tried to shed some light on this direction by evaluating neural sequence labeling models on NER task in Indonesian conversational texts.", "labels": [], "entities": [{"text": "NER task", "start_pos": 109, "end_pos": 117, "type": "TASK", "confidence": 0.8809004426002502}]}, {"text": "Unlike standard NLP corpora, conversational texts are typically noisy and informal.", "labels": [], "entities": []}, {"text": "For example, in Indonesian, the word aku (\"I\") can be written as: aq, akuw, akuh, q.", "labels": [], "entities": []}, {"text": "People also tend to use non-standard words to represent named entities.", "labels": [], "entities": []}, {"text": "This creative use of language results in numerous word variations which may increase the number out-of-vocabulary (OOV) words (.", "labels": [], "entities": []}, {"text": "The most common approach to handle the OOV problem is by representing each OOV word with a single vector representation (embedding).", "labels": [], "entities": [{"text": "OOV problem", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.8113803267478943}]}, {"text": "However, this treatment is not optimal because it ignores the fact that words can share similar morphemes which can be exploited to estimate the OOV word embedding better.", "labels": [], "entities": []}, {"text": "Meanwhile, word representation models based on subword units, such as characters or word segments, have been shown to perform well in many NLP tasks such as POS tagging, language modeling (, machine translation (, dependency parsing (, and sequence labeling (.", "labels": [], "entities": [{"text": "word representation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7439863383769989}, {"text": "POS tagging", "start_pos": 157, "end_pos": 168, "type": "TASK", "confidence": 0.8426945209503174}, {"text": "language modeling", "start_pos": 170, "end_pos": 187, "type": "TASK", "confidence": 0.7797596156597137}, {"text": "machine translation", "start_pos": 191, "end_pos": 210, "type": "TASK", "confidence": 0.8164175450801849}, {"text": "dependency parsing", "start_pos": 214, "end_pos": 232, "type": "TASK", "confidence": 0.754838228225708}, {"text": "sequence labeling", "start_pos": 240, "end_pos": 257, "type": "TASK", "confidence": 0.6720795184373856}]}, {"text": "These representations are effective because they can represent OOV words better by leveraging the orthographic similarity among words.", "labels": [], "entities": []}, {"text": "As for Indonesian NER, the earliest work was done by which relied on a rulebased approach.", "labels": [], "entities": [{"text": "Indonesian NER", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.5488768368959427}]}, {"text": "More recent research mainly used machine learning methods such as conditional random fields (CRF) ( and support vector machines).", "labels": [], "entities": []}, {"text": "The most commonly used datasets are news articles (), Wikipedia/DBPedia articles (, medical texts (, and Twitter data.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there has been no work that used neural networks for Indonesian NER nor NER for Indonesian conversational texts.", "labels": [], "entities": [{"text": "Indonesian NER", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.3970593810081482}]}, {"text": "In this paper, we report the ability of a neural network-based approach for Indonesian NER in conversational data.", "labels": [], "entities": [{"text": "Indonesian NER", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.5207284539937973}]}, {"text": "We employed the neural sequence labeling model of ( and experimented with two word representation models: word-level and character-level.", "labels": [], "entities": []}, {"text": "We evaluated all models on relatively large, manually annotated Indonesian conversational texts.", "labels": [], "entities": []}, {"text": "We aim to address the following questions: 1) How do the character models perform compared to word embedding-only models on NER in Indonesian conversational texts?", "labels": [], "entities": []}, {"text": "2) How much can we gain in terms of performance from using the character models on OOV cases?", "labels": [], "entities": []}, {"text": "3) How robust (in terms of performance) are the character models on different levels of OOV rates?", "labels": [], "entities": [{"text": "OOV", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.7302445769309998}]}, {"text": "Our experiments show that (1) the character models perform really well compared to word embedding-only with an improvement up to 4 F 1 points, (2) we can gain as high as 15 F 1 points on OOV cases by employing character models, and (3) the character models are highly robust against OOV rate as there is no noticeable performance degradation even when the OOV rate approaches 100%.", "labels": [], "entities": [{"text": "F 1", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.9675278067588806}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Number of entities in both datasets.", "labels": [], "entities": []}, {"text": " Table 4: Sentence length (L), number of sentences (N ),  and OOV rate (O) in each dataset. Sentence length is  measured by the number of words. OOV rate is the pro- portion of word types that do not occur in the training  split.", "labels": [], "entities": [{"text": "OOV rate (O)", "start_pos": 62, "end_pos": 74, "type": "METRIC", "confidence": 0.9801793932914734}, {"text": "OOV rate", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.8955984711647034}]}, {"text": " Table 5: F 1 scores on the test set of each dataset. The  scores are computed as in CoNLL evaluation. MEMO:  memorization baseline. CRF: CRF baseline. WORD,  CONCAT, ATTN: Rei et al.'s word embedding-only,  concatenation, and attention model respectively.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9765257835388184}, {"text": "MEMO", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9661405682563782}, {"text": "CONCAT", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.7608429193496704}, {"text": "ATTN", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.8461116552352905}]}, {"text": " Table 7: F 1 scores of word embedding-only (word) and concatenation (concat) model on the test set of SMALL- TALK (left) and TASK-ORIENTED (right) but only for entities containing at least one OOV word. Entries marked  with an asterisk (*) indicate that the model does not recognize any entity at all.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9858846068382263}, {"text": "SMALL- TALK", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.6194613178571066}, {"text": "TASK-ORIENTED", "start_pos": 126, "end_pos": 139, "type": "METRIC", "confidence": 0.9170908331871033}]}]}