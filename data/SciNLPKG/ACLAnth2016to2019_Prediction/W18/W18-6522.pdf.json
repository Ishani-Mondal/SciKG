{"title": [{"text": "Towards making NLG a voice for interpretable Machine Learning", "labels": [], "entities": [{"text": "interpretable Machine Learning", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.6879340906937917}]}], "abstractContent": [{"text": "This paper presents a study to understand the issues related to using NLG to hu-manise explanations from a popular in-terpretable machine learning framework called LIME.", "labels": [], "entities": []}, {"text": "Our study shows that self-reported rating of NLG explanation was higher than that fora non-NLG explanation.", "labels": [], "entities": []}, {"text": "However, when tested for comprehension , the results were not as clear-cut showing the need for performing more studies to uncover the factors responsible for high-quality NLG explanations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine Learning (ML) models are making ever greater numbers of decisions that affect user's lives.", "labels": [], "entities": [{"text": "Machine Learning (ML)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7510552048683167}]}, {"text": "Many of these models are not interpretable and cannot be readily understood by the average person.", "labels": [], "entities": []}, {"text": "This non-interpretability reduces user's acceptance of the models and their ability to make informed decisions, such as challenging an incorrect decision.", "labels": [], "entities": []}, {"text": "Recently interpretable ML has been becoming an increasingly important field in ML (.", "labels": [], "entities": [{"text": "interpretable ML", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7472981512546539}, {"text": "ML", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9895148277282715}]}, {"text": "In this paper, we describe a small explanation system, where a Deep Neural Network is used to make a decision in the area of credit.", "labels": [], "entities": []}, {"text": "Then an explanation of this decision is generated using the popular ML explanation framework LIME ().", "labels": [], "entities": [{"text": "ML explanation framework LIME", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.7252826020121574}]}, {"text": "The explanation generated by LIME is only a list of features and their importance to the decision.", "labels": [], "entities": []}, {"text": "The experiment in this paper compares people's understanding of this LIME explanation against an NLG interpretation of the same data, to test if NLG generates more interpretable explanations of ML decisions than uninterpreted output.", "labels": [], "entities": [{"text": "ML decisions", "start_pos": 194, "end_pos": 206, "type": "TASK", "confidence": 0.8849563002586365}]}], "datasetContent": [{"text": "An experiment was conducted to test if NLG or non-NLG explanations of algorithmic decision making are better at improving the understanding of their recipients.", "labels": [], "entities": []}, {"text": "Participants were shown either NLG or non-NLG explanations, and then asked how well they understood the decision, while also asking questions that test specific parts of their understanding of the decision.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Table of Questions & Results", "labels": [], "entities": []}]}