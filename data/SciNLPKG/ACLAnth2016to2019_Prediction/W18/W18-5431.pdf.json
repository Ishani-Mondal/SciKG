{"title": [{"text": "An Analysis of Encoder Representations in Transformer-Based Machine Translation", "labels": [], "entities": [{"text": "Transformer-Based Machine Translation", "start_pos": 42, "end_pos": 79, "type": "TASK", "confidence": 0.786316990852356}]}], "abstractContent": [{"text": "The attention mechanism is a successful technique in modern NLP, especially in tasks like machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7962554693222046}]}, {"text": "The recently proposed network architecture of the Transformer is based entirely on attention mechanisms and achieves new state of the art results in neu-ral machine translation, outperforming other sequence-to-sequence models.", "labels": [], "entities": [{"text": "neu-ral machine translation", "start_pos": 149, "end_pos": 176, "type": "TASK", "confidence": 0.5571097433567047}]}, {"text": "However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance.", "labels": [], "entities": []}, {"text": "To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality.", "labels": [], "entities": []}, {"text": "We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario.", "labels": [], "entities": []}, {"text": "Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations.", "labels": [], "entities": []}, {"text": "We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics .", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation (MT) is one of the prominent tasks in Natural Language Processing, tackled in several ways (.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9177684426307678}]}, {"text": "Neural MT (NMT) has become the de-facto standard with a performance that clearly outperforms the alternative approach of Statistical Machine Translation (.", "labels": [], "entities": [{"text": "Neural MT (NMT", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7389384359121323}, {"text": "Statistical Machine Translation", "start_pos": 121, "end_pos": 152, "type": "TASK", "confidence": 0.7766336997350057}]}, {"text": "NMT also improves training procedures due to the end-to-end fashion without tedious feature engineering and complex setups.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8903132677078247}]}, {"text": "During recent years, a lot of research has been done on NMT, designing new architectures, starting from the plain sequence-to-sequence model, to an improved version featuring an attention mechanism (), to models that only use attention instead of recurrent layers ( and models that apply convolution networks.", "labels": [], "entities": []}, {"text": "Among the different architectures, the Transformer () has emerged as the dominant NMT paradigm.", "labels": [], "entities": []}, {"text": "Relying only on attention mechanisms, the model is fast, highly accurate and has been proven to outperform the widely used recurrent networks with attention and ensembling () by more than 2 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 190, "end_pos": 194, "type": "METRIC", "confidence": 0.997867226600647}]}, {"text": "Improved translation quality is typically related to better representation of structural information.", "labels": [], "entities": []}, {"text": "While other approaches make use of external information to improve the internal representation of NMT models (, the Transformer seems to be able to encode a lot of structural information without explicitly incorporating any structural constraints.", "labels": [], "entities": []}, {"text": "However, being a rather new architecture, little is known about what the model exactly learns internally.", "labels": [], "entities": []}, {"text": "A better understanding of the internal representations of neural models has become a major challenge in NMT (.", "labels": [], "entities": []}, {"text": "In this work we investigate the kind of linguistic information that is learned by the encoder.", "labels": [], "entities": []}, {"text": "We start by training the Transformer system from English to seven languages, with different training set sizes, resulting in models that are not only trained for different target languages but also with expected differences in translation quality.", "labels": [], "entities": []}, {"text": "First, we visually inspect the attention weights of the encoders, in order to find linguistic patterns.", "labels": [], "entities": []}, {"text": "As the next step, we exploit the attention weights of the network to build a graph and induce tree structures for each sentence, showing whether syntactic dependencies between words have been learned or not in the spirit of and.", "labels": [], "entities": []}, {"text": "Additionally, following previous studies on how to analyze the internal representation of neural systems (), we probe the encoder weights of the trained models to address different sequence labeling tasks: Part-of-Speech tagging, Chunking, Named Entity Recognition and Semantic tagging.", "labels": [], "entities": [{"text": "Part-of-Speech tagging", "start_pos": 206, "end_pos": 228, "type": "TASK", "confidence": 0.7903439402580261}, {"text": "Named Entity Recognition", "start_pos": 240, "end_pos": 264, "type": "TASK", "confidence": 0.6369454761346182}, {"text": "Semantic tagging", "start_pos": 269, "end_pos": 285, "type": "TASK", "confidence": 0.7953469455242157}]}, {"text": "We evaluate the quality of the decoder on a given task to assess how discriminative the encoder representation is for that task.", "labels": [], "entities": []}, {"text": "Lastly, in order to check whether the learned information can be transferred across models, we use the encoder weights of a high-resource language pair to initialize a low-resource language pair, inspired by the work of.", "labels": [], "entities": []}, {"text": "We show that, also for the Transformer, the knowledge of an encoder representation can be shared with other models, helping them to achieve better translation quality.", "labels": [], "entities": []}, {"text": "Overall, our analysis leads to interesting insights about strengths and weaknesses of the attention weights of the Transformer, giving more empirical evidence about the kind of information the model is learning at each layer: \u2022 We find that each layer has at least one attention head that encodes a significant amount of syntactic dependencies.", "labels": [], "entities": []}, {"text": "\u2022 Consistent with previous findings on the sequence-to-sequence paradigm, probing the encoder to four different sequence labeling tasks reveals that lower layers tend to encode more syntactic information, whereas upper layers move towards semantic tasks.", "labels": [], "entities": []}, {"text": "\u2022 The information about the length of the input sentence starts to vanish after the third layer.", "labels": [], "entities": []}, {"text": "\u2022 The study corroborates that attention can be used to transfer knowledge between high-and low-resource languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "One of the most straightforward ways of understanding the weights of a neural network is by visualizing them.", "labels": [], "entities": []}, {"text": "In its base setting, the Transformer employs 6 layers with 8 different attention heads for each of them, making complete visualization difficult.", "labels": [], "entities": []}, {"text": "Therefore, we focus only on attention weights with high scores that are visually interpretable.", "labels": [], "entities": []}, {"text": "We discovered four different patterns shared across models: paying attention to the word itself, to the previous and next word and to the end of the sentence).", "labels": [], "entities": []}, {"text": "We found that, usually on the first layer, i.e., layer 0, more attention heads focus their weights on the word itself, while on the subsequent layers the network moves the attention more on other words, e.g., on the next and previous word, and to the end of the sentence.", "labels": [], "entities": []}, {"text": "This suggests that the transformer tries to find long dependencies between words on higher layers whereas it tends to focus on local dependencies in lower layers.", "labels": [], "entities": []}, {"text": "The architecture of the Transformer, linking each word with each other with an attention weight, can be seen as a weighted graph in which the words are the nodes and from which tree structure can be extracted.", "labels": [], "entities": []}, {"text": "Even though the models are not trained to produce any trees or to a specific syntax task, we used the attention weights in each layer to extract a tree of the input sentences and inspect whether they reflect a dependency tree.", "labels": [], "entities": []}, {"text": "We evaluated the induced trees on the English PUD treebank from the CoNLL 2017 Shared Task ().", "labels": [], "entities": [{"text": "English PUD treebank from the CoNLL 2017 Shared Task", "start_pos": 38, "end_pos": 90, "type": "DATASET", "confidence": 0.8560887045330472}]}, {"text": "The PUD treebank consists of 1000 sentences randomly taken from on-line newswire and Wikipedia.", "labels": [], "entities": [{"text": "PUD treebank", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8731450736522675}]}, {"text": "We measure the performance as Unlabeled Attachment Score (UAS) with the official evaluation script 8 from the shared task, using gold segmentation and tokenization.", "labels": [], "entities": [{"text": "Unlabeled Attachment Score (UAS)", "start_pos": 30, "end_pos": 62, "type": "METRIC", "confidence": 0.8209907760222753}]}, {"text": "Plus, given that our weights have no knowledge about the root of the sentence, we decided to use the gold root as starting node for the maximum spanning   tree algorithm.", "labels": [], "entities": []}, {"text": "Specifically, we run the Chu-LiuEdmonds algorithm for each attention head of each layer of the models to extract the maximum spanning trees.", "labels": [], "entities": []}, {"text": "shows the F1-score of the induced structures.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995583891868591}]}, {"text": "For comparison purposes, in this dataset, a state of the art supervised parser (Dozat et al., 2017) reaches 88.22 UAS F1-score and our random baseline, i.e., induced trees with random weights and gold root, achieves 10.1 UAS F1-score on average.", "labels": [], "entities": [{"text": "UAS", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.8496798872947693}, {"text": "F1-score", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.5357159972190857}, {"text": "UAS F1-score", "start_pos": 221, "end_pos": 233, "type": "METRIC", "confidence": 0.7636798322200775}]}, {"text": "9 Given our findings in Section 5, we also computed a leftand right-branching baseline (with golden root), obtaining 10.39 and 35.08 UAS F1-score respectively.", "labels": [], "entities": [{"text": "UAS", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9234235882759094}, {"text": "F1-score", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.49980464577674866}]}, {"text": "Although our models are not trained to produce trees, the best dependency trees induced on each layer are far better than the random baseline, suggesting that the models are learning some syntactic relationships.", "labels": [], "entities": []}, {"text": "However, the best scores do not achieve results much beyond the right branching baseline, showing that it is difficult to encode more complex and longer dependencies.", "labels": [], "entities": []}, {"text": "Overall, for all language pairs we notice the same performance trend across layers.", "labels": [], "entities": []}, {"text": "shows some examples of induced dependency trees.", "labels": [], "entities": []}, {"text": "Interestingly enough, we can see that the trees with higher scores follow the patterns found in Section 5, in which each word is linked to the next one, so encoding most compounds and multi-word expressions.", "labels": [], "entities": []}, {"text": "From visualizing other trees, even if they do not belong to the best attention head, we can see that they try to capture longer dependencies, as for dress and stuffy in the example in.", "labels": [], "entities": []}, {"text": "We evaluated the encoder representation through four different sequence labeling tasks: Part-ofSpeech (PoS) tagging, Chunking, Named Entity Recognition (NER) and Semantic tagging (SEM).", "labels": [], "entities": [{"text": "Part-ofSpeech (PoS) tagging", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.6101218461990356}, {"text": "Semantic tagging (SEM)", "start_pos": 162, "end_pos": 184, "type": "TASK", "confidence": 0.808259105682373}]}, {"text": "In this test bed we used the trained weights of the encoder, keeping them fixed, training only one de-   coder layer using one attention head and one feedforward layer.", "labels": [], "entities": []}, {"text": "We then assess the quality of the encoder representation across stacked layers.", "labels": [], "entities": []}, {"text": "We used a standard benchmark for each task: the Universal Dependencies English Web Treebank v2.0 (Zeman et al., 2017) for PoS tagging, the CoNLL2000 Chunking shared task), the CoNLL2003 NER shared task, and the annotated data from the Parallel Meaning Bank (PMB) for Semantic tagging (.", "labels": [], "entities": [{"text": "Universal Dependencies English Web Treebank v2.0", "start_pos": 48, "end_pos": 96, "type": "DATASET", "confidence": 0.7197995682557424}, {"text": "PoS tagging", "start_pos": 122, "end_pos": 133, "type": "TASK", "confidence": 0.8280872404575348}, {"text": "CoNLL2000 Chunking shared task", "start_pos": 139, "end_pos": 169, "type": "DATASET", "confidence": 0.881007581949234}, {"text": "CoNLL2003 NER shared task", "start_pos": 176, "end_pos": 201, "type": "DATASET", "confidence": 0.8312073051929474}, {"text": "Semantic tagging", "start_pos": 267, "end_pos": 283, "type": "TASK", "confidence": 0.8455980718135834}]}, {"text": "Each benchmark provides its own training, development and test data, except chunking in which we use 10% of the training corpus as validation, and the PMB in which we used the silver portion for training and the gold portion for test and dev (following the 80-20 split).", "labels": [], "entities": [{"text": "PMB", "start_pos": 151, "end_pos": 154, "type": "DATASET", "confidence": 0.8495956063270569}]}, {"text": "10 reports general statistics on each benchmark, regarding the granularity of each task, the number of training and testing instances, and the average length of the test sentences.", "labels": [], "entities": []}, {"text": "reports the performance for each task and stacked layers, together with the error rate for sentence length prediction.", "labels": [], "entities": [{"text": "error rate", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9595368504524231}, {"text": "sentence length prediction", "start_pos": 91, "end_pos": 117, "type": "TASK", "confidence": 0.7396756509939829}]}, {"text": "For each language pair, we can see that the syntax information, i.e., the PoS task, is encoded mostly in the first 3 layers, corroborating the results in Section 6, while moving towards more semantic tasks, as NER and SEM we can see that in general the decoder needs more encoder layers to achieve better results.", "labels": [], "entities": []}, {"text": "Another interesting finding is provided by the length mismatch between the output of the models and the gold labels.", "labels": [], "entities": [{"text": "length", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9668900966644287}]}, {"text": "Clearly the models encode the information about the sentence length in the first three layers, and then the information starts to vanish with an increase of the error rate.", "labels": [], "entities": []}, {"text": "The only exception is given by the SEM task, but as can be seen from the statistics in  average sentence length is very short and so it is easier to predict.", "labels": [], "entities": [{"text": "SEM task", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.8992829024791718}]}, {"text": "Overall, comparing the performance reached on these probing tasks with the BLEU score of each model, we can see again that the high resource language pairs achieve better results compared to our low resource language pair.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9996781349182129}]}, {"text": "Moreover, we notice that in general higher BLEU score correspond to higher probing results, confirming the trend that encoding linguistic proprieties within the encoder representation goon par with better translation quality ().", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9809289574623108}]}, {"text": "To assess whether the knowledge encoded in the attention units can help other models in a low resource scenario, we additionally carried out an evaluation of the encoder representation in a transfer learning task.", "labels": [], "entities": [{"text": "transfer learning task", "start_pos": 190, "end_pos": 212, "type": "TASK", "confidence": 0.8685175776481628}]}, {"text": "Similar to, we used the encoder weights from one high resource language, i.e., English-German, to train a Transformer system for our low resource language pair, English-Turkish.", "labels": [], "entities": []}, {"text": "We provide two experiments: i) initializing and fine tuning the encoder weights (TL1), ii) initializing and keeping the encoder weights fixed (TL2).", "labels": [], "entities": []}, {"text": "shows the BLEU scores of the systems evaluated with and without transferring the encoder parameters.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985058307647705}]}, {"text": "Both transfer learning settings are helpful to the decoder to reach a better translation quality, with almost 2 BLEU point more on the best scenario.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9994001388549805}]}, {"text": "Starting with a better encoder representation, taken from a high resource language pair, and then fine tuning the parameters on the low resource language achieves the best result, matching and corroborating previous findings on recurrent networks (Zoph et al., 2016).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of training instances used to train  each system.", "labels": [], "entities": []}, {"text": " Table 2: BLEU score for the newstest2017 and new- stest2018 test data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992415904998779}, {"text": "newstest2017", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.9602760076522827}, {"text": "new- stest2018 test data", "start_pos": 46, "end_pos": 70, "type": "DATASET", "confidence": 0.6928524851799012}]}, {"text": " Table 3: UAS F1-score of the induced trees produced by the attention weights on the English PUD treebank  from CoNLL 2017.", "labels": [], "entities": [{"text": "UAS F1-score", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.7069335579872131}, {"text": "English PUD treebank  from CoNLL 2017", "start_pos": 85, "end_pos": 122, "type": "DATASET", "confidence": 0.9035917023817698}]}, {"text": " Table 4: Results in terms of precision for each test set (\u2191, on the left side of each cell), together with the  error rate on the sentence length (\u2193, on the right side of each cell).", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9993863105773926}, {"text": "error rate", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.9755352139472961}]}, {"text": " Table 5: Statistics of the evaluation benchmarks  used for the probing task.", "labels": [], "entities": []}, {"text": " Table 6: BLEU score for the newstest2017 and new- stest2018 test data for the transfer learning experi- ment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994193315505981}, {"text": "newstest2017", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.935554027557373}]}]}