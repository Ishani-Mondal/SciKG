{"title": [], "abstractContent": [{"text": "Text normalization is the task of mapping non-canonical language, typical of speech transcription and computer-mediated communication, to a standardized writing.", "labels": [], "entities": [{"text": "Text normalization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7403955161571503}]}, {"text": "It is an upstream task necessary to enable the subsequent direct employment of standard natural language processing tools and indispensable for languages such as Swiss German, with strong regional variation and no written standard.", "labels": [], "entities": []}, {"text": "Text normalization has been addressed with a variety of methods, most successfully with character-level statistical machine translation (CSMT).", "labels": [], "entities": [{"text": "Text normalization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8371966779232025}, {"text": "character-level statistical machine translation (CSMT)", "start_pos": 88, "end_pos": 142, "type": "TASK", "confidence": 0.7042824753693172}]}, {"text": "In the meantime, machine translation has changed and the new methods, known as neural encoder-decoder (ED) models, resulted in remarkable improvements.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.8010485768318176}]}, {"text": "Text normalization, however, has not yet followed.", "labels": [], "entities": [{"text": "Text normalization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7015607059001923}]}, {"text": "A number of neural methods have been tried, but CSMT remains the state-of-the-art.", "labels": [], "entities": []}, {"text": "In this work, we normalize Swiss German WhatsApp messages using the ED framework.", "labels": [], "entities": [{"text": "normalize Swiss German WhatsApp messages", "start_pos": 17, "end_pos": 57, "type": "TASK", "confidence": 0.54742990732193}, {"text": "ED framework", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.8480803072452545}]}, {"text": "We exploit the flexibility of this framework, which allows us to learn from the same training data in different ways.", "labels": [], "entities": []}, {"text": "In particular, we modify the decoding stage of a plain ED model to include target-side language models operating at different levels of granularity: characters and words.", "labels": [], "entities": []}, {"text": "Our systematic comparison shows that our approach results in an improvement over the CSMT state-of-the-art.", "labels": [], "entities": []}], "introductionContent": [{"text": "Largely influenced by the work on English and other languages with a strong orthographic tradition (e.g. German, Spanish, French), the natural language processing (NLP) pipeline typically requires standardized text as input.", "labels": [], "entities": []}, {"text": "Recently, however, text processing has extended to non-standard varieties, including historical texts, transcribed spoken language and user-generated content (blogs, comments, social media posts, messaging).", "labels": [], "entities": [{"text": "text processing", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.8062896430492401}]}, {"text": "Modern NLP is also increasingly multilingual, starting to address languages that have no writing standard at all.", "labels": [], "entities": []}, {"text": "What is characteristic of non-standard text is a non-uniform way of writing the same word types (e.g. u instead of you in English).", "labels": [], "entities": []}, {"text": "While this might appear as a marginal stylistic variation in English, it is a substantial feature of less standardized varieties.", "labels": [], "entities": []}, {"text": "This is the case, for instance, with Swiss German.", "labels": [], "entities": []}, {"text": "The German-speaking part of Switzerland is characterized by a phenomenon known as diglossia, i.e. two different varieties of the same language are used within a community in different social situations.", "labels": [], "entities": []}, {"text": "One variety is known as standard Swiss German, that is the variety of standard German that is accepted as the norm in Switzerland.", "labels": [], "entities": [{"text": "Swiss German", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.8658179044723511}]}, {"text": "It is used inmost written contexts (literature, newspapers, private correspondence, official documents), informal and official spoken contexts (education, parliament speeches) and in interactions with foreigners.", "labels": [], "entities": []}, {"text": "The second variety, that is the dialect, is known as Swiss German and is used in everyday life, within the family as well as inmost radio and television programs.", "labels": [], "entities": [{"text": "Swiss German", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.9564523696899414}]}, {"text": "Since Swiss German does not have a standardized orthography, it is rarely used in written contexts.", "labels": [], "entities": []}, {"text": "However, nowadays we observe an increasing use of the dialect in written computer-mediated communication.", "labels": [], "entities": []}, {"text": "This phenomenon has multiple and interesting repercussions, as it makes valuable material available for NLP tasks, thus granting Swiss German a stronger position among the languages studied in the NLP community.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 104, "end_pos": 113, "type": "TASK", "confidence": 0.8507176041603088}]}, {"text": "However, given the high degree of variation, the need for text normalization, i.e. mapping different variants of the same word type to a single string, becomes immediately evident.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7315557450056076}]}, {"text": "The aim of this work is to normalize WhatsApp messages written in Swiss German.", "labels": [], "entities": [{"text": "normalize WhatsApp messages written in Swiss German", "start_pos": 27, "end_pos": 78, "type": "TASK", "confidence": 0.7952285749571664}]}, {"text": "Several factors contribute to the high degree of variation of the source text.", "labels": [], "entities": []}, {"text": "Firstly, the lack of a standardized spelling is further complicated by the strong regional variation and the numerous local variants of the same word.", "labels": [], "entities": []}, {"text": "As a result, the word viel ('much') can appear as viel, viil, vill, viu, and many other potential variations.", "labels": [], "entities": []}, {"text": "Secondly, CMC is characterized by various peculiarities, such as vowel reduplication and unconventional abbreviations, which increase variation.", "labels": [], "entities": []}, {"text": "A major breakthrough in performing text normalization was achieved when this task was approached as a case of character-level statistical machine translation (CSMT).", "labels": [], "entities": [{"text": "text normalization", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.8250551819801331}, {"text": "character-level statistical machine translation (CSMT)", "start_pos": 110, "end_pos": 164, "type": "TASK", "confidence": 0.7084766328334808}]}, {"text": "With a small modification of the input, so that the models are estimated over characters rather than over words, well-known off-the-shelf SMT tools like Moses ( could be used to obtain significant improvements in comparison to previous solutions.", "labels": [], "entities": [{"text": "SMT", "start_pos": 138, "end_pos": 141, "type": "TASK", "confidence": 0.9825776219367981}]}, {"text": "Currently widely used for text normalization, SMT is slowly abandoned in proper machine translation.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8452723920345306}, {"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9937115907669067}, {"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7774207293987274}]}, {"text": "New neural methods achieve much better performance, providing at the same time a more flexible framework for designing and testing different models.", "labels": [], "entities": []}, {"text": "They, however, require large training sets, which makes them unsuitable for text normalization, where training sets, unlike in machine translation, are small and created by experts specifically for the task.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.8279916048049927}, {"text": "machine translation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7569923996925354}]}, {"text": "Several attempts have been made to train neural normalization models, but the resulting systems could not reach the performance of CSMT.", "labels": [], "entities": [{"text": "neural normalization", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7044923305511475}]}, {"text": "In this paper, we tackle the issue of introducing neural methods to text normalization.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8254826366901398}]}, {"text": "We work with the neural framework that proved most successful in machine translation: a combination of two recurrent neural networks known as the encoder-decoder (ED) architecture.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8012965619564056}]}, {"text": "Inspired by similar approaches to other tasks (, we enrich the basic ED architecture with a mechanism that allows us to overcome the limitation of having a small training set.", "labels": [], "entities": []}, {"text": "This modification concerns including two kinds of language models at the decoding stage: word-level and character-level.", "labels": [], "entities": []}, {"text": "We compare our approach to a strong baseline and the current state-of-the-art CSMT methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess whether our approach provides an improvement over CSMT, we perform a systematic comparison, training and testing both systems on our datasets.", "labels": [], "entities": []}, {"text": "In this section we describe the details of the experiments.", "labels": [], "entities": []}, {"text": "We run the ED experiments using an extended version of the code from, which offers the possibility to integrate several LM predictors trained on different levels.", "labels": [], "entities": []}, {"text": "Ina character-level framework, where most alignment units consist of single words, evaluation metrics such as precision, recall and BLEU may provide information on the extent to which a unit normalized by the model, viewed as a sequence of characters, differs from its reference.", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9994762539863586}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9962134957313538}, {"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9984679818153381}]}, {"text": "They thus express the magnitude of the intra-word error.", "labels": [], "entities": []}, {"text": "However, we chose to simply assess whether a source sequence has been correctly normalized or not by the system.", "labels": [], "entities": []}, {"text": "For this reason, the accuracy score is used to evaluate the baseline and the various models implemented.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 21, "end_pos": 35, "type": "METRIC", "confidence": 0.9780167043209076}]}], "tableCaptions": []}