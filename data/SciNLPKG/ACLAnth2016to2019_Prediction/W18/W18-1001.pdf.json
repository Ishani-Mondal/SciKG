{"title": [{"text": "Towards Inference-Oriented Reading Comprehension: ParallelQA", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we investigate the tendency of end-to-end neural Machine Reading Comprehension (MRC) models to match shallow patterns rather than perform inference-oriented reasoning on RC benchmarks.", "labels": [], "entities": [{"text": "end-to-end neural Machine Reading Comprehension (MRC)", "start_pos": 46, "end_pos": 99, "type": "TASK", "confidence": 0.7130532488226891}]}, {"text": "We aim to test the ability of these systems to answer questions which focus on referential inference.", "labels": [], "entities": [{"text": "referential inference", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.8367770910263062}]}, {"text": "We propose ParallelQA, a strategy to formulate such questions using parallel passages.", "labels": [], "entities": []}, {"text": "We also demonstrate that existing neural models fail to generalize well to this setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reading Comprehension (RC) is the task of reading a body of text and answering questions about it.", "labels": [], "entities": [{"text": "Reading Comprehension (RC)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8452810049057007}]}, {"text": "It requires a deep understanding of the information presented in order to reason about entities, actions, events, and their interrelationships.", "labels": [], "entities": []}, {"text": "This necessitates language understanding skills as well as the cognitive ability to draw inferences.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.8171917796134949}]}, {"text": "Recent efforts in creating large-scale datasets have triggered a renewed interest in the RC task, with subsequent development of complex endto-end solutions featuring neural models.", "labels": [], "entities": [{"text": "RC task", "start_pos": 89, "end_pos": 96, "type": "TASK", "confidence": 0.9105561077594757}]}, {"text": "While these models do exceedingly well on the specific datasets they are developed for (some reaching or even surpassing human performance), they do not perform proportionally across datasets.", "labels": [], "entities": []}, {"text": "have shown that using a context or type matching heuristic to derive simple neural baseline architectures can achieve comparable results.", "labels": [], "entities": []}, {"text": "Our experiments also indicate that pattern matching can work well on these datasets.", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7965348660945892}]}, {"text": "Inference, an important RC skill, is the ability to understand the meaning of text without all the information being stated explicitly., Section A describes the types of inference that we may encounter while comprehending a passage along with the cues that * Equal Contribution help perform such reasoning.", "labels": [], "entities": []}, {"text": "Although state-ofthe-art deep learning models for machine reading are believed to have such reasoning capabilities, the limited ability of these models to generalize indicates certain shortcomings.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7353236973285675}]}, {"text": "We believe that it is important to develop benchmarks which give a realistic sense of a system's RC capabilities.", "labels": [], "entities": []}, {"text": "Thus, our goal in this paper is two-fold: Proof of Concept: We propose a method to create an RC dataset that assesses a model's ability to: \u2022 move beyond lexical pattern matching between the question and passage, \u2022 infer the correct answers to questions which contain referring expressions, and \u2022 generalize to different language styles.", "labels": [], "entities": []}, {"text": "Analysis of Existing Models: We test three end-to-end neural MRC models, which perform well on SQuAD (, on a few question-answer pairs generated using our methodology.", "labels": [], "entities": []}, {"text": "We demonstrate that it is indeed difficult for these systems to answer such questions, also indicating their tendencies to resort to shallow pattern matching and overfit to training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we focus on datasets with multiword spans as answers rather than cloze-style RC datasets like MCTest (, CNN / Daily Mail () and Children's Book Test.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 108, "end_pos": 114, "type": "DATASET", "confidence": 0.928490400314331}, {"text": "CNN / Daily Mail", "start_pos": 118, "end_pos": 134, "type": "DATASET", "confidence": 0.8759886622428894}, {"text": "Children's Book Test", "start_pos": 142, "end_pos": 162, "type": "DATASET", "confidence": 0.8785152584314346}]}, {"text": "The Stanford Question Answering Dataset (SQuAD) () was one of the first large scale RC datasets (over 100k QA pairs), where the answer to each question is a span in the given passage.", "labels": [], "entities": [{"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 4, "end_pos": 47, "type": "DATASET", "confidence": 0.8541425040790013}]}, {"text": "For its collection, different sets of crowd-workers were asked to formulate questions and answers using passages obtained from \u223c500 Wikipedia articles.", "labels": [], "entities": []}, {"text": "However, this resulted in the questions having similar word patterns to the sentences containing the answers.", "labels": [], "entities": []}, {"text": "We empirically demonstrate this in, where we ob-1 served that the sentence in the passage with the highest lexical similarity to the question contained the answer \u223c80% of the time.", "labels": [], "entities": []}, {"text": "Final answers tend to be short, with an average span length of around 3 tokens, and are largely entities (40.88%). and provide evidence for regular patterns in candidate answers that neural models can exploit.", "labels": [], "entities": []}, {"text": "We show in subsequent sections that models which perform well on SQuAD rely on lexical pattern matching, and are also not robust to variance in language style.: Sentence Retrieval Performance using Jaccard similarity, TF-IDF overlap and BM-25 overlap) scoring metrics", "labels": [], "entities": [{"text": "Sentence Retrieval", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.8985397517681122}, {"text": "TF-IDF overlap", "start_pos": 218, "end_pos": 232, "type": "METRIC", "confidence": 0.8841040432453156}, {"text": "BM-25 overlap) scoring metrics", "start_pos": 237, "end_pos": 267, "type": "METRIC", "confidence": 0.8092808127403259}]}], "tableCaptions": [{"text": " Table 1: Sentence Retrieval Performance using  Jaccard similarity", "labels": [], "entities": [{"text": "Sentence Retrieval", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9819455146789551}]}, {"text": " Table 3: Performance on SQuAD vs ParallelQA", "labels": [], "entities": [{"text": "ParallelQA", "start_pos": 34, "end_pos": 44, "type": "TASK", "confidence": 0.4092784821987152}]}]}