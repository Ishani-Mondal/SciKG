{"title": [{"text": "NLP at IEST 2018: BiLSTM-Attention and LSTM-Attention via Soft Voting in Emotion Classification", "labels": [], "entities": [{"text": "NLP at IEST 2018", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.7049120217561722}, {"text": "BiLSTM-Attention", "start_pos": 18, "end_pos": 34, "type": "METRIC", "confidence": 0.9806273579597473}]}], "abstractContent": [{"text": "This paper describes our method that competed at WASSA2018 Implicit Emotion Shared Task.", "labels": [], "entities": [{"text": "WASSA2018 Implicit Emotion Shared Task", "start_pos": 49, "end_pos": 87, "type": "TASK", "confidence": 0.7685938119888306}]}, {"text": "The goal of this task is to classify the emotions of excluded words in tweets into six different classes: sad, joy, disgust, surprise, anger and fear.", "labels": [], "entities": []}, {"text": "For this, we examine a BiL-STM architecture with attention mechanism (BiLSTM-Attention) and a LSTM architecture with attention mechanism (LSTM-Attention), and try different dropout rates based on these two models.", "labels": [], "entities": []}, {"text": "We then exploit an ensemble of these methods to give the final prediction which improves the model performance significantly compared with the baseline model.", "labels": [], "entities": []}, {"text": "The proposed method achieves 7 th position out of 30 teams and outperforms the baseline method by 12.5% in terms of macro F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.8855831027030945}]}], "introductionContent": [{"text": "Sentiment analysis is a hot and vital research area in the field of natural language processing.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9534332156181335}, {"text": "natural language processing", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6478389104207357}]}, {"text": "It aims at detecting the sentiment expressed in the context written by the authors.", "labels": [], "entities": []}, {"text": "Many advanced deep learning models have been exploited to address this issue in recent years.", "labels": [], "entities": []}, {"text": "The rise of social media, such as twitter and facebook, has fueled the interest of researchers in this field.", "labels": [], "entities": []}, {"text": "Twitter is one of the most popular and influential social media allover the world, which attracts over more than 300 million users with over 500 million tweets everyday . Therefore, it has received great attention in research communities as a data source due to its easy accessibility of data and diversity of the content.", "labels": [], "entities": []}, {"text": "In this shared task, given tweets are incomplete because that certain emotion words are removed from these tweets.", "labels": [], "entities": []}, {"text": "These words belong to one of 1 http://www.internetlivestats.com/twitter-statistics/ the following classes: sad, happy, disgusted, surprised, anger and afraid, or a synonym of one of them.", "labels": [], "entities": []}, {"text": "The goal of the task of WASSA2018 is to classify the emotion of the excluded words into one of the above-mentioned emotions according to the incomplete tweets.", "labels": [], "entities": [{"text": "WASSA2018", "start_pos": 24, "end_pos": 33, "type": "TASK", "confidence": 0.6223441362380981}]}, {"text": "All the data given by WASSA2018 are in English.", "labels": [], "entities": [{"text": "WASSA2018", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.6723231077194214}]}, {"text": "For this task, we put forward to two different models: one is LSTM-Attention which mainly consists of LSTM ( and attention mechanism (, the other is BiLSTM-Attention which mainly consists of BiLSTM and attention mechanism.", "labels": [], "entities": []}, {"text": "We have tried different dropout) rates to get different classification results on each model.", "labels": [], "entities": []}, {"text": "To further better the predictive performance, our final method employs an ensemble of these models, with a strategy called soft voting.", "labels": [], "entities": []}, {"text": "The remainder of the paper is structured as follows: we provide the detailed architecture of proposed methods in Section 2.", "labels": [], "entities": []}, {"text": "We present evaluation metrics and experimental results in Section 3.", "labels": [], "entities": []}, {"text": "And we conclude our works and point to the future works in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the classification performance, there are two available metrics: macro average and micro average.", "labels": [], "entities": []}, {"text": "In this task, we use macro average to measure the performance of proposed methods.", "labels": [], "entities": []}, {"text": "Macro average is the arithmetic mean of the performance metrics for each class, e.g. precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9992177486419678}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9948833584785461}]}, {"text": "Precision is the fraction of relevant instances among the retrieved instances, while recall is the fraction of relevant instances that have been retrieved over the total amount of relevant instances . More specifically, macro F1 score is utilized as a measurable indicator of classification performance.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9921363592147827}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9991666078567505}, {"text": "macro F1 score", "start_pos": 220, "end_pos": 234, "type": "METRIC", "confidence": 0.7062735656897227}]}, {"text": "The F1 score can be interpreted as a weighted average of the precision and recall.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9840982258319855}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9996180534362793}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9980089068412781}]}, {"text": "The relative contributions of precision and recall to the F1 score are equal.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9997568726539612}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9996193647384644}, {"text": "F1 score", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.987566739320755}]}, {"text": "The formula of F1 score can be defined as Eq.4:  Our system is implemented on Keras with a Tensorflow backend . For experiments, we use the datasets downloaded from WASSA2018, they mainly include three splits: 153,383 tweets in the training set, 9,591 tweets in the validation set and 28,757 tweets in the test set.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9825550317764282}, {"text": "Keras", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.9387006163597107}, {"text": "WASSA2018", "start_pos": 165, "end_pos": 174, "type": "DATASET", "confidence": 0.808262825012207}]}, {"text": "We train our model on the training set, and then tune the hyper parameters of models on the validation set.", "labels": [], "entities": []}, {"text": "For training, the mini-batch size is set at 128 and the max length of sentences (namely, the number of words in a tweet) is configured as 37 to ensure the same length of each tweet.", "labels": [], "entities": []}, {"text": "Namely, if the length of a tweet is less than 37, it will be padded with zero; otherwise, it will be truncated from the tail.", "labels": [], "entities": []}, {"text": "And the dropout rates that we have tried are ranged from 0.1 to 0.6 with a step of 0.1.", "labels": [], "entities": []}, {"text": "In our models, the categorical-crossentropy based loss function and the gradient descent algorithm with Adaptive Moment Estimation () are used to learn the model parameters of neural networks as well as the word vectors.", "labels": [], "entities": [{"text": "Adaptive Moment Estimation", "start_pos": 104, "end_pos": 130, "type": "METRIC", "confidence": 0.8225132425626119}]}, {"text": "The default parameters of Adaptive Moment Estimation is learning rate=0.001, beta 1=0.9, beta 2=0.999, eposilon=1e-08.", "labels": [], "entities": [{"text": "Adaptive Moment Estimation", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.8398487766583761}, {"text": "learning rate", "start_pos": 56, "end_pos": 69, "type": "METRIC", "confidence": 0.9516015350818634}, {"text": "eposilon", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9748224020004272}]}, {"text": "The experimental results on different emotion classes are shown in.", "labels": [], "entities": []}, {"text": "There are three evaluation metrics of each emotion class, namely precision, recall and F1 score.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9997302889823914}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9985547661781311}, {"text": "F1 score", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9906719326972961}]}, {"text": "Apparently, our system works best on the emotion class named joy in term of all the metrics.", "labels": [], "entities": []}, {"text": "And the F1 score on the class called anger is the lowest.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9885784685611725}]}, {"text": "The experimental results of different models in our system are shown in.", "labels": [], "entities": []}, {"text": "Obviously, all of our models outperform the baseline model dramatically.", "labels": [], "entities": []}, {"text": "More specifically, the BiLSTM-Attention model performs slightly better than the LSTMAttention model because BiLSTM can learn more features than LSTM.", "labels": [], "entities": []}, {"text": "To achieve better performance, we utilize a simple ensemble method called soft voting.", "labels": [], "entities": [{"text": "soft voting", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.7098923027515411}]}, {"text": "Briefly speaking, if a class gets the highest weighted sum of probabilities from various models, then it is the final class which our sample belongs to.", "labels": [], "entities": []}, {"text": "After ensembling the LSTM-Attention model and the BiLSTM-Attention model with different dropout rates, the macro F1 score reaches to 0.685.", "labels": [], "entities": [{"text": "BiLSTM-Attention", "start_pos": 50, "end_pos": 66, "type": "METRIC", "confidence": 0.8866525292396545}, {"text": "F1 score", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9131253957748413}]}, {"text": "These results demonstrate that the ensemble approach boosts the classification performance dramatically.", "labels": [], "entities": []}, {"text": "shows the predictive accuracy which is measured by F1 score as a function of epoch when dropout rate is 0.2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9466736912727356}, {"text": "F1 score", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9863441586494446}]}, {"text": "The accuracy of our model is optimal when the epoch is equal to 2, so we set the epoch to 2 in our final model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995007514953613}]}], "tableCaptions": [{"text": " Table 1: Experimental Results on Different Classes", "labels": [], "entities": []}, {"text": " Table 2: Experimental Results on Different Models", "labels": [], "entities": []}]}