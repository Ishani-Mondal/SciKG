{"title": [{"text": "Attaining the Unattainable? Reassessing Claims of Human Parity in Neural Machine Translation", "labels": [], "entities": [{"text": "Reassessing Claims of Human Parity in Neural Machine Translation", "start_pos": 28, "end_pos": 92, "type": "TASK", "confidence": 0.6833756268024445}]}], "abstractContent": [{"text": "We reassess a recent study (Hassan et al., 2018) that claimed that machine translation (MT) has reached human parity for the translation of news from Chinese into English, using pairwise ranking and considering three variables that were not taken into account in that previous study: the language in which the source side of the test set was originally written , the translation proficiency of the evalua-tors, and the provision of inter-sentential context.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.7998483180999756}, {"text": "translation of news from Chinese into English", "start_pos": 125, "end_pos": 170, "type": "TASK", "confidence": 0.8633896367890495}]}, {"text": "If we consider only original source text (i.e. not translated from another language, or translationese), then we find evidence showing that human parity has not been achieved.", "labels": [], "entities": []}, {"text": "We compare the judgments of professional translators against those of non-experts and discover that those of the experts result in higher inter-annotator agreement and better discrimination between human and machine translations.", "labels": [], "entities": []}, {"text": "In addition, we analyse the human translations of the test set and identify important translation issues.", "labels": [], "entities": []}, {"text": "Finally, based on these findings , we provide a set of recommendations for future human evaluations of MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 103, "end_pos": 105, "type": "TASK", "confidence": 0.992167055606842}]}], "introductionContent": [{"text": "Neural machine translation (NMT) has revolutionised the field of MT by overcoming many of the weaknesses of the previous state-of-the-art phrase-based machine translation (PBSMT)).", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7813133547703425}, {"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9941510558128357}, {"text": "phrase-based machine translation (PBSMT))", "start_pos": 138, "end_pos": 179, "type": "TASK", "confidence": 0.7932407259941101}]}, {"text": "In only a few years since the first working models, this approach has led to a substantial improvement in translation quality, reported in terms of automatic metrics.", "labels": [], "entities": [{"text": "translation", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.9681838154792786}]}, {"text": "This has ignited higher levels of expectation, fuelled in part by hyperbolic claims from large MT developers.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9388443231582642}]}, {"text": "First we saw in that Google NMT was \"bridging the gap between human and machine translation\".", "labels": [], "entities": [{"text": "human and machine translation", "start_pos": 62, "end_pos": 91, "type": "TASK", "confidence": 0.7140001356601715}]}, {"text": "This was amplified recently by the claim by that Microsoft had \"achieved human parity\" in terms of translation quality on news translation from Chinese to English, and more recently still by SDL who claimed to have \"cracked\" Russian-toEnglish NMT with \"near perfect\" translation quality.", "labels": [], "entities": []}, {"text": "However, when human evaluation is used to compare NMT and SMT, the results do not always favour NMT (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9804577231407166}]}, {"text": "Accompanying the claims regarding the capability of the Microsoft Chinese-to-English NMT system, released their experimental data which permits replicability of their experiments.", "labels": [], "entities": [{"text": "Microsoft Chinese-to-English NMT system", "start_pos": 56, "end_pos": 95, "type": "DATASET", "confidence": 0.7696877121925354}]}, {"text": "In this paper, we provide a detailed examination of Microsoft's claim to have reached human parity for the task of translating news from Chinese (ZH) to English (EN).", "labels": [], "entities": [{"text": "translating news from Chinese (ZH) to English (EN)", "start_pos": 115, "end_pos": 165, "type": "TASK", "confidence": 0.8734135776758194}]}, {"text": "They provide two definitions in this regard, namely: Definition 1.", "labels": [], "entities": [{"text": "Definition", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.732893168926239}]}, {"text": "If a bilingual human judges the quality of a candidate translation produced by a human to be equivalent to one produced by a machine, then the machine has achieved human parity.", "labels": [], "entities": []}, {"text": "If there is no statistically significant difference between human quality scores fora test set of candidate translations from a machine translation system and the scores for the corresponding human translations then the machine has achieved human parity.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "First, we identify and discuss three potential issues in Microsoft's human evaluation, concerning (i) the language in which the source text was originally written, (ii) the competence of the human evaluators with respect to translation, and (iii) the linguistic context available to these evaluators (Section 2).", "labels": [], "entities": []}, {"text": "We then conduct anew modified evaluation of their MT system on the same dataset taking these issues onboard (Section 3).", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9782053828239441}]}, {"text": "In so doing, we reassess whether human parity has indeed been achieved following what we consider to be a fairer evaluation setting.", "labels": [], "entities": [{"text": "parity", "start_pos": 39, "end_pos": 45, "type": "TASK", "confidence": 0.6472789645195007}]}, {"text": "We then take a closer look at the quality of Microsoft's dataset with the help of an English native speaker and a Chinese native speaker, and discover a number of problems in this regard (Section 4).", "labels": [], "entities": [{"text": "Microsoft's dataset", "start_pos": 45, "end_pos": 64, "type": "DATASET", "confidence": 0.909626324971517}]}, {"text": "Finally, we conclude the paper (Section 5) with a set of recommendations for future human evaluations, together with some remarks on the risks for the whole field of overhyping the capability of the systems we build.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct a human evaluation in which at the same time evaluators are shown a source ZH sentence and three EN translations thereof: (i) the human translation produced by Microsoft (file C in: henceforth referred to as HT), (ii) the output of Microsoft's MT system (file D: henceforth MS), and the output of a production system, Google Translate (henceforth GG).", "labels": [], "entities": []}, {"text": "We take these three translations from the data provided by.", "labels": [], "entities": []}, {"text": "Instead of giving evaluators randomly selected sentences, they see them in order.", "labels": [], "entities": []}, {"text": "We randomised the documents in the test set (169) and prepared one evaluation task per document, for the first 49 documents (503 sentences).", "labels": [], "entities": []}, {"text": "Of these 49 documents, 41 were originally written in ZH (amounting to 299 sentences, with each document containing 7.3 sentences on average) and the remaining 8 were originally written in EN (204 sentences, average of 25.5 sentences per document).", "labels": [], "entities": []}, {"text": "Evaluators were asked to annotate all the sentences of each document in one go, so that they can take intersentential context into account.", "labels": [], "entities": []}, {"text": "Rather than direct assessment (DA) (), as in, we conduct a relative ranking evaluation.", "labels": [], "entities": [{"text": "direct assessment (DA)", "start_pos": 12, "end_pos": 34, "type": "METRIC", "confidence": 0.5956957280635834}]}, {"text": "While DA has some advantages over ranking and has replaced the latter at the WMT shared task since 2017 (, ranking is more appropriate for our evaluation due to the fact that we evaluate sentences in consecutive order (rather than randomly).", "labels": [], "entities": [{"text": "DA", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.9470037221908569}, {"text": "WMT shared task", "start_pos": 77, "end_pos": 92, "type": "DATASET", "confidence": 0.6772235830624899}]}, {"text": "This can be accommodated in ranking as we can show all three translations for each source sentence together with the previous and next source sentences at the same time.", "labels": [], "entities": []}, {"text": "In contrast, in DA only one translation is shown at a time, which is of course evaluated in isolation.", "labels": [], "entities": [{"text": "DA", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9213289618492126}]}, {"text": "An important advantage of DA is that the number of annotations required grows linearly (rather than exponentially with ranking) with the number of translations to be evaluated; this is relevant for WMT's shared task as there maybe many MT systems to be evaluated, but not for our research as we have only three translations (HT, MS and GG).", "labels": [], "entities": []}, {"text": "In any case, both approaches have been found to lead to very similar outcomes as their results correlate very strongly (R \u2265 0.92 in).", "labels": [], "entities": [{"text": "R", "start_pos": 120, "end_pos": 121, "type": "METRIC", "confidence": 0.9897789359092712}]}, {"text": "Our human evaluation is performed with the Appraise tool (Federmann, 2012).", "labels": [], "entities": [{"text": "Appraise", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9305991530418396}]}, {"text": "6 shows a snapshot of the evaluation.", "labels": [], "entities": []}, {"text": "Subsequently, we derive an overall score for each translation (HT, MS and GG) based on the rankings.", "labels": [], "entities": [{"text": "HT", "start_pos": 63, "end_pos": 65, "type": "DATASET", "confidence": 0.7122978568077087}, {"text": "GG", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.5899776816368103}]}, {"text": "To this end we use the TrueSkill method adapted to MT evaluation () following its usage at WMT15, 7 i.e. we run 1,000 iterations of the rankings recorded with Appraise followed by clustering (significance level \u03b1 = 0.05).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9805666208267212}, {"text": "WMT15, 7", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.9587831099828085}, {"text": "Appraise", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9962987303733826}, {"text": "significance level \u03b1", "start_pos": 192, "end_pos": 212, "type": "METRIC", "confidence": 0.8632823427518209}]}, {"text": "Five evaluators took part in our evaluation: two professional Chinese-to-English translators and three non-experts.", "labels": [], "entities": []}, {"text": "Of the two professional translators, one is a native English speaker with a fluent level of Chinese, and the other is a Chinese native speaker with a fluent level of English.", "labels": [], "entities": []}, {"text": "The three non-expert bilingual participants are Chinese native speakers with an advanced level of English.", "labels": [], "entities": []}, {"text": "These bilingual participants are researchers in NLP, and so their profile is similar to some of the human evaluators of WMT, namely MT researchers.", "labels": [], "entities": []}, {"text": "All evaluators completed all 49 documents, except the third non-expert, who completed the first 18.", "labels": [], "entities": []}, {"text": "Similarly, all evaluators ranked all the sentences in the documents they evaluated, except the second professional translator, who skipped 3 sentences.", "labels": [], "entities": []}, {"text": "In total we collected 6,675 pairwise judgements.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Ranks of the translations given the original  language of the source side of the test set shown with  their Trueskill score (the higher the better). An aster- isk next to a translation indicates that this translation is  significantly better than the one in the next rank.", "labels": [], "entities": [{"text": "Trueskill score", "start_pos": 118, "end_pos": 133, "type": "METRIC", "confidence": 0.7709171772003174}]}, {"text": " Table 2: Ranks and Trueskill scores (the higher the  better) of the three translations for evaluations carried  out by expert versus non-expert translators. An aster- isk next to a translation indicates that this translation is  significantly better than the one in the next rank.", "labels": [], "entities": [{"text": "Trueskill", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9607827067375183}]}]}