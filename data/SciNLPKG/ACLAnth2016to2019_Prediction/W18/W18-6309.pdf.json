{"title": [{"text": "A neural interlingua for multilingual machine translation", "labels": [], "entities": [{"text": "multilingual machine translation", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.6559951404730479}]}], "abstractContent": [{"text": "We incorporate an explicit neural interlin-gua into a multilingual encoder-decoder neural machine translation (NMT) architecture.", "labels": [], "entities": [{"text": "multilingual encoder-decoder neural machine translation (NMT)", "start_pos": 54, "end_pos": 115, "type": "TASK", "confidence": 0.7724598497152328}]}, {"text": "We demonstrate that our model learns a language-independent representation by performing direct zero-shot translation (without using pivot translation), and by using the source sentence embeddings to create an English Yelp review classifier that, through the mediation of the neural interlingua, can also classify French and German reviews.", "labels": [], "entities": []}, {"text": "Furthermore, we show that, despite using a smaller number of parameters than a pairwise collection of bilingual NMT models, our approach produces comparable BLEU scores for each language pair in WMT15.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 157, "end_pos": 161, "type": "METRIC", "confidence": 0.9992552399635315}, {"text": "WMT15", "start_pos": 195, "end_pos": 200, "type": "DATASET", "confidence": 0.8648742437362671}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Hyperparameters for the multilingual and  bilingual encoder-decoder models.", "labels": [], "entities": []}, {"text": " Table 1. Our multilingual model uses 1  bidirectional LSTM layer in the encoder for each  input language, 1 attentional LSTM layer for the  interlingua and 1 attentional LSTM layer in the  decoder for each output language. The baseline  bilingual models use 2 bidirectional LSTM layers  in the encoder and 1 attentional LSTM layer in the", "labels": [], "entities": []}, {"text": " Table 2: Comparison of BLEU scores across language  pairs in newstest2015 and newsdiscuss2015. We show  the results for the bilingual baseline NMT models and  our multilingual NMT model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9965632557868958}]}, {"text": " Table 5: Zero-shot BLEU scores on the UN Parallel Corpus on selected language pairs. The universal encoder- decoder, pivot and direct NMT results were retrieved from Miura et al. (2017). Our proposed model outperforms  the universal encoder-decoder model (Johnson et al., 2017) on the zero-shot translation task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.984671413898468}, {"text": "UN Parallel Corpus", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.9204720457394918}, {"text": "zero-shot translation task", "start_pos": 286, "end_pos": 312, "type": "TASK", "confidence": 0.7432555158933004}]}]}