{"title": [{"text": "Comparing CNN and LSTM character-level embeddings in BiLSTM-CRF models for chemical and disease named entity recognition", "labels": [], "entities": [{"text": "chemical and disease named entity recognition", "start_pos": 75, "end_pos": 120, "type": "TASK", "confidence": 0.6535822699467341}]}], "abstractContent": [{"text": "We compare the use of LSTM-based and CNN-based character-level word embeddings in BiLSTM-CRF models to approach chemical and disease named entity recognition (NER) tasks.", "labels": [], "entities": [{"text": "chemical and disease named entity recognition (NER) tasks", "start_pos": 112, "end_pos": 169, "type": "TASK", "confidence": 0.7638556689023972}]}, {"text": "Empirical results over the BioCreative V CDR corpus show that the use of either type of character-level word em-beddings in conjunction with the BiLSTM-CRF models leads to comparable state-of-the-art performance.", "labels": [], "entities": [{"text": "BioCreative V CDR corpus", "start_pos": 27, "end_pos": 51, "type": "DATASET", "confidence": 0.8769471794366837}]}, {"text": "However, the models using CNN-based character-level word embeddings have a computational performance advantage, increasing training time over word-based models by 25% while the LSTM-based character-level word embeddings more than double the required training time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bi-directional Long-Short Term Memory Conditional Random Field models (BiLSTM-CRF), in which a BiLSTM is coupled with a CRF layer to connect output tags, have been shown to achieve state-of-art performance in sequence tagging tasks including part of speech (POS) tagging, chunking, and NER (.", "labels": [], "entities": [{"text": "part of speech (POS) tagging", "start_pos": 242, "end_pos": 270, "type": "TASK", "confidence": 0.6875018562589373}]}, {"text": "The combination of word embeddings and character-level word embeddings has been explored in this context, with Ma and Hovy (2016) using Convolutional Neural Networks (CNNs) to construct character-level word embeddings and applying LSTM networks.", "labels": [], "entities": []}, {"text": "This work showed that the use of character-level word embeddings improves the performance of the models, by contributing the ability to recognize unseen words.", "labels": [], "entities": []}, {"text": "Biomedical Named Entity Recognition (BNER) is a vital initial step for information extraction tasks in the biomedical domain, including the Chemical-Disease Relationship (CDR) extraction task where both chemical and disease entities must be identified (.", "labels": [], "entities": [{"text": "Biomedical Named Entity Recognition (BNER)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7227362436907632}, {"text": "information extraction", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.7575385570526123}, {"text": "Chemical-Disease Relationship (CDR) extraction task", "start_pos": 140, "end_pos": 191, "type": "TASK", "confidence": 0.6845223009586334}]}, {"text": "Character-level word embeddings could be particularly significant in this context, given that new entity names are frequently created, and may follow consistent patterns including productive morphology such as common prefixes (e.g., di-) or suffixes (e.g., -ase).", "labels": [], "entities": []}, {"text": "Features that capture word-internal characteristics have been shown to be effective for BNER tasks in CRF models (.", "labels": [], "entities": [{"text": "BNER tasks", "start_pos": 88, "end_pos": 98, "type": "TASK", "confidence": 0.713636189699173}]}, {"text": "applied a BiLSTM-CRF model with LSTM-based character-level word embeddings to a gene and protein NER task, demonstrating state-of-art performance that outperformed traditional feature-based models.", "labels": [], "entities": []}, {"text": "further improved on this result on a chemical NER task by adding an attention layer between the BiL-STM and CRF layers (Att-BiLSTM-CRF).", "labels": [], "entities": [{"text": "NER task", "start_pos": 46, "end_pos": 54, "type": "TASK", "confidence": 0.8847758769989014}]}, {"text": "In an experiment by, optimal hyper-parameters for LSTM networks in sequence tagging tasks were explored, with the finding that incorporation of characterlevel word embeddings significantly improved performance on NER tasks on general datasets including.", "labels": [], "entities": [{"text": "sequence tagging tasks", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.7667972842852274}, {"text": "NER tasks", "start_pos": 213, "end_pos": 222, "type": "TASK", "confidence": 0.8970076739788055}]}, {"text": "However, the choice of CNN-based ( or LSTM-based character-level word embeddings ( did not affect the performance significantly.", "labels": [], "entities": []}, {"text": "Since the CNN has fewer parameters to train than BiLSTM network, it is better in terms of training efficiency, and was recommended as the preferred approach.", "labels": [], "entities": []}, {"text": "In this paper, we implement and compare models with each type of word embedding to generate empirical results for the tasks of chemical and disease NER, using the BioCreative V CDR corpus (.", "labels": [], "entities": [{"text": "chemical and disease NER", "start_pos": 127, "end_pos": 151, "type": "TASK", "confidence": 0.597997859120369}, {"text": "BioCreative V CDR corpus", "start_pos": 163, "end_pos": 187, "type": "DATASET", "confidence": 0.8307413458824158}]}, {"text": "These BNER categories are the most searched entities in the biomedical literature, and hence particularly important to study.", "labels": [], "entities": [{"text": "BNER", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.5105723738670349}]}, {"text": "The results show that models with CNN-based character-level word embeddings achieve stateof-the-art results comparable to LSTM-based character-level word embeddings, while having the advantage of reduced training complexity, demonstrating that the prior results also hold for the BNER task.", "labels": [], "entities": [{"text": "BNER task", "start_pos": 280, "end_pos": 289, "type": "TASK", "confidence": 0.5908509492874146}]}], "datasetContent": [{"text": "This section presents our empirical approach to comparing state-of-the-art neural network models for chemical and disease NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.7554417252540588}]}, {"text": "In our experiments, we use the BioCreative V CDR corpus (.", "labels": [], "entities": [{"text": "BioCreative V CDR corpus", "start_pos": 31, "end_pos": 55, "type": "DATASET", "confidence": 0.8258089274168015}]}, {"text": "This corpus provides a set of 1000 manually-annotated abstracts (9193 sentences) for training and development, and another set of 500 manually-annotated abstracts (4840 sentences) for test.", "labels": [], "entities": []}, {"text": "In particular, we used a pre-processed version of the CDR corpus from, 1 which provides predicted POS-, chunking-and gazetteer-based tags: \u2022 POS and chunking tags are predicted by the GENIA tagger ().", "labels": [], "entities": [{"text": "CDR corpus", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.8284949958324432}, {"text": "GENIA tagger", "start_pos": 184, "end_pos": 196, "type": "DATASET", "confidence": 0.8399054110050201}]}, {"text": "2 \u2022 Gazetteer tags are encoded in BIO tagging scheme based on matching to the external Jochem chemical dictionary (.", "labels": [], "entities": [{"text": "BIO tagging", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.5598292499780655}, {"text": "Jochem chemical dictionary", "start_pos": 87, "end_pos": 113, "type": "DATASET", "confidence": 0.9167503515879313}]}, {"text": "Following, we randomly sample 10% from the set of 1000 abstracts for development, and use the remaining for training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Fixed hyper-parameter configurations.", "labels": [], "entities": []}, {"text": " Table 2: Hyper-parameters for learning character- level word embedding. \"charEmbedSize\" and \"# of  Params.\" denote the vector size of character embed- dings and the total number of parameters, respectively.", "labels": [], "entities": []}, {"text": " Table 3: Results (in %) on the test set.", "labels": [], "entities": []}, {"text": " Table 4: Training time of best performing models (2  BiLSTM layers and 250 LSTM units), computed on a  Intel Core i5 2.9 GHz PC.", "labels": [], "entities": []}]}