{"title": [], "abstractContent": [{"text": "This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2018.", "labels": [], "entities": [{"text": "Machine Translation (WMT)", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.8525985717773438}]}, {"text": "Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7517852485179901}]}, {"text": "The main metric for this task is human judgment of translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.770783007144928}]}, {"text": "This year, we also opened up the task to additional test sets to probe specific aspects of translation .", "labels": [], "entities": [{"text": "translation", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.9631919264793396}]}], "introductionContent": [{"text": "The Third Conference on Machine Translation (WMT) held at EMNLP 2018 1 host a number of shared tasks on various aspects of machine translation.", "labels": [], "entities": [{"text": "Machine Translation (WMT)", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.8495516538619995}, {"text": "EMNLP 2018 1", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.8740301728248596}, {"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7875504493713379}]}, {"text": "This conference builds on twelve previous editions of WMT as workshops and conferences (.", "labels": [], "entities": []}, {"text": "This year we conducted several official tasks.", "labels": [], "entities": []}, {"text": "We report in this paper on the news translation task.", "labels": [], "entities": [{"text": "news translation task", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.7651205758253733}]}, {"text": "Additional shared tasks are described in separate papers in these proceedings: \u2022 biomedical translation (), \u2022 multimodal machine translation (), \u2022 metrics (), \u2022 quality estimation (), \u2022 automatic post-editing (, and \u2022 parallel corpus filtering ().", "labels": [], "entities": [{"text": "biomedical translation", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.769324541091919}, {"text": "multimodal machine translation", "start_pos": 110, "end_pos": 140, "type": "TASK", "confidence": 0.609354188044866}, {"text": "parallel corpus filtering", "start_pos": 218, "end_pos": 243, "type": "TASK", "confidence": 0.6074409981568655}]}, {"text": "In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (\"constrained\" condition).", "labels": [], "entities": [{"text": "news translation task", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.7607171436150869}]}, {"text": "We held 14 translation tasks this year, between English and each of Chinese, Czech, Estonian, German, Finnish, Russian, and Turkish.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.8842044174671173}]}, {"text": "The EstonianEnglish language pair was new this year.", "labels": [], "entities": [{"text": "EstonianEnglish language pair", "start_pos": 4, "end_pos": 33, "type": "DATASET", "confidence": 0.8569132685661316}]}, {"text": "Similarly to Latvian, which we had covered in 2017, Estonian is a lesser resourced data condition on a challenging language pair.", "labels": [], "entities": []}, {"text": "System outputs for each task were evaluated both automatically and manually.", "labels": [], "entities": []}, {"text": "This year the news translation task had two additional sub-tracks: multilingual MT and unsupervised MT.", "labels": [], "entities": [{"text": "news translation task", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.7829704384009043}]}, {"text": "Both sub-tracks were included into the general list of news translation submissions and are described in more detail in corresponding subsections of Section 2.", "labels": [], "entities": [{"text": "news translation submissions", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.7339165012041727}]}, {"text": "The human evaluation (Section 3) involves asking human judges to score sentences output by anonymized systems.", "labels": [], "entities": []}, {"text": "We obtained large numbers of assessments from researchers who contributed evaluations proportional to the number of tasks they entered.", "labels": [], "entities": []}, {"text": "In addition, we used Mechanical Turk to collect further evaluations.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.8443259596824646}]}, {"text": "This year, the official manual evaluation metric is again based on judgments of adequacy on a 100-point scale, a method we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems.", "labels": [], "entities": []}, {"text": "The primary objectives of WMT are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation and estimation methodologies for machine translation.", "labels": [], "entities": [{"text": "WMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8719261288642883}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8308209776878357}, {"text": "machine translation", "start_pos": 242, "end_pos": 261, "type": "TASK", "confidence": 0.8069436550140381}]}, {"text": "As before, all of the data, translations, and collected human judgments are publicly available 2 . We hope these datasets serve as a valuable resource for research into datadriven machine translation, automatic evaluation, or prediction of translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.6950381398200989}, {"text": "prediction of translation", "start_pos": 226, "end_pos": 251, "type": "TASK", "confidence": 0.8045369386672974}]}, {"text": "News transla-tions are also available for interactive visualization and comparison of differences between systems at http://wmt.ufal.cz/ using MT-ComparEval (.", "labels": [], "entities": [{"text": "MT-ComparEval", "start_pos": 143, "end_pos": 156, "type": "DATASET", "confidence": 0.8283823728561401}]}, {"text": "In order to gain further insight into the performance of individual MT systems, we organized a call for dedicated \"test suites\", each focussing on some particular aspect of translation quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9765219688415527}]}, {"text": "A brief overview of the test suites is provided in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "A human evaluation campaign is run each year to assess translation quality and to determine the final ranking of systems taking part in the competition.", "labels": [], "entities": [{"text": "translation", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9729273915290833}]}, {"text": "This section describes how preparation of evaluation data, collection of human assessments, and computation of the official results of the shared task was carried out this year.", "labels": [], "entities": []}, {"text": "Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (, and two years ago the evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with relative ranking (RR) and DA.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7468165457248688}, {"text": "relative ranking (RR)", "start_pos": 275, "end_pos": 296, "type": "METRIC", "confidence": 0.8638506531715393}, {"text": "DA", "start_pos": 301, "end_pos": 303, "type": "METRIC", "confidence": 0.9442159533500671}]}, {"text": "DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carryout evaluations through quality controlled crowd-sourcing.", "labels": [], "entities": [{"text": "RR", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9150974750518799}]}, {"text": "As established in 2016 (), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for English-to-Russian (the only pair evaluated outof-English).", "labels": [], "entities": [{"text": "DA", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.5948041677474976}, {"text": "RR", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9380583167076111}, {"text": "Pearson correlation", "start_pos": 125, "end_pos": 144, "type": "METRIC", "confidence": 0.9547845721244812}]}, {"text": "Last year, we thus employed DA for evaluation of systems taking part in the news task and do so again this year.", "labels": [], "entities": [{"text": "DA", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.6709372997283936}]}, {"text": "Where possible,  we collect DA judgments via the crowd-sourcing platform, Amazon's Mechanical Turk, and as in previous year's we ask participating teams to provide manual evaluation of system outputs via Appraise.", "labels": [], "entities": [{"text": "DA judgments", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.8176858127117157}, {"text": "Amazon's Mechanical Turk", "start_pos": 74, "end_pos": 98, "type": "DATASET", "confidence": 0.8612124174833298}, {"text": "Appraise", "start_pos": 204, "end_pos": 212, "type": "DATASET", "confidence": 0.6195138096809387}]}, {"text": "Researcher involvement was needed particularly for translations into Czech, German, Estonian, Finnish and Turkish.", "labels": [], "entities": []}, {"text": "Human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation (i.e. no bilingual speakers are needed) on an analogue scale, which corresponds to an underlying absolute 0-100 rating scale.", "labels": [], "entities": [{"text": "analogue scale", "start_pos": 186, "end_pos": 200, "type": "METRIC", "confidence": 0.9479961097240448}]}, {"text": "Since DA involves evaluation of a single translation per screen, this allows the sentence length restriction usually applied during manual evaluation to be removed for both researchers and crowd-sourced workers.", "labels": [], "entities": [{"text": "DA", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.8923178911209106}]}, {"text": "Figure 3 The maximum sentence length with RR was 30 in shows one DA screen as completed by researchers on Appraise, while provides a screenshot of DA shown to crowd-sourced workers on Amazon's Mechanical Turk.", "labels": [], "entities": [{"text": "RR", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9883071780204773}, {"text": "Appraise", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.8888129591941833}, {"text": "Amazon's Mechanical Turk", "start_pos": 184, "end_pos": 208, "type": "DATASET", "confidence": 0.696234941482544}]}, {"text": "The annotation is organized into \"HITs\" (following the Mechanical Turk's term \"human intelligence task\"), each containing 100 such screens and requiring about half an hour to finish.", "labels": [], "entities": []}, {"text": "Appraise users were allowed to pause their annotation at anytime, Amazon interface did not allow any pauses.", "labels": [], "entities": [{"text": "Amazon interface", "start_pos": 66, "end_pos": 82, "type": "DATASET", "confidence": 0.9056078493595123}]}, {"text": "More details of composition of HITs are given in Section 3.3 below.", "labels": [], "entities": []}, {"text": "In terms of the News translation task manual evaluation, a total of 584 individual researcher ac-WMT16.", "labels": [], "entities": [{"text": "News translation task manual", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.7234232127666473}]}, {"text": "counts were involved, and 915 turker accounts.", "labels": [], "entities": []}, {"text": "Researchers in the manual evaluation came from 33 different research groups and contributed judgments of 118,705 translations, while 225,900 translation assessment scores were submitted in total by the crowd.", "labels": [], "entities": []}, {"text": "Under ordinary circumstances, each assessed translation would correspond to a single individual scored segment.", "labels": [], "entities": []}, {"text": "However, since distinct systems can produce the same output fora particular input sentence, we are often able to take advantage of this and use a single assessment for multiple systems.", "labels": [], "entities": []}, {"text": "Similar to last year's evaluation, we only combine human assessments in this way if the string of text belonging to multiple systems is exactly identical.", "labels": [], "entities": []}, {"text": "For example, even small differences in punctuation disqualify combination of similar system outputs, and this is due to a general lack of evidence about what kinds of minor differences mayor may not impact human evaluation.", "labels": [], "entities": []}, {"text": "shows the numbers of segments for which distinct MT systems participating in the News Translation Task produced identical outputs.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9405623078346252}, {"text": "News Translation Task", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.779823511838913}]}, {"text": "The biggest saving in terms of exact duplicate translations, being produced by multiple systems, was for German to English, where a 17.4% saving of resources by combining identical outputs before human evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Amount of data collected in the WMT18 manual evaluation campaign (assessments after removal of quality control", "labels": [], "entities": [{"text": "Amount", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8785162568092346}, {"text": "WMT18 manual evaluation campaign", "start_pos": 42, "end_pos": 74, "type": "DATASET", "confidence": 0.7083509862422943}]}, {"text": " Table 7: Number of unique workers, (A) those whose scores for bad reference items were significantly lower than correspond-", "labels": [], "entities": []}, {"text": " Table 8: Official results of WMT18 News Translation Task. Systems ordered by standardized mean DA score, though systems", "labels": [], "entities": [{"text": "WMT18 News Translation Task", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.7931656241416931}, {"text": "standardized mean DA score", "start_pos": 78, "end_pos": 104, "type": "METRIC", "confidence": 0.8977243006229401}]}, {"text": " Table 9: Source-based DA results for English\u2192Czech", "labels": [], "entities": []}, {"text": " Table 10: Head to head comparison for Chinese\u2192English systems.", "labels": [], "entities": []}, {"text": " Table 11: Head to head comparison for English\u2192Chinese systems.", "labels": [], "entities": []}, {"text": " Table 12: Head to head comparison for Czech\u2192English systems.", "labels": [], "entities": []}, {"text": " Table 13: Head to head comparison for English\u2192Czech systems.", "labels": [], "entities": []}, {"text": " Table 14: Head to head comparison for German\u2192English systems.", "labels": [], "entities": []}, {"text": " Table 15: Head to head comparison for English\u2192German systems.", "labels": [], "entities": []}, {"text": " Table 16: Head to head comparison for Estonian\u2192English systems.", "labels": [], "entities": []}, {"text": " Table 17: Head to head comparison for English\u2192Estonian systems.", "labels": [], "entities": []}, {"text": " Table 18: Head to head comparison for Finnish\u2192English systems.", "labels": [], "entities": []}, {"text": " Table 19: Head to head comparison for English\u2192Finnish systems.", "labels": [], "entities": []}, {"text": " Table 20: Head to head comparison for Russian\u2192English systems.", "labels": [], "entities": []}, {"text": " Table 21: Head to head comparison for English\u2192Russian systems.", "labels": [], "entities": []}, {"text": " Table 22: Head to head comparison for Turkish\u2192English systems.", "labels": [], "entities": []}, {"text": " Table 23: Head to head comparison for English\u2192Turkish systems.", "labels": [], "entities": []}]}