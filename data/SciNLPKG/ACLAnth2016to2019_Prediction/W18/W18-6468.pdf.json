{"title": [{"text": "A Transformer-Based Multi-Source Automatic Post-Editing System", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents our English-German Automatic Post-Editing (APE) system submitted to the APE Task organized at WMT 2018 (Chatterjee et al., 2018).", "labels": [], "entities": [{"text": "APE Task organized at WMT 2018", "start_pos": 92, "end_pos": 122, "type": "TASK", "confidence": 0.5564188361167908}]}, {"text": "The proposed model is an extension of the transformer architecture: two separate self-attention-based encoders encode the machine translation output (mt) and the source (src), followed by a joint en-coder that attends over a combination of these two encoded sequences (enc src and enc mt) for generating the post-edited sentence.", "labels": [], "entities": []}, {"text": "We compare this multi-source architecture (i.e, {src, mt} \u2192 pe) to a monolingual transformer (i.e., mt \u2192 pe) model and an ensemble combining the multi-source {src, mt} \u2192 pe and single-source mt \u2192 pe models.", "labels": [], "entities": []}, {"text": "For both the PBSMT and the NMT task, the ensemble yields the best results, followed by the multi-source model and last the single-source approach.", "labels": [], "entities": []}, {"text": "Our best model, the ensemble , achieves a BLEU score of 66.16 and 74.22 for the PBSMT and NMT task, respectively.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9870406985282898}, {"text": "PBSMT", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.7760666608810425}, {"text": "NMT", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.6931625604629517}]}], "introductionContent": [], "datasetContent": [{"text": "In our experiment we investigate (1) how well the transformer-based APE architecture performs in general, (2) if our multi-source architecture using the additional joint encoder improves the performance over a single-source architecture, and (3) if ensembling of single-source and multi-source architectures facilitates APE even further.", "labels": [], "entities": [{"text": "APE", "start_pos": 320, "end_pos": 323, "type": "TASK", "confidence": 0.9629464745521545}]}, {"text": "In this section, we present the training process, using the above datasets, to train mt \u2192 pe, {src, mt} \u2192 pe, and ensemble models for both PBSMT and NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 149, "end_pos": 152, "type": "DATASET", "confidence": 0.8645732998847961}]}], "tableCaptions": [{"text": " Table 1: Statistics of the WMT 2018 APE Shared Task Dataset.", "labels": [], "entities": [{"text": "WMT 2018 APE Shared Task Dataset", "start_pos": 28, "end_pos": 60, "type": "DATASET", "confidence": 0.5805814315875372}]}, {"text": " Table 2: Evaluation result of WMT 2018 PBSMT task for all trained models.", "labels": [], "entities": [{"text": "WMT 2018 PBSMT task", "start_pos": 31, "end_pos": 50, "type": "DATASET", "confidence": 0.794856458902359}]}, {"text": " Table 3: Evaluation result of WMT 2018 NMT task for all trained models.", "labels": [], "entities": [{"text": "WMT 2018 NMT task", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.6295421719551086}]}]}