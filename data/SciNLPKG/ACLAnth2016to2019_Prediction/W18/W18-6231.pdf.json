{"title": [{"text": "NL-FIIT at IEST-2018: Emotion Recognition utilizing Neural Networks and Multi-level Preprocessing", "labels": [], "entities": [{"text": "NL-FIIT at IEST-2018", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.80898517370224}, {"text": "Emotion Recognition", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.928639680147171}]}], "abstractContent": [{"text": "In this paper, we present neural models submitted to Shared Task on Implicit Emotion Recognition, organized as part of WASSA 2018.", "labels": [], "entities": [{"text": "Implicit Emotion Recognition", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.6582497358322144}]}, {"text": "We propose a Bi-LSTM architecture with regularization through dropout and Gaus-sian noise.", "labels": [], "entities": []}, {"text": "Our models use three different embedding layers: GloVe word embeddings trained on Twitter dataset, ELMo embeddings and also sentence embeddings.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.8603260517120361}]}, {"text": "We see pre-processing as one of the most important parts of the task.", "labels": [], "entities": []}, {"text": "We focused on handling emojis, emoticons, hashtags, and also various shortened word forms.", "labels": [], "entities": []}, {"text": "In some cases, we proposed to remove some parts of the text, as they do not affect emotion of the original sentence.", "labels": [], "entities": []}, {"text": "We also experimented with other modifications like category weights for learning and stacking multiple layers.", "labels": [], "entities": []}, {"text": "Our model achieved a macro average F1 score of 65.55 %, significantly outperforming the baseline model produced by a simple logistic regression.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9479404389858246}]}], "introductionContent": [{"text": "Both text reconstruction and sentiment analysis are well studied and highly practical areas of research in the field of natural language processing.", "labels": [], "entities": [{"text": "text reconstruction", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.8109118044376373}, {"text": "sentiment analysis", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.9541487097740173}, {"text": "natural language processing", "start_pos": 120, "end_pos": 147, "type": "TASK", "confidence": 0.6583422621091207}]}, {"text": "Recently, there have been significant advances and improvements, at least partly due to the wider adoption of neural networks (.", "labels": [], "entities": []}, {"text": "As it is, Implicit Emotion Recognition, as proposed by organizers of WASSA 2018 workshop (, can be seen both as a text reconstruction and as a sentiment analysis task.", "labels": [], "entities": [{"text": "Implicit Emotion Recognition", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.9442976315816244}, {"text": "WASSA 2018 workshop", "start_pos": 69, "end_pos": 88, "type": "DATASET", "confidence": 0.6576667229334513}, {"text": "text reconstruction", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7131660431623459}, {"text": "sentiment analysis task", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.9147864977518717}]}, {"text": "This is possible because, in this task, sentiment of a sentence should be equal to the missing word.", "labels": [], "entities": []}, {"text": "In practice, the difference is marginal, nevertheless for both these tasks bi-directional LSTMs are widely used.", "labels": [], "entities": []}, {"text": "In recent years, there have been several competitions, papers and shared tasks dealing with emotion recognition and classification.", "labels": [], "entities": [{"text": "emotion recognition and classification", "start_pos": 92, "end_pos": 130, "type": "TASK", "confidence": 0.7008766084909439}]}, {"text": "Dealing with noisy and ungrammatical user-generated text can be also challenging in other high-level NLP tasks like summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 116, "end_pos": 129, "type": "TASK", "confidence": 0.9834001660346985}]}, {"text": "In this paper, we present a neural network architecture with special focus on the preprocessing phase.", "labels": [], "entities": []}, {"text": "We believe preprocessing can have significant impact on accuracy of each system in natural language processing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9989039897918701}]}, {"text": "We explored many setups and also different types of regularization as dropout, Gaussian noise, kernel and activity regularization -L1 and L2, and also recurrent dropout within LSTM cells.", "labels": [], "entities": []}, {"text": "We also experimented with three different types of embedding layers -GloVe, ELMo and various sentence representations.", "labels": [], "entities": []}, {"text": "Finally, we explored impact of different setups on model accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9890704154968262}]}, {"text": "In this paper, we report on results of these experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we briefly summarize evaluation metrics for this task and also basic information about used dataset and embeddings.", "labels": [], "entities": []}, {"text": "Later, we describe different setups of our model.", "labels": [], "entities": []}, {"text": "For evaluation, standard measures like precision, recall and f-score were used.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9998136162757874}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9996258020401001}, {"text": "f-score", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9913533329963684}]}, {"text": "Then micro and macro measures were computed.", "labels": [], "entities": []}, {"text": "As final official result, macro F1 was taken.", "labels": [], "entities": [{"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.899325430393219}]}, {"text": "The dataset for emotion recognition shared task consists of tweets where emotion word was removed.", "labels": [], "entities": [{"text": "emotion recognition shared task", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.8612374365329742}]}, {"text": "The dataset contains six different categories: anger, disgust, fear, joy, sadness, and surprise.", "labels": [], "entities": []}, {"text": "The train dataset contains approximately 150 thousands of tweets and test contains more than 30 thousands of tweets.", "labels": [], "entities": []}, {"text": "Detailed information can be found in the main paper of the shared task ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison between different models.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of different preprocessing se- tups.", "labels": [], "entities": []}]}