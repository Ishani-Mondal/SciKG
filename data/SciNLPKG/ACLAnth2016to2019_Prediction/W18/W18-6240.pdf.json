{"title": [{"text": "EmojiGAN: learning emojis distributions with a generative model", "labels": [], "entities": []}], "abstractContent": [{"text": "Generative models have recently experienced a surge in popularity due to the development of more efficient training algorithms and increasing computational power.", "labels": [], "entities": []}, {"text": "Models such as adversarial generative networks (GANs) have been successfully used in various areas such as computer vision, medical imaging, style transfer and natural language generation.", "labels": [], "entities": [{"text": "adversarial generative networks (GANs)", "start_pos": 15, "end_pos": 53, "type": "TASK", "confidence": 0.7688588500022888}, {"text": "style transfer", "start_pos": 141, "end_pos": 155, "type": "TASK", "confidence": 0.8223142027854919}, {"text": "natural language generation", "start_pos": 160, "end_pos": 187, "type": "TASK", "confidence": 0.6480664114157358}]}, {"text": "Adver-sarial nets were recently shown to yield results in the image-to-text task, where given a set of images, one has to provide their corresponding text description.", "labels": [], "entities": []}, {"text": "In this paper, we take a similar approach and propose a image-to-emoji architecture , which is trained on data from social networks and can be used to score a given picture using ideograms.", "labels": [], "entities": []}, {"text": "We show empirical results of our algorithm on data obtained from the most influential Instagram accounts.", "labels": [], "entities": [{"text": "Instagram accounts", "start_pos": 86, "end_pos": 104, "type": "DATASET", "confidence": 0.9223397970199585}]}], "introductionContent": [{"text": "The spike in the amount of user-generated visual and textual data shared on social platforms such as Facebook, Twitter, Instagram, Pinterest and many others luckily coincides with the development of efficient deep learning algorithms (.", "labels": [], "entities": []}, {"text": "As humans, we cannot only share our ideas and thoughts through any imaginable media, but also use social networks to analyze and understand complex interpersonal relations.", "labels": [], "entities": []}, {"text": "Researchers have access to a rich set of metadata) on which various computer vision (CV) and natural language processing (NLP) algorithms can be trained.", "labels": [], "entities": []}, {"text": "For instance, recent work in the area of image captioning aims to provide a short description (i.e. caption) of a much larger document or image.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.7155602127313614}]}, {"text": "Such * These authors contributed equally.", "labels": [], "entities": []}, {"text": "methods excel at conveying the dominant idea of the input.", "labels": [], "entities": []}, {"text": "On the other hand, we use ideograms, also popular under the names of emojis or pictographs as a natural amalgam between annotation and summarization tasks.", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.9058129787445068}]}, {"text": "Note that, in this work, we use the terms emoji, ideogram and pictograph interchangeably to represent the intersection of these three domains.", "labels": [], "entities": []}, {"text": "Ideograms bridge together the textual and visual spaces by representing groups of words with a concise illustration.", "labels": [], "entities": []}, {"text": "They can be seen as surrogate functions which convey, up to a degree of accuracy, reactions of social media users.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9928423166275024}]}, {"text": "Furthermore, because each emoji has a corresponding text description, there is a direct mapping from ideograms onto the word space.", "labels": [], "entities": []}, {"text": "In this paper, we model the distribution of emojis conditioned on an image with a deep generative model.", "labels": [], "entities": []}, {"text": "We use generative adversarial networks (GANs) (), which are notoriously known to be harder to train than other distributional models such as variational auto-encoders (VAEs)) but tend to produce sharper results on computer vision tasks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}