{"title": [{"text": "PD3: Better Low-Resource Cross-Lingual Transfer By Combining Direct Transfer and Annotation Projection", "labels": [], "entities": [{"text": "Cross-Lingual Transfer", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.6646278202533722}]}], "abstractContent": [{"text": "We consider unsupervised cross-lingual transfer on two tasks, viz., sentence-level argumen-tation mining and standard POS tagging.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.74130579829216}, {"text": "sentence-level argumen-tation mining", "start_pos": 68, "end_pos": 104, "type": "TASK", "confidence": 0.5961867570877075}, {"text": "POS tagging", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.7028680145740509}]}, {"text": "We combine direct transfer using bilingual em-beddings with annotation projection, which projects labels across unlabeled parallel data.", "labels": [], "entities": []}, {"text": "We do so by either merging respective source and target language datasets or alternatively by using multi-task learning.", "labels": [], "entities": []}, {"text": "Our combination strategy considerably improves upon both direct transfer and projection with few available parallel sentences, the most realistic scenario for many low-resource target languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, interest in multi-and cross-lingual natural language processing (NLP) has steadily increased.", "labels": [], "entities": [{"text": "cross-lingual natural language processing (NLP)", "start_pos": 39, "end_pos": 86, "type": "TASK", "confidence": 0.7548374533653259}]}, {"text": "This has not only to do with the recognition that performances of newly introduced systems should be robust across several tasks (in several languages), but more fundamentally with the idea of truly 'universal' NLP methods which should not only suit English, an arguably particularly simple exemplar of the world's roughly 7,000 languages.", "labels": [], "entities": []}, {"text": "A further motivation for cross-lingual approaches is the fact that many labeled datasets are to this date only available in English and labeled data is generally costly to obtain-be it via expert annotators or through crowd-sourcing.", "labels": [], "entities": []}, {"text": "Therefore, methods which are capable of training on labeled data in a resource-rich language such as English and which can then be applied to typically resourcepoor other languages are highly desirable.", "labels": [], "entities": []}, {"text": "Two standard cross-lingual approaches are projection ( and direct transfer).", "labels": [], "entities": []}, {"text": "Direct transfer trains, in the source language L1, on language-independent or shared features and then directly applies the trained system to the target language of interest L2.", "labels": [], "entities": []}, {"text": "In contrast, projection trains and evaluates on L2 itself.", "labels": [], "entities": []}, {"text": "To do so, it uses parallel data, applies a system trained on L1 to its source side and then projects the inferred labels to the parallel L2 side.", "labels": [], "entities": []}, {"text": "This projection step may involve word alignment information.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.7848252356052399}]}, {"text": "After projection, an annotated L2 dataset is available on which L2 systems can be trained.", "labels": [], "entities": [{"text": "L2 dataset", "start_pos": 31, "end_pos": 41, "type": "DATASET", "confidence": 0.7099424451589584}]}, {"text": "Projection and direct transfer each ignore important information, however.", "labels": [], "entities": []}, {"text": "For example, standard projection ignores the available data in L1 once the L2 dataset has been created and standard direct transfer does not use any L2 information.", "labels": [], "entities": []}, {"text": "In this work, we investigate whether the inclusion of both L1 and L2 data outperforms transfer approaches that exploit only one type of such information, and if so, under what conditions.", "labels": [], "entities": []}, {"text": "More precisely, we first train a system on shared features as in standard direct transfer on labeled L1 data.", "labels": [], "entities": []}, {"text": "Then, we make use of two further datasets.", "labels": [], "entities": []}, {"text": "One is based on the source side of parallel unlabeled data; it is derived similarly as in self-training by applying the trained system to unlabeled data, from which a pseudo-labeled dataset is derived.", "labels": [], "entities": []}, {"text": "The other is based on its target side-using annotation projections-as in standard projection.", "labels": [], "entities": []}, {"text": "Thus, we explore the effects of combining Projection and Direct transfer using three datasets (PD3).", "labels": [], "entities": []}, {"text": "Our approach is detailed in \u00a72.", "labels": [], "entities": []}, {"text": "We report results for two L2 languages (French, German) on one sentence-level problem (argumentation mining) and one token-level problem (POS tagging).", "labels": [], "entities": [{"text": "argumentation mining)", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.8112709124883016}, {"text": "POS tagging)", "start_pos": 138, "end_pos": 150, "type": "TASK", "confidence": 0.7299830317497253}]}, {"text": "We find that our suggested approach PD3 substantially outperforms both direct transfer and projection when little parallel data is available, the most realistic scenario for many L2 languages.", "labels": [], "entities": []}, {"text": "While our approach is general, our focus is particularly on argumentation mining (ArgMin), a rapidly growing research field in NLP.", "labels": [], "entities": [{"text": "argumentation mining (ArgMin)", "start_pos": 60, "end_pos": 89, "type": "TASK", "confidence": 0.7441073298454285}]}, {"text": "Cross-lingual transfer is majorly important for ArgMin because it is inherently costly to get high-quality annotations for ArgMin due to: (i) subjectivity of argumentation as well as divergent and competing ArgMin theories, leading to disagreement among crowdworkers as well as expert annotators, (ii) dependence of argument annotations on background knowledge and parsing of complex pragmatic relations.", "labels": [], "entities": [{"text": "Cross-lingual transfer", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7821949124336243}, {"text": "parsing of complex pragmatic relations", "start_pos": 365, "end_pos": 403, "type": "TASK", "confidence": 0.8077856183052063}]}, {"text": "Thus, in order not to reproduce the same annotation costs for new languages, cross-lingual ArgMin methods are required.", "labels": [], "entities": []}, {"text": "These techniques should both perform well with little available parallel data, to address many languages, and with general (nonargumentative) parallel data, because this is much more likely to be available.", "labels": [], "entities": []}, {"text": "Our experiments address both of these requirements.", "labels": [], "entities": []}], "datasetContent": [{"text": "Sentence level network architecture: In our sentence-level ArgMin experiments, we use a convolutional neural network (CNN) with 1-max pooling to learn a representation of the input sentence and feed this representation into a softmax regression classifier.", "labels": [], "entities": []}, {"text": "We use 800 CNN filters with a window size of 3.", "labels": [], "entities": []}, {"text": "For optimization, we use Adam with a learning rate of 0.001.", "labels": [], "entities": []}, {"text": "Training sentences are processed in minibatches of size 16.", "labels": [], "entities": []}, {"text": "We do not apply dropout or 2 regularization.", "labels": [], "entities": []}, {"text": "We report average macro F1 scores over 20 runs with different random initializations.", "labels": [], "entities": []}, {"text": "For PD3-merge, we shuffle the merged data before trainingi.e., mini-batches can contain L S , \u02c6 D S , and\u02c6\u02c6and\u02c6and\u02c6\u02c6 D T data.", "labels": [], "entities": []}, {"text": "For PD3-MTL, we shuffle L1 and L2 data individually and during training we sample each mini-batch from either task according to its size.", "labels": [], "entities": []}, {"text": "In the MTL setup, we share the CNN layer across tasks and use task-specific softmax regression layers.", "labels": [], "entities": [{"text": "MTL", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9560351371765137}]}, {"text": "Sequence tagging network architecture: For token-level POS tagging, we implement a bidirectional LSTM as in and with a CRF output layer.", "labels": [], "entities": [{"text": "Sequence tagging network", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.889060378074646}, {"text": "POS tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.7905169725418091}]}, {"text": "This is a state-of-the-art system for sequence tagging tasks such as POS and NER.", "labels": [], "entities": [{"text": "sequence tagging tasks", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7908948659896851}]}, {"text": "Our model uses pretrained word embeddings and optionally concatenates these with a learned character-level representation.", "labels": [], "entities": []}, {"text": "For all experiments, we use the same network topology: we use two hidden layers with 100 hidden units each, applying dropout on the hidden units and on the word embeddings.", "labels": [], "entities": []}, {"text": "We use Adam as optimizer.", "labels": [], "entities": []}, {"text": "Our network uses a CRF output layer rather than a softmax classifier to account for dependencies between successive labels.", "labels": [], "entities": []}, {"text": "In the MTL setup, we use the same architecture, but connect the last hidden layer to individual output layers, one for each task.", "labels": [], "entities": [{"text": "MTL", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9467341303825378}]}, {"text": "Our MTL architecture extends the architecture of by replacing the softmax output layer with a CRF output layer, and by including character-level word representations.", "labels": [], "entities": []}, {"text": "The difference between MTL and single-task learning (STL) is illustrated in.", "labels": [], "entities": [{"text": "MTL", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9470774531364441}]}, {"text": "STL is a network with only one task, as in PD3-merge, direct transfer and standard projection.", "labels": [], "entities": []}, {"text": "We report average accuracy over five (or 10, in case of very little data) random weight matrix initializations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9831042289733887}]}, {"text": "In the MTL setup, we choose a mini-batch randomly in each iteration (containing instances from only one of the tasks as in our sentence-level ArgMin experiments).", "labels": [], "entities": [{"text": "MTL", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9403918981552124}]}, {"text": "Cross-lingual Embeddings: For token-level experiments, we initially train 100-d BIVCD embeddings) from Europarl () (for EN-DE) and the UN corpus () (for EN-FR), respectively.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.9647189974784851}]}, {"text": "For sentence-level experiments, we use 300-d BIVCD embeddings.", "labels": [], "entities": []}, {"text": "This means that we initially assume that high-quality bilingual word embeddings are readily available for the two languages involved.", "labels": [], "entities": []}, {"text": "At first sight, this appears a realistic assumption since high-quality bilingual embeddings can already be obtained with very little available bilingual data (.", "labels": [], "entities": []}, {"text": "In low-resource settings, however, even little monolingual data is typically available for L2 and we address this setup subsequently.", "labels": [], "entities": []}, {"text": "Upper bound: For both ArgMin and POS, we report the in-language upper bound, i.e., when the model is trained and evaluated on L2.", "labels": [], "entities": []}, {"text": "For this, we choose random L2 train sets of size |L S |.", "labels": [], "entities": []}, {"text": "Projection strategy for sequence tagging: We first word-align parallel data using fast-align (.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.791199803352356}]}, {"text": "When an L2 word is uniquely aligned to an L1 word, we assign it the L1 word's unique Not cooking fresh food will lead to lack of nutrition Claim To sum up,.] the merits of animal experiments still outweigh the demerits Major claim For example, tourism makes up one third of Czech's economy Premise I will mention some basic reasoning as follows O  label.", "labels": [], "entities": [{"text": "O  label", "start_pos": 345, "end_pos": 353, "type": "METRIC", "confidence": 0.9771191775798798}]}, {"text": "When an L2 word is aligned to several L1 words, we randomly draw one of the aligned source labels.", "labels": [], "entities": []}, {"text": "When an L2 word is not aligned to any L1 word, we draw a label randomly from its unique labels in the remainder of the corpus.", "labels": [], "entities": []}, {"text": "Our projection strategy is standard, cf..", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics for datasets used in this work. |Y| denotes the size of the label space.", "labels": [], "entities": []}]}