{"title": [], "abstractContent": [{"text": "Previous simultaneous translation approaches either use a separate segmentation step followed by a machine translation decoder or rely on the decoder to segment and translate without training the segmenter to minimize delay or increase translation quality.", "labels": [], "entities": [{"text": "simultaneous translation", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.7231995761394501}]}, {"text": "We integrate a segmentation model and an incremental decoding algorithm to create an automatic simultaneous translation framework.", "labels": [], "entities": []}, {"text": "(2014) propose a method to provide annotated data for sentence segmentation.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.7307667881250381}]}, {"text": "This work uses this data to train a segmentation model that is integrated with a novel simultaneous translation decoding algorithm.", "labels": [], "entities": []}, {"text": "We show that this approach is more accurate than previously proposed segmentation models when integrated with a translation decoder.", "labels": [], "entities": []}, {"text": "Our results on the speech translation of TED talks from English to German show that our system can achieve translation quality close to the offline translation system while at the same time minimizing the delay in producing the translations incrementally.", "labels": [], "entities": [{"text": "speech translation of TED talks from English to German", "start_pos": 19, "end_pos": 73, "type": "TASK", "confidence": 0.834116968843672}]}, {"text": "Our approach also outperforms other comparable simultaneous translation systems in terms of translation quality and latency.", "labels": [], "entities": [{"text": "latency", "start_pos": 116, "end_pos": 123, "type": "METRIC", "confidence": 0.9559929966926575}]}], "introductionContent": [{"text": "In simultaneous translation the incoming speech stream is segmented and translated incrementally to reduce the latency.", "labels": [], "entities": []}, {"text": "There are two approaches for simultaneous translation task: sentence segmentation and incremental decoding, also called stream decoding.", "labels": [], "entities": [{"text": "simultaneous translation task", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.6926313042640686}, {"text": "sentence segmentation", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.7463929355144501}]}, {"text": "In incremental decoding, incoming words are fed into the decoder one-by-one, and the decoder updates its internal state.", "labels": [], "entities": []}, {"text": "The decoder is responsible to decide when to begin the translation process and when to output the translation.", "labels": [], "entities": []}, {"text": "Incremental decoding algorithms have been proposed for phrase-based () translation, hierarchical phrase-based () and syntax-based () translation systems.", "labels": [], "entities": []}, {"text": "Real-world speech translation systems estimate the sentence boundaries using punctuation insertion methods.", "labels": [], "entities": []}, {"text": "As a result, recent work in simultaneous machine translation assume the input is already segmented into sentences, and focus on splitting the sentences into shorter subsequences of words (segments).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.6894404590129852}]}, {"text": "This approach is called sentence segmentation.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7785214483737946}]}, {"text": "As soon as a segment is recognized, it is given to a decoder to generate and output the translation for that segment.", "labels": [], "entities": []}, {"text": "Different methods have been proposed for sentence segmentation.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7703964114189148}]}, {"text": "Some use prosodic boundaries for segmentation), while others use classification models.", "labels": [], "entities": []}, {"text": "For example Rangarajan  train a classifier to predict punctuation marks.", "labels": [], "entities": []}, {"text": "The other approaches rely on the reordering probabilities of phrases to predict the segment boundaries ().", "labels": [], "entities": []}, {"text": "propose a method to provide annotated data for sentence segmentation which can be used in training a segmentation model.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7186666876077652}]}, {"text": "This method which later have been extended by) aims to find the best segmentation strategy fora given set of sentence which optimizes the translation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.8802670836448669}]}, {"text": "But the obtained annotated data has never been used in an end-to-end simultaneous translation system.", "labels": [], "entities": []}, {"text": "In this work, we focus on sentence segmentation approach for simultaneous translation.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.74277663230896}, {"text": "simultaneous translation", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.6786048710346222}]}, {"text": "We model the segmentation task as a classification problem and investigate different methods to provide annotated data for training the segmentation model (Section 2).", "labels": [], "entities": [{"text": "segmentation task", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.9175702333450317}]}, {"text": "We modify approach by propose anew formula to compute the latency and use Pareto-optimality for finding good segment boundaries that can balance the trade-off between latency versus translation quality.", "labels": [], "entities": [{"text": "Pareto-optimality", "start_pos": 74, "end_pos": 91, "type": "METRIC", "confidence": 0.9783444404602051}]}, {"text": "We use the obtained annotated data to train a segmentation model.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.9730917811393738}]}, {"text": "We conduct various experiments to evaluate the segmentation model and show that this model outperforms previous segmentation models in terms of accuracy.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.9748665690422058}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9986585378646851}]}, {"text": "Segmentation-based simultaneous translation approaches typically use a traditional phrase-based decoder to translate each input segment individually.", "labels": [], "entities": [{"text": "Segmentation-based simultaneous translation", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7602328062057495}]}, {"text": "Although hierarchical phrase-based (Hiero) translation system usually performs comparable to or better than conventional phrase-based systems, they use CKY based decoding algorithm which requires the entire input sentence to generate the translation.", "labels": [], "entities": [{"text": "Hiero) translation", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.6861374974250793}]}, {"text": "While phrasebased decoders generate translation in a left-to-right manner and it makes phrase-based systems more suitable for simultaneous translation than Hiero.", "labels": [], "entities": []}, {"text": "We use LR-Hiero for simultaneous translation which uses hierarchical phrase-based translation models while generates the translation in left-to-right manner ().", "labels": [], "entities": [{"text": "simultaneous translation", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6512390077114105}]}, {"text": "We modify LR-Hiero decoder and combine it with the segmentation model to incrementally translate the input sentence (stream of words).", "labels": [], "entities": []}, {"text": "We evaluate our simultaneous translation system on the speech translation of TED talks on EnglishGerman.", "labels": [], "entities": [{"text": "TED talks on EnglishGerman", "start_pos": 77, "end_pos": 103, "type": "DATASET", "confidence": 0.6489230990409851}]}, {"text": "The experimental results show that our system can achieve translation quality close to offline SMT system while generate the output translation words around twenty times faster.", "labels": [], "entities": [{"text": "SMT", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9725200533866882}]}, {"text": "We also compare our simultaneous translation system to neural machine translation (NMT) simultaneous translation systems.", "labels": [], "entities": [{"text": "simultaneous translation", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6501631736755371}, {"text": "neural machine translation (NMT) simultaneous translation", "start_pos": 55, "end_pos": 112, "type": "TASK", "confidence": 0.8433437421917915}]}, {"text": "Our system outperforms the state of the art NMT-based simultaneous translation system in both translation quality and latency.", "labels": [], "entities": [{"text": "NMT-based simultaneous translation", "start_pos": 44, "end_pos": 78, "type": "TASK", "confidence": 0.6696280439694723}]}], "datasetContent": [{"text": "Following the International Workshop on Spoken Language Translation (IWSLT) shared task, we evaluate our approach on the speech translation of TED talks for English-German.Section 4.2 describe the experimental setting.", "labels": [], "entities": [{"text": "International Workshop on Spoken Language Translation (IWSLT) shared task", "start_pos": 14, "end_pos": 87, "type": "TASK", "confidence": 0.7037543031302366}, {"text": "speech translation of TED talks", "start_pos": 121, "end_pos": 152, "type": "TASK", "confidence": 0.8341308236122131}]}, {"text": "We conduct many experiments to evaluate our approach.", "labels": [], "entities": []}, {"text": "We first evaluate different approaches to create the segmentation model and experiment on various feature sets to obtain the best segmentation model (Section 4.2).", "labels": [], "entities": [{"text": "segmentation", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.961106538772583}]}, {"text": "Then we use the trained segmentation model in an end-to-end simultaneous translation system (Section 4.3).", "labels": [], "entities": []}, {"text": "We evaluate the performance of end-to-end simultaneous translation system in terms of translation quality and latency and compare it with different baselines.", "labels": [], "entities": [{"text": "latency", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.9583127498626709}]}, {"text": "We evaluate our simultaneous translation framework on a English-German translation task.", "labels": [], "entities": [{"text": "English-German translation task", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.7330055236816406}]}, {"text": "We calculate latency as the total time taken to translate the whole sentence divided by the number of segments.", "labels": [], "entities": [{"text": "latency", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9934155941009521}]}, {"text": "Latency in shows the result of taking the average over 5 different runs for 50 sentences randomly selected from the test set.", "labels": [], "entities": [{"text": "Latency", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.986530601978302}]}, {"text": "The first three rows of compare the results of the end-to-end simultaneous translation using segmentation models trained by translation-based and alignment-based heuristics.", "labels": [], "entities": []}, {"text": "We can see that segmentation model trained on translation-based heuristic outperforms the other segmentation models both in translation accuracy and latency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9471008777618408}]}, {"text": "To evaluate our simultaneous translation framework we use four baselines.", "labels": [], "entities": []}, {"text": "We implemented a heuristic segmenter based on) which segments on surface clues such as punctuation marks.", "labels": [], "entities": []}, {"text": "These segments reflect the idea of segmentation on silence frames of around 100ms in the ASR output used in (.", "labels": [], "entities": []}, {"text": "The results of this heuristic (prosodic) has been shown in the forth row of.", "labels": [], "entities": []}, {"text": "The last row in shows the results of the regular translation strategy (with no segmentation employed).", "labels": [], "entities": [{"text": "regular translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.5829882919788361}]}, {"text": "For a relatively small loss in the BLEU score we obtain a much faster incremental translation system.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9658464193344116}]}, {"text": "To evaluate the impact of segmentation model we add two more baselines in which decoder segments the input stream without using the segmentation model: (i) Fixed Segmentation: a segmentation with equally sized fragments; (ii) Random Segmentation: decoder randomly segments the input.", "labels": [], "entities": []}, {"text": "These two baselines show comparable performance.", "labels": [], "entities": []}, {"text": "The reduction in the BLEU score for these segmentation models shows that we need a more informative segmentation model.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9747329652309418}]}, {"text": "We also compare our output against a state-of-the-art simultaneous neural MT approach (, which uses a reinforcement learning style agent which is trained using a policy gradient algorithm to find segments that minimize delay and maximize the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 242, "end_pos": 252, "type": "METRIC", "confidence": 0.9779467880725861}]}, {"text": "The agent uses a softmax policy over the segmentation outcomes (either read or write, aka segment) and trains its parameters by learning segmentations decisions based on a fully-trained non-simultaneous NMT encoder-decoder.", "labels": [], "entities": []}, {"text": "use anew metric to measure the latency called average proportion proposed by.", "labels": [], "entities": []}, {"text": "Average proportion is defined as the average number of source words being used, when translating each word.", "labels": [], "entities": [{"text": "Average proportion", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.9613235592842102}]}, {"text": "The average proportion d(X, Y ) fora source sentence X and translation output Y is defined as |X||Y | where |X| and |Y | are the length of source and translation sentences respectively, and s(t) is the number of already seen words from source sentence, when translating each word.", "labels": [], "entities": []}, {"text": "We ran the approach on our English-German task.", "labels": [], "entities": []}, {"text": "In order to compare the latency, we compute the average proportion for the output of our translation system which are shown in.", "labels": [], "entities": [{"text": "latency", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9508217573165894}]}, {"text": "In this figure we have shown the results of our translation framework using different segmentation models trained for different segment lengths (\u03bc values 3 to 8).", "labels": [], "entities": []}, {"text": "We trained the NMT system with \u03bc = 8.", "labels": [], "entities": [{"text": "\u03bc", "start_pos": 31, "end_pos": 32, "type": "METRIC", "confidence": 0.9526193737983704}]}, {"text": "also shows the results of offline translation for our approach and the NMT system (which results in average proportion of 1).", "labels": [], "entities": [{"text": "offline translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.5791425555944443}]}, {"text": "There is a substantial loss in translation quality for simultaneous NMT (consistent with the results in).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Corpus statistics in number of sentences and tokens (source side).", "labels": [], "entities": []}, {"text": " Table 3: Results of segmentation model trained on different labeled data using various feature sets.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.9777742624282837}]}, {"text": " Table 4: Results of our simultaneous translation using different segmentation models on English-German translation  task. The last row shows the offline translation (regular SMT without segmentation). Segment length is set to 6 in Fixed  Segmentation and Random Segmentation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 175, "end_pos": 178, "type": "TASK", "confidence": 0.9516903758049011}]}]}