{"title": [{"text": "Automated Fact-Checking of Claims in Argumentative Parliamentary Debates", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an automated approach to distinguish true, false, stretch, and dodge statements in questions and answers in the Cana-dian Parliament.", "labels": [], "entities": [{"text": "Cana-dian Parliament", "start_pos": 123, "end_pos": 143, "type": "DATASET", "confidence": 0.947507232427597}]}, {"text": "We leverage the truthfulness annotations of a U.S. fact-checking corpus by training a neural net model and incorporating the prediction probabilities into our models.", "labels": [], "entities": []}, {"text": "We find that in concert with other linguistic features, these probabilities can improve the multi-class classification results.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.7544332146644592}]}, {"text": "We further show that dodge statements can be detected with an F 1 measure as high as 82.57% in binary classification settings.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9833693106969198}]}], "introductionContent": [{"text": "Governments and parliaments that are selected and chosen by citizens' votes have ipso facto attracted a certain level of trust.", "labels": [], "entities": []}, {"text": "However, governments and parliamentarians use combinations of true statements, false statements, and exaggerations in strategic ways to question other parties' trustworthiness and to thereby create distrust towards them while gaining credibility for themselves.", "labels": [], "entities": []}, {"text": "Creating distrust and alienation maybe achieved by using ad hominem arguments or by raising questions about someone's character and honesty ().", "labels": [], "entities": []}, {"text": "For example, consider the claims made within the following question that was asked in the Canadian Parliament: Example 1.1 The RCMP and Mike Duffy's lawyer have shown us that the Prime Minister has not been honest about this scandal.", "labels": [], "entities": []}, {"text": "When will he come clean and stop hiding his own role in this scandal?", "labels": [], "entities": []}, {"text": "These claims, including the presupposition of the second sentence that the Prime Minister has a role in the scandal that he is hiding, maybe true, false, or simply exaggerations.", "labels": [], "entities": [{"text": "presupposition", "start_pos": 28, "end_pos": 42, "type": "METRIC", "confidence": 0.9762134552001953}]}, {"text": "In order to be able to analyze how these claims serve their presenter's purpose or intention, we need to determine their truth.", "labels": [], "entities": []}, {"text": "Here, we will examine the linguistic characteristics of true statements, false statements, dodges, and stretches in argumentative parliamentary statements.", "labels": [], "entities": []}, {"text": "We examine whether falsehoods told by members of parliament can be identified with previously proposed approaches and we find that while some of these approaches improve the classification, identifying falsehoods by members of parliament remains challenging.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we perform a direct analysis with the PolitiFact dataset.", "labels": [], "entities": [{"text": "PolitiFact dataset", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.961360514163971}]}, {"text": "We first train a GRU model (used a sequence length of 200, other hyperparameters the same as those of the experiment described above) using 3-point scale annotations of PolitiFact (used 10% of the data for validation).", "labels": [], "entities": [{"text": "PolitiFact", "start_pos": 169, "end_pos": 179, "type": "DATASET", "confidence": 0.9183419942855835}]}, {"text": "We treat the top two truthful ratings (true and mostly true) as true; half true and mostly false as stretch; and the last two ratings (false and pants-on-fire false) as false.", "labels": [], "entities": []}, {"text": "We then test the model on three annotations of true, stretch, and false from the Toronto Star project.", "labels": [], "entities": [{"text": "Toronto Star project", "start_pos": 81, "end_pos": 101, "type": "DATASET", "confidence": 0.9702858527501425}]}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "As the results show, none of the false statements are detected as false and the overall F 1 score is lower than the majority baseline.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9936914046605428}]}, {"text": "We further train a GRU model (trained with binary cross-entropy and sequence length of 200, other hyperparameters the same as above) using 2-point scale where we treat the top three truthful ratings as true and the last three false ratings as false.", "labels": [], "entities": []}, {"text": "We then test the model on two annotations of true and false from the Toronto Star project.", "labels": [], "entities": [{"text": "Toronto Star project", "start_pos": 69, "end_pos": 89, "type": "DATASET", "confidence": 0.9694638252258301}]}], "tableCaptions": [{"text": " Table 1: Distribution of labels in the Toronto Star  dataset", "labels": [], "entities": [{"text": "Toronto Star  dataset", "start_pos": 40, "end_pos": 61, "type": "DATASET", "confidence": 0.9644714792569479}]}, {"text": " Table 2: Distribution of labels in the PolitiFact dataset", "labels": [], "entities": [{"text": "PolitiFact dataset", "start_pos": 40, "end_pos": 58, "type": "DATASET", "confidence": 0.8462652564048767}]}, {"text": " Table 3: Five-fold cross-validation results (F 1 and % accuracy) of four-way classification of fact-checking for the  overall dataset and F 1 for each class.", "labels": [], "entities": [{"text": "F 1", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9941567480564117}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.951047420501709}, {"text": "F 1", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.9865058064460754}]}, {"text": " Table 4: Average F 1 of different models for two- way classification of fact-checking (five-fold cross- validation).", "labels": [], "entities": [{"text": "Average F 1", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.7615454792976379}]}, {"text": " Table 5. As the results show, none of the false  statements are detected as false and the overall F 1  score is lower than the majority baseline.", "labels": [], "entities": [{"text": "F 1  score", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9935220082600912}]}, {"text": " Table 5: 3-point scale comparison of the PolitiFact  data and Toronto Star annotations. All: GRU model  is trained with all PolitiFact data and tested on Toronto  Star annotations. DQ: GRU model is trained with only  direct quotes from the PolitiFact data and tested on  Toronto Star annotations.", "labels": [], "entities": [{"text": "PolitiFact  data", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.9281146228313446}, {"text": "Toronto Star annotations", "start_pos": 63, "end_pos": 87, "type": "DATASET", "confidence": 0.9274585247039795}, {"text": "PolitiFact data", "start_pos": 125, "end_pos": 140, "type": "DATASET", "confidence": 0.8433696031570435}, {"text": "PolitiFact data", "start_pos": 241, "end_pos": 256, "type": "DATASET", "confidence": 0.8860584795475006}, {"text": "Toronto Star annotations", "start_pos": 272, "end_pos": 296, "type": "DATASET", "confidence": 0.9324686527252197}]}, {"text": " Table 6: 2-point scale comparison of the PolitiFact  data and Toronto Star annotations. All: GRU model  is trained with all PolitiFact data and tested on Toronto  Star annotations. DQ: GRU model is trained with only  direct quotes from the PolitiFact data and tested on  Toronto Star annotations.", "labels": [], "entities": [{"text": "PolitiFact  data", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.9290308654308319}, {"text": "Toronto Star annotations", "start_pos": 63, "end_pos": 87, "type": "DATASET", "confidence": 0.9271393219629923}, {"text": "PolitiFact data", "start_pos": 125, "end_pos": 140, "type": "DATASET", "confidence": 0.8450185060501099}, {"text": "PolitiFact data", "start_pos": 241, "end_pos": 256, "type": "DATASET", "confidence": 0.8869291245937347}, {"text": "Toronto Star annotations", "start_pos": 272, "end_pos": 296, "type": "DATASET", "confidence": 0.9327918489774069}]}]}