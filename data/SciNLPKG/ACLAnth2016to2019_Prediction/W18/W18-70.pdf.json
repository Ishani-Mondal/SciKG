{"title": [{"text": "The aim of the workshop was to bring together researchers interested in techniques for adapting texts to the needs for various users and NLP applications. This includes studies of end users' needs for text adaptation, techniques for automatic text adaptation, user evaluation of adapted texts, and infrastructure for conducting research on text adaptation", "labels": [], "entities": [{"text": "text adaptation", "start_pos": 201, "end_pos": 216, "type": "TASK", "confidence": 0.7787407040596008}, {"text": "text adaptation", "start_pos": 243, "end_pos": 258, "type": "TASK", "confidence": 0.7189834862947464}, {"text": "text adaptation", "start_pos": 340, "end_pos": 355, "type": "TASK", "confidence": 0.7205991744995117}]}], "abstractContent": [{"text": "For poor-readers and dyslexic children, reading is often a pitfall to social integration and academic progress.", "labels": [], "entities": []}, {"text": "The school support of these children usually requires adapted texts, specialised glossaries and dedicated management tools.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method which exploits French lexical resources to automatically simplify words in order to provide adapted texts.", "labels": [], "entities": []}, {"text": "Despite the difficulty of the task, the conducted evaluations show that the proposed methodology yields better results than the state of the art word2vec techniques for lexical simplification.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text simplification (hereafter TS) has received increasing interest by the scientific community in recent years.", "labels": [], "entities": [{"text": "Text simplification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8308600783348083}, {"text": "TS)", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9099155962467194}]}, {"text": "It aims at producing a simpler version of a source text that is both easier to read and to understand, thus improving the accessibility of text for people suffering from a range of disabilities such as aphasia () or dyslexia (, as well as for second language learners ( and people with low literacy (.", "labels": [], "entities": []}, {"text": "This topic has been researched fora variety of languages such as English (,), Spanish (Saggion et al., 2011), Portuguese (Specia, 2010), Italian () and Japanese (.", "labels": [], "entities": []}, {"text": "One of the main challenges in TS is finding an adequate automatic evaluation metric, which is necessary to avoid the time-consuming human evaluation.", "labels": [], "entities": [{"text": "TS", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9928985834121704}]}, {"text": "Any TS evaluation metric should take into account three properties expected from the output of a TS system, namely: \u2022 Grammaticality: how grammatically correct is the TS system output?", "labels": [], "entities": [{"text": "TS evaluation", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.8679404258728027}]}, {"text": "\u2022 Meaning preservation: how well is the meaning of the source sentence preserved in the TS system output?", "labels": [], "entities": [{"text": "Meaning preservation", "start_pos": 2, "end_pos": 22, "type": "TASK", "confidence": 0.85519078373909}]}, {"text": "\u2022 Simplicity: how simple is the TS system output?", "labels": [], "entities": [{"text": "TS", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.8483132719993591}]}, {"text": "2 TS is often reduced to a sentence-level problem, whereby one sentence is transformed into a simpler version containing one or more sentences.", "labels": [], "entities": [{"text": "2 TS", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.44389915466308594}]}, {"text": "In this paper, we shall make use of the terms source (sentence) and (TS system) output to respectively denote a sentence given as an input to a TS system and the simplified, single or multi-sentence output produced by the system.", "labels": [], "entities": []}, {"text": "TS, seen as a sentence-level problem, is often viewed as a monolingual variant of (sentencelevel) MT.", "labels": [], "entities": [{"text": "TS", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7823945879936218}]}, {"text": "The standard approach to automatic TS evaluation is therefore to view the task as a translation problem and to use machine translation (MT) evaluation metrics such as BLEU ().", "labels": [], "entities": [{"text": "TS evaluation", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.9671967327594757}, {"text": "machine translation (MT) evaluation", "start_pos": 115, "end_pos": 150, "type": "TASK", "confidence": 0.7986076772212982}, {"text": "BLEU", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9972856044769287}]}, {"text": "However, MT evaluation metrics rely on the existence of parallel corpora of source sentences and manually produced reference translations, which are available on a large scale for many language pairs.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.9543337523937225}]}, {"text": "TS datasets are less numerous and smaller.", "labels": [], "entities": [{"text": "TS datasets", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8414106667041779}]}, {"text": "Moreover, they are often automatically extracted from comparable corpora rather than strictly parallel corpora, which results in noisier reference data.", "labels": [], "entities": []}, {"text": "For example, the PWKP dataset ( consists of 100,000 sentences from the English Wikipedia automatically aligned with sentences from the Simple English Wikipedia based on term-based similarity metrics.", "labels": [], "entities": [{"text": "PWKP dataset", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.9664657711982727}, {"text": "Simple English Wikipedia", "start_pos": 135, "end_pos": 159, "type": "DATASET", "confidence": 0.7527833580970764}]}, {"text": "It has been shown by that many of PWKP's \"simplified\" sentences are in fact not simpler or even not related to their corresponding source sentence.", "labels": [], "entities": []}, {"text": "Even if better quality corpora such as Newsela do exist (, they are costly to create, often of limited size, and not necessarily open-access.", "labels": [], "entities": [{"text": "Newsela", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9725474715232849}]}, {"text": "This creates a challenge for the use of referencebased MT metrics for TS evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9358289837837219}, {"text": "TS evaluation", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.9575195908546448}]}, {"text": "However, TS has the advantage of being a monolingual translation-like task, the source being in the same language as the output.", "labels": [], "entities": [{"text": "TS", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9576793909072876}]}, {"text": "This allows for new, nonconventional ways to use MT evaluation metrics, namely by using them to compare the output of a TS system with the source sentence, thus avoiding the need for reference data.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.9058817923069}]}, {"text": "However, such an evaluation method can only capture at most two of the three above-mentioned dimensions, namely meaning preservation and, to a lesser extent, grammaticality.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 112, "end_pos": 132, "type": "TASK", "confidence": 0.8315748274326324}]}, {"text": "Previous works on reference-less TS evaluation includ\u011b, who compare the behaviour of six different MT metrics when used between the source sentence and the corresponding simplified output.", "labels": [], "entities": [{"text": "TS evaluation", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.8717689514160156}, {"text": "MT", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.9558634757995605}]}, {"text": "They evaluate these metrics with respect to meaning preservation and grammaticality.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.8201596736907959}]}, {"text": "We extend their work in two directions.", "labels": [], "entities": []}, {"text": "Firstly, we extend the comparison to include the degree of simplicity achieved by the system.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9922630786895752}]}, {"text": "Secondly, we compare additional features, including those used by\u0160tajnerby\u02c7by\u0160tajner et al.", "labels": [], "entities": []}, {"text": "(2016a), both individually, as elementary metrics, and within multi-feature metrics.", "labels": [], "entities": []}, {"text": "To our knowledge, no previous work has provided as thorough a comparison across such a wide range and combination of features for the reference-less evaluation of TS.", "labels": [], "entities": [{"text": "TS", "start_pos": 163, "end_pos": 165, "type": "TASK", "confidence": 0.9046335220336914}]}, {"text": "First we review available text simplification evaluation methods and traditional quality estimation features.", "labels": [], "entities": [{"text": "text simplification evaluation", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.7920004924138387}]}, {"text": "We then present the QATS shared task and the associated dataset, which we use for our experiments.", "labels": [], "entities": []}, {"text": "Finally we compare all methods in a reference-less setting and analyze the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Although German reference translations were available (Serbian were not, as Serbian is not among the languages investigated at the WMT shared task), using reference translations is not convenient for this type of evaluation since it would penalize too harsh the translations of simplified sentences (especially in the case of syntactic simplification involving sentence splitting and reordering of clauses).", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 131, "end_pos": 146, "type": "TASK", "confidence": 0.5301582018534342}, {"text": "sentence splitting", "start_pos": 361, "end_pos": 379, "type": "TASK", "confidence": 0.7512825429439545}]}, {"text": "The translation outputs were post-edited minimally and the edited translations were used as reference translations to calculate two MT evaluation scores: the character ngram F-score, chrF, and edit distance.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 132, "end_pos": 145, "type": "TASK", "confidence": 0.8363551497459412}, {"text": "edit distance", "start_pos": 193, "end_pos": 206, "type": "METRIC", "confidence": 0.8458033800125122}]}, {"text": "The chrF score operates on sub-word level by matching character sequences, and it correlates very well with human direct assessment scores which are, as mentioned in Section 3.1, based mainly on adequacy and partly on fluency (.", "labels": [], "entities": []}, {"text": "Edit distance represents the amount of words which have to be changed in order to transform the translation output into the reference.", "labels": [], "entities": [{"text": "Edit distance", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.8390419483184814}]}, {"text": "The corpus used for evaluation contains literary and scientific texts usually read in classrooms at primary levels (children aged 7 to 9 years old) in France.", "labels": [], "entities": []}, {"text": "Within the 187 sentences of the corpus, experts have identified 190 complex words that have to be simplified (thus, we have an average of 1 complex word per sentence).", "labels": [], "entities": []}, {"text": "In this paper, the simplifications have been evaluated according to their complexity and the context where they appear.", "labels": [], "entities": []}, {"text": "Three substitutes are proposed for every complex word.", "labels": [], "entities": []}, {"text": "The provided simplifications were assessed by two evaluators following these instructions: \u2022 The substitute must be simpler than the complex word \u2022 The substitute must fit the context of the sentence \u2022 If the complex word appears as a substitute, it is invalidated The table 2 shows examples of the evaluation.", "labels": [], "entities": []}, {"text": "Beau and fort do not match the context.", "labels": [], "entities": [{"text": "Beau", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9285146594047546}, {"text": "fort", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.8326352834701538}]}, {"text": "We computed the inter-rater reliability of the human annotation.", "labels": [], "entities": [{"text": "reliability", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.6357871890068054}]}, {"text": "Even though selecting good candidates for simplification has been regarded as a complex task for human, we obtained a \u03ba of 0.625 for the baseline model and a \u03ba of 0.656 for the annotation of ReSyf's results.", "labels": [], "entities": []}, {"text": "Based on this human annotation, we have computed two evaluation metrics.", "labels": [], "entities": []}, {"text": "Precision1 is a global precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.976200520992279}]}, {"text": "Every simplification is considered as an independent sentence.", "labels": [], "entities": []}, {"text": "This measure aims to calculate the number of valid simplifications among all the provided ones (i.e simplified sentences).", "labels": [], "entities": []}, {"text": "Precision2 allows us to verify, for an initial sentence if, at least, one valid simplification appears among the three proposed ones.", "labels": [], "entities": []}, {"text": "For each original sentence, only one valid simplification is counted, even if there are two or three valid ones.", "labels": [], "entities": []}, {"text": "If none of the three simplifications are correct, then the count is 0.", "labels": [], "entities": []}, {"text": "This measure counts the number of initial sentences that have at least one valid simplification.", "labels": [], "entities": []}, {"text": "By analysing the simplifications produced by the baseline and our method, table 3 shows that ReSyf provides better results than Word2Vec techniques.", "labels": [], "entities": []}, {"text": "In table 3, Precision1 and Precision2 count valid simplifications only if both of the annotators agree on the proposed substitute.", "labels": [], "entities": []}, {"text": "This table also shows that our method produces more suitable simplifications (16.3% and 51.9%) than the baseline (15.7% and 49.4%).", "labels": [], "entities": []}, {"text": "2.1 Using MT metrics to compare the output and a reference TS can be considered as a monolingual translation task.", "labels": [], "entities": []}, {"text": "As a result, MT metrics such as BLEU (), which compare the output of an MT system to a reference translation, have been extensively used for TS).", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.975283682346344}, {"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9986199140548706}, {"text": "TS", "start_pos": 141, "end_pos": 143, "type": "TASK", "confidence": 0.9836033582687378}]}, {"text": "Other successful MT metrics include TER (), ROUGE) and METEOR (), but they have not gained much traction in the TS literature.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.979198694229126}, {"text": "TER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9968031644821167}, {"text": "ROUGE", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9951915740966797}, {"text": "METEOR", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9901493787765503}, {"text": "TS", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.8341436386108398}]}, {"text": "These metrics rely on good quality references, something which is often not available in TS, as discussed by.", "labels": [], "entities": [{"text": "TS", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.5229742527008057}]}, {"text": "Moreover, \u02c7 Stajner et al. and showed that using BLEU to compare the system output with a reference is not a good way to perform TS evaluation, even when good quality references are available.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9951453804969788}, {"text": "TS evaluation", "start_pos": 129, "end_pos": 142, "type": "TASK", "confidence": 0.9429205358028412}]}, {"text": "This is especially true when the TS system produces more than one sentence fora single source sentence.", "labels": [], "entities": [{"text": "TS", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.8651260137557983}]}, {"text": "Evaluation of elementary metrics We rank all features by comparing their behaviour with human judgments on the training set.", "labels": [], "entities": []}, {"text": "We first compute for each elementary metric the Pearson correlation between its results and the manually assigned labels for each of the three dimensions.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 48, "end_pos": 67, "type": "METRIC", "confidence": 0.9587450623512268}]}, {"text": "We then rank our elementary metrics according to the absolute value of the Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 75, "end_pos": 94, "type": "METRIC", "confidence": 0.7180752754211426}]}, {"text": "6 Training and evaluation of a combined metric We use our elementary metrics as features to train classifiers on the training set, and evaluate their performance on the test set.", "labels": [], "entities": []}, {"text": "We therefore scale them and reduce the dimensionality with a 25-component PCA 7 , then train several regression algorithms 8 and classification algorithms 9 using scikit-learn).", "labels": [], "entities": []}, {"text": "For each dimension, we keep the two models performing best on the test set and add them in the leaderboard of the QATS shared task, naming them with the name of the regression algorithm they were built with.", "labels": [], "entities": []}, {"text": "Grammaticality N -gram based MT metrics have the highest correlation with human grammaticality judgments.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.7601305842399597}]}, {"text": "METEOR seems to be the best, probably because of its robustness to synonymy, followed by smoothed BLEU (BLEUSmoothed in 2).", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.85378497838974}, {"text": "smoothed", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9414947628974915}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9223803281784058}, {"text": "BLEUSmoothed", "start_pos": 104, "end_pos": 116, "type": "METRIC", "confidence": 0.9936892986297607}]}, {"text": "This indicates that relevant grammaticality information can be derived from the source sentence.", "labels": [], "entities": []}, {"text": "We were expecting that information contained in a language model would help achieving better results (AvgLMProbsOutput), but MT metrics correlate better with human judgments.", "labels": [], "entities": [{"text": "AvgLMProbsOutput", "start_pos": 102, "end_pos": 118, "type": "METRIC", "confidence": 0.9502686858177185}, {"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9595685601234436}]}, {"text": "We deduce that the grammaticality information contained in the source is more specific and more helpful for evaluation than what is learned by the language model.", "labels": [], "entities": []}, {"text": "We will release our code on github.", "labels": [], "entities": []}, {"text": "We used PCA instead of feature selection because it performed better on the validation set.", "labels": [], "entities": []}, {"text": "The number of component was tuned on the validation set as well.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Size of the reference data and their consensual alignment at the level of sentences.", "labels": [], "entities": []}, {"text": " Table 1: Results for the two versions of the texts (original O and simplified S) and their statistical  analysis. The indicators are the following: training text TRN and control text CRL; duration of the first  fixation DFF, total number of fixations TNF, amplitude of saccades AMP, number of regressions REG;  answers to questions MCQ. Statistically significant p is marked with bold characters.", "labels": [], "entities": [{"text": "TRN", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.594931423664093}, {"text": "duration of the first  fixation DFF", "start_pos": 189, "end_pos": 224, "type": "METRIC", "confidence": 0.8409513632456461}, {"text": "TNF", "start_pos": 252, "end_pos": 255, "type": "METRIC", "confidence": 0.5385527610778809}, {"text": "amplitude of saccades AMP", "start_pos": 257, "end_pos": 282, "type": "METRIC", "confidence": 0.8375333547592163}, {"text": "REG", "start_pos": 306, "end_pos": 309, "type": "METRIC", "confidence": 0.886795163154602}]}, {"text": " Table 2: Evaluation example for simplified versions of the sentence Le castor est un excellent nageur.", "labels": [], "entities": []}, {"text": " Table 3: LS evaluation result of annotators 1 and 2", "labels": [], "entities": []}, {"text": " Table 2: Brief description of 30 of our most relevant elementary metrics", "labels": [], "entities": []}, {"text": " Table 3: Pearson correlation with human judgments of elementary metrics ranked by absolute value on  training set (15 best metrics for each dimension).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8344006538391113}]}, {"text": " Table 4: QATS leaderboard. Results in bold are our additions to the original leaderboard. We only select  the two models that rank highest during cross-validation.", "labels": [], "entities": [{"text": "QATS leaderboard", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.7005911320447922}]}, {"text": " Table 4: Distribution of simplification quality scores (with  meaning preservation as the primary criterion, and grammat- icality as the secondary).", "labels": [], "entities": []}, {"text": " Table 5: chrF / edit rate for Serbian and German translations  of all original English sentences and all their automatic sim- plifications (higher chrF scores and lower edit rates indicate  better translations).", "labels": [], "entities": [{"text": "edit rate", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9667387306690216}, {"text": "Serbian and German translations  of all original English sentences", "start_pos": 31, "end_pos": 97, "type": "TASK", "confidence": 0.6779257025983598}]}, {"text": " Table 6: chrF score / edit rate for translations of good, and  bad simplifications of English sentences into Serbian and into  German. For each group, better scores are presented in bold.", "labels": [], "entities": [{"text": "edit rate", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9475996196269989}]}, {"text": " Table 8: Number / percentage of improved, deteriorated and  unchanged machine translated sentences in terms of the chrF  score (above) and edit rate (below). Results for translations  of correct (good and corrected) simplifications are presented  in bold.", "labels": [], "entities": [{"text": "chrF  score", "start_pos": 116, "end_pos": 127, "type": "METRIC", "confidence": 0.9422905445098877}, {"text": "edit rate", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9901267886161804}]}, {"text": " Table 10: Three classes of edit rates (inflectional, order- ing and lexical) for improved and deteriorated translations  when translating good and corrected simplifications. For each  group, better scores are presented in bold.", "labels": [], "entities": []}, {"text": " Table 11: Number of source sentences whose simplification  improves/deteriorates/does not change the MT scores for dif- ferent MT systems. The numbers in parentheses denote the  number of corresponding sentences in each of the two in- volved translation outputs.", "labels": [], "entities": [{"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9257122278213501}]}]}