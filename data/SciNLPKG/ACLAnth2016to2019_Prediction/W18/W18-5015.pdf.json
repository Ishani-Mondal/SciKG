{"title": [{"text": "Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog", "labels": [], "entities": [{"text": "Multimodal Hierarchical Reinforcement Learning", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.6525414139032364}]}], "abstractContent": [{"text": "Creating an intelligent conversational system that understands vision and language is one of the ultimate goals in Artificial Intelligence (AI) (Winograd, 1972).", "labels": [], "entities": []}, {"text": "Extensive research has focused on vision-to-language generation, however, limited research has touched on combining these two modalities in a goal-driven dialog context.", "labels": [], "entities": [{"text": "vision-to-language generation", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.7400092780590057}]}, {"text": "We propose a multimodal hierarchical reinforcement learning framework that dynamically integrates vision and language for task-oriented visual dialog.", "labels": [], "entities": []}, {"text": "The framework jointly learns the multimodal dialog state representation and the hierarchical dialog policy to improve both dialog task success and efficiency.", "labels": [], "entities": []}, {"text": "We also propose anew technique, state adaptation, to integrate context awareness in the dialog state representation.", "labels": [], "entities": [{"text": "state adaptation", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7623561918735504}]}, {"text": "We evaluate the proposed framework and the state adaptation technique in an image guessing game and achieve promising results.", "labels": [], "entities": [{"text": "state adaptation", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7581778168678284}]}], "introductionContent": [{"text": "The interplay between vision and language has created a range of interesting applications, including image captioning, visual question generation (VQG) (, visual question answering (VQA) (, and reference expressions (.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.740273505449295}, {"text": "visual question generation (VQG)", "start_pos": 119, "end_pos": 151, "type": "TASK", "confidence": 0.7766152322292328}, {"text": "visual question answering (VQA)", "start_pos": 155, "end_pos": 186, "type": "TASK", "confidence": 0.7766144225994746}]}, {"text": "Visual dialog () extends the VQA problem to multi-turn visual-grounded conversations without specific goals.", "labels": [], "entities": []}, {"text": "In this paper, we study the task-oriented visual dialog setting that requires the agent to learn the multimodal representation and dialog policy for decision making.", "labels": [], "entities": [{"text": "decision making", "start_pos": 149, "end_pos": 164, "type": "TASK", "confidence": 0.7804762423038483}]}, {"text": "We argue that a task-oriented visual intelligent conversational system should not only acquire vision and language understanding but also make appropriate decisions efficiently in a situated environment.", "labels": [], "entities": []}, {"text": "Specifically, we designed a 20 images guessing game using the Visual Dialog dataset ().", "labels": [], "entities": [{"text": "Visual Dialog dataset", "start_pos": 62, "end_pos": 83, "type": "DATASET", "confidence": 0.9043414990107218}]}, {"text": "This game is the visual analog of the popular 20 question game.", "labels": [], "entities": []}, {"text": "The agent aims to learn a dialog policy that can guess the correct image through question answering using the minimum number of turns.", "labels": [], "entities": [{"text": "question answering", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7209770828485489}]}, {"text": "Previous work on visual dialogs () focused mainly on vision-to-language understanding and generation instead of dialog policy learning.", "labels": [], "entities": [{"text": "vision-to-language understanding and generation", "start_pos": 53, "end_pos": 100, "type": "TASK", "confidence": 0.7477022856473923}]}, {"text": "They let an agent ask a fixed number of questions to rank the images or let humans make guesses at the end of the conversations.", "labels": [], "entities": []}, {"text": "However, such setting is not realistic in real-world task-oriented applications, because in task-oriented applications, not only completing the task successfully is important but also completing it efficiently.", "labels": [], "entities": []}, {"text": "In addition, the agent should also be informed of the wrong guesses, so that it becomes more aware of the vision context.", "labels": [], "entities": []}, {"text": "However, solving such real-world setting is a challenge.", "labels": [], "entities": []}, {"text": "The system needs to handle the large dynamically updated multimodal stateaction space and also leverage the signals in the feedback loop coming from different sub-tasks.", "labels": [], "entities": []}, {"text": "We propose a multimodal hierarchical reinforcement learning framework that allows learning visual dialog state tracking and dialog policy jointly to complete visual dialog tasks efficiently.", "labels": [], "entities": [{"text": "learning visual dialog state tracking", "start_pos": 82, "end_pos": 119, "type": "TASK", "confidence": 0.6332926988601685}]}, {"text": "The framework we propose takes inspiration from feudal reinforcement learning (FRL), where levels of hierarchy within an agent communicate via explicit goals in a topdown fashion.", "labels": [], "entities": [{"text": "feudal reinforcement learning (FRL)", "start_pos": 48, "end_pos": 83, "type": "TASK", "confidence": 0.7366196165482203}]}, {"text": "In our case, it decomposes the decision into two steps: a first step where a master policy selects between verbal task (information query) and vision task (image retrieval), and a second step where a primitive action (question or im-age) is chosen from the selected task.", "labels": [], "entities": []}, {"text": "Hierarchical RL that relies on space abstraction, such as FRL, is useful to address the challenge of large discrete action space and has been shown to be effective in dialog systems, especially for large domain dialog management.", "labels": [], "entities": [{"text": "Hierarchical RL", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5324814021587372}, {"text": "FRL", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.6482678055763245}, {"text": "large domain dialog management", "start_pos": 198, "end_pos": 228, "type": "TASK", "confidence": 0.6695192754268646}]}, {"text": "Besides, we propose anew technique called state adaptation in order to make the multimodal dialog state more aware of the constantly changing visual context.", "labels": [], "entities": [{"text": "state adaptation", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7177980691194534}]}, {"text": "We demonstrate the efficacy of this technique through ablation analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first describe the simulation of the environment.", "labels": [], "entities": []}, {"text": "Then, we talk about different dialog policy models and implementation details.", "labels": [], "entities": []}, {"text": "Finally, we discuss three different experimental settings to evaluate the proposed framework.", "labels": [], "entities": []}, {"text": "We conduct three sets of experiments to explore the effectiveness of the proposed multimodal hierarchical reinforcement learning framework in a real-world scenario step by step.", "labels": [], "entities": []}, {"text": "The first experiment constrains the agent to select among the 10 human generated question-answer pairs.", "labels": [], "entities": []}, {"text": "This setting enables us to assess the effectiveness of the framework in a less error-prone setting.", "labels": [], "entities": []}, {"text": "The second experiment does not require a human to generate the answer to emulate a more realistic environment.", "labels": [], "entities": []}, {"text": "Specifically, we enlarge the number of questions by including 200 human generated questions for the 20 images, and use a pre-trained visual question answer model to generate answers with respect to the target image.", "labels": [], "entities": []}, {"text": "In the last experiment, we further automate the process by generating questions given the 20 images using a pretrained visual question generation model.", "labels": [], "entities": []}, {"text": "So the agent does not require any human input with respect to any image for training.", "labels": [], "entities": []}, {"text": "The agent selects the next question among the 10 question-answer pairs human generated and want to identify the targeted image accurately and efficiently through natural language conversation.", "labels": [], "entities": []}, {"text": "We terminate the dialog after ten turns.", "labels": [], "entities": []}, {"text": "Each model's performance is shown in.", "labels": [], "entities": []}, {"text": "HRL+SAR achieves the best win rate with statistical significance.", "labels": [], "entities": [{"text": "HRL+SAR", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6740683714548746}, {"text": "win rate", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.952727347612381}, {"text": "statistical significance", "start_pos": 40, "end_pos": 64, "type": "METRIC", "confidence": 0.9313723742961884}]}, {"text": "The HRL+SAR policy model performs much better than methods without hierarchical control structure and state adaptation.", "labels": [], "entities": []}, {"text": "The learning curves in and 5 reveal that the HRL+SAR converges faster.", "labels": [], "entities": [{"text": "HRL+SAR", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.5403141876061758}]}, {"text": "We further perform bootstrap tests by resampling the game results from each experiment with replacement 1,000 times.", "labels": [], "entities": []}, {"text": "Then we calculate the probability of significance level for the difference of average win rates or average turn length to check whether the relative performance improvement from the last baseline is statistically significant.", "labels": [], "entities": [{"text": "significance level", "start_pos": 37, "end_pos": 55, "type": "METRIC", "confidence": 0.9022625982761383}]}, {"text": "The result shows that the question selection (DRRN) and state adaptation bring the most significant performance improvements (p < 0.01) while reward shaping has less impact (p < 0.05).", "labels": [], "entities": [{"text": "state adaptation", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.6873447448015213}, {"text": "reward shaping", "start_pos": 142, "end_pos": 156, "type": "TASK", "confidence": 0.6749299913644791}]}, {"text": "We also observe that the average number of turns with hierarchical policy learning (HRL) is slightly longer than that of Rnd+DQN but with less statistically significant difference.", "labels": [], "entities": []}, {"text": "This is probably because this setting provides the 10 predefined question-answer pairs with a smaller action space, the DQN model tends to encourage the agent to make guesses quicker, while policy models with hierarchical structures tends to optimize the overall task completion rate.", "labels": [], "entities": []}, {"text": "***(p < 0.01), **(p < 0.05) and *(p < 0.1): Model Performance in Experiment 1 Figure 3: Learning curves of win rates for five different policy policies in Experiment 1 We find that RL methods significantly improve the win rate as they learn to select the optimal list of questions to ask.", "labels": [], "entities": []}, {"text": "We also observe that our proposed state adaptation method for vision context state helps achieve the largest performance improvement.", "labels": [], "entities": []}, {"text": "The hierarchical control architecture and the state abstraction sharing) also improve both learning speed and agent performance.", "labels": [], "entities": []}, {"text": "This aligns with the observation in.", "labels": [], "entities": []}, {"text": "Moreover, on average, we observe that after seven turns, the agent was able to select the target image with a sufficiently high success rate.", "labels": [], "entities": []}, {"text": "We further explore if the proposed hierarchical framework enables efficient decision-making when compared to the agent that keeps asking questions and only makes the guess at the end of the dialog.", "labels": [], "entities": []}, {"text": "We refer to such models as the oracle baselines.", "labels": [], "entities": []}, {"text": "For example, the Oracle@7 makes the guess at the 7th turn based on the previous dialog history with the correct order of questionanswer pairs in the dataset.", "labels": [], "entities": []}, {"text": "The oracle baselines are strong, since they represent the best performance the model can get given the optimal question order provided by human.", "labels": [], "entities": []}, {"text": "shows the performance of the oracle baselines with various fixed turns.", "labels": [], "entities": []}, {"text": "We performed significance tests between each oracle baseline and the hierarchical framework.", "labels": [], "entities": []}, {"text": "Since our hierarchical framework requires on average 7.22 turns to complete, so we compared it with Oracle@7 and Oracle@8.", "labels": [], "entities": []}, {"text": "We found that the proposed method outperforms Oracle@7 with p \u2212 value < 0.01, and similar to Oracle@8 (significant difference (p \u2212 value > 0.1).", "labels": [], "entities": []}, {"text": "The reason that the hierarchical framework can outperform Oracle@7 is that it learns to make a guess whenever the agent is confident enough, therefore achieving better win rate.", "labels": [], "entities": []}, {"text": "Oracle@8 in general receives more information as the dialogs are longer, therefore has an advantage over the hierarchical method.", "labels": [], "entities": []}, {"text": "However, it still performs similar to the proposed method, which demonstrates that by learning the hierarchical decision, it enables the agent to achieve the goal more efficiently.", "labels": [], "entities": []}, {"text": "One thing we need to point out is that the proposed method also received extra information about whether the guess is corrector not from the environment.", "labels": [], "entities": []}, {"text": "Oracle baselines do not have such information, as it can only make a guess at the end of the dialog.", "labels": [], "entities": []}, {"text": "Oracle@9 and @10 are better than the hierarchical framework statistically, because they acquire much more information by having longer turns.", "labels": [], "entities": []}, {"text": "To make the experimental setting more realistic, we select 200 questions generated by a human with respect to 20 images provided and create a user simulator that generates the answers related to the target image.", "labels": [], "entities": []}, {"text": "Here, as the questions space is larger, we terminate the dialog after 20 turns.", "labels": [], "entities": []}, {"text": "We follow the supervised training scheme discussed in ( to train the visual question generation module offline.", "labels": [], "entities": [{"text": "visual question generation", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.682526558637619}]}, {"text": "indicate that HRL+SAR significantly outperforms Rnd and Rnd+DQN in both win rate and average number of dialog turns.", "labels": [], "entities": [{"text": "HRL+SAR", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.749026874701182}, {"text": "win rate", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9615764617919922}]}, {"text": "The setting in Experiment 2 is more challenging than that of Experiment 1, because the visual ques-tion module introduces noise that can influence the policy learning.", "labels": [], "entities": [{"text": "policy learning", "start_pos": 151, "end_pos": 166, "type": "TASK", "confidence": 0.859550416469574}]}, {"text": "However, the noise also simulates the real-world scenario that a user might have an implicit goal that may change within the task.", "labels": [], "entities": []}, {"text": "A user can also accidentally make errors in answering the question.", "labels": [], "entities": []}, {"text": "The proposed hierarchical framework (HRL+SAR) with state adaptation and reward shaping achieves the best win rate and the least number of dialog turns in this noisy experiment setting.", "labels": [], "entities": []}, {"text": "As compared to Experiment 1, the policy models with hierarchical structures can both optimize the overall task completion rate and the dialog turns.", "labels": [], "entities": []}, {"text": "We did not report oracle baselines results, since the oracle order of all the questions (ideally generated by humans) was not available.", "labels": [], "entities": []}, {"text": "In this setting, both questions and answers are generated automatically through pre-trained visual question and answer generation models ().", "labels": [], "entities": []}, {"text": "Such setting enables the agent to play the guessing game given any image as no human input of the image is needed.", "labels": [], "entities": []}, {"text": "Notice that the answers should be generated with respect to a target image for our task setting.", "labels": [], "entities": []}, {"text": "In this setting, we also set the maximum number of dialog turns to be 20.", "labels": [], "entities": []}, {"text": "The results in show that the performance of the three policies significantly dropped compared to Experiment 2.", "labels": [], "entities": []}, {"text": "Such observation is expected, as the noise coming from both the visual question and answer generation module increases the task difficulty.", "labels": [], "entities": [{"text": "visual question and answer generation", "start_pos": 64, "end_pos": 101, "type": "TASK", "confidence": 0.6918630301952362}]}, {"text": "However, the proposed HRL+SAR is still more resilient to the noise and achieves a higher win rate and less average number of turns compared to other baselines. from the Appendix shows that in Experiment 2 the agent tends select relevant questions faster to ask although the answers can be misleading.", "labels": [], "entities": [{"text": "HRL+SAR", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.48605971535046893}, {"text": "win rate", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9725793898105621}, {"text": "Appendix", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9690359830856323}]}, {"text": "On the other hand, in Experiment 3, the agent reacts to the generated question and answers slower to complete the task.", "labels": [], "entities": []}, {"text": "The model performance decreases when we increase the task difficulty in order to emulate the real-world scenarios.", "labels": [], "entities": []}, {"text": "It hints that there is a possible limitation of using the VisDial dataset, because the dialog is constructed by users who casually talk about MS COCO images ) instead of exchanging with an explicit contextual goal in the dialog.", "labels": [], "entities": [{"text": "VisDial dataset", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.9212826788425446}]}], "tableCaptions": [{"text": " Table 1: Model Performance in Experiment 1", "labels": [], "entities": []}, {"text": " Table 2: Oracle baselines Performance", "labels": [], "entities": [{"text": "Oracle baselines", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.8808957040309906}]}, {"text": " Table 3: Model Performance in Experiment 2", "labels": [], "entities": []}, {"text": " Table 4: Model Performance in Experiment 3", "labels": [], "entities": []}]}