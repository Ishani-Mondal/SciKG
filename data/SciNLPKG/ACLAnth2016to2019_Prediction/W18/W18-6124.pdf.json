{"title": [{"text": "Using Author Embeddings to Improve Tweet Stance Classification", "labels": [], "entities": [{"text": "Improve Tweet Stance", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.7036658426125845}]}], "abstractContent": [{"text": "Many social media classification tasks analyze the content of a message, but do not consider the context of the message.", "labels": [], "entities": [{"text": "social media classification", "start_pos": 5, "end_pos": 32, "type": "TASK", "confidence": 0.7564468582471212}]}, {"text": "For example , in tweet stance classification-where a tweet is categorized according to a viewpoint it espouses-the expressed viewpoint depends on latent beliefs held by the user.", "labels": [], "entities": [{"text": "tweet stance classification-where a tweet", "start_pos": 17, "end_pos": 58, "type": "TASK", "confidence": 0.8104312419891357}]}, {"text": "In this paper we investigate whether incorporating knowledge about the author can improve tweet stance classification.", "labels": [], "entities": [{"text": "tweet stance classification", "start_pos": 90, "end_pos": 117, "type": "TASK", "confidence": 0.9093493620554606}]}, {"text": "Furthermore , since author information and embed-dings are often unavailable for labeled training examples, we propose a semi-supervised pre-training method to predict user embeddings.", "labels": [], "entities": []}, {"text": "Although the neural stance classifiers we learn are often outperformed by a baseline SVM, author embedding pre-training yields improvements over a non-pre-trained neural network on four out of five domains in the SemEval 2016 6A tweet stance classification task.", "labels": [], "entities": [{"text": "SemEval 2016 6A tweet stance classification task", "start_pos": 213, "end_pos": 261, "type": "TASK", "confidence": 0.8271467174802508}]}, {"text": "Ina tweet gun control stance classification dataset, improvements from pre-training are only apparent when training data is limited.", "labels": [], "entities": [{"text": "gun control stance classification", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.5683223083615303}]}], "introductionContent": [{"text": "Social media analyses often rely on a tweet classification step to produce structured data for analysis, including tasks such as sentiment) and stance () classification.", "labels": [], "entities": [{"text": "tweet classification", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.7503300905227661}, {"text": "sentiment) and stance () classification", "start_pos": 129, "end_pos": 168, "type": "TASK", "confidence": 0.6733117997646332}]}, {"text": "Common approaches feed the text of each message to a classifier which predicts a label based on the content of the tweet.", "labels": [], "entities": []}, {"text": "However, many of these tasks benefit from knowledge about the context of the message, especially since short messages can be difficult to understand.", "labels": [], "entities": []}, {"text": "One of the best sources of context is the message author herself.", "labels": [], "entities": []}, {"text": "Consider the task of stance classification, where a system must identify the stance towards a topic expressed in a tweet.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.9085987508296967}]}, {"text": "Having access to the latent beliefs of the tweet's author would provide a strong prior as to their expressed stance, e.g. general political leanings provide a prior for their statement on a divisive political issue.", "labels": [], "entities": []}, {"text": "Therefore, we propose providing user level information to classification systems to improve classification accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 92, "end_pos": 106, "type": "TASK", "confidence": 0.9533400535583496}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.8865385055541992}]}, {"text": "One of the challenges with accessing this type of information on social media users, and Twitter users in particular, is that it is not provided by the platform.", "labels": [], "entities": []}, {"text": "While political leanings maybe helpful, they are not directly contained in metadata or user provided information.", "labels": [], "entities": []}, {"text": "Furthermore, it is unclear which categories of information will best inform each classification task.", "labels": [], "entities": []}, {"text": "While information about the user maybe helpful in general, what information is relevant to each task maybe unknown.", "labels": [], "entities": []}, {"text": "We propose to represent users based on their online activity as low-dimensional embeddings, and provide these embeddings to the classifier as context fora tweet.", "labels": [], "entities": []}, {"text": "Since a deployed classifier will likely encounter many new users for which we do not have embeddings, we use the user embeddings as a mechanism for pre-training the classification model.", "labels": [], "entities": []}, {"text": "By pre-training the model to be predictive of user information, the classifier can better generalize to new tweets.", "labels": [], "entities": []}, {"text": "This pre-training can be performed on a separate, unlabeled set of tweets and user embeddings, creating flexibility in which tasks can be improved by using this method.", "labels": [], "entities": []}, {"text": "Additionally, we find that this training scheme is most beneficial in low-data settings, further reducing the resource requirement for training new classifiers.", "labels": [], "entities": []}, {"text": "Although semi-supervised approaches to social media stance classification are not new, they have only been performed at the message-levelpredicting held-out hashtags from a tweet for example (.", "labels": [], "entities": [{"text": "social media stance classification", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.7029184997081757}]}, {"text": "Our approach leverages additional user information that may not be contained in a single message.", "labels": [], "entities": []}, {"text": "We evaluate our approach on two stance clas-sification datasets: 1) the SemEval 2016 task of stance classification) and 2) anew gun related Twitter data set that contains messages about gun control and gun rights.", "labels": [], "entities": [{"text": "SemEval 2016 task of stance classification", "start_pos": 72, "end_pos": 114, "type": "TASK", "confidence": 0.6159850607315699}, {"text": "gun related Twitter data set", "start_pos": 128, "end_pos": 156, "type": "DATASET", "confidence": 0.7686046361923218}]}, {"text": "On both datasets, we compare the benefit of pretraining a neural stance classifier to predict user embeddings derived from different types of online user activity: recent user messages, their friend network, and a multiview embedding of both of these views.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider two different tweet stance classification datasets, which provide six domains of English language Twitter data in total.", "labels": [], "entities": [{"text": "tweet stance classification", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.7271166642506918}]}, {"text": "SemEval 2016 Task 6A (Tweet Stance Classification) This is a collection of 2,814 training and 1,249 test set tweets that are about one of five politically-charged targets: Atheism, the Feminist Movement, Climate Change is a Real Concern, Legalization of Abortion, or Hillary Clinton.", "labels": [], "entities": [{"text": "SemEval 2016 Task 6A", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.6937561258673668}, {"text": "Legalization of Abortion", "start_pos": 238, "end_pos": 262, "type": "TASK", "confidence": 0.83887646595637}]}, {"text": "Given the text of a tweet and a target, models must classify the tweet as either FAVOR or AGAINST, or NEITHER if the tweet does not express support or opposition to the target topic.", "labels": [], "entities": [{"text": "FAVOR", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9951267242431641}, {"text": "AGAINST", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9627372622489929}, {"text": "NEITHER", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.9913017749786377}]}, {"text": "Participants strug-gled with this shared task, as it was especially difficult due to imbalanced class sizes, small training sets, short examples, and tweets where the target was not explicitly mentioned.", "labels": [], "entities": []}, {"text": "See fora thorough description of this data.", "labels": [], "entities": []}, {"text": "We report model performance on the provided test set for each topic and perform four-fold CV on the training set for model selection . Guns Our second stance dataset is a collection of tweets related to guns.", "labels": [], "entities": []}, {"text": "Tweets were collected from the Twitter keyword streaming API starting in December 2012 and throughout 2013 . The collection includes all tweets containing guns-related keyphrases, subject to rate limits.", "labels": [], "entities": []}, {"text": "We labeled tweets based on their stance towards gun control: FAVOR was supportive of gun control, AGAINST was supportive of gun rights.", "labels": [], "entities": [{"text": "gun control", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.7538379430770874}, {"text": "FAVOR", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.883444607257843}, {"text": "gun control", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.6983345299959183}, {"text": "AGAINST", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9701377153396606}]}, {"text": "We automatically identified the stance to create labels based on commonly occurring hashtags that were clearly associated with one of these positions (see.1 fora list of keywords and hashtags).", "labels": [], "entities": []}, {"text": "Tweets which contained hashtags from both sets or contained no stance-bearing hashtags were excluded from our data.", "labels": [], "entities": []}, {"text": "We constructed stratified samples from 26,608 labeled tweets in total.", "labels": [], "entities": []}, {"text": "Of these, we sampled 50, 100, 500, and 1,000 examples from each class, five times, to construct five small, balanced training sets.", "labels": [], "entities": []}, {"text": "We then divided the remaining examples equally between development and test sets in each case.", "labels": [], "entities": []}, {"text": "Model performance for each number of examples was macro-averaged over the five training sets.", "labels": [], "entities": []}, {"text": "The hashtags used to assign class labels were removed from the training examples as a preprocessing step.", "labels": [], "entities": []}, {"text": "We constructed this dataset for two reasons.", "labels": [], "entities": []}, {"text": "First, it allows us to compare model performance as a function of training set size.", "labels": [], "entities": []}, {"text": "Second, we are able to pre-train on user embeddings for the same set of users that are annotated with stance.", "labels": [], "entities": []}, {"text": "The SemEval-released dataset does not provide status or user IDs from which we could use to collect and build user embeddings.", "labels": [], "entities": [{"text": "SemEval-released dataset", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.7638441622257233}]}, {"text": "We considered two unlabeled datasets as a source for constructing user embeddings for model pretraining.", "labels": [], "entities": []}, {"text": "Due to data limitations, we were unable to create all of our embedding models for all available datasets.", "labels": [], "entities": []}, {"text": "We describe below which embeddings were created for which datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Positive/negative class macro-averaged F1 model test performance at SemEval 2016 Task 6A. The final  column is macro-averaged F1 across all domains. \u2666 means model performance is significantly better than a non- pre-trained RNN, is worse than SVM, and \u2660 is better than tweet-level hashtag prediction pre-training (RNN-", "labels": [], "entities": [{"text": "SemEval 2016 Task 6A", "start_pos": 78, "end_pos": 98, "type": "DATASET", "confidence": 0.6571790874004364}, {"text": "F1", "start_pos": 136, "end_pos": 138, "type": "METRIC", "confidence": 0.9588000774383545}]}, {"text": " Table 3: Model test accuracy at predicting gun stance.  RNNs were pre-trained on either the guns-related pre- training set (GUNSET) or the general user pre-training  set (GENSET). The best-performing neural model is  bolded. indicates that the model performs signifi- cantly worse than the SVM baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9952203631401062}, {"text": "predicting gun stance", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.8368405302365621}]}, {"text": " Table 4: Test accuracy of an SVM at predicting gun  control stance based on guns-related keyphrase distri- bution (KEY), user's Author Text embedding (TEXT),  and word and character n-gram features (TWEET).  means a model is significantly worse than TWEET and  \u2663 means the feature set is significantly better than", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9964297413825989}, {"text": "predicting gun  control stance", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.8457669019699097}]}]}