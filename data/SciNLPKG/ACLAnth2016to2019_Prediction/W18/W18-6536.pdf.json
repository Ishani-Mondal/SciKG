{"title": [{"text": "Neural Generation of Diverse Questions using Answer Focus, Contextual and Linguistic Features", "labels": [], "entities": [{"text": "Neural Generation of Diverse Questions", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8562760472297668}]}], "abstractContent": [{"text": "Question Generation is the task of automatically creating questions from tex-tual input.", "labels": [], "entities": [{"text": "Question Generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7532637715339661}]}, {"text": "In this work we present anew Attentional Encoder-Decoder Recurrent Neural Network model for automatic question generation.", "labels": [], "entities": [{"text": "automatic question generation", "start_pos": 92, "end_pos": 121, "type": "TASK", "confidence": 0.6013087232907613}]}, {"text": "Our model incorporates linguistic features and an additional sentence embedding to capture meaning at both sentence and word levels.", "labels": [], "entities": []}, {"text": "The linguistic features are designed to capture information related to named entity recognition , word case, and entity coreference resolution.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.6254194378852844}, {"text": "word case", "start_pos": 98, "end_pos": 107, "type": "TASK", "confidence": 0.7506250739097595}, {"text": "entity coreference resolution", "start_pos": 113, "end_pos": 142, "type": "TASK", "confidence": 0.7855396668116251}]}, {"text": "In addition our model uses a copying mechanism and a special answer signal that enables generation of numerous diverse questions on a given sentence.", "labels": [], "entities": []}, {"text": "Our model achieves state of the art results of 19.98 Bleu 4 on a benchmark Question Generation dataset, outperforming all previously published results by a significant margin.", "labels": [], "entities": [{"text": "Bleu 4", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9576992094516754}, {"text": "Question Generation dataset", "start_pos": 75, "end_pos": 102, "type": "DATASET", "confidence": 0.8872995575269064}]}, {"text": "A human evaluation also shows that the added features improve the quality of the generated questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question Generation (QG) is the task of automatically generating questions from textual input (.", "labels": [], "entities": [{"text": "Question Generation (QG)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8207577645778656}]}, {"text": "There area wide variety of question types and forms, e.g., short answer, open ended, multiple choice, and gap questions, each require a different approach to generate.", "labels": [], "entities": []}, {"text": "One distinguishing aspect of a QG system is the type of questions that it produces.", "labels": [], "entities": []}, {"text": "This paper focuses on the generation of factoid short answer questions, i.e., questions that can be answered by a single short phrase, usually appearing directly in the input text.", "labels": [], "entities": [{"text": "generation of factoid short answer questions", "start_pos": 26, "end_pos": 70, "type": "TASK", "confidence": 0.6907960375150045}]}, {"text": "The work of a QG system typically consists of three conceptual subtasks: Target Selection, Question Representation Construction, and Question Realization.", "labels": [], "entities": [{"text": "Question Representation Construction", "start_pos": 91, "end_pos": 127, "type": "TASK", "confidence": 0.8466780384381613}, {"text": "Question Realization", "start_pos": 133, "end_pos": 153, "type": "TASK", "confidence": 0.7939541041851044}]}, {"text": "In Target Selection, important sentences and words within those sentences are identified.", "labels": [], "entities": []}, {"text": "During Question Representation Construction, suitable question-type and syntactic form are determined based on the characteristics of the sentence at hand and the words it contains.", "labels": [], "entities": [{"text": "Question Representation Construction", "start_pos": 7, "end_pos": 43, "type": "TASK", "confidence": 0.8265018661816915}]}, {"text": "An example of this can be seen in who define rules based on the discourse connectives in a sentence to decide which question-type is most appropriate.", "labels": [], "entities": []}, {"text": "In the Question Realization step, the final surface form of the question is created.", "labels": [], "entities": [{"text": "Question Realization", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.8070648312568665}]}, {"text": "It is common for QG systems to use a combination of semantic pattern matching, syntactic features, and template methods to create questions.", "labels": [], "entities": [{"text": "semantic pattern matching", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.6907656590143839}]}, {"text": "Typically these systems look for patterns of syntax, keywords, or semantic roles that appear in the input sentence.", "labels": [], "entities": []}, {"text": "Then they use these patterns to choose an appropriate question template, or use syntactic features to perform manipulations on the sentence to produce a question.", "labels": [], "entities": []}, {"text": "These rule-based systems have some strengths over Neural Network models: they are easier to interpret and allow developers greater control over model behavior.", "labels": [], "entities": []}, {"text": "Furthermore, they typically require less data to develop than a complex Neural Network might need to achieve a similar level of performance.", "labels": [], "entities": []}, {"text": "However, rule-based systems have some weaknesses as well.", "labels": [], "entities": []}, {"text": "They tend to be laborious to develop, or domain specific.", "labels": [], "entities": []}, {"text": "For example the system developed by relies on the presence of one of a set of 239 modal verbs in a sentence, and use 3000 keywords provided by the glossary of a Biology textbook and a test-prep study guide.", "labels": [], "entities": []}, {"text": "The system described in uses roughly 350 hand-crafted rules.", "labels": [], "entities": []}, {"text": "Furthermore, these systems rely heavily on syntactic parsers, and may struggle to recover from parser inaccuracies.", "labels": [], "entities": []}, {"text": "# Sentence S1 The character of midna has the most voice acting -her on-screen dialog is often accompanied by a babble of pseudo-speech , which was produced by scrambling the phonemes of english phrases [ better source needed ] sampled by japanese voice actress akiko komoto.", "labels": [], "entities": []}, {"text": "Gold Standard Q1 which person has the most spoken dialogue in the game?", "labels": [], "entities": [{"text": "Gold Standard Q1", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.5977054437001547}]}, {"text": "Q2 who provided the basis for midna's voice?", "labels": [], "entities": []}, {"text": "Q3 what country does akiko komoto come from?", "labels": [], "entities": []}, {"text": "Baseline Q4 what is her ? Our Model: FocusCR Q5 what character has the most voice acting in english?", "labels": [], "entities": [{"text": "FocusCR Q5", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.8518197238445282}]}, {"text": "Q6 what is the name of the japanese voice actress?", "labels": [], "entities": []}, {"text": "Q7 what is the nationality of akiko komoto?: Sentence and associated questions generated from the baseline and our best model.", "labels": [], "entities": []}, {"text": "Among many different approaches to question generation, our work is most similar to recent work applying neural network models to the task of generating short answer factoid questions for SQUAD (.", "labels": [], "entities": [{"text": "question generation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8309992849826813}]}, {"text": "However these previous models have several limitations.", "labels": [], "entities": []}, {"text": "As illustrated in, the SQUAD corpus () provides multiple gold standard references for each sentence (Q1, Q2, and Q3), but previous work to date can only generate one question for each sentence as represented by the baseline model (Q4), whereas our model can generate multiple questions as shown in.", "labels": [], "entities": [{"text": "SQUAD corpus", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.7986555993556976}]}, {"text": "In Section 2, we present our novel model that introduces additional token supervision representing features of the text as well as an additional lower dimensional word embedding.", "labels": [], "entities": []}, {"text": "The features include a Named Entity Recognition (NER) feature, a word case feature, and a special answer signaling feature.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.6706661631663641}, {"text": "answer signaling", "start_pos": 98, "end_pos": 114, "type": "TASK", "confidence": 0.7274785786867142}]}, {"text": "The answer signaling feature allows our model to generate multiple questions for each sentence, illustrated with Q5, Q6 and Q7 in.", "labels": [], "entities": [{"text": "answer signaling", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7684915661811829}]}, {"text": "We also introduce a coreference resolution model and supplement the sentence input representation with resolved coreferences, as well as a copying mechanism.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.8952315449714661}]}, {"text": "Section 3 presents an evaluation of the final model on the benchmark SQuAD testset using automatic evaluation metrics and shows that it achieves state of the art results of).", "labels": [], "entities": [{"text": "SQuAD testset", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.7949992418289185}]}, {"text": "To our knowledge this model outperforms all previously published results by a significant margin.", "labels": [], "entities": []}, {"text": "A human evaluation also shows that the introduced features and answer-specific sentence embedding improve the quality of the generated questions.", "labels": [], "entities": []}, {"text": "We delay a more detailed review of previous work to Section 4 and conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments exploring the effects of each of our model enhancements and train and evaluate all the models using the SQuAD dataset (  Model Implementation.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 127, "end_pos": 140, "type": "DATASET", "confidence": 0.7895064353942871}]}, {"text": "Our model is implemented using PyTorch 1 and OpenNMT-py 2 which is a PyTorch port of OpenNMT().", "labels": [], "entities": []}, {"text": "The encoder, decoder, and sentence encoder are multi-layer RNNs, each with two layers.", "labels": [], "entities": []}, {"text": "We use bi-directional LSTM cells with 640 units.", "labels": [], "entities": []}, {"text": "The model is trained using Dropout () of 0.3 between RNN layers.", "labels": [], "entities": [{"text": "Dropout", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9560450315475464}]}, {"text": "Word embeddings are initialized using Glove 300 dimensional word vectors () that are not updated during training.", "labels": [], "entities": []}, {"text": "The sentence encoder is initialized using the pre-training process described in Section 2.2.", "labels": [], "entities": []}, {"text": "All other model parameters are initialized using Glorot initialization.", "labels": [], "entities": []}, {"text": "The model parameters are optimized using Stochastic Gradient Descent with mini-batches of size 64.", "labels": [], "entities": []}, {"text": "Beam search with five beams is used during inference and OOV words are replaced using the token of highest attention weight in the source sentence.", "labels": [], "entities": [{"text": "OOV", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.953148365020752}]}, {"text": "We tune our model with the development dataset and select the model of lowest Perplexity to evaluate on the test dataset.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.984257161617279}]}, {"text": "We compare our system's results to that of several other QG systems.", "labels": [], "entities": []}, {"text": "The rows of with labels H&S, Yuan, Du, and S&X refer to the models presented in Heilman and Smith (2010a);;, and, respectfully.", "labels": [], "entities": []}, {"text": "Please refer to Section 4 Related Work for further details on each of these systems.", "labels": [], "entities": []}, {"text": "The results of the H&S system are reported in this work for the sake of comparison.", "labels": [], "entities": [{"text": "H&S", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.5893444319566091}]}, {"text": "The actual experiments were performed by who describe the specific configuration of H&S in greater detail.", "labels": [], "entities": []}, {"text": "We use BLEU score () as an automatic evaluation metric and compare directly to other work.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.984201043844223}]}, {"text": "BLEU measures the similarity between a generated text called a candidate and a set of human written texts called a reference set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9832813143730164}]}, {"text": "The score is calculated by comparing the n-grams of the candidate with the n-grams of the reference texts and then counting the number of matches.", "labels": [], "entities": []}, {"text": "Unfortunately there are inconsistencies in the method by: System performance in automatic evaluation.", "labels": [], "entities": []}, {"text": "First, when calculating BLEU fora given hypothesis question, some publications have used a reference set containing all the ground-truth questions corresponding to the sentence from which the hypothesis was generated.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9962783455848694}]}, {"text": "shows our model's results compared to previous work using this setup of BLEU and the same partitioning of the SQuAD dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9702275991439819}, {"text": "SQuAD dataset", "start_pos": 110, "end_pos": 123, "type": "DATASET", "confidence": 0.8831892907619476}]}, {"text": "Each of our models outperform previously published results in each of the BLEU, METEOR, and ROUGE categories by a significant margin.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9985870122909546}, {"text": "METEOR", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.8848866820335388}, {"text": "ROUGE", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.9821251630783081}]}, {"text": "FocusCR is the second highest performing system and achieves an impressive BLEU 4 score of 19.86, which greatly improves on the third highest BLEU 4 score of 14.37 belonging to S&X.", "labels": [], "entities": [{"text": "FocusCR", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9605094194412231}, {"text": "BLEU 4 score", "start_pos": 75, "end_pos": 87, "type": "METRIC", "confidence": 0.9855584899584452}, {"text": "BLEU 4 score", "start_pos": 142, "end_pos": 154, "type": "METRIC", "confidence": 0.9847123026847839}, {"text": "S&X", "start_pos": 177, "end_pos": 180, "type": "TASK", "confidence": 0.4694741765658061}]}, {"text": "Focus gets a BLEU 4 score of 19.98 and is the best performing system overall.", "labels": [], "entities": [{"text": "BLEU 4 score", "start_pos": 13, "end_pos": 25, "type": "METRIC", "confidence": 0.9805522759755453}]}, {"text": "In the second setup, fora given hypothesis question, used a reference set containing only a single ground-truth question that corresponds to the same sentence and answer span from which the hypothesis was generated.", "labels": [], "entities": []}, {"text": "We use this setup to evaluate our Focus and FocusCR models.", "labels": [], "entities": [{"text": "FocusCR", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.7374340295791626}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Here, Focus and FocusCR are the same models as shown in, with the only difference being the setting under which they are evaluated.", "labels": [], "entities": [{"text": "Focus", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.916198194026947}, {"text": "FocusCR", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.6940151453018188}]}, {"text": "Again, FocusCR achieves the second highest score and Focus gets the highest BLEU 4 score at 14.39.", "labels": [], "entities": [{"text": "FocusCR", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.8818773031234741}, {"text": "Focus", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.8613540530204773}, {"text": "BLEU 4 score", "start_pos": 76, "end_pos": 88, "type": "METRIC", "confidence": 0.9853039781252543}]}, {"text": "While the datasets in aggregate are the same, our partitioning of training, development, and test datasets is different from that of.", "labels": [], "entities": []}, {"text": "We perform ablation experiments to study the effects of each feature incorporated into the model.", "labels": [], "entities": []}, {"text": "The results of these experiments can be seen in  We also examine the effect of pre-training the sentence encoder as described in Section 2.2.", "labels": [], "entities": []}, {"text": "In, the Focus and FocusCR models use a pre-trained sentence encoder.", "labels": [], "entities": []}, {"text": "The sentence encoder used by FocusCR-npt is not pre-trained.", "labels": [], "entities": [{"text": "FocusCR-npt", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.9487404227256775}]}, {"text": "We find that the pre-training has a positive effect on BLEU scores with FocusCR-npt getting BLEU 13.99, compared to the FocusCR getting BLEU 14.16.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9980552196502686}, {"text": "FocusCR-npt", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.7024416327476501}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9771178960800171}, {"text": "FocusCR", "start_pos": 120, "end_pos": 127, "type": "DATASET", "confidence": 0.8590927720069885}, {"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9659681916236877}]}, {"text": "suggests that the coreference mechanism actually hurts performance as measured by BLEU but the example shown in and the additional examples shown in suggest that it is very effective.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9957736134529114}]}, {"text": "provides examples of the effect of the copy mechanism.", "labels": [], "entities": []}, {"text": "Again, as with coreference, it is easy to seethe benefits of the copying mechanism qualititively.", "labels": [], "entities": []}, {"text": "For example, in Q14 and Q15 the model can effectively copy the right date into the question.", "labels": [], "entities": [{"text": "Q14", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.8852399587631226}, {"text": "Q15", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.7113853693008423}]}, {"text": "In Q17, without copying, the model finds an entity von neumann who is associated with teaching and university in the model and uses that entity to generate the question rather than west the entity in context.", "labels": [], "entities": []}, {"text": "We are interested in how the new features effect the quantity of unique questions produced by our model.", "labels": [], "entities": []}, {"text": "Therefore, we counted the number of unique questions output by the model when considering the entire testing set as inputs.", "labels": [], "entities": []}, {"text": "Here, we measure similarity using a strict character match comparison.", "labels": [], "entities": [{"text": "similarity", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9569167494773865}]}, {"text": "We can see that the FocusCR model produces 10,194 unique questions, which is a 55%  We perform human evaluation using crowd workers on Amazon Mechanical Turk 3 . The Turkers rate a pool of questions constructed by randomly selecting questions and their associated text passages from the test set.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk 3", "start_pos": 135, "end_pos": 159, "type": "DATASET", "confidence": 0.9704571217298508}]}, {"text": "We select 114 questions each from the test dataset, the questions generated by the baseline model, and the questions generated by our FocusCR model.", "labels": [], "entities": [{"text": "FocusCR model", "start_pos": 134, "end_pos": 147, "type": "DATASET", "confidence": 0.9146425426006317}]}, {"text": "The questions are selected such that they all correspond to the same declarative sentence.", "labels": [], "entities": []}, {"text": "In other words, we construct a set of 114 tuples where each tuple consists of one text passage, two model generated questions, and one human authored question.", "labels": [], "entities": []}, {"text": "We use a qualification criteria to restrict the participation of Turkers in our evaluation study.", "labels": [], "entities": []}, {"text": "The Turkers must have above 95% HIT approval rate with at least 500 HITs previously approved.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.7929823398590088}, {"text": "HIT approval rate", "start_pos": 32, "end_pos": 49, "type": "METRIC", "confidence": 0.9144435922304789}]}, {"text": "Furthermore, Turkers are required to be located in English speaking countries.", "labels": [], "entities": []}, {"text": "Turkers recieved $0.1 for completing each HIT.", "labels": [], "entities": [{"text": "HIT", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.7071900963783264}]}, {"text": "We closely follow the experiment design described by, who instruct Turkers to produce a single five-point quality rating per question.", "labels": [], "entities": []}, {"text": "They provide Turkers with the following four reasons to downgrade a question: (Un)grammaticality, Incorrect Information, Vagueness, and Awkwardness.", "labels": [], "entities": [{"text": "Incorrect", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.932378888130188}, {"text": "Awkwardness", "start_pos": 136, "end_pos": 147, "type": "METRIC", "confidence": 0.9958859086036682}]}, {"text": "In our evaluation study, we use four categories of evaluation that resemble these criteria.", "labels": [], "entities": []}, {"text": "Turkers are asked to rate each question across 3 www.mturk.com four categories: Grammaticality, Correct Information, Answerability, and Naturalness.", "labels": [], "entities": [{"text": "Answerability", "start_pos": 117, "end_pos": 130, "type": "TASK", "confidence": 0.9017350077629089}]}, {"text": "Grammaticality encompasses things like adherence to rules of syntax, use of the wrong wh-word, verb tense consistency, and overall legitimacy as an English sentence.", "labels": [], "entities": []}, {"text": "The Correct Information category considers whether or not the question is related to the text passage (e.g., asking about Madonna when the passage is about Beyonce), implies something that is obviously incorrect, or contradicts information given in the text passage.", "labels": [], "entities": []}, {"text": "The Answerability category reflects how much of the information required to correctly answer the question is contained within the text passage.", "labels": [], "entities": [{"text": "Answerability", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9581375122070312}]}, {"text": "Also, it considers whether or not the question has a clear answer, or is too vague (e.g., \"What is it?\").", "labels": [], "entities": []}, {"text": "The Naturalness category reflects how natural the question reads and considers whether or not it has some awkward phrasing.", "labels": [], "entities": []}, {"text": "The Naturalness category also encompasses any other problems in the question that do not fall in the previous categories.", "labels": [], "entities": []}, {"text": "During evaluation, the Turker is presented with the text passage and its three corresponding questions in scrambled order.", "labels": [], "entities": []}, {"text": "They are asked to give a rating from worst (1) to best (5) in each category for each question.", "labels": [], "entities": []}, {"text": "Each HIT contains three text passages and a total of nine questions.", "labels": [], "entities": []}, {"text": "Each HIT is assigned to three Turkers resulting in three ratings per question.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.4857989251613617}]}, {"text": "shows an average of the ratings assigned by the Turkers in each category.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9103848338127136}]}, {"text": "Answerability is the category in which the FocusCR model has the greatest improvement over the Baseline.", "labels": [], "entities": [{"text": "Answerability", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9257896542549133}, {"text": "FocusCR", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.7886097431182861}]}, {"text": "In this category, FocusCR receives an average rating of 4.13, compared to the baseline's average rating of 3.73.", "labels": [], "entities": [{"text": "FocusCR", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.8736822009086609}]}, {"text": "FocusCR also outperforms the Baseline model in the Correct Information category with average ratings 4.13 and 3.78, respectfully.", "labels": [], "entities": [{"text": "FocusCR", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.919624924659729}]}, {"text": "In the Grammaticality and Naturalness categories the Baseline model has average ratings of 4.23 and 4.10, respectfully.", "labels": [], "entities": []}, {"text": "The FocusCR model has average ratings of 4.20 and 4.09 in the Grammaticality and Naturalness categories.", "labels": [], "entities": []}, {"text": "The human authored questions outperform both models by a significant margin in all categories.", "labels": [], "entities": []}, {"text": "We note that there is only a slight difference between ratings achieved by the Baseline and FocusCR models in the Grammaticality and Naturalness categories.", "labels": [], "entities": [{"text": "FocusCR", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.8585820198059082}]}, {"text": "Yet, in both these categories the Baseline model slightly outperforms FocusCR.", "labels": [], "entities": [{"text": "FocusCR", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.9327036738395691}]}, {"text": "We suspect this is due to the brevity and generality of questions produced by the Baseline model.", "labels": [], "entities": []}, {"text": "In contrast, FocusCR produces longer sentences with more information content and, at times, increasingly complex sentence structure.", "labels": [], "entities": [{"text": "FocusCR", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.8824127316474915}]}, {"text": "Next, we observe that the average rating of the human-authored questions are surprisingly low across all categories, but particularly in Naturalness with a rating of 4.36.", "labels": [], "entities": []}, {"text": "We attribute this to the crowd-sourcing methodology used to create the original SQuAD dataset.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.8640488982200623}]}, {"text": "Nevertheless, we hypothesize that the average ratings of the gold questions will increase with larger sample sizes in subsequent human evaluation studies.", "labels": [], "entities": []}, {"text": "Inter-rater agreement was measured by comparing the Turkers' ratings to those of an expert annotator who is a native English speaking graduate student in Computational Linguistics.", "labels": [], "entities": [{"text": "agreement", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.906786322593689}]}, {"text": "The expert annotator rated a random sample of 60 questions using a private version of the HIT created on Mechanical Turk.", "labels": [], "entities": [{"text": "HIT created on Mechanical Turk", "start_pos": 90, "end_pos": 120, "type": "DATASET", "confidence": 0.7131030261516571}]}, {"text": "Then, the arithmetic mean of the three Turker ratings was calculated for each question and category of evaluation.", "labels": [], "entities": [{"text": "arithmetic mean", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9626071155071259}, {"text": "Turker", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.8290808796882629}]}, {"text": "The Pearson correlation coefficient between the expert annotator's rating and the means of the Turker ratings was r = 0.47 for the Correct Information category, r = 0.38 for the Answerability category, r = 0.20 for Grammaticality, and r = 0.32 for Naturalness.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 4, "end_pos": 35, "type": "METRIC", "confidence": 0.9678619503974915}]}, {"text": "The significance of each correlation was calculated using a two-tailed test that resulted in p < 0.01 for each category.", "labels": [], "entities": []}, {"text": "We observe a positive correlation between the expert annotator and the Turker ratings in each category, although some of the the correlation strengths are less than ideal, particularly in the Grammaticality category.", "labels": [], "entities": [{"text": "Turker", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.563741147518158}]}, {"text": "The consistent positive correlation across each category and their statistical significance provide evidence that the rating scheme is well defined, and that the Turkers are able to judge the quality of questions with relative reliability.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 162, "end_pos": 169, "type": "DATASET", "confidence": 0.7511181235313416}]}], "tableCaptions": [{"text": " Table 5: System performance in automatic evalu- ation.", "labels": [], "entities": []}, {"text": " Table 6: BLEU-4 scores when using answer- specific ground-truth questions as reference texts.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.999204695224762}]}, {"text": " Table 8: Results of ablation test.", "labels": [], "entities": [{"text": "ablation", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9658615589141846}]}, {"text": " Table 9: Human Evaluation Results", "labels": [], "entities": []}]}