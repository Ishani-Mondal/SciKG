{"title": [{"text": "Predicting and Explaining Human Semantic Search in a Cognitive Model", "labels": [], "entities": [{"text": "Predicting and Explaining Human Semantic Search", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.843559185663859}]}], "abstractContent": [{"text": "Recent work has attempted to characterize the structure of semantic memory and the search algorithms which, together, best approximate human patterns of search revealed in a semantic fluency task.", "labels": [], "entities": []}, {"text": "There area number of models that seek to capture semantic search processes over networks , but they vary in the cognitive plau-sibility of their implementation.", "labels": [], "entities": []}, {"text": "Existing work has also neglected to consider the constraints that the incremental process of language acquisition must place on the structure of semantic memory.", "labels": [], "entities": []}, {"text": "Here we present a model that incrementally updates a semantic network, with limited computational steps, and replicates many patterns found inhuman semantic fluency using a simple random walk.", "labels": [], "entities": []}, {"text": "We also perform thorough analyses showing that a combination of both structural and semantic features are correlated with human performance patterns.", "labels": [], "entities": []}, {"text": "1 Human Semantic Processing The study of human semantic memory-word meanings, their relations, and their storage-is challenging due to the complexity of factors involved.", "labels": [], "entities": []}, {"text": "Finding (1) the right representation for word meanings and their relations, (2) the mechanism responsible for learning the representation, (3) the appropriate search algorithm to efficiently retrieve information from semantic memory, and (4) the suitable empirical data to evaluate the proposed representations and algorithms is a difficult task.", "labels": [], "entities": []}, {"text": "Previous research has extensively explored each of these (e.g., Collins and Loftus, 1975; Steyvers and Tenenbaum, 2005; Griffiths et al., 2007).", "labels": [], "entities": []}, {"text": "Psychologists frequently use a task known as semantic fluency (or verbal fluency) to examine human semantic representation and processing (Troyer et al., 1997; Ardila et al., 2006).", "labels": [], "entities": [{"text": "human semantic representation and processing", "start_pos": 93, "end_pos": 137, "type": "TASK", "confidence": 0.7719565272331238}]}, {"text": "Participants are asked to produce as many words as they can from a given category (e.g., animal) in a fixed amount of time (e.g., three minutes).", "labels": [], "entities": []}, {"text": "The resulting data-which words people recall and in what order-can shed light on how people represent word meanings and their relationships, and how they search such semantic information.", "labels": [], "entities": []}, {"text": "For example , Hills et al.", "labels": [], "entities": []}, {"text": "(2012) found that participants tend to reply in semantically-related bursts of words-e.g., they recall words from the pet subcategory of animals (dog, cat) then switch to a different subcategory, such as African animals (lion, zebra), etc.-indicating that people tend to follow a strategy of exploiting a semantically-related patch of words, then exploring to find anew patch, much like animals foraging in their environment.", "labels": [], "entities": []}, {"text": "Recent work has investigated the properties of semantic representations and processing algorithms that can account for this type of behavior in the semantic fluency task.", "labels": [], "entities": [{"text": "semantic fluency task", "start_pos": 148, "end_pos": 169, "type": "TASK", "confidence": 0.7434297899405161}]}, {"text": "Different researchers have found that a match to human behavior can be achieved in either of two ways: (a) using a simple (vector-based) semantic representation in combination with an informed, two-stage algorithm to exploit and explore the space (Hills et al., 2012); or (b) creating a richer representation-structured as a semantic network-and using a simple random walk to access it (Abbott et al., 2015; Ne-matzadeh et al., 2016).", "labels": [], "entities": []}, {"text": "These findings suggest that the choice of representation and search algorithm are interdependent, such that the same empirical data can be replicated through different combinations of representation and algorithm that make different trade-offs on the locus of complexity (Abbott et al., 2015).", "labels": [], "entities": []}, {"text": "However, if both combinations account for the 35", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In this section, we explain the details of the semantic fluency experiment as well as the seman- tic representation and search algorithm used in our simulations.", "labels": [], "entities": []}, {"text": "All of the code and data necessary to reproduce our experiments are available at https: //github.com/FilipMiscevic/random_walk.", "labels": [], "entities": []}, {"text": "We evaluate our simulations using data from a semantic fluency experiment in which participants were tasked with naming as many animals as they can in three minutes (.", "labels": [], "entities": []}, {"text": "inferred that the recalled words (e.g., dog, cat, lion, zebra) form semanticallyrelated categories or \"patches\", based on their inter-item retrieval times (IRT)-the time elapsed between the naming of two sequential items that have not previously been recalled.", "labels": [], "entities": [{"text": "inter-item retrieval times (IRT)-", "start_pos": 128, "end_pos": 161, "type": "METRIC", "confidence": 0.722811684012413}]}, {"text": "They find that the IRT increases as search within a semanticallyrelated category progresses.", "labels": [], "entities": [{"text": "IRT", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.7516675591468811}]}, {"text": "A switch into a different semantic category occurs when the IRT exceeds the participant's average IRT across the entire trial.", "labels": [], "entities": [{"text": "IRT", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9027501344680786}, {"text": "IRT", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9846182465553284}]}, {"text": "The IRT then decreases and the pattern begins again (see).", "labels": [], "entities": [{"text": "IRT", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9232412576675415}]}, {"text": "This result shows that participants exhibit different behavior when recalling words from within a semantic category compared to switching into anew semantic category.", "labels": [], "entities": []}, {"text": "argue that this pattern is a consequence of an informed two-stage search process: local cues, such as similarity to the most recent response, are used to search within patches, and global cues, such as the overall frequency of a word, are used to switch into new patches.", "labels": [], "entities": []}, {"text": "Here we replicate previous results that demonstrate that the IRT pattern can be predicted by a simple search given structured representations ().", "labels": [], "entities": [{"text": "IRT pattern", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.8868931233882904}]}, {"text": "In addition, we show that this process matches other patterns observed in the semantic fluency experiment (Hills et al., 2015).", "labels": [], "entities": []}, {"text": "Logistic classifier models were trained on a set of Batch and Incremental networks.", "labels": [], "entities": []}, {"text": "During training, we ensure an equal representation of networks that adhere to and do not adhere to MVT.", "labels": [], "entities": []}, {"text": "This is a binary condition satisfied according to the criteria explained in Section 4.1.", "labels": [], "entities": []}, {"text": "Networks were first generated across the entire parameter space of the similarity thresholds (i.e., all combinations of \u03c1 and \u03c1 animal ranging from 0 to 1, in increments of 0.1).", "labels": [], "entities": []}, {"text": "We excluded networks where the number of nodes reachable by the starting word 'animal' was smaller than 30, as they would not be able to produce as many words as human participants did (37 \u00b1 5) (.", "labels": [], "entities": []}, {"text": "Since the number of non-IRT producing networks outnumbered the IRT producing networks, we uniformly sampled the parameter space in which IRT patternproducing networks occurred so that the number of each would be equal.", "labels": [], "entities": []}, {"text": "Using this procedure, 42 Batch and 56 Incremental networks were generated.", "labels": [], "entities": []}, {"text": "In each case, exactly half of the networks produce the IRT pattern consistent with MVT.", "labels": [], "entities": [{"text": "IRT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.7281326651573181}]}, {"text": "For each set of Batch and Incremental networks, we examine which features best predict the human data by building and evaluating logistic regression models for all combinations of features.", "labels": [], "entities": []}, {"text": "Model selection was performed in two steps.", "labels": [], "entities": [{"text": "Model selection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.749828964471817}]}, {"text": "First, the models with the highest stratified-3-fold (SKF) cross-validation score were taken.", "labels": [], "entities": [{"text": "stratified-3-fold (SKF) cross-validation score", "start_pos": 35, "end_pos": 81, "type": "METRIC", "confidence": 0.6684756875038147}]}, {"text": "From these, the model with the fewest number of features was selected.", "labels": [], "entities": []}, {"text": "shows the features that appeared in the logistic regression model that achieved the best SKF cross-validation score for each of the types of networks.", "labels": [], "entities": []}, {"text": "Since each feature was standardized (with mean = 0 and variance = 1), the magnitude of the coefficients can be interpreted directly.", "labels": [], "entities": []}, {"text": "We note that small-worldness (\u03c3) and weighted Fscore are influential predictors for both Batch and Incremental networks.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.803117573261261}]}, {"text": "In both models, weighted F-score is the most influential predictor.", "labels": [], "entities": [{"text": "F-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9635935425758362}]}, {"text": "Although \u03c3 is the least influential predictor, we find it significant that it is a shared predictor for both networks.", "labels": [], "entities": []}, {"text": "Structural properties relating to the number of edges (|E|, sparsity) as well as clustering coefficient (C, \u03b3), are structural properties that have been previously characterized in semantic networks.", "labels": [], "entities": [{"text": "sparsity", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9631868600845337}, {"text": "clustering coefficient (C, \u03b3)", "start_pos": 81, "end_pos": 110, "type": "METRIC", "confidence": 0.8851839900016785}]}], "tableCaptions": [{"text": " Table 1: Features used to train the logistic regression mod-", "labels": [], "entities": []}]}